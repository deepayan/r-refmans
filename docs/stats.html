<!DOCTYPE html><html><head><title>Help for package stats</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {stats}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.checkMFClasses'><p>Functions to Check the Type of Variables passed to Model Frames</p></a></li>
<li><a href='#acf'><p>Auto- and Cross- Covariance and -Correlation Function Estimation</p></a></li>
<li><a href='#acf2AR'><p>Compute an AR Process Exactly Fitting an ACF</p></a></li>
<li><a href='#add1'><p>Add or Drop All Possible Single Terms to a Model</p></a></li>
<li><a href='#addmargins'><p>Puts Arbitrary Margins on Multidimensional Tables or Arrays</p></a></li>
<li><a href='#aggregate'><p>Compute Summary Statistics of Data Subsets</p></a></li>
<li><a href='#AIC'><p>Akaike's An Information Criterion</p></a></li>
<li><a href='#alias'><p>Find Aliases (Dependencies) in a Model</p></a></li>
<li><a href='#anova'><p>ANOVA Tables</p></a></li>
<li><a href='#anova.glm'><p>Analysis of Deviance for Generalized Linear Model Fits</p></a></li>
<li><a href='#anova.lm'><p>ANOVA for Linear Model Fits</p></a></li>
<li><a href='#anova.mlm'><p>Comparisons between Multivariate Linear Models</p></a></li>
<li><a href='#ansari.test'><p>Ansari-Bradley Test</p></a></li>
<li><a href='#aov'><p>Fit an Analysis of Variance Model</p></a></li>
<li><a href='#approxfun'><p>Interpolation Functions</p></a></li>
<li><a href='#ar'><p>Fit Autoregressive Models to Time Series</p></a></li>
<li><a href='#ar.ols'><p>Fit Autoregressive Models to Time Series by OLS</p></a></li>
<li><a href='#arima'><p>ARIMA Modelling of Time Series</p></a></li>
<li><a href='#arima.sim'><p>Simulate from an ARIMA Model</p></a></li>
<li><a href='#arima0'><p>ARIMA Modelling of Time Series &ndash; Preliminary Version</p></a></li>
<li><a href='#ARMAacf'><p>Compute Theoretical ACF for an ARMA Process</p></a></li>
<li><a href='#ARMAtoMA'><p>Convert ARMA Process to Infinite MA Process</p></a></li>
<li><a href='#as.hclust'><p>Convert Objects to Class <code>"hclust"</code></p></a></li>
<li><a href='#asOneSidedFormula'><p>Convert to One-Sided Formula</p></a></li>
<li><a href='#ave'><p>Group Averages Over Level Combinations of Factors</p></a></li>
<li><a href='#bandwidth'><p>Bandwidth Selectors for Kernel Density Estimation</p></a></li>
<li><a href='#bartlett.test'><p>Bartlett Test of Homogeneity of Variances</p></a></li>
<li><a href='#Beta'><p>The Beta Distribution</p></a></li>
<li><a href='#binom.test'><p>Exact Binomial Test</p></a></li>
<li><a href='#Binomial'><p>The Binomial Distribution</p></a></li>
<li><a href='#biplot'><p>Biplot of Multivariate Data</p></a></li>
<li><a href='#biplot.princomp'>
<p>Biplot for Principal Components</p></a></li>
<li><a href='#birthday'><p>Probability of coincidences</p></a></li>
<li><a href='#Box.test'><p>Box-Pierce and Ljung-Box Tests</p></a></li>
<li><a href='#C'><p>Sets Contrasts for a Factor</p></a></li>
<li><a href='#cancor'><p>Canonical Correlations</p></a></li>
<li><a href='#case+variable.names'><p>Case and Variable Names of Fitted Models</p></a></li>
<li><a href='#Cauchy'><p>The Cauchy Distribution</p></a></li>
<li><a href='#chisq.test'><p>Pearson's Chi-squared Test for Count Data</p></a></li>
<li><a href='#Chisquare'><p>The (non-central) Chi-Squared Distribution</p></a></li>
<li><a href='#cmdscale'><p>Classical (Metric) Multidimensional Scaling</p></a></li>
<li><a href='#coef'><p>Extract Model Coefficients</p></a></li>
<li><a href='#complete.cases'><p>Find Complete Cases</p></a></li>
<li><a href='#confint'><p>Confidence Intervals for Model Parameters</p></a></li>
<li><a href='#constrOptim'><p>Linearly Constrained Optimization</p></a></li>
<li><a href='#contrast'><p>(Possibly Sparse) Contrast Matrices</p></a></li>
<li><a href='#contrasts'><p>Get and Set Contrast Matrices</p></a></li>
<li><a href='#convolve'><p>Convolution of Sequences via FFT</p></a></li>
<li><a href='#cophenetic'><p>Cophenetic Distances for a Hierarchical Clustering</p></a></li>
<li><a href='#cor'><p>Correlation, Variance and Covariance (Matrices)</p></a></li>
<li><a href='#cor.test'><p>Test for Association/Correlation Between Paired Samples</p></a></li>
<li><a href='#cov.wt'><p>Weighted Covariance Matrices</p></a></li>
<li><a href='#cpgram'>
<p>Plot Cumulative Periodogram</p></a></li>
<li><a href='#cutree'><p>Cut a Tree into Groups of Data</p></a></li>
<li><a href='#decompose'>
<p>Classical Seasonal Decomposition by Moving Averages</p></a></li>
<li><a href='#delete.response'><p>Modify Terms Objects</p></a></li>
<li><a href='#dendrapply'><p>Apply a Function to All Nodes of a Dendrogram</p></a></li>
<li><a href='#dendrogram'><p>General Tree Structures</p></a></li>
<li><a href='#density'><p>Kernel Density Estimation</p></a></li>
<li><a href='#deriv'><p>Symbolic and Algorithmic Derivatives of Simple Expressions</p></a></li>
<li><a href='#deviance'><p>Model Deviance</p></a></li>
<li><a href='#df.residual'><p>Residual Degrees-of-Freedom</p></a></li>
<li><a href='#diffinv'><p>Discrete Integration: Inverse of Differencing</p></a></li>
<li><a href='#dist'><p>Distance Matrix Computation</p></a></li>
<li><a href='#Distributions'><p>Distributions in the stats package</p></a></li>
<li><a href='#dummy.coef'><p>Extract Coefficients in Original Coding</p></a></li>
<li><a href='#ecdf'><p>Empirical Cumulative Distribution Function</p></a></li>
<li><a href='#eff.aovlist'><p>Compute Efficiencies of Multistratum Analysis of Variance</p></a></li>
<li><a href='#effects'><p>Effects from Fitted Model</p></a></li>
<li><a href='#embed'><p>Embedding a Time Series</p></a></li>
<li><a href='#expand.model.frame'><p>Add new variables to a model frame</p></a></li>
<li><a href='#Exponential'><p>The Exponential Distribution</p></a></li>
<li><a href='#extractAIC'><p>Extract AIC from a Fitted Model</p></a></li>
<li><a href='#factanal'><p>Factor Analysis</p></a></li>
<li><a href='#factor.scope'><p>Compute Allowed Changes in Adding to or Dropping from a Formula</p></a></li>
<li><a href='#family'><p>Family Objects for Models</p></a></li>
<li><a href='#FDist'><p>The F Distribution</p></a></li>
<li><a href='#fft'><p>Fast Discrete Fourier Transform (FFT)</p></a></li>
<li><a href='#filter'><p>Linear Filtering on a Time Series</p></a></li>
<li><a href='#fisher.test'><p>Fisher's Exact Test for Count Data</p></a></li>
<li><a href='#fitted'><p>Extract Model Fitted Values</p></a></li>
<li><a href='#fivenum'><p>Tukey Five-Number Summaries</p></a></li>
<li><a href='#fligner.test'><p>Fligner-Killeen Test of Homogeneity of Variances</p></a></li>
<li><a href='#formula'><p>Model Formulae</p></a></li>
<li><a href='#formula.nls'><p>Extract Model Formula from <code>nls</code> Object</p></a></li>
<li><a href='#friedman.test'><p>Friedman Rank Sum Test</p></a></li>
<li><a href='#ftable'><p>Flat Contingency Tables</p></a></li>
<li><a href='#ftable.formula'><p>Formula Notation for Flat Contingency Tables</p></a></li>
<li><a href='#GammaDist'><p>The Gamma Distribution</p></a></li>
<li><a href='#Geometric'><p>The Geometric Distribution</p></a></li>
<li><a href='#getInitial'><p>Get Initial Parameter Estimates</p></a></li>
<li><a href='#glm'><p>Fitting Generalized Linear Models</p></a></li>
<li><a href='#glm.control'><p>Auxiliary for Controlling GLM Fitting</p></a></li>
<li><a href='#glm.summaries'><p>Accessing Generalized Linear Model Fits</p></a></li>
<li><a href='#hclust'><p>Hierarchical Clustering</p></a></li>
<li><a href='#heatmap'><p> Draw a Heat Map</p></a></li>
<li><a href='#HoltWinters'><p>Holt-Winters Filtering</p></a></li>
<li><a href='#Hypergeometric'><p>The Hypergeometric Distribution</p></a></li>
<li><a href='#identify.hclust'><p>Identify Clusters in a Dendrogram</p></a></li>
<li><a href='#influence.measures'><p>Regression Deletion Diagnostics</p></a></li>
<li><a href='#integrate'><p>Integration of One-Dimensional Functions</p></a></li>
<li><a href='#interaction.plot'><p>Two-way Interaction Plot</p></a></li>
<li><a href='#IQR'><p>The Interquartile Range</p></a></li>
<li><a href='#is.empty.model'><p>Test if a Model's Formula is Empty</p></a></li>
<li><a href='#isoreg'><p>Isotonic / Monotone Regression</p></a></li>
<li><a href='#KalmanLike'><p>Kalman Filtering</p></a></li>
<li><a href='#kernapply'><p>Apply Smoothing Kernel</p></a></li>
<li><a href='#kernel'><p>Smoothing Kernel Objects</p></a></li>
<li><a href='#kmeans'>
<p>K-Means Clustering</p></a></li>
<li><a href='#kruskal.test'><p>Kruskal-Wallis Rank Sum Test</p></a></li>
<li><a href='#ks.test'><p>Kolmogorov-Smirnov Tests</p></a></li>
<li><a href='#ksmooth'><p>Kernel Regression Smoother</p></a></li>
<li><a href='#lag'><p>Lag a Time Series</p></a></li>
<li><a href='#lag.plot'><p>Time Series Lag Plots</p></a></li>
<li><a href='#line'><p>Robust Line Fitting</p></a></li>
<li><a href='#listof'><p>A Class for Lists of (Parts of) Model Fits</p></a></li>
<li><a href='#lm'><p>Fitting Linear Models</p></a></li>
<li><a href='#lm.fit'><p>Fitter Functions for Linear Models</p></a></li>
<li><a href='#lm.influence'><p>Regression Diagnostics</p></a></li>
<li><a href='#lm.summaries'><p>Accessing Linear Model Fits</p></a></li>
<li><a href='#loadings'><p>Print Loadings in Factor Analysis</p></a></li>
<li><a href='#loess'><p>Local Polynomial Regression Fitting</p></a></li>
<li><a href='#loess.control'><p>Set Parameters for <code>loess</code></p></a></li>
<li><a href='#Logistic'><p>The Logistic Distribution</p></a></li>
<li><a href='#logLik'><p>Extract Log-Likelihood</p></a></li>
<li><a href='#loglin'><p>Fitting Log-Linear Models</p></a></li>
<li><a href='#Lognormal'><p>The Log Normal Distribution</p></a></li>
<li><a href='#lowess'><p>Scatter Plot Smoothing</p></a></li>
<li><a href='#ls.diag'><p>Compute Diagnostics for <code>lsfit</code> Regression Results</p></a></li>
<li><a href='#ls.print'><p>Print <code>lsfit</code> Regression Results</p></a></li>
<li><a href='#lsfit'><p>Find the Least Squares Fit</p></a></li>
<li><a href='#mad'><p>Median Absolute Deviation</p></a></li>
<li><a href='#mahalanobis'><p>Mahalanobis Distance</p></a></li>
<li><a href='#make.link'><p>Create a Link for GLM Families</p></a></li>
<li><a href='#makepredictcall'><p>Utility Function for Safe Prediction</p></a></li>
<li><a href='#manova'><p>Multivariate Analysis of Variance</p></a></li>
<li><a href='#mantelhaen.test'><p>Cochran-Mantel-Haenszel Chi-Squared Test for Count Data</p></a></li>
<li><a href='#mauchly.test'><p>Mauchly's Test of Sphericity</p></a></li>
<li><a href='#mcnemar.test'><p>McNemar's Chi-squared Test for Count Data</p></a></li>
<li><a href='#median'><p>Median Value</p></a></li>
<li><a href='#medpolish'><p>Median Polish (Robust Two-way Decomposition) of a Matrix</p></a></li>
<li><a href='#model.extract'><p>Extract Components from a Model Frame</p></a></li>
<li><a href='#model.frame'><p>Extracting the Model Frame from a Formula or Fit</p></a></li>
<li><a href='#model.matrix'><p>Construct Design Matrices</p></a></li>
<li><a href='#model.tables'><p>Compute Tables of Results from an <code>aov</code> Model Fit</p></a></li>
<li><a href='#monthplot'>
<p>Plot a Seasonal or other Subseries from a Time Series</p></a></li>
<li><a href='#mood.test'><p>Mood Two-Sample Test of Scale</p></a></li>
<li><a href='#Multinom'><p>The Multinomial Distribution</p></a></li>
<li><a href='#na.action'><p>NA Action</p></a></li>
<li><a href='#na.contiguous'><p>Find Longest Contiguous Stretch of non-NAs</p></a></li>
<li><a href='#na.fail'><p>Handle Missing Values in Objects</p></a></li>
<li><a href='#naprint'>
<p>Adjust for Missing Values</p></a></li>
<li><a href='#naresid'>
<p>Adjust for Missing Values</p></a></li>
<li><a href='#NegBinomial'><p>The Negative Binomial Distribution</p></a></li>
<li><a href='#nextn'><p>Find Highly Composite Numbers</p></a></li>
<li><a href='#nlm'><p>Non-Linear Minimization</p></a></li>
<li><a href='#nlminb'><p>Optimization using PORT routines</p></a></li>
<li><a href='#nls'><p>Nonlinear Least Squares</p></a></li>
<li><a href='#nls.control'><p>Control the Iterations in <code>nls</code></p></a></li>
<li><a href='#NLSstAsymptotic'><p>Fit the Asymptotic Regression Model</p></a></li>
<li><a href='#NLSstClosestX'><p>Inverse Interpolation</p></a></li>
<li><a href='#NLSstLfAsymptote'><p>Horizontal Asymptote on the Left Side</p></a></li>
<li><a href='#NLSstRtAsymptote'><p>Horizontal Asymptote on the Right Side</p></a></li>
<li><a href='#nobs'>
<p>Extract the Number of Observations from a Fit</p></a></li>
<li><a href='#Normal'><p>The Normal Distribution</p></a></li>
<li><a href='#numericDeriv'><p>Evaluate Derivatives Numerically</p></a></li>
<li><a href='#offset'><p>Include an Offset in a Model Formula</p></a></li>
<li><a href='#oneway.test'><p>Test for Equal Means in a One-Way Layout</p></a></li>
<li><a href='#optim'><p>General-purpose Optimization</p></a></li>
<li><a href='#optimize'><p>One Dimensional Optimization</p></a></li>
<li><a href='#order.dendrogram'><p>Ordering or Labels of the Leaves in a Dendrogram</p></a></li>
<li><a href='#p.adjust'><p>Adjust P-values for Multiple Comparisons</p></a></li>
<li><a href='#Pair'>
<p>Construct a Paired-Data Object</p></a></li>
<li><a href='#pairwise.prop.test'><p> Pairwise comparisons for proportions</p></a></li>
<li><a href='#pairwise.t.test'><p> Pairwise t tests</p></a></li>
<li><a href='#pairwise.table'><p>Tabulate p values for pairwise comparisons</p></a></li>
<li><a href='#pairwise.wilcox.test'><p>Pairwise Wilcoxon Rank Sum Tests</p></a></li>
<li><a href='#plot.acf'><p>Plot Autocovariance and Autocorrelation Functions</p></a></li>
<li><a href='#plot.density'><p>Plot Method for Kernel Density Estimation</p></a></li>
<li><a href='#plot.HoltWinters'><p>Plot function for <code>"HoltWinters"</code> objects</p></a></li>
<li><a href='#plot.isoreg'><p>Plot Method for <code>isoreg</code> Objects</p></a></li>
<li><a href='#plot.lm'><p>Plot Diagnostics for an <code>lm</code> Object</p></a></li>
<li><a href='#plot.ppr'><p>Plot Ridge Functions for Projection Pursuit Regression Fit</p></a></li>
<li><a href='#plot.profile'><p>Plotting Functions for 'profile' Objects</p></a></li>
<li><a href='#plot.profile.nls'><p>Plot a <code>profile.nls</code> Object</p></a></li>
<li><a href='#plot.spec'><p>Plotting Spectral Densities</p></a></li>
<li><a href='#plot.stepfun'><p>Plot Step Functions</p></a></li>
<li><a href='#plot.ts'><p>Plotting Time-Series Objects</p></a></li>
<li><a href='#Poisson'><p>The Poisson Distribution</p></a></li>
<li><a href='#poisson.test'><p>Exact Poisson tests</p></a></li>
<li><a href='#poly'><p>Compute Orthogonal Polynomials</p></a></li>
<li><a href='#power'><p>Create a Power Link Object</p></a></li>
<li><a href='#power.anova.test'><p>Power Calculations for Balanced One-Way Analysis of Variance Tests</p></a></li>
<li><a href='#power.prop.test'><p>Power Calculations for Two-Sample Test for Proportions</p></a></li>
<li><a href='#power.t.test'><p>Power calculations for one and two sample t tests</p></a></li>
<li><a href='#PP.test'><p>Phillips-Perron Test for Unit Roots</p></a></li>
<li><a href='#ppoints'><p>Ordinates for Probability Plotting</p></a></li>
<li><a href='#ppr'><p>Projection Pursuit Regression</p></a></li>
<li><a href='#prcomp'><p>Principal Components Analysis</p></a></li>
<li><a href='#predict'><p>Model Predictions</p></a></li>
<li><a href='#predict.Arima'><p>Forecast from ARIMA fits</p></a></li>
<li><a href='#predict.glm'><p>Predict Method for GLM Fits</p></a></li>
<li><a href='#predict.HoltWinters'><p>Prediction Function for Fitted Holt-Winters Models</p></a></li>
<li><a href='#predict.lm'><p>Predict method for Linear Model Fits</p></a></li>
<li><a href='#predict.loess'><p>Predict <abbr>LOESS</abbr> Curve or Surface</p></a></li>
<li><a href='#predict.nls'><p>Predicting from Nonlinear Least Squares Fits</p></a></li>
<li><a href='#predict.smooth.spline'><p>Predict from Smoothing Spline Fit</p></a></li>
<li><a href='#preplot'><p>Pre-computations for a Plotting Object</p></a></li>
<li><a href='#princomp'><p>Principal Components Analysis</p></a></li>
<li><a href='#print.power.htest'><p>Print Methods for Hypothesis Tests and Power Calculation Objects</p></a></li>
<li><a href='#print.ts'><p>Printing and Formatting of Time-Series Objects</p></a></li>
<li><a href='#printCoefmat'><p>Print Coefficient Matrices</p></a></li>
<li><a href='#profile'><p>Generic Function for Profiling Models</p></a></li>
<li><a href='#profile.glm'><p>Method for Profiling <code>glm</code> Objects</p></a></li>
<li><a href='#profile.nls'><p>Method for Profiling <code>nls</code> Objects</p></a></li>
<li><a href='#proj'><p>Projections of Models</p></a></li>
<li><a href='#prop.test'><p>Test of Equal or Given Proportions</p></a></li>
<li><a href='#prop.trend.test'><p>Test for trend in proportions</p></a></li>
<li><a href='#qqnorm'><p>Quantile-Quantile Plots</p></a></li>
<li><a href='#quade.test'><p>Quade Test</p></a></li>
<li><a href='#quantile'><p>Sample Quantiles</p></a></li>
<li><a href='#r2dtable'><p>Random 2-way Tables with Given Marginals</p></a></li>
<li><a href='#read.ftable'><p>Manipulate Flat Contingency Tables</p></a></li>
<li><a href='#rect.hclust'><p>Draw Rectangles Around Hierarchical Clusters</p></a></li>
<li><a href='#relevel'><p>Reorder Levels of Factor</p></a></li>
<li><a href='#reorder.default'><p>Reorder Levels of a Factor</p></a></li>
<li><a href='#reorder.dendrogram'><p>Reorder a Dendrogram</p></a></li>
<li><a href='#replications'><p>Number of Replications of Terms</p></a></li>
<li><a href='#reshape'><p>Reshape Grouped Data</p></a></li>
<li><a href='#residuals'><p>Extract Model Residuals</p></a></li>
<li><a href='#runmed'><p>Running Medians &ndash; Robust Scatter Plot Smoothing</p></a></li>
<li><a href='#rWishart'><p>Random Wishart Distributed Matrices</p></a></li>
<li><a href='#scatter.smooth'><p>Scatter Plot with Smooth Curve Fitted by <code>loess</code></p></a></li>
<li><a href='#screeplot'><p>Scree Plots</p></a></li>
<li><a href='#sd'><p>Standard Deviation</p></a></li>
<li><a href='#se.contrast'><p>Standard Errors for Contrasts in Model Terms</p></a></li>
<li><a href='#selfStart'><p>Construct Self-starting Nonlinear Models</p></a></li>
<li><a href='#setNames'><p>Set the Names in an Object</p></a></li>
<li><a href='#shapiro.test'><p>Shapiro-Wilk Normality Test</p></a></li>
<li><a href='#sigma'><p>Extract Residual Standard Deviation 'Sigma'</p></a></li>
<li><a href='#SignRank'><p>Distribution of the Wilcoxon Signed Rank Statistic</p></a></li>
<li><a href='#simulate'><p>Simulate Responses</p></a></li>
<li><a href='#Smirnov'><p>Distribution of the Smirnov Statistic</p></a></li>
<li><a href='#smooth'><p>Tukey's (Running Median) Smoothing</p></a></li>
<li><a href='#smooth.spline'><p>Fit a Smoothing Spline</p></a></li>
<li><a href='#smoothEnds'><p>End Points Smoothing (for Running Medians)</p></a></li>
<li><a href='#sortedXyData'><p>Create a <code>sortedXyData</code> Object</p></a></li>
<li><a href='#spec.ar'><p>Estimate Spectral Density of a Time Series from AR Fit</p></a></li>
<li><a href='#spec.pgram'><p>Estimate Spectral Density of a Time Series by a Smoothed</p>
Periodogram</a></li>
<li><a href='#spec.taper'><p>Taper a Time Series by a Cosine Bell</p></a></li>
<li><a href='#spectrum'><p>Spectral Density Estimation</p></a></li>
<li><a href='#splinefun'><p>Interpolating Splines</p></a></li>
<li><a href='#SSasymp'><p>Self-Starting <code>nls</code> Asymptotic Model</p></a></li>
<li><a href='#SSasympOff'><p>Self-Starting <code>nls</code> Asymptotic Model with an Offset</p></a></li>
<li><a href='#SSasympOrig'><p>Self-Starting <code>nls</code> Asymptotic Model through the Origin</p></a></li>
<li><a href='#SSbiexp'><p>Self-Starting <code>nls</code> Biexponential Model</p></a></li>
<li><a href='#SSD'><p><abbr>SSD</abbr> Matrix and Estimated Variance Matrix in Multivariate Models</p></a></li>
<li><a href='#SSfol'><p>Self-Starting <code>nls</code> First-order Compartment Model</p></a></li>
<li><a href='#SSfpl'><p>Self-Starting <code>nls</code> Four-Parameter Logistic Model</p></a></li>
<li><a href='#SSgompertz'><p>Self-Starting <code>nls</code> Gompertz Growth Model</p></a></li>
<li><a href='#SSlogis'><p>Self-Starting <code>nls</code> Logistic Model</p></a></li>
<li><a href='#SSmicmen'><p>Self-Starting <code>nls</code> Michaelis-Menten Model</p></a></li>
<li><a href='#SSweibull'><p>Self-Starting <code>nls</code> Weibull Growth Curve Model</p></a></li>
<li><a href='#start'><p>Encode the Terminal Times of Time Series</p></a></li>
<li><a href='#stat.anova'><p>GLM ANOVA Statistics</p></a></li>
<li><a href='#stats-defunct'><p>Defunct Functions in Package <span class="pkg">stats</span></p></a></li>
<li><a href='#stats-deprecated'><p>Deprecated Functions in Package <span class="pkg">stats</span></p></a></li>
<li><a href='#stats-package'>
<p>The R Stats Package</p></a></li>
<li><a href='#step'>
<p>Choose a model by AIC in a Stepwise Algorithm</p></a></li>
<li><a href='#stepfun'><p>Step Functions - Creation and Class</p></a></li>
<li><a href='#stl'><p>Seasonal Decomposition of Time Series by Loess</p></a></li>
<li><a href='#stlmethods'><p>Methods for <abbr>STL</abbr> Objects</p></a></li>
<li><a href='#StructTS'><p>Fit Structural Time Series</p></a></li>
<li><a href='#summary.aov'><p>Summarize an Analysis of Variance Model</p></a></li>
<li><a href='#summary.glm'><p>Summarizing Generalized Linear Model Fits</p></a></li>
<li><a href='#summary.lm'><p>Summarizing Linear Model Fits</p></a></li>
<li><a href='#summary.manova'><p>Summary Method for Multivariate Analysis of Variance</p></a></li>
<li><a href='#summary.nls'><p>Summarizing Non-Linear Least-Squares Model Fits</p></a></li>
<li><a href='#summary.princomp'><p>Summary method for Principal Components Analysis</p></a></li>
<li><a href='#supsmu'><p>Friedman's SuperSmoother</p></a></li>
<li><a href='#symnum'><p>Symbolic Number Coding</p></a></li>
<li><a href='#t.test'><p>Student's t-Test</p></a></li>
<li><a href='#TDist'><p>The Student t Distribution</p></a></li>
<li><a href='#termplot'><p>Plot Regression Terms</p></a></li>
<li><a href='#terms'><p>Model Terms</p></a></li>
<li><a href='#terms.formula'><p>Construct a terms Object from a Formula</p></a></li>
<li><a href='#terms.object'><p>Description of Terms Objects</p></a></li>
<li><a href='#time'><p>Sampling Times of Time Series</p></a></li>
<li><a href='#toeplitz'><p>Create Symmetric and Asymmetric Toeplitz Matrix</p></a></li>
<li><a href='#ts'><p>Time-Series Objects</p></a></li>
<li><a href='#ts-methods'><p>Methods for Time Series Objects</p></a></li>
<li><a href='#ts.plot'><p>Plot Multiple Time Series</p></a></li>
<li><a href='#ts.union'><p>Bind Two or More Time Series</p></a></li>
<li><a href='#tsdiag'><p>Diagnostic Plots for Time-Series Fits</p></a></li>
<li><a href='#tsp'><p>Tsp Attribute of Time-Series-like Objects</p></a></li>
<li><a href='#tsSmooth'><p>Use Fixed-Interval Smoothing on Time Series</p></a></li>
<li><a href='#Tukey'><p>The Studentized Range Distribution</p></a></li>
<li><a href='#TukeyHSD'><p>Compute Tukey Honest Significant Differences</p></a></li>
<li><a href='#Uniform'><p>The Uniform Distribution</p></a></li>
<li><a href='#uniroot'><p>One Dimensional Root (Zero) Finding</p></a></li>
<li><a href='#update'><p>Update and Re-fit a Model Call</p></a></li>
<li><a href='#update.formula'><p>Model Updating</p></a></li>
<li><a href='#var.test'><p>F Test to Compare Two Variances</p></a></li>
<li><a href='#varimax'><p>Rotation Methods for Factor Analysis</p></a></li>
<li><a href='#vcov'><p>Calculate Variance-Covariance Matrix for a Fitted Model Object</p></a></li>
<li><a href='#Weibull'><p>The Weibull Distribution</p></a></li>
<li><a href='#weighted.mean'><p>Weighted Arithmetic Mean</p></a></li>
<li><a href='#weighted.residuals'><p>Compute Weighted Residuals</p></a></li>
<li><a href='#weights'><p>Extract Model Weights</p></a></li>
<li><a href='#wilcox.test'><p>Wilcoxon Rank Sum and Signed Rank Tests</p></a></li>
<li><a href='#Wilcoxon'><p>Distribution of the Wilcoxon Rank Sum Statistic</p></a></li>
<li><a href='#window'><p>Time (Series) Windows</p></a></li>
<li><a href='#xtabs'><p>Cross Tabulation</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>4.4.0</td>
</tr>
<tr>
<td>Priority:</td>
<td>base</td>
</tr>
<tr>
<td>Title:</td>
<td>The R Stats Package</td>
</tr>
<tr>
<td>Author:</td>
<td>R Core Team and contributors worldwide</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>R Core Team &lt;do-use-Contact-address@r-project.org&gt;</td>
</tr>
<tr>
<td>Contact:</td>
<td>R-help mailing list &lt;r-help@r-project.org&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>R statistical functions.</td>
</tr>
<tr>
<td>License:</td>
<td>Part of R 4.4.0</td>
</tr>
<tr>
<td>Imports:</td>
<td>utils, grDevices, graphics</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS, Matrix, SuppDists, methods, stats4</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Enhances:</td>
<td>Kendall, coin, multcomp, pcaPP, pspearman, robustbase</td>
</tr>
<tr>
<td>Built:</td>
<td>R 4.4.0; x86_64-pc-linux-gnu; 2024-03-26 10:06:08 UTC; unix</td>
</tr>
</table>
<hr>
<h2 id='.checkMFClasses'>Functions to Check the Type of Variables passed to Model Frames</h2><span id='topic+.checkMFClasses'></span><span id='topic+.MFclass'></span><span id='topic+.getXlevels'></span>

<h3>Description</h3>

<p><code>.checkMFClasses</code> checks if the variables used in a predict
method agree in type with those used for fitting.
</p>
<p><code>.MFclass</code> categorizes variables for this purpose.
</p>
<p><code>.getXlevels()</code> extracts factor levels from <code><a href="base.html#topic+factor">factor</a></code> or
<code><a href="base.html#topic+character">character</a></code> variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.checkMFClasses(cl, m, ordNotOK = FALSE)
.MFclass(x)
.getXlevels(Terms, m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".checkMFClasses_+3A_cl">cl</code></td>
<td>
<p>a character vector of class descriptions to match.</p>
</td></tr>
<tr><td><code id=".checkMFClasses_+3A_m">m</code></td>
<td>
<p>a model frame (<code><a href="#topic+model.frame">model.frame</a>()</code> result).</p>
</td></tr>
<tr><td><code id=".checkMFClasses_+3A_x">x</code></td>
<td>
<p>any <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
<tr><td><code id=".checkMFClasses_+3A_ordnotok">ordNotOK</code></td>
<td>
<p>logical: are ordered factors different?</p>
</td></tr>
<tr><td><code id=".checkMFClasses_+3A_terms">Terms</code></td>
<td>
<p>a <code>terms</code> object (<code><a href="#topic+terms.object">terms.object</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For applications involving <code><a href="#topic+model.matrix">model.matrix</a>()</code> such as linear models
we do not need to differentiate between <em>ordered</em> factors and factors as
although these affect the coding, the coding used in the fit is
already recorded and imposed during prediction.  However, other
applications may treat ordered factors differently:
<code><a href="rpart.html#topic+rpart">rpart</a></code> does, for example.
</p>


<h3>Value</h3>

<p><code>.checkMFClasses()</code> checks and either signals an error calling
<code><a href="base.html#topic+stop">stop</a>()</code> or returns <code><a href="base.html#topic+NULL">NULL</a></code> invisibly.
</p>
<p><code>.MFclass()</code> returns a character string, one of <code>"logical"</code>,
<code>"ordered"</code>, <code>"factor"</code>, <code>"numeric"</code>, <code>"nmatrix.*"</code>
(a numeric matrix with a number of columns appended) or <code>"other"</code>.
</p>
<p><code>.getXlevels</code> returns a named <code><a href="base.html#topic+list">list</a></code> of character
vectors, possibly empty, or <code><a href="base.html#topic+NULL">NULL</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sapply(warpbreaks, .MFclass) # "numeric" plus 2 x "factor"
sapply(iris,       .MFclass) # 4 x "numeric" plus "factor"

mf &lt;- model.frame(Sepal.Width ~ Species,      iris)
mc &lt;- model.frame(Sepal.Width ~ Sepal.Length, iris)

.checkMFClasses("numeric", mc) # nothing else
.checkMFClasses(c("numeric", "factor"), mf)

## simple .getXlevels() cases :
(xl &lt;- .getXlevels(terms(mf), mf)) # a list with one entry " $ Species" with 3 levels:
stopifnot(exprs = {
  identical(xl$Species, levels(iris$Species))
  identical(.getXlevels(terms(mc), mc), xl[0]) # a empty named list, as no factors
  is.null(.getXlevels(terms(x~x), list(x=1)))
})
</code></pre>

<hr>
<h2 id='acf'>Auto- and Cross- Covariance and -Correlation Function Estimation</h2><span id='topic+acf'></span><span id='topic+ccf'></span><span id='topic+pacf'></span><span id='topic+pacf.default'></span><span id='topic++5B.acf'></span>

<h3>Description</h3>

<p>The function <code>acf</code> computes (and by default plots) estimates of
the autocovariance or autocorrelation function.  Function <code>pacf</code>
is the function used for the partial autocorrelations.  Function
<code>ccf</code> computes the cross-correlation or cross-covariance of two
univariate series.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>acf(x, lag.max = NULL,
    type = c("correlation", "covariance", "partial"),
    plot = TRUE, na.action = na.fail, demean = TRUE, ...)

pacf(x, lag.max, plot, na.action, ...)

## Default S3 method:
pacf(x, lag.max = NULL, plot = TRUE, na.action = na.fail,
    ...)

ccf(x, y, lag.max = NULL, type = c("correlation", "covariance"),
    plot = TRUE, na.action = na.fail, ...)

## S3 method for class 'acf'
x[i, j]
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="acf_+3A_x">x</code>, <code id="acf_+3A_y">y</code></td>
<td>
<p>a univariate or multivariate (not <code>ccf</code>) numeric time
series object or a numeric vector or matrix, or an <code>"acf"</code> object.</p>
</td></tr>
<tr><td><code id="acf_+3A_lag.max">lag.max</code></td>
<td>
<p>maximum lag at which to calculate the <abbr>acf</abbr>.
Default is <code class="reqn">10\log_{10}(N/m)</code> where <code class="reqn">N</code> is the
number of observations and <code class="reqn">m</code> the number of series.  Will
be automatically limited to one less than the number of observations
in the series.</p>
</td></tr>
<tr><td><code id="acf_+3A_type">type</code></td>
<td>
<p>character string giving the type of <abbr>acf</abbr> to be computed.
Allowed values are
<code>"correlation"</code> (the default), <code>"covariance"</code> or
<code>"partial"</code>.  Will be partially matched.</p>
</td></tr>
<tr><td><code id="acf_+3A_plot">plot</code></td>
<td>
<p>logical. If <code>TRUE</code> (the default) the <abbr>acf</abbr> is plotted.</p>
</td></tr>
<tr><td><code id="acf_+3A_na.action">na.action</code></td>
<td>
<p>function to be called to handle missing
values. <code>na.pass</code> can be used.</p>
</td></tr>
<tr><td><code id="acf_+3A_demean">demean</code></td>
<td>
<p>logical.  Should the covariances be about the sample
means?</p>
</td></tr>
<tr><td><code id="acf_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to <code>plot.acf</code>.</p>
</td></tr>
<tr><td><code id="acf_+3A_i">i</code></td>
<td>
<p>a set of lags (time differences) to retain.</p>
</td></tr>
<tr><td><code id="acf_+3A_j">j</code></td>
<td>
<p>a set of series (names or numbers) to retain.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>type</code> = <code>"correlation"</code> and <code>"covariance"</code>, the
estimates are based on the sample covariance. (The lag 0 autocorrelation
is fixed at 1 by convention.)
</p>
<p>By default, no missing values are allowed.  If the <code>na.action</code>
function passes through missing values (as <code>na.pass</code> does), the
covariances are computed from the complete cases.  This means that the
estimate computed may well not be a valid autocorrelation sequence,
and may contain missing values.  Missing values are not allowed when
computing the <abbr>PACF</abbr> of a multivariate time series.
</p>
<p>The partial correlation coefficient is estimated by fitting
autoregressive models of successively higher orders up to
<code>lag.max</code>.
</p>
<p>The generic function <code>plot</code> has a method for objects of class
<code>"acf"</code>.
</p>
<p>The lag is returned and plotted in units of time, and not numbers of
observations.
</p>
<p>There are <code>print</code> and subsetting methods for objects of class
<code>"acf"</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>"acf"</code>, which is a list with the following
elements:
</p>
<table>
<tr><td><code>lag</code></td>
<td>
<p>A three dimensional array containing the lags at which
the <abbr>acf</abbr> is estimated.</p>
</td></tr>
<tr><td><code>acf</code></td>
<td>
<p>An array with the same dimensions as <code>lag</code> containing
the estimated <abbr>acf</abbr>.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>The type of correlation (same as the <code>type</code>
argument).</p>
</td></tr>
<tr><td><code>n.used</code></td>
<td>
<p>The number of observations in the time series.</p>
</td></tr>
<tr><td><code>series</code></td>
<td>
<p>The name of the series <code>x</code>.</p>
</td></tr>
<tr><td><code>snames</code></td>
<td>
<p>The series names for a multivariate time series.</p>
</td></tr>
</table>
<p>The lag <code>k</code> value returned by <code>ccf(x, y)</code> estimates the
correlation between <code>x[t+k]</code> and <code>y[t]</code>.
</p>
<p>The result is returned invisibly if <code>plot</code> is <code>TRUE</code>.
</p>


<h3>Author(s)</h3>

<p>Original: Paul Gilbert, Martyn Plummer.
Extensive modifications and univariate case of <code>pacf</code> by
B. D. Ripley.
</p>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S</em>.  Fourth Edition.
Springer-Verlag.
</p>
<p>(This contains the exact definitions used.)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.acf">plot.acf</a></code>, <code><a href="#topic+ARMAacf">ARMAacf</a></code> for the exact
autocorrelations of a given ARMA process.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

## Examples from Venables &amp; Ripley
acf(lh)
acf(lh, type = "covariance")
pacf(lh)

acf(ldeaths)
acf(ldeaths, ci.type = "ma")
acf(ts.union(mdeaths, fdeaths))
ccf(mdeaths, fdeaths, ylab = "cross-correlation")
# (just the cross-correlations)

presidents # contains missing values
acf(presidents, na.action = na.pass)
pacf(presidents, na.action = na.pass)
</code></pre>

<hr>
<h2 id='acf2AR'>Compute an AR Process Exactly Fitting an ACF</h2><span id='topic+acf2AR'></span>

<h3>Description</h3>

<p>Compute an AR process exactly fitting an autocorrelation function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>acf2AR(acf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="acf2AR_+3A_acf">acf</code></td>
<td>
<p>An autocorrelation or autocovariance sequence.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix, with one row for the computed AR(p) coefficients for
<code>1 &lt;= p &lt;= length(acf)</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ARMAacf">ARMAacf</a></code>, <code><a href="#topic+ar.yw">ar.yw</a></code> which does this from an
empirical ACF.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>(Acf &lt;- ARMAacf(c(0.6, 0.3, -0.2)))
acf2AR(Acf)
</code></pre>

<hr>
<h2 id='add1'>Add or Drop All Possible Single Terms to a Model</h2><span id='topic+add1'></span><span id='topic+add1.default'></span><span id='topic+add1.lm'></span><span id='topic+add1.glm'></span><span id='topic+drop1'></span><span id='topic+drop1.default'></span><span id='topic+drop1.lm'></span><span id='topic+drop1.glm'></span>

<h3>Description</h3>

<p>Compute all the single terms in the <code>scope</code> argument that can be
added to or dropped from the model, fit those models and compute a
table of the changes in fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add1(object, scope, ...)

## Default S3 method:
add1(object, scope, scale = 0, test = c("none", "Chisq"),
     k = 2, trace = FALSE, ...)

## S3 method for class 'lm'
add1(object, scope, scale = 0, test = c("none", "Chisq", "F"),
     x = NULL, k = 2, ...)

## S3 method for class 'glm'
add1(object, scope, scale = 0,
     test = c("none", "Rao", "LRT", "Chisq", "F"),
     x = NULL, k = 2, ...)

drop1(object, scope, ...)

## Default S3 method:
drop1(object, scope, scale = 0, test = c("none", "Chisq"),
      k = 2, trace = FALSE, ...)

## S3 method for class 'lm'
drop1(object, scope, scale = 0, all.cols = TRUE,
      test = c("none", "Chisq", "F"), k = 2, ...)

## S3 method for class 'glm'
drop1(object, scope, scale = 0,
      test = c("none", "Rao", "LRT", "Chisq", "F"),
      k = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add1_+3A_object">object</code></td>
<td>
<p>a fitted model object.</p>
</td></tr>
<tr><td><code id="add1_+3A_scope">scope</code></td>
<td>
<p>a formula giving the terms to be considered for adding or
dropping.</p>
</td></tr>
<tr><td><code id="add1_+3A_scale">scale</code></td>
<td>
<p>an estimate of the residual mean square to be
used in computing <code class="reqn">C_p</code>. Ignored if <code>0</code> or <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="add1_+3A_test">test</code></td>
<td>
<p>should the results include a test statistic relative to the
original model?  The F test is only appropriate for <code><a href="#topic+lm">lm</a></code> and
<code><a href="#topic+aov">aov</a></code> models or perhaps for <code><a href="#topic+glm">glm</a></code> fits with
estimated dispersion.
The <code class="reqn">\chi^2</code> test can be an exact test
(<code>lm</code> models with known scale) or a likelihood-ratio test or a
test of the reduction in scaled deviance depending on the method.
For <code><a href="#topic+glm">glm</a></code> fits, you can also choose <code>"LRT"</code> and
<code>"Rao"</code> for likelihood ratio tests and Rao's efficient score test.
The former is synonymous with <code>"Chisq"</code> (although both have
an asymptotic chi-square distribution).
Values can be abbreviated.
</p>
</td></tr>
<tr><td><code id="add1_+3A_k">k</code></td>
<td>
<p>the penalty constant in AIC / <code class="reqn">C_p</code>.</p>
</td></tr>
<tr><td><code id="add1_+3A_trace">trace</code></td>
<td>
<p>if <code>TRUE</code>, print out progress reports.</p>
</td></tr>
<tr><td><code id="add1_+3A_x">x</code></td>
<td>
<p>a model matrix containing columns for the fitted model and all
terms in the upper scope.  Useful if <code>add1</code> is to be called
repeatedly.  <b>Warning:</b> no checks are done on its validity.</p>
</td></tr>
<tr><td><code id="add1_+3A_all.cols">all.cols</code></td>
<td>
<p>(Provided for compatibility with S.)  Logical to specify
whether all columns of the design matrix should be used.  If
<code>FALSE</code> then non-estimable columns are dropped, but the result
is not usually statistically meaningful.</p>
</td></tr>
<tr><td><code id="add1_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>drop1</code> methods, a missing <code>scope</code> is taken to be all
terms in the model. The hierarchy is respected when considering terms
to be added or dropped: all main effects contained in a second-order
interaction must remain, and so on.
</p>
<p>In a <code>scope</code> formula <code>.</code> means &lsquo;what is already there&rsquo;.
</p>
<p>The methods for <code><a href="#topic+lm">lm</a></code> and <code><a href="#topic+glm">glm</a></code> are more
efficient in that they do not recompute the model matrix and call the
<code>fit</code> methods directly.
</p>
<p>The default output table gives AIC, defined as minus twice log
likelihood plus <code class="reqn">2p</code> where <code class="reqn">p</code> is the rank of the model (the
number of effective parameters).  This is only defined up to an
additive constant (like log-likelihoods).  For linear Gaussian models
with fixed scale, the constant is chosen to give Mallows' <code class="reqn">C_p</code>,
<code class="reqn">RSS/scale + 2p - n</code>.  Where <code class="reqn">C_p</code> is used,
the column is labelled as <code>Cp</code> rather than <code>AIC</code>.
</p>
<p>The F tests for the <code>"glm"</code> methods are based on analysis of
deviance tests, so if the dispersion is estimated it is based on the
residual deviance, unlike the F tests of <code><a href="#topic+anova.glm">anova.glm</a></code>.
</p>


<h3>Value</h3>

<p>An object of class <code>"anova"</code> summarizing the differences in fit
between the models.
</p>


<h3>Warning</h3>

<p>The model fitting must apply the models to the same dataset. Most
methods will attempt to use a subset of the data with no missing
values for any of the variables if <code>na.action = na.omit</code>, but
this may give biased results.  Only use these functions with data
containing missing values with great care.
</p>
<p>The default methods make calls to the function <code><a href="#topic+nobs">nobs</a></code> to
check that the number of observations involved in the fitting process
remained unchanged.
</p>


<h3>Note</h3>

<p>These are not fully equivalent to the functions in S.  There is no
<code>keep</code> argument, and the methods used are not quite so
computationally efficient.
</p>
<p>Their authors' definitions of Mallows' <code class="reqn">C_p</code> and Akaike's AIC
are used, not those of the authors of the models chapter of S.
</p>


<h3>Author(s)</h3>

<p>The design was inspired by the S functions of the same names described
in Chambers (1992).
</p>


<h3>References</h3>

<p>Chambers, J. M. (1992)
<em>Linear models.</em>
Chapter 4 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+step">step</a></code>, <code><a href="#topic+aov">aov</a></code>, <code><a href="#topic+lm">lm</a></code>,
<code><a href="#topic+extractAIC">extractAIC</a></code>, <code><a href="#topic+anova">anova</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require(graphics); require(utils)
## following example(swiss)
lm1 &lt;- lm(Fertility ~ ., data = swiss)
add1(lm1, ~ I(Education^2) + .^2)
drop1(lm1, test = "F")  # So called 'type II' anova

## following example(glm)

drop1(glm.D93, test = "Chisq")
drop1(glm.D93, test = "F")
add1(glm.D93, scope = ~outcome*treatment, test = "Rao") ## Pearson Chi-square

</code></pre>

<hr>
<h2 id='addmargins'>Puts Arbitrary Margins on Multidimensional Tables or Arrays</h2><span id='topic+addmargins'></span>

<h3>Description</h3>

<p>For a given table one can specify which of the classifying factors to
expand by one or more levels to hold margins to be calculated.  One may for
example form sums and means over the first dimension and medians over the
second.  The resulting table will then have two extra levels for the first
dimension and one extra level for the second.  The default is to sum over
all margins in the table.  Other possibilities may give results that
depend on the order in which the margins are computed.  This is flagged
in the printed output from the function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addmargins(A, margin = seq_along(dim(A)), FUN = sum, quiet = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addmargins_+3A_a">A</code></td>
<td>
<p>table or array.  The function uses the presence of the
<code>"dim"</code> and <code>"dimnames"</code> attributes of <code>A</code>.</p>
</td></tr>
<tr><td><code id="addmargins_+3A_margin">margin</code></td>
<td>
<p>vector of dimensions over which to form margins.  Margins
are formed in the order in which dimensions are specified in
<code>margin</code>.</p>
</td></tr>
<tr><td><code id="addmargins_+3A_fun">FUN</code></td>
<td>
<p><code><a href="base.html#topic+list">list</a></code> of the same length as <code>margin</code>, each
element of the list being either a <code><a href="base.html#topic+function">function</a></code> or a list of
functions.  In the length-1 case, can be a function instead of a list
of one.  Names of
the list elements will appear as levels in dimnames of the result.
Unnamed list elements will have names constructed:  the name
of a function or a constructed name based on the position in the table.</p>
</td></tr>
<tr><td><code id="addmargins_+3A_quiet">quiet</code></td>
<td>
<p>logical which suppresses the message telling the order in
which the margins were computed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the functions used to form margins are not commutative, the result
depends on the order in which margins are computed.  Annotation
of margins is done via naming the <code>FUN</code> list.
</p>


<h3>Value</h3>

<p>A <code><a href="base.html#topic+table">table</a></code> or <code><a href="base.html#topic+array">array</a></code> with the same number of
dimensions as <code>A</code>, but
with extra levels of the dimensions mentioned in <code>margin</code>.  The
number of levels added to each dimension is the length of the entries
in <code>FUN</code>.  A message with the order of computation of margins is
printed.
</p>


<h3>Author(s)</h3>

<p>Bendix Carstensen, Steno Diabetes Center &amp; Department of
Biostatistics, University of Copenhagen,
<a href="https://BendixCarstensen.com">https://BendixCarstensen.com</a>, autumn 2003.
Margin naming enhanced by Duncan Murdoch.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+table">table</a></code>, <code><a href="#topic+ftable">ftable</a></code>, <code><a href="base.html#topic+margin.table">margin.table</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Aye &lt;- sample(c("Yes", "Si", "Oui"), 177, replace = TRUE)
Bee &lt;- sample(c("Hum", "Buzz"), 177, replace = TRUE)
Sea &lt;- sample(c("White", "Black", "Red", "Dead"), 177, replace = TRUE)
(A &lt;- table(Aye, Bee, Sea))
(aA &lt;- addmargins(A))

ftable(A)
ftable(aA)

# Non-commutative functions - note differences between resulting tables:
ftable( addmargins(A, c(3, 1),
                   FUN = list(list(Min = min, Max = max),
                              Sum = sum)))
ftable( addmargins(A, c(1, 3),
                   FUN = list(Sum = sum,
                              list(Min = min, Max = max))))

# Weird function needed to return the N when computing percentages
sqsm &lt;- function(x) sum(x)^2/100
B &lt;- table(Sea, Bee)
round(sweep(addmargins(B, 1, list(list(All = sum, N = sqsm))), 2,
            apply(B, 2, sum)/100, `/`), 1)
round(sweep(addmargins(B, 2, list(list(All = sum, N = sqsm))), 1,
            apply(B, 1, sum)/100, `/`), 1)

# A total over Bee requires formation of the Bee-margin first:
mB &lt;-  addmargins(B, 2, FUN = list(list(Total = sum)))
round(ftable(sweep(addmargins(mB, 1, list(list(All = sum, N = sqsm))), 2,
                   apply(mB, 2, sum)/100, `/`)), 1)

## Zero.Printing table+margins:
set.seed(1)
x &lt;- sample( 1:7, 20, replace = TRUE)
y &lt;- sample( 1:7, 20, replace = TRUE)
tx &lt;- addmargins( table(x, y) )
print(tx, zero.print = ".")
</code></pre>

<hr>
<h2 id='aggregate'>Compute Summary Statistics of Data Subsets</h2><span id='topic+aggregate'></span><span id='topic+aggregate.default'></span><span id='topic+aggregate.data.frame'></span><span id='topic+aggregate.formula'></span><span id='topic+aggregate.ts'></span>

<h3>Description</h3>

<p>Splits the data into subsets, computes summary statistics for each,
and returns the result in a convenient form.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aggregate(x, ...)

## Default S3 method:
aggregate(x, ...)

## S3 method for class 'data.frame'
aggregate(x, by, FUN, ..., simplify = TRUE, drop = TRUE)

## S3 method for class 'formula'
aggregate(x, data, FUN, ...,
          subset, na.action = na.omit)

## S3 method for class 'ts'
aggregate(x, nfrequency = 1, FUN = sum, ndeltat = 1,
          ts.eps = getOption("ts.eps"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aggregate_+3A_x">x</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object.  For the <code>formula</code> method a <code><a href="#topic+formula">formula</a></code>,
such as <code>y ~ x</code> or <code>cbind(y1, y2) ~ x1 + x2</code>, where the
<code>y</code> variables are numeric data to be split into groups according
to the grouping <code>x</code> variables (usually factors).</p>
</td></tr>
<tr><td><code id="aggregate_+3A_by">by</code></td>
<td>
<p>a list of grouping elements, each as long as the variables
in the data frame <code>x</code>, or a formula.  The elements are coerced to factors
before use.</p>
</td></tr>
<tr><td><code id="aggregate_+3A_fun">FUN</code></td>
<td>
<p>a function to compute the summary statistics which can be
applied to all data subsets.</p>
</td></tr>
<tr><td><code id="aggregate_+3A_simplify">simplify</code></td>
<td>
<p>a logical indicating whether results should be
simplified to a vector or matrix if possible.</p>
</td></tr>
<tr><td><code id="aggregate_+3A_drop">drop</code></td>
<td>
<p>a logical indicating whether to drop unused combinations
of grouping values.  The non-default case <code>drop=FALSE</code> has been
amended for <span class="rlang"><b>R</b></span> 3.5.0 to drop unused combinations.</p>
</td></tr>
<tr><td><code id="aggregate_+3A_data">data</code></td>
<td>
<p>a data frame (or list) from which the variables in the formula
should be taken.</p>
</td></tr>
<tr><td><code id="aggregate_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="aggregate_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code> values. The default is to only consider
<em>complete cases</em> 
with respect to the given variables.</p>
</td></tr>
<tr><td><code id="aggregate_+3A_nfrequency">nfrequency</code></td>
<td>
<p>new number of observations per unit of time; must
be a divisor of the frequency of <code>x</code>.</p>
</td></tr>
<tr><td><code id="aggregate_+3A_ndeltat">ndeltat</code></td>
<td>
<p>new fraction of the sampling period between
successive observations; must be a divisor of the sampling
interval of <code>x</code>.</p>
</td></tr>
<tr><td><code id="aggregate_+3A_ts.eps">ts.eps</code></td>
<td>
<p>tolerance used to decide if <code>nfrequency</code> is a
sub-multiple of the original frequency.</p>
</td></tr>
<tr><td><code id="aggregate_+3A_...">...</code></td>
<td>
<p>further arguments passed to or used by methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>aggregate</code> is a generic function with methods for data frames
and time series.
</p>
<p>The default method, <code>aggregate.default</code>, uses the time series
method if <code>x</code> is a time series, and otherwise coerces <code>x</code>
to a data frame and calls the data frame method.
</p>
<p><code>aggregate.data.frame</code> is the data frame method.  If <code>x</code> is
not a data frame, it is coerced to one, which must have a non-zero
number of rows.  Then, each of the variables (columns) in <code>x</code> is
split into subsets of cases (rows) of identical combinations of the
components of <code>by</code>, and <code>FUN</code> is applied to each such subset
with further arguments in <code>...</code> passed to it.  The result is
reformatted into a data frame containing the variables in <code>by</code>
and <code>x</code>.  The ones arising from <code>by</code> contain the unique
combinations of grouping values used for determining the subsets, and
the ones arising from <code>x</code> the corresponding summaries for the
subset of the respective variables in <code>x</code>.  If <code>simplify</code> is
true, summaries are simplified to vectors or matrices if they have a
common length of one or greater than one, respectively; otherwise,
lists of summary results according to subsets are obtained.  Rows with
missing values in any of the <code>by</code> variables will be omitted from
the result.  (Note that versions of <span class="rlang"><b>R</b></span> prior to 2.11.0 required
<code>FUN</code> to be a scalar function.)
</p>
<p>The formula method provides a standard formula interface to
<code>aggregate.data.frame</code>.
The latter invokes the formula method if <code>by</code> is a formula,
in which case <code>aggregate(x, by, FUN)</code> is the same as
<code>aggregate(by, x, FUN)</code> for a data frame <code>x</code>.
</p>
<p><code>aggregate.ts</code> is the time series method, and requires <code>FUN</code>
to be a scalar function.  If <code>x</code> is not a time series, it is
coerced to one.  Then, the variables in <code>x</code> are split into
appropriate blocks of length <code>frequency(x) / nfrequency</code>, and
<code>FUN</code> is applied to each such block, with further (named)
arguments in <code>...</code> passed to it.  The result returned is a time
series with frequency <code>nfrequency</code> holding the aggregated values.
Note that this make most sense for a quarterly or yearly result when
the original series covers a whole number of quarters or years: in
particular aggregating a monthly series to quarters starting in
February does not give a conventional quarterly series.
</p>
<p><code>FUN</code> is passed to <code><a href="base.html#topic+match.fun">match.fun</a></code>, and hence it can be a
function or a symbol or character string naming a function.
</p>


<h3>Value</h3>

<p>For the time series method, a time series of class <code>"ts"</code> or
class <code>c("mts", "ts")</code>.
</p>
<p>For the data frame method, a data frame with columns
corresponding to the grouping variables in <code>by</code> followed by
aggregated columns from <code>x</code>.  If the <code>by</code> has names, the
non-empty times are used to label the columns in the results, with
unnamed grouping variables being named <code>Group.<var>i</var></code> for
<code>by[[<var>i</var>]]</code>.
</p>


<h3>Warning</h3>

<p>The first argument of the <code>"formula"</code> method was named
<code>formula</code> rather than <code>x</code> prior to <span class="rlang"><b>R</b></span> 4.2.0.  Portable uses
should not name that argument.
</p>


<h3>Author(s)</h3>

<p>Kurt Hornik, with contributions by Arni Magnusson.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+apply">apply</a></code>, <code><a href="base.html#topic+lapply">lapply</a></code>, <code><a href="base.html#topic+tapply">tapply</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Compute the averages for the variables in 'state.x77', grouped
## according to the region (Northeast, South, North Central, West) that
## each state belongs to.
aggregate(state.x77, list(Region = state.region), mean)

## Compute the averages according to region and the occurrence of more
## than 130 days of frost.
aggregate(state.x77,
          list(Region = state.region,
               Cold = state.x77[,"Frost"] &gt; 130),
          mean)
## (Note that no state in 'South' is THAT cold.)


## example with character variables and NAs
testDF &lt;- data.frame(v1 = c(1,3,5,7,8,3,5,NA,4,5,7,9),
                     v2 = c(11,33,55,77,88,33,55,NA,44,55,77,99) )
by1 &lt;- c("red", "blue", 1, 2, NA, "big", 1, 2, "red", 1, NA, 12)
by2 &lt;- c("wet", "dry", 99, 95, NA, "damp", 95, 99, "red", 99, NA, NA)
aggregate(x = testDF, by = list(by1, by2), FUN = "mean")

# and if you want to treat NAs as a group
fby1 &lt;- factor(by1, exclude = "")
fby2 &lt;- factor(by2, exclude = "")
aggregate(x = testDF, by = list(fby1, fby2), FUN = "mean")


## Formulas, one ~ one, one ~ many, many ~ one, and many ~ many:
aggregate(weight ~ feed, data = chickwts, mean)
aggregate(breaks ~ wool + tension, data = warpbreaks, mean)
aggregate(cbind(Ozone, Temp) ~ Month, data = airquality, mean)
aggregate(cbind(ncases, ncontrols) ~ alcgp + tobgp, data = esoph, sum)

## "complete cases" vs. "available cases"
colSums(is.na(airquality))  # NAs in Ozone but not Temp
## the default is to summarize *complete cases*:
aggregate(cbind(Ozone, Temp) ~ Month, data = airquality, FUN = mean)
## to handle missing values *per variable*:
aggregate(cbind(Ozone, Temp) ~ Month, data = airquality, FUN = mean,
          na.action = na.pass, na.rm = TRUE)

## Dot notation:
aggregate(. ~ Species, data = iris, mean)
aggregate(len ~ ., data = ToothGrowth, mean)

## Often followed by xtabs():
ag &lt;- aggregate(len ~ ., data = ToothGrowth, mean)
xtabs(len ~ ., data = ag)

## Formula interface via 'by' (for pipe operations)
ToothGrowth |&gt; aggregate(len ~ ., FUN = mean)

## Compute the average annual approval ratings for American presidents.
aggregate(presidents, nfrequency = 1, FUN = mean)
## Give the summer less weight.
aggregate(presidents, nfrequency = 1,
          FUN = weighted.mean, w = c(1, 1, 0.5, 1))
</code></pre>

<hr>
<h2 id='AIC'>Akaike's An Information Criterion</h2><span id='topic+AIC'></span><span id='topic+BIC'></span>

<h3>Description</h3>

<p>Generic function calculating Akaike's &lsquo;An Information Criterion&rsquo; for
one or several fitted model objects for which a log-likelihood value
can be obtained, according to the formula
<code class="reqn">-2 \mbox{log-likelihood} + k n_{par}</code>,
where <code class="reqn">n_{par}</code> represents the number of parameters in the
fitted model, and <code class="reqn">k = 2</code> for the usual AIC, or
<code class="reqn">k = \log(n)</code>
(<code class="reqn">n</code> being the number of observations) for the so-called BIC or <abbr>SBC</abbr>
(Schwarz's Bayesian criterion).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AIC(object, ..., k = 2)

BIC(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AIC_+3A_object">object</code></td>
<td>
<p>a fitted model object for which there exists a
<code>logLik</code> method to extract the corresponding log-likelihood, or
an object inheriting from class <code>logLik</code>.</p>
</td></tr>
<tr><td><code id="AIC_+3A_...">...</code></td>
<td>
<p>optionally more fitted model objects.</p>
</td></tr>
<tr><td><code id="AIC_+3A_k">k</code></td>
<td>
<p>numeric, the <em>penalty</em> per parameter to be used; the
default <code>k = 2</code> is the classical AIC.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When comparing models fitted by maximum likelihood to the same data,
the smaller the AIC or BIC, the better the fit.
</p>
<p>The theory of AIC requires that the log-likelihood has been maximized:
whereas AIC can be computed for models not fitted by maximum
likelihood, their AIC values should not be compared.
</p>
<p>Examples of models not &lsquo;fitted to the same data&rsquo; are where the
response is transformed (accelerated-life models are fitted to
log-times) and where contingency tables have been used to summarize
data.
</p>
<p>These are generic functions (with S4 generics defined in package
<span class="pkg">stats4</span>): however methods should be defined for the
log-likelihood function <code><a href="#topic+logLik">logLik</a></code> rather than these
functions: the action of their default methods is to call <code>logLik</code>
on all the supplied objects and assemble the results.  Note that in
several common cases <code><a href="#topic+logLik">logLik</a></code> does not return the value at
the MLE: see its help page.
</p>
<p>The log-likelihood and hence the AIC/BIC is only defined up to an
additive constant.  Different constants have conventionally been used
for different purposes and so <code><a href="#topic+extractAIC">extractAIC</a></code> and <code>AIC</code>
may give different values (and do for models of class <code>"lm"</code>: see
the help for <code><a href="#topic+extractAIC">extractAIC</a></code>).  Particular care is needed
when comparing fits of different classes (with, for example, a
comparison of a Poisson and gamma GLM being meaningless since one has
a discrete response, the other continuous).
</p>
<p><code>BIC</code> is defined as
<code>AIC(object, ..., k = log(nobs(object)))</code>.
This needs the number of observations to be known: the default method
looks first for a <code>"nobs"</code> attribute on the return value from the
<code><a href="#topic+logLik">logLik</a></code> method, then tries the <code><a href="#topic+nobs">nobs</a></code>
generic, and if neither succeed returns BIC as <code>NA</code>.
</p>


<h3>Value</h3>

<p>If just one object is provided, a numeric value with the corresponding
AIC (or BIC, or ..., depending on <code>k</code>).
</p>
<p>If multiple objects are provided, a <code>data.frame</code> with rows
corresponding to the objects and columns representing the number of
parameters in the model (<code>df</code>) and the AIC or BIC.
</p>


<h3>Author(s)</h3>

<p>Originally by Jos Pinheiro and Douglas Bates,
more recent revisions by R-core.
</p>


<h3>References</h3>

<p>Sakamoto, Y., Ishiguro, M., and Kitagawa G. (1986).
<em>Akaike Information Criterion Statistics</em>.
D. Reidel Publishing Company.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+extractAIC">extractAIC</a></code>, <code><a href="#topic+logLik">logLik</a></code>, <code><a href="#topic+nobs">nobs</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>lm1 &lt;- lm(Fertility ~ . , data = swiss)
AIC(lm1)
stopifnot(all.equal(AIC(lm1),
                    AIC(logLik(lm1))))
BIC(lm1)

lm2 &lt;- update(lm1, . ~ . -Examination)
AIC(lm1, lm2)
BIC(lm1, lm2)
</code></pre>

<hr>
<h2 id='alias'>Find Aliases (Dependencies) in a Model</h2><span id='topic+alias'></span><span id='topic+alias.formula'></span><span id='topic+alias.lm'></span>

<h3>Description</h3>

<p>Find aliases (linearly dependent terms) in a linear model specified by
a formula.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>alias(object, ...)

## S3 method for class 'formula'
alias(object, data, ...)

## S3 method for class 'lm'
alias(object, complete = TRUE, partial = FALSE,
      partial.pattern = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="alias_+3A_object">object</code></td>
<td>
<p>A fitted model object, for example from <code>lm</code> or
<code>aov</code>, or a formula for <code>alias.formula</code>.</p>
</td></tr>
<tr><td><code id="alias_+3A_data">data</code></td>
<td>
<p>Optionally, a data frame to search for the objects
in the formula.</p>
</td></tr>
<tr><td><code id="alias_+3A_complete">complete</code></td>
<td>
<p>Should information on complete aliasing be included?</p>
</td></tr>
<tr><td><code id="alias_+3A_partial">partial</code></td>
<td>
<p>Should information on partial aliasing be included?</p>
</td></tr>
<tr><td><code id="alias_+3A_partial.pattern">partial.pattern</code></td>
<td>
<p>Should partial aliasing be presented in a
schematic way? If this is done, the results are presented in a
more compact way, usually giving the deciles of the coefficients.</p>
</td></tr>
<tr><td><code id="alias_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Although the main method is for class <code>"lm"</code>, <code>alias</code> is
most useful for experimental designs and so is used with fits from
<code>aov</code>.
Complete aliasing refers to effects in linear models that cannot be estimated
independently of the terms which occur earlier in the model and so
have their coefficients omitted from the fit. Partial aliasing refers
to effects that can be estimated less precisely because of
correlations induced by the design.
</p>
<p>Some parts of the <code>"lm"</code> method require recommended package
<a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a> to be installed.
</p>


<h3>Value</h3>

<p>A list (of <code><a href="base.html#topic+class">class</a></code> <code>"<a href="#topic+listof">listof</a>"</code>) containing components
</p>
<table>
<tr><td><code>Model</code></td>
<td>
<p>Description of the model; usually the formula.</p>
</td></tr>
<tr><td><code>Complete</code></td>
<td>
<p>A matrix with columns corresponding to effects that
are linearly dependent on the rows.</p>
</td></tr>
<tr><td><code>Partial</code></td>
<td>
<p>The correlations of the estimable effects, with a zero
diagonal. An object of class <code>"mtable"</code> which has its own
<code><a href="base.html#topic+print">print</a></code> method.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The aliasing pattern may depend on the contrasts in use: Helmert
contrasts are probably most useful.
</p>
<p>The defaults are different from those in S.
</p>


<h3>Author(s)</h3>

<p>The design was inspired by the S function of the same name described
in Chambers <abbr>et al.</abbr> (1992).
</p>


<h3>References</h3>

<p>Chambers, J. M., Freeny, A and Heiberger, R. M. (1992)
<em>Analysis of variance; designed experiments.</em>
Chapter 5 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
op &lt;- options(contrasts = c("contr.helmert", "contr.poly"))
npk.aov &lt;- aov(yield ~ block + N*P*K, npk)
alias(npk.aov)
options(op)  # reset
</code></pre>

<hr>
<h2 id='anova'>ANOVA Tables</h2><span id='topic+anova'></span>

<h3>Description</h3>

<p>Compute analysis of variance (or deviance) tables for one or more
fitted model objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>anova(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="anova_+3A_object">object</code></td>
<td>
<p>an object containing the results returned by a model
fitting function (e.g., <code>lm</code> or <code>glm</code>).</p>
</td></tr>
<tr><td><code id="anova_+3A_...">...</code></td>
<td>
<p>additional objects of the same type.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This (generic) function returns an object of class <code>anova</code>.
These objects represent analysis-of-variance and analysis-of-deviance tables.
When given a single argument it produces a table which
tests whether the model terms are significant.
</p>
<p>When given a sequence of objects, <code>anova</code> tests
the models against one another in the order specified.
</p>
<p>The print method for <code>anova</code> objects prints tables in a
&lsquo;pretty&rsquo; form.
</p>


<h3>Warning</h3>

<p>The comparison between two or more models will only be valid if they
are fitted to the same dataset. This may be a problem if there are
missing values and <span class="rlang"><b>R</b></span>'s default of <code>na.action = na.omit</code> is used.
</p>


<h3>References</h3>

<p>Chambers, J. M. and Hastie, T. J. (1992)
<em>Statistical Models in S</em>, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coefficients">coefficients</a></code>, <code><a href="#topic+effects">effects</a></code>,
<code><a href="#topic+fitted.values">fitted.values</a></code>, <code><a href="#topic+residuals">residuals</a></code>,
<code><a href="base.html#topic+summary">summary</a></code>, <code><a href="#topic+drop1">drop1</a></code>, <code><a href="#topic+add1">add1</a></code>.
</p>

<hr>
<h2 id='anova.glm'>Analysis of Deviance for Generalized Linear Model Fits</h2><span id='topic+anova.glm'></span>

<h3>Description</h3>

<p>Compute an analysis of deviance table for one or more generalized
linear model fits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'glm'
anova(object, ..., dispersion = NULL, test = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="anova.glm_+3A_object">object</code>, <code id="anova.glm_+3A_...">...</code></td>
<td>
<p>objects of class <code>glm</code>, typically
the result of a call to <code><a href="#topic+glm">glm</a></code>, or a list of
<code>objects</code> for the <code>"glmlist"</code> method.</p>
</td></tr>
<tr><td><code id="anova.glm_+3A_dispersion">dispersion</code></td>
<td>
<p>the dispersion parameter for the fitting family.
By default it is obtained from the object(s).</p>
</td></tr>
<tr><td><code id="anova.glm_+3A_test">test</code></td>
<td>
<p>a character string, (partially) matching one of <code>"Chisq"</code>,
<code>"LRT"</code>, <code>"Rao"</code>,
<code>"F"</code> or <code>"Cp"</code>. See <code><a href="#topic+stat.anova">stat.anova</a></code>.
Or logical <code>FALSE</code>, which suppresses any test.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Specifying a single object gives a sequential analysis of deviance
table for that fit.  That is, the reductions in the residual deviance
as each term of the formula is added in turn are given in as
the rows of a table, plus the residual deviances themselves.
</p>
<p>If more than one object is specified, the table has a row for the
residual degrees of freedom and deviance for each model.
For all but the first model, the change in degrees of freedom and
deviance is also given. (This only makes statistical sense if the
models are nested.)  It is conventional to list the models from
smallest to largest, but this is up to the user.
</p>
<p>The table will optionally contain test statistics (and P values)
comparing the reduction in deviance for the row to the residuals.
For models with known dispersion (e.g., binomial and Poisson fits)
the chi-squared test is most appropriate, and for those with
dispersion estimated by moments (e.g., <code>gaussian</code>,
<code>quasibinomial</code> and <code>quasipoisson</code> fits) the F test is
most appropriate.  If <code>anova.glm</code> can determine which of these
cases applies then by default it will use one of the above tests.
If the <code>dispersion</code> argument is supplied, the dispersion is
considered known and the chi-squared test will be used.
Argument <code>test=FALSE</code> suppresses the test statistics and P values.
Mallows' <code class="reqn">C_p</code> statistic is the residual
deviance plus twice the estimate of <code class="reqn">\sigma^2</code> times
the residual degrees of freedom, which is closely related to AIC (and
a multiple of it if the dispersion is known).
You can also choose <code>"LRT"</code> and
<code>"Rao"</code> for likelihood ratio tests and Rao's efficient score test.
The former is synonymous with <code>"Chisq"</code> (although both have
an asymptotic chi-square distribution).
</p>
<p>The dispersion estimate will be taken from the largest model, using
the value returned by <code><a href="#topic+summary.glm">summary.glm</a></code>.  As this will in most
cases use a Chi-squared-based estimate, the F tests are not based on
the residual deviance in the analysis of deviance table shown.
</p>


<h3>Value</h3>

<p>An object of class <code>"anova"</code> inheriting from class <code>"data.frame"</code>.
</p>


<h3>Warning</h3>

<p>The comparison between two or more models will only be valid if they
are fitted to the same dataset. This may be a problem if there are
missing values and <span class="rlang"><b>R</b></span>'s default of <code>na.action = na.omit</code> is used,
and <code>anova</code> will detect this with an error.
</p>


<h3>References</h3>

<p>Hastie, T. J. and Pregibon, D. (1992)
<em>Generalized linear models.</em>
Chapter 6 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm">glm</a></code>, <code><a href="#topic+anova">anova</a></code>.
</p>
<p><code><a href="#topic+drop1">drop1</a></code> for
so-called &lsquo;type II&rsquo; ANOVA where each term is dropped one at a
time respecting their hierarchy.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## --- Continuing the Example from  '?glm':

anova(glm.D93, test = FALSE)
anova(glm.D93, test = "Cp")
anova(glm.D93, test = "Chisq")
glm.D93a &lt;-
   update(glm.D93, ~treatment*outcome) # equivalent to Pearson Chi-square
anova(glm.D93, glm.D93a, test = "Rao")
</code></pre>

<hr>
<h2 id='anova.lm'>ANOVA for Linear Model Fits</h2><span id='topic+anova.lm'></span><span id='topic+anova.lmlist'></span>

<h3>Description</h3>

<p>Compute an analysis of variance table for one or more linear model fits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lm'
anova(object, ...)

## S3 method for class 'lmlist'
anova(object, ..., scale = 0, test = "F")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="anova.lm_+3A_object">object</code>, <code id="anova.lm_+3A_...">...</code></td>
<td>
<p>objects of class <code>lm</code>, usually, a result of a
call to <code><a href="#topic+lm">lm</a></code>.</p>
</td></tr>
<tr><td><code id="anova.lm_+3A_test">test</code></td>
<td>
<p>a character string specifying the test statistic to be
used. Can be one of <code>"F"</code>, <code>"Chisq"</code> or <code>"Cp"</code>,
with partial matching allowed, or <code>NULL</code> for no test.</p>
</td></tr>
<tr><td><code id="anova.lm_+3A_scale">scale</code></td>
<td>
<p>numeric. An estimate of the noise variance
<code class="reqn">\sigma^2</code>. If zero this will be estimated from the
largest model considered.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Specifying a single object gives a sequential analysis of variance
table for that fit.  That is, the reductions in the residual sum of
squares as each term of the formula is added in turn are given in as
the rows of a table, plus the residual sum of squares.
</p>
<p>The table will contain F statistics (and P values) comparing the
mean square for the row to the residual mean square.
</p>
<p>If more than one object is specified, the table has a row for the
residual degrees of freedom and sum of squares for each model.
For all but the first model, the change in degrees of freedom and sum
of squares is also given. (This only make statistical sense if the
models are nested.)  It is conventional to list the models from
smallest to largest, but this is up to the user.
</p>
<p>Optionally the table can include test statistics.  Normally the
F statistic is most appropriate, which compares the mean square for a
row to the residual sum of squares for the largest model considered.
If <code>scale</code> is specified chi-squared tests can be used. Mallows'
<code class="reqn">C_p</code> statistic is the residual sum of squares plus twice the
estimate of <code class="reqn">\sigma^2</code> times the residual degrees of freedom.
</p>


<h3>Value</h3>

<p>An object of class <code>"anova"</code> inheriting from class <code>"data.frame"</code>.
</p>


<h3>Warning</h3>

<p>The comparison between two or more models will only be valid if they
are fitted to the same dataset. This may be a problem if there are
missing values and <span class="rlang"><b>R</b></span>'s default of <code>na.action = na.omit</code> is used,
and <code>anova.lmlist</code> will detect this with an error.
</p>


<h3>References</h3>

<p>Chambers, J. M. (1992)
<em>Linear models.</em>
Chapter 4 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p>The model fitting function <code><a href="#topic+lm">lm</a></code>, <code><a href="#topic+anova">anova</a></code>.
</p>
<p><code><a href="#topic+drop1">drop1</a></code> for
so-called &lsquo;type II&rsquo; ANOVA where each term is dropped one at a
time respecting their hierarchy.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## sequential table
fit &lt;- lm(sr ~ ., data = LifeCycleSavings)
anova(fit)

## same effect via separate models
fit0 &lt;- lm(sr ~ 1, data = LifeCycleSavings)
fit1 &lt;- update(fit0, . ~ . + pop15)
fit2 &lt;- update(fit1, . ~ . + pop75)
fit3 &lt;- update(fit2, . ~ . + dpi)
fit4 &lt;- update(fit3, . ~ . + ddpi)
anova(fit0, fit1, fit2, fit3, fit4, test = "F")

anova(fit4, fit2, fit0, test = "F") # unconventional order
</code></pre>

<hr>
<h2 id='anova.mlm'>Comparisons between Multivariate Linear Models</h2><span id='topic+anova.mlm'></span>

<h3>Description</h3>

<p>Compute a (generalized) analysis of variance table for one or more
multivariate linear models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mlm'
anova(object, ...,
      test = c("Pillai", "Wilks", "Hotelling-Lawley", "Roy",
               "Spherical"),
      Sigma = diag(nrow = p), T = Thin.row(Proj(M) - Proj(X)),
      M = diag(nrow = p), X = ~0,
      idata = data.frame(index = seq_len(p)), tol = 1e-7)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="anova.mlm_+3A_object">object</code></td>
<td>
<p>an object of class <code>"mlm"</code>.</p>
</td></tr>
<tr><td><code id="anova.mlm_+3A_...">...</code></td>
<td>
<p>further objects of class <code>"mlm"</code>.</p>
</td></tr>
<tr><td><code id="anova.mlm_+3A_test">test</code></td>
<td>
<p>choice of test statistic (see below).  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="anova.mlm_+3A_sigma">Sigma</code></td>
<td>
<p>(only relevant if  <code>test == "Spherical"</code>).  Covariance
matrix assumed proportional to <code>Sigma</code>.</p>
</td></tr>
<tr><td><code id="anova.mlm_+3A_t">T</code></td>
<td>
<p>transformation matrix.  By default computed from <code>M</code> and
<code>X</code>.</p>
</td></tr>
<tr><td><code id="anova.mlm_+3A_m">M</code></td>
<td>
<p>formula or matrix describing the outer projection (see below).</p>
</td></tr>
<tr><td><code id="anova.mlm_+3A_x">X</code></td>
<td>
<p>formula or matrix describing the inner projection (see below).</p>
</td></tr>
<tr><td><code id="anova.mlm_+3A_idata">idata</code></td>
<td>
<p>data frame describing intra-block design.</p>
</td></tr>

<tr><td><code id="anova.mlm_+3A_tol">tol</code></td>
<td>
<p>tolerance to be used in deciding if the residuals are
rank-deficient: see <code><a href="Matrix.html#topic+qr">qr</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>anova.mlm</code> method uses either a multivariate test statistic for
the summary table, or a test based on sphericity assumptions (i.e.
that the covariance is proportional to a given matrix).
</p>
<p>For the multivariate test, Wilks' statistic is most popular in the
literature, but the default Pillai&ndash;Bartlett statistic is
recommended by Hand and Taylor (1987).  See
<code><a href="#topic+summary.manova">summary.manova</a></code> for further details.
</p>
<p>For the <code>"Spherical"</code> test, proportionality is usually with the
identity matrix but a different matrix can be specified using <code>Sigma</code>.
Corrections for asphericity known as the Greenhouse&ndash;Geisser,
respectively Huynh&ndash;Feldt, epsilons are given and adjusted <code class="reqn">F</code> tests are
performed.
</p>
<p>It is common to transform the observations prior to testing. This
typically involves
transformation to intra-block differences, but more complicated
within-block designs can be encountered,
making more elaborate transformations necessary.  A
transformation matrix <code>T</code> can be given directly or specified as
the difference between two projections onto the spaces spanned by
<code>M</code> and <code>X</code>, which in turn can be given as matrices or as
model formulas with respect to <code>idata</code> (the tests will be
invariant to parametrization of the quotient space <code>M/X</code>).
</p>
<p>As with <code>anova.lm</code>, all test statistics use the <abbr>SSD</abbr> matrix from
the largest model considered as the (generalized) denominator.
</p>
<p>Contrary to other <code>anova</code> methods, the intercept is not excluded
from the display in the single-model case.  When contrast
transformations are involved, it often makes good sense to test for a
zero intercept.
</p>


<h3>Value</h3>

<p>An object of class <code>"anova"</code> inheriting from class <code>"data.frame"</code>
</p>


<h3>Note</h3>

<p>The Huynh&ndash;Feldt epsilon differs from that calculated by SAS (as of
v. 8.2) except when the <abbr>DF</abbr> is equal to the number of observations
minus one.  This is believed to be a bug in SAS, not in <span class="rlang"><b>R</b></span>.
</p>


<h3>References</h3>

<p>Hand, D. J. and Taylor, C. C.  (1987)
<em>Multivariate Analysis of Variance and Repeated Measures.</em>
Chapman and Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.manova">summary.manova</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)
utils::example(SSD) # Brings in the mlmfit and reacttime objects

mlmfit0 &lt;- update(mlmfit, ~0)

### Traditional tests of intrasubj. contrasts
## Using MANOVA techniques on contrasts:
anova(mlmfit, mlmfit0, X = ~1)

## Assuming sphericity
anova(mlmfit, mlmfit0, X = ~1, test = "Spherical")


### tests using intra-subject 3x2 design
idata &lt;- data.frame(deg = gl(3, 1, 6, labels = c(0, 4, 8)),
                    noise = gl(2, 3, 6, labels = c("A", "P")))

anova(mlmfit, mlmfit0, X = ~ deg + noise,
      idata = idata, test = "Spherical")
anova(mlmfit, mlmfit0, M = ~ deg + noise, X = ~ noise,
      idata = idata, test = "Spherical" )
anova(mlmfit, mlmfit0, M = ~ deg + noise, X = ~ deg,
      idata = idata, test = "Spherical" )

f &lt;- factor(rep(1:2, 5)) # bogus, just for illustration
mlmfit2 &lt;- update(mlmfit, ~f)
anova(mlmfit2, mlmfit, mlmfit0, X = ~1, test = "Spherical")
anova(mlmfit2, X = ~1, test = "Spherical")
# one-model form, eqiv. to previous

### There seems to be a strong interaction in these data
plot(colMeans(reacttime))
</code></pre>

<hr>
<h2 id='ansari.test'>Ansari-Bradley Test</h2><span id='topic+ansari.test'></span><span id='topic+ansari.test.default'></span><span id='topic+ansari.test.formula'></span>

<h3>Description</h3>

<p>Performs the Ansari-Bradley two-sample test for a difference in scale
parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ansari.test(x, ...)

## Default S3 method:
ansari.test(x, y,
            alternative = c("two.sided", "less", "greater"),
            exact = NULL, conf.int = FALSE, conf.level = 0.95,
            ...)

## S3 method for class 'formula'
ansari.test(formula, data, subset, na.action, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ansari.test_+3A_x">x</code></td>
<td>
<p>numeric vector of data values.</p>
</td></tr>
<tr><td><code id="ansari.test_+3A_y">y</code></td>
<td>
<p>numeric vector of data values.</p>
</td></tr>
<tr><td><code id="ansari.test_+3A_alternative">alternative</code></td>
<td>
<p>indicates the alternative hypothesis and must be
one of <code>"two.sided"</code>, <code>"greater"</code> or <code>"less"</code>.  You
can specify just the initial letter.</p>
</td></tr>
<tr><td><code id="ansari.test_+3A_exact">exact</code></td>
<td>
<p>a logical indicating whether an exact p-value
should be computed.</p>
</td></tr>
<tr><td><code id="ansari.test_+3A_conf.int">conf.int</code></td>
<td>
<p>a logical,indicating whether a confidence interval
should be computed.</p>
</td></tr>
<tr><td><code id="ansari.test_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level of the interval.</p>
</td></tr>
<tr><td><code id="ansari.test_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>lhs ~ rhs</code> where <code>lhs</code>
is a numeric variable giving the data values and <code>rhs</code> a factor
with two levels giving the corresponding groups.</p>
</td></tr>
<tr><td><code id="ansari.test_+3A_data">data</code></td>
<td>
<p>an optional matrix or data frame (or similar: see
<code><a href="#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="ansari.test_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="ansari.test_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.  Defaults to
<code>getOption("na.action")</code>.</p>
</td></tr>
<tr><td><code id="ansari.test_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suppose that <code>x</code> and <code>y</code> are independent samples from
distributions with densities <code class="reqn">f((t-m)/s)/s</code> and <code class="reqn">f(t-m)</code>,
respectively, where <code class="reqn">m</code> is an unknown nuisance parameter and
<code class="reqn">s</code>, the ratio of scales, is the parameter of interest.  The
Ansari-Bradley test is used for testing the null that <code class="reqn">s</code> equals
1, the two-sided alternative being that <code class="reqn">s \ne 1</code> (the
distributions differ only in variance), and the one-sided alternatives
being <code class="reqn">s &gt; 1</code> (the distribution underlying <code>x</code> has a larger
variance, <code>"greater"</code>) or <code class="reqn">s &lt; 1</code> (<code>"less"</code>).
</p>
<p>By default (if <code>exact</code> is not specified), an exact p-value
is computed if both samples contain less than 50 finite values and
there are no ties.  Otherwise, a normal approximation is used.
</p>
<p>Optionally, a nonparametric confidence interval and an estimator for
<code class="reqn">s</code> are computed.  If exact p-values are available, an exact
confidence interval is obtained by the algorithm described in Bauer
(1972), and the Hodges-Lehmann estimator is employed.  Otherwise, the
returned confidence interval and point estimate are based on normal
approximations.
</p>
<p>Note that mid-ranks are used in the case of ties rather than average
scores as employed in Hollander &amp; Wolfe (1973).
See, e.g., Hajek, Sidak and Sen (1999), pages 131ff, for
more information.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the Ansari-Bradley test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>the ratio of scales <code class="reqn">s</code> under the null, 1.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative
hypothesis.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the string <code>"Ansari-Bradley test"</code>.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the names of the data.</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>a confidence interval for the scale parameter.
(Only present if argument <code>conf.int = TRUE</code>.)</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>an estimate of the ratio of scales.
(Only present if argument <code>conf.int = TRUE</code>.)</p>
</td></tr>
</table>


<h3>Note</h3>

<p>To compare results of the Ansari-Bradley test to those of the F test
to compare two variances (under the assumption of normality), observe
that <code class="reqn">s</code> is the ratio of scales and hence <code class="reqn">s^2</code> is the ratio
of variances (provided they exist), whereas for the F test the ratio
of variances itself is the parameter of interest.  In particular,
confidence intervals are for <code class="reqn">s</code> in the Ansari-Bradley test but
for <code class="reqn">s^2</code> in the F test.
</p>


<h3>References</h3>

<p>David F. Bauer (1972).
Constructing confidence sets using rank statistics.
<em>Journal of the American Statistical Association</em>,
<b>67</b>, 687&ndash;690.
<a href="https://doi.org/10.1080/01621459.1972.10481279">doi:10.1080/01621459.1972.10481279</a>.
</p>
<p>Jaroslav Hajek, Zbynek Sidak and Pranab K. Sen (1999).
<em>Theory of Rank Tests</em>.
San Diego, London: Academic Press.
</p>
<p>Myles Hollander and Douglas A. Wolfe (1973).
<em>Nonparametric Statistical Methods</em>.
New York: John Wiley &amp; Sons.
Pages 83&ndash;92.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fligner.test">fligner.test</a></code> for a rank-based (nonparametric)
<code class="reqn">k</code>-sample test for homogeneity of variances;
<code><a href="#topic+mood.test">mood.test</a></code> for another rank-based two-sample test for a
difference in scale parameters;
<code><a href="#topic+var.test">var.test</a></code> and <code><a href="#topic+bartlett.test">bartlett.test</a></code> for parametric
tests for the homogeneity in variance.
</p>
<p><code><a href="coin.html#topic+ScaleTests">ansari_test</a></code> in package <a href="https://CRAN.R-project.org/package=coin"><span class="pkg">coin</span></a>
for exact and approximate <em>conditional</em> p-values for the
Ansari-Bradley test, as well as different methods for handling ties.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Hollander &amp; Wolfe (1973, p. 86f):
## Serum iron determination using Hyland control sera
ramsay &lt;- c(111, 107, 100, 99, 102, 106, 109, 108, 104, 99,
            101, 96, 97, 102, 107, 113, 116, 113, 110, 98)
jung.parekh &lt;- c(107, 108, 106, 98, 105, 103, 110, 105, 104,
            100, 96, 108, 103, 104, 114, 114, 113, 108, 106, 99)
ansari.test(ramsay, jung.parekh)

ansari.test(rnorm(10), rnorm(10, 0, 2), conf.int = TRUE)

## try more points - failed in 2.4.1
ansari.test(rnorm(100), rnorm(100, 0, 2), conf.int = TRUE)
</code></pre>

<hr>
<h2 id='aov'>Fit an Analysis of Variance Model</h2><span id='topic+aov'></span><span id='topic+print.aov'></span><span id='topic+print.aovlist'></span><span id='topic+Error'></span>

<h3>Description</h3>

<p>Fit an analysis of variance model by a call to <code>lm</code> (for each
stratum if an <code>Error(.)</code> is used).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aov(formula, data = NULL, projections = FALSE, qr = TRUE,
    contrasts = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aov_+3A_formula">formula</code></td>
<td>
<p>A formula specifying the model.</p>
</td></tr>
<tr><td><code id="aov_+3A_data">data</code></td>
<td>
<p>A data frame in which the variables specified in the
formula will be found. If missing, the variables are searched for in
the standard way.</p>
</td></tr>
<tr><td><code id="aov_+3A_projections">projections</code></td>
<td>
<p>Logical flag: should the projections be returned?</p>
</td></tr>
<tr><td><code id="aov_+3A_qr">qr</code></td>
<td>
<p>Logical flag: should the QR decomposition be returned?</p>
</td></tr>
<tr><td><code id="aov_+3A_contrasts">contrasts</code></td>
<td>
<p>A list of contrasts to be used for some of the factors
in the formula. These are not used for any <code>Error</code> term, and
supplying contrasts for factors only in the <code>Error</code> term will give
a warning.</p>
</td></tr>
<tr><td><code id="aov_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to <code>lm</code>, such as <code>subset</code>
or <code>na.action</code>.  See &lsquo;Details&rsquo; about <code>weights</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This provides a wrapper to <code><a href="#topic+lm">lm</a></code> for fitting linear models to
balanced or unbalanced experimental designs.
</p>
<p>The main difference from <code>lm</code> is in the way <code>print</code>,
<code>summary</code> and so on handle the fit: this is expressed in the
traditional language of the analysis of variance rather than that of
linear models.
</p>
<p>If the formula contains a single <code>Error</code> term, this is used to
specify error strata, and appropriate models are fitted within each
error stratum.
</p>
<p>The formula can specify multiple responses.
</p>
<p>Weights can be specified by a <code>weights</code> argument, but should not
be used with an <code>Error</code> term, and are incompletely supported
(e.g., not by <code><a href="#topic+model.tables">model.tables</a></code>).
</p>


<h3>Value</h3>

<p>An object of class <code>c("aov", "lm")</code> or for multiple responses
of class <code>c("maov", "aov", "mlm", "lm")</code> or for multiple error
strata of class <code>c("aovlist", "<a href="#topic+listof">listof</a>")</code>.  There are
<code><a href="base.html#topic+print">print</a></code> and <code><a href="base.html#topic+summary">summary</a></code> methods available for these.
</p>


<h3>Note</h3>

<p><code>aov</code> is designed for balanced designs, and the results can be
hard to interpret without balance: beware that missing values in the
response(s) will likely lose the balance.  If there are two or more
error strata, the methods used are statistically inefficient without
balance, and it may be better to use <code><a href="nlme.html#topic+lme">lme</a></code> in
package <a href="https://CRAN.R-project.org/package=nlme"><span class="pkg">nlme</span></a>.
</p>
<p>Balance can be checked with the <code><a href="#topic+replications">replications</a></code> function.
</p>
<p>The default &lsquo;contrasts&rsquo; in <span class="rlang"><b>R</b></span> are not orthogonal contrasts, and
<code>aov</code> and its helper functions will work better with such
contrasts: see the examples for how to select these.
</p>


<h3>Author(s)</h3>

<p>The design was inspired by the S function of the same name described
in Chambers <abbr>et al.</abbr> (1992).
</p>


<h3>References</h3>

<p>Chambers, J. M., Freeny, A and Heiberger, R. M. (1992)
<em>Analysis of variance; designed experiments.</em>
Chapter 5 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lm">lm</a></code>, <code><a href="#topic+summary.aov">summary.aov</a></code>,
<code><a href="#topic+replications">replications</a></code>, <code><a href="#topic+alias">alias</a></code>,
<code><a href="#topic+proj">proj</a></code>, <code><a href="#topic+model.tables">model.tables</a></code>, <code><a href="#topic+TukeyHSD">TukeyHSD</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## From Venables and Ripley (2002) p.165.

## Set orthogonal contrasts.
op &lt;- options(contrasts = c("contr.helmert", "contr.poly"))
( npk.aov &lt;- aov(yield ~ block + N*P*K, npk) )
summary(npk.aov)
coefficients(npk.aov)

## to show the effects of re-ordering terms contrast the two fits
aov(yield ~ block + N * P + K, npk)
aov(terms(yield ~ block + N * P + K, keep.order = TRUE), npk)


## as a test, not particularly sensible statistically
npk.aovE &lt;- aov(yield ~  N*P*K + Error(block), npk)
npk.aovE
summary(npk.aovE)
options(op)  # reset to previous
</code></pre>

<hr>
<h2 id='approxfun'>Interpolation Functions</h2><span id='topic+approx'></span><span id='topic+approxfun'></span>

<h3>Description</h3>

<p>Return a list of points which linearly interpolate given data points,
or a function performing the linear (or constant) interpolation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>approx   (x, y = NULL, xout, method = "linear", n = 50,
          yleft, yright, rule = 1, f = 0, ties = mean, na.rm = TRUE)

approxfun(x, y = NULL,       method = "linear",
          yleft, yright, rule = 1, f = 0, ties = mean, na.rm = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="approxfun_+3A_x">x</code>, <code id="approxfun_+3A_y">y</code></td>
<td>
<p>numeric vectors giving the coordinates of the points to be
interpolated.  Alternatively a single plotting structure can be
specified: see <code><a href="grDevices.html#topic+xy.coords">xy.coords</a></code>.</p>
</td></tr>
<tr><td><code id="approxfun_+3A_xout">xout</code></td>
<td>
<p>an optional set of numeric values specifying where
interpolation is to take place.</p>
</td></tr>
<tr><td><code id="approxfun_+3A_method">method</code></td>
<td>
<p>specifies the interpolation method to be used.  Choices
are <code>"linear"</code> or <code>"constant"</code>.</p>
</td></tr>
<tr><td><code id="approxfun_+3A_n">n</code></td>
<td>
<p>If <code>xout</code> is not specified, interpolation takes place at
<code>n</code> equally spaced points spanning the interval [<code>min(x)</code>,
<code>max(x)</code>].</p>
</td></tr>
<tr><td><code id="approxfun_+3A_yleft">yleft</code></td>
<td>
<p>the value to be returned when input <code>x</code> values are
less than <code>min(x)</code>. The default is defined by the value
of <code>rule</code> given below.</p>
</td></tr>
<tr><td><code id="approxfun_+3A_yright">yright</code></td>
<td>
<p>the value to be returned when input <code>x</code> values are
greater than <code>max(x)</code>. The default is defined by the value
of <code>rule</code> given below.</p>
</td></tr>
<tr><td><code id="approxfun_+3A_rule">rule</code></td>
<td>
<p>an integer (of length 1 or 2) describing how interpolation
is to take place outside the interval [<code>min(x)</code>, <code>max(x)</code>].
If <code>rule</code> is <code>1</code> then <code>NA</code>s are returned for such
points and if it is <code>2</code>, the value at the closest data extreme
is used.  Use, e.g., <code>rule = 2:1</code>, if the left and right side
extrapolation should differ.</p>
</td></tr>
<tr><td><code id="approxfun_+3A_f">f</code></td>
<td>
<p>for <code>method = "constant"</code> a number between 0 and 1
inclusive, indicating a compromise between left- and
right-continuous step functions. If <code>y0</code> and <code>y1</code> are
the values to the left and right of the point then the value is
<code>y0</code> if <code>f == 0</code>, <code>y1</code> if <code>f == 1</code>, and
<code> y0*(1-f)+y1*f</code> for intermediate values. In this way the result is
right-continuous for <code>f == 0</code> and left-continuous for <code>f
    == 1</code>, even for non-finite <code>y</code> values.</p>
</td></tr>
<tr><td><code id="approxfun_+3A_ties">ties</code></td>
<td>
<p>handling of tied <code>x</code> values.  The string
<code>"ordered"</code> or a function (or the name of a function)
taking a single vector argument and returning a single number
or a <code><a href="base.html#topic+list">list</a></code> of both, e.g.,
<code>list("ordered", mean)</code>, see &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="approxfun_+3A_na.rm">na.rm</code></td>
<td>
<p>logical specifying how missing values (<code><a href="base.html#topic+NA">NA</a></code>'s)
should be handled.  Setting <code>na.rm=FALSE</code> will propagate
<code>NA</code>'s in <code>y</code> to the interpolated values, also depending on
the <code>rule</code> set.  Note that in this case, <code>NA</code>'s in <code>x</code>
are invalid, see also the examples.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The inputs can contain missing values which are deleted (if <code>na.rm</code>
is true, i.e., by default), so at least
two complete <code>(x, y)</code> pairs are required (for <code>method =
  "linear"</code>, one otherwise).  If there are duplicated (tied) <code>x</code>
values and <code>ties</code> contains a function it is applied to the <code>y</code>
values for each distinct <code>x</code> value to produce <code>(x,y)</code> pairs
with unique <code>x</code>.
Useful functions in this context include <code><a href="base.html#topic+mean">mean</a></code>,
<code><a href="base.html#topic+min">min</a></code>, and <code><a href="base.html#topic+max">max</a></code>.
</p>
<p>If <code>ties = "ordered"</code> the <code>x</code> values are assumed to be already
ordered (and unique) and ties are <em>not</em> checked but kept if present.
This is the fastest option for large <code>length(x)</code>.
</p>
<p>If <code>ties</code> is a <code><a href="base.html#topic+list">list</a></code> of length two, <code>ties[[2]]</code>
must be a function to be applied to ties, see above, but if
<code>ties[[1]]</code> is identical to <code>"ordered"</code>, the <code>x</code> values
are assumed to be sorted and are only checked for ties.  Consequently,
<code>ties = list("ordered", mean)</code> will be slightly more efficient than
the default <code>ties = mean</code> in such a case.
</p>
<p>The first <code>y</code> value will be used for interpolation to the left and the last
one for interpolation to the right.
</p>


<h3>Value</h3>

<p><code>approx</code> returns a list with components <code>x</code> and <code>y</code>,
containing <code>n</code> coordinates which interpolate the given data
points according to the <code>method</code> (and <code>rule</code>) desired.
</p>
<p>The function <code>approxfun</code> returns a function performing (linear or
constant) interpolation of the given data points.  For a given set of
<code>x</code> values, this function will return the corresponding
interpolated values.  It uses data stored in its environment when it
was created, the details of which are subject to change.
</p>


<h3>Warning</h3>

<p>The value returned by <code>approxfun</code> contains references to the code
in the current version of <span class="rlang"><b>R</b></span>: it is not intended to be saved and
loaded into a different <span class="rlang"><b>R</b></span> session.  This is safer for <span class="rlang"><b>R</b></span> &gt;= 3.0.0.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spline">spline</a></code> and <code><a href="#topic+splinefun">splinefun</a></code> for spline
interpolation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

x &lt;- 1:10
y &lt;- rnorm(10)
par(mfrow = c(2,1))
plot(x, y, main = "approx(.) and approxfun(.)")
points(approx(x, y), col = 2, pch = "*")
points(approx(x, y, method = "constant"), col = 4, pch = "*")

f &lt;- approxfun(x, y)
curve(f(x), 0, 11, col = "green2")
points(x, y)
is.function(fc &lt;- approxfun(x, y, method = "const")) # TRUE
curve(fc(x), 0, 10, col = "darkblue", add = TRUE)
## different extrapolation on left and right side :
plot(approxfun(x, y, rule = 2:1), 0, 11,
     col = "tomato", add = TRUE, lty = 3, lwd = 2)

### Treatment of 'NA's -- are kept if  na.rm=FALSE :

xn &lt;- 1:4
yn &lt;- c(1,NA,3:4)
xout &lt;- (1:9)/2
## Default behavior (na.rm = TRUE): NA's omitted; extrapolation gives NA
data.frame(approx(xn,yn, xout))
data.frame(approx(xn,yn, xout, rule = 2))# -&gt; *constant* extrapolation
## New (2019-2020)  na.rm = FALSE: NA's are "kept"
data.frame(approx(xn,yn, xout, na.rm=FALSE, rule = 2))
data.frame(approx(xn,yn, xout, na.rm=FALSE, rule = 2, method="constant"))

## NA's in x[] are not allowed:
stopifnot(inherits( try( approx(yn,yn, na.rm=FALSE) ), "try-error"))

## Give a nice overview of all possibilities  rule * method * na.rm :
##             -----------------------------  ====   ======   =====
## extrapolations "N":= NA;   "C":= Constant :
rules &lt;- list(N=1, C=2, NC=1:2, CN=2:1)
methods &lt;- c("constant","linear")
ry &lt;- sapply(rules, function(R) {
       sapply(methods, function(M)
        sapply(setNames(,c(TRUE,FALSE)), function(na.)
                 approx(xn, yn, xout=xout, method=M, rule=R, na.rm=na.)$y),
        simplify="array")
   }, simplify="array")
names(dimnames(ry)) &lt;- c("x = ", "na.rm", "method", "rule")
dimnames(ry)[[1]] &lt;- format(xout)
ftable(aperm(ry, 4:1)) # --&gt; (4 * 2 * 2) x length(xout)  =  16 x 9 matrix


## Show treatment of 'ties' :

x &lt;- c(2,2:4,4,4,5,5,7,7,7)
y &lt;- c(1:6, 5:4, 3:1)
(amy &lt;- approx(x, y, xout = x)$y) # warning, can be avoided by specifying 'ties=':
op &lt;- options(warn=2) # warnings would be error
stopifnot(identical(amy, approx(x, y, xout = x, ties=mean)$y))
(ay &lt;- approx(x, y, xout = x, ties = "ordered")$y)
stopifnot(amy == c(1.5,1.5, 3, 5,5,5, 4.5,4.5, 2,2,2),
          ay  == c(2, 2,    3, 6,6,6, 4, 4,    1,1,1))
approx(x, y, xout = x, ties = min)$y
approx(x, y, xout = x, ties = max)$y
options(op) # revert 'warn'ing level
</code></pre>

<hr>
<h2 id='ar'>Fit Autoregressive Models to Time Series</h2><span id='topic+ar'></span><span id='topic+ar.burg'></span><span id='topic+ar.burg.default'></span><span id='topic+ar.burg.mts'></span><span id='topic+ar.yw'></span><span id='topic+ar.yw.default'></span><span id='topic+ar.yw.mts'></span><span id='topic+ar.mle'></span><span id='topic+print.ar'></span><span id='topic+predict.ar'></span>

<h3>Description</h3>

<p>Fit an autoregressive time series model to the data, by default
selecting the complexity by AIC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ar(x, aic = TRUE, order.max = NULL,
   method = c("yule-walker", "burg", "ols", "mle", "yw"),
   na.action, series, ...)

ar.burg(x, ...)
## Default S3 method:
ar.burg(x, aic = TRUE, order.max = NULL,
        na.action = na.fail, demean = TRUE, series,
        var.method = 1, ...)
## S3 method for class 'mts'
ar.burg(x, aic = TRUE, order.max = NULL,
        na.action = na.fail, demean = TRUE, series,
        var.method = 1, ...)

ar.yw(x, ...)
## Default S3 method:
ar.yw(x, aic = TRUE, order.max = NULL,
      na.action = na.fail, demean = TRUE, series, ...)
## S3 method for class 'mts'
ar.yw(x, aic = TRUE, order.max = NULL,
      na.action = na.fail, demean = TRUE, series,
      var.method = 1, ...)

ar.mle(x, aic = TRUE, order.max = NULL, na.action = na.fail,
       demean = TRUE, series, ...)

## S3 method for class 'ar'
predict(object, newdata, n.ahead = 1, se.fit = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ar_+3A_x">x</code></td>
<td>
<p>a univariate or multivariate time series.</p>
</td></tr>
<tr><td><code id="ar_+3A_aic">aic</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code>.  If <code>TRUE</code> then the Akaike Information
Criterion is used to choose the order of the autoregressive
model.  If <code>FALSE</code>, the model of order <code>order.max</code> is
fitted.</p>
</td></tr>
<tr><td><code id="ar_+3A_order.max">order.max</code></td>
<td>
<p>maximum order (or order) of model to fit.  Defaults
to the smaller of <code class="reqn">N-1</code> and <code class="reqn">10\log_{10}(N)</code>
where <code class="reqn">N</code> is the number of non-missing observations
except for <code>method = "mle"</code> where it is the minimum of this
quantity and 12.</p>
</td></tr>
<tr><td><code id="ar_+3A_method">method</code></td>
<td>
<p>character string specifying the method to fit the
model.  Must be one of the strings in the default argument
(the first few characters are sufficient).  Defaults to
<code>"yule-walker"</code>.</p>
</td></tr>
<tr><td><code id="ar_+3A_na.action">na.action</code></td>
<td>
<p>function to be called to handle missing
values.  Currently, via <code>na.action = na.pass</code>, only Yule-Walker
method can handle missing values which must be consistent within a
time point: either all variables must be missing or none.</p>
</td></tr>
<tr><td><code id="ar_+3A_demean">demean</code></td>
<td>
<p>should a mean be estimated during fitting?</p>
</td></tr>
<tr><td><code id="ar_+3A_series">series</code></td>
<td>
<p>names for the series.  Defaults to
<code>deparse1(substitute(x))</code>.</p>
</td></tr>
<tr><td><code id="ar_+3A_var.method">var.method</code></td>
<td>
<p>the method to estimate the innovations variance
(see &lsquo;Details&rsquo;).</p>
</td></tr>
<tr><td><code id="ar_+3A_...">...</code></td>
<td>
<p>additional arguments for specific methods.</p>
</td></tr>
<tr><td><code id="ar_+3A_object">object</code></td>
<td>
<p>a fit from <code>ar()</code>.</p>
</td></tr>
<tr><td><code id="ar_+3A_newdata">newdata</code></td>
<td>
<p>data to which to apply the prediction.</p>
</td></tr>
<tr><td><code id="ar_+3A_n.ahead">n.ahead</code></td>
<td>
<p>number of steps ahead at which to predict.</p>
</td></tr>
<tr><td><code id="ar_+3A_se.fit">se.fit</code></td>
<td>
<p>logical: return estimated standard errors of the
prediction error?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For definiteness, note that the AR coefficients have the sign in
</p>
<p style="text-align: center;"><code class="reqn">x_t - \mu = a_1(x_{t-1} - \mu) + \cdots +  a_p(x_{t-p} - \mu) + e_t</code>
</p>

<p><code>ar</code> is just a wrapper for the functions <code>ar.yw</code>,
<code>ar.burg</code>, <code><a href="#topic+ar.ols">ar.ols</a></code> and <code>ar.mle</code>.
</p>
<p>Order selection is done by AIC if <code>aic</code> is true. This is
problematic, as of the methods here only <code>ar.mle</code> performs
true maximum likelihood estimation. The AIC is computed as if the variance
estimate were the MLE, omitting the determinant term from the
likelihood. Note that this is not the same as the Gaussian likelihood
evaluated at the estimated parameter values.  In <code>ar.yw</code> the
variance matrix of the innovations is computed from the fitted
coefficients and the autocovariance of <code>x</code>.
</p>
<p><code>ar.burg</code> allows two methods to estimate the innovations
variance and hence AIC. Method 1 is to use the update given by
the Levinson-Durbin recursion
(Brockwell and Davis, 1991, (8.2.6) on page 242),
and follows S-PLUS.  Method 2 is the mean of the sum
of squares of the forward and backward prediction errors
(as in Brockwell and Davis, 1996, page 145).
Percival and Walden (1998) discuss both.
In the multivariate case the estimated
coefficients will depend (slightly) on the variance estimation method.
</p>
<p>Remember that <code>ar</code> includes by default a constant in the model, by
removing the overall mean of <code>x</code> before fitting the AR model,
or (<code>ar.mle</code>) estimating a constant to subtract.
</p>


<h3>Value</h3>

<p>For <code>ar</code> and its methods a list of class <code>"ar"</code> with
the following elements:
</p>
<table>
<tr><td><code>order</code></td>
<td>
<p>The order of the fitted model.  This is chosen by
minimizing the AIC if <code>aic = TRUE</code>, otherwise it is <code>order.max</code>.</p>
</td></tr>
<tr><td><code>ar</code></td>
<td>
<p>Estimated autoregression coefficients for the fitted model.</p>
</td></tr>
<tr><td><code>var.pred</code></td>
<td>
<p>The prediction variance: an estimate of the portion of the
variance of the time series that is not explained by the
autoregressive model.</p>
</td></tr>
<tr><td><code>x.mean</code></td>
<td>
<p>The estimated mean of the series used in fitting and for
use in prediction.</p>
</td></tr>
<tr><td><code>x.intercept</code></td>
<td>
<p>(<code>ar.ols</code> only.) The intercept in the model for
<code>x - x.mean</code>.</p>
</td></tr>
<tr><td><code>aic</code></td>
<td>
<p>The differences in AIC between each model and the
best-fitting model.  Note that the latter can have an AIC of <code>-Inf</code>.</p>
</td></tr>
<tr><td><code>n.used</code></td>
<td>
<p>The number of observations in the time series, including
missing.</p>
</td></tr>
<tr><td><code>n.obs</code></td>
<td>
<p>The number of non-missing observations in the time series.</p>
</td></tr>
<tr><td><code>order.max</code></td>
<td>
<p>The value of the <code>order.max</code> argument.</p>
</td></tr>
<tr><td><code>partialacf</code></td>
<td>
<p>The estimate of the partial autocorrelation function
up to lag <code>order.max</code>.</p>
</td></tr>
<tr><td><code>resid</code></td>
<td>
<p>residuals from the fitted model, conditioning on the
first <code>order</code> observations. The first <code>order</code> residuals
are set to <code>NA</code>. If <code>x</code> is a time series, so is <code>resid</code>.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>The value of the <code>method</code> argument.</p>
</td></tr>
<tr><td><code>series</code></td>
<td>
<p>The name(s) of the time series.</p>
</td></tr>
<tr><td><code>frequency</code></td>
<td>
<p>The frequency of the time series.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>asy.var.coef</code></td>
<td>
<p>(univariate case, <code>order &gt; 0</code>.)
The asymptotic-theory variance matrix of the coefficient estimates.</p>
</td></tr>
</table>
<p>For <code>predict.ar</code>, a time series of predictions, or if
<code>se.fit = TRUE</code>, a list with components <code>pred</code>, the
predictions, and <code>se</code>, the estimated standard errors.  Both
components are time series.
</p>


<h3>Note</h3>

<p>Only the univariate case of <code>ar.mle</code> is implemented.
</p>
<p>Fitting by <code>method="mle"</code> to long series can be very slow.
</p>
<p>If <code>x</code> contains missing values, see <code><a href="base.html#topic+NA">NA</a></code>, also consider
using <code><a href="#topic+arima">arima</a>()</code>, possibly with <code>method = "ML"</code>.
</p>


<h3>Author(s)</h3>

<p>Martyn Plummer. Univariate case of <code>ar.yw</code>, <code>ar.mle</code>
and C code for univariate case of <code>ar.burg</code> by B. D. Ripley.
</p>


<h3>References</h3>

<p>Brockwell, P. J. and Davis, R. A. (1991).
<em>Time Series and Forecasting Methods</em>, second edition.
Springer, New York.
Section 11.4.
</p>
<p>Brockwell, P. J. and Davis, R. A. (1996).
<em>Introduction to Time Series and Forecasting</em>.
Springer, New York.
Sections 5.1 and 7.6.
</p>
<p>Percival, D. P. and Walden, A. T. (1998).
<em>Spectral Analysis for Physical Applications</em>.
Cambridge University Press.
</p>
<p>Whittle, P. (1963).
On the fitting of multivariate autoregressions and the approximate
canonical factorization of a spectral density matrix.
<em>Biometrika</em>, <b>40</b>, 129&ndash;134.
<a href="https://doi.org/10.2307/2333753">doi:10.2307/2333753</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ar.ols">ar.ols</a></code>, <code><a href="#topic+arima">arima</a></code> for ARMA models;
<code><a href="#topic+acf2AR">acf2AR</a></code>, for AR construction from the ACF.
</p>
<p><code><a href="#topic+arima.sim">arima.sim</a></code> for simulation of AR processes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ar(lh)
ar(lh, method = "burg")
ar(lh, method = "ols")
ar(lh, FALSE, 4) # fit ar(4)

(sunspot.ar &lt;- ar(sunspot.year))
predict(sunspot.ar, n.ahead = 25)
## try the other methods too

ar(ts.union(BJsales, BJsales.lead))
## Burg is quite different here, as is OLS (see ar.ols)
ar(ts.union(BJsales, BJsales.lead), method = "burg")
</code></pre>

<hr>
<h2 id='ar.ols'>Fit Autoregressive Models to Time Series by OLS</h2><span id='topic+ar.ols'></span>

<h3>Description</h3>

<p>Fit an autoregressive time series model to the data by ordinary
least squares, by default selecting the complexity by AIC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ar.ols(x, aic = TRUE, order.max = NULL, na.action = na.fail,
       demean = TRUE, intercept = demean, series, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ar.ols_+3A_x">x</code></td>
<td>
<p>A univariate or multivariate time series.</p>
</td></tr>
<tr><td><code id="ar.ols_+3A_aic">aic</code></td>
<td>
<p>Logical flag.  If <code>TRUE</code> then the Akaike Information
Criterion is used to choose the order of the autoregressive
model. If <code>FALSE</code>, the model of order <code>order.max</code> is
fitted.</p>
</td></tr>
<tr><td><code id="ar.ols_+3A_order.max">order.max</code></td>
<td>
<p>Maximum order (or order) of model to fit. Defaults
to <code class="reqn">10\log_{10}(N)</code> where <code class="reqn">N</code> is the number
of observations.</p>
</td></tr>
<tr><td><code id="ar.ols_+3A_na.action">na.action</code></td>
<td>
<p>function to be called to handle missing values.</p>
</td></tr>
<tr><td><code id="ar.ols_+3A_demean">demean</code></td>
<td>
<p>should the AR model be for <code>x</code> minus its mean?</p>
</td></tr>
<tr><td><code id="ar.ols_+3A_intercept">intercept</code></td>
<td>
<p>should a separate intercept term be fitted?</p>
</td></tr>
<tr><td><code id="ar.ols_+3A_series">series</code></td>
<td>
<p>names for the series.  Defaults to
<code>deparse1(substitute(x))</code>.</p>
</td></tr>
<tr><td><code id="ar.ols_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ar.ols</code> fits the general AR model to a possibly non-stationary
and/or multivariate system of series <code>x</code>. The resulting
unconstrained least squares estimates are consistent, even if
some of the series are non-stationary and/or co-integrated.
For definiteness, note that the AR coefficients have the sign in
</p>
<p style="text-align: center;"><code class="reqn">x_t - \mu = a_0 + a_1(x_{t-1} - \mu) + \cdots + a_p(x_{t-p} - \mu) + e_t</code>
</p>

<p>where <code class="reqn">a_0</code> is zero unless <code>intercept</code> is true, and
<code class="reqn">\mu</code> is the sample mean if <code>demean</code> is true, zero
otherwise.
</p>
<p>Order selection is done by AIC if <code>aic</code> is true. This is
problematic, as <code>ar.ols</code> does not perform
true maximum likelihood estimation. The AIC is computed as if
the variance estimate (computed from the variance matrix of the
residuals) were the MLE, omitting the determinant term from the
likelihood. Note that this is not the same as the Gaussian
likelihood evaluated at the estimated parameter values.
</p>
<p>Some care is needed if <code>intercept</code> is true and <code>demean</code> is
false. Only use this is the series are roughly centred on
zero. Otherwise the computations may be inaccurate or fail entirely.
</p>


<h3>Value</h3>

<p>A list of class <code>"ar"</code> with the following elements:
</p>
<table>
<tr><td><code>order</code></td>
<td>
<p>The order of the fitted model.  This is chosen by
minimizing the AIC if <code>aic = TRUE</code>, otherwise it is
<code>order.max</code>.</p>
</td></tr>
<tr><td><code>ar</code></td>
<td>
<p>Estimated autoregression coefficients for the fitted
model.</p>
</td></tr>
<tr><td><code>var.pred</code></td>
<td>
<p>The prediction variance: an estimate of the portion of
the variance of the time series that is not explained by the
autoregressive model.</p>
</td></tr>
<tr><td><code>x.mean</code></td>
<td>
<p>The estimated mean (or zero if <code>demean</code> is false)
of the series used in fitting and for use in prediction.</p>
</td></tr>
<tr><td><code>x.intercept</code></td>
<td>
<p>The intercept in the model for
<code>x - x.mean</code>, or zero if <code>intercept</code> is false.</p>
</td></tr>
<tr><td><code>aic</code></td>
<td>
<p>The differences in AIC between each model and the
best-fitting model.  Note that the latter can have an AIC of <code>-Inf</code>.</p>
</td></tr>
<tr><td><code>n.used</code></td>
<td>
<p>The number of observations in the time series.</p>
</td></tr>
<tr><td><code>order.max</code></td>
<td>
<p>The value of the <code>order.max</code> argument.</p>
</td></tr>
<tr><td><code>partialacf</code></td>
<td>
<p><code>NULL</code>.  For compatibility with <code>ar</code>.</p>
</td></tr>
<tr><td><code>resid</code></td>
<td>
<p>residuals from the fitted model, conditioning on the
first <code>order</code> observations.  The first <code>order</code> residuals
are set to <code>NA</code>. If <code>x</code> is a time series, so is
<code>resid</code>.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>The character string <code>"Unconstrained LS"</code>.</p>
</td></tr>
<tr><td><code>series</code></td>
<td>
<p>The name(s) of the time series.</p>
</td></tr>
<tr><td><code>frequency</code></td>
<td>
<p>The frequency of the time series.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>asy.se.coef</code></td>
<td>
<p>The asymptotic-theory standard errors of the
coefficient estimates.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Adrian Trapletti, Brian Ripley.</p>


<h3>References</h3>

<p>Luetkepohl, H. (1991): <em>Introduction to Multiple Time Series
Analysis.</em> Springer Verlag, NY, pp. 368&ndash;370.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ar">ar</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ar(lh, method = "burg")
ar.ols(lh)
ar.ols(lh, FALSE, 4) # fit ar(4)

ar.ols(ts.union(BJsales, BJsales.lead))

x &lt;- diff(log(EuStockMarkets))
ar.ols(x, order.max = 6, demean = FALSE, intercept = TRUE)
</code></pre>

<hr>
<h2 id='arima'>ARIMA Modelling of Time Series</h2><span id='topic+arima'></span>

<h3>Description</h3>

<p>Fit an ARIMA model to a univariate time series.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>arima(x, order = c(0L, 0L, 0L),
      seasonal = list(order = c(0L, 0L, 0L), period = NA),
      xreg = NULL, include.mean = TRUE,
      transform.pars = TRUE,
      fixed = NULL, init = NULL,
      method = c("CSS-ML", "ML", "CSS"), n.cond,
      SSinit = c("Gardner1980", "Rossignol2011"),
      optim.method = "BFGS",
      optim.control = list(), kappa = 1e6)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="arima_+3A_x">x</code></td>
<td>
<p>a univariate time series</p>
</td></tr>
<tr><td><code id="arima_+3A_order">order</code></td>
<td>
<p>A specification of the non-seasonal part of the ARIMA
model: the three integer components <code class="reqn">(p, d, q)</code> are the AR order, the
degree of differencing, and the MA order.</p>
</td></tr>
<tr><td><code id="arima_+3A_seasonal">seasonal</code></td>
<td>
<p>A specification of the seasonal part of the ARIMA
model, plus the period (which defaults to <code>frequency(x)</code>).
This may be a list with components <code>order</code> and
<code>period</code>, or just a numeric vector of length 3 which
specifies the seasonal <code>order</code>.  In the latter case the
default period is used.</p>
</td></tr>
<tr><td><code id="arima_+3A_xreg">xreg</code></td>
<td>
<p>Optionally, a vector or matrix of external regressors,
which must have the same number of rows as <code>x</code>.</p>
</td></tr>
<tr><td><code id="arima_+3A_include.mean">include.mean</code></td>
<td>
<p>Should the ARMA model include a mean/intercept term?  The
default is <code>TRUE</code> for undifferenced series, and it is ignored
for ARIMA models with differencing.</p>
</td></tr>
<tr><td><code id="arima_+3A_transform.pars">transform.pars</code></td>
<td>
<p>logical; if true, the AR parameters are
transformed to ensure that they remain in the region of
stationarity.  Not used for <code>method = "CSS"</code>.  For
<code>method = "ML"</code>, it has been advantageous to set
<code>transform.pars = FALSE</code> in some cases, see also <code>fixed</code>.</p>
</td></tr>
<tr><td><code id="arima_+3A_fixed">fixed</code></td>
<td>
<p>optional numeric vector of the same length as the total
number of coefficients to be estimated.  It should be of the form
</p>
<p style="text-align: center;"><code class="reqn">(\phi_1, \ldots, \phi_p, \theta_1, \ldots, \theta_q,
      \Phi_1, \ldots, \Phi_P, \Theta_1, \ldots, \Theta_Q, \mu),
    </code>
</p>

<p>where <code class="reqn">\phi_i</code> are the AR coefficients,
<code class="reqn">\theta_i</code> are the MA coefficients,
<code class="reqn">\Phi_i</code> are the seasonal AR coefficients,
<code class="reqn">\Theta_i</code> are the seasonal MA coefficients and
<code class="reqn">\mu</code> is the intercept term.  Note that the <code class="reqn">\mu</code>
entry is required if and only if <code>include.mean</code> is <code>TRUE</code>.
In particular it should not be present if the model is an ARIMA
model with differencing.
</p>
<p>The entries of the <code>fixed</code> vector should consist of the
values at which the user wishes to &ldquo;fix&rdquo; the corresponding
coefficient, or <code>NA</code> if that coefficient should <em>not</em> be
fixed, but estimated.
</p>
<p>The argument <code>transform.pars</code> will be set to <code>FALSE</code> if any
AR parameters are fixed.  A warning will be given if <code>transform.pars</code>
is set to (or left at its default) <code>TRUE</code>.  It may be wise to set
<code>transform.pars = FALSE</code> even when fixing MA parameters,
especially at values that cause the model to be nearly non-invertible.
</p>
</td></tr>
<tr><td><code id="arima_+3A_init">init</code></td>
<td>
<p>optional numeric vector of initial parameter
values.  Missing values will be filled in, by zeroes except for
regression coefficients.  Values already specified in <code>fixed</code>
will be ignored.</p>
</td></tr>
<tr><td><code id="arima_+3A_method">method</code></td>
<td>
<p>fitting method: maximum likelihood or minimize
conditional sum-of-squares.  The default (unless there are missing
values) is to use conditional-sum-of-squares to find starting
values, then maximum likelihood.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="arima_+3A_n.cond">n.cond</code></td>
<td>
<p>only used if fitting by conditional-sum-of-squares: the
number of initial observations to ignore.  It will be ignored if
less than the maximum lag of an AR term.</p>
</td></tr>
<tr><td><code id="arima_+3A_ssinit">SSinit</code></td>
<td>
<p>a string specifying the algorithm to compute the
state-space initialization of the likelihood; see
<code><a href="#topic+KalmanLike">KalmanLike</a></code> for details.   Can be abbreviated.</p>
</td></tr>
<tr><td><code id="arima_+3A_optim.method">optim.method</code></td>
<td>
<p>The value passed as the <code>method</code> argument to
<code><a href="#topic+optim">optim</a></code>.</p>
</td></tr>
<tr><td><code id="arima_+3A_optim.control">optim.control</code></td>
<td>
<p>List of control parameters for <code><a href="#topic+optim">optim</a></code>.</p>
</td></tr>
<tr><td><code id="arima_+3A_kappa">kappa</code></td>
<td>
<p>the prior variance (as a multiple of the innovations
variance) for the past observations in a differenced model.  Do not
reduce this.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Different definitions of ARMA models have different signs for the
AR and/or MA coefficients.  The definition used here has
</p>
<p style="text-align: center;"><code class="reqn">X_t= a_1 X_{t-1}+\cdots+ a_p X_{t-p} + e_t + b_1 e_{t-1}+\cdots+b_q e_{t-q}
  </code>
</p>

<p>and so the MA coefficients differ in sign from those used in
documentation written for S-PLUS.  Further, if <code>include.mean</code> is
true (the default for an ARMA model), this formula applies to <code class="reqn">X -
  m</code> rather than <code class="reqn">X</code>.  For ARIMA models with differencing, the
differenced series follows a zero-mean ARMA model. If an <code>xreg</code>
term is included, a linear regression (with a constant term if
<code>include.mean</code> is true and there is no differencing) is fitted
with an ARMA model for the error term.
</p>
<p>The variance matrix of the estimates is found from the Hessian of
the log-likelihood, and so may only be a rough guide.
</p>
<p>Optimization is done by <code><a href="#topic+optim">optim</a></code>.  It will work
best if the columns in <code>xreg</code> are roughly scaled to zero mean
and unit variance, but does attempt to estimate suitable scalings.
</p>


<h3>Value</h3>

<p>A list of class <code>"Arima"</code> with components:
</p>
<table>
<tr><td><code>coef</code></td>
<td>
<p>a vector of AR, MA and regression coefficients, which can
be extracted by the <code><a href="#topic+coef">coef</a></code> method.</p>
</td></tr>
<tr><td><code>sigma2</code></td>
<td>
<p>the MLE of the innovations variance.</p>
</td></tr>
<tr><td><code>var.coef</code></td>
<td>
<p>the estimated variance matrix of the coefficients
<code>coef</code>, which can be extracted by the <code><a href="#topic+vcov">vcov</a></code> method.</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>the maximized log-likelihood (of the differenced data),
or the approximation to it used.</p>
</td></tr>
<tr><td><code>arma</code></td>
<td>
<p>A compact form of the specification, as a vector giving
the number of AR, MA, seasonal AR and seasonal MA coefficients,
plus the period and the number of non-seasonal and seasonal
differences.</p>
</td></tr>
<tr><td><code>aic</code></td>
<td>
<p>the AIC value corresponding to the log-likelihood. Only
valid for <code>method = "ML"</code> fits.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>the fitted innovations.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>series</code></td>
<td>
<p>the name of the series <code>x</code>.</p>
</td></tr>
<tr><td><code>code</code></td>
<td>
<p>the convergence value returned by <code><a href="#topic+optim">optim</a></code>.</p>
</td></tr>
<tr><td><code>n.cond</code></td>
<td>
<p>the number of initial observations not used in the fitting.</p>
</td></tr>
<tr><td><code>nobs</code></td>
<td>
<p>the number of &ldquo;used&rdquo; observations for the fitting,
can also be extracted via <code><a href="#topic+nobs">nobs</a>()</code> and is used by
<code><a href="#topic+BIC">BIC</a></code>.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>A list representing the Kalman filter used in the
fitting.  See <code><a href="#topic+KalmanLike">KalmanLike</a></code>.</p>
</td></tr>
</table>


<h3>Fitting methods</h3>

<p>The exact likelihood is computed via a state-space representation of
the ARIMA process, and the innovations and their variance found by a
Kalman filter.  The initialization of the differenced ARMA process uses
stationarity and is based on Gardner <abbr>et al.</abbr> (1980).  For a
differenced process the non-stationary components are given a diffuse
prior (controlled by <code>kappa</code>).  Observations which are still
controlled by the diffuse prior (determined by having a Kalman gain of
at least <code>1e4</code>) are excluded from the likelihood calculations.
(This gives comparable results to <code><a href="#topic+arima0">arima0</a></code> in the absence
of missing values, when the observations excluded are precisely those
dropped by the differencing.)
</p>
<p>Missing values are allowed, and are handled exactly in method <code>"ML"</code>.
</p>
<p>If <code>transform.pars</code> is true, the optimization is done using an
alternative parametrization which is a variation on that suggested by
Jones (1980) and ensures that the model is stationary.  For an AR(p)
model the parametrization is via the inverse tanh of the partial
autocorrelations: the same procedure is applied (separately) to the
AR and seasonal AR terms.  The MA terms are not constrained to be
invertible during optimization, but they will be converted to
invertible form after optimization if <code>transform.pars</code> is true.
</p>
<p>Conditional sum-of-squares is provided mainly for expositional
purposes.  This computes the sum of squares of the fitted innovations
from observation <code>n.cond</code> on, (where <code>n.cond</code> is at least
the maximum lag of an AR term), treating all earlier innovations to
be zero.  Argument <code>n.cond</code> can be used to allow comparability
between different fits.  The &lsquo;part log-likelihood&rsquo; is the first
term, half the log of the estimated mean square.  Missing values
are allowed, but will cause many of the innovations to be missing.
</p>
<p>When regressors are specified, they are orthogonalized prior to
fitting unless any of the coefficients is fixed.  It can be helpful to
roughly scale the regressors to zero mean and unit variance.
</p>


<h3>Note</h3>

<p><code>arima</code> is very similar to <code><a href="#topic+arima0">arima0</a></code> for
ARMA models or for differenced models without missing values,
but handles differenced models with missing values exactly.
It is somewhat slower than <code>arima0</code>, particularly for seasonally
differenced models.
</p>


<h3>References</h3>

<p>Brockwell, P. J. and Davis, R. A. (1996).
<em>Introduction to Time Series and Forecasting</em>.
Springer, New York.
Sections 3.3 and 8.3.
</p>
<p>Durbin, J. and Koopman, S. J. (2001).
<em>Time Series Analysis by State Space Methods</em>.
Oxford University Press.
</p>
<p>Gardner, G, Harvey, A. C. and Phillips, G. D. A. (1980).
Algorithm AS 154: An algorithm for exact maximum likelihood estimation
of autoregressive-moving average models by means of Kalman filtering.
<em>Applied Statistics</em>, <b>29</b>, 311&ndash;322.
<a href="https://doi.org/10.2307/2346910">doi:10.2307/2346910</a>.
</p>
<p>Harvey, A. C. (1993).
<em>Time Series Models</em>. 2nd Edition.
Harvester Wheatsheaf.
Sections 3.3 and 4.4.
</p>
<p>Jones, R. H. (1980).
Maximum likelihood fitting of ARMA models to time series with missing
observations.
<em>Technometrics</em>, <b>22</b>, 389&ndash;395.
<a href="https://doi.org/10.2307/1268324">doi:10.2307/1268324</a>.
</p>
<p>Ripley, B. D. (2002).
&ldquo;Time series in <span class="rlang"><b>R</b></span> 1.5.0&rdquo;.
<em>R News</em>, <b>2</b>(2), 2&ndash;7.
<a href="https://www.r-project.org/doc/Rnews/Rnews_2002-2.pdf">https://www.r-project.org/doc/Rnews/Rnews_2002-2.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.Arima">predict.Arima</a></code>, <code><a href="#topic+arima.sim">arima.sim</a></code> for simulating
from an ARIMA model, <code><a href="#topic+tsdiag">tsdiag</a></code>, <code><a href="#topic+arima0">arima0</a></code>,
<code><a href="#topic+ar">ar</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>arima(lh, order = c(1,0,0))
arima(lh, order = c(3,0,0))
arima(lh, order = c(1,0,1))

arima(lh, order = c(3,0,0), method = "CSS")

arima(USAccDeaths, order = c(0,1,1), seasonal = list(order = c(0,1,1)))
arima(USAccDeaths, order = c(0,1,1), seasonal = list(order = c(0,1,1)),
      method = "CSS") # drops first 13 observations.
# for a model with as few years as this, we want full ML

arima(LakeHuron, order = c(2,0,0), xreg = time(LakeHuron) - 1920)

## presidents contains NAs
## graphs in example(acf) suggest order 1 or 3
require(graphics)
(fit1 &lt;- arima(presidents, c(1, 0, 0)))
nobs(fit1)
tsdiag(fit1)
(fit3 &lt;- arima(presidents, c(3, 0, 0)))  # smaller AIC
tsdiag(fit3)
BIC(fit1, fit3)
## compare a whole set of models; BIC() would choose the smallest
AIC(fit1, arima(presidents, c(2,0,0)),
          arima(presidents, c(2,0,1)), # &lt;- chosen (barely) by AIC
    fit3, arima(presidents, c(3,0,1)))

## An example of using the  'fixed'  argument:
## Note that the period of the seasonal component is taken to be
## frequency(presidents), i.e. 4.
(fitSfx &lt;- arima(presidents, order=c(2,0,1), seasonal=c(1,0,0),
                 fixed=c(NA, NA, 0.5, -0.1, 50), transform.pars=FALSE))
## The partly-fixed &amp; smaller model seems better (as we "knew too much"):
AIC(fitSfx, arima(presidents, order=c(2,0,1), seasonal=c(1,0,0)))

## An example of ARIMA forecasting:
predict(fit3, 3)
</code></pre>

<hr>
<h2 id='arima.sim'>Simulate from an ARIMA Model</h2><span id='topic+arima.sim'></span>

<h3>Description</h3>

<p>Simulate from an ARIMA model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>arima.sim(model, n, rand.gen = rnorm, innov = rand.gen(n, ...),
          n.start = NA, start.innov = rand.gen(n.start, ...),
          ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="arima.sim_+3A_model">model</code></td>
<td>
<p>A list with component <code>ar</code> and/or <code>ma</code> giving
the AR and MA coefficients respectively.  Optionally a component
<code>order</code> can be used.  An empty list gives an ARIMA(0, 0, 0)
model, that is white noise.</p>
</td></tr>
<tr><td><code id="arima.sim_+3A_n">n</code></td>
<td>
<p>length of output series, before un-differencing.  A strictly
positive integer.</p>
</td></tr>
<tr><td><code id="arima.sim_+3A_rand.gen">rand.gen</code></td>
<td>
<p>optional: a function to generate the innovations.</p>
</td></tr>
<tr><td><code id="arima.sim_+3A_innov">innov</code></td>
<td>
<p>an optional times series of innovations.  If not
provided, <code>rand.gen</code> is used.</p>
</td></tr>
<tr><td><code id="arima.sim_+3A_n.start">n.start</code></td>
<td>
<p>length of &lsquo;burn-in&rsquo; period.  If <code>NA</code>, the
default, a reasonable value is computed.</p>
</td></tr>
<tr><td><code id="arima.sim_+3A_start.innov">start.innov</code></td>
<td>
<p>an optional times series of innovations to be used
for the burn-in period.  If supplied there must be at least
<code>n.start</code> values (and <code>n.start</code> is by default computed
inside the function).</p>
</td></tr>
<tr><td><code id="arima.sim_+3A_...">...</code></td>
<td>
<p>additional arguments for <code>rand.gen</code>.  Most usefully,
the standard deviation of the innovations generated by <code>rnorm</code>
can be specified by <code>sd</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+arima">arima</a></code> for the precise definition of an ARIMA model.
</p>
<p>The ARMA model is checked for stationarity.
</p>
<p>ARIMA models are specified via the <code>order</code> component of
<code>model</code>, in the same way as for <code><a href="#topic+arima">arima</a></code>.  Other
aspects of the <code>order</code> component are ignored, but inconsistent
specifications of the MA and AR orders are detected.  The
un-differencing assumes previous values of zero, and to remind the
user of this, those values are returned.
</p>
<p>Random inputs for the &lsquo;burn-in&rsquo; period are generated by calling
<code>rand.gen</code>.
</p>


<h3>Value</h3>

<p>A time-series object of class <code>"ts"</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+arima">arima</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

arima.sim(n = 63, list(ar = c(0.8897, -0.4858), ma = c(-0.2279, 0.2488)),
          sd = sqrt(0.1796))
# mildly long-tailed
arima.sim(n = 63, list(ar = c(0.8897, -0.4858), ma = c(-0.2279, 0.2488)),
          rand.gen = function(n, ...) sqrt(0.1796) * rt(n, df = 5))

# An ARIMA simulation
ts.sim &lt;- arima.sim(list(order = c(1,1,0), ar = 0.7), n = 200)
ts.plot(ts.sim)
</code></pre>

<hr>
<h2 id='arima0'>ARIMA Modelling of Time Series &ndash; Preliminary Version</h2><span id='topic+arima0'></span><span id='topic+print.arima0'></span><span id='topic+predict.arima0'></span>

<h3>Description</h3>

<p>Fit an ARIMA model to a univariate time series, and forecast from
the fitted model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>arima0(x, order = c(0, 0, 0),
       seasonal = list(order = c(0, 0, 0), period = NA),
       xreg = NULL, include.mean = TRUE, delta = 0.01,
       transform.pars = TRUE, fixed = NULL, init = NULL,
       method = c("ML", "CSS"), n.cond, optim.control = list())

## S3 method for class 'arima0'
predict(object, n.ahead = 1, newxreg, se.fit = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="arima0_+3A_x">x</code></td>
<td>
<p>a univariate time series</p>
</td></tr>
<tr><td><code id="arima0_+3A_order">order</code></td>
<td>
<p>A specification of the non-seasonal part of the ARIMA
model: the three components <code class="reqn">(p, d, q)</code> are the AR order, the
degree of differencing, and the MA order.</p>
</td></tr>
<tr><td><code id="arima0_+3A_seasonal">seasonal</code></td>
<td>
<p>A specification of the seasonal part of the ARIMA
model, plus the period (which defaults to <code>frequency(x)</code>).
This should be a list with components <code>order</code> and
<code>period</code>, but a specification of just a numeric vector of
length 3 will be turned into a suitable list with the specification
as the <code>order</code>.</p>
</td></tr>
<tr><td><code id="arima0_+3A_xreg">xreg</code></td>
<td>
<p>Optionally, a vector or matrix of external regressors,
which must have the same number of rows as <code>x</code>.</p>
</td></tr>
<tr><td><code id="arima0_+3A_include.mean">include.mean</code></td>
<td>
<p>Should the ARIMA model include
a mean term? The default is <code>TRUE</code> for undifferenced series,
<code>FALSE</code> for differenced ones (where a mean would not affect
the fit nor predictions).</p>
</td></tr>
<tr><td><code id="arima0_+3A_delta">delta</code></td>
<td>
<p>A value to indicate at which point &lsquo;fast
recursions&rsquo; should be used.  See the &lsquo;Details&rsquo; section.</p>
</td></tr>
<tr><td><code id="arima0_+3A_transform.pars">transform.pars</code></td>
<td>
<p>Logical.  If true, the AR parameters are
transformed to ensure that they remain in the region of
stationarity.  Not used for <code>method = "CSS"</code>.</p>
</td></tr>
<tr><td><code id="arima0_+3A_fixed">fixed</code></td>
<td>
<p>optional numeric vector of the same length as the total
number of parameters.  If supplied, only <code>NA</code> entries in
<code>fixed</code> will be varied.  <code>transform.pars = TRUE</code>
will be overridden (with a warning) if any ARMA parameters are
fixed.</p>
</td></tr>
<tr><td><code id="arima0_+3A_init">init</code></td>
<td>
<p>optional numeric vector of initial parameter
values.  Missing values will be filled in, by zeroes except for
regression coefficients.  Values already specified in <code>fixed</code>
will be ignored.</p>
</td></tr>
<tr><td><code id="arima0_+3A_method">method</code></td>
<td>
<p>Fitting method: maximum likelihood or minimize
conditional sum-of-squares.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="arima0_+3A_n.cond">n.cond</code></td>
<td>
<p>Only used if fitting by conditional-sum-of-squares: the
number of initial observations to ignore.  It will be ignored if
less than the maximum lag of an AR term.</p>
</td></tr>
<tr><td><code id="arima0_+3A_optim.control">optim.control</code></td>
<td>
<p>List of control parameters for <code><a href="#topic+optim">optim</a></code>.</p>
</td></tr>
<tr><td><code id="arima0_+3A_object">object</code></td>
<td>
<p>The result of an <code>arima0</code> fit.</p>
</td></tr>
<tr><td><code id="arima0_+3A_newxreg">newxreg</code></td>
<td>
<p>New values of <code>xreg</code> to be used for
prediction. Must have at least <code>n.ahead</code> rows.</p>
</td></tr>
<tr><td><code id="arima0_+3A_n.ahead">n.ahead</code></td>
<td>
<p>The number of steps ahead for which prediction is required.</p>
</td></tr>
<tr><td><code id="arima0_+3A_se.fit">se.fit</code></td>
<td>
<p>Logical: should standard errors of prediction be returned?</p>
</td></tr>
<tr><td><code id="arima0_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Different definitions of ARMA models have different signs for the
AR and/or MA coefficients. The definition here has
</p>
<p style="text-align: center;"><code class="reqn">X_t = a_1X_{t-1} + \cdots + a_pX_{t-p} + e_t + b_1e_{t-1} + \dots + b_qe_{t-q}</code>
</p>

<p>and so the MA coefficients differ in sign from those given by
S-PLUS.  Further, if <code>include.mean</code> is true, this formula
applies to <code class="reqn">X-m</code> rather than <code class="reqn">X</code>.  For ARIMA models with
differencing, the differenced series follows a zero-mean ARMA model.
</p>
<p>The variance matrix of the estimates is found from the Hessian of
the log-likelihood, and so may only be a rough guide, especially for
fits close to the boundary of invertibility.
</p>
<p>Optimization is done by <code><a href="#topic+optim">optim</a></code>. It will work
best if the columns in <code>xreg</code> are roughly scaled to zero mean
and unit variance, but does attempt to estimate suitable scalings.
</p>
<p>Finite-history prediction is used. This is only statistically
efficient if the MA part of the fit is invertible, so
<code>predict.arima0</code> will give a warning for non-invertible MA
models.
</p>


<h3>Value</h3>

<p>For <code>arima0</code>, a list of class <code>"arima0"</code> with components:
</p>
<table>
<tr><td><code>coef</code></td>
<td>
<p>a vector of AR, MA and regression coefficients,</p>
</td></tr>
<tr><td><code>sigma2</code></td>
<td>
<p>the MLE of the innovations variance.</p>
</td></tr>
<tr><td><code>var.coef</code></td>
<td>
<p>the estimated variance matrix of the coefficients
<code>coef</code>.</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>the maximized log-likelihood (of the differenced data),
or the approximation to it used.</p>
</td></tr>
<tr><td><code>arma</code></td>
<td>
<p>A compact form of the specification, as a vector giving
the number of AR, MA, seasonal AR and seasonal MA coefficients,
plus the period and the number of non-seasonal and seasonal
differences.</p>
</td></tr>
<tr><td><code>aic</code></td>
<td>
<p>the AIC value corresponding to the log-likelihood. Only
valid for <code>method = "ML"</code> fits.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>the fitted innovations.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>series</code></td>
<td>
<p>the name of the series <code>x</code>.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>the value returned by <code><a href="#topic+optim">optim</a></code>.</p>
</td></tr>
<tr><td><code>n.cond</code></td>
<td>
<p>the number of initial observations not used in the fitting.</p>
</td></tr>
</table>
<p>For <code>predict.arima0</code>, a time series of predictions, or if
<code>se.fit = TRUE</code>, a list with components <code>pred</code>, the
predictions, and <code>se</code>, the estimated standard errors. Both
components are time series.
</p>


<h3>Fitting methods</h3>

<p>The exact likelihood is computed via a state-space representation of
the ARMA process, and the innovations and their variance found by a
Kalman filter based on Gardner <abbr>et al.</abbr> (1980).  This has
the option to switch to &lsquo;fast recursions&rsquo; (assume an
effectively infinite past) if the innovations variance is close
enough to its asymptotic bound. The argument <code>delta</code> sets the
tolerance: at its default value the approximation is normally
negligible and the speed-up considerable.  Exact computations can be
ensured by setting <code>delta</code> to a negative value.
</p>
<p>If <code>transform.pars</code> is true, the optimization is done using an
alternative parametrization which is a variation on that suggested by
Jones (1980) and ensures that the model is stationary.  For an AR(p)
model the parametrization is via the inverse tanh of the partial
autocorrelations: the same procedure is applied (separately) to the
AR and seasonal AR terms.  The MA terms are also constrained to be
invertible during optimization by the same transformation if
<code>transform.pars</code> is true.  Note that the MLE for MA terms does
sometimes occur for MA polynomials with unit roots: such models can be
fitted by using <code>transform.pars = FALSE</code> and specifying a good
set of initial values (often obtainable from a fit with
<code>transform.pars = TRUE</code>).
</p>
<p>Missing values are allowed, but any missing values
will force <code>delta</code> to be ignored and full recursions used.
Note that missing values will be propagated by differencing, so the
procedure used in this function is not fully efficient in that case.
</p>
<p>Conditional sum-of-squares is provided mainly for expositional
purposes.  This computes the sum of squares of the fitted innovations
from observation
<code>n.cond</code> on, (where <code>n.cond</code> is at least the maximum lag of
an AR term), treating all earlier innovations to be zero.  Argument
<code>n.cond</code> can be used to allow comparability between different
fits.  The &lsquo;part log-likelihood&rsquo; is the first term, half the
log of the estimated mean square.  Missing values are allowed, but
will cause many of the innovations to be missing.
</p>
<p>When regressors are specified, they are orthogonalized prior to
fitting unless any of the coefficients is fixed.  It can be helpful to
roughly scale the regressors to zero mean and unit variance.
</p>


<h3>Note</h3>

<p>This is a preliminary version, and will be replaced by <code><a href="#topic+arima">arima</a></code>.
</p>
<p>The standard errors of prediction exclude the uncertainty in the
estimation of the ARMA model and the regression coefficients.
</p>
<p>The results are likely to be different from S-PLUS's
<code>arima.mle</code>, which computes a conditional likelihood and does
not include a mean in the model.  Further, the convention used by
<code>arima.mle</code> reverses the signs of the MA coefficients.
</p>


<h3>References</h3>

<p>Brockwell, P. J. and Davis, R. A. (1996).
<em>Introduction to Time Series and Forecasting</em>.
Springer, New York.
Sections 3.3 and 8.3.
</p>
<p>Gardner, G, Harvey, A. C. and Phillips, G. D. A. (1980).
Algorithm AS 154: An algorithm for exact maximum likelihood estimation
of autoregressive-moving average models by means of Kalman filtering.
<em>Applied Statistics</em>, <b>29</b>, 311&ndash;322.
<a href="https://doi.org/10.2307/2346910">doi:10.2307/2346910</a>.
</p>
<p>Harvey, A. C. (1993).
<em>Time Series Models</em>. 2nd Edition.
Harvester Wheatsheaf.
Sections 3.3 and 4.4.
</p>
<p>Harvey, A. C. and McKenzie, C. R. (1982).
Algorithm AS 182: An algorithm for finite sample prediction from ARIMA
processes. 
<em>Applied Statistics</em>, <b>31</b>, 180&ndash;187.
<a href="https://doi.org/10.2307/2347987">doi:10.2307/2347987</a>.
</p>
<p>Jones, R. H. (1980).
Maximum likelihood fitting of ARMA models to time series with missing
observations.
<em>Technometrics</em>, <b>22</b>, 389&ndash;395.
<a href="https://doi.org/10.2307/1268324">doi:10.2307/1268324</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+arima">arima</a></code>, <code><a href="#topic+ar">ar</a></code>, <code><a href="#topic+tsdiag">tsdiag</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: arima0(lh, order = c(1,0,0))
arima0(lh, order = c(3,0,0))
arima0(lh, order = c(1,0,1))
predict(arima0(lh, order = c(3,0,0)), n.ahead = 12)

arima0(lh, order = c(3,0,0), method = "CSS")

# for a model with as few years as this, we want full ML
(fit &lt;- arima0(USAccDeaths, order = c(0,1,1),
               seasonal = list(order=c(0,1,1)), delta = -1))
predict(fit, n.ahead = 6)

arima0(LakeHuron, order = c(2,0,0), xreg = time(LakeHuron)-1920)
## Not run: 
## presidents contains NAs
## graphs in example(acf) suggest order 1 or 3
(fit1 &lt;- arima0(presidents, c(1, 0, 0), delta = -1))  # avoid warning
tsdiag(fit1)
(fit3 &lt;- arima0(presidents, c(3, 0, 0), delta = -1))  # smaller AIC
tsdiag(fit3)
## End(Not run)
</code></pre>

<hr>
<h2 id='ARMAacf'>Compute Theoretical ACF for an ARMA Process</h2><span id='topic+ARMAacf'></span>

<h3>Description</h3>

<p>Compute the theoretical autocorrelation function or partial
autocorrelation function for an ARMA process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ARMAacf(ar = numeric(), ma = numeric(), lag.max = r, pacf = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ARMAacf_+3A_ar">ar</code></td>
<td>
<p>numeric vector of AR coefficients</p>
</td></tr>
<tr><td><code id="ARMAacf_+3A_ma">ma</code></td>
<td>
<p>numeric vector of MA coefficients</p>
</td></tr>
<tr><td><code id="ARMAacf_+3A_lag.max">lag.max</code></td>
<td>
<p>integer.  Maximum lag required.  Defaults to
<code>max(p, q+1)</code>, where <code>p, q</code> are the numbers of AR and MA
terms respectively.</p>
</td></tr>
<tr><td><code id="ARMAacf_+3A_pacf">pacf</code></td>
<td>
<p>logical.  Should the partial autocorrelations be returned?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The methods used follow
Brockwell &amp; Davis (1991, section 3.3).  Their
equations (3.3.8) are solved for the autocovariances at lags
<code class="reqn">0, \dots, \max(p, q+1)</code>,
and the remaining autocorrelations are given by a recursive filter.
</p>


<h3>Value</h3>

<p>A vector of (partial) autocorrelations, named by the lags.
</p>


<h3>References</h3>

<p>Brockwell, P. J. and Davis, R. A. (1991) <em>Time Series: Theory and
Methods</em>, Second Edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+arima">arima</a></code>, <code><a href="#topic+ARMAtoMA">ARMAtoMA</a></code>,
<code><a href="#topic+acf2AR">acf2AR</a></code> for inverting part of <code>ARMAacf</code>; further
<code><a href="#topic+filter">filter</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ARMAacf(c(1.0, -0.25), 1.0, lag.max = 10)

## Example from Brockwell &amp; Davis (1991, pp.92-4)
## answer: 2^(-n) * (32/3 + 8 * n) /(32/3)
n &lt;- 1:10
a.n &lt;- 2^(-n) * (32/3 + 8 * n) /(32/3)
(A.n &lt;- ARMAacf(c(1.0, -0.25), 1.0, lag.max = 10))
stopifnot(all.equal(unname(A.n), c(1, a.n)))

ARMAacf(c(1.0, -0.25), 1.0, lag.max = 10, pacf = TRUE)
zapsmall(ARMAacf(c(1.0, -0.25), lag.max = 10, pacf = TRUE))

## Cov-Matrix of length-7 sub-sample of AR(1) example:
toeplitz(ARMAacf(0.8, lag.max = 7))
</code></pre>

<hr>
<h2 id='ARMAtoMA'>Convert ARMA Process to Infinite MA Process</h2><span id='topic+ARMAtoMA'></span>

<h3>Description</h3>

<p>Convert ARMA process to infinite MA process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ARMAtoMA(ar = numeric(), ma = numeric(), lag.max)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ARMAtoMA_+3A_ar">ar</code></td>
<td>
<p>numeric vector of AR coefficients</p>
</td></tr>
<tr><td><code id="ARMAtoMA_+3A_ma">ma</code></td>
<td>
<p>numeric vector of MA coefficients</p>
</td></tr>
<tr><td><code id="ARMAtoMA_+3A_lag.max">lag.max</code></td>
<td>
<p>Largest MA(Inf) coefficient required.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of coefficients.
</p>


<h3>References</h3>

<p>Brockwell, P. J. and Davis, R. A. (1991) <em>Time Series: Theory and
Methods</em>, Second Edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+arima">arima</a></code>, <code><a href="#topic+ARMAacf">ARMAacf</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>ARMAtoMA(c(1.0, -0.25), 1.0, 10)
## Example from Brockwell &amp; Davis (1991, p.92)
## answer (1 + 3*n)*2^(-n)
n &lt;- 1:10; (1 + 3*n)*2^(-n)
</code></pre>

<hr>
<h2 id='as.hclust'>Convert Objects to Class <code>"hclust"</code></h2><span id='topic+as.hclust'></span><span id='topic+as.hclust.default'></span><span id='topic+as.hclust.twins'></span>

<h3>Description</h3>

<p>Converts objects from other hierarchical clustering functions to
class <code>"hclust"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.hclust(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.hclust_+3A_x">x</code></td>
<td>
<p>Hierarchical clustering object</p>
</td></tr>
<tr><td><code id="as.hclust_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Currently there is only support for converting objects of
class <code>"twins"</code> as produced by the functions <code>diana</code> and
<code>agnes</code> from the package <a href="https://CRAN.R-project.org/package=cluster"><span class="pkg">cluster</span></a>.  The default method
throws an error unless passed an <code>"hclust"</code> object.
</p>


<h3>Value</h3>

<p>An object of class <code>"hclust"</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hclust">hclust</a></code>, and from package <a href="https://CRAN.R-project.org/package=cluster"><span class="pkg">cluster</span></a>,
<code><a href="cluster.html#topic+diana">diana</a></code> and <code><a href="cluster.html#topic+agnes">agnes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(rnorm(30), ncol = 3)
hc &lt;- hclust(dist(x), method = "complete")

if(require("cluster", quietly = TRUE)) {# is a recommended package
  ag &lt;- agnes(x, method = "complete")
  hcag &lt;- as.hclust(ag)
  ## The dendrograms order slightly differently:
  op &lt;- par(mfrow = c(1,2))
  plot(hc) ;  mtext("hclust", side = 1)
  plot(hcag); mtext("agnes",  side = 1)
  detach("package:cluster")
}
</code></pre>

<hr>
<h2 id='asOneSidedFormula'>Convert to One-Sided Formula</h2><span id='topic+asOneSidedFormula'></span>

<h3>Description</h3>

<p>Names, calls, expressions (first element), numeric values, and
character strings are converted to one-sided formulae
associated with the global environment.
If the input is a formula, it must be
one-sided, in which case it is returned unaltered.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>asOneSidedFormula(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="asOneSidedFormula_+3A_object">object</code></td>
<td>
<p>a one-sided formula, name, call, expression, numeric value, or
character string.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a one-sided formula representing <code>object</code>
</p>


<h3>Author(s)</h3>

<p>Jos Pinheiro and Douglas Bates</p>


<h3>See Also</h3>

<p><code><a href="#topic+formula">formula</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>(form &lt;- asOneSidedFormula("age"))
stopifnot(exprs = {
    identical(form, asOneSidedFormula(form))
    identical(form, asOneSidedFormula(as.name("age")))
    identical(form, asOneSidedFormula(expression(age)))
})
asOneSidedFormula(quote(log(age)))
asOneSidedFormula(1)
</code></pre>

<hr>
<h2 id='ave'>Group Averages Over Level Combinations of Factors</h2><span id='topic+ave'></span>

<h3>Description</h3>

<p>Subsets of <code>x[]</code> are averaged, where each subset consist of those
observations with the same factor levels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ave(x, ..., FUN = mean)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ave_+3A_x">x</code></td>
<td>
<p>A numeric.</p>
</td></tr>
<tr><td><code id="ave_+3A_...">...</code></td>
<td>
<p>Grouping variables, typically factors, all of the same
<code>length</code> as <code>x</code>.</p>
</td></tr>
<tr><td><code id="ave_+3A_fun">FUN</code></td>
<td>
<p>Function to apply for each factor level combination.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector, say <code>y</code> of length <code>length(x)</code>.
If <code>...</code> is <code>g1, g2</code>, e.g.,
<code>y[i]</code> is equal to <code>FUN(x[j]</code>, for all <code>j</code> with
<code>g1[j] == g1[i]</code> and <code>g2[j] == g2[i])</code>.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+mean">mean</a></code>, <code><a href="#topic+median">median</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

ave(1:3)  # no grouping -&gt; grand mean

attach(warpbreaks)
ave(breaks, wool)
ave(breaks, tension)
ave(breaks, tension, FUN = function(x) mean(x, trim = 0.1))
plot(breaks, main =
     "ave( Warpbreaks )  for   wool  x  tension  combinations")
lines(ave(breaks, wool, tension              ), type = "s", col = "blue")
lines(ave(breaks, wool, tension, FUN = median), type = "s", col = "green")
legend(40, 70, c("mean", "median"), lty = 1,
      col = c("blue","green"), bg = "gray90")
detach()
</code></pre>

<hr>
<h2 id='bandwidth'>Bandwidth Selectors for Kernel Density Estimation</h2><span id='topic+bw.nrd0'></span><span id='topic+bw.nrd'></span><span id='topic+bw.ucv'></span><span id='topic+bw.bcv'></span><span id='topic+bw.SJ'></span>

<h3>Description</h3>

<p>Bandwidth selectors for Gaussian kernels in <code><a href="#topic+density">density</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bw.nrd0(x)

bw.nrd(x)

bw.ucv(x, nb = 1000, lower = 0.1 * hmax, upper = hmax,
       tol = 0.1 * lower)

bw.bcv(x, nb = 1000, lower = 0.1 * hmax, upper = hmax,
       tol = 0.1 * lower)

bw.SJ(x, nb = 1000, lower = 0.1 * hmax, upper = hmax,
      method = c("ste", "dpi"), tol = 0.1 * lower)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bandwidth_+3A_x">x</code></td>
<td>
<p>numeric vector.</p>
</td></tr>
<tr><td><code id="bandwidth_+3A_nb">nb</code></td>
<td>
<p>number of bins to use.</p>
</td></tr>
<tr><td><code id="bandwidth_+3A_lower">lower</code>, <code id="bandwidth_+3A_upper">upper</code></td>
<td>
<p>range over which to minimize.  The default is
almost always satisfactory.  <code>hmax</code> is calculated internally
from a normal reference bandwidth.</p>
</td></tr>
<tr><td><code id="bandwidth_+3A_method">method</code></td>
<td>
<p>either <code>"ste"</code> (&quot;solve-the-equation&quot;) or
<code>"dpi"</code> (&quot;direct plug-in&quot;).   Can be abbreviated.</p>
</td></tr>
<tr><td><code id="bandwidth_+3A_tol">tol</code></td>
<td>
<p>for method <code>"ste"</code>, the convergence tolerance for
<code><a href="#topic+uniroot">uniroot</a></code>.  The default leads to bandwidth estimates
with only slightly more than one digit accuracy, which is sufficient
for practical density estimation, but possibly not for theoretical
simulation studies.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>bw.nrd0</code> implements a rule-of-thumb for
choosing the bandwidth of a Gaussian kernel density estimator.
It defaults to 0.9 times the
minimum of the standard deviation and the interquartile range divided by
1.34 times the sample size to the negative one-fifth power
(= Silverman's &lsquo;rule of thumb&rsquo;, Silverman (1986, page 48, <abbr>eqn</abbr> (3.31)))
<em>unless</em> the quartiles coincide when a positive result
will be guaranteed.
</p>
<p><code>bw.nrd</code> is the more common variation given by Scott (1992),
using factor 1.06.
</p>
<p><code>bw.ucv</code> and <code>bw.bcv</code> implement unbiased and
biased cross-validation respectively.
</p>
<p><code>bw.SJ</code> implements the methods of Sheather &amp; Jones (1991)
to select the bandwidth using pilot estimation of derivatives.<br />
The algorithm for method <code>"ste"</code> solves an equation (via
<code><a href="#topic+uniroot">uniroot</a></code>) and because of that, enlarges the interval
<code>c(lower, upper)</code> when the boundaries were not user-specified and
do not bracket the root.
</p>
<p>The last three methods use all pairwise binned distances: they are of
complexity <code class="reqn">O(n^2)</code> up to <code>n = nb/2</code> and <code class="reqn">O(n)</code>
thereafter.  Because of the binning, the results differ slightly when
<code>x</code> is translated or sign-flipped.
</p>


<h3>Value</h3>

<p>A bandwidth on a scale suitable for the <code>bw</code> argument
of <code>density</code>.
</p>


<h3>Note</h3>

<p>Long vectors <code>x</code> are not supported, but neither are they by
<code><a href="#topic+density">density</a></code> and kernel density estimation and for more than
a few thousand points a histogram would be preferred.
</p>


<h3>Author(s)</h3>

<p>B. D. Ripley, taken from early versions of package <span class="pkg">MASS</span>.
</p>


<h3>References</h3>

<p>Scott, D. W. (1992)
<em>Multivariate Density Estimation: Theory, Practice, and
Visualization.</em>
New York: Wiley.
</p>
<p>Sheather, S. J. and Jones, M. C. (1991).
A reliable data-based bandwidth selection method for kernel density
estimation.
<em>Journal of the Royal Statistical Society Series B</em>,
<b>53</b>, 683&ndash;690.
<a href="https://doi.org/10.1111/j.2517-6161.1991.tb01857.x">doi:10.1111/j.2517-6161.1991.tb01857.x</a>.

</p>
<p>Silverman, B. W. (1986).
<em>Density Estimation</em>.
London: Chapman and Hall.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002).
<em>Modern Applied Statistics with S</em>.
Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+density">density</a></code>.
</p>
<p><code><a href="MASS.html#topic+bandwidth.nrd">bandwidth.nrd</a></code>, <code><a href="MASS.html#topic+ucv">ucv</a></code>,
<code><a href="MASS.html#topic+bcv">bcv</a></code> and <code><a href="MASS.html#topic+width.SJ">width.SJ</a></code> in
package <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a>, which are all scaled to the <code>width</code> argument
of <code>density</code> and so give answers four times as large.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

plot(density(precip, n = 1000))
rug(precip)
lines(density(precip, bw = "nrd"), col = 2)
lines(density(precip, bw = "ucv"), col = 3)
lines(density(precip, bw = "bcv"), col = 4)
lines(density(precip, bw = "SJ-ste"), col = 5)
lines(density(precip, bw = "SJ-dpi"), col = 6)
legend(55, 0.035,
       legend = c("nrd0", "nrd", "ucv", "bcv", "SJ-ste", "SJ-dpi"),
       col = 1:6, lty = 1)
</code></pre>

<hr>
<h2 id='bartlett.test'>Bartlett Test of Homogeneity of Variances</h2><span id='topic+bartlett.test'></span><span id='topic+bartlett.test.default'></span><span id='topic+bartlett.test.formula'></span>

<h3>Description</h3>

<p>Performs Bartlett's test of the null that the variances in each of the
groups (samples) are the same.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bartlett.test(x, ...)

## Default S3 method:
bartlett.test(x, g, ...)

## S3 method for class 'formula'
bartlett.test(formula, data, subset, na.action, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bartlett.test_+3A_x">x</code></td>
<td>
<p>a numeric vector of data values, or a list of numeric data
vectors representing the respective samples, or fitted linear model
objects (inheriting from class <code>"lm"</code>).</p>
</td></tr>
<tr><td><code id="bartlett.test_+3A_g">g</code></td>
<td>
<p>a vector or factor object giving the group for the
corresponding elements of <code>x</code>.
Ignored if <code>x</code> is a list.</p>
</td></tr>
<tr><td><code id="bartlett.test_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>lhs ~ rhs</code> where <code>lhs</code>
gives the data values and <code>rhs</code> the corresponding groups.</p>
</td></tr>
<tr><td><code id="bartlett.test_+3A_data">data</code></td>
<td>
<p>an optional matrix or data frame (or similar: see
<code><a href="#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="bartlett.test_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="bartlett.test_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.  Defaults to
<code>getOption("na.action")</code>.</p>
</td></tr>
<tr><td><code id="bartlett.test_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>x</code> is a list, its elements are taken as the samples or fitted
linear models to be compared for homogeneity of variances.  In this
case, the elements must either all be numeric data vectors or fitted
linear model objects, <code>g</code> is ignored, and one can simply use
<code>bartlett.test(x)</code> to perform the test.  If the samples are not
yet contained in a list, use <code>bartlett.test(list(x, ...))</code>.
</p>
<p>Otherwise, <code>x</code> must be a numeric data vector, and <code>g</code> must
be a vector or factor object of the same length as <code>x</code> giving the
group for the corresponding elements of <code>x</code>.
</p>


<h3>Value</h3>

<p>A list of class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>Bartlett's K-squared test statistic.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of freedom of the approximate chi-squared
distribution of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the character string
<code>"Bartlett test of homogeneity of variances"</code>.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the names of the data.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Bartlett, M. S. (1937).
Properties of sufficiency and statistical tests.
<em>Proceedings of the Royal Society of London Series A</em>
<b>160</b>, 268&ndash;282.
<a href="https://doi.org/10.1098/rspa.1937.0109">doi:10.1098/rspa.1937.0109</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+var.test">var.test</a></code> for the special case of comparing variances in
two samples from normal distributions;
<code><a href="#topic+fligner.test">fligner.test</a></code> for a rank-based (nonparametric)
<code class="reqn">k</code>-sample test for homogeneity of variances;
<code><a href="#topic+ansari.test">ansari.test</a></code> and <code><a href="#topic+mood.test">mood.test</a></code> for two rank
based two-sample tests for difference in scale.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

plot(count ~ spray, data = InsectSprays)
bartlett.test(InsectSprays$count, InsectSprays$spray)
bartlett.test(count ~ spray, data = InsectSprays)
</code></pre>

<hr>
<h2 id='Beta'>The Beta Distribution</h2><span id='topic+Beta'></span><span id='topic+dbeta'></span><span id='topic+pbeta'></span><span id='topic+qbeta'></span><span id='topic+rbeta'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the Beta distribution with parameters <code>shape1</code> and
<code>shape2</code> (and optional non-centrality parameter <code>ncp</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dbeta(x, shape1, shape2, ncp = 0, log = FALSE)
pbeta(q, shape1, shape2, ncp = 0, lower.tail = TRUE, log.p = FALSE)
qbeta(p, shape1, shape2, ncp = 0, lower.tail = TRUE, log.p = FALSE)
rbeta(n, shape1, shape2, ncp = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Beta_+3A_x">x</code>, <code id="Beta_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="Beta_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="Beta_+3A_n">n</code></td>
<td>
<p>number of observations. If <code>length(n) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="Beta_+3A_shape1">shape1</code>, <code id="Beta_+3A_shape2">shape2</code></td>
<td>
<p>non-negative parameters of the Beta distribution.</p>
</td></tr>
<tr><td><code id="Beta_+3A_ncp">ncp</code></td>
<td>
<p>non-centrality parameter.</p>
</td></tr>
<tr><td><code id="Beta_+3A_log">log</code>, <code id="Beta_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="Beta_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Beta distribution with parameters <code>shape1</code> <code class="reqn">= a</code> and
<code>shape2</code> <code class="reqn">= b</code> has density
</p>
<p style="text-align: center;"><code class="reqn">f(x)=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}{x}^{a-1} {(1-x)}^{b-1}%
  </code>
</p>

<p>for <code class="reqn">a &gt; 0</code>, <code class="reqn">b &gt; 0</code> and <code class="reqn">0 \le x \le 1</code>
where the boundary values at <code class="reqn">x=0</code> or <code class="reqn">x=1</code> are defined as
by continuity (as limits).
<br />
The mean is <code class="reqn">a/(a+b)</code> and the variance is <code class="reqn">ab/((a+b)^2 (a+b+1))</code>.
If <code class="reqn">a,b &gt; 1</code>, (or one of them <code class="reqn">=1</code>), the mode is <code class="reqn">(a-1)/(a+b-2)</code>.
These and all other distributional properties can be defined as
limits (leading to point masses at 0, 1/2, or 1) when <code class="reqn">a</code> or
<code class="reqn">b</code> are zero or infinite, and the corresponding
<code>[dpqr]beta()</code> functions are defined correspondingly.
</p>
<p><code>pbeta</code> is closely related to the incomplete beta function.  As
defined by Abramowitz and Stegun 6.6.1
</p>
<p style="text-align: center;"><code class="reqn">B_x(a,b) = \int_0^x t^{a-1} (1-t)^{b-1} dt,</code>
</p>

<p>and 6.6.2 <code class="reqn">I_x(a,b) = B_x(a,b) / B(a,b)</code> where
<code class="reqn">B(a,b) = B_1(a,b)</code> is the Beta function (<code><a href="base.html#topic+beta">beta</a></code>).
</p>
<p><code class="reqn">I_x(a,b)</code> is <code>pbeta(x, a, b)</code>.
</p>
<p>The noncentral Beta distribution (with <code>ncp</code> <code class="reqn"> = \lambda</code>)
is defined (Johnson <abbr>et al.</abbr>, 1995, pp. 502) as the distribution of
<code class="reqn">X/(X+Y)</code> where <code class="reqn">X \sim \chi^2_{2a}(\lambda)</code>
and <code class="reqn">Y \sim \chi^2_{2b}</code>.
There, <code class="reqn">\chi^2_n(\lambda)</code> is the noncentral
chi-squared distribution with <code class="reqn">n</code> degrees of freedom and
non-centrality parameter <code class="reqn">\lambda</code>, see <a href="#topic+Chisquare">Chisquare</a>.
</p>


<h3>Value</h3>

<p><code>dbeta</code> gives the density, <code>pbeta</code> the distribution
function, <code>qbeta</code> the quantile function, and <code>rbeta</code>
generates random deviates.
</p>
<p>Invalid arguments will result in return value <code>NaN</code>, with a warning.
</p>
<p>The length of the result is determined by <code>n</code> for
<code>rbeta</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.
</p>
<p>The numerical arguments other than <code>n</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.
</p>


<h3>Note</h3>

<p>Supplying <code>ncp = 0</code> uses the algorithm for the non-central
distribution, which is not the same algorithm as when <code>ncp</code> is
omitted.  This is to give consistent behaviour in extreme cases with
values of <code>ncp</code> very near zero.
</p>


<h3>Source</h3>


<ul>
<li><p> The central <code>dbeta</code> is based on a binomial probability, using code
contributed by Catherine Loader (see <code><a href="#topic+dbinom">dbinom</a></code>) if either
shape parameter is larger than one, otherwise directly from the definition.
The non-central case is based on the derivation as a Poisson
mixture of betas (Johnson <abbr>et al.</abbr>, 1995, pp. 502&ndash;3).
</p>
</li>
<li><p> The central <code>pbeta</code> for the default (<code>log_p = FALSE</code>)
uses a C translation based on
</p>
<p>Didonato, A. and Morris, A., Jr, (1992)
Algorithm 708: Significant digit computation of the incomplete beta
function ratios,
<em>ACM Transactions on Mathematical Software</em>, <b>18</b>, 360&ndash;373,
<a href="https://doi.org/10.1145/131766.131776">doi:10.1145/131766.131776</a>.
(See also<br />
Brown, B. and Lawrence Levy, L. (1994)
Certification of algorithm 708: Significant digit computation of the
incomplete beta,
<em>ACM Transactions on Mathematical Software</em>, <b>20</b>, 393&ndash;397,
<a href="https://doi.org/10.1145/192115.192155">doi:10.1145/192115.192155</a>.)
<br /> 
We have slightly tweaked the original &ldquo;TOMS 708&rdquo; algorithm, and
enhanced for <code>log.p = TRUE</code>.  For that (log-scale) case,
underflow to <code>-Inf</code> (i.e., <code class="reqn">P = 0</code>) or <code>0</code>, (i.e.,
<code class="reqn">P = 1</code>) still happens because the original algorithm was designed
without log-scale considerations.  Underflow to <code>-Inf</code> now
typically signals a <code><a href="base.html#topic+warning">warning</a></code>.
</p>
</li>
<li><p> The non-central <code>pbeta</code> uses a C translation of
</p>
<p>Lenth, R. V. (1987) Algorithm AS 226: Computing noncentral beta
probabilities. <em>Applied Statistics</em>, <b>36</b>, 241&ndash;244,
<a href="https://doi.org/10.2307/2347558">doi:10.2307/2347558</a>, 
incorporating<br />
Frick, H. (1990)'s AS R84, <em>Applied Statistics</em>, <b>39</b>,
311&ndash;2, <a href="https://doi.org/10.2307/2347780">doi:10.2307/2347780</a>
and<br />
Lam, M.L. (1995)'s AS R95, <em>Applied Statistics</em>, <b>44</b>,
551&ndash;2, <a href="https://doi.org/10.2307/2986147">doi:10.2307/2986147</a>.
</p>
<p>This computes the lower tail only, so the upper tail suffers from
cancellation and a warning will be given when this is likely to be
significant.
</p>
</li>
<li><p> The central case of <code>qbeta</code> is based on a C translation of
</p>
<p>Cran, G. W., K. J. Martin and G. E. Thomas (1977).
Remark AS R19 and Algorithm AS 109,
<em>Applied Statistics</em>,  <b>26</b>, 111&ndash;114,
<a href="https://doi.org/10.2307/2346887">doi:10.2307/2346887</a>,
and subsequent remarks (AS83 and correction).
</p>
<p>Enhancements, notably for starting values and switching to a log-scale
Newton search, by R Core.
</p>
</li>
<li><p>  The central case of <code>rbeta</code> is based on a C translation of
</p>
<p>R. C. H. Cheng (1978).
Generating beta variates with nonintegral shape parameters.
<em>Communications of the ACM</em>, <b>21</b>, 317&ndash;322.
</p>
</li></ul>



<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Abramowitz, M. and Stegun, I. A. (1972)
<em>Handbook of Mathematical Functions.</em> New York: Dover.
Chapter 6: Gamma and Related Functions.
</p>
<p>Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
<em>Continuous Univariate Distributions</em>, volume 2, especially
chapter 25. Wiley, New York.
</p>


<h3>See Also</h3>

<p><a href="#topic+Distributions">Distributions</a> for other standard distributions.
</p>
<p><code><a href="base.html#topic+beta">beta</a></code> for the Beta function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- seq(0, 1, length.out = 21)
dbeta(x, 1, 1)
pbeta(x, 1, 1)

## Visualization, including limit cases:
pl.beta &lt;- function(a,b, asp = if(isLim) 1, ylim = if(isLim) c(0,1.1)) {
  if(isLim &lt;- a == 0 || b == 0 || a == Inf || b == Inf) {
    eps &lt;- 1e-10
    x &lt;- c(0, eps, (1:7)/16, 1/2+c(-eps,0,eps), (9:15)/16, 1-eps, 1)
  } else {
    x &lt;- seq(0, 1, length.out = 1025)
  }
  fx &lt;- cbind(dbeta(x, a,b), pbeta(x, a,b), qbeta(x, a,b))
  f &lt;- fx; f[fx == Inf] &lt;- 1e100
  matplot(x, f, ylab="", type="l", ylim=ylim, asp=asp,
          main = sprintf("[dpq]beta(x, a=%g, b=%g)", a,b))
  abline(0,1,     col="gray", lty=3)
  abline(h = 0:1, col="gray", lty=3)
  legend("top", paste0(c("d","p","q"), "beta(x, a,b)"),
         col=1:3, lty=1:3, bty = "n")
  invisible(cbind(x, fx))
}
pl.beta(3,1)

pl.beta(2, 4)
pl.beta(3, 7)
pl.beta(3, 7, asp=1)

pl.beta(0, 0)   ## point masses at  {0, 1}

pl.beta(0, 2)   ## point mass at 0 ; the same as
pl.beta(1, Inf)

pl.beta(Inf, 2) ## point mass at 1 ; the same as
pl.beta(3, 0)

pl.beta(Inf, Inf)# point mass at 1/2
</code></pre>

<hr>
<h2 id='binom.test'>Exact Binomial Test</h2><span id='topic+binom.test'></span>

<h3>Description</h3>

<p>Performs an exact test of a simple null hypothesis about the
probability of success in a Bernoulli experiment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>binom.test(x, n, p = 0.5,
           alternative = c("two.sided", "less", "greater"),
           conf.level = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binom.test_+3A_x">x</code></td>
<td>
<p>number of successes, or a vector of length 2 giving the
numbers of successes and failures, respectively.</p>
</td></tr>
<tr><td><code id="binom.test_+3A_n">n</code></td>
<td>
<p>number of trials; ignored if <code>x</code> has length 2.</p>
</td></tr>
<tr><td><code id="binom.test_+3A_p">p</code></td>
<td>
<p>hypothesized probability of success.</p>
</td></tr>
<tr><td><code id="binom.test_+3A_alternative">alternative</code></td>
<td>
<p>indicates the alternative hypothesis and must be
one of <code>"two.sided"</code>, <code>"greater"</code> or <code>"less"</code>.
You can specify just the initial letter.</p>
</td></tr>
<tr><td><code id="binom.test_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level for the returned confidence
interval.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Confidence intervals are obtained by a procedure first given in
Clopper and Pearson (1934).
This guarantees that the confidence level
is at least <code>conf.level</code>, but in general does not give the
shortest-length confidence intervals.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the number of successes.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the number of trials.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>a confidence interval for the probability of success.</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>the estimated probability of success.</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>the probability of success under the null,
<code>p</code>.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative
hypothesis.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the character string <code>"Exact binomial test"</code>.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the names of the data.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Clopper, C. J. &amp; Pearson, E. S. (1934).
The use of confidence or fiducial limits illustrated in the case of
the binomial.
<em>Biometrika</em>, <b>26</b>, 404&ndash;413.
<a href="https://doi.org/10.2307/2331986">doi:10.2307/2331986</a>.
</p>
<p>William J. Conover (1971),
<em>Practical nonparametric statistics</em>.
New York: John Wiley &amp; Sons.
Pages 97&ndash;104.
</p>
<p>Myles Hollander &amp; Douglas A. Wolfe (1973),
<em>Nonparametric Statistical Methods.</em>
New York: John Wiley &amp; Sons.
Pages 15&ndash;22.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prop.test">prop.test</a></code> for a general (approximate) test for equal or
given proportions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Conover (1971), p. 97f.
## Under (the assumption of) simple Mendelian inheritance, a cross
##  between plants of two particular genotypes produces progeny 1/4 of
##  which are "dwarf" and 3/4 of which are "giant", respectively.
##  In an experiment to determine if this assumption is reasonable, a
##  cross results in progeny having 243 dwarf and 682 giant plants.
##  If "giant" is taken as success, the null hypothesis is that p =
##  3/4 and the alternative that p != 3/4.
binom.test(c(682, 243), p = 3/4)
binom.test(682, 682 + 243, p = 3/4)   # The same.
## =&gt; Data are in agreement with the null hypothesis.
</code></pre>

<hr>
<h2 id='Binomial'>The Binomial Distribution</h2><span id='topic+Binomial'></span><span id='topic+dbinom'></span><span id='topic+pbinom'></span><span id='topic+qbinom'></span><span id='topic+rbinom'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the binomial distribution with parameters <code>size</code>
and <code>prob</code>.
</p>
<p>This is conventionally interpreted as the number of &lsquo;successes&rsquo;
in <code>size</code> trials.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dbinom(x, size, prob, log = FALSE)
pbinom(q, size, prob, lower.tail = TRUE, log.p = FALSE)
qbinom(p, size, prob, lower.tail = TRUE, log.p = FALSE)
rbinom(n, size, prob)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Binomial_+3A_x">x</code>, <code id="Binomial_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="Binomial_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="Binomial_+3A_n">n</code></td>
<td>
<p>number of observations. If <code>length(n) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="Binomial_+3A_size">size</code></td>
<td>
<p>number of trials (zero or more).</p>
</td></tr>
<tr><td><code id="Binomial_+3A_prob">prob</code></td>
<td>
<p>probability of success on each trial.</p>
</td></tr>
<tr><td><code id="Binomial_+3A_log">log</code>, <code id="Binomial_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="Binomial_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The binomial distribution with <code>size</code> <code class="reqn">= n</code> and
<code>prob</code> <code class="reqn">= p</code> has density
</p>
<p style="text-align: center;"><code class="reqn">p(x) = {n \choose x} {p}^{x} {(1-p)}^{n-x}</code>
</p>

<p>for <code class="reqn">x = 0, \ldots, n</code>.
Note that binomial <em>coefficients</em> can be computed by
<code><a href="base.html#topic+choose">choose</a></code> in <span class="rlang"><b>R</b></span>.
</p>
<p>If an element of <code>x</code> is not integer, the result of <code>dbinom</code>
is zero, with a warning.
</p>
<p><code class="reqn">p(x)</code> is computed using Loader's algorithm, see the reference below.
</p>
<p>The quantile is defined as the smallest value <code class="reqn">x</code> such that
<code class="reqn">F(x) \ge p</code>, where <code class="reqn">F</code> is the distribution function.
</p>


<h3>Value</h3>

<p><code>dbinom</code> gives the density, <code>pbinom</code> gives the distribution
function, <code>qbinom</code> gives the quantile function and <code>rbinom</code>
generates random deviates.
</p>
<p>If <code>size</code> is not an integer, <code>NaN</code> is returned.
</p>
<p>The length of the result is determined by <code>n</code> for
<code>rbinom</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.
</p>
<p>The numerical arguments other than <code>n</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.
</p>


<h3>Source</h3>

<p>For <code>dbinom</code> a saddle-point expansion is used: see
</p>
<p>Catherine Loader (2000). <em>Fast and Accurate Computation of
Binomial Probabilities</em>; available as
<a href="https://www.r-project.org/doc/reports/CLoader-dbinom-2002.pdf">https://www.r-project.org/doc/reports/CLoader-dbinom-2002.pdf</a>
</p>
<p><code>pbinom</code> uses <code><a href="#topic+pbeta">pbeta</a></code>.
</p>
<p><code>qbinom</code> uses the Cornish&ndash;Fisher Expansion to include a skewness
correction to a normal approximation, followed by a search.
</p>
<p><code>rbinom</code> (for <code>size &lt; .Machine$integer.max</code>) is based on
</p>
<p>Kachitvichyanukul, V. and Schmeiser, B. W. (1988)
Binomial random variate generation.
<em>Communications of the ACM</em>, <b>31</b>, 216&ndash;222.
</p>
<p>For larger values it uses inversion.
</p>


<h3>See Also</h3>

<p><a href="#topic+Distributions">Distributions</a> for other standard distributions, including
<code><a href="#topic+dnbinom">dnbinom</a></code> for the negative binomial, and
<code><a href="#topic+dpois">dpois</a></code> for the Poisson distribution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)
# Compute P(45 &lt; X &lt; 55) for X Binomial(100,0.5)
sum(dbinom(46:54, 100, 0.5))

## Using "log = TRUE" for an extended range :
n &lt;- 2000
k &lt;- seq(0, n, by = 20)
plot (k, dbinom(k, n, pi/10, log = TRUE), type = "l", ylab = "log density",
      main = "dbinom(*, log=TRUE) is better than  log(dbinom(*))")
lines(k, log(dbinom(k, n, pi/10)), col = "red", lwd = 2)
## extreme points are omitted since dbinom gives 0.
mtext("dbinom(k, log=TRUE)", adj = 0)
mtext("extended range", adj = 0, line = -1, font = 4)
mtext("log(dbinom(k))", col = "red", adj = 1)
</code></pre>

<hr>
<h2 id='biplot'>Biplot of Multivariate Data</h2><span id='topic+biplot'></span><span id='topic+biplot.default'></span>

<h3>Description</h3>

<p>Plot a biplot on the current graphics device.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>biplot(x, ...)

## Default S3 method:
biplot(x, y, var.axes = TRUE, col, cex = rep(par("cex"), 2),
       xlabs = NULL, ylabs = NULL, expand = 1,
       xlim  = NULL, ylim  = NULL, arrow.len = 0.1,
       main = NULL, sub = NULL, xlab = NULL, ylab = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="biplot_+3A_x">x</code></td>
<td>
<p>The <code>biplot</code>, a fitted object. For <code>biplot.default</code>,
the first set of points (a two-column matrix), usually associated
with observations.</p>
</td></tr>
<tr><td><code id="biplot_+3A_y">y</code></td>
<td>
<p>The second set of points (a two-column matrix), usually associated
with variables.</p>
</td></tr>
<tr><td><code id="biplot_+3A_var.axes">var.axes</code></td>
<td>
<p>If <code>TRUE</code> the second set of points have arrows
representing them as (unscaled) axes.</p>
</td></tr>
<tr><td><code id="biplot_+3A_col">col</code></td>
<td>
<p>A vector of length 2 giving the colours for the first and
second set of points respectively (and the corresponding axes). If a
single colour is specified it will be used for both sets.  If
missing the default colour is looked for in the
<code><a href="grDevices.html#topic+palette">palette</a></code>: if there it and the next colour as used,
otherwise the first two colours of the palette are used.</p>
</td></tr>
<tr><td><code id="biplot_+3A_cex">cex</code></td>
<td>
<p>The character expansion factor used for labelling the
points. The labels can be of different sizes for the two sets by
supplying a vector of length two.</p>
</td></tr>
<tr><td><code id="biplot_+3A_xlabs">xlabs</code></td>
<td>
<p>A vector of character strings to label the first set of
points: the default is to use the row dimname of <code>x</code>, or
<code>1:n</code> if the dimname is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="biplot_+3A_ylabs">ylabs</code></td>
<td>
<p>A vector of character strings to label the second set of
points: the default is to use the row dimname of <code>y</code>, or
<code>1:n</code> if the dimname is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="biplot_+3A_expand">expand</code></td>
<td>
<p>An expansion factor to apply when plotting the second set
of points relative to the first. This can be used to tweak the
scaling of the two sets to a physically comparable scale.</p>
</td></tr>
<tr><td><code id="biplot_+3A_arrow.len">arrow.len</code></td>
<td>
<p>The length of the arrow heads on the axes plotted in
<code>var.axes</code> is true. The arrow head can be suppressed by
<code>arrow.len = 0</code>.</p>
</td></tr>
<tr><td><code id="biplot_+3A_xlim">xlim</code>, <code id="biplot_+3A_ylim">ylim</code></td>
<td>
<p>Limits for the x and y axes in the units of the
first set of variables.</p>
</td></tr>
<tr><td><code id="biplot_+3A_main">main</code>, <code id="biplot_+3A_sub">sub</code>, <code id="biplot_+3A_xlab">xlab</code>, <code id="biplot_+3A_ylab">ylab</code>, <code id="biplot_+3A_...">...</code></td>
<td>
<p>graphical parameters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A biplot is plot which aims to represent both the observations and
variables of a matrix of multivariate data on the same plot. There are
many variations on biplots (see the references) and perhaps the most
widely used one is implemented by <code><a href="#topic+biplot.princomp">biplot.princomp</a></code>.
The function <code>biplot.default</code> merely provides the
underlying code to plot two sets of variables on the same figure.
</p>
<p>Graphical parameters can also be given to <code>biplot</code>: the size of
<code>xlabs</code> and <code>ylabs</code> is controlled by <code>cex</code>.
</p>


<h3>Side Effects</h3>

<p>a plot is produced on the current graphics device.
</p>


<h3>References</h3>

<p>K. R. Gabriel (1971).
The biplot graphical display of matrices with application to principal
component analysis.
<em>Biometrika</em>, <b>58</b>, 453&ndash;467.
<a href="https://doi.org/10.2307/2334381">doi:10.2307/2334381</a>.
</p>
<p>J.C. Gower and D. J. Hand (1996).
<em>Biplots</em>.
Chapman &amp; Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+biplot.princomp">biplot.princomp</a></code>, also for examples.</p>

<hr>
<h2 id='biplot.princomp'>
Biplot for Principal Components
</h2><span id='topic+biplot.princomp'></span><span id='topic+biplot.prcomp'></span>

<h3>Description</h3>

<p>Produces a biplot (in the strict sense) from the output of
<code><a href="#topic+princomp">princomp</a></code> or <code><a href="#topic+prcomp">prcomp</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'prcomp'
biplot(x, choices = 1:2, scale = 1, pc.biplot = FALSE, ...)

## S3 method for class 'princomp'
biplot(x, choices = 1:2, scale = 1, pc.biplot = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="biplot.princomp_+3A_x">x</code></td>
<td>
<p>an object of class <code>"princomp"</code>.</p>
</td></tr>
<tr><td><code id="biplot.princomp_+3A_choices">choices</code></td>
<td>

<p>length 2 vector specifying the components to plot. Only the default
is a biplot in the strict sense.
</p>
</td></tr>
<tr><td><code id="biplot.princomp_+3A_scale">scale</code></td>
<td>

<p>The variables are scaled by <code>lambda ^ scale</code> and the
observations are scaled by <code>lambda ^ (1-scale)</code> where
<code>lambda</code> are the singular values as computed by
<code><a href="#topic+princomp">princomp</a></code>. Normally <code>0 &lt;= scale &lt;= 1</code>, and a warning
will be issued if the specified <code>scale</code> is outside this range.
</p>
</td></tr>
<tr><td><code id="biplot.princomp_+3A_pc.biplot">pc.biplot</code></td>
<td>

<p>If true, use what Gabriel (1971) refers to as a &quot;principal component
biplot&quot;, with <code>lambda = 1</code> and observations scaled up by sqrt(n) and
variables scaled down by sqrt(n).  Then inner products between
variables approximate covariances and distances between observations
approximate Mahalanobis distance.
</p>
</td></tr>
<tr><td><code id="biplot.princomp_+3A_...">...</code></td>
<td>
<p>optional arguments to be passed to
<code><a href="#topic+biplot.default">biplot.default</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a method for the generic function <code>biplot</code>.  There is
considerable confusion over the precise definitions: those of the
original paper, Gabriel (1971), are followed here.  Gabriel and
Odoroff (1990) use the same definitions, but their plots actually
correspond to <code>pc.biplot = TRUE</code>.
</p>


<h3>Side Effects</h3>

<p>a plot is produced on the current graphics device.
</p>


<h3>References</h3>

<p>Gabriel, K. R. (1971).
The biplot graphical display of matrices with applications to
principal component analysis.
<em>Biometrika</em>, <b>58</b>, 453&ndash;467.
<a href="https://doi.org/10.2307/2334381">doi:10.2307/2334381</a>.
</p>
<p>Gabriel, K. R. and Odoroff, C. L. (1990).
Biplots in biomedical research.
<em>Statistics in Medicine</em>, <b>9</b>, 469&ndash;485.
<a href="https://doi.org/10.1002/sim.4780090502">doi:10.1002/sim.4780090502</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+biplot">biplot</a></code>,
<code><a href="#topic+princomp">princomp</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)
biplot(princomp(USArrests))
</code></pre>

<hr>
<h2 id='birthday'>Probability of coincidences</h2><span id='topic+qbirthday'></span><span id='topic+pbirthday'></span>

<h3>Description</h3>

<p>Computes answers to a generalised <em>birthday paradox</em> problem.
<code>pbirthday</code> computes the probability of a coincidence and
<code>qbirthday</code> computes the smallest number of observations needed
to have at least a specified probability of coincidence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qbirthday(prob = 0.5, classes = 365, coincident = 2)
pbirthday(n, classes = 365, coincident = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="birthday_+3A_classes">classes</code></td>
<td>
<p>How many distinct categories the people could fall into</p>
</td></tr>
<tr><td><code id="birthday_+3A_prob">prob</code></td>
<td>
<p>The desired probability of coincidence</p>
</td></tr>
<tr><td><code id="birthday_+3A_n">n</code></td>
<td>
<p>The number of people</p>
</td></tr>
<tr><td><code id="birthday_+3A_coincident">coincident</code></td>
<td>
<p>The number of people to fall in the same category</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The birthday paradox is that a very small number of people, 23,
suffices to have a 50&ndash;50 chance that two or more of them have the same
birthday.  This function generalises the calculation to probabilities
other than 0.5, numbers of coincident events other than 2, and numbers
of classes other than 365.
</p>
<p>The formula used is approximate for <code>coincident &gt; 2</code>.  The
approximation is very good for moderate values of <code>prob</code> but less
good for very small probabilities.
</p>


<h3>Value</h3>

<table>
<tr><td><code>qbirthday</code></td>
<td>

<p>Minimum number of people needed for a probability of at least
<code>prob</code> that <code>k</code> or more of them have the same one out of
<code>classes</code> equiprobable labels.
</p>
</td></tr>
<tr><td><code>pbirthday</code></td>
<td>
<p>Probability of the specified coincidence.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Diaconis, P. and Mosteller F. (1989).
Methods for studying coincidences.
<em>Journal of the American Statistical Association</em>, <b>84</b>,
853&ndash;861.
<a href="https://doi.org/10.1080/01621459.1989.10478847">doi:10.1080/01621459.1989.10478847</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

## the standard version
qbirthday() # 23
## probability of &gt; 2 people with the same birthday
pbirthday(23, coincident = 3)

## examples from Diaconis &amp; Mosteller p. 858.
## 'coincidence' is that husband, wife, daughter all born on the 16th
qbirthday(classes = 30, coincident = 3) # approximately 18
qbirthday(coincident = 4)  # exact value 187
qbirthday(coincident = 10) # exact value 1181

## same 4-digit PIN number
qbirthday(classes = 10^4)

## 0.9 probability of three or more coincident birthdays
qbirthday(coincident = 3, prob = 0.9)

## Chance of 4 or more coincident birthdays in 150 people
pbirthday(150, coincident = 4)

## 100 or more coincident birthdays in 1000 people: very rare
pbirthday(1000, coincident = 100)
</code></pre>

<hr>
<h2 id='Box.test'>Box-Pierce and Ljung-Box Tests</h2><span id='topic+Box.test'></span>

<h3>Description</h3>

<p>Compute the Box&ndash;Pierce or Ljung&ndash;Box test statistic for examining the
null hypothesis of independence in a given time series.  These are
sometimes known as &lsquo;portmanteau&rsquo; tests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Box.test(x, lag = 1, type = c("Box-Pierce", "Ljung-Box"), fitdf = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Box.test_+3A_x">x</code></td>
<td>
<p>a numeric vector or univariate time series.</p>
</td></tr>
<tr><td><code id="Box.test_+3A_lag">lag</code></td>
<td>
<p>the statistic will be based on <code>lag</code> autocorrelation
coefficients.</p>
</td></tr>
<tr><td><code id="Box.test_+3A_type">type</code></td>
<td>
<p>test to be performed: partial matching is used.</p>
</td></tr>
<tr><td><code id="Box.test_+3A_fitdf">fitdf</code></td>
<td>
<p>number of degrees of freedom to be subtracted if <code>x</code>
is a series of residuals.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These tests are sometimes applied to the residuals from an
<code>ARMA(p, q)</code> fit, in which case the references suggest a better
approximation to the null-hypothesis distribution is obtained by
setting <code>fitdf = p+q</code>, provided of course that <code>lag &gt; fitdf</code>.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of freedom of the approximate chi-squared
distribution of the test statistic (taking <code>fitdf</code> into account).</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string indicating which type of test was
performed.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the name of the data.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Missing values are not handled.
</p>


<h3>Author(s)</h3>

<p>A. Trapletti</p>


<h3>References</h3>

<p>Box, G. E. P. and Pierce, D. A. (1970),
Distribution of residual correlations in autoregressive-integrated
moving average time series models.
<em>Journal of the American Statistical Association</em>, <b>65</b>,
1509&ndash;1526.
<a href="https://doi.org/10.2307/2284333">doi:10.2307/2284333</a>.
</p>
<p>Ljung, G. M. and Box, G. E. P. (1978),
On a measure of lack of fit in time series models.
<em>Biometrika</em>, <b>65</b>, 297&ndash;303.
<a href="https://doi.org/10.2307/2335207">doi:10.2307/2335207</a>.
</p>
<p>Harvey, A. C. (1993)
<em>Time Series Models</em>.
2nd Edition, Harvester Wheatsheaf, NY, pp. 44, 45.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm (100)
Box.test (x, lag = 1)
Box.test (x, lag = 1, type = "Ljung")
</code></pre>

<hr>
<h2 id='C'>Sets Contrasts for a Factor</h2><span id='topic+C'></span>

<h3>Description</h3>

<p>Sets the <code>"contrasts"</code> attribute for the factor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>C(object, contr, how.many, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="C_+3A_object">object</code></td>
<td>
<p>a factor or ordered factor</p>
</td></tr>
<tr><td><code id="C_+3A_contr">contr</code></td>
<td>
<p>which contrasts to use. Can be a matrix with one row for
each level of the factor or a suitable function like
<code>contr.poly</code> or a character string giving the name of the function</p>
</td></tr>
<tr><td><code id="C_+3A_how.many">how.many</code></td>
<td>
<p>the number of contrasts to set, by default one less
than <code>nlevels(object)</code>.</p>
</td></tr>
<tr><td><code id="C_+3A_...">...</code></td>
<td>
<p>additional arguments for the function <code>contr</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For compatibility with S, <code>contr</code> can be <code>treatment</code>,
<code>helmert</code>, <code>sum</code> or <code>poly</code> (without quotes) as shorthand
for <code>contr.treatment</code> and so on.
</p>


<h3>Value</h3>

<p>The factor <code>object</code> with the <code>"contrasts"</code> attribute set.
</p>


<h3>References</h3>

<p>Chambers, J. M. and Hastie, T. J. (1992)
<em>Statistical models.</em>
Chapter 2 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+contrasts">contrasts</a></code>, <code><a href="#topic+contr.sum">contr.sum</a></code>, etc.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## reset contrasts to defaults
options(contrasts = c("contr.treatment", "contr.poly"))
tens &lt;- with(warpbreaks, C(tension, poly, 1))
attributes(tens)
## tension SHOULD be an ordered factor, but as it is not we can use
aov(breaks ~ wool + tens + tension, data = warpbreaks)

## show the use of ...  The default contrast is contr.treatment here
summary(lm(breaks ~ wool + C(tension, base = 2), data = warpbreaks))


# following on from help(esoph)
model3 &lt;- glm(cbind(ncases, ncontrols) ~ agegp + C(tobgp, , 1) +
     C(alcgp, , 1), data = esoph, family = binomial())
summary(model3)
</code></pre>

<hr>
<h2 id='cancor'>Canonical Correlations</h2><span id='topic+cancor'></span>

<h3>Description</h3>

<p>Compute the canonical correlations between two data matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cancor(x, y, xcenter = TRUE, ycenter = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cancor_+3A_x">x</code></td>
<td>
<p>numeric matrix (<code class="reqn">n \times p_1</code>), containing the
x coordinates.</p>
</td></tr>
<tr><td><code id="cancor_+3A_y">y</code></td>
<td>
<p>numeric matrix (<code class="reqn">n \times p_2</code>), containing the
y coordinates.</p>
</td></tr>
<tr><td><code id="cancor_+3A_xcenter">xcenter</code></td>
<td>
<p>logical or numeric vector of length <code class="reqn">p_1</code>,
describing any centering to be done on the x values before the
analysis.  If <code>TRUE</code> (default), subtract the column means.
If <code>FALSE</code>, do not adjust the columns.  Otherwise, a vector
of values to be subtracted from the columns.</p>
</td></tr>
<tr><td><code id="cancor_+3A_ycenter">ycenter</code></td>
<td>
<p>analogous to <code>xcenter</code>, but for the y values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The canonical correlation analysis seeks linear combinations of the
<code>y</code> variables which are well explained by linear combinations
of the <code>x</code> variables. The relationship is symmetric as
&lsquo;well explained&rsquo; is measured by correlations.
</p>


<h3>Value</h3>

<p>A list containing the following components:
</p>
<table>
<tr><td><code>cor</code></td>
<td>
<p>correlations.</p>
</td></tr>
<tr><td><code>xcoef</code></td>
<td>
<p>estimated coefficients for the <code>x</code> variables.</p>
</td></tr>
<tr><td><code>ycoef</code></td>
<td>
<p>estimated coefficients for the <code>y</code> variables.</p>
</td></tr>
<tr><td><code>xcenter</code></td>
<td>
<p>the values used to adjust the <code>x</code> variables.</p>
</td></tr>
<tr><td><code>ycenter</code></td>
<td>
<p>the values used to adjust the <code>x</code> variables.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988).
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Hotelling H. (1936).
Relations between two sets of variables.
<em>Biometrika</em>, <b>28</b>, 321&ndash;327.
<a href="https://doi.org/10.1093/biomet/28.3-4.321">doi:10.1093/biomet/28.3-4.321</a>.
</p>
<p>Seber, G. A. F. (1984).
<em>Multivariate Observations</em>.
New York: Wiley.
Page 506f.
</p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+qr">qr</a></code>, <code><a href="base.html#topic+svd">svd</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## signs of results are random
pop &lt;- LifeCycleSavings[, 2:3]
oec &lt;- LifeCycleSavings[, -(2:3)]
cancor(pop, oec)

x &lt;- matrix(rnorm(150), 50, 3)
y &lt;- matrix(rnorm(250), 50, 5)
(cxy &lt;- cancor(x, y))
all(abs(cor(x %*% cxy$xcoef,
            y %*% cxy$ycoef)[,1:3] - diag(cxy $ cor)) &lt; 1e-15)
all(abs(cor(x %*% cxy$xcoef) - diag(3)) &lt; 1e-15)
all(abs(cor(y %*% cxy$ycoef) - diag(5)) &lt; 1e-15)
</code></pre>

<hr>
<h2 id='case+2Bvariable.names'>Case and Variable Names of Fitted Models</h2><span id='topic+case.names'></span><span id='topic+case.names.lm'></span><span id='topic+variable.names'></span><span id='topic+variable.names.lm'></span>

<h3>Description</h3>

<p>Simple utilities returning (non-missing) case names, and
(non-eliminated) variable names.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>case.names(object, ...)
## S3 method for class 'lm'
case.names(object, full = FALSE, ...)

variable.names(object, ...)
## S3 method for class 'lm'
variable.names(object, full = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="case+2B2Bvariable.names_+3A_object">object</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object, typically a fitted model.</p>
</td></tr>
<tr><td><code id="case+2B2Bvariable.names_+3A_full">full</code></td>
<td>
<p>logical; if <code>TRUE</code>, all names (including zero weights,
...) are returned.</p>
</td></tr>
<tr><td><code id="case+2B2Bvariable.names_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lm">lm</a></code>; further, <code><a href="base.html#topic+all.names">all.names</a></code>,
<code><a href="base.html#topic+all.vars">all.vars</a></code> for functions with a similar name but only slightly
related purpose.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 1:20
y &lt;-  setNames(x + (x/4 - 2)^3 + rnorm(20, sd = 3),
               paste("O", x, sep = "."))
ww &lt;- rep(1, 20); ww[13] &lt;- 0
summary(lmxy &lt;- lm(y ~ x + I(x^2)+I(x^3) + I((x-10)^2), weights = ww),
        correlation = TRUE)
variable.names(lmxy)
variable.names(lmxy, full = TRUE)  # includes the last
case.names(lmxy)
case.names(lmxy, full = TRUE)      # includes the 0-weight case
</code></pre>

<hr>
<h2 id='Cauchy'>The Cauchy Distribution</h2><span id='topic+Cauchy'></span><span id='topic+dcauchy'></span><span id='topic+pcauchy'></span><span id='topic+qcauchy'></span><span id='topic+rcauchy'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the Cauchy distribution with location parameter
<code>location</code> and scale parameter <code>scale</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dcauchy(x, location = 0, scale = 1, log = FALSE)
pcauchy(q, location = 0, scale = 1, lower.tail = TRUE, log.p = FALSE)
qcauchy(p, location = 0, scale = 1, lower.tail = TRUE, log.p = FALSE)
rcauchy(n, location = 0, scale = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cauchy_+3A_x">x</code>, <code id="Cauchy_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="Cauchy_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="Cauchy_+3A_n">n</code></td>
<td>
<p>number of observations. If <code>length(n) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="Cauchy_+3A_location">location</code>, <code id="Cauchy_+3A_scale">scale</code></td>
<td>
<p>location and scale parameters.</p>
</td></tr>
<tr><td><code id="Cauchy_+3A_log">log</code>, <code id="Cauchy_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="Cauchy_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>location</code> or <code>scale</code> are not specified, they assume
the default values of <code>0</code> and <code>1</code> respectively.
</p>
<p>The Cauchy distribution with location <code class="reqn">l</code> and scale <code class="reqn">s</code> has
density
</p>
<p style="text-align: center;"><code class="reqn">f(x) = \frac{1}{\pi s}
    \left( 1 + \left(\frac{x - l}{s}\right)^2 \right)^{-1}%
  </code>
</p>

<p>for all <code class="reqn">x</code>.
</p>


<h3>Value</h3>

<p><code>dcauchy</code>, <code>pcauchy</code>, and <code>qcauchy</code> are respectively
the density, distribution function and quantile function of the Cauchy
distribution.  <code>rcauchy</code> generates random deviates from the
Cauchy.
</p>
<p>The length of the result is determined by <code>n</code> for
<code>rcauchy</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.  
</p>
<p>The numerical arguments other than <code>n</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.    
</p>


<h3>Source</h3>

<p><code>dcauchy</code>, <code>pcauchy</code> and <code>qcauchy</code> are all calculated
from numerically stable versions of the definitions.
</p>
<p><code>rcauchy</code> uses inversion.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
<em>Continuous Univariate Distributions</em>, volume 1, chapter 16.
Wiley, New York.
</p>


<h3>See Also</h3>

<p><a href="#topic+Distributions">Distributions</a> for other standard distributions, including
<code><a href="#topic+dt">dt</a></code> for the t distribution which generalizes
<code>dcauchy(*, l = 0, s = 1)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dcauchy(-1:4)
</code></pre>

<hr>
<h2 id='chisq.test'>Pearson's Chi-squared Test for Count Data</h2><span id='topic+chisq.test'></span>

<h3>Description</h3>

<p><code>chisq.test</code> performs chi-squared contingency table tests
and goodness-of-fit tests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chisq.test(x, y = NULL, correct = TRUE,
           p = rep(1/length(x), length(x)), rescale.p = FALSE,
           simulate.p.value = FALSE, B = 2000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chisq.test_+3A_x">x</code></td>
<td>
<p>a numeric vector or matrix. <code>x</code> and <code>y</code> can also
both be factors.</p>
</td></tr>
<tr><td><code id="chisq.test_+3A_y">y</code></td>
<td>
<p>a numeric vector; ignored if <code>x</code> is a matrix.  If
<code>x</code> is a factor, <code>y</code> should be a factor of the same length.</p>
</td></tr>
<tr><td><code id="chisq.test_+3A_correct">correct</code></td>
<td>
<p>a logical indicating whether to apply continuity
correction when computing the test statistic for 2 by 2 tables: one
half is subtracted from all <code class="reqn">|O - E|</code> differences; however, the
correction will not be bigger than the differences themselves.  No correction
is done if <code>simulate.p.value = TRUE</code>.</p>
</td></tr>
<tr><td><code id="chisq.test_+3A_p">p</code></td>
<td>
<p>a vector of probabilities of the same length as <code>x</code>.
An error is given if any entry of <code>p</code> is negative.</p>
</td></tr>
<tr><td><code id="chisq.test_+3A_rescale.p">rescale.p</code></td>
<td>
<p>a logical scalar; if TRUE then <code>p</code> is rescaled
(if necessary) to sum to 1.  If <code>rescale.p</code> is FALSE, and
<code>p</code> does not sum to 1, an error is given.</p>
</td></tr>
<tr><td><code id="chisq.test_+3A_simulate.p.value">simulate.p.value</code></td>
<td>
<p>a logical indicating whether to compute
p-values by Monte Carlo simulation.</p>
</td></tr>
<tr><td><code id="chisq.test_+3A_b">B</code></td>
<td>
<p>an integer specifying the number of replicates used in the
Monte Carlo test.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>x</code> is a matrix with one row or column, or if <code>x</code> is a
vector and <code>y</code> is not given, then a <em>goodness-of-fit test</em>
is performed (<code>x</code> is treated as a one-dimensional
contingency table).  The entries of <code>x</code> must be non-negative
integers.  In this case, the hypothesis tested is whether the
population probabilities equal those in <code>p</code>, or are all equal if
<code>p</code> is not given.
</p>
<p>If <code>x</code> is a matrix with at least two rows and columns, it is
taken as a two-dimensional contingency table: the entries of <code>x</code>
must be non-negative integers.  Otherwise, <code>x</code> and <code>y</code> must
be vectors or factors of the same length; cases with missing values
are removed, the objects are coerced to factors, and the contingency
table is computed from these.  Then Pearson's chi-squared test is
performed of the null hypothesis that the joint distribution of the
cell counts in a 2-dimensional contingency table is the product of the
row and column marginals.
</p>
<p>If <code>simulate.p.value</code> is <code>FALSE</code>, the p-value is computed
from the asymptotic chi-squared distribution of the test statistic;
continuity correction is only used in the 2-by-2 case (if <code>correct</code>
is <code>TRUE</code>, the default).  Otherwise the p-value is computed for a
Monte Carlo test (Hope, 1968) with <code>B</code> replicates. The default
<code>B = 2000</code> implies a minimum p-value of about 0.0005 (<code class="reqn">1/(B+1)</code>).
</p>
<p>In the contingency table case, simulation is done by random sampling
from the set of all contingency tables with given marginals, and works
only if the marginals are strictly positive.  Continuity correction is
never used, and the statistic is quoted without it.  Note that this is
not the usual sampling situation assumed for the chi-squared test but
rather that for Fisher's exact test.
</p>
<p>In the goodness-of-fit case simulation is done by random sampling from
the discrete distribution specified by <code>p</code>, each sample being
of size <code>n = sum(x)</code>.  This simulation is done in <span class="rlang"><b>R</b></span> and may be
slow.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following
components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value the chi-squared test statistic.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of freedom of the approximate
chi-squared distribution of the test statistic, <code>NA</code> if the
p-value is computed by Monte Carlo simulation.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string indicating the type of test
performed, and whether Monte Carlo simulation or continuity
correction was used.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the name(s) of the data.</p>
</td></tr>
<tr><td><code>observed</code></td>
<td>
<p>the observed counts.</p>
</td></tr>
<tr><td><code>expected</code></td>
<td>
<p>the expected counts under the null hypothesis.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>the Pearson residuals,
<code>(observed - expected) / sqrt(expected)</code>.</p>
</td></tr>
<tr><td><code>stdres</code></td>
<td>
<p>standardized residuals,
<code>(observed - expected) / sqrt(V)</code>, where <code>V</code> is the
residual cell variance (Agresti, 2007, section 2.4.5
for the case where <code>x</code> is a matrix, <code>n * p * (1 - p)</code> otherwise).</p>
</td></tr>
</table>


<h3>Source</h3>

<p>The code for Monte Carlo simulation is a C translation of the Fortran
algorithm of Patefield (1981).
</p>


<h3>References</h3>

<p>Hope, A. C. A. (1968).
A simplified Monte Carlo significance test procedure.
<em>Journal of the Royal Statistical Society Series B</em>, <b>30</b>,
582&ndash;598.
<a href="https://doi.org/10.1111/j.2517-6161.1968.tb00759.x">doi:10.1111/j.2517-6161.1968.tb00759.x</a>.

</p>
<p>Patefield, W. M. (1981).
Algorithm AS 159: An efficient method of generating r x c tables
with given row and column totals.
<em>Applied Statistics</em>, <b>30</b>, 91&ndash;97.
<a href="https://doi.org/10.2307/2346669">doi:10.2307/2346669</a>.
</p>
<p>Agresti, A. (2007).
<em>An Introduction to Categorical Data Analysis</em>, 2nd ed.
New York: John Wiley &amp; Sons.
Page 38.
</p>


<h3>See Also</h3>

<p>For goodness-of-fit testing, notably of continuous distributions,
<code><a href="#topic+ks.test">ks.test</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## From Agresti(2007) p.39
M &lt;- as.table(rbind(c(762, 327, 468), c(484, 239, 477)))
dimnames(M) &lt;- list(gender = c("F", "M"),
                    party = c("Democrat","Independent", "Republican"))
(Xsq &lt;- chisq.test(M))  # Prints test summary
Xsq$observed   # observed counts (same as M)
Xsq$expected   # expected counts under the null
Xsq$residuals  # Pearson residuals
Xsq$stdres     # standardized residuals


## Effect of simulating p-values
x &lt;- matrix(c(12, 5, 7, 7), ncol = 2)
chisq.test(x)$p.value           # 0.4233
chisq.test(x, simulate.p.value = TRUE, B = 10000)$p.value
                                # around 0.29!

## Testing for population probabilities
## Case A. Tabulated data
x &lt;- c(A = 20, B = 15, C = 25)
chisq.test(x)
chisq.test(as.table(x))             # the same
x &lt;- c(89,37,30,28,2)
p &lt;- c(40,20,20,15,5)
try(
chisq.test(x, p = p)                # gives an error
)
chisq.test(x, p = p, rescale.p = TRUE)
                                # works
p &lt;- c(0.40,0.20,0.20,0.19,0.01)
                                # Expected count in category 5
                                # is 1.86 &lt; 5 ==&gt; chi square approx.
chisq.test(x, p = p)            #               maybe doubtful, but is ok!
chisq.test(x, p = p, simulate.p.value = TRUE)

## Case B. Raw data
x &lt;- trunc(5 * runif(100))
chisq.test(table(x))            # NOT 'chisq.test(x)'!
</code></pre>

<hr>
<h2 id='Chisquare'>The (non-central) Chi-Squared Distribution</h2><span id='topic+Chisquare'></span><span id='topic+dchisq'></span><span id='topic+pchisq'></span><span id='topic+qchisq'></span><span id='topic+rchisq'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the chi-squared (<code class="reqn">\chi^2</code>) distribution with
<code>df</code> degrees of freedom and optional non-centrality parameter
<code>ncp</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dchisq(x, df, ncp = 0, log = FALSE)
pchisq(q, df, ncp = 0, lower.tail = TRUE, log.p = FALSE)
qchisq(p, df, ncp = 0, lower.tail = TRUE, log.p = FALSE)
rchisq(n, df, ncp = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Chisquare_+3A_x">x</code>, <code id="Chisquare_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="Chisquare_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="Chisquare_+3A_n">n</code></td>
<td>
<p>number of observations. If <code>length(n) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="Chisquare_+3A_df">df</code></td>
<td>
<p>degrees of freedom (non-negative, but can be non-integer).</p>
</td></tr>
<tr><td><code id="Chisquare_+3A_ncp">ncp</code></td>
<td>
<p>non-centrality parameter (non-negative).</p>
</td></tr>
<tr><td><code id="Chisquare_+3A_log">log</code>, <code id="Chisquare_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="Chisquare_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The chi-squared distribution with <code>df</code><code class="reqn">= n \ge 0</code>
degrees of freedom has density
</p>
<p style="text-align: center;"><code class="reqn">f_n(x) = \frac{1}{{2}^{n/2} \Gamma (n/2)} {x}^{n/2-1} {e}^{-x/2}</code>
</p>

<p>for <code class="reqn">x &gt; 0</code>, where <code class="reqn">f_0(x) := \lim_{n \to 0} f_n(x) =
  \delta_0(x)</code>, a point mass at zero, is not a density function proper, but
a &ldquo;<code class="reqn">\delta</code> distribution&rdquo;.<br />
The mean and variance are <code class="reqn">n</code> and <code class="reqn">2n</code>.
</p>
<p>The non-central chi-squared distribution with <code>df</code><code class="reqn">= n</code>
degrees of freedom and non-centrality parameter <code>ncp</code>
<code class="reqn">= \lambda</code> has density
</p>
<p style="text-align: center;"><code class="reqn">
    f(x) = f_{n,\lambda}(x) = e^{-\lambda / 2}
      \sum_{r=0}^\infty \frac{(\lambda/2)^r}{r!}\, f_{n + 2r}(x)</code>
</p>

<p>for <code class="reqn">x \ge 0</code>.  For integer <code class="reqn">n</code>, this is the distribution of
the sum of squares of <code class="reqn">n</code> normals each with variance one,
<code class="reqn">\lambda</code> being the sum of squares of the normal means; further,
<br />
<code class="reqn">E(X) = n + \lambda</code>, <code class="reqn">Var(X) = 2(n + 2*\lambda)</code>, and
<code class="reqn">E((X - E(X))^3) = 8(n + 3*\lambda)</code>.
</p>
<p>Note that the degrees of freedom <code>df</code><code class="reqn">= n</code>, can be
non-integer, and also <code class="reqn">n = 0</code> which is relevant for
non-centrality <code class="reqn">\lambda &gt; 0</code>,
see Johnson <abbr>et al.</abbr> (1995, chapter 29).
In that (noncentral, zero <abbr>df</abbr>) case, the distribution is a mixture of a
point mass at <code class="reqn">x = 0</code> (of size <code>pchisq(0, df=0, ncp=ncp)</code>) and
a continuous part, and <code>dchisq()</code> is <em>not</em> a density with
respect to that mixture measure but rather the limit of the density
for <code class="reqn">df \to 0</code>.
</p>
<p>Note that <code>ncp</code> values larger than about 1e5 (and even smaller)  may give inaccurate
results with many warnings for <code>pchisq</code> and <code>qchisq</code>.
</p>


<h3>Value</h3>

<p><code>dchisq</code> gives the density, <code>pchisq</code> gives the distribution
function, <code>qchisq</code> gives the quantile function, and <code>rchisq</code>
generates random deviates.
</p>
<p>Invalid arguments will result in return value <code>NaN</code>, with a warning.
</p>
<p>The length of the result is determined by <code>n</code> for
<code>rchisq</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.
</p>
<p>The numerical arguments other than <code>n</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.
</p>


<h3>Note</h3>

<p>Supplying <code>ncp = 0</code> uses the algorithm for the non-central
distribution, which is not the same algorithm used if <code>ncp</code> is
omitted.  This is to give consistent behaviour in extreme cases with
values of <code>ncp</code> very near zero.
</p>
<p>The code for non-zero <code>ncp</code> is principally intended to be used
for moderate values of <code>ncp</code>: it will not be highly accurate,
especially in the tails, for large values.
</p>


<h3>Source</h3>

<p>The central cases are computed via the gamma distribution.
</p>
<p>The non-central <code>dchisq</code> and <code>rchisq</code> are computed as a
Poisson mixture of central chi-squares (Johnson <abbr>et al.</abbr>, 1995, p.436).
</p>
<p>The non-central <code>pchisq</code> is for <code>ncp &lt; 80</code> computed from
the Poisson mixture of central chi-squares and for larger <code>ncp</code>
<em>via</em> a C translation of
</p>
<p>Ding, C. G. (1992)
Algorithm AS275: Computing the non-central chi-squared
distribution function. <em>Appl.Statist.</em>, <b>41</b> 478&ndash;482.
</p>
<p>which computes the lower tail only (so the upper tail suffers from
cancellation and a warning will be given when this is likely to be
significant).
</p>
<p>The non-central <code>qchisq</code> is based on inversion of <code>pchisq</code>.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
<em>Continuous Univariate Distributions</em>, chapters 18 (volume 1)
and 29 (volume 2). Wiley, New York.
</p>


<h3>See Also</h3>

<p><a href="#topic+Distributions">Distributions</a> for other standard distributions.
</p>
<p>A central chi-squared distribution with <code class="reqn">n</code> degrees of freedom
is the same as a Gamma distribution with <code>shape</code> <code class="reqn">\alpha =
    n/2</code> and <code>scale</code> <code class="reqn">\sigma = 2</code>.  Hence, see
<code><a href="#topic+dgamma">dgamma</a></code> for the Gamma distribution.
</p>
<p>The central chi-squared distribution with 2 d.f. is identical to the
exponential distribution with rate 1/2: <code class="reqn">\chi^2_2 = Exp(1/2)</code>, see
<code><a href="#topic+dexp">dexp</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

dchisq(1, df = 1:3)
pchisq(1, df =  3)
pchisq(1, df =  3, ncp = 0:4)  # includes the above

x &lt;- 1:10
## Chi-squared(df = 2) is a special exponential distribution
all.equal(dchisq(x, df = 2), dexp(x, 1/2))
all.equal(pchisq(x, df = 2), pexp(x, 1/2))

## non-central RNG -- df = 0 with ncp &gt; 0:  Z0 has point mass at 0!
Z0 &lt;- rchisq(100, df = 0, ncp = 2.)
graphics::stem(Z0)

## visual testing
## do P-P plots for 1000 points at various degrees of freedom
L &lt;- 1.2; n &lt;- 1000; pp &lt;- ppoints(n)
op &lt;- par(mfrow = c(3,3), mar = c(3,3,1,1)+.1, mgp = c(1.5,.6,0),
          oma = c(0,0,3,0))
for(df in 2^(4*rnorm(9))) {
  plot(pp, sort(pchisq(rr &lt;- rchisq(n, df = df, ncp = L), df = df, ncp = L)),
       ylab = "pchisq(rchisq(.),.)", pch = ".")
  mtext(paste("df = ", formatC(df, digits = 4)), line =  -2, adj = 0.05)
  abline(0, 1, col = 2)
}
mtext(expression("P-P plots : Noncentral  "*
                 chi^2 *"(n=1000, df=X, ncp= 1.2)"),
      cex = 1.5, font = 2, outer = TRUE)
par(op)

## "analytical" test
lam &lt;- seq(0, 100, by = .25)
p00 &lt;- pchisq(0,      df = 0, ncp = lam)
p.0 &lt;- pchisq(1e-300, df = 0, ncp = lam)
stopifnot(all.equal(p00, exp(-lam/2)),
          all.equal(p.0, exp(-lam/2)))
</code></pre>

<hr>
<h2 id='cmdscale'>Classical (Metric) Multidimensional Scaling</h2><span id='topic+cmdscale'></span>

<h3>Description</h3>

<p>Classical multidimensional scaling (<abbr>MDS</abbr>) of a data matrix.
Also known as <em>principal coordinates analysis</em> (Gower, 1966).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cmdscale(d, k = 2, eig = FALSE, add = FALSE, x.ret = FALSE,
         list. = eig || add || x.ret)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cmdscale_+3A_d">d</code></td>
<td>
<p>a distance structure such as that returned by <code>dist</code>
or a full symmetric matrix containing the dissimilarities.</p>
</td></tr>
<tr><td><code id="cmdscale_+3A_k">k</code></td>
<td>
<p>the maximum dimension of the space which the data are to be
represented in; must be in <code class="reqn">\{1, 2, \ldots, n-1\}</code>.</p>
</td></tr>
<tr><td><code id="cmdscale_+3A_eig">eig</code></td>
<td>
<p>indicates whether eigenvalues should be returned.</p>
</td></tr>
<tr><td><code id="cmdscale_+3A_add">add</code></td>
<td>
<p>logical indicating if an additive constant <code class="reqn">c*</code> should
be computed, and added to the non-diagonal dissimilarities such that
the modified dissimilarities are Euclidean.</p>
</td></tr>
<tr><td><code id="cmdscale_+3A_x.ret">x.ret</code></td>
<td>
<p>indicates whether the doubly centred symmetric distance
matrix should be returned.</p>
</td></tr>
<tr><td><code id="cmdscale_+3A_list.">list.</code></td>
<td>
<p>logical indicating if a <code><a href="base.html#topic+list">list</a></code> should be
returned or just the <code class="reqn">n \times k</code> matrix, see &lsquo;Value:&rsquo;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Multidimensional scaling takes a set of dissimilarities and returns a
set of points such that the distances between the points are
approximately equal to the dissimilarities.  (It is a major part of
what ecologists call &lsquo;ordination&rsquo;.)
</p>
<p>A set of Euclidean distances on <code class="reqn">n</code> points can be represented
exactly in at most <code class="reqn">n - 1</code> dimensions.  <code>cmdscale</code> follows
the analysis of Mardia (1978), and returns the best-fitting
<code class="reqn">k</code>-dimensional representation, where <code class="reqn">k</code> may be less than the
argument <code>k</code>.
</p>
<p>The representation is only determined up to location (<code>cmdscale</code>
takes the column means of the configuration to be at the origin),
rotations and reflections.  The configuration returned is given in
principal-component axes, so the reflection chosen may differ between
<span class="rlang"><b>R</b></span> platforms (see <code><a href="#topic+prcomp">prcomp</a></code>).
</p>
<p>When <code>add = TRUE</code>, a minimal additive constant <code class="reqn">c*</code> is
computed such that the dissimilarities <code class="reqn">d_{ij} + c*</code> are Euclidean and hence can be represented in <code>n - 1</code>
dimensions.  Whereas S (Becker <abbr>et al.</abbr>, 1988) computes this
constant using an approximation suggested by Torgerson, <span class="rlang"><b>R</b></span> uses the
analytical solution of Cailliez (1983), see also Cox and Cox (2001).
Note that because of numerical errors the computed eigenvalues need
not all be non-negative, and even theoretically the representation
could be in fewer than <code>n - 1</code> dimensions.
</p>


<h3>Value</h3>

<p>If <code>.list</code> is false (as per default), a matrix with <code>k</code>
columns whose rows give the coordinates of the points chosen to
represent the dissimilarities.
</p>
<p>Otherwise, a <code><a href="base.html#topic+list">list</a></code> containing the following components.
</p>
<table>
<tr><td><code>points</code></td>
<td>
<p>a matrix with up to <code>k</code> columns whose rows give the
coordinates of the points chosen to represent the dissimilarities.</p>
</td></tr>
<tr><td><code>eig</code></td>
<td>
<p>the <code class="reqn">n</code> eigenvalues computed during the scaling process if
<code>eig</code> is true.  <strong>NB</strong>: versions of <span class="rlang"><b>R</b></span> before 2.12.1
returned only <code>k</code> but were documented to return <code class="reqn">n - 1</code>.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>the doubly centered distance matrix if <code>x.ret</code> is true.</p>
</td></tr>
<tr><td><code>ac</code></td>
<td>
<p>the additive constant <code class="reqn">c*</code>, <code>0</code> if <code>add = FALSE</code>.</p>
</td></tr>
<tr><td><code>GOF</code></td>
<td>
<p>a numeric vector of length 2, equal to say
<code class="reqn">(g_1,g_2)</code>, where
<code class="reqn">g_i = (\sum_{j=1}^k \lambda_j)/ (\sum_{j=1}^n T_i(\lambda_j))</code>,
where <code class="reqn">\lambda_j</code> are the eigenvalues (sorted in
decreasing order),
<code class="reqn">T_1(v) = \left| v \right|</code>, and
<code class="reqn">T_2(v) = max( v, 0 )</code>.
</p>
</td></tr>
</table>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988).
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Cailliez, F. (1983).
The analytical solution of the additive constant problem.
<em>Psychometrika</em>, <b>48</b>, 343&ndash;349.
<a href="https://doi.org/10.1007/BF02294026">doi:10.1007/BF02294026</a>.
</p>
<p>Cox, T. F. and Cox, M. A. A. (2001).
<em>Multidimensional Scaling</em>.  Second edition.
Chapman and Hall.
</p>
<p>Gower, J. C. (1966).
Some distance properties of latent root and vector
methods used in multivariate analysis.
<em>Biometrika</em>, <b>53</b>, 325&ndash;328.
<a href="https://doi.org/10.2307/2333639">doi:10.2307/2333639</a>.
</p>
<p>Krzanowski, W. J. and Marriott, F. H. C. (1994).
<em>Multivariate Analysis. Part I. Distributions, Ordination and
Inference.</em>
London: Edward Arnold.
(Especially pp. 108&ndash;111.)
</p>
<p>Mardia, K.V. (1978).
Some properties of classical multidimensional scaling.
<em>Communications on Statistics &ndash; Theory and Methods</em>, <b>A7</b>,
1233&ndash;41.
<a href="https://doi.org/10.1080/03610927808827707">doi:10.1080/03610927808827707</a>
</p>
<p>Mardia, K. V., Kent, J. T. and Bibby, J. M. (1979).
Chapter 14 of <em>Multivariate Analysis</em>, London: Academic Press.
</p>
<p>Seber, G. A. F. (1984).
<em>Multivariate Observations</em>.
New York: Wiley.
</p>
<p>Torgerson, W. S. (1958).
<em>Theory and Methods of Scaling</em>.
New York: Wiley.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dist">dist</a></code>.
</p>
<p><code><a href="MASS.html#topic+isoMDS">isoMDS</a></code> and <code><a href="MASS.html#topic+sammon">sammon</a></code>
in package <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a> provide alternative methods of
multidimensional scaling.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

loc &lt;- cmdscale(eurodist)
x &lt;- loc[, 1]
y &lt;- -loc[, 2] # reflect so North is at the top
## note asp = 1, to ensure Euclidean distances are represented correctly
plot(x, y, type = "n", xlab = "", ylab = "", asp = 1, axes = FALSE,
     main = "cmdscale(eurodist)")
text(x, y, rownames(loc), cex = 0.6)
</code></pre>

<hr>
<h2 id='coef'>Extract Model Coefficients</h2><span id='topic+coef'></span><span id='topic+coefficients'></span><span id='topic+coef.default'></span><span id='topic+coef.aov'></span>

<h3>Description</h3>

<p><code>coef</code> is a generic function which extracts model coefficients
from objects returned by modeling functions.  <code>coefficients</code> is
an <em>alias</em> for it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coef(object, ...)
coefficients(object, ...)
## Default S3 method:
coef(object, complete = TRUE, ...)
## S3 method for class 'aov'
coef(object, complete = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef_+3A_object">object</code></td>
<td>
<p>an object for which the extraction of model coefficients is
meaningful.</p>
</td></tr>
<tr><td><code id="coef_+3A_complete">complete</code></td>
<td>
<p>for the default (used for <code>lm</code>, etc) and
<code>aov</code> methods:
logical indicating if the full coefficient vector should be returned
also in case of an over-determined system where some coefficients
will be set to <code><a href="base.html#topic+NA">NA</a></code>, see also <code><a href="#topic+alias">alias</a></code>.  Note
that the default <em>differs</em> for <code><a href="#topic+lm">lm</a>()</code> and
<code><a href="#topic+aov">aov</a>()</code> results.</p>
</td></tr>
<tr><td><code id="coef_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>All object classes which are returned by model fitting functions
should provide a <code>coef</code> method or use the default one.
(Note that the method is for <code>coef</code> and not <code>coefficients</code>.)
</p>
<p>The <code>"aov"</code> method does not report aliased coefficients (see
<code><a href="#topic+alias">alias</a></code>) by default where <code>complete = FALSE</code>.
</p>
<p>The <code>complete</code> argument also exists for compatibility with
<code><a href="#topic+vcov">vcov</a></code> methods, and <code>coef</code> and <code>aov</code> methods for
other classes should typically also keep the <code>complete = *</code>
behavior in sync.  By that, with <code>p &lt;- length(coef(obj, complete = TF))</code>,
<code>dim(vcov(obj, complete = TF)) == c(p,p)</code> will be fulfilled for both
<code>complete</code> settings and the default.
</p>


<h3>Value</h3>

<p>Coefficients extracted from the model object <code>object</code>.
</p>
<p>For standard model fitting classes this will be a named numeric vector.
For <code>"maov"</code> objects (produced by <code><a href="#topic+aov">aov</a></code>) it will be a matrix.
</p>


<h3>References</h3>

<p>Chambers, J. M. and Hastie, T. J. (1992)
<em>Statistical Models in S</em>.
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fitted.values">fitted.values</a></code> and <code><a href="#topic+residuals">residuals</a></code> for related methods;
<code><a href="#topic+glm">glm</a></code>, <code><a href="#topic+lm">lm</a></code> for model fitting.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 1:5; coef(lm(c(1:3, 7, 6) ~ x))
</code></pre>

<hr>
<h2 id='complete.cases'>Find Complete Cases</h2><span id='topic+complete.cases'></span>

<h3>Description</h3>

<p>Return a logical vector indicating which cases are complete, i.e.,
have no missing values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>complete.cases(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="complete.cases_+3A_...">...</code></td>
<td>
<p>a sequence of vectors, matrices and data frames.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A logical vector specifying which observations/rows have no missing
values across the entire sequence.
</p>


<h3>Note</h3>

<p>A current limitation of this function is that it uses low level
functions to determine lengths and missingness, ignoring the 
class.  This will lead to spurious errors when some columns
have classes with <code><a href="base.html#topic+length">length</a></code> or <code><a href="Matrix.html#topic+is.na">is.na</a></code>
methods, for example <code>"<a href="base.html#topic+POSIXlt">POSIXlt</a>"</code>, as described
in <a href="https://bugs.R-project.org/show_bug.cgi?id=16648">PR#16648</a>.
</p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+is.na">is.na</a></code>,
<code><a href="#topic+na.omit">na.omit</a></code>,
<code><a href="#topic+na.fail">na.fail</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- airquality[, -1] # x is a regression design matrix
y &lt;- airquality[,  1] # y is the corresponding response

stopifnot(complete.cases(y) != is.na(y))
ok &lt;- complete.cases(x, y)
sum(!ok) # how many are not "ok" ?
x &lt;- x[ok,]
y &lt;- y[ok]
</code></pre>

<hr>
<h2 id='confint'>Confidence Intervals for Model Parameters</h2><span id='topic+confint'></span><span id='topic+confint.default'></span><span id='topic+confint.lm'></span><span id='topic+confint.glm'></span><span id='topic+confint.nls'></span>

<h3>Description</h3>

<p>Computes confidence intervals for one or more parameters in a fitted
model.  There is a default and a method for objects inheriting from class
<code>"<a href="#topic+lm">lm</a>"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confint(object, parm, level = 0.95, ...)
## Default S3 method:
confint(object, parm, level = 0.95, ...)
## S3 method for class 'lm'
confint(object, parm, level = 0.95, ...)
## S3 method for class 'glm'
confint(object, parm, level = 0.95, trace = FALSE, test=c("LRT", "Rao"), ...)
## S3 method for class 'nls'
confint(object, parm, level = 0.95, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confint_+3A_object">object</code></td>
<td>
<p>a fitted model object.</p>
</td></tr>
<tr><td><code id="confint_+3A_parm">parm</code></td>
<td>
<p>a specification of which parameters are to be given
confidence intervals, either a vector of numbers or a vector of
names.  If missing, all parameters are considered.</p>
</td></tr>
<tr><td><code id="confint_+3A_level">level</code></td>
<td>
<p>the confidence level required.</p>
</td></tr>
<tr><td><code id="confint_+3A_trace">trace</code></td>
<td>
<p> logical.  Should profiling be traced?</p>
</td></tr>
<tr><td><code id="confint_+3A_test">test</code></td>
<td>
<p>use Likelihood Ratio or Rao Score test in profiling.</p>
</td></tr>
<tr><td><code id="confint_+3A_...">...</code></td>
<td>
<p>additional argument(s) for methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>confint</code> is a generic function.  The default method assumes
normality, and needs suitable <code><a href="#topic+coef">coef</a></code> and
<code><a href="#topic+vcov">vcov</a></code> methods to be available.  The default method can be
called directly for comparison with other methods.
</p>
<p>For objects of class <code>"lm"</code> the direct formulae based on <code class="reqn">t</code>
values are used.
</p>
<p>Methods for classes <code>"glm"</code>
and <code>"nls"</code> call the appropriate profile method,
then find the confidence intervals by interpolation in the profile
traces.  If the profile object is already available it can be used
as the main argument rather than the fitted model object itself.
</p>


<h3>Value</h3>

<p>A matrix (or vector) with columns giving lower and upper confidence
limits for each parameter. These will be labelled as (1-level)/2 and
1 - (1-level)/2 in % (by default 2.5% and 97.5%).
</p>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p>Original versions: <code><a href="MASS.html#topic+confint">confint.glm</a></code> and
<code><a href="MASS.html#topic+confint">confint.nls</a></code> in package <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fit &lt;- lm(100/mpg ~ disp + hp + wt + am, data = mtcars)
confint(fit)
confint(fit, "wt")

## from example(glm)
counts &lt;- c(18,17,15,20,10,20,25,13,12)
outcome &lt;- gl(3, 1, 9); treatment &lt;- gl(3, 3)
glm.D93 &lt;- glm(counts ~ outcome + treatment, family = poisson())
confint(glm.D93) 
confint.default(glm.D93)  # based on asymptotic normality</code></pre>

<hr>
<h2 id='constrOptim'>Linearly Constrained Optimization</h2><span id='topic+constrOptim'></span>

<h3>Description</h3>

<p>Minimise a function subject to linear inequality constraints using an
adaptive barrier algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>constrOptim(theta, f, grad, ui, ci, mu = 1e-04, control = list(),
            method = if(is.null(grad)) "Nelder-Mead" else "BFGS",
            outer.iterations = 100, outer.eps = 1e-05, ...,
            hessian = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="constrOptim_+3A_theta">theta</code></td>
<td>
<p>numeric (vector) starting value (of length <code class="reqn">p</code>): must
be in the feasible region.</p>
</td></tr>
<tr><td><code id="constrOptim_+3A_f">f</code></td>
<td>
<p>function to minimise (see below).</p>
</td></tr>
<tr><td><code id="constrOptim_+3A_grad">grad</code></td>
<td>
<p>gradient of <code>f</code> (a <code><a href="base.html#topic+function">function</a></code> as well),
or <code>NULL</code> (see below).</p>
</td></tr>
<tr><td><code id="constrOptim_+3A_ui">ui</code></td>
<td>
<p>constraint matrix (<code class="reqn">k \times p</code>), see below.</p>
</td></tr>
<tr><td><code id="constrOptim_+3A_ci">ci</code></td>
<td>
<p>constraint vector of length <code class="reqn">k</code> (see below).</p>
</td></tr>
<tr><td><code id="constrOptim_+3A_mu">mu</code></td>
<td>
<p>(Small) tuning parameter.</p>
</td></tr>
<tr><td><code id="constrOptim_+3A_control">control</code>, <code id="constrOptim_+3A_method">method</code>, <code id="constrOptim_+3A_hessian">hessian</code></td>
<td>
<p>passed to <code><a href="#topic+optim">optim</a></code>.</p>
</td></tr>
<tr><td><code id="constrOptim_+3A_outer.iterations">outer.iterations</code></td>
<td>
<p>iterations of the barrier algorithm.</p>
</td></tr>
<tr><td><code id="constrOptim_+3A_outer.eps">outer.eps</code></td>
<td>
<p>non-negative number; the relative convergence
tolerance of the barrier algorithm.</p>
</td></tr>
<tr><td><code id="constrOptim_+3A_...">...</code></td>
<td>
<p>Other named arguments to be passed to <code>f</code> and <code>grad</code>:
needs to be passed through <code><a href="#topic+optim">optim</a></code> so should not match its
argument names.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The feasible region is defined by <code>ui %*% theta - ci &gt;= 0</code>. The
starting value must be in the interior of the feasible region, but the
minimum may be on the boundary.
</p>
<p>A logarithmic barrier is added to enforce the constraints and then
<code><a href="#topic+optim">optim</a></code> is called. The barrier function is chosen so that
the objective function should decrease at each outer iteration. Minima
in the interior of the feasible region are typically found quite
quickly, but a substantial number of outer iterations may be needed
for a minimum on the boundary.
</p>
<p>The tuning parameter <code>mu</code> multiplies the barrier term. Its precise
value is often relatively unimportant. As <code>mu</code> increases the
augmented objective function becomes closer to the original objective
function but also less smooth near the boundary of the feasible
region.
</p>
<p>Any <code>optim</code> method that permits infinite values for the
objective function may be used (currently all but &quot;L-BFGS-B&quot;).
</p>
<p>The objective function <code>f</code> takes as first argument the vector
of parameters over which minimisation is to take place.  It should
return a scalar result. Optional arguments <code>...</code> will be
passed to <code>optim</code> and then (if not used by <code>optim</code>) to
<code>f</code>. As with <code>optim</code>, the default is to minimise, but
maximisation can be performed by setting <code>control$fnscale</code> to a
negative value.
</p>
<p>The gradient function <code>grad</code> must be supplied except with
<code>method = "Nelder-Mead"</code>.  It should take arguments matching
those of <code>f</code> and return a vector containing the gradient.
</p>


<h3>Value</h3>

<p>As for <code><a href="#topic+optim">optim</a></code>, but with two extra components:
<code>barrier.value</code> giving the value of the barrier function at the
optimum and <code>outer.iterations</code> gives the
number of outer iterations (calls to <code>optim</code>).
The <code>counts</code> component contains the <em>sum</em> of all
<code><a href="#topic+optim">optim</a>()$counts</code>.
</p>


<h3>References</h3>

<p>K. Lange <em>Numerical Analysis for Statisticians.</em> Springer
2001, p185ff
</p>


<h3>See Also</h3>

<p><code><a href="#topic+optim">optim</a></code>, especially <code>method = "L-BFGS-B"</code> which
does box-constrained optimisation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## from optim
fr &lt;- function(x) {   ## Rosenbrock Banana function
    x1 &lt;- x[1]
    x2 &lt;- x[2]
    100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}
grr &lt;- function(x) { ## Gradient of 'fr'
    x1 &lt;- x[1]
    x2 &lt;- x[2]
    c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
       200 *      (x2 - x1 * x1))
}

optim(c(-1.2,1), fr, grr)
#Box-constraint, optimum on the boundary
constrOptim(c(-1.2,0.9), fr, grr, ui = rbind(c(-1,0), c(0,-1)), ci = c(-1,-1))
#  x &lt;= 0.9,  y - x &gt; 0.1
constrOptim(c(.5,0), fr, grr, ui = rbind(c(-1,0), c(1,-1)), ci = c(-0.9,0.1))


## Solves linear and quadratic programming problems
## but needs a feasible starting value
#
# from example(solve.QP) in 'quadprog'
# no derivative
fQP &lt;- function(b) {-sum(c(0,5,0)*b)+0.5*sum(b*b)}
Amat       &lt;- matrix(c(-4,-3,0,2,1,0,0,-2,1), 3, 3)
bvec       &lt;- c(-8, 2, 0)
constrOptim(c(2,-1,-1), fQP, NULL, ui = t(Amat), ci = bvec)
# derivative
gQP &lt;- function(b) {-c(0, 5, 0) + b}
constrOptim(c(2,-1,-1), fQP, gQP, ui = t(Amat), ci = bvec)

## Now with maximisation instead of minimisation
hQP &lt;- function(b) {sum(c(0,5,0)*b)-0.5*sum(b*b)}
constrOptim(c(2,-1,-1), hQP, NULL, ui = t(Amat), ci = bvec,
            control = list(fnscale = -1))
</code></pre>

<hr>
<h2 id='contrast'>(Possibly Sparse) Contrast Matrices</h2><span id='topic+contr.helmert'></span><span id='topic+contr.poly'></span><span id='topic+contr.sum'></span><span id='topic+contr.treatment'></span><span id='topic+contr.SAS'></span>

<h3>Description</h3>

<p>Return a matrix of contrasts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>contr.helmert(n, contrasts = TRUE, sparse = FALSE)
contr.poly(n, scores = 1:n, contrasts = TRUE, sparse = FALSE)
contr.sum(n, contrasts = TRUE, sparse = FALSE)
contr.treatment(n, base = 1, contrasts = TRUE, sparse = FALSE)
contr.SAS(n, contrasts = TRUE, sparse = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="contrast_+3A_n">n</code></td>
<td>
<p>a vector of levels for a factor, or the number of levels.</p>
</td></tr>
<tr><td><code id="contrast_+3A_contrasts">contrasts</code></td>
<td>
<p>a logical indicating whether contrasts should be
computed.</p>
</td></tr>
<tr><td><code id="contrast_+3A_sparse">sparse</code></td>
<td>
<p>logical indicating if the result should be sparse
(of class <code><a href="Matrix.html#topic+dgCMatrix-class">dgCMatrix</a></code>), using
package <a href="https://CRAN.R-project.org/package=Matrix"><span class="pkg">Matrix</span></a>.</p>
</td></tr>
<tr><td><code id="contrast_+3A_scores">scores</code></td>
<td>
<p>the set of values over which orthogonal polynomials are
to be computed.</p>
</td></tr>
<tr><td><code id="contrast_+3A_base">base</code></td>
<td>
<p>an integer specifying which group is considered the
baseline group. Ignored if <code>contrasts</code> is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are used for creating contrast matrices for use in
fitting analysis of variance and regression models.  The columns of
the resulting matrices contain contrasts which can be used for coding
a factor with <code>n</code> levels.  The returned value contains the
computed contrasts.  If the argument <code>contrasts</code> is <code>FALSE</code>
a square indicator matrix (the dummy coding) is returned <b>except</b>
for <code>contr.poly</code> (which includes the 0-degree, i.e. constant,
polynomial when <code>contrasts = FALSE</code>).
</p>
<p><code>contr.helmert</code> returns Helmert contrasts, which contrast the
second level with the first, the third with the average of the first
two, and so on.  <code>contr.poly</code> returns contrasts based on
orthogonal polynomials. <code>contr.sum</code> uses &lsquo;sum to zero
contrasts&rsquo;.
</p>
<p><code>contr.treatment</code> contrasts each level with the baseline level
(specified by <code>base</code>): the baseline level is omitted.  Note that
this does not produce &lsquo;contrasts&rsquo; as defined in the standard
theory for linear models as they are not orthogonal to the intercept.
</p>
<p><code>contr.SAS</code> is a wrapper for <code>contr.treatment</code> that sets
the base level to be the last level of the factor.  The coefficients
produced when using these contrasts should be equivalent to those
produced by many (but not all) SAS procedures.
</p>
<p>For consistency, <code>sparse</code> is an argument to all these contrast
functions, however <code>sparse = TRUE</code> for <code>contr.poly</code>
is typically pointless and is rarely useful for
<code>contr.helmert</code>.
</p>


<h3>Value</h3>

<p>A matrix with <code>n</code> rows and <code>k</code> columns, with <code>k=n-1</code> if
<code>contrasts</code> is <code>TRUE</code> and <code>k=n</code> if <code>contrasts</code> is
<code>FALSE</code>.
</p>


<h3>References</h3>

<p>Chambers, J. M. and Hastie, T. J. (1992)
<em>Statistical models.</em>
Chapter 2 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+contrasts">contrasts</a></code>,
<code><a href="#topic+C">C</a></code>,
and
<code><a href="#topic+aov">aov</a></code>,
<code><a href="#topic+glm">glm</a></code>,
<code><a href="#topic+lm">lm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>(cH &lt;- contr.helmert(4))
apply(cH, 2, sum) # column sums are 0
crossprod(cH) # diagonal -- columns are orthogonal
contr.helmert(4, contrasts = FALSE) # just the 4 x 4 identity matrix

(cT &lt;- contr.treatment(5))
all(crossprod(cT) == diag(4)) # TRUE: even orthonormal

(cT. &lt;- contr.SAS(5))
all(crossprod(cT.) == diag(4)) # TRUE

zapsmall(cP &lt;- contr.poly(3)) # Linear and Quadratic
zapsmall(crossprod(cP), digits = 15) # orthonormal up to fuzz
</code></pre>

<hr>
<h2 id='contrasts'>Get and Set Contrast Matrices</h2><span id='topic+contrasts'></span><span id='topic+contrasts+3C-'></span>

<h3>Description</h3>

<p>Set and view the contrasts associated with a factor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>contrasts(x, contrasts = TRUE, sparse = FALSE)
contrasts(x, how.many = NULL) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="contrasts_+3A_x">x</code></td>
<td>
<p>a factor or a logical variable.</p>
</td></tr>
<tr><td><code id="contrasts_+3A_contrasts">contrasts</code></td>
<td>
<p>logical.  See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="contrasts_+3A_sparse">sparse</code></td>
<td>
<p>logical indicating if the result should be sparse
(of class <code><a href="Matrix.html#topic+dgCMatrix-class">dgCMatrix</a></code>), using
package <a href="https://CRAN.R-project.org/package=Matrix"><span class="pkg">Matrix</span></a>.</p>
</td></tr>
<tr><td><code id="contrasts_+3A_how.many">how.many</code></td>
<td>
<p>integer number indicating how many contrasts should be
made.  Defaults to one
less than the number of levels of <code>x</code>.  This need not be the
same as the number of columns of <code>value</code>.</p>
</td></tr>
<tr><td><code id="contrasts_+3A_value">value</code></td>
<td>
<p>either a numeric matrix (or a sparse or dense matrix of a
class extending <code><a href="Matrix.html#topic+dMatrix-class">dMatrix</a></code> from
package <a href="https://CRAN.R-project.org/package=Matrix"><span class="pkg">Matrix</span></a>)  whose columns give coefficients for
contrasts in the levels of <code>x</code>, or (the quoted name of) a
function which computes such matrices.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If contrasts are not set for a factor the default functions from
<code><a href="base.html#topic+options">options</a>("contrasts")</code> are used.
</p>
<p>A logical vector <code>x</code> is converted into a two-level factor with
levels <code>c(FALSE, TRUE)</code> (regardless of which levels occur in the
variable).
</p>
<p>The argument <code>contrasts</code> is ignored if <code>x</code> has a matrix
<code>contrasts</code> attribute set.  Otherwise if <code>contrasts = TRUE</code>
it is passed to a contrasts function such as
<code><a href="#topic+contr.treatment">contr.treatment</a></code> and if <code>contrasts = FALSE</code>
an identity matrix is returned.  Suitable functions have a first
argument which is the character vector of levels, a named argument
<code>contrasts</code> (always called with <code>contrasts = TRUE</code>) and
optionally a logical argument <code>sparse</code>.
</p>
<p>If <code>value</code> supplies more than <code>how.many</code> contrasts, the
first <code>how.many</code> are used.  If too few are supplied, a suitable
contrast matrix is created by extending <code>value</code> after ensuring
its columns are contrasts (orthogonal to the constant term) and not
collinear.
</p>


<h3>References</h3>

<p>Chambers, J. M. and Hastie, T. J. (1992)
<em>Statistical models.</em>
Chapter 2 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+C">C</a></code>,
<code><a href="#topic+contr.helmert">contr.helmert</a></code>,
<code><a href="#topic+contr.poly">contr.poly</a></code>,
<code><a href="#topic+contr.sum">contr.sum</a></code>,
<code><a href="#topic+contr.treatment">contr.treatment</a></code>;
<code><a href="#topic+glm">glm</a></code>,
<code><a href="#topic+aov">aov</a></code>,
<code><a href="#topic+lm">lm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>utils::example(factor)
fff &lt;- ff[, drop = TRUE]  # reduce to 5 levels.
contrasts(fff) # treatment contrasts by default
contrasts(C(fff, sum))
contrasts(fff, contrasts = FALSE) # the 5x5 identity matrix

contrasts(fff) &lt;- contr.sum(5); contrasts(fff)  # set sum contrasts
contrasts(fff, 2) &lt;- contr.sum(5); contrasts(fff)  # set 2 contrasts
# supply 2 contrasts, compute 2 more to make full set of 4.
contrasts(fff) &lt;- contr.sum(5)[, 1:2]; contrasts(fff)

## using sparse contrasts: % useful, once model.matrix() works with these :
ffs &lt;- fff
contrasts(ffs) &lt;- contr.sum(5, sparse = TRUE)[, 1:2]; contrasts(ffs)
stopifnot(all.equal(ffs, fff))
contrasts(ffs) &lt;- contr.sum(5, sparse = TRUE); contrasts(ffs)
</code></pre>

<hr>
<h2 id='convolve'>Convolution of Sequences via FFT</h2><span id='topic+convolve'></span>

<h3>Description</h3>

<p>Use the Fast Fourier Transform to compute the several kinds of
convolutions of two sequences.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convolve(x, y, conj = TRUE, type = c("circular", "open", "filter"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convolve_+3A_x">x</code>, <code id="convolve_+3A_y">y</code></td>
<td>
<p>numeric sequences <em>of the same length</em> to be
convolved.</p>
</td></tr>
<tr><td><code id="convolve_+3A_conj">conj</code></td>
<td>
<p>logical; if <code>TRUE</code>, take the complex <em>conjugate</em>
before back-transforming (default, and used for usual convolution).</p>
</td></tr>
<tr><td><code id="convolve_+3A_type">type</code></td>
<td>
<p>character; partially matched to <code>"circular"</code>, <code>"open"</code>,
<code>"filter"</code>.  For <code>"circular"</code>, the
two sequences are treated as <em>circular</em>, i.e., periodic.
</p>
<p>For <code>"open"</code> and <code>"filter"</code>, the sequences are padded with
<code>0</code>s (from left and right) first; <code>"filter"</code> returns the
middle sub-vector of <code>"open"</code>, namely, the result of running a
weighted mean of <code>x</code> with weights <code>y</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Fast Fourier Transform, <code><a href="#topic+fft">fft</a></code>, is used for efficiency.
</p>
<p>The input sequences <code>x</code> and  <code>y</code> must have the same length if
<code>circular</code> is true.
</p>
<p>Note that the usual definition of convolution of two sequences
<code>x</code> and <code>y</code> is given by <code>convolve(x, rev(y), type = "o")</code>.
</p>


<h3>Value</h3>

<p>If <code>r &lt;- convolve(x, y, type = "open")</code>
and <code>n &lt;- length(x)</code>, <code>m &lt;- length(y)</code>, then
</p>
<p style="text-align: center;"><code class="reqn">r_k = \sum_{i} x_{k-m+i} y_{i}</code>
</p>

<p>where the sum is over all valid indices <code class="reqn">i</code>, for
<code class="reqn">k = 1, \dots, n+m-1</code>.
</p>
<p>If <code>type == "circular"</code>, <code class="reqn">n = m</code> is required, and the above is
true for <code class="reqn">i , k = 1,\dots,n</code> when
<code class="reqn">x_{j} := x_{n+j}</code> for <code class="reqn">j &lt; 1</code>.
</p>


<h3>References</h3>

<p>Brillinger, D. R. (1981)
<em>Time Series: Data Analysis and Theory</em>, Second Edition.
San Francisco: Holden-Day.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fft">fft</a></code>, <code><a href="#topic+nextn">nextn</a></code>, and particularly
<code><a href="#topic+filter">filter</a></code> (from the <span class="pkg">stats</span> package) which may be
more appropriate.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

x &lt;- c(0,0,0,100,0,0,0)
y &lt;- c(0,0,1, 2 ,1,0,0)/4
zapsmall(convolve(x, y))         #  *NOT* what you first thought.
zapsmall(convolve(x, y[3:5], type = "f")) # rather
x &lt;- rnorm(50)
y &lt;- rnorm(50)
# Circular convolution *has* this symmetry:
all.equal(convolve(x, y, conj = FALSE), rev(convolve(rev(y),x)))

n &lt;- length(x &lt;- -20:24)
y &lt;- (x-10)^2/1000 + rnorm(x)/8

Han &lt;- function(y) # Hanning
       convolve(y, c(1,2,1)/4, type = "filter")

plot(x, y, main = "Using  convolve(.) for Hanning filters")
lines(x[-c(1  , n)      ], Han(y), col = "red")
lines(x[-c(1:2, (n-1):n)], Han(Han(y)), lwd = 2, col = "dark blue")
</code></pre>

<hr>
<h2 id='cophenetic'>Cophenetic Distances for a Hierarchical Clustering</h2><span id='topic+cophenetic'></span><span id='topic+cophenetic.default'></span><span id='topic+cophenetic.dendrogram'></span>

<h3>Description</h3>

<p>Computes the cophenetic distances for a hierarchical clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cophenetic(x)
## Default S3 method:
cophenetic(x)
## S3 method for class 'dendrogram'
cophenetic(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cophenetic_+3A_x">x</code></td>
<td>
<p>an R object representing a hierarchical clustering.
For the default method, an object of class <code>"<a href="#topic+hclust">hclust</a>"</code> or
with a method for <code><a href="#topic+as.hclust">as.hclust</a>()</code> such as
<code>"<a href="cluster.html#topic+agnes">agnes</a>"</code> in package <a href="https://CRAN.R-project.org/package=cluster"><span class="pkg">cluster</span></a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cophenetic distance between two observations that have been
clustered is defined to be the intergroup dissimilarity at which the
two observations are first combined into a single cluster.
Note that this distance has many ties and restrictions.
</p>
<p>It can be argued that a dendrogram is an appropriate summary of some
data if the correlation between the original distances and the
cophenetic distances is high.  Otherwise, it should simply be viewed as
the description of the output of the clustering algorithm.
</p>
<p><code>cophenetic</code> is a generic function.  Support for classes which
represent hierarchical clusterings (total indexed hierarchies) can be
added by providing an <code><a href="#topic+as.hclust">as.hclust</a>()</code> or, more directly, a
<code>cophenetic()</code> method for such a class.
</p>
<p>The method for objects of class <code>"<a href="#topic+dendrogram">dendrogram</a>"</code> requires
that all leaves of the dendrogram object have non-null labels.
</p>


<h3>Value</h3>

<p>An object of class <code>"dist"</code>.
</p>


<h3>Author(s)</h3>

<p>Robert Gentleman</p>


<h3>References</h3>

<p>Sneath, P.H.A. and Sokal, R.R. (1973)
<em>Numerical Taxonomy: The Principles and Practice of Numerical
Classification</em>, p. 278 ff;
Freeman, San Francisco.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dist">dist</a></code>,
<code><a href="#topic+hclust">hclust</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

d1 &lt;- dist(USArrests)
hc &lt;- hclust(d1, "ave")
d2 &lt;- cophenetic(hc)
cor(d1, d2) # 0.7659

## Example from Sneath &amp; Sokal, Fig. 5-29, p.279
d0 &lt;- c(1,3.8,4.4,5.1, 4,4.2,5, 2.6,5.3, 5.4)
attributes(d0) &lt;- list(Size = 5, diag = TRUE)
class(d0) &lt;- "dist"
names(d0) &lt;- letters[1:5]
d0
utils::str(upgma &lt;- hclust(d0, method = "average"))
plot(upgma, hang = -1)
#
(d.coph &lt;- cophenetic(upgma))
cor(d0, d.coph) # 0.9911
</code></pre>

<hr>
<h2 id='cor'>Correlation, Variance and Covariance (Matrices)</h2><span id='topic+var'></span><span id='topic+cov'></span><span id='topic+cor'></span><span id='topic+cov2cor'></span>

<h3>Description</h3>

<p><code>var</code>, <code>cov</code> and <code>cor</code> compute the variance of <code>x</code>
and the covariance or correlation of <code>x</code> and <code>y</code> if these
are vectors.   If <code>x</code> and <code>y</code> are matrices then the
covariances (or correlations) between the columns of <code>x</code> and the
columns of <code>y</code> are computed.
</p>
<p><code>cov2cor</code> scales a covariance matrix into the corresponding
correlation matrix <em>efficiently</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var(x, y = NULL, na.rm = FALSE, use)

cov(x, y = NULL, use = "everything",
    method = c("pearson", "kendall", "spearman"))

cor(x, y = NULL, use = "everything",
    method = c("pearson", "kendall", "spearman"))

cov2cor(V)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cor_+3A_x">x</code></td>
<td>
<p>a numeric vector, matrix or data frame.</p>
</td></tr>
<tr><td><code id="cor_+3A_y">y</code></td>
<td>
<p><code>NULL</code> (default) or a vector, matrix or data frame with
compatible dimensions to <code>x</code>.   The default is equivalent to
<code>y = x</code> (but more efficient).</p>
</td></tr>
<tr><td><code id="cor_+3A_na.rm">na.rm</code></td>
<td>
<p>logical. Should missing values be removed?</p>
</td></tr>
<tr><td><code id="cor_+3A_use">use</code></td>
<td>
<p>an optional character string giving a
method for computing covariances in the presence
of missing values.  This must be (an abbreviation of) one of the strings
<code>"everything"</code>, <code>"all.obs"</code>, <code>"complete.obs"</code>,
<code>"na.or.complete"</code>, or <code>"pairwise.complete.obs"</code>.</p>
</td></tr>
<tr><td><code id="cor_+3A_method">method</code></td>
<td>
<p>a character string indicating which correlation
coefficient (or covariance) is to be computed.  One of
<code>"pearson"</code> (default), <code>"kendall"</code>, or <code>"spearman"</code>:
can be abbreviated.</p>
</td></tr>
<tr><td><code id="cor_+3A_v">V</code></td>
<td>
<p>symmetric numeric matrix, usually positive definite such as a
covariance matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>cov</code> and <code>cor</code> one must <em>either</em> give a matrix or
data frame for <code>x</code> <em>or</em> give both <code>x</code> and <code>y</code>.
</p>
<p>The inputs must be numeric (as determined by <code><a href="base.html#topic+is.numeric">is.numeric</a></code>:
logical values are also allowed for historical compatibility): the
<code>"kendall"</code> and <code>"spearman"</code> methods make sense for ordered
inputs but <code><a href="base.html#topic+xtfrm">xtfrm</a></code> can be used to find a suitable prior
transformation to numbers.
</p>
<p><code>var</code> is just another interface to <code>cov</code>, where
<code>na.rm</code> is used to determine the default for <code>use</code> when that
is unspecified.  If <code>na.rm</code> is <code>TRUE</code> then the complete
observations (rows) are used (<code>use = "na.or.complete"</code>) to
compute the variance.  Otherwise, by default <code>use = "everything"</code>.
</p>
<p>If <code>use</code> is <code>"everything"</code>, <code><a href="base.html#topic+NA">NA</a></code>s will
propagate conceptually, i.e., a resulting value will be <code>NA</code>
whenever one of its contributing observations is <code>NA</code>.<br />
If <code>use</code> is <code>"all.obs"</code>, then the presence of missing
observations will produce an error.  If <code>use</code> is
<code>"complete.obs"</code> then missing values are handled by casewise
deletion (and if there are no complete cases, that gives an error).
<br />
<code>"na.or.complete"</code> is the same unless there are no complete
cases, that gives <code>NA</code>.
Finally, if <code>use</code> has the value <code>"pairwise.complete.obs"</code>
then the correlation or covariance between each pair of variables is
computed using all complete pairs of observations on those variables.
This can result in covariance or correlation matrices which are not positive
semi-definite, as well as <code>NA</code> entries if there are no complete
pairs for that pair of variables.   For <code>cov</code> and <code>var</code>,
<code>"pairwise.complete.obs"</code> only works with the <code>"pearson"</code>
method.
Note that (the equivalent of) <code>var(double(0), use = *)</code> gives
<code>NA</code> for <code>use = "everything"</code> and <code>"na.or.complete"</code>,
and gives an error in the other cases.
</p>
<p>The denominator <code class="reqn">n - 1</code> is used which gives an unbiased estimator
of the (co)variance for i.i.d. observations.  These functions return
<code><a href="base.html#topic+NA">NA</a></code> when there is only one observation.
</p>
<p>For <code>cor()</code>, if <code>method</code> is <code>"kendall"</code> or
<code>"spearman"</code>, Kendall's <code class="reqn">\tau</code> or Spearman's
<code class="reqn">\rho</code> statistic is used to estimate a rank-based measure of
association.  These are more robust and have been recommended if the
data do not necessarily come from a bivariate normal distribution.<br />
For <code>cov()</code>, a non-Pearson method is unusual but available for
the sake of completeness.  Note that <code>"spearman"</code> basically
computes <code>cor(R(x), R(y))</code> (or <code>cov(., .)</code>) where <code>R(u)
  := rank(u, na.last = "keep")</code>. In the case of missing values, the
ranks are calculated depending on the value of <code>use</code>, either
based on complete observations, or based on pairwise completeness with
reranking for each pair.
</p>
<p>When there are ties, Kendall's <code class="reqn">\tau_b</code> is computed, as
proposed by Kendall (1945).
</p>
<p>Scaling a covariance matrix into a correlation one can be achieved in
many ways, mathematically most appealing by multiplication with a
diagonal matrix from left and right, or more efficiently by using
<code><a href="base.html#topic+sweep">sweep</a>(.., FUN = "/")</code> twice.  The <code>cov2cor</code> function
is even a bit more efficient, and provided mostly for didactical
reasons.
</p>


<h3>Value</h3>

<p>For <code>r &lt;- cor(*, use = "all.obs")</code>, it is now guaranteed that
<code>all(abs(r) &lt;= 1)</code>.
</p>


<h3>Note</h3>

<p>Some people have noted that the code for Kendall's tau is slow for
very large datasets (many more than 1000 cases).  It rarely makes
sense to do such a computation, but see function
<code><a href="pcaPP.html#topic+cor.fk">cor.fk</a></code> in package <a href="https://CRAN.R-project.org/package=pcaPP"><span class="pkg">pcaPP</span></a>.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988).
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Kendall, M. G. (1938).
A new measure of rank correlation,
<em>Biometrika</em>, <b>30</b>, 81&ndash;93.
<a href="https://doi.org/10.1093/biomet/30.1-2.81">doi:10.1093/biomet/30.1-2.81</a>.
</p>
<p>Kendall, M. G. (1945).
The treatment of ties in rank problems.
<em>Biometrika</em>, <b>33</b> 239&ndash;251.
<a href="https://doi.org/10.1093/biomet/33.3.239">doi:10.1093/biomet/33.3.239</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cor.test">cor.test</a></code> for confidence intervals (and tests).
</p>
<p><code><a href="#topic+cov.wt">cov.wt</a></code> for <em>weighted</em> covariance computation.
</p>
<p><code><a href="#topic+sd">sd</a></code> for standard deviation (vectors).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>var(1:10)  # 9.166667

var(1:5, 1:5) # 2.5

## Two simple vectors
cor(1:10, 2:11) # == 1

## Correlation Matrix of Multivariate sample:
(Cl &lt;- cor(longley))
## Graphical Correlation Matrix:
symnum(Cl) # highly correlated

## Spearman's rho  and  Kendall's tau
symnum(clS &lt;- cor(longley, method = "spearman"))
symnum(clK &lt;- cor(longley, method = "kendall"))
## How much do they differ?
i &lt;- lower.tri(Cl)
cor(cbind(P = Cl[i], S = clS[i], K = clK[i]))


## cov2cor() scales a covariance matrix by its diagonal
##           to become the correlation matrix.
cov2cor # see the function definition {and learn ..}
stopifnot(all.equal(Cl, cov2cor(cov(longley))),
          all.equal(cor(longley, method = "kendall"),
            cov2cor(cov(longley, method = "kendall"))))

##--- Missing value treatment:

C1 &lt;- cov(swiss)
range(eigen(C1, only.values = TRUE)$values) # 6.19        1921

## swM := "swiss" with  3 "missing"s :
swM &lt;- swiss
colnames(swM) &lt;- abbreviate(colnames(swiss), minlength=6)
swM[1,2] &lt;- swM[7,3] &lt;- swM[25,5] &lt;- NA # create 3 "missing"

## Consider all 5 "use" cases :
(C. &lt;- cov(swM)) # use="everything"  quite a few NA's in cov.matrix
try(cov(swM, use = "all")) # Error: missing obs...
C2 &lt;- cov(swM, use = "complete")
stopifnot(identical(C2, cov(swM, use = "na.or.complete")))
range(eigen(C2, only.values = TRUE)$values) # 6.46   1930
C3 &lt;- cov(swM, use = "pairwise")
range(eigen(C3, only.values = TRUE)$values) # 6.19   1938

## Kendall's tau doesn't change much:
symnum(Rc &lt;- cor(swM, method = "kendall", use = "complete"))
symnum(Rp &lt;- cor(swM, method = "kendall", use = "pairwise"))
symnum(R. &lt;- cor(swiss, method = "kendall"))

## "pairwise" is closer componentwise,
summary(abs(c(1 - Rp/R.)))
summary(abs(c(1 - Rc/R.)))

## but "complete" is closer in Eigen space:
EV &lt;- function(m) eigen(m, only.values=TRUE)$values
summary(abs(1 - EV(Rp)/EV(R.)) / abs(1 - EV(Rc)/EV(R.)))
</code></pre>

<hr>
<h2 id='cor.test'>Test for Association/Correlation Between Paired Samples</h2><span id='topic+cor.test'></span><span id='topic+cor.test.default'></span><span id='topic+cor.test.formula'></span>

<h3>Description</h3>

<p>Test for association between paired samples, using one of
Pearson's product moment correlation coefficient,
Kendall's <code class="reqn">\tau</code> or Spearman's <code class="reqn">\rho</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cor.test(x, ...)

## Default S3 method:
cor.test(x, y,
         alternative = c("two.sided", "less", "greater"),
         method = c("pearson", "kendall", "spearman"),
         exact = NULL, conf.level = 0.95, continuity = FALSE, ...)

## S3 method for class 'formula'
cor.test(formula, data, subset, na.action, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cor.test_+3A_x">x</code>, <code id="cor.test_+3A_y">y</code></td>
<td>
<p>numeric vectors of data values.  <code>x</code> and <code>y</code>
must have the same length.</p>
</td></tr>
<tr><td><code id="cor.test_+3A_alternative">alternative</code></td>
<td>
<p>indicates the alternative hypothesis and must be
one of <code>"two.sided"</code>, <code>"greater"</code> or <code>"less"</code>.  You
can specify just the initial letter.  <code>"greater"</code> corresponds
to positive association, <code>"less"</code> to negative association.</p>
</td></tr>
<tr><td><code id="cor.test_+3A_method">method</code></td>
<td>
<p>a character string indicating which correlation
coefficient is to be  used for the test.  One of <code>"pearson"</code>,
<code>"kendall"</code>, or <code>"spearman"</code>, can be abbreviated.</p>
</td></tr>
<tr><td><code id="cor.test_+3A_exact">exact</code></td>
<td>
<p>a logical indicating whether an exact p-value should be
computed.  Used for Kendall's <code class="reqn">\tau</code> and
Spearman's <code class="reqn">\rho</code>.
See &lsquo;Details&rsquo; for the meaning of <code>NULL</code> (the default).</p>
</td></tr>
<tr><td><code id="cor.test_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level for the returned confidence
interval.  Currently only used for the Pearson product moment
correlation coefficient if there are at least 4 complete pairs of
observations.</p>
</td></tr>
<tr><td><code id="cor.test_+3A_continuity">continuity</code></td>
<td>
<p>logical: if true, a continuity correction is used
for Kendall's <code class="reqn">\tau</code> and Spearman's <code class="reqn">\rho</code> when
not computed exactly.</p>
</td></tr>
<tr><td><code id="cor.test_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>~ u + v</code>, where each of
<code>u</code> and <code>v</code> are numeric variables giving the data values
for one sample.  The samples must be of the same length.</p>
</td></tr>
<tr><td><code id="cor.test_+3A_data">data</code></td>
<td>
<p>an optional matrix or data frame (or similar: see
<code><a href="#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="cor.test_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="cor.test_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.  Defaults to
<code>getOption("na.action")</code>.</p>
</td></tr>
<tr><td><code id="cor.test_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The three methods each estimate the association between paired samples
and compute a test of the value being zero.  They use different
measures of association, all in the range <code class="reqn">[-1, 1]</code> with <code class="reqn">0</code>
indicating no association.  These are sometimes referred to as tests
of no <em>correlation</em>, but that term is often confined to the
default method.
</p>
<p>If <code>method</code> is <code>"pearson"</code>, the test statistic is based on
Pearson's product moment correlation coefficient <code>cor(x, y)</code> and
follows a t distribution with <code>length(x)-2</code> degrees of freedom
if the samples follow independent normal distributions.  If there are
at least 4 complete pairs of observation, an asymptotic confidence
interval is given based on Fisher's Z transform.
</p>
<p>If <code>method</code> is <code>"kendall"</code> or <code>"spearman"</code>, Kendall's
<code class="reqn">\tau</code> or Spearman's <code class="reqn">\rho</code> statistic is used to
estimate a rank-based measure of association.  These tests may be used
if the data do not necessarily come from a bivariate normal
distribution.
</p>
<p>For Kendall's test, by default (if <code>exact</code> is NULL), an exact
p-value is computed if there are less than 50 paired samples containing
finite values and there are no ties.  Otherwise, the test statistic is
the estimate scaled to zero mean and unit variance, and is approximately
normally distributed.
</p>
<p>For Spearman's test, p-values are computed using algorithm AS 89 for
<code class="reqn">n &lt; 1290</code> and <code>exact = TRUE</code>, otherwise via the asymptotic
<code class="reqn">t</code> approximation.  Note that these are &lsquo;exact&rsquo; for <code class="reqn">n
  &lt; 10</code>, and use an Edgeworth series approximation for larger sample
sizes (the cutoff has been changed from the original paper).
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of freedom of the test statistic in the
case that it follows a t distribution.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>the estimated measure of association, with name
<code>"cor"</code>, <code>"tau"</code>, or <code>"rho"</code> corresponding
to the method employed.</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>the value of the association measure under the
null hypothesis, always <code>0</code>.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative
hypothesis.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string indicating how the association was
measured.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the names of the data.</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>a confidence interval for the measure of association.
Currently only given for Pearson's product moment correlation
coefficient in case of at least 4 complete pairs of observations.</p>
</td></tr>
</table>


<h3>References</h3>

<p>D. J. Best &amp; D. E. Roberts (1975).
Algorithm AS 89: The Upper Tail Probabilities of Spearman's <code class="reqn">\rho</code>.
<em>Applied Statistics</em>, <b>24</b>, 377&ndash;379.
<a href="https://doi.org/10.2307/2347111">doi:10.2307/2347111</a>.
</p>
<p>Myles Hollander &amp; Douglas A. Wolfe (1973),
<em>Nonparametric Statistical Methods.</em>
New York: John Wiley &amp; Sons.
Pages 185&ndash;194 (Kendall and Spearman tests).
</p>


<h3>See Also</h3>

<p><code><a href="Kendall.html#topic+Kendall">Kendall</a></code> in package <a href="https://CRAN.R-project.org/package=Kendall"><span class="pkg">Kendall</span></a>.
</p>
<p><code><a href="SuppDists.html#topic+Kendall">pKendall</a></code> and
<code><a href="SuppDists.html#topic+Spearman">pSpearman</a></code> in package
<a href="https://CRAN.R-project.org/package=SuppDists"><span class="pkg">SuppDists</span></a>,
<code><a href="pspearman.html#topic+spearman.test">spearman.test</a></code> in package
<a href="https://CRAN.R-project.org/package=pspearman"><span class="pkg">pspearman</span></a>,
which supply different (and often more accurate) approximations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Hollander &amp; Wolfe (1973), p. 187f.
## Assessment of tuna quality.  We compare the Hunter L measure of
##  lightness to the averages of consumer panel scores (recoded as
##  integer values from 1 to 6 and averaged over 80 such values) in
##  9 lots of canned tuna.

x &lt;- c(44.4, 45.9, 41.9, 53.3, 44.7, 44.1, 50.7, 45.2, 60.1)
y &lt;- c( 2.6,  3.1,  2.5,  5.0,  3.6,  4.0,  5.2,  2.8,  3.8)

##  The alternative hypothesis of interest is that the
##  Hunter L value is positively associated with the panel score.

cor.test(x, y, method = "kendall", alternative = "greater")
## =&gt; p=0.05972

cor.test(x, y, method = "kendall", alternative = "greater",
         exact = FALSE) # using large sample approximation
## =&gt; p=0.04765

## Compare this to
cor.test(x, y, method = "spearm", alternative = "g")
cor.test(x, y,                    alternative = "g")

## Formula interface.
require(graphics)
pairs(USJudgeRatings)
cor.test(~ CONT + INTG, data = USJudgeRatings)
</code></pre>

<hr>
<h2 id='cov.wt'>Weighted Covariance Matrices</h2><span id='topic+cov.wt'></span>

<h3>Description</h3>

<p>Returns a list containing estimates of the weighted covariance matrix
and the mean of the data, and optionally of the (weighted) correlation
matrix.</p>


<h3>Usage</h3>

<pre><code class='language-R'>cov.wt(x, wt = rep(1/nrow(x), nrow(x)), cor = FALSE, center = TRUE,
       method = c("unbiased", "ML"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cov.wt_+3A_x">x</code></td>
<td>
<p>a matrix or data frame.  As usual, rows are observations and
columns are variables.</p>
</td></tr>
<tr><td><code id="cov.wt_+3A_wt">wt</code></td>
<td>
<p>a non-negative and non-zero vector of weights for each
observation.  Its length must equal the number of rows of <code>x</code>.</p>
</td></tr>
<tr><td><code id="cov.wt_+3A_cor">cor</code></td>
<td>
<p>a logical indicating whether the estimated correlation
weighted matrix will be returned as well.</p>
</td></tr>
<tr><td><code id="cov.wt_+3A_center">center</code></td>
<td>
<p>either a logical or a numeric vector specifying the
centers to be used when computing covariances.  If <code>TRUE</code>, the
(weighted) mean of each variable is used, if <code>FALSE</code>, zero is
used.  If <code>center</code> is numeric, its length must equal the number
of columns of <code>x</code>.</p>
</td></tr>
<tr><td><code id="cov.wt_+3A_method">method</code></td>
<td>
<p>string specifying how the result is scaled, see
&lsquo;Details&rsquo; below.  Can be abbreviated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, <code>method = "unbiased"</code>,
The covariance matrix is divided by one minus the sum of squares of
the weights, so if the weights are the default (<code class="reqn">1/n</code>) the conventional
unbiased estimate of the covariance matrix with divisor <code class="reqn">(n - 1)</code>
is obtained.
</p>


<h3>Value</h3>

<p>A list containing the following named components:
</p>
<table>
<tr><td><code>cov</code></td>
<td>
<p>the estimated (weighted) covariance matrix</p>
</td></tr>
<tr><td><code>center</code></td>
<td>
<p>an estimate for the center (mean) of the data.</p>
</td></tr>
<tr><td><code>n.obs</code></td>
<td>
<p>the number of observations (rows) in <code>x</code>.</p>
</td></tr>
<tr><td><code>wt</code></td>
<td>
<p>the weights used in the estimation.  Only returned if given
as an argument.</p>
</td></tr>
<tr><td><code>cor</code></td>
<td>
<p>the estimated correlation matrix.  Only returned if
<code>cor</code> is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+cov">cov</a></code> and <code><a href="#topic+var">var</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> (xy &lt;- cbind(x = 1:10, y = c(1:3, 8:5, 8:10)))
 w1 &lt;- c(0,0,0,1,1,1,1,1,0,0)
 cov.wt(xy, wt = w1) # i.e. method = "unbiased"
 cov.wt(xy, wt = w1, method = "ML", cor = TRUE)
</code></pre>

<hr>
<h2 id='cpgram'>
Plot Cumulative Periodogram
</h2><span id='topic+cpgram'></span>

<h3>Description</h3>

<p>Plots a cumulative periodogram.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cpgram(ts, taper = 0.1,
       main = paste("Series: ", deparse1(substitute(ts))),
       ci.col = "blue")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cpgram_+3A_ts">ts</code></td>
<td>
<p>a univariate time series</p>
</td></tr>
<tr><td><code id="cpgram_+3A_taper">taper</code></td>
<td>
<p>proportion tapered in forming the periodogram</p>
</td></tr>
<tr><td><code id="cpgram_+3A_main">main</code></td>
<td>
<p>main title</p>
</td></tr>
<tr><td><code id="cpgram_+3A_ci.col">ci.col</code></td>
<td>
<p>colour for confidence band.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None.
</p>


<h3>Side Effects</h3>

<p>Plots the cumulative periodogram in a square plot.
</p>


<h3>Note</h3>

<p>From package <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a>.
</p>


<h3>Author(s)</h3>

<p>B.D. Ripley</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

par(pty = "s", mfrow = c(1,2))
cpgram(lh)
lh.ar &lt;- ar(lh, order.max = 9)
cpgram(lh.ar$resid, main = "AR(3) fit to lh")

cpgram(ldeaths)
</code></pre>

<hr>
<h2 id='cutree'>Cut a Tree into Groups of Data</h2><span id='topic+cutree'></span>

<h3>Description</h3>

<p>Cuts a tree, e.g., as resulting from <code><a href="#topic+hclust">hclust</a></code>, into several
groups either by specifying the desired number(s) of groups or the cut
height(s).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cutree(tree, k = NULL, h = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cutree_+3A_tree">tree</code></td>
<td>
<p>a tree as produced by <code><a href="#topic+hclust">hclust</a></code>. <code>cutree()</code>
only expects a list with components <code>merge</code>, <code>height</code>, and
<code>labels</code>, of appropriate content each.</p>
</td></tr>
<tr><td><code id="cutree_+3A_k">k</code></td>
<td>
<p>an integer scalar or vector with the desired number of groups</p>
</td></tr>
<tr><td><code id="cutree_+3A_h">h</code></td>
<td>
<p>numeric scalar or vector with heights where the tree should
be cut.</p>
</td></tr>
</table>
<p>At least one of <code>k</code> or <code>h</code> must be specified, <code>k</code>
overrides <code>h</code> if both are given.
</p>


<h3>Details</h3>

<p>Cutting trees at a given height is only possible for ultrametric trees
(with monotone clustering heights).
</p>


<h3>Value</h3>

<p><code>cutree</code> returns a vector with group memberships if <code>k</code> or
<code>h</code> are scalar, otherwise a matrix with group memberships is returned
where each column corresponds to the elements of <code>k</code> or <code>h</code>,
respectively (which are also used as column names).
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hclust">hclust</a></code>, <code><a href="#topic+dendrogram">dendrogram</a></code> for cutting trees themselves.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>hc &lt;- hclust(dist(USArrests))

cutree(hc, k = 1:5) #k = 1 is trivial
cutree(hc, h = 250)

## Compare the 2 and 4 grouping:
g24 &lt;- cutree(hc, k = c(2,4))
table(grp2 = g24[,"2"], grp4 = g24[,"4"])
</code></pre>

<hr>
<h2 id='decompose'>
Classical Seasonal Decomposition by Moving Averages
</h2><span id='topic+decompose'></span><span id='topic+plot.decomposed.ts'></span>

<h3>Description</h3>

<p>Decompose a time series into seasonal, trend and irregular components
using moving averages.  Deals with additive or multiplicative
seasonal component.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decompose(x, type = c("additive", "multiplicative"), filter = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="decompose_+3A_x">x</code></td>
<td>
<p>A time series.</p>
</td></tr>
<tr><td><code id="decompose_+3A_type">type</code></td>
<td>
<p>The type of seasonal component. Can be abbreviated.</p>
</td></tr>
<tr><td><code id="decompose_+3A_filter">filter</code></td>
<td>
<p>A vector of filter coefficients in reverse time order (as for
AR or MA coefficients), used for filtering out the seasonal
component.  If <code>NULL</code>, a moving average with symmetric window is
performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The additive model used is:
</p>
<p style="text-align: center;"><code class="reqn">Y_t = T_t + S_t + e_t</code>
</p>

<p>The multiplicative model used is:
</p>
<p style="text-align: center;"><code class="reqn">Y_t = T_t\,S_t\, e_t</code>
</p>

<p>The function first determines the trend component using a moving
average (if <code>filter</code> is <code>NULL</code>, a symmetric window with
equal weights is used), and removes it from the time series.  Then,
the seasonal figure is computed by averaging, for each time unit, over
all periods.  The seasonal figure is then centered.   Finally, the error
component is determined by removing trend and seasonal figure
(recycled as needed) from the original time series.
</p>
<p>This only works well if <code>x</code> covers an integer number of complete
periods.
</p>


<h3>Value</h3>

<p>An object of class <code>"decomposed.ts"</code> with following components:
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>The original series.</p>
</td></tr>
<tr><td><code>seasonal</code></td>
<td>
<p>The seasonal component (i.e., the repeated seasonal figure).</p>
</td></tr>
<tr><td><code>figure</code></td>
<td>
<p>The estimated seasonal figure only.</p>
</td></tr>
<tr><td><code>trend</code></td>
<td>
<p>The trend component.</p>
</td></tr>
<tr><td><code>random</code></td>
<td>
<p>The remainder part.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>The value of <code>type</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The function <code><a href="#topic+stl">stl</a></code> provides a much more sophisticated
decomposition.
</p>


<h3>Author(s)</h3>

<p>David Meyer <a href="mailto:David.Meyer@wu.ac.at">David.Meyer@wu.ac.at</a>
</p>


<h3>References</h3>

<p>M. Kendall and A. Stuart (1983)
<em>The Advanced Theory of Statistics</em>, Vol.3,
Griffin. pp. 410&ndash;414.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stl">stl</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

m &lt;- decompose(co2)
m$figure
plot(m)

## example taken from Kendall/Stuart
x &lt;- c(-50, 175, 149, 214, 247, 237, 225, 329, 729, 809,
       530, 489, 540, 457, 195, 176, 337, 239, 128, 102, 232, 429, 3,
       98, 43, -141, -77, -13, 125, 361, -45, 184)
x &lt;- ts(x, start = c(1951, 1), end = c(1958, 4), frequency = 4)
m &lt;- decompose(x)
## seasonal figure: 6.25, 8.62, -8.84, -6.03
round(decompose(x)$figure / 10, 2)
</code></pre>

<hr>
<h2 id='delete.response'>Modify Terms Objects</h2><span id='topic+reformulate'></span><span id='topic+drop.terms'></span><span id='topic+delete.response'></span><span id='topic++5B.terms'></span>

<h3>Description</h3>

<p><code>delete.response</code> returns a <code>terms</code> object for the same
model but with no response variable.
</p>
<p><code>drop.terms</code> removes variables from the right-hand side of the
model. There is also a <code>"[.terms"</code> method to perform the same
function (with <code>keep.response = TRUE</code>).
</p>
<p><code>reformulate</code> creates a formula from a character vector.  If
<code>length(termlabels) &gt; 1</code>, its elements are concatenated with <code>+</code>.
Non-syntactic names (e.g. containing spaces or special characters; see
<code><a href="base.html#topic+make.names">make.names</a></code>) must be protected with backticks  (see examples).
A non-<code><a href="base.html#topic+parse">parse</a></code>able <code>response</code> still works for now,
back compatibly, with a deprecation warning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delete.response(termobj)

reformulate(termlabels, response = NULL, intercept = TRUE, env = parent.frame())

drop.terms(termobj, dropx = NULL, keep.response = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="delete.response_+3A_termobj">termobj</code></td>
<td>
<p>A <code>terms</code> object</p>
</td></tr>
<tr><td><code id="delete.response_+3A_termlabels">termlabels</code></td>
<td>
<p>character vector giving the right-hand side of a
model formula.  Cannot be zero-length.</p>
</td></tr>
<tr><td><code id="delete.response_+3A_response">response</code></td>
<td>
<p>character string, symbol or call giving the left-hand
side of a model formula, or <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="delete.response_+3A_intercept">intercept</code></td>
<td>
<p>logical: should the formula have an intercept?</p>
</td></tr>
<tr><td><code id="delete.response_+3A_env">env</code></td>
<td>
<p>the <code><a href="base.html#topic+environment">environment</a></code> of the <code><a href="#topic+formula">formula</a></code>
returned.</p>
</td></tr>
<tr><td><code id="delete.response_+3A_dropx">dropx</code></td>
<td>
<p>vector of positions of variables to drop from the
right-hand side of the model.</p>
</td></tr>
<tr><td><code id="delete.response_+3A_keep.response">keep.response</code></td>
<td>
<p>Keep the response in the resulting object?</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>delete.response</code> and <code>drop.terms</code> return a <code>terms</code>
object.
</p>
<p><code>reformulate</code> returns a <code>formula</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+terms">terms</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>ff &lt;- y ~ z + x + w
tt &lt;- terms(ff)
tt
delete.response(tt)
drop.terms(tt, 2:3, keep.response = TRUE)
tt[-1]
tt[2:3]
reformulate(attr(tt, "term.labels"))

## keep LHS :
reformulate("x*w", ff[[2]])
fS &lt;- surv(ft, case) ~ a + b
reformulate(c("a", "b*f"), fS[[2]])

## using non-syntactic names:
reformulate(c("`P/E`", "`% Growth`"), response = as.name("+-"))

x &lt;- c("a name", "another name")
tryCatch( reformulate(x), error = function(e) "Syntax error." )
## rather backquote the strings in x :
reformulate(sprintf("`%s`", x))

stopifnot(identical(      ~ var, reformulate("var")),
          identical(~ a + b + c, reformulate(letters[1:3])),
          identical(  y ~ a + b, reformulate(letters[1:2], "y"))
         )
</code></pre>

<hr>
<h2 id='dendrapply'>Apply a Function to All Nodes of a Dendrogram</h2><span id='topic+dendrapply'></span>

<h3>Description</h3>

<p>Apply function <code>FUN</code> to each node of a <code><a href="#topic+dendrogram">dendrogram</a></code>
recursively.  When  <code>y &lt;- dendrapply(x, fn)</code>, then <code>y</code> is a
dendrogram of the same graph structure as <code>x</code> and for each node,
<code>y.node[j] &lt;- FUN( x.node[j], ...)</code> (where <code>y.node[j]</code> is an
(invalid!) notation for the j-th node of y).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dendrapply(X, FUN, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dendrapply_+3A_x">X</code></td>
<td>
<p>an object of class <code>"<a href="#topic+dendrogram">dendrogram</a>"</code>.</p>
</td></tr>
<tr><td><code id="dendrapply_+3A_fun">FUN</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> function to be applied to each dendrogram node,
typically working on its <code><a href="base.html#topic+attributes">attributes</a></code> alone, returning an
altered version of the same node.</p>
</td></tr>
<tr><td><code id="dendrapply_+3A_...">...</code></td>
<td>
<p>potential further arguments passed to <code>FUN</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Usually a dendrogram of the same (graph) structure as <code>X</code>.
For that, the function must be conceptually of the form
<code>FUN &lt;- function(X) { attributes(X) &lt;- .....;  X }</code>,
i.e., returning the node with some attributes added or changed.
</p>


<h3>Note</h3>

<p>The implementation is somewhat experimental and suggestions for
enhancements (or nice examples of usage) are very welcome.  The
current implementation is <em>recursive</em> and inefficient for
dendrograms with many non-leaves.  See the &lsquo;Warning&rsquo; in
<code><a href="#topic+dendrogram">dendrogram</a></code>.
</p>


<h3>Author(s)</h3>

<p>Martin Maechler</p>


<h3>See Also</h3>

<p><code><a href="#topic+as.dendrogram">as.dendrogram</a></code>, <code><a href="base.html#topic+lapply">lapply</a></code> for applying
a function to each component of a <code>list</code>, <code><a href="base.html#topic+rapply">rapply</a></code>
for doing so to each non-list component of a nested list.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

## a smallish simple dendrogram
dhc &lt;- as.dendrogram(hc &lt;- hclust(dist(USArrests), "ave"))
(dhc21 &lt;- dhc[[2]][[1]])

## too simple:
dendrapply(dhc21, function(n) utils::str(attributes(n)))

## toy example to set colored leaf labels :
local({
  colLab &lt;&lt;- function(n) {
      if(is.leaf(n)) {
        a &lt;- attributes(n)
        i &lt;&lt;- i+1
        attr(n, "nodePar") &lt;-
            c(a$nodePar, list(lab.col = mycols[i], lab.font = i%%3))
      }
      n
  }
  mycols &lt;- grDevices::rainbow(attr(dhc21,"members"))
  i &lt;- 0
 })
dL &lt;- dendrapply(dhc21, colLab)
op &lt;- par(mfrow = 2:1)
 plot(dhc21)
 plot(dL) ## --&gt; colored labels!
par(op)
</code></pre>

<hr>
<h2 id='dendrogram'>General Tree Structures</h2><span id='topic+dendrogram'></span><span id='topic+as.dendrogram'></span><span id='topic+as.dendrogram.dendrogram'></span><span id='topic+as.dendrogram.hclust'></span><span id='topic+as.hclust.dendrogram'></span><span id='topic+cut.dendrogram'></span><span id='topic++5B+5B.dendrogram'></span><span id='topic+merge.dendrogram'></span><span id='topic+nobs.dendrogram'></span><span id='topic+plot.dendrogram'></span><span id='topic+print.dendrogram'></span><span id='topic+rev.dendrogram'></span><span id='topic+str.dendrogram'></span><span id='topic+is.leaf'></span>

<h3>Description</h3>

<p>Class <code>"dendrogram"</code> provides general functions for handling
tree-like structures.  It is intended as a replacement for similar
functions in hierarchical clustering and classification/regression
trees, such that all of these can use the same engine for plotting or
cutting trees.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.dendrogram(object, ...)
## S3 method for class 'hclust'
as.dendrogram(object, hang = -1, check = TRUE, ...)

## S3 method for class 'dendrogram'
as.hclust(x, ...)

## S3 method for class 'dendrogram'
plot(x, type = c("rectangle", "triangle"),
      center = FALSE,
      edge.root = is.leaf(x) || !is.null(attr(x,"edgetext")),
      nodePar = NULL, edgePar = list(),
      leaflab = c("perpendicular", "textlike", "none"),
      dLeaf = NULL, xlab = "", ylab = "", xaxt = "n", yaxt = "s",
      horiz = FALSE, frame.plot = FALSE, xlim, ylim, ...)

## S3 method for class 'dendrogram'
cut(x, h, ...)

## S3 method for class 'dendrogram'
merge(x, y, ..., height,
      adjust = c("auto", "add.max", "none"))

## S3 method for class 'dendrogram'
nobs(object, ...)

## S3 method for class 'dendrogram'
print(x, digits, ...)

## S3 method for class 'dendrogram'
rev(x)

## S3 method for class 'dendrogram'
str(object, max.level = NA, digits.d = 3,
    give.attr = FALSE, wid = getOption("width"),
    nest.lev = 0, indent.str = "",
    last.str = getOption("str.dendrogram.last"), stem = "--",
    ...)

is.leaf(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dendrogram_+3A_object">object</code></td>
<td>
<p>any <span class="rlang"><b>R</b></span> object that can be made into one of class
<code>"dendrogram"</code>.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_x">x</code>, <code id="dendrogram_+3A_y">y</code></td>
<td>
<p>object(s) of class <code>"dendrogram"</code>.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_hang">hang</code></td>
<td>
<p>numeric scalar indicating how the <em>height</em> of leaves
should be computed from the heights of their parents; see
<code><a href="#topic+plot.hclust">plot.hclust</a></code>.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_check">check</code></td>
<td>
<p>logical indicating if <code>object</code> should be checked for
validity.  This check is not necessary when <code>x</code> is known to be
valid such as when it is the direct result of <code>hclust()</code>.  The
default is <code>check=TRUE</code>, e.g. for protecting against memory
explosion with invalid inputs.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_type">type</code></td>
<td>
<p>type of plot.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_center">center</code></td>
<td>
<p>logical; if <code>TRUE</code>, nodes are plotted centered with
respect to the leaves in the branch.  Otherwise (default), plot them
in the middle of all direct child nodes.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_edge.root">edge.root</code></td>
<td>
<p>logical; if true, draw an edge to the root node.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_nodepar">nodePar</code></td>
<td>
<p>a <code>list</code> of plotting parameters to use for the
nodes (see <code><a href="graphics.html#topic+points">points</a></code>) or <code>NULL</code> by default which
does not draw symbols at the nodes.  The list may contain components
named <code>pch</code>, <code>cex</code>, <code>col</code>, <code>xpd</code>,
and/or <code>bg</code> each of
which can have length two for specifying separate attributes for
<em>inner</em> nodes and <em>leaves</em>.  Note that the default of
<code>pch</code> is <code>1:2</code>, so you may want to use <code>pch = NA</code> if
you specify <code>nodePar</code>.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_edgepar">edgePar</code></td>
<td>
<p>a <code>list</code> of plotting parameters to use for the
edge <code><a href="graphics.html#topic+segments">segments</a></code> and labels (if there's an
<code>edgetext</code>).  The list may contain components
named <code>col</code>, <code>lty</code> and <code>lwd</code> (for the segments),
<code>p.col</code>, <code>p.lwd</code>, and <code>p.lty</code> (for the
<code><a href="graphics.html#topic+polygon">polygon</a></code> around the text) and <code>t.col</code> for the text
color.  As with <code>nodePar</code>, each can have length two for
differentiating leaves and inner nodes.
</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_leaflab">leaflab</code></td>
<td>
<p>a string specifying how leaves are labeled.  The
default <code>"perpendicular"</code> write text vertically (by default).<br />
<code>"textlike"</code> writes text horizontally (in a rectangle), and <br />
<code>"none"</code> suppresses leaf labels.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_dleaf">dLeaf</code></td>
<td>
<p>a number specifying the <b>d</b>istance in user
coordinates between the tip of a leaf and its label.  If <code>NULL</code>
as per default, 3/4 of a letter width or height is used.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_horiz">horiz</code></td>
<td>
<p>logical indicating if the dendrogram should be drawn
<em>horizontally</em> or not.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_frame.plot">frame.plot</code></td>
<td>
<p>logical indicating if a box around the plot should
be drawn, see <code><a href="graphics.html#topic+plot.default">plot.default</a></code>.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_h">h</code></td>
<td>
<p>height at which the tree is cut.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_height">height</code></td>
<td>
<p>height at which the two dendrograms should be merged.  If not
specified (or <code>NULL</code>), the default is ten percent larger than
the (larger of the) two component heights.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_adjust">adjust</code></td>
<td>
<p>a string determining if the leaf values should be
adjusted.  The default, <code>"auto"</code>, checks if the (first) two
dendrograms both start at <code>1</code>; if they do, <code>"add.max"</code> is
chosen, which adds the maximum of the previous dendrogram leaf
values to each leaf of the &ldquo;next&rdquo; dendrogram.  Specifying
<code>adjust</code> to another value skips the check and hence is a tad
more efficient.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_xlim">xlim</code>, <code id="dendrogram_+3A_ylim">ylim</code></td>
<td>
<p>optional x- and y-limits of the plot, passed to
<code><a href="graphics.html#topic+plot.default">plot.default</a></code>.  The defaults for these show the full
dendrogram.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_...">...</code>, <code id="dendrogram_+3A_xlab">xlab</code>, <code id="dendrogram_+3A_ylab">ylab</code>, <code id="dendrogram_+3A_xaxt">xaxt</code>, <code id="dendrogram_+3A_yaxt">yaxt</code></td>
<td>
<p>graphical parameters, or arguments for
other methods.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_digits">digits</code></td>
<td>
<p>integer specifying the precision for printing, see
<code><a href="base.html#topic+print.default">print.default</a></code>.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_max.level">max.level</code>, <code id="dendrogram_+3A_digits.d">digits.d</code>, <code id="dendrogram_+3A_give.attr">give.attr</code>, <code id="dendrogram_+3A_wid">wid</code>, <code id="dendrogram_+3A_nest.lev">nest.lev</code>, <code id="dendrogram_+3A_indent.str">indent.str</code></td>
<td>
<p>arguments
to <code>str</code>, see <code><a href="utils.html#topic+str.default">str.default</a>()</code>.  Note that
<code>give.attr = FALSE</code> still shows <code>height</code> and <code>members</code>
attributes for each node.</p>
</td></tr>
<tr><td><code id="dendrogram_+3A_last.str">last.str</code>, <code id="dendrogram_+3A_stem">stem</code></td>
<td>
<p>strings used for <code>str()</code> specifying how the
last branch (at each level) should start and the <em>stem</em>
to use for each dendrogram branch.  In some environments, using
<code>last.str = "'"</code> will provide much nicer looking output, than
the historical default <code>last.str = "`"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The dendrogram is directly represented as a nested list where each
component corresponds to a branch of the tree.  Hence, the first
branch of tree <code>z</code> is <code>z[[1]]</code>, the second branch of the
corresponding subtree is <code>z[[1]][[2]]</code>, or shorter
<code>z[[c(1,2)]]</code>, etc..  Each node of the tree
carries some information needed for efficient plotting or cutting as
attributes, of which only <code>members</code>, <code>height</code> and
<code>leaf</code> for leaves are compulsory:
</p>

<dl>
<dt><code>members</code></dt><dd><p>total number of leaves in the branch</p>
</dd>
<dt><code>height</code></dt><dd><p>numeric non-negative height at which the node
is plotted.</p>
</dd>
<dt><code>midpoint</code></dt><dd><p>numeric horizontal distance of the node from
the left border (the leftmost leaf) of the branch (unit 1 between
all leaves).  This is used for <code>plot(*, center = FALSE)</code>.</p>
</dd>
<dt><code>label</code></dt><dd><p>character; the label of the node</p>
</dd>
<dt><code>x.member</code></dt><dd><p>for <code>cut()$upper</code>,
the number of <em>former</em> members; more generally a substitute
for the <code>members</code> component used for &lsquo;horizontal&rsquo;
(when <code>horiz = FALSE</code>, else &lsquo;vertical&rsquo;) alignment.</p>
</dd>
<dt><code>edgetext</code></dt><dd><p>character; the label for the edge leading to
the node</p>
</dd>
<dt><code>nodePar</code></dt><dd><p>a named list (of length-1 components)
specifying node-specific attributes for <code><a href="graphics.html#topic+points">points</a></code>
plotting, see the <code>nodePar</code> argument above.</p>
</dd>
<dt><code>edgePar</code></dt><dd><p>a named list (of length-1 components)
specifying attributes for <code><a href="graphics.html#topic+segments">segments</a></code> plotting of the
edge leading to the node, and drawing of the <code>edgetext</code> if
available, see the <code>edgePar</code> argument above.</p>
</dd>
<dt><code>leaf</code></dt><dd><p>logical, if <code>TRUE</code>, the node is a leaf of
the tree.</p>
</dd>

</dl>

<p><code>cut.dendrogram()</code> returns a list with components <code>$upper</code>
and <code>$lower</code>, the first is a truncated version of the original
tree, also of class <code>dendrogram</code>, the latter a list with the
branches obtained from cutting the tree, each a <code>dendrogram</code>.
</p>
<p>There are <code><a href="base.html#topic++5B+5B">[[</a></code>, <code><a href="base.html#topic+print">print</a></code>, and <code><a href="utils.html#topic+str">str</a></code>
methods for <code>"dendrogram"</code> objects where the first one
(extraction) ensures that selecting sub-branches keeps the class,
i.e., returns a dendrogram even if only a leaf.
On the other hand, <code><a href="Matrix.html#topic++5B">[</a></code> (<em>single</em> bracket) extraction
returns the underlying list structure.
</p>
<p>Objects of class <code>"hclust"</code> can be converted to class
<code>"dendrogram"</code> using method <code>as.dendrogram()</code>, and since R
2.13.0, there is also a <code><a href="#topic+as.hclust">as.hclust</a>()</code> method as an inverse.
</p>
<p><code>rev.dendrogram</code> simply returns the dendrogram <code>x</code> with
reversed nodes, see also <code><a href="#topic+reorder.dendrogram">reorder.dendrogram</a></code>.
</p>
<p>The <code><a href="base.html#topic+merge">merge</a>(x, y, ...)</code> method merges two or more
dendrograms into a new one which has <code>x</code> and <code>y</code> (and
optional further arguments) as branches.  Note that before <span class="rlang"><b>R</b></span> 3.1.2,
<code>adjust = "none"</code> was used implicitly, which is invalid when,
e.g., the dendrograms are from <code><a href="#topic+as.dendrogram">as.dendrogram</a>(hclust(..))</code>.
</p>
<p><code><a href="#topic+nobs">nobs</a>(object)</code> returns the total number of leaves (the
<code>members</code> attribute, see above).
</p>
<p><code>is.leaf(object)</code> returns logical indicating if <code>object</code> is a
leaf (the most simple dendrogram).
</p>
<p><code>plotNode()</code> and <code>plotNodeLimit()</code> are helper functions.
</p>


<h3>Warning</h3>

<p>Some operations on dendrograms such as <code>merge()</code> make use of
recursion.  For deep trees it may be necessary to increase
<code><a href="base.html#topic+options">options</a>("expressions")</code>: if you do, you are likely to need
to set the C stack size (<code><a href="base.html#topic+Cstack_info">Cstack_info</a>()[["size"]]</code>) larger
than the default where possible.
</p>


<h3>Note</h3>


<dl>
<dt><code>plot()</code>:</dt><dd><p>When using <code>type = "triangle"</code>,
<code>center = TRUE</code> often looks better.</p>
</dd>
<dt><code>str(d)</code>:</dt><dd><p>If you really want to see the <em>internal</em>
structure, use <code>str(unclass(d))</code> instead.</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+dendrapply">dendrapply</a></code> for applying a function to <em>each</em> node.
<code><a href="#topic+order.dendrogram">order.dendrogram</a></code> and <code><a href="#topic+reorder.dendrogram">reorder.dendrogram</a></code>;
further, the <code><a href="base.html#topic+labels">labels</a></code> method.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics); require(utils)

hc &lt;- hclust(dist(USArrests), "ave")
(dend1 &lt;- as.dendrogram(hc)) # "print()" method
str(dend1)          # "str()" method
str(dend1, max.level = 2, last.str =  "'") # only the first two sub-levels
oo &lt;- options(str.dendrogram.last = "\\") # yet another possibility
str(dend1, max.level = 2) # only the first two sub-levels
options(oo)  # .. resetting them

op &lt;- par(mfrow =  c(2,2), mar = c(5,2,1,4))
plot(dend1)
## "triangle" type and show inner nodes:
plot(dend1, nodePar = list(pch = c(1,NA), cex = 0.8, lab.cex = 0.8),
      type = "t", center = TRUE)
plot(dend1, edgePar = list(col = 1:2, lty = 2:3),
     dLeaf = 1, edge.root = TRUE)
plot(dend1, nodePar = list(pch = 2:1, cex = .4*2:1, col = 2:3),
     horiz = TRUE)

## simple test for as.hclust() as the inverse of as.dendrogram():
stopifnot(identical(as.hclust(dend1)[1:4], hc[1:4]))

dend2 &lt;- cut(dend1, h = 70)
## leaves are wrong horizontally in R 4.0 and earlier:
plot(dend2$upper)
plot(dend2$upper, nodePar = list(pch = c(1,7), col = 2:1))
##  dend2$lower is *NOT* a dendrogram, but a list of .. :
plot(dend2$lower[[3]], nodePar = list(col = 4), horiz = TRUE, type = "tr")
## "inner" and "leaf" edges in different type &amp; color :
plot(dend2$lower[[2]], nodePar = list(col = 1),   # non empty list
     edgePar = list(lty = 1:2, col = 2:1), edge.root = TRUE)
par(op)
d3 &lt;- dend2$lower[[2]][[2]][[1]]
stopifnot(identical(d3, dend2$lower[[2]][[c(2,1)]]))
str(d3, last.str = "'")

## to peek at the inner structure "if you must", use '[..]' indexing :
str(d3[2][[1]]) ## or the full
str(d3[])

## merge() to join dendrograms:
(d13 &lt;- merge(dend2$lower[[1]], dend2$lower[[3]]))
## merge() all parts back (using default 'height' instead of original one):
den.1 &lt;- Reduce(merge, dend2$lower)
## or merge() all four parts at same height --&gt; 4 branches (!)
d. &lt;- merge(dend2$lower[[1]], dend2$lower[[2]], dend2$lower[[3]],
            dend2$lower[[4]])
## (with a warning) or the same using  do.call :
stopifnot(identical(d., do.call(merge, dend2$lower)))
plot(d., main = "merge(d1, d2, d3, d4)  |-&gt;  dendrogram with a 4-split")

## "Zoom" in to the first dendrogram :
plot(dend1, xlim = c(1,20), ylim = c(1,50))

nP &lt;- list(col = 3:2, cex = c(2.0, 0.75), pch =  21:22,
           bg =  c("light blue", "pink"),
           lab.cex = 0.75, lab.col = "tomato")
plot(d3, nodePar= nP, edgePar = list(col = "gray", lwd = 2), horiz = TRUE)

addE &lt;- function(n) {
      if(!is.leaf(n)) {
        attr(n, "edgePar") &lt;- list(p.col = "plum")
        attr(n, "edgetext") &lt;- paste(attr(n,"members"),"members")
      }
      n
}
d3e &lt;- dendrapply(d3, addE)
plot(d3e, nodePar =  nP)
plot(d3e, nodePar =  nP, leaflab = "textlike")



</code></pre>

<hr>
<h2 id='density'>Kernel Density Estimation</h2><span id='topic+density'></span><span id='topic+density.default'></span>

<h3>Description</h3>

<p>The (S3) generic function <code>density</code> computes kernel density
estimates.  Its default method does so with the given kernel and
bandwidth for univariate observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>density(x, ...)
## Default S3 method:
density(x, bw = "nrd0", adjust = 1,
        kernel = c("gaussian", "epanechnikov", "rectangular",
                   "triangular", "biweight",
                   "cosine", "optcosine"),
        weights = NULL, window = kernel, width,
        give.Rkern = FALSE, subdensity = FALSE,
        warnWbw = var(weights) &gt; 0,
        n = 512, from, to, cut = 3, ext = 4,
        old.coords = FALSE,
        na.rm = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="density_+3A_x">x</code></td>
<td>
<p>the data from which the estimate is to be computed.  For the
default method a numeric vector: long vectors are not supported.</p>
</td></tr>
<tr><td><code id="density_+3A_bw">bw</code></td>
<td>
<p>the smoothing bandwidth to be used.  The kernels are scaled
such that this is the standard deviation of the smoothing kernel.
(Note this differs from the reference books cited below.)
</p>
<p><code>bw</code> can also be a character string giving a rule to choose the
bandwidth.  See <code><a href="#topic+bw.nrd">bw.nrd</a></code>. <br /> The default,
<code>"nrd0"</code>, has remained the default for historical and
compatibility reasons, rather than as a general recommendation,
where e.g., <code>"SJ"</code> would rather fit, see also
Venables and Ripley (2002).
</p>
<p>The specified (or computed) value of <code>bw</code> is multiplied by
<code>adjust</code>.
</p>
</td></tr>
<tr><td><code id="density_+3A_adjust">adjust</code></td>
<td>
<p>the bandwidth used is actually <code>adjust*bw</code>.
This makes it easy to specify values like &lsquo;half the default&rsquo;
bandwidth.</p>
</td></tr>
<tr><td><code id="density_+3A_kernel">kernel</code>, <code id="density_+3A_window">window</code></td>
<td>
<p>a character string giving the smoothing kernel
to be used. This must partially match one of <code>"gaussian"</code>,
<code>"rectangular"</code>, <code>"triangular"</code>, <code>"epanechnikov"</code>,
<code>"biweight"</code>, <code>"cosine"</code> or <code>"optcosine"</code>, with default
<code>"gaussian"</code>, and may be abbreviated to a unique prefix (single
letter).
</p>
<p><code>"cosine"</code> is smoother than <code>"optcosine"</code>, which is the
usual &lsquo;cosine&rsquo; kernel in the literature and almost MSE-efficient.
However, <code>"cosine"</code> is the version used by S.
</p>
</td></tr>
<tr><td><code id="density_+3A_weights">weights</code></td>
<td>
<p>numeric vector of non-negative observation weights,
hence of same length as <code>x</code>. The default <code>NULL</code> is
equivalent to <code>weights = rep(1/nx, nx)</code> where <code>nx</code> is the
length of (the finite entries of) <code>x[]</code>.  If <code>na.rm = TRUE</code>
and there are <code>NA</code>'s in <code>x</code>, they <em>and</em> the
corresponding weights are removed before computations.  In that case,
when the original weights have summed to one, they are re-scaled to
keep doing so.
</p>
<p>Note that weights are <em>not</em> taken into account for automatic
bandwidth rules, i.e., when <code>bw</code> is a string.  When the weights
are proportional to true counts <code>cn</code>, <code>density(x = rep(x, cn))</code>
may be used instead of <code>weights</code>.
</p>
</td></tr>
<tr><td><code id="density_+3A_width">width</code></td>
<td>
<p>this exists for compatibility with S; if given, and
<code>bw</code> is not, will set <code>bw</code> to <code>width</code> if this is a
character string, or to a kernel-dependent multiple of <code>width</code>
if this is numeric.</p>
</td></tr>
<tr><td><code id="density_+3A_give.rkern">give.Rkern</code></td>
<td>
<p>logical; if true, <em>no</em> density is estimated, and
the &lsquo;canonical bandwidth&rsquo; of the chosen <code>kernel</code> is returned
instead.</p>
</td></tr>
<tr><td><code id="density_+3A_subdensity">subdensity</code></td>
<td>
<p>used only when <code>weights</code> are specified which do not sum
to one.  When true, it indicates that a &ldquo;sub-density&rdquo;
is desired and no warning should be signalled.  By default, when false,
a <code><a href="base.html#topic+warning">warning</a></code> is signalled when the weights do not sum to one.</p>
</td></tr>
<tr><td><code id="density_+3A_warnwbw">warnWbw</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code>, used only when <code>weights</code> are specified <em>and</em>
<code>bw</code> is <code>character</code>, i.e., automatic bandwidth selection is
chosen (as by default).  When true (as by default), a
<code><a href="base.html#topic+warning">warning</a></code> is signalled to alert the user that automatic
bandwidth selection will not take the weights into account and hence
may be suboptimal.</p>
</td></tr>
<tr><td><code id="density_+3A_n">n</code></td>
<td>
<p>the number of equally spaced points at which the density is
to be estimated.  When <code>n &gt; 512</code>, it is rounded up to a power
of 2 during the calculations (as <code><a href="#topic+fft">fft</a></code> is used) and the
final result is interpolated by <code><a href="#topic+approx">approx</a></code>.  So it almost
always makes sense to specify <code>n</code> as a power of two.
</p>
</td></tr>
<tr><td><code id="density_+3A_from">from</code>, <code id="density_+3A_to">to</code></td>
<td>
<p>the left and right-most points of the grid at which the
density is to be estimated; the defaults are <code>cut * bw</code> outside
of <code>range(x)</code>.</p>
</td></tr>
<tr><td><code id="density_+3A_cut">cut</code></td>
<td>
<p>by default, the values of <code>from</code> and <code>to</code> are
<code>cut</code> bandwidths beyond the extremes of the data.  This allows
the estimated density to drop to approximately zero at the extremes.</p>
</td></tr>
<tr><td><code id="density_+3A_ext">ext</code></td>
<td>
<p>a positive extension factor, <code>4</code> by default.  The values
<code>from</code> and <code>to</code> are further extended on both sides to
<code>lo &lt;- from - ext * bw</code> and <code>up &lt;- to + ext * bw</code> which are
then used to build the grid used for the FFT and interpolation, see
<code>n</code> above.
Do not change unless you know what you are doing!</p>
</td></tr>
<tr><td><code id="density_+3A_old.coords">old.coords</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> to require pre-R 4.4.0 behaviour
which gives too large values by a factor of about <code class="reqn">(1 + 1/(2n-2))</code>.</p>
</td></tr>
<tr><td><code id="density_+3A_na.rm">na.rm</code></td>
<td>
<p>logical; if <code>TRUE</code>, missing values are removed
from <code>x</code>. If <code>FALSE</code> any missing values cause an error.</p>
</td></tr>
<tr><td><code id="density_+3A_...">...</code></td>
<td>
<p>further arguments for (non-default) methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm used in <code>density.default</code> disperses the mass of the
empirical distribution function over a regular grid of at least 512
points and then uses the fast Fourier transform to convolve this
approximation with a discretized version of the kernel and then uses
linear approximation to evaluate the density at the specified points.
</p>
<p>The statistical properties of a kernel are determined by
<code class="reqn">\sigma^2_K = \int t^2 K(t) dt</code>
which is always <code class="reqn">= 1</code> for our kernels (and hence the bandwidth
<code>bw</code> is the standard deviation of the kernel) and
<code class="reqn">R(K) = \int K^2(t) dt</code>.<br />
MSE-equivalent bandwidths (for different kernels) are proportional to
<code class="reqn">\sigma_K R(K)</code> which is scale invariant and for our
kernels equal to <code class="reqn">R(K)</code>.  This value is returned when
<code>give.Rkern = TRUE</code>.  See the examples for using exact equivalent
bandwidths.
</p>
<p>Infinite values in <code>x</code> are assumed to correspond to a point mass at
<code>+/-Inf</code> and the density estimate is of the sub-density on
<code>(-Inf, +Inf)</code>.
</p>


<h3>Value</h3>

<p>If <code>give.Rkern</code> is true, the number <code class="reqn">R(K)</code>, otherwise
an object with class <code>"density"</code> whose
underlying structure is a list containing the following components.
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>the <code>n</code> coordinates of the points where the density is
estimated.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the estimated density values.  These will be non-negative,
but can be zero.</p>
</td></tr>
<tr><td><code>bw</code></td>
<td>
<p>the bandwidth used.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>the sample size after elimination of missing values.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call which produced the result.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>the deparsed name of the <code>x</code> argument.</p>
</td></tr>
<tr><td><code>has.na</code></td>
<td>
<p>logical, for compatibility (always <code>FALSE</code>).</p>
</td></tr>
</table>
<p>The <code>print</code> method reports <code><a href="base.html#topic+summary">summary</a></code> values on the
<code>x</code> and <code>y</code> components.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988).
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole (for S version).
</p>
<p>Scott, D. W. (1992).
<em>Multivariate Density Estimation. Theory, Practice and Visualization</em>.
New York: Wiley.
</p>
<p>Sheather, S. J. and Jones, M. C. (1991).
A reliable data-based bandwidth selection method for kernel density
estimation.
<em>Journal of the Royal Statistical Society Series B</em>,
<b>53</b>, 683&ndash;690.
<a href="https://doi.org/10.1111/j.2517-6161.1991.tb01857.x">doi:10.1111/j.2517-6161.1991.tb01857.x</a>.

</p>
<p>Silverman, B. W. (1986).
<em>Density Estimation</em>.
London: Chapman and Hall.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002).
<em>Modern Applied Statistics with S</em>.
New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bw.nrd">bw.nrd</a></code>,
<code><a href="#topic+plot.density">plot.density</a></code>, <code><a href="graphics.html#topic+hist">hist</a></code>;
<code><a href="#topic+fft">fft</a></code> and <code><a href="#topic+convolve">convolve</a></code> for the computational short
cut used.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

plot(density(c(-20, rep(0,98), 20)), xlim = c(-4, 4))  # IQR = 0

# The Old Faithful geyser data
d &lt;- density(faithful$eruptions, bw = "sj")
d
plot(d)

plot(d, type = "n")
polygon(d, col = "wheat")

## Missing values:
x &lt;- xx &lt;- faithful$eruptions
x[i.out &lt;- sample(length(x), 10)] &lt;- NA
doR &lt;- density(x, bw = 0.15, na.rm = TRUE)
lines(doR, col = "blue")
points(xx[i.out], rep(0.01, 10))

## Weighted observations:
fe &lt;- sort(faithful$eruptions) # has quite a few non-unique values
## use 'counts / n' as weights:
dw &lt;- density(unique(fe), weights = table(fe)/length(fe), bw = d$bw)
utils::str(dw) ## smaller n: only 126, but identical estimate:
stopifnot(all.equal(d[1:3], dw[1:3]))

## simulation from a density() fit:
# a kernel density fit is an equally-weighted mixture.
fit &lt;- density(xx)
N &lt;- 1e6
x.new &lt;- rnorm(N, sample(xx, size = N, replace = TRUE), fit$bw)
plot(fit)
lines(density(x.new), col = "blue")


## The available kernels:
(kernels &lt;- eval(formals(density.default)$kernel))

## show the kernels in the R parametrization
plot (density(0, bw = 1), xlab = "",
      main = "R's density() kernels with bw = 1")
for(i in 2:length(kernels))
   lines(density(0, bw = 1, kernel =  kernels[i]), col = i)
legend(1.5,.4, legend = kernels, col = seq(kernels),
       lty = 1, cex = .8, y.intersp = 1)

## show the kernels in the S parametrization
plot(density(0, from = -1.2, to = 1.2, width = 2, kernel = "gaussian"),
     type = "l", ylim = c(0, 1), xlab = "",
     main = "R's density() kernels with width = 1")
for(i in 2:length(kernels))
   lines(density(0, width = 2, kernel =  kernels[i]), col = i)
legend(0.6, 1.0, legend = kernels, col = seq(kernels), lty = 1)

##-------- Semi-advanced theoretic from here on -------------


## Explore the old.coords TRUE --&gt; FALSE change:
set.seed(7); x &lt;- runif(2^12) # N = 4096
den  &lt;- density(x) # -&gt; grid of n = 512 points
den0 &lt;- density(x, old.coords = TRUE)
summary(den0$y / den$y) # 1.001 ... 1.011
summary(    den0$y / den$y - 1) # ~= 1/(2n-2)
summary(1/ (den0$y / den$y - 1))# ~=    2n-2 = 1022
corr0 &lt;- 1 - 1/(2*512-2) # 1 - 1/(2n-2)
all.equal(den$y, den0$y * corr0)# ~ 0.0001
plot(den$x, (den0$y - den$y)/den$y, type='o', cex=1/4)
title("relative error of density(runif(2^12), old.coords=TRUE)")
abline(h = 1/1022, v = range(x), lty=2); axis(2, at=1/1022, "1/(2n-2)", las=1)


## The R[K] for our kernels:
(RKs &lt;- cbind(sapply(kernels,
                     function(k) density(kernel = k, give.Rkern = TRUE))))
100*round(RKs["epanechnikov",]/RKs, 4) ## Efficiencies

bw &lt;- bw.SJ(precip) ## sensible automatic choice
plot(density(precip, bw = bw),
     main = "same sd bandwidths, 7 different kernels")
for(i in 2:length(kernels))
   lines(density(precip, bw = bw, kernel = kernels[i]), col = i)

## Bandwidth Adjustment for "Exactly Equivalent Kernels"
h.f &lt;- sapply(kernels, function(k)density(kernel = k, give.Rkern = TRUE))
(h.f &lt;- (h.f["gaussian"] / h.f)^ .2)
## -&gt; 1, 1.01, .995, 1.007,... close to 1 =&gt; adjustment barely visible..

plot(density(precip, bw = bw),
     main = "equivalent bandwidths, 7 different kernels")
for(i in 2:length(kernels))
   lines(density(precip, bw = bw, adjust = h.f[i], kernel = kernels[i]),
         col = i)
legend(55, 0.035, legend = kernels, col = seq(kernels), lty = 1)
</code></pre>

<hr>
<h2 id='deriv'>Symbolic and Algorithmic Derivatives of Simple Expressions</h2><span id='topic+D'></span><span id='topic+deriv'></span><span id='topic+deriv.default'></span><span id='topic+deriv.formula'></span><span id='topic+deriv3'></span><span id='topic+deriv3.default'></span><span id='topic+deriv3.formula'></span>

<h3>Description</h3>

<p>Compute derivatives of simple expressions, symbolically and algorithmically.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    D (expr, name)
 deriv(expr, ...)
deriv3(expr, ...)

 ## Default S3 method:
deriv(expr, namevec, function.arg = NULL, tag = ".expr",
       hessian = FALSE, ...)
 ## S3 method for class 'formula'
deriv(expr, namevec, function.arg = NULL, tag = ".expr",
       hessian = FALSE, ...)

## Default S3 method:
deriv3(expr, namevec, function.arg = NULL, tag = ".expr",
       hessian = TRUE, ...)
## S3 method for class 'formula'
deriv3(expr, namevec, function.arg = NULL, tag = ".expr",
       hessian = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deriv_+3A_expr">expr</code></td>
<td>
<p>a <code><a href="base.html#topic+expression">expression</a></code> or <code><a href="base.html#topic+call">call</a></code> or
(except <code>D</code>) a formula with no lhs.</p>
</td></tr>
<tr><td><code id="deriv_+3A_name">name</code>, <code id="deriv_+3A_namevec">namevec</code></td>
<td>
<p>character vector, giving the variable names (only
one for <code>D()</code>) with respect to which derivatives will be
computed.</p>
</td></tr>
<tr><td><code id="deriv_+3A_function.arg">function.arg</code></td>
<td>
<p>if specified and non-<code>NULL</code>, a character
vector of arguments for a function return, or a function (with empty
body) or <code>TRUE</code>, the latter indicating that a function with
argument names <code>namevec</code> should be used.</p>
</td></tr>
<tr><td><code id="deriv_+3A_tag">tag</code></td>
<td>
<p>character; the prefix to be used for the locally created
variables in result.  Must be no longer than 60 bytes when translated
to the native encoding.</p>
</td></tr>
<tr><td><code id="deriv_+3A_hessian">hessian</code></td>
<td>
<p>a logical value indicating whether the second derivatives
should be calculated and incorporated in the return value.</p>
</td></tr>
<tr><td><code id="deriv_+3A_...">...</code></td>
<td>
<p>arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>D</code> is modelled after its S namesake for taking simple symbolic
derivatives.
</p>
<p><code>deriv</code> is a <em>generic</em> function with a default and a
<code><a href="#topic+formula">formula</a></code> method.  It returns a <code><a href="base.html#topic+call">call</a></code> for
computing the <code>expr</code> and its (partial) derivatives,
simultaneously.  It uses so-called <em>algorithmic derivatives</em>.  If
<code>function.arg</code> is a function, its arguments can have default
values, see the <code>fx</code> example below.
</p>
<p>Currently, <code>deriv.formula</code> just calls <code>deriv.default</code> after
extracting the expression to the right of <code>~</code>.
</p>
<p><code>deriv3</code> and its methods are equivalent to <code>deriv</code> and its
methods except that <code>hessian</code> defaults to <code>TRUE</code> for
<code>deriv3</code>.
</p>
<p>The internal code knows about the arithmetic operators <code>+</code>,
<code>-</code>, <code>*</code>, <code>/</code> and <code>^</code>, and the single-variable
functions <code>exp</code>, <code>log</code>, <code>sin</code>, <code>cos</code>, <code>tan</code>,
<code>sinh</code>, <code>cosh</code>, <code>sqrt</code>, <code>pnorm</code>, <code>dnorm</code>,
<code>asin</code>, <code>acos</code>, <code>atan</code>, <code>gamma</code>, <code>lgamma</code>,
<code>digamma</code> and <code>trigamma</code>, as well as <code>psigamma</code> for one
or two arguments (but derivative only with respect to the first).
(Note that only the standard normal distribution is considered.)
<br />
Since <span class="rlang"><b>R</b></span> 3.4.0, the single-variable functions <code><a href="base.html#topic+log1p">log1p</a></code>,
<code>expm1</code>, <code>log2</code>, <code>log10</code>, <code><a href="base.html#topic+cospi">cospi</a></code>,
<code>sinpi</code>, <code>tanpi</code>, <code><a href="base.html#topic+factorial">factorial</a></code>, and
<code>lfactorial</code> are supported as well.
</p>


<h3>Value</h3>

<p><code>D</code> returns a call and therefore can easily be iterated
for higher derivatives.
</p>
<p><code>deriv</code> and <code>deriv3</code> normally return an
<code><a href="base.html#topic+expression">expression</a></code> object whose evaluation returns the function
values with a <code>"gradient"</code> attribute containing the gradient
matrix.  If <code>hessian</code> is <code>TRUE</code> the evaluation also returns
a <code>"hessian"</code> attribute containing the Hessian array.
</p>
<p>If <code>function.arg</code> is not <code>NULL</code>, <code>deriv</code> and
<code>deriv3</code> return a function with those arguments rather than an
expression.
</p>


<h3>References</h3>

<p>Griewank, A.  and  Corliss, G. F. (1991)
<em>Automatic Differentiation of Algorithms: Theory, Implementation,
and Application</em>.
SIAM proceedings, Philadelphia.
</p>
<p>Bates, D. M. and Chambers, J. M. (1992)
<em>Nonlinear models.</em>
Chapter 10 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nlm">nlm</a></code> and <code><a href="#topic+optim">optim</a></code> for numeric minimization
which could make use of derivatives,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## formula argument :
dx2x &lt;- deriv(~ x^2, "x") ; dx2x
## Not run: expression({
         .value &lt;- x^2
         .grad &lt;- array(0, c(length(.value), 1), list(NULL, c("x")))
         .grad[, "x"] &lt;- 2 * x
         attr(.value, "gradient") &lt;- .grad
         .value
})
## End(Not run)
mode(dx2x)
x &lt;- -1:2
eval(dx2x)

## Something 'tougher':
trig.exp &lt;- expression(sin(cos(x + y^2)))
( D.sc &lt;- D(trig.exp, "x") )
all.equal(D(trig.exp[[1]], "x"), D.sc)

( dxy &lt;- deriv(trig.exp, c("x", "y")) )
y &lt;- 1
eval(dxy)
eval(D.sc)

## function returned:
deriv((y ~ sin(cos(x) * y)), c("x","y"), function.arg = TRUE)

## function with defaulted arguments:
(fx &lt;- deriv(y ~ b0 + b1 * 2^(-x/th), c("b0", "b1", "th"),
             function(b0, b1, th, x = 1:7){} ) )
fx(2, 3, 4)

## First derivative

D(expression(x^2), "x")
stopifnot(D(as.name("x"), "x") == 1)

## Higher derivatives
deriv3(y ~ b0 + b1 * 2^(-x/th), c("b0", "b1", "th"),
     c("b0", "b1", "th", "x") )

## Higher derivatives:
DD &lt;- function(expr, name, order = 1) {
   if(order &lt; 1) stop("'order' must be &gt;= 1")
   if(order == 1) D(expr, name)
   else DD(D(expr, name), name, order - 1)
}
DD(expression(sin(x^2)), "x", 3)
## showing the limits of the internal "simplify()" :
## Not run: 
-sin(x^2) * (2 * x) * 2 + ((cos(x^2) * (2 * x) * (2 * x) + sin(x^2) *
    2) * (2 * x) + sin(x^2) * (2 * x) * 2)

## End(Not run)

## New (R 3.4.0, 2017):
D(quote(log1p(x^2)), "x") ## log1p(x) = log(1 + x)
stopifnot(identical(
       D(quote(log1p(x^2)), "x"),
       D(quote(log(1+x^2)), "x")))
D(quote(expm1(x^2)), "x") ## expm1(x) = exp(x) - 1
stopifnot(identical(
       D(quote(expm1(x^2)), "x") -&gt; Dex1,
       D(quote(exp(x^2)-1), "x")),
       identical(Dex1, quote(exp(x^2) * (2 * x))))

D(quote(sinpi(x^2)), "x") ## sinpi(x) = sin(pi*x)
D(quote(cospi(x^2)), "x") ## cospi(x) = cos(pi*x)
D(quote(tanpi(x^2)), "x") ## tanpi(x) = tan(pi*x)

stopifnot(identical(D(quote(log2 (x^2)), "x"),
                    quote(2 * x/(x^2 * log(2)))),
          identical(D(quote(log10(x^2)), "x"),
                    quote(2 * x/(x^2 * log(10)))))

</code></pre>

<hr>
<h2 id='deviance'>Model Deviance</h2><span id='topic+deviance'></span>

<h3>Description</h3>

<p>Returns the deviance of a fitted model object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deviance(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deviance_+3A_object">object</code></td>
<td>
<p>an object for which the deviance is desired.</p>
</td></tr>
<tr><td><code id="deviance_+3A_...">...</code></td>
<td>
<p>additional optional argument.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function which can be used to extract deviances for
fitted models.  Consult the individual modeling functions for details
on how to use this function.
</p>


<h3>Value</h3>

<p>The value of the deviance extracted from the object <code>object</code>.
</p>


<h3>References</h3>

<p>Chambers, J. M. and Hastie, T. J. (1992)
<em>Statistical Models in S.</em>
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+df.residual">df.residual</a></code>,
<code><a href="#topic+extractAIC">extractAIC</a></code>,
<code><a href="#topic+glm">glm</a></code>,
<code><a href="#topic+lm">lm</a></code>.
</p>

<hr>
<h2 id='df.residual'>Residual Degrees-of-Freedom</h2><span id='topic+df.residual'></span>

<h3>Description</h3>

<p>Returns the residual degrees-of-freedom extracted from a fitted model
object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>df.residual(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="df.residual_+3A_object">object</code></td>
<td>
<p>an object for which the degrees-of-freedom are desired.</p>
</td></tr>
<tr><td><code id="df.residual_+3A_...">...</code></td>
<td>
<p>additional optional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function which can be used to extract residual
degrees-of-freedom for fitted models.  Consult the individual modeling
functions for details on how to use this function.
</p>
<p>The default method just extracts the <code>df.residual</code> component.
</p>


<h3>Value</h3>

<p>The value of the residual degrees-of-freedom extracted from the object
<code>x</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+deviance">deviance</a></code>, <code><a href="#topic+glm">glm</a></code>, <code><a href="#topic+lm">lm</a></code>.
</p>

<hr>
<h2 id='diffinv'>Discrete Integration: Inverse of Differencing</h2><span id='topic+diffinv'></span><span id='topic+diffinv.default'></span><span id='topic+diffinv.ts'></span>

<h3>Description</h3>

<p>Computes the inverse function of the lagged differences function
<code><a href="base.html#topic+diff">diff</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diffinv(x, ...)

## Default S3 method:
diffinv(x, lag = 1, differences = 1, xi, ...)
## S3 method for class 'ts'
diffinv(x, lag = 1, differences = 1, xi, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="diffinv_+3A_x">x</code></td>
<td>
<p>a numeric vector, matrix, or time series.</p>
</td></tr>
<tr><td><code id="diffinv_+3A_lag">lag</code></td>
<td>
<p>a scalar lag parameter.</p>
</td></tr>
<tr><td><code id="diffinv_+3A_differences">differences</code></td>
<td>
<p>an integer representing the order of the
difference.</p>
</td></tr>
<tr><td><code id="diffinv_+3A_xi">xi</code></td>
<td>
<p>a numeric vector, matrix, or time series containing the
initial values for the integrals.  If missing, zeros are used.</p>
</td></tr>
<tr><td><code id="diffinv_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>diffinv</code> is a generic function with methods for class <code>"ts"</code>
and <code>default</code> for vectors and matrices.
</p>
<p>Missing values are not handled.
</p>


<h3>Value</h3>

<p>A numeric vector, matrix, or time series (the latter for the
<code>"ts"</code> method) representing the discrete integral of <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>A. Trapletti</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+diff">diff</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>s &lt;- 1:10
d &lt;- diff(s)
diffinv(d, xi = 1)
</code></pre>

<hr>
<h2 id='dist'>Distance Matrix Computation</h2><span id='topic+dist'></span><span id='topic+print.dist'></span><span id='topic+format.dist'></span><span id='topic+labels.dist'></span><span id='topic+as.matrix.dist'></span><span id='topic+as.dist'></span><span id='topic+as.dist.default'></span>

<h3>Description</h3>

<p>This function computes and returns the distance matrix computed by
using the specified distance measure to compute the distances between
the rows of a data matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist(x, method = "euclidean", diag = FALSE, upper = FALSE, p = 2)

as.dist(m, diag = FALSE, upper = FALSE)
## Default S3 method:
as.dist(m, diag = FALSE, upper = FALSE)

## S3 method for class 'dist'
print(x, diag = NULL, upper = NULL,
      digits = getOption("digits"), justify = "none",
      right = TRUE, ...)

## S3 method for class 'dist'
as.matrix(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist_+3A_x">x</code></td>
<td>
<p>a numeric matrix, data frame or <code>"dist"</code> object.</p>
</td></tr>
<tr><td><code id="dist_+3A_method">method</code></td>
<td>
<p>the distance measure to be used.  This must be one of
<code>"euclidean"</code>, <code>"maximum"</code>, <code>"manhattan"</code>,
<code>"canberra"</code>, <code>"binary"</code> or <code>"minkowski"</code>.
Any unambiguous substring can be given.</p>
</td></tr>
<tr><td><code id="dist_+3A_diag">diag</code></td>
<td>
<p>logical value indicating whether the diagonal of the
distance matrix should be printed by <code>print.dist</code>.</p>
</td></tr>
<tr><td><code id="dist_+3A_upper">upper</code></td>
<td>
<p>logical value indicating whether the upper triangle of the
distance matrix should be printed by <code>print.dist</code>.</p>
</td></tr>
<tr><td><code id="dist_+3A_p">p</code></td>
<td>
<p>The power of the Minkowski distance.</p>
</td></tr>
<tr><td><code id="dist_+3A_m">m</code></td>
<td>
<p>An object with distance information to be converted to a
<code>"dist"</code> object.  For the default method, a <code>"dist"</code>
object, or a matrix (of distances) or an object which can be coerced
to such a matrix using <code><a href="base.html#topic+as.matrix">as.matrix</a>()</code>.  (Only the lower
triangle of the matrix is used, the rest is ignored).</p>
</td></tr>
<tr><td><code id="dist_+3A_digits">digits</code>, <code id="dist_+3A_justify">justify</code></td>
<td>
<p>passed to <code><a href="base.html#topic+format">format</a></code> inside of
<code>print()</code>.</p>
</td></tr>
<tr><td><code id="dist_+3A_right">right</code>, <code id="dist_+3A_...">...</code></td>
<td>
<p>further arguments, passed to other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Available distance measures are (written for two vectors <code class="reqn">x</code> and
<code class="reqn">y</code>):
</p>

<dl>
<dt><code>euclidean</code>:</dt><dd><p>Usual distance between the two vectors (2
norm aka <code class="reqn">L_2</code>), <code class="reqn">\sqrt{\sum_i (x_i - y_i)^2}</code>.</p>
</dd>
<dt><code>maximum</code>:</dt><dd><p>Maximum distance between two components of <code class="reqn">x</code>
and <code class="reqn">y</code> (supremum norm)</p>
</dd>
<dt><code>manhattan</code>:</dt><dd><p>Absolute distance between the two vectors (1 norm aka <code class="reqn">L_1</code>).</p>
</dd>
<dt><code>canberra</code>:</dt><dd>

<p><code class="reqn">\sum_i |x_i - y_i| / (|x_i| + |y_i|)</code>.
Terms with zero numerator and denominator are omitted from the sum
and treated as if the values were missing.
</p>
<p>This is intended for non-negative values (e.g., counts), in which
case the denominator can be written in various equivalent ways;
Originally, <span class="rlang"><b>R</b></span> used <code class="reqn">x_i + y_i</code>, then from 1998 to 2017,
<code class="reqn">|x_i + y_i|</code>, and then the correct <code class="reqn">|x_i| + |y_i|</code>.
</p>
</dd>
<dt><code>binary</code>:</dt><dd><p>(aka <em>asymmetric binary</em>): The vectors
are regarded as binary bits, so non-zero elements are &lsquo;on&rsquo;
and zero elements are &lsquo;off&rsquo;.  The distance is the
<em>proportion</em> of bits in which only one is on amongst those in
which at least one is on.
This also called &ldquo;Jaccard&rdquo; distance in some contexts.
Here, two all-zero observations have distance <code>0</code>, whereas in
traditional Jaccard definitions, the distance would be undefined for
that case and give <code><a href="base.html#topic+NaN">NaN</a></code> numerically.</p>
</dd>
<dt><code>minkowski</code>:</dt><dd><p>The <code class="reqn">p</code> norm, the <code class="reqn">p</code>-th root of the
sum of the <code class="reqn">p</code>-th powers of the differences of the components.</p>
</dd>
</dl>

<p>Missing values are allowed, and are excluded from all computations
involving the rows within which they occur.
Further, when <code>Inf</code> values are involved, all pairs of values are
excluded when their contribution to the distance gave <code>NaN</code> or
<code>NA</code>.
If some columns are excluded in calculating a Euclidean, Manhattan,
Canberra or Minkowski distance, the sum is scaled up proportionally to
the number of columns used.  If all pairs are excluded when
calculating a particular distance, the value is <code>NA</code>.
</p>
<p>The <code>"dist"</code> method of <code>as.matrix()</code> and <code>as.dist()</code>
can be used for conversion between objects of class <code>"dist"</code>
and conventional distance matrices.
</p>
<p><code>as.dist()</code> is a generic function.  Its default method handles
objects inheriting from class <code>"dist"</code>, or coercible to matrices
using <code><a href="base.html#topic+as.matrix">as.matrix</a>()</code>.  Support for classes representing
distances (also known as dissimilarities) can be added by providing an
<code><a href="base.html#topic+as.matrix">as.matrix</a>()</code> or, more directly, an <code>as.dist</code> method
for such a class.
</p>


<h3>Value</h3>

<p><code>dist</code> returns an object of class <code>"dist"</code>.
</p>
<p>The lower triangle of the distance matrix stored by columns in a
vector, say <code>do</code>. If <code>n</code> is the number of
observations, i.e., <code>n &lt;- attr(do, "Size")</code>, then
for <code class="reqn">i &lt; j \le n</code>, the dissimilarity between (row) i and j is
<code>do[n*(i-1) - i*(i-1)/2 + j-i]</code>.
The length of the vector is <code class="reqn">n*(n-1)/2</code>, i.e., of order <code class="reqn">n^2</code>.
</p>
<p>The object has the following attributes (besides <code>"class"</code> equal
to <code>"dist"</code>):
</p>
<table>
<tr><td><code>Size</code></td>
<td>
<p>integer, the number of observations in the dataset.</p>
</td></tr>
<tr><td><code>Labels</code></td>
<td>
<p>optionally, contains the labels, if any, of the
observations of the dataset.</p>
</td></tr>
<tr><td><code>Diag</code>, <code>Upper</code></td>
<td>
<p>logicals corresponding to the arguments <code>diag</code>
and <code>upper</code> above, specifying how the object should be printed.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>optionally, the <code><a href="base.html#topic+call">call</a></code> used to create the
object.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>optionally, the distance method used; resulting from
<code><a href="#topic+dist">dist</a>()</code>, the (<code><a href="base.html#topic+match.arg">match.arg</a>()</code>ed) <code>method</code>
argument.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Mardia, K. V., Kent, J. T. and Bibby, J. M. (1979)
<em>Multivariate Analysis.</em> Academic Press.
</p>
<p>Borg, I. and Groenen, P. (1997)
<em>Modern Multidimensional Scaling.  Theory and Applications.</em>
Springer.
</p>


<h3>See Also</h3>

<p><code><a href="cluster.html#topic+daisy">daisy</a></code> in the <a href="https://CRAN.R-project.org/package=cluster"><span class="pkg">cluster</span></a> package with more
possibilities in the case of <em>mixed</em> (continuous / categorical)
variables.
<code><a href="#topic+hclust">hclust</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

x &lt;- matrix(rnorm(100), nrow = 5)
dist(x)
dist(x, diag = TRUE)
dist(x, upper = TRUE)
m &lt;- as.matrix(dist(x))
d &lt;- as.dist(m)
stopifnot(d == dist(x))

## Use correlations between variables "as distance"
dd &lt;- as.dist((1 - cor(USJudgeRatings))/2)
round(1000 * dd) # (prints more nicely)
plot(hclust(dd)) # to see a dendrogram of clustered variables

## example of binary and canberra distances.
x &lt;- c(0, 0, 1, 1, 1, 1)
y &lt;- c(1, 0, 1, 1, 0, 1)
dist(rbind(x, y), method = "binary")
## answer 0.4 = 2/5
dist(rbind(x, y), method = "canberra")
## answer 2 * (6/5)

## To find the names
labels(eurodist)

## Examples involving "Inf" :
## 1)
x[6] &lt;- Inf
(m2 &lt;- rbind(x, y))
dist(m2, method = "binary")   # warning, answer 0.5 = 2/4
## These all give "Inf":
stopifnot(Inf == dist(m2, method =  "euclidean"),
          Inf == dist(m2, method =  "maximum"),
          Inf == dist(m2, method =  "manhattan"))
##  "Inf" is same as very large number:
x1 &lt;- x; x1[6] &lt;- 1e100
stopifnot(dist(cbind(x, y), method = "canberra") ==
    print(dist(cbind(x1, y), method = "canberra")))

## 2)
y[6] &lt;- Inf #-&gt; 6-th pair is excluded
dist(rbind(x, y), method = "binary"  )   # warning; 0.5
dist(rbind(x, y), method = "canberra"  ) # 3
dist(rbind(x, y), method = "maximum")    # 1
dist(rbind(x, y), method = "manhattan")  # 2.4
</code></pre>

<hr>
<h2 id='Distributions'>Distributions in the stats package</h2><span id='topic+distribution'></span><span id='topic+distributions'></span><span id='topic+Distributions'></span>

<h3>Description</h3>

<p>Density, cumulative distribution function, quantile function and random
variate generation for many standard probability distributions are
available in the <span class="pkg">stats</span> package.
</p>


<h3>Details</h3>

<p>The functions for the density/mass function, cumulative distribution
function, quantile function and random variate generation are named in the
form <code>dxxx</code>, <code>pxxx</code>, <code>qxxx</code> and <code>rxxx</code> respectively.
</p>
<p>For the beta distribution see <code><a href="#topic+dbeta">dbeta</a></code>.
</p>
<p>For the binomial (including Bernoulli) distribution see
<code><a href="#topic+dbinom">dbinom</a></code>.
</p>
<p>For the Cauchy distribution see <code><a href="#topic+dcauchy">dcauchy</a></code>.
</p>
<p>For the chi-squared distribution see <code><a href="#topic+dchisq">dchisq</a></code>.
</p>
<p>For the exponential distribution see <code><a href="#topic+dexp">dexp</a></code>.
</p>
<p>For the F distribution see <code><a href="#topic+df">df</a></code>.
</p>
<p>For the gamma distribution see <code><a href="#topic+dgamma">dgamma</a></code>.
</p>
<p>For the geometric distribution see <code><a href="#topic+dgeom">dgeom</a></code>.  (This is also
a special case of the negative binomial.)
</p>
<p>For the hypergeometric distribution see <code><a href="#topic+dhyper">dhyper</a></code>.
</p>
<p>For the log-normal distribution see <code><a href="#topic+dlnorm">dlnorm</a></code>.
</p>
<p>For the multinomial distribution see <code><a href="#topic+dmultinom">dmultinom</a></code>.
</p>
<p>For the negative binomial distribution see <code><a href="#topic+dnbinom">dnbinom</a></code>.
</p>
<p>For the normal distribution see <code><a href="#topic+dnorm">dnorm</a></code>.
</p>
<p>For the Poisson distribution see <code><a href="#topic+dpois">dpois</a></code>.
</p>
<p>For the Student's t distribution see <code><a href="#topic+dt">dt</a></code>.
</p>
<p>For the uniform distribution see <code><a href="#topic+dunif">dunif</a></code>.
</p>
<p>For the Weibull distribution see <code><a href="#topic+dweibull">dweibull</a></code>.
</p>
<p>For less common distributions of test statistics see
<code><a href="#topic+pbirthday">pbirthday</a></code>, <code><a href="#topic+dsignrank">dsignrank</a></code>,
<code><a href="#topic+ptukey">ptukey</a></code> and <code><a href="#topic+dwilcox">dwilcox</a></code> (and see the
&lsquo;See Also&rsquo; section of <code><a href="#topic+cor.test">cor.test</a></code>).
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+RNG">RNG</a></code> about random number generation in <span class="rlang"><b>R</b></span>.
</p>
<p>The CRAN task view on distributions,
<a href="https://CRAN.R-project.org/view=Distributions">https://CRAN.R-project.org/view=Distributions</a>,
mentioning several CRAN packages for additional distributions.
</p>

<hr>
<h2 id='dummy.coef'>Extract Coefficients in Original Coding</h2><span id='topic+dummy.coef'></span><span id='topic+dummy.coef.lm'></span><span id='topic+dummy.coef.aovlist'></span>

<h3>Description</h3>

<p>This extracts coefficients in terms of the original levels of the
coefficients rather than the coded variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dummy.coef(object, ...)

## S3 method for class 'lm'
dummy.coef(object, use.na = FALSE, ...)

## S3 method for class 'aovlist'
dummy.coef(object, use.na = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dummy.coef_+3A_object">object</code></td>
<td>
<p>a linear model fit.</p>
</td></tr>
<tr><td><code id="dummy.coef_+3A_use.na">use.na</code></td>
<td>
<p>logical flag for coefficients in a singular model. If
<code>use.na</code> is true, undetermined coefficients will be missing; if
false they will get one possible value.</p>
</td></tr>
<tr><td><code id="dummy.coef_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A fitted linear model has coefficients for the contrasts of the factor
terms, usually one less in number than the number of levels.  This
function re-expresses the coefficients in the original coding; as the
coefficients will have been fitted in the reduced basis, any implied
constraints (e.g., zero sum for <code>contr.helmert</code> or <code>contr.sum</code>)
will be respected.  There will be little point in using
<code>dummy.coef</code> for <code>contr.treatment</code> contrasts, as the missing
coefficients are by definition zero.
</p>
<p>The method used has some limitations, and will give incomplete results
for terms such as <code>poly(x, 2)</code>.  However, it is adequate for
its main purpose, <code>aov</code> models.
</p>


<h3>Value</h3>

<p>A list giving for each term the values of the coefficients. For a
multistratum <code>aov</code> model, such a list for each stratum.
</p>


<h3>Warning</h3>

<p>This function is intended for human inspection of the
output: it should not be used for calculations.  Use coded variables
for all calculations.
</p>
<p>The results differ from S for singular values, where S can be incorrect.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+aov">aov</a></code>, <code><a href="#topic+model.tables">model.tables</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>options(contrasts = c("contr.helmert", "contr.poly"))
## From Venables and Ripley (2002) p.165.
npk.aov &lt;- aov(yield ~ block + N*P*K, npk)
dummy.coef(npk.aov)

npk.aovE &lt;- aov(yield ~  N*P*K + Error(block), npk)
dummy.coef(npk.aovE)
</code></pre>

<hr>
<h2 id='ecdf'>Empirical Cumulative Distribution Function</h2><span id='topic+ecdf'></span><span id='topic+plot.ecdf'></span><span id='topic+print.ecdf'></span><span id='topic+summary.ecdf'></span><span id='topic+quantile.ecdf'></span>

<h3>Description</h3>

<p>Compute an empirical cumulative distribution function, with several
methods for plotting, printing and computing with such an
&ldquo;<abbr>ecdf</abbr>&rdquo; object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ecdf(x)

## S3 method for class 'ecdf'
plot(x, ..., ylab="Fn(x)", verticals = FALSE,
     col.01line = "gray70", pch = 19)

## S3 method for class 'ecdf'
print(x, digits= getOption("digits") - 2, ...)

## S3 method for class 'ecdf'
summary(object, ...)
## S3 method for class 'ecdf'
quantile(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ecdf_+3A_x">x</code>, <code id="ecdf_+3A_object">object</code></td>
<td>
<p>numeric vector of the observations for <code>ecdf</code>;  for
the methods, an object inheriting from class <code>"ecdf"</code>.</p>
</td></tr>
<tr><td><code id="ecdf_+3A_...">...</code></td>
<td>
<p>arguments to be passed to subsequent methods, e.g.,
<code><a href="#topic+plot.stepfun">plot.stepfun</a></code> for the <code>plot</code> method.</p>
</td></tr>
<tr><td><code id="ecdf_+3A_ylab">ylab</code></td>
<td>
<p>label for the y-axis.</p>
</td></tr>
<tr><td><code id="ecdf_+3A_verticals">verticals</code></td>
<td>
<p>see <code><a href="#topic+plot.stepfun">plot.stepfun</a></code>.</p>
</td></tr>
<tr><td><code id="ecdf_+3A_col.01line">col.01line</code></td>
<td>
<p>numeric or character specifying the color of the
horizontal lines at y = 0 and 1, see <code><a href="grDevices.html#topic+colors">colors</a></code>.</p>
</td></tr>
<tr><td><code id="ecdf_+3A_pch">pch</code></td>
<td>
<p>plotting character.</p>
</td></tr>
<tr><td><code id="ecdf_+3A_digits">digits</code></td>
<td>
<p>number of significant digits to use, see
<code><a href="base.html#topic+print">print</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The e.c.d.f. (empirical cumulative distribution function)
<code class="reqn">F_n</code> is a step function with jumps <code class="reqn">i/n</code> at
observation values, where <code class="reqn">i</code> is the number of tied observations
at that value.  Missing values are ignored.
</p>
<p>For observations
<code>x</code><code class="reqn">= (</code><code class="reqn">x_1,x_2</code>, ... <code class="reqn">x_n)</code>,
<code class="reqn">F_n</code> is the fraction of observations less or equal to <code class="reqn">t</code>,
i.e.,
</p>
<p style="text-align: center;"><code class="reqn">F_n(t) = \#\{x_i\le t\}\ / n
               = \frac1 n\sum_{i=1}^n \mathbf{1}_{[x_i \le t]}.</code>
</p>

<p>The function <code>plot.ecdf</code> which implements the <code><a href="graphics.html#topic+plot">plot</a></code>
method for <code>ecdf</code> objects, is implemented via a call to
<code><a href="#topic+plot.stepfun">plot.stepfun</a></code>; see its documentation.
</p>


<h3>Value</h3>

<p>For <code>ecdf</code>, a function of class <code>"ecdf"</code>, inheriting from the
<code>"<a href="#topic+stepfun">stepfun</a>"</code> class, and hence inheriting a
<code><a href="#topic+knots">knots</a>()</code> method.
</p>
<p>For the <code>summary</code> method, a summary of the knots of <code>object</code>
with a <code>"header"</code> attribute.
</p>
<p>The <code><a href="#topic+quantile">quantile</a>(obj, ...)</code> method computes the same quantiles as
<code>quantile(x, ...)</code> would where <code>x</code> is the original sample.
</p>


<h3>Note</h3>

<p>The objects of class <code>"ecdf"</code> are not intended to be used for
permanent storage and may change structure between versions of <span class="rlang"><b>R</b></span> (and
did at <span class="rlang"><b>R</b></span> 3.0.0).  They can usually be re-created by
</p>
<pre>    eval(attr(old_obj, "call"), environment(old_obj))</pre>
<p>since the data used is stored as part of the object's environment.
</p>


<h3>Author(s)</h3>

<p>Martin Maechler; fixes and new features by other R-core members.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stepfun">stepfun</a></code>, the more general class of step functions,
<code><a href="#topic+approxfun">approxfun</a></code> and <code><a href="#topic+splinefun">splinefun</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##-- Simple didactical  ecdf  example :
x &lt;- rnorm(12)
Fn &lt;- ecdf(x)
Fn     # a *function*
Fn(x)  # returns the percentiles for x
tt &lt;- seq(-2, 2, by = 0.1)
12 * Fn(tt) # Fn is a 'simple' function {with values k/12}
summary(Fn)
##--&gt; see below for graphics
knots(Fn)  # the unique data values {12 of them if there were no ties}

y &lt;- round(rnorm(12), 1); y[3] &lt;- y[1]
Fn12 &lt;- ecdf(y)
Fn12
knots(Fn12) # unique values (always less than 12!)
summary(Fn12)
summary.stepfun(Fn12)

## Advanced: What's inside the function closure?
ls(environment(Fn12))
## "f"     "method" "na.rm"  "nobs"   "x"     "y"    "yleft"  "yright"
utils::ls.str(environment(Fn12))
stopifnot(all.equal(quantile(Fn12), quantile(y)))

###----------------- Plotting --------------------------
require(graphics)

op &lt;- par(mfrow = c(3, 1), mgp = c(1.5, 0.8, 0), mar =  .1+c(3,3,2,1))

F10 &lt;- ecdf(rnorm(10))
summary(F10)

plot(F10)
plot(F10, verticals = TRUE, do.points = FALSE)

plot(Fn12 , lwd = 2) ; mtext("lwd = 2", adj = 1)
xx &lt;- unique(sort(c(seq(-3, 2, length.out = 201), knots(Fn12))))
lines(xx, Fn12(xx), col = "blue")
abline(v = knots(Fn12), lty = 2, col = "gray70")

plot(xx, Fn12(xx), type = "o", cex = .1)  #- plot.default {ugly}
plot(Fn12, col.hor = "red", add =  TRUE)  #- plot method
abline(v = knots(Fn12), lty = 2, col = "gray70")
## luxury plot
plot(Fn12, verticals = TRUE, col.points = "blue",
     col.hor = "red", col.vert = "bisque")

##-- this works too (automatic call to  ecdf(.)):
plot.ecdf(rnorm(24))
title("via  simple  plot.ecdf(x)", adj = 1)

par(op)
</code></pre>

<hr>
<h2 id='eff.aovlist'>Compute Efficiencies of Multistratum Analysis of Variance</h2><span id='topic+eff.aovlist'></span>

<h3>Description</h3>

<p>Computes the efficiencies of fixed-effect terms in an analysis of
variance model with multiple strata.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eff.aovlist(aovlist)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="eff.aovlist_+3A_aovlist">aovlist</code></td>
<td>

<p>The result of a call to <code>aov</code> with an <code>Error</code> term.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fixed-effect terms in an analysis of variance model with multiple strata
may be estimable in more than one stratum, in which case there is less
than complete information in each.  The efficiency for a term
is the fraction of the maximum possible precision (inverse variance)
obtainable by estimating in just that stratum.  Under the assumption
of balance, this is the same for all contrasts involving that term.
</p>
<p>This function is used to pick strata in which to estimate terms in
<code><a href="#topic+model.tables.aovlist">model.tables.aovlist</a></code> and
<code><a href="#topic+se.contrast.aovlist">se.contrast.aovlist</a></code>.
</p>
<p>In many cases terms will only occur in one stratum, when all the
efficiencies will be one: this is detected and no further calculations
are done.
</p>
<p>The calculation used requires orthogonal contrasts for each term, and
will throw an error if non-orthogonal contrasts (e.g., treatment
contrasts or an unbalanced design) are detected.
</p>


<h3>Value</h3>

<p>A matrix giving for each non-pure-error stratum (row) the efficiencies
for each fixed-effect term in the model.
</p>


<h3>References</h3>

<p>Heiberger, R. M. (1989)
<em>Computation for the Analysis of Designed Experiments</em>.  Wiley.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+aov">aov</a></code>, <code><a href="#topic+model.tables.aovlist">model.tables.aovlist</a></code>,
<code><a href="#topic+se.contrast.aovlist">se.contrast.aovlist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## An example from Yates (1932),
## a 2^3 design in 2 blocks replicated 4 times

Block &lt;- gl(8, 4)
A &lt;- factor(c(0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,
              0,1,0,1,0,1,0,1,0,1,0,1))
B &lt;- factor(c(0,0,1,1,0,0,1,1,0,1,0,1,1,0,1,0,0,0,1,1,
              0,0,1,1,0,0,1,1,0,0,1,1))
C &lt;- factor(c(0,1,1,0,1,0,0,1,0,0,1,1,0,0,1,1,0,1,0,1,
              1,0,1,0,0,0,1,1,1,1,0,0))
Yield &lt;- c(101, 373, 398, 291, 312, 106, 265, 450, 106, 306, 324, 449,
           272, 89, 407, 338, 87, 324, 279, 471, 323, 128, 423, 334,
           131, 103, 445, 437, 324, 361, 302, 272)
aovdat &lt;- data.frame(Block, A, B, C, Yield)

old &lt;- getOption("contrasts")
options(contrasts = c("contr.helmert", "contr.poly"))

(fit &lt;- aov(Yield ~ A*B*C + Error(Block), data = aovdat))

eff.aovlist(fit)
options(contrasts = old)
</code></pre>

<hr>
<h2 id='effects'>Effects from Fitted Model</h2><span id='topic+effects'></span><span id='topic+effects.lm'></span><span id='topic+effects.glm'></span>

<h3>Description</h3>

<p>Returns (orthogonal) effects from a fitted model, usually a linear
model. This is a generic function, but currently only has a methods for
objects inheriting from classes <code>"lm"</code> and <code>"glm"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>effects(object, ...)

## S3 method for class 'lm'
effects(object, set.sign = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="effects_+3A_object">object</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object; typically, the result of a model fitting function
such as <code><a href="#topic+lm">lm</a></code>.</p>
</td></tr>
<tr><td><code id="effects_+3A_set.sign">set.sign</code></td>
<td>
<p>logical. If <code>TRUE</code>, the sign of the effects
corresponding to coefficients in the model will be set to agree with the
signs of the corresponding coefficients, otherwise the sign is
arbitrary.</p>
</td></tr>
<tr><td><code id="effects_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a linear model fitted by <code><a href="#topic+lm">lm</a></code> or <code><a href="#topic+aov">aov</a></code>,
the effects are the uncorrelated single-degree-of-freedom values
obtained by projecting the data onto the successive orthogonal
subspaces generated by the QR decomposition during the fitting
process. The first <code class="reqn">r</code> (the rank of the model) are associated with
coefficients and the remainder span the space of residuals (but are
not associated with particular residuals).
</p>
<p>Empty models do not have effects.
</p>


<h3>Value</h3>

<p>A (named) numeric vector of the same length as
<code><a href="#topic+residuals">residuals</a></code>, or a matrix if there were multiple responses
in the fitted model, in either case of class <code>"coef"</code>.
</p>
<p>The first <code class="reqn">r</code> rows are labelled by the corresponding coefficients,
and the remaining rows are unlabelled.  Note that in rank-deficient
models the corresponding coefficients will be in a different
order if pivoting occurred.
</p>


<h3>References</h3>

<p>Chambers, J. M. and Hastie, T. J. (1992)
<em>Statistical Models in S.</em>
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coef">coef</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- c(1:3, 7, 5)
x &lt;- c(1:3, 6:7)
( ee &lt;- effects(lm(y ~ x)) )
c( round(ee - effects(lm(y+10 ~ I(x-3.8))), 3) )
# just the first is different
</code></pre>

<hr>
<h2 id='embed'>Embedding a Time Series</h2><span id='topic+embed'></span>

<h3>Description</h3>

<p>Embeds the time series <code>x</code> into a low-dimensional
Euclidean space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>embed (x, dimension = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="embed_+3A_x">x</code></td>
<td>
<p>a numeric vector, matrix, or time series.</p>
</td></tr>
<tr><td><code id="embed_+3A_dimension">dimension</code></td>
<td>
<p>a scalar representing the embedding dimension.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each row of the resulting matrix consists of sequences
<code>x[t]</code>, <code>x[t-1]</code>, ..., <code>x[t-dimension+1]</code>, where
<code>t</code> is the original index of <code>x</code>. If <code>x</code> is a matrix,
i.e., <code>x</code> contains more than one variable, then <code>x[t]</code>
consists of the <code>t</code>-th observation on each variable.
</p>


<h3>Value</h3>

<p>A matrix containing the embedded time series <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>A. Trapletti, B.D. Ripley</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 1:10
embed (x, 3)
</code></pre>

<hr>
<h2 id='expand.model.frame'>Add new variables to a model frame</h2><span id='topic+expand.model.frame'></span>

<h3>Description</h3>

<p>Evaluates new variables as if they had been part of the formula of the
specified model.  This ensures that the same <code>na.action</code> and
<code>subset</code> arguments are applied and allows, for example, <code>x</code>
to be recovered for a model using <code>sin(x)</code> as a predictor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>expand.model.frame(model, extras,
                   envir = environment(formula(model)),
                   na.expand = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="expand.model.frame_+3A_model">model</code></td>
<td>
<p>a fitted model</p>
</td></tr>
<tr><td><code id="expand.model.frame_+3A_extras">extras</code></td>
<td>
<p>one-sided formula or vector of character strings
describing new variables to be added</p>
</td></tr>
<tr><td><code id="expand.model.frame_+3A_envir">envir</code></td>
<td>
<p>an environment to evaluate things in</p>
</td></tr>
<tr><td><code id="expand.model.frame_+3A_na.expand">na.expand</code></td>
<td>
<p>logical; see below</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>na.expand = FALSE</code> then <code>NA</code> values in the extra variables
will be passed to the <code>na.action</code> function used in
<code>model</code>.  This may result in a shorter data frame (with
<code><a href="#topic+na.omit">na.omit</a></code>) or an error (with <code><a href="#topic+na.fail">na.fail</a></code>).  If
<code>na.expand = TRUE</code> the returned data frame will have precisely the
same rows as <code>model.frame(model)</code>, but the columns corresponding to
the extra variables may contain <code>NA</code>.
</p>


<h3>Value</h3>

<p>A data frame.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+model.frame">model.frame</a></code>, <code><a href="#topic+predict">predict</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(log(Volume) ~ log(Girth) + log(Height), data = trees)
expand.model.frame(model, ~ Girth) # prints data.frame like

dd &lt;- data.frame(x = 1:5, y = rnorm(5), z = c(1,2,NA,4,5))
model &lt;- glm(y ~ x, data = dd, subset = 1:4, na.action = na.omit)
expand.model.frame(model, "z", na.expand = FALSE) # = default
expand.model.frame(model, "z", na.expand = TRUE)
</code></pre>

<hr>
<h2 id='Exponential'>The Exponential Distribution</h2><span id='topic+Exponential'></span><span id='topic+dexp'></span><span id='topic+pexp'></span><span id='topic+qexp'></span><span id='topic+rexp'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the exponential distribution with rate <code>rate</code>
(i.e., mean <code>1/rate</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dexp(x, rate = 1, log = FALSE)
pexp(q, rate = 1, lower.tail = TRUE, log.p = FALSE)
qexp(p, rate = 1, lower.tail = TRUE, log.p = FALSE)
rexp(n, rate = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Exponential_+3A_x">x</code>, <code id="Exponential_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="Exponential_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="Exponential_+3A_n">n</code></td>
<td>
<p>number of observations. If <code>length(n) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="Exponential_+3A_rate">rate</code></td>
<td>
<p>vector of rates.</p>
</td></tr>
<tr><td><code id="Exponential_+3A_log">log</code>, <code id="Exponential_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="Exponential_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>rate</code> is not specified, it assumes the default value of
<code>1</code>.
</p>
<p>The exponential distribution with rate <code class="reqn">\lambda</code> has density
</p>
<p style="text-align: center;"><code class="reqn">f(x) = \lambda {e}^{- \lambda x}</code>
</p>
<p> for <code class="reqn">x \ge 0</code>.
</p>


<h3>Value</h3>

<p><code>dexp</code> gives the density,
<code>pexp</code> gives the distribution function,
<code>qexp</code> gives the quantile function, and
<code>rexp</code> generates random deviates.
</p>
<p>The length of the result is determined by <code>n</code> for
<code>rexp</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.  
</p>
<p>The numerical arguments other than <code>n</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.
</p>


<h3>Note</h3>

<p>The cumulative hazard <code class="reqn">H(t) = - \log(1 - F(t))</code>
is <code>-pexp(t, r, lower = FALSE, log = TRUE)</code>.
</p>


<h3>Source</h3>

<p><code>dexp</code>, <code>pexp</code> and <code>qexp</code> are all calculated
from numerically stable versions of the definitions.
</p>
<p><code>rexp</code> uses
</p>
<p>Ahrens, J. H. and Dieter, U. (1972).
Computer methods for sampling from the exponential and normal distributions.
<em>Communications of the ACM</em>, <b>15</b>, 873&ndash;882.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
<em>Continuous Univariate Distributions</em>, volume 1, chapter 19.
Wiley, New York.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+exp">exp</a></code> for the exponential function.
</p>
<p><a href="#topic+Distributions">Distributions</a> for other standard distributions, including
<code><a href="#topic+dgamma">dgamma</a></code> for the gamma distribution and
<code><a href="#topic+dweibull">dweibull</a></code> for the Weibull distribution, both of which
generalize the exponential.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dexp(1) - exp(-1) #-&gt; 0

## a fast way to generate *sorted*  U[0,1]  random numbers:
rsunif &lt;- function(n) { n1 &lt;- n+1
   cE &lt;- cumsum(rexp(n1)); cE[seq_len(n)]/cE[n1] }
plot(rsunif(1000), ylim=0:1, pch=".")
abline(0,1/(1000+1), col=adjustcolor(1, 0.5))
</code></pre>

<hr>
<h2 id='extractAIC'>Extract AIC from a Fitted Model</h2><span id='topic+extractAIC'></span>

<h3>Description</h3>

<p>Computes the (generalized) Akaike <b>A</b>n <b>I</b>nformation
<b>C</b>riterion for a fitted parametric model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extractAIC(fit, scale, k = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extractAIC_+3A_fit">fit</code></td>
<td>
<p>fitted model, usually the result of a fitter like
<code><a href="#topic+lm">lm</a></code>.</p>
</td></tr>
<tr><td><code id="extractAIC_+3A_scale">scale</code></td>
<td>
<p>optional numeric specifying the scale parameter of the
model, see <code>scale</code> in <code><a href="#topic+step">step</a></code>.  Currently only used
in the <code>"lm"</code> method, where <code>scale</code> specifies the estimate
of the error variance, and <code>scale = 0</code> indicates that it is to
be estimated by maximum likelihood.
</p>
</td></tr>
<tr><td><code id="extractAIC_+3A_k">k</code></td>
<td>
<p>numeric specifying the &lsquo;weight&rsquo; of the
<em>equivalent degrees of freedom</em> (<code class="reqn">\equiv</code> <code>edf</code>)
part in the AIC formula.</p>
</td></tr>
<tr><td><code id="extractAIC_+3A_...">...</code></td>
<td>
<p>further arguments (currently unused in base <span class="rlang"><b>R</b></span>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function, with methods in base <span class="rlang"><b>R</b></span> for classes
<code>"aov"</code>, <code>"glm"</code> and <code>"lm"</code> as well as for
<code>"negbin"</code> (package <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a>) and <code>"coxph"</code> and
<code>"survreg"</code> (package <a href="https://CRAN.R-project.org/package=survival"><span class="pkg">survival</span></a>).
</p>
<p>The criterion used is
</p>
<p style="text-align: center;"><code class="reqn">AIC = - 2\log L +  k \times \mbox{edf},</code>
</p>

<p>where <code class="reqn">L</code> is the likelihood and <code>edf</code> the equivalent degrees
of freedom (i.e., the number of free parameters for usual parametric
models) of <code>fit</code>.
</p>
<p>For linear models with unknown scale (i.e., for <code><a href="#topic+lm">lm</a></code> and
<code><a href="#topic+aov">aov</a></code>), <code class="reqn">-2\log L</code> is computed from the
<em>deviance</em> and uses a different additive constant to
<code><a href="#topic+logLik">logLik</a></code> and hence <code><a href="#topic+AIC">AIC</a></code>.  If <code class="reqn">RSS</code>
denotes the (weighted) residual sum of squares then <code>extractAIC</code>
uses for <code class="reqn">- 2\log L</code> the formulae <code class="reqn">RSS/s - n</code> (corresponding
to Mallows' <code class="reqn">C_p</code>) in the case of known scale <code class="reqn">s</code> and
<code class="reqn">n \log (RSS/n)</code> for unknown scale.
<code><a href="#topic+AIC">AIC</a></code> only handles unknown scale and uses the formula
<code class="reqn">n \log (RSS/n) + n + n \log 2\pi - \sum \log w</code>
where <code class="reqn">w</code> are the weights.  Further <code>AIC</code> counts the scale
estimation as a parameter in the <code>edf</code> and <code>extractAIC</code> does not.
</p>
<p>For <code>glm</code> fits the family's <code>aic()</code> function is used to
compute the AIC: see the note under <code>logLik</code> about the
assumptions this makes.
</p>
<p><code>k = 2</code> corresponds to the traditional AIC, using <code>k =
    log(n)</code> provides the BIC (Bayesian <abbr>IC</abbr>) instead.
</p>
<p>Note that the methods for this function may differ in their
assumptions from those of methods for <code><a href="#topic+AIC">AIC</a></code> (usually
<em>via</em> a method for <code><a href="#topic+logLik">logLik</a></code>).  We have already
mentioned the case of <code>"lm"</code> models with estimated scale, and
there are similar issues in the <code>"glm"</code> and <code>"negbin"</code>
methods where the dispersion parameter may or may not be taken as
&lsquo;free&rsquo;.  This is immaterial as <code>extractAIC</code> is only used
to compare models of the same class (where only differences in AIC
values are considered).
</p>


<h3>Value</h3>

<p>A numeric vector of length 2, with first and second elements giving
</p>
<table>
<tr><td><code>edf</code></td>
<td>
<p>the &lsquo;<b>e</b>quivalent <b>d</b>egrees of <b>f</b>reedom&rsquo;
for the fitted model <code>fit</code>.</p>
</td></tr>
<tr><td><code>AIC</code></td>
<td>
<p>the (generalized) Akaike Information Criterion for <code>fit</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function is used in <code><a href="#topic+add1">add1</a></code>, <code><a href="#topic+drop1">drop1</a></code>
and <code><a href="#topic+step">step</a></code> and the similar functions in package
<a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a> from which it was adopted.
</p>


<h3>Author(s)</h3>

<p>B. D. Ripley</p>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em>
New York: Springer (4th ed).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+AIC">AIC</a></code>, <code><a href="#topic+deviance">deviance</a></code>, <code><a href="#topic+add1">add1</a></code>,
<code><a href="#topic+step">step</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
utils::example(glm)
extractAIC(glm.D93)  #&gt;&gt;  5  15.129
</code></pre>

<hr>
<h2 id='factanal'>Factor Analysis</h2><span id='topic+factanal'></span>

<h3>Description</h3>

<p>Perform maximum-likelihood factor analysis on a covariance matrix or
data matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>factanal(x, factors, data = NULL, covmat = NULL, n.obs = NA,
         subset, na.action, start = NULL,
         scores = c("none", "regression", "Bartlett"),
         rotation = "varimax", control = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="factanal_+3A_x">x</code></td>
<td>
<p>A formula or a numeric matrix or an object that can be
coerced to a numeric matrix.</p>
</td></tr>
<tr><td><code id="factanal_+3A_factors">factors</code></td>
<td>
<p>The number of factors to be fitted.</p>
</td></tr>
<tr><td><code id="factanal_+3A_data">data</code></td>
<td>
<p>An optional data frame (or similar: see
<code><a href="#topic+model.frame">model.frame</a></code>), used only if <code>x</code> is a formula.  By
default the variables are taken from <code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="factanal_+3A_covmat">covmat</code></td>
<td>
<p>A covariance matrix, or a covariance list as returned by
<code><a href="#topic+cov.wt">cov.wt</a></code>.  Of course, correlation matrices are covariance
matrices.</p>
</td></tr>
<tr><td><code id="factanal_+3A_n.obs">n.obs</code></td>
<td>
<p>The number of observations, used if <code>covmat</code> is a
covariance matrix.</p>
</td></tr>
<tr><td><code id="factanal_+3A_subset">subset</code></td>
<td>
<p>A specification of the cases to be used, if <code>x</code> is
used as a matrix or formula.</p>
</td></tr>
<tr><td><code id="factanal_+3A_na.action">na.action</code></td>
<td>
<p>The <code>na.action</code> to be used if <code>x</code> is
used as a formula.</p>
</td></tr>
<tr><td><code id="factanal_+3A_start">start</code></td>
<td>
<p><code>NULL</code> or a matrix of starting values, each column
giving an initial set of uniquenesses.</p>
</td></tr>
<tr><td><code id="factanal_+3A_scores">scores</code></td>
<td>
<p>Type of scores to produce, if any.  The default is none,
<code>"regression"</code> gives Thompson's scores, <code>"Bartlett"</code> given
Bartlett's weighted least-squares scores. Partial matching allows
these names to be abbreviated.</p>
</td></tr>
<tr><td><code id="factanal_+3A_rotation">rotation</code></td>
<td>
<p>character. <code>"none"</code> or the name of a function
to be used to rotate the factors: it will be called with first
argument the loadings matrix, and should return a list with component
<code>loadings</code> giving the rotated loadings, or just the rotated loadings.</p>
</td></tr>
<tr><td><code id="factanal_+3A_control">control</code></td>
<td>
<p>A list of control values,
</p>

<dl>
<dt>nstart</dt><dd><p>The number of starting values to be tried if
<code>start = NULL</code>. Default 1.</p>
</dd>
<dt>trace</dt><dd><p>logical. Output tracing information? Default <code>FALSE</code>.</p>
</dd>
<dt>lower</dt><dd><p>The lower bound for uniquenesses during
optimization. Should be &gt; 0. Default 0.005.</p>
</dd>
<dt>opt</dt><dd><p>A list of control values to be passed to
<code><a href="#topic+optim">optim</a></code>'s <code>control</code> argument.</p>
</dd>
<dt>rotate</dt><dd><p>a list of additional arguments for the rotation function.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="factanal_+3A_...">...</code></td>
<td>
<p>Components of <code>control</code> can also be supplied as
named arguments to <code>factanal</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The factor analysis model is
</p>
<p style="text-align: center;"><code class="reqn">x = \Lambda f + e</code>
</p>

<p>for a <code class="reqn">p</code>&ndash;element vector <code class="reqn">x</code>, a <code class="reqn">p \times k</code>
matrix <code class="reqn">\Lambda</code> of <em>loadings</em>, a <code class="reqn">k</code>&ndash;element vector
<code class="reqn">f</code> of <em>scores</em> and a <code class="reqn">p</code>&ndash;element vector <code class="reqn">e</code> of
errors.  None of the components other than <code class="reqn">x</code> is observed, but
the major restriction is that the scores be uncorrelated and of unit
variance, and that the errors be independent with variances
<code class="reqn">\Psi</code>, the <em>uniquenesses</em>.  It is also common to
scale the observed variables to unit variance, and done in this function.
</p>
<p>Thus factor analysis is in essence a model for the correlation matrix
of <code class="reqn">x</code>,
</p>
<p style="text-align: center;"><code class="reqn">\Sigma = \Lambda\Lambda^\prime + \Psi</code>
</p>

<p>There is still some indeterminacy in the model for it is unchanged
if <code class="reqn">\Lambda</code> is replaced by <code class="reqn">G \Lambda</code> for
any orthogonal matrix <code class="reqn">G</code>.  Such matrices <code class="reqn">G</code> are known as
<em>rotations</em> (although the term is applied also to non-orthogonal
invertible matrices).
</p>
<p>If <code>covmat</code> is supplied it is used.  Otherwise <code>x</code> is used
if it is a matrix, or a formula <code>x</code> is used with <code>data</code> to
construct a model matrix, and that is used to construct a covariance
matrix.  (It makes no sense for the formula to have a response, and
all the variables must be numeric.)  Once a covariance matrix is found
or calculated from <code>x</code>, it is converted to a correlation matrix
for analysis.  The correlation matrix is returned as component
<code>correlation</code> of the result.
</p>
<p>The fit is done by optimizing the log likelihood assuming multivariate
normality over the uniquenesses.  (The maximizing loadings for given
uniquenesses can be found analytically:
Lawley &amp; Maxwell (1971, p. 27).)
All the starting values supplied in <code>start</code> are tried
in turn and the best fit obtained is used.  If <code>start = NULL</code>
then the first fit is started at the value suggested by
Jreskog (1963) and given by
Lawley &amp; Maxwell (1971, p. 31), and then <code>control$nstart - 1</code> other values are
tried, randomly selected as equal values of the uniquenesses.
</p>
<p>The uniquenesses are technically constrained to lie in <code class="reqn">[0, 1]</code>,
but near-zero values are problematical, and the optimization is
done with a lower bound of <code>control$lower</code>, default 0.005
(Lawley &amp; Maxwell, 1971, p. 32).
</p>
<p>Scores can only be produced if a data matrix is supplied and used.
The first method is the regression method of Thomson (1951), the
second the weighted least squares method of Bartlett (1937, 8).
Both are estimates of the unobserved scores <code class="reqn">f</code>.  Thomson's method
regresses (in the population) the unknown <code class="reqn">f</code> on <code class="reqn">x</code> to yield
</p>
<p style="text-align: center;"><code class="reqn">\hat f = \Lambda^\prime \Sigma^{-1} x</code>
</p>

<p>and then substitutes the sample estimates of the quantities on the
right-hand side.  Bartlett's method minimizes the sum of squares of
standardized errors over the choice of <code class="reqn">f</code>, given (the fitted)
<code class="reqn">\Lambda</code>.
</p>
<p>If <code>x</code> is a formula then the standard <code>NA</code>-handling is
applied to the scores (if requested): see <code><a href="#topic+napredict">napredict</a></code>.
</p>
<p>The <code>print</code> method (documented under <code><a href="#topic+loadings">loadings</a></code>)
follows the factor analysis convention of drawing attention to the
patterns of the results, so the default precision is three decimal
places, and small loadings are suppressed.
</p>


<h3>Value</h3>

<p>An object of class <code>"factanal"</code> with components
</p>
<table>
<tr><td><code>loadings</code></td>
<td>
<p>A matrix of loadings, one column for each factor.  The
factors are ordered in decreasing order of sums of squares of
loadings, and given the sign that will make the sum of the loadings
positive.  This is of class <code>"loadings"</code>: see
<code><a href="#topic+loadings">loadings</a></code> for its <code>print</code> method.</p>
</td></tr>
<tr><td><code>uniquenesses</code></td>
<td>
<p>The uniquenesses computed.</p>
</td></tr>
<tr><td><code>correlation</code></td>
<td>
<p>The correlation matrix used.</p>
</td></tr>
<tr><td><code>criteria</code></td>
<td>
<p>The results of the optimization: the value of the
criterion (a linear function of the negative log-likelihood) and information on the iterations used.</p>
</td></tr>
<tr><td><code>factors</code></td>
<td>
<p>The argument <code>factors</code>.</p>
</td></tr>
<tr><td><code>dof</code></td>
<td>
<p>The number of degrees of freedom of the factor analysis model.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>The method: always <code>"mle"</code>.</p>
</td></tr>
<tr><td><code>rotmat</code></td>
<td>
<p>The rotation matrix if relevant.</p>
</td></tr>
<tr><td><code>scores</code></td>
<td>
<p>If requested, a matrix of scores.  <code>napredict</code> is
applied to handle the treatment of values omitted by the <code>na.action</code>.</p>
</td></tr>
<tr><td><code>n.obs</code></td>
<td>
<p>The number of observations if available, or <code>NA</code>.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>na.action</code></td>
<td>
<p>If relevant.</p>
</td></tr>
<tr><td><code>STATISTIC</code>, <code>PVAL</code></td>
<td>
<p>The significance-test statistic and P value, if
it can be computed.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>There are so many variations on factor analysis that it is hard to
compare output from different programs.  Further, the optimization in
maximum likelihood factor analysis is hard, and many other examples we
compared had less good fits than produced by this function.  In
particular, solutions which are &lsquo;Heywood cases&rsquo; (with one or more
uniquenesses essentially zero) are much more common than most texts
and some other programs would lead one to believe.
</p>


<h3>References</h3>

<p>Bartlett, M. S. (1937).
The statistical conception of mental factors.
<em>British Journal of Psychology</em>, <b>28</b>, 97&ndash;104.
<a href="https://doi.org/10.1111/j.2044-8295.1937.tb00863.x">doi:10.1111/j.2044-8295.1937.tb00863.x</a>.
</p>
<p>Bartlett, M. S. (1938).
Methods of estimating mental factors.
<em>Nature</em>, <b>141</b>, 609&ndash;610.
<a href="https://doi.org/10.1038/141246a0">doi:10.1038/141246a0</a>.
</p>
<p>Jreskog, K. G. (1963).
<em>Statistical Estimation in Factor Analysis</em>.
Almqvist and Wicksell.
</p>
<p>Lawley, D. N. and Maxwell, A. E. (1971).
<em>Factor Analysis as a Statistical Method</em>. Second edition.
Butterworths.
</p>
<p>Thomson, G. H. (1951).
<em>The Factorial Analysis of Human Ability</em>.
London University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+loadings">loadings</a></code> (which explains some details of the
<code>print</code> method), <code><a href="#topic+varimax">varimax</a></code>, <code><a href="#topic+princomp">princomp</a></code>,
<code><a href="datasets.html#topic+ability.cov">ability.cov</a></code>, <code><a href="datasets.html#topic+Harman23.cor">Harman23.cor</a></code>,
<code><a href="datasets.html#topic+Harman74.cor">Harman74.cor</a></code>.
</p>
<p>Other rotation methods are available in various contributed packages,
including <a href="https://CRAN.R-project.org/package=GPArotation"><span class="pkg">GPArotation</span></a> and <a href="https://CRAN.R-project.org/package=psych"><span class="pkg">psych</span></a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># A little demonstration, v2 is just v1 with noise,
# and same for v4 vs. v3 and v6 vs. v5
# Last four cases are there to add noise
# and introduce a positive manifold (g factor)
v1 &lt;- c(1,1,1,1,1,1,1,1,1,1,3,3,3,3,3,4,5,6)
v2 &lt;- c(1,2,1,1,1,1,2,1,2,1,3,4,3,3,3,4,6,5)
v3 &lt;- c(3,3,3,3,3,1,1,1,1,1,1,1,1,1,1,5,4,6)
v4 &lt;- c(3,3,4,3,3,1,1,2,1,1,1,1,2,1,1,5,6,4)
v5 &lt;- c(1,1,1,1,1,3,3,3,3,3,1,1,1,1,1,6,4,5)
v6 &lt;- c(1,1,1,2,1,3,3,3,4,3,1,1,1,2,1,6,5,4)
m1 &lt;- cbind(v1,v2,v3,v4,v5,v6)
cor(m1)
factanal(m1, factors = 3) # varimax is the default
factanal(m1, factors = 3, rotation = "promax")
# The following shows the g factor as PC1
prcomp(m1) # signs may depend on platform

## formula interface
factanal(~v1+v2+v3+v4+v5+v6, factors = 3,
         scores = "Bartlett")$scores

## a realistic example from Bartholomew (1987, pp. 61-65)
utils::example(ability.cov)
</code></pre>

<hr>
<h2 id='factor.scope'>Compute Allowed Changes in Adding to or Dropping from a Formula</h2><span id='topic+add.scope'></span><span id='topic+drop.scope'></span><span id='topic+factor.scope'></span>

<h3>Description</h3>

<p><code>add.scope</code> and <code>drop.scope</code> compute those terms that can be
individually added to or dropped from a model while respecting the
hierarchy of terms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add.scope(terms1, terms2)

drop.scope(terms1, terms2)

factor.scope(factor, scope)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="factor.scope_+3A_terms1">terms1</code></td>
<td>
<p>the terms or formula for the base model.</p>
</td></tr>
<tr><td><code id="factor.scope_+3A_terms2">terms2</code></td>
<td>
<p>the terms or formula for the upper (<code>add.scope</code>) or
lower (<code>drop.scope</code>) scope. If missing for <code>drop.scope</code> it is
taken to be the null formula, so all terms (except any intercept) are
candidates to be dropped.</p>
</td></tr>
<tr><td><code id="factor.scope_+3A_factor">factor</code></td>
<td>
<p>the <code>"factor"</code> attribute of the terms of the base object.</p>
</td></tr>
<tr><td><code id="factor.scope_+3A_scope">scope</code></td>
<td>
<p>a list with one or both components <code>drop</code> and
<code>add</code> giving the <code>"factor"</code> attribute of the lower and
upper scopes respectively.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>factor.scope</code> is not intended to be called directly by users.
</p>


<h3>Value</h3>

<p>For <code>add.scope</code> and <code>drop.scope</code> a character vector of
terms labels.  For <code>factor.scope</code>, a list with components
<code>drop</code> and <code>add</code>, character vectors of terms labels.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+add1">add1</a></code>, <code><a href="#topic+drop1">drop1</a></code>,
<code><a href="#topic+aov">aov</a></code>, <code><a href="#topic+lm">lm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>add.scope( ~ a + b + c + a:b,  ~ (a + b + c)^3)
# [1] "a:c" "b:c"
drop.scope( ~ a + b + c + a:b)
# [1] "c"   "a:b"
</code></pre>

<hr>
<h2 id='family'>Family Objects for Models</h2><span id='topic+family'></span><span id='topic+binomial'></span><span id='topic+gaussian'></span><span id='topic+Gamma'></span><span id='topic+inverse.gaussian'></span><span id='topic+poisson'></span><span id='topic+quasi'></span><span id='topic+quasibinomial'></span><span id='topic+quasipoisson'></span>

<h3>Description</h3>

<p>Family objects provide a convenient way to specify the details of the
models used by functions such as <code><a href="#topic+glm">glm</a></code>.  See the
documentation for <code><a href="#topic+glm">glm</a></code> for the details on how such model
fitting takes place.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>family(object, ...)

binomial(link = "logit")
gaussian(link = "identity")
Gamma(link = "inverse")
inverse.gaussian(link = "1/mu^2")
poisson(link = "log")
quasi(link = "identity", variance = "constant")
quasibinomial(link = "logit")
quasipoisson(link = "log")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="family_+3A_link">link</code></td>
<td>
<p>a specification for the model link function.  This can be
a name/expression, a literal character string, a length-one character
vector, or an object of class
<code>"<a href="#topic+make.link">link-glm</a>"</code> (such as generated by
<code><a href="#topic+make.link">make.link</a></code>) provided it is not specified
<em>via</em> one of the standard names given next.
</p>
<p>The <code>gaussian</code> family accepts the links (as names)
<code>identity</code>, <code>log</code> and <code>inverse</code>;
the <code>binomial</code> family the links <code>logit</code>,
<code>probit</code>, <code>cauchit</code>, (corresponding to logistic,
normal and Cauchy CDFs respectively) <code>log</code> and
<code>cloglog</code> (complementary log-log);
the <code>Gamma</code> family the links <code>inverse</code>, <code>identity</code>
and <code>log</code>;
the <code>poisson</code> family the links <code>log</code>, <code>identity</code>,
and <code>sqrt</code>; and the <code>inverse.gaussian</code> family the links
<code>1/mu^2</code>, <code>inverse</code>, <code>identity</code>
and <code>log</code>.
</p>
<p>The <code>quasi</code> family accepts the links <code>logit</code>, <code>probit</code>,
<code>cloglog</code>,  <code>identity</code>, <code>inverse</code>,
<code>log</code>, <code>1/mu^2</code> and <code>sqrt</code>, and
the function <code><a href="#topic+power">power</a></code> can be used to create a
power link function.
</p>
</td></tr>
<tr><td><code id="family_+3A_variance">variance</code></td>
<td>
<p>for all families other than <code>quasi</code>, the variance
function is determined by the family.  The <code>quasi</code> family will
accept the literal character string (or unquoted as a name/expression)
specifications <code>"constant"</code>, <code>"mu(1-mu)"</code>, <code>"mu"</code>,
<code>"mu^2"</code> and <code>"mu^3"</code>, a length-one character vector
taking one of those values, or a list containing components
<code>varfun</code>, <code>validmu</code>, <code>dev.resids</code>, <code>initialize</code>
and <code>name</code>.
</p>
</td></tr>
<tr><td><code id="family_+3A_object">object</code></td>
<td>
<p>the function <code>family</code> accesses the <code>family</code>
objects which are stored within objects created by modelling
functions (e.g., <code>glm</code>).</p>
</td></tr>
<tr><td><code id="family_+3A_...">...</code></td>
<td>
<p>further arguments passed to methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>family</code> is a generic function with methods for classes
<code>"glm"</code> and <code>"lm"</code> (the latter returning <code>gaussian()</code>).
</p>
<p>For the <code>binomial</code> and <code>quasibinomial</code> families the response
can be specified in one of three ways:
</p>

<ol>
<li><p> As a factor: &lsquo;success&rsquo; is interpreted as the factor not
having the first level (and hence usually of having the second level).
</p>
</li>
<li><p> As a numerical vector with values  between <code>0</code> and
<code>1</code>, interpreted as the proportion of successful cases (with the
total number of cases given by the <code>weights</code>).
</p>
</li>
<li><p> As a two-column integer matrix: the first column gives the
number of successes and the second the number of failures.
</p>
</li></ol>

<p>The <code>quasibinomial</code> and <code>quasipoisson</code> families differ from
the <code>binomial</code> and <code>poisson</code> families only in that the
dispersion parameter is not fixed at one, so they can model
over-dispersion.  For the binomial case see
McCullagh and Nelder (1989, pp. 124&ndash;8).
Although they show that there is (under some
restrictions) a model with
variance proportional to mean as in the quasi-binomial model, note
that <code>glm</code> does not compute maximum-likelihood estimates in that
model.  The behaviour of S is closer to the quasi- variants.
</p>


<h3>Value</h3>

<p>An object of class <code>"family"</code> (which has a concise print method).
This is a list with elements
</p>
<table>
<tr><td><code>family</code></td>
<td>
<p>character: the family name.</p>
</td></tr>
<tr><td><code>link</code></td>
<td>
<p>character: the link name.</p>
</td></tr>
<tr><td><code>linkfun</code></td>
<td>
<p>function: the link.</p>
</td></tr>
<tr><td><code>linkinv</code></td>
<td>
<p>function: the inverse of the link function.</p>
</td></tr>
<tr><td><code>variance</code></td>
<td>
<p>function: the variance as a function of the mean.</p>
</td></tr>
<tr><td><code>dev.resids</code></td>
<td>
<p>function giving the deviance for each observation
as a function of <code>(y, mu, wt)</code>, used by the
<code><a href="#topic+residuals.glm">residuals</a></code> method when computing
deviance residuals.</p>
</td></tr>
<tr><td><code>aic</code></td>
<td>
<p>function giving the AIC value if appropriate (but <code>NA</code>
for the quasi- families).  More precisely, this function
returns <code class="reqn">-2\ell + 2 s</code>, where <code class="reqn">\ell</code> is the
log-likelihood and <code class="reqn">s</code> is the number of estimated scale
parameters.  Note that the penalty term for the location parameters
(typically the &ldquo;regression coefficients&rdquo;) is added elsewhere,
e.g., in <code><a href="#topic+glm.fit">glm.fit</a>()</code>, or <code><a href="#topic+AIC">AIC</a>()</code>, see the
AIC example in <code><a href="#topic+glm">glm</a></code>.
See <code><a href="#topic+logLik">logLik</a></code> for the assumptions made about the
dispersion parameter.</p>
</td></tr>
<tr><td><code>mu.eta</code></td>
<td>
<p>function: derivative of the inverse-link function
with respect to the linear predictor.  If the inverse-link
function is <code class="reqn">\mu = g^{-1}(\eta)</code> where
<code class="reqn">\eta</code> is the value of the linear predictor, then this
function returns
<code class="reqn">d(g^{-1})/d\eta = d\mu/d\eta</code>.</p>
</td></tr>
<tr><td><code>initialize</code></td>
<td>
<p>expression.  This needs to set up whatever data
objects are needed for the family as well as <code>n</code> (needed for
AIC in the binomial family) and <code>mustart</code> (see <code><a href="#topic+glm">glm</a></code>).</p>
</td></tr>
<tr><td><code>validmu</code></td>
<td>
<p>logical function.  Returns <code>TRUE</code> if a mean
vector <code>mu</code> is within the domain of <code>variance</code>.</p>
</td></tr>
<tr><td><code>valideta</code></td>
<td>
<p>logical function.   Returns <code>TRUE</code> if a linear
predictor <code>eta</code> is within the domain of <code>linkinv</code>.</p>
</td></tr>
<tr><td><code>simulate</code></td>
<td>
<p>(optional) function <code>simulate(object, nsim)</code> to be
called by the <code>"lm"</code> method of <code><a href="#topic+simulate">simulate</a></code>.  It will
normally return a matrix with <code>nsim</code> columns and one row for
each fitted value, but it can also return a list of length
<code>nsim</code>. Clearly this will be missing for &lsquo;quasi-&rsquo;
families.</p>
</td></tr>
<tr><td><code>dispersion</code></td>
<td>
<p>(optional since <span class="rlang"><b>R</b></span> version 4.3.0) numeric: value of the
dispersion parameter, if fixed, or <code>NA_real_</code> if free.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The <code>link</code> and <code>variance</code> arguments have rather awkward
semantics for back-compatibility.  The recommended way is to supply
them as quoted character strings, but they can also be supplied
unquoted (as names or expressions).  Additionally, they can be
supplied as a length-one character vector giving the name of one of
the options, or as a list (for <code>link</code>, of class
<code>"link-glm"</code>).  The restrictions apply only to links given as
names: when given as a character string all the links known to
<code><a href="#topic+make.link">make.link</a></code> are accepted.
</p>
<p>This is potentially ambiguous: supplying <code>link = logit</code> could mean
the unquoted name of a link or the value of object <code>logit</code>.  It
is interpreted if possible as the name of an allowed link, then
as an object.  (You can force the interpretation to always be the value of
an object via <code>logit[1]</code>.)
</p>


<h3>Author(s)</h3>

<p>The design was inspired by S functions of the same names described
in Hastie &amp; Pregibon (1992) (except <code>quasibinomial</code> and
<code>quasipoisson</code>).
</p>


<h3>References</h3>

<p>McCullagh P. and Nelder, J. A. (1989)
<em>Generalized Linear Models.</em>
London: Chapman and Hall.
</p>
<p>Dobson, A. J. (1983)
<em>An Introduction to Statistical Modelling.</em>
London: Chapman and Hall.
</p>
<p>Cox, D. R. and  Snell, E. J. (1981).
<em>Applied Statistics; Principles and Examples.</em>
London: Chapman and Hall.
</p>
<p>Hastie, T. J. and Pregibon, D. (1992)
<em>Generalized linear models.</em>
Chapter 6 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm">glm</a></code>, <code><a href="#topic+power">power</a></code>, <code><a href="#topic+make.link">make.link</a></code>.
</p>
<p>For binomial <em>coefficients</em>, <code><a href="base.html#topic+choose">choose</a></code>;
the binomial and negative binomial <em>distributions</em>,
<code><a href="#topic+Binomial">Binomial</a></code>, and <code><a href="#topic+NegBinomial">NegBinomial</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(utils) # for str

nf &lt;- gaussian()  # Normal family
nf
str(nf)

gf &lt;- Gamma()
gf
str(gf)
gf$linkinv
gf$variance(-3:4) #- == (.)^2

## Binomial with default 'logit' link:  Check some properties visually:
bi &lt;- binomial()
et &lt;- seq(-10,10, by=1/8)
plot(et, bi$mu.eta(et), type="l")
## show that mu.eta() is derivative of linkinv() :
lines((et[-1]+et[-length(et)])/2, col=adjustcolor("red", 1/4),
      diff(bi$linkinv(et))/diff(et), type="l", lwd=4)
## which here is the logistic density:
lines(et, dlogis(et), lwd=3, col=adjustcolor("blue", 1/4))
stopifnot(exprs = {
  all.equal(bi$ mu.eta(et), dlogis(et))
  all.equal(bi$linkinv(et), plogis(et) -&gt; m)
  all.equal(bi$linkfun(m ), qlogis(m))    #  logit(.) == qlogis(.) !
})

## Data from example(glm) :
d.AD &lt;- data.frame(treatment = gl(3,3),
                   outcome   = gl(3,1,9),
                   counts    = c(18,17,15, 20,10,20, 25,13,12))
glm.D93 &lt;- glm(counts ~ outcome + treatment, d.AD, family = poisson())
## Quasipoisson: compare with above / example(glm) :
glm.qD93 &lt;- glm(counts ~ outcome + treatment, d.AD, family = quasipoisson())

glm.qD93
anova  (glm.qD93, test = "F")
summary(glm.qD93)
## for Poisson results (same as from 'glm.D93' !) use
anova  (glm.qD93, dispersion = 1, test = "Chisq")
summary(glm.qD93, dispersion = 1)



## Example of user-specified link, a logit model for p^days
## See Shaffer, T.  2004. Auk 121(2): 526-540.
logexp &lt;- function(days = 1)
{
    linkfun &lt;- function(mu) qlogis(mu^(1/days))
    linkinv &lt;- function(eta) plogis(eta)^days
    mu.eta  &lt;- function(eta) days * plogis(eta)^(days-1) *
                  binomial()$mu.eta(eta)
    valideta &lt;- function(eta) TRUE
    link &lt;- paste0("logexp(", days, ")")
    structure(list(linkfun = linkfun, linkinv = linkinv,
                   mu.eta = mu.eta, valideta = valideta, name = link),
              class = "link-glm")
}
(bil3 &lt;- binomial(logexp(3)))

## in practice this would be used with a vector of 'days', in
## which case use an offset of 0 in the corresponding formula
## to get the null deviance right.

## Binomial with identity link: often not a good idea, as both
## computationally and conceptually difficult:
binomial(link = "identity")  ## is exactly the same as
binomial(link = make.link("identity"))



## tests of quasi
x &lt;- rnorm(100)
y &lt;- rpois(100, exp(1+x))
glm(y ~ x, family = quasi(variance = "mu", link = "log"))
# which is the same as
glm(y ~ x, family = poisson)
glm(y ~ x, family = quasi(variance = "mu^2", link = "log"))
## Not run: glm(y ~ x, family = quasi(variance = "mu^3", link = "log")) # fails
y &lt;- rbinom(100, 1, plogis(x))
# need to set a starting value for the next fit
glm(y ~ x, family = quasi(variance = "mu(1-mu)", link = "logit"), start = c(0,1))
</code></pre>

<hr>
<h2 id='FDist'>The F Distribution</h2><span id='topic+FDist'></span><span id='topic+df'></span><span id='topic+pf'></span><span id='topic+qf'></span><span id='topic+rf'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the F distribution with <code>df1</code> and <code>df2</code>
degrees of freedom (and optional non-centrality parameter <code>ncp</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>df(x, df1, df2, ncp, log = FALSE)
pf(q, df1, df2, ncp, lower.tail = TRUE, log.p = FALSE)
qf(p, df1, df2, ncp, lower.tail = TRUE, log.p = FALSE)
rf(n, df1, df2, ncp)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FDist_+3A_x">x</code>, <code id="FDist_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="FDist_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="FDist_+3A_n">n</code></td>
<td>
<p>number of observations. If <code>length(n) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="FDist_+3A_df1">df1</code>, <code id="FDist_+3A_df2">df2</code></td>
<td>
<p>degrees of freedom.  <code>Inf</code> is allowed.</p>
</td></tr>
<tr><td><code id="FDist_+3A_ncp">ncp</code></td>
<td>
<p>non-centrality parameter. If omitted the central F is assumed.</p>
</td></tr>
<tr><td><code id="FDist_+3A_log">log</code>, <code id="FDist_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="FDist_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The F distribution with <code>df1 =</code> <code class="reqn">\nu_1</code> and <code>df2 =</code>
<code class="reqn">\nu_2</code> degrees of freedom has density
</p>
<p style="text-align: center;"><code class="reqn">
    f(x) = \frac{\Gamma(\nu_1/2 + \nu_2/2)}{\Gamma(\nu_1/2)\Gamma(\nu_2/2)}
    \left(\frac{\nu_1}{\nu_2}\right)^{\nu_1/2} x^{\nu_1/2 -1}
    \left(1 + \frac{\nu_1 x}{\nu_2}\right)^{-(\nu_1 + \nu_2) / 2}%
  </code>
</p>

<p>for <code class="reqn">x &gt; 0</code>.
</p>
<p>The F distribution's cumulative distribution function (<abbr>cdf</abbr>),
<code class="reqn">F_{\nu_1,\nu_2}</code> fulfills (Abramowitz &amp; Stegun 26.6.2, p.946)
<code class="reqn">F_{\nu_1,\nu_2}(qF) = 1 - I_x(\nu_2/2, \nu_1/2) = I_{1-x}(\nu_1/2, \nu_2/2),</code>  where
<code class="reqn">x := \frac{\nu_2}{\nu_2 + \nu_1*qF}</code>,  and
<code class="reqn">I_x(a,b)</code> is the incomplete beta function; in <span class="rlang"><b>R</b></span>, <code class="reqn">=</code> <code><a href="#topic+pbeta">pbeta</a>(x, a,b)</code>.
</p>
<p>It is the distribution of the ratio of the mean squares of
<code class="reqn">\nu_1</code> and <code class="reqn">\nu_2</code> independent standard normals, and hence
of the ratio of two independent chi-squared variates each divided by its
degrees of freedom.  Since the ratio of a normal and the root
mean-square of <code class="reqn">m</code> independent normals has a Student's <code class="reqn">t_m</code>
distribution, the square of a <code class="reqn">t_m</code> variate has a F distribution on
1 and <code class="reqn">m</code> degrees of freedom.
</p>
<p>The non-central F distribution is again the ratio of mean squares of
independent normals of unit variance, but those in the numerator are
allowed to have non-zero means and <code>ncp</code> is the sum of squares of
the means.  See <a href="#topic+Chisquare">Chisquare</a> for further details on
non-central distributions.
</p>


<h3>Value</h3>

<p><code>df</code> gives the density,
<code>pf</code> gives the distribution function
<code>qf</code> gives the quantile function, and
<code>rf</code> generates random deviates.
</p>
<p>Invalid arguments will result in return value <code>NaN</code>, with a warning.
</p>
<p>The length of the result is determined by <code>n</code> for
<code>rf</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.
</p>
<p>The numerical arguments other than <code>n</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.
</p>


<h3>Note</h3>

<p>Supplying <code>ncp = 0</code> uses the algorithm for the non-central
distribution, which is not the same algorithm used if <code>ncp</code> is
omitted.  This is to give consistent behaviour in extreme cases with
values of <code>ncp</code> very near zero.
</p>
<p>The code for non-zero <code>ncp</code> is principally intended to be used
for moderate values of <code>ncp</code>: it will not be highly accurate,
especially in the tails, for large values.
</p>


<h3>Source</h3>

<p>For the central case of <code>df</code>, computed <em>via</em> a binomial
probability, code contributed by Catherine Loader (see
<code><a href="#topic+dbinom">dbinom</a></code>); for the non-central case computed <em>via</em>
<code><a href="#topic+dbeta">dbeta</a></code>, code contributed by Peter Ruckdeschel.
</p>
<p>For <code>pf</code>, <em>via</em> <code><a href="#topic+pbeta">pbeta</a></code> (or for large
<code>df2</code>, <em>via</em> <code><a href="#topic+pchisq">pchisq</a></code>).
</p>
<p>For <code>qf</code>, <em>via</em> <code><a href="#topic+qchisq">qchisq</a></code> for large <code>df2</code>,
else <em>via</em> <code><a href="#topic+qbeta">qbeta</a></code>.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
<em>Continuous Univariate Distributions</em>, volume 2, chapters 27 and 30.
Wiley, New York.
</p>


<h3>See Also</h3>

<p><a href="#topic+Distributions">Distributions</a> for other standard distributions, including
<code><a href="#topic+dchisq">dchisq</a></code> for chi-squared and <code><a href="#topic+dt">dt</a></code> for Student's
t distributions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Equivalence of pt(.,nu) with pf(.^2, 1,nu):
x &lt;- seq(0.001, 5, length.out = 100)
nu &lt;- 4
stopifnot(all.equal(2*pt(x,nu) - 1, pf(x^2, 1,nu)),
          ## upper tails:
 	  all.equal(2*pt(x,     nu, lower.tail=FALSE),
		      pf(x^2, 1,nu, lower.tail=FALSE)))

## the density of the square of a t_m is 2*dt(x, m)/(2*x)
# check this is the same as the density of F_{1,m}
all.equal(df(x^2, 1, 5), dt(x, 5)/x)

## Identity (F &lt;-&gt; t):  qf(2*p - 1, 1, df) == qt(p, df)^2  for  p &gt;= 1/2
p &lt;- seq(1/2, .99, length.out = 50); df &lt;- 10
rel.err &lt;- function(x, y) ifelse(x == y, 0, abs(x-y)/mean(abs(c(x,y))))
stopifnot(all.equal(qf(2*p - 1, df1 = 1, df2 = df),
                    qt(p, df)^2))

## Identity (F &lt;-&gt; Beta &lt;-&gt; incompl.beta):
n1 &lt;- 7 ; n2 &lt;- 12; qF &lt;- c((0:4)/4, 1.5, 2:16)
x &lt;- n2/(n2 + n1*qF)
stopifnot(all.equal(pf(qF, n1, n2, lower.tail=FALSE),
                    pbeta(x, n2/2, n1/2)))
</code></pre>

<hr>
<h2 id='fft'>Fast Discrete Fourier Transform (FFT)</h2><span id='topic+fft'></span><span id='topic+mvfft'></span>

<h3>Description</h3>

<p>Computes the Discrete Fourier Transform (<abbr>DFT</abbr>) of an array with a fast
algorithm, the &ldquo;Fast Fourier Transform&rdquo; (FFT).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fft(z, inverse = FALSE)
mvfft(z, inverse = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fft_+3A_z">z</code></td>
<td>
<p>a real or complex array containing the values to be
transformed.  Long vectors are not supported.</p>
</td></tr>
<tr><td><code id="fft_+3A_inverse">inverse</code></td>
<td>
<p>if <code>TRUE</code>, the unnormalized inverse transform is
computed (the inverse has a <code>+</code> in the exponent of <code class="reqn">e</code>,
but here, we do <em>not</em> divide by <code>1/length(x)</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>When <code>z</code> is a vector, the value computed and returned by
<code>fft</code> is the unnormalized univariate discrete Fourier transform of the
sequence of values in <code>z</code>.  Specifically, <code>y &lt;- fft(z)</code> returns
</p>
<p style="text-align: center;"><code class="reqn">y[h] = \sum_{k=1}^n z[k] \exp(-2\pi i (k-1) (h-1)/n)</code>
</p>

<p>for <code class="reqn">h = 1,\ldots,n</code> where n = <code>length(y)</code>.  If
<code>inverse</code> is <code>TRUE</code>, <code class="reqn">\exp(-2\pi\ldots)</code>
is replaced with <code class="reqn">\exp(2\pi\ldots)</code>.
</p>
<p>When <code>z</code> contains an array, <code>fft</code> computes and returns the
multivariate (spatial) transform.  If <code>inverse</code> is <code>TRUE</code>,
the (unnormalized) inverse Fourier transform is returned, i.e.,
if <code>y &lt;- fft(z)</code>, then <code>z</code> is
<code>fft(y, inverse = TRUE) / length(y)</code>.
</p>
<p>By contrast, <code>mvfft</code> takes a real or complex matrix as argument,
and returns a similar shaped matrix, but with each column replaced by
its discrete Fourier transform.  This is useful for analyzing
vector-valued series.
</p>
<p>The FFT is fastest when the length of the series being transformed
is highly composite (i.e., has many factors).  If this is not the
case, the transform may take a long time to compute and will use a
large amount of memory.
</p>


<h3>Source</h3>

<p>Uses C translation of Fortran code in Singleton (1979).
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988).
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Singleton, R. C. (1979).
Mixed Radix Fast Fourier Transforms,
in <em>Programs for Digital Signal Processing</em>,
IEEE Digital Signal Processing Committee eds.
IEEE Press.
</p>
<p>Cooley, James W., and Tukey, John W. (1965).
An algorithm for the machine calculation of complex Fourier series,
<em>Mathematics of Computation</em>, <b>19</b>(90), 297&ndash;301.
<a href="https://doi.org/10.2307/2003354">doi:10.2307/2003354</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+convolve">convolve</a></code>, <code><a href="#topic+nextn">nextn</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 1:4
fft(x)
fft(fft(x), inverse = TRUE)/length(x)

## Slow Discrete Fourier Transform (DFT) - e.g., for checking the formula
fft0 &lt;- function(z, inverse=FALSE) {
  n &lt;- length(z)
  if(n == 0) return(z)
  k &lt;- 0:(n-1)
  ff &lt;- (if(inverse) 1 else -1) * 2*pi * 1i * k/n
  vapply(1:n, function(h) sum(z * exp(ff*(h-1))), complex(1))
}

relD &lt;- function(x,y) 2* abs(x - y) / abs(x + y)
n &lt;- 2^8
z &lt;- complex(n, rnorm(n), rnorm(n))
## relative differences in the order of 4*10^{-14} :
summary(relD(fft(z), fft0(z)))
summary(relD(fft(z, inverse=TRUE), fft0(z, inverse=TRUE)))
</code></pre>

<hr>
<h2 id='filter'>Linear Filtering on a Time Series</h2><span id='topic+filter'></span>

<h3>Description</h3>

<p>Applies linear filtering to a univariate time series or to each series
separately of a multivariate time series.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filter(x, filter, method = c("convolution", "recursive"),
       sides = 2, circular = FALSE, init)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="filter_+3A_x">x</code></td>
<td>
<p>a univariate or multivariate time series.</p>
</td></tr>
<tr><td><code id="filter_+3A_filter">filter</code></td>
<td>
<p>a vector of filter coefficients in reverse time order
(as for AR or MA coefficients).</p>
</td></tr>
<tr><td><code id="filter_+3A_method">method</code></td>
<td>
<p>Either <code>"convolution"</code> or <code>"recursive"</code> (and
can be abbreviated). If <code>"convolution"</code> a moving average is
used: if <code>"recursive"</code> an autoregression is used.</p>
</td></tr>
<tr><td><code id="filter_+3A_sides">sides</code></td>
<td>
<p>for convolution filters only. If <code>sides = 1</code> the
filter coefficients are for past values only; if <code>sides = 2</code>
they are centred around lag 0.  In this case the length of the
filter should be odd, but if it is even, more of the filter
is forward in time than backward.</p>
</td></tr>
<tr><td><code id="filter_+3A_circular">circular</code></td>
<td>
<p>for convolution filters only.  If <code>TRUE</code>, wrap
the filter around the ends of the series, otherwise assume
external values are missing (<code>NA</code>).</p>
</td></tr>
<tr><td><code id="filter_+3A_init">init</code></td>
<td>
<p>for recursive filters only. Specifies the initial values
of the time series just prior to the start value, in reverse
time order. The default is a set of zeros.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Missing values are allowed in <code>x</code> but not in <code>filter</code>
(where they would lead to missing values everywhere in the output).
</p>
<p>Note that there is an implied coefficient 1 at lag 0 in the
recursive filter, which gives
</p>
<p style="text-align: center;"><code class="reqn">y_i = x_i + f_1y_{i-1} + \cdots + f_py_{i-p}</code>
</p>

<p>No check is made to see if recursive filter is invertible:
the output may diverge if it is not.
</p>
<p>The convolution filter is
</p>
<p style="text-align: center;"><code class="reqn">y_i = f_1x_{i+o} + \cdots + f_px_{i+o-(p-1)}</code>
</p>

<p>where <code>o</code> is the offset: see <code>sides</code> for how it is determined.
</p>


<h3>Value</h3>

<p>A time series object.
</p>


<h3>Note</h3>

<p><code><a href="#topic+convolve">convolve</a>(, type = "filter")</code> uses the FFT for computations
and so <em>may</em> be faster for long filters on univariate series,
but it does not return a time series (and so the  time alignment is
unclear), nor does it handle missing values.  <code>filter</code> is
faster for a filter of length 100 on a series of length 1000,
for example.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+convolve">convolve</a></code>, <code><a href="#topic+arima.sim">arima.sim</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 1:100
filter(x, rep(1, 3))
filter(x, rep(1, 3), sides = 1)
filter(x, rep(1, 3), sides = 1, circular = TRUE)

filter(presidents, rep(1, 3))
</code></pre>

<hr>
<h2 id='fisher.test'>Fisher's Exact Test for Count Data</h2><span id='topic+fisher.test'></span>

<h3>Description</h3>

<p>Performs Fisher's exact test for testing the null of independence of
rows and columns in a contingency table with fixed marginals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fisher.test(x, y = NULL, workspace = 200000, hybrid = FALSE,
            hybridPars = c(expect = 5, percent = 80, Emin = 1),
            control = list(), or = 1, alternative = "two.sided",
            conf.int = TRUE, conf.level = 0.95,
            simulate.p.value = FALSE, B = 2000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fisher.test_+3A_x">x</code></td>
<td>
<p>either a two-dimensional contingency table in matrix form,
or a factor object.</p>
</td></tr>
<tr><td><code id="fisher.test_+3A_y">y</code></td>
<td>
<p>a factor object; ignored if <code>x</code> is a matrix.</p>
</td></tr>
<tr><td><code id="fisher.test_+3A_workspace">workspace</code></td>
<td>
<p>an integer specifying the size of the workspace
used in the network algorithm.  In units of 4 bytes.  Only used for
non-simulated p-values larger than <code class="reqn">2 \times 2</code> tables.
Since <span class="rlang"><b>R</b></span> version 3.5.0, this also increases the internal stack size
which allows larger problems to be solved, however sometimes needing
hours.  In such cases, <code>simulate.p.values=TRUE</code> may be more
reasonable.</p>
</td></tr>
<tr><td><code id="fisher.test_+3A_hybrid">hybrid</code></td>
<td>
<p>a logical. Only used for larger than <code class="reqn">2 \times 2</code>
tables, in which cases it indicates whether the exact probabilities
(default) or a hybrid approximation thereof should be computed.</p>
</td></tr>
<tr><td><code id="fisher.test_+3A_hybridpars">hybridPars</code></td>
<td>
<p>a numeric vector of length 3, by default describing
&ldquo;Cochran's conditions&rdquo; for the validity of the chi-squared
approximation, see &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="fisher.test_+3A_control">control</code></td>
<td>
<p>a list with named components for low level algorithm
control.  At present the only one used is <code>"mult"</code>, a positive
integer <code class="reqn">\ge 2</code> with default 30 used only for larger than
<code class="reqn">2 \times 2</code> tables.  This says how many times as much
space should be allocated to paths as to keys: see file
&lsquo;<span class="file">fexact.c</span>&rsquo; in the sources of this package.</p>
</td></tr>
<tr><td><code id="fisher.test_+3A_or">or</code></td>
<td>
<p>the hypothesized odds ratio.  Only used in the
<code class="reqn">2 \times 2</code> case.</p>
</td></tr>
<tr><td><code id="fisher.test_+3A_alternative">alternative</code></td>
<td>
<p>indicates the alternative hypothesis and must be
one of <code>"two.sided"</code>, <code>"greater"</code> or <code>"less"</code>.
You can specify just the initial letter.  Only used in the
<code class="reqn">2 \times 2</code> case.</p>
</td></tr>
<tr><td><code id="fisher.test_+3A_conf.int">conf.int</code></td>
<td>
<p>logical indicating if a confidence interval for the
odds ratio in a <code class="reqn">2 \times 2</code> table should be
computed (and returned).</p>
</td></tr>
<tr><td><code id="fisher.test_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level for the returned confidence
interval.  Only used in the <code class="reqn">2 \times 2</code> case and if
<code>conf.int = TRUE</code>.</p>
</td></tr>
<tr><td><code id="fisher.test_+3A_simulate.p.value">simulate.p.value</code></td>
<td>
<p>a logical indicating whether to compute
p-values by Monte Carlo simulation, in larger than <code class="reqn">2 \times
      2</code> tables.</p>
</td></tr>
<tr><td><code id="fisher.test_+3A_b">B</code></td>
<td>
<p>an integer specifying the number of replicates used in the
Monte Carlo test.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>x</code> is a matrix, it is taken as a two-dimensional contingency
table, and hence its entries should be nonnegative integers.
Otherwise, both <code>x</code> and <code>y</code> must be vectors or factors of the same
length.  Incomplete cases are removed, vectors are coerced into
factor objects, and the contingency table is computed from these.
</p>
<p>For <code class="reqn">2 \times 2</code> cases, p-values are obtained directly
using the (central or non-central) hypergeometric
distribution. Otherwise, computations are based on a C version of the
FORTRAN subroutine <code>FEXACT</code> which implements the network developed by
Mehta and Patel (1983, 1986) and improved by
Clarkson, Fan and Joe (1993).
The FORTRAN code can be obtained from
<a href="https://netlib.org/toms/643">https://netlib.org/toms/643</a>.  Note this fails (with an error
message) when the entries of the table are too large.  (It transposes
the table if necessary so it has no more rows than columns.  One
constraint is that the product of the row marginals be less than
<code class="reqn">2^{31} - 1</code>.)
</p>
<p>For <code class="reqn">2 \times 2</code> tables, the null of conditional
independence is equivalent to the hypothesis that the odds ratio
equals one.  &lsquo;Exact&rsquo; inference can be based on observing that in
general, given all marginal totals fixed, the first element of the
contingency table has a non-central hypergeometric distribution with
non-centrality parameter given by the odds ratio (Fisher, 1935).  The
alternative for a one-sided test is based on the odds ratio, so
<code>alternative = "greater"</code> is a test of the odds ratio being bigger
than <code>or</code>.
</p>
<p>Two-sided tests are based on the probabilities of the tables, and take
as &lsquo;more extreme&rsquo; all tables with probabilities less than or
equal to that of the observed table, the p-value being the sum of such
probabilities.
</p>
<p>For larger than <code class="reqn">2 \times 2</code> tables and <code>hybrid = TRUE</code>,
asymptotic chi-squared probabilities are only used if the
&lsquo;Cochran conditions&rsquo; (or modified version thereof) specified by
<code>hybridPars = c(expect = 5, percent = 80, Emin = 1)</code> are
satisfied, that is if no cell has expected counts less than
<code>1</code> (<code>= Emin</code>) and more than 80% (<code>= percent</code>) of the
cells have expected counts at least 5 (<code>= expect</code>), otherwise
the exact calculation is used.  A corresponding <code>if()</code> decision
is made for all sub-tables considered.

Accidentally, <span class="rlang"><b>R</b></span> has used <code>180</code> instead of <code>80</code> as
<code>percent</code>, i.e., <code>hybridPars[2]</code> in <span class="rlang"><b>R</b></span> versions between
3.0.0 and 3.4.1 (inclusive), i.e., the 2nd of the <code>hybridPars</code>
(all of which used to be hard-coded previous to <span class="rlang"><b>R</b></span> 3.5.0).
Consequently, in these versions of <span class="rlang"><b>R</b></span>, <code>hybrid=TRUE</code> never made a
difference.
</p>
<p>In the <code class="reqn">r \times c</code> case with <code class="reqn">r &gt; 2</code> or <code class="reqn">c &gt; 2</code>,
internal tables can get too large for the exact test in which case an
error is signalled.  Apart from increasing <code>workspace</code>
sufficiently, which then may lead to very long running times, using
<code>simulate.p.value = TRUE</code> may then often be sufficient and hence
advisable.
</p>
<p>Simulation is done conditional on the row and column marginals, and
works only if the marginals are strictly positive.  (A C translation
of the algorithm of Patefield (1981) is used.)
Note that the default number of replicates (<code>B = 2000</code>) implies a
minimum p-value of about 0.0005 (<code class="reqn">1/(B+1)</code>).
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>a confidence interval for the odds ratio.
Only present in the <code class="reqn">2 \times 2</code> case and if argument
<code>conf.int = TRUE</code>.</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>an estimate of the odds ratio.  Note that the
<em>conditional</em> Maximum Likelihood Estimate (MLE) rather than the
unconditional MLE (the sample odds ratio) is used.
Only present in the <code class="reqn">2 \times 2</code> case.</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>the odds ratio under the null, <code>or</code>.
Only present in the <code class="reqn">2 \times 2</code> case.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative
hypothesis.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the character string
<code>"Fisher's Exact Test for Count Data"</code>.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the name(s) of the data.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Agresti, A. (1990).
<em>Categorical data analysis</em>.
New York: Wiley.
Pages 59&ndash;66.
</p>
<p>Agresti, A. (2002).
<em>Categorical data analysis</em>. Second edition.
New York: Wiley.
Pages 91&ndash;101.
</p>
<p>Fisher, R. A. (1935).
The logic of inductive inference.
<em>Journal of the Royal Statistical Society Series A</em>, <b>98</b>,
39&ndash;54.
<a href="https://doi.org/10.2307/2342435">doi:10.2307/2342435</a>.
</p>
<p>Fisher, R. A. (1962).
Confidence limits for a cross-product ratio.
<em>Australian Journal of Statistics</em>, <b>4</b>, 41.
<a href="https://doi.org/10.1111/j.1467-842X.1962.tb00285.x">doi:10.1111/j.1467-842X.1962.tb00285.x</a>.
</p>
<p>Fisher, R. A. (1970).
<em>Statistical Methods for Research Workers</em>.
Oliver &amp; Boyd.
</p>
<p>Mehta, Cyrus R. and Patel, Nitin R. (1983).
A network algorithm for performing Fisher's exact test in <code class="reqn">r
  \times c</code> contingency tables.
<em>Journal of the American Statistical Association</em>, <b>78</b>,
427&ndash;434.
<a href="https://doi.org/10.1080/01621459.1983.10477989">doi:10.1080/01621459.1983.10477989</a>.
</p>
<p>Mehta, C. R. and Patel, N. R. (1986).
Algorithm 643: FEXACT, a FORTRAN subroutine for Fisher's exact test
on unordered <code class="reqn">r \times c</code> contingency tables.
<em>ACM Transactions on Mathematical Software</em>, <b>12</b>,
154&ndash;161.
<a href="https://doi.org/10.1145/6497.214326">doi:10.1145/6497.214326</a>.
</p>
<p>Clarkson, D. B., Fan, Y. and Joe, H. (1993)
A Remark on Algorithm 643: FEXACT: An Algorithm for Performing
Fisher's Exact Test in <code class="reqn">r \times c</code> Contingency Tables.
<em>ACM Transactions on Mathematical Software</em>, <b>19</b>,
484&ndash;488.
<a href="https://doi.org/10.1145/168173.168412">doi:10.1145/168173.168412</a>.
</p>
<p>Patefield, W. M. (1981).
Algorithm AS 159: An efficient method of generating r x c tables
with given row and column totals.
<em>Applied Statistics</em>, <b>30</b>, 91&ndash;97.
<a href="https://doi.org/10.2307/2346669">doi:10.2307/2346669</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+chisq.test">chisq.test</a></code>
</p>
<p><code>fisher.exact</code> in package <a href="https://CRAN.R-project.org/package=exact2x2"><span class="pkg">exact2x2</span></a> for alternative
interpretations of two-sided tests and confidence intervals for
<code class="reqn">2 \times 2</code> tables.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Agresti (1990, p. 61f; 2002, p. 91) Fisher's Tea Drinker
## A British woman claimed to be able to distinguish whether milk or
##  tea was added to the cup first.  To test, she was given 8 cups of
##  tea, in four of which milk was added first.  The null hypothesis
##  is that there is no association between the true order of pouring
##  and the woman's guess, the alternative that there is a positive
##  association (that the odds ratio is greater than 1).
TeaTasting &lt;-
matrix(c(3, 1, 1, 3),
       nrow = 2,
       dimnames = list(Guess = c("Milk", "Tea"),
                       Truth = c("Milk", "Tea")))
fisher.test(TeaTasting, alternative = "greater")
## =&gt; p = 0.2429, association could not be established

## Fisher (1962, 1970), Criminal convictions of like-sex twins
Convictions &lt;- matrix(c(2, 10, 15, 3), nrow = 2,
	              dimnames =
	       list(c("Dizygotic", "Monozygotic"),
		    c("Convicted", "Not convicted")))
Convictions
fisher.test(Convictions, alternative = "less")
fisher.test(Convictions, conf.int = FALSE)
fisher.test(Convictions, conf.level = 0.95)$conf.int
fisher.test(Convictions, conf.level = 0.99)$conf.int

## A r x c table  Agresti (2002, p. 57) Job Satisfaction
Job &lt;- matrix(c(1,2,1,0, 3,3,6,1, 10,10,14,9, 6,7,12,11), 4, 4,
           dimnames = list(income = c("&lt; 15k", "15-25k", "25-40k", "&gt; 40k"),
                     satisfaction = c("VeryD", "LittleD", "ModerateS", "VeryS")))
fisher.test(Job) # 0.7827
fisher.test(Job, simulate.p.value = TRUE, B = 1e5) # also close to 0.78

## 6th example in Mehta &amp; Patel's JASA paper
MP6 &lt;- rbind(
        c(1,2,2,1,1,0,1),
        c(2,0,0,2,3,0,0),
        c(0,1,1,1,2,7,3),
        c(1,1,2,0,0,0,1),
        c(0,1,1,1,1,0,0))
fisher.test(MP6)
# Exactly the same p-value, as Cochran's conditions are never met:
fisher.test(MP6, hybrid=TRUE)
</code></pre>

<hr>
<h2 id='fitted'>Extract Model Fitted Values</h2><span id='topic+fitted.values'></span><span id='topic+fitted'></span><span id='topic+fitted.default'></span>

<h3>Description</h3>

<p><code>fitted</code> is a generic function which extracts fitted values from
objects returned by modeling functions.  <code>fitted.values</code> is an
alias for it.
</p>
<p>All object classes which are returned by model fitting functions
should provide a <code>fitted</code> method.  (Note that the generic is
<code>fitted</code> and not <code>fitted.values</code>.)
</p>
<p>Methods can make use of <code><a href="#topic+napredict">napredict</a></code> methods to compensate
for the omission of missing values.  The default and <code><a href="#topic+nls">nls</a></code>
methods do.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fitted(object, ...)
fitted.values(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitted_+3A_object">object</code></td>
<td>
<p>an object for which the extraction of model fitted values is
meaningful.</p>
</td></tr>
<tr><td><code id="fitted_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Fitted values extracted from the object <code>object</code>.
</p>


<h3>References</h3>

<p>Chambers, J. M. and Hastie, T. J. (1992)
<em>Statistical Models in S</em>.
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coefficients">coefficients</a></code>, <code><a href="#topic+glm">glm</a></code>, <code><a href="#topic+lm">lm</a></code>,
<code><a href="#topic+residuals">residuals</a></code>.
</p>

<hr>
<h2 id='fivenum'>Tukey Five-Number Summaries</h2><span id='topic+fivenum'></span>

<h3>Description</h3>

<p>Returns Tukey's five number summary (minimum, lower-hinge, median,
upper-hinge, maximum) for the input data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fivenum(x, na.rm = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fivenum_+3A_x">x</code></td>
<td>
<p>numeric, maybe including <code><a href="base.html#topic+NA">NA</a></code>s and
<code class="reqn">\pm</code><code><a href="base.html#topic+Inf">Inf</a></code>s.</p>
</td></tr>
<tr><td><code id="fivenum_+3A_na.rm">na.rm</code></td>
<td>
<p>logical; if <code>TRUE</code>, all <code><a href="base.html#topic+NA">NA</a></code> and
<code><a href="base.html#topic+NaN">NaN</a></code>s are dropped, before the statistics are computed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of length 5 containing the summary information.  See
<code><a href="grDevices.html#topic+boxplot.stats">boxplot.stats</a></code> for more details.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+IQR">IQR</a></code>,
<code><a href="grDevices.html#topic+boxplot.stats">boxplot.stats</a></code>,
<code><a href="#topic+median">median</a></code>,
<code><a href="#topic+quantile">quantile</a></code>,
<code><a href="base.html#topic+range">range</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fivenum(c(rnorm(100), -1:1/0))
</code></pre>

<hr>
<h2 id='fligner.test'>Fligner-Killeen Test of Homogeneity of Variances</h2><span id='topic+fligner.test'></span><span id='topic+fligner.test.default'></span><span id='topic+fligner.test.formula'></span>

<h3>Description</h3>

<p>Performs a Fligner-Killeen (median) test of the null that the
variances in each of the groups (samples) are the same.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fligner.test(x, ...)

## Default S3 method:
fligner.test(x, g, ...)

## S3 method for class 'formula'
fligner.test(formula, data, subset, na.action, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fligner.test_+3A_x">x</code></td>
<td>
<p>a numeric vector of data values, or a list of numeric data
vectors.</p>
</td></tr>
<tr><td><code id="fligner.test_+3A_g">g</code></td>
<td>
<p>a vector or factor object giving the group for the
corresponding elements of <code>x</code>.
Ignored if <code>x</code> is a list.</p>
</td></tr>
<tr><td><code id="fligner.test_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>lhs ~ rhs</code> where <code>lhs</code>
gives the data values and <code>rhs</code> the corresponding groups.</p>
</td></tr>
<tr><td><code id="fligner.test_+3A_data">data</code></td>
<td>
<p>an optional matrix or data frame (or similar: see
<code><a href="#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="fligner.test_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="fligner.test_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.  Defaults to
<code>getOption("na.action")</code>.</p>
</td></tr>
<tr><td><code id="fligner.test_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>x</code> is a list, its elements are taken as the samples to be
compared for homogeneity of variances, and hence have to be numeric
data vectors.  In this case, <code>g</code> is ignored, and one can simply
use <code>fligner.test(x)</code> to perform the test.  If the samples are
not yet contained in a list, use <code>fligner.test(list(x, ...))</code>.
</p>
<p>Otherwise, <code>x</code> must be a numeric data vector, and <code>g</code> must
be a vector or factor object of the same length as <code>x</code> giving the
group for the corresponding elements of <code>x</code>.
</p>
<p>The Fligner-Killeen (median) test has been determined in a simulation
study as one of the many tests for homogeneity of variances which is
most robust against departures from normality, see
Conover, Johnson &amp; Johnson (1981).
It is a <code class="reqn">k</code>-sample simple linear rank which uses
the ranks of the absolute values of the centered samples and weights
<code class="reqn">a(i) = \mathrm{qnorm}((1 + i/(n+1))/2)</code>.  The version implemented here uses median centering in
each of the samples (F-K:med <code class="reqn">X^2</code> in the reference).
</p>


<h3>Value</h3>

<p>A list of class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the Fligner-Killeen:med <code class="reqn">X^2</code> test statistic.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of freedom of the approximate chi-squared
distribution of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the character string
<code>"Fligner-Killeen test of homogeneity of variances"</code>.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the names of the data.</p>
</td></tr>
</table>


<h3>References</h3>

<p>William J. Conover, Mark E. Johnson and Myrle M. Johnson (1981).
A comparative study of tests for homogeneity of variances, with
applications to the outer continental shelf bidding data.
<em>Technometrics</em>, <b>23</b>, 351&ndash;361.
<a href="https://doi.org/10.2307/1268225">doi:10.2307/1268225</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ansari.test">ansari.test</a></code> and <code><a href="#topic+mood.test">mood.test</a></code> for rank-based
two-sample test for a difference in scale parameters;
<code><a href="#topic+var.test">var.test</a></code> and <code><a href="#topic+bartlett.test">bartlett.test</a></code> for parametric
tests for the homogeneity of variances.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

plot(count ~ spray, data = InsectSprays)
fligner.test(InsectSprays$count, InsectSprays$spray)
fligner.test(count ~ spray, data = InsectSprays)
## Compare this to bartlett.test()
</code></pre>

<hr>
<h2 id='formula'>Model Formulae</h2><span id='topic+formula'></span><span id='topic+formula.default'></span><span id='topic+formula.formula'></span><span id='topic+formula.terms'></span><span id='topic+formula.data.frame'></span><span id='topic+DF2formula'></span><span id='topic+as.formula'></span><span id='topic+print.formula'></span><span id='topic++5B.formula'></span>

<h3>Description</h3>

<p>The generic function <code>formula</code> and its specific methods provide a
way of extracting formulae which have been included in other objects.
</p>
<p><code>as.formula</code> is almost identical, additionally preserving
attributes when <code>object</code> already inherits from
<code>"formula"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>formula(x, ...)
DF2formula(x, env = parent.frame())
as.formula(object, env = parent.frame())

## S3 method for class 'formula'
print(x, showEnv = !identical(e, .GlobalEnv), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="formula_+3A_x">x</code>, <code id="formula_+3A_object">object</code></td>
<td>
<p><span class="rlang"><b>R</b></span> object, for <code>DF2formula()</code> a <code><a href="base.html#topic+data.frame">data.frame</a></code>.</p>
</td></tr>
<tr><td><code id="formula_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="formula_+3A_env">env</code></td>
<td>
<p>the environment to associate with the result, if not
already a formula.</p>
</td></tr>
<tr><td><code id="formula_+3A_showenv">showEnv</code></td>
<td>
<p>logical indicating if the environment should be printed
as well.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The models fitted by, e.g., the <code><a href="#topic+lm">lm</a></code> and <code><a href="#topic+glm">glm</a></code>
functions are specified in a compact symbolic form.
The <code><a href="base.html#topic+~">~</a></code> operator is basic in the formation of such models.
An expression of the form <code>y ~ model</code> is interpreted
as a specification that the response <code>y</code> is modelled
by a linear predictor specified symbolically by <code>model</code>.
Such a model consists of a series of terms separated
by <code>+</code> operators.
The terms themselves consist of variable and factor
names separated by <code>:</code> operators.
Such a term is interpreted as the interaction of
all the variables and factors appearing in the term.
</p>
<p>In addition to <code>+</code> and <code>:</code>, a number of other operators are
useful in model formulae.
</p>

<ul>
<li><p> The <code>*</code> operator denotes factor crossing: <code>a*b</code> is
interpreted as <code>a + b + a:b</code>.
</p>
</li>
<li><p> The <code>^</code>
operator indicates crossing to the specified degree.  For example
<code>(a+b+c)^2</code> is identical to <code>(a+b+c)*(a+b+c)</code> which in turn
expands to a formula containing the main effects for <code>a</code>,
<code>b</code> and <code>c</code> together with their second-order interactions.
</p>
</li>
<li><p> The <code>%in%</code> operator indicates that the terms on its left are
nested within those on the right.  For example <code>a + b %in% a</code>
expands to the formula <code>a + a:b</code>.
</p>
</li>
<li><p> The <code>/</code> operator provides a shorthand, so that
<code>a / b</code> is equivalent to <code>a + b %in% a</code>.
</p>
</li>
<li><p> The <code>-</code> operator removes the specified terms, hence
<code>(a+b+c)^2 - a:b</code> is identical to <code>a + b + c + b:c + a:c</code>.
It can also used to remove the intercept term: when fitting a linear
model <code>y ~ x - 1</code> specifies a line through the origin.
A model with no intercept can be also specified as <code>y ~ x + 0</code>
or <code>y ~ 0 + x</code>.
</p>
</li></ul>

<p>While formulae usually involve just variable and factor
names, they can also involve arithmetic expressions.
The formula <code>log(y) ~ a + log(x)</code> is quite legal.
When such arithmetic expressions involve
operators which are also used symbolically
in model formulae, there can be confusion between
arithmetic and symbolic operator use.
</p>
<p>To avoid this confusion, the function <code><a href="base.html#topic+I">I</a>()</code>
can be used to bracket those portions of a model
formula where the operators are used in their
arithmetic sense.  For example, in the formula
<code>y ~ a + I(b+c)</code>, the term <code>b+c</code> is to be
interpreted as the sum of <code>b</code> and <code>c</code>.
</p>
<p>Variable names can be quoted by backticks <code>`like this`</code> in
formulae, although there is no guarantee that all code using formulae
will accept such non-syntactic names.
</p>
<p>Most model-fitting functions accept formulae with right-hand-side
including the function <code><a href="#topic+offset">offset</a></code> to indicate terms with a
fixed coefficient of one.  Some functions accept other
&lsquo;specials&rsquo; such as <code>strata</code> or <code>cluster</code> (see the
<code>specials</code> argument of <code><a href="#topic+terms.formula">terms.formula</a></code>).
</p>
<p>There are two special interpretations of <code>.</code> in a formula.  The
usual one is in the context of a <code>data</code> argument of model
fitting functions and means &lsquo;all columns not otherwise in the
formula&rsquo;: see <code><a href="#topic+terms.formula">terms.formula</a></code>.  In the context of
<code><a href="#topic+update.formula">update.formula</a></code>, <b>only</b>, it means &lsquo;what was
previously in this part of the formula&rsquo;.
</p>
<p>When <code>formula</code> is called on a fitted model object, either a
specific method is used (such as that for class <code>"nls"</code>) or the
default method.  The default first looks for a <code>"formula"</code>
component of the object (and evaluates it), then a <code>"terms"</code>
component, then a <code>formula</code> parameter of the call (and evaluates
its value) and finally a <code>"formula"</code> attribute.
</p>
<p>There is a <code>formula</code> method for data frames.  When there's
<code>"terms"</code> attribute with a formula, e.g., for a
<code><a href="#topic+model.frame">model.frame</a>()</code>, that formula is returned.  If you'd like the
previous (<span class="rlang"><b>R</b></span> <code class="reqn">\le</code> 3.5.x) behavior, use the auxiliary
<code>DF2formula()</code> which does not consider a <code>"terms"</code> attribute.
Otherwise, if
there is only
one column this forms the RHS with an empty LHS.  For more columns,
the first column is the LHS of the formula and the remaining columns
separated by <code>+</code> form the RHS.
</p>


<h3>Value</h3>

<p>All the functions above produce an object of class <code>"formula"</code>
which contains a symbolic model formula.
</p>


<h3>Environments</h3>

<p>A formula object has an associated environment, and
this environment (rather than the parent
environment) is used by <code><a href="#topic+model.frame">model.frame</a></code> to evaluate variables
that are not found in the supplied <code>data</code> argument.
</p>
<p>Formulas created with the <code>~</code> operator use the
environment in which they were created.  Formulas created with
<code>as.formula</code> will use the <code>env</code> argument for their
environment.
</p>


<h3>Note</h3>

<p>In <span class="rlang"><b>R</b></span> versions up to 3.6.0, <code><a href="base.html#topic+character">character</a></code> <code>x</code> of length
more than one were parsed as separate lines of <span class="rlang"><b>R</b></span> code and the first
complete expression was evaluated into a formula when possible.  This
silently truncates such vectors of characters inefficiently and to some
extent inconsistently as this behaviour had been undocumented.  For this
reason, such use has been deprecated.  If you must work via character
<code>x</code>, do use a string, i.e., a character vector of length one.
</p>
<p>E.g., <code>eval(call("~", quote(foo + bar)))</code> has been an order of magnitude
more efficient 
than <code>formula(c("~", "foo + bar"))</code>.
</p>
<p>Further, character &ldquo;expressions&rdquo; needing an <code><a href="base.html#topic+eval">eval</a>()</code>
to return a formula are now deprecated.
</p>


<h3>References</h3>

<p>Chambers, J. M. and Hastie, T. J. (1992)
<em>Statistical models.</em>
Chapter 2 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+~">~</a></code>, <code><a href="base.html#topic+I">I</a></code>, <code><a href="#topic+offset">offset</a></code>.
</p>
<p>For formula manipulation: <code><a href="#topic+update.formula">update.formula</a></code>,
<code><a href="#topic+terms.formula">terms.formula</a></code>, and <code><a href="base.html#topic+all.vars">all.vars</a></code>.
For typical use: <code><a href="#topic+lm">lm</a></code>, <code><a href="#topic+glm">glm</a></code>, and
<code><a href="graphics.html#topic+coplot">coplot</a></code>.
For formula construction: <code><a href="#topic+reformulate">reformulate</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>class(fo &lt;- y ~ x1*x2) # "formula"
fo
typeof(fo)  # R internal : "language"
terms(fo)

environment(fo)
environment(as.formula("y ~ x"))
environment(as.formula("y ~ x", env = new.env()))


## Create a formula for a model with a large number of variables:
xnam &lt;- paste0("x", 1:25)
(fmla &lt;- as.formula(paste("y ~ ", paste(xnam, collapse= "+"))))
## Equivalent with reformulate():
fmla2 &lt;- reformulate(xnam, response = "y")
stopifnot(identical(fmla, fmla2))
</code></pre>

<hr>
<h2 id='formula.nls'>Extract Model Formula from <code>nls</code> Object</h2><span id='topic+formula.nls'></span>

<h3>Description</h3>

<p>Returns the model used to fit <code>object</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nls'
formula(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="formula.nls_+3A_x">x</code></td>
<td>
<p>an object inheriting from class <code>"nls"</code>, representing
a nonlinear least squares fit.</p>
</td></tr>
<tr><td><code id="formula.nls_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a formula representing the model used to obtain <code>object</code>.
</p>


<h3>Author(s)</h3>

<p>Jos Pinheiro and Douglas Bates</p>


<h3>See Also</h3>

<p><code><a href="#topic+nls">nls</a></code>, <code><a href="#topic+formula">formula</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>fm1 &lt;- nls(circumference ~ A/(1+exp((B-age)/C)), Orange,
           start = list(A = 160, B = 700, C = 350))
formula(fm1)
</code></pre>

<hr>
<h2 id='friedman.test'>Friedman Rank Sum Test</h2><span id='topic+friedman.test'></span><span id='topic+friedman.test.default'></span><span id='topic+friedman.test.formula'></span>

<h3>Description</h3>

<p>Performs a Friedman rank sum test with unreplicated blocked data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>friedman.test(y, ...)

## Default S3 method:
friedman.test(y, groups, blocks, ...)

## S3 method for class 'formula'
friedman.test(formula, data, subset, na.action, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="friedman.test_+3A_y">y</code></td>
<td>
<p>either a numeric vector of data values, or a data matrix.</p>
</td></tr>
<tr><td><code id="friedman.test_+3A_groups">groups</code></td>
<td>
<p>a vector giving the group for the corresponding
elements of <code>y</code> if this is a vector;  ignored if <code>y</code>
is a matrix.  If not a factor object, it is coerced to one.</p>
</td></tr>
<tr><td><code id="friedman.test_+3A_blocks">blocks</code></td>
<td>
<p>a vector giving the block for the corresponding
elements of <code>y</code> if this is a vector;  ignored if <code>y</code>
is a matrix.  If not a factor object, it is coerced to one.</p>
</td></tr>
<tr><td><code id="friedman.test_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>a ~ b | c</code>, where <code>a</code>,
<code>b</code> and <code>c</code> give the data values and corresponding groups
and blocks, respectively.</p>
</td></tr>
<tr><td><code id="friedman.test_+3A_data">data</code></td>
<td>
<p>an optional matrix or data frame (or similar: see
<code><a href="#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="friedman.test_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="friedman.test_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.  Defaults to
<code>getOption("na.action")</code>.</p>
</td></tr>
<tr><td><code id="friedman.test_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>friedman.test</code> can be used for analyzing unreplicated complete
block designs (i.e., there is exactly one observation in <code>y</code>
for each combination of levels of <code>groups</code> and <code>blocks</code>)
where the normality assumption may be violated.
</p>
<p>The null hypothesis is that apart from an effect of <code>blocks</code>,
the location parameter of <code>y</code> is the same in each of the
<code>groups</code>.
</p>
<p>If <code>y</code> is a matrix, <code>groups</code> and <code>blocks</code> are
obtained from the column and row indices, respectively.  <code>NA</code>'s
are not allowed in <code>groups</code> or <code>blocks</code>;  if <code>y</code>
contains <code>NA</code>'s, corresponding blocks are removed.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of Friedman's chi-squared statistic.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of freedom of the approximate
chi-squared distribution of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the character string <code>"Friedman rank sum test"</code>.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the names of the data.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Myles Hollander and Douglas A. Wolfe (1973),
<em>Nonparametric Statistical Methods.</em>
New York: John Wiley &amp; Sons.
Pages 139&ndash;146.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+quade.test">quade.test</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Hollander &amp; Wolfe (1973), p. 140ff.
## Comparison of three methods ("round out", "narrow angle", and
##  "wide angle") for rounding first base.  For each of 18 players
##  and the three method, the average time of two runs from a point on
##  the first base line 35ft from home plate to a point 15ft short of
##  second base is recorded.
RoundingTimes &lt;-
matrix(c(5.40, 5.50, 5.55,
         5.85, 5.70, 5.75,
         5.20, 5.60, 5.50,
         5.55, 5.50, 5.40,
         5.90, 5.85, 5.70,
         5.45, 5.55, 5.60,
         5.40, 5.40, 5.35,
         5.45, 5.50, 5.35,
         5.25, 5.15, 5.00,
         5.85, 5.80, 5.70,
         5.25, 5.20, 5.10,
         5.65, 5.55, 5.45,
         5.60, 5.35, 5.45,
         5.05, 5.00, 4.95,
         5.50, 5.50, 5.40,
         5.45, 5.55, 5.50,
         5.55, 5.55, 5.35,
         5.45, 5.50, 5.55,
         5.50, 5.45, 5.25,
         5.65, 5.60, 5.40,
         5.70, 5.65, 5.55,
         6.30, 6.30, 6.25),
       nrow = 22,
       byrow = TRUE,
       dimnames = list(1 : 22,
                       c("Round Out", "Narrow Angle", "Wide Angle")))
friedman.test(RoundingTimes)
## =&gt; strong evidence against the null that the methods are equivalent
##    with respect to speed

wb &lt;- aggregate(warpbreaks$breaks,
                by = list(w = warpbreaks$wool,
                          t = warpbreaks$tension),
                FUN = mean)
wb
friedman.test(wb$x, wb$w, wb$t)
friedman.test(x ~ w | t, data = wb)
</code></pre>

<hr>
<h2 id='ftable'>Flat Contingency Tables</h2><span id='topic+ftable'></span><span id='topic+ftable.default'></span>

<h3>Description</h3>

<p>Create &lsquo;flat&rsquo; contingency tables.</p>


<h3>Usage</h3>

<pre><code class='language-R'>ftable(x, ...)

## Default S3 method:
ftable(..., exclude = c(NA, NaN), row.vars = NULL,
       col.vars = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ftable_+3A_x">x</code>, <code id="ftable_+3A_...">...</code></td>
<td>
<p><span class="rlang"><b>R</b></span> objects which can be interpreted as factors (including
character strings), or a list (or data frame) whose components can
be so interpreted, or a contingency table object of class
<code>"table"</code> or <code>"ftable"</code>.</p>
</td></tr>
<tr><td><code id="ftable_+3A_exclude">exclude</code></td>
<td>
<p>values to use in the exclude argument of <code>factor</code>
when interpreting non-factor objects.</p>
</td></tr>
<tr><td><code id="ftable_+3A_row.vars">row.vars</code></td>
<td>
<p>a vector of integers giving the numbers of the
variables, or a character vector giving the names of the variables
to be used for the rows of the flat contingency table.</p>
</td></tr>
<tr><td><code id="ftable_+3A_col.vars">col.vars</code></td>
<td>
<p>a vector of integers giving the numbers of the
variables, or a character vector giving the names of the variables
to be used for the columns of the flat contingency table.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ftable</code> creates &lsquo;flat&rsquo; contingency tables.  Similar to the
usual contingency tables, these contain the counts of each combination
of the levels of the variables (factors) involved.  This information
is then re-arranged as a matrix whose rows and columns correspond to
unique combinations of the levels of the row and column variables (as
specified by <code>row.vars</code> and <code>col.vars</code>, respectively).  The
combinations are created by looping over the variables in reverse
order (so that the levels of the left-most variable vary the
slowest).  Displaying a contingency table in this flat matrix form
(via <code>print.ftable</code>, the print method for objects of class
<code>"ftable"</code>) is often preferable to showing it as a
higher-dimensional array.
</p>
<p><code>ftable</code> is a generic function.  Its default method,
<code>ftable.default</code>, first creates a contingency table in array
form from all arguments except <code>row.vars</code> and <code>col.vars</code>.
If the first argument is of class <code>"table"</code>, it represents a
contingency table and is used as is; if it is a flat table of class
<code>"ftable"</code>, the information it contains is converted to the usual
array representation using <code><a href="base.html#topic+as.table">as.table</a></code>.  Otherwise, the arguments
should be <span class="rlang"><b>R</b></span> objects which can be interpreted as factors (including
character strings), or a list (or data frame) whose components can be
so interpreted, which are cross-tabulated using <code><a href="base.html#topic+table">table</a></code>.
Then, the arguments <code>row.vars</code> and <code>col.vars</code> are used to
collapse the contingency table into flat form.  If neither of these
two is given, the last variable is used for the columns.  If both are
given and their union is a proper subset of all variables involved,
the other variables are summed out.
</p>
<p>When the arguments are <span class="rlang"><b>R</b></span> expressions interpreted as factors,
additional arguments will be passed to <code>table</code> to control how
the variable names are displayed; see the last example below.
</p>
<p>Function <code><a href="#topic+ftable.formula">ftable.formula</a></code> provides a formula method for
creating flat contingency tables.
</p>
<p>There are methods for <code><a href="base.html#topic+as.table">as.table</a></code>, <code><a href="base.html#topic+as.matrix">as.matrix</a></code>
and <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>.
</p>


<h3>Value</h3>

<p><code>ftable</code> returns an object of class <code>"ftable"</code>, which is a
matrix with counts of each combination of the levels of variables with
information on the names and levels of the (row and columns) variables
stored as attributes <code>"row.vars"</code> and <code>"col.vars"</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ftable.formula">ftable.formula</a></code> for the formula interface (which allows a
<code>data = .</code> argument);
<code><a href="#topic+read.ftable">read.ftable</a></code> for information on reading, writing and
coercing flat contingency tables;
<code><a href="base.html#topic+table">table</a></code> for ordinary cross-tabulation;
<code><a href="#topic+xtabs">xtabs</a></code> for formula-based cross-tabulation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Start with a contingency table.
ftable(Titanic, row.vars = 1:3)
ftable(Titanic, row.vars = 1:2, col.vars = "Survived")
ftable(Titanic, row.vars = 2:1, col.vars = "Survived")

## Start with a data frame.
x &lt;- ftable(mtcars[c("cyl", "vs", "am", "gear")])
x
ftable(x, row.vars = c(2, 4))

## Start with expressions, use table()'s "dnn" to change labels
ftable(mtcars$cyl, mtcars$vs, mtcars$am, mtcars$gear, row.vars = c(2, 4),
       dnn = c("Cylinders", "V/S", "Transmission", "Gears"))
</code></pre>

<hr>
<h2 id='ftable.formula'>Formula Notation for Flat Contingency Tables</h2><span id='topic+ftable.formula'></span>

<h3>Description</h3>

<p>Produce or manipulate a flat contingency table using
formula notation.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'formula'
ftable(formula, data = NULL, subset, na.action, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ftable.formula_+3A_formula">formula</code></td>
<td>
<p>a formula object with both left and right hand sides
specifying the column and row variables of the flat table.</p>
</td></tr>
<tr><td><code id="ftable.formula_+3A_data">data</code></td>
<td>
<p>a data frame, list or environment (or similar: see
<code><a href="#topic+model.frame">model.frame</a></code>) containing the variables
to be cross-tabulated, or a contingency table (see below).</p>
</td></tr>
<tr><td><code id="ftable.formula_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used.
Ignored if <code>data</code> is a contingency table.</p>
</td></tr>
<tr><td><code id="ftable.formula_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.
Ignored if <code>data</code> is a contingency table.</p>
</td></tr>
<tr><td><code id="ftable.formula_+3A_...">...</code></td>
<td>
<p>further arguments to the default <code>ftable</code> method may also
be passed as arguments, see <code><a href="#topic+ftable.default">ftable.default</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a method of the generic function <code><a href="#topic+ftable">ftable</a></code>.
</p>
<p>The left and right hand side of <code>formula</code> specify the column and
row variables, respectively, of the flat contingency table to be
created.  Only the <code>+</code> operator is allowed for combining the
variables.  A <code>.</code> may be used once in the formula to indicate
inclusion of all the remaining variables.
</p>
<p>If <code>data</code> is an object of class <code>"table"</code> or an array with
more than 2 dimensions, it is taken as a contingency table, and hence
all entries should be nonnegative.  Otherwise, if it is not a flat
contingency table (i.e., an object of class <code>"ftable"</code>), it
should be a data frame or matrix, list or environment containing the
variables to be cross-tabulated.  In this case, <code>na.action</code> is
applied to the data to handle missing values, and, after possibly
selecting a subset of the data as specified by the <code>subset</code>
argument, a contingency table is computed from the variables.
</p>
<p>The contingency table is then collapsed to a flat table, according to
the row and column variables specified by <code>formula</code>.
</p>


<h3>Value</h3>

<p>A flat contingency table which contains the counts of each combination
of the levels of the variables, collapsed into a matrix for suitably
displaying the counts.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ftable">ftable</a></code>,
<code><a href="#topic+ftable.default">ftable.default</a></code>;
<code><a href="base.html#topic+table">table</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Titanic
x &lt;- ftable(Survived ~ ., data = Titanic)
x
ftable(Sex ~ Class + Age, data = x)
</code></pre>

<hr>
<h2 id='GammaDist'>The Gamma Distribution</h2><span id='topic+GammaDist'></span><span id='topic+dgamma'></span><span id='topic+pgamma'></span><span id='topic+qgamma'></span><span id='topic+rgamma'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the Gamma distribution with parameters <code>shape</code> and
<code>scale</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dgamma(x, shape, rate = 1, scale = 1/rate, log = FALSE)
pgamma(q, shape, rate = 1, scale = 1/rate, lower.tail = TRUE,
       log.p = FALSE)
qgamma(p, shape, rate = 1, scale = 1/rate, lower.tail = TRUE,
       log.p = FALSE)
rgamma(n, shape, rate = 1, scale = 1/rate)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GammaDist_+3A_x">x</code>, <code id="GammaDist_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="GammaDist_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="GammaDist_+3A_n">n</code></td>
<td>
<p>number of observations. If <code>length(n) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="GammaDist_+3A_rate">rate</code></td>
<td>
<p>an alternative way to specify the scale.</p>
</td></tr>
<tr><td><code id="GammaDist_+3A_shape">shape</code>, <code id="GammaDist_+3A_scale">scale</code></td>
<td>
<p>shape and scale parameters.  Must be positive,
<code>scale</code> strictly.</p>
</td></tr>
<tr><td><code id="GammaDist_+3A_log">log</code>, <code id="GammaDist_+3A_log.p">log.p</code></td>
<td>
<p>logical; if <code>TRUE</code>, probabilities/densities <code class="reqn">p</code>
are returned as <code class="reqn">log(p)</code>.</p>
</td></tr>
<tr><td><code id="GammaDist_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>scale</code> is omitted, it assumes the default value of <code>1</code>.
</p>
<p>The Gamma distribution with parameters <code>shape</code> <code class="reqn">=\alpha</code>
and <code>scale</code> <code class="reqn">=\sigma</code> has density
</p>
<p style="text-align: center;"><code class="reqn">
    f(x)= \frac{1}{{\sigma}^{\alpha}\Gamma(\alpha)} {x}^{\alpha-1} e^{-x/\sigma}%
  </code>
</p>

<p>for <code class="reqn">x \ge 0</code>, <code class="reqn">\alpha &gt; 0</code> and <code class="reqn">\sigma &gt; 0</code>.
(Here <code class="reqn">\Gamma(\alpha)</code> is the function implemented by <span class="rlang"><b>R</b></span>'s
<code><a href="base.html#topic+gamma">gamma</a>()</code> and defined in its help.  Note that <code class="reqn">a = 0</code>
corresponds to the trivial distribution with all mass at point 0.)
</p>
<p>The mean and variance are
<code class="reqn">E(X) = \alpha\sigma</code> and
<code class="reqn">Var(X) = \alpha\sigma^2</code>.
</p>
<p>The cumulative hazard <code class="reqn">H(t) = - \log(1 - F(t))</code>
is
</p>
<pre>-pgamma(t, ..., lower = FALSE, log = TRUE)
</pre>
<p>Note that for smallish values of <code>shape</code> (and moderate
<code>scale</code>) a large parts of the mass of the Gamma distribution is
on values of <code class="reqn">x</code> so near zero that they will be represented as
zero in computer arithmetic.  So <code>rgamma</code> may well return values
which will be represented as zero.  (This will also happen for very
large values of <code>scale</code> since the actual generation is done for
<code>scale = 1</code>.)
</p>


<h3>Value</h3>

<p><code>dgamma</code> gives the density,
<code>pgamma</code> gives the distribution function,
<code>qgamma</code> gives the quantile function, and
<code>rgamma</code> generates random deviates.
</p>
<p>Invalid arguments will result in return value <code>NaN</code>, with a warning.
</p>
<p>The length of the result is determined by <code>n</code> for
<code>rgamma</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.
</p>
<p>The numerical arguments other than <code>n</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.
</p>


<h3>Note</h3>

<p>The S (Becker <abbr>et al.</abbr>, 1988) parametrization was via <code>shape</code>
and <code>rate</code>: S had no <code>scale</code> parameter. It is an error
to supply both <code>scale</code> and <code>rate</code>.
</p>
<p><code>pgamma</code> is closely related to the incomplete gamma function.  As
defined by Abramowitz and Stegun 6.5.1 (and by &lsquo;Numerical
Recipes&rsquo;) this is
</p>
<p style="text-align: center;"><code class="reqn">P(a,x) = \frac{1}{\Gamma(a)} \int_0^x t^{a-1} e^{-t} dt</code>
</p>

<p><code class="reqn">P(a, x)</code> is <code>pgamma(x, a)</code>.  Other authors (for example
Karl Pearson in his 1922 tables) omit the normalizing factor,
defining the incomplete gamma function <code class="reqn">\gamma(a,x)</code> as
<code class="reqn">\gamma(a,x) = \int_0^x t^{a-1} e^{-t} dt,</code> i.e., <code>pgamma(x, a) * gamma(a)</code>.
Yet other use the &lsquo;upper&rsquo; incomplete gamma function,
</p>
<p style="text-align: center;"><code class="reqn">\Gamma(a,x) = \int_x^\infty t^{a-1} e^{-t} dt,</code>
</p>

<p>which can be computed by
<code>pgamma(x, a, lower = FALSE) * gamma(a)</code>.
</p>
<p>Note however that <code>pgamma(x, a, ..)</code> currently requires <code class="reqn">a &gt; 0</code>,
whereas the incomplete gamma function is also defined for negative
<code class="reqn">a</code>.  In that case, you can use <code>gamma_inc(a,x)</code> (for
<code class="reqn">\Gamma(a,x)</code>) from package <a href="https://CRAN.R-project.org/package=gsl"><span class="pkg">gsl</span></a>.
</p>
<p>See also
<a href="https://en.wikipedia.org/wiki/Incomplete_gamma_function">https://en.wikipedia.org/wiki/Incomplete_gamma_function</a>, or
<a href="https://dlmf.nist.gov/8.2#i">https://dlmf.nist.gov/8.2#i</a>.
</p>


<h3>Source</h3>

<p><code>dgamma</code> is computed via the Poisson density, using code contributed
by Catherine Loader (see <code><a href="#topic+dbinom">dbinom</a></code>).
</p>
<p><code>pgamma</code> uses an unpublished (and not otherwise documented)
algorithm &lsquo;mainly by Morten Welinder&rsquo;.
</p>
<p><code>qgamma</code> is based on a C translation of
</p>
<p>Best, D. J. and D. E. Roberts (1975).
Algorithm AS91. Percentage points of the chi-squared distribution.
<em>Applied Statistics</em>,  <b>24</b>, 385&ndash;388.
</p>
<p>plus a final Newton step to improve the approximation.
</p>
<p><code>rgamma</code> for <code>shape &gt;= 1</code> uses
</p>
<p>Ahrens, J. H. and Dieter, U. (1982).
Generating gamma variates by a modified rejection technique.
<em>Communications of the ACM</em>, <b>25</b>, 47&ndash;54,
</p>
<p>and for <code>0 &lt; shape &lt; 1</code> uses
</p>
<p>Ahrens, J. H. and Dieter, U. (1974).
Computer methods for sampling from gamma, beta, Poisson and binomial
distributions. <em>Computing</em>, <b>12</b>, 223&ndash;246.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988).
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Shea, B. L. (1988).
Algorithm AS 239: Chi-squared and incomplete Gamma integral,
<em>Applied Statistics (JRSS C)</em>, <b>37</b>, 466&ndash;473.
<a href="https://doi.org/10.2307/2347328">doi:10.2307/2347328</a>.
</p>
<p>Abramowitz, M. and Stegun, I. A. (1972)
<em>Handbook of Mathematical Functions.</em> New York: Dover.
Chapter 6: Gamma and Related Functions.
</p>
<p>NIST Digital Library of Mathematical Functions.
<a href="https://dlmf.nist.gov/">https://dlmf.nist.gov/</a>, section 8.2.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+gamma">gamma</a></code> for the gamma function.
</p>
<p><a href="#topic+Distributions">Distributions</a> for other standard distributions, including
<code><a href="#topic+dbeta">dbeta</a></code> for the Beta distribution and <code><a href="#topic+dchisq">dchisq</a></code>
for the chi-squared distribution which is a special case of the Gamma
distribution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>-log(dgamma(1:4, shape = 1))
p &lt;- (1:9)/10
pgamma(qgamma(p, shape = 2), shape = 2)
1 - 1/exp(qgamma(p, shape = 1))

# even for shape = 0.001 about half the mass is on numbers
# that cannot be represented accurately (and most of those as zero)
pgamma(.Machine$double.xmin, 0.001)
pgamma(5e-324, 0.001)  # on most machines 5e-324 is the smallest
                       # representable non-zero number
table(rgamma(1e4, 0.001) == 0)/1e4
</code></pre>

<hr>
<h2 id='Geometric'>The Geometric Distribution</h2><span id='topic+Geometric'></span><span id='topic+dgeom'></span><span id='topic+pgeom'></span><span id='topic+qgeom'></span><span id='topic+rgeom'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the geometric distribution with parameter <code>prob</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dgeom(x, prob, log = FALSE)
pgeom(q, prob, lower.tail = TRUE, log.p = FALSE)
qgeom(p, prob, lower.tail = TRUE, log.p = FALSE)
rgeom(n, prob)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Geometric_+3A_x">x</code>, <code id="Geometric_+3A_q">q</code></td>
<td>
<p>vector of quantiles representing the number of failures in
a sequence of Bernoulli trials before success occurs.</p>
</td></tr>
<tr><td><code id="Geometric_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="Geometric_+3A_n">n</code></td>
<td>
<p>number of observations. If <code>length(n) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="Geometric_+3A_prob">prob</code></td>
<td>
<p>probability of success in each trial. <code>0 &lt; prob &lt;= 1</code>.</p>
</td></tr>
<tr><td><code id="Geometric_+3A_log">log</code>, <code id="Geometric_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="Geometric_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The geometric distribution with <code>prob</code> <code class="reqn">= p</code> has density
</p>
<p style="text-align: center;"><code class="reqn">p(x) = p {(1-p)}^{x}</code>
</p>

<p>for <code class="reqn">x = 0, 1, 2, \ldots</code>, <code class="reqn">0 &lt; p \le 1</code>.
</p>
<p>If an element of <code>x</code> is not integer, the result of <code>dgeom</code>
is zero, with a warning.
</p>
<p>The quantile is defined as the smallest value <code class="reqn">x</code> such that
<code class="reqn">F(x) \ge p</code>, where <code class="reqn">F</code> is the distribution function.
</p>


<h3>Value</h3>

<p><code>dgeom</code> gives the density,
<code>pgeom</code> gives the distribution function,
<code>qgeom</code> gives the quantile function, and
<code>rgeom</code> generates random deviates.
</p>
<p>Invalid <code>prob</code> will result in return value <code>NaN</code>, with a warning.
</p>
<p>The length of the result is determined by <code>n</code> for
<code>rgeom</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.
</p>
<p>The numerical arguments other than <code>n</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.
</p>
<p><code>rgeom</code> returns a vector of type <a href="base.html#topic+integer">integer</a> unless generated
values exceed the maximum representable integer when <code><a href="base.html#topic+double">double</a></code>
values are returned.
</p>


<h3>Source</h3>

<p><code>dgeom</code> computes via <code>dbinom</code>, using code contributed by
Catherine Loader (see <code><a href="#topic+dbinom">dbinom</a></code>).
</p>
<p><code>pgeom</code> and <code>qgeom</code> are based on the closed-form formulae.
</p>
<p><code>rgeom</code> uses the derivation as an exponential mixture of Poisson
distributions, see 
</p>
<p>Devroye, L. (1986) <em>Non-Uniform Random Variate Generation.</em>
Springer-Verlag, New York. Page 480.
</p>


<h3>See Also</h3>

<p><a href="#topic+Distributions">Distributions</a> for other standard distributions, including
<code><a href="#topic+dnbinom">dnbinom</a></code> for the negative binomial which generalizes
the geometric distribution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>qgeom((1:9)/10, prob = .2)
Ni &lt;- rgeom(20, prob = 1/4); table(factor(Ni, 0:max(Ni)))
</code></pre>

<hr>
<h2 id='getInitial'>Get Initial Parameter Estimates</h2><span id='topic+getInitial'></span><span id='topic+getInitial.default'></span><span id='topic+getInitial.formula'></span><span id='topic+getInitial.selfStart'></span>

<h3>Description</h3>

<p>This function evaluates initial parameter estimates for a nonlinear
regression model.  If <code>data</code> is a parameterized data frame or
<code>pframe</code> object, its <code>parameters</code> attribute is returned.
Otherwise the object is examined to see if it contains a call to a
<code>selfStart</code> object whose <code>initial</code> attribute can be
evaluated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getInitial(object, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getInitial_+3A_object">object</code></td>
<td>
<p>a formula or a <code><a href="#topic+selfStart">selfStart</a></code> model that defines a
nonlinear regression model</p>
</td></tr>
<tr><td><code id="getInitial_+3A_data">data</code></td>
<td>
<p>a data frame in which the expressions in the formula or
arguments to the <code>selfStart</code> model can be evaluated</p>
</td></tr>
<tr><td><code id="getInitial_+3A_...">...</code></td>
<td>
<p>optional additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named numeric vector or list of starting estimates for the
parameters.  The construction of many <code>selfStart</code> models is such
that these &quot;starting&quot; estimates are, in fact, the converged parameter
estimates.
</p>


<h3>Author(s)</h3>

<p>Jos Pinheiro and Douglas Bates</p>


<h3>See Also</h3>

<p><code><a href="#topic+nls">nls</a></code>, <code><a href="#topic+selfStart">selfStart</a></code>,
<code><a href="#topic+selfStart.default">selfStart.default</a></code>, <code><a href="#topic+selfStart.formula">selfStart.formula</a></code>.
Further,  <code><a href="nlme.html#topic+nlsList">nlsList</a></code> from <a href="https://CRAN.R-project.org/package=nlme"><span class="pkg">nlme</span></a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>PurTrt &lt;- Puromycin[ Puromycin$state == "treated", ]
print(getInitial( rate ~ SSmicmen( conc, Vm, K ), PurTrt ), digits = 3)
</code></pre>

<hr>
<h2 id='glm'>Fitting Generalized Linear Models</h2><span id='topic+glm'></span><span id='topic+glm.fit'></span><span id='topic+weights.glm'></span>

<h3>Description</h3>

<p><code>glm</code> is used to fit generalized linear models, specified by
giving a symbolic description of the linear predictor and a
description of the error distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glm(formula, family = gaussian, data, weights, subset,
    na.action, start = NULL, etastart, mustart, offset,
    control = list(...), model = TRUE, method = "glm.fit",
    x = FALSE, y = TRUE, singular.ok = TRUE, contrasts = NULL, ...)

glm.fit(x, y, weights = rep.int(1, nobs),
        start = NULL, etastart = NULL, mustart = NULL,
        offset = rep.int(0, nobs), family = gaussian(),
        control = list(), intercept = TRUE, singular.ok = TRUE)

## S3 method for class 'glm'
weights(object, type = c("prior", "working"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glm_+3A_formula">formula</code></td>
<td>
<p>an object of class <code>"<a href="#topic+formula">formula</a>"</code> (or one that
can be coerced to that class): a symbolic description of the
model to be fitted.  The details of model specification are given
under &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="glm_+3A_family">family</code></td>
<td>
<p>a description of the error distribution and link
function to be used in the model.  For <code>glm</code> this can be a
character string naming a family function, a family function or the
result of a call to a family function.  For <code>glm.fit</code> only the
third option is supported.  (See <code><a href="#topic+family">family</a></code> for details of
family functions.)</p>
</td></tr>
<tr><td><code id="glm_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment (or object
coercible by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code> to a data frame) containing
the variables in the model.  If not found in <code>data</code>, the
variables are taken from <code>environment(formula)</code>,
typically the environment from which <code>glm</code> is called.</p>
</td></tr>
<tr><td><code id="glm_+3A_weights">weights</code></td>
<td>
<p>an optional vector of &lsquo;prior weights&rsquo; to be used
in the fitting process.  Should be <code>NULL</code> or a numeric vector.</p>
</td></tr>
<tr><td><code id="glm_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used in the fitting process.</p>
</td></tr>
<tr><td><code id="glm_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen
when the data contain <code>NA</code>s.  The default is set by
the <code>na.action</code> setting of <code><a href="base.html#topic+options">options</a></code>, and is
<code><a href="#topic+na.fail">na.fail</a></code> if that is unset.  The &lsquo;factory-fresh&rsquo;
default is <code><a href="#topic+na.omit">na.omit</a></code>.  Another possible value is
<code>NULL</code>, no action.  Value <code><a href="#topic+na.exclude">na.exclude</a></code> can be useful.</p>
</td></tr>
<tr><td><code id="glm_+3A_start">start</code></td>
<td>
<p>starting values for the parameters in the linear predictor.</p>
</td></tr>
<tr><td><code id="glm_+3A_etastart">etastart</code></td>
<td>
<p>starting values for the linear predictor.</p>
</td></tr>
<tr><td><code id="glm_+3A_mustart">mustart</code></td>
<td>
<p>starting values for the vector of means.</p>
</td></tr>
<tr><td><code id="glm_+3A_offset">offset</code></td>
<td>
<p>this can be used to specify an <em>a priori</em> known
component to be included in the linear predictor during fitting.
This should be <code>NULL</code> or a numeric vector of length equal to
the number of cases.  One or more <code><a href="#topic+offset">offset</a></code> terms can be
included in the formula instead or as well, and if more than one is
specified their sum is used.  See <code><a href="#topic+model.offset">model.offset</a></code>.</p>
</td></tr>
<tr><td><code id="glm_+3A_control">control</code></td>
<td>
<p>a list of parameters for controlling the fitting
process.  For <code>glm.fit</code> this is passed to
<code><a href="#topic+glm.control">glm.control</a></code>.</p>
</td></tr>
<tr><td><code id="glm_+3A_model">model</code></td>
<td>
<p>a logical value indicating whether <em>model frame</em>
should be included as a component of the returned value.</p>
</td></tr>
<tr><td><code id="glm_+3A_method">method</code></td>
<td>
<p>the method to be used in fitting the model.  The default
method <code>"glm.fit"</code> uses iteratively reweighted least squares
(IWLS): the alternative <code>"model.frame"</code> returns the model frame
and does no fitting.
</p>
<p>User-supplied fitting functions can be supplied either as a function
or a character string naming a function, with a function which takes
the same arguments as <code>glm.fit</code>.  If specified as a character
string it is looked up from within the <span class="pkg">stats</span> namespace.
</p>
</td></tr>
<tr><td><code id="glm_+3A_x">x</code>, <code id="glm_+3A_y">y</code></td>
<td>
<p>For <code>glm</code>:
logical values indicating whether the response vector and model
matrix used in the fitting process should be returned as components
of the returned value.
</p>
<p>For <code>glm.fit</code>: <code>x</code> is a design matrix of dimension
<code>n * p</code>, and <code>y</code> is a vector of observations of length
<code>n</code>.
</p>
</td></tr>
<tr><td><code id="glm_+3A_singular.ok">singular.ok</code></td>
<td>
<p>logical; if <code>FALSE</code> a singular fit is an
error.</p>
</td></tr>
<tr><td><code id="glm_+3A_contrasts">contrasts</code></td>
<td>
<p>an optional list. See the <code>contrasts.arg</code>
of <code>model.matrix.default</code>.</p>
</td></tr>
<tr><td><code id="glm_+3A_intercept">intercept</code></td>
<td>
<p>logical. Should an intercept be included in the
<em>null</em> model?</p>
</td></tr>
<tr><td><code id="glm_+3A_object">object</code></td>
<td>
<p>an object inheriting from class <code>"glm"</code>.</p>
</td></tr>
<tr><td><code id="glm_+3A_type">type</code></td>
<td>
<p>character, partial matching allowed.  Type of weights to
extract from the fitted model object.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="glm_+3A_...">...</code></td>
<td>

<p>For <code>glm</code>: arguments to be used to form the default
<code>control</code> argument if it is not supplied directly.
</p>
<p>For <code>weights</code>: further arguments passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A typical predictor has the form <code>response ~ terms</code> where
<code>response</code> is the (numeric) response vector and <code>terms</code> is a
series of terms which specifies a linear predictor for
<code>response</code>.  For <code>binomial</code> and <code>quasibinomial</code>
families the response can also be specified as a <code><a href="base.html#topic+factor">factor</a></code>
(when the first level denotes failure and all others success) or as a
two-column matrix with the columns giving the numbers of successes and
failures.  A terms specification of the form <code>first + second</code>
indicates all the terms in <code>first</code> together with all the terms in
<code>second</code> with any duplicates removed.
</p>
<p>A specification of the form <code>first:second</code> indicates the set
of terms obtained by taking the interactions of all terms in
<code>first</code> with all terms in <code>second</code>.  The specification
<code>first*second</code> indicates the <em>cross</em> of <code>first</code> and
<code>second</code>.  This is the same as <code>first + second +
  first:second</code>.
</p>
<p>The terms in the formula will be re-ordered so that main effects come
first, followed by the interactions, all second-order, all third-order
and so on: to avoid this pass a <code>terms</code> object as the formula.
</p>
<p>Non-<code>NULL</code> <code>weights</code> can be used to indicate that different
observations have different dispersions (with the values in
<code>weights</code> being inversely proportional to the dispersions); or
equivalently, when the elements of <code>weights</code> are positive
integers <code class="reqn">w_i</code>, that each response <code class="reqn">y_i</code> is the mean of
<code class="reqn">w_i</code> unit-weight observations.  For a binomial GLM prior weights
are used to give the number of trials when the response is the
proportion of successes: they would rarely be used for a Poisson GLM.
</p>
<p><code>glm.fit</code> is the workhorse function: it is not normally called
directly but can be more efficient where the response vector, design
matrix and family have already been calculated.
</p>
<p>If more than one of <code>etastart</code>, <code>start</code> and <code>mustart</code>
is specified, the first in the list will be used.  It is often
advisable to supply starting values for a <code><a href="#topic+quasi">quasi</a></code> family,
and also for families with unusual links such as <code>gaussian("log")</code>.
</p>
<p>All of <code>weights</code>, <code>subset</code>, <code>offset</code>, <code>etastart</code>
and <code>mustart</code> are evaluated in the same way as variables in
<code>formula</code>, that is first in <code>data</code> and then in the
environment of <code>formula</code>.
</p>
<p>For the background to warning messages about
&lsquo;fitted probabilities numerically 0 or 1 occurred&rsquo;
for binomial GLMs, see
Venables &amp; Ripley (2002, pp. 197&ndash;8).
</p>


<h3>Value</h3>

<p><code>glm</code> returns an object of class inheriting from <code>"glm"</code>
which inherits from the class <code>"lm"</code>. See later in this section.
If a non-standard <code>method</code> is used, the object will also inherit
from the class (if any) returned by that function.
</p>
<p>The function <code><a href="base.html#topic+summary">summary</a></code> (i.e., <code><a href="#topic+summary.glm">summary.glm</a></code>) can
be used to obtain or print a summary of the results and the function
<code><a href="#topic+anova">anova</a></code> (i.e., <code><a href="#topic+anova.glm">anova.glm</a></code>)
to produce an analysis of variance table.
</p>
<p>The generic accessor functions <code><a href="#topic+coefficients">coefficients</a></code>,
<code>effects</code>, <code>fitted.values</code> and <code>residuals</code> can be used to
extract various useful features of the value returned by <code>glm</code>.
</p>
<p><code>weights</code> extracts a vector of weights, one for each case in the
fit (after subsetting and <code>na.action</code>).
</p>
<p>An object of class <code>"glm"</code> is a list containing at least the
following components:
</p>
<table>
<tr><td><code>coefficients</code></td>
<td>
<p>a named vector of coefficients</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>the <em>working</em> residuals, that is the residuals
in the final iteration of the IWLS fit.  Since cases with zero
weights are omitted, their working residuals are <code>NA</code>.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>the fitted mean values, obtained by transforming
the linear predictors by the inverse of the link function.</p>
</td></tr>
<tr><td><code>rank</code></td>
<td>
<p>the numeric rank of the fitted linear model.</p>
</td></tr>
<tr><td><code>family</code></td>
<td>
<p>the <code><a href="#topic+family">family</a></code> object used.</p>
</td></tr>
<tr><td><code>linear.predictors</code></td>
<td>
<p>the linear fit on link scale.</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>
<p>up to a constant, minus twice the maximized
log-likelihood.  Where sensible, the constant is chosen so that a
saturated model has deviance zero.</p>
</td></tr>
<tr><td><code>aic</code></td>
<td>
<p>A version of Akaike's <em>An Information Criterion</em>,
minus twice the maximized log-likelihood plus twice the number of
parameters, computed via the <code>aic</code> component of the family.
For binomial and Poison families the dispersion is
fixed at one and the number of parameters is the number of
coefficients.  For gaussian, Gamma and inverse gaussian families the
dispersion is estimated from the residual deviance, and the number
of parameters is the number of coefficients plus one.  For a
gaussian family the MLE of the dispersion is used so this is a valid
value of AIC, but for Gamma and inverse gaussian families it is not.
For families fitted by quasi-likelihood the value is <code>NA</code>.</p>
</td></tr>
<tr><td><code>null.deviance</code></td>
<td>
<p>The deviance for the null model, comparable with
<code>deviance</code>. The null model will include the offset, and an
intercept if there is one in the model.  Note that this will be
incorrect if the link function depends on the data other than
through the fitted mean: specify a zero offset to force a correct
calculation.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>the number of iterations of IWLS used.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>the <em>working</em> weights, that is the weights
in the final iteration of the IWLS fit.</p>
</td></tr>
<tr><td><code>prior.weights</code></td>
<td>
<p>the weights initially supplied, a vector of
<code>1</code>s if none were.</p>
</td></tr>
<tr><td><code>df.residual</code></td>
<td>
<p>the residual degrees of freedom.</p>
</td></tr>
<tr><td><code>df.null</code></td>
<td>
<p>the residual degrees of freedom for the null model.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>if requested (the default) the <code>y</code> vector
used. (It is a vector even for a binomial model.)</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>if requested, the model matrix.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>if requested (the default), the model frame.</p>
</td></tr>
<tr><td><code>converged</code></td>
<td>
<p>logical. Was the IWLS algorithm judged to have converged?</p>
</td></tr>
<tr><td><code>boundary</code></td>
<td>
<p>logical. Is the fitted value on the boundary of the
attainable values?</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>formula</code></td>
<td>
<p>the formula supplied.</p>
</td></tr>
<tr><td><code>terms</code></td>
<td>
<p>the <code><a href="#topic+terms">terms</a></code> object used.</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>the <code>data argument</code>.</p>
</td></tr>
<tr><td><code>offset</code></td>
<td>
<p>the offset vector used.</p>
</td></tr>
<tr><td><code>control</code></td>
<td>
<p>the value of the <code>control</code> argument used.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the name of the fitter function used (when provided as a
<code><a href="base.html#topic+character">character</a></code> string to <code>glm()</code>) or the fitter
<code><a href="base.html#topic+function">function</a></code> (when provided as that).</p>
</td></tr>
<tr><td><code>contrasts</code></td>
<td>
<p>(where relevant) the contrasts used.</p>
</td></tr>
<tr><td><code>xlevels</code></td>
<td>
<p>(where relevant) a record of the levels of the factors
used in fitting.</p>
</td></tr>
<tr><td><code>na.action</code></td>
<td>
<p>(where relevant) information returned by
<code><a href="#topic+model.frame">model.frame</a></code> on the special handling of <code>NA</code>s.</p>
</td></tr>
</table>
<p>In addition, non-empty fits will have components <code>qr</code>, <code>R</code>
and <code>effects</code> relating to the final weighted linear fit.
</p>
<p>Objects of class <code>"glm"</code> are normally of class <code>c("glm",
    "lm")</code>, that is inherit from class <code>"lm"</code>, and well-designed
methods for class <code>"lm"</code> will be applied to the weighted linear
model at the final iteration of IWLS.  However, care is needed, as
extractor functions for class <code>"glm"</code> such as
<code><a href="#topic+residuals">residuals</a></code> and <code>weights</code> do <b>not</b> just pick out
the component of the fit with the same name.
</p>
<p>If a <code><a href="#topic+binomial">binomial</a></code> <code>glm</code> model was specified by giving a
two-column response, the weights returned by <code>prior.weights</code> are
the total numbers of cases (factored by the supplied case weights) and
the component <code>y</code> of the result is the proportion of successes.
</p>


<h3>Fitting functions</h3>

<p>The argument <code>method</code> serves two purposes.  One is to allow the
model frame to be recreated with no fitting.  The other is to allow
the default fitting function <code>glm.fit</code> to be replaced by a
function which takes the same arguments and uses a different fitting
algorithm.  If <code>glm.fit</code> is supplied as a character string it is
used to search for a function of that name, starting in the
<span class="pkg">stats</span> namespace.
</p>
<p>The class of the object return by the fitter (if any) will be
prepended to the class returned by <code>glm</code>.
</p>


<h3>Author(s)</h3>

<p>The original <span class="rlang"><b>R</b></span> implementation of <code>glm</code> was written by Simon
Davies working for Ross Ihaka at the University of Auckland, but has
since been extensively re-written by members of the R Core team.
</p>
<p>The design was inspired by the S function of the same name described
in Hastie &amp; Pregibon (1992).
</p>


<h3>References</h3>

<p>Dobson, A. J. (1990)
<em>An Introduction to Generalized Linear Models.</em>
London: Chapman and Hall.
</p>
<p>Hastie, T. J. and Pregibon, D. (1992)
<em>Generalized linear models.</em>
Chapter 6 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>
<p>McCullagh P. and Nelder, J. A. (1989)
<em>Generalized Linear Models.</em>
London: Chapman and Hall.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em>
New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anova.glm">anova.glm</a></code>, <code><a href="#topic+summary.glm">summary.glm</a></code>, etc. for
<code>glm</code> methods,
and the generic functions <code><a href="#topic+anova">anova</a></code>, <code><a href="base.html#topic+summary">summary</a></code>,
<code><a href="#topic+effects">effects</a></code>, <code><a href="#topic+fitted.values">fitted.values</a></code>,
and <code><a href="#topic+residuals">residuals</a></code>.
</p>
<p><code><a href="#topic+lm">lm</a></code> for non-generalized <em>linear</em> models (which SAS
calls GLMs, for &lsquo;general&rsquo; linear models).
</p>
<p><code><a href="#topic+loglin">loglin</a></code> and <code><a href="MASS.html#topic+loglm">loglm</a></code> (package
<a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a>) for fitting log-linear models (which binomial and
Poisson GLMs are) to contingency tables.
</p>
<p><code>bigglm</code> in package <a href="https://CRAN.R-project.org/package=biglm"><span class="pkg">biglm</span></a> for an alternative
way to fit GLMs to large datasets (especially those with many cases).
</p>
<p><code><a href="datasets.html#topic+esoph">esoph</a></code>, <code><a href="datasets.html#topic+infert">infert</a></code> and
<code><a href="#topic+predict.glm">predict.glm</a></code> have examples of fitting binomial <abbr>GLM</abbr>s.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Dobson (1990) Page 93: Randomized Controlled Trial :
counts &lt;- c(18,17,15,20,10,20,25,13,12)
outcome &lt;- gl(3,1,9)
treatment &lt;- gl(3,3)
data.frame(treatment, outcome, counts) # showing data
glm.D93 &lt;- glm(counts ~ outcome + treatment, family = poisson())
anova(glm.D93)
summary(glm.D93)
## Computing AIC [in many ways]:
(A0 &lt;- AIC(glm.D93))
(ll &lt;- logLik(glm.D93))
A1 &lt;- -2*c(ll) + 2*attr(ll, "df")
A2 &lt;- glm.D93$family$aic(counts, mu=fitted(glm.D93), wt=1) +
        2 * length(coef(glm.D93))
stopifnot(exprs = {
  all.equal(A0, A1)
  all.equal(A1, A2)
  all.equal(A1, glm.D93$aic)
})


## an example with offsets from Venables &amp; Ripley (2002, p.189)
utils::data(anorexia, package = "MASS")

anorex.1 &lt;- glm(Postwt ~ Prewt + Treat + offset(Prewt),
                family = gaussian, data = anorexia)
summary(anorex.1)


# A Gamma example, from McCullagh &amp; Nelder (1989, pp. 300-2)
clotting &lt;- data.frame(
    u = c(5,10,15,20,30,40,60,80,100),
    lot1 = c(118,58,42,35,27,25,21,19,18),
    lot2 = c(69,35,26,21,18,16,13,12,12))
summary(glm(lot1 ~ log(u), data = clotting, family = Gamma))
summary(glm(lot2 ~ log(u), data = clotting, family = Gamma))
## Aliased ("S"ingular) -&gt; 1 NA coefficient
(fS &lt;- glm(lot2 ~ log(u) + log(u^2), data = clotting, family = Gamma))
tools::assertError(update(fS, singular.ok=FALSE), verbose=interactive())
## -&gt; .. "singular fit encountered"

## Not run: 
## for an example of the use of a terms object as a formula
demo(glm.vr)

## End(Not run)</code></pre>

<hr>
<h2 id='glm.control'>Auxiliary for Controlling GLM Fitting</h2><span id='topic+glm.control'></span>

<h3>Description</h3>

<p>Auxiliary function for <code><a href="#topic+glm">glm</a></code> fitting.
Typically only used internally by <code><a href="#topic+glm.fit">glm.fit</a></code>, but may be
used to construct a <code>control</code> argument to either function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glm.control(epsilon = 1e-8, maxit = 25, trace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glm.control_+3A_epsilon">epsilon</code></td>
<td>
<p>positive convergence tolerance <code class="reqn">\epsilon</code>;
the iterations converge when
<code class="reqn">|dev - dev_{old}|/(|dev| + 0.1) &lt; \epsilon</code>.</p>
</td></tr>
<tr><td><code id="glm.control_+3A_maxit">maxit</code></td>
<td>
<p>integer giving the maximal number of IWLS iterations.</p>
</td></tr>
<tr><td><code id="glm.control_+3A_trace">trace</code></td>
<td>
<p>logical indicating if output should be produced for each
iteration.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>control</code> argument of <code><a href="#topic+glm">glm</a></code> is by default passed
to the <code>control</code> argument of <code><a href="#topic+glm.fit">glm.fit</a></code>, which uses
its elements as arguments to <code>glm.control</code>: the latter provides
defaults and sanity checking.
</p>
<p>If <code>epsilon</code> is small (less than <code class="reqn">10^{-10}</code>) it is
also used as the tolerance for the detection of collinearity in the
least squares solution.
</p>
<p>When <code>trace</code> is true, calls to <code><a href="base.html#topic+cat">cat</a></code> produce the
output for each IWLS iteration.  Hence, <code><a href="base.html#topic+options">options</a>(digits = *)</code>
can be used to increase the precision, see the example.
</p>


<h3>Value</h3>

<p>A list with components named as the arguments.
</p>


<h3>References</h3>

<p>Hastie, T. J. and Pregibon, D. (1992)
<em>Generalized linear models.</em>
Chapter 6 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm.fit">glm.fit</a></code>, the fitting procedure used by <code><a href="#topic+glm">glm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### A variation on  example(glm) :

## Annette Dobson's example ...
counts &lt;- c(18,17,15,20,10,20,25,13,12)
outcome &lt;- gl(3,1,9)
treatment &lt;- gl(3,3)
oo &lt;- options(digits = 12) # to see more when tracing :
glm.D93X &lt;- glm(counts ~ outcome + treatment, family = poisson(),
                trace = TRUE, epsilon = 1e-14)
options(oo)
coef(glm.D93X) # the last two are closer to 0 than in ?glm's  glm.D93
</code></pre>

<hr>
<h2 id='glm.summaries'>Accessing Generalized Linear Model Fits</h2><span id='topic+family.glm'></span><span id='topic+residuals.glm'></span>

<h3>Description</h3>

<p>These functions are all <code><a href="utils.html#topic+methods">methods</a></code> for class <code>glm</code> or
<code>summary.glm</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'glm'
family(object, ...)

## S3 method for class 'glm'
residuals(object, type = c("deviance", "pearson", "working",
                           "response", "partial"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glm.summaries_+3A_object">object</code></td>
<td>
<p>an object of class <code>glm</code>, typically the result of
a call to <code><a href="#topic+glm">glm</a></code>.</p>
</td></tr>
<tr><td><code id="glm.summaries_+3A_type">type</code></td>
<td>
<p>the type of residuals which should be returned.
The alternatives are: <code>"deviance"</code> (default), <code>"pearson"</code>,
<code>"working"</code>, <code>"response"</code>, and <code>"partial"</code>.
Can be abbreviated.</p>
</td></tr>
<tr><td><code id="glm.summaries_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The references define the types of residuals: Davison &amp; Snell is a
good reference for the usages of each.
</p>
<p>The partial residuals are a matrix of working residuals, with each
column formed by omitting a term from the model.
</p>
<p>How <code>residuals</code> treats cases with missing values in the original
fit is determined by the <code>na.action</code> argument of that fit.
If <code>na.action = na.omit</code> omitted cases will not appear in the
residuals, whereas if <code>na.action = na.exclude</code> they will appear,
with residual value <code>NA</code>.  See also <code><a href="#topic+naresid">naresid</a></code>.
</p>
<p>For fits done with <code>y = FALSE</code> the response values are computed
from other components.
</p>


<h3>References</h3>

<p>Davison, A. C. and Snell, E. J. (1991)
<em>Residuals and diagnostics.</em>  In: Statistical Theory
and Modelling. In Honour of Sir David Cox, FRS, eds.
Hinkley, D. V., Reid, N. and Snell, E. J., Chapman &amp; Hall.
</p>
<p>Hastie, T. J. and Pregibon, D. (1992)
<em>Generalized linear models.</em>
Chapter 6 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>
<p>McCullagh P. and Nelder, J. A. (1989)
<em>Generalized Linear Models.</em>
London: Chapman and Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm">glm</a></code> for computing <code>glm.obj</code>, <code><a href="#topic+anova.glm">anova.glm</a></code>;
the corresponding <em>generic</em> functions, <code><a href="#topic+summary.glm">summary.glm</a></code>,
<code><a href="#topic+coef">coef</a></code>, <code><a href="#topic+deviance">deviance</a></code>,
<code><a href="#topic+df.residual">df.residual</a></code>,
<code><a href="#topic+effects">effects</a></code>, <code><a href="#topic+fitted">fitted</a></code>,
<code><a href="#topic+residuals">residuals</a></code>.
</p>
<p><a href="#topic+influence.measures">influence.measures</a> for deletion diagnostics, including
standardized (<code><a href="#topic+rstandard">rstandard</a></code>)
and studentized (<code><a href="#topic+rstudent">rstudent</a></code>) residuals.
</p>

<hr>
<h2 id='hclust'>Hierarchical Clustering</h2><span id='topic+hclust'></span><span id='topic+plot.hclust'></span><span id='topic+print.hclust'></span>

<h3>Description</h3>

<p>Hierarchical cluster analysis on a set of dissimilarities and
methods for analyzing it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hclust(d, method = "complete", members = NULL)

## S3 method for class 'hclust'
plot(x, labels = NULL, hang = 0.1, check = TRUE,
     axes = TRUE, frame.plot = FALSE, ann = TRUE,
     main = "Cluster Dendrogram",
     sub = NULL, xlab = NULL, ylab = "Height", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hclust_+3A_d">d</code></td>
<td>
<p>a dissimilarity structure as produced by <code>dist</code>.</p>
</td></tr>
<tr><td><code id="hclust_+3A_method">method</code></td>
<td>
<p>the agglomeration method to be used.  This should
be (an unambiguous abbreviation of) one of
<code>"ward.D"</code>, <code>"ward.D2"</code>, <code>"single"</code>, <code>"complete"</code>,
<code>"average"</code> (= <abbr>UPGMA</abbr>), <code>"mcquitty"</code> (= <abbr>WPGMA</abbr>),
<code>"median"</code> (= <abbr>WPGMC</abbr>) or <code>"centroid"</code> (= <abbr>UPGMC</abbr>).</p>
</td></tr>
<tr><td><code id="hclust_+3A_members">members</code></td>
<td>
<p><code>NULL</code> or a vector with length size of
<code>d</code>. See the &lsquo;Details&rsquo; section.</p>
</td></tr>
<tr><td><code id="hclust_+3A_x">x</code></td>
<td>
<p>an object of the type produced by <code>hclust</code>.</p>
</td></tr>
<tr><td><code id="hclust_+3A_hang">hang</code></td>
<td>
<p>The fraction of the plot height by which labels should hang
below the rest of the plot.
A negative value will cause the labels to hang down from 0.</p>
</td></tr>
<tr><td><code id="hclust_+3A_check">check</code></td>
<td>
<p>logical indicating if the <code>x</code> object should be
checked for validity.  This check is not necessary when <code>x</code>
is known to be valid such as when it is the direct result of
<code>hclust()</code>.  The default is <code>check=TRUE</code>, as invalid
inputs may crash <span class="rlang"><b>R</b></span> due to memory violation in the internal C
plotting code.</p>
</td></tr>
<tr><td><code id="hclust_+3A_labels">labels</code></td>
<td>
<p>A character vector of labels for the leaves of the
tree.  By default the row names or row numbers of the original data are
used.  If <code>labels = FALSE</code> no labels at all are plotted.</p>
</td></tr>
<tr><td><code id="hclust_+3A_axes">axes</code>, <code id="hclust_+3A_frame.plot">frame.plot</code>, <code id="hclust_+3A_ann">ann</code></td>
<td>
<p>logical flags as in <code><a href="graphics.html#topic+plot.default">plot.default</a></code>.</p>
</td></tr>
<tr><td><code id="hclust_+3A_main">main</code>, <code id="hclust_+3A_sub">sub</code>, <code id="hclust_+3A_xlab">xlab</code>, <code id="hclust_+3A_ylab">ylab</code></td>
<td>
<p>character strings for
<code><a href="graphics.html#topic+title">title</a></code>.  <code>sub</code> and <code>xlab</code> have a non-NULL
default when there's a <code>tree$call</code>.</p>
</td></tr>
<tr><td><code id="hclust_+3A_...">...</code></td>
<td>
<p>Further graphical arguments.  E.g., <code>cex</code> controls
the size of the labels (if plotted) in the same way as <code><a href="graphics.html#topic+text">text</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs a hierarchical cluster analysis
using a set of dissimilarities for the <code class="reqn">n</code> objects being
clustered.  Initially, each object is assigned to its own
cluster and then the algorithm proceeds iteratively,
at each stage joining the two most similar clusters,
continuing until there is just a single cluster.
At each stage distances between clusters are recomputed
by the Lance&ndash;Williams dissimilarity update formula
according to the particular clustering method being used.
</p>
<p>A number of different clustering methods are provided.  <em>Ward's</em>
minimum variance method aims at finding compact, spherical clusters.
The <em>complete linkage</em> method finds similar clusters. The
<em>single linkage</em> method (which is closely related to the minimal
spanning tree) adopts a &lsquo;friends of friends&rsquo; clustering
strategy.  The other methods can be regarded as aiming for clusters
with characteristics somewhere between the single and complete link
methods.  Note however, that methods <code>"median"</code> and
<code>"centroid"</code> are <em>not</em> leading to a <em>monotone distance</em>
measure, or equivalently the resulting dendrograms can have so called
<em>inversions</em> or <em>reversals</em> which are hard to interpret,
but note the trichotomies in Legendre and Legendre (2012).
</p>
<p>Two different algorithms are found in the literature for Ward clustering.
The one used by option <code>"ward.D"</code> (equivalent to the only Ward
option <code>"ward"</code> in <span class="rlang"><b>R</b></span> versions <code class="reqn">\le</code> 3.0.3) <em>does not</em> implement
Ward's (1963) clustering criterion, whereas option <code>"ward.D2"</code> implements
that criterion (Murtagh and Legendre 2014).  With the latter, the
dissimilarities are <em>squared</em> before cluster updating.
Note that <code><a href="cluster.html#topic+agnes">agnes</a>(*, method="ward")</code> corresponds
to <code>hclust(*, "ward.D2")</code>.
</p>
<p>If <code>members != NULL</code>, then <code>d</code> is taken to be a
dissimilarity matrix between clusters instead of dissimilarities
between singletons and <code>members</code> gives the number of observations
per cluster.  This way the hierarchical cluster algorithm can be
&lsquo;started in the middle of the dendrogram&rsquo;, e.g., in order to
reconstruct the part of the tree above a cut (see examples).
Dissimilarities between clusters can be efficiently computed (i.e.,
without <code>hclust</code> itself) only for a limited number of
distance/linkage combinations, the simplest one being <em>squared</em>
Euclidean distance and centroid linkage.  In this case the
dissimilarities between the clusters are the squared Euclidean
distances between cluster means.
</p>
<p>In hierarchical cluster displays, a decision is needed at each merge to
specify which subtree should go on the left and which on the right.
Since, for <code class="reqn">n</code> observations there are <code class="reqn">n-1</code> merges,
there are <code class="reqn">2^{(n-1)}</code> possible orderings for the leaves
in a cluster tree, or dendrogram.
The algorithm used in <code>hclust</code> is to order the subtree so that
the tighter cluster is on the left (the last, i.e., most recent,
merge of the left subtree is at a lower value than the last
merge of the right subtree).
Single observations are the tightest clusters possible,
and merges involving two observations place them in order by their
observation sequence number.
</p>


<h3>Value</h3>

<p>An object of class <code>"hclust"</code> which describes the
tree produced by the clustering process.
The object is a list with components:
</p>
<table>
<tr><td><code>merge</code></td>
<td>
<p>an <code class="reqn">n-1</code> by 2 matrix.
Row <code class="reqn">i</code> of <code>merge</code> describes the merging of clusters
at step <code class="reqn">i</code> of the clustering.
If an element <code class="reqn">j</code> in the row is negative,
then observation <code class="reqn">-j</code> was merged at this stage.
If <code class="reqn">j</code> is positive then the merge
was with the cluster formed at the (earlier) stage <code class="reqn">j</code>
of the algorithm.
Thus negative entries in <code>merge</code> indicate agglomerations
of singletons, and positive entries indicate agglomerations
of non-singletons.</p>
</td></tr>
<tr><td><code>height</code></td>
<td>
<p>a set of <code class="reqn">n-1</code> real values (non-decreasing for
ultrametric trees).
The clustering <em>height</em>: that is, the value of
the criterion associated with the clustering
<code>method</code> for the particular agglomeration.</p>
</td></tr>
<tr><td><code>order</code></td>
<td>
<p>a vector giving the permutation of the original
observations suitable for plotting, in the sense that a cluster
plot using this ordering and matrix <code>merge</code> will not have
crossings of the branches.</p>
</td></tr>
<tr><td><code>labels</code></td>
<td>
<p>labels for each of the objects being clustered.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call which produced the result.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the cluster method that has been used.</p>
</td></tr>
<tr><td><code>dist.method</code></td>
<td>
<p>the distance that has been used to create <code>d</code>
(only returned if the distance object has a <code>"method"</code>
attribute).</p>
</td></tr>
</table>
<p>There are <code><a href="base.html#topic+print">print</a></code>, <code><a href="graphics.html#topic+plot">plot</a></code> and <code>identify</code>
(see <code><a href="#topic+identify.hclust">identify.hclust</a></code>) methods and the
<code><a href="#topic+rect.hclust">rect.hclust</a>()</code> function for <code>hclust</code> objects.
</p>


<h3>Note</h3>

<p>Method <code>"centroid"</code> is typically meant to be used with
<em>squared</em> Euclidean distances.
</p>


<h3>Author(s)</h3>

<p>The <code>hclust</code> function is based on Fortran code
contributed to STATLIB by F. Murtagh.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988).
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole. (S version.)
</p>
<p>Everitt, B. (1974).
<em>Cluster Analysis</em>.
London: Heinemann Educ. Books.
</p>
<p>Hartigan, J.A. (1975).
<em>Clustering  Algorithms</em>.
New York: Wiley.
</p>
<p>Sneath, P. H. A. and R. R. Sokal (1973).
<em>Numerical Taxonomy</em>.
San Francisco: Freeman.
</p>
<p>Anderberg, M. R. (1973).
<em>Cluster Analysis for Applications</em>.
Academic Press: New York.
</p>
<p>Gordon, A. D. (1999).
<em>Classification</em>. Second Edition.
London: Chapman and Hall / CRC
</p>
<p>Murtagh, F. (1985).
&ldquo;Multidimensional Clustering Algorithms&rdquo;, in
<em>COMPSTAT Lectures 4</em>.
Wuerzburg: Physica-Verlag
(for algorithmic details of algorithms used).
</p>
<p>McQuitty, L.L. (1966).
Similarity Analysis by Reciprocal Pairs for Discrete and Continuous
Data.
<em>Educational and Psychological Measurement</em>, <b>26</b>, 825&ndash;831.
<a href="https://doi.org/10.1177/001316446602600402">doi:10.1177/001316446602600402</a>.
</p>
<p>Legendre, P. and L. Legendre (2012).
<em>Numerical Ecology</em>,
3rd English ed. Amsterdam: Elsevier Science BV.
</p>
<p>Murtagh, Fionn and Legendre, Pierre (2014).
Ward's hierarchical agglomerative clustering method: which algorithms
implement Ward's criterion?
<em>Journal of Classification</em>, <b>31</b>, 274&ndash;295.
<a href="https://doi.org/10.1007/s00357-014-9161-z">doi:10.1007/s00357-014-9161-z</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+identify.hclust">identify.hclust</a></code>, <code><a href="#topic+rect.hclust">rect.hclust</a></code>,
<code><a href="#topic+cutree">cutree</a></code>, <code><a href="#topic+dendrogram">dendrogram</a></code>, <code><a href="#topic+kmeans">kmeans</a></code>.
</p>
<p>For the Lance&ndash;Williams formula and methods that apply it generally,
see <code><a href="cluster.html#topic+agnes">agnes</a></code> from package <a href="https://CRAN.R-project.org/package=cluster"><span class="pkg">cluster</span></a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

### Example 1: Violent crime rates by US state

hc &lt;- hclust(dist(USArrests), "ave")
plot(hc)
plot(hc, hang = -1)

## Do the same with centroid clustering and *squared* Euclidean distance,
## cut the tree into ten clusters and reconstruct the upper part of the
## tree from the cluster centers.
hc &lt;- hclust(dist(USArrests)^2, "cen")
memb &lt;- cutree(hc, k = 10)
cent &lt;- NULL
for(k in 1:10){
  cent &lt;- rbind(cent, colMeans(USArrests[memb == k, , drop = FALSE]))
}
hc1 &lt;- hclust(dist(cent)^2, method = "cen", members = table(memb))
opar &lt;- par(mfrow = c(1, 2))
plot(hc,  labels = FALSE, hang = -1, main = "Original Tree")
plot(hc1, labels = FALSE, hang = -1, main = "Re-start from 10 clusters")
par(opar)

### Example 2: Straight-line distances among 10 US cities
##  Compare the results of algorithms "ward.D" and "ward.D2"

mds2 &lt;- -cmdscale(UScitiesD)
plot(mds2, type="n", axes=FALSE, ann=FALSE)
text(mds2, labels=rownames(mds2), xpd = NA)

hcity.D  &lt;- hclust(UScitiesD, "ward.D") # "wrong"
hcity.D2 &lt;- hclust(UScitiesD, "ward.D2")
opar &lt;- par(mfrow = c(1, 2))
plot(hcity.D,  hang=-1)
plot(hcity.D2, hang=-1)
par(opar)
</code></pre>

<hr>
<h2 id='heatmap'> Draw a Heat Map </h2><span id='topic+heatmap'></span>

<h3>Description</h3>

<p>A heat map is a false color image (basically
<code><a href="Matrix.html#topic+image">image</a>(t(x))</code>) with a dendrogram added to the left side
and to the top.  Typically, reordering of the rows and columns
according to some set of values (row or column means) within the
restrictions imposed by the dendrogram is carried out.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>heatmap(x, Rowv = NULL, Colv = if(symm)"Rowv" else NULL,
        distfun = dist, hclustfun = hclust,
        reorderfun = function(d, w) reorder(d, w),
        add.expr, symm = FALSE, revC = identical(Colv, "Rowv"),
        scale = c("row", "column", "none"), na.rm = TRUE,
        margins = c(5, 5), ColSideColors, RowSideColors,
        cexRow = 0.2 + 1/log10(nr), cexCol = 0.2 + 1/log10(nc),
        labRow = NULL, labCol = NULL, main = NULL,
        xlab = NULL, ylab = NULL,
        keep.dendro = FALSE, verbose = getOption("verbose"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="heatmap_+3A_x">x</code></td>
<td>
<p>numeric matrix of the values to be plotted. </p>
</td></tr>
<tr><td><code id="heatmap_+3A_rowv">Rowv</code></td>
<td>
<p>determines if and how the <em>row</em> dendrogram should be
computed and reordered.  Either a <code><a href="#topic+dendrogram">dendrogram</a></code> or a
vector of values used to reorder the row dendrogram or
<code><a href="base.html#topic+NA">NA</a></code> to suppress any row dendrogram (and reordering) or
by default, <code><a href="base.html#topic+NULL">NULL</a></code>, see &lsquo;Details&rsquo; below.</p>
</td></tr>
<tr><td><code id="heatmap_+3A_colv">Colv</code></td>
<td>
<p>determines if and how the <em>column</em> dendrogram should be
reordered.  Has the same options as the <code>Rowv</code> argument above and
<em>additionally</em> when <code>x</code> is a square matrix, <code>Colv =
      "Rowv"</code> means that columns should be treated identically to the
rows (and so if there is to be no row dendrogram there will not be a
column one either).</p>
</td></tr>
<tr><td><code id="heatmap_+3A_distfun">distfun</code></td>
<td>
<p>function used to compute the distance (dissimilarity)
between both rows and columns.  Defaults to <code><a href="#topic+dist">dist</a></code>.</p>
</td></tr>
<tr><td><code id="heatmap_+3A_hclustfun">hclustfun</code></td>
<td>
<p>function used to compute the hierarchical clustering
when <code>Rowv</code> or <code>Colv</code> are not dendrograms.  Defaults to
<code><a href="#topic+hclust">hclust</a></code>. Should take as argument a result of <code>distfun</code>
and return an object to which <code><a href="#topic+as.dendrogram">as.dendrogram</a></code> can be applied.</p>
</td></tr>
<tr><td><code id="heatmap_+3A_reorderfun">reorderfun</code></td>
<td>
<p><code>function(d, w)</code> of dendrogram and weights for
reordering the row and column dendrograms.  The default uses
<code><a href="#topic+reorder.dendrogram">reorder.dendrogram</a></code>.</p>
</td></tr>
<tr><td><code id="heatmap_+3A_add.expr">add.expr</code></td>
<td>
<p>expression that will be evaluated after the call to
<code>image</code>.  Can be used to add components to the plot.</p>
</td></tr>
<tr><td><code id="heatmap_+3A_symm">symm</code></td>
<td>
<p>logical indicating if <code>x</code> should be treated
<b>symm</b>etrically; can only be true when <code>x</code> is a square matrix.</p>
</td></tr>
<tr><td><code id="heatmap_+3A_revc">revC</code></td>
<td>
<p>logical indicating if the column order should be
<code><a href="base.html#topic+rev">rev</a></code>ersed for plotting, such that e.g., for the
symmetric case, the symmetry axis is as usual.</p>
</td></tr>
<tr><td><code id="heatmap_+3A_scale">scale</code></td>
<td>
<p>character indicating if the values should be centered and
scaled in either the row direction or the column direction, or
none.  The default is <code>"row"</code> if <code>symm</code> false, and
<code>"none"</code> otherwise.</p>
</td></tr>
<tr><td><code id="heatmap_+3A_na.rm">na.rm</code></td>
<td>
<p>logical indicating whether <code>NA</code>'s should be removed.</p>
</td></tr>
<tr><td><code id="heatmap_+3A_margins">margins</code></td>
<td>
<p>numeric vector of length 2 containing the margins
(see <code><a href="graphics.html#topic+par">par</a>(mar = *)</code>) for column and row names, respectively.</p>
</td></tr>
<tr><td><code id="heatmap_+3A_colsidecolors">ColSideColors</code></td>
<td>
<p>(optional) character vector of length <code>ncol(x)</code>
containing the color names for a horizontal side bar that may be used to
annotate the columns of <code>x</code>.</p>
</td></tr>
<tr><td><code id="heatmap_+3A_rowsidecolors">RowSideColors</code></td>
<td>
<p>(optional) character vector of length <code>nrow(x)</code>
containing the color names for a vertical side bar that may be used to
annotate the rows of <code>x</code>.</p>
</td></tr>
<tr><td><code id="heatmap_+3A_cexrow">cexRow</code>, <code id="heatmap_+3A_cexcol">cexCol</code></td>
<td>
<p>positive numbers, used as <code>cex.axis</code> in
for the row or column axis labeling.  The defaults currently only
use number of rows or columns, respectively.</p>
</td></tr>
<tr><td><code id="heatmap_+3A_labrow">labRow</code>, <code id="heatmap_+3A_labcol">labCol</code></td>
<td>
<p>character vectors with row and column labels to
use; these default to <code>rownames(x)</code> or <code>colnames(x)</code>,
respectively.</p>
</td></tr>
<tr><td><code id="heatmap_+3A_main">main</code>, <code id="heatmap_+3A_xlab">xlab</code>, <code id="heatmap_+3A_ylab">ylab</code></td>
<td>
<p>main, x- and y-axis titles; defaults to none.</p>
</td></tr>
<tr><td><code id="heatmap_+3A_keep.dendro">keep.dendro</code></td>
<td>
<p>logical indicating if the dendrogram(s) should be
kept as part of the result (when <code>Rowv</code> and/or <code>Colv</code> are
not NA).</p>
</td></tr>
<tr><td><code id="heatmap_+3A_verbose">verbose</code></td>
<td>
<p>logical indicating if information should be printed.</p>
</td></tr>
<tr><td><code id="heatmap_+3A_...">...</code></td>
<td>
<p>additional arguments passed on to <code><a href="Matrix.html#topic+image">image</a></code>,
e.g., <code>col</code> specifying the colors.</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>If either <code>Rowv</code> or <code>Colv</code> are dendrograms they are honored
(and not reordered).  Otherwise, dendrograms are computed as
<code>dd &lt;- as.dendrogram(hclustfun(distfun(X)))</code> where <code>X</code> is
either <code>x</code> or <code>t(x)</code>.
</p>
<p>If either is a vector (of &lsquo;weights&rsquo;) then the appropriate
dendrogram is reordered according to the supplied values subject to
the constraints imposed by the dendrogram, by <code><a href="#topic+reorder">reorder</a>(dd,
    Rowv)</code>, in the row case.
If either is missing, as by default, then the ordering of the
corresponding dendrogram is by the mean value of the rows/columns,
i.e., in the case of rows, <code>Rowv &lt;- rowMeans(x, na.rm = na.rm)</code>.
If either is <code><a href="base.html#topic+NA">NA</a></code>, <em>no reordering</em> will be done for
the corresponding side.
</p>
<p>By default (<code>scale = "row"</code>) the rows are scaled to have mean
zero and standard deviation one.  There is some empirical evidence
from genomic plotting that this is useful.
</p>


<h3>Value</h3>

<p>Invisibly, a list with components
</p>
<table>
<tr><td><code>rowInd</code></td>
<td>
<p><b>r</b>ow index permutation vector as returned by
<code><a href="#topic+order.dendrogram">order.dendrogram</a></code>.</p>
</td></tr>
<tr><td><code>colInd</code></td>
<td>
<p><b>c</b>olumn index permutation vector.</p>
</td></tr>
<tr><td><code>Rowv</code></td>
<td>
<p>the row dendrogram; only if input <code>Rowv</code> was not NA
and <code>keep.dendro</code> is true.</p>
</td></tr>
<tr><td><code>Colv</code></td>
<td>
<p>the column dendrogram; only if input <code>Colv</code> was not NA
and <code>keep.dendro</code> is true.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Unless <code>Rowv = NA</code> (or <code>Colw = NA</code>), the original rows and
columns are reordered <em>in any case</em> to match the dendrogram,
e.g., the rows by <code><a href="#topic+order.dendrogram">order.dendrogram</a>(Rowv)</code> where
<code>Rowv</code> is the (possibly <code><a href="#topic+reorder">reorder</a>()</code>ed) row
dendrogram.
</p>
<p><code>heatmap()</code> uses <code><a href="graphics.html#topic+layout">layout</a></code> and draws the
<code><a href="Matrix.html#topic+image">image</a></code> in the lower right corner of a 2x2 layout.
Consequentially, it can <b>not</b> be used in a multi column/row
layout, i.e., when <code><a href="graphics.html#topic+par">par</a>(mfrow = *)</code> or <code>(mfcol = *)</code>
has been called.
</p>


<h3>Author(s)</h3>

<p>Andy Liaw, original; R. Gentleman, M. Maechler, W. Huber, revisions.</p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+image">image</a></code>, <code><a href="#topic+hclust">hclust</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics); require(grDevices)
x  &lt;- as.matrix(mtcars)
rc &lt;- rainbow(nrow(x), start = 0, end = .3)
cc &lt;- rainbow(ncol(x), start = 0, end = .3)
hv &lt;- heatmap(x, col = cm.colors(256), scale = "column",
              RowSideColors = rc, ColSideColors = cc, margins = c(5,10),
              xlab = "specification variables", ylab =  "Car Models",
              main = "heatmap(&lt;Mtcars data&gt;, ..., scale = \"column\")")
utils::str(hv) # the two re-ordering index vectors

## no column dendrogram (nor reordering) at all:
heatmap(x, Colv = NA, col = cm.colors(256), scale = "column",
        RowSideColors = rc, margins = c(5,10),
        xlab = "specification variables", ylab =  "Car Models",
        main = "heatmap(&lt;Mtcars data&gt;, ..., scale = \"column\")")

## "no nothing"
heatmap(x, Rowv = NA, Colv = NA, scale = "column",
        main = "heatmap(*, NA, NA) ~= image(t(x))")


round(Ca &lt;- cor(attitude), 2)
symnum(Ca) # simple graphic
heatmap(Ca,               symm = TRUE, margins = c(6,6)) # with reorder()
heatmap(Ca, Rowv = FALSE, symm = TRUE, margins = c(6,6)) # _NO_ reorder()
## slightly artificial with color bar, without and with ordering:
cc &lt;- rainbow(nrow(Ca))
heatmap(Ca, Rowv = FALSE, symm = TRUE, RowSideColors = cc, ColSideColors = cc,
	margins = c(6,6))
heatmap(Ca,		symm = TRUE, RowSideColors = cc, ColSideColors = cc,
	margins = c(6,6))

## For variable clustering, rather use distance based on cor():
symnum( cU &lt;- cor(USJudgeRatings) )

hU &lt;- heatmap(cU, Rowv = FALSE, symm = TRUE, col = topo.colors(16),
             distfun = function(c) as.dist(1 - c), keep.dendro = TRUE)
## The Correlation matrix with same reordering:
round(100 * cU[hU[[1]], hU[[2]]])
## The column dendrogram:
utils::str(hU$Colv)
</code></pre>

<hr>
<h2 id='HoltWinters'>Holt-Winters Filtering</h2><span id='topic+HoltWinters'></span><span id='topic+print.HoltWinters'></span><span id='topic+residuals.HoltWinters'></span>

<h3>Description</h3>

<p>Computes Holt-Winters Filtering of a given time series.
Unknown parameters are determined by minimizing the squared
prediction error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HoltWinters(x, alpha = NULL, beta = NULL, gamma = NULL,
            seasonal = c("additive", "multiplicative"),
            start.periods = 2, l.start = NULL, b.start = NULL,
            s.start = NULL,
            optim.start = c(alpha = 0.3, beta = 0.1, gamma = 0.1),
            optim.control = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HoltWinters_+3A_x">x</code></td>
<td>
<p>An object of class <code>ts</code></p>
</td></tr>
<tr><td><code id="HoltWinters_+3A_alpha">alpha</code></td>
<td>
<p><code class="reqn">alpha</code> parameter of Holt-Winters Filter.</p>
</td></tr>
<tr><td><code id="HoltWinters_+3A_beta">beta</code></td>
<td>
<p><code class="reqn">beta</code> parameter of Holt-Winters Filter. If set to
<code>FALSE</code>, the function will do exponential smoothing.</p>
</td></tr>
<tr><td><code id="HoltWinters_+3A_gamma">gamma</code></td>
<td>
<p><code class="reqn">gamma</code> parameter used for the seasonal component.
If set to <code>FALSE</code>, an non-seasonal model is fitted.</p>
</td></tr>
<tr><td><code id="HoltWinters_+3A_seasonal">seasonal</code></td>
<td>
<p>Character string to select an <code>"additive"</code>
(the default) or <code>"multiplicative"</code> seasonal model. The first
few characters are sufficient. (Only takes effect if
<code>gamma</code> is non-zero).</p>
</td></tr>
<tr><td><code id="HoltWinters_+3A_start.periods">start.periods</code></td>
<td>
<p>Start periods used in the autodetection of start
values. Must be at least 2.</p>
</td></tr>
<tr><td><code id="HoltWinters_+3A_l.start">l.start</code></td>
<td>
<p>Start value for level (a[0]).</p>
</td></tr>
<tr><td><code id="HoltWinters_+3A_b.start">b.start</code></td>
<td>
<p>Start value for trend (b[0]).</p>
</td></tr>
<tr><td><code id="HoltWinters_+3A_s.start">s.start</code></td>
<td>
<p>Vector of start values for the seasonal component
(<code class="reqn">s_1[0] \ldots s_p[0]</code>)</p>
</td></tr>
<tr><td><code id="HoltWinters_+3A_optim.start">optim.start</code></td>
<td>
<p>Vector with named components <code>alpha</code>,
<code>beta</code>, and <code>gamma</code> containing the starting values for the
optimizer. Only the values needed must be specified.  Ignored in the
one-parameter case.</p>
</td></tr>
<tr><td><code id="HoltWinters_+3A_optim.control">optim.control</code></td>
<td>
<p>Optional list with additional control parameters
passed to <code>optim</code> if this is used.  Ignored in the
one-parameter case.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The additive Holt-Winters prediction function (for time series with
period length p) is
</p>
<p style="text-align: center;"><code class="reqn">\hat Y[t+h] = a[t] + h b[t] + s[t - p + 1 + (h - 1) \bmod p],</code>
</p>

<p>where <code class="reqn">a[t]</code>, <code class="reqn">b[t]</code> and <code class="reqn">s[t]</code> are given by
</p>
<p style="text-align: center;"><code class="reqn">a[t] = \alpha (Y[t] - s[t-p])  + (1-\alpha) (a[t-1] + b[t-1])</code>
</p>

<p style="text-align: center;"><code class="reqn">b[t] = \beta (a[t] -a[t-1]) + (1-\beta)  b[t-1]</code>
</p>

<p style="text-align: center;"><code class="reqn">s[t] = \gamma (Y[t] - a[t])   + (1-\gamma) s[t-p]</code>
</p>

<p>The multiplicative Holt-Winters prediction function (for time series
with period length p) is
</p>
<p style="text-align: center;"><code class="reqn">\hat Y[t+h] = (a[t] + h b[t]) \times s[t - p + 1 + (h - 1) \bmod p].</code>
</p>

<p>where <code class="reqn">a[t]</code>, <code class="reqn">b[t]</code> and <code class="reqn">s[t]</code> are given by
</p>
<p style="text-align: center;"><code class="reqn">a[t] = \alpha (Y[t] / s[t-p])  + (1-\alpha) (a[t-1] + b[t-1])</code>
</p>

<p style="text-align: center;"><code class="reqn">b[t] = \beta (a[t] - a[t-1]) + (1-\beta) b[t-1]</code>
</p>

<p style="text-align: center;"><code class="reqn">s[t] = \gamma (Y[t] / a[t])   + (1-\gamma) s[t-p]</code>
</p>

<p>The data in <code>x</code> are required to be non-zero for a multiplicative
model, but it makes most sense if they are all positive.
</p>
<p>The function tries to find the optimal values of <code class="reqn">\alpha</code> and/or
<code class="reqn">\beta</code> and/or <code class="reqn">\gamma</code> by minimizing the squared one-step
prediction error if they are <code>NULL</code> (the default). <code>optimize</code>
will be used for the single-parameter case, and <code>optim</code> otherwise.
</p>
<p>For seasonal models, start values for <code>a</code>, <code>b</code> and <code>s</code>
are inferred by performing a simple decomposition in trend and
seasonal component using moving averages (see function
<code><a href="#topic+decompose">decompose</a></code>) on the <code>start.periods</code> first periods (a simple
linear regression on the trend component is used for starting level
and trend). For level/trend-models (no seasonal component), start
values for <code>a</code> and <code>b</code> are <code>x[2]</code> and <code>x[2] -
  x[1]</code>, respectively. For level-only models (ordinary exponential
smoothing), the start value for <code>a</code> is <code>x[1]</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>"HoltWinters"</code>, a list with components:
</p>
<table>
<tr><td><code>fitted</code></td>
<td>
<p>A multiple time series with one column for the
filtered series as well as for the level, trend and seasonal
components, estimated contemporaneously (that is at time t and not
at the end of the series).</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>The original series</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>alpha used for filtering</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>beta used for filtering</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p>gamma used for filtering</p>
</td></tr>
<tr><td><code>coefficients</code></td>
<td>
<p>A vector with named components <code>a, b, s1, ..., sp</code>
containing the estimated values for the level, trend and seasonal
components</p>
</td></tr>
<tr><td><code>seasonal</code></td>
<td>
<p>The specified <code>seasonal</code> parameter</p>
</td></tr>
<tr><td><code>SSE</code></td>
<td>
<p>The final sum of squared errors achieved in optimizing</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The call used</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>David Meyer <a href="mailto:David.Meyer@wu.ac.at">David.Meyer@wu.ac.at</a>
</p>


<h3>References</h3>

<p>C. C. Holt (1957)
Forecasting seasonals and trends by exponentially weighted moving averages,
<em>ONR Research Memorandum, Carnegie Institute of Technology</em> <b>52</b>.
(reprint at <a href="https://doi.org/10.1016/j.ijforecast.2003.09.015">doi:10.1016/j.ijforecast.2003.09.015</a>).
</p>
<p>P. R. Winters (1960).
Forecasting sales by exponentially weighted moving averages.
<em>Management Science</em>, <b>6</b>, 324&ndash;342.
<a href="https://doi.org/10.1287/mnsc.6.3.324">doi:10.1287/mnsc.6.3.324</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.HoltWinters">predict.HoltWinters</a></code>, <code><a href="#topic+optim">optim</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require(graphics)

## Seasonal Holt-Winters
(m &lt;- HoltWinters(co2))
plot(m)
plot(fitted(m))

(m &lt;- HoltWinters(AirPassengers, seasonal = "mult"))
plot(m)

## Non-Seasonal Holt-Winters
x &lt;- uspop + rnorm(uspop, sd = 5)
m &lt;- HoltWinters(x, gamma = FALSE)
plot(m)

## Exponential Smoothing
m2 &lt;- HoltWinters(x, gamma = FALSE, beta = FALSE)
lines(fitted(m2)[,1], col = 3)

</code></pre>

<hr>
<h2 id='Hypergeometric'>The Hypergeometric Distribution</h2><span id='topic+Hypergeometric'></span><span id='topic+dhyper'></span><span id='topic+phyper'></span><span id='topic+qhyper'></span><span id='topic+rhyper'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the hypergeometric distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dhyper(x, m, n, k, log = FALSE)
phyper(q, m, n, k, lower.tail = TRUE, log.p = FALSE)
qhyper(p, m, n, k, lower.tail = TRUE, log.p = FALSE)
rhyper(nn, m, n, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Hypergeometric_+3A_x">x</code>, <code id="Hypergeometric_+3A_q">q</code></td>
<td>
<p>vector of quantiles representing the number of white balls
drawn without replacement from an urn which contains both black and
white balls.</p>
</td></tr>
<tr><td><code id="Hypergeometric_+3A_m">m</code></td>
<td>
<p>the number of white balls in the urn.</p>
</td></tr>
<tr><td><code id="Hypergeometric_+3A_n">n</code></td>
<td>
<p>the number of black balls in the urn.</p>
</td></tr>
<tr><td><code id="Hypergeometric_+3A_k">k</code></td>
<td>
<p>the number of balls drawn from the urn, hence must be in
<code class="reqn">0,1,\dots, m+n</code>.</p>
</td></tr>
<tr><td><code id="Hypergeometric_+3A_p">p</code></td>
<td>
<p>probability, it must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="Hypergeometric_+3A_nn">nn</code></td>
<td>
<p>number of observations.  If <code>length(nn) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="Hypergeometric_+3A_log">log</code>, <code id="Hypergeometric_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="Hypergeometric_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The hypergeometric distribution is used for sampling <em>without</em>
replacement.  The density of this distribution with parameters
<code>m</code>, <code>n</code> and <code>k</code> (named <code class="reqn">Np</code>, <code class="reqn">N-Np</code>, and
<code class="reqn">n</code>, respectively in the reference below, where <code class="reqn">N := m+n</code> is also used
in other references) is given by
</p>
<p style="text-align: center;"><code class="reqn">
    p(x) = \left. {m \choose x}{n \choose k-x} \right/ {m+n \choose k}%
  </code>
</p>

<p>for <code class="reqn">x = 0, \ldots, k</code>.
</p>
<p>Note that <code class="reqn">p(x)</code> is non-zero only for
<code class="reqn">\max(0, k-n) \le x \le \min(k, m)</code>.
</p>
<p>With <code class="reqn">p := m/(m+n)</code> (hence <code class="reqn">Np = N \times p</code> in the
reference's notation), the first two moments are mean
</p>
<p style="text-align: center;"><code class="reqn">E[X] = \mu = k p</code>
</p>
<p> and variance
</p>
<p style="text-align: center;"><code class="reqn">\mbox{Var}(X) = k p (1 - p) \frac{m+n-k}{m+n-1},</code>
</p>

<p>which shows the closeness to the Binomial<code class="reqn">(k,p)</code> (where the
hypergeometric has smaller variance unless <code class="reqn">k = 1</code>).
</p>
<p>The quantile is defined as the smallest value <code class="reqn">x</code> such that
<code class="reqn">F(x) \ge p</code>, where <code class="reqn">F</code> is the distribution function.
</p>
<p>In <code>rhyper()</code>, if one of <code class="reqn">m, n, k</code> exceeds <code><a href="base.html#topic+.Machine">.Machine</a>$integer.max</code>,
currently the equivalent of <code>qhyper(runif(nn), m,n,k)</code> is used
which is comparably slow while instead a binomial approximation may be
considerably more efficient.
</p>


<h3>Value</h3>

<p><code>dhyper</code> gives the density,
<code>phyper</code> gives the distribution function,
<code>qhyper</code> gives the quantile function, and
<code>rhyper</code> generates random deviates.
</p>
<p>Invalid arguments will result in return value <code>NaN</code>, with a warning.
</p>
<p>The length of the result is determined by <code>n</code> for
<code>rhyper</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.
</p>
<p>The numerical arguments other than <code>n</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.
</p>


<h3>Source</h3>

<p><code>dhyper</code> computes via binomial probabilities, using code
contributed by Catherine Loader (see <code><a href="#topic+dbinom">dbinom</a></code>).
</p>
<p><code>phyper</code> is based on calculating <code>dhyper</code> and
<code>phyper(...)/dhyper(...)</code> (as a summation), based on ideas of Ian
Smith and Morten Welinder.
</p>
<p><code>qhyper</code> is based on inversion (of an earlier <code>phyper()</code> algorithm).
</p>
<p><code>rhyper</code> is based on a corrected version of
</p>
<p>Kachitvichyanukul, V. and Schmeiser, B. (1985).
Computer generation of hypergeometric random variates.
<em>Journal of Statistical Computation and Simulation</em>,
<b>22</b>, 127&ndash;145.
</p>


<h3>References</h3>

<p>Johnson, N. L., Kotz, S., and Kemp, A. W. (1992)
<em>Univariate Discrete Distributions</em>,
Second Edition. New York: Wiley.
</p>


<h3>See Also</h3>

<p><a href="#topic+Distributions">Distributions</a> for other standard distributions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>m &lt;- 10; n &lt;- 7; k &lt;- 8
x &lt;- 0:(k+1)
rbind(phyper(x, m, n, k), dhyper(x, m, n, k))
all(phyper(x, m, n, k) == cumsum(dhyper(x, m, n, k)))  # FALSE
## but errors are very small:
signif(phyper(x, m, n, k) - cumsum(dhyper(x, m, n, k)), digits = 3)

stopifnot(abs(phyper(x, m, n, k) - cumsum(dhyper(x, m, n, k))) &lt; 5e-16)
</code></pre>

<hr>
<h2 id='identify.hclust'>Identify Clusters in a Dendrogram</h2><span id='topic+identify.hclust'></span>

<h3>Description</h3>

<p><code>identify.hclust</code> reads the position of the graphics pointer when the
(first) mouse button is pressed.  It then cuts the tree at the
vertical position of the pointer and highlights the cluster containing
the horizontal position of the pointer.  Optionally a function is
applied to the index of data points contained in the cluster.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hclust'
identify(x, FUN = NULL, N = 20, MAXCLUSTER = 20, DEV.FUN = NULL,
          ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="identify.hclust_+3A_x">x</code></td>
<td>
<p>an object of the type produced by <code>hclust</code>.</p>
</td></tr>
<tr><td><code id="identify.hclust_+3A_fun">FUN</code></td>
<td>
<p>(optional) function to be applied to the index numbers of
the data points in a cluster (see &lsquo;Details&rsquo; below).</p>
</td></tr>
<tr><td><code id="identify.hclust_+3A_n">N</code></td>
<td>
<p>the maximum number of clusters to be identified.</p>
</td></tr>
<tr><td><code id="identify.hclust_+3A_maxcluster">MAXCLUSTER</code></td>
<td>
<p>the maximum number of clusters that can be produced
by a cut (limits the effective vertical range of the pointer). </p>
</td></tr>
<tr><td><code id="identify.hclust_+3A_dev.fun">DEV.FUN</code></td>
<td>
<p>(optional) integer scalar. If specified, the
corresponding graphics device is made active before <code>FUN</code> is
applied.</p>
</td></tr>
<tr><td><code id="identify.hclust_+3A_...">...</code></td>
<td>
<p>further arguments to <code>FUN</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default clusters can be identified using the mouse and an
<code><a href="base.html#topic+invisible">invisible</a></code> list of indices of the respective data points
is returned.
</p>
<p>If <code>FUN</code> is not <code>NULL</code>, then the index vector of data points
is passed to this function as first argument, see the examples
below.  The active graphics device for <code>FUN</code> can be specified using
<code>DEV.FUN</code>.
</p>
<p>The identification process is terminated by pressing any mouse
button other than the first, see also <code><a href="graphics.html#topic+identify">identify</a></code>.
</p>


<h3>Value</h3>

<p>Either a list of data point index vectors or a list of return values
of <code>FUN</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hclust">hclust</a></code>,
<code><a href="#topic+rect.hclust">rect.hclust</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
require(graphics)

hca &lt;- hclust(dist(USArrests))
plot(hca)
(x &lt;- identify(hca)) ##  Terminate with 2nd mouse button !!

hci &lt;- hclust(dist(iris[,1:4]))
plot(hci)
identify(hci, function(k) print(table(iris[k,5])))

# open a new device (one for dendrogram, one for bars):
dev.new() # &lt;&lt; make that narrow (&amp; small)
          # and *beside* 1st one
nD &lt;- dev.cur()            # to be for the barplot
dev.set(dev.prev())  # old one for dendrogram
plot(hci)
## select subtrees in dendrogram and "see" the species distribution:
identify(hci, function(k) barplot(table(iris[k,5]), col = 2:4), DEV.FUN = nD)

## End(Not run)</code></pre>

<hr>
<h2 id='influence.measures'>Regression Deletion Diagnostics</h2><span id='topic+influence.measures'></span><span id='topic+hat'></span><span id='topic+hatvalues'></span><span id='topic+hatvalues.lm'></span><span id='topic+rstandard'></span><span id='topic+rstandard.lm'></span><span id='topic+rstandard.glm'></span><span id='topic+rstudent'></span><span id='topic+rstudent.lm'></span><span id='topic+rstudent.glm'></span><span id='topic+dfbeta'></span><span id='topic+dfbeta.lm'></span><span id='topic+dfbetas'></span><span id='topic+dfbetas.lm'></span><span id='topic+dffits'></span><span id='topic+covratio'></span><span id='topic+cooks.distance'></span><span id='topic+cooks.distance.lm'></span><span id='topic+cooks.distance.glm'></span>

<h3>Description</h3>

<p>This suite of functions can be used to compute some of the regression
(leave-one-out deletion) diagnostics for linear and generalized linear
models discussed in Belsley, Kuh and Welsch (1980),
Cook and Weisberg (1982), etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>influence.measures(model, infl = influence(model))

rstandard(model, ...)
## S3 method for class 'lm'
rstandard(model, infl = lm.influence(model, do.coef = FALSE),
          sd = sqrt(deviance(model)/df.residual(model)),
          type = c("sd.1", "predictive"), ...)
## S3 method for class 'glm'
rstandard(model, infl = influence(model, do.coef = FALSE),
          type = c("deviance", "pearson"), ...)

rstudent(model, ...)
## S3 method for class 'lm'
rstudent(model, infl = lm.influence(model, do.coef = FALSE),
         res = infl$wt.res, ...)
## S3 method for class 'glm'
rstudent(model, infl = influence(model, do.coef = FALSE), ...)

dffits(model, infl = , res = )

dfbeta(model, ...)
## S3 method for class 'lm'
dfbeta(model, infl = lm.influence(model, do.coef = TRUE), ...)

dfbetas(model, ...)
## S3 method for class 'lm'
dfbetas(model, infl = lm.influence(model, do.coef = TRUE), ...)

covratio(model, infl = lm.influence(model, do.coef = FALSE),
         res = weighted.residuals(model))

cooks.distance(model, ...)
## S3 method for class 'lm'
cooks.distance(model, infl = lm.influence(model, do.coef = FALSE),
               res = weighted.residuals(model),
               sd = sqrt(deviance(model)/df.residual(model)),
               hat = infl$hat, ...)
## S3 method for class 'glm'
cooks.distance(model, infl = influence(model, do.coef = FALSE),
               res = infl$pear.res,
               dispersion = summary(model)$dispersion,
               hat = infl$hat, ...)

hatvalues(model, ...)
## S3 method for class 'lm'
hatvalues(model, infl = lm.influence(model, do.coef = FALSE), ...)

hat(x, intercept = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="influence.measures_+3A_model">model</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object, typically returned by <code><a href="#topic+lm">lm</a></code> or
<code><a href="#topic+glm">glm</a></code>.</p>
</td></tr>
<tr><td><code id="influence.measures_+3A_infl">infl</code></td>
<td>
<p>influence structure as returned by
<code><a href="#topic+lm.influence">lm.influence</a></code> or <code><a href="#topic+influence">influence</a></code> (the latter
only for the <code>glm</code> method of <code>rstudent</code> and
<code>cooks.distance</code>).</p>
</td></tr>
<tr><td><code id="influence.measures_+3A_res">res</code></td>
<td>
<p>(possibly weighted) residuals, with proper default.</p>
</td></tr>
<tr><td><code id="influence.measures_+3A_sd">sd</code></td>
<td>
<p>standard deviation to use, see default.</p>
</td></tr>
<tr><td><code id="influence.measures_+3A_dispersion">dispersion</code></td>
<td>
<p>dispersion (for <code><a href="#topic+glm">glm</a></code> objects) to use,
see default.</p>
</td></tr>
<tr><td><code id="influence.measures_+3A_hat">hat</code></td>
<td>
<p>hat values <code class="reqn">H_{ii}</code>, see default.</p>
</td></tr>
<tr><td><code id="influence.measures_+3A_type">type</code></td>
<td>
<p>type of residuals for <code>rstandard</code>, with different
options and meanings for <code>lm</code> and <code>glm</code>.  Can be
abbreviated.</p>
</td></tr>
<tr><td><code id="influence.measures_+3A_x">x</code></td>
<td>
<p>the <code class="reqn">X</code> or design matrix.</p>
</td></tr>
<tr><td><code id="influence.measures_+3A_intercept">intercept</code></td>
<td>
<p>should an intercept column be prepended to <code>x</code>?</p>
</td></tr>
<tr><td><code id="influence.measures_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The primary high-level function is <code>influence.measures</code> which produces a
class <code>"infl"</code> object tabular display showing the DFBETAs for
each model variable, DFFITs, covariance ratios, Cook's distances and
the diagonal elements of the hat matrix.  Cases which are influential
with respect to any of these measures are marked with an asterisk.
</p>
<p>The functions <code>dfbetas</code>, <code>dffits</code>,
<code>covratio</code> and <code>cooks.distance</code> provide direct access to the
corresponding diagnostic quantities.  Functions <code>rstandard</code> and
<code>rstudent</code> give the standardized and Studentized residuals
respectively. (These re-normalize the residuals to have unit variance,
using an overall and leave-one-out measure of the error variance
respectively.)
</p>
<p>Note that for <em>multivariate</em> <code>lm()</code> models (of class
<code>"mlm"</code>), these functions return 3d arrays instead of matrices,
or matrices instead of vectors.
</p>
<p>Values for generalized linear models are approximations, as described
in Williams (1987) (except that Cook's distances are scaled as
<code class="reqn">F</code> rather than as chi-square values).  The approximations can be
poor when some cases have large influence.
</p>
<p>The optional <code>infl</code>, <code>res</code> and <code>sd</code> arguments are there
to encourage the use of these direct access functions, in situations
where, e.g., the underlying basic influence measures (from
<code><a href="#topic+lm.influence">lm.influence</a></code> or the generic <code><a href="#topic+influence">influence</a></code>) are
already available.
</p>
<p>Note that cases with <code>weights == 0</code> are <em>dropped</em> from all
these functions, but that if a linear model has been fitted with
<code>na.action = na.exclude</code>, suitable values are filled in for the
cases excluded during fitting.
</p>
<p>For linear models, <code>rstandard(*, type = "predictive")</code> provides
leave-one-out cross validation residuals, and the &ldquo;PRESS&rdquo;
statistic (<b>PRE</b>dictive <b>S</b>um of <b>S</b>quares, the same as
the CV score) of model <code>model</code> is </p>
<pre>   PRESS &lt;- sum(rstandard(model, type="pred")^2)</pre>
<p>The function <code>hat()</code> exists mainly for S (version 2)
compatibility; we recommend using <code>hatvalues()</code> instead.
</p>


<h3>Note</h3>

<p>For <code>hatvalues</code>, <code>dfbeta</code>, and <code>dfbetas</code>, the method
for linear models also works for generalized linear models.
</p>


<h3>Author(s)</h3>

<p>Several R core team members and John Fox, originally in his &lsquo;<span class="file">car</span>&rsquo;
package.
</p>


<h3>References</h3>

<p>Belsley, D. A., Kuh, E. and Welsch, R. E. (1980).
<em>Regression Diagnostics</em>.
New York: Wiley.
</p>
<p>Cook, R. D. and Weisberg, S. (1982).
<em>Residuals and Influence in Regression</em>.
London: Chapman and Hall.
</p>
<p>Williams, D. A. (1987).
Generalized linear model diagnostics using the deviance and single
case deletions.
<em>Applied Statistics</em>, <b>36</b>, 181&ndash;191.
<a href="https://doi.org/10.2307/2347550">doi:10.2307/2347550</a>.
</p>
<p>Fox, J. (1997).
<em>Applied Regression, Linear Models, and Related Methods</em>.
Sage.
</p>
<p>Fox, J. (2002)
<em>An R and S-Plus Companion to Applied Regression</em>.
Sage Publ.
</p>
<p>Fox, J. and Weisberg, S. (2011).
<em>An R Companion to Applied Regression</em>, second edition.
Sage Publ;
<a href="https://socialsciences.mcmaster.ca/jfox/Books/Companion/">https://socialsciences.mcmaster.ca/jfox/Books/Companion/</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+influence">influence</a></code> (containing <code><a href="#topic+lm.influence">lm.influence</a></code>).
</p>
<p>&lsquo;<a href="grDevices.html#topic+plotmath">plotmath</a>&rsquo; for the use of <code>hat</code> in plot annotation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

## Analysis of the life-cycle savings data
## given in Belsley, Kuh and Welsch.
lm.SR &lt;- lm(sr ~ pop15 + pop75 + dpi + ddpi, data = LifeCycleSavings)

inflm.SR &lt;- influence.measures(lm.SR)
which(apply(inflm.SR$is.inf, 1, any))
# which observations 'are' influential
summary(inflm.SR) # only these
inflm.SR          # all
plot(rstudent(lm.SR) ~ hatvalues(lm.SR)) # recommended by some
plot(lm.SR, which = 5) # an enhanced version of that via plot(&lt;lm&gt;)

## The 'infl' argument is not needed, but avoids recomputation:
rs &lt;- rstandard(lm.SR)
iflSR &lt;- influence(lm.SR)
all.equal(rs, rstandard(lm.SR, infl = iflSR), tolerance = 1e-10)
## to "see" the larger values:
1000 * round(dfbetas(lm.SR, infl = iflSR), 3)
cat("PRESS :"); (PRESS &lt;- sum( rstandard(lm.SR, type = "predictive")^2 ))
stopifnot(all.equal(PRESS, sum( (residuals(lm.SR) / (1 - iflSR$hat))^2)))

## Show that "PRE-residuals"  ==  L.O.O. Crossvalidation (CV) errors:
X &lt;- model.matrix(lm.SR)
y &lt;- model.response(model.frame(lm.SR))
## Leave-one-out CV least-squares prediction errors (relatively fast)
rCV &lt;- vapply(seq_len(nrow(X)), function(i)
              y[i] - X[i,] %*% .lm.fit(X[-i,], y[-i])$coefficients,
              numeric(1))
## are the same as the *faster* rstandard(*, "pred") :
stopifnot(all.equal(rCV, unname(rstandard(lm.SR, type = "predictive"))))


## Huber's data [Atkinson 1985]
xh &lt;- c(-4:0, 10)
yh &lt;- c(2.48, .73, -.04, -1.44, -1.32, 0)
lmH &lt;- lm(yh ~ xh)
summary(lmH)
im &lt;- influence.measures(lmH)
 im 
is.inf &lt;- apply(im$is.inf, 1, any)
plot(xh,yh, main = "Huber's data: L.S. line and influential obs.")
abline(lmH); points(xh[is.inf], yh[is.inf], pch = 20, col = 2)

## Irwin's data [Williams 1987]
xi &lt;- 1:5
yi &lt;- c(0,2,14,19,30)    # number of mice responding to dose xi
mi &lt;- rep(40, 5)         # number of mice exposed
glmI &lt;- glm(cbind(yi, mi -yi) ~ xi, family = binomial)
summary(glmI)
signif(cooks.distance(glmI), 3)   # ~= Ci in Table 3, p.184
imI &lt;- influence.measures(glmI)
 imI 
stopifnot(all.equal(imI$infmat[,"cook.d"],
          cooks.distance(glmI)))
</code></pre>

<hr>
<h2 id='integrate'>Integration of One-Dimensional Functions</h2><span id='topic+integrate'></span><span id='topic+print.integrate'></span>

<h3>Description</h3>

<p>Adaptive quadrature of functions of one variable over a finite or
infinite interval.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>integrate(f, lower, upper, ..., subdivisions = 100L,
          rel.tol = .Machine$double.eps^0.25, abs.tol = rel.tol,
          stop.on.error = TRUE, keep.xy = FALSE, aux = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="integrate_+3A_f">f</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> function taking a numeric first argument and returning
a numeric vector of the same length.  Returning a non-finite element will
generate an error.</p>
</td></tr>
<tr><td><code id="integrate_+3A_lower">lower</code>, <code id="integrate_+3A_upper">upper</code></td>
<td>
<p>the limits of integration.  Can be infinite.</p>
</td></tr>
<tr><td><code id="integrate_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed to <code>f</code>.</p>
</td></tr>
<tr><td><code id="integrate_+3A_subdivisions">subdivisions</code></td>
<td>
<p>the maximum number of subintervals.</p>
</td></tr>
<tr><td><code id="integrate_+3A_rel.tol">rel.tol</code></td>
<td>
<p>relative accuracy requested.</p>
</td></tr>
<tr><td><code id="integrate_+3A_abs.tol">abs.tol</code></td>
<td>
<p>absolute accuracy requested.</p>
</td></tr>
<tr><td><code id="integrate_+3A_stop.on.error">stop.on.error</code></td>
<td>
<p>logical. If true (the default) an error stops the
function.  If false some errors will give a result with a warning in
the <code>message</code> component.</p>
</td></tr>
<tr><td><code id="integrate_+3A_keep.xy">keep.xy</code></td>
<td>
<p>unused.  For compatibility with S.</p>
</td></tr>
<tr><td><code id="integrate_+3A_aux">aux</code></td>
<td>
<p>unused.  For compatibility with S.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that arguments after <code>...</code> must be matched exactly.
</p>
<p>If one or both limits are infinite, the infinite range is mapped onto
a finite interval.
</p>
<p>For a finite interval, globally adaptive interval subdivision is used
in connection with extrapolation by Wynn's Epsilon algorithm, with the
basic step being Gauss&ndash;Kronrod quadrature.
</p>
<p><code>rel.tol</code> cannot be less than <code>max(50*.Machine$double.eps,
    0.5e-28)</code> if <code>abs.tol &lt;= 0</code>.
</p>
<p>Note that the comments in the C source code in
&lsquo;<span class="file"><var>R</var>/src/appl/integrate.c</span>&rsquo; give more details, particularly about
reasons for failure (internal error code <code>ier &gt;= 1</code>).
</p>
<p>In <span class="rlang"><b>R</b></span> versions <code class="reqn">\le</code> 3.2.x, the first entries of
<code>lower</code> and <code>upper</code> were used whereas an error is signalled
now if they are not of length one.
</p>


<h3>Value</h3>

<p>A list of class <code>"integrate"</code> with components
</p>
<table>
<tr><td><code>value</code></td>
<td>
<p>the final estimate of the integral.</p>
</td></tr>
<tr><td><code>abs.error</code></td>
<td>
<p>estimate of the modulus of the absolute error.</p>
</td></tr>
<tr><td><code>subdivisions</code></td>
<td>
<p>the number of subintervals produced in the
subdivision process.</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p><code>"OK"</code> or a character string giving the error message.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Like all numerical integration routines, these evaluate the function
on a finite set of points.  If the function is approximately constant
(in particular, zero) over nearly all its range it is possible that
the result and error estimate may be seriously wrong.
</p>
<p>When integrating over infinite intervals do so explicitly, rather than
just using a large number as the endpoint.  This increases the chance
of a correct answer &ndash; any function whose integral over an infinite
interval is finite must be near zero for most of that interval.
</p>
<p>For values at a finite set of points to be a fair reflection of the
behaviour of the function elsewhere, the function needs to be
well-behaved, for example differentiable except perhaps for a small
number of jumps or integrable singularities.
</p>
<p><code>f</code> must accept a vector of inputs and produce a vector of function
evaluations at those points.  The <code><a href="base.html#topic+Vectorize">Vectorize</a></code> function
may be helpful to convert <code>f</code> to this form.
</p>


<h3>Source</h3>

<p>Based on QUADPACK routines <code>dqags</code> and <code>dqagi</code> by
R. Piessens and E. deDoncker&ndash;Kapenga, available from Netlib.
</p>


<h3>References</h3>

<p>R. Piessens, E. deDoncker&ndash;Kapenga, C. Uberhuber, D. Kahaner (1983)
<em>Quadpack: a Subroutine Package for Automatic Integration</em>;
Springer Verlag.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>integrate(dnorm, -1.96, 1.96)
integrate(dnorm, -Inf, Inf)

## a slowly-convergent integral
integrand &lt;- function(x) {1/((x+1)*sqrt(x))}
integrate(integrand, lower = 0, upper = Inf)

## don't do this if you really want the integral from 0 to Inf
integrate(integrand, lower = 0, upper = 10)
integrate(integrand, lower = 0, upper = 100000)
integrate(integrand, lower = 0, upper = 1000000, stop.on.error = FALSE)

## some functions do not handle vector input properly
f &lt;- function(x) 2.0
try(integrate(f, 0, 1))
integrate(Vectorize(f), 0, 1)  ## correct
integrate(function(x) rep(2.0, length(x)), 0, 1)  ## correct

## integrate can fail if misused
integrate(dnorm, 0, 2)
integrate(dnorm, 0, 20)
integrate(dnorm, 0, 200)
integrate(dnorm, 0, 2000)
integrate(dnorm, 0, 20000) ## fails on many systems
integrate(dnorm, 0, Inf)   ## works

integrate(dnorm, 0:1, 20) #-&gt; error!
## "silently" gave  integrate(dnorm, 0, 20)  in earlier versions of R

</code></pre>

<hr>
<h2 id='interaction.plot'>Two-way Interaction Plot</h2><span id='topic+interaction.plot'></span>

<h3>Description</h3>

<p>Plots the mean (or other summary) of the response for two-way
combinations of factors, thereby illustrating possible interactions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interaction.plot(x.factor, trace.factor, response, fun = mean,
                 type = c("l", "p", "b", "o", "c"), legend = TRUE,
                 trace.label = deparse1(substitute(trace.factor)),
                 fixed = FALSE,
                 xlab = deparse1(substitute(x.factor)),
                 ylab = ylabel,
                 ylim = range(cells, na.rm = TRUE),
                 lty = nc:1, col = 1, pch = c(1:9, 0, letters),
                 xpd = NULL, leg.bg = par("bg"), leg.bty = "n",
                 xtick = FALSE, xaxt = par("xaxt"), axes = TRUE,
                 ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interaction.plot_+3A_x.factor">x.factor</code></td>
<td>
<p>a factor whose levels will form the x axis.</p>
</td></tr>
<tr><td><code id="interaction.plot_+3A_trace.factor">trace.factor</code></td>
<td>
<p>another factor whose levels will form the traces.</p>
</td></tr>
<tr><td><code id="interaction.plot_+3A_response">response</code></td>
<td>
<p>a numeric variable giving the response.</p>
</td></tr>
<tr><td><code id="interaction.plot_+3A_fun">fun</code></td>
<td>
<p>the function to compute the summary. Should return a single
real value.</p>
</td></tr>
<tr><td><code id="interaction.plot_+3A_type">type</code></td>
<td>
<p>the type of plot (see <code><a href="graphics.html#topic+plot.default">plot.default</a></code>): lines
or points or both.</p>
</td></tr>
<tr><td><code id="interaction.plot_+3A_legend">legend</code></td>
<td>
<p>logical. Should a legend be included?</p>
</td></tr>
<tr><td><code id="interaction.plot_+3A_trace.label">trace.label</code></td>
<td>
<p>overall label for the legend.</p>
</td></tr>
<tr><td><code id="interaction.plot_+3A_fixed">fixed</code></td>
<td>
<p>logical.  Should the legend be in the order of the levels
of <code>trace.factor</code> (<code>TRUE</code>) or in the order of the
traces at their right-hand ends (<code>FALSE</code>, the default)?</p>
</td></tr>
<tr><td><code id="interaction.plot_+3A_xlab">xlab</code>, <code id="interaction.plot_+3A_ylab">ylab</code></td>
<td>
<p>the x and y label of the plot each with a sensible default.</p>
</td></tr>
<tr><td><code id="interaction.plot_+3A_ylim">ylim</code></td>
<td>
<p>numeric of length 2 giving the y limits for the plot.</p>
</td></tr>
<tr><td><code id="interaction.plot_+3A_lty">lty</code></td>
<td>
<p>line type for the lines drawn, with sensible default.</p>
</td></tr>
<tr><td><code id="interaction.plot_+3A_col">col</code></td>
<td>
<p>the color to be used for plotting.</p>
</td></tr>
<tr><td><code id="interaction.plot_+3A_pch">pch</code></td>
<td>
<p>a vector of plotting symbols or characters, with sensible
default.</p>
</td></tr>
<tr><td><code id="interaction.plot_+3A_xpd">xpd</code></td>
<td>
<p>determines clipping behaviour for the <code><a href="graphics.html#topic+legend">legend</a></code>
used, see <code><a href="graphics.html#topic+par">par</a>(xpd)</code>.  Per default, the legend is
<em>not</em> clipped at the figure border.</p>
</td></tr>
<tr><td><code id="interaction.plot_+3A_leg.bg">leg.bg</code>, <code id="interaction.plot_+3A_leg.bty">leg.bty</code></td>
<td>
<p>arguments passed to <code><a href="graphics.html#topic+legend">legend</a>()</code>.</p>
</td></tr>
<tr><td><code id="interaction.plot_+3A_xtick">xtick</code></td>
<td>
<p>logical. Should tick marks be used on the x axis?</p>
</td></tr>
<tr><td><code id="interaction.plot_+3A_xaxt">xaxt</code>, <code id="interaction.plot_+3A_axes">axes</code>, <code id="interaction.plot_+3A_...">...</code></td>
<td>
<p>graphics parameters to be passed to the plotting routines.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default the levels of <code>x.factor</code> are plotted on the x axis in
their given order, with extra space on the right for the legend
(if specified). If <code>x.factor</code> is an ordered factor and the levels
are numeric, these numeric values are used for the x axis.
</p>
<p>The response and hence its summary can contain missing values. If so,
the missing values and the line segments joining them are omitted from
the plot (and this can be somewhat disconcerting).
</p>
<p>The graphics parameters <code>xlab</code>, <code>ylab</code>, <code>ylim</code>,
<code>lty</code>, <code>col</code> and <code>pch</code> are given suitable defaults
(and <code>xlim</code> and <code>xaxs</code> are set and cannot be overridden).
The defaults are to cycle through the line types, use the foreground
colour, and to use the symbols 1:9, 0, and the small letters to plot
the traces.
</p>


<h3>Note</h3>

<p>Some of the argument names and the precise behaviour are chosen for
S-compatibility.
</p>


<h3>References</h3>

<p>Chambers, J. M., Freeny, A and Heiberger, R. M. (1992)
<em>Analysis of variance; designed experiments.</em>
Chapter 5 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

with(ToothGrowth, {
interaction.plot(dose, supp, len, fixed = TRUE)
dose &lt;- ordered(dose)
interaction.plot(dose, supp, len, fixed = TRUE,
                 col = 2:3, leg.bty = "o", xtick = TRUE)
interaction.plot(dose, supp, len, fixed = TRUE, col = 2:3, type = "p")
})

with(OrchardSprays, {
  interaction.plot(treatment, rowpos, decrease)
  interaction.plot(rowpos, treatment, decrease, cex.axis = 0.8)
  ## order the rows by their mean effect
  rowpos &lt;- factor(rowpos,
                   levels = sort.list(tapply(decrease, rowpos, mean)))
  interaction.plot(rowpos, treatment, decrease, col = 2:9, lty = 1)
})




















</code></pre>

<hr>
<h2 id='IQR'>The Interquartile Range</h2><span id='topic+IQR'></span>

<h3>Description</h3>

<p>computes interquartile range of the <code>x</code> values.</p>


<h3>Usage</h3>

<pre><code class='language-R'>IQR(x, na.rm = FALSE, type = 7)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IQR_+3A_x">x</code></td>
<td>
<p>a numeric vector.</p>
</td></tr>
<tr><td><code id="IQR_+3A_na.rm">na.rm</code></td>
<td>
<p>logical. Should missing values be removed?</p>
</td></tr>
<tr><td><code id="IQR_+3A_type">type</code></td>
<td>
<p>an integer selecting one of the many quantile algorithms,
see <code><a href="#topic+quantile">quantile</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that this function computes the quartiles using the
<code><a href="#topic+quantile">quantile</a></code> function rather than following
Tukey's recommendations,
i.e., <code>IQR(x) = quantile(x, 3/4) - quantile(x, 1/4)</code>.
</p>
<p>For normally <code class="reqn">N(m,1)</code> distributed <code class="reqn">X</code>, the expected value of
<code>IQR(X)</code> is <code>2*qnorm(3/4) = 1.3490</code>, i.e., for a normal-consistent
estimate of the standard deviation, use <code>IQR(x) / 1.349</code>.
</p>


<h3>References</h3>

<p>Tukey, J. W. (1977).
<em>Exploratory Data Analysis.</em>
Reading: Addison-Wesley.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fivenum">fivenum</a></code>, <code><a href="#topic+mad">mad</a></code> which is more robust,
<code><a href="base.html#topic+range">range</a></code>, <code><a href="#topic+quantile">quantile</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>IQR(rivers)
</code></pre>

<hr>
<h2 id='is.empty.model'>Test if a Model's Formula is Empty</h2><span id='topic+is.empty.model'></span>

<h3>Description</h3>

<p><span class="rlang"><b>R</b></span>'s formula notation allows models with no intercept and no
predictors. These require special handling internally.
<code>is.empty.model()</code> checks whether an object describes an empty
model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.empty.model(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.empty.model_+3A_x">x</code></td>
<td>
<p>A <code>terms</code> object or an object with a <code>terms</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the model is empty
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lm">lm</a></code>, <code><a href="#topic+glm">glm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(20)
is.empty.model(y ~ 0)
is.empty.model(y ~ -1)
is.empty.model(lm(y ~ 0))
</code></pre>

<hr>
<h2 id='isoreg'>Isotonic / Monotone Regression</h2><span id='topic+isoreg'></span>

<h3>Description</h3>

<p>Compute the isotonic (monotonically increasing nonparametric) least
squares regression which is piecewise constant.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isoreg(x, y = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="isoreg_+3A_x">x</code>, <code id="isoreg_+3A_y">y</code></td>
<td>

<p>coordinate vectors of the regression points.  Alternatively a single
plotting structure can be specified: see <code><a href="grDevices.html#topic+xy.coords">xy.coords</a></code>.
The y values, and even <code>sum(y)</code> must be finite, currently.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm determines the convex minorant <code class="reqn">m(x)</code> of the
<em>cumulative</em> data (i.e., <code>cumsum(y)</code>) which is piecewise
linear and the result is <code class="reqn">m'(x)</code>, a step function with level
changes at locations where the convex <code class="reqn">m(x)</code> touches the
cumulative data polygon and changes slope.<br />
<code><a href="#topic+as.stepfun">as.stepfun</a>()</code> returns a <code><a href="#topic+stepfun">stepfun</a></code>
object which can be more parsimonious.
</p>


<h3>Value</h3>

<p><code>isoreg()</code> returns an object of class <code>isoreg</code> which is
basically a list with components
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>original (constructed) abscissa values <code>x</code>.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>corresponding y values.</p>
</td></tr>
<tr><td><code>yf</code></td>
<td>
<p>fitted values corresponding to <em>ordered</em> x values.</p>
</td></tr>
<tr><td><code>yc</code></td>
<td>
<p>cumulative y values corresponding to <em>ordered</em> x values.</p>
</td></tr>
<tr><td><code>iKnots</code></td>
<td>
<p>integer vector giving indices where the fitted curve jumps,
i.e., where the convex minorant has kinks.</p>
</td></tr>
<tr><td><code>isOrd</code></td>
<td>
<p>logical indicating if original x values were ordered
increasingly already.</p>
</td></tr>
<tr><td><code>ord</code></td>
<td>
<p><code>if(!isOrd)</code>: integer permutation <code><a href="base.html#topic+order">order</a>(x)</code> of
<em>original</em> <code>x</code>.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the <code><a href="base.html#topic+call">call</a></code> to <code>isoreg()</code> used.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The code should be improved to accept <em>weights</em> additionally and
solve the corresponding weighted least squares problem.<br />
&lsquo;Patches are welcome!&rsquo;
</p>


<h3>References</h3>

<p>Barlow, R. E., Bartholomew, D. J., Bremner, J. M., and Brunk, H. D. (1972)
<em>Statistical inference under order restrictions</em>; Wiley, London.
</p>
<p>Robertson, T., Wright, F. T. and Dykstra, R. L. (1988)
<em>Order Restricted Statistical Inference</em>; Wiley, New York.
</p>


<h3>See Also</h3>

<p>the plotting method <code><a href="#topic+plot.isoreg">plot.isoreg</a></code> with more examples;
<code><a href="MASS.html#topic+isoMDS">isoMDS</a>()</code> from the <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a> package internally
uses isotonic regression.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

(ir &lt;- isoreg(c(1,0,4,3,3,5,4,2,0)))
plot(ir, plot.type = "row")

(ir3 &lt;- isoreg(y3 &lt;- c(1,0,4,3,3,5,4,2, 3))) # last "3", not "0"
(fi3 &lt;- as.stepfun(ir3))
(ir4 &lt;- isoreg(1:10, y4 &lt;- c(5, 9, 1:2, 5:8, 3, 8)))
cat(sprintf("R^2 = %.2f\n",
            1 - sum(residuals(ir4)^2) / ((10-1)*var(y4))))

## If you are interested in the knots alone :
with(ir4, cbind(iKnots, yf[iKnots]))

## Example of unordered x[] with ties:
x &lt;- sample((0:30)/8)
y &lt;- exp(x)
x. &lt;- round(x) # ties!
plot(m &lt;- isoreg(x., y))
stopifnot(all.equal(with(m, yf[iKnots]),
                    as.vector(tapply(y, x., mean))))
</code></pre>

<hr>
<h2 id='KalmanLike'>Kalman Filtering</h2><span id='topic+KalmanLike'></span><span id='topic+KalmanRun'></span><span id='topic+KalmanSmooth'></span><span id='topic+KalmanForecast'></span><span id='topic+makeARIMA'></span>

<h3>Description</h3>

<p>Use Kalman Filtering to find the (Gaussian) log-likelihood, or for
forecasting or smoothing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KalmanLike(y, mod, nit = 0L, update = FALSE)
KalmanRun(y, mod, nit = 0L, update = FALSE)
KalmanSmooth(y, mod, nit = 0L)
KalmanForecast(n.ahead = 10L, mod, update = FALSE)

makeARIMA(phi, theta, Delta, kappa = 1e6,
          SSinit = c("Gardner1980", "Rossignol2011"),
          tol = .Machine$double.eps)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KalmanLike_+3A_y">y</code></td>
<td>
<p>a univariate time series.</p>
</td></tr>
<tr><td><code id="KalmanLike_+3A_mod">mod</code></td>
<td>
<p>a list describing the state-space model: see &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="KalmanLike_+3A_nit">nit</code></td>
<td>
<p>the time at which the initialization is computed.
<code>nit = 0L</code> implies that the initialization is for a one-step
prediction, so <code>Pn</code> should not be computed at the first step.</p>
</td></tr>
<tr><td><code id="KalmanLike_+3A_update">update</code></td>
<td>
<p>if <code>TRUE</code> the update <code>mod</code> object will be
returned as attribute <code>"mod"</code> of the result.</p>
</td></tr>
<tr><td><code id="KalmanLike_+3A_n.ahead">n.ahead</code></td>
<td>
<p>the number of steps ahead for which prediction is
required.</p>
</td></tr>
<tr><td><code id="KalmanLike_+3A_phi">phi</code>, <code id="KalmanLike_+3A_theta">theta</code></td>
<td>
<p>numeric vectors of length <code class="reqn">\ge 0</code> giving AR
and MA parameters.</p>
</td></tr>
<tr><td><code id="KalmanLike_+3A_delta">Delta</code></td>
<td>
<p>vector of differencing coefficients, so an ARMA model is
fitted to <code>y[t] - Delta[1]*y[t-1] - ...</code>.</p>
</td></tr>
<tr><td><code id="KalmanLike_+3A_kappa">kappa</code></td>
<td>
<p>the prior variance (as a multiple of the innovations
variance) for the past observations in a differenced model.</p>
</td></tr>
<tr><td><code id="KalmanLike_+3A_ssinit">SSinit</code></td>
<td>
<p>a string specifying the algorithm to compute the
<code>Pn</code> part of the state-space initialization; see
&lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="KalmanLike_+3A_tol">tol</code></td>
<td>
<p>tolerance eventually passed to <code><a href="base.html#topic+solve.default">solve.default</a></code>
when <code>SSinit = "Rossignol2011"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions work with a general univariate state-space model
with state vector &lsquo;<span class="samp">&#8288;a&#8288;</span>&rsquo;, transitions &lsquo;<span class="samp">&#8288;a &lt;- T a + R e&#8288;</span>&rsquo;,
<code class="reqn">e \sim {\cal N}(0, \kappa Q)</code> and observation
equation &lsquo;<span class="samp">&#8288;y = Z'a + eta&#8288;</span>&rsquo;,
<code class="reqn">(eta\equiv\eta), \eta \sim {\cal N}(0, \kappa h)</code>.
The likelihood is a profile likelihood after estimation of
<code class="reqn">\kappa</code>.
</p>
<p>The model is specified as a list with at least components
</p>

<dl>
<dt><code>T</code></dt><dd><p>the transition matrix</p>
</dd>
<dt><code>Z</code></dt><dd><p>the observation coefficients</p>
</dd>
<dt><code>h</code></dt><dd><p>the observation variance</p>
</dd>
<dt><code>V</code></dt><dd><p>&lsquo;<span class="samp">&#8288;RQR'&#8288;</span>&rsquo;</p>
</dd>
<dt><code>a</code></dt><dd><p>the current state estimate</p>
</dd>
<dt><code>P</code></dt><dd><p>the current estimate of the state uncertainty matrix <code class="reqn">Q</code></p>
</dd>
<dt><code>Pn</code></dt><dd><p>the estimate at time <code class="reqn">t-1</code> of the state
uncertainty matrix <code class="reqn">Q</code> (not updated by <code>KalmanForecast</code>).</p>
</dd>
</dl>

<p><code>KalmanSmooth</code> is the workhorse function for <code><a href="#topic+tsSmooth">tsSmooth</a></code>.
</p>
<p><code>makeARIMA</code> constructs the state-space model for an ARIMA model,
see also <code><a href="#topic+arima">arima</a></code>.
</p>
<p>The state-space initialization has used Gardner <abbr>et al.</abbr>'s method
(<code>SSinit = "Gardner1980"</code>), as only method for years.  However,
that suffers sometimes from deficiencies when close to non-stationarity.
For this reason, it may be replaced as default in the future and only
kept for reproducibility reasons.  Explicit specification of
<code>SSinit</code> is therefore recommended, notably also in
<code><a href="#topic+arima">arima</a>()</code>.
The <code>"Rossignol2011"</code> method has been proposed and partly
documented by Raphael Rossignol, Univ. Grenoble, on 2011-09-20 (see
PR#14682, below), and later been ported to C by Matwey V. Kornilov.
It computes the covariance matrix of
<code class="reqn">(X_{t-1},...,X_{t-p},Z_t,...,Z_{t-q})</code>
by the method of difference equations
(page 93 of Brockwell and Davis (1991)),
apparently suggested by a referee of Gardner <abbr>et al.</abbr> (see p.314 of
their paper).
</p>


<h3>Value</h3>

<p>For <code>KalmanLike</code>, a list with components <code>Lik</code> (the
log-likelihood less some constants) and <code>s2</code>, the estimate of
<code class="reqn">\kappa</code>.
</p>
<p>For <code>KalmanRun</code>, a list with components <code>values</code>, a vector
of length 2 giving the output of <code>KalmanLike</code>, <code>resid</code> (the
residuals) and <code>states</code>, the contemporaneous state estimates,
a matrix with one row for each observation time.
</p>
<p>For <code>KalmanSmooth</code>, a list with two components.
Component <code>smooth</code> is a <code>n</code> by <code>p</code> matrix of state
estimates based on all the observations, with one row for each time.
Component <code>var</code> is a <code>n</code> by <code>p</code> by <code>p</code> array of
variance matrices.
</p>
<p>For <code>KalmanForecast</code>, a list with components <code>pred</code>, the
predictions, and <code>var</code>, the unscaled variances of the prediction
errors (to be multiplied by <code>s2</code>).
</p>
<p>For <code>makeARIMA</code>, a model list including components for
its arguments.
</p>


<h3>Warning</h3>

<p>These functions are designed to be called from other functions which
check the validity of the arguments passed, so very little checking is
done.
</p>


<h3>References</h3>

<p>Brockwell, P. J. and Davis, R. A. (1991).
<em>Time Series: Theory and Methods</em>, second edition.
Springer.
</p>
<p>Durbin, J. and Koopman, S. J. (2001).
<em>Time Series Analysis by State Space Methods</em>.
Oxford University Press.
</p>
<p>Gardner, G, Harvey, A. C. and Phillips, G. D. A. (1980).
Algorithm AS 154: An algorithm for exact maximum likelihood estimation
of  autoregressive-moving average models by means of Kalman filtering.
<em>Applied Statistics</em>, <b>29</b>, 311&ndash;322.
<a href="https://doi.org/10.2307/2346910">doi:10.2307/2346910</a>.
</p>
<p>R bug report PR#14682 (2011-2013)
<a href="https://bugs.r-project.org/show_bug.cgi?id=14682">https://bugs.r-project.org/show_bug.cgi?id=14682</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+arima">arima</a></code>, <code><a href="#topic+StructTS">StructTS</a></code>. <code><a href="#topic+tsSmooth">tsSmooth</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## an ARIMA fit
fit3 &lt;- arima(presidents, c(3, 0, 0))
predict(fit3, 12)
## reconstruct this
pr &lt;- KalmanForecast(12, fit3$model)
pr$pred + fit3$coef[4]
sqrt(pr$var * fit3$sigma2)
## and now do it year by year
mod &lt;- fit3$model
for(y in 1:3) {
  pr &lt;- KalmanForecast(4, mod, TRUE)
  print(list(pred = pr$pred + fit3$coef["intercept"], 
             se = sqrt(pr$var * fit3$sigma2)))
  mod &lt;- attr(pr, "mod")
}
</code></pre>

<hr>
<h2 id='kernapply'>Apply Smoothing Kernel</h2><span id='topic+kernapply'></span><span id='topic+kernapply.default'></span><span id='topic+kernapply.ts'></span><span id='topic+kernapply.tskernel'></span><span id='topic+kernapply.vector'></span>

<h3>Description</h3>

<p><code>kernapply</code> computes the convolution between an input sequence
and a specific kernel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kernapply(x, ...)

## Default S3 method:
kernapply(x, k, circular = FALSE, ...)
## S3 method for class 'ts'
kernapply(x, k, circular = FALSE, ...)
## S3 method for class 'vector'
kernapply(x, k, circular = FALSE, ...)

## S3 method for class 'tskernel'
kernapply(x, k, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernapply_+3A_x">x</code></td>
<td>
<p>an input vector, matrix, time series or kernel to be smoothed.</p>
</td></tr>
<tr><td><code id="kernapply_+3A_k">k</code></td>
<td>
<p>smoothing <code>"tskernel"</code> object.</p>
</td></tr>
<tr><td><code id="kernapply_+3A_circular">circular</code></td>
<td>
<p>a logical indicating whether the input sequence to be
smoothed is treated as circular, i.e., periodic.</p>
</td></tr>
<tr><td><code id="kernapply_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A smoothed version of the input sequence.
</p>


<h3>Note</h3>

<p>This uses <code><a href="#topic+fft">fft</a></code> to perform the convolution, so is fastest
when <code>NROW(x)</code> is a power of 2 or some other highly composite
integer.
</p>


<h3>Author(s)</h3>

<p>A. Trapletti</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernel">kernel</a></code>, <code><a href="#topic+convolve">convolve</a></code>, <code><a href="#topic+filter">filter</a></code>,
<code><a href="#topic+spectrum">spectrum</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## see 'kernel' for examples
</code></pre>

<hr>
<h2 id='kernel'>Smoothing Kernel Objects</h2><span id='topic+kernel'></span><span id='topic+bandwidth.kernel'></span><span id='topic+df.kernel'></span><span id='topic+is.tskernel'></span><span id='topic+plot.tskernel'></span>

<h3>Description</h3>

<p>The <code>"tskernel"</code> class is designed to represent discrete
symmetric normalized smoothing kernels.  These kernels can be used to
smooth vectors, matrices, or time series objects.
</p>
<p>There are <code><a href="base.html#topic+print">print</a></code>, <code><a href="graphics.html#topic+plot">plot</a></code> and <code><a href="Matrix.html#topic++5B">[</a></code>
methods for these kernel objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kernel(coef, m = 2, r, name)

df.kernel(k)
bandwidth.kernel(k)
is.tskernel(k)

## S3 method for class 'tskernel'
plot(x, type = "h", xlab = "k", ylab = "W[k]",
     main = attr(x,"name"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernel_+3A_coef">coef</code></td>
<td>
<p>the upper half of the smoothing kernel coefficients
(including coefficient zero) <em>or</em> the name of a kernel
(currently <code>"daniell"</code>, <code>"dirichlet"</code>, <code>"fejer"</code> or
<code>"modified.daniell"</code>).</p>
</td></tr>
<tr><td><code id="kernel_+3A_m">m</code></td>
<td>
<p>the kernel dimension(s) if <code>coef</code> is a name.  When <code>m</code>
has length larger than one, it means the convolution of
kernels of dimension <code>m[j]</code>, for <code>j in 1:length(m)</code>.
Currently this is supported only for the named <code>"*daniell"</code> kernels.</p>
</td></tr>
<tr><td><code id="kernel_+3A_name">name</code></td>
<td>
<p>the name the kernel will be called.</p>
</td></tr>
<tr><td><code id="kernel_+3A_r">r</code></td>
<td>
<p>the kernel order for a Fejer kernel.</p>
</td></tr>
<tr><td><code id="kernel_+3A_k">k</code>, <code id="kernel_+3A_x">x</code></td>
<td>
<p>a <code>"tskernel"</code> object.</p>
</td></tr>
<tr><td><code id="kernel_+3A_type">type</code>, <code id="kernel_+3A_xlab">xlab</code>, <code id="kernel_+3A_ylab">ylab</code>, <code id="kernel_+3A_main">main</code>, <code id="kernel_+3A_...">...</code></td>
<td>
<p>arguments passed to
<code><a href="graphics.html#topic+plot.default">plot.default</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>kernel</code> is used to construct a general kernel or named specific
kernels.  The modified Daniell kernel halves the end coefficients.
</p>
<p>The <code><a href="Matrix.html#topic++5B">[</a></code> method allows natural indexing of kernel objects
with indices in <code>(-m) : m</code>.  The normalization is such that for
<code>k &lt;- kernel(*)</code>, <code>sum(k[ -k$m : k$m ])</code> is one.
</p>
<p><code>df.kernel</code> returns the &lsquo;equivalent degrees of freedom&rsquo; of
a smoothing kernel as defined in
Brockwell and Davis (1991), page 362,
and <code>bandwidth.kernel</code> returns the equivalent bandwidth as
defined in Bloomfield (1976), p. 201,
with a continuity correction.
</p>


<h3>Value</h3>

<p><code>kernel()</code> returns an object of class <code>"tskernel"</code> which is
basically a list with the two components <code>coef</code> and the kernel
dimension <code>m</code>.  An additional attribute is <code>"name"</code>.
</p>


<h3>Author(s)</h3>

<p>A. Trapletti; modifications by B.D. Ripley</p>


<h3>References</h3>

<p>Bloomfield, P. (1976)
<em>Fourier Analysis of Time Series: An Introduction.</em>
Wiley.
</p>
<p>Brockwell, P.J. and Davis, R.A. (1991)
<em>Time Series: Theory and Methods.</em>
Second edition. Springer, pp. 350&ndash;365.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernapply">kernapply</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

## Demonstrate a simple trading strategy for the
## financial time series German stock index DAX.
x &lt;- EuStockMarkets[,1]
k1 &lt;- kernel("daniell", 50)  # a long moving average
k2 &lt;- kernel("daniell", 10)  # and a short one
plot(k1)
plot(k2)
x1 &lt;- kernapply(x, k1)
x2 &lt;- kernapply(x, k2)
plot(x)
lines(x1, col = "red")    # go long if the short crosses the long upwards
lines(x2, col = "green")  # and go short otherwise

## More interesting kernels
kd &lt;- kernel("daniell", c(3, 3))
kd # note the unusual indexing
kd[-2:2]
plot(kernel("fejer", 100, r = 6))
plot(kernel("modified.daniell", c(7,5,3)))

# Reproduce example 10.4.3 from Brockwell and Davis (1991)
spectrum(sunspot.year, kernel = kernel("daniell", c(11,7,3)), log = "no")
</code></pre>

<hr>
<h2 id='kmeans'>
K-Means Clustering
</h2><span id='topic+kmeans'></span><span id='topic+print.kmeans'></span><span id='topic+fitted.kmeans'></span>

<h3>Description</h3>

<p>Perform k-means clustering on a data matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmeans(x, centers, iter.max = 10, nstart = 1,
       algorithm = c("Hartigan-Wong", "Lloyd", "Forgy",
                     "MacQueen"), trace = FALSE)
## S3 method for class 'kmeans'
fitted(object, method = c("centers", "classes"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kmeans_+3A_x">x</code></td>
<td>
<p>numeric matrix of data, or an object that can be coerced to
such a matrix (such as a numeric vector or a data frame with all
numeric columns).</p>
</td></tr>
<tr><td><code id="kmeans_+3A_centers">centers</code></td>
<td>
<p>either the number of clusters, say <code class="reqn">k</code>, or a set of
initial (distinct) cluster centres.  If a number, a random set of
(distinct) rows in <code>x</code> is chosen as the initial centres.</p>
</td></tr>
<tr><td><code id="kmeans_+3A_iter.max">iter.max</code></td>
<td>
<p>the maximum number of iterations allowed.</p>
</td></tr>
<tr><td><code id="kmeans_+3A_nstart">nstart</code></td>
<td>
<p>if <code>centers</code> is a number, how many random sets
should be chosen?</p>
</td></tr>
<tr><td><code id="kmeans_+3A_algorithm">algorithm</code></td>
<td>
<p>character: may be abbreviated.  Note that
<code>"Lloyd"</code> and <code>"Forgy"</code> are alternative names for one
algorithm.</p>
</td></tr>
<tr><td><code id="kmeans_+3A_object">object</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object of class <code>"kmeans"</code>, typically the
result <code>ob</code> of <code>ob &lt;- kmeans(..)</code>.</p>
</td></tr>
<tr><td><code id="kmeans_+3A_method">method</code></td>
<td>
<p>character: may be abbreviated. <code>"centers"</code> causes
<code>fitted</code> to return cluster centers (one for each input point) and
<code>"classes"</code> causes <code>fitted</code> to return a vector of class
assignments.</p>
</td></tr>
<tr><td><code id="kmeans_+3A_trace">trace</code></td>
<td>
<p>logical or integer number, currently only used in the
default method (<code>"Hartigan-Wong"</code>): if positive (or true),
tracing information on the progress of the algorithm is
produced.  Higher values may produce more tracing information.</p>
</td></tr>
<tr><td><code id="kmeans_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data given by <code>x</code> are clustered by the <code class="reqn">k</code>-means method,
which aims to partition the points into <code class="reqn">k</code> groups such that the
sum of squares from points to the assigned cluster centres is minimized.
At the minimum, all cluster centres are at the mean of their Voronoi
sets (the set of data points which are nearest to the cluster centre).
</p>
<p>The algorithm of Hartigan and Wong (1979) is used by
default.  Note that some authors use <code class="reqn">k</code>-means to refer to a
specific algorithm rather than the general method: most commonly the
algorithm given by MacQueen (1967) but sometimes that given
by Lloyd (1957) and Forgy (1965).
The Hartigan&ndash;Wong algorithm generally does a better job than
either of those, but trying several random starts (<code>nstart</code><code class="reqn">&gt;
  1</code>) is often recommended.  In rare cases, when some of the points
(rows of <code>x</code>) are extremely close, the algorithm may not converge
in the &ldquo;Quick-Transfer&rdquo; stage, signalling a warning (and
returning <code>ifault = 4</code>).  Slight
rounding of the data may be advisable in that case.
</p>
<p>For ease of programmatic exploration, <code class="reqn">k = 1</code> is allowed, notably
returning the center and <code>withinss</code>.
</p>
<p>Except for the Lloyd&ndash;Forgy method, <code class="reqn">k</code> clusters will always be
returned if a number is specified.
If an initial matrix of centres is supplied, it is possible that
no point will be closest to one or more centres, which is currently
an error for the Hartigan&ndash;Wong method.
</p>


<h3>Value</h3>

<p><code>kmeans</code> returns an object of class <code>"kmeans"</code> which has a
<code>print</code> and a <code>fitted</code> method.  It is a list with at least
the following components:
</p>
<table>
<tr><td><code>cluster</code></td>
<td>

<p>A vector of integers (from <code>1:k</code>) indicating the cluster to
which each point is allocated.
</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>A matrix of cluster centres.</p>
</td></tr>
<tr><td><code>totss</code></td>
<td>
<p>The total sum of squares.</p>
</td></tr>
<tr><td><code>withinss</code></td>
<td>
<p>Vector of within-cluster sum of squares,
one component per cluster.</p>
</td></tr>
<tr><td><code>tot.withinss</code></td>
<td>
<p>Total within-cluster sum of squares,
i.e. <code>sum(withinss)</code>.</p>
</td></tr>
<tr><td><code>betweenss</code></td>
<td>
<p>The between-cluster sum of squares,
i.e. <code>totss-tot.withinss</code>.</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>The number of points in each cluster.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>The number of (outer) iterations.</p>
</td></tr>
<tr><td><code>ifault</code></td>
<td>
<p>integer: indicator of a possible algorithm problem
&ndash; for experts.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The clusters are numbered in the returned object, but they are a
<em>set</em> and no ordering is implied.  (Their apparent ordering may
differ by platform.)
</p>


<h3>References</h3>

<p>Forgy, E. W. (1965).
Cluster analysis of multivariate data: efficiency vs interpretability
of classifications. 
<em>Biometrics</em>, <b>21</b>, 768&ndash;769.
</p>
<p>Hartigan, J. A. and Wong, M. A. (1979).
Algorithm AS 136: A K-means clustering algorithm.
<em>Applied Statistics</em>, <b>28</b>, 100&ndash;108.
<a href="https://doi.org/10.2307/2346830">doi:10.2307/2346830</a>.
</p>
<p>Lloyd, S. P. (1957, 1982).
Least squares quantization in PCM.
Technical Note, Bell Laboratories.
Published in 1982 in <em>IEEE Transactions on Information Theory</em>,
<b>28</b>, 128&ndash;137. 
</p>
<p>MacQueen, J. (1967).
Some methods for classification and analysis of multivariate
observations.
In <em>Proceedings of the Fifth Berkeley Symposium on  Mathematical
Statistics and Probability</em>, 
eds L. M. Le Cam &amp; J. Neyman,
<b>1</b>, pp. 281&ndash;297.
Berkeley, CA: University of California Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

# a 2-dimensional example
x &lt;- rbind(matrix(rnorm(100, sd = 0.3), ncol = 2),
           matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2))
colnames(x) &lt;- c("x", "y")
(cl &lt;- kmeans(x, 2))
plot(x, col = cl$cluster)
points(cl$centers, col = 1:2, pch = 8, cex = 2)

# sum of squares
ss &lt;- function(x) sum(scale(x, scale = FALSE)^2)

## cluster centers "fitted" to each obs.:
fitted.x &lt;- fitted(cl);  head(fitted.x)
resid.x &lt;- x - fitted(cl)

## Equalities : ----------------------------------
cbind(cl[c("betweenss", "tot.withinss", "totss")], # the same two columns
         c(ss(fitted.x), ss(resid.x),    ss(x)))
stopifnot(all.equal(cl$ totss,        ss(x)),
	  all.equal(cl$ tot.withinss, ss(resid.x)),
	  ## these three are the same:
	  all.equal(cl$ betweenss,    ss(fitted.x)),
	  all.equal(cl$ betweenss, cl$totss - cl$tot.withinss),
	  ## and hence also
	  all.equal(ss(x), ss(fitted.x) + ss(resid.x))
	  )

kmeans(x,1)$withinss # trivial one-cluster, (its W.SS == ss(x))

## random starts do help here with too many clusters
## (and are often recommended anyway!):
## The ordering of the clusters may be platform-dependent.

(cl &lt;- kmeans(x, 5, nstart = 25))

plot(x, col = cl$cluster)
points(cl$centers, col = 1:5, pch = 8)
</code></pre>

<hr>
<h2 id='kruskal.test'>Kruskal-Wallis Rank Sum Test</h2><span id='topic+kruskal.test'></span><span id='topic+kruskal.test.default'></span><span id='topic+kruskal.test.formula'></span>

<h3>Description</h3>

<p>Performs a Kruskal-Wallis rank sum test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kruskal.test(x, ...)

## Default S3 method:
kruskal.test(x, g, ...)

## S3 method for class 'formula'
kruskal.test(formula, data, subset, na.action, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kruskal.test_+3A_x">x</code></td>
<td>
<p>a numeric vector of data values, or a list of numeric data
vectors.  Non-numeric elements of a list will be coerced, with a
warning.</p>
</td></tr>
<tr><td><code id="kruskal.test_+3A_g">g</code></td>
<td>
<p>a vector or factor object giving the group for the
corresponding elements of <code>x</code>.  Ignored with a warning if
<code>x</code> is a list.</p>
</td></tr>
<tr><td><code id="kruskal.test_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>response ~ group</code> where
<code>response</code> gives the data values and <code>group</code> a vector or
factor of the corresponding groups.</p>
</td></tr> 
<tr><td><code id="kruskal.test_+3A_data">data</code></td>
<td>
<p>an optional matrix or data frame (or similar: see
<code><a href="#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="kruskal.test_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="kruskal.test_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.  Defaults to
<code>getOption("na.action")</code>.</p>
</td></tr>
<tr><td><code id="kruskal.test_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>kruskal.test</code> performs a Kruskal-Wallis rank sum test of the
null that the location parameters of the distribution of <code>x</code>
are the same in each group (sample).  The alternative is that they
differ in at least one.
</p>
<p>If <code>x</code> is a list, its elements are taken as the samples to be
compared, and hence have to be numeric data vectors.  In this case,
<code>g</code> is ignored, and one can simply use <code>kruskal.test(x)</code>
to perform the test.  If the samples are not yet contained in a
list, use <code>kruskal.test(list(x, ...))</code>.
</p>
<p>Otherwise, <code>x</code> must be a numeric data vector, and <code>g</code> must
be a vector or factor object of the same length as <code>x</code> giving
the group for the corresponding elements of <code>x</code>.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the Kruskal-Wallis rank sum statistic.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of freedom of the approximate
chi-squared distribution of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the character string <code>"Kruskal-Wallis rank sum test"</code>.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the names of the data.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Myles Hollander and Douglas A. Wolfe (1973),
<em>Nonparametric Statistical Methods.</em>
New York: John Wiley &amp; Sons.
Pages 115&ndash;120.
</p>


<h3>See Also</h3>

<p>The Wilcoxon rank sum test (<code><a href="#topic+wilcox.test">wilcox.test</a></code>) as the special
case for two samples;
<code><a href="#topic+lm">lm</a></code> together with <code><a href="#topic+anova">anova</a></code> for performing
one-way location analysis under normality assumptions; with Student's
t test (<code><a href="#topic+t.test">t.test</a></code>) as the special case for two samples.
</p>
<p><code><a href="coin.html#topic+LocationTests">wilcox_test</a></code> in package
<a href="https://CRAN.R-project.org/package=coin"><span class="pkg">coin</span></a> for exact, asymptotic and Monte Carlo
<em>conditional</em> p-values, including in the presence of ties.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Hollander &amp; Wolfe (1973), 116.
## Mucociliary efficiency from the rate of removal of dust in normal
##  subjects, subjects with obstructive airway disease, and subjects
##  with asbestosis.
x &lt;- c(2.9, 3.0, 2.5, 2.6, 3.2) # normal subjects
y &lt;- c(3.8, 2.7, 4.0, 2.4)      # with obstructive airway disease
z &lt;- c(2.8, 3.4, 3.7, 2.2, 2.0) # with asbestosis
kruskal.test(list(x, y, z))
## Equivalently,
x &lt;- c(x, y, z)
g &lt;- factor(rep(1:3, c(5, 4, 5)),
            labels = c("Normal subjects",
                       "Subjects with obstructive airway disease",
                       "Subjects with asbestosis"))
kruskal.test(x, g)

## Formula interface.
require(graphics)
boxplot(Ozone ~ Month, data = airquality)
kruskal.test(Ozone ~ Month, data = airquality)
</code></pre>

<hr>
<h2 id='ks.test'>Kolmogorov-Smirnov Tests</h2><span id='topic+ks.test'></span><span id='topic+ks.test.default'></span><span id='topic+ks.test.formula'></span>

<h3>Description</h3>

<p>Perform a one- or two-sample Kolmogorov-Smirnov test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ks.test(x, ...)
## Default S3 method:
ks.test(x, y, ...,
        alternative = c("two.sided", "less", "greater"),
        exact = NULL, simulate.p.value = FALSE, B = 2000)
## S3 method for class 'formula'
ks.test(formula, data, subset, na.action, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ks.test_+3A_x">x</code></td>
<td>
<p>a numeric vector of data values.</p>
</td></tr>
<tr><td><code id="ks.test_+3A_y">y</code></td>
<td>
<p>either a numeric vector of data values, or a character string
naming a cumulative distribution function or an actual cumulative
distribution function such as <code>pnorm</code>.  Only continuous CDFs
are valid.</p>
</td></tr>
<tr><td><code id="ks.test_+3A_...">...</code></td>
<td>
<p>for the default method, parameters of the distribution
specified (as a character string) by <code>y</code>.  Otherwise, further
arguments to be passed to or from methods.</p>
</td></tr>
<tr><td><code id="ks.test_+3A_alternative">alternative</code></td>
<td>
<p>indicates the alternative hypothesis and must be
one of <code>"two.sided"</code> (default), <code>"less"</code>, or
<code>"greater"</code>.  You can specify just the initial letter of the
value, but the argument name must be given in full.
See &lsquo;Details&rsquo; for the meanings of the possible values.</p>
</td></tr>
<tr><td><code id="ks.test_+3A_exact">exact</code></td>
<td>
<p><code>NULL</code> or a logical indicating whether an exact
p-value should be computed.  See &lsquo;Details&rsquo; for the meaning of
<code>NULL</code>.</p>
</td></tr>
<tr><td><code id="ks.test_+3A_simulate.p.value">simulate.p.value</code></td>
<td>
<p>a logical indicating whether to compute
p-values by Monte Carlo simulation.  (Ignored for the one-sample
test.)</p>
</td></tr>
<tr><td><code id="ks.test_+3A_b">B</code></td>
<td>
<p>an integer specifying the number of replicates used in the
Monte Carlo test.</p>
</td></tr>
<tr><td><code id="ks.test_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>lhs ~ rhs</code> where <code>lhs</code>
is a numeric variable giving the data values and <code>rhs</code> either
<code>1</code> for a one-sample test or a factor with two levels giving
the corresponding groups for a two-sample test.</p>
</td></tr>
<tr><td><code id="ks.test_+3A_data">data</code></td>
<td>
<p>an optional matrix or data frame (or similar: see
<code><a href="#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="ks.test_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="ks.test_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.  Defaults to
<code>getOption("na.action")</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>y</code> is numeric, a two-sample (Smirnov) test of the null
hypothesis that <code>x</code> and <code>y</code> were drawn from the same
distribution is performed.
</p>
<p>Alternatively, <code>y</code> can be a character string naming a continuous
(cumulative) distribution function, or such a function.  In this case,
a one-sample (Kolmogorov) test is carried out of the null that the
distribution function which generated <code>x</code> is distribution
<code>y</code> with parameters specified by <code>...</code>.
The presence of ties always generates a warning in the one-sample case, as continuous
distributions do not generate them.  If the ties arose from rounding
the tests may be approximately valid, but even modest amounts of
rounding can have a significant effect on the calculated statistic.
</p>
<p>Missing values are silently omitted from <code>x</code> and (in the
two-sample case) <code>y</code>.
</p>
<p>The possible values <code>"two.sided"</code>, <code>"less"</code> and
<code>"greater"</code> of <code>alternative</code> specify the null hypothesis
that the true cumulative distribution function (CDF) of <code>x</code> is equal
to, not less than or not greater than the hypothesized CDF (one-sample
case) or the CDF of <code>y</code> (two-sample case), respectively.  The test
compares the CDFs taking their maximal difference as test statistic,
with the statistic in the <code>"greater"</code> alternative being
<code class="reqn">D^+ = \max_u [ F_x(u) - F_y(u) ]</code>.
Thus in the two-sample case <code>alternative = "greater"</code> includes
distributions for which <code>x</code> is stochastically <em>smaller</em> than
<code>y</code> (the CDF of <code>x</code> lies above and hence to the left of that
for <code>y</code>), in contrast to <code><a href="#topic+t.test">t.test</a></code> or
<code><a href="#topic+wilcox.test">wilcox.test</a></code>.
</p>
<p>Exact p-values are not available for the one-sample case in the
presence of ties.
If <code>exact = NULL</code> (the default), an
exact p-value is computed if the sample size is less than 100 in the
one-sample case <em>and there are no ties</em>, and if the product of
the sample sizes is less than 10000 in the two-sample case, with or
without ties (using the algorithm described in
Schrer and Trenkler (1995)).
Otherwise, the p-value is computed via Monte Carlo simulation in the
two-sample case if <code>simulate.p.value</code> is <code>TRUE</code>, or else
asymptotic distributions are used whose approximations may
be inaccurate in small samples.  In the one-sample two-sided case,
exact p-values are obtained as described in
Marsaglia, Tsang &amp; Wang (2003)
(but not using the optional approximation in the right tail, so
this can be slow for small p-values).  The formula of
Birnbaum &amp; Tingey (1951) is used for the one-sample
one-sided case. 
</p>
<p>If a one-sample test is used, the parameters specified in
<code>...</code> must be pre-specified and not estimated from the data.
There is some more refined distribution theory for the KS test with
estimated parameters (see Durbin, 1973), but that is not implemented
in <code>ks.test</code>.
</p>


<h3>Value</h3>

<p>A list inheriting from classes <code>"ks.test"</code> and <code>"htest"</code>
containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative
hypothesis.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string indicating what type of test was
performed.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the name(s) of the data.</p>
</td></tr>
</table>


<h3>Source</h3>

<p>The two-sided one-sample distribution comes <em>via</em>
Marsaglia, Tsang and Wang (2003).
</p>
<p>Exact distributions for the two-sample (Smirnov) test are computed
by the algorithm proposed by Schrer (1991) and Schrer &amp; Trenkler (1995)
using numerical improvements along the lines of Viehmann (2021).
</p>


<h3>References</h3>

<p>Z. W. Birnbaum and Fred H. Tingey (1951).
One-sided confidence contours for probability distribution functions.
<em>The Annals of Mathematical Statistics</em>, <b>22</b>/4, 592&ndash;596.
<a href="https://doi.org/10.1214/aoms/1177729550">doi:10.1214/aoms/1177729550</a>.
</p>
<p>William J. Conover (1971).
<em>Practical Nonparametric Statistics</em>.
New York: John Wiley &amp; Sons.
Pages 295&ndash;301 (one-sample Kolmogorov test),
309&ndash;314 (two-sample Smirnov test).
</p>
<p>Durbin, J. (1973).
<em>Distribution theory for tests based on the sample distribution
function</em>.
SIAM.
</p>
<p>W. Feller (1948).
On the Kolmogorov-Smirnov limit theorems for empirical distributions.
<em>The Annals of Mathematical Statistics</em>, <b>19</b>(2), 177&ndash;189.
<a href="https://doi.org/10.1214/aoms/1177730243">doi:10.1214/aoms/1177730243</a>.
</p>
<p>George Marsaglia, Wai Wan Tsang and Jingbo Wang (2003).
Evaluating Kolmogorov's distribution.
<em>Journal of Statistical Software</em>, <b>8</b>/18.
<a href="https://doi.org/10.18637/jss.v008.i18">doi:10.18637/jss.v008.i18</a>.
</p>
<p>Gunar Schrer (1991).
Computergesttzte statistische Inferenz am Beispiel der
Kolmogorov-Smirnov Tests.
Diplomarbeit Universitt Osnabrck.
</p>
<p>Gunar Schrer and Dietrich Trenkler (1995).
Exact and Randomization Distributions of Kolmogorov-Smirnov Tests for
Two or Three Samples.
<em>Computational Statistics &amp; Data Analysis</em>, <b>20</b>(2),
185&ndash;202.
<a href="https://doi.org/10.1016/0167-9473%2894%2900040-P">doi:10.1016/0167-9473(94)00040-P</a>.
</p>
<p>Thomas Viehmann (2021).
Numerically more stable computation of the p-values for the two-sample
Kolmogorov-Smirnov test.
<a href="https://arxiv.org/abs/2102.08037">https://arxiv.org/abs/2102.08037</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+psmirnov">psmirnov</a></code>.
</p>
<p><code><a href="#topic+shapiro.test">shapiro.test</a></code> which performs the Shapiro-Wilk test for
normality.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require("graphics")

x &lt;- rnorm(50)
y &lt;- runif(30)
# Do x and y come from the same distribution?
ks.test(x, y)
# Does x come from a shifted gamma distribution with shape 3 and rate 2?
ks.test(x+2, "pgamma", 3, 2) # two-sided, exact
ks.test(x+2, "pgamma", 3, 2, exact = FALSE)
ks.test(x+2, "pgamma", 3, 2, alternative = "gr")

# test if x is stochastically larger than x2
x2 &lt;- rnorm(50, -1)
plot(ecdf(x), xlim = range(c(x, x2)))
plot(ecdf(x2), add = TRUE, lty = "dashed")
t.test(x, x2, alternative = "g")
wilcox.test(x, x2, alternative = "g")
ks.test(x, x2, alternative = "l")

# with ties, example from Schrer and Trenkler (1995)
# D = 3/7, p = 8/33 = 0.242424..
ks.test(c(1, 2, 2, 3, 3),
        c(1, 2, 3, 3, 4, 5, 6))# -&gt; exact

# formula interface, see ?wilcox.test
ks.test(Ozone ~ Month, data = airquality,
        subset = Month %in% c(5, 8))
</code></pre>

<hr>
<h2 id='ksmooth'>Kernel Regression Smoother</h2><span id='topic+ksmooth'></span>

<h3>Description</h3>

<p>The Nadaraya&ndash;Watson kernel regression estimate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ksmooth(x, y, kernel = c("box", "normal"), bandwidth = 0.5,
        range.x = range(x),
        n.points = max(100L, length(x)), x.points)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ksmooth_+3A_x">x</code></td>
<td>
<p>input x values.  <a href="base.html#topic+Long+20vectors">Long vectors</a> are supported.</p>
</td></tr>
<tr><td><code id="ksmooth_+3A_y">y</code></td>
<td>
<p>input y values.  Long vectors are supported.</p>
</td></tr>
<tr><td><code id="ksmooth_+3A_kernel">kernel</code></td>
<td>
<p>the kernel to be used.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="ksmooth_+3A_bandwidth">bandwidth</code></td>
<td>
<p>the bandwidth. The kernels are scaled so that their
quartiles (viewed as probability densities) are at
<code class="reqn">\pm</code> <code>0.25*bandwidth</code>.</p>
</td></tr>
<tr><td><code id="ksmooth_+3A_range.x">range.x</code></td>
<td>
<p>the range of points to be covered in the output.</p>
</td></tr>
<tr><td><code id="ksmooth_+3A_n.points">n.points</code></td>
<td>
<p>the number of points at which to evaluate the fit.</p>
</td></tr>
<tr><td><code id="ksmooth_+3A_x.points">x.points</code></td>
<td>
<p>points at which to evaluate the smoothed fit.  If
missing, <code>n.points</code> are chosen uniformly to cover
<code>range.x</code>.  Long vectors are supported.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>values at which the smoothed fit is evaluated. Guaranteed to
be in increasing order.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>fitted values corresponding to <code>x</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function was implemented for compatibility with S,
although it is nowhere near as slow as the S function.  Better kernel
smoothers are available in other packages such as <a href="https://CRAN.R-project.org/package=KernSmooth"><span class="pkg">KernSmooth</span></a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

with(cars, {
    plot(speed, dist)
    lines(ksmooth(speed, dist, "normal", bandwidth = 2), col = 2)
    lines(ksmooth(speed, dist, "normal", bandwidth = 5), col = 3)
})
</code></pre>

<hr>
<h2 id='lag'>Lag a Time Series</h2><span id='topic+lag'></span><span id='topic+lag.default'></span>

<h3>Description</h3>

<p>Compute a lagged version of a time series, shifting the time base
back by a given number of observations.
</p>
<p><code>lag</code> is a generic function; this page documents its default
method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lag(x, ...)

## Default S3 method:
lag(x, k = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lag_+3A_x">x</code></td>
<td>
<p>A vector or matrix or univariate or multivariate time series</p>
</td></tr>
<tr><td><code id="lag_+3A_k">k</code></td>
<td>
<p>The number of lags (in units of observations).</p>
</td></tr>
<tr><td><code id="lag_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

  
<p>Vector or matrix arguments <code>x</code> are given a <code>tsp</code> attribute
<em>via</em> <code><a href="#topic+hasTsp">hasTsp</a></code>.
</p>


<h3>Value</h3>

<p>A time series object with the same class as <code>x</code>.
</p>


<h3>Note</h3>

<p>Note the sign of <code>k</code>: a series lagged by a positive <code>k</code>
starts <em>earlier</em>.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+diff">diff</a></code>, <code><a href="#topic+deltat">deltat</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>lag(ldeaths, 12) # starts one year earlier
</code></pre>

<hr>
<h2 id='lag.plot'>Time Series Lag Plots</h2><span id='topic+lag.plot'></span>

<h3>Description</h3>

<p>Plot time series against lagged versions of themselves.
Helps visualizing &lsquo;auto-dependence&rsquo; even when auto-correlations
vanish.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lag.plot(x, lags = 1, layout = NULL, set.lags = 1:lags,
         main = NULL, asp = 1,
         diag = TRUE, diag.col = "gray", type = "p", oma = NULL,
         ask = NULL, do.lines = (n &lt;= 150), labels = do.lines,
         ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lag.plot_+3A_x">x</code></td>
<td>
<p>time-series (univariate or multivariate)</p>
</td></tr>
<tr><td><code id="lag.plot_+3A_lags">lags</code></td>
<td>
<p>number of lag plots desired, see argument <code>set.lags</code>.</p>
</td></tr>
<tr><td><code id="lag.plot_+3A_layout">layout</code></td>
<td>
<p>the layout of multiple plots, basically the <code>mfrow</code>
<code><a href="graphics.html#topic+par">par</a>()</code> argument.  The default uses about a square
layout (see <code><a href="grDevices.html#topic+n2mfrow">n2mfrow</a></code>) such that all plots are on one page.</p>
</td></tr>
<tr><td><code id="lag.plot_+3A_set.lags">set.lags</code></td>
<td>
<p>vector of positive integers allowing specification of
the set of lags used; defaults to <code>1:lags</code>.</p>
</td></tr>
<tr><td><code id="lag.plot_+3A_main">main</code></td>
<td>
<p>character with a main header title to be done on the top
of each page.</p>
</td></tr>
<tr><td><code id="lag.plot_+3A_asp">asp</code></td>
<td>
<p>Aspect ratio to be fixed, see <code><a href="graphics.html#topic+plot.default">plot.default</a></code>.</p>
</td></tr>
<tr><td><code id="lag.plot_+3A_diag">diag</code></td>
<td>
<p>logical indicating if the x=y diagonal should be drawn.</p>
</td></tr>
<tr><td><code id="lag.plot_+3A_diag.col">diag.col</code></td>
<td>
<p>color to be used for the diagonal <code>if(diag)</code>.</p>
</td></tr>
<tr><td><code id="lag.plot_+3A_type">type</code></td>
<td>
<p>plot type to be used, but see <code><a href="#topic+plot.ts">plot.ts</a></code> about
its restricted meaning.</p>
</td></tr>
<tr><td><code id="lag.plot_+3A_oma">oma</code></td>
<td>
<p>outer margins, see <code><a href="graphics.html#topic+par">par</a></code>.</p>
</td></tr>
<tr><td><code id="lag.plot_+3A_ask">ask</code></td>
<td>
<p>logical or <code>NULL</code>; if true, the user is asked to
confirm before a new page is started.</p>
</td></tr>
<tr><td><code id="lag.plot_+3A_do.lines">do.lines</code></td>
<td>
<p>logical indicating if lines should be drawn.</p>
</td></tr>
<tr><td><code id="lag.plot_+3A_labels">labels</code></td>
<td>
<p>logical indicating if labels should be used.</p>
</td></tr>
<tr><td><code id="lag.plot_+3A_...">...</code></td>
<td>
<p>Further arguments to <code><a href="#topic+plot.ts">plot.ts</a></code>.  Several
graphical parameters are set in this function and so cannot be
changed: these include <code>xlab</code>, <code>ylab</code>, <code>mgp</code>,
<code>col.lab</code> and <code>font.lab</code>: this also applies to the
arguments <code>xy.labels</code> and <code>xy.lines</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If just one plot is produced, this is a conventional plot.  If more
than one plot is to be produced, <code>par(mfrow)</code> and several other
graphics parameters will be set, so it is not (easily) possible to mix
such lag plots with other plots on the same page.
</p>
<p>If <code>ask = NULL</code>, <code>par(ask = TRUE)</code> will be called if more than
one page of plots is to be produced and the device is interactive.
</p>


<h3>Note</h3>

<p>It is more flexible and has different default behaviour than
the S version.  We use <code>main =</code> instead of <code>head = </code> for
internal consistency.
</p>


<h3>Author(s)</h3>

<p>Martin Maechler</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.ts">plot.ts</a></code> which is the basic work horse.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

lag.plot(nhtemp, 8, diag.col = "forest green")
lag.plot(nhtemp, 5, main = "Average Temperatures in New Haven")
## ask defaults to TRUE when we have more than one page:
lag.plot(nhtemp, 6, layout = c(2,1), asp = NA,
         main = "New Haven Temperatures", col.main = "blue")

## Multivariate (but non-stationary! ...)
lag.plot(freeny.x, lags = 3)

## no lines for long series :
lag.plot(sqrt(sunspots), set.lags = c(1:4, 9:12), pch = ".", col = "gold")
</code></pre>

<hr>
<h2 id='line'>Robust Line Fitting</h2><span id='topic+line'></span><span id='topic+residuals.tukeyline'></span>

<h3>Description</h3>

<p>Fit a line robustly as recommended in <em>Exploratory Data Analysis</em>.
</p>
<p>Currently by default (<code>iter = 1</code>) the initial median-median line is <em>not</em> iterated (as
opposed to Tukey's &ldquo;resistant line&rdquo; in the references).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>line(x, y, iter = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="line_+3A_x">x</code>, <code id="line_+3A_y">y</code></td>
<td>
<p>the arguments can be any way of specifying x-y pairs.  See
<code><a href="grDevices.html#topic+xy.coords">xy.coords</a></code>.</p>
</td></tr>
<tr><td><code id="line_+3A_iter">iter</code></td>
<td>
<p>positive integer specifying the number of
&ldquo;polishing&rdquo; iterations.  Note that this was hard coded to
<code>1</code> in <span class="rlang"><b>R</b></span> versions before 3.5.0, and more importantly that such
simple iterations may not converge, see Siegel's 9-point example.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Cases with missing values are omitted.
</p>
<p>Contrary to the references where the data is split in three (almost)
equally sized groups with symmetric sizes depending on <code class="reqn">n</code> and
<code>n %% 3</code> and computes medians inside each group, the
<code>line()</code> code splits into three groups using all observations
with <code>x[.] &lt;= q1</code> and <code>x[.] &gt;= q2</code>, where <code>q1, q2</code> are
(a kind of) quantiles for probabilities <code class="reqn">p = 1/3</code> and <code class="reqn">p = 2/3</code>
of the form <code>(x[j1]+x[j2])/2</code> where <code>j1 = floor(p*(n-1))</code>
and <code>j2 = ceiling(p(n-1))</code>, <code>n = length(x)</code>.
</p>
<p>Long vectors are not supported yet.
</p>


<h3>Value</h3>

<p>An object of class <code>"tukeyline"</code>.
</p>
<p>Methods are available for the generic functions <code>coef</code>,
<code>residuals</code>, <code>fitted</code>, and <code>print</code>.
</p>


<h3>References</h3>

<p>Tukey, J. W. (1977).
<em>Exploratory Data Analysis</em>,
Reading Massachusetts: Addison-Wesley.
</p>
<p>Velleman, P. F. and Hoaglin, D. C. (1981).
<em>Applications, Basics and Computing of Exploratory Data Analysis</em>,
Duxbury Press.
Chapter 5.

</p>
<p>Emerson, J. D. and Hoaglin, D. C. (1983).
Resistant Lines for <code class="reqn">y</code> versus <code class="reqn">x</code>.
Chapter 5 of <em>Understanding Robust and Exploratory Data Analysis</em>,
eds. David C. Hoaglin, Frederick Mosteller and John W. Tukey.  Wiley.
</p>
<p>Iain M. Johnstone and Paul F. Velleman (1985).
The Resistant Line and Related Regression Methods.
<em>Journal of the American Statistical Association</em>, <b>80</b>,
1041&ndash;1054.
<a href="https://doi.org/10.2307/2288572">doi:10.2307/2288572</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lm">lm</a></code>.
</p>
<p>There are alternatives for robust linear regression more robust and
more (statistically) efficient,
see <code><a href="MASS.html#topic+rlm">rlm</a>()</code> from <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a>, or
<code><a href="robustbase.html#topic+lmrob">lmrob</a>()</code>  
from <a href="https://CRAN.R-project.org/package=robustbase"><span class="pkg">robustbase</span></a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

plot(cars)
(z &lt;- line(cars))
abline(coef(z))
## Tukey-Anscombe Plot :
plot(residuals(z) ~ fitted(z), main = deparse(z$call))

## Andrew Siegel's pathological 9-point data, y-values multiplied by 3:
d.AS &lt;- data.frame(x = c(-4:3, 12), y = 3*c(rep(0,6), -5, 5, 1))
cAS &lt;- with(d.AS, t(sapply(1:10,
                   function(it) line(x,y, iter=it)$coefficients)))
dimnames(cAS) &lt;- list(paste("it =", format(1:10)), c("intercept", "slope"))
cAS
## iterations started to oscillate, repeating iteration 7,8 indefinitely
</code></pre>

<hr>
<h2 id='listof'>A Class for Lists of (Parts of) Model Fits</h2><span id='topic+listof'></span>

<h3>Description</h3>

<p>Class <code>"listof"</code> is used by <code><a href="#topic+aov">aov</a></code> and the <code>"lm"</code>
method of <code><a href="#topic+alias">alias</a></code> for lists of model fits or parts
thereof.  It is simply a list with an assigned class to control the way
methods, especially printing, act on it.
</p>
<p>It has a <code><a href="#topic+coef">coef</a></code> method in this package (which returns an
object of this class), and <code>[</code> and <code>print</code> methods in
package <span class="pkg">base</span>.
</p>

<hr>
<h2 id='lm'>Fitting Linear Models</h2><span id='topic+lm'></span><span id='topic+print.lm'></span>

<h3>Description</h3>

<p><code>lm</code> is used to fit linear models, including multivariate ones.
It can be used to carry out regression,
single stratum analysis of variance and
analysis of covariance (although <code><a href="#topic+aov">aov</a></code> may provide a more
convenient interface for these).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lm(formula, data, subset, weights, na.action,
   method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,
   singular.ok = TRUE, contrasts = NULL, offset, ...)

## S3 method for class 'lm'
print(x, digits = max(3L, getOption("digits") - 3L), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lm_+3A_formula">formula</code></td>
<td>
<p>an object of class <code>"<a href="#topic+formula">formula</a>"</code> (or one that
can be coerced to that class): a symbolic description of the
model to be fitted.  The details of model specification are given
under &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="lm_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment (or object
coercible by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code> to a data frame) containing
the variables in the model.  If not found in <code>data</code>, the
variables are taken from <code>environment(formula)</code>,
typically the environment from which <code>lm</code> is called.</p>
</td></tr>
<tr><td><code id="lm_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used in the fitting process.  (See additional details about how
this argument interacts with data-dependent bases in the
&lsquo;Details&rsquo; section of the <code><a href="#topic+model.frame">model.frame</a></code>
documentation.)</p>
</td></tr>
<tr><td><code id="lm_+3A_weights">weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting
process.  Should be <code>NULL</code> or a numeric vector.
If non-NULL, weighted least squares is used with weights
<code>weights</code> (that is, minimizing <code>sum(w*e^2)</code>); otherwise
ordinary least squares is used.  See also &lsquo;Details&rsquo;,</p>
</td></tr>
<tr><td><code id="lm_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen
when the data contain <code>NA</code>s.  The default is set by
the <code>na.action</code> setting of <code><a href="base.html#topic+options">options</a></code>, and is
<code><a href="#topic+na.fail">na.fail</a></code> if that is unset.  The &lsquo;factory-fresh&rsquo;
default is <code><a href="#topic+na.omit">na.omit</a></code>.  Another possible value is
<code>NULL</code>, no action.  Value <code><a href="#topic+na.exclude">na.exclude</a></code> can be useful.</p>
</td></tr>
<tr><td><code id="lm_+3A_method">method</code></td>
<td>
<p>the method to be used; for fitting, currently only
<code>method = "qr"</code> is supported; <code>method = "model.frame"</code> returns
the model frame (the same as with <code>model = TRUE</code>, see below).</p>
</td></tr>
<tr><td><code id="lm_+3A_model">model</code>, <code id="lm_+3A_x">x</code>, <code id="lm_+3A_y">y</code>, <code id="lm_+3A_qr">qr</code></td>
<td>
<p>logicals.  If <code>TRUE</code> the corresponding
components of the fit (the model frame, the model matrix, the
response, the QR decomposition) are returned.
</p>
</td></tr>
<tr><td><code id="lm_+3A_singular.ok">singular.ok</code></td>
<td>
<p>logical. If <code>FALSE</code> (the default in S but
not in <span class="rlang"><b>R</b></span>) a singular fit is an error.</p>
</td></tr>
<tr><td><code id="lm_+3A_contrasts">contrasts</code></td>
<td>
<p>an optional list. See the <code>contrasts.arg</code>
of <code><a href="#topic+model.matrix.default">model.matrix.default</a></code>.</p>
</td></tr>
<tr><td><code id="lm_+3A_offset">offset</code></td>
<td>
<p>this can be used to specify an <em>a priori</em> known
component to be included in the linear predictor during fitting.
This should be <code>NULL</code> or a numeric vector or matrix of extents
matching those of the response.  One or more <code><a href="#topic+offset">offset</a></code> terms can be
included in the formula instead or as well, and if more than one are
specified their sum is used.  See <code><a href="#topic+model.offset">model.offset</a></code>.</p>
</td></tr>
<tr><td><code id="lm_+3A_...">...</code></td>
<td>
<p>For <code>lm()</code>: additional arguments to be passed to the low level
regression fitting functions (see below).</p>
</td></tr>

<tr><td><code id="lm_+3A_digits">digits</code></td>
<td>
<p>the number of <em>significant</em> digits to be
passed to <code><a href="base.html#topic+format">format</a>(<a href="#topic+coef">coef</a>(x), .)</code> when
<code><a href="base.html#topic+print">print</a>()</code>ing.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Models for <code>lm</code> are specified symbolically.  A typical model has
the form <code>response ~ terms</code> where <code>response</code> is the (numeric)
response vector and <code>terms</code> is a series of terms which specifies a
linear predictor for <code>response</code>.  A terms specification of the form
<code>first + second</code> indicates all the terms in <code>first</code> together
with all the terms in <code>second</code> with duplicates removed.  A
specification of the form <code>first:second</code> indicates the set of
terms obtained by taking the interactions of all terms in <code>first</code>
with all terms in <code>second</code>.  The specification <code>first*second</code>
indicates the <em>cross</em> of <code>first</code> and <code>second</code>.  This is
the same as <code>first + second + first:second</code>.
</p>
<p>If the formula includes an <code><a href="#topic+offset">offset</a></code>, this is evaluated and
subtracted from the response.
</p>
<p>If <code>response</code> is a matrix a linear model is fitted separately by
least-squares to each column of the matrix and the result inherits from
<code>"mlm"</code> (&ldquo;multivariate linear model&rdquo;).
</p>
<p>See <code><a href="#topic+model.matrix">model.matrix</a></code> for some further details.  The terms in
the formula will be re-ordered so that main effects come first,
followed by the interactions, all second-order, all third-order and so
on: to avoid this pass a <code>terms</code> object as the formula (see
<code><a href="#topic+aov">aov</a></code> and <code>demo(glm.vr)</code> for an example).
</p>
<p>A formula has an implied intercept term.  To remove this use either
<code>y ~ x - 1</code> or <code>y ~ 0 + x</code>.  See <code><a href="#topic+formula">formula</a></code> for
more details of allowed formulae.
</p>
<p>Non-<code>NULL</code> <code>weights</code> can be used to indicate that
different observations have different variances (with the values in
<code>weights</code> being inversely proportional to the variances); or
equivalently, when the elements of <code>weights</code> are positive
integers <code class="reqn">w_i</code>, that each response <code class="reqn">y_i</code> is the mean of
<code class="reqn">w_i</code> unit-weight observations (including the case that there
are <code class="reqn">w_i</code> observations equal to <code class="reqn">y_i</code> and the data have been
summarized). However, in the latter case, notice that within-group
variation is not used.  Therefore, the sigma estimate and residual
degrees of freedom may be suboptimal; in the case of replication
weights, even wrong. Hence, standard errors and analysis of variance
tables should be treated with care.
</p>
<p><code>lm</code> calls the lower level functions <code><a href="#topic+lm.fit">lm.fit</a></code>, etc,
see below, for the actual numerical computations.  For programming
only, you may consider doing likewise.
</p>
<p>All of <code>weights</code>, <code>subset</code> and <code>offset</code> are evaluated
in the same way as variables in <code>formula</code>, that is first in
<code>data</code> and then in the environment of <code>formula</code>.
</p>


<h3>Value</h3>

<p><code>lm</code> returns an object of <code><a href="base.html#topic+class">class</a></code> <code>"lm"</code> or for
multivariate (&lsquo;multiple&rsquo;) responses of class <code>c("mlm", "lm")</code>.
</p>
<p>The functions <code>summary</code> and <code><a href="#topic+anova">anova</a></code> are used to
obtain and print a summary and analysis of variance table of the
results.  The generic accessor functions <code>coefficients</code>,
<code>effects</code>, <code>fitted.values</code> and <code>residuals</code> extract
various useful features of the value returned by <code>lm</code>.
</p>
<p>An object of class <code>"lm"</code> is a list containing at least the
following components:
</p>
<table>
<tr><td><code>coefficients</code></td>
<td>
<p>a named vector of coefficients</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>the residuals, that is response minus fitted values.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>the fitted mean values.</p>
</td></tr>
<tr><td><code>rank</code></td>
<td>
<p>the numeric rank of the fitted linear model.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>(only for weighted fits) the specified weights.</p>
</td></tr>
<tr><td><code>df.residual</code></td>
<td>
<p>the residual degrees of freedom.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>terms</code></td>
<td>
<p>the <code><a href="#topic+terms">terms</a></code> object used.</p>
</td></tr>
<tr><td><code>contrasts</code></td>
<td>
<p>(only where relevant) the contrasts used.</p>
</td></tr>
<tr><td><code>xlevels</code></td>
<td>
<p>(only where relevant) a record of the levels of the
factors used in fitting.</p>
</td></tr>
<tr><td><code>offset</code></td>
<td>
<p>the offset used (missing if none were used).</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>if requested, the response used.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>if requested, the model matrix used.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>if requested (the default), the model frame used.</p>
</td></tr>
<tr><td><code>na.action</code></td>
<td>
<p>(where relevant) information returned by
<code><a href="#topic+model.frame">model.frame</a></code> on the special handling of <code>NA</code>s.</p>
</td></tr>
</table>
<p>In addition, non-null fits will have components <code>assign</code>,
<code>effects</code> and (unless not requested) <code>qr</code> relating to the linear
fit, for use by extractor functions such as <code>summary</code> and
<code><a href="#topic+effects">effects</a></code>.
</p>


<h3>Using time series</h3>

<p>Considerable care is needed when using <code>lm</code> with time series.
</p>
<p>Unless <code>na.action = NULL</code>, the time series attributes are
stripped from the variables before the regression is done.  (This is
necessary as omitting <code>NA</code>s would invalidate the time series
attributes, and if <code>NA</code>s are omitted in the middle of the series
the result would no longer be a regular time series.)
</p>
<p>Even if the time series attributes are retained, they are not used to
line up series, so that the time shift of a lagged or differenced
regressor would be ignored.  It is good practice to prepare a
<code>data</code> argument by <code><a href="#topic+ts.intersect">ts.intersect</a>(..., dframe = TRUE)</code>,
then apply a suitable <code>na.action</code> to that data frame and call
<code>lm</code> with <code>na.action = NULL</code> so that residuals and fitted
values are time series.
</p>


<h3>Author(s)</h3>

<p>The design was inspired by the S function of the same name described
in Chambers (1992).  The implementation of model formula by Ross Ihaka
was based on Wilkinson &amp; Rogers (1973).
</p>


<h3>References</h3>

<p>Chambers, J. M. (1992)
<em>Linear models.</em>
Chapter 4 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>
<p>Wilkinson, G. N. and Rogers, C. E. (1973).
Symbolic descriptions of factorial models for analysis of variance.
<em>Applied Statistics</em>, <b>22</b>, 392&ndash;399.
<a href="https://doi.org/10.2307/2346786">doi:10.2307/2346786</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.lm">summary.lm</a></code> for more detailed summaries and <code><a href="#topic+anova.lm">anova.lm</a></code> for
the ANOVA table; <code><a href="#topic+aov">aov</a></code> for a different interface.
</p>
<p>The generic functions <code><a href="#topic+coef">coef</a></code>, <code><a href="#topic+effects">effects</a></code>,
<code><a href="#topic+residuals">residuals</a></code>, <code><a href="#topic+fitted">fitted</a></code>, <code><a href="#topic+vcov">vcov</a></code>.
</p>
<p><code><a href="#topic+predict.lm">predict.lm</a></code> (via <code><a href="#topic+predict">predict</a></code>) for prediction,
including confidence and prediction intervals;
<code><a href="#topic+confint">confint</a></code> for confidence intervals of <em>parameters</em>.
</p>
<p><code><a href="#topic+lm.influence">lm.influence</a></code> for regression diagnostics, and
<code><a href="#topic+glm">glm</a></code> for <b>generalized</b> linear models.
</p>
<p>The underlying low level functions,
<code><a href="#topic+lm.fit">lm.fit</a></code> for plain, and <code><a href="#topic+lm.wfit">lm.wfit</a></code> for weighted
regression fitting.
</p>
<p>More <code>lm()</code> examples are available e.g., in
<code><a href="datasets.html#topic+anscombe">anscombe</a></code>, <code><a href="datasets.html#topic+attitude">attitude</a></code>, <code><a href="datasets.html#topic+freeny">freeny</a></code>,
<code><a href="datasets.html#topic+LifeCycleSavings">LifeCycleSavings</a></code>, <code><a href="datasets.html#topic+longley">longley</a></code>,
<code><a href="datasets.html#topic+stackloss">stackloss</a></code>, <code><a href="datasets.html#topic+swiss">swiss</a></code>.
</p>
<p><code>biglm</code> in package <a href="https://CRAN.R-project.org/package=biglm"><span class="pkg">biglm</span></a> for an alternative
way to fit linear models to large datasets (especially those with many
cases).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

## Annette Dobson (1990) "An Introduction to Generalized Linear Models".
## Page 9: Plant Weight Data.
ctl &lt;- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt &lt;- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group &lt;- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight &lt;- c(ctl, trt)
lm.D9 &lt;- lm(weight ~ group)
lm.D90 &lt;- lm(weight ~ group - 1) # omitting intercept

anova(lm.D9)
summary(lm.D90)

opar &lt;- par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))
plot(lm.D9, las = 1)      # Residuals, Fitted, ...
par(opar)

### less simple examples in "See Also" above
</code></pre>

<hr>
<h2 id='lm.fit'>Fitter Functions for Linear Models</h2><span id='topic+lm.fit'></span><span id='topic+lm.wfit'></span><span id='topic+.lm.fit'></span>

<h3>Description</h3>

<p>These are the basic computing engines called by <code><a href="#topic+lm">lm</a></code> used
to fit linear models.  These should usually <em>not</em> be used
directly unless by experienced users.  <code>.lm.fit()</code> is a bare-bones
wrapper to the innermost QR-based C code, on which
<code><a href="#topic+glm.fit">glm.fit</a></code> and <code><a href="#topic+lsfit">lsfit</a></code> are also based, for
even more experienced users.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lm.fit (x, y,    offset = NULL, method = "qr", tol = 1e-7,
       singular.ok = TRUE, ...)

lm.wfit(x, y, w, offset = NULL, method = "qr", tol = 1e-7,
        singular.ok = TRUE, ...)

.lm.fit(x, y, tol = 1e-7)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lm.fit_+3A_x">x</code></td>
<td>
<p>design matrix of dimension <code>n * p</code>.</p>
</td></tr>
<tr><td><code id="lm.fit_+3A_y">y</code></td>
<td>
<p>vector of observations of length <code>n</code>, or a matrix with
<code>n</code> rows.</p>
</td></tr>
<tr><td><code id="lm.fit_+3A_w">w</code></td>
<td>
<p>vector of weights (length <code>n</code>) to be used in the fitting
process for the <code>wfit</code> functions.  Weighted least squares is
used with weights <code>w</code>, i.e., <code>sum(w * e^2)</code> is minimized.</p>
</td></tr>
<tr><td><code id="lm.fit_+3A_offset">offset</code></td>
<td>
<p>(numeric of length <code>n</code>).  This can be used to
specify an <em>a priori</em> known component to be included in the
linear predictor during fitting.</p>
</td></tr>
<tr><td><code id="lm.fit_+3A_method">method</code></td>
<td>
<p>currently, only <code>method = "qr"</code> is supported.</p>
</td></tr>
<tr><td><code id="lm.fit_+3A_tol">tol</code></td>
<td>
<p>tolerance for the <code><a href="Matrix.html#topic+qr">qr</a></code> decomposition.  Default
is 1e-7.</p>
</td></tr>
<tr><td><code id="lm.fit_+3A_singular.ok">singular.ok</code></td>
<td>
<p>logical. If <code>FALSE</code>, a singular model is an
error.</p>
</td></tr>
<tr><td><code id="lm.fit_+3A_...">...</code></td>
<td>
<p>currently disregarded.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>y</code> is a matrix, <code>offset</code> can be a numeric matrix of the
same dimensions, in which case each column is applied to the
corresponding column of <code>y</code>.
</p>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+list">list</a></code> with components (for <code>lm.fit</code> and <code>lm.wfit</code>)
</p>
<table>
<tr><td><code>coefficients</code></td>
<td>
<p><code>p</code> vector</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p><code>n</code> vector or matrix</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p><code>n</code> vector or matrix</p>
</td></tr>
<tr><td><code>effects</code></td>
<td>
<p><code>n</code> vector of orthogonal single-<abbr>df</abbr>
effects.  The first <code>rank</code> of them correspond to non-aliased
coefficients, and are named accordingly.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p><code>n</code> vector &mdash; <em>only</em> for the <code>*wfit*</code>
functions.</p>
</td></tr>
<tr><td><code>rank</code></td>
<td>
<p>integer, giving the rank</p>
</td></tr>
<tr><td><code>df.residual</code></td>
<td>
<p>degrees of freedom of residuals</p>
</td></tr>
<tr><td><code>qr</code></td>
<td>
<p>the QR decomposition, see <code><a href="Matrix.html#topic+qr">qr</a></code>.</p>
</td></tr>
</table>
<p>Fits without any columns or non-zero weights do not have the
<code>effects</code> and <code>qr</code> components.
</p>
<p><code>.lm.fit()</code> returns a subset of the above, the <code>qr</code> part
unwrapped, plus a logical component <code>pivoted</code> indicating if the
underlying QR algorithm did pivot.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lm">lm</a></code> which you should use for linear least squares regression,
unless you know better.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(utils)

set.seed(129)

n &lt;- 7 ; p &lt;- 2
X &lt;- matrix(rnorm(n * p), n, p) # no intercept!
y &lt;- rnorm(n)
w &lt;- rnorm(n)^2

str(lmw &lt;- lm.wfit(x = X, y = y, w = w))

str(lm. &lt;- lm.fit (x = X, y = y))

## fits w/o intercept:
all.equal(unname(coef(lm(y ~ X-1))),
          unname(coef( lm.fit(X,y))))
all.equal(unname(coef( lm.fit(X,y))),
                 coef(.lm.fit(X,y)))

if(require("microbenchmark")) {
  mb &lt;- microbenchmark(lm(y~X-1), lm.fit(X,y), .lm.fit(X,y))
  print(mb)
  boxplot(mb, notch=TRUE)
}


</code></pre>

<hr>
<h2 id='lm.influence'>Regression Diagnostics</h2><span id='topic+lm.influence'></span><span id='topic+influence'></span><span id='topic+influence.lm'></span><span id='topic+influence.glm'></span>

<h3>Description</h3>

<p>This function provides the basic quantities which are
used in forming a wide variety of diagnostics for
checking the quality of regression fits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>influence(model, ...)
## S3 method for class 'lm'
influence(model, do.coef = TRUE, ...)
## S3 method for class 'glm'
influence(model, do.coef = TRUE, ...)

lm.influence(model, do.coef = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lm.influence_+3A_model">model</code></td>
<td>
<p>an object as returned by <code><a href="#topic+lm">lm</a></code> or <code><a href="#topic+glm">glm</a></code>.</p>
</td></tr>
<tr><td><code id="lm.influence_+3A_do.coef">do.coef</code></td>
<td>
<p>logical indicating if the changed <code>coefficients</code>
(see below) are desired.  These need <code class="reqn">O(n^2 p)</code> computing time.</p>
</td></tr>
<tr><td><code id="lm.influence_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="#topic+influence.measures">influence.measures</a>()</code> and other functions listed in
<b>See Also</b> provide a more user oriented way of computing a
variety of regression diagnostics.  These all build on
<code>lm.influence</code>.  Note that for GLMs (other than the Gaussian
family with identity link) these are based on one-step approximations
which may be inadequate if a case has high influence.
</p>
<p>An attempt is made to ensure that computed hat values that are
probably one are treated as one, and the corresponding rows in
<code>sigma</code> and <code>coefficients</code> are <code>NaN</code>.  (Dropping such a
case would normally result in a variable being dropped, so it is not
possible to give simple drop-one diagnostics.)
</p>
<p><code><a href="#topic+naresid">naresid</a></code> is applied to the results and so will fill in
with <code>NA</code>s it the fit had <code>na.action = na.exclude</code>.
</p>


<h3>Value</h3>

<p>A list containing the following components of the same length or
number of rows <code class="reqn">n</code>, which is the number of non-zero weights.
Cases omitted in the fit are omitted unless a <code><a href="#topic+na.action">na.action</a></code>
method was used (such as <code><a href="#topic+na.exclude">na.exclude</a></code>) which restores them.
</p>
<table>
<tr><td><code>hat</code></td>
<td>
<p>a vector containing the diagonal of the &lsquo;hat&rsquo; matrix.</p>
</td></tr>
<tr><td><code>coefficients</code></td>
<td>
<p>(unless <code>do.coef</code> is false) a matrix whose
i-th row contains the change in the estimated coefficients which
results when the i-th case is dropped from the regression.  Note
that aliased coefficients are not included in the matrix.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>a vector whose i-th element contains the estimate
of the residual standard deviation obtained when the i-th
case is dropped from the regression.  (The approximations needed for
GLMs can result in this being <code>NaN</code>.)</p>
</td></tr>
<tr><td><code>wt.res</code></td>
<td>
<p>a vector of <em>weighted</em> (or for class <code>glm</code>
rather <em>deviance</em>) residuals.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The <code>coefficients</code> returned by the <span class="rlang"><b>R</b></span> version
of <code>lm.influence</code> differ from those computed by S.
Rather than returning the coefficients which result
from dropping each case, we return the changes in the coefficients.
This is more directly useful in many diagnostic measures.<br />
Since these need <code class="reqn">O(n p^2)</code> computing time, they can be omitted by
<code>do.coef = FALSE</code>.
</p>
<p>Note that cases with <code>weights == 0</code> are <em>dropped</em> (contrary
to the situation in S).
</p>
<p>If a model has been fitted with <code>na.action = na.exclude</code> (see
<code><a href="#topic+na.exclude">na.exclude</a></code>), cases excluded in the fit <em>are</em>
considered here.
</p>


<h3>References</h3>

<p>See the list in the documentation for <code><a href="#topic+influence.measures">influence.measures</a></code>.
</p>
<p>Chambers, J. M. (1992)
<em>Linear models.</em>
Chapter 4 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.lm">summary.lm</a></code> for <code><a href="base.html#topic+summary">summary</a></code> and related methods;<br />
<code><a href="#topic+influence.measures">influence.measures</a></code>,<br />
<code><a href="#topic+hat">hat</a></code> for the hat matrix diagonals,<br />
<code><a href="#topic+dfbetas">dfbetas</a></code>,
<code><a href="#topic+dffits">dffits</a></code>,
<code><a href="#topic+covratio">covratio</a></code>,
<code><a href="#topic+cooks.distance">cooks.distance</a></code>,
<code><a href="#topic+lm">lm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Analysis of the life-cycle savings data
## given in Belsley, Kuh and Welsch.
summary(lm.SR &lt;- lm(sr ~ pop15 + pop75 + dpi + ddpi,
                    data = LifeCycleSavings),
        correlation = TRUE)
utils::str(lmI &lt;- lm.influence(lm.SR))

## For more "user level" examples, use example(influence.measures)
</code></pre>

<hr>
<h2 id='lm.summaries'>Accessing Linear Model Fits</h2><span id='topic+family.lm'></span><span id='topic+formula.lm'></span><span id='topic+residuals.lm'></span><span id='topic+labels.lm'></span>

<h3>Description</h3>

<p>All these functions are <code><a href="utils.html#topic+methods">methods</a></code> for class <code>"lm"</code>  objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lm'
family(object, ...)

## S3 method for class 'lm'
formula(x, ...)

## S3 method for class 'lm'
residuals(object,
          type = c("working", "response", "deviance", "pearson",
                   "partial"),
          ...)

## S3 method for class 'lm'
labels(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lm.summaries_+3A_object">object</code>, <code id="lm.summaries_+3A_x">x</code></td>
<td>
<p>an object inheriting from class <code>lm</code>, usually
the result of a call to <code><a href="#topic+lm">lm</a></code> or <code><a href="#topic+aov">aov</a></code>.</p>
</td></tr>
<tr><td><code id="lm.summaries_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="lm.summaries_+3A_type">type</code></td>
<td>
<p>the type of residuals which should be returned.  Can be abbreviated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The generic accessor functions <code>coef</code>, <code>effects</code>,
<code>fitted</code> and <code>residuals</code> can be used to extract
various useful features of the value returned by <code>lm</code>.
</p>
<p>The working and response residuals are &lsquo;observed - fitted&rsquo;.  The
deviance and Pearson residuals are weighted residuals, scaled by the
square root of the weights used in fitting.  The partial residuals
are a matrix with each column formed by omitting a term from the
model.  In all these, zero weight cases are never omitted (as opposed
to the standardized <code><a href="#topic+rstudent">rstudent</a></code> residuals, and the
<code><a href="#topic+weighted.residuals">weighted.residuals</a></code>).
</p>
<p>How <code>residuals</code> treats cases with missing values in the original
fit is determined by the <code>na.action</code> argument of that fit.
If <code>na.action = na.omit</code> omitted cases will not appear in the
residuals, whereas if <code>na.action = na.exclude</code> they will appear,
with residual value <code>NA</code>.  See also <code><a href="#topic+naresid">naresid</a></code>.
</p>
<p>The <code>"lm"</code> method for generic <code><a href="base.html#topic+labels">labels</a></code> returns the
term labels for estimable terms, that is the names of the terms with
an least one estimable coefficient.
</p>


<h3>References</h3>

<p>Chambers, J. M. (1992)
<em>Linear models.</em>
Chapter 4 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p>The model fitting function <code><a href="#topic+lm">lm</a></code>, <code><a href="#topic+anova.lm">anova.lm</a></code>.
</p>
<p><code><a href="#topic+coef">coef</a></code>, <code><a href="#topic+deviance">deviance</a></code>,
<code><a href="#topic+df.residual">df.residual</a></code>,
<code><a href="#topic+effects">effects</a></code>, <code><a href="#topic+fitted">fitted</a></code>,
<code><a href="#topic+glm">glm</a></code> for <b>generalized</b> linear models,
<code><a href="#topic+influence">influence</a></code> (etc on that page) for regression diagnostics,
<code><a href="#topic+weighted.residuals">weighted.residuals</a></code>,
<code><a href="#topic+residuals">residuals</a></code>, <code><a href="#topic+residuals.glm">residuals.glm</a></code>,
<code><a href="#topic+summary.lm">summary.lm</a></code>, <code><a href="#topic+weights">weights</a></code>.
</p>
<p><a href="#topic+influence.measures">influence.measures</a> for deletion diagnostics, including
standardized (<code><a href="#topic+rstandard">rstandard</a></code>)
and studentized (<code><a href="#topic+rstudent">rstudent</a></code>) residuals.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##-- Continuing the  lm(.) example:
coef(lm.D90) # the bare coefficients

## The 2 basic regression diagnostic plots [plot.lm(.) is preferred]
plot(resid(lm.D90), fitted(lm.D90)) # Tukey-Anscombe's
abline(h = 0, lty = 2, col = "gray")

qqnorm(residuals(lm.D90))
</code></pre>

<hr>
<h2 id='loadings'>Print Loadings in Factor Analysis</h2><span id='topic+loadings'></span><span id='topic+print.loadings'></span><span id='topic+print.factanal'></span>

<h3>Description</h3>

<p>Extract or print loadings in factor analysis (or principal
components analysis).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loadings(x, ...)

## S3 method for class 'loadings'
print(x, digits = 3, cutoff = 0.1, sort = FALSE, ...)

## S3 method for class 'factanal'
print(x, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loadings_+3A_x">x</code></td>
<td>
<p>an object of class <code>"<a href="#topic+factanal">factanal</a>"</code> or
<code>"<a href="#topic+princomp">princomp</a>"</code> or the <code>loadings</code> component of such an
object.</p>
</td></tr>
<tr><td><code id="loadings_+3A_digits">digits</code></td>
<td>
<p>number of decimal places to use in printing uniquenesses
and loadings.</p>
</td></tr>
<tr><td><code id="loadings_+3A_cutoff">cutoff</code></td>
<td>
<p>loadings smaller than this (in absolute value) are suppressed.</p>
</td></tr>
<tr><td><code id="loadings_+3A_sort">sort</code></td>
<td>
<p>logical. If true, the variables are sorted by their
importance on each factor.  Each variable with any loading larger
than 0.5 (in modulus) is assigned to the factor with the largest
loading, and the variables are printed in the order of the factor
they are assigned to, then those unassigned.</p>
</td></tr>
<tr><td><code id="loadings_+3A_...">...</code></td>
<td>
<p>further arguments for other methods,
ignored for <code>loadings</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>&lsquo;Loadings&rsquo; is a term from <em>factor analysis</em>, but because
factor analysis and principal component analysis (PCA) are often
conflated in the social science literature, it was used for PCA by
SPSS and hence by <code><a href="#topic+princomp">princomp</a></code> in S-PLUS to help SPSS users.
</p>
<p>Small loadings are conventionally not printed (replaced by spaces), to
draw the eye to the pattern of the larger loadings.
</p>
<p>The <code>print</code> method for class <code>"<a href="#topic+factanal">factanal</a>"</code> calls the
<code>"loadings"</code> method to print the loadings, and so passes down
arguments such as <code>cutoff</code> and <code>sort</code>.
</p>
<p>The signs of the loadings vectors are arbitrary for both factor
analysis and PCA.
</p>


<h3>Note</h3>

<p>There are other functions called <code>loadings</code> in contributed
packages which are S3 or S4 generic: the <code>...</code> argument is to
make it easier for this one to become a default method.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+factanal">factanal</a></code>, <code><a href="#topic+princomp">princomp</a></code>
</p>

<hr>
<h2 id='loess'>Local Polynomial Regression Fitting</h2><span id='topic+loess'></span>

<h3>Description</h3>

<p>Fit a locally polynomial surface determined by one or more numerical
predictors, using local fitting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loess(formula, data, weights, subset, na.action, model = FALSE,
      span = 0.75, enp.target, degree = 2,
      parametric = FALSE, drop.square = FALSE, normalize = TRUE,
      family = c("gaussian", "symmetric"),
      method = c("loess", "model.frame"),
      control = loess.control(...), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loess_+3A_formula">formula</code></td>
<td>
<p>a <a href="#topic+formula">formula</a> specifying the numeric response and
one to four numeric predictors (best specified via an interaction,
but can also be specified additively).  Will be coerced to a formula
if necessary.</p>
</td></tr>
<tr><td><code id="loess_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment (or object
coercible by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code> to a data frame) containing
the variables in the model.  If not found in <code>data</code>, the
variables are taken from <code>environment(formula)</code>,
typically the environment from which <code>loess</code> is called.</p>
</td></tr>
<tr><td><code id="loess_+3A_weights">weights</code></td>
<td>
<p>optional weights for each case.</p>
</td></tr>
<tr><td><code id="loess_+3A_subset">subset</code></td>
<td>
<p>an optional specification of a subset of the data to be
used.</p>
</td></tr>
<tr><td><code id="loess_+3A_na.action">na.action</code></td>
<td>
<p>the action to be taken with missing values in the
response or predictors.  The default is given by
<code>getOption("na.action")</code>.</p>
</td></tr>
<tr><td><code id="loess_+3A_model">model</code></td>
<td>
<p>should the model frame be returned?</p>
</td></tr>
<tr><td><code id="loess_+3A_span">span</code></td>
<td>
<p>the parameter <code class="reqn">\alpha</code> which controls the degree of
smoothing.</p>
</td></tr>
<tr><td><code id="loess_+3A_enp.target">enp.target</code></td>
<td>
<p>an alternative way to specify <code>span</code>, as the
approximate equivalent number of parameters to be used.</p>
</td></tr>
<tr><td><code id="loess_+3A_degree">degree</code></td>
<td>
<p>the degree of the polynomials to be used, normally 1 or
2. (Degree 0 is also allowed, but see the &lsquo;Note&rsquo;.)</p>
</td></tr>
<tr><td><code id="loess_+3A_parametric">parametric</code></td>
<td>
<p>should any terms be fitted globally rather than
locally?  Terms can be specified by name, number or as a logical
vector of the same length as the number of predictors.</p>
</td></tr>
<tr><td><code id="loess_+3A_drop.square">drop.square</code></td>
<td>
<p>for fits with more than one predictor and
<code>degree = 2</code>, should the quadratic term be dropped for particular
predictors?  Terms are specified in the same way as for
<code>parametric</code>.</p>
</td></tr>
<tr><td><code id="loess_+3A_normalize">normalize</code></td>
<td>
<p>should the predictors be normalized to a common scale
if there is more than one?  The normalization used is to set the
10% trimmed standard deviation to one.  Set to false for spatial
coordinate predictors and others known to be on a common scale.</p>
</td></tr>
<tr><td><code id="loess_+3A_family">family</code></td>
<td>
<p>if <code>"gaussian"</code> fitting is by least-squares, and if
<code>"symmetric"</code> a re-descending M estimator is used with Tukey's
biweight function.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="loess_+3A_method">method</code></td>
<td>
<p>fit the model or just extract the model frame.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="loess_+3A_control">control</code></td>
<td>
<p>control parameters: see <code><a href="#topic+loess.control">loess.control</a></code>.</p>
</td></tr>
<tr><td><code id="loess_+3A_...">...</code></td>
<td>
<p>control parameters can also be supplied directly
(<em>if</em> <code>control</code> is not specified).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fitting is done locally.  That is, for the fit at point <code class="reqn">x</code>, the
fit is made using points in a neighbourhood of <code class="reqn">x</code>, weighted by
their distance from <code class="reqn">x</code> (with differences in &lsquo;parametric&rsquo;
variables being ignored when computing the distance).  The size of the
neighbourhood is controlled by <code class="reqn">\alpha</code> (set by <code>span</code> or
<code>enp.target</code>).  For <code class="reqn">\alpha &lt; 1</code>, the
neighbourhood includes proportion <code class="reqn">\alpha</code> of the points,
and these have tricubic weighting (proportional to <code class="reqn">(1 -
    \mathrm{(dist/maxdist)}^3)^3</code>).  For
<code class="reqn">\alpha &gt; 1</code>, all points are used, with the
&lsquo;maximum distance&rsquo; assumed to be <code class="reqn">\alpha^{1/p}</code>
times the actual maximum distance for <code class="reqn">p</code> explanatory variables.
</p>
<p>For the default family, fitting is by (weighted) least squares.  For
<code>family="symmetric"</code> a few iterations of an M-estimation
procedure with Tukey's biweight are used.  Be aware that as the initial
value is the least-squares fit, this need not be a very resistant fit.
</p>
<p>It can be important to tune the control list to achieve acceptable
speed.  See <code><a href="#topic+loess.control">loess.control</a></code> for details.
</p>


<h3>Value</h3>

<p>An object of class <code>"loess"</code>, 
with <code>print()</code>, <code><a href="base.html#topic+summary">summary</a>()</code>, <code><a href="#topic+predict">predict</a></code> and
<code><a href="#topic+anova">anova</a></code> methods.
</p>


<h3>Note</h3>

<p>As this is based on <code>cloess</code>, it is similar to but not identical to
the <code>loess</code> function of S.  In particular, conditioning is not
implemented.
</p>
<p>The memory usage of this implementation of <code>loess</code> is roughly
quadratic in the number of points, with 1000 points taking about 10Mb.
</p>
<p><code>degree = 0</code>, local constant fitting, is allowed in this
implementation but not documented in the reference.  It seems very little
tested, so use with caution.
</p>


<h3>Author(s)</h3>

<p>B. D. Ripley, based on the <code>cloess</code> package of Cleveland,
Grosse and Shyu.
</p>


<h3>Source</h3>

<p>The 1998 version of <code>cloess</code> package of Cleveland,
Grosse and Shyu.  A later version is available as <code>dloess</code> at
<a href="https://netlib.org/a/">https://netlib.org/a/</a>.
</p>


<h3>References</h3>

<p>W. S. Cleveland, E. Grosse and W. M. Shyu (1992) Local regression
models. Chapter 8 of <em>Statistical Models in S</em> eds J.M. Chambers
and T.J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+loess.control">loess.control</a></code>,
<code><a href="#topic+predict.loess">predict.loess</a></code>.
</p>
<p><code><a href="#topic+lowess">lowess</a></code>, the ancestor of <code>loess</code> (with
different defaults!).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cars.lo &lt;- loess(dist ~ speed, cars)
predict(cars.lo, data.frame(speed = seq(5, 30, 1)), se = TRUE)
# to allow extrapolation
cars.lo2 &lt;- loess(dist ~ speed, cars,
                  control = loess.control(surface = "direct"))
predict(cars.lo2, data.frame(speed = seq(5, 30, 1)), se = TRUE)
</code></pre>

<hr>
<h2 id='loess.control'>Set Parameters for <code>loess</code></h2><span id='topic+loess.control'></span>

<h3>Description</h3>

<p>Set control parameters for <code>loess</code> fits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loess.control(surface = c("interpolate", "direct"),
              statistics = c("approximate", "exact", "none"),
              trace.hat = c("exact", "approximate"),
              cell = 0.2, iterations = 4, iterTrace = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loess.control_+3A_surface">surface</code></td>
<td>
<p>should the fitted surface be computed exactly
(<code>"direct"</code>) or via interpolation from a k-d tree?  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="loess.control_+3A_statistics">statistics</code></td>
<td>
<p>should the statistics be computed exactly,
approximately or not at all?  Exact computation can be very slow.
Can be abbreviated.</p>
</td></tr>
<tr><td><code id="loess.control_+3A_trace.hat">trace.hat</code></td>
<td>
<p>Only for the (default) case <code>(surface =
     "interpolate", statistics = "approximate")</code>: should the trace of
the smoother matrix be computed exactly or approximately?  It is recommended to use the approximation
for more than about 1000 data points.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="loess.control_+3A_cell">cell</code></td>
<td>
<p>if interpolation is used this controls the accuracy of the
approximation via the maximum number of points in a  cell in the k-d
tree. Cells with more than <code>floor(n*span*cell)</code> points are subdivided.</p>
</td></tr>
<tr><td><code id="loess.control_+3A_iterations">iterations</code></td>
<td>
<p>the number of iterations used in robust fitting,
i.e. only if <code>family</code> is <code>"symmetric"</code>.</p>
</td></tr>
<tr><td><code id="loess.control_+3A_itertrace">iterTrace</code></td>
<td>
<p>logical (or integer) determining if tracing
information during the robust iterations (<code>iterations</code><code class="reqn">\ge
     2</code>) is produced.</p>
</td></tr>
<tr><td><code id="loess.control_+3A_...">...</code></td>
<td>
<p>further arguments which are ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>surface</code></td>
<td>
</td></tr>
<tr><td><code>statistics</code></td>
<td>
</td></tr>
<tr><td><code>trace.hat</code></td>
<td>
</td></tr>
<tr><td><code>cell</code></td>
<td>
</td></tr>
<tr><td><code>iterations</code></td>
<td>
</td></tr>
<tr><td><code>iterTrace</code></td>
<td>
</td></tr>
</table>
<p>with meanings as explained under &lsquo;Arguments&rsquo;.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+loess">loess</a></code></p>

<hr>
<h2 id='Logistic'>The Logistic Distribution</h2><span id='topic+Logistic'></span><span id='topic+dlogis'></span><span id='topic+plogis'></span><span id='topic+qlogis'></span><span id='topic+rlogis'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the logistic distribution with parameters
<code>location</code> and <code>scale</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dlogis(x, location = 0, scale = 1, log = FALSE)
plogis(q, location = 0, scale = 1, lower.tail = TRUE, log.p = FALSE)
qlogis(p, location = 0, scale = 1, lower.tail = TRUE, log.p = FALSE)
rlogis(n, location = 0, scale = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Logistic_+3A_x">x</code>, <code id="Logistic_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="Logistic_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="Logistic_+3A_n">n</code></td>
<td>
<p>number of observations. If <code>length(n) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="Logistic_+3A_location">location</code>, <code id="Logistic_+3A_scale">scale</code></td>
<td>
<p>location and scale parameters.</p>
</td></tr>
<tr><td><code id="Logistic_+3A_log">log</code>, <code id="Logistic_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="Logistic_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>location</code> or <code>scale</code> are omitted, they assume the
default values of <code>0</code> and <code>1</code> respectively.
</p>
<p>The Logistic distribution with <code>location</code> <code class="reqn">= \mu</code> and
<code>scale</code> <code class="reqn">= \sigma</code> has distribution function
</p>
<p style="text-align: center;"><code class="reqn">
    F(x) = \frac{1}{1 + e^{-(x-\mu)/\sigma}}%
  </code>
</p>
<p>  and density
</p>
<p style="text-align: center;"><code class="reqn">
    f(x)= \frac{1}{\sigma}\frac{e^{(x-\mu)/\sigma}}{(1 + e^{(x-\mu)/\sigma})^2}%
  </code>
</p>

<p>It is a long-tailed distribution with mean <code class="reqn">\mu</code> and variance
<code class="reqn">\pi^2/3 \sigma^2</code>.
</p>


<h3>Value</h3>

<p><code>dlogis</code> gives the density,
<code>plogis</code> gives the distribution function,
<code>qlogis</code> gives the quantile function, and
<code>rlogis</code> generates random deviates.
</p>
<p>The length of the result is determined by <code>n</code> for
<code>rlogis</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.  
</p>
<p>The numerical arguments other than <code>n</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.
</p>


<h3>Note</h3>

<p><code>qlogis(p)</code> is the same as the well known &lsquo;<em>logit</em>&rsquo;
function, <code class="reqn">logit(p) = \log p/(1-p)</code>,
and <code>plogis(x)</code> has consequently been called the &lsquo;inverse logit&rsquo;.
</p>
<p>The distribution function is a rescaled hyperbolic tangent,
<code>plogis(x) == (1+ <a href="base.html#topic+tanh">tanh</a>(x/2))/2</code>, and it is called a
<em>sigmoid function</em> in contexts such as neural networks.
</p>


<h3>Source</h3>

<p><code>[dpq]logis</code> are calculated directly from the definitions.
</p>
<p><code>rlogis</code> uses inversion.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
<em>Continuous Univariate Distributions</em>, volume 2, chapter 23.
Wiley, New York.
</p>


<h3>See Also</h3>

<p><a href="#topic+Distributions">Distributions</a> for other standard distributions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>var(rlogis(4000, 0, scale = 5))  # approximately (+/- 3)
pi^2/3 * 5^2
</code></pre>

<hr>
<h2 id='logLik'>Extract Log-Likelihood</h2><span id='topic+logLik'></span><span id='topic+logLik.lm'></span>

<h3>Description</h3>

<p>This function is generic; method functions can be written to handle
specific classes of objects.  Classes which have methods for this
function include: <code>"glm"</code>, <code>"lm"</code>, <code>"nls"</code> and
<code>"Arima"</code>.  Packages contain methods for other classes, such as
<code>"fitdistr"</code>, <code>"negbin"</code> and <code>"polr"</code> in package
<a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a>, <code>"multinom"</code> in package <a href="https://CRAN.R-project.org/package=nnet"><span class="pkg">nnet</span></a> and
<code>"gls"</code>, <code>"gnls"</code> <code>"lme"</code> and others in package
<a href="https://CRAN.R-project.org/package=nlme"><span class="pkg">nlme</span></a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logLik(object, ...)

## S3 method for class 'lm'
logLik(object, REML = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logLik_+3A_object">object</code></td>
<td>
<p>any object from which a log-likelihood value, or a
contribution to a log-likelihood value, can be extracted.</p>
</td></tr>
<tr><td><code id="logLik_+3A_...">...</code></td>
<td>
<p>some methods for this generic function require additional
arguments.</p>
</td></tr>
<tr><td><code id="logLik_+3A_reml">REML</code></td>
<td>
<p>an optional logical value.  If <code>TRUE</code> the restricted
log-likelihood is returned, else, if <code>FALSE</code>, the
log-likelihood is returned.  Defaults to <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>logLik</code> is most commonly used for a model fitted by maximum
likelihood, and some uses, e.g. by <code><a href="#topic+AIC">AIC</a></code>, assume
this.  So care is needed where other fit criteria have been used, for
example REML (the default for <code>"lme"</code>).
</p>
<p>For a <code>"glm"</code> fit the <code><a href="#topic+family">family</a></code> does not have to
specify how to calculate the log-likelihood, so this is based on using
the family's <code>aic()</code> function to compute the AIC.  For the
<code><a href="#topic+gaussian">gaussian</a></code>, <code><a href="#topic+Gamma">Gamma</a></code> and
<code><a href="#topic+inverse.gaussian">inverse.gaussian</a></code> families it assumed that the dispersion
of the GLM is estimated and has been counted as a parameter in the AIC
value, and for all other families it is assumed that the dispersion is
known.  Note that this procedure does not give the maximized
likelihood for <code>"glm"</code> fits from the Gamma and inverse gaussian
families, as the estimate of dispersion used is not the MLE.
</p>
<p>For <code>"lm"</code> fits it is assumed that the scale has been estimated
(by maximum likelihood or REML), and all the constants in the
log-likelihood are included.  That method is only applicable to
single-response fits.
</p>


<h3>Value</h3>

<p>Returns an object of class <code>logLik</code>.  This is a number with at
least one attribute, <code>"df"</code> (<b>d</b>egrees of <b>f</b>reedom),
giving the number of (estimated) parameters in the model.
</p>
<p>There is a simple <code>print</code> method for <code>"logLik"</code> objects.
</p>
<p>There may be other attributes depending on the method used: see the
appropriate documentation.  One that is used by several methods is
<code>"nobs"</code>, the number of observations used in estimation (after
the restrictions if <code>REML = TRUE</code>).
</p>


<h3>Author(s)</h3>

<p>Jos Pinheiro and Douglas Bates
</p>


<h3>References</h3>

<p>For <code>logLik.lm</code>:
</p>
<p>Harville, D.A. (1974).
Bayesian inference for variance components using only error contrasts.
<em>Biometrika</em>, <b>61</b>, 383&ndash;385.
<a href="https://doi.org/10.2307/2334370">doi:10.2307/2334370</a>.
</p>


<h3>See Also</h3>

<p><code><a href="nlme.html#topic+logLik.lme">logLik.gls</a></code>, <code><a href="nlme.html#topic+logLik.lme">logLik.lme</a></code>, in
package <a href="https://CRAN.R-project.org/package=nlme"><span class="pkg">nlme</span></a>, etc.
</p>
<p><code><a href="#topic+AIC">AIC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 1:5
lmx &lt;- lm(x ~ 1)
logLik(lmx) # using print.logLik() method
utils::str(logLik(lmx))

## lm method
(fm1 &lt;- lm(rating ~ ., data = attitude))
logLik(fm1)
logLik(fm1, REML = TRUE)

utils::data(Orthodont, package = "nlme")
fm1 &lt;- lm(distance ~ Sex * age, Orthodont)
logLik(fm1)
logLik(fm1, REML = TRUE)
</code></pre>

<hr>
<h2 id='loglin'>Fitting Log-Linear Models</h2><span id='topic+loglin'></span>

<h3>Description</h3>

<p><code>loglin</code> is used to fit log-linear models to multidimensional
contingency tables by Iterative Proportional Fitting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loglin(table, margin, start = rep(1, length(table)), fit = FALSE,
       eps = 0.1, iter = 20, param = FALSE, print = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loglin_+3A_table">table</code></td>
<td>
<p>a contingency table to be fit, typically the output from
<code>table</code>.</p>
</td></tr>
<tr><td><code id="loglin_+3A_margin">margin</code></td>
<td>
<p>a list of vectors with the marginal totals to be fit.
</p>
<p>(Hierarchical) log-linear models can be specified in terms of these
marginal totals which give the &lsquo;maximal&rsquo; factor subsets contained
in the model.  For example, in a three-factor model,
<code>list(c(1, 2), c(1, 3))</code> specifies a model which contains
parameters for the grand mean, each factor, and the 1-2 and 1-3
interactions, respectively (but no 2-3 or 1-2-3 interaction), i.e.,
a model where factors 2 and 3 are independent conditional on factor
1 (sometimes represented as &lsquo;[12][13]&rsquo;).
</p>
<p>The names of factors (i.e., <code>names(dimnames(table))</code>) may be
used rather than numeric indices.
</p>
</td></tr>
<tr><td><code id="loglin_+3A_start">start</code></td>
<td>
<p>a starting estimate for the fitted table.  This optional
argument is important for incomplete tables with structural zeros
in <code>table</code> which should be preserved in the fit.  In this
case, the corresponding entries in <code>start</code> should be zero and
the others can be taken as one.</p>
</td></tr>
<tr><td><code id="loglin_+3A_fit">fit</code></td>
<td>
<p>a logical indicating whether the fitted values should be
returned.</p>
</td></tr>
<tr><td><code id="loglin_+3A_eps">eps</code></td>
<td>
<p>maximum deviation allowed between observed and fitted
margins.</p>
</td></tr>
<tr><td><code id="loglin_+3A_iter">iter</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code id="loglin_+3A_param">param</code></td>
<td>
<p>a logical indicating whether the parameter values should
be returned.</p>
</td></tr>
<tr><td><code id="loglin_+3A_print">print</code></td>
<td>
<p>a logical.  If <code>TRUE</code>, the number of iterations and
the final deviation are printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Iterative Proportional Fitting algorithm as presented in
Haberman (1972) is used for fitting the model.  At most <code>iter</code>
iterations are performed, convergence is taken to occur when the
maximum deviation between observed and fitted margins is less than
<code>eps</code>.  All internal computations are done in double precision;
there is no limit on the number of factors (the dimension of the
table) in the model.
</p>
<p>Assuming that there are no structural zeros, both the Likelihood
Ratio Test and Pearson test statistics have an asymptotic chi-squared
distribution with <code>df</code> degrees of freedom.
</p>
<p>Note that the <abbr>IPF</abbr> steps are applied to the factors in the order given
in <code>margin</code>.  Hence if the model is decomposable and the order
given in <code>margin</code> is a running intersection property ordering
then <abbr>IPF</abbr> will converge in one iteration.
</p>
<p>Package <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a> contains <code>loglm</code>, a front-end to
<code>loglin</code> which allows the log-linear model to be specified and
fitted in a formula-based manner similar to that of other fitting
functions such as <code>lm</code> or <code>glm</code>.
</p>


<h3>Value</h3>

<p>A list with the following components.
</p>
<table>
<tr><td><code>lrt</code></td>
<td>
<p>the Likelihood Ratio Test statistic.</p>
</td></tr>
<tr><td><code>pearson</code></td>
<td>
<p>the Pearson test statistic (X-squared).</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>the degrees of freedom for the fitted model.  There is no
adjustment for structural zeros.</p>
</td></tr>
<tr><td><code>margin</code></td>
<td>
<p>list of the margins that were fit.  Basically the same
as the input <code>margin</code>, but with numbers replaced by names
where possible.</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>An array like <code>table</code> containing the fitted values.
Only returned if <code>fit</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code>param</code></td>
<td>
<p>A list containing the estimated parameters of the
model.  The &lsquo;standard&rsquo; constraints of zero marginal sums
(e.g., zero row and column sums for a two factor parameter) are
employed.  Only returned if <code>param</code> is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kurt Hornik</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988).
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Haberman, S. J. (1972).
Algorithm AS 51: Log-linear fit for contingency tables.
<em>Applied Statistics</em>, <b>21</b>, 218&ndash;225.
<a href="https://doi.org/10.2307/2346506">doi:10.2307/2346506</a>.
</p>
<p>Agresti, A. (1990).
<em>Categorical data analysis</em>.
New York: Wiley.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+table">table</a></code>.
</p>
<p><code><a href="MASS.html#topic+loglm">loglm</a></code> in package <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a> for a
user-friendly wrapper.
</p>
<p><code><a href="#topic+glm">glm</a></code> for another way to fit log-linear models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Model of joint independence of sex from hair and eye color.
fm &lt;- loglin(HairEyeColor, list(c(1, 2), c(1, 3), c(2, 3)))
fm
1 - pchisq(fm$lrt, fm$df)
## Model with no three-factor interactions fits well.
</code></pre>

<hr>
<h2 id='Lognormal'>The Log Normal Distribution</h2><span id='topic+Lognormal'></span><span id='topic+dlnorm'></span><span id='topic+plnorm'></span><span id='topic+qlnorm'></span><span id='topic+rlnorm'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the log normal distribution whose logarithm has mean
equal to <code>meanlog</code> and standard deviation equal to <code>sdlog</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dlnorm(x, meanlog = 0, sdlog = 1, log = FALSE)
plnorm(q, meanlog = 0, sdlog = 1, lower.tail = TRUE, log.p = FALSE)
qlnorm(p, meanlog = 0, sdlog = 1, lower.tail = TRUE, log.p = FALSE)
rlnorm(n, meanlog = 0, sdlog = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Lognormal_+3A_x">x</code>, <code id="Lognormal_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="Lognormal_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="Lognormal_+3A_n">n</code></td>
<td>
<p>number of observations. If <code>length(n) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="Lognormal_+3A_meanlog">meanlog</code>, <code id="Lognormal_+3A_sdlog">sdlog</code></td>
<td>
<p>mean and standard deviation of the distribution
on the log scale with default values of <code>0</code> and <code>1</code> respectively.</p>
</td></tr>
<tr><td><code id="Lognormal_+3A_log">log</code>, <code id="Lognormal_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="Lognormal_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The log normal distribution has density
</p>
<p style="text-align: center;"><code class="reqn">
    f(x) = \frac{1}{\sqrt{2\pi}\sigma x} e^{-(\log(x) - \mu)^2/2 \sigma^2}%
  </code>
</p>

<p>where <code class="reqn">\mu</code> and <code class="reqn">\sigma</code> are the mean and standard
deviation of the logarithm.
The mean is <code class="reqn">E(X) = exp(\mu + 1/2 \sigma^2)</code>,
the median is <code class="reqn">med(X) = exp(\mu)</code>, and the variance
<code class="reqn">Var(X) = exp(2\mu + \sigma^2)(exp(\sigma^2) - 1)</code>
and hence the coefficient of variation is
<code class="reqn">\sqrt{exp(\sigma^2) - 1}</code> which is
approximately <code class="reqn">\sigma</code> when that is small (e.g., <code class="reqn">\sigma &lt; 1/2</code>).
</p>


<h3>Value</h3>

<p><code>dlnorm</code> gives the density,
<code>plnorm</code> gives the distribution function,
<code>qlnorm</code> gives the quantile function, and
<code>rlnorm</code> generates random deviates.
</p>
<p>The length of the result is determined by <code>n</code> for
<code>rlnorm</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.
</p>
<p>The numerical arguments other than <code>n</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.
</p>


<h3>Note</h3>

<p>The cumulative hazard <code class="reqn">H(t) = - \log(1 - F(t))</code>
is <code>-plnorm(t, r, lower = FALSE, log = TRUE)</code>.
</p>


<h3>Source</h3>

<p><code>dlnorm</code> is calculated from the definition (in &lsquo;Details&rsquo;).
<code>[pqr]lnorm</code> are based on the relationship to the normal.
</p>
<p>Consequently, they model a single point mass at <code>exp(meanlog)</code>
for the boundary case <code>sdlog = 0</code>.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
<em>Continuous Univariate Distributions</em>, volume 1, chapter 14.
Wiley, New York.
</p>


<h3>See Also</h3>

<p><a href="#topic+Distributions">Distributions</a> for other standard distributions, including
<code><a href="#topic+dnorm">dnorm</a></code> for the normal distribution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dlnorm(1) == dnorm(0)
</code></pre>

<hr>
<h2 id='lowess'>Scatter Plot Smoothing</h2><span id='topic+lowess'></span>

<h3>Description</h3>

<p>This function performs the computations for the
<em>LOWESS</em> smoother which uses locally-weighted polynomial
regression (see the references).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lowess(x, y = NULL, f = 2/3, iter = 3, delta = 0.01 * diff(range(x)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lowess_+3A_x">x</code>, <code id="lowess_+3A_y">y</code></td>
<td>
<p>vectors giving the coordinates of the points in the scatter plot.
Alternatively a single plotting structure can be specified &ndash; see
<code><a href="grDevices.html#topic+xy.coords">xy.coords</a></code>.</p>
</td></tr>
<tr><td><code id="lowess_+3A_f">f</code></td>
<td>
<p>the smoother span. This gives the proportion of points in
the plot which influence the smooth at each value.
Larger values give more smoothness.</p>
</td></tr>
<tr><td><code id="lowess_+3A_iter">iter</code></td>
<td>
<p>the number of &lsquo;robustifying&rsquo; iterations which should be
performed.
Using smaller values of <code>iter</code> will make <code>lowess</code> run faster.</p>
</td></tr>
<tr><td><code id="lowess_+3A_delta">delta</code></td>
<td>
<p>See &lsquo;Details&rsquo;.  Defaults to 1/100th of the range
of <code>x</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>lowess</code> is defined by a complex algorithm, the Ratfor original
of which (by W. S. Cleveland) can be found in the <span class="rlang"><b>R</b></span> sources as file
&lsquo;<span class="file">src/library/stats/src/lowess.doc</span>&rsquo;.  Normally a local linear polynomial fit is
used, but under some circumstances (see the file) a local constant fit
can be used.  &lsquo;Local&rsquo; is defined by the distance to the
<code>floor(f*n)</code>-th nearest neighbour, and tricubic weighting is used
for <code>x</code> which fall within the neighbourhood.
</p>
<p>The initial fit is done using weighted least squares.  If
<code>iter &gt; 0</code>, further weighted fits are done using the product of
the weights from the proximity of the <code>x</code> values and case weights
derived from the residuals at the previous iteration.  Specifically,
the case weight is Tukey's biweight, with cutoff 6 times the MAD of the
residuals.  (The current <span class="rlang"><b>R</b></span> implementation differs from the original
in stopping iteration if the MAD is effectively zero since the
algorithm is highly unstable in that case.)
</p>
<p><code>delta</code> is used to speed up computation: instead of computing the
local polynomial fit at each data point it is not computed for points
within <code>delta</code> of the last computed point, and linear
interpolation is used to fill in the fitted values for the skipped
points.
</p>


<h3>Value</h3>

<p><code>lowess</code> returns a list containing components
<code>x</code> and <code>y</code> which give the coordinates of the smooth.
The smooth can be added to a plot of the original
points with the function <code>lines</code>: see the examples.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988).
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Cleveland, W. S. (1979).
Robust locally weighted regression and smoothing scatterplots.
<em>Journal of the American Statistical Association</em>, <b>74</b>,
829&ndash;836.
<a href="https://doi.org/10.1080/01621459.1979.10481038">doi:10.1080/01621459.1979.10481038</a>.
</p>
<p>Cleveland, W. S. (1981)
LOWESS: A program for smoothing scatterplots by robust locally
weighted regression.
<em>The American Statistician</em>, <b>35</b>, 54.
<a href="https://doi.org/10.2307/2683591">doi:10.2307/2683591</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+loess">loess</a></code>, a newer
formula based version of <code>lowess</code> (with different defaults!).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

plot(cars, main = "lowess(cars)")
lines(lowess(cars), col = 2)
lines(lowess(cars, f = .2), col = 3)
legend(5, 120, c(paste("f = ", c("2/3", ".2"))), lty = 1, col = 2:3)
</code></pre>

<hr>
<h2 id='ls.diag'>Compute Diagnostics for <code>lsfit</code> Regression Results</h2><span id='topic+ls.diag'></span>

<h3>Description</h3>

<p>Computes basic statistics, including standard errors, t- and p-values
for the regression coefficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ls.diag(ls.out)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ls.diag_+3A_ls.out">ls.out</code></td>
<td>
<p>Typically the result of <code><a href="#topic+lsfit">lsfit</a>()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> with the following numeric components.
</p>
<table>
<tr><td><code>std.dev</code></td>
<td>
<p>The standard deviation of the errors, an estimate of
<code class="reqn">\sigma</code>.</p>
</td></tr>
<tr><td><code>hat</code></td>
<td>
<p>diagonal entries <code class="reqn">h_{ii}</code> of the hat matrix <code class="reqn">H</code></p>
</td></tr>
<tr><td><code>std.res</code></td>
<td>
<p>standardized residuals</p>
</td></tr>
<tr><td><code>stud.res</code></td>
<td>
<p>studentized residuals</p>
</td></tr>
<tr><td><code>cooks</code></td>
<td>
<p>Cook's distances</p>
</td></tr>
<tr><td><code>dfits</code></td>
<td>
<p>DFITS statistics</p>
</td></tr>
<tr><td><code>correlation</code></td>
<td>
<p>correlation matrix</p>
</td></tr>
<tr><td><code>std.err</code></td>
<td>
<p>standard errors of the regression coefficients</p>
</td></tr>
<tr><td><code>cov.scaled</code></td>
<td>
<p>Scaled covariance matrix of the coefficients</p>
</td></tr>
<tr><td><code>cov.unscaled</code></td>
<td>
<p>Unscaled covariance matrix of the coefficients</p>
</td></tr>
</table>


<h3>References</h3>

<p>Belsley, D. A., Kuh, E. and Welsch, R. E. (1980)
<em>Regression Diagnostics.</em>
New York: Wiley.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hat">hat</a></code> for the hat matrix diagonals,
<code><a href="#topic+ls.print">ls.print</a></code>,
<code><a href="#topic+lm.influence">lm.influence</a></code>, <code><a href="#topic+summary.lm">summary.lm</a></code>,
<code><a href="#topic+anova">anova</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##-- Using the same data as the lm(.) example:
lsD9 &lt;- lsfit(x = as.numeric(gl(2, 10, 20)), y = weight)
dlsD9 &lt;- ls.diag(lsD9)
utils::str(dlsD9, give.attr = FALSE)
abs(1 - sum(dlsD9$hat) / 2) &lt; 10*.Machine$double.eps # sum(h.ii) = p
plot(dlsD9$hat, dlsD9$stud.res, xlim = c(0, 0.11))
abline(h = 0, lty = 2, col = "lightgray")
</code></pre>

<hr>
<h2 id='ls.print'>Print <code>lsfit</code> Regression Results</h2><span id='topic+ls.print'></span>

<h3>Description</h3>

<p>Computes basic statistics, including standard errors, t- and p-values
for the regression coefficients and prints them if <code>print.it</code> is
<code>TRUE</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ls.print(ls.out, digits = 4, print.it = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ls.print_+3A_ls.out">ls.out</code></td>
<td>
<p>Typically the result of <code><a href="#topic+lsfit">lsfit</a>()</code></p>
</td></tr>
<tr><td><code id="ls.print_+3A_digits">digits</code></td>
<td>
<p>The number of significant digits used for printing</p>
</td></tr>
<tr><td><code id="ls.print_+3A_print.it">print.it</code></td>
<td>
<p>a logical indicating whether the result should also be
printed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the components
</p>
<table>
<tr><td><code>summary</code></td>
<td>
<p>The ANOVA table of the regression</p>
</td></tr>
<tr><td><code>coef.table</code></td>
<td>
<p>matrix with regression coefficients, standard
errors, t- and p-values</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Usually you would use <code>summary(lm(...))</code> and
<code>anova(lm(...))</code> to obtain similar output.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ls.diag">ls.diag</a></code>, <code><a href="#topic+lsfit">lsfit</a></code>, also for examples;
<code><a href="#topic+lm">lm</a></code>, <code><a href="#topic+lm.influence">lm.influence</a></code> which usually are
preferable.
</p>

<hr>
<h2 id='lsfit'>Find the Least Squares Fit</h2><span id='topic+lsfit'></span>

<h3>Description</h3>

<p>The least squares estimate of <b><code class="reqn">\beta</code></b> in the model
</p>
<p style="text-align: center;"><code class="reqn">\bold{Y} = \bold{X \beta} + \bold{\epsilon}</code>
</p>

<p>is found.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lsfit(x, y, wt = NULL, intercept = TRUE, tolerance = 1e-07,
      yname = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lsfit_+3A_x">x</code></td>
<td>
<p>a matrix whose rows correspond to cases and whose columns
correspond to variables.</p>
</td></tr>
<tr><td><code id="lsfit_+3A_y">y</code></td>
<td>
<p>the responses, possibly a matrix if you want to fit multiple
left hand sides.</p>
</td></tr>
<tr><td><code id="lsfit_+3A_wt">wt</code></td>
<td>
<p>an optional vector of weights for performing weighted least squares.</p>
</td></tr>
<tr><td><code id="lsfit_+3A_intercept">intercept</code></td>
<td>
<p>whether or not an intercept term should be used.</p>
</td></tr>
<tr><td><code id="lsfit_+3A_tolerance">tolerance</code></td>
<td>
<p>the tolerance to be used in the matrix decomposition.</p>
</td></tr>
<tr><td><code id="lsfit_+3A_yname">yname</code></td>
<td>
<p>names to be used for the response variables.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If weights are specified then a weighted least squares is performed
with the weight given to the <em>j</em>-th case specified by the <em>j</em>-th
entry in <code>wt</code>.
</p>
<p>If any observation has a missing value in any field, that observation
is removed before the analysis is carried out.
This can be quite inefficient if there is a lot of missing data.
</p>
<p>The implementation is via a modification of the LINPACK subroutines
which allow for multiple left-hand sides.
</p>


<h3>Value</h3>

<p>A list with the following named components:
</p>
<table>
<tr><td><code>coef</code></td>
<td>
<p>the least squares estimates of the coefficients in
the model (<b><code class="reqn">\beta</code></b> as stated above).</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>residuals from the fit.</p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>indicates whether an intercept was fitted.</p>
</td></tr>
<tr><td><code>qr</code></td>
<td>
<p>the QR decomposition of the design matrix.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lm">lm</a></code> which usually is preferable;
<code><a href="#topic+ls.print">ls.print</a></code>, <code><a href="#topic+ls.diag">ls.diag</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##-- Using the same data as the lm(.) example:
lsD9 &lt;- lsfit(x = unclass(gl(2, 10)), y = weight)
ls.print(lsD9)
</code></pre>

<hr>
<h2 id='mad'>Median Absolute Deviation</h2><span id='topic+mad'></span>

<h3>Description</h3>

<p>Compute the median absolute deviation, i.e., the (lo-/hi-) median of
the absolute deviations from the median, and (by default) adjust by a
factor for asymptotically normal consistency.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mad(x, center = median(x), constant = 1.4826, na.rm = FALSE,
    low = FALSE, high = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mad_+3A_x">x</code></td>
<td>
<p>a numeric vector.</p>
</td></tr>
<tr><td><code id="mad_+3A_center">center</code></td>
<td>
<p>Optionally, the centre: defaults to the median.</p>
</td></tr>
<tr><td><code id="mad_+3A_constant">constant</code></td>
<td>
<p>scale factor.</p>
</td></tr>
<tr><td><code id="mad_+3A_na.rm">na.rm</code></td>
<td>
<p>if <code>TRUE</code> then <code>NA</code> values are stripped
from <code>x</code> before computation takes place.</p>
</td></tr>
<tr><td><code id="mad_+3A_low">low</code></td>
<td>
<p>if <code>TRUE</code>, compute the &lsquo;lo-median&rsquo;, i.e., for even
sample size, do not average the two middle values, but take the
smaller one.</p>
</td></tr>
<tr><td><code id="mad_+3A_high">high</code></td>
<td>
<p>if <code>TRUE</code>, compute the &lsquo;hi-median&rsquo;, i.e., take the
larger of the two middle values for even sample size.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The actual value calculated is <code>constant * cMedian(abs(x - center))</code>
with the default value of <code>center</code> being <code>median(x)</code>, and
<code>cMedian</code> being the usual, the &lsquo;low&rsquo; or &lsquo;high&rsquo; median, see
the arguments description for <code>low</code> and <code>high</code> above.
</p>
<p>In the case of <code class="reqn">n = 1</code> non-missing values and default <code>center</code>, the
result is <code>0</code>, consistent with &ldquo;no deviation from the center&rdquo;.
</p>
<p>The default <code>constant = 1.4826</code> (approximately
<code class="reqn">1/\Phi^{-1}(\frac 3 4)</code> = <code>1/qnorm(3/4)</code>)
ensures consistency, i.e.,
</p>
<p style="text-align: center;"><code class="reqn">E[mad(X_1,\dots,X_n)] = \sigma</code>
</p>

<p>for <code class="reqn">X_i</code> distributed as <code class="reqn">N(\mu, \sigma^2)</code>
and large <code class="reqn">n</code>.
</p>
<p>If <code>na.rm</code> is <code>TRUE</code> then <code>NA</code>
values are stripped from <code>x</code> before computation takes place.
If this is not done then an <code>NA</code> value in
<code>x</code> will cause <code>mad</code> to return <code>NA</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+IQR">IQR</a></code> which is simpler but less robust,
<code><a href="#topic+median">median</a></code>, <code><a href="#topic+var">var</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mad(c(1:9))
print(mad(c(1:9),     constant = 1)) ==
      mad(c(1:8, 100), constant = 1)       # = 2 ; TRUE
x &lt;- c(1,2,3,5,7,8)
sort(abs(x - median(x)))
c(mad(x, constant = 1),
  mad(x, constant = 1, low = TRUE),
  mad(x, constant = 1, high = TRUE))
</code></pre>

<hr>
<h2 id='mahalanobis'>Mahalanobis Distance</h2><span id='topic+mahalanobis'></span>

<h3>Description</h3>

<p>Returns the squared Mahalanobis distance of all rows in <code>x</code> and the
vector <code class="reqn">\mu</code> = <code>center</code> with respect to
<code class="reqn">\Sigma</code> = <code>cov</code>.
This is (for vector <code>x</code>) defined as
</p>
<p style="text-align: center;"><code class="reqn">D^2 = (x - \mu)' \Sigma^{-1} (x - \mu)</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>mahalanobis(x, center, cov, inverted = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mahalanobis_+3A_x">x</code></td>
<td>
<p>vector or matrix of data with, say, <code class="reqn">p</code> columns.</p>
</td></tr>
<tr><td><code id="mahalanobis_+3A_center">center</code></td>
<td>
<p>mean vector of the distribution or second data vector of
length <code class="reqn">p</code> or recyclable to that length.  If set to
<code><a href="base.html#topic+FALSE">FALSE</a></code>, the centering step is skipped.</p>
</td></tr>
<tr><td><code id="mahalanobis_+3A_cov">cov</code></td>
<td>
<p>covariance matrix (<code class="reqn">p \times p</code>) of the distribution.</p>
</td></tr>
<tr><td><code id="mahalanobis_+3A_inverted">inverted</code></td>
<td>
<p>logical.  If <code>TRUE</code>, <code>cov</code> is supposed to
contain the <em>inverse</em> of the covariance matrix.</p>
</td></tr>
<tr><td><code id="mahalanobis_+3A_...">...</code></td>
<td>
<p>passed to <code><a href="Matrix.html#topic+solve">solve</a></code> for computing the inverse of
the covariance matrix (if <code>inverted</code> is false).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+cov">cov</a></code>, <code><a href="#topic+var">var</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

ma &lt;- cbind(1:6, 1:3)
(S &lt;-  var(ma))
mahalanobis(c(0, 0), 1:2, S)

x &lt;- matrix(rnorm(100*3), ncol = 3)
stopifnot(mahalanobis(x, 0, diag(ncol(x))) == rowSums(x*x))
        ##- Here, D^2 = usual squared Euclidean distances

Sx &lt;- cov(x)
D2 &lt;- mahalanobis(x, colMeans(x), Sx)
plot(density(D2, bw = 0.5),
     main="Squared Mahalanobis distances, n=100, p=3") ; rug(D2)
qqplot(qchisq(ppoints(100), df = 3), D2,
       main = expression("Q-Q plot of Mahalanobis" * ~D^2 *
                         " vs. quantiles of" * ~ chi[3]^2))
abline(0, 1, col = 'gray')
</code></pre>

<hr>
<h2 id='make.link'>Create a Link for GLM Families</h2><span id='topic+make.link'></span>

<h3>Description</h3>

<p>This function is used with the <code><a href="#topic+family">family</a></code> functions in
<code><a href="#topic+glm">glm</a>()</code>.
Given the name of a link, it returns a link function, an inverse link
function, the derivative <code class="reqn">d\mu / d\eta</code> and a function
for domain checking.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.link(link)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.link_+3A_link">link</code></td>
<td>
<p>character; one of <code>"logit"</code>,
<code>"probit"</code>, <code>"cauchit"</code>, <code>"cloglog"</code>, <code>"identity"</code>,
<code>"log"</code>,  <code>"sqrt"</code>,  <code>"1/mu^2"</code>, <code>"inverse"</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A object of class <code>"link-glm"</code>, a list with components
</p>
<table>
<tr><td><code>linkfun</code></td>
<td>
<p>Link function <code>function(mu)</code></p>
</td></tr>
<tr><td><code>linkinv</code></td>
<td>
<p>Inverse link function <code>function(eta)</code></p>
</td></tr>
<tr><td><code>mu.eta</code></td>
<td>
<p>Derivative <code>function(eta)</code> <code class="reqn">d\mu / d\eta</code></p>
</td></tr>
<tr><td><code>valideta</code></td>
<td>
<p><code>function(eta)</code>{ <code>TRUE</code> if
<code>eta</code> is in the domain of <code>linkinv</code> }.</p>
</td></tr>
<tr><td><code>name</code></td>
<td>
<p>a name to be used for the link</p>
</td></tr></table>
<p>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+power">power</a></code>, <code><a href="#topic+glm">glm</a></code>, <code><a href="#topic+family">family</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>utils::str(make.link("logit"))
</code></pre>

<hr>
<h2 id='makepredictcall'>Utility Function for Safe Prediction</h2><span id='topic+makepredictcall'></span><span id='topic+makepredictcall.default'></span><span id='topic+SafePrediction'></span>

<h3>Description</h3>

<p>A utility to help <code><a href="#topic+model.frame.default">model.frame.default</a></code> create the right
matrices when predicting from models with terms like (univariate)
<code>poly</code> or <code>ns</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makepredictcall(var, call)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makepredictcall_+3A_var">var</code></td>
<td>
<p>A variable.</p>
</td></tr>
<tr><td><code id="makepredictcall_+3A_call">call</code></td>
<td>
<p>The term in the formula, as a call.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function with methods for <code>poly</code>, <code>bs</code> and
<code>ns</code>: the default method handles <code>scale</code>.  If
<code>model.frame.default</code> encounters such a term when
creating a model frame, it modifies the <code>predvars</code> attribute of
the terms supplied by replacing the term with one which will work for
predicting new data.  For example <code>makepredictcall.ns</code> adds
arguments for the knots and intercept.
</p>
<p>To make use of this, have your model-fitting function return the
<code>terms</code> attribute of the model frame, or copy the <code>predvars</code>
attribute of the <code>terms</code> attribute of the model frame to your
<code>terms</code> object.
</p>
<p>To extend this, make sure the term creates variables with a class,
and write a suitable method for that class.
</p>


<h3>Value</h3>

<p>A replacement for <code>call</code> for the <code>predvars</code> attribute of
the terms.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+model.frame">model.frame</a></code>, <code><a href="#topic+poly">poly</a></code>, <code><a href="base.html#topic+scale">scale</a></code>;
<code><a href="splines.html#topic+bs">bs</a></code> and <code><a href="splines.html#topic+ns">ns</a></code> in package <span class="pkg">splines</span>.
</p>
<p><code><a href="datasets.html#topic+cars">cars</a></code> for an example of prediction from a polynomial fit.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

## using poly: this did not work in R &lt; 1.5.0
fm &lt;- lm(weight ~ poly(height, 2), data = women)
plot(women, xlab = "Height (in)", ylab = "Weight (lb)")
ht &lt;- seq(57, 73, length.out = 200)
nD &lt;- data.frame(height = ht)
pfm &lt;- predict(fm, nD)
lines(ht, pfm)
pf2 &lt;- predict(update(fm, ~ stats::poly(height, 2)), nD)
stopifnot(all.equal(pfm, pf2)) ## was off (rel.diff. 0.0766) in R &lt;= 3.5.0

## see also example(cars)

## see bs and ns for spline examples.
</code></pre>

<hr>
<h2 id='manova'>Multivariate Analysis of Variance</h2><span id='topic+manova'></span>

<h3>Description</h3>

<p>A class for the multivariate analysis of variance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>manova(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="manova_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to <code><a href="#topic+aov">aov</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Class <code>"manova"</code> differs from class <code>"aov"</code> in selecting a
different <code>summary</code> method.  Function <code>manova</code> calls
<code><a href="#topic+aov">aov</a></code> and then add class <code>"manova"</code> to the result
object for each stratum.
</p>


<h3>Value</h3>

<p>See <code><a href="#topic+aov">aov</a></code> and the comments in &lsquo;Details&rsquo; here.
</p>


<h3>References</h3>

<p>Krzanowski, W. J. (1988) <em>Principles of Multivariate Analysis. A
User's Perspective.</em> Oxford.
</p>
<p>Hand, D. J. and Taylor, C. C.  (1987)
<em>Multivariate Analysis of Variance and Repeated Measures.</em>
Chapman and Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+aov">aov</a></code>, <code><a href="#topic+summary.manova">summary.manova</a></code>, the latter containing
more examples.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Set orthogonal contrasts.
op &lt;- options(contrasts = c("contr.helmert", "contr.poly"))

## Fake a 2nd response variable
npk2 &lt;- within(npk, foo &lt;- rnorm(24))
( npk2.aov &lt;- manova(cbind(yield, foo) ~ block + N*P*K, npk2) )
summary(npk2.aov)

( npk2.aovE &lt;- manova(cbind(yield, foo) ~  N*P*K + Error(block), npk2) )
summary(npk2.aovE)
</code></pre>

<hr>
<h2 id='mantelhaen.test'>Cochran-Mantel-Haenszel Chi-Squared Test for Count Data</h2><span id='topic+mantelhaen.test'></span>

<h3>Description</h3>

<p>Performs a Cochran-Mantel-Haenszel chi-squared test of the null that
two nominal variables are conditionally independent in each stratum,
assuming that there is no three-way interaction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mantelhaen.test(x, y = NULL, z = NULL,
                alternative = c("two.sided", "less", "greater"),
                correct = TRUE, exact = FALSE, conf.level = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mantelhaen.test_+3A_x">x</code></td>
<td>
<p>either a 3-dimensional contingency table in array form where
each dimension is at least 2 and the last dimension corresponds to
the strata, or a factor object with at least 2 levels.</p>
</td></tr>
<tr><td><code id="mantelhaen.test_+3A_y">y</code></td>
<td>
<p>a factor object with at least 2 levels; ignored if <code>x</code>
is an array.</p>
</td></tr>
<tr><td><code id="mantelhaen.test_+3A_z">z</code></td>
<td>
<p>a factor object with at least 2 levels identifying to which
stratum the corresponding elements in <code>x</code> and <code>y</code> belong;
ignored if <code>x</code> is an array.</p>
</td></tr>
<tr><td><code id="mantelhaen.test_+3A_alternative">alternative</code></td>
<td>
<p>indicates the alternative hypothesis and must be
one of <code>"two.sided"</code>, <code>"greater"</code> or <code>"less"</code>.
You can specify just the initial letter.
Only used in the 2 by 2 by <code class="reqn">K</code> case.</p>
</td></tr>
<tr><td><code id="mantelhaen.test_+3A_correct">correct</code></td>
<td>
<p>a logical indicating whether to apply continuity
correction when computing the test statistic.
Only used in the 2 by 2 by <code class="reqn">K</code> case.</p>
</td></tr>
<tr><td><code id="mantelhaen.test_+3A_exact">exact</code></td>
<td>
<p>a logical indicating whether the Mantel-Haenszel test or
the exact conditional test (given the strata margins) should be
computed.
Only used in the 2 by 2 by <code class="reqn">K</code> case.</p>
</td></tr>
<tr><td><code id="mantelhaen.test_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level for the returned confidence
interval.
Only used in the 2 by 2 by <code class="reqn">K</code> case.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>x</code> is an array, each dimension must be at least 2, and
the entries should be nonnegative integers.  <code>NA</code>'s are not
allowed.  Otherwise, <code>x</code>, <code>y</code> and <code>z</code> must have the
same length.  Triples containing <code>NA</code>'s are removed.  All
variables must take at least two different values.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>Only present if no exact test is performed.  In the
classical case of a 2 by 2 by <code class="reqn">K</code> table (i.e., of dichotomous
underlying variables), the Mantel-Haenszel chi-squared statistic;
otherwise, the generalized Cochran-Mantel-Haenszel statistic.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of freedom of the approximate chi-squared
distribution of the test statistic (<code class="reqn">1</code> in the classical case).
Only present if no exact test is performed.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>a confidence interval for the common odds ratio.
Only present in the 2 by 2 by <code class="reqn">K</code> case.</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>an estimate of the common odds ratio.  If an exact
test is performed, the conditional Maximum Likelihood Estimate is
given; otherwise, the Mantel-Haenszel estimate.
Only present in the 2 by 2 by <code class="reqn">K</code> case.</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>the common odds ratio under the null of
independence, <code>1</code>.
Only present in the 2 by 2 by <code class="reqn">K</code> case.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative
hypothesis.
Only present in the 2 by 2 by <code class="reqn">K</code> case.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string indicating the method employed, and whether
or not continuity correction was used.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the names of the data.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The asymptotic distribution is only valid if there is no three-way
interaction.  In the classical 2 by 2 by <code class="reqn">K</code> case, this is
equivalent to the conditional odds ratios in each stratum being
identical.  Currently, no inference on homogeneity of the odds ratios
is performed.
</p>
<p>See also the example below.
</p>


<h3>References</h3>

<p>Alan Agresti (1990).
<em>Categorical data analysis</em>.
New York: Wiley.
Pages 230&ndash;235.
</p>
<p>Alan Agresti (2002).
<em>Categorical data analysis</em> (second edition).
New York: Wiley.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Agresti (1990), pages 231--237, Penicillin and Rabbits
## Investigation of the effectiveness of immediately injected or 1.5
##  hours delayed penicillin in protecting rabbits against a lethal
##  injection with beta-hemolytic streptococci.
Rabbits &lt;-
array(c(0, 0, 6, 5,
        3, 0, 3, 6,
        6, 2, 0, 4,
        5, 6, 1, 0,
        2, 5, 0, 0),
      dim = c(2, 2, 5),
      dimnames = list(
          Delay = c("None", "1.5h"),
          Response = c("Cured", "Died"),
          Penicillin.Level = c("1/8", "1/4", "1/2", "1", "4")))
Rabbits
## Classical Mantel-Haenszel test
mantelhaen.test(Rabbits)
## =&gt; p = 0.047, some evidence for higher cure rate of immediate
##               injection
## Exact conditional test
mantelhaen.test(Rabbits, exact = TRUE)
## =&gt; p - 0.040
## Exact conditional test for one-sided alternative of a higher
## cure rate for immediate injection
mantelhaen.test(Rabbits, exact = TRUE, alternative = "greater")
## =&gt; p = 0.020

## UC Berkeley Student Admissions
mantelhaen.test(UCBAdmissions)
## No evidence for association between admission and gender
## when adjusted for department.  However,
apply(UCBAdmissions, 3, function(x) (x[1,1]*x[2,2])/(x[1,2]*x[2,1]))
## This suggests that the assumption of homogeneous (conditional)
## odds ratios may be violated.  The traditional approach would be
## using the Woolf test for interaction:
woolf &lt;- function(x) {
  x &lt;- x + 1 / 2
  k &lt;- dim(x)[3]
  or &lt;- apply(x, 3, function(x) (x[1,1]*x[2,2])/(x[1,2]*x[2,1]))
  w &lt;-  apply(x, 3, function(x) 1 / sum(1 / x))
  1 - pchisq(sum(w * (log(or) - weighted.mean(log(or), w)) ^ 2), k - 1)
}
woolf(UCBAdmissions)
## =&gt; p = 0.003, indicating that there is significant heterogeneity.
## (And hence the Mantel-Haenszel test cannot be used.)

## Agresti (2002), p. 287f and p. 297.
## Job Satisfaction example.
Satisfaction &lt;-
    as.table(array(c(1, 2, 0, 0, 3, 3, 1, 2,
                     11, 17, 8, 4, 2, 3, 5, 2,
                     1, 0, 0, 0, 1, 3, 0, 1,
                     2, 5, 7, 9, 1, 1, 3, 6),
                   dim = c(4, 4, 2),
                   dimnames =
                   list(Income =
                        c("&lt;5000", "5000-15000",
                          "15000-25000", "&gt;25000"),
                        "Job Satisfaction" =
                        c("V_D", "L_S", "M_S", "V_S"),
                        Gender = c("Female", "Male"))))
## (Satisfaction categories abbreviated for convenience.)
ftable(. ~ Gender + Income, Satisfaction)
## Table 7.8 in Agresti (2002), p. 288.
mantelhaen.test(Satisfaction)
## See Table 7.12 in Agresti (2002), p. 297.
</code></pre>

<hr>
<h2 id='mauchly.test'>Mauchly's Test of Sphericity</h2><span id='topic+mauchly.test'></span><span id='topic+mauchly.test.SSD'></span><span id='topic+mauchly.test.mlm'></span>

<h3>Description</h3>

<p>Tests whether a Wishart-distributed covariance matrix (or
transformation thereof) is proportional to a given matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mauchly.test(object, ...)
## S3 method for class 'mlm'
mauchly.test(object, ...)
## S3 method for class 'SSD'
mauchly.test(object, Sigma = diag(nrow = p),
   T = Thin.row(Proj(M) - Proj(X)), M = diag(nrow = p), X = ~0,
   idata = data.frame(index = seq_len(p)), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mauchly.test_+3A_object">object</code></td>
<td>
<p>object of class <code>SSD</code> or <code>mlm</code>.</p>
</td></tr>
<tr><td><code id="mauchly.test_+3A_sigma">Sigma</code></td>
<td>
<p>matrix to be proportional to.</p>
</td></tr>
<tr><td><code id="mauchly.test_+3A_t">T</code></td>
<td>
<p>transformation matrix. By default computed from <code>M</code> and
<code>X</code>.</p>
</td></tr>
<tr><td><code id="mauchly.test_+3A_m">M</code></td>
<td>
<p>formula or matrix describing the outer projection (see below).</p>
</td></tr>
<tr><td><code id="mauchly.test_+3A_x">X</code></td>
<td>
<p>formula or matrix describing the inner projection (see below).</p>
</td></tr>
<tr><td><code id="mauchly.test_+3A_idata">idata</code></td>
<td>
<p>data frame describing intra-block design.</p>
</td></tr>
<tr><td><code id="mauchly.test_+3A_...">...</code></td>
<td>
<p>arguments to be passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function with methods for classes <code>"mlm"</code> and
<code>"<a href="#topic+SSD">SSD</a>"</code>.
</p>
<p>The basic method is for objects of
class <code>SSD</code> the method for <code>mlm</code> objects just extracts the
<abbr>SSD</abbr> matrix and invokes the corresponding method with the same options
and arguments.
</p>
<p>The <code>T</code> argument is used to transform the observations prior to
testing. This typically involves transformation to intra-block
differences, but more complicated within-block designs can be
encountered, making more elaborate transformations necessary. A
matrix <code>T</code> can be given directly or specified as
the difference between two projections onto the spaces spanned by
<code>M</code> and <code>X</code>, which in turn can be given as matrices or as
model formulas with respect to <code>idata</code> (the tests will be
invariant to parametrization of the quotient space <code>M/X</code>).
</p>
<p>The common use of this test is in repeated measurements designs, with
<code>X = ~1</code>. This is almost, but not quite the same as testing for
compound symmetry in the untransformed covariance matrix.
</p>
<p>Notice that the defaults involve <code>p</code>, which is calculated
internally as the dimension of the <abbr>SSD</abbr> matrix, and a couple of hidden
functions in the <span class="pkg">stats</span> namespace, namely <code>proj</code> which
calculates projection matrices from design matrices or model formulas
and <code>Thin.row</code> which removes linearly dependent rows from a
matrix until it has full row rank.
</p>


<h3>Value</h3>

<p>An object of class <code>"htest"</code></p>


<h3>Note</h3>

<p>The p-value differs slightly from that of SAS because a second order term
is included in the asymptotic approximation in <span class="rlang"><b>R</b></span>.
</p>


<h3>References</h3>

<p>T. W. Anderson (1958).  <em>An Introduction to Multivariate
Statistical Analysis.</em> Wiley.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SSD">SSD</a></code>, <code><a href="#topic+anova.mlm">anova.mlm</a></code>,
<code><a href="#topic+rWishart">rWishart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>utils::example(SSD) # Brings in the mlmfit and reacttime objects

### traditional test of intrasubj. contrasts
mauchly.test(mlmfit, X = ~1)

### tests using intra-subject 3x2 design
idata &lt;- data.frame(deg = gl(3, 1, 6, labels = c(0,4,8)),
                    noise = gl(2, 3, 6, labels = c("A","P")))
mauchly.test(mlmfit, X = ~ deg + noise, idata = idata)
mauchly.test(mlmfit, M = ~ deg + noise, X = ~ noise, idata = idata)
</code></pre>

<hr>
<h2 id='mcnemar.test'>McNemar's Chi-squared Test for Count Data</h2><span id='topic+mcnemar.test'></span>

<h3>Description</h3>

<p>Performs McNemar's chi-squared test for symmetry of rows and columns
in a two-dimensional contingency table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcnemar.test(x, y = NULL, correct = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcnemar.test_+3A_x">x</code></td>
<td>
<p>either a two-dimensional contingency table in matrix form,
or a factor object.</p>
</td></tr>
<tr><td><code id="mcnemar.test_+3A_y">y</code></td>
<td>
<p>a factor object; ignored if <code>x</code> is a matrix.</p>
</td></tr>
<tr><td><code id="mcnemar.test_+3A_correct">correct</code></td>
<td>
<p>a logical indicating whether to apply continuity
correction when computing the test statistic.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The null is that the probabilities of being classified into cells
<code>[i,j]</code> and <code>[j,i]</code> are the same.
</p>
<p>If <code>x</code> is a matrix, it is taken as a two-dimensional contingency
table, and hence its entries should be nonnegative integers.
Otherwise, both <code>x</code> and <code>y</code> must be vectors or factors of the
same length.  Incomplete cases are removed, vectors are coerced into
factors, and the contingency table is computed from these.
</p>
<p>Continuity correction is only used in the 2-by-2 case if
<code>correct</code> is <code>TRUE</code>.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of McNemar's statistic.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of freedom of the approximate
chi-squared distribution of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string indicating the type of test
performed, and whether continuity correction was used.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the name(s) of the data.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Alan Agresti (1990).
<em>Categorical data analysis</em>.
New York: Wiley.
Pages 350&ndash;354.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Agresti (1990), p. 350.
## Presidential Approval Ratings.
##  Approval of the President's performance in office in two surveys,
##  one month apart, for a random sample of 1600 voting-age Americans.
Performance &lt;-
matrix(c(794, 86, 150, 570),
       nrow = 2,
       dimnames = list("1st Survey" = c("Approve", "Disapprove"),
                       "2nd Survey" = c("Approve", "Disapprove")))
Performance
mcnemar.test(Performance)
## =&gt; significant change (in fact, drop) in approval ratings
</code></pre>

<hr>
<h2 id='median'>Median Value</h2><span id='topic+median'></span><span id='topic+median.default'></span>

<h3>Description</h3>

<p>Compute the sample median.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>median(x, na.rm = FALSE, ...)
## Default S3 method:
median(x, na.rm = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="median_+3A_x">x</code></td>
<td>
<p>an object for which a method has been defined, or a
numeric vector containing the values whose median is to be computed.</p>
</td></tr>
<tr><td><code id="median_+3A_na.rm">na.rm</code></td>
<td>
<p>a logical value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="median_+3A_...">...</code></td>
<td>
<p>potentially further arguments for methods; not used in
the default method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function for which methods can be written.  However,
the default method makes use of <code>is.na</code>, <code>sort</code> and
<code>mean</code> from package <span class="pkg">base</span> all of which are generic, and so
the default method will work for most classes
(e.g., <code>"<a href="base.html#topic+Date">Date</a>"</code>) for which a median is a reasonable
concept.
</p>


<h3>Value</h3>

<p>The default method returns a length-one object of the same type as
<code>x</code>, except when <code>x</code> is logical or integer of even length,
when the result will be double.
</p>
<p>If there are no values or if <code>na.rm = FALSE</code> and there are <code>NA</code>
values the result is <code>NA</code> of the same type as <code>x</code> (or more
generally the result of <code>x[NA_integer_]</code>).
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+quantile">quantile</a></code> for general quantiles.</p>


<h3>Examples</h3>

<pre><code class='language-R'>median(1:4)                # = 2.5 [even number]
median(c(1:3, 100, 1000))  # = 3 [odd, robust]
</code></pre>

<hr>
<h2 id='medpolish'>Median Polish (Robust Two-way Decomposition) of a Matrix</h2><span id='topic+medpolish'></span>

<h3>Description</h3>

<p>Fits an additive model (two-way decomposition) using Tukey's
<em>median polish</em> procedure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>medpolish(x, eps = 0.01, maxiter = 10, trace.iter = TRUE,
          na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="medpolish_+3A_x">x</code></td>
<td>
<p>a numeric matrix.</p>
</td></tr>
<tr><td><code id="medpolish_+3A_eps">eps</code></td>
<td>
<p>real number greater than 0. A tolerance for convergence:
see &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="medpolish_+3A_maxiter">maxiter</code></td>
<td>
<p>the maximum number of iterations</p>
</td></tr>
<tr><td><code id="medpolish_+3A_trace.iter">trace.iter</code></td>
<td>
<p>logical. Should progress in convergence be reported?</p>
</td></tr>
<tr><td><code id="medpolish_+3A_na.rm">na.rm</code></td>
<td>
<p>logical. Should missing values be removed?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model fitted is additive (constant + rows + columns). The
algorithm works by alternately removing the row and column medians,
and continues until the proportional reduction in the sum
of absolute residuals is less than <code>eps</code>
or until there have been <code>maxiter</code> iterations.
The sum of absolute residuals is printed at
each iteration of the fitting process, if <code>trace.iter</code> is <code>TRUE</code>.
If <code>na.rm</code> is <code>FALSE</code> the presence of any <code>NA</code> value in
<code>x</code> will cause an error, otherwise <code>NA</code> values are ignored.
</p>
<p><code>medpolish</code> returns an object of class <code>medpolish</code> (see below).
There are printing and plotting methods for this
class, which are invoked via by the generics
<code><a href="base.html#topic+print">print</a></code> and <code><a href="graphics.html#topic+plot">plot</a></code>.
</p>


<h3>Value</h3>

<p>An object of class <code>medpolish</code> with the following named components:
</p>
<table>
<tr><td><code>overall</code></td>
<td>
<p>the fitted constant term.</p>
</td></tr>
<tr><td><code>row</code></td>
<td>
<p>the fitted row effects.</p>
</td></tr>
<tr><td><code>col</code></td>
<td>
<p>the fitted column effects.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>the residuals.</p>
</td></tr>
<tr><td><code>name</code></td>
<td>
<p>the name of the dataset.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Tukey, J. W. (1977).
<em>Exploratory Data Analysis</em>,
Reading Massachusetts: Addison-Wesley.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+median">median</a></code>; <code><a href="#topic+aov">aov</a></code> for a <em>mean</em>
instead of <em>median</em> decomposition.</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

## Deaths from sport parachuting;  from ABC of EDA, p.224:
deaths &lt;-
    rbind(c(14,15,14),
          c( 7, 4, 7),
          c( 8, 2,10),
          c(15, 9,10),
          c( 0, 2, 0))
dimnames(deaths) &lt;- list(c("1-24", "25-74", "75-199", "200++", "NA"),
                         paste(1973:1975))
deaths
(med.d &lt;- medpolish(deaths))
plot(med.d)
## Check decomposition:
all(deaths ==
    med.d$overall + outer(med.d$row,med.d$col, `+`) + med.d$residuals)
</code></pre>

<hr>
<h2 id='model.extract'>Extract Components from a Model Frame</h2><span id='topic+model.extract'></span><span id='topic+model.offset'></span><span id='topic+model.response'></span><span id='topic+model.weights'></span>

<h3>Description</h3>

<p>Returns the response, offset, subset, weights or other
special components of a model frame passed as optional arguments to
<code><a href="#topic+model.frame">model.frame</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>model.extract(frame, component)
model.offset(x)
model.response(data, type = "any")
model.weights(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model.extract_+3A_frame">frame</code>, <code id="model.extract_+3A_x">x</code>, <code id="model.extract_+3A_data">data</code></td>
<td>
<p>a model frame, see <code><a href="#topic+model.frame">model.frame</a></code>.</p>
</td></tr>
<tr><td><code id="model.extract_+3A_component">component</code></td>
<td>
<p>literal character string or name.  The name of a
component to extract, such as <code>"weights"</code> or <code>"subset"</code>.</p>
</td></tr>
<tr><td><code id="model.extract_+3A_type">type</code></td>
<td>
<p>One of <code>"any"</code>, <code>"numeric"</code> or <code>"double"</code>.
Using either of latter two coerces the result to have storage mode
<code>"double"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>model.extract</code> is provided for compatibility with S, which does
not have the more specific functions.  It is also useful to extract
e.g. the <code>etastart</code> and <code>mustart</code> components of a
<code><a href="#topic+glm">glm</a></code> fit.
</p>
<p><code>model.extract(m, "offset")</code> and <code>model.extract(m, "response")</code>
are equivalent to <code>model.offset(m)</code> and <code>model.response(m)</code>
respectively.  <code>model.offset</code> sums any terms specified by
<code><a href="#topic+offset">offset</a></code> terms in the formula or by <code>offset</code> arguments
in the call producing the model frame: it does check that the offset
is numeric.
</p>
<p><code>model.weights</code> is slightly different from
<code>model.extract(, "weights")</code> in not naming the vector it returns.
</p>


<h3>Value</h3>

<p>The specified component of the model frame, usually a vector.
<code>model.response()</code> now <em>drops</em> a possible <code>"Asis"</code> class
(stemming from <code><a href="base.html#topic+I">I</a>(.)</code>).
</p>
<p><code>model.offset</code> returns <code>NULL</code> if no offset was specified.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+model.frame">model.frame</a></code>, <code><a href="#topic+offset">offset</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- model.frame(cbind(ncases,ncontrols) ~ agegp + tobgp + alcgp, data = esoph)
model.extract(a, "response")
stopifnot(model.extract(a, "response") == model.response(a))

a &lt;- model.frame(ncases/(ncases+ncontrols) ~ agegp + tobgp + alcgp,
                 data = esoph, weights = ncases+ncontrols)
model.response(a)
(mw &lt;- model.extract(a, "weights"))
stopifnot(identical(unname(mw), model.weights(a)))

a &lt;- model.frame(cbind(ncases,ncontrols) ~ agegp,
                 something = tobgp, data = esoph)
names(a)
stopifnot(model.extract(a, "something") == esoph$tobgp)
</code></pre>

<hr>
<h2 id='model.frame'>Extracting the Model Frame from a Formula or Fit</h2><span id='topic+model.frame'></span><span id='topic+model.frame.default'></span><span id='topic+model.frame.lm'></span><span id='topic+model.frame.glm'></span><span id='topic+model.frame.aovlist'></span><span id='topic+get_all_vars'></span>

<h3>Description</h3>

<p><code>model.frame</code> (a generic function) and its methods return a
<code><a href="base.html#topic+data.frame">data.frame</a></code> with the variables needed to use
<code>formula</code> and any <code>...</code> arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>model.frame(formula, ...)

## Default S3 method:
model.frame(formula, data = NULL,
            subset = NULL, na.action,
            drop.unused.levels = FALSE, xlev = NULL, ...)

## S3 method for class 'aovlist'
model.frame(formula, data = NULL, ...)

## S3 method for class 'glm'
model.frame(formula, ...)

## S3 method for class 'lm'
model.frame(formula, ...)

get_all_vars(formula, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model.frame_+3A_formula">formula</code></td>
<td>
<p>a model <code><a href="#topic+formula">formula</a></code> or <code><a href="#topic+terms">terms</a></code>
object or an <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
<tr><td><code id="model.frame_+3A_data">data</code></td>
<td>
<p>a data frame, list or environment (or object
coercible by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code> to a data frame),
containing the variables in <code>formula</code>.  Neither a matrix nor an
array will be accepted.</p>
</td></tr>
<tr><td><code id="model.frame_+3A_subset">subset</code></td>
<td>
<p>a specification of the rows/observations to be used:
defaults to all. This can be any valid indexing vector (see
<code><a href="base.html#topic++5B.data.frame">[.data.frame</a></code>) for the rows of <code>data</code>, or
a (logical) expression using variables in <code>data</code> or
if that is not supplied, in
<code>formula</code>.  (See additional details about how this argument
interacts with data-dependent bases under &lsquo;Details&rsquo; below.)</p>
</td></tr>
<tr><td><code id="model.frame_+3A_na.action">na.action</code></td>
<td>
<p>an optional (name of a) function for treating missing
values (<code>NA</code>s). The default is first,
any <code>na.action</code> attribute of <code>data</code>, second
a <code>na.action</code> setting of <code><a href="base.html#topic+options">options</a></code>, and third
<code><a href="#topic+na.fail">na.fail</a></code> if that is unset.  The &lsquo;factory-fresh&rsquo;
default is <code><a href="#topic+na.omit">na.omit</a></code>.  Another possible value is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="model.frame_+3A_drop.unused.levels">drop.unused.levels</code></td>
<td>
<p>should factors have unused levels dropped?
Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="model.frame_+3A_xlev">xlev</code></td>
<td>
<p>a named list of character vectors giving the full set of levels
to be assumed for each factor.</p>
</td></tr>
<tr><td><code id="model.frame_+3A_...">...</code></td>
<td>
<p>for <code>model.frame</code> methods, a mix of further
arguments such as <code>data</code>, <code>na.action</code>, <code>subset</code> to pass
to the default method.  Any additional arguments (such as
<code>offset</code> and <code>weights</code> or other named arguments) which
reach the default method are used to create further columns in the
model frame, with parenthesised names such as <code>"(offset)"</code>.
</p>
<p>For <code>get_all_vars</code>, further named columns to include
in the model frame.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Exactly what happens depends on the class and attributes of the object
<code>formula</code>.  If this is an object of fitted-model class such as
<code>"lm"</code>, the method will either return the saved model frame
used when fitting the model (if any, often selected by argument
<code>model = TRUE</code>) or pass the call used when fitting on to the
default method.  The default method itself can cope with rather
standard model objects such as those of class
<code>"<a href="MASS.html#topic+lqs">lqs</a>"</code> from package <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a> if no other
arguments are supplied.
</p>
<p>The rest of this section applies only to the default method.
</p>
<p>If either <code>formula</code> or <code>data</code> is already a model frame (a
data frame with a <code>"terms"</code> attribute) and the other is missing,
the model frame is returned.  Unless <code>formula</code> is a terms object,
<code>as.formula</code> and then <code>terms</code> is called on it.  (If you wish
to use the <code>keep.order</code> argument of <code>terms.formula</code>, pass a
terms object rather than a formula.)
</p>
<p>Row names for the model frame are taken from the <code>data</code> argument
if present, then from the names of the response in the formula (or
rownames if it is a matrix), if there is one.
</p>
<p>All the variables in <code>formula</code>, <code>subset</code> and in <code>...</code>
are looked for first in <code>data</code> and then in the environment of
<code>formula</code> (see the help for <code><a href="#topic+formula">formula</a>()</code> for further
details) and collected into a data frame.  Then the <code>subset</code>
expression is evaluated, and it is used as a row index to the data
frame.  Then the <code>na.action</code> function is applied to the data frame
(and may well add attributes).  The levels of any factors in the data
frame are adjusted according to the <code>drop.unused.levels</code> and
<code>xlev</code> arguments: if <code>xlev</code> specifies a factor and a
character variable is found, it is converted to a factor (as from <span class="rlang"><b>R</b></span>
2.10.0).
</p>
<p>Because variables in the formula are evaluated before rows are dropped
based on <code>subset</code>, the characteristics of data-dependent bases such
as orthogonal polynomials (i.e. from terms using <code><a href="#topic+poly">poly</a></code>) or
splines will be computed based on the full data set rather than the
subsetted one.
</p>
<p>Unless <code>na.action = NULL</code>, time-series attributes will be removed
from the variables found (since they will be wrong if <code>NA</code>s are
removed).
</p>
<p>Note that <em>all</em> the variables in the formula are included in the
data frame, even those preceded by <code>-</code>.
</p>
<p>Only variables whose type is raw, logical, integer, real, complex or
character can be included in a model frame: this includes classed
variables such as factors (whose underlying type is integer), but
excludes lists.
</p>
<p><code>get_all_vars</code> returns a <code><a href="base.html#topic+data.frame">data.frame</a></code> containing the
variables used in <code>formula</code> plus those specified in <code>...</code>
which are recycled to the number of data frame rows.
Unlike <code>model.frame.default</code>, it returns the input variables and
not those resulting from function calls in <code>formula</code>.
</p>


<h3>Value</h3>

<p>A <code><a href="base.html#topic+data.frame">data.frame</a></code> containing the variables used in
<code>formula</code> plus those specified in <code>...</code>.  It will have
additional attributes, including <code>"terms"</code> for an object of class
<code>"<a href="#topic+terms.object">terms</a>"</code> derived from <code>formula</code>,
and possibly <code>"na.action"</code> giving information on the handling of
<code>NA</code>s (which will not be present if no special handling was done,
e.g. by <code><a href="#topic+na.pass">na.pass</a></code>).
</p>


<h3>References</h3>

<p>Chambers, J. M. (1992)
<em>Data for models.</em>
Chapter 3 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+model.matrix">model.matrix</a></code> for the &lsquo;design matrix&rsquo;,
<code><a href="#topic+formula">formula</a></code> for formulas,
<code><a href="#topic+model.extract">model.extract</a></code> to extract components, and
<code><a href="#topic+expand.model.frame">expand.model.frame</a></code> for model.frame manipulation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data.class(model.frame(dist ~ speed, data = cars))

## using a subset and an extra variable
model.frame(dist ~ speed, data = cars, subset = speed &lt; 10, z = log(dist))

## get_all_vars(): new var.s are recycled (iff length matches: 50 = 2*25)
ncars &lt;- get_all_vars(sqrt(dist) ~ I(speed/2), data = cars, newVar = 2:3)
stopifnot(is.data.frame(ncars),
          identical(cars, ncars[,names(cars)]),
          ncol(ncars) == ncol(cars) + 1)
</code></pre>

<hr>
<h2 id='model.matrix'>Construct Design Matrices</h2><span id='topic+model.matrix'></span><span id='topic+model.matrix.default'></span><span id='topic+model.matrix.lm'></span>

<h3>Description</h3>

<p><code>model.matrix</code> creates a design (or model) matrix, e.g., by
expanding factors to a set of dummy variables (depending on the
contrasts) and expanding interactions similarly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>model.matrix(object, ...)

## Default S3 method:
model.matrix(object, data = environment(object),
             contrasts.arg = NULL, xlev = NULL, ...)
## S3 method for class 'lm'
model.matrix(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model.matrix_+3A_object">object</code></td>
<td>
<p>an object of an appropriate class.  For the default
method, a model <a href="#topic+formula">formula</a> or a <code><a href="#topic+terms">terms</a></code> object.</p>
</td></tr>
<tr><td><code id="model.matrix_+3A_data">data</code></td>
<td>
<p>a data frame created with <code><a href="#topic+model.frame">model.frame</a></code>.  If
another sort of object, <code>model.frame</code> is called first.</p>
</td></tr>
<tr><td><code id="model.matrix_+3A_contrasts.arg">contrasts.arg</code></td>
<td>
<p>a list, whose entries are values (numeric
matrices, <code><a href="base.html#topic+function">function</a></code>s or character strings naming
functions) to be used
as replacement values for the <code><a href="#topic+contrasts">contrasts</a></code>
replacement function and whose names are the names of
columns of <code>data</code> containing <code><a href="base.html#topic+factor">factor</a></code>s.</p>
</td></tr>
<tr><td><code id="model.matrix_+3A_xlev">xlev</code></td>
<td>
<p>to be used as argument of <code><a href="#topic+model.frame">model.frame</a></code> if
<code>data</code> is such that <code>model.frame</code> is called.</p>
</td></tr>
<tr><td><code id="model.matrix_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>model.matrix</code> creates a design matrix from the description
given in <code>terms(object)</code>, using the data in <code>data</code> which
must supply variables with the same names as would be created by a
call to <code>model.frame(object)</code> or, more precisely, by evaluating
<code>attr(terms(object), "variables")</code>.  If <code>data</code> is a data
frame, there may be other columns and the order of columns is not
important.  Any character variables are coerced to factors.  After
coercion, all the variables used on the right-hand side of the
formula must be logical, integer, numeric or factor.
</p>
<p>If <code>contrasts.arg</code> is specified for a factor it overrides the
default factor coding for that variable and any <code>"contrasts"</code>
attribute set by <code><a href="#topic+C">C</a></code> or <code><a href="#topic+contrasts">contrasts</a></code>.
Whereas invalid <code>contrasts.arg</code>s have been ignored always, they are
warned about since <span class="rlang"><b>R</b></span> version 3.6.0.
</p>
<p>In an interaction term, the variable whose levels vary fastest is the
first one to appear in the formula (and not in the term), so in
<code>~ a + b + b:a</code> the interaction will have <code>a</code> varying
fastest.
</p>
<p>By convention, if the response variable also appears on the
right-hand side of the formula it is dropped (with a warning),
although interactions involving the term are retained.
</p>


<h3>Value</h3>

<p>The design matrix for a regression-like model with the specified formula
and data.
</p>
<p>There is an attribute <code>"assign"</code>, an integer vector with an entry
for each column in the matrix giving the term in the formula which
gave rise to the column.  Value <code>0</code> corresponds to the intercept
(if any), and positive values to terms in the order given by the
<code>term.labels</code> attribute of the <code>terms</code> structure
corresponding to <code>object</code>.
</p>
<p>If there are any factors in terms in the model, there is an attribute
<code>"contrasts"</code>, a named list with an entry for each factor.  This
specifies the contrasts that would be used in terms in which the
factor is coded by contrasts (in some terms dummy coding may be used),
either as a character vector naming a function or as a numeric matrix.
</p>


<h3>References</h3>

<p>Chambers, J. M. (1992)
<em>Data for models.</em>
Chapter 3 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+model.frame">model.frame</a></code>, <code><a href="#topic+model.extract">model.extract</a></code>,
<code><a href="#topic+terms">terms</a></code>
</p>
<p><code><a href="Matrix.html#topic+sparse.model.matrix">sparse.model.matrix</a></code> from package
<a href="https://CRAN.R-project.org/package=Matrix"><span class="pkg">Matrix</span></a> for creating <em>sparse</em> model matrices, which may
be more efficient in large dimensions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ff &lt;- log(Volume) ~ log(Height) + log(Girth)
utils::str(m &lt;- model.frame(ff, trees))
mat &lt;- model.matrix(ff, m)

dd &lt;- data.frame(a = gl(3,4), b = gl(4,1,12)) # balanced 2-way
options("contrasts") # typically 'treatment' (for unordered factors)
model.matrix(~ a + b, dd)
model.matrix(~ a + b, dd, contrasts.arg = list(a = "contr.sum"))
model.matrix(~ a + b, dd, contrasts.arg = list(a = "contr.sum", b = contr.poly))
m.orth &lt;- model.matrix(~a+b, dd, contrasts.arg = list(a = "contr.helmert"))
crossprod(m.orth) # m.orth is  ALMOST  orthogonal
# invalid contrasts.. ignored with a warning:
stopifnot(identical(
   model.matrix(~ a + b, dd),
   model.matrix(~ a + b, dd, contrasts.arg = "contr.FOO")))
</code></pre>

<hr>
<h2 id='model.tables'>Compute Tables of Results from an <code>aov</code> Model Fit</h2><span id='topic+model.tables'></span><span id='topic+model.tables.aov'></span><span id='topic+model.tables.aovlist'></span>

<h3>Description</h3>

<p>Computes summary tables for model fits, especially complex <code>aov</code>
fits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>model.tables(x, ...)

## S3 method for class 'aov'
model.tables(x, type = "effects", se = FALSE, cterms, ...)

## S3 method for class 'aovlist'
model.tables(x, type = "effects", se = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model.tables_+3A_x">x</code></td>
<td>
<p>a model object, usually produced by <code>aov</code></p>
</td></tr>
<tr><td><code id="model.tables_+3A_type">type</code></td>
<td>
<p>type of table: currently only <code>"effects"</code> and
<code>"means"</code> are implemented.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="model.tables_+3A_se">se</code></td>
<td>
<p>should standard errors be computed?</p>
</td></tr>
<tr><td><code id="model.tables_+3A_cterms">cterms</code></td>
<td>
<p>A character vector giving the names of the terms for
which tables should be computed. The default is all tables.</p>
</td></tr>
<tr><td><code id="model.tables_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>type = "effects"</code> give tables of the coefficients for each
term, optionally with standard errors.
</p>
<p>For <code>type = "means"</code> give tables of the mean response for each
combinations of levels of the factors in a term.
</p>
<p>The <code>"aov"</code> method cannot be applied to components of a
<code>"aovlist"</code> fit.
</p>


<h3>Value</h3>

<p>An object of class <code>"tables.aov"</code>, as list which may contain components
</p>
<table>
<tr><td><code>tables</code></td>
<td>
<p>A list of tables for each requested term.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>The replication information for each term.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>Standard error information.</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>The implementation is incomplete, and only the simpler cases have been
tested thoroughly.
</p>
<p>Weighted <code>aov</code> fits are not supported.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+aov">aov</a></code>, <code><a href="#topic+proj">proj</a></code>,
<code><a href="#topic+replications">replications</a></code>, <code><a href="#topic+TukeyHSD">TukeyHSD</a></code>,
<code><a href="#topic+se.contrast">se.contrast</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
options(contrasts = c("contr.helmert", "contr.treatment"))
npk.aov &lt;- aov(yield ~ block + N*P*K, npk)
model.tables(npk.aov, "means", se = TRUE)

## as a test, not particularly sensible statistically
npk.aovE &lt;- aov(yield ~  N*P*K + Error(block), npk)
model.tables(npk.aovE, se = TRUE)
model.tables(npk.aovE, "means")
</code></pre>

<hr>
<h2 id='monthplot'>
Plot a Seasonal or other Subseries from a Time Series
</h2><span id='topic+monthplot'></span><span id='topic+monthplot.default'></span><span id='topic+monthplot.ts'></span><span id='topic+monthplot.stl'></span><span id='topic+monthplot.StructTS'></span>

<h3>Description</h3>

<p>These functions plot seasonal (or other) subseries of a time series.
For each season (or other category), a time series is plotted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>monthplot(x, ...)

## S3 method for class 'stl'
monthplot(x, labels = NULL, ylab = choice, choice = "seasonal",
          ...)

## S3 method for class 'StructTS'
monthplot(x, labels = NULL, ylab = choice, choice = "sea", ...)

## S3 method for class 'ts'
monthplot(x, labels = NULL, times = time(x), phase = cycle(x),
             ylab = deparse1(substitute(x)), ...)

## Default S3 method:
monthplot(x, labels = 1L:12L,
          ylab = deparse1(substitute(x)),
          times = seq_along(x),
          phase = (times - 1L)%%length(labels) + 1L, base = mean,
          axes = TRUE, type = c("l", "h"), box = TRUE,
          add = FALSE,
          col = par("col"), lty = par("lty"), lwd = par("lwd"),
          col.base = col, lty.base = lty, lwd.base = lwd, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="monthplot_+3A_x">x</code></td>
<td>
<p>Time series or related object.</p>
</td></tr>
<tr><td><code id="monthplot_+3A_labels">labels</code></td>
<td>
<p>Labels to use for each &lsquo;season&rsquo;.</p>
</td></tr>
<tr><td><code id="monthplot_+3A_ylab">ylab</code></td>
<td>
<p>y label.</p>
</td></tr>
<tr><td><code id="monthplot_+3A_times">times</code></td>
<td>
<p>Time of each observation.</p>
</td></tr>
<tr><td><code id="monthplot_+3A_phase">phase</code></td>
<td>
<p>Indicator for each &lsquo;season&rsquo;.</p>
</td></tr>
<tr><td><code id="monthplot_+3A_base">base</code></td>
<td>
<p>Function to use for reference line for subseries.</p>
</td></tr>
<tr><td><code id="monthplot_+3A_choice">choice</code></td>
<td>
<p>Which series of an <code>stl</code> or <code>StructTS</code> object?</p>
</td></tr>
<tr><td><code id="monthplot_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to the default method or
graphical parameters.</p>
</td></tr>
<tr><td><code id="monthplot_+3A_axes">axes</code></td>
<td>
<p>Should axes be drawn (ignored if <code>add = TRUE</code>)?</p>
</td></tr>
<tr><td><code id="monthplot_+3A_type">type</code></td>
<td>
<p>Type of plot.  The default is to join the points with
lines, and <code>"h"</code> is for histogram-like vertical lines.</p>
</td></tr>
<tr><td><code id="monthplot_+3A_box">box</code></td>
<td>
<p>Should a box be drawn (ignored if <code>add = TRUE</code>)?</p>
</td></tr>
<tr><td><code id="monthplot_+3A_add">add</code></td>
<td>
<p>Should thus just add on an existing plot.</p>
</td></tr>
<tr><td><code id="monthplot_+3A_col">col</code>, <code id="monthplot_+3A_lty">lty</code>, <code id="monthplot_+3A_lwd">lwd</code></td>
<td>
<p>Graphics parameters for the series.</p>
</td></tr>
<tr><td><code id="monthplot_+3A_col.base">col.base</code>, <code id="monthplot_+3A_lty.base">lty.base</code>, <code id="monthplot_+3A_lwd.base">lwd.base</code></td>
<td>
<p>Graphics parameters for the
segments used for the reference lines.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions extract subseries from a time series and plot them
all in one frame.  The <code><a href="#topic+ts">ts</a></code>, <code><a href="#topic+stl">stl</a></code>, and
<code><a href="#topic+StructTS">StructTS</a></code> methods use the internally recorded frequency and
start and finish times to set the scale and the seasons.  The default
method assumes observations come in groups of 12 (though this can be
changed).
</p>
<p>If the <code>labels</code> are not given but the <code>phase</code> is given, then
the <code>labels</code> default to the unique values of the <code>phase</code>.  If
both are given, then the <code>phase</code> values are assumed to be indices
into the <code>labels</code> array, i.e., they should be in the range
from 1 to <code>length(labels)</code>.
</p>


<h3>Value</h3>

<p>These functions are executed for their side effect of
drawing a seasonal subseries plot on the current graphical
window.
</p>


<h3>Author(s)</h3>

<p>Duncan Murdoch</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ts">ts</a></code>, <code><a href="#topic+stl">stl</a></code>, <code><a href="#topic+StructTS">StructTS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

## The CO2 data
fit &lt;- stl(log(co2), s.window = 20, t.window = 20)
plot(fit)
op &lt;- par(mfrow = c(2,2))
monthplot(co2, ylab = "data", cex.axis = 0.8)
monthplot(fit, choice = "seasonal", cex.axis = 0.8)
monthplot(fit, choice = "trend", cex.axis = 0.8)
monthplot(fit, choice = "remainder", type = "h", cex.axis = 0.8)
par(op)

## The CO2 data, grouped quarterly
quarter &lt;- (cycle(co2) - 1) %/% 3
monthplot(co2, phase = quarter)

## see also JohnsonJohnson
</code></pre>

<hr>
<h2 id='mood.test'>Mood Two-Sample Test of Scale</h2><span id='topic+mood.test'></span><span id='topic+mood.test.default'></span><span id='topic+mood.test.formula'></span>

<h3>Description</h3>

<p>Performs Mood's two-sample test for a difference in scale parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mood.test(x, ...)

## Default S3 method:
mood.test(x, y,
          alternative = c("two.sided", "less", "greater"), ...)

## S3 method for class 'formula'
mood.test(formula, data, subset, na.action, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mood.test_+3A_x">x</code>, <code id="mood.test_+3A_y">y</code></td>
<td>
<p>numeric vectors of data values.</p>
</td></tr>
<tr><td><code id="mood.test_+3A_alternative">alternative</code></td>
<td>
<p>indicates the alternative hypothesis and must be
one of <code>"two.sided"</code> (default), <code>"greater"</code> or
<code>"less"</code> all of which can be abbreviated.</p>
</td></tr>
<tr><td><code id="mood.test_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>lhs ~ rhs</code> where <code>lhs</code>
is a numeric variable giving the data values and <code>rhs</code> a factor
with two levels giving the corresponding groups.</p>
</td></tr>
<tr><td><code id="mood.test_+3A_data">data</code></td>
<td>
<p>an optional matrix or data frame (or similar: see
<code><a href="#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="mood.test_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="mood.test_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.  Defaults to
<code>getOption("na.action")</code>.</p>
</td></tr>
<tr><td><code id="mood.test_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The underlying model is that the two samples are drawn from
<code class="reqn">f(x-l)</code> and <code class="reqn">f((x-l)/s)/s</code>, respectively, where <code class="reqn">l</code> is a
common location parameter and <code class="reqn">s</code> is a scale parameter.
</p>
<p>The null hypothesis is <code class="reqn">s = 1</code>.
</p>
<p>There are more useful tests for this problem.
</p>
<p>In the case of ties, the formulation of Mielke (1967) is employed.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative
hypothesis.   You can specify just the initial letter.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the character string <code>"Mood two-sample test of scale"</code>.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the names of the data.</p>
</td></tr>
</table>


<h3>References</h3>

<p>William J. Conover (1971),
<em>Practical nonparametric statistics</em>.
New York: John Wiley &amp; Sons.
Pages 234f.
</p>
<p>Paul W. Mielke, Jr. (1967).
Note on some squared rank tests with existing ties.
<em>Technometrics</em>, <b>9</b>/2, 312&ndash;314.
<a href="https://doi.org/10.2307/1266427">doi:10.2307/1266427</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fligner.test">fligner.test</a></code> for a rank-based (nonparametric) k-sample
test for homogeneity of variances;
<code><a href="#topic+ansari.test">ansari.test</a></code> for another rank-based two-sample test for a
difference in scale parameters;
<code><a href="#topic+var.test">var.test</a></code> and <code><a href="#topic+bartlett.test">bartlett.test</a></code> for parametric
tests for the homogeneity in variance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Same data as for the Ansari-Bradley test:
## Serum iron determination using Hyland control sera
ramsay &lt;- c(111, 107, 100, 99, 102, 106, 109, 108, 104, 99,
            101, 96, 97, 102, 107, 113, 116, 113, 110, 98)
jung.parekh &lt;- c(107, 108, 106, 98, 105, 103, 110, 105, 104,
            100, 96, 108, 103, 104, 114, 114, 113, 108, 106, 99)
mood.test(ramsay, jung.parekh)
## Compare this to ansari.test(ramsay, jung.parekh)
</code></pre>

<hr>
<h2 id='Multinom'>The Multinomial Distribution</h2><span id='topic+Multinomial'></span><span id='topic+rmultinom'></span><span id='topic+dmultinom'></span>

<h3>Description</h3>

<p>Generate multinomially distributed random number vectors and
compute multinomial probabilities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmultinom(n, size, prob)
dmultinom(x, size = NULL, prob, log = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Multinom_+3A_x">x</code></td>
<td>
<p>vector of length <code class="reqn">K</code> of integers in <code>0:size</code>.</p>
</td></tr>

<tr><td><code id="Multinom_+3A_n">n</code></td>
<td>
<p>number of random vectors to draw.</p>
</td></tr>
<tr><td><code id="Multinom_+3A_size">size</code></td>
<td>
<p>integer, say <code class="reqn">N</code>, specifying the total number
of objects that are put into <code class="reqn">K</code> boxes in the typical multinomial
experiment. For <code>dmultinom</code>, it defaults to <code>sum(x)</code>.</p>
</td></tr>
<tr><td><code id="Multinom_+3A_prob">prob</code></td>
<td>
<p>numeric non-negative vector of length <code class="reqn">K</code>, specifying
the probability for the <code class="reqn">K</code> classes; is internally normalized to
sum 1. Infinite and missing values are not allowed.</p>
</td></tr>
<tr><td><code id="Multinom_+3A_log">log</code></td>
<td>
<p>logical; if TRUE, log probabilities are computed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>x</code> is a <code class="reqn">K</code>-component vector, <code>dmultinom(x, prob)</code>
is the probability
</p>
<p style="text-align: center;"><code class="reqn">P(X_1=x_1,\ldots,X_K=x_k) = C \times \prod_{j=1}^K
    \pi_j^{x_j}</code>
</p>

<p>where <code class="reqn">C</code> is the &lsquo;multinomial coefficient&rsquo;
<code class="reqn">C = N! / (x_1! \cdots x_K!)</code>
and <code class="reqn">N = \sum_{j=1}^K x_j</code>.
<br />
By definition, each component <code class="reqn">X_j</code> is binomially distributed as
<code>Bin(size, prob[j])</code> for <code class="reqn">j = 1, \ldots, K</code>.
</p>
<p>The <code>rmultinom()</code> algorithm draws binomials <code class="reqn">X_j</code> from
<code class="reqn">Bin(n_j,P_j)</code> sequentially, where
<code class="reqn">n_1 = N</code> (N := <code>size</code>),
<code class="reqn">P_1 = \pi_1</code> (<code class="reqn">\pi</code> is <code>prob</code> scaled to sum 1),
and for <code class="reqn">j \ge 2</code>, recursively,
<code class="reqn">n_j = N - \sum_{k=1}^{j-1} X_k</code>
and
<code class="reqn">P_j = \pi_j / (1 - \sum_{k=1}^{j-1} \pi_k)</code>.
</p>


<h3>Value</h3>

<p>For <code>rmultinom()</code>,
an integer <code class="reqn">K \times n</code> matrix where each column is a
random vector generated according to the desired multinomial law, and
hence summing to <code>size</code>.  Whereas the <em>transposed</em> result
would seem more natural at first, the returned matrix is more
efficient because of columnwise storage.
</p>


<h3>Note</h3>

<p><code>dmultinom</code> is currently <em>not vectorized</em> at all and has
no C interface (API); this may be amended in the future.
</p>


<h3>See Also</h3>

<p><a href="#topic+Distributions">Distributions</a> for standard distributions, including
<code><a href="#topic+dbinom">dbinom</a></code> which is a special case conceptually.

</p>


<h3>Examples</h3>

<pre><code class='language-R'>rmultinom(10, size = 12, prob = c(0.1,0.2,0.8))

pr &lt;- c(1,3,6,10) # normalization not necessary for generation
rmultinom(10, 20, prob = pr)

## all possible outcomes of Multinom(N = 3, K = 3)
X &lt;- t(as.matrix(expand.grid(0:3, 0:3))); X &lt;- X[, colSums(X) &lt;= 3]
X &lt;- rbind(X, 3:3 - colSums(X)); dimnames(X) &lt;- list(letters[1:3], NULL)
X
round(apply(X, 2, function(x) dmultinom(x, prob = c(1,2,5))), 3)
</code></pre>

<hr>
<h2 id='na.action'>NA Action</h2><span id='topic+na.action'></span><span id='topic+na.action.default'></span>

<h3>Description</h3>

<p>Extract information on the NA action used to create an object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>na.action(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="na.action_+3A_object">object</code></td>
<td>
<p>any object whose <code><a href="base.html#topic+NA">NA</a></code> action is given.</p>
</td></tr>
<tr><td><code id="na.action_+3A_...">...</code></td>
<td>
<p>further arguments special methods could require.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>na.action</code> is a generic function, and <code>na.action.default</code> its
default method.  The latter extracts the <code>"na.action"</code> component
of a list if present, otherwise the <code>"na.action"</code> attribute.
</p>
<p>When <code><a href="#topic+model.frame">model.frame</a></code> is called, it records any information
on <code>NA</code> handling in a <code>"na.action"</code> attribute.  Most
model-fitting functions return this as a component of their result.
</p>


<h3>Value</h3>

<p>Information from the action which was applied to <code>object</code> if
<code>NA</code>s were handled specially, or <code>NULL</code>.
</p>


<h3>References</h3>

<p>Chambers, J. M. and Hastie, T. J. (1992)
<em>Statistical Models in S.</em>
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+options">options</a>("na.action")</code>, <code><a href="#topic+na.omit">na.omit</a></code>,
<code><a href="#topic+na.fail">na.fail</a></code>, also for <code>na.exclude</code>, <code>na.pass</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>na.action(na.omit(c(1, NA)))
</code></pre>

<hr>
<h2 id='na.contiguous'>Find Longest Contiguous Stretch of non-NAs</h2><span id='topic+na.contiguous'></span><span id='topic+na.contiguous.default'></span>

<h3>Description</h3>

<p>Find the longest consecutive stretch of non-missing values in a time
series object.  (In the event of a tie, the first such stretch.)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>na.contiguous(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="na.contiguous_+3A_object">object</code></td>
<td>
<p>a univariate or multivariate time series.</p>
</td></tr>
<tr><td><code id="na.contiguous_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A time series without missing values.  The class of <code>object</code> will
be preserved.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+na.omit">na.omit</a></code> and <code><a href="#topic+na.omit.ts">na.omit.ts</a></code>;
<code><a href="#topic+na.fail">na.fail</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>na.contiguous(presidents)
</code></pre>

<hr>
<h2 id='na.fail'>Handle Missing Values in Objects</h2><span id='topic+na.fail'></span><span id='topic+na.fail.default'></span><span id='topic+na.omit'></span><span id='topic+na.omit.data.frame'></span><span id='topic+na.omit.default'></span><span id='topic+na.exclude'></span><span id='topic+na.exclude.data.frame'></span><span id='topic+na.exclude.default'></span><span id='topic+na.pass'></span>

<h3>Description</h3>

<p>These generic functions are useful for dealing with <code><a href="base.html#topic+NA">NA</a></code>s
in e.g., data frames.
<code>na.fail</code> returns the object if it does not contain any
missing values, and signals an error otherwise.
<code>na.omit</code> returns the object with incomplete cases removed.
<code>na.pass</code> returns the object unchanged.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>na.fail(object, ...)
na.omit(object, ...)
na.exclude(object, ...)
na.pass(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="na.fail_+3A_object">object</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object, typically a data frame</p>
</td></tr>
<tr><td><code id="na.fail_+3A_...">...</code></td>
<td>
<p>further arguments special methods could require.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>At present these will handle vectors, matrices and data frames
comprising vectors and matrices (only).
</p>
<p>If <code>na.omit</code> removes cases, the row numbers of the cases form the
<code>"na.action"</code> attribute of the result, of class <code>"omit"</code>.
</p>
<p><code>na.exclude</code> differs from <code>na.omit</code> only in the class of the
<code>"na.action"</code> attribute of the result, which is
<code>"exclude"</code>.  This gives different behaviour in functions making
use of <code><a href="#topic+naresid">naresid</a></code> and <code><a href="#topic+napredict">napredict</a></code>: when
<code>na.exclude</code> is used the residuals and predictions are padded to
the correct length by inserting <code>NA</code>s for cases omitted by
<code>na.exclude</code>.
</p>


<h3>References</h3>

<p>Chambers, J. M. and Hastie, T. J. (1992)
<em>Statistical Models in S.</em>
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+na.action">na.action</a></code>;
<code><a href="base.html#topic+options">options</a></code> with argument <code>na.action</code> for setting NA actions;
and <code><a href="#topic+lm">lm</a></code> and <code><a href="#topic+glm">glm</a></code> for functions using these.
<code><a href="#topic+na.contiguous">na.contiguous</a></code> as alternative for time series.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>DF &lt;- data.frame(x = c(1, 2, 3), y = c(0, 10, NA))
na.omit(DF)
m &lt;- as.matrix(DF)
na.omit(m)
stopifnot(all(na.omit(1:3) == 1:3))  # does not affect objects with no NA's
try(na.fail(DF))   #&gt; Error: missing values in ...

options("na.action")
</code></pre>

<hr>
<h2 id='naprint'>
Adjust for Missing Values
</h2><span id='topic+naprint'></span><span id='topic+naprint.default'></span><span id='topic+naprint.exclude'></span><span id='topic+naprint.omit'></span>

<h3>Description</h3>

<p>Use missing value information to report the effects of an <code>na.action</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>naprint(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="naprint_+3A_x">x</code></td>
<td>

<p>An object produced by an <code>na.action</code> function.
</p>
</td></tr>
<tr><td><code id="naprint_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function, and the exact information differs by
method. <code>naprint.omit</code> reports the number of rows omitted:
<code>naprint.default</code> reports an empty string.
</p>


<h3>Value</h3>

<p>A character string providing information on missing values, for
example the number.
</p>

<hr>
<h2 id='naresid'>
Adjust for Missing Values
</h2><span id='topic+naresid'></span><span id='topic+naresid.default'></span><span id='topic+naresid.exclude'></span><span id='topic+napredict'></span><span id='topic+napredict.default'></span><span id='topic+napredict.exclude'></span>

<h3>Description</h3>

<p>Use missing value information to adjust residuals and predictions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>naresid(omit, x, ...)
napredict(omit, x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="naresid_+3A_omit">omit</code></td>
<td>
<p>an object produced by an <code><a href="#topic+na.action">na.action</a></code> function,
typically the <code>"na.action"</code> attribute of the result of
<code><a href="#topic+na.omit">na.omit</a></code> or <code><a href="#topic+na.exclude">na.exclude</a></code>.</p>
</td></tr>
<tr><td><code id="naresid_+3A_x">x</code></td>
<td>
<p>a vector, data frame, or matrix to be adjusted based upon the
missing value information.</p>
</td></tr>
<tr><td><code id="naresid_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These are utility functions used to allow <code><a href="#topic+predict">predict</a></code>,
<code><a href="#topic+fitted">fitted</a></code> and <code><a href="#topic+residuals">residuals</a></code> methods for modelling
functions to compensate for the removal of <code>NA</code>s in the fitting
process.  They are used by the default, <code>"lm"</code>, <code>"glm"</code> and
<code>"nls"</code> methods, and by further methods in packages <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a>,
<a href="https://CRAN.R-project.org/package=rpart"><span class="pkg">rpart</span></a> and <a href="https://CRAN.R-project.org/package=survival"><span class="pkg">survival</span></a>.  Also used for the scores returned by
<code><a href="#topic+factanal">factanal</a></code>, <code><a href="#topic+prcomp">prcomp</a></code> and <code><a href="#topic+princomp">princomp</a></code>.
</p>
<p>The default methods do nothing.  The default method for the <code>na.exclude</code>
action is to pad the object with <code>NA</code>s in the correct positions to
have the same number of rows as the original data frame.
</p>
<p>Currently <code>naresid</code> and <code>napredict</code> are identical, but
future methods need not be.  <code>naresid</code> is used for residuals, and
<code>napredict</code> for fitted values, predictions and <code><a href="#topic+weights">weights</a></code>.
</p>


<h3>Value</h3>

<p>These return a similar object to <code>x</code>.
</p>


<h3>Note</h3>

<p>In the early 2000s, packages <a href="https://CRAN.R-project.org/package=rpart"><span class="pkg">rpart</span></a> and <span class="pkg">survival5</span> contained
versions of these functions that had an <code>na.omit</code> action
equivalent to that now used for <code>na.exclude</code>.
</p>

<hr>
<h2 id='NegBinomial'>The Negative Binomial Distribution</h2><span id='topic+NegBinomial'></span><span id='topic+dnbinom'></span><span id='topic+pnbinom'></span><span id='topic+qnbinom'></span><span id='topic+rnbinom'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the negative binomial distribution with parameters
<code>size</code> and <code>prob</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dnbinom(x, size, prob, mu, log = FALSE)
pnbinom(q, size, prob, mu, lower.tail = TRUE, log.p = FALSE)
qnbinom(p, size, prob, mu, lower.tail = TRUE, log.p = FALSE)
rnbinom(n, size, prob, mu)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NegBinomial_+3A_x">x</code></td>
<td>
<p>vector of (non-negative integer) quantiles.</p>
</td></tr>
<tr><td><code id="NegBinomial_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="NegBinomial_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="NegBinomial_+3A_n">n</code></td>
<td>
<p>number of observations.  If <code>length(n) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="NegBinomial_+3A_size">size</code></td>
<td>
<p>target for number of successful trials, or dispersion
parameter (the shape parameter of the gamma mixing distribution).
Must be strictly positive, need not be integer.</p>
</td></tr>
<tr><td><code id="NegBinomial_+3A_prob">prob</code></td>
<td>
<p>probability of success in each trial. <code>0 &lt; prob &lt;= 1</code>.</p>
</td></tr>
<tr><td><code id="NegBinomial_+3A_mu">mu</code></td>
<td>
<p>alternative parametrization via mean: see &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="NegBinomial_+3A_log">log</code>, <code id="NegBinomial_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="NegBinomial_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The negative binomial distribution with <code>size</code> <code class="reqn">= n</code> and
<code>prob</code> <code class="reqn">= p</code> has density
</p>
<p style="text-align: center;"><code class="reqn">
    p(x) = \frac{\Gamma(x+n)}{\Gamma(n) x!} p^n (1-p)^x</code>
</p>

<p>for <code class="reqn">x = 0, 1, 2, \ldots</code>, <code class="reqn">n &gt; 0</code> and <code class="reqn">0 &lt; p \le 1</code>.
</p>
<p>This represents the number of failures which occur in a sequence of
Bernoulli trials before a target number of successes is reached.
The mean is <code class="reqn">\mu = n(1-p)/p</code> and variance <code class="reqn">n(1-p)/p^2</code>.
</p>
<p>A negative binomial distribution can also arise as a mixture of
Poisson distributions with mean distributed as a gamma distribution
(see <code><a href="#topic+pgamma">pgamma</a></code>) with scale parameter <code>(1 - prob)/prob</code>
and shape parameter <code>size</code>.  (This definition allows non-integer
values of <code>size</code>.)
</p>
<p>An alternative parametrization (often used in ecology) is by the
<em>mean</em> <code>mu</code> (see above), and <code>size</code>, the <em>dispersion
parameter</em>, where <code>prob</code> = <code>size/(size+mu)</code>.  The variance
is <code>mu + mu^2/size</code> in this parametrization.
</p>
<p>If an element of <code>x</code> is not integer, the result of <code>dnbinom</code>
is zero, with a warning.
</p>
<p>The case <code>size == 0</code> is the distribution concentrated at zero.
This is the limiting distribution for <code>size</code> approaching zero,
even if <code>mu</code> rather than <code>prob</code> is held constant.  Notice
though, that the mean of the limit distribution is 0, whatever the
value of <code>mu</code>.
</p>
<p>The quantile is defined as the smallest value <code class="reqn">x</code> such that
<code class="reqn">F(x) \ge p</code>, where <code class="reqn">F</code> is the distribution function.
</p>


<h3>Value</h3>

<p><code>dnbinom</code> gives the density,
<code>pnbinom</code> gives the distribution function,
<code>qnbinom</code> gives the quantile function, and
<code>rnbinom</code> generates random deviates.
</p>
<p>Invalid <code>size</code> or <code>prob</code> will result in return value
<code>NaN</code>, with a warning.
</p>
<p>The length of the result is determined by <code>n</code> for
<code>rnbinom</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.
</p>
<p>The numerical arguments other than <code>n</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.
</p>
<p><code>rnbinom</code> returns a vector of type <a href="base.html#topic+integer">integer</a> unless generated
values exceed the maximum representable integer when <code><a href="base.html#topic+double">double</a></code>
values are returned.
</p>


<h3>Source</h3>

<p><code>dnbinom</code> computes via binomial probabilities, using code
contributed by Catherine Loader (see <code><a href="#topic+dbinom">dbinom</a></code>).
</p>
<p><code>pnbinom</code> uses <code><a href="#topic+pbeta">pbeta</a></code>.
</p>
<p><code>qnbinom</code> uses the Cornish&ndash;Fisher Expansion to include a skewness
correction to a normal approximation, followed by a search.
</p>
<p><code>rnbinom</code> uses the derivation as a gamma mixture of Poisson
distributions, see
</p>
<p>Devroye, L. (1986) <em>Non-Uniform Random Variate Generation.</em>
Springer-Verlag, New York. Page 480.
</p>


<h3>See Also</h3>

<p><a href="#topic+Distributions">Distributions</a> for standard distributions, including
<code><a href="#topic+dbinom">dbinom</a></code> for the binomial, <code><a href="#topic+dpois">dpois</a></code> for the
Poisson and <code><a href="#topic+dgeom">dgeom</a></code> for the geometric distribution, which
is a special case of the negative binomial.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)
x &lt;- 0:11
dnbinom(x, size = 1, prob = 1/2) * 2^(1 + x) # == 1
126 /  dnbinom(0:8, size  = 2, prob  = 1/2) #- theoretically integer

## Cumulative ('p') = Sum of discrete prob.s ('d');  Relative error :
summary(1 - cumsum(dnbinom(x, size = 2, prob = 1/2)) /
                  pnbinom(x, size  = 2, prob = 1/2))

x &lt;- 0:15
size &lt;- (1:20)/4
persp(x, size, dnb &lt;- outer(x, size, function(x,s) dnbinom(x, s, prob = 0.4)),
      xlab = "x", ylab = "s", zlab = "density", theta = 150)
title(tit &lt;- "negative binomial density(x,s, pr = 0.4)  vs.  x &amp; s")

image  (x, size, log10(dnb), main = paste("log [", tit, "]"))
contour(x, size, log10(dnb), add = TRUE)

## Alternative parametrization
x1 &lt;- rnbinom(500, mu = 4, size = 1)
x2 &lt;- rnbinom(500, mu = 4, size = 10)
x3 &lt;- rnbinom(500, mu = 4, size = 100)
h1 &lt;- hist(x1, breaks = 20, plot = FALSE)
h2 &lt;- hist(x2, breaks = h1$breaks, plot = FALSE)
h3 &lt;- hist(x3, breaks = h1$breaks, plot = FALSE)
barplot(rbind(h1$counts, h2$counts, h3$counts),
        beside = TRUE, col = c("red","blue","cyan"),
        names.arg = round(h1$breaks[-length(h1$breaks)]))
</code></pre>

<hr>
<h2 id='nextn'>Find Highly Composite Numbers</h2><span id='topic+nextn'></span>

<h3>Description</h3>

<p><code>nextn</code> returns the smallest integer,
greater than or equal to <code>n</code>, which can be obtained
as a product of powers of the values contained in <code>factors</code>.
</p>
<p><code>nextn()</code> is intended to be used to find a suitable length
to zero-pad the argument of <code><a href="#topic+fft">fft</a></code>
so that the transform is computed quickly.
The default value for <code>factors</code> ensures this.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nextn(n, factors = c(2,3,5))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nextn_+3A_n">n</code></td>
<td>
<p>a vector of integer numbers (of type <code>"integer"</code>
<em>or</em> <code>"double"</code>).</p>
</td></tr>
<tr><td><code id="nextn_+3A_factors">factors</code></td>
<td>
<p>a vector of positive integer factors (at least <code class="reqn">2</code>
and preferably relative prime, see the note).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of the same <code><a href="base.html#topic+length">length</a></code> as <code>n</code>, of type
<code>"integer"</code> when the values are small enough (determined before
computing them) and <code>"double"</code> otherwise.
</p>


<h3>Note</h3>

<p>If the factors in <code>factors</code> are <em>not</em> relative prime,
i.e., have themselves a common factor larger than one, the result may
be wrong in the sense that it may not be the <em>smallest</em> integer.
E.g., <code>nextn(91, c(2,6))</code> returns 128 instead of 96 as
<code>nextn(91, c(2,3))</code> returns.
</p>
<p>When the resulting <code>N &lt;- nextn(..)</code> is larger than <code>2^53</code>, a
warning with the true 64-bit integer value is signalled, as integers
above that range may not be representable in double precision.
</p>
<p>If you really need to deal with such large integers, it may be
advisable to use package <a href="https://CRAN.R-project.org/package=gmp"><span class="pkg">gmp</span></a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+convolve">convolve</a></code>, <code><a href="#topic+fft">fft</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nextn(1001) # 1024
table(nextn(599:630))
n &lt;- 1:100 ; plot(n, nextn(n) - n, type = "o", lwd=2, cex=1/2)
</code></pre>

<hr>
<h2 id='nlm'>Non-Linear Minimization</h2><span id='topic+nlm'></span>

<h3>Description</h3>

<p>This function carries out a minimization of the function <code>f</code>
using a Newton-type algorithm.  See the references for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlm(f, p, ..., hessian = FALSE, typsize = rep(1, length(p)),
    fscale = 1, print.level = 0, ndigit = 12, gradtol = 1e-6,
    stepmax = max(1000 * sqrt(sum((p/typsize)^2)), 1000),
    steptol = 1e-6, iterlim = 100, check.analyticals = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nlm_+3A_f">f</code></td>
<td>
<p>the function to be minimized, returning a single numeric
value.  This should be a function with first argument a vector of
the length of <code>p</code> followed by any other arguments specified by
the <code>...</code> argument.
</p>
<p>If the function value has an attribute called <code>gradient</code> or
both <code>gradient</code> and <code>hessian</code> attributes, these will be
used in the calculation of updated parameter values.  Otherwise,
numerical derivatives are used. <code><a href="#topic+deriv">deriv</a></code> returns a
function with suitable <code>gradient</code> attribute and optionally a
<code>hessian</code> attribute.</p>
</td></tr>
<tr><td><code id="nlm_+3A_p">p</code></td>
<td>
<p>starting parameter values for the minimization.</p>
</td></tr>
<tr><td><code id="nlm_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed to <code>f</code>.</p>
</td></tr>
<tr><td><code id="nlm_+3A_hessian">hessian</code></td>
<td>
<p>if <code>TRUE</code>, the hessian of <code>f</code>
at the minimum is returned.</p>
</td></tr>
<tr><td><code id="nlm_+3A_typsize">typsize</code></td>
<td>
<p>an estimate of the size of each parameter
at the minimum.</p>
</td></tr>
<tr><td><code id="nlm_+3A_fscale">fscale</code></td>
<td>
<p>an estimate of the size of <code>f</code> at the minimum.</p>
</td></tr>
<tr><td><code id="nlm_+3A_print.level">print.level</code></td>
<td>
<p>this argument determines the level of printing
which is done during the minimization process.  The default
value of <code>0</code> means that no printing occurs, a value of <code>1</code>
means that initial and final details are printed and a value
of 2 means that full tracing information is printed.</p>
</td></tr>
<tr><td><code id="nlm_+3A_ndigit">ndigit</code></td>
<td>
<p>the number of significant digits in the function <code>f</code>.</p>
</td></tr>
<tr><td><code id="nlm_+3A_gradtol">gradtol</code></td>
<td>
<p>a positive scalar giving the tolerance at which the
scaled gradient is considered close enough to zero to
terminate the algorithm.  The scaled gradient is a
measure of the relative change in <code>f</code> in each direction
<code>p[i]</code> divided by the relative change in <code>p[i]</code>.</p>
</td></tr>
<tr><td><code id="nlm_+3A_stepmax">stepmax</code></td>
<td>
<p>a positive scalar which gives the maximum allowable
scaled step length.  <code>stepmax</code> is used to prevent steps which
would cause the optimization function to overflow, to prevent the
algorithm from leaving the area of interest in parameter space, or to
detect divergence in the algorithm. <code>stepmax</code> would be chosen
small enough to prevent the first two of these occurrences, but should
be larger than any anticipated reasonable step.</p>
</td></tr>
<tr><td><code id="nlm_+3A_steptol">steptol</code></td>
<td>
<p>A positive scalar providing the minimum allowable
relative step length.</p>
</td></tr>
<tr><td><code id="nlm_+3A_iterlim">iterlim</code></td>
<td>
<p>a positive integer specifying the maximum number of
iterations to be performed before the program is terminated.</p>
</td></tr>
<tr><td><code id="nlm_+3A_check.analyticals">check.analyticals</code></td>
<td>
<p>a logical scalar specifying whether the
analytic gradients and Hessians, if they are supplied, should be
checked against numerical derivatives at the initial parameter
values. This can help detect incorrectly formulated gradients or
Hessians.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that arguments after <code>...</code> must be matched exactly.
</p>
<p>If a gradient or hessian is supplied but evaluates to the wrong mode
or length, it will be ignored if <code>check.analyticals = TRUE</code> (the
default) with a warning.  The hessian is not even checked unless the
gradient is present and passes the sanity checks.
</p>
<p>The C code for the &ldquo;perturbed&rdquo; Cholesky, <code>choldc()</code> has
had a bug in all <span class="rlang"><b>R</b></span> versions before 3.4.1.
</p>
<p>From the three methods available in the original source, we always use
method &ldquo;1&rdquo; which is line search.
</p>
<p>The functions supplied should always return finite (including not
<code>NA</code> and not <code>NaN</code>) values: for the function value itself
non-finite values are replaced by the maximum positive value with a warning.
</p>


<h3>Value</h3>

<p>A list containing the following components:
</p>
<table>
<tr><td><code>minimum</code></td>
<td>
<p>the value of the estimated minimum of <code>f</code>.</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>the point at which the minimum value of
<code>f</code> is obtained.</p>
</td></tr>
<tr><td><code>gradient</code></td>
<td>
<p>the gradient at the estimated minimum of <code>f</code>.</p>
</td></tr>
<tr><td><code>hessian</code></td>
<td>
<p>the hessian at the estimated minimum of <code>f</code> (if
requested).</p>
</td></tr>
<tr><td><code>code</code></td>
<td>
<p>an integer indicating why the optimization process terminated.
</p>

<dl>
<dt>1:</dt><dd><p>relative gradient is close to zero, current iterate is
probably solution.</p>
</dd>
<dt>2:</dt><dd><p>successive iterates within tolerance, current iterate
is probably solution.</p>
</dd>
<dt>3:</dt><dd><p>last global step failed to locate a point lower than
<code>estimate</code>.  Either <code>estimate</code> is an approximate local
minimum of the function or <code>steptol</code> is too small.</p>
</dd>
<dt>4:</dt><dd><p>iteration limit exceeded.</p>
</dd>
<dt>5:</dt><dd><p>maximum step size <code>stepmax</code> exceeded five consecutive
times.  Either the function is unbounded below,
becomes asymptotic to a finite value from above in
some direction or <code>stepmax</code> is too small.</p>
</dd>
</dl>

</td></tr>
<tr><td><code>iterations</code></td>
<td>
<p>the number of iterations performed.</p>
</td></tr>
</table>


<h3>Source</h3>

<p>The current code is by Saikat DebRoy and the R Core team, using a C
translation of Fortran code by Richard H. Jones.
</p>


<h3>References</h3>

<p>Dennis, J. E. and Schnabel, R. B. (1983).
<em>Numerical Methods for Unconstrained Optimization and Nonlinear
Equations</em>.
Prentice-Hall, Englewood Cliffs, NJ.
</p>
<p>Schnabel, R. B., Koontz, J. E. and Weiss, B. E. (1985).
A modular system of algorithms for unconstrained minimization.
<em>ACM Transactions on Mathematical Software</em>, <b>11</b>, 419&ndash;440.
<a href="https://doi.org/10.1145/6187.6192">doi:10.1145/6187.6192</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+optim">optim</a></code> and <code><a href="#topic+nlminb">nlminb</a></code>.
</p>
<p><code><a href="#topic+constrOptim">constrOptim</a></code> for constrained optimization,
<code><a href="#topic+optimize">optimize</a></code> for one-dimensional
minimization and <code><a href="#topic+uniroot">uniroot</a></code> for root finding.
<code><a href="#topic+deriv">deriv</a></code> to calculate analytical derivatives.
</p>
<p>For nonlinear regression, <code><a href="#topic+nls">nls</a></code> may be better.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f &lt;- function(x) sum((x-1:length(x))^2)
nlm(f, c(10,10))
nlm(f, c(10,10), print.level = 2)
utils::str(nlm(f, c(5), hessian = TRUE))

f &lt;- function(x, a) sum((x-a)^2)
nlm(f, c(10,10), a = c(3,5))
f &lt;- function(x, a)
{
    res &lt;- sum((x-a)^2)
    attr(res, "gradient") &lt;- 2*(x-a)
    res
}
nlm(f, c(10,10), a = c(3,5))

## more examples, including the use of derivatives.
## Not run: demo(nlm)
</code></pre>

<hr>
<h2 id='nlminb'>Optimization using PORT routines </h2><span id='topic+nlminb'></span>

<h3>Description</h3>

<p>Unconstrained and box-constrained optimization using PORT routines.
</p>
<p>For historical compatibility.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlminb(start, objective, gradient = NULL, hessian = NULL, ...,
       scale = 1, control = list(), lower = -Inf, upper = Inf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nlminb_+3A_start">start</code></td>
<td>

<p>numeric vector, initial values for the parameters to be optimized.
</p>
</td></tr>
<tr><td><code id="nlminb_+3A_objective">objective</code></td>
<td>

<p>Function to be minimized.  Must return a scalar value.  The first
argument to <code>objective</code> is the vector of parameters to be
optimized, whose initial values are supplied through <code>start</code>.
Further arguments (fixed during the course of the optimization) to
<code>objective</code> may be specified as well (see <code>...</code>).
</p>
</td></tr>
<tr><td><code id="nlminb_+3A_gradient">gradient</code></td>
<td>

<p>Optional function that takes the same arguments as <code>objective</code> and
evaluates the gradient of <code>objective</code> at its first argument.  Must
return a vector as long as <code>start</code>.
</p>
</td></tr>
<tr><td><code id="nlminb_+3A_hessian">hessian</code></td>
<td>

<p>Optional function that takes the same arguments as <code>objective</code> and
evaluates the hessian of <code>objective</code> at its first argument.  Must
return a square matrix of order <code>length(start)</code>.  Only the
lower triangle is used.
</p>
</td></tr>
<tr><td><code id="nlminb_+3A_...">...</code></td>
<td>
<p>Further arguments to be supplied to <code>objective</code>.</p>
</td></tr>
<tr><td><code id="nlminb_+3A_scale">scale</code></td>
<td>
<p>See PORT documentation (or leave alone).</p>
</td></tr>
<tr><td><code id="nlminb_+3A_control">control</code></td>
<td>
<p>A list of control parameters. See below for details.</p>
</td></tr>
<tr><td><code id="nlminb_+3A_lower">lower</code>, <code id="nlminb_+3A_upper">upper</code></td>
<td>

<p>vectors of lower and upper bounds, replicated to be as long as
<code>start</code>.  If unspecified, all parameters are assumed to be
unconstrained.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Any names of <code>start</code> are passed on to <code>objective</code> and where
applicable, <code>gradient</code> and <code>hessian</code>.  The parameter vector
will be coerced to double.
</p>


<p>If any of the functions returns <code>NA</code> or <code>NaN</code> this is an
error for the gradient and Hessian, and such values for function
evaluation are replaced by <code>+Inf</code> with a warning.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>The best set of parameters found.</p>
</td></tr>
<tr><td><code>objective</code></td>
<td>
<p>The value of <code>objective</code> corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>An integer code. <code>0</code> indicates successful
convergence.
</p>
</td></tr>
<tr><td><code>message</code></td>
<td>

<p>A character string giving any additional information returned by the
optimizer, or <code>NULL</code>. For details, see PORT documentation.
</p>
</td></tr>
<tr><td><code>iterations</code></td>
<td>
<p>Number of iterations performed.</p>
</td></tr>
<tr><td><code>evaluations</code></td>
<td>
<p>Number of objective function and gradient function evaluations</p>
</td></tr>
</table>


<h3>Control parameters</h3>

<p>Possible names in the <code>control</code> list and their default values
are:
</p>

<dl>
<dt><code>eval.max</code></dt><dd><p>Maximum number of evaluations of the objective
function allowed.  Defaults to 200.</p>
</dd>
<dt><code>iter.max</code></dt><dd><p>Maximum number of iterations allowed.
Defaults to 150.</p>
</dd>
<dt><code>trace</code></dt><dd><p>The value of the objective function and the parameters
is printed every trace'th iteration.  Defaults to 0 which
indicates no trace information is to be printed.</p>
</dd>
<dt><code>abs.tol</code></dt><dd><p>Absolute tolerance.  Defaults
to 0 so the absolute convergence test is not used.  If the objective
function is known to be non-negative, the previous default of
<code>1e-20</code> would be more appropriate.</p>
</dd>
<dt><code>rel.tol</code></dt><dd><p>Relative tolerance.  Defaults to
<code>1e-10</code>.</p>
</dd>
<dt><code>x.tol</code></dt><dd><p>X tolerance.  Defaults to <code>1.5e-8</code>.</p>
</dd>
<dt><code>xf.tol</code></dt><dd><p>false convergence tolerance.  Defaults to
<code>2.2e-14</code>.</p>
</dd>
<dt><code>step.min, step.max</code></dt><dd><p>Minimum and maximum step size.  Both
default to <code>1.</code>.</p>
</dd>
<dt>sing.tol</dt><dd><p>singular convergence tolerance; defaults to
<code>rel.tol</code>.</p>
</dd>
<dt>scale.init</dt><dd><p>...</p>
</dd>
<dt>diff.g</dt><dd><p>an estimated bound on the relative error in the
objective function value.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p><span class="rlang"><b>R</b></span> port: Douglas Bates and Deepayan Sarkar.
</p>
<p>Underlying Fortran code by David M. Gay
</p>


<h3>Source</h3>

<p><a href="https://netlib.org/port/">https://netlib.org/port/</a>
</p>


<h3>References</h3>

<p>David M. Gay (1990),
Usage summary for selected optimization routines.
Computing Science Technical Report 153, AT&amp;T Bell Laboratories, Murray
Hill.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+optim">optim</a></code> (which is preferred) and <code><a href="#topic+nlm">nlm</a></code>.
</p>
<p><code><a href="#topic+optimize">optimize</a></code> for one-dimensional minimization and
<code><a href="#topic+constrOptim">constrOptim</a></code> for constrained optimization.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- rnbinom(100, mu = 10, size = 10)
hdev &lt;- function(par)
    -sum(dnbinom(x, mu = par[1], size = par[2], log = TRUE))
nlminb(c(9, 12), hdev)
nlminb(c(20, 20), hdev, lower = 0, upper = Inf)
nlminb(c(20, 20), hdev, lower = 0.001, upper = Inf)

## slightly modified from the S-PLUS help page for nlminb
# this example minimizes a sum of squares with known solution y
sumsq &lt;- function( x, y) {sum((x-y)^2)}
y &lt;- rep(1,5)
x0 &lt;- rnorm(length(y))
nlminb(start = x0, sumsq, y = y)
# now use bounds with a y that has some components outside the bounds
y &lt;- c( 0, 2, 0, -2, 0)
nlminb(start = x0, sumsq, lower = -1, upper = 1, y = y)
# try using the gradient
sumsq.g &lt;- function(x, y) 2*(x-y)
nlminb(start = x0, sumsq, sumsq.g,
       lower = -1, upper = 1, y = y)
# now use the hessian, too
sumsq.h &lt;- function(x, y) diag(2, nrow = length(x))
nlminb(start = x0, sumsq, sumsq.g, sumsq.h,
       lower = -1, upper = 1, y = y)

## Rest lifted from optim help page

fr &lt;- function(x) {   ## Rosenbrock Banana function
    x1 &lt;- x[1]
    x2 &lt;- x[2]
    100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}
grr &lt;- function(x) { ## Gradient of 'fr'
    x1 &lt;- x[1]
    x2 &lt;- x[2]
    c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
       200 *      (x2 - x1 * x1))
}
nlminb(c(-1.2,1), fr)
nlminb(c(-1.2,1), fr, grr)


flb &lt;- function(x)
    { p &lt;- length(x); sum(c(1, rep(4, p-1)) * (x - c(1, x[-p])^2)^2) }
## 25-dimensional box constrained
## par[24] is *not* at boundary
nlminb(rep(3, 25), flb, lower = rep(2, 25), upper = rep(4, 25))
## trying to use a too small tolerance:
r &lt;- nlminb(rep(3, 25), flb, control = list(rel.tol = 1e-16))
stopifnot(grepl("rel.tol", r$message))
</code></pre>

<hr>
<h2 id='nls'>Nonlinear Least Squares</h2><span id='topic+nls'></span>

<h3>Description</h3>

<p>Determine the nonlinear (weighted) least-squares estimates of the
parameters of a nonlinear model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nls(formula, data, start, control, algorithm,
    trace, subset, weights, na.action, model,
    lower, upper, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nls_+3A_formula">formula</code></td>
<td>
<p>a nonlinear model <a href="#topic+formula">formula</a> including variables and
parameters.  Will be coerced to a formula if necessary.</p>
</td></tr>
<tr><td><code id="nls_+3A_data">data</code></td>
<td>
<p>an optional data frame in which to evaluate the variables in
<code>formula</code> and <code>weights</code>.  Can also be a list or an
environment, but not a matrix.</p>
</td></tr>
<tr><td><code id="nls_+3A_start">start</code></td>
<td>
<p>a named list or named numeric vector of starting
estimates.  When <code>start</code> is missing (and <code>formula</code> is not
a self-starting model, see <code><a href="#topic+selfStart">selfStart</a></code>), a very cheap
guess for <code>start</code> is tried (if <code>algorithm != "plinear"</code>).
</p>
</td></tr>
<tr><td><code id="nls_+3A_control">control</code></td>
<td>
<p>an optional <code><a href="base.html#topic+list">list</a></code> of control settings.  See
<code><a href="#topic+nls.control">nls.control</a></code> for the names of the settable control
values and their effect.</p>
</td></tr>
<tr><td><code id="nls_+3A_algorithm">algorithm</code></td>
<td>
<p>character string specifying the algorithm to use.
The default algorithm is a Gauss-Newton algorithm.  Other possible
values are <code>"plinear"</code> for the Golub-Pereyra algorithm for
partially linear least-squares models and <code>"port"</code> for the
&lsquo;nl2sol&rsquo; algorithm from the Port library &ndash; see the
references.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="nls_+3A_trace">trace</code></td>
<td>
<p>logical value indicating if a trace of the iteration
progress should be printed.  Default is <code>FALSE</code>.  If
<code>TRUE</code> the residual (weighted) sum-of-squares, the convergence
criterion and the parameter values are printed at the conclusion of
each iteration.  Note that <code><a href="base.html#topic+format">format</a>()</code> is used, so these
mostly depend on <code><a href="base.html#topic+getOption">getOption</a>("digits")</code>.
When the <code>"plinear"</code> algorithm is used, the conditional
estimates of the linear parameters are printed after the nonlinear
parameters.  When the <code>"port"</code> algorithm is used the
objective function value printed is half the residual (weighted)
sum-of-squares.</p>
</td></tr>
<tr><td><code id="nls_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used in the fitting process.</p>
</td></tr>
<tr><td><code id="nls_+3A_weights">weights</code></td>
<td>
<p>an optional numeric vector of (fixed) weights.  When
present, the objective function is weighted least squares.</p>
</td></tr>
<tr><td><code id="nls_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen
when the data contain <code>NA</code>s.  The default is set by
the <code>na.action</code> setting of <code><a href="base.html#topic+options">options</a></code>, and is
<code><a href="#topic+na.fail">na.fail</a></code> if that is unset.  The &lsquo;factory-fresh&rsquo;
default is <code><a href="#topic+na.omit">na.omit</a></code>.  Value <code><a href="#topic+na.exclude">na.exclude</a></code>
can be useful.</p>
</td></tr>
<tr><td><code id="nls_+3A_model">model</code></td>
<td>
<p>logical.  If true, the model frame is returned as part of
the object. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="nls_+3A_lower">lower</code>, <code id="nls_+3A_upper">upper</code></td>
<td>
<p>vectors of lower and upper bounds, replicated to
be as long as <code>start</code>.  If unspecified, all parameters are
assumed to be unconstrained.  Bounds can only be used with the
<code>"port"</code> algorithm.  They are ignored, with a warning, if given
for other algorithms.</p>
</td></tr>
<tr><td><code id="nls_+3A_...">...</code></td>
<td>
<p>Additional optional arguments.  None are used at present.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An <code>nls</code> object is a type of fitted model object.  It has methods
for the generic functions <code><a href="#topic+anova">anova</a></code>, <code><a href="#topic+coef">coef</a></code>,
<code><a href="#topic+confint">confint</a></code>, <code><a href="#topic+deviance">deviance</a></code>,
<code><a href="#topic+df.residual">df.residual</a></code>, <code><a href="#topic+fitted">fitted</a></code>,
<code><a href="#topic+formula">formula</a></code>, <code><a href="#topic+logLik">logLik</a></code>, <code><a href="#topic+predict">predict</a></code>,
<code><a href="base.html#topic+print">print</a></code>, <code><a href="#topic+profile">profile</a></code>, <code><a href="#topic+residuals">residuals</a></code>,
<code><a href="base.html#topic+summary">summary</a></code>, <code><a href="#topic+vcov">vcov</a></code> and <code><a href="#topic+weights">weights</a></code>.
</p>
<p>Variables in <code>formula</code> (and <code>weights</code> if not missing) are
looked for first in <code>data</code>, then the environment of
<code>formula</code> and finally along the search path.  Functions in
<code>formula</code> are searched for first in the environment of
<code>formula</code> and then along the search path.
</p>
<p>Arguments <code>subset</code> and <code>na.action</code> are supported only when
all the variables in the formula taken from <code>data</code> are of the
same length: other cases give a warning.
</p>
<p>Note that the <code><a href="#topic+anova">anova</a></code> method does not check that the
models are nested: this cannot easily be done automatically, so use
with care.
</p>


<h3>Value</h3>

<p>A list of
</p>
<table>
<tr><td><code>m</code></td>
<td>
<p>an <code>nlsModel</code> object incorporating the model.</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>the expression that was passed to <code>nls</code> as the data
argument.  The actual data values are present in the <code><a href="base.html#topic+environment">environment</a></code> of
the <code>m</code> components, e.g., <code>environment(m$conv)</code>.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call with several components, notably
<code>algorithm</code>.</p>
</td></tr>
<tr><td><code>na.action</code></td>
<td>
<p>the <code>"na.action"</code> attribute (if any) of the
model frame.</p>
</td></tr>
<tr><td><code>dataClasses</code></td>
<td>
<p>the <code>"dataClasses"</code> attribute (if any) of the
<code>"terms"</code> attribute of the model frame.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>if <code>model = TRUE</code>, the model frame.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>if <code>weights</code> is supplied, the weights.</p>
</td></tr>
<tr><td><code>convInfo</code></td>
<td>
<p>a list with convergence information.</p>
</td></tr>
<tr><td><code>control</code></td>
<td>
<p>the control <code>list</code> used, see the <code>control</code>
argument.</p>
</td></tr>
<tr><td><code>convergence</code>, <code>message</code></td>
<td>
<p>for an <code>algorithm = "port"</code> fit only,
a convergence code (<code>0</code> for convergence) and message.
</p>
<p>To use these is <em>deprecated</em>, as they are available from
<code>convInfo</code> now.
</p>
</td></tr>
</table>


<h3>Warning</h3>

<p><b>The default settings of <code>nls</code> generally fail on artificial
&ldquo;zero-residual&rdquo; data problems.</b>
</p>
<p>The <code>nls</code> function uses a relative-offset convergence criterion
that compares the numerical imprecision at the current parameter
estimates to the residual sum-of-squares.  This performs well on data of
the form </p>
<p style="text-align: center;"><code class="reqn">y=f(x, \theta) + \varepsilon</code>
</p>
<p> (with
<code class="reqn">var(\varepsilon) &gt; 0</code>).  It fails to indicate convergence on data of the form
</p>
<p style="text-align: center;"><code class="reqn">y = f(x, \theta)</code>
</p>
<p> because the criterion amounts to
comparing two components of the round-off error.
To avoid a zero-divide in computing the convergence testing value, a
positive constant <code>scaleOffset</code> should be added to the denominator
sum-of-squares; it is set in <code>control</code>, as in the example below;
this does not yet apply to <code>algorithm = "port"</code>.
</p>
<p>The <code>algorithm = "port"</code> code appears unfinished, and does
not even check that the starting value is within the bounds.
Use with caution, especially where bounds are supplied.
</p>


<h3>Note</h3>

<p>Setting <code>warnOnly = TRUE</code> in the <code>control</code>
argument (see <code><a href="#topic+nls.control">nls.control</a></code>) returns a non-converged
object (since <span class="rlang"><b>R</b></span> version 2.5.0) which might be useful for further
convergence analysis, <em>but <b>not</b> for inference</em>.
</p>


<h3>Author(s)</h3>

<p>Douglas M. Bates and Saikat DebRoy: David M. Gay for the Fortran code
used by <code>algorithm = "port"</code>.
</p>


<h3>References</h3>

<p>Bates, D. M. and Watts, D. G. (1988)
<em>Nonlinear Regression Analysis and Its Applications</em>,
Wiley
</p>
<p>Bates, D. M. and Chambers, J. M. (1992)
<em>Nonlinear models.</em>
Chapter 10 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>
<p><a href="https://netlib.org/port/">https://netlib.org/port/</a> for the Port library
documentation.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.nls">summary.nls</a></code>, <code><a href="#topic+predict.nls">predict.nls</a></code>,
<code><a href="#topic+profile.nls">profile.nls</a></code>.
</p>
<p>Self starting models (with &lsquo;automatic initial values&rsquo;):
<code><a href="#topic+selfStart">selfStart</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require(graphics)

DNase1 &lt;- subset(DNase, Run == 1)

## using a selfStart model
fm1DNase1 &lt;- nls(density ~ SSlogis(log(conc), Asym, xmid, scal), DNase1)
summary(fm1DNase1)
## the coefficients only:
coef(fm1DNase1)
## including their SE, etc:
coef(summary(fm1DNase1))

## using conditional linearity
fm2DNase1 &lt;- nls(density ~ 1/(1 + exp((xmid - log(conc))/scal)),
                 data = DNase1,
                 start = list(xmid = 0, scal = 1),
                 algorithm = "plinear")
summary(fm2DNase1)

## without conditional linearity
fm3DNase1 &lt;- nls(density ~ Asym/(1 + exp((xmid - log(conc))/scal)),
                 data = DNase1,
                 start = list(Asym = 3, xmid = 0, scal = 1))
summary(fm3DNase1)

## using Port's nl2sol algorithm
fm4DNase1 &lt;- nls(density ~ Asym/(1 + exp((xmid - log(conc))/scal)),
                 data = DNase1,
                 start = list(Asym = 3, xmid = 0, scal = 1),
                 algorithm = "port")
summary(fm4DNase1)

## weighted nonlinear regression
Treated &lt;- Puromycin[Puromycin$state == "treated", ]
weighted.MM &lt;- function(resp, conc, Vm, K)
{
    ## Purpose: exactly as white book p. 451 -- RHS for nls()
    ##  Weighted version of Michaelis-Menten model
    ## ----------------------------------------------------------
    ## Arguments: 'y', 'x' and the two parameters (see book)
    ## ----------------------------------------------------------
    ## Author: Martin Maechler, Date: 23 Mar 2001

    pred &lt;- (Vm * conc)/(K + conc)
    (resp - pred) / sqrt(pred)
}

Pur.wt &lt;- nls( ~ weighted.MM(rate, conc, Vm, K), data = Treated,
              start = list(Vm = 200, K = 0.1))
summary(Pur.wt)

## Passing arguments using a list that can not be coerced to a data.frame
lisTreat &lt;- with(Treated,
                 list(conc1 = conc[1], conc.1 = conc[-1], rate = rate))

weighted.MM1 &lt;- function(resp, conc1, conc.1, Vm, K)
{
     conc &lt;- c(conc1, conc.1)
     pred &lt;- (Vm * conc)/(K + conc)
    (resp - pred) / sqrt(pred)
}
Pur.wt1 &lt;- nls( ~ weighted.MM1(rate, conc1, conc.1, Vm, K),
               data = lisTreat, start = list(Vm = 200, K = 0.1))
stopifnot(all.equal(coef(Pur.wt), coef(Pur.wt1)))

## Chambers and Hastie (1992) Statistical Models in S  (p. 537):
## If the value of the right side [of formula] has an attribute called
## 'gradient' this should be a matrix with the number of rows equal
## to the length of the response and one column for each parameter.

weighted.MM.grad &lt;- function(resp, conc1, conc.1, Vm, K)
{
  conc &lt;- c(conc1, conc.1)

  K.conc &lt;- K+conc
  dy.dV &lt;- conc/K.conc
  dy.dK &lt;- -Vm*dy.dV/K.conc
  pred &lt;- Vm*dy.dV
  pred.5 &lt;- sqrt(pred)
  dev &lt;- (resp - pred) / pred.5
  Ddev &lt;- -0.5*(resp+pred)/(pred.5*pred)
  attr(dev, "gradient") &lt;- Ddev * cbind(Vm = dy.dV, K = dy.dK)
  dev
}

Pur.wt.grad &lt;- nls( ~ weighted.MM.grad(rate, conc1, conc.1, Vm, K),
                   data = lisTreat, start = list(Vm = 200, K = 0.1))

rbind(coef(Pur.wt), coef(Pur.wt1), coef(Pur.wt.grad))

## In this example, there seems no advantage to providing the gradient.
## In other cases, there might be.


## The two examples below show that you can fit a model to
## artificial data with noise but not to artificial data
## without noise.
x &lt;- 1:10
y &lt;- 2*x + 3                            # perfect fit
## terminates in an error, because convergence cannot be confirmed:
try(nls(y ~ a + b*x, start = list(a = 0.12345, b = 0.54321)))
## adjusting the convergence test by adding 'scaleOffset' to its denominator RSS:
nls(y ~ a + b*x, start = list(a = 0.12345, b = 0.54321),
    control = list(scaleOffset = 1, printEval=TRUE))
## Alternatively jittering the "too exact" values, slightly:
set.seed(27)
yeps &lt;- y + rnorm(length(y), sd = 0.01) # added noise
nls(yeps ~ a + b*x, start = list(a = 0.12345, b = 0.54321))


## the nls() internal cheap guess for starting values can be sufficient:
x &lt;- -(1:100)/10
y &lt;- 100 + 10 * exp(x / 2) + rnorm(x)/10
nlmod &lt;- nls(y ~  Const + A * exp(B * x))

plot(x,y, main = "nls(*), data, true function and fit, n=100")
curve(100 + 10 * exp(x / 2), col = 4, add = TRUE)
lines(x, predict(nlmod), col = 2)


## Here, requiring close convergence, must use more accurate numerical differentiation,
## as this typically gives Error: "step factor .. reduced below 'minFactor' .."

try(nlm1 &lt;- update(nlmod, control = list(tol = 1e-7)))
o2 &lt;- options(digits = 10) # more accuracy for 'trace'
## central differencing works here typically (PR#18165: not converging on *some*):
ctr2 &lt;- nls.control(nDcentral=TRUE, tol = 8e-8, # &lt;- even smaller than above
   warnOnly =
        TRUE || # &lt;&lt; work around; e.g. needed on some ATLAS-Lapack setups
        (grepl("^aarch64.*linux", R.version$platform) &amp;&amp; grepl("^NixOS", osVersion)
              ))
(nlm2 &lt;- update(nlmod, control = ctr2, trace = TRUE)); options(o2)
## --&gt; convergence tolerance  4.997e-8 (in 11 iter.)


## The muscle dataset in MASS is from an experiment on muscle
## contraction on 21 animals.  The observed variables are Strip
## (identifier of muscle), Conc (Cacl concentration) and Length
## (resulting length of muscle section).

if(requireNamespace("MASS", quietly = TRUE)) withAutoprint({
## The non linear model considered is
##       Length = alpha + beta*exp(-Conc/theta) + error
## where theta is constant but alpha and beta may vary with Strip.

with(MASS::muscle, table(Strip)) # 2, 3 or 4 obs per strip

## We first use the plinear algorithm to fit an overall model,
## ignoring that alpha and beta might vary with Strip.
musc.1 &lt;- nls(Length ~ cbind(1, exp(-Conc/th)), MASS::muscle,
              start = list(th = 1), algorithm = "plinear")
summary(musc.1)

## Then we use nls' indexing feature for parameters in non-linear
## models to use the conventional algorithm to fit a model in which
## alpha and beta vary with Strip.  The starting values are provided
## by the previously fitted model.
## Note that with indexed parameters, the starting values must be
## given in a list (with names):
b &lt;- coef(musc.1)
musc.2 &lt;- nls(Length ~ a[Strip] + b[Strip]*exp(-Conc/th), MASS::muscle,
              start = list(a = rep(b[2], 21), b = rep(b[3], 21), th = b[1]))
summary(musc.2)
})


</code></pre>

<hr>
<h2 id='nls.control'>Control the Iterations in <code>nls</code></h2><span id='topic+nls.control'></span>

<h3>Description</h3>

<p>Allow the user to set some characteristics of the <code><a href="#topic+nls">nls</a></code>
nonlinear least squares algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nls.control(maxiter = 50, tol = 1e-05, minFactor = 1/1024,
            printEval = FALSE, warnOnly = FALSE, scaleOffset = 0,
            nDcentral = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nls.control_+3A_maxiter">maxiter</code></td>
<td>
<p>A positive integer specifying the maximum number of
iterations allowed.</p>
</td></tr>
<tr><td><code id="nls.control_+3A_tol">tol</code></td>
<td>
<p>A positive numeric value specifying the tolerance level for
the relative offset convergence criterion.</p>
</td></tr>
<tr><td><code id="nls.control_+3A_minfactor">minFactor</code></td>
<td>
<p>A positive numeric value specifying the minimum
step-size factor allowed on any step in the iteration.  The
increment is calculated with a Gauss-Newton algorithm and
successively halved until the residual sum of squares has been
decreased or until the step-size factor has been reduced below this
limit.</p>
</td></tr>
<tr><td><code id="nls.control_+3A_printeval">printEval</code></td>
<td>
<p>a logical specifying whether the number of evaluations
(steps in the gradient direction taken each iteration) is printed.</p>
</td></tr>
<tr><td><code id="nls.control_+3A_warnonly">warnOnly</code></td>
<td>
<p>a logical specifying whether <code><a href="#topic+nls">nls</a>()</code> should
return instead of signalling an error in the case of termination
before convergence.
Termination before convergence happens upon completion of <code>maxiter</code>
iterations, in the case of a singular gradient, and in the case that the
step-size factor is reduced below <code>minFactor</code>.</p>
</td></tr>
<tr><td><code id="nls.control_+3A_scaleoffset">scaleOffset</code></td>
<td>
<p>a constant to be added to the denominator of the relative
offset convergence criterion calculation to avoid a zero divide in the case
where the fit of a model to data is very close.  The default value of
<code>0</code> keeps the legacy behaviour of <code>nls()</code>.  A value such as
<code>1</code> seems to work for problems of reasonable scale with very small
residuals.</p>
</td></tr>
<tr><td><code id="nls.control_+3A_ndcentral">nDcentral</code></td>
<td>
<p>only when <em>numerical</em> derivatives are used:
<code><a href="base.html#topic+logical">logical</a></code> indicating if <em>central</em> differences
should be employed, i.e., <code><a href="#topic+numericDeriv">numericDeriv</a>(*, central=TRUE)</code>
be used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="base.html#topic+list">list</a></code> with components
</p>
<table>
<tr><td><code>maxiter</code></td>
<td>
</td></tr>
<tr><td><code>tol</code></td>
<td>
</td></tr>
<tr><td><code>minFactor</code></td>
<td>
</td></tr>
<tr><td><code>printEval</code></td>
<td>
</td></tr>
<tr><td><code>warnOnly</code></td>
<td>
</td></tr>
<tr><td><code>scaleOffset</code></td>
<td>
</td></tr>
<tr><td><code>nDcentreal</code></td>
<td>
</td></tr>
</table>
<p>with meanings as explained under &lsquo;Arguments&rsquo;.
</p>


<h3>Author(s)</h3>

<p>Douglas Bates and Saikat DebRoy; John C. Nash for part of the
<code>scaleOffset</code> option.</p>


<h3>References</h3>

<p>Bates, D. M. and Watts, D. G. (1988),
<em>Nonlinear Regression Analysis and Its Applications</em>, Wiley.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nls">nls</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nls.control(minFactor = 1/2048)
</code></pre>

<hr>
<h2 id='NLSstAsymptotic'>Fit the Asymptotic Regression Model</h2><span id='topic+NLSstAsymptotic'></span><span id='topic+NLSstAsymptotic.sortedXyData'></span>

<h3>Description</h3>

<p>Fits the asymptotic regression model, in the form <code>b0 +
      b1*(1-exp(-exp(lrc) * x))</code> to the <code>xy</code> data.
This can be used as a building block in determining starting estimates
for more complicated models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NLSstAsymptotic(xy)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NLSstAsymptotic_+3A_xy">xy</code></td>
<td>
<p>a <code>sortedXyData</code> object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric value of length 3 with components labelled <code>b0</code>,
<code>b1</code>, and <code>lrc</code>.  <code>b0</code> is the estimated intercept on
the <code>y</code>-axis, <code>b1</code> is the estimated difference between the
asymptote and the <code>y</code>-intercept, and <code>lrc</code> is the estimated
logarithm of the rate constant.
</p>


<h3>Author(s)</h3>

<p>Jos Pinheiro and Douglas Bates</p>


<h3>See Also</h3>

<p><code><a href="#topic+SSasymp">SSasymp</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>Lob.329 &lt;- Loblolly[ Loblolly$Seed == "329", ]
print(NLSstAsymptotic(sortedXyData(expression(age),
                                   expression(height),
                                   Lob.329)), digits = 3)
</code></pre>

<hr>
<h2 id='NLSstClosestX'>Inverse Interpolation</h2><span id='topic+NLSstClosestX'></span><span id='topic+NLSstClosestX.sortedXyData'></span>

<h3>Description</h3>

<p>Use inverse linear interpolation to approximate the <code>x</code> value at
which the function represented by <code>xy</code> is equal to <code>yval</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NLSstClosestX(xy, yval)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NLSstClosestX_+3A_xy">xy</code></td>
<td>
<p>a <code>sortedXyData</code> object</p>
</td></tr>
<tr><td><code id="NLSstClosestX_+3A_yval">yval</code></td>
<td>
<p>a numeric value on the <code>y</code> scale</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single numeric value on the <code>x</code> scale.
</p>


<h3>Author(s)</h3>

<p>Jos Pinheiro and Douglas Bates</p>


<h3>See Also</h3>

<p><code><a href="#topic+sortedXyData">sortedXyData</a></code>, <code><a href="#topic+NLSstLfAsymptote">NLSstLfAsymptote</a></code>,
<code><a href="#topic+NLSstRtAsymptote">NLSstRtAsymptote</a></code>, <code><a href="#topic+selfStart">selfStart</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>DNase.2 &lt;- DNase[ DNase$Run == "2", ]
DN.srt &lt;- sortedXyData(expression(log(conc)), expression(density), DNase.2)
NLSstClosestX(DN.srt, 1.0)
</code></pre>

<hr>
<h2 id='NLSstLfAsymptote'>Horizontal Asymptote on the Left Side</h2><span id='topic+NLSstLfAsymptote'></span><span id='topic+NLSstLfAsymptote.sortedXyData'></span>

<h3>Description</h3>

<p>Provide an initial guess at the horizontal asymptote on the left side
(i.e., small values of <code>x</code>) of the graph of <code>y</code> versus
<code>x</code> from the <code>xy</code> object.  Primarily used within
<code>initial</code> functions for self-starting nonlinear regression
models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NLSstLfAsymptote(xy)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NLSstLfAsymptote_+3A_xy">xy</code></td>
<td>
<p>a <code>sortedXyData</code> object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single numeric value estimating the horizontal asymptote for small
<code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Jos Pinheiro and Douglas Bates</p>


<h3>See Also</h3>

<p><code><a href="#topic+sortedXyData">sortedXyData</a></code>,
<code><a href="#topic+NLSstClosestX">NLSstClosestX</a></code>,
<code><a href="#topic+NLSstRtAsymptote">NLSstRtAsymptote</a></code>,
<code><a href="#topic+selfStart">selfStart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>DNase.2 &lt;- DNase[ DNase$Run == "2", ]
DN.srt &lt;- sortedXyData( expression(log(conc)), expression(density), DNase.2 )
NLSstLfAsymptote( DN.srt )
</code></pre>

<hr>
<h2 id='NLSstRtAsymptote'>Horizontal Asymptote on the Right Side</h2><span id='topic+NLSstRtAsymptote'></span><span id='topic+NLSstRtAsymptote.sortedXyData'></span>

<h3>Description</h3>

<p>Provide an initial guess at the horizontal asymptote on the right side
(i.e., large values of <code>x</code>) of the graph of <code>y</code> versus
<code>x</code> from the <code>xy</code> object.  Primarily used within
<code>initial</code> functions for self-starting nonlinear regression
models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NLSstRtAsymptote(xy)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NLSstRtAsymptote_+3A_xy">xy</code></td>
<td>
<p>a <code>sortedXyData</code> object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single numeric value estimating the horizontal asymptote for large
<code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Jos Pinheiro and Douglas Bates</p>


<h3>See Also</h3>

<p><code><a href="#topic+sortedXyData">sortedXyData</a></code>,
<code><a href="#topic+NLSstClosestX">NLSstClosestX</a></code>,
<code><a href="#topic+NLSstRtAsymptote">NLSstRtAsymptote</a></code>,
<code><a href="#topic+selfStart">selfStart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>DNase.2 &lt;- DNase[ DNase$Run == "2", ]
DN.srt &lt;- sortedXyData( expression(log(conc)), expression(density), DNase.2 )
NLSstRtAsymptote( DN.srt )
</code></pre>

<hr>
<h2 id='nobs'>
Extract the Number of Observations from a Fit
</h2><span id='topic+nobs'></span><span id='topic+nobs.default'></span>

<h3>Description</h3>

<p>Extract the number of &lsquo;observations&rsquo; from a model fit.  This is
principally intended to be used in computing BIC (see <code><a href="#topic+AIC">AIC</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nobs(object, ...)

## Default S3 method:
nobs(object, use.fallback = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nobs_+3A_object">object</code></td>
<td>
<p>a fitted model object.</p>
</td></tr>
<tr><td><code id="nobs_+3A_use.fallback">use.fallback</code></td>
<td>
<p>logical: should fallback methods be used to try to
guess the value?</p>
</td></tr>
<tr><td><code id="nobs_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function, with an S4 generic in package <span class="pkg">stats4</span>.
There are methods in this package for objects of classes
<code>"<a href="#topic+lm">lm</a>"</code>, <code>"<a href="#topic+glm">glm</a>"</code>, <code>"<a href="#topic+nls">nls</a>"</code> and
<code>"<a href="#topic+logLik">logLik</a>"</code>, as well as a default method (which throws an
error, unless <code>use.fallback = TRUE</code> when it looks for
<code>weights</code> and <code>residuals</code> components &ndash; use with care!).
</p>
<p>The main usage is in determining the appropriate penalty for BIC, but
<code>nobs</code> is also used by the stepwise fitting methods
<code><a href="#topic+step">step</a></code>, <code><a href="#topic+add1">add1</a></code> and <code><a href="#topic+drop1">drop1</a></code> as a
quick check that different fits have been fitted to the same set of
data (and not, say, that further rows have been dropped because of NAs
in the new predictors).
</p>
<p>For <code>lm</code>, <code>glm</code> and <code>nls</code> fits, observations with zero
weight are not included.
</p>


<h3>Value</h3>

<p>A single number, normally an integer.  Could be <code>NA</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+AIC">AIC</a></code>.
</p>

<hr>
<h2 id='Normal'>The Normal Distribution</h2><span id='topic+Normal'></span><span id='topic+dnorm'></span><span id='topic+pnorm'></span><span id='topic+qnorm'></span><span id='topic+rnorm'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the normal distribution with mean equal to <code>mean</code>
and standard deviation equal to <code>sd</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dnorm(x, mean = 0, sd = 1, log = FALSE)
pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
rnorm(n, mean = 0, sd = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Normal_+3A_x">x</code>, <code id="Normal_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="Normal_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="Normal_+3A_n">n</code></td>
<td>
<p>number of observations. If <code>length(n) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="Normal_+3A_mean">mean</code></td>
<td>
<p>vector of means.</p>
</td></tr>
<tr><td><code id="Normal_+3A_sd">sd</code></td>
<td>
<p>vector of standard deviations.</p>
</td></tr>
<tr><td><code id="Normal_+3A_log">log</code>, <code id="Normal_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="Normal_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code> otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>mean</code> or <code>sd</code> are not specified they assume the default
values of <code>0</code> and <code>1</code>, respectively.
</p>
<p>The normal distribution has density
</p>
<p style="text-align: center;"><code class="reqn">
    f(x) =
    \frac{1}{\sqrt{2\pi}\sigma} e^{-(x-\mu)^2/2\sigma^2}</code>
</p>

<p>where <code class="reqn">\mu</code> is the mean of the distribution and
<code class="reqn">\sigma</code> the standard deviation.
</p>


<h3>Value</h3>

<p><code>dnorm</code> gives the density,
<code>pnorm</code> gives the distribution function,
<code>qnorm</code> gives the quantile function, and
<code>rnorm</code> generates random deviates.
</p>
<p>The length of the result is determined by <code>n</code> for
<code>rnorm</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.
</p>
<p>The numerical arguments other than <code>n</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.
</p>
<p>For <code>sd = 0</code> this gives the limit as <code>sd</code> decreases to 0, a
point mass at <code>mu</code>.
<code>sd &lt; 0</code> is an error and returns <code>NaN</code>.
</p>


<h3>Source</h3>

<p>For <code>pnorm</code>, based on
</p>
<p>Cody, W. D. (1993)
Algorithm 715: SPECFUN &ndash; A portable FORTRAN package of special
function routines and test drivers.
<em>ACM Transactions on Mathematical Software</em> <b>19</b>, 22&ndash;32.
</p>
<p>For <code>qnorm</code>, the code is based on a C translation of
</p>
<p>Wichura, M. J. (1988)
Algorithm AS 241: The percentage points of the normal distribution.
<em>Applied Statistics</em>, <b>37</b>, 477&ndash;484; <a href="https://doi.org/10.2307/2347330">doi:10.2307/2347330</a>.
</p>
<p>which provides precise results up to about 16 digits for
<code>log.p=FALSE</code>.  For log scale probabilities in the extreme tails,
since <span class="rlang"><b>R</b></span> version 4.1.0, extensively since 4.3.0, asymptotic expansions
are used which have been derived and explored in
</p>
<p>Maechler, M. (2022)
Asymptotic tail formulas for gaussian quantiles; <a href="https://CRAN.R-project.org/package=DPQ"><span class="pkg">DPQ</span></a> vignette
<a href="https://CRAN.R-project.org/package=DPQ/vignettes/qnorm-asymp.pdf">https://CRAN.R-project.org/package=DPQ/vignettes/qnorm-asymp.pdf</a>.
</p>
<p>For <code>rnorm</code>, see <a href="base.html#topic+RNG">RNG</a> for how to select the algorithm and
for references to the supplied methods.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
<em>Continuous Univariate Distributions</em>, volume 1, chapter 13.
Wiley, New York.
</p>


<h3>See Also</h3>

<p><a href="#topic+Distributions">Distributions</a> for other standard distributions, including
<code><a href="#topic+dlnorm">dlnorm</a></code> for the <em>Log</em>normal distribution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

dnorm(0) == 1/sqrt(2*pi)
dnorm(1) == exp(-1/2)/sqrt(2*pi)
dnorm(1) == 1/sqrt(2*pi*exp(1))

## Using "log = TRUE" for an extended range :
par(mfrow = c(2,1))
plot(function(x) dnorm(x, log = TRUE), -60, 50,
     main = "log { Normal density }")
curve(log(dnorm(x)), add = TRUE, col = "red", lwd = 2)
mtext("dnorm(x, log=TRUE)", adj = 0)
mtext("log(dnorm(x))", col = "red", adj = 1)

plot(function(x) pnorm(x, log.p = TRUE), -50, 10,
     main = "log { Normal Cumulative }")
curve(log(pnorm(x)), add = TRUE, col = "red", lwd = 2)
mtext("pnorm(x, log=TRUE)", adj = 0)
mtext("log(pnorm(x))", col = "red", adj = 1)

## if you want the so-called 'error function'
erf &lt;- function(x) 2 * pnorm(x * sqrt(2)) - 1
## (see Abramowitz and Stegun 29.2.29)
## and the so-called 'complementary error function'
erfc &lt;- function(x) 2 * pnorm(x * sqrt(2), lower = FALSE)
## and the inverses
erfinv &lt;- function (x) qnorm((1 + x)/2)/sqrt(2)
erfcinv &lt;- function (x) qnorm(x/2, lower = FALSE)/sqrt(2)
</code></pre>

<hr>
<h2 id='numericDeriv'>Evaluate Derivatives Numerically</h2><span id='topic+numericDeriv'></span>

<h3>Description</h3>

<p><code>numericDeriv</code> numerically evaluates the gradient of an expression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>numericDeriv(expr, theta, rho = parent.frame(), dir = 1,
             eps = .Machine$double.eps ^ (1/if(central) 3 else 2), central = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="numericDeriv_+3A_expr">expr</code></td>
<td>
<p><code><a href="base.html#topic+expression">expression</a></code> or <code><a href="base.html#topic+call">call</a></code> to be
differentiated.  Should evaluate to a <code><a href="base.html#topic+numeric">numeric</a></code> vector.</p>
</td></tr>
<tr><td><code id="numericDeriv_+3A_theta">theta</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> vector of names of numeric variables
used in <code>expr</code>.</p>
</td></tr>
<tr><td><code id="numericDeriv_+3A_rho">rho</code></td>
<td>
<p><code><a href="base.html#topic+environment">environment</a></code> containing all the variables needed to
evaluate <code>expr</code>.</p>
</td></tr>
<tr><td><code id="numericDeriv_+3A_dir">dir</code></td>
<td>
<p>numeric vector of directions, typically with values in
<code>-1, 1</code> to use for the finite differences;
will be recycled to the length of <code>theta</code>.</p>
</td></tr>
<tr><td><code id="numericDeriv_+3A_eps">eps</code></td>
<td>
<p>a positive number, to be used as unit step size <code class="reqn">h</code> for
the approximate numerical derivative <code class="reqn"> (f(x+h)-f(x))/h </code> or the
central version, see <code>central</code>.</p>
</td></tr>
<tr><td><code id="numericDeriv_+3A_central">central</code></td>
<td>
<p>logical indicating if <em>central</em> divided differences
should be computed, i.e., <code class="reqn"> (f(x+h) - f(x-h)) / 2h </code>.  These are
typically more accurate but need more evaluations of <code class="reqn">f()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a front end to the C function <code>numeric_deriv</code>, which is
described in <em>Writing R Extensions</em>.
</p>
<p>The numeric variables must be of type <code>double</code> and not <code>integer</code>.

</p>


<h3>Value</h3>

<p>The value of <code>eval(expr, envir = rho)</code> plus a matrix
attribute <code>"gradient"</code>.  The columns of this matrix are
the derivatives of the value with respect to the variables listed in
<code>theta</code>.
</p>


<h3>Author(s)</h3>

<p>Saikat DebRoy <a href="mailto:saikat@stat.wisc.edu">saikat@stat.wisc.edu</a>;
tweaks and <code>eps</code>, <code>central</code> options by R Core Team.</p>


<h3>Examples</h3>

<pre><code class='language-R'>myenv &lt;- new.env()
myenv$mean &lt;- 0.
myenv$sd   &lt;- 1.
myenv$x    &lt;- seq(-3., 3., length.out = 31)
nD &lt;- numericDeriv(quote(pnorm(x, mean, sd)), c("mean", "sd"), myenv)
str(nD)

## Visualize :
require(graphics)
matplot(myenv$x, cbind(c(nD), attr(nD, "gradient")), type="l")
abline(h=0, lty=3)
## "gradient" is close to the true derivatives, you don't see any diff.:
curve( - dnorm(x), col=2, lty=3, lwd=2, add=TRUE)
curve(-x*dnorm(x), col=3, lty=3, lwd=2, add=TRUE)
##
# shows 1.609e-8 on most platforms
all.equal(attr(nD,"gradient"),
          with(myenv, cbind(-dnorm(x), -x*dnorm(x))))

</code></pre>

<hr>
<h2 id='offset'>Include an Offset in a Model Formula</h2><span id='topic+offset'></span>

<h3>Description</h3>

<p>An offset is a term to be added to a linear predictor, such as in a
generalised linear model, with known coefficient 1 rather than an
estimated coefficient.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>offset(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="offset_+3A_object">object</code></td>
<td>
<p>An offset to be included in a model frame</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There can be more than one offset in a model formula, but <code>-</code> is
not supported for <code>offset</code> terms (and is equivalent to <code>+</code>).
</p>


<h3>Value</h3>

<p>The input value.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+model.offset">model.offset</a></code>, <code><a href="#topic+model.frame">model.frame</a></code>.
</p>
<p>For examples see <code><a href="#topic+glm">glm</a></code> and
<code><a href="MASS.html#topic+Insurance">Insurance</a></code> in package <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a>.
</p>

<hr>
<h2 id='oneway.test'>Test for Equal Means in a One-Way Layout</h2><span id='topic+oneway.test'></span>

<h3>Description</h3>

<p>Test whether two or more samples from normal distributions have the
same means.  The variances are not necessarily assumed to be equal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>oneway.test(formula, data, subset, na.action, var.equal = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="oneway.test_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>lhs ~ rhs</code> where <code>lhs</code>
gives the sample values and <code>rhs</code> the corresponding groups.</p>
</td></tr>
<tr><td><code id="oneway.test_+3A_data">data</code></td>
<td>
<p>an optional matrix or data frame (or similar: see
<code><a href="#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="oneway.test_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="oneway.test_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.  Defaults to
<code>getOption("na.action")</code>.</p>
</td></tr>
<tr><td><code id="oneway.test_+3A_var.equal">var.equal</code></td>
<td>
<p>a logical variable indicating whether to treat the
variances in the samples as equal.  If <code>TRUE</code>, then a simple F
test for the equality of means in a one-way analysis of variance is
performed.  If <code>FALSE</code>, an approximate method of
Welch (1951)
is used, which generalizes the commonly known 2-sample Welch test to
the case of arbitrarily many samples.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the right-hand side of the formula contains more than one term,
their interaction is taken to form the grouping.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of freedom of the exact or approximate F
distribution of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string indicating the test performed.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the names of the data.</p>
</td></tr>
</table>


<h3>References</h3>

<p>B. L. Welch (1951).
On the comparison of several mean values: an alternative approach.
<em>Biometrika</em>, <b>38</b>, 330&ndash;336.
<a href="https://doi.org/10.2307/2332579">doi:10.2307/2332579</a>.
</p>


<h3>See Also</h3>

<p>The standard t test (<code><a href="#topic+t.test">t.test</a></code>) as the special case for two
samples;
the Kruskal-Wallis test <code><a href="#topic+kruskal.test">kruskal.test</a></code> for a nonparametric
test for equal location parameters in a one-way layout.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not assuming equal variances
oneway.test(extra ~ group, data = sleep)
## Assuming equal variances
oneway.test(extra ~ group, data = sleep, var.equal = TRUE)
## which gives the same result as
anova(lm(extra ~ group, data = sleep))
</code></pre>

<hr>
<h2 id='optim'>General-purpose Optimization</h2><span id='topic+optim'></span><span id='topic+optimHess'></span>

<h3>Description</h3>

<p>General-purpose optimization based on Nelder&ndash;Mead, quasi-Newton and
conjugate-gradient algorithms. It includes an option for
box-constrained optimization and simulated annealing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim(par, fn, gr = NULL, ...,
      method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN",
                 "Brent"),
      lower = -Inf, upper = Inf,
      control = list(), hessian = FALSE)

optimHess(par, fn, gr = NULL, ..., control = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim_+3A_par">par</code></td>
<td>
<p>Initial values for the parameters to be optimized over.</p>
</td></tr>
<tr><td><code id="optim_+3A_fn">fn</code></td>
<td>
<p>A function to be minimized (or maximized), with first
argument the vector of parameters over which minimization is to take
place.  It should return a scalar result.</p>
</td></tr>
<tr><td><code id="optim_+3A_gr">gr</code></td>
<td>
<p>A function to return the gradient for the <code>"BFGS"</code>,
<code>"CG"</code> and <code>"L-BFGS-B"</code> methods.  If it is <code>NULL</code>, a
finite-difference approximation will be used.
</p>
<p>For the <code>"SANN"</code> method it specifies a function to generate a new
candidate point.  If it is <code>NULL</code> a default Gaussian Markov
kernel is used.</p>
</td></tr>
<tr><td><code id="optim_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code>fn</code> and <code>gr</code>.</p>
</td></tr>
<tr><td><code id="optim_+3A_method">method</code></td>
<td>
<p>The method to be used. See &lsquo;Details&rsquo;.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="optim_+3A_lower">lower</code>, <code id="optim_+3A_upper">upper</code></td>
<td>
<p>Bounds on the variables for the <code>"L-BFGS-B"</code>
method, or bounds in which to <em>search</em> for method <code>"Brent"</code>.</p>
</td></tr>
<tr><td><code id="optim_+3A_control">control</code></td>
<td>
<p>a <code><a href="base.html#topic+list">list</a></code> of control parameters.  See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="optim_+3A_hessian">hessian</code></td>
<td>
<p>Logical. Should a numerically differentiated Hessian
matrix be returned?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that arguments after <code>...</code> must be matched exactly.
</p>
<p>By default <code>optim</code> performs minimization, but it will maximize
if <code>control$fnscale</code> is negative.  <code>optimHess</code> is an
auxiliary function to compute the Hessian at a later stage if
<code>hessian = TRUE</code> was forgotten.
</p>
<p>The default method is an implementation of that of
Nelder and Mead (1965),
that uses only function values and is robust but relatively slow.
It will work reasonably well for non-differentiable functions.
</p>
<p>Method <code>"BFGS"</code> is a quasi-Newton method (also known as a variable
metric algorithm), specifically that published simultaneously in 1970
by Broyden, Fletcher, Goldfarb and Shanno.
This uses function values
and gradients to build up a picture of the surface to be optimized.
</p>
<p>Method <code>"CG"</code> is a conjugate gradients method based on that by
Fletcher and Reeves (1964) (but with the option of
Polak&ndash;Ribiere or Beale&ndash;Sorenson updates).
Conjugate gradient methods will generally
be more fragile than the BFGS method, but as they do not store a
matrix they may be successful in much larger optimization problems.
</p>
<p>Method <code>"L-BFGS-B"</code> is that of Byrd <abbr>et al.</abbr> (1995) which
allows <em>box constraints</em>, that is each variable can be given a lower
and/or upper bound. The initial value must satisfy the constraints.
This uses a limited-memory modification of the BFGS quasi-Newton
method.  If non-trivial bounds are supplied, this method will be
selected, with a warning.
</p>
<p>Nocedal and Wright (1999) is a comprehensive reference for the
previous three methods.
</p>
<p>Method <code>"SANN"</code> is by default a variant of simulated annealing
given in Belisle (1992). Simulated-annealing belongs to the class of
stochastic global optimization methods. It uses only function values
but is relatively slow. It will also work for non-differentiable
functions. This implementation uses the Metropolis function for the
acceptance probability. By default the next candidate point is
generated from a Gaussian Markov kernel with scale proportional to the
actual temperature. If a function to generate a new candidate point is
given, method <code>"SANN"</code> can also be used to solve combinatorial
optimization problems. Temperatures are decreased according to the
logarithmic cooling schedule as given in
Belisle (1992, p. 890);
specifically, the temperature is set to
<code>temp / log(((t-1) %/% tmax)*tmax + exp(1))</code>, where <code>t</code> is
the current iteration step and <code>temp</code> and <code>tmax</code> are
specifiable via <code>control</code>, see below.  Note that the
<code>"SANN"</code> method depends critically on the settings of the control
parameters. It is not a general-purpose method but can be very useful
in getting to a good value on a very rough surface.
</p>
<p>Method <code>"Brent"</code> is for one-dimensional problems only, using
<code><a href="#topic+optimize">optimize</a>()</code>.  It can be useful in cases where
<code>optim()</code> is used inside other functions where only <code>method</code>
can be specified, such as in <code><a href="#topic+mle">mle</a></code> from package <span class="pkg">stats4</span>.
</p>
<p>Function <code>fn</code> can return <code>NA</code> or <code>Inf</code> if the function
cannot be evaluated at the supplied value, but the initial value must
have a computable finite value of <code>fn</code>.
(Except for method <code>"L-BFGS-B"</code> where the values should always be
finite.)
</p>
<p><code>optim</code> can be used recursively, and for a single parameter
as well as many.  It also accepts a zero-length <code>par</code>, and just
evaluates the function with that argument.
</p>
<p>The <code>control</code> argument is a list that can supply any of the
following components:
</p>

<dl>
<dt><code>trace</code></dt><dd><p>Non-negative integer. If positive,
tracing information on the
progress of the optimization is produced. Higher values may
produce more tracing information: for method <code>"L-BFGS-B"</code>
there are six levels of tracing.  (To understand exactly what
these do see the source code: higher levels give more detail.)</p>
</dd>
<dt><code>fnscale</code></dt><dd><p>An overall scaling to be applied to the value
of <code>fn</code> and <code>gr</code> during optimization. If negative,
turns the problem into a maximization problem. Optimization is
performed on <code>fn(par)/fnscale</code>.</p>
</dd>
<dt><code>parscale</code></dt><dd><p>A vector of scaling values for the parameters.
Optimization is performed on <code>par/parscale</code> and these should be
comparable in the sense that a unit change in any element produces
about a unit change in the scaled value.  Not used (nor needed)
for <code>method = "Brent"</code>.</p>
</dd>
<dt><code>ndeps</code></dt><dd><p>A vector of step sizes for the finite-difference
approximation to the gradient, on <code>par/parscale</code>
scale. Defaults to <code>1e-3</code>.</p>
</dd>
<dt><code>maxit</code></dt><dd><p>The maximum number of iterations. Defaults to
<code>100</code> for the derivative-based methods, and
<code>500</code> for <code>"Nelder-Mead"</code>.
</p>
<p>For <code>"SANN"</code> <code>maxit</code> gives the total number of function
evaluations: there is no other stopping criterion. Defaults to
<code>10000</code>.
</p>
</dd>
<dt><code>abstol</code></dt><dd><p>The absolute convergence tolerance. Only
useful for non-negative functions, as a tolerance for reaching zero.</p>
</dd>
<dt><code>reltol</code></dt><dd><p>Relative convergence tolerance.  The algorithm
stops if it is unable to reduce the value by a factor of
<code>reltol * (abs(val) + reltol)</code> at a step.  Defaults to
<code>sqrt(.Machine$double.eps)</code>, typically about <code>1e-8</code>.</p>
</dd>
<dt><code>alpha</code>, <code>beta</code>, <code>gamma</code></dt><dd><p>Scaling parameters
for the <code>"Nelder-Mead"</code> method. <code>alpha</code> is the reflection
factor (default 1.0), <code>beta</code> the contraction factor (0.5) and
<code>gamma</code> the expansion factor (2.0).</p>
</dd>
<dt><code>REPORT</code></dt><dd><p>The frequency of reports for the <code>"BFGS"</code>,
<code>"L-BFGS-B"</code> and <code>"SANN"</code> methods if <code>control$trace</code>
is positive. Defaults to every 10 iterations for <code>"BFGS"</code> and
<code>"L-BFGS-B"</code>, or every 100 temperatures for <code>"SANN"</code>.</p>
</dd>
<dt><code>warn.1d.NelderMead</code></dt><dd><p>a <code><a href="base.html#topic+logical">logical</a></code> indicating
if the (default) <code>"Nelder-Mead"</code> method should signal a
warning when used for one-dimensional minimization.  As the
warning is sometimes inappropriate, you can suppress it by setting
this option to false.</p>
</dd>
<dt><code>type</code></dt><dd><p>for the conjugate-gradients method.  Takes value
<code>1</code> for the Fletcher&ndash;Reeves update, <code>2</code> for
Polak&ndash;Ribiere and <code>3</code> for Beale&ndash;Sorenson.</p>
</dd>
<dt><code>lmm</code></dt><dd><p>is an integer giving the number of BFGS updates
retained in the <code>"L-BFGS-B"</code> method, It defaults to <code>5</code>.</p>
</dd>
<dt><code>factr</code></dt><dd><p>controls the convergence of the <code>"L-BFGS-B"</code>
method. Convergence occurs when the reduction in the objective is
within this factor of the machine tolerance. Default is <code>1e7</code>,
that is a tolerance of about <code>1e-8</code>.</p>
</dd>
<dt><code>pgtol</code></dt><dd><p>helps control the convergence of the <code>"L-BFGS-B"</code>
method. It is a tolerance on the projected gradient in the current
search direction. This defaults to zero, when the check is
suppressed.</p>
</dd>
<dt><code>temp</code></dt><dd><p>controls the <code>"SANN"</code> method. It is the
starting temperature for the cooling schedule. Defaults to
<code>10</code>.</p>
</dd>
<dt><code>tmax</code></dt><dd><p>is the number of function evaluations at each
temperature for the <code>"SANN"</code> method. Defaults to <code>10</code>.</p>
</dd>
</dl>

<p>Any names given to <code>par</code> will be copied to the vectors passed to
<code>fn</code> and <code>gr</code>.  Note that no other attributes of <code>par</code>
are copied over.
</p>
<p>The parameter vector passed to <code>fn</code> has special semantics and may
be shared between calls: the function should not change or copy it.
</p>


<h3>Value</h3>

<p>For <code>optim</code>, a list with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>The best set of parameters found.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>The value of <code>fn</code> corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>A two-element integer vector giving the number of calls
to <code>fn</code> and <code>gr</code> respectively. This excludes those calls needed
to compute the Hessian, if requested, and any calls to <code>fn</code> to
compute a finite-difference approximation to the gradient.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>An integer code. <code>0</code> indicates successful
completion (which is always the case for <code>"SANN"</code> and
<code>"Brent"</code>).  Possible error codes are
</p>

<dl>
<dt><code>1</code></dt><dd><p>indicates that the iteration limit <code>maxit</code>
had been reached.</p>
</dd>
<dt><code>10</code></dt><dd><p>indicates degeneracy of the Nelder&ndash;Mead simplex.</p>
</dd>
<dt><code>51</code></dt><dd><p>indicates a warning from the <code>"L-BFGS-B"</code>
method; see component <code>message</code> for further details.</p>
</dd>
<dt><code>52</code></dt><dd><p>indicates an error from the <code>"L-BFGS-B"</code>
method; see component <code>message</code> for further details.</p>
</dd>
</dl>

</td></tr>
<tr><td><code>message</code></td>
<td>
<p>A character string giving any additional information
returned by the optimizer, or <code>NULL</code>.</p>
</td></tr>
<tr><td><code>hessian</code></td>
<td>
<p>Only if argument <code>hessian</code> is true. A symmetric
matrix giving an estimate of the Hessian at the solution found.  Note
that this is the Hessian of the unconstrained problem even if the
box constraints are active.</p>
</td></tr>
</table>
<p>For <code>optimHess</code>, the description of the <code>hessian</code> component
applies.
</p>


<h3>Note</h3>

<p><code>optim</code> will work with one-dimensional <code>par</code>s, but the
default method does not work well (and will warn).  Method
<code>"Brent"</code> uses <code><a href="#topic+optimize">optimize</a></code> and needs bounds to be available;
<code>"BFGS"</code> often works well enough if not.
</p>


<h3>Source</h3>

<p>The code for methods <code>"Nelder-Mead"</code>, <code>"BFGS"</code> and
<code>"CG"</code> was based originally on Pascal code in Nash (1990) that was
translated by <code>p2c</code> and then hand-optimized.  Dr Nash has agreed
that the code can be made freely available.
</p>
<p>The code for method <code>"L-BFGS-B"</code> is based on Fortran code by Zhu,
Byrd, Lu-Chen and Nocedal obtained from Netlib (file
&lsquo;<span class="file">opt/lbfgs_bcm.shar</span>&rsquo;: another version is in &lsquo;<span class="file">toms/778</span>&rsquo;).
</p>
<p>The code for method <code>"SANN"</code> was contributed by A. Trapletti.
</p>


<h3>References</h3>

<p>Belisle, C. J. P. (1992).
Convergence theorems for a class of simulated annealing algorithms on
<code class="reqn">R^d</code>.
<em>Journal of Applied Probability</em>, <b>29</b>, 885&ndash;895.
<a href="https://doi.org/10.2307/3214721">doi:10.2307/3214721</a>.
</p>
<p>Byrd, R. H., Lu, P., Nocedal, J. and Zhu, C. (1995).
A limited memory algorithm for bound constrained optimization.
<em>SIAM Journal on Scientific Computing</em>, <b>16</b>, 1190&ndash;1208.
<a href="https://doi.org/10.1137/0916069">doi:10.1137/0916069</a>.
</p>
<p>Fletcher, R. and Reeves, C. M. (1964).
Function minimization by conjugate gradients.
<em>Computer Journal</em> <b>7</b>, 148&ndash;154.
<a href="https://doi.org/10.1093/comjnl/7.2.149">doi:10.1093/comjnl/7.2.149</a>.
</p>
<p>Nash, J. C. (1990).
<em>Compact Numerical Methods for Computers. Linear Algebra and
Function Minimisation</em>.
Adam Hilger.
</p>
<p>Nelder, J. A. and Mead, R. (1965).
A simplex algorithm for function minimization.
<em>Computer Journal</em>, <b>7</b>, 308&ndash;313.
<a href="https://doi.org/10.1093/comjnl/7.4.308">doi:10.1093/comjnl/7.4.308</a>.
</p>
<p>Nocedal, J. and Wright, S. J. (1999).
<em>Numerical Optimization</em>.
Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nlm">nlm</a></code>, <code><a href="#topic+nlminb">nlminb</a></code>.
</p>
<p><code><a href="#topic+optimize">optimize</a></code> for one-dimensional minimization and
<code><a href="#topic+constrOptim">constrOptim</a></code> for constrained optimization.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require(graphics)

fr &lt;- function(x) {   ## Rosenbrock Banana function
    x1 &lt;- x[1]
    x2 &lt;- x[2]
    100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}
grr &lt;- function(x) { ## Gradient of 'fr'
    x1 &lt;- x[1]
    x2 &lt;- x[2]
    c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
       200 *      (x2 - x1 * x1))
}
optim(c(-1.2,1), fr)
(res &lt;- optim(c(-1.2,1), fr, grr, method = "BFGS"))
optimHess(res$par, fr, grr)
optim(c(-1.2,1), fr, NULL, method = "BFGS", hessian = TRUE)
## These do not converge in the default number of steps
optim(c(-1.2,1), fr, grr, method = "CG")
optim(c(-1.2,1), fr, grr, method = "CG", control = list(type = 2))
optim(c(-1.2,1), fr, grr, method = "L-BFGS-B")

flb &lt;- function(x)
    { p &lt;- length(x); sum(c(1, rep(4, p-1)) * (x - c(1, x[-p])^2)^2) }
## 25-dimensional box constrained
optim(rep(3, 25), flb, NULL, method = "L-BFGS-B",
      lower = rep(2, 25), upper = rep(4, 25)) # par[24] is *not* at boundary


## "wild" function , global minimum at about -15.81515
fw &lt;- function (x)
    10*sin(0.3*x)*sin(1.3*x^2) + 0.00001*x^4 + 0.2*x+80
plot(fw, -50, 50, n = 1000, main = "optim() minimising 'wild function'")

res &lt;- optim(50, fw, method = "SANN",
             control = list(maxit = 20000, temp = 20, parscale = 20))
res
## Now improve locally {typically only by a small bit}:
(r2 &lt;- optim(res$par, fw, method = "BFGS"))
points(r2$par,  r2$value,  pch = 8, col = "red", cex = 2)

## Combinatorial optimization: Traveling salesman problem
library(stats) # normally loaded

eurodistmat &lt;- as.matrix(eurodist)

distance &lt;- function(sq) {  # Target function
    sq2 &lt;- embed(sq, 2)
    sum(eurodistmat[cbind(sq2[,2], sq2[,1])])
}

genseq &lt;- function(sq) {  # Generate new candidate sequence
    idx &lt;- seq(2, NROW(eurodistmat)-1)
    changepoints &lt;- sample(idx, size = 2, replace = FALSE)
    tmp &lt;- sq[changepoints[1]]
    sq[changepoints[1]] &lt;- sq[changepoints[2]]
    sq[changepoints[2]] &lt;- tmp
    sq
}

sq &lt;- c(1:nrow(eurodistmat), 1)  # Initial sequence: alphabetic
distance(sq)
# rotate for conventional orientation
loc &lt;- -cmdscale(eurodist, add = TRUE)$points
x &lt;- loc[,1]; y &lt;- loc[,2]
s &lt;- seq_len(nrow(eurodistmat))
tspinit &lt;- loc[sq,]

plot(x, y, type = "n", asp = 1, xlab = "", ylab = "",
     main = "initial solution of traveling salesman problem", axes = FALSE)
arrows(tspinit[s,1], tspinit[s,2], tspinit[s+1,1], tspinit[s+1,2],
       angle = 10, col = "green")
text(x, y, labels(eurodist), cex = 0.8)

set.seed(123) # chosen to get a good soln relatively quickly
res &lt;- optim(sq, distance, genseq, method = "SANN",
             control = list(maxit = 30000, temp = 2000, trace = TRUE,
                            REPORT = 500))
res  # Near optimum distance around 12842

tspres &lt;- loc[res$par,]
plot(x, y, type = "n", asp = 1, xlab = "", ylab = "",
     main = "optim() 'solving' traveling salesman problem", axes = FALSE)
arrows(tspres[s,1], tspres[s,2], tspres[s+1,1], tspres[s+1,2],
       angle = 10, col = "red")
text(x, y, labels(eurodist), cex = 0.8)

## 1-D minimization: "Brent" or optimize() being preferred.. but NM may be ok and "unavoidable",
## ----------------   so we can suppress the check+warning :
system.time(rO &lt;- optimize(function(x) (x-pi)^2, c(0, 10)))
system.time(ro &lt;- optim(1, function(x) (x-pi)^2, control=list(warn.1d.NelderMead = FALSE)))
rO$minimum - pi # 0 (perfect), on one platform
ro$par - pi     # ~= 1.9e-4    on one platform
utils::str(ro)
</code></pre>

<hr>
<h2 id='optimize'>One Dimensional Optimization</h2><span id='topic+optimize'></span><span id='topic+optimise'></span>

<h3>Description</h3>

<p>The function <code>optimize</code> searches the interval from
<code>lower</code> to <code>upper</code> for a minimum or maximum of
the function <code>f</code> with respect to its first argument.
</p>
<p><code>optimise</code> is an alias for <code>optimize</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimize(f, interval, ..., lower = min(interval), upper = max(interval),
         maximum = FALSE,
         tol = .Machine$double.eps^0.25)
optimise(f, interval, ..., lower = min(interval), upper = max(interval),
         maximum = FALSE,
         tol = .Machine$double.eps^0.25)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimize_+3A_f">f</code></td>
<td>
<p>the function to be optimized.  The function is
either minimized or maximized over its first argument
depending on the value of <code>maximum</code>.</p>
</td></tr>
<tr><td><code id="optimize_+3A_interval">interval</code></td>
<td>
<p>a vector containing the end-points of the interval
to be searched for the minimum.</p>
</td></tr>
<tr><td><code id="optimize_+3A_...">...</code></td>
<td>
<p>additional named or unnamed arguments to be passed
to <code>f</code>.</p>
</td></tr>
<tr><td><code id="optimize_+3A_lower">lower</code></td>
<td>
<p>the lower end point of the interval
to be searched.</p>
</td></tr>
<tr><td><code id="optimize_+3A_upper">upper</code></td>
<td>
<p>the upper end point of the interval
to be searched.</p>
</td></tr>
<tr><td><code id="optimize_+3A_maximum">maximum</code></td>
<td>
<p>logical.  Should we maximize or minimize (the default)?</p>
</td></tr>
<tr><td><code id="optimize_+3A_tol">tol</code></td>
<td>
<p>the desired accuracy.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that arguments after <code>...</code> must be matched exactly.
</p>
<p>The method used is a combination of golden section search and
successive parabolic interpolation, and was designed for use with
continuous functions.  Convergence is never much slower
than that for a Fibonacci search.  If <code>f</code> has a continuous second
derivative which is positive at the minimum (which is not at <code>lower</code> or
<code>upper</code>), then convergence is superlinear, and usually of the
order of about 1.324.
</p>
<p>The function <code>f</code> is never evaluated at two points closer together
than <code class="reqn">\epsilon</code><code class="reqn"> |x_0| + (tol/3)</code>, where
<code class="reqn">\epsilon</code> is approximately <code>sqrt(<a href="base.html#topic+.Machine">.Machine</a>$double.eps)</code>
and <code class="reqn">x_0</code> is the final abscissa <code>optimize()$minimum</code>.<br />
If <code>f</code> is a unimodal function and the computed values of <code>f</code>
are always unimodal when separated by at least <code class="reqn">\epsilon</code>
<code class="reqn"> |x| + (tol/3)</code>, then <code class="reqn">x_0</code> approximates the abscissa of the
global minimum of <code>f</code> on the interval <code>lower,upper</code> with an
error less than <code class="reqn">\epsilon</code><code class="reqn"> |x_0|+ tol</code>.<br />
If <code>f</code> is not unimodal, then <code>optimize()</code> may approximate a
local, but perhaps non-global, minimum to the same accuracy.
</p>
<p>The first evaluation of <code>f</code> is always at
<code class="reqn">x_1 = a + (1-\phi)(b-a)</code> where <code>(a,b) = (lower, upper)</code> and
<code class="reqn">\phi = (\sqrt 5 - 1)/2 = 0.61803..</code>
is the golden section ratio.
Almost always, the second evaluation is at
<code class="reqn">x_2 = a + \phi(b-a)</code>.
Note that a local minimum inside <code class="reqn">[x_1,x_2]</code> will be found as
solution, even when <code>f</code> is constant in there, see the last
example.
</p>
<p><code>f</code> will be called as <code>f(<var>x</var>, ...)</code> for a numeric value
of <var>x</var>.
</p>
<p>The argument passed to <code>f</code> has special semantics and used to be
shared between calls.  The function should not copy it.
</p>


<h3>Value</h3>

<p>A list with components <code>minimum</code> (or <code>maximum</code>)
and <code>objective</code> which give the location of the minimum (or maximum)
and the value of the function at that point.
</p>


<h3>Source</h3>

<p>A C translation of Fortran code <a href="https://netlib.org/fmm/fmin.f">https://netlib.org/fmm/fmin.f</a>
(author(s) unstated)
based on the Algol 60 procedure <code>localmin</code> given in the reference.
</p>


<h3>References</h3>

<p>Brent, R. (1973)
<em>Algorithms for Minimization without Derivatives.</em>
Englewood Cliffs N.J.: Prentice-Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nlm">nlm</a></code>, <code><a href="#topic+uniroot">uniroot</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

f &lt;- function (x, a) (x - a)^2
xmin &lt;- optimize(f, c(0, 1), tol = 0.0001, a = 1/3)
xmin

## See where the function is evaluated:
optimize(function(x) x^2*(print(x)-1), lower = 0, upper = 10)

## "wrong" solution with unlucky interval and piecewise constant f():
f  &lt;- function(x) ifelse(x &gt; -1, ifelse(x &lt; 4, exp(-1/abs(x - 1)), 10), 10)
fp &lt;- function(x) { print(x); f(x) }

plot(f, -2,5, ylim = 0:1, col = 2)
optimize(fp, c(-4, 20))   # doesn't see the minimum
optimize(fp, c(-7, 20))   # ok
</code></pre>

<hr>
<h2 id='order.dendrogram'>Ordering or Labels of the Leaves in a Dendrogram</h2><span id='topic+order.dendrogram'></span><span id='topic+labels.dendrogram'></span>

<h3>Description</h3>

<p>Theses functions return the order (index) or the <code>"label"</code>
attribute for the leaves in a
dendrogram.  These indices can then be used to access the appropriate
components of any additional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>order.dendrogram(x)

## S3 method for class 'dendrogram'
labels(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="order.dendrogram_+3A_x">x</code>, <code id="order.dendrogram_+3A_object">object</code></td>
<td>
<p>a dendrogram (see <code><a href="#topic+as.dendrogram">as.dendrogram</a></code>).</p>
</td></tr>
<tr><td><code id="order.dendrogram_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The indices or labels for the leaves in left to right order are retrieved.
</p>


<h3>Value</h3>

<p>A vector with length equal to the number of leaves in the dendrogram
is returned.  From <code>r &lt;- order.dendrogram()</code>, each element is the
index into the original data (from which the dendrogram was computed).
</p>


<h3>Author(s)</h3>

<p>R. Gentleman (<code>order.dendrogram</code>) and Martin Maechler
(<code>labels.dendrogram</code>).</p>


<h3>See Also</h3>

<p><code><a href="#topic+reorder">reorder</a></code>, <code><a href="#topic+dendrogram">dendrogram</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
x &lt;- rnorm(10)
hc &lt;- hclust(dist(x))
hc$order
dd &lt;- as.dendrogram(hc)
order.dendrogram(dd) ## the same :
stopifnot(hc$order == order.dendrogram(dd))

d2 &lt;- as.dendrogram(hclust(dist(USArrests)))
labels(d2) ## in this case the same as
stopifnot(identical(labels(d2),
   rownames(USArrests)[order.dendrogram(d2)]))
</code></pre>

<hr>
<h2 id='p.adjust'>Adjust P-values for Multiple Comparisons</h2><span id='topic+p.adjust'></span><span id='topic+p.adjust.methods'></span>

<h3>Description</h3>

<p>Given a set of p-values, returns p-values adjusted using
one of several methods.</p>


<h3>Usage</h3>

<pre><code class='language-R'>p.adjust(p, method = p.adjust.methods, n = length(p))

p.adjust.methods
# c("holm", "hochberg", "hommel", "bonferroni", "BH", "BY",
#   "fdr", "none")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="p.adjust_+3A_p">p</code></td>
<td>
<p>numeric vector of p-values (possibly with
<code><a href="base.html#topic+NA">NA</a></code>s).  Any other <span class="rlang"><b>R</b></span> object is coerced by
<code><a href="base.html#topic+as.numeric">as.numeric</a></code>.</p>
</td></tr>
<tr><td><code id="p.adjust_+3A_method">method</code></td>
<td>
<p>correction method, a <code><a href="base.html#topic+character">character</a></code> string.
Can be abbreviated.</p>
</td></tr>
<tr><td><code id="p.adjust_+3A_n">n</code></td>
<td>
<p>number of comparisons, must be at least <code>length(p)</code>;
only set this (to non-default) when you know what you are doing!</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The adjustment methods include the Bonferroni correction
(<code>"bonferroni"</code>) in which the p-values are multiplied by the
number of comparisons.  Less conservative corrections are also
included by
Holm (1979) (<code>"holm"</code>),
Hochberg (1988) (<code>"hochberg"</code>),
Hommel (1988) (<code>"hommel"</code>),
Benjamini &amp; Hochberg (1995) (<code>"BH"</code> or its alias
<code>"fdr"</code>), and
Benjamini &amp; Yekutieli (2001) (<code>"BY"</code>), respectively.
A pass-through option (<code>"none"</code>) is also included.
The set of methods are contained in the <code>p.adjust.methods</code> vector
for the benefit of methods that need to have the method as an option
and pass it on to <code>p.adjust</code>.
</p>
<p>The first four methods are designed to give strong control of the
family-wise error rate.  There seems no reason to use the unmodified
Bonferroni correction because it is dominated by Holm's method, which
is also valid under arbitrary assumptions.
</p>
<p>Hochberg's and Hommel's methods are valid when the hypothesis tests
are independent or when they are non-negatively associated (
Sarkar, 1998; Sarkar and Chang, 1997).
Hommel's method is more powerful than
Hochberg's, but the difference is usually small and the Hochberg
p-values are faster to compute.
</p>
<p>The <code>"BH"</code> (aka <code>"fdr"</code>) and <code>"BY"</code> methods of
Benjamini, Hochberg, and Yekutieli control the false discovery rate,
the expected proportion of false discoveries amongst the rejected
hypotheses.  The false discovery rate is a less stringent condition
than the family-wise error rate, so these methods are more powerful
than the others.
</p>
<p>Note that you can set <code>n</code> larger than <code>length(p)</code> which
means the unobserved p-values are assumed to be greater than all the
observed p for <code>"bonferroni"</code> and <code>"holm"</code> methods and equal
to 1 for the other methods.
</p>


<h3>Value</h3>

<p>A numeric vector of corrected p-values (of the same length as
<code>p</code>, with names copied from <code>p</code>).
</p>


<h3>References</h3>

<p>Benjamini, Y., and Hochberg, Y. (1995).
Controlling the false discovery rate: a practical and powerful
approach to multiple testing.
<em>Journal of the Royal Statistical Society Series B</em>, <b>57</b>,
289&ndash;300.
<a href="https://doi.org/10.1111/j.2517-6161.1995.tb02031.x">doi:10.1111/j.2517-6161.1995.tb02031.x</a>.

</p>
<p>Benjamini, Y., and Yekutieli, D. (2001).
The control of the false discovery rate in multiple testing under
dependency.
<em>Annals of Statistics</em>, <b>29</b>, 1165&ndash;1188.
<a href="https://doi.org/10.1214/aos/1013699998">doi:10.1214/aos/1013699998</a>.
</p>
<p>Holm, S. (1979).
A simple sequentially rejective multiple test procedure.
<em>Scandinavian Journal of Statistics</em>, <b>6</b>, 65&ndash;70.
<a href="https://www.jstor.org/stable/4615733">https://www.jstor.org/stable/4615733</a>.
</p>
<p>Hommel, G. (1988).
A stagewise rejective multiple test procedure based on a modified
Bonferroni test.
<em>Biometrika</em>, <b>75</b>, 383&ndash;386.
<a href="https://doi.org/10.2307/2336190">doi:10.2307/2336190</a>.
</p>
<p>Hochberg, Y. (1988).
A sharper Bonferroni procedure for multiple tests of significance.
<em>Biometrika</em>, <b>75</b>, 800&ndash;803.
<a href="https://doi.org/10.2307/2336325">doi:10.2307/2336325</a>.
</p>
<p>Shaffer, J. P. (1995).
Multiple hypothesis testing.
<em>Annual Review of Psychology</em>, <b>46</b>, 561&ndash;584.
<a href="https://doi.org/10.1146/annurev.ps.46.020195.003021">doi:10.1146/annurev.ps.46.020195.003021</a>.
(An excellent review of the area.)
</p>
<p>Sarkar, S. (1998).
Some probability inequalities for ordered MTP2 random variables: a
proof of Simes conjecture.
<em>Annals of Statistics</em>, <b>26</b>, 494&ndash;504.
<a href="https://doi.org/10.1214/aos/1028144846">doi:10.1214/aos/1028144846</a>.
</p>
<p>Sarkar, S., and Chang, C. K. (1997).
The Simes method for multiple hypothesis testing with positively
dependent test statistics.
<em>Journal of the American Statistical Association</em>, <b>92</b>,
1601&ndash;1608.
<a href="https://doi.org/10.2307/2965431">doi:10.2307/2965431</a>.
</p>
<p>Wright, S. P. (1992).
Adjusted P-values for simultaneous inference.
<em>Biometrics</em>, <b>48</b>, 1005&ndash;1013.
<a href="https://doi.org/10.2307/2532694">doi:10.2307/2532694</a>.
(Explains the adjusted P-value approach.)
</p>


<h3>See Also</h3>

<p><code>pairwise.*</code> functions such as <code><a href="#topic+pairwise.t.test">pairwise.t.test</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

set.seed(123)
x &lt;- rnorm(50, mean = c(rep(0, 25), rep(3, 25)))
p &lt;- 2*pnorm(sort(-abs(x)))

round(p, 3)
round(p.adjust(p), 3)
round(p.adjust(p, "BH"), 3)

## or all of them at once (dropping the "fdr" alias):
p.adjust.M &lt;- p.adjust.methods[p.adjust.methods != "fdr"]
p.adj    &lt;- sapply(p.adjust.M, function(meth) p.adjust(p, meth))
p.adj.60 &lt;- sapply(p.adjust.M, function(meth) p.adjust(p, meth, n = 60))
stopifnot(identical(p.adj[,"none"], p), p.adj &lt;= p.adj.60)
round(p.adj, 3)
## or a bit nicer:
noquote(apply(p.adj, 2, format.pval, digits = 3))


## and a graphic:
matplot(p, p.adj, ylab="p.adjust(p, meth)", type = "l", asp = 1, lty = 1:6,
        main = "P-value adjustments")
legend(0.7, 0.6, p.adjust.M, col = 1:6, lty = 1:6)

## Can work with NA's:
pN &lt;- p; iN &lt;- c(46, 47); pN[iN] &lt;- NA
pN.a &lt;- sapply(p.adjust.M, function(meth) p.adjust(pN, meth))
## The smallest 20 P-values all affected by the NA's :
round((pN.a / p.adj)[1:20, ] , 4)
</code></pre>

<hr>
<h2 id='Pair'>
Construct a Paired-Data Object
</h2><span id='topic+Pair'></span>

<h3>Description</h3>

<p>Combines two vectors into an object of class <code>"Pair"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Pair(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Pair_+3A_x">x</code></td>
<td>

<p>a vector, the 1st element of the pair.
</p>
</td></tr>
<tr><td><code id="Pair_+3A_y">y</code></td>
<td>

<p>a vector, the 2nd element of the pair.
Should have the same length as <code>x</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 2-column matrix of class <code>"Pair"</code>.
</p>


<h3>Note</h3>

<p>Mostly designed as part of the formula interface to paired tests.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+t.test">t.test</a></code> and <code><a href="#topic+wilcox.test">wilcox.test</a></code>
</p>

<hr>
<h2 id='pairwise.prop.test'> Pairwise comparisons for proportions</h2><span id='topic+pairwise.prop.test'></span>

<h3>Description</h3>

<p>Calculate pairwise comparisons between pairs of proportions with
correction for multiple testing
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pairwise.prop.test(x, n, p.adjust.method = p.adjust.methods, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pairwise.prop.test_+3A_x">x</code></td>
<td>
<p> Vector of counts of successes or a matrix with 2 columns
giving the counts of successes and failures, respectively. </p>
</td></tr>
<tr><td><code id="pairwise.prop.test_+3A_n">n</code></td>
<td>
<p> Vector of counts of trials; ignored if <code>x</code> is a matrix.</p>
</td></tr>
<tr><td><code id="pairwise.prop.test_+3A_p.adjust.method">p.adjust.method</code></td>
<td>
<p>Method for adjusting p values
(see <code><a href="#topic+p.adjust">p.adjust</a></code>).  Can be abbreviated. </p>
</td></tr>
<tr><td><code id="pairwise.prop.test_+3A_...">...</code></td>
<td>
<p> Additional arguments to pass to <code>prop.test</code> </p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>"pairwise.htest"</code>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+prop.test">prop.test</a></code>, <code><a href="#topic+p.adjust">p.adjust</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>smokers  &lt;- c( 83, 90, 129, 70 )
patients &lt;- c( 86, 93, 136, 82 )
pairwise.prop.test(smokers, patients)
</code></pre>

<hr>
<h2 id='pairwise.t.test'> Pairwise t tests</h2><span id='topic+pairwise.t.test'></span>

<h3>Description</h3>

<p>Calculate pairwise comparisons between group levels with corrections
for multiple testing
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pairwise.t.test(x, g, p.adjust.method = p.adjust.methods,
                pool.sd = !paired, paired = FALSE,
                alternative = c("two.sided", "less", "greater"),
                ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pairwise.t.test_+3A_x">x</code></td>
<td>
<p> response vector. </p>
</td></tr>
<tr><td><code id="pairwise.t.test_+3A_g">g</code></td>
<td>
<p> grouping vector or factor. </p>
</td></tr>
<tr><td><code id="pairwise.t.test_+3A_p.adjust.method">p.adjust.method</code></td>
<td>
<p> Method for adjusting p values (see <code><a href="#topic+p.adjust">p.adjust</a></code>). </p>
</td></tr>
<tr><td><code id="pairwise.t.test_+3A_pool.sd">pool.sd</code></td>
<td>
<p> switch to allow/disallow the use of a pooled SD </p>
</td></tr>
<tr><td><code id="pairwise.t.test_+3A_paired">paired</code></td>
<td>
<p> a logical indicating whether you want paired
t-tests. </p>
</td></tr>
<tr><td><code id="pairwise.t.test_+3A_alternative">alternative</code></td>
<td>
<p> a character string specifying the alternative
hypothesis, must be one of <code>"two.sided"</code> (default),
<code>"greater"</code> or <code>"less"</code>.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="pairwise.t.test_+3A_...">...</code></td>
<td>
<p> additional arguments to pass to <code>t.test</code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p> The <code>pool.sd</code> switch calculates a common SD for all
groups and uses that for all comparisons (this can be useful if some
groups are small). This method does not actually call <code>t.test</code>,
so extra arguments are ignored. Pooling does not generalize to paired tests
so <code>pool.sd</code> and <code>paired</code> cannot both be <code>TRUE</code>.
</p>
<p>Only the lower triangle of the matrix of possible comparisons is being
calculated, so setting <code>alternative</code> to anything other than
<code>"two.sided"</code> requires that the levels of <code>g</code> are ordered
sensibly.
</p>


<h3>Value</h3>

<p>Object of class <code>"pairwise.htest"</code>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+t.test">t.test</a></code>, <code><a href="#topic+p.adjust">p.adjust</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>attach(airquality)
Month &lt;- factor(Month, labels = month.abb[5:9])
pairwise.t.test(Ozone, Month)
pairwise.t.test(Ozone, Month, p.adjust.method = "bonf")
pairwise.t.test(Ozone, Month, pool.sd = FALSE)
detach()
</code></pre>

<hr>
<h2 id='pairwise.table'>Tabulate p values for pairwise comparisons</h2><span id='topic+pairwise.table'></span>

<h3>Description</h3>

<p>Creates  table of p values for pairwise comparisons
with corrections for multiple testing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pairwise.table(compare.levels, level.names, p.adjust.method)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pairwise.table_+3A_compare.levels">compare.levels</code></td>
<td>
<p>a <code><a href="base.html#topic+function">function</a></code> to compute (raw) p value
given indices <code>i</code> and <code>j</code>.</p>
</td></tr>
<tr><td><code id="pairwise.table_+3A_level.names">level.names</code></td>
<td>
<p>names of the group levels</p>
</td></tr>
<tr><td><code id="pairwise.table_+3A_p.adjust.method">p.adjust.method</code></td>
<td>
<p>a character string specifying the method for
multiple testing adjustment; almost always one of
<code><a href="#topic+p.adjust.methods">p.adjust.methods</a></code>.  Can be abbreviated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Functions that do multiple group comparisons create separate
<code>compare.levels</code> functions (assumed to be symmetrical in <code>i</code>
and <code>j</code>) and passes them to this function.
</p>


<h3>Value</h3>

<p>Table of p values in lower triangular form.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+pairwise.t.test">pairwise.t.test</a></code> </p>

<hr>
<h2 id='pairwise.wilcox.test'>Pairwise Wilcoxon Rank Sum Tests</h2><span id='topic+pairwise.wilcox.test'></span>

<h3>Description</h3>

<p>Calculate pairwise comparisons between group levels with corrections
for multiple testing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pairwise.wilcox.test(x, g, p.adjust.method = p.adjust.methods,
                      paired = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pairwise.wilcox.test_+3A_x">x</code></td>
<td>
<p> response vector. </p>
</td></tr>
<tr><td><code id="pairwise.wilcox.test_+3A_g">g</code></td>
<td>
<p> grouping vector or factor. </p>
</td></tr>
<tr><td><code id="pairwise.wilcox.test_+3A_p.adjust.method">p.adjust.method</code></td>
<td>
<p> method for adjusting p values (see
<code><a href="#topic+p.adjust">p.adjust</a></code>). Can be abbreviated.</p>
</td></tr>
<tr><td><code id="pairwise.wilcox.test_+3A_paired">paired</code></td>
<td>
<p>a logical indicating whether you want a paired test.</p>
</td></tr>
<tr><td><code id="pairwise.wilcox.test_+3A_...">...</code></td>
<td>
<p>additional arguments to pass to <code><a href="#topic+wilcox.test">wilcox.test</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Extra arguments that are passed on to <code>wilcox.test</code> may or may
not be sensible in this context. In particular,
only the lower triangle of the matrix of possible comparisons is being
calculated, so setting <code>alternative</code> to anything other than
<code>"two.sided"</code> requires that the levels of <code>g</code> are ordered
sensibly.
</p>


<h3>Value</h3>

<p>Object of class <code>"pairwise.htest"</code>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+wilcox.test">wilcox.test</a></code>, <code><a href="#topic+p.adjust">p.adjust</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>attach(airquality)
Month &lt;- factor(Month, labels = month.abb[5:9])
## These give warnings because of ties :
pairwise.wilcox.test(Ozone, Month)
pairwise.wilcox.test(Ozone, Month, p.adjust.method = "bonf")
detach()
</code></pre>

<hr>
<h2 id='plot.acf'>Plot Autocovariance and Autocorrelation Functions</h2><span id='topic+plot.acf'></span>

<h3>Description</h3>

<p>Plot method for objects of class <code>"acf"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'acf'
plot(x, ci = 0.95, type = "h", xlab = "Lag", ylab = NULL,
     ylim = NULL, main = NULL,
     ci.col = "blue", ci.type = c("white", "ma"),
     max.mfrow = 6, ask = Npgs &gt; 1 &amp;&amp; dev.interactive(),
     mar = if(nser &gt; 2) c(3,2,2,0.8) else par("mar"),
     oma = if(nser &gt; 2) c(1,1.2,1,1) else par("oma"),
     mgp = if(nser &gt; 2) c(1.5,0.6,0) else par("mgp"),
     xpd = par("xpd"),
     cex.main = if(nser &gt; 2) 1 else par("cex.main"),
     verbose = getOption("verbose"),
     ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.acf_+3A_x">x</code></td>
<td>
<p>an object of class <code>"acf"</code>.</p>
</td></tr>
<tr><td><code id="plot.acf_+3A_ci">ci</code></td>
<td>
<p>coverage probability for confidence interval.  Plotting of
the confidence interval is suppressed if <code>ci</code> is zero or
negative.</p>
</td></tr>
<tr><td><code id="plot.acf_+3A_type">type</code></td>
<td>
<p>the type of plot to be drawn, default to histogram like
vertical lines.</p>
</td></tr>
<tr><td><code id="plot.acf_+3A_xlab">xlab</code></td>
<td>
<p>the x label of the plot.</p>
</td></tr>
<tr><td><code id="plot.acf_+3A_ylab">ylab</code></td>
<td>
<p>the y label of the plot.</p>
</td></tr>
<tr><td><code id="plot.acf_+3A_ylim">ylim</code></td>
<td>
<p>numeric of length 2 giving the y limits for the plot.</p>
</td></tr>
<tr><td><code id="plot.acf_+3A_main">main</code></td>
<td>
<p>overall title for the plot.</p>
</td></tr>
<tr><td><code id="plot.acf_+3A_ci.col">ci.col</code></td>
<td>
<p>colour to plot the confidence interval lines.</p>
</td></tr>
<tr><td><code id="plot.acf_+3A_ci.type">ci.type</code></td>
<td>
<p>should the confidence limits assume a white noise
input or for lag <code class="reqn">k</code> an MA(<code class="reqn">k-1</code>) input?   Can be abbreviated.</p>
</td></tr>
<tr><td><code id="plot.acf_+3A_max.mfrow">max.mfrow</code></td>
<td>
<p>positive integer; for multivariate <code>x</code>
indicating how many rows and columns of plots should be put on one
page, using <code><a href="graphics.html#topic+par">par</a>(mfrow = c(m,m))</code>.</p>
</td></tr>
<tr><td><code id="plot.acf_+3A_ask">ask</code></td>
<td>
<p>logical; if <code>TRUE</code>, the user is asked before a new
page is started.</p>
</td></tr>
<tr><td><code id="plot.acf_+3A_mar">mar</code>, <code id="plot.acf_+3A_oma">oma</code>, <code id="plot.acf_+3A_mgp">mgp</code>, <code id="plot.acf_+3A_xpd">xpd</code>, <code id="plot.acf_+3A_cex.main">cex.main</code></td>
<td>
<p>graphics parameters as in
<code><a href="graphics.html#topic+par">par</a>(*)</code>, by default adjusted to use smaller than
default margins for multivariate <code>x</code> only.
</p>
</td></tr>
<tr><td><code id="plot.acf_+3A_verbose">verbose</code></td>
<td>
<p>logical.  Should <span class="rlang"><b>R</b></span> report extra information on
progress?</p>
</td></tr>
<tr><td><code id="plot.acf_+3A_...">...</code></td>
<td>
<p>graphics parameters to be passed to the plotting
routines.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The confidence interval plotted in <code>plot.acf</code> is based on an
<em>uncorrelated</em> series and should be treated with appropriate
caution.  Using <code>ci.type = "ma"</code> may be less potentially
misleading.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+acf">acf</a></code> which calls <code>plot.acf</code> by default.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)


z4  &lt;- ts(matrix(rnorm(400), 100, 4), start = c(1961, 1), frequency = 12)
z7  &lt;- ts(matrix(rnorm(700), 100, 7), start = c(1961, 1), frequency = 12)
acf(z4)
acf(z7, max.mfrow = 7)   # squeeze onto 1 page
acf(z7) # multi-page
</code></pre>

<hr>
<h2 id='plot.density'>Plot Method for Kernel Density Estimation</h2><span id='topic+plot.density'></span>

<h3>Description</h3>

<p>The <code>plot</code> method for density objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'density'
plot(x, main = NULL, xlab = NULL, ylab = "Density", type = "l",
     zero.line = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.density_+3A_x">x</code></td>
<td>
<p>a <code>"density"</code> object.</p>
</td></tr>
<tr><td><code id="plot.density_+3A_main">main</code>, <code id="plot.density_+3A_xlab">xlab</code>, <code id="plot.density_+3A_ylab">ylab</code>, <code id="plot.density_+3A_type">type</code></td>
<td>
<p>plotting parameters with useful defaults.</p>
</td></tr>
<tr><td><code id="plot.density_+3A_...">...</code></td>
<td>
<p>further plotting parameters.</p>
</td></tr>
<tr><td><code id="plot.density_+3A_zero.line">zero.line</code></td>
<td>
<p>logical; if <code>TRUE</code>, add a base line at <code class="reqn">y = 0</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>None.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+density">density</a></code>.
</p>

<hr>
<h2 id='plot.HoltWinters'>Plot function for <code>"HoltWinters"</code> objects</h2><span id='topic+plot.HoltWinters'></span>

<h3>Description</h3>

<p>Produces a chart of the original time series along with the fitted
values. Optionally, predicted values (and their confidence bounds) can
also be plotted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'HoltWinters'
plot(x, predicted.values = NA, intervals = TRUE,
        separator = TRUE, col = 1, col.predicted = 2,
        col.intervals = 4, col.separator = 1, lty = 1,
        lty.predicted = 1, lty.intervals = 1, lty.separator = 3,
        ylab = "Observed / Fitted",
        main = "Holt-Winters filtering",
        ylim = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.HoltWinters_+3A_x">x</code></td>
<td>
<p>Object of class <code>"HoltWinters"</code></p>
</td></tr>
<tr><td><code id="plot.HoltWinters_+3A_predicted.values">predicted.values</code></td>
<td>
<p>Predicted values as returned by <code>predict.HoltWinters</code></p>
</td></tr>
<tr><td><code id="plot.HoltWinters_+3A_intervals">intervals</code></td>
<td>
<p>If <code>TRUE</code>, the prediction intervals are plotted (default).</p>
</td></tr>
<tr><td><code id="plot.HoltWinters_+3A_separator">separator</code></td>
<td>
<p>If <code>TRUE</code>, a separating line between fitted and predicted values is plotted (default).</p>
</td></tr>
<tr><td><code id="plot.HoltWinters_+3A_col">col</code>, <code id="plot.HoltWinters_+3A_lty">lty</code></td>
<td>
<p>Color/line type of original data (default: black solid).</p>
</td></tr>
<tr><td><code id="plot.HoltWinters_+3A_col.predicted">col.predicted</code>, <code id="plot.HoltWinters_+3A_lty.predicted">lty.predicted</code></td>
<td>
<p>Color/line type of
fitted and predicted values (default: red solid).</p>
</td></tr>
<tr><td><code id="plot.HoltWinters_+3A_col.intervals">col.intervals</code>, <code id="plot.HoltWinters_+3A_lty.intervals">lty.intervals</code></td>
<td>
<p>Color/line type of prediction
intervals (default: blue solid).</p>
</td></tr>
<tr><td><code id="plot.HoltWinters_+3A_col.separator">col.separator</code>, <code id="plot.HoltWinters_+3A_lty.separator">lty.separator</code></td>
<td>
<p>Color/line type of
observed/predicted values separator (default: black dashed).</p>
</td></tr>
<tr><td><code id="plot.HoltWinters_+3A_ylab">ylab</code></td>
<td>
<p>Label of the y-axis.</p>
</td></tr>
<tr><td><code id="plot.HoltWinters_+3A_main">main</code></td>
<td>
<p>Main title.</p>
</td></tr>
<tr><td><code id="plot.HoltWinters_+3A_ylim">ylim</code></td>
<td>
<p>Limits of the y-axis. If <code>NULL</code>, the range is chosen
such that the plot contains the original series, the fitted values,
and the predicted values if any.</p>
</td></tr>
<tr><td><code id="plot.HoltWinters_+3A_...">...</code></td>
<td>
<p>Other graphics parameters.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>David Meyer <a href="mailto:David.Meyer@wu.ac.at">David.Meyer@wu.ac.at</a>
</p>


<h3>References</h3>

<p>C. C. Holt (1957)
Forecasting trends and seasonals by exponentially weighted
moving averages,
<em>ONR Research Memorandum, Carnegie Institute of Technology</em> <b>52</b>.
</p>
<p>P. R. Winters (1960).
Forecasting sales by exponentially weighted moving averages.
<em>Management Science</em>, <b>6</b>, 324&ndash;342.
<a href="https://doi.org/10.1287/mnsc.6.3.324">doi:10.1287/mnsc.6.3.324</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+HoltWinters">HoltWinters</a></code>, <code><a href="#topic+predict.HoltWinters">predict.HoltWinters</a></code>
</p>

<hr>
<h2 id='plot.isoreg'>Plot Method for <code>isoreg</code> Objects</h2><span id='topic+plot.isoreg'></span><span id='topic+lines.isoreg'></span>

<h3>Description</h3>

<p>The <code><a href="graphics.html#topic+plot">plot</a></code> and <code><a href="graphics.html#topic+lines">lines</a></code> method for
<span class="rlang"><b>R</b></span> objects of class <code><a href="#topic+isoreg">isoreg</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'isoreg'
plot(x, plot.type = c("single", "row.wise", "col.wise"),
      main = paste("Isotonic regression", deparse(x$call)),
      main2 = "Cumulative Data and Convex Minorant",
      xlab = "x0", ylab = "x$y",
      par.fit = list(col = "red", cex = 1.5, pch = 13, lwd = 1.5),
      mar = if (both) 0.1 + c(3.5, 2.5, 1, 1) else par("mar"),
      mgp = if (both) c(1.6, 0.7, 0) else par("mgp"),
      grid = length(x$x) &lt; 12, ...)

## S3 method for class 'isoreg'
lines(x, col = "red", lwd = 1.5,
       do.points = FALSE, cex = 1.5, pch = 13, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.isoreg_+3A_x">x</code></td>
<td>
<p>an <code><a href="#topic+isoreg">isoreg</a></code> object.</p>
</td></tr>
<tr><td><code id="plot.isoreg_+3A_plot.type">plot.type</code></td>
<td>
<p>character indicating which type of plot is desired.
The first (default) only draws the data and the fit, where the
others add a plot of the cumulative data and fit.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="plot.isoreg_+3A_main">main</code></td>
<td>
<p>main title of plot, see <code><a href="graphics.html#topic+title">title</a></code>.</p>
</td></tr>
<tr><td><code id="plot.isoreg_+3A_main2">main2</code></td>
<td>
<p>title for second (cumulative) plot.</p>
</td></tr>
<tr><td><code id="plot.isoreg_+3A_xlab">xlab</code>, <code id="plot.isoreg_+3A_ylab">ylab</code></td>
<td>
<p>x- and y- axis annotation.</p>
</td></tr>
<tr><td><code id="plot.isoreg_+3A_par.fit">par.fit</code></td>
<td>
<p>a <code><a href="base.html#topic+list">list</a></code> of arguments (for
<code><a href="graphics.html#topic+points">points</a></code> and <code><a href="graphics.html#topic+lines">lines</a></code>) for drawing the fit.</p>
</td></tr>
<tr><td><code id="plot.isoreg_+3A_mar">mar</code>, <code id="plot.isoreg_+3A_mgp">mgp</code></td>
<td>
<p>graphical parameters, see <code><a href="graphics.html#topic+par">par</a></code>, mainly
for the case of two plots.</p>
</td></tr>
<tr><td><code id="plot.isoreg_+3A_grid">grid</code></td>
<td>
<p>logical indicating if grid lines should be drawn.  If
true, <code><a href="graphics.html#topic+grid">grid</a>()</code> is used for the first plot, where as
vertical lines are drawn at &lsquo;touching&rsquo; points for the
cumulative plot.</p>
</td></tr>
<tr><td><code id="plot.isoreg_+3A_do.points">do.points</code></td>
<td>
<p>for <code>lines()</code>: logical indicating if the step
points should be drawn as well (and as they are drawn in <code>plot()</code>).</p>
</td></tr>
<tr><td><code id="plot.isoreg_+3A_col">col</code>, <code id="plot.isoreg_+3A_lwd">lwd</code>, <code id="plot.isoreg_+3A_cex">cex</code>, <code id="plot.isoreg_+3A_pch">pch</code></td>
<td>
<p>graphical arguments for <code>lines()</code>,
where <code>cex</code> and <code>pch</code> are only used when <code>do.points</code>
is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="plot.isoreg_+3A_...">...</code></td>
<td>
<p>further arguments passed to and from methods.</p>
</td></tr>

</table>


<h3>See Also</h3>

<p><code><a href="#topic+isoreg">isoreg</a></code> for computation of <code>isoreg</code> objects.</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

utils::example(isoreg) # for the examples there

plot(y3, main = "simple plot(.)  +  lines(&lt;isoreg&gt;)")
lines(ir3)

## 'same' plot as above, "proving" that only ranks of 'x' are important
plot(isoreg(2^(1:9), c(1,0,4,3,3,5,4,2,0)), plot.type = "row", log = "x")

plot(ir3, plot.type = "row", ylab = "y3")
plot(isoreg(y3 - 4), plot.type = "r", ylab = "y3 - 4")
plot(ir4, plot.type = "ro",  ylab = "y4", xlab = "x = 1:n")

## experiment a bit with these (C-c C-j):
plot(isoreg(sample(9),  y3), plot.type = "row")
plot(isoreg(sample(9),  y3), plot.type = "col.wise")

plot(ir &lt;- isoreg(sample(10), sample(10, replace = TRUE)),
                  plot.type = "r")
</code></pre>

<hr>
<h2 id='plot.lm'>Plot Diagnostics for an <code>lm</code> Object</h2><span id='topic+plot.lm'></span>

<h3>Description</h3>

<p>Six plots (selectable by <code>which</code>) are currently available: a plot
of residuals against fitted values, a Scale-Location plot of
<code class="reqn">\sqrt{| residuals |}</code>
against fitted values, a Q-Q plot of residuals, a
plot of Cook's distances versus row labels, a plot of residuals
against leverages, and a plot of Cook's distances against
leverage/(1-leverage).  By default, the first three and <code>5</code> are
provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lm'
plot(x, which = c(1,2,3,5), 
     caption = list("Residuals vs Fitted", "Q-Q Residuals",
       "Scale-Location", "Cook's distance",
       "Residuals vs Leverage",
       expression("Cook's dist vs Leverage* " * h[ii] / (1 - h[ii]))),
     panel = if(add.smooth) function(x, y, ...)
              panel.smooth(x, y, iter=iter.smooth, ...) else points,
     sub.caption = NULL, main = "",
     ask = prod(par("mfcol")) &lt; length(which) &amp;&amp; dev.interactive(),
     ...,
     id.n = 3, labels.id = names(residuals(x)), cex.id = 0.75,
     qqline = TRUE, cook.levels = c(0.5, 1.0),
     cook.col = 8, cook.lty = 2, cook.legendChanges = list(),
     add.smooth = getOption("add.smooth"),
     iter.smooth = if(isGlm) 0 else 3,
     label.pos = c(4,2),
     cex.caption = 1, cex.oma.main = 1.25
   , extend.ylim.f = 0.08
     )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.lm_+3A_x">x</code></td>
<td>
<p><code>lm</code> object, typically result of <code><a href="#topic+lm">lm</a></code> or
<code><a href="#topic+glm">glm</a></code>.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_which">which</code></td>
<td>
<p>a subset of the numbers <code>1:6</code>, by default <code>1:3, 5</code>,
referring to </p>

<ol>
<li><p> &quot;Residuals vs Fitted&quot;, aka &lsquo;Tukey-Anscombe&rsquo; plot 
</p>
</li>
<li><p> &quot;Residual Q-Q&quot; plot  
</p>
</li>
<li><p> &quot;Scale-Location&quot;     
</p>
</li>
<li><p> &quot;Cook's distance&quot;    
</p>
</li>
<li><p> &quot;Residuals   vs  Leverage&quot;      
</p>
</li>
<li><p> &quot;Cook's dist vs Lev./(1-Lev.)&quot;  
</p>
</li></ol>

<p>See also &lsquo;Details&rsquo; below.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_caption">caption</code></td>
<td>
<p>captions to appear above the plots;
<code><a href="base.html#topic+character">character</a></code> vector or <code><a href="base.html#topic+list">list</a></code> of valid
graphics annotations, see <code><a href="grDevices.html#topic+as.graphicsAnnot">as.graphicsAnnot</a></code>, of length
6, the j-th entry corresponding to <code>which[j]</code>, see also the
default vector in &lsquo;Usage&rsquo;.  Can be set to
<code>""</code> or <code>NA</code> to suppress all captions.
</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_panel">panel</code></td>
<td>
<p>panel function.  The useful alternative to
<code><a href="graphics.html#topic+points">points</a></code>, <code><a href="graphics.html#topic+panel.smooth">panel.smooth</a></code> can be chosen
by <code>add.smooth = TRUE</code>.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_sub.caption">sub.caption</code></td>
<td>
<p>common title&mdash;above the figures if there are more
than one; used as <code>sub</code> (s.<code><a href="graphics.html#topic+title">title</a></code>) otherwise.  If
<code>NULL</code>, as by default, a possible abbreviated version of
<code>deparse(x$call)</code> is used.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_main">main</code></td>
<td>
<p>title to each plot&mdash;in addition to <code>caption</code>.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_ask">ask</code></td>
<td>
<p>logical; if <code>TRUE</code>, the user is <em>ask</em>ed before
each plot, see <code><a href="graphics.html#topic+par">par</a>(ask=.)</code>.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_...">...</code></td>
<td>
<p>other parameters to be passed through to plotting
functions.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_id.n">id.n</code></td>
<td>
<p>number of points to be labelled in each plot, starting
with the most extreme.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_labels.id">labels.id</code></td>
<td>
<p>vector of labels, from which the labels for extreme
points will be chosen.  <code>NULL</code> uses observation numbers.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_cex.id">cex.id</code></td>
<td>
<p>magnification of point labels.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_qqline">qqline</code></td>
<td>
<p>logical indicating if a <code><a href="#topic+qqline">qqline</a>()</code> should be
added to the normal Q-Q plot.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_cook.levels">cook.levels</code></td>
<td>
<p>levels of Cook's distance at which to draw contours.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_cook.col">cook.col</code>, <code id="plot.lm_+3A_cook.lty">cook.lty</code></td>
<td>
<p>color and line type to use for these contour
lines.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_cook.legendchanges">cook.legendChanges</code></td>
<td>
<p>a <code><a href="base.html#topic+list">list</a></code> (or <code>NULL</code> to
suppress the call) of arguments to <code><a href="graphics.html#topic+legend">legend</a></code> which should be
<em>modified</em> from (or added to) the <code>plot.lm()</code> default <code>
      list(x = "bottomleft", legend = "Cook's distance",
           lty = cook.lty, col = cook.col, text.col = cook.col,
           bty = "n", x.intersp = 1/4, y.intersp = 1/8) </code>.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_add.smooth">add.smooth</code></td>
<td>
<p>logical indicating if a smoother should be added to
most plots; see also <code>panel</code> above.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_iter.smooth">iter.smooth</code></td>
<td>
<p>the number of robustness iterations, the argument
<code>iter</code> in <code><a href="graphics.html#topic+panel.smooth">panel.smooth</a>()</code>; the default uses no such
iterations for <code><a href="#topic+glm">glm</a></code> fits which is
particularly desirable for the (predominant) case of binary
observations, but also for other models where the response
distribution can be highly skewed.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_label.pos">label.pos</code></td>
<td>
<p>positioning of labels, for the left half and right
half of the graph respectively, for plots 1-3, 5, 6.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_cex.caption">cex.caption</code></td>
<td>
<p>controls the size of <code>caption</code>.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_cex.oma.main">cex.oma.main</code></td>
<td>
<p>controls the size of the <code>sub.caption</code> only if
that is <em>above</em> the figures when there is more than one.</p>
</td></tr>
<tr><td><code id="plot.lm_+3A_extend.ylim.f">extend.ylim.f</code></td>
<td>
<p>a numeric vector of length 1 or 2, to be used in
<code>ylim &lt;- <a href="grDevices.html#topic+extendrange">extendrange</a>(r=ylim, f = *)</code>
for plots <code>1</code> and <code>5</code> when <code>id.n</code> is non-empty.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>sub.caption</code>&mdash;by default the function call&mdash;is shown as
a subtitle (under the x-axis title) on each plot when plots are on
separate pages, or as a subtitle in the outer margin (if any) when
there are multiple plots per page.
</p>
<p>The &lsquo;Scale-Location&rsquo; plot (<code>which=3</code>), also called &lsquo;Spread-Location&rsquo; or
&lsquo;S-L&rsquo; plot, takes the square root of the absolute residuals in
order to diminish skewness (<code class="reqn">\sqrt{| E |}</code> is much less skewed
than <code class="reqn">| E |</code> for Gaussian zero-mean <code class="reqn">E</code>).
</p>
<p>The &lsquo;S-L&rsquo;, the Q-Q, and the Residual-Leverage (<code>which=5</code>) plot use
<em>standardized</em> residuals which have identical variance (under the
hypothesis).  They are given as
<code class="reqn">R_i / (s \times \sqrt{1 - h_{ii}})</code>
where the <em>&lsquo;leverages&rsquo;</em> <code class="reqn">h_{ii}</code> are the diagonal entries
of the hat matrix,
<code><a href="#topic+influence">influence</a>()$hat</code> (see also <code><a href="#topic+hat">hat</a></code>), and
where the Residual-Leverage plot uses the standardized Pearson residuals
(<code><a href="#topic+residuals.glm">residuals.glm</a>(type = "pearson")</code>) for <code class="reqn">R[i]</code>.
</p>
<p>The Residual-Leverage plot (<code>which=5</code>) shows contours of equal Cook's distance,
for values of <code>cook.levels</code> (by default 0.5 and 1) and omits
cases with leverage one with a warning.  If the leverages are constant
(as is typically the case in a balanced <code><a href="#topic+aov">aov</a></code> situation)
the plot uses factor level combinations instead of the leverages for
the x-axis.  (The factor levels are ordered by mean fitted value.)
</p>
<p>In the Cook's distance vs leverage/(1-leverage) (= &ldquo;leverage*&rdquo;)
plot (<code>which=6</code>), contours of
standardized residuals (<code><a href="#topic+rstandard">rstandard</a>(.)</code>) that are equal in
magnitude are lines through the origin.  These lines are labelled with
the magnitudes.  The x-axis is labeled with the (non equidistant)
leverages <code class="reqn">h_{ii}</code>.
</p>
<p>For the <code>glm</code> case, the Q-Q plot is based on the absolute value
of the standardized deviance residuals. When the saddlepoint
approximation applies, these have an approximate half-normal
distribution.  The saddlepoint approximation is exact for the normal
and inverse Gaussian family, and holds approximately for the Gamma
family with small dispersion (large shape) and for the Poisson and
binomial families with large counts (Dunn and Smyth 2018).
</p>


<h3>Author(s)</h3>

<p>John Maindonald and Martin Maechler.
</p>


<h3>References</h3>

<p>Belsley, D. A., Kuh, E. and Welsch, R. E. (1980).
<em>Regression Diagnostics</em>.
New York: Wiley.
</p>
<p>Cook, R. D. and Weisberg, S. (1982).
<em>Residuals and Influence in Regression</em>.
London: Chapman and Hall.
</p>
<p>Firth, D. (1991) Generalized Linear Models.
In Hinkley, D. V. and Reid, N. and Snell, E. J., eds:
Pp. 55-82 in Statistical Theory and Modelling.
In Honour of Sir David Cox, FRS.
London: Chapman and Hall.
</p>
<p>Hinkley, D. V. (1975).
On power transformations to symmetry.
<em>Biometrika</em>, <b>62</b>, 101&ndash;111.
<a href="https://doi.org/10.2307/2334491">doi:10.2307/2334491</a>.
</p>
<p>McCullagh, P. and Nelder, J. A. (1989).
<em>Generalized Linear Models</em>.
London: Chapman and Hall.
</p>
<p>Dunn, P.K. and Smyth G.K. (2018)
<em>Generalized Linear Models with Examples in R</em>.
New York: Springer-Verlag.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+termplot">termplot</a></code>, <code><a href="#topic+lm.influence">lm.influence</a></code>,
<code><a href="#topic+cooks.distance">cooks.distance</a></code>, <code><a href="#topic+hatvalues">hatvalues</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

## Analysis of the life-cycle savings data
## given in Belsley, Kuh and Welsch.
lm.SR &lt;- lm(sr ~ pop15 + pop75 + dpi + ddpi, data = LifeCycleSavings)
plot(lm.SR)

## 4 plots on 1 page;
## allow room for printing model formula in outer margin:
par(mfrow = c(2, 2), oma = c(0, 0, 2, 0)) -&gt; opar
plot(lm.SR)
plot(lm.SR, id.n = NULL)                 # no id's
plot(lm.SR, id.n = 5, labels.id = NULL)  # 5 id numbers

## Was default in R &lt;= 2.1.x:
## Cook's distances instead of Residual-Leverage plot
plot(lm.SR, which = 1:4)

## All the above fit a smooth curve where applicable
## by default unless "add.smooth" is changed.
## Give a smoother curve by increasing the lowess span :
plot(lm.SR, panel = function(x, y) panel.smooth(x, y, span = 1))

par(mfrow = c(2,1)) # same oma as above
plot(lm.SR, which = 1:2, sub.caption = "Saving Rates, n=50, p=5")

## Cook's distance tweaking
par(mfrow = c(2,3)) # same oma ...
plot(lm.SR, which = 1:6, cook.col = "royalblue")

## A case where over plotting of the "legend" is to be avoided:
if(dev.interactive(TRUE)) getOption("device")(height = 6, width = 4)
par(mfrow = c(3,1), mar = c(5,5,4,2)/2 +.1, mgp = c(1.4, .5, 0))
plot(lm.SR, which = 5, extend.ylim.f = c(0.2, 0.08))
plot(lm.SR, which = 5, cook.lty = "dotdash",
     cook.legendChanges = list(x = "bottomright", legend = "Cook"))
plot(lm.SR, which = 5, cook.legendChanges = NULL)  # no "legend"


par(opar) # reset par()s
</code></pre>

<hr>
<h2 id='plot.ppr'>Plot Ridge Functions for Projection Pursuit Regression Fit</h2><span id='topic+plot.ppr'></span>

<h3>Description</h3>

<p>Plot the ridge functions for a projection pursuit regression
(<code><a href="#topic+ppr">ppr</a></code>) fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ppr'
plot(x, ask, type = "o", cex = 1/2,
     main = quote(bquote(
         "term"[.(i)]*":" ~~ hat(beta[.(i)]) == .(bet.i))),
     xlab = quote(bquote(bold(alpha)[.(i)]^T * bold(x))),
     ylab = "", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.ppr_+3A_x">x</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object of class <code>"ppr"</code> as produced by a call to
<code>ppr</code>.</p>
</td></tr>
<tr><td><code id="plot.ppr_+3A_ask">ask</code></td>
<td>

<p>the graphics parameter <code>ask</code>: see <code><a href="graphics.html#topic+par">par</a></code> for details.
If set to <code>TRUE</code> will ask between the plot of each cross-section.
</p>
</td></tr>
<tr><td><code id="plot.ppr_+3A_type">type</code></td>
<td>
<p>the type of line (see <code><a href="graphics.html#topic+plot.default">plot.default</a></code>) to draw.</p>
</td></tr>
<tr><td><code id="plot.ppr_+3A_cex">cex</code></td>
<td>
<p>plot symbol expansion factor (<em>relative</em> to
<code><a href="graphics.html#topic+par">par</a>("cex")</code>).</p>
</td></tr>
<tr><td><code id="plot.ppr_+3A_main">main</code>, <code id="plot.ppr_+3A_xlab">xlab</code>, <code id="plot.ppr_+3A_ylab">ylab</code></td>
<td>
<p>axis annotations, see also
<code><a href="graphics.html#topic+title">title</a></code>.  Can be an expression (depending on <code>i</code> and
<code>bet.i</code>), as by default which will be <code>eval()</code>uated.</p>
</td></tr>
<tr><td><code id="plot.ppr_+3A_...">...</code></td>
<td>
<p>further graphical parameters, passed to
<code><a href="graphics.html#topic+plot">plot</a>()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>Side Effects</h3>

<p>A series of plots are drawn on the current graphical device, one for
each term in the fit.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ppr">ppr</a></code>, <code><a href="graphics.html#topic+par">par</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

rock1 &lt;- within(rock, { area1 &lt;- area/10000; peri1 &lt;- peri/10000 })
par(mfrow = c(3,2)) # maybe: , pty = "s"
rock.ppr &lt;- ppr(log(perm) ~ area1 + peri1 + shape,
                data = rock1, nterms = 2, max.terms = 5)
plot(rock.ppr, main = "ppr(log(perm)~ ., nterms=2, max.terms=5)")
plot(update(rock.ppr, bass = 5), main = "update(..., bass = 5)")
plot(update(rock.ppr, sm.method = "gcv", gcvpen = 2),
     main = "update(..., sm.method=\"gcv\", gcvpen=2)")
</code></pre>

<hr>
<h2 id='plot.profile'>Plotting Functions for 'profile' Objects</h2><span id='topic+plot.profile'></span><span id='topic+pairs.profile'></span>

<h3>Description</h3>

<p><code><a href="graphics.html#topic+plot">plot</a></code> and <code><a href="graphics.html#topic+pairs">pairs</a></code> methods for objects of
class <code>"profile"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'profile'
plot(x, ...)
## S3 method for class 'profile'
pairs(x, colours = 2:3, which = names(x), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.profile_+3A_x">x</code></td>
<td>
<p>an object inheriting from class <code>"profile"</code>.</p>
</td></tr>
<tr><td><code id="plot.profile_+3A_colours">colours</code></td>
<td>
<p>colours to be used for the mean curves conditional on
<code>x</code> and <code>y</code> respectively.</p>
</td></tr>
<tr><td><code id="plot.profile_+3A_which">which</code></td>
<td>
<p>names or number of parameters in pairs plot</p>
</td></tr>
<tr><td><code id="plot.profile_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is the main <code>plot</code> method for objects created by
<code><a href="MASS.html#topic+profile.glm">profile.glm</a></code>.  It can also be called on objects created
by <code><a href="#topic+profile.nls">profile.nls</a></code>, but they have a specific method,
<code><a href="#topic+plot.profile.nls">plot.profile.nls</a></code>.
</p>
<p>The <code>pairs</code> method shows, for each pair of parameters x and
y, two curves intersecting at the maximum likelihood estimate, which
give the loci of the points at which the tangents to the contours of
the bivariate profile likelihood become vertical and horizontal,
respectively.  In the case of an exactly bivariate normal profile
likelihood, these two curves would be straight lines giving the
conditional means of y|x and x|y, and the contours would be exactly
elliptical. The <code>which</code> argument allows you to select a subset
of parameters; the default corresponds to the set of parameters that have 
been profiled. 
</p>


<h3>Author(s)</h3>

<p>Originally, D. M. Bates and W. N. Venables for S (in 1996).
Taken from <span class="pkg">MASS</span> where these functions were re-written by
B. D. Ripley for <span class="rlang"><b>R</b></span> (by 1998).
</p>


<h3>See Also</h3>

<p><code><a href="MASS.html#topic+profile.glm">profile.glm</a></code>, <code><a href="#topic+profile.nls">profile.nls</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## see ?profile.glm for another example using glm fits.

## a version of example(profile.nls) from R &gt;= 2.8.0
fm1 &lt;- nls(demand ~ SSasympOrig(Time, A, lrc), data = BOD)
pr1 &lt;- profile(fm1, alphamax = 0.1)
stats:::plot.profile(pr1) ## override dispatch to plot.profile.nls
pairs(pr1) # a little odd since the parameters are highly correlated

## an example from ?nls
x &lt;- -(1:100)/10
y &lt;- 100 + 10 * exp(x / 2) + rnorm(x)/10
nlmod &lt;- nls(y ~  Const + A * exp(B * x), start=list(Const=100, A=10, B=1))
pairs(profile(nlmod))

## example from Dobson (1990) (see ?glm)
counts &lt;- c(18,17,15,20,10,20,25,13,12)
outcome &lt;- gl(3,1,9)
treatment &lt;- gl(3,3)
## this example is only formally a Poisson model. It is really a 
## comparison of 3 multinomials. Only the interaction parameters are of 
## interest.
glm.D93i &lt;- glm(counts ~ outcome * treatment, family = poisson())
pr1 &lt;- profile(glm.D93i)
pr2 &lt;- profile(glm.D93i, which=6:9)
plot(pr1)
plot(pr2)
pairs(pr1)
pairs(pr2)
</code></pre>

<hr>
<h2 id='plot.profile.nls'>Plot a <code>profile.nls</code> Object</h2><span id='topic+plot.profile.nls'></span>

<h3>Description</h3>

<p>Displays a series of plots of the profile t function and interpolated
confidence intervals for the parameters in a nonlinear regression
model that has been fit with <code>nls</code> and profiled with
<code>profile.nls</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'profile.nls'
plot(x, levels, conf = c(99, 95, 90, 80, 50)/100,
     absVal = TRUE, ylab = NULL, lty = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.profile.nls_+3A_x">x</code></td>
<td>
<p>an object of class <code>"profile.nls"</code> </p>
</td></tr>
<tr><td><code id="plot.profile.nls_+3A_levels">levels</code></td>
<td>
<p>levels, on the scale of the absolute value of a t
statistic, at which to interpolate intervals.  Usually <code>conf</code>
is used instead of giving <code>levels</code> explicitly.</p>
</td></tr>
<tr><td><code id="plot.profile.nls_+3A_conf">conf</code></td>
<td>
<p>a numeric vector of confidence levels for profile-based
confidence intervals on the parameters.
Defaults to <code>c(0.99, 0.95, 0.90, 0.80, 0.50).</code></p>
</td></tr>
<tr><td><code id="plot.profile.nls_+3A_absval">absVal</code></td>
<td>
<p>a logical value indicating whether or not the plots
should be on the scale of the absolute value of the profile t.
Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="plot.profile.nls_+3A_lty">lty</code></td>
<td>
<p>the line type to be used for axis and dropped lines.</p>
</td></tr>
<tr><td><code id="plot.profile.nls_+3A_ylab">ylab</code>, <code id="plot.profile.nls_+3A_...">...</code></td>
<td>
<p>other arguments to the <code><a href="graphics.html#topic+plot.default">plot.default</a></code>
function can be passed here (but not <code>xlab</code>, <code>xlim</code>,
<code>ylim</code> nor <code>type</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The plots are produced in a set of hard-coded colours, but as these
are coded by number their effect can be changed by setting the
<code><a href="grDevices.html#topic+palette">palette</a></code>.  Colour 1 is used for the axes and 4 for the
profile itself.  Colours 3 and 6 are used for the axis line at zero and
the horizontal/vertical lines dropping to the axes.
</p>


<h3>Author(s)</h3>

<p>Douglas M. Bates and Saikat DebRoy</p>


<h3>References</h3>

<p>Bates, D.M. and Watts, D.G. (1988),
<em>Nonlinear Regression Analysis and Its Applications</em>,
Wiley (chapter 6)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nls">nls</a></code>,
<code><a href="#topic+profile">profile</a></code>,
<code><a href="#topic+profile.nls">profile.nls</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

# obtain the fitted object
fm1 &lt;- nls(demand ~ SSasympOrig(Time, A, lrc), data = BOD)
# get the profile for the fitted model
pr1 &lt;- profile(fm1, alphamax = 0.05)
opar &lt;- par(mfrow = c(2,2), oma = c(1.1, 0, 1.1, 0), las = 1)
plot(pr1, conf = c(95, 90, 80, 50)/100)
plot(pr1, conf = c(95, 90, 80, 50)/100, absVal = FALSE)
mtext("Confidence intervals based on the profile sum of squares",
      side = 3, outer = TRUE)
mtext("BOD data - confidence levels of 50%, 80%, 90% and 95%",
      side = 1, outer = TRUE)
par(opar)
</code></pre>

<hr>
<h2 id='plot.spec'>Plotting Spectral Densities</h2><span id='topic+plot.spec'></span><span id='topic+plot.spec.coherency'></span><span id='topic+plot.spec.phase'></span>

<h3>Description</h3>

<p>Plotting method for objects of class <code>"spec"</code>.  For multivariate
time series it plots the marginal spectra of the series or pairs plots
of the coherency and phase of the cross-spectra.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'spec'
plot(x, add = FALSE, ci = 0.95, log = c("yes", "dB", "no"),
     xlab = "frequency", ylab = NULL, type = "l",
     ci.col = "blue", ci.lty = 3,
     main = NULL, sub = NULL,
     plot.type = c("marginal", "coherency", "phase"),
     ...)

plot.spec.phase(x, ci = 0.95,
                xlab = "frequency", ylab = "phase",
                ylim = c(-pi, pi), type = "l",
                main = NULL, ci.col = "blue", ci.lty = 3, ...)

plot.spec.coherency(x, ci = 0.95,
                    xlab = "frequency",
                    ylab = "squared coherency",
                    ylim = c(0, 1), type = "l",
                    main = NULL, ci.col = "blue", ci.lty = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.spec_+3A_x">x</code></td>
<td>
<p>an object of class <code>"spec"</code>.</p>
</td></tr>
<tr><td><code id="plot.spec_+3A_add">add</code></td>
<td>
<p>logical.  If <code>TRUE</code>, add to already existing plot.
Only valid for <code>plot.type = "marginal"</code>.
</p>
</td></tr>
<tr><td><code id="plot.spec_+3A_ci">ci</code></td>
<td>
<p>coverage probability for confidence interval.  Plotting of
the confidence bar/limits is omitted unless <code>ci</code> is strictly
positive.</p>
</td></tr>
<tr><td><code id="plot.spec_+3A_log">log</code></td>
<td>
<p>If <code>"dB"</code>, plot on log10 (decibel) scale,
otherwise use conventional log scale or linear scale.  Logical
values are also accepted.  The default is <code>"yes"</code> unless
<code>options(ts.S.compat = TRUE)</code> has been set, when it is
<code>"dB"</code>.  Only valid for <code>plot.type = "marginal"</code>.
</p>
</td></tr>
<tr><td><code id="plot.spec_+3A_xlab">xlab</code></td>
<td>
<p>the x label of the plot.</p>
</td></tr>
<tr><td><code id="plot.spec_+3A_ylab">ylab</code></td>
<td>
<p>the y label of the plot.  If missing a suitable label will
be constructed.</p>
</td></tr>
<tr><td><code id="plot.spec_+3A_type">type</code></td>
<td>
<p>the type of plot to be drawn, defaults to lines.</p>
</td></tr>
<tr><td><code id="plot.spec_+3A_ci.col">ci.col</code></td>
<td>
<p>colour for plotting confidence bar or confidence
intervals for coherency and phase.</p>
</td></tr>
<tr><td><code id="plot.spec_+3A_ci.lty">ci.lty</code></td>
<td>
<p>line type for confidence intervals for coherency and
phase.</p>
</td></tr>
<tr><td><code id="plot.spec_+3A_main">main</code></td>
<td>
<p>overall title for the plot. If missing, a suitable title
is constructed.</p>
</td></tr>
<tr><td><code id="plot.spec_+3A_sub">sub</code></td>
<td>
<p>a subtitle for the plot.  Only used for <code>plot.type =
      "marginal"</code>.  If missing, a description of the smoothing is used.</p>
</td></tr>
<tr><td><code id="plot.spec_+3A_plot.type">plot.type</code></td>
<td>
<p>For multivariate time series, the type of plot
required.  Only the first character is needed.</p>
</td></tr>
<tr><td><code id="plot.spec_+3A_ylim">ylim</code>, <code id="plot.spec_+3A_...">...</code></td>
<td>
<p>Graphical parameters.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+spectrum">spectrum</a></code>
</p>

<hr>
<h2 id='plot.stepfun'>Plot Step Functions</h2><span id='topic+plot.stepfun'></span><span id='topic+lines.stepfun'></span>

<h3>Description</h3>

<p>Method of the generic <code><a href="graphics.html#topic+plot">plot</a></code> for <code><a href="#topic+stepfun">stepfun</a></code>
objects and utility for plotting piecewise constant functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'stepfun'
plot(x, xval, xlim, ylim = range(c(y, Fn.kn)),
     xlab = "x", ylab = "f(x)", main = NULL,
     add = FALSE, verticals = TRUE, do.points = (n &lt; 1000),
     pch = par("pch"), col = par("col"),
     col.points = col, cex.points = par("cex"),
     col.hor = col, col.vert = col,
     lty = par("lty"), lwd = par("lwd"), ...)

## S3 method for class 'stepfun'
lines(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.stepfun_+3A_x">x</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object inheriting from <code>"stepfun"</code>.</p>
</td></tr>
<tr><td><code id="plot.stepfun_+3A_xval">xval</code></td>
<td>
<p>numeric vector of abscissa values at which to evaluate
<code>x</code>.  Defaults to <code><a href="#topic+knots">knots</a>(x)</code> restricted to <code>xlim</code>.</p>
</td></tr>
<tr><td><code id="plot.stepfun_+3A_xlim">xlim</code>, <code id="plot.stepfun_+3A_ylim">ylim</code></td>
<td>
<p>limits for the plot region: see
<code><a href="graphics.html#topic+plot.window">plot.window</a></code>.  Both have sensible defaults if omitted.</p>
</td></tr>
<tr><td><code id="plot.stepfun_+3A_xlab">xlab</code>, <code id="plot.stepfun_+3A_ylab">ylab</code></td>
<td>
<p>labels for x and y axis.</p>
</td></tr>
<tr><td><code id="plot.stepfun_+3A_main">main</code></td>
<td>
<p>main title.</p>
</td></tr>
<tr><td><code id="plot.stepfun_+3A_add">add</code></td>
<td>
<p>logical; if <code>TRUE</code> only <em>add</em> to an existing plot.</p>
</td></tr>
<tr><td><code id="plot.stepfun_+3A_verticals">verticals</code></td>
<td>
<p>logical;  if <code>TRUE</code>, draw vertical lines at steps.</p>
</td></tr>
<tr><td><code id="plot.stepfun_+3A_do.points">do.points</code></td>
<td>
<p>logical;  if <code>TRUE</code>, also draw points at the
(<code>xlim</code> restricted) knot locations.  Default is true, for
sample size <code class="reqn">&lt; 1000</code>.</p>
</td></tr>
<tr><td><code id="plot.stepfun_+3A_pch">pch</code></td>
<td>
<p>character; point character if <code>do.points</code>.</p>
</td></tr>
<tr><td><code id="plot.stepfun_+3A_col">col</code></td>
<td>
<p>default color of all points and lines.</p>
</td></tr>
<tr><td><code id="plot.stepfun_+3A_col.points">col.points</code></td>
<td>
<p>character or integer code; color of points if
<code>do.points</code>.</p>
</td></tr>
<tr><td><code id="plot.stepfun_+3A_cex.points">cex.points</code></td>
<td>
<p>numeric; character expansion factor if <code>do.points</code>.</p>
</td></tr>
<tr><td><code id="plot.stepfun_+3A_col.hor">col.hor</code></td>
<td>
<p>color of horizontal lines.</p>
</td></tr>
<tr><td><code id="plot.stepfun_+3A_col.vert">col.vert</code></td>
<td>
<p>color of vertical lines.</p>
</td></tr>
<tr><td><code id="plot.stepfun_+3A_lty">lty</code>, <code id="plot.stepfun_+3A_lwd">lwd</code></td>
<td>
<p>line type and thickness for all lines.</p>
</td></tr>
<tr><td><code id="plot.stepfun_+3A_...">...</code></td>
<td>
<p>further arguments of <code><a href="graphics.html#topic+plot">plot</a>(.)</code>, or if<code>(add)</code>
<code><a href="graphics.html#topic+segments">segments</a>(.)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two components
</p>
<table>
<tr><td><code>t</code></td>
<td>
<p>abscissa (x) values, including the two outermost ones.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>y values &lsquo;in between&rsquo; the <code>t[]</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Martin Maechler <a href="mailto:maechler@stat.math.ethz.ch">maechler@stat.math.ethz.ch</a>, 1990,
1993; ported to <span class="rlang"><b>R</b></span>, 1997.</p>


<h3>See Also</h3>

<p><code><a href="#topic+ecdf">ecdf</a></code> for empirical distribution functions as
special step functions,
<code><a href="#topic+approxfun">approxfun</a></code> and <code><a href="#topic+splinefun">splinefun</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

y0 &lt;- c(1,2,4,3)
sfun0  &lt;- stepfun(1:3, y0, f = 0)
sfun.2 &lt;- stepfun(1:3, y0, f = .2)
sfun1  &lt;- stepfun(1:3, y0, right = TRUE)

tt &lt;- seq(0, 3, by = 0.1)
op &lt;- par(mfrow = c(2,2))
plot(sfun0); plot(sfun0, xval = tt, add = TRUE, col.hor = "bisque")
plot(sfun.2);plot(sfun.2, xval = tt, add = TRUE, col = "orange") # all colors
plot(sfun1);lines(sfun1, xval = tt, col.hor = "coral")
##-- This is  revealing :
plot(sfun0, verticals = FALSE,
     main = "stepfun(x, y0, f=f)  for f = 0, .2, 1")
for(i in 1:3)
  lines(list(sfun0, sfun.2, stepfun(1:3, y0, f = 1))[[i]], col = i)
legend(2.5, 1.9, paste("f =", c(0, 0.2, 1)), col = 1:3, lty = 1, y.intersp = 1)
par(op)

# Extend and/or restrict 'viewport':
plot(sfun0, xlim = c(0,5), ylim = c(0, 3.5),
     main = "plot(stepfun(*), xlim= . , ylim = .)")

##-- this works too (automatic call to  ecdf(.)):
plot.stepfun(rt(50, df = 3), col.vert = "gray20")
</code></pre>

<hr>
<h2 id='plot.ts'>Plotting Time-Series Objects</h2><span id='topic+plot.ts'></span><span id='topic+lines.ts'></span>

<h3>Description</h3>

<p>Plotting method for objects inheriting from class <code>"ts"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ts'
plot(x, y = NULL, plot.type = c("multiple", "single"),
        xy.labels, xy.lines, panel = lines, nc, yax.flip = FALSE,
        mar.multi = c(0, 5.1, 0, if(yax.flip) 5.1 else 2.1),
        oma.multi = c(6, 0, 5, 0), axes = TRUE, ...)

## S3 method for class 'ts'
lines(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.ts_+3A_x">x</code>, <code id="plot.ts_+3A_y">y</code></td>
<td>
<p>time series objects, usually inheriting from class <code>"ts"</code>.</p>
</td></tr>
<tr><td><code id="plot.ts_+3A_plot.type">plot.type</code></td>
<td>
<p>for multivariate time series, should the series by
plotted separately (with a common time axis) or on a single plot?
Can be abbreviated.</p>
</td></tr>
<tr><td><code id="plot.ts_+3A_xy.labels">xy.labels</code></td>
<td>
<p>logical, indicating if <code><a href="graphics.html#topic+text">text</a>()</code> labels
should be used for an x-y plot, <em>or</em> character, supplying a
vector of labels to be used.  The default is to label for up to 150
points, and not for more.</p>
</td></tr>
<tr><td><code id="plot.ts_+3A_xy.lines">xy.lines</code></td>
<td>
<p>logical, indicating if <code><a href="graphics.html#topic+lines">lines</a></code>
should be drawn for an x-y plot.  Defaults to the value of
<code>xy.labels</code> if that is logical, otherwise to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="plot.ts_+3A_panel">panel</code></td>
<td>
<p>a <code>function(x, col, bg, pch, type, ...)</code> which gives the
action to be carried out in each panel of the display for
<code>plot.type = "multiple"</code>.  The default is <code>lines</code>.</p>
</td></tr>
<tr><td><code id="plot.ts_+3A_nc">nc</code></td>
<td>
<p>the number of columns to use when <code>type = "multiple"</code>.
Defaults to 1 for up to 4 series, otherwise to 2.</p>
</td></tr>
<tr><td><code id="plot.ts_+3A_yax.flip">yax.flip</code></td>
<td>
<p>logical indicating if the y-axis (ticks and numbering)
should flip from side 2 (left) to 4 (right) from series to series
when <code>type = "multiple"</code>.</p>
</td></tr>
<tr><td><code id="plot.ts_+3A_mar.multi">mar.multi</code>, <code id="plot.ts_+3A_oma.multi">oma.multi</code></td>
<td>
<p>the (default) <code><a href="graphics.html#topic+par">par</a></code> settings
for <code>plot.type = "multiple"</code>.  Modify with care!</p>
</td></tr>
<tr><td><code id="plot.ts_+3A_axes">axes</code></td>
<td>
<p>logical indicating if x- and y- axes should be drawn.</p>
</td></tr>
<tr><td><code id="plot.ts_+3A_...">...</code></td>
<td>
<p>additional graphical arguments, see <code><a href="graphics.html#topic+plot">plot</a></code>,
<code><a href="graphics.html#topic+plot.default">plot.default</a></code> and <code><a href="graphics.html#topic+par">par</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>y</code> is missing, this function creates a time series
plot, for multivariate series of one of two kinds depending on
<code>plot.type</code>.
</p>
<p>If <code>y</code> is present, both <code>x</code> and <code>y</code> must be univariate,
and a scatter plot <code>y ~ x</code> will be drawn, enhanced by
using <code><a href="graphics.html#topic+text">text</a></code> if <code>xy.labels</code> is
<code><a href="base.html#topic+TRUE">TRUE</a></code> or <code>character</code>, and <code><a href="graphics.html#topic+lines">lines</a></code> if
<code>xy.lines</code> is <code>TRUE</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ts">ts</a></code> for basic time series construction and access
functionality.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

## Multivariate
z &lt;- ts(matrix(rt(200 * 8, df = 3), 200, 8),
        start = c(1961, 1), frequency = 12)
plot(z, yax.flip = TRUE)
plot(z, axes = FALSE, ann = FALSE, frame.plot = TRUE,
     mar.multi = c(0,0,0,0), oma.multi = c(1,1,5,1))
title("plot(ts(..), axes=FALSE, ann=FALSE, frame.plot=TRUE, mar..., oma...)")

z &lt;- window(z[,1:3], end = c(1969,12))
plot(z, type = "b")    # multiple
plot(z, plot.type = "single", lty = 1:3, col = 4:2)

## A phase plot:
plot(nhtemp, lag(nhtemp, 1), cex = .8, col = "blue",
     main = "Lag plot of New Haven temperatures")

## xy.lines and xy.labels are FALSE for large series:
plot(lag(sunspots, 1), sunspots, pch = ".")

SMI &lt;- EuStockMarkets[, "SMI"]
plot(lag(SMI,  1), SMI, pch = ".")
plot(lag(SMI, 20), SMI, pch = ".", log = "xy",
     main = "4 weeks lagged SMI stocks -- log scale", xy.lines =  TRUE)
</code></pre>

<hr>
<h2 id='Poisson'>The Poisson Distribution</h2><span id='topic+Poisson'></span><span id='topic+dpois'></span><span id='topic+ppois'></span><span id='topic+qpois'></span><span id='topic+rpois'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the Poisson distribution with parameter <code>lambda</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dpois(x, lambda, log = FALSE)
ppois(q, lambda, lower.tail = TRUE, log.p = FALSE)
qpois(p, lambda, lower.tail = TRUE, log.p = FALSE)
rpois(n, lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Poisson_+3A_x">x</code></td>
<td>
<p>vector of (non-negative integer) quantiles.</p>
</td></tr>
<tr><td><code id="Poisson_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="Poisson_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="Poisson_+3A_n">n</code></td>
<td>
<p>number of random values to return.</p>
</td></tr>
<tr><td><code id="Poisson_+3A_lambda">lambda</code></td>
<td>
<p>vector of (non-negative) means.</p>
</td></tr>
<tr><td><code id="Poisson_+3A_log">log</code>, <code id="Poisson_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="Poisson_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Poisson distribution has density

</p>
<p style="text-align: center;"><code class="reqn">p(x) = \frac{\lambda^x e^{-\lambda}}{x!}</code>
</p>

<p>for <code class="reqn">x = 0, 1, 2, \ldots</code> .
The mean and variance are <code class="reqn">E(X) = Var(X) = \lambda</code>.
</p>
<p>Note that <code class="reqn">\lambda = 0</code> is really a limit case (setting
<code class="reqn">0^0 = 1</code>) resulting in a point mass at <code class="reqn">0</code>, see also the example.
</p>
<p>If an element of <code>x</code> is not integer, the result of <code>dpois</code>
is zero, with a warning.
<code class="reqn">p(x)</code> is computed using Loader's algorithm, see the reference in
<code><a href="#topic+dbinom">dbinom</a></code>.
</p>
<p>The quantile is right continuous: <code>qpois(p, lambda)</code> is the smallest
integer <code class="reqn">x</code> such that <code class="reqn">P(X \le x) \ge p</code>.
</p>
<p>Setting <code>lower.tail = FALSE</code> allows to get much more precise
results when the default, <code>lower.tail = TRUE</code> would return 1, see
the example below.
</p>


<h3>Value</h3>

<p><code>dpois</code> gives the (log) density,
<code>ppois</code> gives the (log) distribution function,
<code>qpois</code> gives the quantile function, and
<code>rpois</code> generates random deviates.
</p>
<p>Invalid <code>lambda</code> will result in return value <code>NaN</code>, with a warning.
</p>
<p>The length of the result is determined by <code>n</code> for
<code>rpois</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.
</p>
<p>The numerical arguments other than <code>n</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.
</p>
<p><code>rpois</code> returns a vector of type <a href="base.html#topic+integer">integer</a> unless generated
values exceed the maximum representable integer when <code><a href="base.html#topic+double">double</a></code>
values are returned.
</p>


<h3>Source</h3>

<p><code>dpois</code> uses C code contributed by Catherine Loader
(see <code><a href="#topic+dbinom">dbinom</a></code>).
</p>
<p><code>ppois</code> uses <code>pgamma</code>.
</p>
<p><code>qpois</code> uses the Cornish&ndash;Fisher Expansion to include a skewness
correction to a normal approximation, followed by a search.
</p>
<p><code>rpois</code> uses
</p>
<p>Ahrens, J. H. and Dieter, U. (1982).
Computer generation of Poisson deviates from modified normal distributions.
<em>ACM Transactions on Mathematical Software</em>, <b>8</b>, 163&ndash;179.
</p>


<h3>See Also</h3>

<p><a href="#topic+Distributions">Distributions</a> for other standard distributions, including
<code><a href="#topic+dbinom">dbinom</a></code> for the binomial and <code><a href="#topic+dnbinom">dnbinom</a></code> for
the negative binomial distribution.
</p>
<p><code><a href="#topic+poisson.test">poisson.test</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

-log(dpois(0:7, lambda = 1) * gamma(1+ 0:7)) # == 1
Ni &lt;- rpois(50, lambda = 4); table(factor(Ni, 0:max(Ni)))

1 - ppois(10*(15:25), lambda = 100)  # becomes 0 (cancellation)
    ppois(10*(15:25), lambda = 100, lower.tail = FALSE)  # no cancellation

par(mfrow = c(2, 1))
x &lt;- seq(-0.01, 5, 0.01)
plot(x, ppois(x, 1), type = "s", ylab = "F(x)", main = "Poisson(1) CDF")
plot(x, pbinom(x, 100, 0.01), type = "s", ylab = "F(x)",
     main = "Binomial(100, 0.01) CDF")

## The (limit) case  lambda = 0 :
stopifnot(identical(dpois(0,0), 1),
	  identical(ppois(0,0), 1),
	  identical(qpois(1,0), 0))
</code></pre>

<hr>
<h2 id='poisson.test'>Exact Poisson tests</h2><span id='topic+poisson.test'></span>

<h3>Description</h3>

<p>Performs an exact test of a simple null hypothesis about the
rate parameter in Poisson distribution, or for the
ratio between two rate parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>poisson.test(x, T = 1, r = 1,
    alternative = c("two.sided", "less", "greater"),
    conf.level = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="poisson.test_+3A_x">x</code></td>
<td>
<p>number of events. A vector of length one or two.</p>
</td></tr>
<tr><td><code id="poisson.test_+3A_t">T</code></td>
<td>
<p>time base for event count. A vector of length one or two. </p>
</td></tr>
<tr><td><code id="poisson.test_+3A_r">r</code></td>
<td>
<p>hypothesized rate or rate ratio</p>
</td></tr>
<tr><td><code id="poisson.test_+3A_alternative">alternative</code></td>
<td>
<p>indicates the alternative hypothesis and must be
one of <code>"two.sided"</code>, <code>"greater"</code> or <code>"less"</code>.
You can specify just the initial letter.</p>
</td></tr>
<tr><td><code id="poisson.test_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level for the returned confidence
interval.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Confidence intervals are computed similarly to those of
<code><a href="#topic+binom.test">binom.test</a></code> in the one-sample case, and using
<code><a href="#topic+binom.test">binom.test</a></code> in the two sample case.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the number of events (in the first sample if there
are two.)</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the corresponding expected count</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>a confidence interval for the rate or rate ratio.</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>the estimated rate or rate ratio.</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>the rate or rate ratio under the null,
<code>r</code>.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative
hypothesis.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the character string <code>"Exact Poisson test"</code> or
<code>"Comparison of Poisson rates"</code> as appropriate.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the names of the data.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The rate parameter in Poisson data is often given based on a
&ldquo;time on test&rdquo; or similar quantity (person-years, population
size, or expected number of cases from mortality tables). This is the
role of the <code>T</code> argument.
</p>
<p>The one-sample case is effectively the binomial test with a very large
<code>n</code>. The two sample case is converted to a binomial test by
conditioning on the total event count, and the rate ratio is directly
related to the odds in that binomial distribution.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+binom.test">binom.test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### These are paraphrased from data sets in the ISwR package

## SMR, Welsh Nickel workers
poisson.test(137, 24.19893)

## eba1977, compare Fredericia to other three cities for ages 55-59
poisson.test(c(11, 6+8+7), c(800, 1083+1050+878))
</code></pre>

<hr>
<h2 id='poly'>Compute Orthogonal Polynomials</h2><span id='topic+poly'></span><span id='topic+polym'></span><span id='topic+predict.poly'></span><span id='topic+makepredictcall.poly'></span>

<h3>Description</h3>

<p>Returns or evaluates orthogonal polynomials of degree 1 to
<code>degree</code> over the specified set of points <code>x</code>: these are all
orthogonal to the constant polynomial of degree 0.  Alternatively,
evaluate raw polynomials.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>poly(x, ..., degree = 1, coefs = NULL, raw = FALSE, simple = FALSE)
polym  (..., degree = 1, coefs = NULL, raw = FALSE)

## S3 method for class 'poly'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="poly_+3A_x">x</code>, <code id="poly_+3A_newdata">newdata</code></td>
<td>
<p>a numeric vector or an object with <code><a href="base.html#topic+mode">mode</a></code>
<code>"numeric"</code> (such as a <code><a href="base.html#topic+Date">Date</a></code>) at which to evaluate the
polynomial. <code>x</code> can also be a matrix.  Missing values are not
allowed in <code>x</code>.</p>
</td></tr>
<tr><td><code id="poly_+3A_degree">degree</code></td>
<td>
<p>the degree of the polynomial.  Must be less than the
number of unique points when <code>raw</code> is false, as by default.</p>
</td></tr>
<tr><td><code id="poly_+3A_coefs">coefs</code></td>
<td>
<p>for prediction, coefficients from a previous fit.</p>
</td></tr>
<tr><td><code id="poly_+3A_raw">raw</code></td>
<td>
<p>if true, use raw and not orthogonal polynomials.</p>
</td></tr>
<tr><td><code id="poly_+3A_simple">simple</code></td>
<td>
<p>logical indicating if a simple matrix (with no further
<code><a href="base.html#topic+attributes">attributes</a></code> but <code><a href="base.html#topic+dimnames">dimnames</a></code>) should be
returned.  For speedup only.</p>
</td></tr>
<tr><td><code id="poly_+3A_object">object</code></td>
<td>
<p>an object inheriting from class <code>"poly"</code>, normally
the result of a call to <code>poly</code> with a single vector argument.</p>
</td></tr>
<tr><td><code id="poly_+3A_...">...</code></td>
<td>
<p><code>poly</code>, <code>polym</code>: further vectors.<br />
<code>predict.poly</code>: arguments to be passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Although formally <code>degree</code> should be named (as it follows
<code>...</code>), an unnamed second argument of length 1 will be
interpreted as the degree, such that <code>poly(x, 3)</code> can be used in
formulas.
</p>
<p>The orthogonal polynomial is summarized by the coefficients, which can
be used to evaluate it via the three-term recursion given in Kennedy
&amp; Gentle (1980, pp. 343&ndash;4), and used in the <code>predict</code> part of
the code.
</p>
<p><code>poly</code> using <code>...</code> is just a convenience wrapper for
<code>polym</code>: <code>coef</code> is ignored.  Conversely, if <code>polym</code> is
called with a single argument in <code>...</code> it is a wrapper for
<code>poly</code>.
</p>


<h3>Value</h3>

<p>For <code>poly</code> and <code>polym()</code> (when <code>simple=FALSE</code> and
<code>coefs=NULL</code> as per default):<br />
A matrix with rows corresponding to points in <code>x</code> and columns
corresponding to the degree, with attributes <code>"degree"</code> specifying
the degrees of the columns and (unless <code>raw = TRUE</code>)
<code>"coefs"</code> which contains the centering and normalization
constants used in constructing the orthogonal polynomials and
class <code>c("poly", "matrix")</code>.
</p>
<p>For <code>poly(*, simple=TRUE)</code>, <code>polym(*, coefs=&lt;non-NULL&gt;)</code>,
and <code>predict.poly()</code>: a matrix.
</p>


<h3>Note</h3>

<p>This routine is intended for statistical purposes such as
<code>contr.poly</code>: it does not attempt to orthogonalize to
machine accuracy.
</p>


<h3>Author(s)</h3>

<p>R Core Team.  Keith Jewell (Campden BRI Group, UK) contributed
improvements for correct prediction on subsets.
</p>


<h3>References</h3>

<p>Chambers, J. M. and Hastie, T. J. (1992)
<em>Statistical Models in S</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Kennedy, W. J. Jr and Gentle, J. E. (1980)
<em>Statistical Computing</em> Marcel Dekker.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+contr.poly">contr.poly</a></code>.
</p>
<p><code><a href="datasets.html#topic+cars">cars</a></code> for an example of polynomial regression.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>od &lt;- options(digits = 3) # avoid too much visual clutter
(z &lt;- poly(1:10, 3))
predict(z, seq(2, 4, 0.5))
zapsmall(poly(seq(4, 6, 0.5), 3, coefs = attr(z, "coefs")))

 zm &lt;- zapsmall(polym (    1:4, c(1, 4:6),  degree = 3)) # or just poly():
(z1 &lt;- zapsmall(poly(cbind(1:4, c(1, 4:6)), degree = 3)))
## they are the same :
stopifnot(all.equal(zm, z1, tolerance = 1e-15))

## poly(&lt;matrix&gt;, df) --- used to fail till July 14 (vive la France!), 2017:
m2 &lt;- cbind(1:4, c(1, 4:6))
pm2 &lt;- zapsmall(poly(m2, 3)) # "unnamed degree = 3"
stopifnot(all.equal(pm2, zm, tolerance = 1e-15))

options(od)
</code></pre>

<hr>
<h2 id='power'>Create a Power Link Object</h2><span id='topic+power'></span>

<h3>Description</h3>

<p>Creates a link object based on the link function
<code class="reqn">\eta = \mu ^ \lambda</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>power(lambda = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="power_+3A_lambda">lambda</code></td>
<td>
<p>a real number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>lambda</code> is non-positive, it is taken as zero, and the log
link is obtained.  The default <code>lambda = 1</code> gives the identity
link.
</p>


<h3>Value</h3>

<p>A list with components <code>linkfun</code>, <code>linkinv</code>,
<code>mu.eta</code>, and <code>valideta</code>.
See <code><a href="#topic+make.link">make.link</a></code> for information on their meaning.
</p>


<h3>References</h3>

<p>Chambers, J. M. and Hastie, T. J. (1992)
<em>Statistical Models in S.</em>
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+make.link">make.link</a></code>,
<code><a href="#topic+family">family</a></code>
</p>
<p>To raise a number to a power, see <code><a href="base.html#topic+Arithmetic">Arithmetic</a></code>.
</p>
<p>To calculate the power of a test, see various functions in the
<span class="pkg">stats</span> package, e.g., <code><a href="#topic+power.t.test">power.t.test</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>power()
quasi(link = power(1/3))[c("linkfun", "linkinv")]
</code></pre>

<hr>
<h2 id='power.anova.test'>Power Calculations for Balanced One-Way Analysis of Variance Tests</h2><span id='topic+power.anova.test'></span>

<h3>Description</h3>

<p>Compute power of test or determine parameters to obtain target power.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>power.anova.test(groups = NULL, n = NULL,
                 between.var = NULL, within.var = NULL,
                 sig.level = 0.05, power = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="power.anova.test_+3A_groups">groups</code></td>
<td>
<p>Number of groups</p>
</td></tr>
<tr><td><code id="power.anova.test_+3A_n">n</code></td>
<td>
<p>Number of observations (per group)</p>
</td></tr>
<tr><td><code id="power.anova.test_+3A_between.var">between.var</code></td>
<td>
<p>Between group variance</p>
</td></tr>
<tr><td><code id="power.anova.test_+3A_within.var">within.var</code></td>
<td>
<p>Within group variance</p>
</td></tr>
<tr><td><code id="power.anova.test_+3A_sig.level">sig.level</code></td>
<td>
<p>Significance level (Type I error probability)</p>
</td></tr>
<tr><td><code id="power.anova.test_+3A_power">power</code></td>
<td>
<p>Power of test (1 minus Type II error probability)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Exactly one of the parameters <code>groups</code>, <code>n</code>, <code>between.var</code>,
<code>power</code>, <code>within.var</code>, and <code>sig.level</code> must be passed as NULL,
and that parameter is determined from the others. Notice that
<code>sig.level</code> has non-NULL default so NULL must be explicitly
passed if you want it computed.
</p>


<h3>Value</h3>

<p>Object of class <code>"power.htest"</code>, a list of the arguments
(including the computed one) augmented with <code>method</code> and
<code>note</code> elements.
</p>


<h3>Note</h3>

<p><code>uniroot</code> is used to solve power equation for unknowns, so
you may see errors from it, notably about inability to bracket the
root when invalid arguments are given.
</p>


<h3>Author(s)</h3>

<p>Claus Ekstrm</p>


<h3>See Also</h3>

<p><code><a href="#topic+anova">anova</a></code>, <code><a href="#topic+lm">lm</a></code>, <code><a href="#topic+uniroot">uniroot</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>power.anova.test(groups = 4, n = 5, between.var = 1, within.var = 3)
# Power = 0.3535594

power.anova.test(groups = 4, between.var = 1, within.var = 3,
                 power = .80)
# n = 11.92613

## Assume we have prior knowledge of the group means:
groupmeans &lt;- c(120, 130, 140, 150)
power.anova.test(groups = length(groupmeans),
                 between.var = var(groupmeans),
                 within.var = 500, power = .90) # n = 15.18834
</code></pre>

<hr>
<h2 id='power.prop.test'>Power Calculations for Two-Sample Test for Proportions</h2><span id='topic+power.prop.test'></span>

<h3>Description</h3>

<p>Compute the power of the two-sample test for proportions, or determine
parameters to obtain a target power.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>power.prop.test(n = NULL, p1 = NULL, p2 = NULL, sig.level = 0.05,
                power = NULL,
                alternative = c("two.sided", "one.sided"),
                strict = FALSE, tol = .Machine$double.eps^0.25)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="power.prop.test_+3A_n">n</code></td>
<td>
<p>number of observations (per group)</p>
</td></tr>
<tr><td><code id="power.prop.test_+3A_p1">p1</code></td>
<td>
<p>probability in one group</p>
</td></tr>
<tr><td><code id="power.prop.test_+3A_p2">p2</code></td>
<td>
<p>probability in other group</p>
</td></tr>
<tr><td><code id="power.prop.test_+3A_sig.level">sig.level</code></td>
<td>
<p>significance level (Type I error probability)</p>
</td></tr>
<tr><td><code id="power.prop.test_+3A_power">power</code></td>
<td>
<p>power of test (1 minus Type II error probability)</p>
</td></tr>
<tr><td><code id="power.prop.test_+3A_alternative">alternative</code></td>
<td>
<p>one- or two-sided test.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="power.prop.test_+3A_strict">strict</code></td>
<td>
<p>use strict interpretation in two-sided case</p>
</td></tr>
<tr><td><code id="power.prop.test_+3A_tol">tol</code></td>
<td>
<p>numerical tolerance used in root finding, the default
providing (at least) four significant digits.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Exactly one of the parameters <code>n</code>, <code>p1</code>, <code>p2</code>,
<code>power</code>, and <code>sig.level</code> must be passed as NULL, and that
parameter is determined from the others.  Notice that <code>sig.level</code>
has a non-NULL default so <code>NULL</code> must be explicitly passed if you
want it computed.
</p>
<p>If <code>strict = TRUE</code> is used, the power will include the probability of
rejection in the opposite direction of the true effect, in the two-sided
case.  Without this the power will be half the significance level if the
true difference is zero.
</p>
<p>Note that not all conditions can be satisfied, e.g., for </p>
<pre>power.prop.test(n=30, p1=0.90, p2=NULL, power=0.8, strict=TRUE)</pre>
<p>there is no proportion <code>p2</code> between <code>p1 = 0.9</code> and 1, as
you'd need a sample size of at least <code class="reqn">n = 74</code> to yield the
desired power for <code class="reqn">(p1,p2) = (0.9, 1)</code>.
</p>
<p>For these impossible conditions, currently a warning
(<code><a href="base.html#topic+warning">warning</a></code>) is signalled which may become an error
(<code><a href="base.html#topic+stop">stop</a></code>) in the future.
</p>


<h3>Value</h3>

<p>Object of class <code>"power.htest"</code>, a list of the arguments
(including the computed one) augmented with <code>method</code> and
<code>note</code> elements.
</p>


<h3>Note</h3>

<p><code><a href="#topic+uniroot">uniroot</a></code> is used to solve power equation for unknowns, so
you may see errors from it, notably about inability to bracket the
root when invalid arguments are given.   If one of <code>p1</code> and
<code>p2</code> is computed, then <code class="reqn">p1 &lt; p2</code> is assumed and will hold,
but if you specify both, <code class="reqn">p2 \le p1</code> is allowed.
</p>


<h3>Author(s)</h3>

<p>Peter Dalgaard.  Based on previous work by Claus
Ekstrm</p>


<h3>See Also</h3>

<p><code><a href="#topic+prop.test">prop.test</a></code>, <code><a href="#topic+uniroot">uniroot</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>power.prop.test(n = 50, p1 = .50, p2 = .75)      ## =&gt; power = 0.740
power.prop.test(p1 = .50, p2 = .75, power = .90) ## =&gt;     n = 76.7
power.prop.test(n = 50, p1 = .5, power = .90)    ## =&gt;    p2 = 0.8026
power.prop.test(n = 50, p1 = .5, p2 = 0.9, power = .90, sig.level=NULL)
                                                 ## =&gt; sig.l = 0.00131
power.prop.test(p1 = .5, p2 = 0.501, sig.level=.001, power=0.90)
                                                 ## =&gt; n = 10451937
try(
 power.prop.test(n=30, p1=0.90, p2=NULL, power=0.8)
) # a warning  (which may become an error)
## Reason:
power.prop.test(      p1=0.90, p2= 1.0, power=0.8) ##-&gt; n = 73.37
</code></pre>

<hr>
<h2 id='power.t.test'>Power calculations for one and two sample t tests</h2><span id='topic+power.t.test'></span>

<h3>Description</h3>

<p>Compute the power of the one- or two- sample t test,
or determine parameters to obtain a target power.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>power.t.test(n = NULL, delta = NULL, sd = 1, sig.level = 0.05,
             power = NULL,
             type = c("two.sample", "one.sample", "paired"),
             alternative = c("two.sided", "one.sided"),
             strict = FALSE, tol = .Machine$double.eps^0.25)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="power.t.test_+3A_n">n</code></td>
<td>
<p>number of observations (per group)</p>
</td></tr>
<tr><td><code id="power.t.test_+3A_delta">delta</code></td>
<td>
<p>true difference in means</p>
</td></tr>
<tr><td><code id="power.t.test_+3A_sd">sd</code></td>
<td>
<p>standard deviation</p>
</td></tr>
<tr><td><code id="power.t.test_+3A_sig.level">sig.level</code></td>
<td>
<p>significance level (Type I error probability)</p>
</td></tr>
<tr><td><code id="power.t.test_+3A_power">power</code></td>
<td>
<p>power of test (1 minus Type II error probability)</p>
</td></tr>
<tr><td><code id="power.t.test_+3A_type">type</code></td>
<td>
<p>string specifying the type of t test.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="power.t.test_+3A_alternative">alternative</code></td>
<td>
<p>one- or two-sided test.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="power.t.test_+3A_strict">strict</code></td>
<td>
<p>use strict interpretation in two-sided case</p>
</td></tr>
<tr><td><code id="power.t.test_+3A_tol">tol</code></td>
<td>
<p>numerical tolerance used in root finding, the default
providing (at least) four significant digits.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Exactly one of the parameters <code>n</code>, <code>delta</code>, <code>power</code>,
<code>sd</code>, and <code>sig.level</code> must be passed as <code>NULL</code>, and that
parameter is determined from the others.  Notice that the last two have
non-NULL defaults, so NULL must be explicitly passed if you want to
compute them.
</p>
<p>If <code>strict = TRUE</code> is used, the power will include the probability of
rejection in the opposite direction of the true effect, in the two-sided
case.  Without this the power will be half the significance level if the
true difference is zero.
</p>


<h3>Value</h3>

<p>Object of class <code>"power.htest"</code>, a list of the arguments
(including the computed one) augmented with <code>method</code> and
<code>note</code> elements.
</p>


<h3>Note</h3>

<p><code>uniroot</code> is used to solve the power equation for unknowns, so
you may see errors from it, notably about inability to bracket the
root when invalid arguments are given.
</p>


<h3>Author(s)</h3>

<p>Peter Dalgaard.  Based on previous work by Claus
Ekstrm</p>


<h3>See Also</h3>

<p><code><a href="#topic+t.test">t.test</a></code>, <code><a href="#topic+uniroot">uniroot</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'> power.t.test(n = 20, delta = 1)
 power.t.test(power = .90, delta = 1)
 power.t.test(power = .90, delta = 1, alternative = "one.sided")
</code></pre>

<hr>
<h2 id='PP.test'>Phillips-Perron Test for Unit Roots</h2><span id='topic+PP.test'></span>

<h3>Description</h3>

<p>Computes the Phillips-Perron test for the null hypothesis that
<code>x</code> has a unit root against a stationary alternative.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PP.test(x, lshort = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PP.test_+3A_x">x</code></td>
<td>
<p>a numeric vector or univariate time series.</p>
</td></tr>
<tr><td><code id="PP.test_+3A_lshort">lshort</code></td>
<td>
<p>a logical indicating whether the short or long version
of the truncation lag parameter is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The general regression equation which incorporates a constant and a
linear trend is used and the corrected t-statistic for a first order
autoregressive coefficient equals one is computed.  To estimate
<code>sigma^2</code> the Newey-West estimator is used.  If <code>lshort</code>
is <code>TRUE</code>, then the truncation lag parameter is set to
<code>trunc(4*(n/100)^0.25)</code>, otherwise
<code>trunc(12*(n/100)^0.25)</code> is used.  The p-values are
interpolated from Table 4.2, page 103 of
Banerjee <abbr>et al.</abbr> (1993).
</p>
<p>Missing values are not handled.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the truncation lag parameter.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string indicating what type of test was
performed.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the name of the data.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>A. Trapletti</p>


<h3>References</h3>

<p>A. Banerjee, J. J. Dolado, J. W. Galbraith, and D. F. Hendry (1993).
<em>Cointegration, Error Correction, and the Econometric Analysis
of Non-Stationary Data</em>.
Oxford University Press, Oxford.
</p>
<p>P. Perron (1988).
Trends and random walks in macroeconomic time series.
<em>Journal of Economic Dynamics and Control</em>, <b>12</b>, 297&ndash;332.
<a href="https://doi.org/10.1016/0165-1889%2888%2990043-7">doi:10.1016/0165-1889(88)90043-7</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(1000)
PP.test(x)
y &lt;- cumsum(x) # has unit root
PP.test(y)
</code></pre>

<hr>
<h2 id='ppoints'>Ordinates for Probability Plotting</h2><span id='topic+ppoints'></span>

<h3>Description</h3>

<p>Generates the sequence of probability points
<code>(1:m - a)/(m + (1-a)-a)</code>
where <code>m</code> is either <code>n</code>, if <code>length(n)==1</code>, or
<code>length(n)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ppoints(n, a = if(n &lt;= 10) 3/8 else 1/2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ppoints_+3A_n">n</code></td>
<td>
<p>either the number of points generated or a vector of
observations.</p>
</td></tr>
<tr><td><code id="ppoints_+3A_a">a</code></td>
<td>
<p>the offset fraction to be used; typically in <code class="reqn">(0,1)</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code class="reqn">0 &lt; a &lt; 1</code>, the resulting values are within <code class="reqn">(0,1)</code>
(excluding boundaries).
In any case, the resulting sequence is symmetric in <code class="reqn">[0,1]</code>, i.e.,
<code>p + rev(p) == 1</code>.
</p>
<p><code>ppoints()</code> is used in <code>qqplot</code> and <code>qqnorm</code> to generate
the set of probabilities at which to evaluate the inverse distribution.
</p>
<p>The choice of <code>a</code> follows the documentation of the function of the
same name in Becker <abbr>et al.</abbr> (1988), and appears to have been
motivated by results from Blom (1958) on approximations to expect normal
order statistics (see also <code><a href="#topic+quantile">quantile</a></code>).
</p>
<p>The probability points for the continuous sample quantile types 5 to 9
(see <code><a href="#topic+quantile">quantile</a></code>) can be obtained by taking <code>a</code> as,
respectively, 1/2, 0, 1, 1/3, and 3/8.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Blom, G. (1958)
<em>Statistical Estimates and Transformed Beta Variables.</em>
Wiley
</p>


<h3>See Also</h3>

<p><code><a href="#topic+qqplot">qqplot</a></code>, <code><a href="#topic+qqnorm">qqnorm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ppoints(4) # the same as  ppoints(1:4)
ppoints(10)
ppoints(10, a = 1/2)

## Visualize including the fractions :
require(graphics)
p.ppoints &lt;- function(n, ..., add = FALSE, col = par("col")) {
  pn &lt;- ppoints(n, ...)
  if(add)
      points(pn, pn, col = col)
  else {
      tit &lt;- match.call(); tit[[1]] &lt;- quote(ppoints)
      plot(pn,pn, main = deparse(tit), col=col,
           xlim = 0:1, ylim = 0:1, xaxs = "i", yaxs = "i")
      abline(0, 1, col = adjustcolor(1, 1/4), lty = 3)
  }
  if(!add &amp;&amp; requireNamespace("MASS", quietly = TRUE))
    text(pn, pn, as.character(MASS::fractions(pn)),
         adj = c(0,0)-1/4, cex = 3/4, xpd = NA, col=col)
  abline(h = pn, v = pn, col = adjustcolor(col, 1/2), lty = 2, lwd = 1/2)
}

p.ppoints(4)
p.ppoints(10)
p.ppoints(10, a = 1/2)
p.ppoints(21)
p.ppoints(8) ; p.ppoints(8, a = 1/2, add=TRUE, col="tomato")

</code></pre>

<hr>
<h2 id='ppr'>Projection Pursuit Regression</h2><span id='topic+ppr'></span><span id='topic+ppr.default'></span><span id='topic+ppr.formula'></span>

<h3>Description</h3>

<p>Fit a projection pursuit regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ppr(x, ...)

## S3 method for class 'formula'
ppr(formula, data, weights, subset, na.action,
    contrasts = NULL, ..., model = FALSE)

## Default S3 method:
ppr(x, y, weights = rep(1, n),
    ww = rep(1, q), nterms, max.terms = nterms, optlevel = 2,
    sm.method = c("supsmu", "spline", "gcvspline"),
    bass = 0, span = 0, df = 5, gcvpen = 1, trace = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ppr_+3A_formula">formula</code></td>
<td>

<p>a formula specifying one or more numeric response variables and the
explanatory variables.
</p>
</td></tr>
<tr><td><code id="ppr_+3A_x">x</code></td>
<td>

<p>numeric matrix of explanatory variables.  Rows represent observations, and
columns represent variables.  Missing values are not accepted.
</p>
</td></tr>
<tr><td><code id="ppr_+3A_y">y</code></td>
<td>

<p>numeric matrix of response variables.  Rows represent observations, and
columns represent variables.  Missing values are not accepted.
</p>
</td></tr>
<tr><td><code id="ppr_+3A_nterms">nterms</code></td>
<td>
<p>number of terms to include in the final model.</p>
</td></tr>
<tr><td><code id="ppr_+3A_data">data</code></td>
<td>

<p>a data frame (or similar: see <code><a href="#topic+model.frame">model.frame</a></code>) from which
variables specified in <code>formula</code> are preferentially to be taken.
</p>
</td></tr>
<tr><td><code id="ppr_+3A_weights">weights</code></td>
<td>
<p>a vector of weights <code>w_i</code> for each <em>case</em>.</p>
</td></tr>
<tr><td><code id="ppr_+3A_ww">ww</code></td>
<td>

<p>a vector of weights for each <em>response</em>, so the fit criterion is
the sum over case <code>i</code> and responses <code>j</code> of
<code>w_i ww_j (y_ij - fit_ij)^2</code> divided by the sum of <code>w_i</code>.
</p>
</td></tr>
<tr><td><code id="ppr_+3A_subset">subset</code></td>
<td>

<p>an index vector specifying the cases to be used in the training
sample.  (NOTE: If given, this argument must be named.)
</p>
</td></tr>
<tr><td><code id="ppr_+3A_na.action">na.action</code></td>
<td>

<p>a function to specify the action to be taken if <code><a href="base.html#topic+NA">NA</a></code>s are
found. The default action is given by <code>getOption("na.action")</code>.
(NOTE: If given, this argument must be named.)
</p>
</td></tr>
<tr><td><code id="ppr_+3A_contrasts">contrasts</code></td>
<td>

<p>the contrasts to be used when any factor explanatory variables are coded.
</p>
</td></tr>
<tr><td><code id="ppr_+3A_max.terms">max.terms</code></td>
<td>

<p>maximum number of terms to choose from when building the model.
</p>
</td></tr>
<tr><td><code id="ppr_+3A_optlevel">optlevel</code></td>
<td>

<p>integer from 0 to 3 which determines the thoroughness of an
optimization routine in the SMART program. See the &lsquo;Details&rsquo;
section.
</p>
</td></tr>
<tr><td><code id="ppr_+3A_sm.method">sm.method</code></td>
<td>

<p>the method used for smoothing the ridge functions.  The default is
to use Friedman's super smoother <code><a href="#topic+supsmu">supsmu</a></code>.  The
alternatives are to use the smoothing spline code underlying
<code><a href="#topic+smooth.spline">smooth.spline</a></code>, either with a specified (equivalent)
degrees of freedom for each ridge functions, or to allow the
smoothness to be chosen by GCV.
</p>
<p>Can be abbreviated.
</p>
</td></tr>
<tr><td><code id="ppr_+3A_bass">bass</code></td>
<td>

<p>super smoother bass tone control used with automatic span selection
(see <code>supsmu</code>); the range of values is 0 to 10, with larger values
resulting in increased smoothing.
</p>
</td></tr>
<tr><td><code id="ppr_+3A_span">span</code></td>
<td>

<p>super smoother span control (see <code><a href="#topic+supsmu">supsmu</a></code>).  The default, <code>0</code>,
results in automatic span selection by local cross validation. <code>span</code>
can also take a value in <code>(0, 1]</code>.
</p>
</td></tr>
<tr><td><code id="ppr_+3A_df">df</code></td>
<td>

<p>if <code>sm.method</code> is <code>"spline"</code> specifies the smoothness of
each ridge term via the requested equivalent degrees of freedom.
</p>
</td></tr>
<tr><td><code id="ppr_+3A_gcvpen">gcvpen</code></td>
<td>

<p>if <code>sm.method</code> is <code>"gcvspline"</code> this is the penalty used
in the GCV selection for each degree of freedom used.
</p>
</td></tr>
<tr><td><code id="ppr_+3A_trace">trace</code></td>
<td>
<p>logical indicating if each spline fit should produce
diagnostic output (about <code>lambda</code> and <code>df</code>), and the
<code>supsmu</code> fit about its steps.</p>
</td></tr>
<tr><td><code id="ppr_+3A_...">...</code></td>
<td>
<p>arguments to be passed to or from other methods.</p>
</td></tr>
<tr><td><code id="ppr_+3A_model">model</code></td>
<td>
<p>logical.  If true, the model frame is returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The basic method is given by Friedman (1984) and based on his
code. This code has been shown to be extremely sensitive to the
Fortran compiler used.
</p>
<p>The algorithm first adds up to <code>max.terms</code> ridge terms one at a
time; it will use less if it is unable to find a term to add that makes
sufficient difference.  It then removes the least
important term at each step until <code>nterms</code> terms
are left.
</p>
<p>The levels of optimization (argument <code>optlevel</code>)
differ in how thoroughly the models are refitted during this process.
At level 0 the existing ridge terms are not refitted.  At level 1
the projection directions are not refitted, but the ridge
functions and the regression coefficients are.

Levels 2 and 3 refit all the terms and are equivalent for one
response; level 3 is more careful to re-balance the contributions
from each regressor at each step and so is a little less likely to
converge to a saddle point of the sum of squares criterion.
</p>


<h3>Value</h3>

<p>A list with the following components, many of which are for use by the
method functions.
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the matched call</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>the number of explanatory variables (after any coding)</p>
</td></tr>
<tr><td><code>q</code></td>
<td>
<p>the number of response variables</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>the argument <code>nterms</code></p>
</td></tr>
<tr><td><code>ml</code></td>
<td>
<p>the argument <code>max.terms</code></p>
</td></tr>
<tr><td><code>gof</code></td>
<td>
<p>the overall residual (weighted) sum of squares for the
selected model</p>
</td></tr>
<tr><td><code>gofn</code></td>
<td>
<p>the overall residual (weighted) sum of squares against the
number of terms, up to <code>max.terms</code>.  Will be invalid (and zero)
for less than <code>nterms</code>.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>the argument <code>df</code></p>
</td></tr>
<tr><td><code>edf</code></td>
<td>
<p>if <code>sm.method</code> is <code>"spline"</code> or <code>"gcvspline"</code>
the equivalent number of degrees of freedom for each ridge term used.</p>
</td></tr>
<tr><td><code>xnames</code></td>
<td>
<p>the names of the explanatory variables</p>
</td></tr>
<tr><td><code>ynames</code></td>
<td>
<p>the names of the response variables</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>a matrix of the projection directions, with a column for
each ridge term</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>a matrix of the coefficients applied for each response to
the ridge terms: the rows are the responses and the columns the ridge terms</p>
</td></tr>
<tr><td><code>yb</code></td>
<td>
<p>the weighted means of each response</p>
</td></tr>
<tr><td><code>ys</code></td>
<td>
<p>the overall scale factor used: internally the responses are
divided by <code>ys</code> to have unit total weighted sum of squares.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>the fitted values, as a matrix if <code>q &gt; 1</code>.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>the residuals, as a matrix if <code>q &gt; 1</code>.</p>
</td></tr>
<tr><td><code>smod</code></td>
<td>
<p>internal work array, which includes the ridge functions
evaluated at the training set points.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>(only if <code>model = TRUE</code>) the model frame.</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Friedman (1984): converted to double precision and added interface to
smoothing splines by B. D. Ripley, originally for the <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a>
package.
</p>


<h3>References</h3>

<p>Friedman, J. H. and Stuetzle, W. (1981).
Projection pursuit regression.
<em>Journal of the American Statistical Association</em>,
<b>76</b>, 817&ndash;823.
<a href="https://doi.org/10.2307/2287576">doi:10.2307/2287576</a>.
</p>
<p>Friedman, J. H. (1984).
SMART User's Guide.
Laboratory for Computational Statistics, Stanford University Technical
Report No. 1.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002).
<em>Modern Applied Statistics with S</em>.
Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.ppr">plot.ppr</a></code>, <code><a href="#topic+supsmu">supsmu</a></code>, <code><a href="#topic+smooth.spline">smooth.spline</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

# Note: your numerical values may differ
attach(rock)
area1 &lt;- area/10000; peri1 &lt;- peri/10000
rock.ppr &lt;- ppr(log(perm) ~ area1 + peri1 + shape,
                data = rock, nterms = 2, max.terms = 5)
rock.ppr
# Call:
# ppr.formula(formula = log(perm) ~ area1 + peri1 + shape, data = rock,
#     nterms = 2, max.terms = 5)
#
# Goodness of fit:
#  2 terms  3 terms  4 terms  5 terms
# 8.737806 5.289517 4.745799 4.490378

summary(rock.ppr)
# .....  (same as above)
# .....
#
# Projection direction vectors ('alpha'):
#       term 1      term 2
# area1  0.34357179  0.37071027
# peri1 -0.93781471 -0.61923542
# shape  0.04961846  0.69218595
#
# Coefficients of ridge terms:
#    term 1    term 2
# 1.6079271 0.5460971

par(mfrow = c(3,2))   # maybe: , pty = "s")
plot(rock.ppr, main = "ppr(log(perm)~ ., nterms=2, max.terms=5)")
plot(update(rock.ppr, bass = 5), main = "update(..., bass = 5)")
plot(update(rock.ppr, sm.method = "gcv", gcvpen = 2),
     main = "update(..., sm.method=\"gcv\", gcvpen=2)")
cbind(perm = rock$perm, prediction = round(exp(predict(rock.ppr)), 1))
detach()
</code></pre>

<hr>
<h2 id='prcomp'>Principal Components Analysis</h2><span id='topic+prcomp'></span><span id='topic+prcomp.formula'></span><span id='topic+prcomp.default'></span><span id='topic+plot.prcomp'></span><span id='topic+predict.prcomp'></span><span id='topic+print.prcomp'></span><span id='topic+summary.prcomp'></span><span id='topic+print.summary.prcomp'></span>

<h3>Description</h3>

<p>Performs a principal components analysis on the given data matrix
and returns the results as an object of class <code>prcomp</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>prcomp(x, ...)

## S3 method for class 'formula'
prcomp(formula, data = NULL, subset, na.action, ...)

## Default S3 method:
prcomp(x, retx = TRUE, center = TRUE, scale. = FALSE,
       tol = NULL, rank. = NULL, ...)

## S3 method for class 'prcomp'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prcomp_+3A_formula">formula</code></td>
<td>
<p>a formula with no response variable, referring only to
numeric variables.</p>
</td></tr>
<tr><td><code id="prcomp_+3A_data">data</code></td>
<td>
<p>an optional data frame (or similar: see
<code><a href="#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="prcomp_+3A_subset">subset</code></td>
<td>
<p>an optional vector used to select rows (observations) of the
data matrix <code>x</code>.</p>
</td></tr>
<tr><td><code id="prcomp_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen
when the data contain <code>NA</code>s.  The default is set by
the <code>na.action</code> setting of <code><a href="base.html#topic+options">options</a></code>, and is
<code><a href="#topic+na.fail">na.fail</a></code> if that is unset. The &lsquo;factory-fresh&rsquo;
default is <code><a href="#topic+na.omit">na.omit</a></code>.</p>
</td></tr>
<tr><td><code id="prcomp_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.  If <code>x</code> is
a formula one might specify <code>scale.</code> or <code>tol</code>.</p>
</td></tr>
<tr><td><code id="prcomp_+3A_x">x</code></td>
<td>
<p>a numeric or complex matrix (or data frame) which provides
the data for the principal components analysis.</p>
</td></tr>
<tr><td><code id="prcomp_+3A_retx">retx</code></td>
<td>
<p>a logical value indicating whether the rotated variables
should be returned.</p>
</td></tr>
<tr><td><code id="prcomp_+3A_center">center</code></td>
<td>
<p>a logical value indicating whether the variables
should be shifted to be zero centered. Alternately, a vector of
length equal the number of columns of <code>x</code> can be supplied.
The value is passed to <code>scale</code>.</p>
</td></tr>
<tr><td><code id="prcomp_+3A_scale.">scale.</code></td>
<td>
<p>a logical value indicating whether the variables should
be scaled to have unit variance before the analysis takes
place.  The default is <code>FALSE</code> for consistency with S, but
in general scaling is advisable.  Alternatively, a vector of length
equal the number of columns of <code>x</code> can be supplied.  The
value is passed to <code><a href="base.html#topic+scale">scale</a></code>.</p>
</td></tr>
<tr><td><code id="prcomp_+3A_tol">tol</code></td>
<td>
<p>a value indicating the magnitude below which components
should be omitted. (Components are omitted if their
standard deviations are less than or equal to <code>tol</code> times the
standard deviation of the first component.)  With the default null
setting, no components are omitted (unless <code>rank.</code> is specified
less than <code>min(dim(x))</code>.).  Other settings for <code>tol</code> could be
<code>tol = 0</code> or <code>tol = sqrt(.Machine$double.eps)</code>, which
would omit essentially constant components.</p>
</td></tr>
<tr><td><code id="prcomp_+3A_rank.">rank.</code></td>
<td>
<p>optionally, a number specifying the maximal rank, i.e.,
maximal number of principal components to be used.  Can be set as
alternative or in addition to <code>tol</code>, useful notably when the
desired rank is considerably smaller than the dimensions of the matrix.</p>
</td></tr>
<tr><td><code id="prcomp_+3A_object">object</code></td>
<td>
<p>object of class inheriting from <code>"prcomp"</code></p>
</td></tr>
<tr><td><code id="prcomp_+3A_newdata">newdata</code></td>
<td>
<p>An optional data frame or matrix in which to look for
variables with which to predict.  If omitted, the scores are used.
If the original fit used a formula or a data frame or a matrix with
column names, <code>newdata</code> must contain columns with the same
names. Otherwise it must contain the same number of columns, to be
used in the same order.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The calculation is done by a singular value decomposition of the
(centered and possibly scaled) data matrix, not by using
<code>eigen</code> on the covariance matrix.  This
is generally the preferred method for numerical accuracy.  The
<code>print</code> method for these objects prints the results in a nice
format and the <code>plot</code> method produces a scree plot.
</p>
<p>Unlike <code><a href="#topic+princomp">princomp</a></code>, variances are computed with the usual
divisor <code class="reqn">N - 1</code>.
</p>
<p>Note that <code>scale = TRUE</code> cannot be used if there are zero or
constant (for <code>center = TRUE</code>) variables.
</p>


<h3>Value</h3>

<p><code>prcomp</code> returns a list with class <code>"prcomp"</code>
containing the following components:
</p>
<table>
<tr><td><code>sdev</code></td>
<td>
<p>the standard deviations of the principal components
(i.e., the square roots of the eigenvalues of the
covariance/correlation matrix, though the calculation
is actually done with the singular values of the data matrix).</p>
</td></tr>
<tr><td><code>rotation</code></td>
<td>
<p>the matrix of variable loadings (i.e., a matrix
whose columns contain the eigenvectors).  The function
<code>princomp</code> returns this in the element <code>loadings</code>.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>if <code>retx</code> is true the value of the rotated data (the
centred (and scaled if requested) data multiplied by the
<code>rotation</code> matrix) is returned.  Hence, <code>cov(x)</code> is the
diagonal matrix <code>diag(sdev^2)</code>.  For the formula method,
<code><a href="#topic+napredict">napredict</a>()</code> is applied to handle the treatment of values
omitted by the <code>na.action</code>.</p>
</td></tr>
<tr><td><code>center</code>, <code>scale</code></td>
<td>
<p>the centering and scaling used, or <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The signs of the columns of the rotation matrix are arbitrary, and
so may differ between different programs for PCA, and even between
different builds of <span class="rlang"><b>R</b></span>.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Mardia, K. V., J. T. Kent, and J. M. Bibby (1979)
<em>Multivariate Analysis</em>, London: Academic Press.
</p>
<p>Venables, W. N. and B. D. Ripley (2002)
<em>Modern Applied Statistics with S</em>, Springer-Verlag.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+biplot.prcomp">biplot.prcomp</a></code>, <code><a href="#topic+screeplot">screeplot</a></code>,
<code><a href="#topic+princomp">princomp</a></code>, <code><a href="#topic+cor">cor</a></code>, <code><a href="#topic+cov">cov</a></code>,
<code><a href="base.html#topic+svd">svd</a></code>, <code><a href="base.html#topic+eigen">eigen</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>C &lt;- chol(S &lt;- toeplitz(.9 ^ (0:31))) # Cov.matrix and its root
all.equal(S, crossprod(C))
set.seed(17)
X &lt;- matrix(rnorm(32000), 1000, 32)
Z &lt;- X %*% C  ## ==&gt;  cov(Z) ~=  C'C = S
all.equal(cov(Z), S, tolerance = 0.08)
pZ &lt;- prcomp(Z, tol = 0.1)
summary(pZ) # only ~14 PCs (out of 32)
## or choose only 3 PCs more directly:
pz3 &lt;- prcomp(Z, rank. = 3)
summary(pz3) # same numbers as the first 3 above
stopifnot(ncol(pZ$rotation) == 14, ncol(pz3$rotation) == 3,
          all.equal(pz3$sdev, pZ$sdev, tolerance = 1e-15)) # exactly equal typically

## signs are random
require(graphics)
## the variances of the variables in the
## USArrests data vary by orders of magnitude, so scaling is appropriate
prcomp(USArrests)  # inappropriate
prcomp(USArrests, scale. = TRUE)
prcomp(~ Murder + Assault + Rape, data = USArrests, scale. = TRUE)
plot(prcomp(USArrests))
summary(prcomp(USArrests, scale. = TRUE))
biplot(prcomp(USArrests, scale. = TRUE))

</code></pre>

<hr>
<h2 id='predict'>Model Predictions</h2><span id='topic+predict'></span>

<h3>Description</h3>

<p><code>predict</code> is a generic function for predictions from the results of
various model fitting functions.  The function invokes particular
<em>methods</em> which depend on the <code><a href="base.html#topic+class">class</a></code> of
the first argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict (object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_+3A_object">object</code></td>
<td>
<p>a model object for which prediction is desired.</p>
</td></tr>
<tr><td><code id="predict_+3A_...">...</code></td>
<td>
<p>additional arguments affecting the predictions produced.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Most prediction methods which are similar to those for linear models
have an argument <code>newdata</code> specifying the first place to look for
explanatory variables to be used for prediction.  Some considerable
attempts are made to match up the columns in <code>newdata</code> to those
used for fitting, for example that they are of comparable types and
that any factors have the same level set in the same order (or can be
transformed to be so).
</p>
<p>Time series prediction methods in package <span class="pkg">stats</span> have an argument
<code>n.ahead</code> specifying how many time steps ahead to predict.
</p>
<p>Many methods have a logical argument <code>se.fit</code> saying if standard
errors are to returned.
</p>


<h3>Value</h3>

<p>The form of the value returned by <code>predict</code> depends on the
class of its argument.  See the documentation of the
particular methods for details of what is produced by that method.
</p>


<h3>References</h3>

<p>Chambers, J. M. and Hastie, T. J. (1992)
<em>Statistical Models in S</em>.
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.glm">predict.glm</a></code>,
<code><a href="#topic+predict.lm">predict.lm</a></code>,
<code><a href="#topic+predict.loess">predict.loess</a></code>,
<code><a href="#topic+predict.nls">predict.nls</a></code>,
<code><a href="#topic+predict.poly">predict.poly</a></code>,
<code><a href="#topic+predict.princomp">predict.princomp</a></code>,
<code><a href="#topic+predict.smooth.spline">predict.smooth.spline</a></code>.
</p>
<p><a href="#topic+SafePrediction">SafePrediction</a> for prediction from (univariable) polynomial and
spline fits.
</p>
<p>For time-series prediction,
<code><a href="#topic+predict.ar">predict.ar</a></code>,
<code><a href="#topic+predict.Arima">predict.Arima</a></code>,
<code><a href="#topic+predict.arima0">predict.arima0</a></code>,
<code><a href="#topic+predict.HoltWinters">predict.HoltWinters</a></code>,
<code><a href="#topic+predict.StructTS">predict.StructTS</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require(utils)

## All the "predict" methods found
## NB most of the methods in the standard packages are hidden.
## Output will depend on what namespaces are (or have been) loaded.

for(fn in methods("predict"))
   try({
       f &lt;- eval(substitute(getAnywhere(fn)$objs[[1]], list(fn = fn)))
       cat(fn, ":\n\t", deparse(args(f)), "\n")
       }, silent = TRUE)


</code></pre>

<hr>
<h2 id='predict.Arima'>Forecast from ARIMA fits</h2><span id='topic+predict.Arima'></span>

<h3>Description</h3>

<p>Forecast from models fitted by <code><a href="#topic+arima">arima</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Arima'
predict(object, n.ahead = 1, newxreg = NULL,
        se.fit = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.Arima_+3A_object">object</code></td>
<td>
<p>The result of an <code>arima</code> fit.</p>
</td></tr>
<tr><td><code id="predict.Arima_+3A_n.ahead">n.ahead</code></td>
<td>
<p>The number of steps ahead for which prediction is required.</p>
</td></tr>
<tr><td><code id="predict.Arima_+3A_newxreg">newxreg</code></td>
<td>
<p>New values of <code>xreg</code> to be used for
prediction. Must have at least <code>n.ahead</code> rows.</p>
</td></tr>
<tr><td><code id="predict.Arima_+3A_se.fit">se.fit</code></td>
<td>
<p>Logical: should standard errors of prediction be returned?</p>
</td></tr>
<tr><td><code id="predict.Arima_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Finite-history prediction is used, via <code><a href="#topic+KalmanForecast">KalmanForecast</a></code>.
This is only statistically efficient if the MA part of the fit is
invertible, so <code>predict.Arima</code> will give a warning for
non-invertible MA models.
</p>
<p>The standard errors of prediction exclude the uncertainty in the
estimation of the ARMA model and the regression coefficients.
According to Harvey (1993, pp. 58&ndash;9) the effect is small.
</p>


<h3>Value</h3>

<p>A time series of predictions, or if <code>se.fit = TRUE</code>, a list
with components <code>pred</code>, the predictions, and <code>se</code>,
the estimated standard errors.  Both components are time series.
</p>


<h3>References</h3>

<p>Durbin, J. and Koopman, S. J. (2001).
<em>Time Series Analysis by State Space Methods</em>.
Oxford University Press.
</p>
<p>Harvey, A. C. and McKenzie, C. R. (1982).
Algorithm AS 182: An algorithm for finite sample prediction from ARIMA
processes. 
<em>Applied Statistics</em>, <b>31</b>, 180&ndash;187.
<a href="https://doi.org/10.2307/2347987">doi:10.2307/2347987</a>.
</p>
<p>Harvey, A. C. (1993).
<em>Time Series Models</em>, 2nd Edition.
Harvester Wheatsheaf.
Sections 3.3 and 4.4.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+arima">arima</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>od &lt;- options(digits = 5) # avoid too much spurious accuracy
predict(arima(lh, order = c(3,0,0)), n.ahead = 12)

(fit &lt;- arima(USAccDeaths, order = c(0,1,1),
              seasonal = list(order = c(0,1,1))))
predict(fit, n.ahead = 6)
options(od)
</code></pre>

<hr>
<h2 id='predict.glm'>Predict Method for GLM Fits</h2><span id='topic+predict.glm'></span>

<h3>Description</h3>

<p>Obtains predictions and optionally estimates standard errors of those
predictions from a fitted generalized linear model object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'glm'
predict(object, newdata = NULL,
            type = c("link", "response", "terms"),
            se.fit = FALSE, dispersion = NULL, terms = NULL,
            na.action = na.pass, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.glm_+3A_object">object</code></td>
<td>
<p>a fitted object of class inheriting from <code>"glm"</code>.</p>
</td></tr>
<tr><td><code id="predict.glm_+3A_newdata">newdata</code></td>
<td>
<p>optionally, a data frame in which to look for variables with
which to predict.  If omitted, the fitted linear predictors are used.</p>
</td></tr>
<tr><td><code id="predict.glm_+3A_type">type</code></td>
<td>
<p>the type of prediction required.  The default is on the
scale of the linear predictors; the alternative <code>"response"</code>
is on the scale of the response variable.  Thus for a default
binomial model the default predictions are of log-odds (probabilities
on logit scale) and <code>type = "response"</code> gives the predicted
probabilities.  The <code>"terms"</code> option returns a matrix giving the
fitted values of each term in the model formula on the linear predictor
scale.
</p>
<p>The value of this argument can be abbreviated.
</p>
</td></tr>
<tr><td><code id="predict.glm_+3A_se.fit">se.fit</code></td>
<td>
<p>logical switch indicating if standard errors are required.</p>
</td></tr>
<tr><td><code id="predict.glm_+3A_dispersion">dispersion</code></td>
<td>
<p>the dispersion of the GLM fit to be assumed in
computing the standard errors.  If omitted, that returned by
<code>summary</code> applied to the object is used.</p>
</td></tr>
<tr><td><code id="predict.glm_+3A_terms">terms</code></td>
<td>
<p>with <code>type = "terms"</code> by default all terms are returned.
A character vector specifies which terms are to be returned</p>
</td></tr>
<tr><td><code id="predict.glm_+3A_na.action">na.action</code></td>
<td>
<p>function determining what should be done with missing
values in <code>newdata</code>.  The default is to predict <code>NA</code>.</p>
</td></tr>
<tr><td><code id="predict.glm_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>newdata</code> is omitted the predictions are based on the data
used for the fit.  In that case how cases with missing values in the
original fit is determined by the <code>na.action</code> argument of that
fit.  If <code>na.action = na.omit</code> omitted cases will not appear in
the residuals, whereas if <code>na.action = na.exclude</code> they will
appear (in predictions and standard errors), with residual value
<code>NA</code>.  See also <code><a href="#topic+napredict">napredict</a></code>.
</p>


<h3>Value</h3>

<p>If <code>se.fit = FALSE</code>, a vector or matrix of predictions.
For <code>type = "terms"</code> this is a matrix with a column per term, and
may have an attribute <code>"constant"</code>.
</p>
<p>If <code>se.fit = TRUE</code>, a list with components
</p>
<table>
<tr><td><code>fit</code></td>
<td>
<p>Predictions, as for <code>se.fit = FALSE</code>.</p>
</td></tr>
<tr><td><code>se.fit</code></td>
<td>
<p>Estimated standard errors.</p>
</td></tr>
<tr><td><code>residual.scale</code></td>
<td>
<p>A scalar giving the square root of the
dispersion used in computing the standard errors.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Variables are first looked for in <code>newdata</code> and then searched for
in the usual way (which will include the environment of the formula
used in the fit).  A warning will be given if the
variables found are not of the same length as those in <code>newdata</code>
if it was supplied.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm">glm</a></code>, <code><a href="#topic+SafePrediction">SafePrediction</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

## example from Venables and Ripley (2002, pp. 190-2.)
ldose &lt;- rep(0:5, 2)
numdead &lt;- c(1, 4, 9, 13, 18, 20, 0, 2, 6, 10, 12, 16)
sex &lt;- factor(rep(c("M", "F"), c(6, 6)))
SF &lt;- cbind(numdead, numalive = 20-numdead)
budworm.lg &lt;- glm(SF ~ sex*ldose, family = binomial)
summary(budworm.lg)

plot(c(1,32), c(0,1), type = "n", xlab = "dose",
     ylab = "prob", log = "x")
text(2^ldose, numdead/20, as.character(sex))
ld &lt;- seq(0, 5, 0.1)
lines(2^ld, predict(budworm.lg, data.frame(ldose = ld,
   sex = factor(rep("M", length(ld)), levels = levels(sex))),
   type = "response"))
lines(2^ld, predict(budworm.lg, data.frame(ldose = ld,
   sex = factor(rep("F", length(ld)), levels = levels(sex))),
   type = "response"))
</code></pre>

<hr>
<h2 id='predict.HoltWinters'>Prediction Function for Fitted Holt-Winters Models</h2><span id='topic+predict.HoltWinters'></span>

<h3>Description</h3>

<p>Computes predictions and prediction intervals for models fitted by
the Holt-Winters method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'HoltWinters'
predict(object, n.ahead = 1, prediction.interval = FALSE,
       level = 0.95, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.HoltWinters_+3A_object">object</code></td>
<td>
<p>An object of class <code>HoltWinters</code>.</p>
</td></tr>
<tr><td><code id="predict.HoltWinters_+3A_n.ahead">n.ahead</code></td>
<td>
<p>Number of future periods to predict.</p>
</td></tr>
<tr><td><code id="predict.HoltWinters_+3A_prediction.interval">prediction.interval</code></td>
<td>
<p>logical. If <code>TRUE</code>, the lower and
upper bounds of the corresponding prediction intervals are computed.</p>
</td></tr>
<tr><td><code id="predict.HoltWinters_+3A_level">level</code></td>
<td>
<p>Confidence level for the prediction interval.</p>
</td></tr>
<tr><td><code id="predict.HoltWinters_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A time series of the predicted values. If prediction intervals are
requested, a multiple time series is returned with columns <code>fit</code>,
<code>lwr</code> and <code>upr</code> for the predicted values and the lower and
upper bounds respectively.
</p>


<h3>Author(s)</h3>

<p>David Meyer <a href="mailto:David.Meyer@wu.ac.at">David.Meyer@wu.ac.at</a>
</p>


<h3>References</h3>

<p>C. C. Holt (1957)
Forecasting trends and seasonals by exponentially weighted
moving averages,
<em>ONR Research Memorandum, Carnegie Institute of Technology</em> <b>52</b>.
</p>
<p>P. R. Winters (1960).
Forecasting sales by exponentially weighted moving averages.
<em>Management Science</em>, <b>6</b>, 324&ndash;342.
<a href="https://doi.org/10.1287/mnsc.6.3.324">doi:10.1287/mnsc.6.3.324</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+HoltWinters">HoltWinters</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

m &lt;- HoltWinters(co2)
p &lt;- predict(m, 50, prediction.interval = TRUE)
plot(m, p)
</code></pre>

<hr>
<h2 id='predict.lm'>Predict method for Linear Model Fits</h2><span id='topic+predict.lm'></span>

<h3>Description</h3>

<p>Predicted values based on linear model object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lm'
predict(object, newdata, se.fit = FALSE, scale = NULL, df = Inf,
        interval = c("none", "confidence", "prediction"),
        level = 0.95, type = c("response", "terms"),
        terms = NULL, na.action = na.pass,
        pred.var = res.var/weights, weights = 1,
        rankdeficient = c("warnif", "simple", "non-estim", "NA", "NAwarn"),
        tol = 1e-6, verbose = FALSE,
        ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.lm_+3A_object">object</code></td>
<td>
<p>Object of class inheriting from <code>"lm"</code></p>
</td></tr>
<tr><td><code id="predict.lm_+3A_newdata">newdata</code></td>
<td>
<p>An optional data frame in which to look for variables with
which to predict.  If omitted, the fitted values are used.</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_se.fit">se.fit</code></td>
<td>
<p>A switch indicating if standard errors are required.</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_scale">scale</code></td>
<td>
<p>Scale parameter for std.err. calculation.</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_df">df</code></td>
<td>
<p>Degrees of freedom for scale.</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_interval">interval</code></td>
<td>
<p>Type of interval calculation.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_level">level</code></td>
<td>
<p>Tolerance/confidence level.</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_type">type</code></td>
<td>
<p>Type of prediction (response or model term).  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_terms">terms</code></td>
<td>
<p>If <code>type = "terms"</code>, which terms (default is all
terms), a <code><a href="base.html#topic+character">character</a></code> vector.</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_na.action">na.action</code></td>
<td>
<p>function determining what should be done with missing
values in <code>newdata</code>.  The default is to predict <code>NA</code>.</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_pred.var">pred.var</code></td>
<td>
<p>the variance(s) for future observations to be assumed
for prediction intervals.  See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_weights">weights</code></td>
<td>
<p>variance weights for prediction.  This can be a numeric
vector or a one-sided model formula.  In the latter case, it is
interpreted as an expression evaluated in <code>newdata</code>.</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_rankdeficient">rankdeficient</code></td>
<td>
<p>a <code><a href="base.html#topic+character">character</a></code> string specifying what
should happen in the case of a rank deficient model, i.e., when
<code>object$rank &lt; ncol(model.matrix(object))</code>.
</p>

<dl>
<dt><code>"warnif"</code>:</dt><dd><p>gives a <code><a href="base.html#topic+warning">warning</a></code> only in case of
predicting &lsquo;non-estimable&rsquo; cases, i.e., vectors not in the
same predictor subspace as the original data (with tolerance
<code>tol</code>).  In that case, the non-estimable indices are also
returned as attribute <code>"non-estim"</code> (see <code>rankdeficient="non-estim"</code>).</p>
</dd>
<dt><code>"simple"</code>:</dt><dd><p>is back compatible to <span class="rlang"><b>R</b></span> &lt; 4.3.0, possibly giving dubious
predictions in non-estimable cases, and always signalling a <code><a href="base.html#topic+warning">warning</a></code>.</p>
</dd>
<dt><code>"non-estim"</code>:</dt><dd><p>gives the same predictions without
<code><a href="base.html#topic+warning">warning</a></code>, and with an attribute <code><a href="base.html#topic+attr">attr</a>(*, "non-estim")</code>
with indices in <code>1:nrow(newdata)</code> of new data observations
which are deemed non-estimable.</p>
</dd>
<dt><code>"NA"</code>:    </dt><dd><p>predicts <code>NA</code> for non-estimable new data,
silently.  Often recommended in new code.</p>
</dd>
<dt><code>"NAwarn"</code>:</dt><dd><p>predicts <code>NA</code> for non-estimable new data
with a <code><a href="base.html#topic+warning">warning</a></code>.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="predict.lm_+3A_tol">tol</code></td>
<td>
<p>non-negative number determining how non-estimability is
determined in rank deficient cases.</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_verbose">verbose</code></td>
<td>
<p><code><a href="base.html#topic+logical">logical</a></code> indicating if messages should be
produced about rank deficiency handling.</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>predict.lm</code> produces predicted values, obtained by evaluating
the regression function in the frame <code>newdata</code> (which defaults to
<code>model.frame(object)</code>).  If the logical <code>se.fit</code> is
<code>TRUE</code>, standard errors of the predictions are calculated.  If
the numeric argument <code>scale</code> is set (with optional <code>df</code>), it
is used as the residual standard deviation in the computation of the
standard errors, otherwise this is extracted from the model fit.
Setting <code>intervals</code> specifies computation of confidence or
prediction (tolerance) intervals at the specified <code>level</code>, sometimes
referred to as narrow vs. wide intervals.
</p>
<p>If the fit is rank-deficient, some of the columns of the design matrix
will have been dropped during the <code><a href="#topic+lm">lm</a></code> computations, and
corresponding <code><a href="#topic+coef">coef</a>()</code> components set to <code><a href="base.html#topic+NA">NA</a></code>.
Prediction from such a fit only makes sense if <code>newdata</code> is
contained in the same subspace as the original data.  Other
<code>newdata</code> entries (rows) are <code>non-estimable</code>.  This is now
checked (up to numerical tolerance <code>tol</code>) unless <code>rankdeficient
    == "simple"</code>, which corresponds to previous behaviour, warns always and
predicts using the non-<code>NA</code> coefficients with the corresponding
columns of the design matrix.  The new default option,
<code>rankdeficient == "warnif"</code> checks if there are
&ldquo;non-estimable&rdquo; cases (up to tolerance <code>tol</code>) and only warns
in that case.  All further <code>rankdeficient</code> options also check and
either predict <code>NA</code> or mark the non-estimable cases differently.
</p>
<p>If <code>newdata</code> is omitted the predictions are based on the data
used for the fit.  In that case how cases with missing values in the
original fit are handled is determined by the <code>na.action</code> argument of that
fit.  If <code>na.action = na.omit</code> omitted cases will not appear in
the predictions, whereas if <code>na.action = na.exclude</code> they will
appear (in predictions, standard errors or interval limits),
with value <code>NA</code>.  See also <code><a href="#topic+napredict">napredict</a></code>.
</p>
<p>The prediction intervals are for a single observation at each case in
<code>newdata</code> (or by default, the data used for the fit) with error
variance(s) <code>pred.var</code>.  This can be a multiple of <code>res.var</code>,
the estimated value of <code class="reqn">\sigma^2</code>: the default is to assume that
future observations have the same error variance as those
used for fitting.  If <code>weights</code> is supplied, the inverse of this
is used as a scale factor.  For a weighted fit, if the prediction
is for the original data frame, <code>weights</code> defaults to the weights
used for the  model fit, with a warning since it might not be the
intended result.  If the fit was weighted and <code>newdata</code> is given, the
default is to assume constant prediction variance, with a warning.
</p>


<h3>Value</h3>

<p><code>predict.lm</code> produces a vector of predictions or a matrix of
predictions and bounds with column names <code>fit</code>, <code>lwr</code>, and
<code>upr</code> if <code>interval</code> is set.  For <code>type = "terms"</code> this
is a matrix with a column per term and may have an attribute
<code>"constant"</code>.
</p>
<p>If <code>se.fit</code> is
<code>TRUE</code>, a list with the following components is returned:
</p>
<table>
<tr><td><code>fit</code></td>
<td>
<p>vector or matrix as above</p>
</td></tr>
<tr><td><code>se.fit</code></td>
<td>
<p>standard error of predicted means</p>
</td></tr>
<tr><td><code>residual.scale</code></td>
<td>
<p>residual standard deviations</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>degrees of freedom for residual</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Variables are first looked for in <code>newdata</code> and then searched for
in the usual way (which will include the environment of the formula
used in the fit).  A warning will be given if the
variables found are not of the same length as those in <code>newdata</code>
if it was supplied.
</p>
<p>Notice that prediction variances and prediction intervals always refer
to <em>future</em> observations, possibly corresponding to the same
predictors as used for the fit.  The variance of the <em>residuals</em>
will be smaller.
</p>
<p>Strictly speaking, the formula used for prediction limits assumes that
the degrees of freedom for the fit are the same as those for the
residual variance.  This may not be the case if <code>res.var</code> is
not obtained from the fit.
</p>


<h3>See Also</h3>

<p>The model fitting function <code><a href="#topic+lm">lm</a></code>, <code><a href="#topic+predict">predict</a></code>.
</p>
<p><a href="#topic+SafePrediction">SafePrediction</a> for prediction from (univariable) polynomial and
spline fits.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

## Predictions
x &lt;- rnorm(15)
y &lt;- x + rnorm(15)
predict(lm(y ~ x))
new &lt;- data.frame(x = seq(-3, 3, 0.5))
predict(lm(y ~ x), new, se.fit = TRUE)
pred.w.plim &lt;- predict(lm(y ~ x), new, interval = "prediction")
pred.w.clim &lt;- predict(lm(y ~ x), new, interval = "confidence")
matplot(new$x, cbind(pred.w.clim, pred.w.plim[,-1]),
        lty = c(1,2,2,3,3), type = "l", ylab = "predicted y")

## Prediction intervals, special cases
##  The first three of these throw warnings
w &lt;- 1 + x^2
fit &lt;- lm(y ~ x)
wfit &lt;- lm(y ~ x, weights = w)
predict(fit, interval = "prediction")
predict(wfit, interval = "prediction")
predict(wfit, new, interval = "prediction")
predict(wfit, new, interval = "prediction", weights = (new$x)^2)
predict(wfit, new, interval = "prediction", weights = ~x^2)

##-- From  aov(.) example ---- predict(.. terms)
npk.aov &lt;- aov(yield ~ block + N*P*K, npk)
(termL &lt;- attr(terms(npk.aov), "term.labels"))
(pt &lt;- predict(npk.aov, type = "terms"))
pt. &lt;- predict(npk.aov, type = "terms", terms = termL[1:4])
stopifnot(all.equal(pt[,1:4], pt.,
                    tolerance = 1e-12, check.attributes = FALSE))
</code></pre>

<hr>
<h2 id='predict.loess'>Predict <abbr>LOESS</abbr> Curve or Surface</h2><span id='topic+predict.loess'></span>

<h3>Description</h3>

<p>Predictions from a <code>loess</code> fit, optionally with standard errors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'loess'
predict(object, newdata = NULL, se = FALSE,
        na.action = na.pass, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.loess_+3A_object">object</code></td>
<td>
<p>an object fitted by <code>loess</code>.</p>
</td></tr>
<tr><td><code id="predict.loess_+3A_newdata">newdata</code></td>
<td>
<p>an optional data frame in which to look for variables with
which to predict, or a matrix or vector containing exactly the variables
needs for prediction.  If missing, the original data points are used.</p>
</td></tr>
<tr><td><code id="predict.loess_+3A_se">se</code></td>
<td>
<p>should standard errors be computed?</p>
</td></tr>
<tr><td><code id="predict.loess_+3A_na.action">na.action</code></td>
<td>
<p>function determining what should be done with missing
values in data frame <code>newdata</code>.  The default is to predict <code>NA</code>.</p>
</td></tr>
<tr><td><code id="predict.loess_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The standard errors calculation <code>se = TRUE</code> is slower than
prediction, notably as it needs a relatively large workspace (memory),
notably matrices of dimension <code class="reqn">N \times Nf</code> where <code class="reqn">f =
  </code><code>span</code>, i.e., <code>se = TRUE</code> is <code class="reqn">O(N^2)</code>
and hence stops when the sample size <code class="reqn">N</code> is larger than about 40'600
(for default <code>span = 0.75</code>).
</p>
<p>When the fit was made using <code>surface = "interpolate"</code> (the
default), <code>predict.loess</code> will not extrapolate &ndash; so points outside
an axis-aligned hypercube enclosing the original data will have
missing (<code>NA</code>) predictions and standard errors.
</p>


<h3>Value</h3>

<p>If <code>se = FALSE</code>, a vector giving the prediction for each row of
<code>newdata</code> (or the original data). If <code>se = TRUE</code>, a list
containing components
</p>
<table>
<tr><td><code>fit</code></td>
<td>
<p>the predicted values.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>an estimated standard error for each predicted value.</p>
</td></tr>
<tr><td><code>residual.scale</code></td>
<td>
<p>the estimated scale of the residuals used in
computing the standard errors.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>an estimate of the effective degrees of freedom used in
estimating the residual scale, intended for use with t-based
confidence intervals. </p>
</td></tr>
</table>
<p>If <code>newdata</code> was the result of a call to
<code><a href="base.html#topic+expand.grid">expand.grid</a></code>, the predictions (and s.e.'s if requested)
will be an array of the appropriate dimensions.
</p>
<p>Predictions from infinite inputs will be <code>NA</code> since <code>loess</code>
does not support extrapolation.
</p>


<h3>Note</h3>

<p>Variables are first looked for in <code>newdata</code> and then searched for
in the usual way (which will include the environment of the formula
used in the fit).  A warning will be given if the
variables found are not of the same length as those in <code>newdata</code>
if it was supplied.
</p>


<h3>Author(s)</h3>

<p>B. D. Ripley, based on the <code>cloess</code> package of Cleveland,
Grosse and Shyu.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+loess">loess</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>cars.lo &lt;- loess(dist ~ speed, cars)
predict(cars.lo, data.frame(speed = seq(5, 30, 1)), se = TRUE)
# to get extrapolation
cars.lo2 &lt;- loess(dist ~ speed, cars,
  control = loess.control(surface = "direct"))
predict(cars.lo2, data.frame(speed = seq(5, 30, 1)), se = TRUE)
</code></pre>

<hr>
<h2 id='predict.nls'>Predicting from Nonlinear Least Squares Fits</h2><span id='topic+predict.nls'></span>

<h3>Description</h3>

<p><code>predict.nls</code> produces predicted values, obtained by evaluating
the regression function in the frame <code>newdata</code>.  If the logical
<code>se.fit</code> is <code>TRUE</code>, standard errors of the predictions are
calculated.  If the numeric argument <code>scale</code> is set (with
optional <code>df</code>), it is used as the residual standard deviation in
the computation of the standard errors, otherwise this is extracted
from the model fit.  Setting <code>intervals</code> specifies computation of
confidence or prediction (tolerance) intervals at the specified
<code>level</code>.
</p>
<p>At present <code>se.fit</code> and <code>interval</code> are ignored.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nls'
predict(object, newdata , se.fit = FALSE, scale = NULL, df = Inf,
        interval = c("none", "confidence", "prediction"),
        level = 0.95, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.nls_+3A_object">object</code></td>
<td>
<p>An object that inherits from class <code>nls</code>.</p>
</td></tr>
<tr><td><code id="predict.nls_+3A_newdata">newdata</code></td>
<td>
<p>A named list or data frame in which to look for variables with
which to predict.  If <code>newdata</code> is
missing the fitted values at the original data points are returned.</p>
</td></tr>
<tr><td><code id="predict.nls_+3A_se.fit">se.fit</code></td>
<td>
<p>A logical value indicating if the standard errors of the
predictions should be calculated.  Defaults to <code>FALSE</code>.  At
present this argument is ignored.</p>
</td></tr>
<tr><td><code id="predict.nls_+3A_scale">scale</code></td>
<td>
<p>A numeric scalar.  If it is set (with optional
<code>df</code>), it is used as the residual standard deviation in the
computation of the standard errors, otherwise this information is
extracted from the model fit. At present this argument is ignored.</p>
</td></tr>
<tr><td><code id="predict.nls_+3A_df">df</code></td>
<td>
<p>A positive numeric scalar giving the number of degrees of
freedom for the <code>scale</code> estimate. At present this argument is
ignored.</p>
</td></tr>
<tr><td><code id="predict.nls_+3A_interval">interval</code></td>
<td>
<p>A character string indicating if prediction intervals
or a confidence interval on the mean responses are to be
calculated. At present this argument is ignored.</p>
</td></tr>
<tr><td><code id="predict.nls_+3A_level">level</code></td>
<td>
<p>A numeric scalar between 0 and 1 giving the confidence
level for the intervals (if any) to be calculated.  At present
this argument is ignored.</p>
</td></tr>
<tr><td><code id="predict.nls_+3A_...">...</code></td>
<td>
<p>Additional optional arguments.  At present no optional
arguments are used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>predict.nls</code> produces a vector of predictions.
When implemented, <code>interval</code> will produce a matrix of
predictions and bounds with column names <code>fit</code>, <code>lwr</code>, and
<code>upr</code>.  When implemented, if <code>se.fit</code> is
<code>TRUE</code>, a list with the following components will be returned:
</p>
<table>
<tr><td><code>fit</code></td>
<td>
<p>vector or matrix as above</p>
</td></tr>
<tr><td><code>se.fit</code></td>
<td>
<p>standard error of predictions</p>
</td></tr>
<tr><td><code>residual.scale</code></td>
<td>
<p>residual standard deviations</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>degrees of freedom for residual</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Variables are first looked for in <code>newdata</code> and then searched for
in the usual way (which will include the environment of the formula
used in the fit).  A warning will be given if the
variables found are not of the same length as those in <code>newdata</code>
if it was supplied.
</p>


<h3>See Also</h3>

<p>The model fitting function <code><a href="#topic+nls">nls</a></code>,
<code><a href="#topic+predict">predict</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require(graphics)

fm &lt;- nls(demand ~ SSasympOrig(Time, A, lrc), data = BOD)
predict(fm)              # fitted values at observed times
## Form data plot and smooth line for the predictions
opar &lt;- par(las = 1)
plot(demand ~ Time, data = BOD, col = 4,
     main = "BOD data and fitted first-order curve",
     xlim = c(0,7), ylim = c(0, 20) )
tt &lt;- seq(0, 8, length.out = 101)
lines(tt, predict(fm, list(Time = tt)))
par(opar)

</code></pre>

<hr>
<h2 id='predict.smooth.spline'>Predict from Smoothing Spline Fit</h2><span id='topic+predict.smooth.spline'></span>

<h3>Description</h3>

<p>Predict a smoothing spline fit at new points, return the derivative if
desired. The predicted fit is linear beyond the original data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'smooth.spline'
predict(object, x, deriv = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.smooth.spline_+3A_object">object</code></td>
<td>
<p>a fit from <code>smooth.spline</code>.</p>
</td></tr>
<tr><td><code id="predict.smooth.spline_+3A_x">x</code></td>
<td>
<p>the new values of x.</p>
</td></tr>
<tr><td><code id="predict.smooth.spline_+3A_deriv">deriv</code></td>
<td>
<p>integer; the order of the derivative required.</p>
</td></tr>
<tr><td><code id="predict.smooth.spline_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>The input <code>x</code>.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>The fitted values or derivatives at <code>x</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+smooth.spline">smooth.spline</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

attach(cars)
cars.spl &lt;- smooth.spline(speed, dist, df = 6.4)


## "Proof" that the derivatives are okay, by comparing with approximation
diff.quot &lt;- function(x, y) {
  ## Difference quotient (central differences where available)
  n &lt;- length(x); i1 &lt;- 1:2; i2 &lt;- (n-1):n
  c(diff(y[i1]) / diff(x[i1]), (y[-i1] - y[-i2]) / (x[-i1] - x[-i2]),
    diff(y[i2]) / diff(x[i2]))
}

xx &lt;- unique(sort(c(seq(0, 30, by = .2), kn &lt;- unique(speed))))
i.kn &lt;- match(kn, xx)   # indices of knots within xx
op &lt;- par(mfrow = c(2,2))
plot(speed, dist, xlim = range(xx), main = "Smooth.spline &amp; derivatives")
lines(pp &lt;- predict(cars.spl, xx), col = "red")
points(kn, pp$y[i.kn], pch = 3, col = "dark red")
mtext("s(x)", col = "red")
for(d in 1:3){
  n &lt;- length(pp$x)
  plot(pp$x, diff.quot(pp$x,pp$y), type = "l", xlab = "x", ylab = "",
       col = "blue", col.main = "red",
       main = paste0("s" ,paste(rep("'", d), collapse = ""), "(x)"))
  mtext("Difference quotient approx.(last)", col = "blue")
  lines(pp &lt;- predict(cars.spl, xx, deriv = d), col = "red")

  points(kn, pp$y[i.kn], pch = 3, col = "dark red")
  abline(h = 0, lty = 3, col = "gray")
}
detach(); par(op)
</code></pre>

<hr>
<h2 id='preplot'>Pre-computations for a Plotting Object</h2><span id='topic+preplot'></span>

<h3>Description</h3>

<p>Compute an object to be used for plots relating to the given model object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>preplot(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="preplot_+3A_object">object</code></td>
<td>
<p>a fitted model object.</p>
</td></tr>
<tr><td><code id="preplot_+3A_...">...</code></td>
<td>
<p>additional arguments for specific methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Only the generic function is currently provided in base <span class="rlang"><b>R</b></span>, but some
add-on packages have methods. Principally here for S compatibility.
</p>


<h3>Value</h3>

<p>An object set up to make a plot that describes <code>object</code>.
</p>

<hr>
<h2 id='princomp'>Principal Components Analysis</h2><span id='topic+princomp'></span><span id='topic+princomp.formula'></span><span id='topic+princomp.default'></span><span id='topic+plot.princomp'></span><span id='topic+print.princomp'></span><span id='topic+predict.princomp'></span>

<h3>Description</h3>

<p><code>princomp</code> performs a principal components analysis on the given
numeric data matrix and returns the results as an object of class
<code>princomp</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>princomp(x, ...)

## S3 method for class 'formula'
princomp(formula, data = NULL, subset, na.action, ...)

## Default S3 method:
princomp(x, cor = FALSE, scores = TRUE, covmat = NULL,
         subset = rep_len(TRUE, nrow(as.matrix(x))), fix_sign = TRUE, ...)

## S3 method for class 'princomp'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="princomp_+3A_formula">formula</code></td>
<td>
<p>a formula with no response variable, referring only to
numeric variables.</p>
</td></tr>
<tr><td><code id="princomp_+3A_data">data</code></td>
<td>
<p>an optional data frame (or similar: see
<code><a href="#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="princomp_+3A_subset">subset</code></td>
<td>
<p>an optional vector used to select rows (observations) of the
data matrix <code>x</code>.</p>
</td></tr>
<tr><td><code id="princomp_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen
when the data contain <code>NA</code>s.  The default is set by
the <code>na.action</code> setting of <code><a href="base.html#topic+options">options</a></code>, and is
<code><a href="#topic+na.fail">na.fail</a></code> if that is unset. The &lsquo;factory-fresh&rsquo;
default is <code><a href="#topic+na.omit">na.omit</a></code>.</p>
</td></tr>
<tr><td><code id="princomp_+3A_x">x</code></td>
<td>
<p>a numeric matrix or data frame which provides the data for the
principal components analysis.</p>
</td></tr>
<tr><td><code id="princomp_+3A_cor">cor</code></td>
<td>
<p>a logical value indicating whether the calculation should
use the correlation matrix or the covariance matrix.  (The
correlation matrix can only be used if there are no constant variables.)</p>
</td></tr>
<tr><td><code id="princomp_+3A_scores">scores</code></td>
<td>
<p>a logical value indicating whether the score on each
principal component should be calculated.</p>
</td></tr>
<tr><td><code id="princomp_+3A_covmat">covmat</code></td>
<td>
<p>a covariance matrix, or a covariance list as returned by
<code><a href="#topic+cov.wt">cov.wt</a></code> (and <code><a href="MASS.html#topic+cov.rob">cov.mve</a></code> or
<code><a href="MASS.html#topic+cov.rob">cov.mcd</a></code> from package <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a>).
If supplied, this is used rather than the covariance matrix of
<code>x</code>.</p>
</td></tr>
<tr><td><code id="princomp_+3A_fix_sign">fix_sign</code></td>
<td>
<p>Should the signs of the loadings and scores be chosen
so that the first element of each loading is non-negative?</p>
</td></tr>
<tr><td><code id="princomp_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods. If <code>x</code> is
a formula one might specify <code>cor</code> or <code>scores</code>.</p>
</td></tr>
<tr><td><code id="princomp_+3A_object">object</code></td>
<td>
<p>Object of class inheriting from <code>"princomp"</code>.</p>
</td></tr>
<tr><td><code id="princomp_+3A_newdata">newdata</code></td>
<td>
<p>An optional data frame or matrix in which to look for
variables with which to predict.  If omitted, the scores are used.
If the original fit used a formula or a data frame or a matrix with
column names, <code>newdata</code> must contain columns with the same
names. Otherwise it must contain the same number of columns, to be
used in the same order.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>princomp</code> is a generic function with <code>"formula"</code> and
<code>"default"</code> methods.
</p>
<p>The calculation is done using <code><a href="base.html#topic+eigen">eigen</a></code> on the correlation
or covariance matrix, as determined by <code><a href="#topic+cor">cor</a></code>.  (This was
done for compatibility with the S-PLUS result.)  A preferred method of
calculation is to use <code><a href="base.html#topic+svd">svd</a></code> on <code>x</code>, as is done in
<code>prcomp</code>.
</p>
<p>Note that the default calculation uses divisor <code>N</code> for the
covariance matrix.
</p>
<p>The <code><a href="base.html#topic+print">print</a></code> method for these objects prints the
results in a nice format and the <code><a href="graphics.html#topic+plot">plot</a></code> method produces
a scree plot (<code><a href="#topic+screeplot">screeplot</a></code>).  There is also a
<code><a href="#topic+biplot">biplot</a></code> method.
</p>
<p>If <code>x</code> is a formula then the standard NA-handling is applied to
the scores (if requested): see <code><a href="#topic+napredict">napredict</a></code>.
</p>
<p><code>princomp</code> only handles so-called R-mode PCA, that is feature
extraction of variables.  If a data matrix is supplied (possibly via a
formula) it is required that there are at least as many units as
variables.  For Q-mode PCA use <code><a href="#topic+prcomp">prcomp</a></code>.
</p>


<h3>Value</h3>

<p><code>princomp</code> returns a list with class <code>"princomp"</code>
containing the following components:
</p>
<table>
<tr><td><code>sdev</code></td>
<td>
<p>the standard deviations of the principal components.</p>
</td></tr>
<tr><td><code>loadings</code></td>
<td>
<p>the matrix of variable loadings (i.e., a matrix
whose columns contain the eigenvectors).  This is of class
<code>"loadings"</code>: see <code><a href="#topic+loadings">loadings</a></code> for its <code>print</code>
method.</p>
</td></tr>
<tr><td><code>center</code></td>
<td>
<p>the means that were subtracted.</p>
</td></tr>
<tr><td><code>scale</code></td>
<td>
<p>the scalings applied to each variable.</p>
</td></tr>
<tr><td><code>n.obs</code></td>
<td>
<p>the number of observations.</p>
</td></tr>
<tr><td><code>scores</code></td>
<td>
<p>if <code>scores = TRUE</code>, the scores of the supplied
data on the principal components.  These are non-null only if
<code>x</code> was supplied, and if <code>covmat</code> was also supplied if it
was a covariance list.  For the formula method,
<code><a href="#topic+napredict">napredict</a>()</code> is applied to handle the treatment of
values omitted by the <code>na.action</code>.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>na.action</code></td>
<td>
<p>If relevant.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The signs of the columns of the loadings and scores are arbitrary, and
so may differ between different programs for PCA, and even between
different builds of <span class="rlang"><b>R</b></span>: <code>fix_sign = TRUE</code> alleviates that.
</p>


<h3>References</h3>

<p>Mardia, K. V., J. T. Kent and J. M. Bibby (1979).
<em>Multivariate Analysis</em>, London: Academic Press.
</p>
<p>Venables, W. N. and B. D. Ripley (2002).
<em>Modern Applied Statistics with S</em>, Springer-Verlag.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.princomp">summary.princomp</a></code>, <code><a href="#topic+screeplot">screeplot</a></code>,
<code><a href="#topic+biplot.princomp">biplot.princomp</a></code>,
<code><a href="#topic+prcomp">prcomp</a></code>, <code><a href="#topic+cor">cor</a></code>, <code><a href="#topic+cov">cov</a></code>,
<code><a href="base.html#topic+eigen">eigen</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

## The variances of the variables in the
## USArrests data vary by orders of magnitude, so scaling is appropriate
(pc.cr &lt;- princomp(USArrests))  # inappropriate
princomp(USArrests, cor = TRUE) # =^= prcomp(USArrests, scale=TRUE)
## Similar, but different:
## The standard deviations differ by a factor of sqrt(49/50)

summary(pc.cr &lt;- princomp(USArrests, cor = TRUE))
loadings(pc.cr)  # note that blank entries are small but not zero
## The signs of the columns of the loadings are arbitrary
plot(pc.cr) # shows a screeplot.
biplot(pc.cr)

## Formula interface
princomp(~ ., data = USArrests, cor = TRUE)

## NA-handling
USArrests[1, 2] &lt;- NA
pc.cr &lt;- princomp(~ Murder + Assault + UrbanPop,
                  data = USArrests, na.action = na.exclude, cor = TRUE)
pc.cr$scores[1:5, ]

## (Simple) Robust PCA:
## Classical:
(pc.cl  &lt;- princomp(stackloss))
## Robust:
(pc.rob &lt;- princomp(stackloss, covmat = MASS::cov.rob(stackloss)))
</code></pre>

<hr>
<h2 id='print.power.htest'>Print Methods for Hypothesis Tests and Power Calculation Objects</h2><span id='topic+print.htest'></span><span id='topic+print.power.htest'></span>

<h3>Description</h3>

<p>Printing objects of class <code>"htest"</code> or <code>"power.htest"</code>,
respectively, by simple <code><a href="base.html#topic+print">print</a></code> methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'htest'
print(x, digits = getOption("digits"), prefix = "\t", ...)

## S3 method for class 'power.htest'
print(x, digits = getOption("digits"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.power.htest_+3A_x">x</code></td>
<td>
<p>object of class <code>"htest"</code> or <code>"power.htest"</code>.</p>
</td></tr>
<tr><td><code id="print.power.htest_+3A_digits">digits</code></td>
<td>
<p>number of significant digits to be used.</p>
</td></tr>
<tr><td><code id="print.power.htest_+3A_prefix">prefix</code></td>
<td>
<p>string, passed to <code><a href="base.html#topic+strwrap">strwrap</a></code> for displaying
the <code>method</code> component of the <code>htest</code> object.</p>
</td></tr>
<tr><td><code id="print.power.htest_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Both <code><a href="base.html#topic+print">print</a></code> methods traditionally have not obeyed the
<code>digits</code> argument properly.  They now do, the <code>htest</code>
method mostly in expressions like <code>max(1, digits - 2)</code>.
</p>
<p>A <code>power.htest</code> object is just a named list of numbers and
character strings, supplemented with <code>method</code> and <code>note</code>
elements.  The <code>method</code> is displayed as a title, the <code>note</code>
as a footnote, and the remaining elements are given in an aligned
&lsquo;name = value&rsquo; format.
</p>


<h3>Value</h3>

<p>the argument <code>x</code>, invisibly, as for all <code><a href="base.html#topic+print">print</a></code>
methods.
</p>


<h3>Author(s)</h3>

<p>Peter Dalgaard</p>


<h3>See Also</h3>

<p><code><a href="#topic+power.t.test">power.t.test</a></code>,
<code><a href="#topic+power.prop.test">power.prop.test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>(ptt &lt;- power.t.test(n = 20, delta = 1))
print(ptt, digits =  4) # using less digits than default
print(ptt, digits = 12) # using more  "       "     "
</code></pre>

<hr>
<h2 id='print.ts'>Printing and Formatting of Time-Series Objects</h2><span id='topic+.preformat.ts'></span><span id='topic+print.ts'></span>

<h3>Description</h3>

<p>Notably for calendar related time series objects,  
<code><a href="base.html#topic+format">format</a></code> and <code><a href="base.html#topic+print">print</a></code> methods showing years,
months and or quarters respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ts'
print(x, calendar, ...)
.preformat.ts(x, calendar, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.ts_+3A_x">x</code></td>
<td>
<p>a time series object.</p>
</td></tr>
<tr><td><code id="print.ts_+3A_calendar">calendar</code></td>
<td>
<p>enable/disable the display of information about
month names, quarter names or year when printing.  The default is
<code>TRUE</code> for a frequency of 4 or 12, <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="print.ts_+3A_...">...</code></td>
<td>
<p>additional arguments to <code><a href="base.html#topic+print">print</a></code> (or
<code><a href="base.html#topic+format">format</a></code> methods).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="base.html#topic+print">print</a></code> method for <code>"ts"</code> objects prints a
header (basically of <code><a href="#topic+tsp">tsp</a>(x)</code>), if <code>calendar</code> is
false, and then prints the result of <code>.preformat.ts(x, *)</code>, which
is typically a <code><a href="base.html#topic+matrix">matrix</a></code> with <code><a href="base.html#topic+rownames">rownames</a></code> built
from the calendar times where applicable.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+print">print</a></code>,
<code><a href="#topic+ts">ts</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>print(ts(1:10, frequency = 7, start = c(12, 2)), calendar = TRUE)

print(sunsp.1 &lt;- window(sunspot.month, end=c(1756, 12)))
m &lt;- .preformat.ts(sunsp.1) # a character matrix
</code></pre>

<hr>
<h2 id='printCoefmat'>Print Coefficient Matrices</h2><span id='topic+printCoefmat'></span>

<h3>Description</h3>

<p>Utility function to be used in higher-level <code><a href="base.html#topic+print">print</a></code>
methods, such as those for <code><a href="#topic+summary.lm">summary.lm</a></code>,
<code><a href="#topic+summary.glm">summary.glm</a></code> and <code><a href="#topic+anova">anova</a></code>.  The
goal is to provide a flexible interface with smart defaults such
that often, only <code>x</code> needs to be specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>printCoefmat(x, digits = max(3, getOption("digits") - 2),
             signif.stars = getOption("show.signif.stars"),
             signif.legend = signif.stars,
             dig.tst = max(1, min(5, digits - 1)),
             cs.ind = 1L:k, tst.ind = k + 1L,
             zap.ind = integer(), P.values = NULL,
             has.Pvalue = nc &gt;= 4L &amp;&amp; length(cn &lt;- colnames(x)) &amp;&amp;
                          substr(cn[nc], 1L, 3L) %in% c("Pr(", "p-v"),
             eps.Pvalue = .Machine$double.eps,
             na.print = "NA", quote = FALSE, right = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="printCoefmat_+3A_x">x</code></td>
<td>
<p>a numeric matrix like object, to be printed.</p>
</td></tr>
<tr><td><code id="printCoefmat_+3A_digits">digits</code></td>
<td>
<p>minimum number of significant digits to be used for
most numbers.</p>
</td></tr>
<tr><td><code id="printCoefmat_+3A_signif.stars">signif.stars</code></td>
<td>
<p>logical; if <code>TRUE</code>, P-values are additionally
encoded visually as &lsquo;significance stars&rsquo; in order to help scanning
of long coefficient tables.  It defaults to the
<code>show.signif.stars</code> slot of <code><a href="base.html#topic+options">options</a></code>.</p>
</td></tr>
<tr><td><code id="printCoefmat_+3A_signif.legend">signif.legend</code></td>
<td>
<p>logical; if <code>TRUE</code>, a legend for the
&lsquo;significance stars&rsquo; is printed provided <code>signif.stars =
      TRUE</code>.</p>
</td></tr>
<tr><td><code id="printCoefmat_+3A_dig.tst">dig.tst</code></td>
<td>
<p>minimum number of significant digits for the test statistics,
see <code>tst.ind</code>.</p>
</td></tr>
<tr><td><code id="printCoefmat_+3A_cs.ind">cs.ind</code></td>
<td>
<p>indices (integer) of column numbers which are (like)
<b>c</b>oefficients and <b>s</b>tandard errors to be formatted
together.</p>
</td></tr>
<tr><td><code id="printCoefmat_+3A_tst.ind">tst.ind</code></td>
<td>
<p>indices (integer) of column numbers for test
statistics.</p>
</td></tr>
<tr><td><code id="printCoefmat_+3A_zap.ind">zap.ind</code></td>
<td>
<p>indices (integer) of column numbers which should be
formatted by <code><a href="base.html#topic+zapsmall">zapsmall</a></code>, i.e., by &lsquo;zapping&rsquo; values
close to 0.</p>
</td></tr>
<tr><td><code id="printCoefmat_+3A_p.values">P.values</code></td>
<td>
<p>logical or <code>NULL</code>; if <code>TRUE</code>, the last
column of <code>x</code> is formatted by <code><a href="base.html#topic+format.pval">format.pval</a></code> as P
values.  If <code>P.values = NULL</code>, the default, it is set to
<code>TRUE</code> only if <code><a href="base.html#topic+options">options</a>("show.coef.Pvalue")</code> is
<code>TRUE</code> <em>and</em> <code>x</code> has at least 4 columns <em>and</em>
the last column name of <code>x</code> starts with <code>"Pr("</code>.</p>
</td></tr>
<tr><td><code id="printCoefmat_+3A_has.pvalue">has.Pvalue</code></td>
<td>
<p>logical; if <code>TRUE</code>, the last column of <code>x</code>
contains P values; in that case, it is printed if and only if
<code>P.values</code> (above) is true.</p>
</td></tr>
<tr><td><code id="printCoefmat_+3A_eps.pvalue">eps.Pvalue</code></td>
<td>
<p>number, passed to <code><a href="base.html#topic+format.pval">format.pval</a>()</code> as <code>eps</code>.</p>
</td></tr>
<tr><td><code id="printCoefmat_+3A_na.print">na.print</code></td>
<td>
<p>a character string to code <code><a href="base.html#topic+NA">NA</a></code> values in
printed output.</p>
</td></tr>
<tr><td><code id="printCoefmat_+3A_quote">quote</code>, <code id="printCoefmat_+3A_right">right</code>, <code id="printCoefmat_+3A_...">...</code></td>
<td>
<p>further arguments passed to
<code><a href="base.html#topic+print.default">print.default</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly returns its argument, <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Martin Maechler</p>


<h3>See Also</h3>

<p><code><a href="#topic+print.summary.lm">print.summary.lm</a></code>,
<code><a href="base.html#topic+format.pval">format.pval</a></code>,
<code><a href="base.html#topic+format">format</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cmat &lt;- cbind(rnorm(3, 10), sqrt(rchisq(3, 12)))
cmat &lt;- cbind(cmat, cmat[, 1]/cmat[, 2])
cmat &lt;- cbind(cmat, 2*pnorm(-cmat[, 3]))
colnames(cmat) &lt;- c("Estimate", "Std.Err", "Z value", "Pr(&gt;z)")
printCoefmat(cmat[, 1:3])
printCoefmat(cmat)
op &lt;- options(show.coef.Pvalues = FALSE)
printCoefmat(cmat, digits = 2)
printCoefmat(cmat, digits = 2, P.values = TRUE)
options(op) # restore
</code></pre>

<hr>
<h2 id='profile'>Generic Function for Profiling Models</h2><span id='topic+profile'></span>

<h3>Description</h3>

<p>Investigates the behavior of the objective function near the solution
represented by <code>fitted</code>.
</p>
<p>See documentation on method functions for further details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>profile(fitted, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="profile_+3A_fitted">fitted</code></td>
<td>
<p>the original fitted model object.</p>
</td></tr>
<tr><td><code id="profile_+3A_...">...</code></td>
<td>
<p>additional parameters. See documentation on individual
methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with an element for each parameter being profiled.  See the
individual methods for further details.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+profile.nls">profile.nls</a></code>, <code><a href="MASS.html#topic+profile.glm">profile.glm</a></code> ...
</p>
<p><code><a href="#topic+plot.profile">plot.profile</a></code>.
</p>
<p>For profiling R code, see <code><a href="utils.html#topic+Rprof">Rprof</a></code>.
</p>

<hr>
<h2 id='profile.glm'>Method for Profiling <code>glm</code> Objects</h2><span id='topic+profile.glm'></span>

<h3>Description</h3>

<p>Investigates the profile log-likelihood function for a fitted model of
class <code>"glm"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'glm'
profile(fitted, which = 1:p, alpha = 0.01, maxsteps = 10,
        del = zmax/5, trace = FALSE, test = c("LRT", "Rao"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="profile.glm_+3A_fitted">fitted</code></td>
<td>
<p>the original fitted model object.</p>
</td></tr>
<tr><td><code id="profile.glm_+3A_which">which</code></td>
<td>
<p>the original model parameters which should be profiled.
This can be a numeric or character vector.
By default, all parameters are profiled.</p>
</td></tr>
<tr><td><code id="profile.glm_+3A_alpha">alpha</code></td>
<td>
<p>highest significance level allowed for the
profile z-statistics.</p>
</td></tr>
<tr><td><code id="profile.glm_+3A_maxsteps">maxsteps</code></td>
<td>
<p>maximum number of points to be used for profiling each
parameter.</p>
</td></tr>
<tr><td><code id="profile.glm_+3A_del">del</code></td>
<td>
<p>suggested change on the scale of the profile
t-statistics.  Default value chosen to allow profiling at about
10 parameter values.</p>
</td></tr>
<tr><td><code id="profile.glm_+3A_trace">trace</code></td>
<td>
<p>logical: should the progress of profiling be reported?</p>
</td></tr>
<tr><td><code id="profile.glm_+3A_test">test</code></td>
<td>
<p>profile Likelihood Ratio test or Rao Score test.</p>
</td></tr>
<tr><td><code id="profile.glm_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The profile z-statistic is defined either as (case <code>test = "LRT"</code>) the square root of change in
deviance with an
appropriate sign, or (case <code>test = "Rao"</code>) as the similarly
signed square root of the Rao Score test
statistic. The latter is defined as the squared gradient of the profile log likelihood
divided by the profile Fisher information, but more conveniently
calculated via the deviance of a Gaussian GLM fitted to the residuals
of the profiled model.
</p>


<h3>Value</h3>

<p>A list of classes <code>"profile.glm"</code> and <code>"profile"</code> with an
element for each parameter being profiled.  The elements are
data-frames with two variables
</p>
<table>
<tr><td><code>par.vals</code></td>
<td>
<p>a matrix of parameter values for each fitted model.</p>
</td></tr>
<tr><td><code>tau or z</code></td>
<td>
<p>the profile t or z-statistics (the name depends on whether  there is an estimated dispersion parameter.)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Originally, D. M. Bates and W. N. Venables.  (For S in 1996.)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm">glm</a></code>, <code><a href="#topic+profile">profile</a></code>, <code><a href="#topic+plot.profile">plot.profile</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>options(contrasts = c("contr.treatment", "contr.poly"))
ldose &lt;- rep(0:5, 2)
numdead &lt;- c(1, 4, 9, 13, 18, 20, 0, 2, 6, 10, 12, 16)
sex &lt;- factor(rep(c("M", "F"), c(6, 6)))
SF &lt;- cbind(numdead, numalive = 20 - numdead)
budworm.lg &lt;- glm(SF ~ sex*ldose, family = binomial)
pr1 &lt;- profile(budworm.lg)
plot(pr1)
pairs(pr1)
</code></pre>

<hr>
<h2 id='profile.nls'>Method for Profiling <code>nls</code> Objects</h2><span id='topic+profile.nls'></span>

<h3>Description</h3>

<p>Investigates the profile log-likelihood function for a fitted model of
class <code>"nls"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nls'
profile(fitted, which = 1:npar, maxpts = 100, alphamax = 0.01,
        delta.t = cutoff/5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="profile.nls_+3A_fitted">fitted</code></td>
<td>
<p>the original fitted model object.</p>
</td></tr>
<tr><td><code id="profile.nls_+3A_which">which</code></td>
<td>
<p>the original model parameters which should be profiled.
This can be a numeric or character vector.
By default, all non-linear parameters are profiled.</p>
</td></tr>
<tr><td><code id="profile.nls_+3A_maxpts">maxpts</code></td>
<td>
<p>maximum number of points to be used for profiling each
parameter.</p>
</td></tr>
<tr><td><code id="profile.nls_+3A_alphamax">alphamax</code></td>
<td>
<p>highest significance level allowed
for the profile t-statistics.</p>
</td></tr>
<tr><td><code id="profile.nls_+3A_delta.t">delta.t</code></td>
<td>
<p>suggested change on the scale of the profile
t-statistics.  Default value chosen to allow profiling at about
10 parameter values.</p>
</td></tr>
<tr><td><code id="profile.nls_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The profile t-statistics is defined as the square root of change in
sum-of-squares divided by residual standard error with an
appropriate sign.
</p>


<h3>Value</h3>

<p>A list with an element for each parameter being profiled. The elements
are data-frames with two variables
</p>
<table>
<tr><td><code>par.vals</code></td>
<td>
<p>a matrix of parameter values for each fitted model.</p>
</td></tr>
<tr><td><code>tau</code></td>
<td>
<p>the profile t-statistics.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Of the original version,
Douglas M. Bates and Saikat DebRoy
</p>


<h3>References</h3>

<p>Bates, D. M. and Watts, D. G. (1988), <em>Nonlinear Regression Analysis
and Its Applications</em>, Wiley (chapter 6).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nls">nls</a></code>, <code><a href="#topic+profile">profile</a></code>, <code><a href="#topic+plot.profile.nls">plot.profile.nls</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# obtain the fitted object
fm1 &lt;- nls(demand ~ SSasympOrig(Time, A, lrc), data = BOD)
# get the profile for the fitted model: default level is too extreme
pr1 &lt;- profile(fm1, alphamax = 0.05)
# profiled values for the two parameters

pr1$A
pr1$lrc

# see also example(plot.profile.nls)

</code></pre>

<hr>
<h2 id='proj'>Projections of Models</h2><span id='topic+proj'></span><span id='topic+proj.default'></span><span id='topic+proj.lm'></span><span id='topic+proj.aov'></span><span id='topic+proj.aovlist'></span>

<h3>Description</h3>

<p><code>proj</code> returns a matrix or list of matrices giving the projections
of the data onto the terms of a linear model.  It is most frequently
used for <code><a href="#topic+aov">aov</a></code> models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>proj(object, ...)

## S3 method for class 'aov'
proj(object, onedf = FALSE, unweighted.scale = FALSE, ...)

## S3 method for class 'aovlist'
proj(object, onedf = FALSE, unweighted.scale = FALSE, ...)

## Default S3 method:
proj(object, onedf = TRUE, ...)

## S3 method for class 'lm'
proj(object, onedf = FALSE, unweighted.scale = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="proj_+3A_object">object</code></td>
<td>
<p>An object of class <code>"lm"</code> or a class inheriting from
it, or an object with a similar structure including in particular
components <code>qr</code> and <code>effects</code>.</p>
</td></tr>
<tr><td><code id="proj_+3A_onedf">onedf</code></td>
<td>
<p>A logical flag. If <code>TRUE</code>, a projection is returned for all
the columns of the model matrix. If <code>FALSE</code>, the single-column
projections are collapsed by terms of the model (as represented in
the analysis of variance table).</p>
</td></tr>
<tr><td><code id="proj_+3A_unweighted.scale">unweighted.scale</code></td>
<td>
<p>If the fit producing <code>object</code> used
weights, this determines if the projections correspond to weighted or
unweighted observations.</p>
</td></tr>
<tr><td><code id="proj_+3A_...">...</code></td>
<td>
<p>Swallow and ignore any other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A projection is given for each stratum of the object, so for <code>aov</code>
models with an <code>Error</code> term the result is a list of projections.
</p>


<h3>Value</h3>

<p>A projection matrix or (for multi-stratum objects) a list of
projection matrices.
</p>
<p>Each projection is a matrix with a row for each observations and
either a column for each term (<code>onedf = FALSE</code>) or for each
coefficient (<code>onedf = TRUE</code>). Projection matrices from the
default method have orthogonal columns representing the projection of
the response onto the column space of the Q matrix from the QR
decomposition.  The fitted values are the sum of the projections, and
the sum of squares for each column is the reduction in sum of squares
from fitting that column (after those to the left of it).
</p>
<p>The methods for <code>lm</code> and <code>aov</code> models add a column to the
projection matrix giving the residuals (the projection of the data
onto the orthogonal complement of the model space).
</p>
<p>Strictly, when <code>onedf = FALSE</code> the result is not a projection,
but the columns represent sums of projections onto the columns of the
model matrix corresponding to that term. In this case the matrix does
not depend on the coding used.
</p>


<h3>Author(s)</h3>

<p>The design was inspired by the S function of the same name described
in Chambers <abbr>et al.</abbr> (1992).
</p>


<h3>References</h3>

<p>Chambers, J. M., Freeny, A and Heiberger, R. M. (1992)
<em>Analysis of variance; designed experiments.</em>
Chapter 5 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+aov">aov</a></code>, <code><a href="#topic+lm">lm</a></code>, <code><a href="#topic+model.tables">model.tables</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N &lt;- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
P &lt;- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
K &lt;- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
yield &lt;- c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,62.8,55.8,69.5,
55.0, 62.0,48.8,45.5,44.2,52.0,51.5,49.8,48.8,57.2,59.0,53.2,56.0)

npk &lt;- data.frame(block = gl(6,4), N = factor(N), P = factor(P),
                  K = factor(K), yield = yield)
npk.aov &lt;- aov(yield ~ block + N*P*K, npk)
proj(npk.aov)

## as a test, not particularly sensible
options(contrasts = c("contr.helmert", "contr.treatment"))
npk.aovE &lt;- aov(yield ~  N*P*K + Error(block), npk)
proj(npk.aovE)
</code></pre>

<hr>
<h2 id='prop.test'>Test of Equal or Given Proportions</h2><span id='topic+prop.test'></span>

<h3>Description</h3>

<p><code>prop.test</code> can be used for testing the null that the
proportions (probabilities of success) in several groups are the
same, or that they equal certain given values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prop.test(x, n, p = NULL,
          alternative = c("two.sided", "less", "greater"),
          conf.level = 0.95, correct = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prop.test_+3A_x">x</code></td>
<td>
<p>a vector of counts of successes, a one-dimensional table with
two entries, or a two-dimensional table (or matrix) with 2 columns,
giving the counts of successes and failures, respectively.</p>
</td></tr>
<tr><td><code id="prop.test_+3A_n">n</code></td>
<td>
<p>a vector of counts of trials; ignored if <code>x</code> is a
matrix or a table.</p>
</td></tr>
<tr><td><code id="prop.test_+3A_p">p</code></td>
<td>
<p>a vector of probabilities of success.  The length of
<code>p</code> must be the same as the number of groups specified by
<code>x</code>, and its elements must be greater than 0 and less than 1.</p>
</td></tr>
<tr><td><code id="prop.test_+3A_alternative">alternative</code></td>
<td>
<p>a character string specifying the alternative
hypothesis, must be one of <code>"two.sided"</code> (default),
<code>"greater"</code> or <code>"less"</code>.  You can specify just the initial
letter.  Only used for testing the null that a single proportion
equals a given value, or that two proportions are equal; ignored
otherwise.</p>
</td></tr>
<tr><td><code id="prop.test_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level of the returned confidence
interval.  Must be a single number between 0 and 1.  Only used
when testing the null that a single proportion equals a given
value, or that two proportions are equal; ignored otherwise.</p>
</td></tr>
<tr><td><code id="prop.test_+3A_correct">correct</code></td>
<td>
<p>a logical indicating whether Yates' continuity
correction should be applied where possible.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Only groups with finite numbers of successes and failures are used.
Counts of successes and failures must be nonnegative and hence not
greater than the corresponding numbers of trials which must be
positive.  All finite counts should be integers.
</p>
<p>If <code>p</code> is <code>NULL</code> and there is more than one group, the null
tested is that the proportions in each group are the same.  If there
are two groups, the alternatives are that the probability of success
in the first group is less than, not equal to, or greater than the
probability of success in the second group, as specified by
<code>alternative</code>.  A confidence interval for the difference of
proportions with confidence level as specified by <code>conf.level</code>
and clipped to <code class="reqn">[-1,1]</code> is returned.  Continuity correction is
used only if it does not exceed the difference of the sample
proportions in absolute value.  Otherwise, if there are more than 2
groups, the alternative is always <code>"two.sided"</code>, the returned
confidence interval is <code>NULL</code>, and continuity correction is never
used.
</p>
<p>If there is only one group, then the null tested is that the
underlying probability of success is <code>p</code>, or .5 if <code>p</code> is
not given.  The alternative is that the probability of success is less
than, not equal to, or greater than <code>p</code> or 0.5, respectively, as
specified by <code>alternative</code>.  A confidence interval for the
underlying proportion with confidence level as specified by
<code>conf.level</code> and clipped to <code class="reqn">[0,1]</code> is returned.  Continuity
correction is used only if it does not exceed the difference between
sample and null proportions in absolute value. The confidence interval
is computed by inverting the score test.
</p>
<p>Finally, if <code>p</code> is given and there are more than 2 groups, the
null tested is that the underlying probabilities of success are those
given by <code>p</code>.  The alternative is always <code>"two.sided"</code>, the
returned confidence interval is <code>NULL</code>, and continuity correction
is never used.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following
components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of Pearson's chi-squared test statistic.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of freedom of the approximate
chi-squared distribution of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>a vector with the sample proportions <code>x/n</code>.</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>a confidence interval for the true proportion if
there is one group, or for the difference in proportions if
there are 2 groups and <code>p</code> is not given, or <code>NULL</code>
otherwise.  In the cases where it is not <code>NULL</code>, the
returned confidence interval has an asymptotic confidence level
as specified by <code>conf.level</code>, and is appropriate to the
specified alternative hypothesis.</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>the value of <code>p</code> if specified by the null, or
<code>NULL</code> otherwise.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string indicating the method used, and
whether Yates' continuity correction was applied.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the names of the data.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Wilson, E.B. (1927).
Probable inference, the law of succession, and statistical inference.
<em>Journal of the American Statistical Association</em>, <b>22</b>,
209&ndash;212.
<a href="https://doi.org/10.2307/2276774">doi:10.2307/2276774</a>.
</p>
<p>Newcombe R.G. (1998).
Two-Sided Confidence Intervals for the Single Proportion: Comparison
of Seven Methods.
<em>Statistics in Medicine</em>, <b>17</b>, 857&ndash;872.
<a href="https://doi.org/10.1002/%28SICI%291097-0258%2819980430%2917%3A8%3C857%3A%3AAID-SIM777%3E3.0.CO%3B2-E">doi:10.1002/(SICI)1097-0258(19980430)17:8&lt;857::AID-SIM777&gt;3.0.CO;2-E</a>.
</p>
<p>Newcombe R.G. (1998).
Interval Estimation for the Difference Between Independent
Proportions: Comparison of Eleven Methods.
<em>Statistics in Medicine</em>, <b>17</b>, 873&ndash;890.
<a href="https://doi.org/10.1002/%28SICI%291097-0258%2819980430%2917%3A8%3C873%3A%3AAID-SIM779%3E3.0.CO%3B2-I">doi:10.1002/(SICI)1097-0258(19980430)17:8&lt;873::AID-SIM779&gt;3.0.CO;2-I</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+binom.test">binom.test</a></code> for an <em>exact</em> test of a binomial
hypothesis.</p>


<h3>Examples</h3>

<pre><code class='language-R'>heads &lt;- rbinom(1, size = 100, prob = .5)
prop.test(heads, 100)          # continuity correction TRUE by default
prop.test(heads, 100, correct = FALSE)

## Data from Fleiss (1981), p. 139.
## H0: The null hypothesis is that the four populations from which
##     the patients were drawn have the same true proportion of smokers.
## A:  The alternative is that this proportion is different in at
##     least one of the populations.

smokers  &lt;- c( 83, 90, 129, 70 )
patients &lt;- c( 86, 93, 136, 82 )
prop.test(smokers, patients)
</code></pre>

<hr>
<h2 id='prop.trend.test'>Test for trend in proportions</h2><span id='topic+prop.trend.test'></span>

<h3>Description</h3>

<p>Performs chi-squared test for trend in proportions, i.e., a test
asymptotically optimal for local alternatives where the log odds vary
in proportion with <code>score</code>.  By default, <code>score</code> is chosen
as the group numbers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prop.trend.test(x, n, score = seq_along(x))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prop.trend.test_+3A_x">x</code></td>
<td>
<p> Number of events </p>
</td></tr>
<tr><td><code id="prop.trend.test_+3A_n">n</code></td>
<td>
<p> Number of trials </p>
</td></tr>
<tr><td><code id="prop.trend.test_+3A_score">score</code></td>
<td>
<p> Group score </p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"htest"</code> with title, test statistic, p-value,
etc.
</p>


<h3>Note</h3>

<p> This really should get integrated with <code>prop.test</code> </p>


<h3>Author(s)</h3>

<p> Peter Dalgaard </p>


<h3>See Also</h3>

 <p><code><a href="#topic+prop.test">prop.test</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>smokers  &lt;- c( 83, 90, 129, 70 )
patients &lt;- c( 86, 93, 136, 82 )
prop.test(smokers, patients)
prop.trend.test(smokers, patients)
prop.trend.test(smokers, patients, c(0,0,0,1))
</code></pre>

<hr>
<h2 id='qqnorm'>Quantile-Quantile Plots</h2><span id='topic+qqnorm'></span><span id='topic+qqnorm.default'></span><span id='topic+qqplot'></span><span id='topic+qqline'></span>

<h3>Description</h3>

<p><code>qqnorm</code> is a generic function the default method of which
produces a normal QQ plot of the values in <code>y</code>.
<code>qqline</code> adds a line to a &ldquo;theoretical&rdquo;, by default
normal, quantile-quantile plot which passes through the <code>probs</code>
quantiles, by default the first and third quartiles.
</p>
<p><code>qqplot</code> produces a QQ plot of two datasets. If <code>conf.level</code> is
given, a confidence band for a function transforming the distribution of
<code>x</code> into the distribution of <code>y</code> is plotted based on
Switzer (1976). The QQ plot can be understood as an estimate of such a 
treatment function. If <code>exact = NULL</code> (the default), an
exact confidence band is computed if the product of
the sample sizes is less than 10000, with or
without ties. Otherwise, asymptotic distributions are used whose approximations may
be inaccurate in small samples. Monte-Carlo approximations based on
<code>B</code> random permutations are computed when <code>simulate = TRUE</code>.
Confidence bands are in agreement with Smirnov's test, that is, the
bisecting line is covered by the band iff the null of both samples coming 
from the same distribution cannot be rejected at the same level.
</p>
<p>Graphical parameters may be given as arguments to <code>qqnorm</code>,
<code>qqplot</code> and <code>qqline</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qqnorm(y, ...)
## Default S3 method:
qqnorm(y, ylim, main = "Normal Q-Q Plot",
       xlab = "Theoretical Quantiles", ylab = "Sample Quantiles",
       plot.it = TRUE, datax = FALSE, ...)

qqline(y, datax = FALSE, distribution = qnorm,
       probs = c(0.25, 0.75), qtype = 7, ...)

qqplot(x, y, plot.it = TRUE,
       xlab = deparse1(substitute(x)),
       ylab = deparse1(substitute(y)), ...,
       conf.level = NULL, 
       conf.args = list(exact = NULL, simulate.p.value = FALSE,
                        B = 2000, col = NA, border = NULL))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qqnorm_+3A_x">x</code></td>
<td>
<p>The first sample for <code>qqplot</code>.</p>
</td></tr>
<tr><td><code id="qqnorm_+3A_y">y</code></td>
<td>
<p>The second or only data sample.</p>
</td></tr>
<tr><td><code id="qqnorm_+3A_xlab">xlab</code>, <code id="qqnorm_+3A_ylab">ylab</code>, <code id="qqnorm_+3A_main">main</code></td>
<td>
<p>plot labels.  The <code>xlab</code> and <code>ylab</code>
refer to the y and x axes respectively if <code>datax = TRUE</code>.</p>
</td></tr>
<tr><td><code id="qqnorm_+3A_plot.it">plot.it</code></td>
<td>
<p>logical. Should the result be plotted?</p>
</td></tr>
<tr><td><code id="qqnorm_+3A_datax">datax</code></td>
<td>
<p>logical. Should data values be on the x-axis?</p>
</td></tr>
<tr><td><code id="qqnorm_+3A_distribution">distribution</code></td>
<td>
<p>quantile function for reference theoretical distribution.</p>
</td></tr>
<tr><td><code id="qqnorm_+3A_probs">probs</code></td>
<td>
<p>numeric vector of length two, representing probabilities.
Corresponding quantile pairs define the line drawn.</p>
</td></tr>
<tr><td><code id="qqnorm_+3A_qtype">qtype</code></td>
<td>
<p>the <code>type</code> of quantile computation used in <code><a href="#topic+quantile">quantile</a></code>.</p>
</td></tr>
<tr><td><code id="qqnorm_+3A_ylim">ylim</code>, <code id="qqnorm_+3A_...">...</code></td>
<td>
<p>graphical parameters.</p>
</td></tr>
<tr><td><code id="qqnorm_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level of the band. The default, <code>NULL</code>,
does not lead to the computation of a confidence band.</p>
</td></tr>
<tr><td><code id="qqnorm_+3A_conf.args">conf.args</code></td>
<td>
<p>list of arguments defining confidence band computation
and visualisation: <code>exact</code> is <code>NULL</code> (see details) or a logical indicating whether an exact
p-value should be computed,  <code>simulate.p.value</code> is a logical indicating whether to compute
p-values by Monte Carlo simulation, <code>B</code> defines the number of replicates used in the
Monte Carlo test, <code>col</code> and <code>border</code> define the color for filling
and border of the confidence band (the default, <code>NA</code> and <code>NULL</code>, is to leave the band unfilled
with black borders.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>qqnorm</code> and <code>qqplot</code>, a list with components
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>The x coordinates of the points that were/would be plotted</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>The original <code>y</code> vector, i.e., the corresponding y
coordinates <em>including <code><a href="base.html#topic+NA">NA</a></code>s</em>. If <code>conf.level</code>
was specified to <code>qqplot</code>, the list contains additional components 
<code>lwr</code> and <code>upr</code> defining the confidence band.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988).
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Switzer, P. (1976).
Confidence procedures for two-sample problems.
<em>Biometrika</em>, <b>63</b>(1), 13&ndash;25.
<a href="https://doi.org/10.1093/biomet/63.1.13">doi:10.1093/biomet/63.1.13</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ppoints">ppoints</a></code>, used by <code>qqnorm</code> to generate
approximations to expected order statistics for a normal distribution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

y &lt;- rt(200, df = 5)
qqnorm(y); qqline(y, col = 2)
qqplot(y, rt(300, df = 5))

qqnorm(precip, ylab = "Precipitation [in/yr] for 70 US cities")

## "QQ-Chisquare" : --------------------------
y &lt;- rchisq(500, df = 3)
## Q-Q plot for Chi^2 data against true theoretical distribution:
qqplot(qchisq(ppoints(500), df = 3), y,
       main = expression("Q-Q plot for" ~~ {chi^2}[nu == 3]))
qqline(y, distribution = function(p) qchisq(p, df = 3),
       probs = c(0.1, 0.6), col = 2)
mtext("qqline(*, dist = qchisq(., df=3), prob = c(0.1, 0.6))")
## (Note that the above uses ppoints() with a = 1/2, giving the
## probability points for quantile type 5: so theoretically, using
## qqline(qtype = 5) might be preferable.) 

## Figure 1 in Switzer (1976), knee angle data
switzer &lt;- data.frame(
    angle = c(-31, -30, -25, -25, -23, -23, -22, -20, -20, -18,
              -18, -18, -16, -15, -15, -14, -13, -11, -10, - 9,
              - 8, - 7, - 7, - 7, - 6, - 6, - 4, - 4, - 3, - 2,
              - 2, - 1,   1,   1,   4,   5,  11,  12,  16,  34,
              -31, -20, -18, -16, -16, -16, -15, -14, -14, -14,
              -14, -13, -13, -11, -11, -10, - 9, - 9, - 8, - 7,
              - 7, - 6, - 6,  -5, - 5, - 5, - 4, - 2, - 2, - 2,
                0,   0,   1,   1,   2,   4,   5,   5,   6,  17),
    sex = gl(2, 40, labels = c("Female", "Male")))

ks.test(angle ~ sex, data = switzer)
d &lt;- with(switzer, split(angle, sex))
with(d, qqplot(Female, Male, pch = 19, xlim = c(-31, 31), ylim = c(-31, 31),
               conf.level = 0.945, 
               conf.args = list(col = "lightgrey", exact = TRUE))
)
abline(a = 0, b = 1)

## agreement with ks.test
set.seed(1)
x &lt;- rnorm(50)
y &lt;- rnorm(50, mean = .5, sd = .95)
ex &lt;- TRUE
### p = 0.112
(pval &lt;- ks.test(x, y, exact = ex)$p.value)
## 88.8% confidence band with bisecting line
## touching the lower bound
qqplot(x, y, pch = 19, conf.level = 1 - pval, 
       conf.args = list(exact = ex, col = "lightgrey"))
abline(a = 0, b = 1)

</code></pre>

<hr>
<h2 id='quade.test'>Quade Test</h2><span id='topic+quade.test'></span><span id='topic+quade.test.default'></span><span id='topic+quade.test.formula'></span>

<h3>Description</h3>

<p>Performs a Quade test with unreplicated blocked data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quade.test(y, ...)

## Default S3 method:
quade.test(y, groups, blocks, ...)

## S3 method for class 'formula'
quade.test(formula, data, subset, na.action, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quade.test_+3A_y">y</code></td>
<td>
<p>either a numeric vector of data values, or a data matrix.</p>
</td></tr>
<tr><td><code id="quade.test_+3A_groups">groups</code></td>
<td>
<p>a vector giving the group for the corresponding elements
of <code>y</code> if this is a vector;  ignored if <code>y</code> is a matrix.
If not a factor object, it is coerced to one.</p>
</td></tr>
<tr><td><code id="quade.test_+3A_blocks">blocks</code></td>
<td>
<p>a vector giving the block for the corresponding elements
of <code>y</code> if this is a vector;  ignored if <code>y</code> is a matrix.
If not a factor object, it is coerced to one.</p>
</td></tr>
<tr><td><code id="quade.test_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>a ~ b | c</code>, where <code>a</code>,
<code>b</code> and <code>c</code> give the data values and corresponding groups
and blocks, respectively.</p>
</td></tr>
<tr><td><code id="quade.test_+3A_data">data</code></td>
<td>
<p>an optional matrix or data frame (or similar: see
<code><a href="#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="quade.test_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="quade.test_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.  Defaults to
<code>getOption("na.action")</code>.</p>
</td></tr>
<tr><td><code id="quade.test_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>quade.test</code> can be used for analyzing unreplicated complete
block designs (i.e., there is exactly one observation in <code>y</code>
for each combination of levels of <code>groups</code> and <code>blocks</code>)
where the normality assumption may be violated.
</p>
<p>The null hypothesis is that apart from an effect of <code>blocks</code>,
the location parameter of <code>y</code> is the same in each of the
<code>groups</code>.
</p>
<p>If <code>y</code> is a matrix, <code>groups</code> and <code>blocks</code> are obtained
from the column and row indices, respectively.  <code>NA</code>'s are not
allowed in <code>groups</code> or <code>blocks</code>;  if <code>y</code> contains
<code>NA</code>'s, corresponding blocks are removed.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of Quade's F statistic.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>a vector with the numerator and denominator degrees
of freedom of the approximate F distribution of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the character string <code>"Quade test"</code>.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the names of the data.</p>
</td></tr>
</table>


<h3>References</h3>

<p>D. Quade (1979),
Using weighted rankings in the analysis of complete blocks with
additive block effects.
<em>Journal of the American Statistical Association</em> <b>74</b>,
680&ndash;683.
</p>
<p>William J. Conover (1999),
<em>Practical nonparametric statistics</em>.
New York: John Wiley &amp; Sons.
Pages 373&ndash;380.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+friedman.test">friedman.test</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Conover (1999, p. 375f):
## Numbers of five brands of a new hand lotion sold in seven stores
## during one week.
y &lt;- matrix(c( 5,  4,  7, 10, 12,
               1,  3,  1,  0,  2,
              16, 12, 22, 22, 35,
               5,  4,  3,  5,  4,
              10,  9,  7, 13, 10,
              19, 18, 28, 37, 58,
              10,  7,  6,  8,  7),
            nrow = 7, byrow = TRUE,
            dimnames =
            list(Store = as.character(1:7),
                 Brand = LETTERS[1:5]))
y
(qTst &lt;- quade.test(y))

## Show equivalence of different versions of test :
utils::str(dy &lt;- as.data.frame(as.table(y)))
qT. &lt;- quade.test(Freq ~ Brand|Store, data = dy)
qT.$data.name &lt;- qTst$data.name
stopifnot(all.equal(qTst, qT., tolerance = 1e-15))
dys &lt;- dy[order(dy[,"Freq"]),]
qTs &lt;- quade.test(Freq ~ Brand|Store, data = dys)
qTs$data.name &lt;- qTst$data.name
stopifnot(all.equal(qTst, qTs, tolerance = 1e-15))
</code></pre>

<hr>
<h2 id='quantile'>Sample Quantiles</h2><span id='topic+quantile'></span><span id='topic+quantile.default'></span>

<h3>Description</h3>

<p>The generic function <code>quantile</code> produces sample quantiles
corresponding to the given probabilities.
The smallest observation corresponds to a probability of 0 and the
largest to a probability of 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quantile(x, ...)

## Default S3 method:
quantile(x, probs = seq(0, 1, 0.25), na.rm = FALSE,
         names = TRUE, type = 7, digits = 7, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quantile_+3A_x">x</code></td>
<td>
<p>numeric vector whose sample quantiles are wanted, or an
object of a class for which a method has been defined (see also
&lsquo;details&rsquo;). <code><a href="base.html#topic+NA">NA</a></code> and <code>NaN</code> values are not
allowed in numeric vectors unless <code>na.rm</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="quantile_+3A_probs">probs</code></td>
<td>
<p>numeric vector of probabilities with values in
<code class="reqn">[0,1]</code>.  (Values up to &lsquo;<span class="samp">&#8288;2e-14&#8288;</span>&rsquo; outside that
range are accepted and moved to the nearby endpoint.)</p>
</td></tr>
<tr><td><code id="quantile_+3A_na.rm">na.rm</code></td>
<td>
<p>logical; if true, any <code><a href="base.html#topic+NA">NA</a></code> and <code>NaN</code>'s
are removed from <code>x</code> before the quantiles are computed.</p>
</td></tr>
<tr><td><code id="quantile_+3A_names">names</code></td>
<td>
<p>logical; if true, the result has a <code><a href="base.html#topic+names">names</a></code>
attribute.  Set to <code>FALSE</code> for speedup with many <code>probs</code>.</p>
</td></tr>
<tr><td><code id="quantile_+3A_type">type</code></td>
<td>
<p>an integer between 1 and 9 selecting one of the
nine quantile algorithms detailed below to be used.</p>
</td></tr>
<tr><td><code id="quantile_+3A_digits">digits</code></td>
<td>
<p>used only when <code>names</code> is true: the precision to use
when formatting the percentages.  In <span class="rlang"><b>R</b></span> versions up to 4.0.x, this had
been set to <code>max(2, getOption("digits"))</code>, internally.</p>
</td></tr>
<tr><td><code id="quantile_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A vector of length <code>length(probs)</code> is returned;
if <code>names = TRUE</code>, it has a <code><a href="base.html#topic+names">names</a></code> attribute.
</p>
<p><code><a href="base.html#topic+NA">NA</a></code> and <code><a href="base.html#topic+NaN">NaN</a></code> values in <code>probs</code> are
propagated to the result.
</p>
<p>The default method works with classed objects sufficiently like
numeric vectors that <code>sort</code> and (not needed by types 1 and 3)
addition of elements and multiplication by a number work correctly.
Note that as this is in a namespace, the copy of <code>sort</code> in
<span class="pkg">base</span> will be used, not some S4 generic of that name.  Also note
that that is no check on the &lsquo;correctly&rsquo;, and so
e.g. <code>quantile</code> can be applied to complex vectors which (apart
from ties) will be ordered on their real parts.
</p>
<p>There is a method for the date-time classes (see
<code>"<a href="base.html#topic+POSIXt">POSIXt</a>"</code>).  Types 1 and 3 can be used for class
<code>"<a href="base.html#topic+Date">Date</a>"</code> and for ordered factors.
</p>


<h3>Types</h3>

<p><code>quantile</code> returns estimates of underlying distribution quantiles
based on one or two order statistics from the supplied elements in
<code>x</code> at probabilities in <code>probs</code>.  One of the nine quantile
algorithms discussed in Hyndman and Fan (1996), selected by
<code>type</code>, is employed.
</p>
<p>All sample quantiles are defined as weighted averages of
consecutive order statistics. Sample quantiles of type <code class="reqn">i</code>
are defined by:
</p>
<p style="text-align: center;"><code class="reqn">Q_{i}(p) = (1 - \gamma)x_{j} + \gamma x_{j+1}</code>
</p>

<p>where <code class="reqn">1 \le i \le 9</code>,
<code class="reqn">\frac{j - m}{n} \le p &lt; \frac{j - m + 1}{n}</code>,
<code class="reqn">x_{j}</code> is the <code class="reqn">j</code>-th order statistic, <code class="reqn">n</code> is the
sample size, the value of <code class="reqn">\gamma</code> is a function of
<code class="reqn">j = \lfloor np + m\rfloor</code> and <code class="reqn">g = np + m - j</code>,
and <code class="reqn">m</code> is a constant determined by the sample quantile type.
</p>
<p><strong>Discontinuous sample quantile types 1, 2, and 3</strong>
</p>
<p>For types 1, 2 and 3, <code class="reqn">Q_i(p)</code> is a discontinuous
function of <code class="reqn">p</code>, with <code class="reqn">m = 0</code> when <code class="reqn">i = 1</code> and <code class="reqn">i =
  2</code>, and <code class="reqn">m = -1/2</code> when <code class="reqn">i = 3</code>.
</p>

<dl>
<dt>Type 1</dt><dd><p>Inverse of empirical distribution function.
<code class="reqn">\gamma = 0</code> if <code class="reqn">g = 0</code>, and 1 otherwise.</p>
</dd>
<dt>Type 2</dt><dd><p>Similar to type 1 but with averaging at discontinuities.
<code class="reqn">\gamma = 0.5</code> if <code class="reqn">g = 0</code>, and 1 otherwise (SAS default, see
Wicklin (2017)).</p>
</dd>
<dt>Type 3</dt><dd><p>Nearest even order statistic (SAS default till ca. 2010).
<code class="reqn">\gamma = 0</code> if <code class="reqn">g = 0</code> and <code class="reqn">j</code> is even,
and 1 otherwise.</p>
</dd>
</dl>

<p><strong>Continuous sample quantile types 4 through 9</strong>
</p>
<p>For types 4 through 9, <code class="reqn">Q_i(p)</code> is a continuous function
of <code class="reqn">p</code>, with <code class="reqn">\gamma = g</code> and <code class="reqn">m</code> given below. The
sample quantiles can be obtained equivalently by linear interpolation
between the points <code class="reqn">(p_k,x_k)</code> where <code class="reqn">x_k</code>
is the <code class="reqn">k</code>-th order statistic.  Specific expressions for
<code class="reqn">p_k</code> are given below.
</p>

<dl>
<dt>Type 4</dt><dd><p><code class="reqn">m = 0</code>. <code class="reqn">p_k = \frac{k}{n}</code>.
That is, linear interpolation of the empirical <abbr>cdf</abbr>.
</p>
</dd>
<dt>Type 5</dt><dd><p><code class="reqn">m = 1/2</code>.
<code class="reqn">p_k = \frac{k - 0.5}{n}</code>.
That is a piecewise linear function where the knots are the values
midway through the steps of the empirical <abbr>cdf</abbr>.  This is popular
amongst hydrologists.
</p>
</dd>
<dt>Type 6</dt><dd><p><code class="reqn">m = p</code>. <code class="reqn">p_k = \frac{k}{n + 1}</code>.
Thus <code class="reqn">p_k = \mbox{E}[F(x_{k})]</code>.
This is used by Minitab and by SPSS.
</p>
</dd>
<dt>Type 7</dt><dd><p><code class="reqn">m = 1-p</code>.
<code class="reqn">p_k = \frac{k - 1}{n - 1}</code>.
In this case, <code class="reqn">p_k = \mbox{mode}[F(x_{k})]</code>.
This is used by S.
</p>
</dd>
<dt>Type 8</dt><dd><p><code class="reqn">m = (p+1)/3</code>.
<code class="reqn">p_k = \frac{k - 1/3}{n + 1/3}</code>.
Then <code class="reqn">p_k \approx \mbox{median}[F(x_{k})]</code>.
The resulting quantile estimates are approximately median-unbiased
regardless of the distribution of <code>x</code>.
</p>
</dd>
<dt>Type 9</dt><dd><p><code class="reqn">m = p/4 + 3/8</code>.
<code class="reqn">p_k = \frac{k - 3/8}{n + 1/4}</code>.
The resulting quantile estimates are approximately unbiased for
the expected order statistics if <code>x</code> is normally distributed.
</p>
</dd>
</dl>

<p>Further details are provided in Hyndman and Fan (1996) who
recommended type 8. 
The default method is type 7, as used by S and by <span class="rlang"><b>R</b></span> &lt; 2.0.0.
Makkonen argues for type 6, also as already proposed by Weibull in 1939.
The Wikipedia page contains further information about availability of
these 9 types in software.
</p>


<h3>Author(s)</h3>

<p>of the version used in <span class="rlang"><b>R</b></span> &gt;= 2.0.0, Ivan Frohne and Rob J Hyndman.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Hyndman, R. J. and Fan, Y. (1996) Sample quantiles in statistical
packages, <em>American Statistician</em> <b>50</b>, 361&ndash;365.
<a href="https://doi.org/10.2307/2684934">doi:10.2307/2684934</a>.
</p>
<p>Wicklin, R. (2017) Sample quantiles: A comparison of 9 definitions; SAS Blog.
<a href="https://blogs.sas.com/content/iml/2017/05/24/definitions-sample-quantiles.html">https://blogs.sas.com/content/iml/2017/05/24/definitions-sample-quantiles.html</a>
</p>




<p>Wikipedia: <a href="https://en.wikipedia.org/wiki/Quantile#Estimating_quantiles_from_a_sample">https://en.wikipedia.org/wiki/Quantile#Estimating_quantiles_from_a_sample</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ecdf">ecdf</a></code> for empirical distributions of which
<code>quantile</code> is an inverse;
<code><a href="grDevices.html#topic+boxplot.stats">boxplot.stats</a></code> and <code><a href="#topic+fivenum">fivenum</a></code> for computing
other versions of quartiles, etc.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>quantile(x &lt;- rnorm(1001)) # Extremes &amp; Quartiles by default
quantile(x,  probs = c(0.1, 0.5, 1, 2, 5, 10, 50, NA)/100)

### Compare different types
quantAll &lt;- function(x, prob, ...)
  t(vapply(1:9, function(typ) quantile(x, probs = prob, type = typ, ...),
           quantile(x, prob, type=1, ...)))
p &lt;- c(0.1, 0.5, 1, 2, 5, 10, 50)/100
signif(quantAll(x, p), 4)

## 0% and 100% are equal to min(), max() for all types:
stopifnot(t(quantAll(x, prob=0:1)) == range(x))

## for complex numbers:
z &lt;- complex(real = x, imaginary = -10*x)
signif(quantAll(z, p), 4)
</code></pre>

<hr>
<h2 id='r2dtable'>Random 2-way Tables with Given Marginals</h2><span id='topic+r2dtable'></span>

<h3>Description</h3>

<p>Generate random 2-way tables with given marginals using Patefield's
algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2dtable(n, r, c)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r2dtable_+3A_n">n</code></td>
<td>
<p>a non-negative numeric giving the number of tables to be
drawn.</p>
</td></tr>
<tr><td><code id="r2dtable_+3A_r">r</code></td>
<td>
<p>a non-negative vector of length at least 2 giving the row
totals, to be coerced to <code>integer</code>.  Must sum to the same as
<code>c</code>.</p>
</td></tr>
<tr><td><code id="r2dtable_+3A_c">c</code></td>
<td>
<p>a non-negative vector of length at least 2 giving the column
totals, to be coerced to <code>integer</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of length <code>n</code> containing the generated tables as its
components.
</p>


<h3>References</h3>

<p>Patefield, W. M. (1981).
Algorithm AS 159: An efficient method of generating r x c tables
with given row and column totals.
<em>Applied Statistics</em>, <b>30</b>, 91&ndash;97.
<a href="https://doi.org/10.2307/2346669">doi:10.2307/2346669</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Fisher's Tea Drinker data.
TeaTasting &lt;-
matrix(c(3, 1, 1, 3),
       nrow = 2,
       dimnames = list(Guess = c("Milk", "Tea"),
                       Truth = c("Milk", "Tea")))
## Simulate permutation test for independence based on the maximum
## Pearson residuals (rather than their sum).
rowTotals &lt;- rowSums(TeaTasting)
colTotals &lt;- colSums(TeaTasting)
nOfCases &lt;- sum(rowTotals)
expected &lt;- outer(rowTotals, colTotals) / nOfCases
maxSqResid &lt;- function(x) max((x - expected) ^ 2 / expected)
simMaxSqResid &lt;-
    sapply(r2dtable(1000, rowTotals, colTotals), maxSqResid)
sum(simMaxSqResid &gt;= maxSqResid(TeaTasting)) / 1000
## Fisher's exact test gives p = 0.4857 ...
</code></pre>

<hr>
<h2 id='read.ftable'>Manipulate Flat Contingency Tables</h2><span id='topic+read.ftable'></span><span id='topic+write.ftable'></span><span id='topic+format.ftable'></span><span id='topic+print.ftable'></span>

<h3>Description</h3>

<p>Read, write and coerce &lsquo;flat&rsquo; (contingency) tables, aka
<code>ftable</code>s.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read.ftable(file, sep = "", quote = "\"",
            row.var.names, col.vars, skip = 0)

write.ftable(x, file = "", quote = TRUE, append = FALSE,
             digits = getOption("digits"), sep = " ", ...)

## S3 method for class 'ftable'
format(x, quote = TRUE, digits = getOption("digits"),
       method = c("non.compact", "row.compact", "col.compact", "compact"),
       lsep = " | ",
       justify = c("left", "right"),
       ...)

## S3 method for class 'ftable'
print(x, digits = getOption("digits"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read.ftable_+3A_file">file</code></td>
<td>
<p>either a character string naming a file or a <code><a href="base.html#topic+connection">connection</a></code>
which the data are to be read from or written to.  <code>""</code>
indicates input from the console for reading and output to the
console for writing.</p>
</td></tr>
<tr><td><code id="read.ftable_+3A_sep">sep</code></td>
<td>
<p>the field separator string.  Values on each line of the file
are separated by this string.</p>
</td></tr>
<tr><td><code id="read.ftable_+3A_quote">quote</code></td>
<td>
<p>a character string giving the set of quoting characters
for <code>read.ftable</code>; to disable quoting altogether, use
<code>quote=""</code>.  For <code>write.table</code>, a logical indicating
whether strings in the data will be surrounded by double quotes.</p>
</td></tr>
<tr><td><code id="read.ftable_+3A_row.var.names">row.var.names</code></td>
<td>
<p>a character vector with the names of the row
variables, in case these cannot be determined automatically.</p>
</td></tr>
<tr><td><code id="read.ftable_+3A_col.vars">col.vars</code></td>
<td>
<p>a list giving the names and levels of the column
variables, in case these cannot be determined automatically.</p>
</td></tr>
<tr><td><code id="read.ftable_+3A_skip">skip</code></td>
<td>
<p>the number of lines of the data file to skip before
beginning to read data.</p>
</td></tr>
<tr><td><code id="read.ftable_+3A_x">x</code></td>
<td>
<p>an object of class <code>"ftable"</code>.</p>
</td></tr>
<tr><td><code id="read.ftable_+3A_append">append</code></td>
<td>
<p>logical.  If <code>TRUE</code> and <code>file</code> is the name of
a file (and not a connection or <code>"|cmd"</code>), the output from
<code>write.ftable</code> is appended to the file.  If <code>FALSE</code>,
the contents of <code>file</code> will be overwritten.</p>
</td></tr>
<tr><td><code id="read.ftable_+3A_digits">digits</code></td>
<td>
<p>an integer giving the number of significant digits to
use for (the cell entries of) <code>x</code>.</p>
</td></tr>
<tr><td><code id="read.ftable_+3A_method">method</code></td>
<td>
<p>string specifying how the <code>"ftable"</code> object is formatted
(and printed if used as in <code>write.ftable()</code> or the <code>print</code>
method).  Can be abbreviated.  Available methods are (see the examples):
</p>

<dl>
<dt>&quot;non.compact&quot;</dt><dd><p>the default representation of an
<code>"ftable"</code> object.</p>
</dd>
<dt>&quot;row.compact&quot;</dt><dd><p>a row-compact version without empty cells
below the column labels.</p>
</dd>
<dt>&quot;col.compact&quot;</dt><dd><p>a column-compact version without empty cells
to the right of the row labels.</p>
</dd>
<dt>&quot;compact&quot;</dt><dd><p>a row- and column-compact version.  This may imply
a row and a column label sharing the same cell.  They are then
separated by the string <code>lsep</code>.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="read.ftable_+3A_lsep">lsep</code></td>
<td>
<p>only for <code>method = "compact"</code>, the separation string
for row and column labels.</p>
</td></tr>
<tr><td><code id="read.ftable_+3A_justify">justify</code></td>
<td>
<p><code><a href="base.html#topic+character">character</a></code> vector of length (one or) two,
specifying how string justification should happen in <code>format(..)</code>,
first for the labels, then the table entries.</p>
</td></tr>
<tr><td><code id="read.ftable_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or
from methods; for <code>write()</code> and <code>print()</code>, notably
arguments such as <code>method</code>, passed to <code>format()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>read.ftable</code> reads in a flat-like contingency table from a
file.  If the file contains the written representation of a flat
table (more precisely, a header with all information on names and
levels of column variables, followed by a line with the names of the
row variables), no further arguments are needed.  Similarly, flat
tables with only one column variable the name of which is the only
entry in the first line are handled automatically.  Other variants can
be dealt with by skipping all header information using <code>skip</code>,
and providing the names of the row variables and the names and levels
of the column variable using <code>row.var.names</code> and <code>col.vars</code>,
respectively.  See the examples below.
</p>
<p>Note that flat tables are characterized by their &lsquo;ragged&rsquo;
display of row (and maybe also column) labels.  If the full grid of
levels of the row variables is given, one should instead use
<code><a href="utils.html#topic+read.table">read.table</a></code> to read in the data, and create the
contingency table from this using <code><a href="#topic+xtabs">xtabs</a></code>.
</p>
<p><code>write.ftable</code> writes a flat table to a file, which is useful for
generating &lsquo;pretty&rsquo; ASCII representations of contingency
tables.  Different versions are available via the <code>method</code>
argument, which may be useful, for example, for constructing LaTeX tables.
</p>


<h3>References</h3>

<p>Agresti, A. (1990)
<em>Categorical data analysis</em>.
New York: Wiley.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ftable">ftable</a></code> for more information on flat contingency tables.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Agresti (1990), page 157, Table 5.8.
## Not in ftable standard format, but o.k.
file &lt;- tempfile()
cat("             Intercourse\n",
    "Race  Gender     Yes  No\n",
    "White Male        43 134\n",
    "      Female      26 149\n",
    "Black Male        29  23\n",
    "      Female      22  36\n",
    file = file)
file.show(file)
ft1 &lt;- read.ftable(file)
ft1
unlink(file)

## Agresti (1990), page 297, Table 8.16.
## Almost o.k., but misses the name of the row variable.
file &lt;- tempfile()
cat("                      \"Tonsil Size\"\n",
    "            \"Not Enl.\" \"Enl.\" \"Greatly Enl.\"\n",
    "Noncarriers       497     560           269\n",
    "Carriers           19      29            24\n",
    file = file)
file.show(file)
ft &lt;- read.ftable(file, skip = 2,
                  row.var.names = "Status",
                  col.vars = list("Tonsil Size" =
                      c("Not Enl.", "Enl.", "Greatly Enl.")))
ft
unlink(file)

ft22 &lt;- ftable(Titanic, row.vars = 2:1, col.vars = 4:3)
write.ftable(ft22, quote = FALSE) # is the same as
print(ft22)#method="non.compact" is default
print(ft22, method="row.compact")
print(ft22, method="col.compact")
print(ft22, method="compact")

## using 'justify' and 'quote' :
format(ftable(wool + tension ~ breaks, warpbreaks),
       justify = "none", quote = FALSE)

</code></pre>

<hr>
<h2 id='rect.hclust'>Draw Rectangles Around Hierarchical Clusters</h2><span id='topic+rect.hclust'></span>

<h3>Description</h3>

<p>Draws rectangles around the branches of a dendrogram highlighting the
corresponding clusters. First the dendrogram is cut at a certain
level, then a rectangle is drawn around selected branches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rect.hclust(tree, k = NULL, which = NULL, x = NULL, h = NULL,
            border = 2, cluster = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rect.hclust_+3A_tree">tree</code></td>
<td>
<p>an object of the type produced by <code>hclust</code>.</p>
</td></tr>
<tr><td><code id="rect.hclust_+3A_k">k</code>, <code id="rect.hclust_+3A_h">h</code></td>
<td>
<p>Scalar. Cut the dendrogram such that either exactly
<code>k</code> clusters are produced or by cutting at height <code>h</code>.</p>
</td></tr>
<tr><td><code id="rect.hclust_+3A_which">which</code>, <code id="rect.hclust_+3A_x">x</code></td>
<td>
<p>A vector selecting the clusters around which a
rectangle should be drawn. <code>which</code> selects clusters by number
(from left to right in the tree), <code>x</code> selects clusters
containing the respective horizontal coordinates. Default is
<code>which = 1:k</code>.</p>
</td></tr>
<tr><td><code id="rect.hclust_+3A_border">border</code></td>
<td>
<p>Vector with border colors for the rectangles.</p>
</td></tr>
<tr><td><code id="rect.hclust_+3A_cluster">cluster</code></td>
<td>
<p>Optional vector with cluster memberships as returned by
<code>cutree(hclust.obj, k = k)</code>, can be specified for efficiency if
already computed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>(Invisibly) returns a list where each element contains a vector of
data points contained in the respective cluster.</p>


<h3>See Also</h3>

<p><code><a href="#topic+hclust">hclust</a></code>, <code><a href="#topic+identify.hclust">identify.hclust</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

hca &lt;- hclust(dist(USArrests))
plot(hca)
rect.hclust(hca, k = 3, border = "red")
x &lt;- rect.hclust(hca, h = 50, which = c(2,7), border = 3:4)
x
</code></pre>

<hr>
<h2 id='relevel'>Reorder Levels of Factor</h2><span id='topic+relevel'></span><span id='topic+relevel.default'></span><span id='topic+relevel.factor'></span><span id='topic+relevel.ordered'></span>

<h3>Description</h3>

<p>The levels of a factor are re-ordered so that the level specified by
<code>ref</code> is first and the others are moved down. This is useful
for <code>contr.treatment</code> contrasts which take the first level as
the reference.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relevel(x, ref, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relevel_+3A_x">x</code></td>
<td>
<p>an unordered factor.</p>
</td></tr>
<tr><td><code id="relevel_+3A_ref">ref</code></td>
<td>
<p>the reference level, typically a string.</p>
</td></tr>
<tr><td><code id="relevel_+3A_...">...</code></td>
<td>
<p>additional arguments for future methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This, as <code><a href="#topic+reorder">reorder</a>()</code>, is a special case of simply calling
<code><a href="base.html#topic+factor">factor</a>(x, levels = levels(x)[....])</code>.
</p>


<h3>Value</h3>

<p>A factor of the same length as <code>x</code>.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+factor">factor</a></code>, <code><a href="#topic+contr.treatment">contr.treatment</a></code>,
<code><a href="base.html#topic+levels">levels</a></code>, <code><a href="#topic+reorder">reorder</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>warpbreaks$tension &lt;- relevel(warpbreaks$tension, ref = "M")
summary(lm(breaks ~ wool + tension, data = warpbreaks))
</code></pre>

<hr>
<h2 id='reorder.default'>Reorder Levels of a Factor</h2><span id='topic+reorder'></span><span id='topic+reorder.default'></span>

<h3>Description</h3>

<p><code>reorder</code> is a generic function.  The <code>"default"</code> method
treats its first argument as a categorical variable, and reorders its
levels based on the values of a second variable, usually numeric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reorder(x, ...)

## Default S3 method:
reorder(x, X, FUN = mean, ...,
        order = is.ordered(x), decreasing = FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reorder.default_+3A_x">x</code></td>
<td>
<p>an atomic vector, usually a <code><a href="base.html#topic+factor">factor</a></code> (possibly
ordered).  The vector is treated as a categorical variable whose
levels will be reordered.  If <code>x</code> is not a factor, its unique
values will be used as the implicit levels.
</p>
</td></tr>
<tr><td><code id="reorder.default_+3A_x">X</code></td>
<td>
<p>a vector of the same length as <code>x</code>, whose subset
of values for each unique level of <code>x</code> determines the
eventual order of that level.
</p>
</td></tr>
<tr><td><code id="reorder.default_+3A_fun">FUN</code></td>
<td>
<p>a <code><a href="base.html#topic+function">function</a></code> whose first argument is a vector
and returns a scalar, to be applied to each subset of <code>X</code>
determined by the levels of <code>x</code>.</p>
</td></tr>
<tr><td><code id="reorder.default_+3A_...">...</code></td>
<td>
<p> optional: extra arguments supplied to <code>FUN</code></p>
</td></tr>
<tr><td><code id="reorder.default_+3A_order">order</code></td>
<td>
<p> logical, whether return value will be an ordered factor
rather than a factor.
</p>
</td></tr>
<tr><td><code id="reorder.default_+3A_decreasing">decreasing</code></td>
<td>
<p>logical, whether the levels will be ordered in
increasing or decreasing order.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This, as <code><a href="#topic+relevel">relevel</a>()</code>, is a special case of simply calling
<code><a href="base.html#topic+factor">factor</a>(x, levels = levels(x)[....])</code>.
</p>


<h3>Value</h3>

<p>A factor or an ordered factor (depending on the value of
<code>order</code>), with the order of the levels determined by
<code>FUN</code> applied to <code>X</code> grouped by <code>x</code>.  By default, the
levels are ordered such that the values returned by <code>FUN</code>
are in increasing order.  Empty levels will be dropped.
</p>
<p>Additionally, the values of <code>FUN</code> applied to the subsets of
<code>X</code> (in the original order of the levels of <code>x</code>) is returned
as the <code>"scores"</code> attribute.
</p>


<h3>Author(s)</h3>

<p>Deepayan Sarkar <a href="mailto:deepayan.sarkar@r-project.org">deepayan.sarkar@r-project.org</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+reorder.dendrogram">reorder.dendrogram</a></code>, <code><a href="base.html#topic+levels">levels</a></code>,
<code><a href="#topic+relevel">relevel</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

bymedian &lt;- with(InsectSprays, reorder(spray, count, median))
boxplot(count ~ bymedian, data = InsectSprays,
        xlab = "Type of spray", ylab = "Insect count",
        main = "InsectSprays data", varwidth = TRUE,
        col = "lightgray")

bymedianR &lt;- with(InsectSprays, reorder(spray, count, median, decreasing=TRUE))
stopifnot(exprs = {
    identical(attr(bymedian, "scores") -&gt; sc,
              attr(bymedianR,"scores"))
    identical(nms &lt;- names(sc), LETTERS[1:6])
    identical(levels(bymedian ), nms[isc &lt;- order(sc)])
    identical(levels(bymedianR), nms[rev(isc)])
})
</code></pre>

<hr>
<h2 id='reorder.dendrogram'>Reorder a Dendrogram</h2><span id='topic+reorder.dendrogram'></span>

<h3>Description</h3>

<p>A method for the generic function <code><a href="#topic+reorder">reorder</a></code>.
</p>
<p>There are many different orderings of a dendrogram that are consistent
with the structure imposed.  This function takes a dendrogram and a
vector of values and reorders the dendrogram in the order of the
supplied vector, maintaining the constraints on the dendrogram.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dendrogram'
reorder(x, wts, agglo.FUN = sum, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reorder.dendrogram_+3A_x">x</code></td>
<td>
<p>the (dendrogram) object to be reordered</p>
</td></tr>
<tr><td><code id="reorder.dendrogram_+3A_wts">wts</code></td>
<td>
<p>numeric weights (arbitrary values) for reordering.</p>
</td></tr>
<tr><td><code id="reorder.dendrogram_+3A_agglo.fun">agglo.FUN</code></td>
<td>
<p>a function for weights agglomeration, see below.</p>
</td></tr>
<tr><td><code id="reorder.dendrogram_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Using the weights <code>wts</code>, the leaves of the dendrogram are
reordered so as to be in an order as consistent as possible with the
weights.  At each node, the branches are ordered in
increasing weights where the weight of a branch is defined as
<code class="reqn">f(w_j)</code> where <code class="reqn">f</code> is <code>agglo.FUN</code> and <code class="reqn">w_j</code> is the
weight of the <code class="reqn">j</code>-th sub branch.
</p>


<h3>Value</h3>

<p>A dendrogram where each node has a further attribute <code>value</code> with
its corresponding weight.
</p>


<h3>Author(s)</h3>

<p>R. Gentleman and M. Maechler</p>


<h3>See Also</h3>

<p><code><a href="#topic+reorder">reorder</a></code>.
</p>
<p><code><a href="#topic+rev.dendrogram">rev.dendrogram</a></code> which simply reverses the nodes'
order; <code><a href="#topic+heatmap">heatmap</a></code>, <code><a href="#topic+cophenetic">cophenetic</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

set.seed(123)
x &lt;- rnorm(10)
hc &lt;- hclust(dist(x))
dd &lt;- as.dendrogram(hc)
dd.reorder &lt;- reorder(dd, 10:1)
plot(dd, main = "random dendrogram 'dd'")

op &lt;- par(mfcol = 1:2)
plot(dd.reorder, main = "reorder(dd, 10:1)")
plot(reorder(dd, 10:1, agglo.FUN = mean), main = "reorder(dd, 10:1, mean)")
par(op)
</code></pre>

<hr>
<h2 id='replications'>Number of Replications of Terms</h2><span id='topic+replications'></span>

<h3>Description</h3>

<p>Returns a vector or a list of the number of replicates for
each term in the formula.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>replications(formula, data = NULL, na.action)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="replications_+3A_formula">formula</code></td>
<td>
<p>a formula or a terms object or a data frame.</p>
</td></tr>
<tr><td><code id="replications_+3A_data">data</code></td>
<td>
<p>a data frame used  to  find  the  objects in <code>formula</code>.</p>
</td></tr>
<tr><td><code id="replications_+3A_na.action">na.action</code></td>
<td>
<p>function for handling missing values.  Defaults to
a <code>na.action</code> attribute of <code>data</code>, then
a setting of the option <code>na.action</code>, or <code>na.fail</code> if that
is not set.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>formula</code> is a data frame and <code>data</code> is missing,
<code>formula</code> is used for <code>data</code> with the formula <code>~ .</code>.
</p>
<p>Any character vectors in the formula are coerced to factors.
</p>


<h3>Value</h3>

<p>A vector or list with one entry for each term in the formula giving
the number(s) of replications for each level. If all levels are
balanced (have the same number of replications) the result is a
vector, otherwise it is a list with a component for each terms,
as a vector, matrix or array as required.
</p>
<p>A test for balance is <code>!is.list(replications(formula,data))</code>.
</p>


<h3>Author(s)</h3>

<p>The design was inspired by the S function of the same name described
in Chambers <abbr>et al.</abbr> (1992).
</p>


<h3>References</h3>

<p>Chambers, J. M., Freeny, A and Heiberger, R. M. (1992)
<em>Analysis of variance; designed experiments.</em>
Chapter 5 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+model.tables">model.tables</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## From Venables and Ripley (2002) p.165.
N &lt;- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
P &lt;- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
K &lt;- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
yield &lt;- c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,62.8,55.8,69.5,
55.0, 62.0,48.8,45.5,44.2,52.0,51.5,49.8,48.8,57.2,59.0,53.2,56.0)

npk &lt;- data.frame(block = gl(6,4), N = factor(N), P = factor(P),
                  K = factor(K), yield = yield)
replications(~ . - yield, npk)
</code></pre>

<hr>
<h2 id='reshape'>Reshape Grouped Data</h2><span id='topic+reshape'></span>

<h3>Description</h3>

<p>This function reshapes a data frame between &lsquo;wide&rsquo; format (with
repeated measurements in separate columns of the same row) and
&lsquo;long&rsquo; format (with the repeated measurements in separate
rows).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reshape(data, varying = NULL, v.names = NULL, timevar = "time",
        idvar = "id", ids = 1:NROW(data),
        times = seq_along(varying[[1]]),
        drop = NULL, direction, new.row.names = NULL,
        sep = ".",
        split = if (sep == "") {
            list(regexp = "[A-Za-z][0-9]", include = TRUE)
        } else {
            list(regexp = sep, include = FALSE, fixed = TRUE)}
        )

### Typical usage for converting from long to wide format:

# reshape(data, direction = "wide",
#         idvar = "___", timevar = "___", # mandatory
#         v.names = c(___),    # time-varying variables
#         varying = list(___)) # auto-generated if missing

### Typical usage for converting from wide to long format:

### If names of wide-format variables are in a 'nice' format

# reshape(data, direction = "long",
#         varying = c(___), # vector 
#         sep)              # to help guess 'v.names' and 'times'

### To specify long-format variable names explicitly

# reshape(data, direction = "long",
#         varying = ___,  # list / matrix / vector (use with care)
#         v.names = ___,  # vector of variable names in long format
#         timevar, times, # name / values of constructed time variable
#         idvar, ids)     # name / values of constructed id variable

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reshape_+3A_data">data</code></td>
<td>
<p>a data frame</p>
</td></tr>
<tr><td><code id="reshape_+3A_varying">varying</code></td>
<td>
<p>names of sets of variables in the wide format that
correspond to single variables in long format
(&lsquo;time-varying&rsquo;).  This is canonically a list of vectors of
variable names, but it can optionally be a matrix of names, or a
single vector of names.  In each case, when <code>direction =
    "long"</code>, the names can be replaced by indices which are interpreted
as referring to <code>names(data)</code>.  See &lsquo;Details&rsquo; for more
details and options.</p>
</td></tr>
<tr><td><code id="reshape_+3A_v.names">v.names</code></td>
<td>
<p>names of variables in the long format that correspond
to multiple variables in the wide format.  See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="reshape_+3A_timevar">timevar</code></td>
<td>
<p>the variable in long format that differentiates multiple
records from the same group or individual.  If more than one record
matches, the first will be taken (with a warning). </p>
</td></tr>
<tr><td><code id="reshape_+3A_idvar">idvar</code></td>
<td>
<p>Names of one or more variables in long format that
identify multiple records from the same group/individual.  These
variables may also be present in wide format.</p>
</td></tr>
<tr><td><code id="reshape_+3A_ids">ids</code></td>
<td>
<p>the values to use for a newly created <code>idvar</code>
variable in long format.</p>
</td></tr>
<tr><td><code id="reshape_+3A_times">times</code></td>
<td>
<p>the values to use for a newly created <code>timevar</code>
variable in long format.  See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="reshape_+3A_drop">drop</code></td>
<td>
<p>a vector of names of variables to drop before reshaping.</p>
</td></tr>
<tr><td><code id="reshape_+3A_direction">direction</code></td>
<td>
<p>character string, partially matched to either
<code>"wide"</code> to reshape to wide format, or <code>"long"</code> to reshape
to long format.</p>
</td></tr>
<tr><td><code id="reshape_+3A_new.row.names">new.row.names</code></td>
<td>
<p>character or <code>NULL</code>: a non-null value will be
used for the row names of the result.</p>
</td></tr>
<tr><td><code id="reshape_+3A_sep">sep</code></td>
<td>
<p>A character vector of length 1, indicating a separating
character in the variable names in the wide format.  This is used for
guessing <code>v.names</code> and <code>times</code> arguments based on the
names in <code>varying</code>.  If <code>sep == ""</code>, the split is just before
the first numeral that follows an alphabetic character.  This is
also used to create variable names when reshaping to wide format.</p>
</td></tr>
<tr><td><code id="reshape_+3A_split">split</code></td>
<td>
<p>A list with three components, <code>regexp</code>,
<code>include</code>, and (optionally) <code>fixed</code>.  This allows an
extended interface to variable name splitting.  See &lsquo;Details&rsquo;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Although <code>reshape()</code> can be used in a variety of contexts, the
motivating application is data from longitudinal studies, and the
arguments of this function are named and described in those terms. A
longitudinal study is characterized by repeated measurements of the
same variable(s), e.g., height and weight, on each unit being studied
(e.g., individual persons) at different time points (which are assumed
to be the same for all units). These variables are called time-varying
variables. The study may include other variables that are measured
only once for each unit and do not vary with time (e.g., gender and
race); these are called time-constant variables.
</p>
<p>A &lsquo;wide&rsquo; format representation of a longitudinal dataset will
have one record (row) for each unit, typically with some time-constant
variables that occupy single columns, and some time-varying variables
that occupy multiple columns (one column for each time point).  A
&lsquo;long&rsquo; format representation of the same dataset will have
multiple records (rows) for each individual, with the time-constant
variables being constant across these records and the time-varying
variables varying across the records.  The &lsquo;long&rsquo; format
dataset will have two additional variables: a &lsquo;time&rsquo; variable
identifying which time point each record comes from, and an
&lsquo;id&rsquo; variable showing which records refer to the same unit.
</p>
<p>The type of conversion (long to wide or wide to long) is determined by
the <code>direction</code> argument, which is mandatory unless the
<code>data</code> argument is the result of a previous call to
<code>reshape</code>.  In that case, the operation can be reversed simply
using <code>reshape(data)</code> (the other arguments are stored as
attributes on the data frame).
</p>
<p>Conversion from long to wide format with <code>direction = "wide"</code> is
the simpler operation, and is mainly useful in the context of
multivariate analysis where data is often expected as a wide-format
matrix. In this case, the time variable <code>timevar</code> and id variable
<code>idvar</code> must be specified. All other variables are assumed to be
time-varying, unless the time-varying variables are explicitly
specified via the <code>v.names</code> argument.  A warning is issued if
time-constant variables are not actually constant.
</p>
<p>Each time-varying variable is expanded into multiple variables in the
wide format.  The names of these expanded variables are generated
automatically, unless they are specified as the <code>varying</code>
argument in the form of a list (or matrix) with one component (or row)
for each time-varying variable. If <code>varying</code> is a vector of
names, it is implicitly converted into a matrix, with one row for each
time-varying variable. Use this option with care if there are multiple
time-varying variables, as the ordering (by column, the default in the
<code><a href="base.html#topic+matrix">matrix</a></code> constructor) may be unintuitive, whereas the
explicit list or matrix form is unambiguous.
</p>



<p>Conversion from wide to long with <code>direction = "long"</code> is the
more common operation as most (univariate) statistical modeling
functions expect data in the long format. In the simpler case where
there is only one time-varying variable, the corresponding columns in
the wide format input can be specified as the <code>varying</code> argument,
which can be either a vector of column names or the corresponding
column indices. The name of the corresponding variable in the long
format output combining these columns can be optionally specified as
the <code>v.names</code> argument, and the name of the time variables as the
<code>timevar</code> argument. The values to use as the time values
corresponding to the different columns in the wide format can be
specified as the <code>times</code> argument.  If <code>v.names</code> is
unspecified, the function will attempt to guess <code>v.names</code> and
<code>times</code> from <code>varying</code> (an explicitly specified <code>times</code>
argument is unused in that case).  The default expects variable names
like <code>x.1</code>, <code>x.2</code>, where <code>sep = "."</code>  specifies to
split at the dot and drop it from the name.  To have alphabetic
followed by numeric times use <code>sep = ""</code>.
</p>
<p>Multiple time-varying variables can be specified in two ways, either
with <code>varying</code> as an atomic vector as above, or as a list (or a
matrix). The first form is useful (and mandatory) if the automatic
variable name splitting as described above is used; this requires the
names of all time-varying variables to be suitably formatted in the
same manner, and <code>v.names</code> to be unspecified. If <code>varying</code>
is a list (with one component for each time-varying variable) or a
matrix (one row for each time-varying variable), variable name
splitting is not attempted, and <code>v.names</code> and <code>times</code> will
generally need to be specified, although they will default to,
respectively, the first variable name in each set, and sequential
times.
</p>
<p>Also, guessing is not attempted if <code>v.names</code> is given explicitly,
even if <code>varying</code> is an atomic vector. In that case, the number
of time-varying variables is taken to be the length of <code>v.names</code>,
and <code>varying</code> is implicitly converted into a matrix, with one row
for each time-varying variable. As in the case of long to wide
conversion, the matrix is filled up by column, so careful attention needs
to be paid to the order of variable names (or indices) in
<code>varying</code>, which is taken to be like <code>x.1</code>, <code>y.1</code>,
<code>x.2</code>, <code>y.2</code> (i.e., variables corresponding to the same time
point need to be grouped together).
</p>
<p>The <code>split</code> argument should not usually be necessary.  The
<code>split$regexp</code> component is passed to either
<code><a href="base.html#topic+strsplit">strsplit</a></code> or <code><a href="base.html#topic+regexpr">regexpr</a></code>, where the latter is
used if <code>split$include</code> is <code>TRUE</code>, in which case the
splitting occurs after the first character of the matched string.  In
the <code><a href="base.html#topic+strsplit">strsplit</a></code> case, the separator is not included in the
result, and it is possible to specify fixed-string matching using
<code>split$fixed</code>.
</p>


<h3>Value</h3>

<p>The reshaped data frame with added attributes to simplify reshaping
back to the original form.
</p>


<h3>See Also</h3>

<p><code><a href="utils.html#topic+stack">stack</a></code>, <code><a href="base.html#topic+aperm">aperm</a></code>;
<code><a href="utils.html#topic+relist">relist</a></code> for reshaping the result of
<code><a href="base.html#topic+unlist">unlist</a></code>. <code><a href="#topic+xtabs">xtabs</a></code> and
<code><a href="base.html#topic+as.data.frame.table">as.data.frame.table</a></code> for creating contingency tables and
converting them back to data frames.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>summary(Indometh) # data in long format

## long to wide (direction = "wide") requires idvar and timevar at a minimum
reshape(Indometh, direction = "wide", idvar = "Subject", timevar = "time")

## can also explicitly specify name of combined variable
wide &lt;- reshape(Indometh, direction = "wide", idvar = "Subject",
                timevar = "time", v.names = "conc", sep= "_")
wide

## reverse transformation
reshape(wide, direction = "long")
reshape(wide, idvar = "Subject", varying = list(2:12),
        v.names = "conc", direction = "long")

## times need not be numeric
df &lt;- data.frame(id = rep(1:4, rep(2,4)),
                 visit = I(rep(c("Before","After"), 4)),
                 x = rnorm(4), y = runif(4))
df
reshape(df, timevar = "visit", idvar = "id", direction = "wide")
## warns that y is really varying
reshape(df, timevar = "visit", idvar = "id", direction = "wide", v.names = "x")


##  unbalanced 'long' data leads to NA fill in 'wide' form
df2 &lt;- df[1:7, ]
df2
reshape(df2, timevar = "visit", idvar = "id", direction = "wide")

## Alternative regular expressions for guessing names
df3 &lt;- data.frame(id = 1:4, age = c(40,50,60,50), dose1 = c(1,2,1,2),
                  dose2 = c(2,1,2,1), dose4 = c(3,3,3,3))
reshape(df3, direction = "long", varying = 3:5, sep = "")


## an example that isn't longitudinal data
state.x77 &lt;- as.data.frame(state.x77)
long &lt;- reshape(state.x77, idvar = "state", ids = row.names(state.x77),
                times = names(state.x77), timevar = "Characteristic",
                varying = list(names(state.x77)), direction = "long")

reshape(long, direction = "wide")

reshape(long, direction = "wide", new.row.names = unique(long$state))

## multiple id variables
df3 &lt;- data.frame(school = rep(1:3, each = 4), class = rep(9:10, 6),
                  time = rep(c(1,1,2,2), 3), score = rnorm(12))
wide &lt;- reshape(df3, idvar = c("school", "class"), direction = "wide")
wide
## transform back
reshape(wide)

</code></pre>

<hr>
<h2 id='residuals'>Extract Model Residuals</h2><span id='topic+residuals'></span><span id='topic+resid'></span><span id='topic+residuals.default'></span>

<h3>Description</h3>

<p><code>residuals</code> is a generic function which extracts model residuals
from objects returned by modeling functions.
</p>
<p><code>resid</code> is an alias for <code>residuals</code>,
abbreviated to encourage users to access object components through
an accessor function rather than by directly referencing an object
slot.
</p>
<p>All object classes which are returned by model fitting functions
should provide a <code>residuals</code> method.  (Note that the method is
for &lsquo;<span class="samp">&#8288;residuals&#8288;</span>&rsquo; and not &lsquo;<span class="samp">&#8288;resid&#8288;</span>&rsquo;.)
</p>
<p>Methods can make use of <code><a href="#topic+naresid">naresid</a></code> methods to compensate
for the omission of missing values.  The default, <code><a href="#topic+nls">nls</a></code> and
<code><a href="#topic+smooth.spline">smooth.spline</a></code> methods do.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>residuals(object, ...)
resid(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals_+3A_object">object</code></td>
<td>
<p>an object for which the extraction of model residuals is
meaningful.</p>
</td></tr>
<tr><td><code id="residuals_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Residuals extracted from the object <code>object</code>.
</p>


<h3>References</h3>

<p>Chambers, J. M. and Hastie, T. J. (1992)
<em>Statistical Models in S</em>.
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coefficients">coefficients</a></code>, <code><a href="#topic+fitted.values">fitted.values</a></code>,
<code><a href="#topic+glm">glm</a></code>, <code><a href="#topic+lm">lm</a></code>.
</p>
<p><a href="#topic+influence.measures">influence.measures</a> for standardized (<code><a href="#topic+rstandard">rstandard</a></code>)
and studentized (<code><a href="#topic+rstudent">rstudent</a></code>) residuals.
</p>

<hr>
<h2 id='runmed'>Running Medians &ndash; Robust Scatter Plot Smoothing</h2><span id='topic+runmed'></span>

<h3>Description</h3>

<p>Compute running medians of odd span.  This is the &lsquo;most robust&rsquo;
scatter plot smoothing possible.  For efficiency (and historical
reason), you can use one of two different algorithms giving identical
results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>runmed(x, k, endrule = c("median", "keep", "constant"),
       algorithm = NULL,
       na.action = c("+Big_alternate", "-Big_alternate", "na.omit", "fail"),
       print.level = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="runmed_+3A_x">x</code></td>
<td>
<p>numeric vector, the &lsquo;dependent&rsquo; variable to be
smoothed.</p>
</td></tr>
<tr><td><code id="runmed_+3A_k">k</code></td>
<td>
<p>integer width of median window; must be odd.  Turlach had a
default of <code>k &lt;- 1 + 2 * min((n-1)%/% 2, ceiling(0.1*n))</code>.
Use <code>k = 3</code> for &lsquo;minimal&rsquo; robust smoothing eliminating
isolated outliers.</p>
</td></tr>
<tr><td><code id="runmed_+3A_endrule">endrule</code></td>
<td>
<p>character string indicating how the values at the
beginning and the end (of the data) should be treated.
Can be abbreviated.  Possible values are:
</p>

<dl>
<dt><code>"keep"</code></dt><dd><p>keeps the first and last <code class="reqn">k_2</code> values
at both ends, where <code class="reqn">k_2</code> is the half-bandwidth
<code>k2 = k %/% 2</code>,
i.e., <code>y[j] = x[j]</code> for <code class="reqn">j \in \{1,\ldots,k_2;
          n-k_2+1,\ldots,n\}</code>;</p>
</dd>
<dt><code>"constant"</code></dt><dd><p>copies <code>median(y[1:k2])</code> to the first
values and analogously for the last ones making the smoothed ends
<em>constant</em>;</p>
</dd>
<dt><code>"median"</code></dt><dd><p>the default, smooths the ends by using
symmetrical medians of subsequently smaller bandwidth, but for
the very first and last value where Tukey's robust end-point
rule is applied, see <code><a href="#topic+smoothEnds">smoothEnds</a></code>.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="runmed_+3A_algorithm">algorithm</code></td>
<td>
<p>character string (partially matching <code>"Turlach"</code> or
<code>"Stuetzle"</code>) or the default <code>NULL</code>, specifying which algorithm
should be applied.  The default choice depends on <code>n = length(x)</code>
and <code>k</code> where <code>"Turlach"</code> will be used for larger problems.</p>
</td></tr>
<tr><td><code id="runmed_+3A_na.action">na.action</code></td>
<td>
<p>character string determining the behavior in the case of
<code><a href="base.html#topic+NA">NA</a></code> or <code><a href="base.html#topic+NaN">NaN</a></code> in <code>x</code>, (partially matching)
one of
</p>

<dl>
<dt><code>"+Big_alternate"</code></dt><dd><p>Here, all the NAs in <code>x</code> are
first replaced by alternating <code class="reqn">\pm B</code> where <code class="reqn">B</code> is a
&ldquo;Big&rdquo; number (with <code class="reqn">2B &lt; M*</code>, where
<code class="reqn">M*=</code><code><a href="base.html#topic+.Machine">.Machine</a> $ double.xmax</code>).  The replacement
values are &ldquo;from left&rdquo; <code class="reqn">(+B, -B, +B, \ldots)</code>,
i.e. start with <code>"+"</code>.</p>
</dd>
<dt><code>"-Big_alternate"</code></dt><dd><p>almost the same as
<code>"+Big_alternate"</code>, just starting with <code class="reqn">-B</code> (<code>"-Big..."</code>).</p>
</dd>
<dt><code>"na.omit"</code></dt><dd><p>the result is the same as
<code>runmed(x[!is.na(x)], k, ..)</code>.</p>
</dd>
<dt><code>"fail"</code></dt><dd><p>the presence of NAs in <code>x</code> will raise an error.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="runmed_+3A_print.level">print.level</code></td>
<td>
<p>integer, indicating verboseness of algorithm;
should rarely be changed by average users.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Apart from the end values, the result <code>y = runmed(x, k)</code> simply has
<code>y[j] = median(x[(j-k2):(j+k2)])</code> (<code>k = 2*k2+1</code>), computed very
efficiently.
</p>
<p>The two algorithms are internally entirely different:
</p>

<dl>
<dt><code>"Turlach"</code></dt><dd><p>is the Hrdle&ndash;Steiger
algorithm (see Ref.) as implemented by Berwin Turlach.
A tree algorithm is used, ensuring performance <code class="reqn">O(n \log
        k)</code> where <code>n = length(x)</code> which is
asymptotically optimal.</p>
</dd>
<dt><code>"Stuetzle"</code></dt><dd><p>is the (older) Stuetzle&ndash;Friedman implementation
which makes use of median <em>updating</em> when one observation
enters and one leaves the smoothing window.  While this performs as
<code class="reqn">O(n \times k)</code> which is slower asymptotically, it is
considerably faster for small <code class="reqn">k</code> or <code class="reqn">n</code>.</p>
</dd>
</dl>

<p>Note that, both algorithms (and the <code><a href="#topic+smoothEnds">smoothEnds</a>()</code> utility)
now &ldquo;work&rdquo; also when <code>x</code> contains non-finite entries
(<code class="reqn">\pm</code><code><a href="base.html#topic+Inf">Inf</a></code>, <code><a href="base.html#topic+NaN">NaN</a></code>, and
<code><a href="base.html#topic+NA">NA</a></code>):
</p>

<dl>
<dt><code>"Turlach"</code></dt><dd><p>.......
</p>
</dd>
<dt><code>"Stuetzle"</code></dt><dd><p>currently simply works by applying the
underlying math library (&lsquo;<span class="file">libm</span>&rsquo;) arithmetic for the non-finite
numbers; this may optionally change in the future.
</p>
</dd>
</dl>

<p>Currently <a href="base.html#topic+long+20vectors">long vectors</a> are only supported for <code>algorithm = "Stuetzle"</code>.
</p>


<h3>Value</h3>

<p>vector of smoothed values of the same length as <code>x</code> with an
<code><a href="base.html#topic+attr">attr</a></code>ibute <code>k</code> containing (the &lsquo;oddified&rsquo;)
<code>k</code>.
</p>


<h3>Author(s)</h3>

<p>Martin Maechler <a href="mailto:maechler@stat.math.ethz.ch">maechler@stat.math.ethz.ch</a>,
based on Fortran code from Werner Stuetzle and S-PLUS and C code from
Berwin Turlach.
</p>


<h3>References</h3>

<p>Hrdle, W. and Steiger, W. (1995)
Algorithm AS 296: Optimal median smoothing,
<em>Applied Statistics</em> <b>44</b>, 258&ndash;264.
<a href="https://doi.org/10.2307/2986349">doi:10.2307/2986349</a>.
</p>
<p>Jerome H. Friedman and Werner Stuetzle (1982)
<em>Smoothing of Scatterplots</em>;
Report, Dep. Statistics, Stanford U., Project Orion 003.




</p>


<h3>See Also</h3>

<p><code><a href="#topic+smoothEnds">smoothEnds</a></code> which implements Tukey's end point rule and
is called by default from <code>runmed(*, endrule = "median")</code>.
<code><a href="#topic+smooth">smooth</a></code> uses running
medians of 3 for its compound smoothers.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

utils::example(nhtemp)
myNHT &lt;- as.vector(nhtemp)
myNHT[20] &lt;- 2 * nhtemp[20]
plot(myNHT, type = "b", ylim = c(48, 60), main = "Running Medians Example")
lines(runmed(myNHT, 7), col = "red")

## special: multiple y values for one x
plot(cars, main = "'cars' data and runmed(dist, 3)")
lines(cars, col = "light gray", type = "c")
with(cars, lines(speed, runmed(dist, k = 3), col = 2))


## nice quadratic with a few outliers
y &lt;- ys &lt;- (-20:20)^2
y [c(1,10,21,41)] &lt;- c(150, 30, 400, 450)
all(y == runmed(y, 1)) # 1-neighbourhood &lt;==&gt; interpolation
plot(y) ## lines(y, lwd = .1, col = "light gray")
lines(lowess(seq(y), y, f = 0.3), col = "brown")
lines(runmed(y, 7), lwd = 2, col = "blue")
lines(runmed(y, 11), lwd = 2, col = "red")

## Lowess is not robust
y &lt;- ys ; y[21] &lt;- 6666 ; x &lt;- seq(y)
col &lt;- c("black", "brown","blue")
plot(y, col = col[1])
lines(lowess(x, y, f = 0.3), col = col[2])


lines(runmed(y, 7),      lwd = 2, col = col[3])
legend(length(y),max(y), c("data", "lowess(y, f = 0.3)", "runmed(y, 7)"),
       xjust = 1, col = col, lty = c(0, 1, 1), pch = c(1,NA,NA))

## An example with initial NA's - used to fail badly (notably for "Turlach"):
x15 &lt;- c(rep(NA, 4), c(9, 9, 4, 22, 6, 1, 7, 5, 2, 8, 3))
rS15 &lt;- cbind(Sk.3 = runmed(x15, k = 3, algorithm="S"),
              Sk.7 = runmed(x15, k = 7, algorithm="S"),
              Sk.11= runmed(x15, k =11, algorithm="S"))
rT15 &lt;- cbind(Tk.3 = runmed(x15, k = 3, algorithm="T", print.level=1),
              Tk.7 = runmed(x15, k = 7, algorithm="T", print.level=1),
              Tk.9 = runmed(x15, k = 9, algorithm="T", print.level=1),
              Tk.11= runmed(x15, k =11, algorithm="T", print.level=1))
cbind(x15, rS15, rT15) # result for k=11  maybe a bit surprising ..
Tv &lt;- rT15[-(1:3),]
stopifnot(3 &lt;= Tv, Tv &lt;= 9, 5 &lt;= Tv[1:10,])
matplot(y = cbind(x15, rT15), type = "b", ylim = c(1,9), pch=1:5, xlab = NA,
        main = "runmed(x15, k, algo = \"Turlach\")")
mtext(paste("x15 &lt;-", deparse(x15)))
points(x15, cex=2)
legend("bottomleft", legend=c("data", paste("k = ", c(3,7,9,11))),
       bty="n", col=1:5, lty=1:5, pch=1:5)
</code></pre>

<hr>
<h2 id='rWishart'>Random Wishart Distributed Matrices</h2><span id='topic+rWishart'></span>

<h3>Description</h3>

<p>Generate <code>n</code> random matrices, distributed according to the
Wishart distribution with parameters <code>Sigma</code> and <code>df</code>,
<code class="reqn">W_p(\Sigma, m),\ m=\code{df},\ \Sigma=\code{Sigma}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rWishart(n, df, Sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rWishart_+3A_n">n</code></td>
<td>
<p>integer sample size.</p>
</td></tr>
<tr><td><code id="rWishart_+3A_df">df</code></td>
<td>
<p>numeric parameter, &ldquo;degrees of freedom&rdquo;.</p>
</td></tr>
<tr><td><code id="rWishart_+3A_sigma">Sigma</code></td>
<td>
<p>positive definite (<code class="reqn">p\times p</code>) &ldquo;scale&rdquo;
matrix, the matrix parameter of the distribution.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code class="reqn">X_1,\dots, X_m, \ X_i\in\mathbf{R}^p</code> is
a sample of <code class="reqn">m</code> independent multivariate Gaussians with mean (vector) 0, and
covariance matrix <code class="reqn">\Sigma</code>, the distribution of
<code class="reqn">M = X'X</code> is <code class="reqn">W_p(\Sigma, m)</code>.
</p>
<p>Consequently, the expectation of <code class="reqn">M</code> is
</p>
<p style="text-align: center;"><code class="reqn">E[M] = m\times\Sigma.</code>
</p>

<p>Further, if <code>Sigma</code> is scalar (<code class="reqn">p = 1</code>), the Wishart
distribution is a scaled chi-squared (<code class="reqn">\chi^2</code>)
distribution with <code>df</code> degrees of freedom,
<code class="reqn">W_1(\sigma^2, m) = \sigma^2 \chi^2_m</code>.
</p>
<p>The component wise variance is
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{Var}(M_{ij}) = m(\Sigma_{ij}^2 + \Sigma_{ii} \Sigma_{jj}).</code>
</p>



<h3>Value</h3>

<p>a numeric <code><a href="base.html#topic+array">array</a></code>, say <code>R</code>, of dimension
<code class="reqn">p \times p \times n</code>, where each <code>R[,,i]</code> is a
positive definite matrix, a realization of the Wishart distribution
<code class="reqn">W_p(\Sigma, m),\ \ m=\code{df},\ \Sigma=\code{Sigma}</code>.
</p>


<h3>Author(s)</h3>

<p>Douglas Bates</p>


<h3>References</h3>

<p>Mardia, K. V., J. T. Kent, and J. M. Bibby (1979)
<em>Multivariate Analysis</em>, London: Academic Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cov">cov</a></code>, <code><a href="#topic+rnorm">rnorm</a></code>, <code><a href="#topic+rchisq">rchisq</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Artificial
S &lt;- toeplitz((10:1)/10)
set.seed(11)
R &lt;- rWishart(1000, 20, S)
dim(R)  #  10 10  1000
mR &lt;- apply(R, 1:2, mean)  # ~= E[ Wish(S, 20) ] = 20 * S
stopifnot(all.equal(mR, 20*S, tolerance = .009))

## See Details, the variance is
Va &lt;- 20*(S^2 + tcrossprod(diag(S)))
vR &lt;- apply(R, 1:2, var)
stopifnot(all.equal(vR, Va, tolerance = 1/16))
</code></pre>

<hr>
<h2 id='scatter.smooth'>Scatter Plot with Smooth Curve Fitted by <code>loess</code></h2><span id='topic+scatter.smooth'></span><span id='topic+loess.smooth'></span>

<h3>Description</h3>

<p>Plot and add a smooth curve computed by <code>loess</code> to a scatter plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scatter.smooth(x, y = NULL, span = 2/3, degree = 1,
    family = c("symmetric", "gaussian"),
    xlab = NULL, ylab = NULL,
    ylim = range(y, pred$y, na.rm = TRUE),
    evaluation = 50, ..., lpars = list())

loess.smooth(x, y, span = 2/3, degree = 1,
    family = c("symmetric", "gaussian"), evaluation = 50, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scatter.smooth_+3A_x">x</code>, <code id="scatter.smooth_+3A_y">y</code></td>
<td>
<p>the <code>x</code> and <code>y</code> arguments provide the x and y
coordinates for the plot.  Any reasonable way of defining the
coordinates is acceptable.  See the function <code><a href="grDevices.html#topic+xy.coords">xy.coords</a></code>
for details.</p>
</td></tr>
<tr><td><code id="scatter.smooth_+3A_span">span</code></td>
<td>
<p>smoothness parameter for <code>loess</code>.</p>
</td></tr>
<tr><td><code id="scatter.smooth_+3A_degree">degree</code></td>
<td>
<p>degree of local polynomial used.</p>
</td></tr>
<tr><td><code id="scatter.smooth_+3A_family">family</code></td>
<td>
<p>if <code>"gaussian"</code> fitting is by least-squares, and if
<code>family = "symmetric"</code> a re-descending M estimator is used.
Can be abbreviated.</p>
</td></tr>
<tr><td><code id="scatter.smooth_+3A_xlab">xlab</code></td>
<td>
<p>label for x axis.</p>
</td></tr>
<tr><td><code id="scatter.smooth_+3A_ylab">ylab</code></td>
<td>
<p>label for y axis.</p>
</td></tr>
<tr><td><code id="scatter.smooth_+3A_ylim">ylim</code></td>
<td>
<p>the y limits of the plot.</p>
</td></tr>
<tr><td><code id="scatter.smooth_+3A_evaluation">evaluation</code></td>
<td>
<p>number of points at which to evaluate the smooth
curve.</p>
</td></tr>
<tr><td><code id="scatter.smooth_+3A_...">...</code></td>
<td>
<p>For <code>scatter.smooth()</code>, graphical parameters, passed
to <code>plot()</code> only.  For <code>loess.smooth</code>, control parameters
passed to <code><a href="#topic+loess.control">loess.control</a></code>.</p>
</td></tr>
<tr><td><code id="scatter.smooth_+3A_lpars">lpars</code></td>
<td>
<p>a <code><a href="base.html#topic+list">list</a></code> of arguments to be passed to
<code><a href="graphics.html#topic+lines">lines</a>()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>loess.smooth</code> is an auxiliary function which evaluates the
<code>loess</code> smooth at <code>evaluation</code> equally spaced points
covering the range of <code>x</code>.
</p>


<h3>Value</h3>

<p>For <code>scatter.smooth</code>, none.
</p>
<p>For <code>loess.smooth</code>, a list with two components, <code>x</code> (the
grid of evaluation points) and <code>y</code> (the smoothed values at the
grid points).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+loess">loess</a></code>; <code><a href="graphics.html#topic+smoothScatter">smoothScatter</a></code> for scatter plots
with smoothed <em>density</em> color representation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

with(cars, scatter.smooth(speed, dist))
## or with dotted thick smoothed line results :
with(cars, scatter.smooth(speed, dist, lpars =
                    list(col = "red", lwd = 3, lty = 3)))
</code></pre>

<hr>
<h2 id='screeplot'>Scree Plots</h2><span id='topic+screeplot'></span><span id='topic+screeplot.default'></span>

<h3>Description</h3>

<p><code>screeplot.default</code> plots the variances against the number of the
principal component. This is also the <code>plot</code> method for classes
<code>"princomp"</code> and <code>"prcomp"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>screeplot(x, ...)
## Default S3 method:
screeplot(x, npcs = min(10, length(x$sdev)),
          type = c("barplot", "lines"),
          main = deparse1(substitute(x)), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="screeplot_+3A_x">x</code></td>
<td>
<p>an object containing a <code>sdev</code> component, such as that
returned by <code><a href="#topic+princomp">princomp</a>()</code> and <code><a href="#topic+prcomp">prcomp</a>()</code>.</p>
</td></tr>
<tr><td><code id="screeplot_+3A_npcs">npcs</code></td>
<td>
<p>the number of components to be plotted.</p>
</td></tr>
<tr><td><code id="screeplot_+3A_type">type</code></td>
<td>
<p>the type of plot.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="screeplot_+3A_main">main</code>, <code id="screeplot_+3A_...">...</code></td>
<td>
<p>graphics parameters.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Mardia, K. V., J. T. Kent and J. M. Bibby (1979).
<em>Multivariate Analysis</em>, London: Academic Press.
</p>
<p>Venables, W. N. and B. D. Ripley (2002).
<em>Modern Applied Statistics with S</em>, Springer-Verlag.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+princomp">princomp</a></code> and <code><a href="#topic+prcomp">prcomp</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

## The variances of the variables in the
## USArrests data vary by orders of magnitude, so scaling is appropriate
(pc.cr &lt;- princomp(USArrests, cor = TRUE))  # inappropriate
screeplot(pc.cr)

fit &lt;- princomp(covmat = Harman74.cor)
screeplot(fit)
screeplot(fit, npcs = 24, type = "lines")
</code></pre>

<hr>
<h2 id='sd'>Standard Deviation</h2><span id='topic+sd'></span>

<h3>Description</h3>

<p>This function computes the standard deviation of the values in
<code>x</code>.
If <code>na.rm</code> is <code>TRUE</code> then missing values are removed before
computation proceeds.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sd(x, na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sd_+3A_x">x</code></td>
<td>
<p>a numeric vector or an <span class="rlang"><b>R</b></span> object but not a
<code><a href="base.html#topic+factor">factor</a></code> coercible to numeric by <code>as.double(x)</code>.</p>
</td></tr>
<tr><td><code id="sd_+3A_na.rm">na.rm</code></td>
<td>
<p>logical.  Should missing values be removed?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Like <code><a href="#topic+var">var</a></code> this uses denominator <code class="reqn">n - 1</code>.
</p>
<p>The standard deviation of a length-one or zero-length vector is <code>NA</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+var">var</a></code> for its square, and <code><a href="#topic+mad">mad</a></code>, the most
robust alternative.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sd(1:2) ^ 2
</code></pre>

<hr>
<h2 id='se.contrast'>Standard Errors for Contrasts in Model Terms</h2><span id='topic+se.contrast'></span><span id='topic+se.contrast.aov'></span><span id='topic+se.contrast.aovlist'></span>

<h3>Description</h3>

<p>Returns the standard errors for one or more contrasts in an <code>aov</code>
object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>se.contrast(object, ...)
## S3 method for class 'aov'
se.contrast(object, contrast.obj,
           coef = contr.helmert(ncol(contrast))[, 1],
           data = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="se.contrast_+3A_object">object</code></td>
<td>
<p>A suitable fit, usually from <code>aov</code>.</p>
</td></tr>
<tr><td><code id="se.contrast_+3A_contrast.obj">contrast.obj</code></td>
<td>
<p>The contrasts for which standard errors are
requested.  This can be specified via a list or via a matrix.  A
single contrast can be specified by a list of logical vectors giving
the cells to be contrasted.  Multiple contrasts should be specified
by a matrix, each column of which is a numerical contrast vector
(summing to zero).
</p>
</td></tr>
<tr><td><code id="se.contrast_+3A_coef">coef</code></td>
<td>
<p>used when <code>contrast.obj</code> is a list; it should be a
vector of the same length as the list with zero sum.  The default
value is the first Helmert contrast, which contrasts the first and
second cell means specified by the list.</p>
</td></tr>
<tr><td><code id="se.contrast_+3A_data">data</code></td>
<td>
<p>The data frame used to evaluate <code>contrast.obj</code>.</p>
</td></tr>
<tr><td><code id="se.contrast_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Contrasts are usually used to test if certain means are
significantly different; it can be easier to use <code>se.contrast</code>
than compute them directly from the coefficients.
</p>
<p>In multistratum models, the contrasts can appear in more than one
stratum, in which case the standard errors are computed in the lowest
stratum and adjusted for efficiencies and comparisons between
strata. (See the comments in the note in the help for
<code><a href="#topic+aov">aov</a></code> about using orthogonal contrasts.)  Such standard
errors are often conservative.
</p>
<p>Suitable matrices for use with <code>coef</code> can be found by
calling <code><a href="#topic+contrasts">contrasts</a></code> and indexing the columns by a factor.
</p>


<h3>Value</h3>

<p>A vector giving the standard errors for each contrast.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+contrasts">contrasts</a></code>, <code><a href="#topic+model.tables">model.tables</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## From Venables and Ripley (2002) p.165.
N &lt;- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
P &lt;- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
K &lt;- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
yield &lt;- c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,62.8,55.8,69.5,
55.0, 62.0,48.8,45.5,44.2,52.0,51.5,49.8,48.8,57.2,59.0,53.2,56.0)

npk &lt;- data.frame(block = gl(6,4), N = factor(N), P = factor(P),
                  K = factor(K), yield = yield)
## Set suitable contrasts.
options(contrasts = c("contr.helmert", "contr.poly"))
npk.aov1 &lt;- aov(yield ~ block + N + K, data = npk)
se.contrast(npk.aov1, list(N == "0", N == "1"), data = npk)
# or via a matrix
cont &lt;- matrix(c(-1,1), 2, 1, dimnames = list(NULL, "N"))
se.contrast(npk.aov1, cont[N, , drop = FALSE]/12, data = npk)

## test a multi-stratum model
npk.aov2 &lt;- aov(yield ~ N + K + Error(block/(N + K)), data = npk)
se.contrast(npk.aov2, list(N == "0", N == "1"))


## an example looking at an interaction contrast
## Dataset from R.E. Kirk (1995)
## 'Experimental Design: procedures for the behavioral sciences'
score &lt;- c(12, 8,10, 6, 8, 4,10,12, 8, 6,10,14, 9, 7, 9, 5,11,12,
            7,13, 9, 9, 5,11, 8, 7, 3, 8,12,10,13,14,19, 9,16,14)
A &lt;- gl(2, 18, labels = c("a1", "a2"))
B &lt;- rep(gl(3, 6, labels = c("b1", "b2", "b3")), 2)
fit &lt;- aov(score ~ A*B)
cont &lt;- c(1, -1)[A] * c(1, -1, 0)[B]
sum(cont)       # 0
sum(cont*score) # value of the contrast
se.contrast(fit, as.matrix(cont))
(t.stat &lt;- sum(cont*score)/se.contrast(fit, as.matrix(cont)))
summary(fit, split = list(B = 1:2), expand.split = TRUE)
## t.stat^2 is the F value on the A:B: C1 line (with Helmert contrasts)
## Now look at all three interaction contrasts
cont &lt;- c(1, -1)[A] * cbind(c(1, -1, 0), c(1, 0, -1), c(0, 1, -1))[B,]
se.contrast(fit, cont)  # same, due to balance.
rm(A, B, score)


## multi-stratum example where efficiencies play a role
## An example from Yates (1932),
## a 2^3 design in 2 blocks replicated 4 times

Block &lt;- gl(8, 4)
A &lt;- factor(c(0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,
              0,1,0,1,0,1,0,1,0,1,0,1))
B &lt;- factor(c(0,0,1,1,0,0,1,1,0,1,0,1,1,0,1,0,0,0,1,1,
              0,0,1,1,0,0,1,1,0,0,1,1))
C &lt;- factor(c(0,1,1,0,1,0,0,1,0,0,1,1,0,0,1,1,0,1,0,1,
              1,0,1,0,0,0,1,1,1,1,0,0))
Yield &lt;- c(101, 373, 398, 291, 312, 106, 265, 450, 106, 306, 324, 449,
           272, 89, 407, 338, 87, 324, 279, 471, 323, 128, 423, 334,
           131, 103, 445, 437, 324, 361, 302, 272)
aovdat &lt;- data.frame(Block, A, B, C, Yield)
fit &lt;- aov(Yield ~ A + B * C + Error(Block), data = aovdat)
cont1 &lt;- c(-1, 1)[A]/32  # Helmert contrasts
cont2 &lt;- c(-1, 1)[B] * c(-1, 1)[C]/32
cont &lt;- cbind(A = cont1, BC = cont2)
colSums(cont*Yield) # values of the contrasts
se.contrast(fit, as.matrix(cont))
# comparison with lme
library(nlme)
fit2 &lt;- lme(Yield ~ A + B*C, random = ~1 | Block, data = aovdat)
summary(fit2)$tTable # same estimates, similar (but smaller) se's.
</code></pre>

<hr>
<h2 id='selfStart'>Construct Self-starting Nonlinear Models</h2><span id='topic+selfStart'></span><span id='topic+selfStart.default'></span><span id='topic+selfStart.formula'></span>

<h3>Description</h3>

<p>Construct self-starting nonlinear models to be used in
<code><a href="#topic+nls">nls</a></code>, etc.  Via function <code>initial</code> to compute
approximate parameter values from data, such models are
&ldquo;self-starting&rdquo;, i.e., do not need a <code>start</code> argument in,
e.g., <code><a href="#topic+nls">nls</a>()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selfStart(model, initial, parameters, template)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selfStart_+3A_model">model</code></td>
<td>
<p>a function object defining a nonlinear model or
a nonlinear <code><a href="#topic+formula">formula</a></code> object of the form <code>~ expression</code>.</p>
</td></tr>
<tr><td><code id="selfStart_+3A_initial">initial</code></td>
<td>
<p>a function object, taking arguments <code>mCall</code>,
<code>data</code>, and <code>LHS</code>, <em>and</em> <code>...</code>, representing,
respectively, a matched call to the function <code>model</code>, a data frame in
which to interpret the variables in <code>mCall</code>, and the expression
from the left-hand side of the model formula in the call to <code>nls</code>.
This function should return initial values for the parameters in
<code>model</code>.  The <code>...</code> is used by <code><a href="#topic+nls">nls</a>()</code> to pass its
<code>control</code> and <code>trace</code> arguments for the cases where
<code>initial()</code> itself calls <code>nls()</code> as it does for the ten
self-starting nonlinear models in <span class="rlang"><b>R</b></span>'s <span class="pkg">stats</span> package.</p>
</td></tr>
<tr><td><code id="selfStart_+3A_parameters">parameters</code></td>
<td>
<p>a character vector specifying the terms on the right
hand side of <code>model</code> for which initial estimates should be
calculated.  Passed as the <code>namevec</code> argument to the
<code>deriv</code> function.</p>
</td></tr>
<tr><td><code id="selfStart_+3A_template">template</code></td>
<td>
<p>an optional prototype for the calling sequence of the
returned object, passed as the <code>function.arg</code> argument to the
<code>deriv</code> function.  By default, a template is generated with the
covariates in <code>model</code> coming first and the parameters in
<code>model</code> coming last in the calling sequence.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code><a href="#topic+nls">nls</a>()</code> calls <code><a href="#topic+getInitial">getInitial</a></code> and the
<code>initial</code> function for these self-starting models.
</p>
<p>This function is generic; methods functions can be written to handle
specific classes of objects.
</p>


<h3>Value</h3>

<p>a <code><a href="base.html#topic+function">function</a></code> object of class <code>"selfStart"</code>, for the
<code>formula</code> method obtained by applying <code><a href="#topic+deriv">deriv</a></code>
to the right hand side of the <code>model</code> formula.  An
<code>initial</code> attribute (defined by the <code>initial</code> argument) is
added to the function to calculate starting estimates for the
parameters in the model automatically.
</p>


<h3>Author(s)</h3>

<p>Jos Pinheiro and Douglas Bates</p>


<h3>See Also</h3>

<p><code><a href="#topic+nls">nls</a></code>, <code><a href="#topic+getInitial">getInitial</a></code>.
</p>
<p>Each of the following are <code>"selfStart"</code> models (with examples)
<code><a href="#topic+SSasymp">SSasymp</a></code>, <code><a href="#topic+SSasympOff">SSasympOff</a></code>, <code><a href="#topic+SSasympOrig">SSasympOrig</a></code>,
<code><a href="#topic+SSbiexp">SSbiexp</a></code>, <code><a href="#topic+SSfol">SSfol</a></code>, <code><a href="#topic+SSfpl">SSfpl</a></code>,
<code><a href="#topic+SSgompertz">SSgompertz</a></code>, <code><a href="#topic+SSlogis">SSlogis</a></code>, <code><a href="#topic+SSmicmen">SSmicmen</a></code>,
<code><a href="#topic+SSweibull">SSweibull</a></code>.
</p>
<p>Further, package <a href="https://CRAN.R-project.org/package=nlme"><span class="pkg">nlme</span></a>'s <code><a href="nlme.html#topic+nlsList">nlsList</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## self-starting logistic model

## The "initializer" (finds initial values for parameters from data):
initLogis &lt;- function(mCall, data, LHS, ...) {
    xy &lt;- sortedXyData(mCall[["x"]], LHS, data)
    if(nrow(xy) &lt; 4)
        stop("too few distinct input values to fit a logistic model")
    z &lt;- xy[["y"]]
    ## transform to proportion, i.e. in (0,1) :
    rng &lt;- range(z); dz &lt;- diff(rng)
    z &lt;- (z - rng[1L] + 0.05 * dz)/(1.1 * dz)
    xy[["z"]] &lt;- log(z/(1 - z))		# logit transformation
    aux &lt;- coef(lm(x ~ z, xy))
    pars &lt;- coef(nls(y ~ 1/(1 + exp((xmid - x)/scal)),
                     data = xy,
                     start = list(xmid = aux[[1L]], scal = aux[[2L]]),
                     algorithm = "plinear", ...))
    setNames(pars [c(".lin", "xmid", "scal")],
             mCall[c("Asym", "xmid", "scal")])
}

mySSlogis &lt;- selfStart(~ Asym/(1 + exp((xmid - x)/scal)),
                       initial = initLogis,
                       parameters = c("Asym", "xmid", "scal"))

getInitial(weight ~ mySSlogis(Time, Asym, xmid, scal),
           data = subset(ChickWeight, Chick == 1))


# 'first.order.log.model' is a function object defining a first order
# compartment model
# 'first.order.log.initial' is a function object which calculates initial
# values for the parameters in 'first.order.log.model'
#
# self-starting first order compartment model
## Not run: 
SSfol &lt;- selfStart(first.order.log.model, first.order.log.initial)

## End(Not run)

## Explore the self-starting models already available in R's  "stats":
pos.st &lt;- which("package:stats" == search())
mSS &lt;- apropos("^SS..", where = TRUE, ignore.case = FALSE)
(mSS &lt;- unname(mSS[names(mSS) == pos.st]))
fSS &lt;- sapply(mSS, get, pos = pos.st, mode = "function")
all(sapply(fSS, inherits, "selfStart"))  # -&gt; TRUE

## Show the argument list of each self-starting function:
str(fSS, give.attr = FALSE)
</code></pre>

<hr>
<h2 id='setNames'>Set the Names in an Object</h2><span id='topic+setNames'></span>

<h3>Description</h3>

<p>This is a convenience function that sets the names on an object and
returns the object.  It is most useful at the end of a function
definition where one is creating the object to be returned and would
prefer not to store it under a name just so the names can be assigned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setNames(object = nm, nm)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setNames_+3A_object">object</code></td>
<td>
<p>an object for which a <code>names</code> attribute will be meaningful </p>
</td></tr>
<tr><td><code id="setNames_+3A_nm">nm</code></td>
<td>
<p>a character vector of names to assign to the object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of the same sort as <code>object</code> with the new names assigned.
</p>


<h3>Author(s)</h3>

<p>Douglas M. Bates and Saikat DebRoy </p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+unname">unname</a></code> for removing names.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>setNames( 1:3, c("foo", "bar", "baz") )
# this is just a short form of
tmp &lt;- 1:3
names(tmp) &lt;-  c("foo", "bar", "baz")
tmp

## special case of character vector, using default
setNames(nm = c("First", "2nd"))
</code></pre>

<hr>
<h2 id='shapiro.test'>Shapiro-Wilk Normality Test</h2><span id='topic+shapiro.test'></span>

<h3>Description</h3>

<p>Performs the Shapiro-Wilk test of normality.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shapiro.test(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shapiro.test_+3A_x">x</code></td>
<td>
<p>a numeric vector of data values. Missing values are allowed,
but the number of non-missing values must be between 3 and 5000.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the Shapiro-Wilk statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>an approximate p-value for the test.  This is
said in Royston (1995) to be adequate for <code>p.value &lt; 0.1</code>.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the character string <code>"Shapiro-Wilk normality test"</code>.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the name(s) of the data.</p>
</td></tr>
</table>


<h3>Source</h3>

<p>The algorithm used is a C translation of the Fortran code described in
Royston (1995). 
The calculation of the p value is exact for <code class="reqn">n = 3</code>, otherwise
approximations are used, separately for <code class="reqn">4 \le n \le 11</code> and
<code class="reqn">n \ge 12</code>.
</p>


<h3>References</h3>

<p>Patrick Royston (1982).
An extension of Shapiro and Wilk's <code class="reqn">W</code> test for normality to large
samples.
<em>Applied Statistics</em>, <b>31</b>, 115&ndash;124.
<a href="https://doi.org/10.2307/2347973">doi:10.2307/2347973</a>.
</p>
<p>Patrick Royston (1982).
Algorithm AS 181: The <code class="reqn">W</code> test for Normality.
<em>Applied Statistics</em>, <b>31</b>, 176&ndash;180.
<a href="https://doi.org/10.2307/2347986">doi:10.2307/2347986</a>.
</p>
<p>Patrick Royston (1995).
Remark AS R94: A remark on Algorithm AS 181: The <code class="reqn">W</code> test for
normality.
<em>Applied Statistics</em>, <b>44</b>, 547&ndash;551.
<a href="https://doi.org/10.2307/2986146">doi:10.2307/2986146</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+qqnorm">qqnorm</a></code> for producing a normal quantile-quantile plot.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>shapiro.test(rnorm(100, mean = 5, sd = 3))
shapiro.test(runif(100, min = 2, max = 4))
</code></pre>

<hr>
<h2 id='sigma'>Extract Residual Standard Deviation 'Sigma'</h2><span id='topic+sigma'></span><span id='topic+sigma.default'></span><span id='topic+sigma.mlm'></span><span id='topic+sigma.glm'></span>

<h3>Description</h3>

<p>Extract the estimated standard deviation of the errors, the
&ldquo;residual standard deviation&rdquo; (misnamed also
&ldquo;residual standard error&rdquo;, e.g., in
<code><a href="#topic+summary.lm">summary.lm</a>()</code>'s output, from a fitted model).
</p>
<p>Many classical statistical models have a <em>scale parameter</em>,
typically the standard deviation of a zero-mean normal (or Gaussian)
random variable which is denoted as <code class="reqn">\sigma</code>.
<code>sigma(.)</code> extracts the <em>estimated</em> parameter from a fitted
model, i.e., <code class="reqn">\hat\sigma</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sigma(object, ...)

## Default S3 method:
sigma(object, use.fallback = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sigma_+3A_object">object</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object, typically resulting from a model fitting
function such as <code><a href="#topic+lm">lm</a></code>.</p>
</td></tr>
<tr><td><code id="sigma_+3A_use.fallback">use.fallback</code></td>
<td>
<p>logical, passed to <code><a href="#topic+nobs">nobs</a></code>.</p>
</td></tr>
<tr><td><code id="sigma_+3A_...">...</code></td>
<td>
<p>potentially further arguments passed to and from
methods.  Passed to <code><a href="#topic+deviance">deviance</a>(*, ...)</code> for the default method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <span class="pkg">stats</span> package provides the S3 generic, a default method,
and a method for objects of class <code>"glm"</code>. 
The default method is correct typically for (asymptotically / approximately)
generalized gaussian (&ldquo;least squares&rdquo;) problems, since it is
defined as
</p>
<pre>
   sigma.default &lt;- function (object, use.fallback = TRUE, ...)
       sqrt( deviance(object, ...) / (NN - PP) )
 </pre>
<p>where <code>NN &lt;- <a href="#topic+nobs">nobs</a>(object, use.fallback = use.fallback)</code>
and <code>PP &lt;- sum(!is.na(<a href="#topic+coef">coef</a>(object)))</code> &ndash; where in older <span class="rlang"><b>R</b></span>
versions this was <code>length(coef(object))</code> which is too large in
case of undetermined coefficients, e.g., for rank deficient model fits.
</p>


<h3>Value</h3>

<p>Typically a number, the estimated standard deviation of the
errors (&ldquo;residual standard deviation&rdquo;) for Gaussian
models, and&mdash;less interpretably&mdash;the square root of the residual
deviance per degree of freedom in more general models.
</p>
<p>Very strictly speaking, <code class="reqn">\hat{\sigma}</code> (&ldquo;<code class="reqn">\sigma</code> hat&rdquo;)
is actually <code class="reqn">\sqrt{\widehat{\sigma^2}}</code>.
</p>
<p>For generalized linear models (class <code>"glm"</code>), the
<code>sigma.glm</code> method returns the square root of the dispersion
parameter (See <code><a href="#topic+summary.glm">summary.glm</a></code>). For families with free
dispersion parameter, <code class="reqn">sigma</code> is estimated from the root mean
square of the Pearson residuals.  For families with fixed dispersion,
<code>sigma</code> is not estimated from the residuals but extracted directly
from the family of the fitted model.  Consequently, for binomial or
Poisson GLMs, <code>sigma</code> is exactly 1.
</p>
<p>For multivariate linear models (class <code>"mlm"</code>), a <em>vector</em>
of sigmas is returned, each corresponding to one column of <code class="reqn">Y</code>.
</p>


<h3>Note</h3>

<p>The misnomer &ldquo;Residual standard <b>error</b>&rdquo; has been part of
too many <span class="rlang"><b>R</b></span> (and S) outputs to be easily changed there.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+deviance">deviance</a></code>, <code><a href="#topic+nobs">nobs</a></code>, <code><a href="#topic+vcov">vcov</a></code>,
<code><a href="#topic+summary.glm">summary.glm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## -- lm() ------------------------------
lm1 &lt;- lm(Fertility ~ . , data = swiss)
sigma(lm1) # ~= 7.165  = "Residual standard error"  printed from summary(lm1)
stopifnot(all.equal(sigma(lm1), summary(lm1)$sigma, tolerance=1e-15))

## -- nls() -----------------------------
DNase1 &lt;- subset(DNase, Run == 1)
fm.DN1 &lt;- nls(density ~ SSlogis(log(conc), Asym, xmid, scal), DNase1)
sigma(fm.DN1) # ~= 0.01919  as from summary(..)
stopifnot(all.equal(sigma(fm.DN1), summary(fm.DN1)$sigma, tolerance=1e-15))


## -- glm() -----------------------------
## -- a) Binomial -- Example from MASS
ldose &lt;- rep(0:5, 2)
numdead &lt;- c(1, 4, 9, 13, 18, 20, 0, 2, 6, 10, 12, 16)
sex &lt;- factor(rep(c("M", "F"), c(6, 6)))
SF &lt;- cbind(numdead, numalive = 20-numdead)
sigma(budworm.lg &lt;- glm(SF ~ sex*ldose, family = binomial))

## -- b) Poisson -- from ?glm :
## Dobson (1990) Page 93: Randomized Controlled Trial :
counts &lt;- c(18,17,15,20,10,20,25,13,12)
outcome &lt;- gl(3,1,9)
treatment &lt;- gl(3,3)
sigma(glm.D93 &lt;- glm(counts ~ outcome + treatment, family = poisson()))
## equal to
sqrt(summary(glm.D93)$dispersion) # == 1
## and the *Quasi*poisson's dispersion
sigma(glm.qD93 &lt;- update(glm.D93, family = quasipoisson()))
sigma (glm.qD93)^2 # 1.2933 equal to
summary(glm.qD93)$dispersion # == 1.2933

## -- Multivariate lm() "mlm" -----------
utils::example("SSD", echo=FALSE)
sigma(mlmfit) # is the same as {but more efficient than}
sqrt(diag(estVar(mlmfit)))

</code></pre>

<hr>
<h2 id='SignRank'>Distribution of the Wilcoxon Signed Rank Statistic</h2><span id='topic+SignRank'></span><span id='topic+dsignrank'></span><span id='topic+psignrank'></span><span id='topic+qsignrank'></span><span id='topic+rsignrank'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the distribution of the Wilcoxon Signed Rank statistic
obtained from a sample with size <code>n</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dsignrank(x, n, log = FALSE)
psignrank(q, n, lower.tail = TRUE, log.p = FALSE)
qsignrank(p, n, lower.tail = TRUE, log.p = FALSE)
rsignrank(nn, n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SignRank_+3A_x">x</code>, <code id="SignRank_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="SignRank_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="SignRank_+3A_nn">nn</code></td>
<td>
<p>number of observations. If <code>length(nn) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="SignRank_+3A_n">n</code></td>
<td>
<p>number(s) of observations in the sample(s).  A positive
integer, or a vector of such integers.</p>
</td></tr>
<tr><td><code id="SignRank_+3A_log">log</code>, <code id="SignRank_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="SignRank_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This distribution is obtained as follows.  Let <code>x</code> be a sample of
size <code>n</code> from a continuous distribution symmetric about the
origin.  Then the Wilcoxon signed rank statistic is the sum of the
ranks of the absolute values <code>x[i]</code> for which <code>x[i]</code> is
positive.  This statistic takes values between <code class="reqn">0</code> and
<code class="reqn">n(n+1)/2</code>, and its mean and variance are <code class="reqn">n(n+1)/4</code> and
<code class="reqn">n(n+1)(2n+1)/24</code>, respectively.
</p>
<p>If either of the first two arguments is a vector, the recycling rule is
used to do the calculations for all combinations of the two up to
the length of the longer vector.
</p>


<h3>Value</h3>

<p><code>dsignrank</code> gives the density,
<code>psignrank</code> gives the distribution function,
<code>qsignrank</code> gives the quantile function, and
<code>rsignrank</code> generates random deviates.
</p>
<p>The length of the result is determined by <code>nn</code> for
<code>rsignrank</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.  
</p>
<p>The numerical arguments other than <code>nn</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.
</p>


<h3>Author(s)</h3>

<p>Kurt Hornik; efficiency improvement by Ivo Ugrina.</p>


<h3>See Also</h3>

<p><code><a href="#topic+wilcox.test">wilcox.test</a></code> to calculate the statistic from data, find p
values and so on.
</p>
<p><a href="#topic+Distributions">Distributions</a> for standard distributions, including
<code><a href="#topic+dwilcox">dwilcox</a></code> for the distribution of <em>two-sample</em>
Wilcoxon rank sum statistic.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

par(mfrow = c(2,2))
for(n in c(4:5,10,40)) {
  x &lt;- seq(0, n*(n+1)/2, length.out = 501)
  plot(x, dsignrank(x, n = n), type = "l",
       main = paste0("dsignrank(x, n = ", n, ")"))
}

</code></pre>

<hr>
<h2 id='simulate'>Simulate Responses</h2><span id='topic+simulate'></span>

<h3>Description</h3>

<p>Simulate one or more responses from the distribution
corresponding to a fitted model object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate(object, nsim = 1, seed = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_+3A_object">object</code></td>
<td>
<p>an object representing a fitted model.</p>
</td></tr>
<tr><td><code id="simulate_+3A_nsim">nsim</code></td>
<td>
<p>number of response vectors to simulate.  Defaults to <code>1</code>.</p>
</td></tr>
<tr><td><code id="simulate_+3A_seed">seed</code></td>
<td>
<p>an object specifying if and how the random number
generator should be initialized (&lsquo;seeded&rsquo;).<br />
For the <code>"lm"</code> method, either <code>NULL</code> or an integer that will be
used in a call to <code>set.seed</code> before simulating the response
vectors.  If set, the value is saved as the <code>"seed"</code> attribute
of the returned value.  The default, <code>NULL</code> will not change the
random generator state, and return <code><a href="base.html#topic+.Random.seed">.Random.seed</a></code> as the
<code>"seed"</code> attribute, see &lsquo;Value&rsquo;.
</p>
</td></tr>
<tr><td><code id="simulate_+3A_...">...</code></td>
<td>
<p>additional optional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function.  Consult the individual modeling functions
for details on how to use this function.
</p>
<p>Package <span class="pkg">stats</span> has a method for <code>"lm"</code> objects which is used
for <code><a href="#topic+lm">lm</a></code> and <code><a href="#topic+glm">glm</a></code> fits.  There is a method
for fits from <code>glm.nb</code> in package <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a>, and hence the
case of negative binomial families is not covered by the <code>"lm"</code>
method.
</p>
<p>The methods for linear models fitted by <code>lm</code> or <code>glm(family
  = "gaussian")</code> assume that any weights which have been supplied are
inversely proportional to the error variance.  For other GLMs the
(optional) <code>simulate</code> component of the <code><a href="#topic+family">family</a></code>
object is used&mdash;there is no appropriate simulation method for
&lsquo;quasi&rsquo; models as they are specified only up to two moments.
</p>
<p>For binomial and Poisson GLMs the dispersion is fixed at one.  Integer
prior weights <code class="reqn">w_i</code> can be interpreted as meaning that
observation <code class="reqn">i</code> is an average of <code class="reqn">w_i</code> observations, which is
natural for binomials specified as proportions but less so for a
Poisson, for which prior weights are ignored with a warning.
</p>
<p>For a gamma GLM the shape parameter is estimated by maximum likelihood
(using function <code><a href="MASS.html#topic+gamma.shape.glm">gamma.shape</a></code> in package
<a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a>).  The interpretation of weights is as multipliers to a
basic shape parameter, since dispersion is inversely proportional to
shape.
</p>
<p>For an inverse gaussian GLM the model assumed is
<code class="reqn">IG(\mu_i, \lambda w_i)</code> (see
<a href="https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution">https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution</a>)
where <code class="reqn">\lambda</code> is estimated by the inverse of the dispersion
estimate for the fit.  The variance is
<code class="reqn">\mu_i^3/(\lambda w_i)</code> and
hence inversely proportional to the prior weights.  The simulation is
done by function <code><a href="SuppDists.html#topic+invGauss">rinvGauss</a></code> from the
<a href="https://CRAN.R-project.org/package=SuppDists"><span class="pkg">SuppDists</span></a> package, which must be installed.
</p>


<h3>Value</h3>

<p>Typically, a list of length <code>nsim</code> of simulated responses.  Where
appropriate the result can be a data frame (which is a special type of
list).


</p>
<p>For the <code>"lm"</code> method, the result is a data frame with an
attribute <code>"seed"</code>.  If argument <code>seed</code> is <code>NULL</code>, the
attribute is the value of <code><a href="base.html#topic+.Random.seed">.Random.seed</a></code> before the
simulation was started; otherwise it is the value of the argument with
a <code>"kind"</code> attribute with value <code>as.list(<a href="base.html#topic+RNGkind">RNGkind</a>())</code>.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+RNG">RNG</a></code> about random number generation in <span class="rlang"><b>R</b></span>,
<code><a href="#topic+fitted.values">fitted.values</a></code> and <code><a href="#topic+residuals">residuals</a></code> for related methods;
<code><a href="#topic+glm">glm</a></code>, <code><a href="#topic+lm">lm</a></code> for model fitting.
</p>
<p>There are further examples in the &lsquo;<span class="file">simulate.R</span>&rsquo; tests file in the
sources for package <span class="pkg">stats</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 1:5
mod1 &lt;- lm(c(1:3, 7, 6) ~ x)
S1 &lt;- simulate(mod1, nsim = 4)
## repeat the simulation:
.Random.seed &lt;- attr(S1, "seed")
identical(S1, simulate(mod1, nsim = 4))

S2 &lt;- simulate(mod1, nsim = 200, seed = 101)
rowMeans(S2) # should be about the same as
fitted(mod1)

## repeat identically:
(sseed &lt;- attr(S2, "seed")) # seed; RNGkind as attribute
stopifnot(identical(S2, simulate(mod1, nsim = 200, seed = sseed)))

## To be sure about the proper RNGkind, e.g., after
RNGversion("2.7.0")
## first set the RNG kind, then simulate
do.call(RNGkind, attr(sseed, "kind"))
identical(S2, simulate(mod1, nsim = 200, seed = sseed))

## Binomial GLM examples
yb1 &lt;- matrix(c(4, 4, 5, 7, 8, 6, 6, 5, 3, 2), ncol = 2)
modb1 &lt;- glm(yb1 ~ x, family = binomial)
S3 &lt;- simulate(modb1, nsim = 4)
# each column of S3 is a two-column matrix.

x2 &lt;- sort(runif(100))
yb2 &lt;- rbinom(100, prob = plogis(2*(x2-1)), size = 1)
yb2 &lt;- factor(1 + yb2, labels = c("failure", "success"))
modb2 &lt;- glm(yb2 ~ x2, family = binomial)
S4 &lt;- simulate(modb2, nsim = 4)
# each column of S4 is a factor
</code></pre>

<hr>
<h2 id='Smirnov'>Distribution of the Smirnov Statistic</h2><span id='topic+Smirnov'></span><span id='topic+psmirnov'></span><span id='topic+qsmirnov'></span><span id='topic+rsmirnov'></span>

<h3>Description</h3>

<p>Distribution function, quantile function and random generation for the
distribution of the Smirnov statistic.</p>


<h3>Usage</h3>

<pre><code class='language-R'>psmirnov(q, sizes, z = NULL,
         alternative = c("two.sided", "less", "greater"),
         exact = TRUE, simulate = FALSE, B = 2000,
         lower.tail = TRUE, log.p = FALSE)
qsmirnov(p, sizes, z = NULL,
         alternative = c("two.sided", "less", "greater"),
         exact = TRUE, simulate = FALSE, B = 2000)
rsmirnov(n, sizes, z = NULL,
         alternative = c("two.sided", "less", "greater")) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Smirnov_+3A_q">q</code></td>
<td>
<p>a numeric vector of quantiles.</p>
</td></tr>
<tr><td><code id="Smirnov_+3A_p">p</code></td>
<td>
<p>a numeric vector of probabilities.</p>
</td></tr>
<tr><td><code id="Smirnov_+3A_sizes">sizes</code></td>
<td>
<p>an integer vector of length two giving the sample sizes.</p>
</td></tr>
<tr><td><code id="Smirnov_+3A_z">z</code></td>
<td>
<p>a numeric vector of the pooled data values in both samples
when the exact conditional distribution of the Smirnov statistic
given the data shall be computed.</p>
</td></tr>
<tr><td><code id="Smirnov_+3A_alternative">alternative</code></td>
<td>
<p>one of <code>"two.sided"</code> (default), <code>"less"</code>,
or <code>"greater"</code> indicating whether absolute (two-sided, default)
or raw (one-sided) differences of frequencies define the test
statistic.  See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="Smirnov_+3A_exact">exact</code></td>
<td>
<p><code>NULL</code> or a logical indicating whether the exact
(conditional on the pooled data values in <code>z</code>) distribution
or the asymptotic distribution should be used.</p>
</td></tr>
<tr><td><code id="Smirnov_+3A_simulate">simulate</code></td>
<td>
<p>a logical indicating whether to compute the
distribution function by Monte Carlo simulation.</p>
</td></tr>
<tr><td><code id="Smirnov_+3A_b">B</code></td>
<td>
<p>an integer specifying the number of replicates used in the
Monte Carlo test.</p>
</td></tr>
<tr><td><code id="Smirnov_+3A_lower.tail">lower.tail</code></td>
<td>
<p>a logical, if <code>TRUE</code> (default), probabilities are
<code class="reqn">P[D &lt; q]</code>, otherwise, <code class="reqn">P[D \ge q]</code>.</p>
</td></tr>
<tr><td><code id="Smirnov_+3A_log.p">log.p</code></td>
<td>
<p>a logical, if <code>TRUE</code> (default), probabilities are given
as log-probabilities.</p>
</td></tr>
<tr><td><code id="Smirnov_+3A_n">n</code></td>
<td>
<p>an integer giving number of observations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For samples <code class="reqn">x</code> and <code class="reqn">y</code> with respective sizes <code class="reqn">n_x</code> and
<code class="reqn">n_y</code> and empirical cumulative distribution functions
<code class="reqn">F_{x,n_x}</code> and <code class="reqn">F_{y,n_y}</code>, the Smirnov statistic is
</p>
<p style="text-align: center;"><code class="reqn">D = \sup_c | F_{x,n_x}(c) - F_{y,n_y}(c) |</code>
</p>

<p>in the two-sided case,
</p>
<p style="text-align: center;"><code class="reqn">D^+ = \sup_c ( F_{x,n_x}(c) - F_{y,n_y}(c) )</code>
</p>

<p>in the one-sided <code>"greater"</code> case, and
</p>
<p style="text-align: center;"><code class="reqn">D^- = \sup_c ( F_{y,n_y}(c) - F_{x,n_x}(c) )</code>
</p>

<p>in the one-sided <code>"less"</code> case.
</p>
<p>These statistics are used in the Smirnov test of the null that <code class="reqn">x</code>
and <code class="reqn">y</code> were drawn from the same distribution, see
<code><a href="#topic+ks.test">ks.test</a></code>.
</p>
<p>If the underlying common distribution function <code class="reqn">F</code> is continuous,
the distribution of the test statistics does not depend on <code class="reqn">F</code>,
and has a simple asymptotic approximation.  For arbitrary <code class="reqn">F</code>, one
can compute the conditional distribution given the pooled data values
<code class="reqn">z</code> of <code class="reqn">x</code> and <code class="reqn">y</code>, either exactly (feasible provided that
the product <code class="reqn">n_x n_y</code> of the sample sizes is &ldquo;small enough&rdquo;) or
approximately Monte Carlo simulation. If the pooled data values <code class="reqn">z</code>
are not specified, a pooled sample without ties is assumed.
</p>


<h3>Value</h3>

<p><code>psmirnov</code> gives the distribution function,
<code>qsmirnov</code> gives the quantile function, and
<code>rsmirnov</code> generates random deviates.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ks.test">ks.test</a></code> for references on the algorithms used for
computing exact distributions.
</p>

<hr>
<h2 id='smooth'>Tukey's (Running Median) Smoothing</h2><span id='topic+smooth'></span>

<h3>Description</h3>

<p>Tukey's smoothers, <em>3RS3R</em>, <em>3RSS</em>, <em>3R</em>, etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smooth(x, kind = c("3RS3R", "3RSS", "3RSR", "3R", "3", "S"),
       twiceit = FALSE, endrule = c("Tukey", "copy"), do.ends = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smooth_+3A_x">x</code></td>
<td>
<p>a vector or time series</p>
</td></tr>
<tr><td><code id="smooth_+3A_kind">kind</code></td>
<td>
<p>a character string indicating the kind of smoother required;
defaults to <code>"3RS3R"</code>.</p>
</td></tr>
<tr><td><code id="smooth_+3A_twiceit">twiceit</code></td>
<td>
<p>logical, indicating if the result should be &lsquo;twiced&rsquo;.
Twicing a smoother <code class="reqn">S(y)</code> means <code class="reqn">S(y) + S(y - S(y))</code>, i.e.,
adding smoothed residuals to the smoothed values.  This decreases
bias (increasing variance).</p>
</td></tr>
<tr><td><code id="smooth_+3A_endrule">endrule</code></td>
<td>
<p>a character string indicating the rule for smoothing at the
boundary.  Either <code>"Tukey"</code> (default) or <code>"copy"</code>.</p>
</td></tr>
<tr><td><code id="smooth_+3A_do.ends">do.ends</code></td>
<td>
<p>logical, indicating if the 3-splitting of ties should
also happen at the boundaries (ends).  This is only used for
<code>kind = "S"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><em><code>3</code></em> is Tukey's short notation for running <code><a href="#topic+median">median</a></code>s
of length <b>3</b>,
<br />
<em><code>3R</code></em> stands for <b>R</b>epeated <em><code>3</code></em> until
convergence, and
<br />
<em><code>S</code></em> for <b>S</b>plitting of horizontal stretches of length 2 or 3.
</p>
<p>Hence, <em><code>3RS3R</code></em> is a concatenation of <code>3R</code>, <code>S</code>
and <code>3R</code>, <em><code>3RSS</code></em> similarly,
whereas <em><code>3RSR</code></em> means first <code>3R</code>
and then <code>(S and 3)</code> <b>R</b>epeated until convergence &ndash; which
can be bad.
</p>


<h3>Value</h3>

<p>An object of class <code>"tukeysmooth"</code> (which has <code>print</code> and
<code>summary</code> methods) and is a vector or time series containing the
smoothed values with additional attributes.
</p>


<h3>Note</h3>

<p>Note that there are other smoothing methods which provide
rather better results.  These were designed for hand calculations
and may be used mainly for didactical purposes.
</p>
<p>Since <span class="rlang"><b>R</b></span> version 1.2, <code>smooth</code> <em>does</em> really implement
Tukey's end-point rule correctly (see argument <code>endrule</code>).
</p>
<p><code>kind = "3RSR"</code> had been the default till <span class="rlang"><b>R</b></span>-1.1,
but it can have very bad properties, see the examples.
</p>
<p>Note that repeated application of <code>smooth(*)</code> <em>does</em>
smooth more, for the <code>"3RS*"</code> kinds.
</p>


<h3>References</h3>

<p>Tukey, J. W. (1977).
<em>Exploratory Data Analysis</em>,
Reading Massachusetts: Addison-Wesley.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+runmed">runmed</a></code> for running medians;
<code><a href="#topic+lowess">lowess</a></code> and <code><a href="#topic+loess">loess</a></code>;
<code><a href="#topic+supsmu">supsmu</a></code> and
<code><a href="#topic+smooth.spline">smooth.spline</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

## see also   demo(smooth) !

x1 &lt;- c(4, 1, 3, 6, 6, 4, 1, 6, 2, 4, 2) # very artificial
(x3R &lt;- smooth(x1, "3R")) # 2 iterations of "3"
smooth(x3R, kind = "S")

sm.3RS &lt;- function(x, ...)
   smooth(smooth(x, "3R", ...), "S", ...)

y &lt;- c(1, 1, 19:1)
plot(y, main = "misbehaviour of \"3RSR\"", col.main = 3)
lines(sm.3RS(y))
lines(smooth(y))
lines(smooth(y, "3RSR"), col = 3, lwd = 2)  # the horror

x &lt;- c(8:10, 10, 0, 0, 9, 9)
plot(x, main = "breakdown of  3R  and  S  and hence  3RSS")
matlines(cbind(smooth(x, "3R"), smooth(x, "S"), smooth(x, "3RSS"), smooth(x)))

presidents[is.na(presidents)] &lt;- 0 # silly
summary(sm3 &lt;- smooth(presidents, "3R"))
summary(sm2 &lt;- smooth(presidents,"3RSS"))
summary(sm  &lt;- smooth(presidents))

all.equal(c(sm2), c(smooth(smooth(sm3, "S"), "S")))  # 3RSS  === 3R S S
all.equal(c(sm),  c(smooth(smooth(sm3, "S"), "3R"))) # 3RS3R === 3R S 3R

plot(presidents, main = "smooth(presidents0, *) :  3R and default 3RS3R")
lines(sm3, col = 3, lwd = 1.5)
lines(sm, col = 2, lwd = 1.25)
</code></pre>

<hr>
<h2 id='smooth.spline'>Fit a Smoothing Spline</h2><span id='topic+smooth.spline'></span><span id='topic+.nknots.smspl'></span>

<h3>Description</h3>

<p>Fits a cubic smoothing spline to the supplied data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smooth.spline(x, y = NULL, w = NULL, df, spar = NULL, lambda = NULL, cv = FALSE,
              all.knots = FALSE, nknots = .nknots.smspl,
              keep.data = TRUE, df.offset = 0, penalty = 1,
              control.spar = list(), tol = 1e-6 * IQR(x), keep.stuff = FALSE)

.nknots.smspl(n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smooth.spline_+3A_x">x</code></td>
<td>
<p>a vector giving the values of the predictor variable, or  a
list or a two-column matrix specifying x and y. </p>
</td></tr>
<tr><td><code id="smooth.spline_+3A_y">y</code></td>
<td>
<p>responses. If <code>y</code> is missing or <code>NULL</code>, the responses
are assumed to be specified by <code>x</code>, with <code>x</code> the index
vector.</p>
</td></tr>
<tr><td><code id="smooth.spline_+3A_w">w</code></td>
<td>
<p>optional vector of weights of the same length as <code>x</code>;
defaults to all 1.</p>
</td></tr>
<tr><td><code id="smooth.spline_+3A_df">df</code></td>
<td>
<p>the desired equivalent number of degrees of freedom (trace of
the smoother matrix).  Must be in <code class="reqn">(1,n_x]</code>,
<code class="reqn">n_x</code> the number of unique x values, see below.</p>
</td></tr>
<tr><td><code id="smooth.spline_+3A_spar">spar</code></td>
<td>
<p>smoothing parameter, typically (but not necessarily) in
<code class="reqn">(0,1]</code>.  When <code>spar</code> is specified, the coefficient
<code class="reqn">\lambda</code> of the integral of the squared second derivative in the
fit (penalized log likelihood) criterion is a monotone function of
<code>spar</code>, see the details below.  Alternatively <code>lambda</code> may
be specified instead of the <em>scale free</em> <code>spar</code>=<code class="reqn">s</code>.</p>
</td></tr>
<tr><td><code id="smooth.spline_+3A_lambda">lambda</code></td>
<td>
<p>if desired, the internal (design-dependent) smoothing
parameter <code class="reqn">\lambda</code> can be specified instead of <code>spar</code>.
This may be desirable for resampling algorithms such as cross
validation or the bootstrap.</p>
</td></tr>
<tr><td><code id="smooth.spline_+3A_cv">cv</code></td>
<td>
<p>ordinary leave-one-out (<code>TRUE</code>) or &lsquo;generalized&rsquo;
cross-validation (GCV) when <code>FALSE</code>; is used for smoothing
parameter computation only when both <code>spar</code> and <code>df</code> are
not specified; it is used however to determine <code>cv.crit</code> in the
result.  Setting it to <code>NA</code> for speedup skips the evaluation of
leverages and any score.</p>
</td></tr>
<tr><td><code id="smooth.spline_+3A_all.knots">all.knots</code></td>
<td>
<p>if <code>TRUE</code>, all distinct points in <code>x</code> are used
as knots.  If <code>FALSE</code> (default), a subset of <code>x[]</code> is used,
specifically <code>x[j]</code> where the <code>nknots</code> indices are evenly
spaced in <code>1:n</code>, see also the next argument <code>nknots</code>.
</p>
<p>Alternatively, a strictly increasing <code><a href="base.html#topic+numeric">numeric</a></code> vector
specifying &ldquo;all the knots&rdquo; to be used; must be rescaled
to <code class="reqn">[0, 1]</code> already such that it corresponds to the
<code>ans $ fit$knots</code> sequence returned, not repeating the boundary
knots.</p>
</td></tr>
<tr><td><code id="smooth.spline_+3A_nknots">nknots</code></td>
<td>
<p>integer or <code><a href="base.html#topic+function">function</a></code> giving the number of
knots to use when <code>all.knots = FALSE</code>.  If a function (as by
default), the number of knots is <code>nknots(nx)</code>.  By default using
<code>.nknots.smspl()</code>, for
<code class="reqn">n_x &gt; 49</code> this is less than <code class="reqn">n_x</code>, the number
of unique <code>x</code> values, see the Note.</p>
</td></tr>
<tr><td><code id="smooth.spline_+3A_keep.data">keep.data</code></td>
<td>
<p>logical specifying if the input data should be kept
in the result.  If <code>TRUE</code> (as per default), fitted values and
residuals are available from the result.</p>
</td></tr>
<tr><td><code id="smooth.spline_+3A_df.offset">df.offset</code></td>
<td>
<p>allows the degrees of freedom to be increased by
<code>df.offset</code> in the GCV criterion.</p>
</td></tr>
<tr><td><code id="smooth.spline_+3A_penalty">penalty</code></td>
<td>
<p>the coefficient of the penalty for degrees of freedom
in the GCV criterion.</p>
</td></tr>
<tr><td><code id="smooth.spline_+3A_control.spar">control.spar</code></td>
<td>
<p>optional list with named components controlling the
root finding when the smoothing parameter <code>spar</code> is computed,
i.e., missing or <code>NULL</code>, see below.
</p>
<p><b>Note</b> that this is partly <em>experimental</em> and may change
with general spar computation improvements!
</p>

<dl>
<dt>low:</dt><dd><p>lower bound for <code>spar</code>; defaults to -1.5 (used to
implicitly default to 0 in <span class="rlang"><b>R</b></span> versions earlier than 1.4).</p>
</dd>
<dt>high:</dt><dd><p>upper bound for <code>spar</code>; defaults to +1.5.</p>
</dd>
<dt>tol:</dt><dd><p>the absolute precision (<b>tol</b>erance) used; defaults
to 1e-4 (formerly 1e-3).</p>
</dd>
<dt>eps:</dt><dd><p>the relative precision used; defaults to 2e-8 (formerly
0.00244).</p>
</dd>
<dt>trace:</dt><dd><p>logical indicating if iterations should be traced.</p>
</dd>
<dt>maxit:</dt><dd><p>integer giving the maximal number of iterations;
defaults to 500.</p>
</dd>
</dl>

<p>Note that <code>spar</code> is only searched for in the interval
<code class="reqn">[low, high]</code>.
</p>
</td></tr>
<tr><td><code id="smooth.spline_+3A_tol">tol</code></td>
<td>
<p>a tolerance for sameness or uniqueness of the <code>x</code>
values.  The values are binned into bins of size <code>tol</code> and
values which fall into the same bin are regarded as the same.  Must
be strictly positive (and finite).</p>
</td></tr>
<tr><td><code id="smooth.spline_+3A_keep.stuff">keep.stuff</code></td>
<td>
<p>an experimental <code><a href="base.html#topic+logical">logical</a></code> indicating if
the result should keep extras from the internal computations.  Should
allow to reconstruct the <code class="reqn">X</code> matrix and more.</p>
</td></tr>
<tr><td><code id="smooth.spline_+3A_n">n</code></td>
<td>
<p>for <code>.nknots.smspl</code>; typically the number of unique
<code>x</code> values (aka <code class="reqn">n_x</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Neither <code>x</code> nor <code>y</code> are allowed to containing missing or
infinite values.
</p>
<p>The <code>x</code> vector should contain at least four distinct values.
&lsquo;Distinct&rsquo; here is controlled by <code>tol</code>: values which are
regarded as the same are replaced by the first of their values and the
corresponding <code>y</code> and <code>w</code> are pooled accordingly.
</p>
<p>Unless <code>lambda</code> has been specified instead of <code>spar</code>,
the computational <code class="reqn">\lambda</code> used (as a function of
<code class="reqn">s=spar</code>) is
<code class="reqn">\lambda = r \cdot 256^{3 s - 1}</code>
where
<code class="reqn">r = tr(X' W X) / tr(\Sigma)</code>,
<code class="reqn">\Sigma</code> is the matrix given by
<code class="reqn">\Sigma_{ij} = \int B_i''(t) B_j''(t) dt</code>,
<code class="reqn">X</code> is given by <code class="reqn">X_{ij} = B_j(x_i)</code>,
<code class="reqn">W</code> is the diagonal matrix of weights (scaled such that
its trace is <code class="reqn">n</code>, the original number of observations)
and <code class="reqn">B_k(.)</code> is the <code class="reqn">k</code>-th B-spline.
</p>
<p>Note that with these definitions, <code class="reqn">f_i = f(x_i)</code>, and the B-spline
basis representation <code class="reqn">f = X c</code> (i.e., <code class="reqn">c</code> is
the vector of spline coefficients), the penalized log likelihood is
<code class="reqn">L = (y - f)' W (y - f) + \lambda c' \Sigma c</code>, and hence
<code class="reqn">c</code> is the solution of the (ridge regression)
<code class="reqn">(X' W X + \lambda \Sigma) c = X' W y</code>.
</p>
<p>If <code>spar</code> and <code>lambda</code> are missing or <code>NULL</code>, the value
of <code>df</code> is used to determine the degree of smoothing.  If
<code>df</code> is missing as well, leave-one-out cross-validation (ordinary
or &lsquo;generalized&rsquo; as determined by <code>cv</code>) is used to
determine <code class="reqn">\lambda</code>.
</p>







<p>Note that from the above relation,
<code>spar</code> is <code class="reqn">s = s0 + 0.0601 \cdot \log\lambda</code>.





</p>
<p>Note however that currently the results may become very unreliable
for <code>spar</code> values smaller than about -1 or -2.  The same may
happen for values larger than 2 or so.  Don't think of setting
<code>spar</code> or the controls <code>low</code> and <code>high</code> outside such a
safe range, unless you know what you are doing!
Similarly, specifying <code>lambda</code> instead of <code>spar</code> is
delicate, notably as the range of &ldquo;safe&rdquo; values for
<code>lambda</code> is not scale-invariant and hence entirely data dependent.
</p>
<p>The &lsquo;generalized&rsquo; cross-validation method GCV will work correctly when
there are duplicated points in <code>x</code>.  However, it is ambiguous what
leave-one-out cross-validation means with duplicated points, and the
internal code uses an approximation that involves leaving out groups
of duplicated points.  <code>cv = TRUE</code> is best avoided in that case.
</p>


<h3>Value</h3>

<p>An object of class <code>"smooth.spline"</code> with components
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>the <em>distinct</em> <code>x</code> values in increasing order, see
the &lsquo;Details&rsquo; above.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the fitted values corresponding to <code>x</code>.</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>the weights used at the unique values of <code>x</code>.</p>
</td></tr>
<tr><td><code>yin</code></td>
<td>
<p>the y values used at the unique <code>y</code> values.</p>
</td></tr>
<tr><td><code>tol</code></td>
<td>
<p>the <code>tol</code> argument (whose default depends on <code>x</code>).</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>only if <code>keep.data = TRUE</code>: itself a
<code><a href="base.html#topic+list">list</a></code> with components <code>x</code>, <code>y</code> and <code>w</code>
of the same length.  These are the original <code class="reqn">(x_i,y_i,w_i),
      i = 1, \dots, n</code>, values where <code>data$x</code> may have repeated values and
hence be longer than the above <code>x</code> component; see details.
</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>an integer; the (original) sample size.</p>
</td></tr>
<tr><td><code>lev</code></td>
<td>
<p>(when <code>cv</code> was not <code>NA</code>) leverages, the diagonal
values of the smoother matrix.</p>
</td></tr>
<tr><td><code>cv</code></td>
<td>
<p>the <code>cv</code> argument used; i.e., <code>FALSE</code>, <code>TRUE</code>, or <code>NA</code>.</p>
</td></tr>
<tr><td><code>cv.crit</code></td>
<td>
<p>cross-validation score, &lsquo;generalized&rsquo; or true, depending
on <code>cv</code>.  The CV score is often called &ldquo;PRESS&rdquo; (and
labeled on <code><a href="base.html#topic+print">print</a>()</code>), for
&lsquo;<b>PRE</b>diction <b>S</b>um of <b>S</b>quares&rsquo;.
Note that this is <em>not</em> the same
as the (CV or GCV) score which is minimized during fitting (and
returned in <code>crit</code>), e.g., in the case of <code>nx &lt; n</code> (where
<code>nx</code><code class="reqn">=n_x</code> is the number of unique x values).</p>
</td></tr>
<tr><td><code>pen.crit</code></td>
<td>
<p>the penalized criterion, a non-negative number; simply
the (weighted) residual sum of squares (RSS), <code> sum(.$w * residuals(.)^2) </code>.</p>
</td></tr>
<tr><td><code>crit</code></td>
<td>
<p>the criterion value minimized in the underlying
<code>.Fortran</code> routine &lsquo;<span class="file">sslvrg</span>&rsquo;.  When <code>df</code> has been specified,
the criterion is <code class="reqn">3 + (tr(S_\lambda) - df)^2</code>,
where the <code class="reqn">3 +</code> is there for numerical (and historical) reasons.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>equivalent degrees of freedom used.  Note that (currently)
this value may become quite imprecise when the true <code>df</code> is
between and 1 and 2.
</p>
</td></tr>
<tr><td><code>spar</code></td>
<td>
<p>the value of <code>spar</code> computed or given, unless it has been
given as <code>c(lambda = *)</code>, when it set to <code>NA</code> here.</p>
</td></tr>
<tr><td><code>ratio</code></td>
<td>
<p>(when <code>spar</code> above is not <code>NA</code>), the value
<code class="reqn">r</code>, the ratio of two matrix traces.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>the value of <code class="reqn">\lambda</code> corresponding to <code>spar</code>,
see the details above.</p>
</td></tr>
<tr><td><code>iparms</code></td>
<td>
<p>named integer(3) vector where <code>..$ipars["iter"]</code>
gives number of spar computing iterations used.</p>
</td></tr>
<tr><td><code>auxMat</code></td>
<td>
<p>experimental; when <code>keep.stuff</code> was true, a
&ldquo;flat&rdquo; numeric vector containing parts of the internal computations.</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>list for use by <code><a href="#topic+predict.smooth.spline">predict.smooth.spline</a></code>, with
components
</p>

<dl>
<dt>knot:</dt><dd><p>the knot sequence (including the repeated boundary
knots), scaled into <code class="reqn">[0, 1]</code> (via <code>min</code> and
<code>range</code>).</p>
</dd>
<dt>nk:</dt><dd><p>number of coefficients or number of &lsquo;proper&rsquo;
knots plus 2.</p>
</dd>
<dt>coef:</dt><dd><p>coefficients for the spline basis used.</p>
</dd>
<dt>min, range:</dt><dd><p>numbers giving the corresponding quantities of
<code>x</code>.</p>
</dd>
</dl>

</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
</table>
<p><code>method(class = "smooth.spline")</code> shows a
<code><a href="#topic+hatvalues">hatvalues</a>()</code> method based on the <code>lev</code> vector above.
</p>


<h3>Note</h3>

<p>The number of unique <code>x</code> values, <code class="reqn">\code{nx} = n_x</code>, are
determined by the <code>tol</code> argument, equivalently to
</p>
<pre>
    nx &lt;- length(x) - sum(duplicated( round((x - mean(x)) / tol) ))
  </pre>
<p>The default <code>all.knots = FALSE</code> and <code>nknots = .nknots.smspl</code>,
entails using only <code class="reqn">O({n_x}^{0.2})</code>
knots instead of <code class="reqn">n_x</code> for <code class="reqn">n_x &gt; 49</code>.  This cuts
speed and memory requirements, but not drastically anymore since <span class="rlang"><b>R</b></span>
version 1.5.1 where it is only <code class="reqn">O(n_k) + O(n)</code> where
<code class="reqn">n_k</code> is the number of knots.
</p>
<p>In this case where not all unique <code>x</code> values are
used as knots, the result is a <em>regression spline</em> rather than a
smoothing spline in the strict
sense, but very close unless a small smoothing parameter (or large
<code>df</code>) is used.
</p>


<h3>Author(s)</h3>

<p><span class="rlang"><b>R</b></span> implementation by B. D. Ripley and Martin Maechler
(<code>spar/lambda</code>, etc).
</p>


<h3>Source</h3>

<p>This function is based on code in the <code>GAMFIT</code> Fortran program by
T. Hastie and R. Tibshirani (originally taken from
<a href="http://lib.stat.cmu.edu/general/gamfit">http://lib.stat.cmu.edu/general/gamfit</a>)
which makes use of spline code by Finbarr O'Sullivan.  Its design
parallels the <code>smooth.spline</code> function of Chambers &amp; Hastie (1992).
</p>


<h3>References</h3>

<p>Chambers, J. M. and Hastie, T. J. (1992)
<em>Statistical Models in S</em>, Wadsworth &amp; Brooks/Cole.
</p>
<p>Green, P. J. and Silverman, B. W. (1994)
<em>Nonparametric Regression and Generalized Linear Models:
A Roughness Penalty Approach.</em> Chapman and Hall.
</p>
<p>Hastie, T. J. and Tibshirani, R. J. (1990)
<em>Generalized Additive Models.</em>  Chapman and Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.smooth.spline">predict.smooth.spline</a></code> for evaluating the spline
and its derivatives.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)
plot(dist ~ speed, data = cars, main = "data(cars)  &amp;  smoothing splines")
cars.spl &lt;- with(cars, smooth.spline(speed, dist))
cars.spl
## This example has duplicate points, so avoid cv = TRUE

lines(cars.spl, col = "blue")
ss10 &lt;- smooth.spline(cars[,"speed"], cars[,"dist"], df = 10)
lines(ss10, lty = 2, col = "red")
legend(5,120,c(paste("default [C.V.] =&gt; df =",round(cars.spl$df,1)),
               "s( * , df = 10)"), col = c("blue","red"), lty = 1:2,
       bg = 'bisque')


## Residual (Tukey Anscombe) plot:
plot(residuals(cars.spl) ~ fitted(cars.spl))
abline(h = 0, col = "gray")

## consistency check:
stopifnot(all.equal(cars$dist,
                    fitted(cars.spl) + residuals(cars.spl)))
## The chosen inner knots in original x-scale :
with(cars.spl$fit, min + range * knot[-c(1:3, nk+1 +1:3)]) # == unique(cars$speed)

## Visualize the behavior of  .nknots.smspl()
nKnots &lt;- Vectorize(.nknots.smspl) ; c.. &lt;- adjustcolor("gray20",.5)
curve(nKnots, 1, 250, n=250)
abline(0,1, lty=2, col=c..); text(90,90,"y = x", col=c.., adj=-.25)
abline(h=100,lty=2); abline(v=200, lty=2)

n &lt;- c(1:799, seq(800, 3490, by=10), seq(3500, 10000, by = 50))
plot(n, nKnots(n), type="l", main = "Vectorize(.nknots.smspl) (n)")
abline(0,1, lty=2, col=c..); text(180,180,"y = x", col=c..)
n0 &lt;- c(50, 200, 800, 3200); c0 &lt;- adjustcolor("blue3", .5)
lines(n0, nKnots(n0), type="h", col=c0)
axis(1, at=n0, line=-2, col.ticks=c0, col=NA, col.axis=c0)
axis(4, at=.nknots.smspl(10000), line=-.5, col=c..,col.axis=c.., las=1)

##-- artificial example
y18 &lt;- c(1:3, 5, 4, 7:3, 2*(2:5), rep(10, 4))
xx  &lt;- seq(1, length(y18), length.out = 201)
(s2   &lt;- smooth.spline(y18)) # GCV
(s02  &lt;- smooth.spline(y18, spar = 0.2))
(s02. &lt;- smooth.spline(y18, spar = 0.2, cv = NA))
plot(y18, main = deparse(s2$call), col.main = 2)
lines(s2, col = "gray"); lines(predict(s2, xx), col = 2)
lines(predict(s02, xx), col = 3); mtext(deparse(s02$call), col = 3)

## Specifying 'lambda' instead of usual spar :
(s2. &lt;- smooth.spline(y18, lambda = s2$lambda, tol = s2$tol))



## The following shows the problematic behavior of 'spar' searching:
(s2  &lt;- smooth.spline(y18, control =
                      list(trace = TRUE, tol = 1e-6, low = -1.5)))
(s2m &lt;- smooth.spline(y18, cv = TRUE, control =
                      list(trace = TRUE, tol = 1e-6, low = -1.5)))
## both above do quite similarly (Df = 8.5 +- 0.2)
</code></pre>

<hr>
<h2 id='smoothEnds'>End Points Smoothing (for Running Medians)</h2><span id='topic+smoothEnds'></span>

<h3>Description</h3>

<p>Smooth end points of a vector <code>y</code> using subsequently smaller
medians and Tukey's end point rule at the very end. (of odd span),
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smoothEnds(y, k = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smoothEnds_+3A_y">y</code></td>
<td>
<p>dependent variable to be smoothed (vector).</p>
</td></tr>
<tr><td><code id="smoothEnds_+3A_k">k</code></td>
<td>
<p>width of largest median window; must be odd.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>smoothEnds</code> is used to only do the &lsquo;end point smoothing&rsquo;,
i.e., change at most the observations closer to the beginning/end
than half the window <code>k</code>.  The first and last value are computed using
<em>Tukey's end point rule</em>, i.e.,
<code>sm[1] = median(y[1], sm[2], 3*sm[2] - 2*sm[3], na.rm=TRUE)</code>.
</p>
<p>In <span class="rlang"><b>R</b></span> versions 3.6.0 and earlier, missing values (<code><a href="base.html#topic+NA">NA</a></code>)
in <code>y</code> typically lead to an error, whereas now the equivalent of
<code><a href="#topic+median">median</a>(*, na.rm=TRUE)</code> is used.
</p>


<h3>Value</h3>

<p>vector of smoothed values, the same length as <code>y</code>.
</p>


<h3>Author(s)</h3>

<p>Martin Maechler</p>


<h3>References</h3>

<p>John W. Tukey (1977)
<em>Exploratory Data Analysis</em>, Addison.
</p>
<p>Velleman, P.F., and Hoaglin, D.C. (1981)
<em>ABC of EDA (Applications, Basics, and Computing of Exploratory
Data Analysis)</em>; Duxbury.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+runmed">runmed</a>(*, endrule = "median")</code> which calls
<code>smoothEnds()</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

y &lt;- ys &lt;- (-20:20)^2
y [c(1,10,21,41)] &lt;-  c(100, 30, 400, 470)
s7k &lt;- runmed(y, 7, endrule = "keep")
s7. &lt;- runmed(y, 7, endrule = "const")
s7m &lt;- runmed(y, 7)
col3 &lt;- c("midnightblue","blue","steelblue")
plot(y, main = "Running Medians -- runmed(*, k=7, endrule = X)")
lines(ys, col = "light gray")
matlines(cbind(s7k, s7.,s7m), lwd = 1.5, lty = 1, col = col3)
eRules &lt;- c("keep","constant","median")
legend("topleft", paste("endrule", eRules, sep = " = "),
       col = col3, lwd = 1.5, lty = 1, bty = "n")

stopifnot(identical(s7m, smoothEnds(s7k, 7)))

## With missing values (for R &gt;= 3.6.1):
yN &lt;- y; yN[c(2,40)] &lt;- NA
rN &lt;- sapply(eRules, function(R) runmed(yN, 7, endrule=R))
matlines(rN, type = "b", pch = 4, lwd = 3, lty=2,
         col = adjustcolor(c("red", "orange4", "orange1"), 0.5))
yN[c(1, 20:21)] &lt;- NA # additionally
rN. &lt;- sapply(eRules, function(R) runmed(yN, 7, endrule=R))
head(rN., 4); tail(rN.) # more NA's too, still not *so* many:
stopifnot(exprs = {
   !anyNA(rN[,2:3])
   identical(which(is.na(rN[,"keep"])), c(2L, 40L))
   identical(which(is.na(rN.), arr.ind=TRUE, useNames=FALSE),
             cbind(c(1:2,40L), 1L))
   identical(rN.[38:41, "median"], c(289,289, 397, 470))
})
</code></pre>

<hr>
<h2 id='sortedXyData'>Create a <code>sortedXyData</code> Object</h2><span id='topic+sortedXyData'></span><span id='topic+sortedXyData.default'></span>

<h3>Description</h3>

<p>This is a constructor function for the class of <code>sortedXyData</code>
objects.  These objects are mostly used in the <code>initial</code>
function for a self-starting nonlinear regression model, which will be
of the <code>selfStart</code> class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sortedXyData(x, y, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sortedXyData_+3A_x">x</code></td>
<td>
<p> a numeric vector or an expression that will evaluate in
<code>data</code> to a numeric vector </p>
</td></tr>
<tr><td><code id="sortedXyData_+3A_y">y</code></td>
<td>
<p> a numeric vector or an expression that will evaluate in
<code>data</code> to a numeric vector </p>
</td></tr>
<tr><td><code id="sortedXyData_+3A_data">data</code></td>
<td>
<p> an optional data frame in which to evaluate expressions
for <code>x</code> and <code>y</code>, if they are given as expressions </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>sortedXyData</code> object. This is a data frame with exactly
two numeric columns, named <code>x</code> and <code>y</code>.  The rows are
sorted so the <code>x</code> column is in increasing order.  Duplicate
<code>x</code> values are eliminated by averaging the corresponding <code>y</code>
values.
</p>


<h3>Author(s)</h3>

<p>Jos Pinheiro and Douglas Bates</p>


<h3>See Also</h3>

<p><code><a href="#topic+selfStart">selfStart</a></code>, <code><a href="#topic+NLSstClosestX">NLSstClosestX</a></code>,
<code><a href="#topic+NLSstLfAsymptote">NLSstLfAsymptote</a></code>, <code><a href="#topic+NLSstRtAsymptote">NLSstRtAsymptote</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>DNase.2 &lt;- DNase[ DNase$Run == "2", ]
sortedXyData( expression(log(conc)), expression(density), DNase.2 )
</code></pre>

<hr>
<h2 id='spec.ar'>Estimate Spectral Density of a Time Series from AR Fit</h2><span id='topic+spec.ar'></span>

<h3>Description</h3>

<p>Fits an AR model to <code>x</code> (or uses the existing fit) and computes
(and by default plots) the spectral density of the fitted model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spec.ar(x, n.freq, order = NULL, plot = TRUE, na.action = na.fail,
        method = "yule-walker", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spec.ar_+3A_x">x</code></td>
<td>
<p>A univariate (not yet:or multivariate) time series or the
result of a fit by <code><a href="#topic+ar">ar</a></code>.</p>
</td></tr>
<tr><td><code id="spec.ar_+3A_n.freq">n.freq</code></td>
<td>
<p>The number of points at which to plot.</p>
</td></tr>
<tr><td><code id="spec.ar_+3A_order">order</code></td>
<td>
<p>The order of the AR model to be fitted.  If omitted,
the order is chosen by AIC.</p>
</td></tr>
<tr><td><code id="spec.ar_+3A_plot">plot</code></td>
<td>
<p>Plot the periodogram?</p>
</td></tr>
<tr><td><code id="spec.ar_+3A_na.action">na.action</code></td>
<td>
<p><code>NA</code> action function.</p>
</td></tr>
<tr><td><code id="spec.ar_+3A_method">method</code></td>
<td>
<p><code>method</code> for <code><a href="#topic+ar">ar</a></code> fit.</p>
</td></tr>
<tr><td><code id="spec.ar_+3A_...">...</code></td>
<td>
<p>Graphical arguments passed to <code><a href="#topic+plot.spec">plot.spec</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"spec"</code>.
The result is returned invisibly if <code>plot</code> is true.
</p>


<h3>Warning</h3>

<p>Some authors, for example Thomson (1990), warn strongly
that AR spectra can be misleading.
</p>


<h3>Note</h3>

<p>The multivariate case is not yet implemented.</p>


<h3>References</h3>

<p>Thompson, D.J. (1990).
Time series analysis of Holocene climate data.
<em>Philosophical Transactions of the Royal Society of London Series
A</em>, <b>330</b>, 601&ndash;616.
<a href="https://doi.org/10.1098/rsta.1990.0041">doi:10.1098/rsta.1990.0041</a>.

</p>
<p>Venables, W.N. and Ripley, B.D. (2002) <em>Modern Applied
Statistics with S.</em> Fourth edition. Springer. (Especially
page 402.)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ar">ar</a></code>, <code><a href="#topic+spectrum">spectrum</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

spec.ar(lh)

spec.ar(ldeaths)
spec.ar(ldeaths, method = "burg")

spec.ar(log(lynx))
spec.ar(log(lynx), method = "burg", add = TRUE, col = "purple")
spec.ar(log(lynx), method = "mle", add = TRUE, col = "forest green")
spec.ar(log(lynx), method = "ols", add = TRUE, col = "blue")
</code></pre>

<hr>
<h2 id='spec.pgram'>Estimate Spectral Density of a Time Series by a Smoothed
Periodogram</h2><span id='topic+spec.pgram'></span>

<h3>Description</h3>

<p><code>spec.pgram</code> calculates the periodogram using a fast Fourier
transform, and optionally smooths the result with a series of
modified Daniell smoothers (moving averages giving half weight to
the end values).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spec.pgram(x, spans = NULL, kernel, taper = 0.1,
           pad = 0, fast = TRUE, demean = FALSE, detrend = TRUE,
           plot = TRUE, na.action = na.fail, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spec.pgram_+3A_x">x</code></td>
<td>
<p>univariate or multivariate time series.</p>
</td></tr>
<tr><td><code id="spec.pgram_+3A_spans">spans</code></td>
<td>
<p>vector of odd integers giving the widths of modified
Daniell smoothers to be used to smooth the periodogram.</p>
</td></tr>
<tr><td><code id="spec.pgram_+3A_kernel">kernel</code></td>
<td>
<p>alternatively, a kernel smoother of class
<code>"tskernel"</code>.</p>
</td></tr>
<tr><td><code id="spec.pgram_+3A_taper">taper</code></td>
<td>
<p>specifies the proportion of data to taper.  A split
cosine bell taper is applied to this proportion of the data at the
beginning and end of the series.</p>
</td></tr>
<tr><td><code id="spec.pgram_+3A_pad">pad</code></td>
<td>
<p>proportion of data to pad. Zeros are added to the end of
the series to increase its length by the proportion <code>pad</code>.</p>
</td></tr>
<tr><td><code id="spec.pgram_+3A_fast">fast</code></td>
<td>
<p>logical; if <code>TRUE</code>, pad the series to a highly composite
length.</p>
</td></tr>
<tr><td><code id="spec.pgram_+3A_demean">demean</code></td>
<td>
<p>logical. If <code>TRUE</code>, subtract the mean of the
series.</p>
</td></tr>
<tr><td><code id="spec.pgram_+3A_detrend">detrend</code></td>
<td>
<p>logical. If <code>TRUE</code>, remove a linear trend from
the series. This will also remove the mean.</p>
</td></tr>
<tr><td><code id="spec.pgram_+3A_plot">plot</code></td>
<td>
<p>plot the periodogram?</p>
</td></tr>
<tr><td><code id="spec.pgram_+3A_na.action">na.action</code></td>
<td>
<p><code>NA</code> action function.</p>
</td></tr>
<tr><td><code id="spec.pgram_+3A_...">...</code></td>
<td>
<p>graphical arguments passed to <code>plot.spec</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The raw periodogram is not a consistent estimator of the spectral density,
but adjacent values are asymptotically independent. Hence a consistent
estimator can be derived by smoothing the raw periodogram, assuming that
the spectral density is smooth.
</p>
<p>The series will be automatically padded with zeros until the series
length is a highly composite number in order to help the Fast Fourier
Transform. This is controlled by the <code>fast</code> and not the <code>pad</code>
argument.
</p>
<p>The periodogram at zero is in theory zero as the mean of the series
is removed (but this may be affected by tapering): it is replaced by
an interpolation of adjacent values during smoothing, and no value
is returned for that frequency.
</p>


<h3>Value</h3>

<p>A list object of class <code>"spec"</code> (see <code><a href="#topic+spectrum">spectrum</a></code>)
with the following additional components:
</p>
<table>
<tr><td><code>kernel</code></td>
<td>
<p>The <code>kernel</code> argument, or the kernel constructed
from <code>spans</code>.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>The distribution of the spectral density estimate can be
approximated by a (scaled) chi square distribution with <code>df</code> degrees
of freedom.</p>
</td></tr>
<tr><td><code>bandwidth</code></td>
<td>
<p>The equivalent bandwidth of the kernel smoother as
defined by Bloomfield (1976, page 201).</p>
</td></tr>
<tr><td><code>taper</code></td>
<td>
<p>The value of the <code>taper</code> argument.</p>
</td></tr>
<tr><td><code>pad</code></td>
<td>
<p>The value of the <code>pad</code> argument.</p>
</td></tr>
<tr><td><code>detrend</code></td>
<td>
<p>The value of the <code>detrend</code> argument.</p>
</td></tr>
<tr><td><code>demean</code></td>
<td>
<p>The value of the <code>demean</code> argument.</p>
</td></tr>
</table>
<p>The result is returned invisibly if <code>plot</code> is true.
</p>


<h3>Author(s)</h3>

<p>Originally Martyn Plummer; kernel smoothing by Adrian Trapletti,
synthesis by B.D. Ripley
</p>


<h3>References</h3>

<p>Bloomfield, P. (1976) <em>Fourier Analysis of Time Series: An
Introduction.</em> Wiley.
</p>
<p>Brockwell, P.J. and Davis, R.A. (1991) <em>Time Series: Theory and
Methods.</em> Second edition. Springer.
</p>
<p>Venables, W.N. and Ripley, B.D. (2002) <em>Modern Applied
Statistics with S.</em> Fourth edition. Springer.
(Especially pp. 392&ndash;7.)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spectrum">spectrum</a></code>, <code><a href="#topic+spec.taper">spec.taper</a></code>,
<code><a href="#topic+plot.spec">plot.spec</a></code>, <code><a href="#topic+fft">fft</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

## Examples from Venables &amp; Ripley
spectrum(ldeaths)
spectrum(ldeaths, spans = c(3,5))
spectrum(ldeaths, spans = c(5,7))
spectrum(mdeaths, spans = c(3,3))
spectrum(fdeaths, spans = c(3,3))

## bivariate example
mfdeaths.spc &lt;- spec.pgram(ts.union(mdeaths, fdeaths), spans = c(3,3))
# plots marginal spectra: now plot coherency and phase
plot(mfdeaths.spc, plot.type = "coherency")
plot(mfdeaths.spc, plot.type = "phase")

## now impose a lack of alignment
mfdeaths.spc &lt;- spec.pgram(ts.intersect(mdeaths, lag(fdeaths, 4)),
   spans = c(3,3), plot = FALSE)
plot(mfdeaths.spc, plot.type = "coherency")
plot(mfdeaths.spc, plot.type = "phase")

stocks.spc &lt;- spectrum(EuStockMarkets, kernel("daniell", c(30,50)),
                       plot = FALSE)
plot(stocks.spc, plot.type = "marginal") # the default type
plot(stocks.spc, plot.type = "coherency")
plot(stocks.spc, plot.type = "phase")

sales.spc &lt;- spectrum(ts.union(BJsales, BJsales.lead),
                      kernel("modified.daniell", c(5,7)))
plot(sales.spc, plot.type = "coherency")
plot(sales.spc, plot.type = "phase")
</code></pre>

<hr>
<h2 id='spec.taper'>Taper a Time Series by a Cosine Bell</h2><span id='topic+spec.taper'></span>

<h3>Description</h3>

<p>Apply a cosine-bell taper to a time series.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spec.taper(x, p = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spec.taper_+3A_x">x</code></td>
<td>
<p>A univariate or multivariate time series</p>
</td></tr>
<tr><td><code id="spec.taper_+3A_p">p</code></td>
<td>
<p>The proportion to be tapered at each end of the series,
either a scalar (giving the proportion for all series)
or a vector of the length of the number of series (giving the
proportion for each series).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cosine-bell taper is applied to the first and last <code>p[i]</code>
observations of time series <code>x[, i]</code>.
</p>


<h3>Value</h3>

<p>A new time series object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spec.pgram">spec.pgram</a></code>, <code><a href="#topic+cpgram">cpgram</a></code>
</p>

<hr>
<h2 id='spectrum'>Spectral Density Estimation</h2><span id='topic+spectrum'></span><span id='topic+spec'></span>

<h3>Description</h3>

<p>The <code>spectrum</code> function estimates the spectral density of a
time series.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spectrum(x, ..., method = c("pgram", "ar"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spectrum_+3A_x">x</code></td>
<td>
<p>A univariate or multivariate time series.</p>
</td></tr>
<tr><td><code id="spectrum_+3A_method">method</code></td>
<td>
<p>String specifying the method used to estimate the
spectral density.  Allowed methods are <code>"pgram"</code> (the default)
and <code>"ar"</code>.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="spectrum_+3A_...">...</code></td>
<td>
<p>Further arguments to specific spec methods or
<code>plot.spec</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>spectrum</code> is a wrapper function which calls the methods
<code><a href="#topic+spec.pgram">spec.pgram</a></code> and <code><a href="#topic+spec.ar">spec.ar</a></code>.
</p>
<p>The spectrum here is defined (for historical compatibility) with
scaling <code>1/<a href="#topic+frequency">frequency</a>(x)</code>.  This makes the spectral density
a density over the range <code>(-frequency(x)/2, +frequency(x)/2]</code>,
whereas a more common scaling is <code class="reqn">2\pi</code> and range <code class="reqn">(-0.5,
  0.5]</code> (e.g., Bloomfield) or 1 and range <code class="reqn">(-\pi, \pi]</code>.
</p>
<p>If available, a confidence interval will be plotted by
<code>plot.spec</code>: this is asymmetric, and the width of the centre
mark indicates the equivalent bandwidth.
</p>


<h3>Value</h3>

<p>An object of class <code>"spec"</code>, which is a list containing at
least the following components:
</p>
<table>
<tr><td><code>freq</code></td>
<td>
<p>vector of frequencies at which the spectral
density is estimated. (Possibly approximate Fourier frequencies.)
The units are the reciprocal of cycles per unit time (and not per
observation spacing): see &lsquo;Details&rsquo; below.</p>
</td></tr>
<tr><td><code>spec</code></td>
<td>
<p>Vector (for univariate series) or matrix (for multivariate
series) of estimates of the spectral density at frequencies
corresponding to <code>freq</code>.</p>
</td></tr>
<tr><td><code>coh</code></td>
<td>
<p><code>NULL</code> for univariate series. For multivariate time
series, a matrix containing the <em>squared</em> coherency
between different
series. Column <code class="reqn"> i + (j - 1) * (j - 2)/2</code> of <code>coh</code>
contains the squared coherency between columns <code class="reqn">i</code> and <code class="reqn">j</code>
of <code>x</code>, where <code class="reqn">i &lt; j</code>.</p>
</td></tr>
<tr><td><code>phase</code></td>
<td>
<p><code>NULL</code> for univariate series. For multivariate
time series a matrix containing the cross-spectrum phase between
different series. The format is the same as <code>coh</code>.</p>
</td></tr>
<tr><td><code>series</code></td>
<td>
<p>The name of the time series.</p>
</td></tr>
<tr><td><code>snames</code></td>
<td>
<p>For multivariate input, the names of the component series.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>The method used to calculate the spectrum.</p>
</td></tr>
</table>
<p>The result is returned invisibly if <code>plot</code> is true.
</p>


<h3>Note</h3>

<p>The default plot for objects of class <code>"spec"</code> is quite complex,
including an error bar and default title, subtitle and axis
labels.  The defaults can all be overridden by supplying the
appropriate graphical parameters.
</p>


<h3>Author(s)</h3>

<p>Martyn Plummer, B.D. Ripley</p>


<h3>References</h3>

<p>Bloomfield, P. (1976) <em>Fourier Analysis of Time Series: An
Introduction.</em> Wiley.
</p>
<p>Brockwell, P. J. and Davis, R. A. (1991) <em>Time Series: Theory and
Methods.</em> Second edition. Springer.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002) <em>Modern Applied
Statistics with S-PLUS.</em> Fourth edition. Springer. (Especially
pages 392&ndash;7.)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spec.ar">spec.ar</a></code>,
<code><a href="#topic+spec.pgram">spec.pgram</a></code>;
<code><a href="#topic+plot.spec">plot.spec</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

## Examples from Venables &amp; Ripley
## spec.pgram
par(mfrow = c(2,2))
spectrum(lh)
spectrum(lh, spans = 3)
spectrum(lh, spans = c(3,3))
spectrum(lh, spans = c(3,5))

spectrum(ldeaths)
spectrum(ldeaths, spans = c(3,3))
spectrum(ldeaths, spans = c(3,5))
spectrum(ldeaths, spans = c(5,7))
spectrum(ldeaths, spans = c(5,7), log = "dB", ci = 0.8)

# for multivariate examples see the help for spec.pgram

## spec.ar
spectrum(lh, method = "ar")
spectrum(ldeaths, method = "ar")
</code></pre>

<hr>
<h2 id='splinefun'>Interpolating Splines</h2><span id='topic+spline'></span><span id='topic+splinefun'></span><span id='topic+splinefunH'></span>

<h3>Description</h3>

<p>Perform cubic (or Hermite) spline interpolation of given data points,
returning either a list of points obtained by the interpolation or a
<em>function</em> performing the interpolation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>splinefun(x, y = NULL,
          method = c("fmm", "periodic", "natural", "monoH.FC", "hyman"),
          ties = mean)

spline(x, y = NULL, n = 3*length(x), method = "fmm",
       xmin = min(x), xmax = max(x), xout, ties = mean)

splinefunH(x, y, m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="splinefun_+3A_x">x</code>, <code id="splinefun_+3A_y">y</code></td>
<td>
<p>vectors giving the coordinates of the points to be
interpolated.  Alternatively a single plotting structure can be
specified: see <code><a href="grDevices.html#topic+xy.coords">xy.coords</a></code>.
</p>
<p><code>y</code> must be increasing or decreasing for <code>method = "hyman"</code>.
</p>
</td></tr>
<tr><td><code id="splinefun_+3A_m">m</code></td>
<td>
<p>(for <code>splinefunH()</code>): vector of <em>slopes</em>
<code class="reqn">m_i</code> at the points <code class="reqn">(x_i,y_i)</code>; these
together determine the <b>H</b>ermite &ldquo;spline&rdquo; which is
piecewise cubic, (only) <em>once</em> differentiable continuously.</p>
</td></tr>
<tr><td><code id="splinefun_+3A_method">method</code></td>
<td>
<p>specifies the type of spline to be used.  Possible
values are <code>"fmm"</code>, <code>"natural"</code>, <code>"periodic"</code>,
<code>"monoH.FC"</code> and <code>"hyman"</code>.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="splinefun_+3A_n">n</code></td>
<td>
<p>if <code>xout</code> is left unspecified, interpolation takes place
at <code>n</code> equally spaced points spanning the interval
[<code>xmin</code>, <code>xmax</code>].</p>
</td></tr>
<tr><td><code id="splinefun_+3A_xmin">xmin</code>, <code id="splinefun_+3A_xmax">xmax</code></td>
<td>
<p>left-hand and right-hand endpoint of the
interpolation interval (when <code>xout</code> is unspecified).</p>
</td></tr>
<tr><td><code id="splinefun_+3A_xout">xout</code></td>
<td>
<p>an optional set of values specifying where interpolation
is to take place.</p>
</td></tr>
<tr><td><code id="splinefun_+3A_ties">ties</code></td>
<td>
<p>handling of tied <code>x</code> values.  The string
<code>"ordered"</code> or a function (or the name of a function) taking a
single vector argument and returning a single number or a length-2
<code><a href="base.html#topic+list">list</a></code> of both, see <code><a href="#topic+approx">approx</a></code> and its
&lsquo;Details&rsquo; section, and the example below.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The inputs can contain missing values which are deleted, so at least
one complete <code>(x, y)</code> pair is required.
If <code>method = "fmm"</code>, the spline used is that of
Forsythe, Malcolm and Moler
(an exact cubic is fitted through the four points at each
end of the data, and this is used to determine the end conditions).
Natural splines are used when <code>method = "natural"</code>, and periodic
splines when <code>method = "periodic"</code>.
</p>
<p>The method <code>"monoH.FC"</code> computes a <em>monotone</em> Hermite spline
according to the method of Fritsch and Carlson.  It does so by
determining slopes such that the Hermite spline, determined by
<code class="reqn">(x_i,y_i,m_i)</code>, is monotone (increasing or
decreasing) <b>iff</b> the data are.
</p>
<p>Method <code>"hyman"</code> computes a <em>monotone</em> cubic spline using
Hyman filtering of an <code>method = "fmm"</code> fit for strictly monotonic
inputs.
</p>
<p>These interpolation splines can also be used for extrapolation, that is
prediction at points outside the range of <code>x</code>.  Extrapolation
makes little sense for <code>method = "fmm"</code>; for natural splines it
is linear using the slope of the interpolating curve at the nearest
data point.
</p>


<h3>Value</h3>

<p><code>spline</code> returns a list containing components <code>x</code> and
<code>y</code> which give the ordinates where interpolation took place and
the interpolated values.
</p>
<p><code>splinefun</code> returns a function with formal arguments <code>x</code> and
<code>deriv</code>, the latter defaulting to zero.  This function
can be used to evaluate the interpolating cubic spline
(<code>deriv</code> = 0), or its derivatives (<code>deriv</code> = 1, 2, 3) at the
points <code>x</code>, where the spline function interpolates the data
points originally specified.  It uses data stored in its environment
when it was created, the details of which are subject to change.
</p>


<h3>Warning</h3>

<p>The value returned by <code>splinefun</code> contains references to the code
in the current version of <span class="rlang"><b>R</b></span>: it is not intended to be saved and
loaded into a different <span class="rlang"><b>R</b></span> session.  This is safer in <span class="rlang"><b>R</b></span> &gt;= 3.0.0.
</p>


<h3>Author(s)</h3>

<p>R Core Team.
</p>
<p>Simon Wood for the original code for Hyman filtering.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988).
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>
<p>Dougherty, R. L., Edelman, A. and Hyman, J. M. (1989)
Positivity-, monotonicity-, or convexity-preserving cubic and quintic
Hermite interpolation.
<em>Mathematics of Computation</em>, <b>52</b>, 471&ndash;494.
<a href="https://doi.org/10.1090/S0025-5718-1989-0962209-1">doi:10.1090/S0025-5718-1989-0962209-1</a>.
</p>
<p>Forsythe, G. E., Malcolm, M. A. and Moler, C. B. (1977).
<em>Computer Methods for Mathematical Computations</em>.
Wiley.
</p>
<p>Fritsch, F. N. and Carlson, R. E. (1980).
Monotone piecewise cubic interpolation.
<em>SIAM Journal on Numerical Analysis</em>, <b>17</b>, 238&ndash;246.
<a href="https://doi.org/10.1137/0717021">doi:10.1137/0717021</a>.
</p>
<p>Hyman, J. M. (1983).
Accurate monotonicity preserving cubic interpolation.
<em>SIAM Journal on Scientific and Statistical Computing</em>, <b>4</b>,
645&ndash;654.
<a href="https://doi.org/10.1137/0904045">doi:10.1137/0904045</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+approx">approx</a></code> and <code><a href="#topic+approxfun">approxfun</a></code> for constant and
linear interpolation.
</p>
<p>Package <span class="pkg">splines</span>, especially <code><a href="splines.html#topic+interpSpline">interpSpline</a></code>
and <code><a href="splines.html#topic+periodicSpline">periodicSpline</a></code> for interpolation splines.
That package also generates spline bases that can be used for
regression splines.
</p>
<p><code><a href="#topic+smooth.spline">smooth.spline</a></code> for smoothing splines.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

op &lt;- par(mfrow = c(2,1), mgp = c(2,.8,0), mar = 0.1+c(3,3,3,1))
n &lt;- 9
x &lt;- 1:n
y &lt;- rnorm(n)
plot(x, y, main = paste("spline[fun](.) through", n, "points"))
lines(spline(x, y))
lines(spline(x, y, n = 201), col = 2)

y &lt;- (x-6)^2
plot(x, y, main = "spline(.) -- 3 methods")
lines(spline(x, y, n = 201), col = 2)
lines(spline(x, y, n = 201, method = "natural"), col = 3)
lines(spline(x, y, n = 201, method = "periodic"), col = 4)
legend(6, 25, c("fmm","natural","periodic"), col = 2:4, lty = 1)

y &lt;- sin((x-0.5)*pi)
f &lt;- splinefun(x, y)
ls(envir = environment(f))
splinecoef &lt;- get("z", envir = environment(f))
curve(f(x), 1, 10, col = "green", lwd = 1.5)
points(splinecoef, col = "purple", cex = 2)
curve(f(x, deriv = 1), 1, 10, col = 2, lwd = 1.5)
curve(f(x, deriv = 2), 1, 10, col = 2, lwd = 1.5, n = 401)
curve(f(x, deriv = 3), 1, 10, col = 2, lwd = 1.5, n = 401)
par(op)

## Manual spline evaluation --- demo the coefficients :
.x &lt;- splinecoef$x
u &lt;- seq(3, 6, by = 0.25)
(ii &lt;- findInterval(u, .x))
dx &lt;- u - .x[ii]
f.u &lt;- with(splinecoef,
            y[ii] + dx*(b[ii] + dx*(c[ii] + dx* d[ii])))
stopifnot(all.equal(f(u), f.u))

## An example with ties (non-unique  x values):
set.seed(1); x &lt;- round(rnorm(30), 1); y &lt;- sin(pi * x) + rnorm(30)/10
plot(x, y, main = "spline(x,y)  when x has ties")
lines(spline(x, y, n = 201), col = 2)
## visualizes the non-unique ones:
tx &lt;- table(x); mx &lt;- as.numeric(names(tx[tx &gt; 1]))
ry &lt;- matrix(unlist(tapply(y, match(x, mx), range, simplify = FALSE)),
             ncol = 2, byrow = TRUE)
segments(mx, ry[, 1], mx, ry[, 2], col = "blue", lwd = 2)

## Another example with sorted x, but ties:
set.seed(8); x &lt;- sort(round(rnorm(30), 1)); y &lt;- round(sin(pi * x) + rnorm(30)/10, 3)
summary(diff(x) == 0) # -&gt; 7 duplicated x-values
str(spline(x, y, n = 201, ties="ordered")) # all '$y' entries are NaN
## The default (ties=mean) is ok, but most efficient to use instead is
sxyo &lt;- spline(x, y, n = 201, ties= list("ordered", mean))
sapply(sxyo, summary)# all fine now
plot(x, y, main = "spline(x,y, ties=list(\"ordered\", mean))  for when x has ties")
lines(sxyo, col="blue")

## An example of monotone interpolation
n &lt;- 20
set.seed(11)
x. &lt;- sort(runif(n)) ; y. &lt;- cumsum(abs(rnorm(n)))
plot(x., y.)
curve(splinefun(x., y.)(x), add = TRUE, col = 2, n = 1001)
curve(splinefun(x., y., method = "monoH.FC")(x), add = TRUE, col = 3, n = 1001)
curve(splinefun(x., y., method = "hyman")   (x), add = TRUE, col = 4, n = 1001)
legend("topleft",
       paste0("splinefun( \"", c("fmm", "monoH.FC", "hyman"), "\" )"),
       col = 2:4, lty = 1, bty = "n")

## and one from Fritsch and Carlson (1980), Dougherty et al (1989)
x. &lt;- c(7.09, 8.09, 8.19, 8.7, 9.2, 10, 12, 15, 20)
f &lt;- c(0, 2.76429e-5, 4.37498e-2, 0.169183, 0.469428, 0.943740,
       0.998636, 0.999919, 0.999994)
s0 &lt;- splinefun(x., f)
s1 &lt;- splinefun(x., f, method = "monoH.FC")
s2 &lt;- splinefun(x., f, method = "hyman")
plot(x., f, ylim = c(-0.2, 1.2))
curve(s0(x), add = TRUE, col = 2, n = 1001) -&gt; m0
curve(s1(x), add = TRUE, col = 3, n = 1001)
curve(s2(x), add = TRUE, col = 4, n = 1001)
legend("right",
       paste0("splinefun( \"", c("fmm", "monoH.FC", "hyman"), "\" )"),
       col = 2:4, lty = 1, bty = "n")

## they seem identical, but are not quite:
xx &lt;- m0$x
plot(xx, s1(xx) - s2(xx), type = "l",  col = 2, lwd = 2,
     main = "Difference   monoH.FC - hyman"); abline(h = 0, lty = 3)

x &lt;- xx[xx &lt; 10.2] ## full range: x &lt;- xx .. does not show enough
ccol &lt;- adjustcolor(2:4, 0.8)
matplot(x, cbind(s0(x, deriv = 2), s1(x, deriv = 2), s2(x, deriv = 2))^2,
        lwd = 2, col = ccol, type = "l", ylab = quote({{f*second}(x)}^2),
        main = expression({{f*second}(x)}^2 ~" for the three 'splines'"))
legend("topright",
       paste0("splinefun( \"", c("fmm", "monoH.FC", "hyman"), "\" )"),
       lwd = 2, col  =  ccol, lty = 1:3, bty = "n")
## --&gt; "hyman" has slightly smaller  Integral f''(x)^2 dx  than "FC",
## here, and both are 'much worse' than the regular fmm spline.
</code></pre>

<hr>
<h2 id='SSasymp'>Self-Starting <code>nls</code> Asymptotic Model</h2><span id='topic+SSasymp'></span>

<h3>Description</h3>

<p>This <code>selfStart</code> model evaluates the asymptotic regression
function and its gradient.  It has an <code>initial</code> attribute that
will evaluate initial estimates of the parameters <code>Asym</code>, <code>R0</code>,
and <code>lrc</code> for a given set of data.
</p>
<p>Note that <code><a href="#topic+SSweibull">SSweibull</a>()</code> generalizes this asymptotic model
with an extra parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSasymp(input, Asym, R0, lrc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSasymp_+3A_input">input</code></td>
<td>
<p>a numeric vector of values at which to evaluate the model.</p>
</td></tr>
<tr><td><code id="SSasymp_+3A_asym">Asym</code></td>
<td>
<p>a numeric parameter representing the horizontal asymptote on
the right side (very large values of <code>input</code>).</p>
</td></tr>
<tr><td><code id="SSasymp_+3A_r0">R0</code></td>
<td>
<p>a numeric parameter representing the response when
<code>input</code> is zero.</p>
</td></tr>
<tr><td><code id="SSasymp_+3A_lrc">lrc</code></td>
<td>
<p>a numeric parameter representing the natural logarithm of
the rate constant.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric vector of the same length as <code>input</code>.  It is the value of
the expression <code>Asym+(R0-Asym)*exp(-exp(lrc)*input)</code>.  If all of
the arguments <code>Asym</code>, <code>R0</code>, and <code>lrc</code> are
names of objects, the gradient matrix with respect to these names is
attached as an attribute named <code>gradient</code>.
</p>


<h3>Author(s)</h3>

<p>Jos Pinheiro and Douglas Bates</p>


<h3>See Also</h3>

<p><code><a href="#topic+nls">nls</a></code>, <code><a href="#topic+selfStart">selfStart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
Lob.329 &lt;- Loblolly[ Loblolly$Seed == "329", ]
SSasymp( Lob.329$age, 100, -8.5, -3.2 )   # response only
local({
  Asym &lt;- 100 ; resp0 &lt;- -8.5 ; lrc &lt;- -3.2
  SSasymp( Lob.329$age, Asym, resp0, lrc) # response _and_ gradient
})
getInitial(height ~ SSasymp( age, Asym, resp0, lrc), data = Lob.329)
## Initial values are in fact the converged values
fm1 &lt;- nls(height ~ SSasymp( age, Asym, resp0, lrc), data = Lob.329)
summary(fm1)

## Visualize the SSasymp()  model  parametrization :

  xx &lt;- seq(-.3, 5, length.out = 101)
  ##  Asym + (R0-Asym) * exp(-exp(lrc)* x) :
  yy &lt;- 5 - 4 * exp(-xx / exp(3/4))
  stopifnot( all.equal(yy, SSasymp(xx, Asym = 5, R0 = 1, lrc = -3/4)) )
  require(graphics)
  op &lt;- par(mar = c(0, .2, 4.1, 0))
  plot(xx, yy, type = "l", axes = FALSE, ylim = c(0,5.2), xlim = c(-.3, 5),
       xlab = "", ylab = "", lwd = 2,
       main = quote("Parameters in the SSasymp model " ~
                    {f[phi](x) == phi[1] + (phi[2]-phi[1])*~e^{-e^{phi[3]}*~x}}))
  mtext(quote(list(phi[1] == "Asym", phi[2] == "R0", phi[3] == "lrc")))
  usr &lt;- par("usr")
  arrows(usr[1], 0, usr[2], 0, length = 0.1, angle = 25)
  arrows(0, usr[3], 0, usr[4], length = 0.1, angle = 25)
  text(usr[2] - 0.2, 0.1, "x", adj = c(1, 0))
  text(     -0.1, usr[4], "y", adj = c(1, 1))
  abline(h = 5, lty = 3)
  arrows(c(0.35, 0.65), 1,
         c(0  ,  1   ), 1, length = 0.08, angle = 25); text(0.5, 1, quote(1))
  y0 &lt;- 1 + 4*exp(-3/4) ; t.5 &lt;- log(2) / exp(-3/4) ; AR2 &lt;- 3 # (Asym + R0)/2
  segments(c(1, 1), c( 1, y0),
           c(1, 0), c(y0,  1),  lty = 2, lwd = 0.75)
  text(1.1, 1/2+y0/2, quote((phi[1]-phi[2])*e^phi[3]), adj = c(0,.5))
  axis(2, at = c(1,AR2,5), labels= expression(phi[2], frac(phi[1]+phi[2],2), phi[1]),
       pos=0, las=1)
  arrows(c(.6,t.5-.6), AR2,
         c(0, t.5   ), AR2, length = 0.08, angle = 25)
  text(   t.5/2,   AR2, quote(t[0.5]))
  text(   t.5 +.4, AR2,
       quote({f(t[0.5]) == frac(phi[1]+phi[2],2)}~{} %=&gt;% {}~~
                {t[0.5] == frac(log(2), e^{phi[3]})}), adj = c(0, 0.5))
  par(op)
</code></pre>

<hr>
<h2 id='SSasympOff'>Self-Starting <code>nls</code> Asymptotic Model with an Offset</h2><span id='topic+SSasympOff'></span>

<h3>Description</h3>

<p>This <code>selfStart</code> model evaluates an alternative parametrization
of the asymptotic
regression function and the gradient with respect to those parameters.
It has an <code>initial</code>
attribute that creates initial estimates of the parameters
<code>Asym</code>, <code>lrc</code>, and <code>c0</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSasympOff(input, Asym, lrc, c0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSasympOff_+3A_input">input</code></td>
<td>
<p>a numeric vector of values at which to evaluate the model.</p>
</td></tr>
<tr><td><code id="SSasympOff_+3A_asym">Asym</code></td>
<td>
<p>a numeric parameter representing the horizontal asymptote on
the right side (very large values of <code>input</code>).</p>
</td></tr>
<tr><td><code id="SSasympOff_+3A_lrc">lrc</code></td>
<td>
<p>a numeric parameter representing the natural logarithm of
the rate constant.</p>
</td></tr>
<tr><td><code id="SSasympOff_+3A_c0">c0</code></td>
<td>
<p>a numeric parameter representing the <code>input</code> for which the
response is zero.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric vector of the same length as <code>input</code>.  It is the value of
the expression <code>Asym*(1 - exp(-exp(lrc)*(input - c0)))</code>.  If all of
the arguments <code>Asym</code>, <code>lrc</code>, and <code>c0</code> are
names of objects, the gradient matrix with respect to these names is
attached as an attribute named <code>gradient</code>.
</p>


<h3>Author(s)</h3>

<p>Jos Pinheiro and Douglas Bates</p>


<h3>See Also</h3>

<p><code><a href="#topic+nls">nls</a></code>, <code><a href="#topic+selfStart">selfStart</a></code>;
<code>example(SSasympOff)</code> gives graph showing the <code>SSasympOff</code>
parametrization.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>CO2.Qn1 &lt;- CO2[CO2$Plant == "Qn1", ]
SSasympOff(CO2.Qn1$conc, 32, -4, 43)  # response only
local({  Asym &lt;- 32; lrc &lt;- -4; c0 &lt;- 43
  SSasympOff(CO2.Qn1$conc, Asym, lrc, c0) # response and gradient
})
getInitial(uptake ~ SSasympOff(conc, Asym, lrc, c0), data = CO2.Qn1)
## Initial values are in fact the converged values
fm1 &lt;- nls(uptake ~ SSasympOff(conc, Asym, lrc, c0), data = CO2.Qn1)
summary(fm1)

## Visualize the SSasympOff()  model  parametrization :

  xx &lt;- seq(0.25, 8,  by=1/16)
  yy &lt;- 5 * (1 -  exp(-(xx - 3/4)*0.4))
  stopifnot( all.equal(yy, SSasympOff(xx, Asym = 5, lrc = log(0.4), c0 = 3/4)) )
  require(graphics)
  op &lt;- par(mar = c(0, 0, 4.0, 0))
  plot(xx, yy, type = "l", axes = FALSE, ylim = c(-.5,6), xlim = c(-1, 8),
       xlab = "", ylab = "", lwd = 2,
       main = "Parameters in the SSasympOff model")
  mtext(quote(list(phi[1] == "Asym", phi[2] == "lrc", phi[3] == "c0")))
  usr &lt;- par("usr")
  arrows(usr[1], 0, usr[2], 0, length = 0.1, angle = 25)
  arrows(0, usr[3], 0, usr[4], length = 0.1, angle = 25)
  text(usr[2] - 0.2, 0.1, "x", adj = c(1, 0))
  text(     -0.1, usr[4], "y", adj = c(1, 1))
  abline(h = 5, lty = 3)
  arrows(-0.8, c(2.1, 2.9),
         -0.8, c(0  , 5  ), length = 0.1, angle = 25)
  text  (-0.8, 2.5, quote(phi[1]))
  segments(3/4, -.2, 3/4, 1.6, lty = 2)
  text    (3/4,    c(-.3, 1.7), quote(phi[3]))
  arrows(c(1.1, 1.4), -.15,
         c(3/4, 7/4), -.15, length = 0.07, angle = 25)
  text    (3/4 + 1/2, -.15, quote(1))
  segments(c(3/4, 7/4, 7/4), c(0, 0, 2),   # 5 * exp(log(0.4)) = 2
           c(7/4, 7/4, 3/4), c(0, 2, 0),  lty = 2, lwd = 2)
  text(      7/4 +.1, 2./2, quote(phi[1]*e^phi[2]), adj = c(0, .5))
  par(op)
</code></pre>

<hr>
<h2 id='SSasympOrig'>Self-Starting <code>nls</code> Asymptotic Model through the Origin</h2><span id='topic+SSasympOrig'></span>

<h3>Description</h3>

<p>This <code><a href="#topic+selfStart">selfStart</a></code> model evaluates the asymptotic regression
function through the origin and its gradient.  It has an
<code>initial</code> attribute that will evaluate initial estimates of the
parameters <code>Asym</code> and <code>lrc</code> for a given set of data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSasympOrig(input, Asym, lrc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSasympOrig_+3A_input">input</code></td>
<td>
<p>a numeric vector of values at which to evaluate the model.</p>
</td></tr>
<tr><td><code id="SSasympOrig_+3A_asym">Asym</code></td>
<td>
<p>a numeric parameter representing the horizontal asymptote.</p>
</td></tr>
<tr><td><code id="SSasympOrig_+3A_lrc">lrc</code></td>
<td>
<p>a numeric parameter representing the natural logarithm of
the rate constant.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric vector of the same length as <code>input</code>.  It is the value of
the expression <code>Asym*(1 - exp(-exp(lrc)*input))</code>.  If all of
the arguments <code>Asym</code> and <code>lrc</code> are
names of objects, the gradient matrix with respect to these names is
attached as an attribute named <code>gradient</code>.
</p>


<h3>Author(s)</h3>

<p>Jos Pinheiro and Douglas Bates</p>


<h3>See Also</h3>

<p><code><a href="#topic+nls">nls</a></code>, <code><a href="#topic+selfStart">selfStart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
Lob.329 &lt;- Loblolly[ Loblolly$Seed == "329", ]
SSasympOrig(Lob.329$age, 100, -3.2)  # response only
local({   Asym &lt;- 100; lrc &lt;- -3.2
  SSasympOrig(Lob.329$age, Asym, lrc) # response and gradient
})
getInitial(height ~ SSasympOrig(age, Asym, lrc), data = Lob.329)
## Initial values are in fact the converged values
fm1 &lt;- nls(height ~ SSasympOrig(age, Asym, lrc), data = Lob.329)
summary(fm1)


## Visualize the SSasympOrig()  model  parametrization :

  xx &lt;- seq(0, 5, length.out = 101)
  yy &lt;- 5 * (1- exp(-xx * log(2)))
  stopifnot( all.equal(yy, SSasympOrig(xx, Asym = 5, lrc = log(log(2)))) )

  require(graphics)
  op &lt;- par(mar = c(0, 0, 3.5, 0))
  plot(xx, yy, type = "l", axes = FALSE, ylim = c(0,5), xlim = c(-1/4, 5),
       xlab = "", ylab = "", lwd = 2,
       main = quote("Parameters in the SSasympOrig model"~~ f[phi](x)))
  mtext(quote(list(phi[1] == "Asym", phi[2] == "lrc")))
  usr &lt;- par("usr")
  arrows(usr[1], 0, usr[2], 0, length = 0.1, angle = 25)
  arrows(0, usr[3], 0, usr[4], length = 0.1, angle = 25)
  text(usr[2] - 0.2, 0.1, "x", adj = c(1, 0))
  text(   -0.1,   usr[4], "y", adj = c(1, 1))
  abline(h = 5, lty = 3)
  axis(2, at = 5*c(1/2,1), labels= expression(frac(phi[1],2), phi[1]), pos=0, las=1)
  arrows(c(.3,.7), 5/2,
         c(0, 1 ), 5/2, length = 0.08, angle = 25)
  text(   0.5,     5/2, quote(t[0.5]))
  text(   1 +.4,   5/2,
       quote({f(t[0.5]) == frac(phi[1],2)}~{} %=&gt;% {}~~{t[0.5] == frac(log(2), e^{phi[2]})}),
       adj = c(0, 0.5))
  par(op)
</code></pre>

<hr>
<h2 id='SSbiexp'>Self-Starting <code>nls</code> Biexponential Model</h2><span id='topic+SSbiexp'></span>

<h3>Description</h3>

<p>This <code>selfStart</code> model evaluates the biexponential model function
and its gradient.  It has an <code>initial</code> attribute that
creates initial estimates of the parameters <code>A1</code>, <code>lrc1</code>,
<code>A2</code>, and <code>lrc2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSbiexp(input, A1, lrc1, A2, lrc2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSbiexp_+3A_input">input</code></td>
<td>
<p>a numeric vector of values at which to evaluate the model.</p>
</td></tr>
<tr><td><code id="SSbiexp_+3A_a1">A1</code></td>
<td>
<p>a numeric parameter representing the multiplier of the first
exponential.</p>
</td></tr>
<tr><td><code id="SSbiexp_+3A_lrc1">lrc1</code></td>
<td>
<p>a numeric parameter representing the natural logarithm of
the rate constant of the first exponential.</p>
</td></tr>
<tr><td><code id="SSbiexp_+3A_a2">A2</code></td>
<td>
<p>a numeric parameter representing the multiplier of the second
exponential.</p>
</td></tr>
<tr><td><code id="SSbiexp_+3A_lrc2">lrc2</code></td>
<td>
<p>a numeric parameter representing the natural logarithm of
the rate constant of the second exponential.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric vector of the same length as <code>input</code>.  It is the value of
the expression
<code>A1*exp(-exp(lrc1)*input)+A2*exp(-exp(lrc2)*input)</code>.
If all of the arguments <code>A1</code>, <code>lrc1</code>, <code>A2</code>, and
<code>lrc2</code> are names of objects, the gradient matrix with respect to
these names is attached as an attribute named <code>gradient</code>.
</p>


<h3>Author(s)</h3>

<p>Jos Pinheiro and Douglas Bates</p>


<h3>See Also</h3>

<p><code><a href="#topic+nls">nls</a></code>, <code><a href="#topic+selfStart">selfStart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Indo.1 &lt;- Indometh[Indometh$Subject == 1, ]
SSbiexp( Indo.1$time, 3, 1, 0.6, -1.3 )  # response only
A1 &lt;- 3; lrc1 &lt;- 1; A2 &lt;- 0.6; lrc2 &lt;- -1.3
SSbiexp( Indo.1$time, A1, lrc1, A2, lrc2 ) # response and gradient
print(getInitial(conc ~ SSbiexp(time, A1, lrc1, A2, lrc2), data = Indo.1),
      digits = 5)
## Initial values are in fact the converged values
fm1 &lt;- nls(conc ~ SSbiexp(time, A1, lrc1, A2, lrc2), data = Indo.1)
summary(fm1)

## Show the model components visually
  require(graphics)

  xx &lt;- seq(0, 5, length.out = 101)
  y1 &lt;- 3.5 * exp(-4*xx)
  y2 &lt;- 1.5 * exp(-xx)
  plot(xx, y1 + y2, type = "l", lwd=2, ylim = c(-0.2,6), xlim = c(0, 5),
       main = "Components of the SSbiexp model")
  lines(xx, y1, lty = 2, col="tomato"); abline(v=0, h=0, col="gray40")
  lines(xx, y2, lty = 3, col="blue2" )
  legend("topright", c("y1+y2", "y1 = 3.5 * exp(-4*x)", "y2 = 1.5 * exp(-x)"),
         lty=1:3, col=c("black","tomato","blue2"), bty="n")
  axis(2, pos=0, at = c(3.5, 1.5), labels = c("A1","A2"), las=2)

## and how you could have got their sum via SSbiexp():
  ySS &lt;- SSbiexp(xx, 3.5, log(4), 1.5, log(1))
  ##                      ---          ---
  stopifnot(all.equal(y1+y2, ySS, tolerance = 1e-15))

## Show a no-noise example
datN &lt;- data.frame(time = (0:600)/64)
datN$conc &lt;- predict(fm1, newdata=datN)
plot(conc ~ time, data=datN) # perfect, no noise

## Fails by default (scaleOffset=0) on most platforms {also after increasing maxiter !}
## Not run: 
        nls(conc ~ SSbiexp(time, A1, lrc1, A2, lrc2), data = datN, trace=TRUE)
## End(Not run)

fmX1 &lt;- nls(conc ~ SSbiexp(time, A1, lrc1, A2, lrc2), data = datN,
            control = list(scaleOffset=1))
fmX  &lt;- nls(conc ~ SSbiexp(time, A1, lrc1, A2, lrc2), data = datN,
            control = list(scaleOffset=1, printEval=TRUE, tol=1e-11, nDcentral=TRUE), trace=TRUE)
all.equal(coef(fm1), coef(fmX1), tolerance=0) # ... rel.diff.: 1.57e-6
all.equal(coef(fm1), coef(fmX),  tolerance=0) # ... rel.diff.: 1.03e-12

stopifnot(all.equal(coef(fm1), coef(fmX1), tolerance = 6e-6),
          all.equal(coef(fm1), coef(fmX ), tolerance = 1e-11))
</code></pre>

<hr>
<h2 id='SSD'><abbr>SSD</abbr> Matrix and Estimated Variance Matrix in Multivariate Models</h2><span id='topic+SSD'></span><span id='topic+estVar'></span>

<h3>Description</h3>

<p>Functions to compute matrix of residual sums of squares and products,
or the estimated variance matrix for multivariate linear models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
# S3 method for class 'mlm'
SSD(object, ...)

# S3 methods for class 'SSD' and 'mlm'
estVar(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSD_+3A_object">object</code></td>
<td>
<p><code>object</code> of class <code>"mlm"</code>, or <code>"SSD"</code> in
the case of <code>estVar</code>.</p>
</td></tr>
<tr><td><code id="SSD_+3A_...">...</code></td>
<td>
<p>Unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>SSD()</code> returns a list of class <code>"SSD"</code> containing the
following components
</p>
<table>
<tr><td><code>SSD</code></td>
<td>
<p>The residual sums of squares and products matrix</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>Degrees of freedom</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>Copied from <code>object</code></p>
</td></tr>
</table>
<p><code>estVar</code> returns a matrix with the estimated variances and
covariances.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mauchly.test">mauchly.test</a></code>, <code><a href="#topic+anova.mlm">anova.mlm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># Lifted from Baron+Li:
# "Notes on the use of R for psychology experiments and questionnaires"
# Maxwell and Delaney, p. 497
reacttime &lt;- matrix(c(
420, 420, 480, 480, 600, 780,
420, 480, 480, 360, 480, 600,
480, 480, 540, 660, 780, 780,
420, 540, 540, 480, 780, 900,
540, 660, 540, 480, 660, 720,
360, 420, 360, 360, 480, 540,
480, 480, 600, 540, 720, 840,
480, 600, 660, 540, 720, 900,
540, 600, 540, 480, 720, 780,
480, 420, 540, 540, 660, 780),
ncol = 6, byrow = TRUE,
dimnames = list(subj = 1:10,
              cond = c("deg0NA", "deg4NA", "deg8NA",
                       "deg0NP", "deg4NP", "deg8NP")))

mlmfit &lt;- lm(reacttime ~ 1)
SSD(mlmfit)
estVar(mlmfit)
</code></pre>

<hr>
<h2 id='SSfol'>Self-Starting <code>nls</code> First-order Compartment Model</h2><span id='topic+SSfol'></span>

<h3>Description</h3>

<p>This <code>selfStart</code> model evaluates the first-order compartment
function and its gradient.  It has an <code>initial</code> attribute that
creates initial estimates of the parameters <code>lKe</code>, <code>lKa</code>,
and <code>lCl</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSfol(Dose, input, lKe, lKa, lCl)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSfol_+3A_dose">Dose</code></td>
<td>
<p>a numeric value representing the initial dose.</p>
</td></tr>
<tr><td><code id="SSfol_+3A_input">input</code></td>
<td>
<p>a numeric vector at which to evaluate the model.</p>
</td></tr>
<tr><td><code id="SSfol_+3A_lke">lKe</code></td>
<td>
<p>a numeric parameter representing the natural logarithm of
the elimination rate constant.</p>
</td></tr>
<tr><td><code id="SSfol_+3A_lka">lKa</code></td>
<td>
<p>a numeric parameter representing the natural logarithm of
the absorption rate constant.</p>
</td></tr>
<tr><td><code id="SSfol_+3A_lcl">lCl</code></td>
<td>
<p>a numeric parameter representing the natural logarithm of
the clearance.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric vector of the same length as <code>input</code>, which is the
value of the expression
</p>
<pre>Dose * exp(lKe+lKa-lCl) * (exp(-exp(lKe)*input) - exp(-exp(lKa)*input))
    / (exp(lKa) - exp(lKe))
</pre>
<p>If all of the arguments <code>lKe</code>, <code>lKa</code>, and <code>lCl</code> are
names of objects, the gradient matrix with respect to these names is
attached as an attribute named <code>gradient</code>.
</p>


<h3>Author(s)</h3>

<p>Jos Pinheiro and Douglas Bates</p>


<h3>See Also</h3>

<p><code><a href="#topic+nls">nls</a></code>, <code><a href="#topic+selfStart">selfStart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Theoph.1 &lt;- Theoph[ Theoph$Subject == 1, ]
with(Theoph.1, SSfol(Dose, Time, -2.5, 0.5, -3)) # response only
with(Theoph.1, local({  lKe &lt;- -2.5; lKa &lt;- 0.5; lCl &lt;- -3
  SSfol(Dose, Time, lKe, lKa, lCl) # response _and_ gradient
}))
getInitial(conc ~ SSfol(Dose, Time, lKe, lKa, lCl), data = Theoph.1)
## Initial values are in fact the converged values
fm1 &lt;- nls(conc ~ SSfol(Dose, Time, lKe, lKa, lCl), data = Theoph.1)
summary(fm1)
</code></pre>

<hr>
<h2 id='SSfpl'>Self-Starting <code>nls</code> Four-Parameter Logistic Model</h2><span id='topic+SSfpl'></span>

<h3>Description</h3>

<p>This <code>selfStart</code> model evaluates the four-parameter logistic
function and its gradient.  It has an <code>initial</code> attribute computing
initial estimates of the parameters <code>A</code>, <code>B</code>,
<code>xmid</code>, and <code>scal</code> for a given set of data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSfpl(input, A, B, xmid, scal)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSfpl_+3A_input">input</code></td>
<td>
<p>a numeric vector of values at which to evaluate the model.</p>
</td></tr>
<tr><td><code id="SSfpl_+3A_a">A</code></td>
<td>
<p>a numeric parameter representing the horizontal asymptote on
the left side (very small values of <code>input</code>).</p>
</td></tr>
<tr><td><code id="SSfpl_+3A_b">B</code></td>
<td>
<p>a numeric parameter representing the horizontal asymptote on
the right side (very large values of <code>input</code>).</p>
</td></tr>
<tr><td><code id="SSfpl_+3A_xmid">xmid</code></td>
<td>
<p>a numeric parameter representing the <code>input</code> value at the
inflection point of the curve.  The value of <code>SSfpl</code> will be
midway between <code>A</code> and <code>B</code> at <code>xmid</code>.</p>
</td></tr>
<tr><td><code id="SSfpl_+3A_scal">scal</code></td>
<td>
<p>a numeric scale parameter on the <code>input</code> axis.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric vector of the same length as <code>input</code>.  It is the value of
the expression <code>A+(B-A)/(1+exp((xmid-input)/scal))</code>.  If all of
the arguments <code>A</code>, <code>B</code>, <code>xmid</code>, and <code>scal</code> are
names of objects, the gradient matrix with respect to these names is
attached as an attribute named <code>gradient</code>.
</p>


<h3>Author(s)</h3>

<p>Jos Pinheiro and Douglas Bates</p>


<h3>See Also</h3>

<p><code><a href="#topic+nls">nls</a></code>, <code><a href="#topic+selfStart">selfStart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Chick.1 &lt;- ChickWeight[ChickWeight$Chick == 1, ]
SSfpl(Chick.1$Time, 13, 368, 14, 6)  # response only
local({
  A &lt;- 13; B &lt;- 368; xmid &lt;- 14; scal &lt;- 6
  SSfpl(Chick.1$Time, A, B, xmid, scal) # response _and_ gradient
})
print(getInitial(weight ~ SSfpl(Time, A, B, xmid, scal), data = Chick.1),
      digits = 5)
## Initial values are in fact the converged values
fm1 &lt;- nls(weight ~ SSfpl(Time, A, B, xmid, scal), data = Chick.1)
summary(fm1)

## Visualizing the  SSfpl()  parametrization
  xx &lt;- seq(-0.5, 5, length.out = 101)
  yy &lt;- 1 + 4 / (1 + exp((2-xx))) # == SSfpl(xx, *) :
  stopifnot( all.equal(yy, SSfpl(xx, A = 1, B = 5, xmid = 2, scal = 1)) )
  require(graphics)
  op &lt;- par(mar = c(0, 0, 3.5, 0))
  plot(xx, yy, type = "l", axes = FALSE, ylim = c(0,6), xlim = c(-1, 5),
       xlab = "", ylab = "", lwd = 2,
       main = "Parameters in the SSfpl model")
  mtext(quote(list(phi[1] == "A", phi[2] == "B", phi[3] == "xmid", phi[4] == "scal")))
  usr &lt;- par("usr")
  arrows(usr[1], 0, usr[2], 0, length = 0.1, angle = 25)
  arrows(0, usr[3], 0, usr[4], length = 0.1, angle = 25)
  text(usr[2] - 0.2, 0.1, "x", adj = c(1, 0))
  text(     -0.1, usr[4], "y", adj = c(1, 1))
  abline(h = c(1, 5), lty = 3)
  arrows(-0.8, c(2.1, 2.9),
         -0.8, c(0,   5  ), length = 0.1, angle = 25)
  text  (-0.8, 2.5, quote(phi[1]))
  arrows(-0.3, c(1/4, 3/4),
         -0.3, c(0,   1  ), length = 0.07, angle = 25)
  text  (-0.3, 0.5, quote(phi[2]))
  text(2, -.1, quote(phi[3]))
  segments(c(2,3,3), c(0,3,4), # SSfpl(x = xmid = 2) = 3
           c(2,3,2), c(3,4,3),    lty = 2, lwd = 0.75)
  arrows(c(2.3, 2.7), 3,
         c(2.0, 3  ), 3, length = 0.08, angle = 25)
  text(      2.5,     3, quote(phi[4])); text(3.1, 3.5, "1")
  par(op)
</code></pre>

<hr>
<h2 id='SSgompertz'>Self-Starting <code>nls</code> Gompertz Growth Model</h2><span id='topic+SSgompertz'></span>

<h3>Description</h3>

<p>This <code>selfStart</code> model evaluates the Gompertz growth model
and its gradient.  It has an <code>initial</code> attribute that
creates initial estimates of the parameters <code>Asym</code>,
<code>b2</code>, and <code>b3</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSgompertz(x, Asym, b2, b3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSgompertz_+3A_x">x</code></td>
<td>
<p>a numeric vector of values at which to evaluate the model.</p>
</td></tr>
<tr><td><code id="SSgompertz_+3A_asym">Asym</code></td>
<td>
<p>a numeric parameter representing the asymptote.</p>
</td></tr>
<tr><td><code id="SSgompertz_+3A_b2">b2</code></td>
<td>
<p>a numeric parameter related to the value of the function at
<code>x = 0</code></p>
</td></tr>
<tr><td><code id="SSgompertz_+3A_b3">b3</code></td>
<td>
<p>a numeric parameter related to the scale the <code>x</code> axis.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric vector of the same length as <code>input</code>.  It is the value of
the expression <code>Asym*exp(-b2*b3^x)</code>.  If all of
the arguments <code>Asym</code>, <code>b2</code>, and <code>b3</code> are
names of objects the gradient matrix with respect to these names is attached as
an attribute named <code>gradient</code>.
</p>


<h3>Author(s)</h3>

<p>Douglas Bates</p>


<h3>See Also</h3>

<p><code><a href="#topic+nls">nls</a></code>, <code><a href="#topic+selfStart">selfStart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>DNase.1 &lt;- subset(DNase, Run == 1)
SSgompertz(log(DNase.1$conc), 4.5, 2.3, 0.7)  # response only
local({  Asym &lt;- 4.5; b2 &lt;- 2.3; b3 &lt;- 0.7
  SSgompertz(log(DNase.1$conc), Asym, b2, b3) # response _and_ gradient
})
print(getInitial(density ~ SSgompertz(log(conc), Asym, b2, b3),
                 data = DNase.1), digits = 5)
## Initial values are in fact the converged values
fm1 &lt;- nls(density ~ SSgompertz(log(conc), Asym, b2, b3),
           data = DNase.1)
summary(fm1)
plot(density ~ log(conc), DNase.1, # xlim = c(0, 21),
     main = "SSgompertz() fit to DNase.1")
ux &lt;- par("usr")[1:2]; x &lt;- seq(ux[1], ux[2], length.out=250)
lines(x, do.call(SSgompertz, c(list(x=x), coef(fm1))), col = "red", lwd=2)
As &lt;- coef(fm1)[["Asym"]]; abline(v = 0, h = 0, lty = 3)
axis(2, at= exp(-coef(fm1)[["b2"]]), quote(e^{-b[2]}), las=1, pos=0)
</code></pre>

<hr>
<h2 id='SSlogis'>Self-Starting <code>nls</code> Logistic Model</h2><span id='topic+SSlogis'></span>

<h3>Description</h3>

<p>This <code>selfStart</code> model evaluates the logistic
function and its gradient.  It has an <code>initial</code> attribute that
creates initial estimates of the parameters <code>Asym</code>,
<code>xmid</code>, and <code>scal</code>.   In <span class="rlang"><b>R</b></span> 3.4.2 and earlier, that
init function failed when <code>min(input)</code> was exactly zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSlogis(input, Asym, xmid, scal)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSlogis_+3A_input">input</code></td>
<td>
<p>a numeric vector of values at which to evaluate the model.</p>
</td></tr>
<tr><td><code id="SSlogis_+3A_asym">Asym</code></td>
<td>
<p>a numeric parameter representing the asymptote.</p>
</td></tr>
<tr><td><code id="SSlogis_+3A_xmid">xmid</code></td>
<td>
<p>a numeric parameter representing the <code>x</code> value at the
inflection point of the curve.  The value of <code>SSlogis</code> will be
<code>Asym/2</code> at <code>xmid</code>.</p>
</td></tr>
<tr><td><code id="SSlogis_+3A_scal">scal</code></td>
<td>
<p>a numeric scale parameter on the <code>input</code> axis.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric vector of the same length as <code>input</code>.  It is the value of
the expression <code>Asym/(1+exp((xmid-input)/scal))</code>.  If all of
the arguments <code>Asym</code>, <code>xmid</code>, and <code>scal</code> are
names of objects the gradient matrix with respect to these names is
attached as an attribute named <code>gradient</code>.
</p>


<h3>Author(s)</h3>

<p>Jos Pinheiro and Douglas Bates</p>


<h3>See Also</h3>

<p><code><a href="#topic+nls">nls</a></code>, <code><a href="#topic+selfStart">selfStart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
Chick.1 &lt;- ChickWeight[ChickWeight$Chick == 1, ]
SSlogis(Chick.1$Time, 368, 14, 6)  # response only
local({
  Asym &lt;- 368; xmid &lt;- 14; scal &lt;- 6
  SSlogis(Chick.1$Time, Asym, xmid, scal) # response _and_ gradient
})
getInitial(weight ~ SSlogis(Time, Asym, xmid, scal), data = Chick.1)
## Initial values are in fact the converged one here, "Number of iter...: 0" :
fm1 &lt;- nls(weight ~ SSlogis(Time, Asym, xmid, scal), data = Chick.1)
summary(fm1)
## but are slightly improved here:
fm2 &lt;- update(fm1, control=nls.control(tol = 1e-9, warnOnly=TRUE), trace = TRUE)
all.equal(coef(fm1), coef(fm2)) # "Mean relative difference: 9.6e-6"
str(fm2$convInfo) # 3 iterations


dwlg1 &lt;- data.frame(Prop = c(rep(0,5), 2, 5, rep(9, 9)), end = 1:16)
iPar &lt;- getInitial(Prop ~ SSlogis(end, Asym, xmid, scal), data = dwlg1)
## failed in R &lt;= 3.4.2 (because of the '0's in 'Prop')
stopifnot(all.equal(tolerance = 1e-6,
   iPar, c(Asym = 9.0678, xmid = 6.79331, scal = 0.499934)))

## Visualize the SSlogis()  model  parametrization :
  xx &lt;- seq(-0.75, 5, by=1/32)
  yy &lt;- 5 / (1 + exp((2-xx)/0.6)) # == SSlogis(xx, *):
  stopifnot( all.equal(yy, SSlogis(xx, Asym = 5, xmid = 2, scal = 0.6)) )
  require(graphics)
  op &lt;- par(mar = c(0.5, 0, 3.5, 0))
  plot(xx, yy, type = "l", axes = FALSE, ylim = c(0,6), xlim = c(-1, 5),
       xlab = "", ylab = "", lwd = 2,
       main = "Parameters in the SSlogis model")
  mtext(quote(list(phi[1] == "Asym", phi[2] == "xmid", phi[3] == "scal")))
  usr &lt;- par("usr")
  arrows(usr[1], 0, usr[2], 0, length = 0.1, angle = 25)
  arrows(0, usr[3], 0, usr[4], length = 0.1, angle = 25)
  text(usr[2] - 0.2, 0.1, "x", adj = c(1, 0))
  text(     -0.1, usr[4], "y", adj = c(1, 1))
  abline(h = 5, lty = 3)
  arrows(-0.8, c(2.1, 2.9),
         -0.8, c(0,   5  ), length = 0.1, angle = 25)
  text  (-0.8, 2.5, quote(phi[1]))
  segments(c(2,2.6,2.6), c(0,  2.5,3.5),   # NB.  SSlogis(x = xmid = 2) = 2.5
           c(2,2.6,2  ), c(2.5,3.5,2.5), lty = 2, lwd = 0.75)
  text(2, -.1, quote(phi[2]))
  arrows(c(2.2, 2.4), 2.5,
         c(2.0, 2.6), 2.5, length = 0.08, angle = 25)
  text(      2.3,     2.5, quote(phi[3])); text(2.7, 3, "1")
  par(op)
</code></pre>

<hr>
<h2 id='SSmicmen'>Self-Starting <code>nls</code> Michaelis-Menten Model</h2><span id='topic+SSmicmen'></span>

<h3>Description</h3>

<p>This <code>selfStart</code> model evaluates the Michaelis-Menten model and
its gradient.  It has an <code>initial</code> attribute that
will evaluate initial estimates of the parameters <code>Vm</code> and <code>K</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSmicmen(input, Vm, K)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSmicmen_+3A_input">input</code></td>
<td>
<p>a numeric vector of values at which to evaluate the model.</p>
</td></tr>
<tr><td><code id="SSmicmen_+3A_vm">Vm</code></td>
<td>
<p>a numeric parameter representing the maximum value of the response.</p>
</td></tr>
<tr><td><code id="SSmicmen_+3A_k">K</code></td>
<td>
<p>a numeric parameter representing the <code>input</code> value at
which half the maximum response is attained.  In the field of enzyme
kinetics this is called the Michaelis parameter.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric vector of the same length as <code>input</code>.  It is the value of
the expression <code>Vm*input/(K+input)</code>.  If both
the arguments <code>Vm</code> and <code>K</code> are
names of objects, the gradient matrix with respect to these names is
attached as an attribute named <code>gradient</code>.
</p>


<h3>Author(s)</h3>

<p>Jos Pinheiro and Douglas Bates</p>


<h3>See Also</h3>

<p><code><a href="#topic+nls">nls</a></code>, <code><a href="#topic+selfStart">selfStart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>PurTrt &lt;- Puromycin[ Puromycin$state == "treated", ]
SSmicmen(PurTrt$conc, 200, 0.05)  # response only
local({  Vm &lt;- 200; K &lt;- 0.05
  SSmicmen(PurTrt$conc, Vm, K)    # response _and_ gradient
})
print(getInitial(rate ~ SSmicmen(conc, Vm, K), data = PurTrt), digits = 3)
## Initial values are in fact the converged values
fm1 &lt;- nls(rate ~ SSmicmen(conc, Vm, K), data = PurTrt)
summary(fm1)
## Alternative call using the subset argument
fm2 &lt;- nls(rate ~ SSmicmen(conc, Vm, K), data = Puromycin,
           subset = state == "treated")
summary(fm2) # The same indeed:
stopifnot(all.equal(coef(summary(fm1)), coef(summary(fm2))))

## Visualize the SSmicmen()  Michaelis-Menton model parametrization :

  xx &lt;- seq(0, 5, length.out = 101)
  yy &lt;- 5 * xx/(1+xx)
  stopifnot(all.equal(yy, SSmicmen(xx, Vm = 5, K = 1)))
  require(graphics)
  op &lt;- par(mar = c(0, 0, 3.5, 0))
  plot(xx, yy, type = "l", lwd = 2, ylim = c(-1/4,6), xlim = c(-1, 5),
       ann = FALSE, axes = FALSE, main = "Parameters in the SSmicmen model")
  mtext(quote(list(phi[1] == "Vm", phi[2] == "K")))
  usr &lt;- par("usr")
  arrows(usr[1], 0, usr[2], 0, length = 0.1, angle = 25)
  arrows(0, usr[3], 0, usr[4], length = 0.1, angle = 25)
  text(usr[2] - 0.2, 0.1, "x", adj = c(1, 0))
  text(     -0.1, usr[4], "y", adj = c(1, 1))
  abline(h = 5, lty = 3)
  arrows(-0.8, c(2.1, 2.9),
         -0.8, c(0,   5  ),  length = 0.1, angle = 25)
  text(  -0.8,     2.5, quote(phi[1]))
  segments(1, 0, 1, 2.7, lty = 2, lwd = 0.75)
  text(1, 2.7, quote(phi[2]))
  par(op)
</code></pre>

<hr>
<h2 id='SSweibull'>Self-Starting <code>nls</code> Weibull Growth Curve Model</h2><span id='topic+SSweibull'></span>

<h3>Description</h3>

<p>This <code>selfStart</code> model evaluates the Weibull model for growth
curve data and its gradient.  It has an <code>initial</code> attribute that
will evaluate initial estimates of the parameters <code>Asym</code>, <code>Drop</code>,
<code>lrc</code>, and <code>pwr</code> for a given set of data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSweibull(x, Asym, Drop, lrc, pwr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSweibull_+3A_x">x</code></td>
<td>
<p>a numeric vector of values at which to evaluate the model.</p>
</td></tr>
<tr><td><code id="SSweibull_+3A_asym">Asym</code></td>
<td>
<p>a numeric parameter representing the horizontal asymptote on
the right side (very small values of <code>x</code>).</p>
</td></tr>
<tr><td><code id="SSweibull_+3A_drop">Drop</code></td>
<td>
<p>a numeric parameter representing the change from
<code>Asym</code> to the <code>y</code> intercept.</p>
</td></tr>
<tr><td><code id="SSweibull_+3A_lrc">lrc</code></td>
<td>
<p>a numeric parameter representing the natural logarithm of
the rate constant.</p>
</td></tr>
<tr><td><code id="SSweibull_+3A_pwr">pwr</code></td>
<td>
<p>a numeric parameter representing the power to which <code>x</code>
is raised.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This model is a generalization of the <code><a href="#topic+SSasymp">SSasymp</a></code> model in
that it reduces to <code>SSasymp</code> when <code>pwr</code> is unity.
</p>


<h3>Value</h3>

<p>a numeric vector of the same length as <code>x</code>.  It is the value of
the expression <code>Asym-Drop*exp(-exp(lrc)*x^pwr)</code>.  If all of
the arguments <code>Asym</code>, <code>Drop</code>, <code>lrc</code>, and <code>pwr</code> are
names of objects, the gradient matrix with respect to these names is
attached as an attribute named <code>gradient</code>.
</p>


<h3>Author(s)</h3>

<p>Douglas Bates</p>


<h3>References</h3>

<p>Ratkowsky, David A. (1983), <em>Nonlinear Regression Modeling</em>,
Dekker. (section 4.4.5)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nls">nls</a></code>, <code><a href="#topic+selfStart">selfStart</a></code>, <code><a href="#topic+SSasymp">SSasymp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Chick.6 &lt;- subset(ChickWeight, (Chick == 6) &amp; (Time &gt; 0))
SSweibull(Chick.6$Time, 160, 115, -5.5, 2.5)   # response only
local({ Asym &lt;- 160; Drop &lt;- 115; lrc &lt;- -5.5; pwr &lt;- 2.5
  SSweibull(Chick.6$Time, Asym, Drop, lrc, pwr) # response _and_ gradient
})

getInitial(weight ~ SSweibull(Time, Asym, Drop, lrc, pwr), data = Chick.6)
## Initial values are in fact the converged values
fm1 &lt;- nls(weight ~ SSweibull(Time, Asym, Drop, lrc, pwr), data = Chick.6)
summary(fm1)

## Data and Fit:
plot(weight ~ Time, Chick.6, xlim = c(0, 21), main = "SSweibull() fit to Chick.6")
ux &lt;- par("usr")[1:2]; x &lt;- seq(ux[1], ux[2], length.out=250)
lines(x, do.call(SSweibull, c(list(x=x), coef(fm1))), col = "red", lwd=2)
As &lt;- coef(fm1)[["Asym"]]; abline(v = 0, h = c(As, As - coef(fm1)[["Drop"]]), lty = 3)
</code></pre>

<hr>
<h2 id='start'>Encode the Terminal Times of Time Series</h2><span id='topic+start'></span><span id='topic+end'></span>

<h3>Description</h3>

<p>Extract and encode the times the first and last observations were
taken. Provided only for compatibility with S version 2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>start(x, ...)
end(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="start_+3A_x">x</code></td>
<td>
<p>a univariate or multivariate time-series, or a vector or matrix.</p>
</td></tr>
<tr><td><code id="start_+3A_...">...</code></td>
<td>
<p>extra arguments for future methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These are generic functions, which will use the
<code><a href="#topic+tsp">tsp</a></code> attribute of <code>x</code> if it exists.
Their default methods decode the start time from the original time
units, so that for a monthly series <code>1995.5</code> is represented
as <code>c(1995, 7)</code>. For a series of frequency <code>f</code>, time
<code>n+i/f</code> is presented as <code>c(n, i+1)</code> (even for <code>i = 0</code>
and <code>f = 1</code>).
</p>


<h3>Warning</h3>

<p>The representation used by <code>start</code> and <code>end</code> has no
meaning unless the frequency is supplied.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ts">ts</a></code>, <code><a href="#topic+time">time</a></code>, <code><a href="#topic+tsp">tsp</a></code>.
</p>

<hr>
<h2 id='stat.anova'>GLM ANOVA Statistics</h2><span id='topic+stat.anova'></span>

<h3>Description</h3>

<p>This is a utility function, used in <code>lm</code> and
<code>glm</code> methods for <code><a href="#topic+anova">anova</a>(..., test != NULL)</code>
and should not be used by the average user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stat.anova(table, test = c("Rao","LRT", "Chisq", "F", "Cp"),
           scale, df.scale, n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stat.anova_+3A_table">table</code></td>
<td>
<p>numeric matrix as results from
<code><a href="#topic+anova.glm">anova.glm</a>(..., test = NULL)</code>.</p>
</td></tr>
<tr><td><code id="stat.anova_+3A_test">test</code></td>
<td>
<p>a character string, partially matching one of <code>"Rao"</code>,
<code>"LRT"</code>, <code>"Chisq"</code>, <code>"F"</code> or <code>"Cp"</code>.</p>
</td></tr>
<tr><td><code id="stat.anova_+3A_scale">scale</code></td>
<td>
<p>a residual mean square or other scale estimate to be used
as the denominator in an F test.</p>
</td></tr>
<tr><td><code id="stat.anova_+3A_df.scale">df.scale</code></td>
<td>
<p>degrees of freedom corresponding to <code>scale</code>.</p>
</td></tr>
<tr><td><code id="stat.anova_+3A_n">n</code></td>
<td>
<p>number of observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix which is the original <code>table</code>, augmented by a column
of test statistics, depending on the <code>test</code> argument.
</p>


<h3>References</h3>

<p>Hastie, T. J. and Pregibon, D. (1992)
<em>Generalized linear models.</em>
Chapter 6 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anova.lm">anova.lm</a></code>, <code><a href="#topic+anova.glm">anova.glm</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>##-- Continued from '?glm':

print(ag &lt;- anova(glm.D93))
stat.anova(ag$table, test = "Cp",
           scale = sum(resid(glm.D93, "pearson")^2)/4,
           df.scale = 4, n = 9)
</code></pre>

<hr>
<h2 id='stats-defunct'>Defunct Functions in Package <span class="pkg">stats</span></h2><span id='topic+stats-defunct'></span><span id='topic+arima0.diag'></span><span id='topic+dnchisq'></span><span id='topic+pnchisq'></span><span id='topic+qnchisq'></span><span id='topic+rnchisq'></span><span id='topic+reshapeLong'></span><span id='topic+reshapeWide'></span><span id='topic+plot.mts'></span><span id='topic+print.coefmat'></span><span id='topic+anovalist.lm'></span><span id='topic+lm.fit.null'></span><span id='topic+lm.wfit.null'></span><span id='topic+glm.fit.null'></span><span id='topic+mauchley.test'></span><span id='topic+clearNames'></span><span id='topic+plclust'></span>

<h3>Description</h3>

<p>The functions or variables listed here are no longer part of <span class="rlang"><b>R</b></span> as
they are not needed (any more).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Defunct in R 1.x
dnchisq(.)
pnchisq(.)
qnchisq(.)
rnchisq(.)
reshapeWide(x, i, j, val, jnames = levels(j))
reshapeLong(x, jvars,  ilev = row.names(x),
            jlev = names(x)[jvars], iname = "reshape.i",
            jname = "reshape.j", vname = "reshape.v")
arima0.diag(...)
plot.mts(x, plot.type = c("multiple", "single"), panel = lines,
         log = "", col = par("col"),  bg = NA, pch = par("pch"),
         cex = par("cex"), lty = par("lty"), lwd = par("lwd"),
         ann = par("ann"),  xlab = "Time", type = "l", main = NULL,
         oma = c(6, 0, 5, 0), ...)
print.coefmat(x, digits = max(3, getOption("digits") - 2),
              signif.stars = getOption("show.signif.stars"),
              dig.tst = max(1, min(5, digits - 1)),
              cs.ind, tst.ind, zap.ind = integer(0),
              P.values = NULL, has.Pvalue,
              eps.Pvalue = .Machine$double.eps,
              na.print = "", ...)
anovalist.lm(object, ..., test = NULL)
lm.fit.null(x, y, method = "qr", tol = 1e-07, ...)
lm.wfit.null(x, y, w, method = "qr", tol = 1e-07, ...)
glm.fit.null(x, y, weights, start = NULL,
             etastart = NULL, mustart = NULL, offset,
             family = gaussian(), control = glm.control(),
             intercept = FALSE)

# Defunct in 2.4.0
mauchley.test(object, Sigma = diag(nrow = p),
   T = Thin.row(proj(M) - proj(X)), M = diag(nrow = p), X = ~0,
   idata = data.frame(index = seq(length = p)), ...)

# Defunct in 2.10.0
clearNames(object)

# Defunct in 4.1.0
plclust(tree, hang = 0.1, unit = FALSE, level = FALSE, hmin = 0,
        square = TRUE, labels = NULL, plot. = TRUE,
        axes = TRUE, frame.plot = FALSE, ann = TRUE,
        main = "", sub = NULL, xlab = NULL, ylab = "Height")
</code></pre>


<h3>Details</h3>

<p>The <code>*chisq()</code> functions now take an optional non-centrality
argument, so the <code>*nchisq()</code> functions are no longer needed.
</p>
<p><code>reshape*</code>, which were experimental, are replaced by
<code><a href="#topic+reshape">reshape</a></code>.  This has a different syntax and allows
multiple time-varying variables.
</p>
<p><code>arima0.diag</code> has been replaced by <code><a href="#topic+tsdiag.arima0">tsdiag.arima0</a></code>.
</p>
<p><code>plot.mts</code> has been removed, as <code><a href="#topic+plot.ts">plot.ts</a></code> now has the
same functionality.
</p>
<p><code>print.coefmat</code> was an older name for <code><a href="#topic+printCoefmat">printCoefmat</a></code>
with a different default for <code>na.print</code>.
</p>
<p><code>anovalist.lm</code> was replaced by <code><a href="#topic+anova.lmlist">anova.lmlist</a></code> in
<span class="rlang"><b>R</b></span> 1.2.0.
</p>
<p><code>lm.fit.null</code> and <code>lm.wfit.null</code> are superseded by
<code>lm.fit</code> and <code>lm.wfit</code> which handle null models now.
Similarly, <code>glm.fit.null</code> is superseded by <code>glm.fit</code>.
</p>
<p><code>mauchley.test</code> was a misspelling of Mauchly's name, corrected
by the introduction of <code><a href="#topic+mauchly.test">mauchly.test</a></code>.
</p>
<p><code>clearNames</code> had been introduced at about the same time as
<code><a href="base.html#topic+unname">unname</a></code>, but is less general and has been used rarely.
</p>
<p><code>plclust</code> has been drawing dendrograms (&ldquo;cluster - trees&rdquo;) and been replaced by the
<code>plot()</code> method for class <code>"dendrogram"</code>.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+Defunct">Defunct</a></code>
</p>

<hr>
<h2 id='stats-deprecated'>Deprecated Functions in Package <span class="pkg">stats</span></h2><span id='topic+stats-deprecated'></span>

<h3>Description</h3>

<p>These functions are provided for compatibility with older versions of
<span class="rlang"><b>R</b></span> only, and may be defunct as soon as the next release.
</p>


<h3>Details</h3>


<p>There are currently no deprecated functions in this package.




</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+Deprecated">Deprecated</a></code>
</p>

<hr>
<h2 id='stats-package'>
The R Stats Package
</h2><span id='topic+stats-package'></span><span id='topic+stats'></span>

<h3>Description</h3>

<p>R statistical functions
</p>


<h3>Details</h3>

<p>This package contains functions for statistical calculations
and random number generation.
</p>
<p>For a complete
list of functions, use <code>library(help = "stats")</code>.
</p>


<h3>Author(s)</h3>

<p>R Core Team and contributors worldwide
</p>
<p>Maintainer: R Core Team <a href="mailto:R-core@r-project.org">R-core@r-project.org</a>
</p>

<hr>
<h2 id='step'>
Choose a model by AIC in a Stepwise Algorithm
</h2><span id='topic+step'></span>

<h3>Description</h3>

<p>Select a formula-based model by AIC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step(object, scope, scale = 0,
     direction = c("both", "backward", "forward"),
     trace = 1, keep = NULL, steps = 1000, k = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="step_+3A_object">object</code></td>
<td>

<p>an object representing a model of an appropriate class (mainly
<code>"lm"</code> and <code>"glm"</code>).
This is used as the initial model in the stepwise search.
</p>
</td></tr>
<tr><td><code id="step_+3A_scope">scope</code></td>
<td>

<p>defines the range of models examined in the stepwise search.
This should be either a single formula, or a list containing
components <code>upper</code> and <code>lower</code>, both formulae.  See the
details for how to specify the formulae and how they are used.
</p>
</td></tr>
<tr><td><code id="step_+3A_scale">scale</code></td>
<td>

<p>used in the definition of the AIC statistic for selecting the models,
currently only for <code><a href="#topic+lm">lm</a></code>, <code><a href="#topic+aov">aov</a></code> and
<code><a href="#topic+glm">glm</a></code> models.  The default value, <code>0</code>, indicates
the scale should be estimated: see <code><a href="#topic+extractAIC">extractAIC</a></code>.
</p>
</td></tr>
<tr><td><code id="step_+3A_direction">direction</code></td>
<td>

<p>the mode of stepwise search, can be one of <code>"both"</code>,
<code>"backward"</code>, or <code>"forward"</code>, with a default of <code>"both"</code>.
If the <code>scope</code> argument is missing the default for
<code>direction</code> is <code>"backward"</code>.  Values can be abbreviated.
</p>
</td></tr>
<tr><td><code id="step_+3A_trace">trace</code></td>
<td>

<p>if positive, information is printed during the running of <code>step</code>.
Larger values may give more detailed information.
</p>
</td></tr>
<tr><td><code id="step_+3A_keep">keep</code></td>
<td>

<p>a filter function whose input is a fitted model object and the
associated <code>AIC</code> statistic, and whose output is arbitrary.
Typically <code>keep</code> will select a subset of the components of
the object and return them. The default is not to keep anything.
</p>
</td></tr>
<tr><td><code id="step_+3A_steps">steps</code></td>
<td>

<p>the maximum number of steps to be considered.  The default is 1000
(essentially as many as required).  It is typically used to stop the
process early.
</p>
</td></tr>
<tr><td><code id="step_+3A_k">k</code></td>
<td>

<p>the multiple of the number of degrees of freedom used for the penalty.
Only <code>k = 2</code> gives the genuine AIC: <code>k = log(n)</code> is sometimes
referred to as BIC or <abbr>SBC</abbr>.
</p>
</td></tr>
<tr><td><code id="step_+3A_...">...</code></td>
<td>

<p>any additional arguments to <code><a href="#topic+extractAIC">extractAIC</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>step</code> uses <code><a href="#topic+add1">add1</a></code> and <code><a href="#topic+drop1">drop1</a></code>
repeatedly; it will work for any method for which they work, and that
is determined by having a valid method for <code><a href="#topic+extractAIC">extractAIC</a></code>.
When the additive constant can be chosen so that AIC is equal to
Mallows' <code class="reqn">C_p</code>, this is done and the tables are labelled
appropriately.
</p>
<p>The set of models searched is determined by the <code>scope</code> argument.
The right-hand-side of its <code>lower</code> component is always included
in the model, and right-hand-side of the model is included in the
<code>upper</code> component.  If <code>scope</code> is a single formula, it
specifies the <code>upper</code> component, and the <code>lower</code> model is
empty.  If <code>scope</code> is missing, the initial model is used as the
<code>upper</code> model.
</p>
<p>Models specified by <code>scope</code> can be templates to update
<code>object</code> as used by <code><a href="#topic+update.formula">update.formula</a></code>.  So using
<code>.</code> in a <code>scope</code> formula means &lsquo;what is
already there&rsquo;, with <code>.^2</code> indicating all interactions of
existing terms.
</p>
<p>There is a potential problem in using <code><a href="#topic+glm">glm</a></code> fits with a
variable <code>scale</code>, as in that case the deviance is not simply
related to the maximized log-likelihood.  The <code>"glm"</code> method for
function <code><a href="#topic+extractAIC">extractAIC</a></code> makes the
appropriate adjustment for a <code>gaussian</code> family, but may need to be
amended for other cases.  (The <code>binomial</code> and <code>poisson</code>
families have fixed <code>scale</code> by default and do not correspond
to a particular maximum-likelihood problem for variable <code>scale</code>.)
</p>


<h3>Value</h3>

<p>the stepwise-selected model is returned, with up to two additional
components.  There is an <code>"anova"</code> component corresponding to the
steps taken in the search, as well as a <code>"keep"</code> component if the
<code>keep=</code> argument was supplied in the call. The
<code>"Resid. Dev"</code> column of the analysis of deviance table refers
to a constant minus twice the maximized log likelihood: it will be a
deviance only in cases where a saturated model is well-defined
(thus excluding <code>lm</code>, <code>aov</code> and <code>survreg</code> fits,
for example).
</p>


<h3>Warning</h3>

<p>The model fitting must apply the models to the same dataset. This
may be a problem if there are missing values and <span class="rlang"><b>R</b></span>'s default of
<code>na.action = na.omit</code> is used.  We suggest you remove the
missing values first.
</p>
<p>Calls to the function <code><a href="#topic+nobs">nobs</a></code> are used to check that the
number of observations involved in the fitting process remains unchanged.
</p>


<h3>Note</h3>

<p>This function differs considerably from the function in S, which uses a
number of approximations and does not in general compute the correct AIC.
</p>
<p>This is a minimal implementation.  Use <code><a href="MASS.html#topic+stepAIC">stepAIC</a></code>
in package <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a> for a wider range of object classes.
</p>


<h3>Author(s)</h3>

<p>B. D. Ripley: <code>step</code> is a slightly simplified version of
<code><a href="MASS.html#topic+stepAIC">stepAIC</a></code> in package <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a> (Venables &amp;
Ripley, 2002 and earlier editions).
</p>
<p>The idea of a <code>step</code> function follows that described in Hastie &amp;
Pregibon (1992); but the implementation in <span class="rlang"><b>R</b></span> is more general.
</p>


<h3>References</h3>

<p>Hastie, T. J. and Pregibon, D. (1992)
<em>Generalized linear models.</em>
Chapter 6 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em>
New York: Springer (4th ed).
</p>


<h3>See Also</h3>

<p><code><a href="MASS.html#topic+stepAIC">stepAIC</a></code> in <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a>, <code><a href="#topic+add1">add1</a></code>,
<code><a href="#topic+drop1">drop1</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## following on from example(lm)

step(lm.D9)

summary(lm1 &lt;- lm(Fertility ~ ., data = swiss))
slm1 &lt;- step(lm1)
summary(slm1)
slm1$anova
</code></pre>

<hr>
<h2 id='stepfun'>Step Functions - Creation and Class</h2><span id='topic+stepfun'></span><span id='topic+is.stepfun'></span><span id='topic+as.stepfun'></span><span id='topic+print.stepfun'></span><span id='topic+summary.stepfun'></span><span id='topic+knots'></span>

<h3>Description</h3>

<p>Given the vectors <code class="reqn">(x_1, \ldots, x_n)</code> and
<code class="reqn">(y_0,y_1,\ldots, y_n)</code> (one value
more!), <code>stepfun(x, y, ...)</code> returns an interpolating
&lsquo;step&rsquo; function, say <code>fn</code>. I.e., <code class="reqn">fn(t) =
    c</code><code class="reqn">_i</code> (constant) for <code class="reqn">t \in (x_i, x_{i+1})</code> and at the abscissa values, if (by default)
<code>right = FALSE</code>, <code class="reqn">fn(x_i) = y_i</code> and for
<code>right = TRUE</code>, <code class="reqn">fn(x_i) = y_{i-1}</code>, for
<code class="reqn">i=1,\ldots,n</code>.
</p>
<p>The value of the constant <code class="reqn">c_i</code> above depends on the
&lsquo;continuity&rsquo; parameter <code>f</code>.
For the default, <code>right = FALSE, f = 0</code>,
<code>fn</code> is a <em>cadlag</em> function, i.e., continuous from the right,
limits from the left, so that the function is piecewise constant on
intervals that include their <em>left</em> endpoint.
In general, <code class="reqn">c_i</code> is interpolated in between the
neighbouring <code class="reqn">y</code> values,
<code class="reqn">c_i= (1-f) y_i + f\cdot y_{i+1}</code>.
Therefore, for non-0 values of <code>f</code>, <code>fn</code> may no longer be a proper
step function, since it can be discontinuous from both sides, unless
<code>right = TRUE, f = 1</code> which is left-continuous (i.e., constant
pieces contain their right endpoint).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stepfun(x, y, f = as.numeric(right), ties = "ordered",
        right = FALSE)

is.stepfun(x)
knots(Fn, ...)
as.stepfun(x, ...)

## S3 method for class 'stepfun'
print(x, digits = getOption("digits") - 2, ...)

## S3 method for class 'stepfun'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stepfun_+3A_x">x</code></td>
<td>
<p>numeric vector giving the knots or jump locations of the step
function for <code>stepfun()</code>.  For the other functions, <code>x</code> is
as <code>object</code> below.</p>
</td></tr>
<tr><td><code id="stepfun_+3A_y">y</code></td>
<td>
<p>numeric vector one longer than <code>x</code>, giving the heights of
the function values <em>between</em> the x values.</p>
</td></tr>
<tr><td><code id="stepfun_+3A_f">f</code></td>
<td>
<p>a number between 0 and 1, indicating how interpolation outside
the given x values should happen.  See <code><a href="#topic+approxfun">approxfun</a></code>.</p>
</td></tr>
<tr><td><code id="stepfun_+3A_ties">ties</code></td>
<td>
<p>Handling of tied <code>x</code> values. Either a function or
the string <code>"ordered"</code>.  See  <code><a href="#topic+approxfun">approxfun</a></code>.</p>
</td></tr>
<tr><td><code id="stepfun_+3A_right">right</code></td>
<td>
<p>logical, indicating if the intervals should be closed on
the right (and open on the left) or vice versa.</p>
</td></tr>
<tr><td><code id="stepfun_+3A_fn">Fn</code>, <code id="stepfun_+3A_object">object</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object inheriting from <code>"stepfun"</code>.</p>
</td></tr>
<tr><td><code id="stepfun_+3A_digits">digits</code></td>
<td>
<p>number of significant digits to use, see <code><a href="base.html#topic+print">print</a></code>.</p>
</td></tr>
<tr><td><code id="stepfun_+3A_...">...</code></td>
<td>
<p>potentially further arguments (required by the generic).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A function of class <code>"stepfun"</code>, say <code>fn</code>.
</p>
<p>There are methods available for summarizing (<code>"summary(.)"</code>),
representing (<code>"print(.)"</code>) and plotting  (<code>"plot(.)"</code>, see
<code><a href="#topic+plot.stepfun">plot.stepfun</a></code>) <code>"stepfun"</code> objects.
</p>
<p>The <code><a href="base.html#topic+environment">environment</a></code> of <code>fn</code> contains all the
information needed;
</p>
<table>
<tr><td><code>"x"</code>, <code>"y"</code></td>
<td>
<p>the original arguments</p>
</td></tr>
<tr><td><code>"n"</code></td>
<td>
<p>number of knots (x values)</p>
</td></tr>
<tr><td><code>"f"</code></td>
<td>
<p>continuity parameter</p>
</td></tr>
<tr><td><code>"yleft"</code>, <code>"yright"</code></td>
<td>
<p>the function values <em>outside</em> the knots</p>
</td></tr>
<tr><td><code>"method"</code></td>
<td>
<p>(always <code>== "constant"</code>, from
<code><a href="#topic+approxfun">approxfun</a>(.)</code>).</p>
</td></tr>
</table>
<p>The knots are also available via <code><a href="#topic+knots">knots</a>(fn)</code>.
</p>


<h3>Note</h3>

<p>The objects of class <code>"stepfun"</code> are not intended to be used for
permanent storage and may change structure between versions of <span class="rlang"><b>R</b></span> (and
did at <span class="rlang"><b>R</b></span> 3.0.0).  They can usually be re-created by
</p>
<pre>    eval(attr(old_obj, "call"), environment(old_obj))</pre>
<p>since the data used is stored as part of the object's environment.
</p>


<h3>Author(s)</h3>

<p>Martin Maechler, <a href="mailto:maechler@stat.math.ethz.ch">maechler@stat.math.ethz.ch</a> with some basic
code from Thomas Lumley.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ecdf">ecdf</a></code> for empirical distribution functions as
special step functions and <code><a href="#topic+plot.stepfun">plot.stepfun</a></code> for <em>plotting</em>
step functions.
</p>
<p><code><a href="#topic+approxfun">approxfun</a></code> and <code><a href="#topic+splinefun">splinefun</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y0 &lt;- c(1., 2., 4., 3.)
sfun0  &lt;- stepfun(1:3, y0, f = 0)
sfun.2 &lt;- stepfun(1:3, y0, f = 0.2)
sfun1  &lt;- stepfun(1:3, y0, f = 1)
sfun1c &lt;- stepfun(1:3, y0, right = TRUE) # hence f=1
sfun0
summary(sfun0)
summary(sfun.2)

## look at the internal structure:
unclass(sfun0)
ls(envir = environment(sfun0))

x0 &lt;- seq(0.5, 3.5, by = 0.25)
rbind(x = x0, f.f0 = sfun0(x0), f.f02 = sfun.2(x0),
      f.f1 = sfun1(x0), f.f1c = sfun1c(x0))
## Identities :
stopifnot(identical(y0[-1], sfun0 (1:3)), # right = FALSE
          identical(y0[-4], sfun1c(1:3))) # right = TRUE
</code></pre>

<hr>
<h2 id='stl'>Seasonal Decomposition of Time Series by Loess</h2><span id='topic+stl'></span>

<h3>Description</h3>

<p>Decompose a time series into seasonal, trend and irregular components
using <code>loess</code>, acronym <abbr>STL</abbr>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stl(x, s.window, s.degree = 0,
    t.window = NULL, t.degree = 1,
    l.window = nextodd(period), l.degree = t.degree,
    s.jump = ceiling(s.window/10),
    t.jump = ceiling(t.window/10),
    l.jump = ceiling(l.window/10),
    robust = FALSE,
    inner = if(robust)  1 else 2,
    outer = if(robust) 15 else 0,
    na.action = na.fail)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stl_+3A_x">x</code></td>
<td>
<p>univariate time series to be decomposed.
This should be an object of class <code>"ts"</code> with a frequency
greater than one.</p>
</td></tr>
<tr><td><code id="stl_+3A_s.window">s.window</code></td>
<td>
<p>either the character string <code>"periodic"</code> or the span (in
lags) of the loess window for seasonal extraction, which should
be odd and at least 7, according to Cleveland <abbr>et al.</abbr>  This has no default.</p>
</td></tr>
<tr><td><code id="stl_+3A_s.degree">s.degree</code></td>
<td>
<p>degree of locally-fitted polynomial in seasonal
extraction.  Should be zero or one.</p>
</td></tr>
<tr><td><code id="stl_+3A_t.window">t.window</code></td>
<td>
<p>the span (in lags) of the loess window for trend
extraction, which should be odd.  If <code>NULL</code>, the default,
<code>nextodd(ceiling((1.5*period) / (1-(1.5/s.window))))</code>, is taken.</p>
</td></tr>
<tr><td><code id="stl_+3A_t.degree">t.degree</code></td>
<td>
<p>degree of locally-fitted polynomial in trend
extraction.  Should be zero or one.</p>
</td></tr>
<tr><td><code id="stl_+3A_l.window">l.window</code></td>
<td>
<p>the span (in lags) of the loess window of the low-pass
filter used for each subseries.  Defaults to the smallest odd
integer greater than or equal to <code>frequency(x)</code> which is
recommended since it prevents competition between the trend and
seasonal components.  If not an odd integer its given value is
increased to the next odd one.</p>
</td></tr>
<tr><td><code id="stl_+3A_l.degree">l.degree</code></td>
<td>
<p>degree of locally-fitted polynomial for the subseries
low-pass filter.  Must be 0 or 1.</p>
</td></tr>
<tr><td><code id="stl_+3A_s.jump">s.jump</code>, <code id="stl_+3A_t.jump">t.jump</code>, <code id="stl_+3A_l.jump">l.jump</code></td>
<td>
<p>integers at least one to increase speed of
the respective smoother.  Linear interpolation happens between every
<code>*.jump</code>-th value.</p>
</td></tr>
<tr><td><code id="stl_+3A_robust">robust</code></td>
<td>
<p>logical indicating if robust fitting be used in the
<code>loess</code> procedure.</p>
</td></tr>
<tr><td><code id="stl_+3A_inner">inner</code></td>
<td>
<p>integer; the number of &lsquo;inner&rsquo; (backfitting)
iterations; usually very few (2) iterations suffice.</p>
</td></tr>
<tr><td><code id="stl_+3A_outer">outer</code></td>
<td>
<p>integer; the number of &lsquo;outer&rsquo; robustness
iterations.</p>
</td></tr>
<tr><td><code id="stl_+3A_na.action">na.action</code></td>
<td>
<p>action on missing values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The seasonal component is found by <em>loess</em> smoothing the
seasonal sub-series (the series of all January values, ...); if
<code>s.window = "periodic"</code> smoothing is effectively replaced by
taking the mean. The seasonal values are removed, and the remainder
smoothed to find the trend. The overall level is removed from the
seasonal component and added to the trend component. This process is
iterated a few times.  The <code>remainder</code> component is the
residuals from the seasonal plus trend fit.
</p>
<p>Several methods for the resulting class <code>"stl"</code> objects, see,
<code><a href="#topic+plot.stl">plot.stl</a></code>.
</p>


<h3>Value</h3>

<p><code>stl</code> returns an object of class <code>"stl"</code> with components
</p>
<table>
<tr><td><code>time.series</code></td>
<td>
<p>a multiple time series with columns
<code>seasonal</code>, <code>trend</code> and <code>remainder</code>.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>the final robust weights (all one if fitting is not
done robustly).</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>win</code></td>
<td>
<p>integer (length 3 vector) with the spans used for the <code>"s"</code>,
<code>"t"</code>, and <code>"l"</code> smoothers.</p>
</td></tr>
<tr><td><code>deg</code></td>
<td>
<p>integer (length 3) vector with the polynomial degrees for
these smoothers.</p>
</td></tr>
<tr><td><code>jump</code></td>
<td>
<p>integer (length 3) vector with the &lsquo;jumps&rsquo; (skips)
used for these smoothers.</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>
<p>number of <b>i</b>nner iterations</p>
</td></tr>
<tr><td><code>no</code></td>
<td>
<p>number of <b>o</b>uter robustness iterations</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>B.D. Ripley; Fortran code by Cleveland <abbr>et al.</abbr> (1990) from
&lsquo;<span class="file">netlib</span>&rsquo;.</p>


<h3>References</h3>

<p>R. B. Cleveland, W. S. Cleveland, J.E.  McRae, and I. Terpenning (1990)
STL:  A  Seasonal-Trend  Decomposition  Procedure Based on Loess.
<em>Journal of Official Statistics</em>, <b>6</b>, 3&ndash;73.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.stl">plot.stl</a></code> for <code>stl</code> methods;
<code><a href="#topic+loess">loess</a></code> in package <span class="pkg">stats</span> (which is not actually
used in <code>stl</code>).
</p>
<p><code><a href="#topic+StructTS">StructTS</a></code> for different kind of decomposition.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

plot(stl(nottem, "per"))
plot(stl(nottem, s.window = 7, t.window = 50, t.jump = 1))

plot(stllc &lt;- stl(log(co2), s.window = 21))
summary(stllc)
## linear trend, strict period.
plot(stl(log(co2), s.window = "per", t.window = 1000))

## Two STL plotted side by side :
        stmd &lt;- stl(mdeaths, s.window = "per") # non-robust
summary(stmR &lt;- stl(mdeaths, s.window = "per", robust = TRUE))
op &lt;- par(mar = c(0, 4, 0, 3), oma = c(5, 0, 4, 0), mfcol = c(4, 2))
plot(stmd, set.pars = NULL, labels  =  NULL,
     main = "stl(mdeaths, s.w = \"per\",  robust = FALSE / TRUE )")
plot(stmR, set.pars = NULL)
# mark the 'outliers' :
(iO &lt;- which(stmR $ weights  &lt; 1e-8)) # 10 were considered outliers
sts &lt;- stmR$time.series
points(time(sts)[iO], 0.8* sts[,"remainder"][iO], pch = 4, col = "red")
par(op)   # reset
</code></pre>

<hr>
<h2 id='stlmethods'>Methods for <abbr>STL</abbr> Objects</h2><span id='topic+plot.stl'></span>

<h3>Description</h3>

<p>Methods for objects of class <code>stl</code>, typically the result of
<code><a href="#topic+stl">stl</a></code>.  The <code>plot</code> method does a multiple figure plot
with some flexibility.
</p>
<p>There are also (non-visible) <code>print</code> and <code>summary</code> methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'stl'
plot(x, labels = colnames(X),
     set.pars = list(mar = c(0, 6, 0, 6), oma = c(6, 0, 4, 0),
                     tck = -0.01, mfrow = c(nplot, 1)),
     main = NULL, range.bars = TRUE, ...,
     col.range = "light gray")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stlmethods_+3A_x">x</code></td>
<td>
<p><code><a href="#topic+stl">stl</a></code> object.</p>
</td></tr>
<tr><td><code id="stlmethods_+3A_labels">labels</code></td>
<td>
<p>character of length 4 giving the names of the component
time-series.</p>
</td></tr>
<tr><td><code id="stlmethods_+3A_set.pars">set.pars</code></td>
<td>
<p>settings for <code><a href="graphics.html#topic+par">par</a>(.)</code> when setting up the plot.</p>
</td></tr>
<tr><td><code id="stlmethods_+3A_main">main</code></td>
<td>
<p>plot main title.</p>
</td></tr>
<tr><td><code id="stlmethods_+3A_range.bars">range.bars</code></td>
<td>
<p>logical indicating if each plot should have a bar at
its right side which are of equal heights in user coordinates.</p>
</td></tr>
<tr><td><code id="stlmethods_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="stlmethods_+3A_col.range">col.range</code></td>
<td>
<p>colour to be used for the range bars, if plotted.
Note this appears after <code>...</code> and so cannot be abbreviated.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+plot.ts">plot.ts</a></code> and <code><a href="#topic+stl">stl</a></code>, particularly for
examples.</p>

<hr>
<h2 id='StructTS'>Fit Structural Time Series</h2><span id='topic+StructTS'></span><span id='topic+print.StructTS'></span><span id='topic+predict.StructTS'></span>

<h3>Description</h3>

<p>Fit a structural model for a time series by maximum likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>StructTS(x, type = c("level", "trend", "BSM"), init = NULL,
         fixed = NULL, optim.control = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="StructTS_+3A_x">x</code></td>
<td>
<p>a univariate numeric time series. Missing values are allowed.</p>
</td></tr>
<tr><td><code id="StructTS_+3A_type">type</code></td>
<td>
<p>the class of structural model.  If omitted, a <abbr>BSM</abbr> is used
for a time series with <code>frequency(x) &gt; 1</code>, and a local trend
model otherwise.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="StructTS_+3A_init">init</code></td>
<td>
<p>initial values of the variance parameters.</p>
</td></tr>
<tr><td><code id="StructTS_+3A_fixed">fixed</code></td>
<td>
<p>optional numeric vector of the same length as the total
number of parameters.  If supplied, only <code>NA</code> entries in
<code>fixed</code> will be varied.  Probably most useful for setting
variances to zero.</p>
</td></tr>
<tr><td><code id="StructTS_+3A_optim.control">optim.control</code></td>
<td>
<p>List of control parameters for
<code><a href="#topic+optim">optim</a></code>.  Method <code>"L-BFGS-B"</code> is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><em>Structural time series</em> models are (linear Gaussian) state-space
models for (univariate) time series based on a decomposition of the
series into a number of components. They are specified by a set of
error variances, some of which may be zero.
</p>
<p>The simplest model is the <em>local level</em> model specified by
<code>type = "level"</code>.  This has an underlying level <code class="reqn">\mu_t</code> which
evolves by
</p>
<p style="text-align: center;"><code class="reqn">\mu_{t+1} = \mu_t + \xi_t,  \qquad \xi_t \sim N(0, \sigma^2_\xi)</code>
</p>

<p>The observations are
</p>
<p style="text-align: center;"><code class="reqn">x_t = \mu_t + \epsilon_t, \qquad \epsilon_t \sim  N(0, \sigma^2_\epsilon)</code>
</p>

<p>There are two parameters, <code class="reqn">\sigma^2_\xi</code>
and <code class="reqn">\sigma^2_\epsilon</code>.  It is an ARIMA(0,1,1) model,
but with restrictions on the parameter set.
</p>
<p>The <em>local linear trend model</em>, <code>type = "trend"</code>, has the same
measurement equation, but with a time-varying slope in the dynamics for
<code class="reqn">\mu_t</code>, given by
</p>
<p style="text-align: center;"><code class="reqn">
   \mu_{t+1} = \mu_t + \nu_t + \xi_t, \qquad  \xi_t \sim N(0, \sigma^2_\xi)
 </code>
</p>

<p style="text-align: center;"><code class="reqn">
   \nu_{t+1} = \nu_t + \zeta_t, \qquad \zeta_t \sim N(0, \sigma^2_\zeta)
 </code>
</p>

<p>with three variance parameters.  It is not uncommon to find
<code class="reqn">\sigma^2_\zeta = 0</code> (which reduces to the local
level model) or <code class="reqn">\sigma^2_\xi = 0</code>, which ensures a
smooth trend.  This is a restricted ARIMA(0,2,2) model.
</p>
<p>The <em>basic structural model</em>, <code>type = "BSM"</code>, is a local
trend model with an additional seasonal component. Thus the measurement
equation is
</p>
<p style="text-align: center;"><code class="reqn">x_t = \mu_t + \gamma_t + \epsilon_t, \qquad \epsilon_t \sim  N(0, \sigma^2_\epsilon)</code>
</p>

<p>where <code class="reqn">\gamma_t</code> is a seasonal component with dynamics
</p>
<p style="text-align: center;"><code class="reqn">
   \gamma_{t+1} = -\gamma_t + \cdots + \gamma_{t-s+2} + \omega_t, \qquad
   \omega_t \sim N(0, \sigma^2_\omega)
 </code>
</p>

<p>The boundary case <code class="reqn">\sigma^2_\omega = 0</code> corresponds
to a deterministic (but arbitrary) seasonal pattern.  (This is
sometimes known as the &lsquo;dummy variable&rsquo; version of the <abbr>BSM</abbr>.)
</p>


<h3>Value</h3>

<p>A list of class <code>"StructTS"</code> with components:
</p>
<table>
<tr><td><code>coef</code></td>
<td>
<p>the estimated variances of the components.</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>the maximized log-likelihood.  Note that as all these
models are non-stationary this includes a diffuse prior for some
observations and hence is not comparable to <code><a href="#topic+arima">arima</a></code>
nor different types of structural models.</p>
</td></tr>
<tr><td><code>loglik0</code></td>
<td>
<p>the maximized log-likelihood with the constant used
prior to <span class="rlang"><b>R</b></span> 3.0.0, for backwards compatibility.</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>the time series <code>x</code>.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>the standardized residuals.</p>
</td></tr>
<tr><td><code>fitted</code></td>
<td>
<p>a multiple time series with one component for the level,
slope and seasonal components, estimated contemporaneously (that is
at time <code class="reqn">t</code> and not at the end of the series).</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>series</code></td>
<td>
<p>the name of the series <code>x</code>.</p>
</td></tr>
<tr><td><code>code</code></td>
<td>
<p>the <code>convergence</code> code returned by <code><a href="#topic+optim">optim</a></code>.</p>
</td></tr>
<tr><td><code>model</code>, <code>model0</code></td>
<td>
<p>Lists representing the Kalman filter used in the
fitting.  See <code><a href="#topic+KalmanLike">KalmanLike</a></code>.  <code>model0</code> is the
initial state of the filter, <code>model</code> its final state.</p>
</td></tr>
<tr><td><code>xtsp</code></td>
<td>
<p>the <code>tsp</code> attributes of <code>x</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Optimization of structural models is a lot harder than many of the
references admit.  For example, the <code><a href="datasets.html#topic+AirPassengers">AirPassengers</a></code> data
are considered in Brockwell &amp; Davis (1996): their solution appears to
be a local maximum, but nowhere near as good a fit as that produced by
<code>StructTS</code>.  It is quite common to find fits with one or more
variances zero, and this can include <code class="reqn">\sigma^2_\epsilon</code>.
</p>


<h3>References</h3>

<p>Brockwell, P. J. &amp; Davis, R. A. (1996).
<em>Introduction to Time Series and Forecasting</em>.
Springer, New York.
Sections 8.2 and 8.5.
</p>
<p>Durbin, J. and Koopman, S. J. (2001) <em>Time Series Analysis by
State Space Methods.</em>  Oxford University Press.
</p>
<p>Harvey, A. C. (1989)
<em>Forecasting, Structural Time Series Models and the Kalman Filter</em>.
Cambridge University Press.
</p>
<p>Harvey, A. C. (1993) <em>Time Series Models</em>.
2nd Edition, Harvester Wheatsheaf.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KalmanLike">KalmanLike</a></code>, <code><a href="#topic+tsSmooth">tsSmooth</a></code>;
<code><a href="#topic+stl">stl</a></code> for different kind of (seasonal) decomposition.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## see also JohnsonJohnson, Nile and AirPassengers
require(graphics)

trees &lt;- window(treering, start = 0)
(fit &lt;- StructTS(trees, type = "level"))
plot(trees)
lines(fitted(fit), col = "green")
tsdiag(fit)

(fit &lt;- StructTS(log10(UKgas), type = "BSM"))
par(mfrow = c(4, 1)) # to give appropriate aspect ratio for next plot.
plot(log10(UKgas))
plot(cbind(fitted(fit), resids=resid(fit)), main = "UK gas consumption")

## keep some parameters fixed; trace optimizer:
StructTS(log10(UKgas), type = "BSM", fixed = c(0.1,0.001,NA,NA),
         optim.control = list(trace = TRUE))
</code></pre>

<hr>
<h2 id='summary.aov'>Summarize an Analysis of Variance Model</h2><span id='topic+summary.aov'></span><span id='topic+summary.aovlist'></span><span id='topic+print.summary.aov'></span><span id='topic+print.summary.aovlist'></span>

<h3>Description</h3>

<p>Summarize an analysis of variance model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'aov'
summary(object, intercept = FALSE, split,
        expand.split = TRUE, keep.zero.df = TRUE, ...)

## S3 method for class 'aovlist'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.aov_+3A_object">object</code></td>
<td>
<p>An object of class <code>"aov"</code> or <code>"aovlist"</code>.</p>
</td></tr>
<tr><td><code id="summary.aov_+3A_intercept">intercept</code></td>
<td>
<p>logical: should intercept terms be included?</p>
</td></tr>
<tr><td><code id="summary.aov_+3A_split">split</code></td>
<td>
<p>an optional named list, with names corresponding to terms
in the model.  Each component is itself a list with integer
components giving contrasts whose contributions are to be summed.</p>
</td></tr>
<tr><td><code id="summary.aov_+3A_expand.split">expand.split</code></td>
<td>
<p>logical: should the split apply also to
interactions involving the factor?</p>
</td></tr>
<tr><td><code id="summary.aov_+3A_keep.zero.df">keep.zero.df</code></td>
<td>
<p>logical: should terms with no degrees of freedom
be included?</p>
</td></tr>
<tr><td><code id="summary.aov_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to or from other methods,
for <code>summary.aovlist</code> including those for <code>summary.aov</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>c("summary.aov", "listof")</code> or
<code>"summary.aovlist"</code> respectively.
</p>
<p>For fits with a single stratum the result will be a list of
ANOVA tables, one for each response (even if there is only one response):
the tables are of class <code>"anova"</code> inheriting from class
<code>"data.frame"</code>.  They have columns <code>"Df"</code>, <code>"Sum Sq"</code>,
<code>"Mean Sq"</code>, as well as <code>"F value"</code> and <code>"Pr(&gt;F)"</code> if
there are non-zero residual degrees of freedom.  There is a row for
each term in the model, plus one for <code>"Residuals"</code> if there
are any.
</p>
<p>For multistratum fits the return value is a list of such summaries,
one for each stratum.
</p>


<h3>Note</h3>

<p>The use of <code>expand.split = TRUE</code> is little tested: it is always
possible to set it to <code>FALSE</code> and specify exactly all
the splits required.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+aov">aov</a></code>, <code><a href="base.html#topic+summary">summary</a></code>, <code><a href="#topic+model.tables">model.tables</a></code>,
<code><a href="#topic+TukeyHSD">TukeyHSD</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## For a simple example see example(aov)

# Cochran and Cox (1957, p.164)
# 3x3 factorial with ordered factors, each is average of 12.
CC &lt;- data.frame(
    y = c(449, 413, 326, 409, 358, 291, 341, 278, 312)/12,
    P = ordered(gl(3, 3)), N = ordered(gl(3, 1, 9))
)
CC.aov &lt;- aov(y ~ N * P, data = CC , weights = rep(12, 9))
summary(CC.aov)

# Split both main effects into linear and quadratic parts.
summary(CC.aov, split = list(N = list(L = 1, Q = 2),
                             P = list(L = 1, Q = 2)))

# Split only the interaction
summary(CC.aov, split = list("N:P" = list(L.L = 1, Q = 2:4)))

# split on just one var
summary(CC.aov, split = list(P = list(lin = 1, quad = 2)))
summary(CC.aov, split = list(P = list(lin = 1, quad = 2)),
        expand.split = FALSE)</code></pre>

<hr>
<h2 id='summary.glm'>Summarizing Generalized Linear Model Fits</h2><span id='topic+summary.glm'></span><span id='topic+print.summary.glm'></span>

<h3>Description</h3>

<p>These functions are all <code><a href="utils.html#topic+methods">methods</a></code> for class <code>glm</code> or
<code>summary.glm</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'glm'
summary(object, dispersion = NULL, correlation = FALSE,
        symbolic.cor = FALSE, ...)

## S3 method for class 'summary.glm'
print(x, digits = max(3, getOption("digits") - 3),
      symbolic.cor = x$symbolic.cor,
      signif.stars = getOption("show.signif.stars"),
      show.residuals = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.glm_+3A_object">object</code></td>
<td>
<p>an object of class <code>"glm"</code>, usually, a result of a
call to <code><a href="#topic+glm">glm</a></code>.</p>
</td></tr>
<tr><td><code id="summary.glm_+3A_x">x</code></td>
<td>
<p>an object of class <code>"summary.glm"</code>, usually, a result of a
call to <code>summary.glm</code>.</p>
</td></tr>
<tr><td><code id="summary.glm_+3A_dispersion">dispersion</code></td>
<td>
<p>the dispersion parameter for the family used.
Either a single numerical value or <code>NULL</code> (the default), when
it is inferred from <code>object</code> (see &lsquo;Details&rsquo;).</p>
</td></tr>
<tr><td><code id="summary.glm_+3A_correlation">correlation</code></td>
<td>
<p>logical; if <code>TRUE</code>, the correlation matrix of
the estimated parameters is returned and printed.</p>
</td></tr>
<tr><td><code id="summary.glm_+3A_digits">digits</code></td>
<td>
<p>the number of significant digits to use when printing.</p>
</td></tr>
<tr><td><code id="summary.glm_+3A_symbolic.cor">symbolic.cor</code></td>
<td>
<p>logical. If <code>TRUE</code>, print the correlations in
a symbolic form (see <code><a href="#topic+symnum">symnum</a></code>) rather than as numbers.</p>
</td></tr>
<tr><td><code id="summary.glm_+3A_signif.stars">signif.stars</code></td>
<td>
<p>logical. If <code>TRUE</code>, &lsquo;significance stars&rsquo;
are printed for each coefficient.</p>
</td></tr>
<tr><td><code id="summary.glm_+3A_show.residuals">show.residuals</code></td>
<td>
<p>logical. If <code>TRUE</code> then a summary of
the deviance residuals is printed at the head of the output.</p>
</td></tr>
<tr><td><code id="summary.glm_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>print.summary.glm</code> tries to be smart about formatting the
coefficients, standard errors, etc. and additionally gives
&lsquo;significance stars&rsquo; if <code>signif.stars</code> is <code>TRUE</code>.
The <code>coefficients</code> component of the result gives the estimated
coefficients and their estimated standard errors, together with their
ratio.  This third column is labelled <code>t ratio</code> if the
dispersion is estimated, and <code>z ratio</code> if the dispersion is known
(or fixed by the family).  A fourth column gives the two-tailed
p-value corresponding to the t or z ratio based on a Student t or
Normal reference distribution.  (It is possible that the dispersion is
not known and there are no residual degrees of freedom from which to
estimate it.  In that case the estimate is <code>NaN</code>.)
</p>
<p>Aliased coefficients are omitted in the returned object but restored
by the <code>print</code> method.
</p>
<p>Correlations are printed to two decimal places (or symbolically): to
see the actual correlations print <code>summary(object)$correlation</code>
directly.
</p>
<p>The dispersion of a GLM is not used in the fitting process, but it is
needed to find standard errors.
If <code>dispersion</code> is not supplied or <code>NULL</code>,
the dispersion is taken as <code>1</code> for the <code>binomial</code> and
<code>Poisson</code> families, and otherwise estimated by the residual
Chi-squared statistic (calculated from cases with non-zero weights)
divided by the residual degrees of freedom.
</p>
<p><code>summary</code> can be used with Gaussian <code>glm</code> fits to handle the
case of a linear regression with known error variance, something not
handled by <code><a href="#topic+summary.lm">summary.lm</a></code>.
</p>


<h3>Value</h3>

<p><code>summary.glm</code> returns an object of class <code>"summary.glm"</code>, a
list with components
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the component from <code>object</code>.</p>
</td></tr>
<tr><td><code>family</code></td>
<td>
<p>the component from <code>object</code>.</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>
<p>the component from <code>object</code>.</p>
</td></tr>
<tr><td><code>contrasts</code></td>
<td>
<p>the component from <code>object</code>.</p>
</td></tr>
<tr><td><code>df.residual</code></td>
<td>
<p>the component from <code>object</code>.</p>
</td></tr>
<tr><td><code>null.deviance</code></td>
<td>
<p>the component from <code>object</code>.</p>
</td></tr>
<tr><td><code>df.null</code></td>
<td>
<p>the component from <code>object</code>.</p>
</td></tr>
<tr><td><code>deviance.resid</code></td>
<td>
<p>the deviance residuals:
see <code><a href="#topic+residuals.glm">residuals.glm</a></code>.</p>
</td></tr>
<tr><td><code>coefficients</code></td>
<td>
<p>the matrix of coefficients, standard errors,
z-values and p-values.  Aliased coefficients are omitted.</p>
</td></tr>
<tr><td><code>aliased</code></td>
<td>
<p>named logical vector showing if the original
coefficients are aliased.</p>
</td></tr>
<tr><td><code>dispersion</code></td>
<td>
<p>either the supplied argument or the inferred/estimated
dispersion if the former is <code>NULL</code>.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>a 3-vector of the rank of the model and the number of
residual degrees of freedom, plus number of coefficients (including
aliased ones).</p>
</td></tr>
<tr><td><code>cov.unscaled</code></td>
<td>
<p>the unscaled (<code>dispersion = 1</code>) estimated covariance
matrix of the estimated coefficients.</p>
</td></tr>
<tr><td><code>cov.scaled</code></td>
<td>
<p>ditto, scaled by <code>dispersion</code>.</p>
</td></tr>
<tr><td><code>correlation</code></td>
<td>
<p>(only if <code>correlation</code> is true.)  The estimated
correlations of the estimated coefficients.</p>
</td></tr>
<tr><td><code>symbolic.cor</code></td>
<td>
<p>(only if <code>correlation</code> is true.)  The value
of the argument <code>symbolic.cor</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+glm">glm</a></code>, <code><a href="base.html#topic+summary">summary</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## For examples see example(glm)
</code></pre>

<hr>
<h2 id='summary.lm'>Summarizing Linear Model Fits</h2><span id='topic+summary.lm'></span><span id='topic+summary.mlm'></span><span id='topic+print.summary.lm'></span>

<h3>Description</h3>

<p><code>summary</code> method for class <code>"lm"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lm'
summary(object, correlation = FALSE, symbolic.cor = FALSE, ...)

## S3 method for class 'summary.lm'
print(x, digits = max(3, getOption("digits") - 3),
      symbolic.cor = x$symbolic.cor,
      signif.stars = getOption("show.signif.stars"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.lm_+3A_object">object</code></td>
<td>
<p>an object of class <code>"lm"</code>, usually, a result of a
call to <code><a href="#topic+lm">lm</a></code>.</p>
</td></tr>
<tr><td><code id="summary.lm_+3A_x">x</code></td>
<td>
<p>an object of class <code>"summary.lm"</code>, usually, a result of a
call to <code>summary.lm</code>.</p>
</td></tr>
<tr><td><code id="summary.lm_+3A_correlation">correlation</code></td>
<td>
<p>logical; if <code>TRUE</code>, the correlation matrix of
the estimated parameters is returned and printed.</p>
</td></tr>
<tr><td><code id="summary.lm_+3A_digits">digits</code></td>
<td>
<p>the number of significant digits to use when printing.</p>
</td></tr>
<tr><td><code id="summary.lm_+3A_symbolic.cor">symbolic.cor</code></td>
<td>
<p>logical. If <code>TRUE</code>, print the correlations in
a symbolic form (see <code><a href="#topic+symnum">symnum</a></code>) rather than as numbers.</p>
</td></tr>
<tr><td><code id="summary.lm_+3A_signif.stars">signif.stars</code></td>
<td>
<p>logical. If <code>TRUE</code>, &lsquo;significance stars&rsquo;
are printed for each coefficient.</p>
</td></tr>
<tr><td><code id="summary.lm_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>print.summary.lm</code> tries to be smart about formatting the
coefficients, standard errors, etc. and additionally gives
&lsquo;significance stars&rsquo; if <code>signif.stars</code> is <code>TRUE</code>.
</p>
<p>Aliased coefficients are omitted in the returned object but restored
by the <code>print</code> method.
</p>
<p>Correlations are printed to two decimal places (or symbolically): to
see the actual correlations print <code>summary(object)$correlation</code>
directly.
</p>


<h3>Value</h3>

<p>The function <code>summary.lm</code> computes and returns a list of summary
statistics of the fitted linear model given in <code>object</code>, using
the components (list elements) <code>"call"</code> and <code>"terms"</code>
from its argument, plus
</p>
<table>
<tr><td><code>residuals</code></td>
<td>
<p>the <em>weighted</em> residuals, the usual residuals
rescaled by the square root of the weights specified in the call to
<code>lm</code>.</p>
</td></tr>
<tr><td><code>coefficients</code></td>
<td>
<p>a <code class="reqn">p \times 4</code> matrix with columns for
the estimated coefficient, its standard error, t-statistic and
corresponding (two-sided) p-value.  Aliased coefficients are omitted.</p>
</td></tr>
<tr><td><code>aliased</code></td>
<td>
<p>named logical vector showing if the original
coefficients are aliased.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>the square root of the estimated variance of the random
error
</p>
<p style="text-align: center;"><code class="reqn">\hat\sigma^2 = \frac{1}{n-p}\sum_i{w_i R_i^2},</code>
</p>

<p>where <code class="reqn">R_i</code> is the <code class="reqn">i</code>-th residual, <code>residuals[i]</code>.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>degrees of freedom, a 3-vector <code class="reqn">(p, n-p, p*)</code>, the first
being the number of non-aliased coefficients, the last being the total
number of coefficients.</p>
</td></tr>
<tr><td><code>fstatistic</code></td>
<td>
<p>(for models including non-intercept terms)
a 3-vector with the value of the F-statistic with
its numerator and denominator degrees of freedom.</p>
</td></tr>
<tr><td><code>r.squared</code></td>
<td>
<p><code class="reqn">R^2</code>, the &lsquo;fraction of variance explained by
the model&rsquo;,
</p>
<p style="text-align: center;"><code class="reqn">R^2 = 1 - \frac{\sum_i{R_i^2}}{\sum_i(y_i- y^*)^2},</code>
</p>

<p>where <code class="reqn">y^*</code> is the mean of <code class="reqn">y_i</code> if there is an
intercept and zero otherwise.</p>
</td></tr>
<tr><td><code>adj.r.squared</code></td>
<td>
<p>the above <code class="reqn">R^2</code> statistic
&lsquo;<em>adjusted</em>&rsquo;, penalizing for higher <code class="reqn">p</code>.</p>
</td></tr>
<tr><td><code>cov.unscaled</code></td>
<td>
<p>a <code class="reqn">p \times p</code> matrix of (unscaled)
covariances of the <code class="reqn">\hat\beta_j</code>, <code class="reqn">j=1, \dots, p</code>.</p>
</td></tr>
<tr><td><code>correlation</code></td>
<td>
<p>the correlation matrix corresponding to the above
<code>cov.unscaled</code>, if <code>correlation = TRUE</code> is specified.</p>
</td></tr>
<tr><td><code>symbolic.cor</code></td>
<td>
<p>(only if <code>correlation</code> is true.)  The value
of the argument <code>symbolic.cor</code>.</p>
</td></tr>
<tr><td><code>na.action</code></td>
<td>
<p>from <code>object</code>, if present there.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>The model fitting function <code><a href="#topic+lm">lm</a></code>, <code><a href="base.html#topic+summary">summary</a></code>.
</p>
<p>Function <code><a href="#topic+coef">coef</a></code> will extract the matrix of coefficients
with standard errors, t-statistics and p-values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##-- Continuing the  lm(.) example:
coef(lm.D90)  # the bare coefficients
sld90 &lt;- summary(lm.D90 &lt;- lm(weight ~ group -1))  # omitting intercept
sld90
coef(sld90)  # much more

## model with *aliased* coefficient:
lm.D9. &lt;- lm(weight ~ group + I(group != "Ctl"))
Sm.D9. &lt;- summary(lm.D9.)
Sm.D9. #  shows the NA NA NA NA  line
stopifnot(length(cc &lt;- coef(lm.D9.)) == 3, is.na(cc[3]),
          dim(coef(Sm.D9.)) == c(2,4), Sm.D9.$df == c(2, 18, 3))
</code></pre>

<hr>
<h2 id='summary.manova'>Summary Method for Multivariate Analysis of Variance</h2><span id='topic+summary.manova'></span><span id='topic+print.summary.manova'></span>

<h3>Description</h3>

<p>A <code>summary</code> method for class <code>"manova"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'manova'
summary(object,
        test = c("Pillai", "Wilks", "Hotelling-Lawley", "Roy"),
        intercept = FALSE, tol = 1e-7, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.manova_+3A_object">object</code></td>
<td>
<p>An object of class <code>"manova"</code> or an <code>aov</code>
object with multiple responses.</p>
</td></tr>
<tr><td><code id="summary.manova_+3A_test">test</code></td>
<td>
<p>The name of the test statistic to be used.  Partial
matching is used so the name can be abbreviated.</p>
</td></tr>
<tr><td><code id="summary.manova_+3A_intercept">intercept</code></td>
<td>
<p>logical.  If <code>TRUE</code>, the intercept term is
included in the table.</p>
</td></tr>
<tr><td><code id="summary.manova_+3A_tol">tol</code></td>
<td>
<p>tolerance to be used in deciding if the residuals are
rank-deficient: see <code><a href="Matrix.html#topic+qr">qr</a></code>.</p>
</td></tr>
<tr><td><code id="summary.manova_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>summary.manova</code> method uses a multivariate test statistic
for the summary table.  Wilks' statistic is most popular in the
literature, but the default Pillai&ndash;Bartlett statistic is recommended
by Hand and Taylor (1987).
</p>
<p>The table gives a transformation of the test statistic which has
approximately an F distribution.  The approximations used follow
S-PLUS and SAS (the latter apart from some cases of the
Hotelling&ndash;Lawley statistic), but many other distributional
approximations exist: see Anderson (1984) and
Krzanowski and Marriott (1994) for further references.  All four approximate F statistics are
the same when the term being tested has one degree of freedom, but in
other cases that for the Roy statistic is an upper bound.
</p>
<p>The tolerance <code>tol</code> is applied to the QR decomposition of the
residual correlation matrix (unless some response has essentially zero
residuals, when it is unscaled).  Thus the default value guards
against very highly correlated responses: it can be reduced but doing
so will allow rather inaccurate results and it will normally be better
to transform the responses to remove the high correlation.
</p>


<h3>Value</h3>

<p>An object of class <code>"summary.manova"</code>.  If there is a positive
residual degrees of freedom, this is a list with components
</p>
<table>
<tr><td><code>row.names</code></td>
<td>
<p>The names of the terms, the row names of the
<code>stats</code> table if present.</p>
</td></tr>
<tr><td><code>SS</code></td>
<td>
<p>A named list of sums of squares and product matrices.</p>
</td></tr>
<tr><td><code>Eigenvalues</code></td>
<td>
<p>A matrix of eigenvalues.</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>
<p>A matrix of the statistics, approximate F value,
degrees of freedom and P value.</p>
</td></tr>
</table>
<p>otherwise components <code>row.names</code>, <code>SS</code> and <code>Df</code>
(degrees of freedom) for the terms (and not the residuals).
</p>


<h3>References</h3>

<p>Anderson, T. W. (1994) <em>An Introduction to Multivariate
Statistical Analysis.</em> Wiley.
</p>
<p>Hand, D. J. and Taylor, C. C.  (1987)
<em>Multivariate Analysis of Variance and Repeated Measures.</em>
Chapman and Hall.
</p>
<p>Krzanowski, W. J. (1988) <em>Principles of Multivariate Analysis. A
User's Perspective.</em> Oxford.
</p>
<p>Krzanowski, W. J. and Marriott, F. H. C. (1994) <em>Multivariate
Analysis. Part I: Distributions, Ordination and Inference.</em> Edward Arnold.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+manova">manova</a></code>, <code><a href="#topic+aov">aov</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Example on producing plastic film from Krzanowski (1998, p. 381)
tear &lt;- c(6.5, 6.2, 5.8, 6.5, 6.5, 6.9, 7.2, 6.9, 6.1, 6.3,
          6.7, 6.6, 7.2, 7.1, 6.8, 7.1, 7.0, 7.2, 7.5, 7.6)
gloss &lt;- c(9.5, 9.9, 9.6, 9.6, 9.2, 9.1, 10.0, 9.9, 9.5, 9.4,
           9.1, 9.3, 8.3, 8.4, 8.5, 9.2, 8.8, 9.7, 10.1, 9.2)
opacity &lt;- c(4.4, 6.4, 3.0, 4.1, 0.8, 5.7, 2.0, 3.9, 1.9, 5.7,
             2.8, 4.1, 3.8, 1.6, 3.4, 8.4, 5.2, 6.9, 2.7, 1.9)
Y &lt;- cbind(tear, gloss, opacity)
rate     &lt;- gl(2,10, labels = c("Low", "High"))
additive &lt;- gl(2, 5, length = 20, labels = c("Low", "High"))

fit &lt;- manova(Y ~ rate * additive)
summary.aov(fit)             # univariate ANOVA tables
summary(fit, test = "Wilks") # ANOVA table of Wilks' lambda
summary(fit)                # same F statistics as single-df terms
</code></pre>

<hr>
<h2 id='summary.nls'>Summarizing Non-Linear Least-Squares Model Fits</h2><span id='topic+summary.nls'></span><span id='topic+print.summary.nls'></span>

<h3>Description</h3>

<p><code>summary</code> method for class <code>"nls"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nls'
summary(object, correlation = FALSE, symbolic.cor = FALSE, ...)

## S3 method for class 'summary.nls'
print(x, digits = max(3, getOption("digits") - 3),
      symbolic.cor = x$symbolic.cor,
      signif.stars = getOption("show.signif.stars"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.nls_+3A_object">object</code></td>
<td>
<p>an object of class <code>"nls"</code>.</p>
</td></tr>
<tr><td><code id="summary.nls_+3A_x">x</code></td>
<td>
<p>an object of class <code>"summary.nls"</code>, usually the result of a
call to <code>summary.nls</code>.</p>
</td></tr>
<tr><td><code id="summary.nls_+3A_correlation">correlation</code></td>
<td>
<p>logical; if <code>TRUE</code>, the correlation matrix of
the estimated parameters is returned and printed.</p>
</td></tr>
<tr><td><code id="summary.nls_+3A_digits">digits</code></td>
<td>
<p>the number of significant digits to use when printing.</p>
</td></tr>
<tr><td><code id="summary.nls_+3A_symbolic.cor">symbolic.cor</code></td>
<td>
<p>logical.  If <code>TRUE</code>, print the correlations in
a symbolic form (see <code><a href="#topic+symnum">symnum</a></code>) rather than as numbers.</p>
</td></tr>
<tr><td><code id="summary.nls_+3A_signif.stars">signif.stars</code></td>
<td>
<p>logical.  If <code>TRUE</code>, &lsquo;significance stars&rsquo;
are printed for each coefficient.</p>
</td></tr>
<tr><td><code id="summary.nls_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The distribution theory used to find the distribution of the
standard errors and of the residual standard error (for t ratios) is
based on linearization and is approximate, maybe very approximate.
</p>
<p><code>print.summary.nls</code> tries to be smart about formatting the
coefficients, standard errors, etc. and additionally gives
&lsquo;significance stars&rsquo; if <code>signif.stars</code> is <code>TRUE</code>.
</p>
<p>Correlations are printed to two decimal places (or symbolically): to
see the actual correlations print <code>summary(object)$correlation</code>
directly.
</p>


<h3>Value</h3>

<p>The function <code>summary.nls</code> computes and returns a list of summary
statistics of the fitted model given in <code>object</code>, using
the component  <code>"formula"</code> from its argument, plus
</p>
<table>
<tr><td><code>residuals</code></td>
<td>
<p>the <em>weighted</em> residuals, the usual residuals
rescaled by the square root of the weights specified in the call to
<code>nls</code>.</p>
</td></tr>
<tr><td><code>coefficients</code></td>
<td>
<p>a <code class="reqn">p \times 4</code> matrix with columns for
the estimated coefficient, its standard error, t-statistic and
corresponding (two-sided) p-value.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>the square root of the estimated variance of the random
error
</p>
<p style="text-align: center;"><code class="reqn">\hat\sigma^2 = \frac{1}{n-p}\sum_i{R_i^2},</code>
</p>

<p>where <code class="reqn">R_i</code> is the <code class="reqn">i</code>-th weighted residual.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>degrees of freedom, a 2-vector <code class="reqn">(p, n-p)</code>.  (Here and
elsewhere <code class="reqn">n</code> omits observations with zero weights.)</p>
</td></tr>
<tr><td><code>cov.unscaled</code></td>
<td>
<p>a <code class="reqn">p \times p</code> matrix of (unscaled)
covariances of the parameter estimates.</p>
</td></tr>
<tr><td><code>correlation</code></td>
<td>
<p>the correlation matrix corresponding to the above
<code>cov.unscaled</code>, if <code>correlation = TRUE</code> is specified and
there are a non-zero number of residual degrees of freedom.</p>
</td></tr>
<tr><td><code>symbolic.cor</code></td>
<td>
<p>(only if <code>correlation</code> is true.)  The value
of the argument <code>symbolic.cor</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>The model fitting function <code><a href="#topic+nls">nls</a></code>, <code><a href="base.html#topic+summary">summary</a></code>.
</p>
<p>Function <code><a href="#topic+coef">coef</a></code> will extract the matrix of coefficients
with standard errors, t-statistics and p-values.
</p>

<hr>
<h2 id='summary.princomp'>Summary method for Principal Components Analysis</h2><span id='topic+summary.princomp'></span><span id='topic+print.summary.princomp'></span>

<h3>Description</h3>

<p>The <code><a href="base.html#topic+summary">summary</a></code> method for class <code>"princomp"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'princomp'
summary(object, loadings = FALSE, cutoff = 0.1, ...)

## S3 method for class 'summary.princomp'
print(x, digits = 3, loadings = x$print.loadings,
      cutoff = x$cutoff, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.princomp_+3A_object">object</code></td>
<td>
<p>an object of class <code>"princomp"</code>, as
from <code>princomp()</code>.</p>
</td></tr>
<tr><td><code id="summary.princomp_+3A_loadings">loadings</code></td>
<td>
<p>logical. Should loadings be included?</p>
</td></tr>
<tr><td><code id="summary.princomp_+3A_cutoff">cutoff</code></td>
<td>
<p>numeric. Loadings below this cutoff in absolute value
are shown as blank in the output.</p>
</td></tr>
<tr><td><code id="summary.princomp_+3A_x">x</code></td>
<td>
<p>an object of class <code>"summary.princomp"</code>.</p>
</td></tr>
<tr><td><code id="summary.princomp_+3A_digits">digits</code></td>
<td>
<p>the number of significant digits to be used in listing
loadings.</p>
</td></tr>
<tr><td><code id="summary.princomp_+3A_...">...</code></td>
<td>
<p>arguments to be passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>object</code> with additional components <code>cutoff</code> and
<code>print.loadings</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+princomp">princomp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>summary(pc.cr &lt;- princomp(USArrests, cor = TRUE))
## The signs of the loading columns are arbitrary
print(summary(princomp(USArrests, cor = TRUE),
              loadings = TRUE, cutoff = 0.2), digits = 2)
</code></pre>

<hr>
<h2 id='supsmu'>Friedman's SuperSmoother</h2><span id='topic+supsmu'></span>

<h3>Description</h3>

<p>Smooth the (x, y) values by Friedman's &lsquo;super smoother&rsquo;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>supsmu(x, y, wt =, span = "cv", periodic = FALSE, bass = 0, trace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="supsmu_+3A_x">x</code></td>
<td>
<p>x values for smoothing</p>
</td></tr>
<tr><td><code id="supsmu_+3A_y">y</code></td>
<td>
<p>y values for smoothing</p>
</td></tr>
<tr><td><code id="supsmu_+3A_wt">wt</code></td>
<td>
<p>case weights, by default all equal</p>
</td></tr>
<tr><td><code id="supsmu_+3A_span">span</code></td>
<td>
<p>the fraction of the observations in the span of the running
lines smoother, or <code>"cv"</code> to choose this by leave-one-out
cross-validation.</p>
</td></tr>
<tr><td><code id="supsmu_+3A_periodic">periodic</code></td>
<td>
<p>if <code>TRUE</code>, the x values are assumed to be in
<code>[0, 1]</code> and of period 1.</p>
</td></tr>
<tr><td><code id="supsmu_+3A_bass">bass</code></td>
<td>
<p>controls the smoothness of the fitted curve.  Values of up
to 10 indicate increasing smoothness.</p>
</td></tr>
<tr><td><code id="supsmu_+3A_trace">trace</code></td>
<td>
<p>logical, if true, prints one line of info &ldquo;per
spar&rdquo;, notably useful for <code>"cv"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>supsmu</code> is a running lines smoother which chooses between three
spans for the lines. The running lines smoothers are symmetric, with
<code>k/2</code> data points each side of the predicted point, and values of
<code>k</code> as <code>0.5 * n</code>, <code>0.2 * n</code> and <code>0.05 * n</code>, where
<code>n</code> is the number of data points.  If <code>span</code> is specified,
a single smoother with span <code>span * n</code> is used.
</p>
<p>The best of the three smoothers is chosen by cross-validation for each
prediction. The best spans are then smoothed by a running lines
smoother and the final prediction chosen by linear interpolation.
</p>
<p>The FORTRAN code says: &ldquo;For small samples (<code>n &lt; 40</code>) or if
there are substantial serial correlations between observations close
in x-value, then a pre-specified fixed span smoother (<code>span &gt;
      0</code>) should be used.  Reasonable span values are 0.2 to 0.4.&rdquo;
</p>
<p>Cases with non-finite values of <code>x</code>, <code>y</code> or <code>wt</code> are
dropped, with a warning.
</p>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>the input values in increasing order with duplicates removed.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the corresponding y values on the fitted curve.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Friedman, J. H. (1984)
SMART User's Guide.
Laboratory for Computational Statistics, Stanford University Technical
Report No. 1.
</p>
<p>Friedman, J. H. (1984)
A variable span scatterplot smoother.
Laboratory for Computational Statistics, Stanford University Technical
Report No. 5.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ppr">ppr</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

with(cars, {
    plot(speed, dist)
    lines(supsmu(speed, dist))
    lines(supsmu(speed, dist, bass = 7), lty = 2)
    })
</code></pre>

<hr>
<h2 id='symnum'>Symbolic Number Coding</h2><span id='topic+symnum'></span>

<h3>Description</h3>

<p>Symbolically encode a given numeric or logical vector or array.
Particularly useful for visualization of structured matrices,
e.g., correlation, sparse, or logical ones.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>symnum(x, cutpoints = c(0.3, 0.6, 0.8, 0.9, 0.95),
       symbols = if(numeric.x) c(" ", ".", ",", "+", "*", "B")
                 else c(".", "|"),
       legend = length(symbols) &gt;= 3,
       na = "?", eps = 1e-5, numeric.x = is.numeric(x),
       corr = missing(cutpoints) &amp;&amp; numeric.x,
       show.max = if(corr) "1", show.min = NULL,
       abbr.colnames = has.colnames,
       lower.triangular = corr &amp;&amp; is.numeric(x) &amp;&amp; is.matrix(x),
       diag.lower.tri   = corr &amp;&amp; !is.null(show.max))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="symnum_+3A_x">x</code></td>
<td>
<p>numeric or logical vector or array.</p>
</td></tr>
<tr><td><code id="symnum_+3A_cutpoints">cutpoints</code></td>
<td>
<p>numeric vector whose values <code>cutpoints[j]</code>
<code class="reqn">= c_j</code>  (<em>after</em> augmentation, see <code>corr</code>
below) are used for intervals.</p>
</td></tr>
<tr><td><code id="symnum_+3A_symbols">symbols</code></td>
<td>
<p>character vector, one shorter than (the
<em>augmented</em>, see <code>corr</code> below) <code>cutpoints</code>.
<code>symbols[j]</code><code class="reqn">= s_j</code> are used as &lsquo;code&rsquo; for
the (half open) interval <code class="reqn">(c_j,c_{j+1}]</code>.
</p>
<p>When <code>numeric.x</code> is <code>FALSE</code>, i.e., by default when
argument <code>x</code> is <code>logical</code>, the default is
<code>c(".","|")</code> (graphical 0 / 1 s).</p>
</td></tr>
<tr><td><code id="symnum_+3A_legend">legend</code></td>
<td>
<p>logical indicating if a <code>"legend"</code> attribute is
desired.</p>
</td></tr>
<tr><td><code id="symnum_+3A_na">na</code></td>
<td>
<p>character or logical. How <code><a href="base.html#topic+NA">NA</a>s</code> are coded.  If
<code>na == FALSE</code>, <code>NA</code>s are coded invisibly, <em>including</em>
the <code>"legend"</code> attribute below, which otherwise mentions NA
coding.</p>
</td></tr>
<tr><td><code id="symnum_+3A_eps">eps</code></td>
<td>
<p>absolute precision to be used at left and right boundary.</p>
</td></tr>
<tr><td><code id="symnum_+3A_numeric.x">numeric.x</code></td>
<td>
<p>logical indicating if <code>x</code> should be treated as numbers,
otherwise as logical.</p>
</td></tr>
<tr><td><code id="symnum_+3A_corr">corr</code></td>
<td>
<p>logical.  If <code>TRUE</code>, <code>x</code> contains correlations.
The cutpoints are augmented by <code>0</code> and <code>1</code> and
<code>abs(x)</code> is coded.</p>
</td></tr>
<tr><td><code id="symnum_+3A_show.max">show.max</code></td>
<td>
<p>if <code>TRUE</code>, or of mode <code>character</code>, the
maximal cutpoint is coded especially.</p>
</td></tr>
<tr><td><code id="symnum_+3A_show.min">show.min</code></td>
<td>
<p>if <code>TRUE</code>, or of mode <code>character</code>, the
minimal cutpoint is coded especially.</p>
</td></tr>
<tr><td><code id="symnum_+3A_abbr.colnames">abbr.colnames</code></td>
<td>
<p>logical, integer or <code>NULL</code> indicating how
column names should be abbreviated (if they are); if <code>NULL</code>
(or <code>FALSE</code> and <code>x</code> has no column names),
the column names will all be empty, i.e., <code>""</code>; otherwise if
<code>abbr.colnames</code> is false, they are left unchanged.  If
<code>TRUE</code> or integer, existing column names will be abbreviated to
<code><a href="base.html#topic+abbreviate">abbreviate</a>(*, minlength = abbr.colnames)</code>.</p>
</td></tr>
<tr><td><code id="symnum_+3A_lower.triangular">lower.triangular</code></td>
<td>
<p>logical.  If <code>TRUE</code> and <code>x</code> is a
matrix, only the <em>lower triangular</em> part of the matrix is coded
as non-blank.</p>
</td></tr>
<tr><td><code id="symnum_+3A_diag.lower.tri">diag.lower.tri</code></td>
<td>
<p>logical.  If <code>lower.triangular</code> <em>and</em>
this are <code>TRUE</code>, the <em>diagonal</em> part of the matrix is
shown.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An atomic character object of class <code><a href="base.html#topic+noquote">noquote</a></code> and the same
dimensions as <code>x</code>.
</p>
<p>If <code>legend</code> is <code>TRUE</code> (as by default when there are more
than two classes), the result has an attribute <code>"legend"</code>
containing a legend of the returned character codes, in the form
</p>
<p style="text-align: center;"><code class="reqn">c_1 s_1 c_2 s_2 \dots s_n c_{n+1}</code>
</p>

<p>where <code class="reqn">c_j</code><code> = cutpoints[j]</code> and
<code class="reqn">s_j</code><code> = symbols[j]</code>.
</p>


<h3>Note</h3>

<p>The optional (mostly logical) arguments all try to use smart defaults.
Specifying them explicitly may lead to considerably improved output in
many cases.
</p>


<h3>Author(s)</h3>

<p>Martin Maechler <a href="mailto:maechler@stat.math.ethz.ch">maechler@stat.math.ethz.ch</a></p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+as.character">as.character</a></code>; <code><a href="Matrix.html#topic+image">image</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ii &lt;- setNames(0:8, 0:8)
symnum(ii, cutpoints =  2*(0:4), symbols = c(".", "-", "+", "$"))
symnum(ii, cutpoints =  2*(0:4), symbols = c(".", "-", "+", "$"), show.max = TRUE)

symnum(1:12 %% 3 == 0)  # --&gt; "|" = TRUE, "." = FALSE  for logical

## Pascal's Triangle modulo 2 -- odd and even numbers:
N &lt;- 38
pascal &lt;- t(sapply(0:N, function(n) round(choose(n, 0:N - (N-n)%/%2))))
rownames(pascal) &lt;- rep("", 1+N) # &lt;-- to improve "graphic"
symnum(pascal %% 2, symbols = c(" ", "A"), numeric.x = FALSE)

##-- Symbolic correlation matrices:
symnum(cor(attitude), diag.lower.tri = FALSE)
symnum(cor(attitude), abbr.colnames = NULL)
symnum(cor(attitude), abbr.colnames = FALSE)
symnum(cor(attitude), abbr.colnames = 2)

symnum(cor(rbind(1, rnorm(25), rnorm(25)^2)))
symnum(cor(matrix(rexp(30, 1), 5, 18))) # &lt;&lt;-- PATTERN ! --
symnum(cm1 &lt;- cor(matrix(rnorm(90) ,  5, 18))) # &lt; White Noise SMALL n
symnum(cm1, diag.lower.tri = FALSE)
symnum(cm2 &lt;- cor(matrix(rnorm(900), 50, 18))) # &lt; White Noise "BIG" n
symnum(cm2, lower.triangular = FALSE)

## NA's:
Cm &lt;- cor(matrix(rnorm(60),  10, 6)); Cm[c(3,6), 2] &lt;- NA
symnum(Cm, show.max = NULL)

## Graphical P-values (aka "significance stars"):
pval &lt;- rev(sort(c(outer(1:6, 10^-(1:3)))))
symp &lt;- symnum(pval, corr = FALSE,
               cutpoints = c(0,  .001,.01,.05, .1, 1),
               symbols = c("***","**","*","."," "))
noquote(cbind(P.val = format(pval), Signif = symp))
</code></pre>

<hr>
<h2 id='t.test'>Student's t-Test</h2><span id='topic+t.test'></span><span id='topic+t.test.default'></span><span id='topic+t.test.formula'></span>

<h3>Description</h3>

<p>Performs one and two sample t-tests on vectors of data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>t.test(x, ...)

## Default S3 method:
t.test(x, y = NULL,
       alternative = c("two.sided", "less", "greater"),
       mu = 0, paired = FALSE, var.equal = FALSE,
       conf.level = 0.95, ...)

## S3 method for class 'formula'
t.test(formula, data, subset, na.action = na.pass, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="t.test_+3A_x">x</code></td>
<td>
<p>a (non-empty) numeric vector of data values.</p>
</td></tr>
<tr><td><code id="t.test_+3A_y">y</code></td>
<td>
<p>an optional (non-empty) numeric vector of data values.</p>
</td></tr>
<tr><td><code id="t.test_+3A_alternative">alternative</code></td>
<td>
<p>a character string specifying the alternative
hypothesis, must be one of <code>"two.sided"</code> (default),
<code>"greater"</code> or <code>"less"</code>.  You can specify just the initial
letter.</p>
</td></tr>
<tr><td><code id="t.test_+3A_mu">mu</code></td>
<td>
<p>a number indicating the true value of the mean (or
difference in means if you are performing a two sample test).</p>
</td></tr>
<tr><td><code id="t.test_+3A_paired">paired</code></td>
<td>
<p>a logical indicating whether you want a paired
t-test.</p>
</td></tr>
<tr><td><code id="t.test_+3A_var.equal">var.equal</code></td>
<td>
<p>a logical variable indicating whether to treat the
two variances as being equal. If <code>TRUE</code> then the pooled
variance is used to estimate the variance otherwise the Welch
(or Satterthwaite) approximation to the degrees of freedom is used.</p>
</td></tr>
<tr><td><code id="t.test_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level of the interval.</p>
</td></tr>
<tr><td><code id="t.test_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>lhs ~ rhs</code> where <code>lhs</code>
is a numeric variable giving the data values and <code>rhs</code> either 
<code>1</code> for a one-sample or paired test or a factor
with two levels giving the corresponding groups. If <code>lhs</code> is of 
class <code>"<a href="#topic+Pair">Pair</a>"</code> and <code>rhs</code> is <code>1</code>, a paired test
is done, see Examples.</p>
</td></tr>
<tr><td><code id="t.test_+3A_data">data</code></td>
<td>
<p>an optional matrix or data frame (or similar: see
<code><a href="#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="t.test_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="t.test_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code><a href="base.html#topic+NA">NA</a></code>s.</p>
</td></tr>
<tr><td><code id="t.test_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.
For the <code>formula</code> method, this includes arguments of the
default method, but not <code>paired</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>alternative = "greater"</code> is the alternative that <code>x</code> has a
larger mean than <code>y</code>. For the one-sample case: that the mean is positive.
</p>
<p>If <code>paired</code> is <code>TRUE</code> then both <code>x</code> and <code>y</code> must
be specified and they must be the same length.  Missing values are
silently removed (in pairs if <code>paired</code> is <code>TRUE</code>).  If
<code>var.equal</code> is <code>TRUE</code> then the pooled estimate of the
variance is used.  By default, if <code>var.equal</code> is <code>FALSE</code>
then the variance is estimated separately for both groups and the
Welch modification to the degrees of freedom is used.
</p>
<p>If the input data are effectively constant (compared to the larger of the
two means) an error is generated.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the t-statistic.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of freedom for the t-statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>a confidence interval for the mean appropriate to the
specified alternative hypothesis.</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>the estimated mean or difference in means depending on
whether it was a one-sample test or a two-sample test.</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>the specified hypothesized value of the mean or mean
difference depending on whether it was a one-sample test or a
two-sample test.</p>
</td></tr>
<tr><td><code>stderr</code></td>
<td>
<p>the standard error of the mean (difference), used as
denominator in the t-statistic formula.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative
hypothesis.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string indicating what type of t-test was
performed.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the name(s) of the data.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+prop.test">prop.test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Two-sample t-test
t.test(1:10, y = c(7:20))      # P = .00001855
t.test(1:10, y = c(7:20, 200)) # P = .1245    -- NOT significant anymore

## Traditional interface
with(mtcars, t.test(mpg[am == 0], mpg[am == 1]))

## Formula interface
t.test(mpg ~ am, data = mtcars)

## One-sample t-test
## Traditional interface
t.test(sleep$extra)

## Formula interface
t.test(extra ~ 1, data = sleep)

## Paired t-test
## The sleep data is actually paired, so could have been in wide format:
sleep2 &lt;- reshape(sleep, direction = "wide",
                  idvar = "ID", timevar = "group")

## Traditional interface
t.test(sleep2$extra.1, sleep2$extra.2, paired = TRUE)

## Formula interface
t.test(Pair(extra.1, extra.2) ~ 1, data = sleep2)
</code></pre>

<hr>
<h2 id='TDist'>The Student t Distribution</h2><span id='topic+TDist'></span><span id='topic+dt'></span><span id='topic+pt'></span><span id='topic+qt'></span><span id='topic+rt'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the t distribution with <code>df</code> degrees of freedom
(and optional non-centrality parameter <code>ncp</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dt(x, df, ncp, log = FALSE)
pt(q, df, ncp, lower.tail = TRUE, log.p = FALSE)
qt(p, df, ncp, lower.tail = TRUE, log.p = FALSE)
rt(n, df, ncp)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TDist_+3A_x">x</code>, <code id="TDist_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="TDist_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="TDist_+3A_n">n</code></td>
<td>
<p>number of observations. If <code>length(n) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="TDist_+3A_df">df</code></td>
<td>
<p>degrees of freedom (<code class="reqn">&gt; 0</code>, maybe non-integer).  <code>df
      = Inf</code> is allowed.</p>
</td></tr>
<tr><td><code id="TDist_+3A_ncp">ncp</code></td>
<td>
<p>non-centrality parameter <code class="reqn">\delta</code>;
currently except for <code>rt()</code>, only for <code>abs(ncp) &lt;= 37.62</code>.
If omitted, use the central t distribution.</p>
</td></tr>
<tr><td><code id="TDist_+3A_log">log</code>, <code id="TDist_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="TDist_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code class="reqn">t</code> distribution with <code>df</code> <code class="reqn">= \nu</code> degrees of
freedom has density
</p>
<p style="text-align: center;"><code class="reqn">
    f(x) = \frac{\Gamma ((\nu+1)/2)}{\sqrt{\pi \nu} \Gamma (\nu/2)}
    (1 + x^2/\nu)^{-(\nu+1)/2}%
  </code>
</p>

<p>for all real <code class="reqn">x</code>.
It has mean <code class="reqn">0</code> (for <code class="reqn">\nu &gt; 1</code>) and
variance <code class="reqn">\frac{\nu}{\nu-2}</code> (for <code class="reqn">\nu &gt; 2</code>).
</p>
<p>The general <em>non-central</em> <code class="reqn">t</code>
with parameters <code class="reqn">(\nu, \delta)</code> <code>= (df, ncp)</code>
is defined as the distribution of
<code class="reqn">T_{\nu}(\delta) := (U + \delta)/\sqrt{V/\nu}</code>
where <code class="reqn">U</code> and <code class="reqn">V</code>  are independent random
variables, <code class="reqn">U \sim {\cal N}(0,1)</code> and
<code class="reqn">V \sim \chi^2_\nu</code> (see <a href="#topic+Chisquare">Chisquare</a>).
</p>
<p>The most used applications are power calculations for <code class="reqn">t</code>-tests:<br />
Let <code class="reqn">T = \frac{\bar{X} - \mu_0}{S/\sqrt{n}}</code>
where
<code class="reqn">\bar{X}</code> is the <code><a href="base.html#topic+mean">mean</a></code> and <code class="reqn">S</code> the sample standard
deviation (<code><a href="#topic+sd">sd</a></code>) of <code class="reqn">X_1, X_2, \dots, X_n</code> which are
i.i.d. <code class="reqn">{\cal N}(\mu, \sigma^2)</code>
Then <code class="reqn">T</code> is distributed as non-central <code class="reqn">t</code> with
<code>df</code><code class="reqn">{} = n-1</code>
degrees of freedom and <b>n</b>on-<b>c</b>entrality <b>p</b>arameter
<code>ncp</code><code class="reqn">{} = (\mu - \mu_0) \sqrt{n}/\sigma</code>.
</p>
<p>The <code class="reqn">t</code> distribution's cumulative distribution function (<abbr>cdf</abbr>),
<code class="reqn">F_{\nu}</code> fulfills
<code class="reqn">F_{\nu}(t) =    \frac 1 2 I_x(\frac{\nu}{2}, \frac 1 2),</code>  for <code class="reqn">t \le 0</code>, and
<code class="reqn">F_{\nu}(t) = 1- \frac 1 2 I_x(\frac{\nu}{2}, \frac 1 2),</code>  for <code class="reqn">t \ge 0</code>,
where
<code class="reqn">x := \nu/(\nu + t^2)</code>, and <code class="reqn">I_x(a,b)</code> is the
incomplete beta function, in <span class="rlang"><b>R</b></span> this is <code><a href="#topic+pbeta">pbeta</a>(x, a,b)</code>.
</p>


<h3>Value</h3>

<p><code>dt</code> gives the density,
<code>pt</code> gives the distribution function,
<code>qt</code> gives the quantile function, and
<code>rt</code> generates random deviates.
</p>
<p>Invalid arguments will result in return value <code>NaN</code>, with a warning.
</p>
<p>The length of the result is determined by <code>n</code> for
<code>rt</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.
</p>
<p>The numerical arguments other than <code>n</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.
</p>


<h3>Note</h3>

<p>Supplying <code>ncp = 0</code> uses the algorithm for the non-central
distribution, which is not the same algorithm used if <code>ncp</code> is
omitted.  This is to give consistent behaviour in extreme cases with
values of <code>ncp</code> very near zero.
</p>
<p>The code for non-zero <code>ncp</code> is principally intended to be used
for moderate values of <code>ncp</code>: it will not be highly accurate,
especially in the tails, for large values.
</p>


<h3>Source</h3>

<p>The central <code>dt</code> is computed via an accurate formula
provided by Catherine Loader (see the reference in <code><a href="#topic+dbinom">dbinom</a></code>).
</p>
<p>For the non-central case of <code>dt</code>, C code contributed by
Claus Ekstrm based on the relationship (for
<code class="reqn">x \neq 0</code>) to the cumulative distribution.
</p>
<p>For the central case of <code>pt</code>, a normal approximation in the
tails, otherwise via <code><a href="#topic+pbeta">pbeta</a></code>.
</p>
<p>For the non-central case of <code>pt</code> based on a C translation of
</p>
<p>Lenth, R. V. (1989). <em>Algorithm AS 243</em> &mdash;
Cumulative distribution function of the non-central <code class="reqn">t</code> distribution,
<em>Applied Statistics</em> <b>38</b>, 185&ndash;189.
</p>
<p>This computes the lower tail only, so the upper tail suffers from
cancellation and a warning will be given when this is likely to be
significant.
</p>
<p>For central <code>qt</code>, a C translation of
</p>
<p>Hill, G. W. (1970) Algorithm 396: Student's t-quantiles.
<em>Communications of the ACM</em>, <b>13(10)</b>, 619&ndash;620. 
</p>
<p>altered to take account of
</p>
<p>Hill, G. W. (1981) Remark on Algorithm 396, <em>ACM Transactions on
Mathematical Software</em>, <b>7</b>, 250&ndash;1. 
</p>
<p>The non-central case is done by inversion.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole. (Except non-central versions.)
</p>
<p>Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
<em>Continuous Univariate Distributions</em>, volume 2, chapters 28 and 31.
Wiley, New York.
</p>


<h3>See Also</h3>

<p><a href="#topic+Distributions">Distributions</a> for other standard distributions, including
<code><a href="#topic+df">df</a></code> for the F distribution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

1 - pt(1:5, df = 1)
qt(.975, df = c(1:10,20,50,100,1000))

tt &lt;- seq(0, 10, length.out = 21)
ncp &lt;- seq(0, 6, length.out = 31)
ptn &lt;- outer(tt, ncp, function(t, d) pt(t, df = 3, ncp = d))
t.tit &lt;- "Non-central t - Probabilities"
image(tt, ncp, ptn, zlim = c(0,1), main = t.tit)
persp(tt, ncp, ptn, zlim = 0:1, r = 2, phi = 20, theta = 200, main = t.tit,
      xlab = "t", ylab = "non-centrality parameter",
      zlab = "Pr(T &lt;= t)")

plot(function(x) dt(x, df = 3, ncp = 2), -3, 11, ylim = c(0, 0.32),
     main = "Non-central t - Density", yaxs = "i")

## Relation between F_t(.) = pt(x, n) and pbeta():
ptBet &lt;- function(t, n) {
    x &lt;- n/(n + t^2)
    r &lt;- pb &lt;- pbeta(x, n/2, 1/2) / 2
    pos &lt;- t &gt; 0
    r[pos] &lt;- 1 - pb[pos]
    r
}
x &lt;- seq(-5, 5, by = 1/8)
nu &lt;- 3:10
pt. &lt;- outer(x, nu, pt)
ptB &lt;- outer(x, nu, ptBet)
## matplot(x, pt., type = "l")
stopifnot(all.equal(pt., ptB, tolerance = 1e-15))
</code></pre>

<hr>
<h2 id='termplot'>Plot Regression Terms</h2><span id='topic+termplot'></span>

<h3>Description</h3>

<p>Plots regression terms against their predictors, optionally with
standard errors and partial residuals added.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>termplot(model, data = NULL, envir = environment(formula(model)),
         partial.resid = FALSE, rug = FALSE,
         terms = NULL, se = FALSE,
         xlabs = NULL, ylabs = NULL, main = NULL,
         col.term = 2, lwd.term = 1.5,
         col.se = "orange", lty.se = 2, lwd.se = 1,
         col.res = "gray", cex = 1, pch = par("pch"),
         col.smth = "darkred", lty.smth = 2, span.smth = 2/3,
         ask = dev.interactive() &amp;&amp; nb.fig &lt; n.tms,
         use.factor.levels = TRUE, smooth = NULL, ylim = "common",
         plot = TRUE, transform.x = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="termplot_+3A_model">model</code></td>
<td>
<p>fitted model object</p>
</td></tr>
<tr><td><code id="termplot_+3A_data">data</code></td>
<td>
<p>data frame in which variables in <code>model</code> can be
found</p>
</td></tr>
<tr><td><code id="termplot_+3A_envir">envir</code></td>
<td>
<p>environment in which variables in <code>model</code> can be found</p>
</td></tr>
<tr><td><code id="termplot_+3A_partial.resid">partial.resid</code></td>
<td>
<p>logical; should partial residuals be plotted?</p>
</td></tr>
<tr><td><code id="termplot_+3A_rug">rug</code></td>
<td>
<p>add <a href="graphics.html#topic+rug">rug</a>plots (jittered 1-d histograms) to the axes?</p>
</td></tr>
<tr><td><code id="termplot_+3A_terms">terms</code></td>
<td>
<p>which terms to plot (default <code>NULL</code> means all
terms); a vector passed to
<code><a href="#topic+predict">predict</a>(.., type = "terms", terms = *)</code>.</p>
</td></tr>
<tr><td><code id="termplot_+3A_se">se</code></td>
<td>
<p>plot pointwise standard errors?</p>
</td></tr>
<tr><td><code id="termplot_+3A_xlabs">xlabs</code></td>
<td>
<p>vector of labels for the x axes</p>
</td></tr>
<tr><td><code id="termplot_+3A_ylabs">ylabs</code></td>
<td>
<p>vector of labels for the y axes</p>
</td></tr>
<tr><td><code id="termplot_+3A_main">main</code></td>
<td>
<p>logical, or vector of main titles;  if <code>TRUE</code>, the
model's call is taken as main title, <code>NULL</code> or <code>FALSE</code> mean
no titles.</p>
</td></tr>
<tr><td><code id="termplot_+3A_col.term">col.term</code>, <code id="termplot_+3A_lwd.term">lwd.term</code></td>
<td>
<p>color and line width for the &lsquo;term curve&rsquo;,
see <code><a href="graphics.html#topic+lines">lines</a></code>.</p>
</td></tr>
<tr><td><code id="termplot_+3A_col.se">col.se</code>, <code id="termplot_+3A_lty.se">lty.se</code>, <code id="termplot_+3A_lwd.se">lwd.se</code></td>
<td>
<p>color, line type and line width for the
&lsquo;twice-standard-error curve&rsquo; when <code>se = TRUE</code>.</p>
</td></tr>
<tr><td><code id="termplot_+3A_col.res">col.res</code>, <code id="termplot_+3A_cex">cex</code>, <code id="termplot_+3A_pch">pch</code></td>
<td>
<p>color, plotting character expansion and type
for partial residuals, when <code>partial.resid = TRUE</code>, see
<code><a href="graphics.html#topic+points">points</a></code>.</p>
</td></tr>
<tr><td><code id="termplot_+3A_ask">ask</code></td>
<td>
<p>logical; if <code>TRUE</code>, the user is <em>ask</em>ed before
each plot, see <code><a href="graphics.html#topic+par">par</a>(ask=.)</code>.</p>
</td></tr>
<tr><td><code id="termplot_+3A_use.factor.levels">use.factor.levels</code></td>
<td>
<p>Should x-axis ticks use factor levels or
numbers for factor terms?</p>
</td></tr>
<tr><td><code id="termplot_+3A_smooth">smooth</code></td>
<td>
<p><code>NULL</code> or a function with the same arguments as
<code><a href="graphics.html#topic+panel.smooth">panel.smooth</a></code> to draw a smooth through the partial
residuals for non-factor terms</p>
</td></tr>
<tr><td><code id="termplot_+3A_lty.smth">lty.smth</code>, <code id="termplot_+3A_col.smth">col.smth</code>, <code id="termplot_+3A_span.smth">span.smth</code></td>
<td>
<p>Passed to <code>smooth</code></p>
</td></tr>
<tr><td><code id="termplot_+3A_ylim">ylim</code></td>
<td>
<p>an optional range for the y axis, or <code>"common"</code> when
a range sufficient for all the plot will be computed, or
<code>"free"</code> when limits are computed for each plot.</p>
</td></tr>
<tr><td><code id="termplot_+3A_plot">plot</code></td>
<td>
<p>if set to <code>FALSE</code> plots are not produced: instead a
list is returned containing the data that would have been plotted.</p>
</td></tr>
<tr><td><code id="termplot_+3A_transform.x">transform.x</code></td>
<td>
<p>logical vector; if an element (recycled as necessary)
is <code>TRUE</code>, partial residuals for the corresponding term are
plotted against transformed values.
The model response is then a straight line, allowing a
ready comparison against the data or against the curve
obtained from <code>smooth-panel.smooth</code>.</p>
</td></tr>
<tr><td><code id="termplot_+3A_...">...</code></td>
<td>
<p>other graphical parameters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>model</code> object must have a <code>predict</code> method that accepts
<code>type = "terms"</code>, e.g., <code><a href="#topic+glm">glm</a></code> in the <span class="pkg">stats</span> package,
<code><a href="survival.html#topic+coxph">coxph</a></code> and <code><a href="survival.html#topic+survreg">survreg</a></code> in
the <a href="https://CRAN.R-project.org/package=survival"><span class="pkg">survival</span></a> package.
</p>
<p>For the <code>partial.resid = TRUE</code> option <code>model</code> must have a
<code><a href="#topic+residuals">residuals</a></code> method that accepts <code>type = "partial"</code>,
which <code><a href="#topic+lm">lm</a></code> and <code><a href="#topic+glm">glm</a></code> do.
</p>
<p>The <code>data</code> argument should rarely be needed, but in some cases
<code>termplot</code> may be unable to reconstruct the original data
frame. Using <code>na.action=na.exclude</code> makes these problems less likely.
</p>
<p>Nothing sensible happens for interaction terms, and they may cause errors.
</p>
<p>The <code>plot = FALSE</code> option is useful when some special action is needed,
e.g. to overlay the results of two different models or to plot
confidence bands.
</p>


<h3>Value</h3>

<p>For <code>plot = FALSE</code>, a list with one element for each plot which
would have been produced.  Each element of the list is a data frame
with variables <code>x</code>, <code>y</code>, and optionally the pointwise
standard errors <code>se</code>. For continuous predictors <code>x</code> will
contain the ordered unique values and for a factor it will be a factor
containing one instance of each level.  The list has attribute
<code>"constant"</code> copied from the predicted terms object.
</p>
<p>Otherwise, the number of terms, invisibly.
</p>


<h3>See Also</h3>

<p>For (generalized) linear models, <code><a href="#topic+plot.lm">plot.lm</a></code> and
<code><a href="#topic+predict.glm">predict.glm</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

had.splines &lt;- "package:splines" %in% search()
if(!had.splines) rs &lt;- require(splines)
x &lt;- 1:100
z &lt;- factor(rep(LETTERS[1:4], 25))
y &lt;- rnorm(100, sin(x/10)+as.numeric(z))
model &lt;- glm(y ~ ns(x, 6) + z)

par(mfrow = c(2,2)) ## 2 x 2 plots for same model :
termplot(model, main = paste("termplot( ", deparse(model$call)," ...)"))
termplot(model, rug = TRUE)
termplot(model, partial.resid = TRUE, se = TRUE, main = TRUE)
termplot(model, partial.resid = TRUE, smooth = panel.smooth, span.smth = 1/4)
if(!had.splines &amp;&amp; rs) detach("package:splines")

if(requireNamespace("MASS", quietly = TRUE)) {
hills.lm &lt;- lm(log(time) ~ log(climb)+log(dist), data = MASS::hills)
termplot(hills.lm, partial.resid = TRUE, smooth = panel.smooth,
        terms = "log(dist)", main = "Original")
termplot(hills.lm, transform.x = TRUE,
         partial.resid = TRUE, smooth = panel.smooth,
	 terms = "log(dist)", main = "Transformed")

}</code></pre>

<hr>
<h2 id='terms'>Model Terms</h2><span id='topic+terms'></span><span id='topic+labels.terms'></span>

<h3>Description</h3>

<p>The function <code>terms</code> is a generic function
which can be used to extract <em>terms</em> objects
from various kinds of <span class="rlang"><b>R</b></span> data objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>terms(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="terms_+3A_x">x</code></td>
<td>
<p>object used to select a method to dispatch.</p>
</td></tr>
<tr><td><code id="terms_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are methods for classes <code>"aovlist"</code>, and <code>"terms"</code>
<code>"formula"</code> (see <code><a href="#topic+terms.formula">terms.formula</a></code>):
the default method just extracts the <code>terms</code> component of the
object, or failing that a <code>"terms"</code> attribute (as used by
<code><a href="#topic+model.frame">model.frame</a></code>).
</p>
<p>There are <code><a href="base.html#topic+print">print</a></code> and <code><a href="base.html#topic+labels">labels</a></code> methods for
class <code>"terms"</code>: the latter prints the term labels (see
<code><a href="#topic+terms.object">terms.object</a></code>).
</p>


<h3>Value</h3>

<p>An object of class <code>c("terms", "formula")</code> which contains the
<em>terms</em> representation of a symbolic model.  See
<code><a href="#topic+terms.object">terms.object</a></code> for its structure.
</p>


<h3>References</h3>

<p>Chambers, J. M. and Hastie, T. J. (1992)
<em>Statistical models.</em>
Chapter 2 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+terms.object">terms.object</a></code>, <code><a href="#topic+terms.formula">terms.formula</a></code>,
<code><a href="#topic+lm">lm</a></code>, <code><a href="#topic+glm">glm</a></code>, <code><a href="#topic+formula">formula</a></code>.
</p>

<hr>
<h2 id='terms.formula'>Construct a terms Object from a Formula</h2><span id='topic+terms.formula'></span>

<h3>Description</h3>

<p>This function takes a formula and some optional arguments and
constructs a terms object. The terms object can then be used to
construct a <code><a href="#topic+model.matrix">model.matrix</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'formula'
terms(x, specials = NULL, abb = NULL, data = NULL, neg.out = TRUE,
      keep.order = FALSE, simplify = FALSE, ...,
      allowDotAsName = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="terms.formula_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+formula">formula</a></code>.</p>
</td></tr>
<tr><td><code id="terms.formula_+3A_specials">specials</code></td>
<td>
<p>which functions in the formula should be marked as
special in the <code>terms</code> object?  A character vector or <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="terms.formula_+3A_abb">abb</code></td>
<td>
<p>Not implemented in <span class="rlang"><b>R</b></span>; deprecated.</p>
</td></tr>
<tr><td><code id="terms.formula_+3A_data">data</code></td>
<td>
<p>a data frame from which the meaning of the special symbol
<code>.</code> can be inferred.  It is used only if there is a <code>.</code> in
the formula.</p>
</td></tr>
<tr><td><code id="terms.formula_+3A_neg.out">neg.out</code></td>
<td>
<p>Not implemented in <span class="rlang"><b>R</b></span>; deprecated.</p>
</td></tr>
<tr><td><code id="terms.formula_+3A_keep.order">keep.order</code></td>
<td>
<p>a logical value indicating whether the terms should
keep their positions.  By default, when <code>FALSE</code>, the terms are reordered so
that main effects come first, followed by the interactions,
all second-order, all third-order and so on.  Effects of a given
order are kept in the order specified.</p>
</td></tr>
<tr><td><code id="terms.formula_+3A_simplify">simplify</code></td>
<td>
<p>should the formula be expanded and simplified, the
pre-1.7.0 behaviour?</p>
</td></tr>
<tr><td><code id="terms.formula_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="terms.formula_+3A_allowdotasname">allowDotAsName</code></td>
<td>
<p>normally <code>.</code> in a formula refers to the
remaining variables contained in <code>data</code>.  Exceptionally,
<code>.</code> can be treated as a name for non-standard uses of formulae.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Not all of the options work in the same way that they do in S and not
all are implemented.
</p>


<h3>Value</h3>

<p>A <code>terms</code> object is returned.  It is the re-ordered formula (unless
<code>keep.order = TRUE</code>) with several <code><a href="base.html#topic+attributes">attributes</a></code>, see
<code><a href="#topic+terms.object">terms.object</a></code> for details.  In all
cases variables within an interaction term in the formula are
re-ordered by the ordering of the <code>"variables"</code> attribute, which
is the order in which the variables occur in the formula.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+terms">terms</a></code>, <code><a href="#topic+terms.object">terms.object</a></code>, also for examples.
</p>

<hr>
<h2 id='terms.object'>Description of Terms Objects</h2><span id='topic+terms.object'></span>

<h3>Description</h3>

<p>An object of class <code><a href="#topic+terms">terms</a></code> holds information about a
model.  Usually the model was specified in terms of a
<code><a href="#topic+formula">formula</a></code> and that formula was used to determine the terms
object.
</p>


<h3>Details</h3>

<p>The object itself is simply the result of <code><a href="#topic+terms.formula">terms.formula</a>(&lt;formula&gt;)</code>.
It has a number of attributes
and they are used to construct the model frame:
</p>

<dl>
<dt><code>factors</code></dt><dd><p>An integer matrix of variables by terms showing which variables
appear in which terms.  The entries are </p>

<dl>
<dt>0 </dt><dd><p>if the variable does not occur in the term,</p>
</dd>
<dt>1 </dt><dd><p>if it does occur and should be coded by contrasts, and</p>
</dd>
<dt>2 </dt><dd><p>if it occurs and should be coded via dummy variables for
all levels (as when a lower-order term is missing).</p>
</dd> </dl>

<p>Note that variables in main effects always receive 1,
even if the intercept is missing (in which case the first one
should be coded with dummy variables).
If there are no terms other than an intercept and offsets,
this is <code><a href="base.html#topic+integer">integer</a>(0)</code>.
</p>
</dd>
<dt><code>term.labels</code></dt><dd><p>A character vector containing the labels for each
of the terms in the model, except for offsets.  Note that these are
after possible re-ordering of terms.
</p>
<p>Non-syntactic names will be quoted by backticks: this makes it
easier to re-construct the formula from the term labels.
</p>
</dd>
<dt><code>variables</code></dt><dd><p>A call to <code>list</code> of the variables in the model.</p>
</dd>
<dt><code>intercept</code></dt><dd><p>Either 0, indicating no intercept is to be fit, or 1
indicating that an intercept is to be fit.</p>
</dd>
<dt><code>order</code></dt><dd><p>A vector of the same length as <code>term.labels</code>
indicating the order of interaction for each term.</p>
</dd>
<dt><code>response</code></dt><dd><p>The index of the variable (in variables) of the
response (the left hand side of the formula).  Zero, if there is no
response.</p>
</dd>
<dt><code>offset</code></dt><dd><p>If the model contains <code><a href="#topic+offset">offset</a></code> terms there
is an <code>offset</code> attribute indicating which variable(s) are offsets</p>
</dd>
<dt><code>specials</code></dt><dd><p>If a <code>specials</code> argument was given to
<code><a href="#topic+terms.formula">terms.formula</a></code> there is a <code>specials</code> attribute, a
pairlist of vectors (one for each specified special function) giving
numeric indices of the arguments of the list returned as the
<code>variables</code> attribute which contain these special functions.</p>
</dd>
<dt><code>dataClasses</code></dt><dd><p>optional.  A named character vector giving the classes
(as given by <code><a href="#topic+.MFclass">.MFclass</a></code>) of the variables used in a fit.</p>
</dd>
<dt><code>predvars</code></dt><dd><p>optional.  An expression to help in computing
predictions at new covariate values; see <code><a href="#topic+makepredictcall">makepredictcall</a></code>.</p>
</dd>
</dl>

<p>The object has class <code>c("terms", "formula")</code>.
</p>


<h3>Note</h3>

<p>These objects are different from those found in S.  In particular
there is no <code>formula</code> attribute: instead the object is itself a
formula.  (Thus, the mode of a terms object is different.)
</p>
<p>Examples of the <code>specials</code> argument can be seen in the
<code><a href="#topic+aov">aov</a></code> and <code><a href="survival.html#topic+coxph">coxph</a></code> functions, the
latter from package <a href="https://CRAN.R-project.org/package=survival"><span class="pkg">survival</span></a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+terms">terms</a></code>, <code><a href="#topic+formula">formula</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use of specials (as used for gam() in packages mgcv and gam)
(tf &lt;- terms(y ~ x + x:z + s(x), specials = "s"))
## Note that the "factors" attribute has variables as row names
## and term labels as column names, both as character vectors.
attr(tf, "specials")    # index 's' variable(s)
rownames(attr(tf, "factors"))[attr(tf, "specials")$s]

## we can keep the order by
terms(y ~ x + x:z + s(x), specials = "s", keep.order = TRUE)
</code></pre>

<hr>
<h2 id='time'>Sampling Times of Time Series</h2><span id='topic+time'></span><span id='topic+cycle'></span><span id='topic+frequency'></span><span id='topic+deltat'></span><span id='topic+time.default'></span>

<h3>Description</h3>

<p><code>time</code> creates the vector of times at which a time series was sampled.
</p>
<p><code>cycle</code> gives the positions in the cycle of each observation.
</p>
<p><code>frequency</code> returns the number of samples per unit time and
<code>deltat</code> the time interval between observations (see
<code><a href="#topic+ts">ts</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>time(x, ...)
## Default S3 method:
time(x, offset = 0, ts.eps = getOption("ts.eps"), ...)

cycle(x, ...)
frequency(x, ...)
deltat(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="time_+3A_x">x</code></td>
<td>
<p>a univariate or multivariate time-series, or a vector or matrix.</p>
</td></tr>
<tr><td><code id="time_+3A_offset">offset</code></td>
<td>
<p>can be used to indicate when sampling took place
in the time unit. <code>0</code> (the default) indicates the start
of the unit, <code>0.5</code> the middle and <code>1</code> the end of
the interval.</p>
</td></tr>
<tr><td><code id="time_+3A_ts.eps">ts.eps</code></td>
<td>
<p>time series comparison tolerance, used in <code>time()</code> to
determine if values close than <code>ts.eps</code> to an integer should be
<code><a href="base.html#topic+round">round</a>()</code>ed to it in order to preserve the &ldquo;year&rdquo;.</p>
</td></tr>
<tr><td><code id="time_+3A_...">...</code></td>
<td>
<p>extra arguments for future methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These are all generic functions, which will use the
<code><a href="#topic+tsp">tsp</a></code> attribute of <code>x</code> if it exists. <code>time</code>
and <code>cycle</code> have methods for class <code><a href="#topic+ts">ts</a></code> that coerce
the result to that class.
</p>
<p><code>time()</code> <code><a href="base.html#topic+round">round</a>()</code>s values close to an integer, i.e.,
closer than <code>ts.eps</code>, since <span class="rlang"><b>R</b></span> 4.3.0.  For previous behaviour,
you can call it with <code>ts.eps = 0</code>.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ts">ts</a></code>, <code><a href="#topic+start">start</a></code>, <code><a href="#topic+tsp">tsp</a></code>,
<code><a href="#topic+window">window</a></code>.
</p>
<p><code><a href="base.html#topic+date">date</a></code> for clock time, <code><a href="base.html#topic+system.time">system.time</a></code>
for CPU usage.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

cycle(presidents)
# a simple series plot
plot(as.vector(time(presidents)), as.vector(presidents), type = "l")
</code></pre>

<hr>
<h2 id='toeplitz'>Create Symmetric and Asymmetric Toeplitz Matrix</h2><span id='topic+toeplitz'></span><span id='topic+toeplitz2'></span>

<h3>Description</h3>

<p>In its simplest use, <code>toeplitz()</code> forms a symmetric Toeplitz matrix
given its first column (or row).  For the general case, asymmetric and
non-square Toeplitz matrices are formed either by specifying the first
column and row separately,
</p>
<pre>T1 &lt;- toeplitz(col, row)</pre>
<p>or by
</p>
<pre>T &lt;- toeplitz2(x, nr, nc)</pre>
<p>where only one of <code>(nr, nc)</code> needs to be specified.
In the latter case, the simple equivalence <code class="reqn">T_{i,j} = x_{i-j + n_c}</code>
is fulfilled where <code class="reqn">n_c =</code><code>ncol(T)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
toeplitz (x, r = NULL, symmetric = is.null(r))
toeplitz2(x, nrow = length(x) +1 - ncol, ncol = length(x) +1 - nrow)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="toeplitz_+3A_x">x</code></td>
<td>
<p>for <code>toeplitz(x, *)</code>: the first column of the Toeplitz matrix;
for <code>toeplitz2(x, *)</code> it is the upper-and-left border of the
Toeplitz matrix, i.e., from top-right to bottom-left, such that
<code>T[i,j] == x[i-j + ncol]</code>.</p>
</td></tr>
<tr><td><code id="toeplitz_+3A_r">r</code></td>
<td>
<p>the first row of the target Toeplitz matrix; only needed in
asymmetric cases.</p>
</td></tr>
<tr><td><code id="toeplitz_+3A_symmetric">symmetric</code></td>
<td>
<p>optional <code><a href="base.html#topic+logical">logical</a></code> indicating if the
matrix should be symmetric.</p>
</td></tr>
<tr><td><code id="toeplitz_+3A_nrow">nrow</code>, <code id="toeplitz_+3A_ncol">ncol</code></td>
<td>
<p>the number of rows and columns; only one needs to be specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code class="reqn">n \times m</code> Toeplitz matrix <code class="reqn">T</code>; for
</p>

<dl>
<dt><code>toeplitz()</code>: </dt><dd><p><code>dim(T)</code> is <code>(n,m)</code> and <code>m == length(x)</code> and
<code>n == m</code> in the symmetric case or <code>n == length(r)</code> otherwise.</p>
</dd>
<dt><code>toeplitz2()</code>: </dt><dd><p><code>dim(T) == c(nrow, ncol)</code>.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>A. Trapletti and Martin Maechler (speedup and asymmetric extensions)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 1:5
toeplitz (x)

T. &lt;- toeplitz (1:5, 11:13) # with a  *Warning* x[1] != r[1]
T2 &lt;- toeplitz2(c(13:12, 1:5), 5, 3)# this is the same matrix:
stopifnot(identical(T., T2))

# Matrix of character (could also have logical, raw, complex ..) {also warning}:
noquote(toeplitz(letters[1:4], LETTERS[20:26]))

## A convolution/smoother weight matrix :
m &lt;- 17
k &lt;- length(wts &lt;- c(76, 99, 60, 20, 1))
n &lt;- m-k+1
## Convolution
W &lt;- toeplitz2(c(rep(0, m-k), wts, rep(0, m-k)), ncol=n)

## "display" nicely :
if(requireNamespace("Matrix"))
   print(Matrix::Matrix(W))    else {
   colnames(W) &lt;- paste0(",", if(n &lt;= 9) 1:n else c(1:9, letters[seq_len(n-9)]))
   print(W)
}

## scale W to have column sums 1:
W. &lt;- W / sum(wts)
all.equal(rep(1, ncol(W.)), colSums(W.), check.attributes = FALSE)
## Visualize "mass-preserving" convolution
x &lt;- 1:n; f &lt;- function(x) exp(-((x - .4*n)/3)^2)
y &lt;- f(x) + rep_len(3:-2, n)/10
## Smoothing convolution:
y.hat &lt;- W. %*% y # y.hat := smoothed(y) ("mass preserving" -&gt; longer than y)
stopifnot(length(y.hat) == m, m == n + (k-1))
plot(x,y, type="b", xlim=c(1,m)); curve(f(x), 1,n, col="gray", lty=2, add=TRUE)
lines(1:m, y.hat, col=2, lwd=3)
rbind(sum(y), sum(y.hat)) ## mass preserved

## And, yes, convolve(y, *) does the same when called appropriately:
all.equal(c(y.hat), convolve(y, rev(wts/sum(wts)), type="open"))
</code></pre>

<hr>
<h2 id='ts'>Time-Series Objects</h2><span id='topic+ts'></span><span id='topic+as.ts'></span><span id='topic+as.ts.default'></span><span id='topic+is.ts'></span><span id='topic+Ops.ts'></span><span id='topic+cbind.ts'></span><span id='topic+is.mts'></span><span id='topic++5B.ts'></span><span id='topic+t.ts'></span>

<h3>Description</h3>

<p>The function <code>ts</code> is used to create time-series objects.
</p>
<p><code>as.ts</code> and <code>is.ts</code> coerce an object to a time-series and
test whether an object is a time series.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts(data = NA, start = 1, end = numeric(), frequency = 1,
   deltat = 1, ts.eps = getOption("ts.eps"),
   class = if(nseries &gt; 1) c("mts", "ts", "matrix", "array") else "ts",
   names = )
as.ts(x, ...)
is.ts(x)

is.mts(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_+3A_data">data</code></td>
<td>
<p>a vector or matrix of the observed time-series
values. A data frame will be coerced to a numeric matrix via
<code>data.matrix</code>.  (See also &lsquo;Details&rsquo;.)</p>
</td></tr>
<tr><td><code id="ts_+3A_start">start</code></td>
<td>
<p>the time of the first observation.  Either a single
number or a vector of two numbers (the second of which is an integer),
which specify a natural time
unit and a (1-based) number of samples into the time unit.  See
the examples for the use of the second form.</p>
</td></tr>
<tr><td><code id="ts_+3A_end">end</code></td>
<td>
<p>the time of the last observation, specified in the same way
as <code>start</code>.</p>
</td></tr>
<tr><td><code id="ts_+3A_frequency">frequency</code></td>
<td>
<p>the number of observations per unit of time.</p>
</td></tr>
<tr><td><code id="ts_+3A_deltat">deltat</code></td>
<td>
<p>the fraction of the sampling period between successive
observations; e.g., 1/12 for monthly data.  Only one of
<code>frequency</code> or <code>deltat</code> should be provided.</p>
</td></tr>
<tr><td><code id="ts_+3A_ts.eps">ts.eps</code></td>
<td>
<p>time series comparison tolerance.  Frequencies are
considered equal if their absolute difference is less than
<code>ts.eps</code>.</p>
</td></tr>
<tr><td><code id="ts_+3A_class">class</code></td>
<td>
<p>class to be given to the result, or none if <code>NULL</code>
or <code>"none"</code>.  The default is <code>"ts"</code> for a single series, or
<code>c("mts", "ts", "matrix", "array")</code> for multiple series.</p>
</td></tr>
<tr><td><code id="ts_+3A_names">names</code></td>
<td>
<p>a character vector of names for the series in a multiple
series: defaults to the colnames of <code>data</code>, or <code>"Series 1"</code>,
<code>"Series 2"</code>, ....</p>
</td></tr>
<tr><td><code id="ts_+3A_x">x</code></td>
<td>
<p>an arbitrary <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
<tr><td><code id="ts_+3A_...">...</code></td>
<td>
<p>arguments passed to methods (unused for the default method).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>ts</code> is used to create time-series objects.  These
are vectors or matrices which inherit from class <code>"ts"</code> (and have
additional
attributes) which represent data which has been sampled at equispaced
points in time.  In the matrix case, each column of the matrix
<code>data</code> is assumed to contain a single (univariate) time series.
Time series must have at least one observation, and although they need
not be numeric there is very limited support for non-numeric series.
</p>
<p>Class <code>"ts"</code> has a number of methods.  In particular arithmetic
will attempt to align time axes, and subsetting to extract subsets of
series can be used (e.g., <code>EuStockMarkets[, "DAX"]</code>).  However,
subsetting the first (or only) dimension will return a matrix or
vector, as will matrix subsetting.  Subassignment can be used to
replace values but not to extend a series (see <code><a href="#topic+window">window</a></code>).
There is a method for <code><a href="base.html#topic+t">t</a></code> that transposes the series as a
matrix (a one-column matrix if a vector) and hence returns a result
that does not inherit from class <code>"ts"</code>.
</p>
<p>Argument <code>frequency</code> indicates the sampling frequency of the
time series, with the default value <code>1</code> indicating one sample in
each unit time interval.  For
example, one could use a value of <code>7</code> for <code>frequency</code> when
the data are sampled daily, and the natural time period is a week, or
<code>12</code> when the data are sampled monthly and the natural time
period is a year.  Values of <code>4</code> and <code>12</code> are assumed in
(e.g.) <code>print</code> methods to imply a quarterly and monthly series
respectively.  <code>frequency</code> need not be a whole
number:  for example, <code>frequency = 0.2</code> would imply sampling
once every five time units.
</p>
<p><code>as.ts</code> is generic.  Its default method will use the
<code><a href="#topic+tsp">tsp</a></code> attribute of the object if it has one to set the
start and end times and frequency.
</p>
<p><code>is.ts()</code> tests if an object is a time series, i.e., inherits from
<code>"ts"</code> and is of positive length.
</p>
<p><code>is.mts(x)</code> tests if an object <code>x</code> is a multivariate time series,
i.e., fulfills <code>is.ts(x)</code>, <code>is.matrix(x)</code> and inherits from
class <code>"mts"</code>.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tsp">tsp</a></code>,
<code><a href="#topic+frequency">frequency</a></code>,
<code><a href="#topic+start">start</a></code>,
<code><a href="#topic+end">end</a></code>,
<code><a href="#topic+time">time</a></code>,
<code><a href="#topic+window">window</a></code>;
<code><a href="#topic+print.ts">print.ts</a></code>, the print method for time series objects;
<code><a href="#topic+plot.ts">plot.ts</a></code>, the plot method for time series objects.
</p>
<p>For other definitions of &lsquo;time series&rsquo; (e.g.,
time-ordered observations) see the CRAN task view at
<a href="https://CRAN.R-project.org/view=TimeSeries">https://CRAN.R-project.org/view=TimeSeries</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

ts(1:10, frequency = 4, start = c(1959, 2)) # 2nd Quarter of 1959
print( ts(1:10, frequency = 7, start = c(12, 2)), calendar = TRUE)
# print.ts(.)
## Using July 1954 as start date:
gnp &lt;- ts(cumsum(1 + round(rnorm(100), 2)),
          start = c(1954, 7), frequency = 12)
plot(gnp) # using 'plot.ts' for time-series plot

## Multivariate
z &lt;- ts(matrix(rnorm(300), 100, 3), start = c(1961, 1), frequency = 12)
class(z)
is.mts(z)
head(z) # as "matrix"
plot(z)
plot(z, plot.type = "single", lty = 1:3)

## A phase plot:
plot(nhtemp, lag(nhtemp, 1), cex = .8, col = "blue",
     main = "Lag plot of New Haven temperatures")
</code></pre>

<hr>
<h2 id='ts-methods'>Methods for Time Series Objects</h2><span id='topic+diff.ts'></span><span id='topic+na.omit.ts'></span>

<h3>Description</h3>

<p>Methods for objects of class <code>"ts"</code>, typically the result of
<code><a href="#topic+ts">ts</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ts'
diff(x, lag = 1, differences = 1, ...)

## S3 method for class 'ts'
na.omit(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts-methods_+3A_x">x</code></td>
<td>
<p>an object of class <code>"ts"</code> containing the values to be
differenced.</p>
</td></tr>
<tr><td><code id="ts-methods_+3A_lag">lag</code></td>
<td>
<p>an integer indicating which lag to use.</p>
</td></tr>
<tr><td><code id="ts-methods_+3A_differences">differences</code></td>
<td>
<p>an integer indicating the order of the difference.</p>
</td></tr>
<tr><td><code id="ts-methods_+3A_object">object</code></td>
<td>
<p>a univariate or multivariate time series.</p>
</td></tr>
<tr><td><code id="ts-methods_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>na.omit</code> method omits initial and final segments with
missing values in one or more of the series.  &lsquo;Internal&rsquo;
missing values will lead to failure.
</p>


<h3>Value</h3>

<p>For the <code>na.omit</code> method, a time series without missing values.
The class of <code>object</code> will be preserved.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+diff">diff</a></code>;
<code><a href="#topic+na.omit">na.omit</a></code>, <code><a href="#topic+na.fail">na.fail</a></code>,
<code><a href="#topic+na.contiguous">na.contiguous</a></code>.
</p>

<hr>
<h2 id='ts.plot'>Plot Multiple Time Series</h2><span id='topic+ts.plot'></span>

<h3>Description</h3>

<p>Plot several time series on a common plot. Unlike
<code><a href="#topic+plot.ts">plot.ts</a></code> the series can have a different time bases,
but they should have the same frequency.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts.plot(..., gpars = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts.plot_+3A_...">...</code></td>
<td>
<p>one or more univariate or multivariate time series.</p>
</td></tr>
<tr><td><code id="ts.plot_+3A_gpars">gpars</code></td>
<td>
<p>list of named graphics parameters to be passed to the
plotting functions.  Those commonly used can be supplied directly in
<code>...</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None.
</p>


<h3>Note</h3>

<p>Although this can be used for a single time series, <code>plot</code> is
easier to use and is preferred.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.ts">plot.ts</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

ts.plot(ldeaths, mdeaths, fdeaths,
        gpars=list(xlab="year", ylab="deaths", lty=c(1:3)))
</code></pre>

<hr>
<h2 id='ts.union'>Bind Two or More Time Series</h2><span id='topic+ts.union'></span><span id='topic+ts.intersect'></span>

<h3>Description</h3>

<p>Bind time series which have a common frequency. <code>ts.union</code> pads
with <code>NA</code>s to the total time coverage, <code>ts.intersect</code>
restricts to the time covered by all the series.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts.intersect(..., dframe = FALSE)
ts.union(..., dframe = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts.union_+3A_...">...</code></td>
<td>
<p>two or more univariate or multivariate time series, or
objects which can coerced to time series.</p>
</td></tr>
<tr><td><code id="ts.union_+3A_dframe">dframe</code></td>
<td>
<p>logical; if <code>TRUE</code> return the result as a data
frame.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As a special case, <code>...</code> can contain vectors or matrices of the
same length as the combined time series of the time series present, as
well as those of a single row.
</p>


<h3>Value</h3>

<p>A time series object if <code>dframe</code> is <code>FALSE</code>, otherwise a
data frame.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+cbind">cbind</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>ts.union(mdeaths, fdeaths)
cbind(mdeaths, fdeaths) # same as the previous line
ts.intersect(window(mdeaths, 1976), window(fdeaths, 1974, 1978))

sales1 &lt;- ts.union(BJsales, lead = BJsales.lead)
ts.intersect(sales1, lead3 = lag(BJsales.lead, -3))
</code></pre>

<hr>
<h2 id='tsdiag'>Diagnostic Plots for Time-Series Fits</h2><span id='topic+tsdiag'></span><span id='topic+tsdiag.arima0'></span><span id='topic+tsdiag.Arima'></span><span id='topic+tsdiag.StructTS'></span>

<h3>Description</h3>

<p>A generic function to plot time-series diagnostics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tsdiag(object, gof.lag, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tsdiag_+3A_object">object</code></td>
<td>
<p>a fitted time-series model</p>
</td></tr>
<tr><td><code id="tsdiag_+3A_gof.lag">gof.lag</code></td>
<td>
<p>the maximum number of lags for a Portmanteau
goodness-of-fit test</p>
</td></tr>
<tr><td><code id="tsdiag_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to particular methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function. It will generally plot the residuals,
often standardized, the autocorrelation function of the residuals, and
the p-values of a Portmanteau test for all lags up to <code>gof.lag</code>.
</p>
<p>The methods for <code><a href="#topic+arima">arima</a></code> and <code><a href="#topic+StructTS">StructTS</a></code> objects
plots residuals scaled by the estimate of their (individual) variance,
and use the Ljung&ndash;Box version of the portmanteau test.
</p>


<h3>Value</h3>

<p>None. Diagnostics are plotted.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+arima">arima</a></code>, <code><a href="#topic+StructTS">StructTS</a></code>, <code><a href="#topic+Box.test">Box.test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

fit &lt;- arima(lh, c(1,0,0))
tsdiag(fit)

## see also examples(arima)

(fit &lt;- StructTS(log10(JohnsonJohnson), type = "BSM"))
tsdiag(fit)
</code></pre>

<hr>
<h2 id='tsp'>Tsp Attribute of Time-Series-like Objects</h2><span id='topic+tsp'></span><span id='topic+tsp+3C-'></span><span id='topic+hasTsp'></span>

<h3>Description</h3>

<p><code>tsp</code> returns the <code>tsp</code> attribute (or <code>NULL</code>).
It is included for compatibility with S version 2. <code>tsp&lt;-</code>
sets the <code>tsp</code> attribute. <code>hasTsp</code> ensures <code>x</code> has a
<code>tsp</code> attribute, by adding one if needed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tsp(x)
tsp(x) &lt;- value
hasTsp(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tsp_+3A_x">x</code></td>
<td>
<p>a vector or matrix or univariate or multivariate time-series.</p>
</td></tr>
<tr><td><code id="tsp_+3A_value">value</code></td>
<td>
<p>a numeric vector of length 3 or <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>tsp</code> attribute gives the start time <em>in time units</em>,
the end time and the frequency (the number of observations per unit of
time, e.g. 12 for a monthly series).
</p>
<p>Assignments are checked for consistency.
</p>
<p>Assigning <code>NULL</code> which removes the <code>tsp</code> attribute
<em>and</em> any <code>"ts"</code> (or <code>"mts"</code>) class of <code>x</code>.
</p>


<h3>Value</h3>

<p>An object which differs from <code>x</code> only in the <code>tsp</code> attribute
(unless <code>NULL</code> is assigned).
</p>
<p><code>hasTsp</code> adds, if needed, an attribute with a start time and
frequency of 1 and end time <code><a href="base.html#topic+NROW">NROW</a>(x)</code>.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ts">ts</a></code>, <code><a href="#topic+time">time</a></code>, <code><a href="#topic+start">start</a></code>.
</p>

<hr>
<h2 id='tsSmooth'>Use Fixed-Interval Smoothing on Time Series</h2><span id='topic+tsSmooth'></span><span id='topic+tsSmooth.StructTS'></span>

<h3>Description</h3>

<p>Performs fixed-interval smoothing on a univariate time series via a
state-space model.  Fixed-interval smoothing gives the best estimate
of the state at each time point based on the whole observed series.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tsSmooth(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tsSmooth_+3A_object">object</code></td>
<td>
<p>a time-series fit.  Currently only class
<code>"<a href="#topic+StructTS">StructTS</a>"</code> is supported</p>
</td></tr>
<tr><td><code id="tsSmooth_+3A_...">...</code></td>
<td>
<p>possible arguments for future methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A time series, with as many dimensions as the state space and results
at each time point of the original series.  (For seasonal models, only
the current seasonal component is returned.)
</p>


<h3>Author(s)</h3>

<p> B. D. Ripley </p>


<h3>References</h3>

<p>Durbin, J. and Koopman, S. J. (2001) <em>Time Series Analysis by
State Space Methods.</em>  Oxford University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KalmanSmooth">KalmanSmooth</a></code>, <code><a href="#topic+StructTS">StructTS</a></code>.
</p>
<p>For examples consult <code><a href="datasets.html#topic+AirPassengers">AirPassengers</a></code>,
<code><a href="datasets.html#topic+JohnsonJohnson">JohnsonJohnson</a></code> and <code><a href="datasets.html#topic+Nile">Nile</a></code>.
</p>

<hr>
<h2 id='Tukey'>The Studentized Range Distribution</h2><span id='topic+Tukey'></span><span id='topic+ptukey'></span><span id='topic+qtukey'></span>

<h3>Description</h3>

<p>Functions of the distribution of the studentized range, <code class="reqn">R/s</code>,
where <code class="reqn">R</code> is the range of a standard normal sample and
<code class="reqn">df \times s^2</code> is independently distributed as
chi-squared with <code class="reqn">df</code> degrees of freedom, see <code><a href="#topic+pchisq">pchisq</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ptukey(q, nmeans, df, nranges = 1, lower.tail = TRUE, log.p = FALSE)
qtukey(p, nmeans, df, nranges = 1, lower.tail = TRUE, log.p = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Tukey_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="Tukey_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="Tukey_+3A_nmeans">nmeans</code></td>
<td>
<p>sample size for range (same for each group).</p>
</td></tr>
<tr><td><code id="Tukey_+3A_df">df</code></td>
<td>
<p>degrees of freedom for <code class="reqn">s</code> (see below).</p>
</td></tr>
<tr><td><code id="Tukey_+3A_nranges">nranges</code></td>
<td>
<p>number of <em>groups</em> whose <b>maximum</b> range is
considered.</p>
</td></tr>
<tr><td><code id="Tukey_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="Tukey_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code class="reqn">n_g =</code><code>nranges</code> is greater than one, <code class="reqn">R</code> is
the <em>maximum</em> of <code class="reqn">n_g</code> groups of <code>nmeans</code>
observations each.
</p>


<h3>Value</h3>

<p><code>ptukey</code> gives the distribution function and <code>qtukey</code> its
inverse, the quantile function.
</p>
<p>The length of the result is the maximum of the lengths of the
numerical arguments.  The other numerical arguments are recycled
to that length.  Only the first elements of the logical arguments
are used.  
</p>


<h3>Note</h3>

<p>A Legendre 16-point formula is used for the integral of <code>ptukey</code>.
The computations are relatively expensive, especially for
<code>qtukey</code> which uses a simple secant method for finding the
inverse of <code>ptukey</code>.
<code>qtukey</code> will be accurate to the 4th decimal place.
</p>


<h3>Source</h3>

<p><code>qtukey</code> is in part adapted from Odeh and Evans (1974).
</p>


<h3>References</h3>

<p>Copenhaver, Margaret Diponzio and Holland, Burt S. (1988).
Computation of the distribution of the maximum studentized range
statistic with application to multiple significance testing of simple
effects.
<em>Journal of Statistical Computation and Simulation</em>, <b>30</b>,
1&ndash;15.
<a href="https://doi.org/10.1080/00949658808811082">doi:10.1080/00949658808811082</a>.
</p>
<p>Odeh, R. E.  and  Evans, J. O. (1974).
Algorithm AS 70: Percentage Points of the Normal Distribution.
<em>Applied Statistics</em>, <b>23</b>, 96&ndash;97.
<a href="https://doi.org/10.2307/2347061">doi:10.2307/2347061</a>.
</p>


<h3>See Also</h3>

<p><a href="#topic+Distributions">Distributions</a> for standard distributions, including
<code><a href="#topic+pnorm">pnorm</a></code> and <code><a href="#topic+qnorm">qnorm</a></code> for the corresponding
functions for the normal distribution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(interactive())
  curve(ptukey(x, nm = 6, df = 5), from = -1, to = 8, n = 101)
(ptt &lt;- ptukey(0:10, 2, df =  5))
(qtt &lt;- qtukey(.95, 2, df =  2:11))
## The precision may be not much more than about 8 digits:
summary(abs(.95 - ptukey(qtt, 2, df = 2:11)))
</code></pre>

<hr>
<h2 id='TukeyHSD'>Compute Tukey Honest Significant Differences</h2><span id='topic+TukeyHSD'></span>

<h3>Description</h3>

<p>Create a set of confidence intervals on the differences between the
means of the levels of a factor with the specified family-wise
probability of coverage.  The intervals are based on the Studentized
range statistic, Tukey's &lsquo;Honest Significant Difference&rsquo;
method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TukeyHSD(x, which, ordered = FALSE, conf.level = 0.95, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TukeyHSD_+3A_x">x</code></td>
<td>
<p>A fitted model object, usually an <code><a href="#topic+aov">aov</a></code> fit.</p>
</td></tr>
<tr><td><code id="TukeyHSD_+3A_which">which</code></td>
<td>
<p>A character vector listing terms in the fitted model for
which the intervals should be calculated.  Defaults to all the
terms.</p>
</td></tr>
<tr><td><code id="TukeyHSD_+3A_ordered">ordered</code></td>
<td>
<p>A logical value indicating if the levels of the factor
should be ordered according to increasing average in the sample
before taking differences.  If <code>ordered</code> is true then
the calculated differences in the means will all be positive.  The
significant differences will be those for which the <code>lwr</code> end
point is positive.</p>
</td></tr>
<tr><td><code id="TukeyHSD_+3A_conf.level">conf.level</code></td>
<td>
<p>A numeric value between zero and one giving the
family-wise confidence level to use.</p>
</td></tr>
<tr><td><code id="TukeyHSD_+3A_...">...</code></td>
<td>
<p>Optional additional arguments.  None are used at present.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function: the description here applies to the method
for fits of class <code>"aov"</code>.
</p>
<p>When comparing the means for the levels of a factor in an analysis of
variance, a simple comparison using t-tests will inflate the
probability of declaring a significant difference when it is not in
fact present.  This because the intervals are calculated with a
given coverage probability for each interval but the interpretation of
the coverage is usually with respect to the entire family of
intervals.
</p>
<p>John Tukey introduced intervals based on the range of the
sample means rather than the individual differences.  The intervals
returned by this function are based on this Studentized range
statistics.
</p>
<p>The intervals constructed in this way would only apply exactly to
balanced designs where there are the same number of observations made
at each level of the factor.  This function incorporates an adjustment
for sample size that produces sensible intervals for mildly unbalanced
designs.
</p>
<p>If <code>which</code> specifies non-factor terms these will be dropped with
a warning: if no terms are left this is an error.
</p>


<h3>Value</h3>

<p>A list of class <code>c("multicomp", "TukeyHSD")</code>,
with one component for each term requested in <code>which</code>.
Each component is a matrix with columns <code>diff</code> giving the
difference in the observed means, <code>lwr</code> giving the lower
end point of the interval, <code>upr</code> giving the upper end point
and <code>p adj</code> giving the p-value after adjustment for the multiple
comparisons.
</p>
<p>There are <code>print</code> and <code>plot</code> methods for class
<code>"TukeyHSD"</code>.  The <code>plot</code> method does not accept
<code>xlab</code>, <code>ylab</code> or <code>main</code> arguments and creates its own
values for each plot.
</p>


<h3>Author(s)</h3>

<p>Douglas Bates
</p>


<h3>References</h3>

<p>Miller, R. G. (1981)
<em>Simultaneous Statistical Inference</em>. Springer.
</p>
<p>Yandell, B. S. (1997)
<em>Practical Data Analysis for Designed Experiments</em>.
Chapman &amp; Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+aov">aov</a></code>, <code><a href="#topic+qtukey">qtukey</a></code>, <code><a href="#topic+model.tables">model.tables</a></code>,
<code><a href="multcomp.html#topic+glht">glht</a></code> in package <a href="https://CRAN.R-project.org/package=multcomp"><span class="pkg">multcomp</span></a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

summary(fm1 &lt;- aov(breaks ~ wool + tension, data = warpbreaks))
TukeyHSD(fm1, "tension", ordered = TRUE)
plot(TukeyHSD(fm1, "tension"))
</code></pre>

<hr>
<h2 id='Uniform'>The Uniform Distribution</h2><span id='topic+Uniform'></span><span id='topic+dunif'></span><span id='topic+punif'></span><span id='topic+qunif'></span><span id='topic+runif'></span>

<h3>Description</h3>

<p>These functions provide information about the uniform distribution
on the interval from <code>min</code> to <code>max</code>.  <code>dunif</code> gives the
density, <code>punif</code> gives the distribution function <code>qunif</code>
gives the quantile function and <code>runif</code> generates random
deviates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dunif(x, min = 0, max = 1, log = FALSE)
punif(q, min = 0, max = 1, lower.tail = TRUE, log.p = FALSE)
qunif(p, min = 0, max = 1, lower.tail = TRUE, log.p = FALSE)
runif(n, min = 0, max = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Uniform_+3A_x">x</code>, <code id="Uniform_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="Uniform_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="Uniform_+3A_n">n</code></td>
<td>
<p>number of observations. If <code>length(n) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="Uniform_+3A_min">min</code>, <code id="Uniform_+3A_max">max</code></td>
<td>
<p>lower and upper limits of the distribution.  Must be finite.</p>
</td></tr>
<tr><td><code id="Uniform_+3A_log">log</code>, <code id="Uniform_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="Uniform_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>min</code> or <code>max</code> are not specified they assume the default
values of <code>0</code> and <code>1</code> respectively.
</p>
<p>The uniform distribution has density
</p>
<p style="text-align: center;"><code class="reqn">f(x) = \frac{1}{max-min}</code>
</p>

<p>for <code class="reqn">min \le x \le max</code>.
</p>
<p>For the case of <code class="reqn">u := min == max</code>, the limit case of
<code class="reqn">X \equiv u</code> is assumed, although there is no density in
that case and <code>dunif</code> will return <code>NaN</code> (the error condition).
</p>
<p><code>runif</code> will not generate either of the extreme values unless
<code>max = min</code> or <code>max-min</code> is small compared to <code>min</code>,
and in particular not for the default arguments.
</p>


<h3>Value</h3>

<p><code>dunif</code> gives the density,
<code>punif</code> gives the distribution function,
<code>qunif</code> gives the quantile function, and
<code>runif</code> generates random deviates.
</p>
<p>The length of the result is determined by <code>n</code> for
<code>runif</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.  
</p>
<p>The numerical arguments other than <code>n</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.
</p>


<h3>Note</h3>

<p>The characteristics of output from pseudo-random number generators
(such as precision and periodicity) vary widely.  See
<code><a href="base.html#topic+.Random.seed">.Random.seed</a></code> for more information on <span class="rlang"><b>R</b></span>'s random number
generation algorithms.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+RNG">RNG</a></code> about random number generation in <span class="rlang"><b>R</b></span>.
</p>
<p><a href="#topic+Distributions">Distributions</a> for other standard distributions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>u &lt;- runif(20)

## The following relations always hold :
punif(u) == u
dunif(u) == 1

var(runif(10000))  #- ~ = 1/12 = .08333
</code></pre>

<hr>
<h2 id='uniroot'>One Dimensional Root (Zero) Finding</h2><span id='topic+uniroot'></span>

<h3>Description</h3>

<p>The function <code>uniroot</code> searches the interval from <code>lower</code>
to <code>upper</code> for a root (i.e., zero) of the function <code>f</code> with
respect to its first argument.
</p>
<p>Setting <code>extendInt</code> to a non-<code>"no"</code> string, means searching
for the correct <code>interval = c(lower,upper)</code> if <code>sign(f(x))</code>
does not satisfy the requirements at the interval end points; see the
&lsquo;Details&rsquo; section.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>uniroot(f, interval, ...,
        lower = min(interval), upper = max(interval),
        f.lower = f(lower, ...), f.upper = f(upper, ...),
        extendInt = c("no", "yes", "downX", "upX"), check.conv = FALSE,
        tol = .Machine$double.eps^0.25, maxiter = 1000, trace = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="uniroot_+3A_f">f</code></td>
<td>
<p>the function for which the root is sought.</p>
</td></tr>
<tr><td><code id="uniroot_+3A_interval">interval</code></td>
<td>
<p>a vector containing the end-points of the interval
to be searched for the root.</p>
</td></tr>
<tr><td><code id="uniroot_+3A_...">...</code></td>
<td>
<p>additional named or unnamed arguments to be passed
to <code>f</code></p>
</td></tr>
<tr><td><code id="uniroot_+3A_lower">lower</code>, <code id="uniroot_+3A_upper">upper</code></td>
<td>
<p>the lower and upper end points of the interval to
be searched.</p>
</td></tr>
<tr><td><code id="uniroot_+3A_f.lower">f.lower</code>, <code id="uniroot_+3A_f.upper">f.upper</code></td>
<td>
<p>the same as <code>f(upper)</code> and
<code>f(lower)</code>, respectively.  Passing these values from the caller
where they are often known is more economical as soon as <code>f()</code>
contains non-trivial computations.</p>
</td></tr>
<tr><td><code id="uniroot_+3A_extendint">extendInt</code></td>
<td>
<p>character string specifying if the interval
<code>c(lower,upper)</code> should be extended or directly produce an error
when <code>f()</code> does not have differing signs at the endpoints.  The
default, <code>"no"</code>, keeps the search interval and hence produces
an error.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="uniroot_+3A_check.conv">check.conv</code></td>
<td>
<p>logical indicating whether a convergence warning of the
underlying <code><a href="#topic+uniroot">uniroot</a></code> should be caught as an error and if
non-convergence in <code>maxiter</code> iterations should be an error
instead of a warning.</p>
</td></tr>
<tr><td><code id="uniroot_+3A_tol">tol</code></td>
<td>
<p>the desired accuracy (convergence tolerance).</p>
</td></tr>
<tr><td><code id="uniroot_+3A_maxiter">maxiter</code></td>
<td>
<p>the maximum number of iterations.</p>
</td></tr>
<tr><td><code id="uniroot_+3A_trace">trace</code></td>
<td>
<p>integer number; if positive, tracing information is
produced.  Higher values giving more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that arguments after <code>...</code> must be matched exactly.
</p>
<p>Either <code>interval</code> or both <code>lower</code> and <code>upper</code> must be
specified: the upper endpoint must be strictly larger than the lower
endpoint.
The function values at the endpoints must be of opposite signs (or
zero), for <code>extendInt="no"</code>, the default.  Otherwise, if
<code>extendInt="yes"</code>, the interval is extended on both sides, in
search of a sign change, i.e., until the search interval <code class="reqn">[l,u]</code>
satisfies <code class="reqn">f(l) \cdot f(u) \le 0</code>.
</p>
<p>If it is <em>known how</em> <code class="reqn">f</code> changes sign at the root
<code class="reqn">x_0</code>, that is, if the function is increasing or decreasing there,
<code>extendInt</code> can (and typically should) be specified as
<code>"upX"</code> (for &ldquo;upward crossing&rdquo;) or <code>"downX"</code>,
respectively.  Equivalently, define <code class="reqn">S := \pm 1</code>, to
require <code class="reqn">S = \mathrm{sign}(f(x_0 + \epsilon))</code> at the solution.  In that case, the search interval <code class="reqn">[l,u]</code>
possibly is extended to be such that <code class="reqn">S\cdot f(l)\le 0</code> and <code class="reqn">S \cdot f(u) \ge 0</code>.
</p>
<p><code>uniroot()</code> uses Fortran subroutine <code>zeroin</code> (from Netlib)
based on algorithms given in the reference below.  They assume a
continuous function (which then is known to have at least one root in
the interval).
</p>
<p>Convergence is declared either if <code>f(x) == 0</code> or the change in
<code>x</code> for one step of the algorithm is less than <code>tol</code> (plus an
allowance for representation error in <code>x</code>).
</p>
<p>If the algorithm does not converge in <code>maxiter</code> steps, a warning
is printed and the current approximation is returned.
</p>
<p><code>f</code> will be called as <code>f(<var>x</var>, ...)</code> for a numeric value
of <var>x</var>.
</p>
<p>The argument passed to <code>f</code> has special semantics and used to be
shared between calls.  The function should not copy it.
</p>


<h3>Value</h3>

<p>A list with at least five components: <code>root</code> and <code>f.root</code>
give the location of the root and the value of the function evaluated
at that point. <code>iter</code> and <code>estim.prec</code> give the number of
iterations used and an approximate estimated precision for
<code>root</code>.  (If the root occurs at one of the endpoints, the
estimated precision is <code>NA</code>.)
<code>init.it</code> contains the number of <em>init</em>ial <code>extendInt</code>
<em>it</em>erations if there were any and is <code>NA</code> otherwise.
In the case of such <code>extendInt</code> iterations, <code>iter</code> contains the
<code>sum</code> of these and the <code>zeroin</code> iterations.
</p>
<p>Further components may be added in the future.
</p>


<h3>Source</h3>

<p>Based on &lsquo;<span class="file">zeroin.c</span>&rsquo; in <a href="https://netlib.org/c/brent.shar">https://netlib.org/c/brent.shar</a>.
</p>


<h3>References</h3>

<p>Brent, R. (1973)
<em>Algorithms for Minimization without Derivatives.</em>
Englewood Cliffs, NJ: Prentice-Hall.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+polyroot">polyroot</a></code> for all complex roots of a polynomial;
<code><a href="#topic+optimize">optimize</a></code>, <code><a href="#topic+nlm">nlm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require(utils) # for str

## some platforms hit zero exactly on the first step:
## if so the estimated precision is 2/3.
f &lt;- function (x, a) x - a
str(xmin &lt;- uniroot(f, c(0, 1), tol = 0.0001, a = 1/3))

## handheld calculator example: fixed point of cos(.):
uniroot(function(x) cos(x) - x, lower = -pi, upper = pi, tol = 1e-9)$root

str(uniroot(function(x) x*(x^2-1) + .5, lower = -2, upper = 2,
            tol = 0.0001))
str(uniroot(function(x) x*(x^2-1) + .5, lower = -2, upper = 2,
            tol = 1e-10))

## Find the smallest value x for which exp(x) &gt; 0 (numerically):
r &lt;- uniroot(function(x) 1e80*exp(x) - 1e-300, c(-1000, 0), tol = 1e-15)
str(r, digits.d = 15) # around -745, depending on the platform.

exp(r$root)     # = 0, but not for r$root * 0.999...
minexp &lt;- r$root * (1 - 10*.Machine$double.eps)
exp(minexp)     # typically denormalized


##--- uniroot() with new interval extension + checking features: --------------

f1 &lt;- function(x) (121 - x^2)/(x^2+1)
f2 &lt;- function(x) exp(-x)*(x - 12)

try(uniroot(f1, c(0,10)))
try(uniroot(f2, c(0, 2)))
##--&gt; error: f() .. end points not of opposite sign

## where as  'extendInt="yes"'  simply first enlarges the search interval:
u1 &lt;- uniroot(f1, c(0,10),extendInt="yes", trace=1)
u2 &lt;- uniroot(f2, c(0,2), extendInt="yes", trace=2)
stopifnot(all.equal(u1$root, 11, tolerance = 1e-5),
          all.equal(u2$root, 12, tolerance = 6e-6))

## The *danger* of interval extension:
## No way to find a zero of a positive function, but
## numerically, f(-|M|) becomes zero :
u3 &lt;- uniroot(exp, c(0,2), extendInt="yes", trace=TRUE)

## Nonsense example (must give an error):
tools::assertCondition( uniroot(function(x) 1, 0:1, extendInt="yes"),
                       "error", verbose=TRUE)

## Convergence checking :
sinc &lt;- function(x) ifelse(x == 0, 1, sin(x)/x)
curve(sinc, -6,18); abline(h=0,v=0, lty=3, col=adjustcolor("gray", 0.8))

uniroot(sinc, c(0,5), extendInt="yes", maxiter=4) #-&gt; "just" a warning


## now with  check.conv=TRUE, must signal a convergence error :

uniroot(sinc, c(0,5), extendInt="yes", maxiter=4, check.conv=TRUE)


### Weibull cumulative hazard (example origin, Ravi Varadhan):
cumhaz &lt;- function(t, a, b) b * (t/b)^a
froot &lt;- function(x, u, a, b) cumhaz(x, a, b) - u

n &lt;- 1000
u &lt;- -log(runif(n))
a &lt;- 1/2
b &lt;- 1
## Find failure times
ru &lt;- sapply(u, function(x)
   uniroot(froot, u=x, a=a, b=b, interval= c(1.e-14, 1e04),
           extendInt="yes")$root)
ru2 &lt;- sapply(u, function(x)
   uniroot(froot, u=x, a=a, b=b, interval= c(0.01,  10),
           extendInt="yes")$root)
stopifnot(all.equal(ru, ru2, tolerance = 6e-6))

r1 &lt;- uniroot(froot, u= 0.99, a=a, b=b, interval= c(0.01, 10),
             extendInt="up")
stopifnot(all.equal(0.99, cumhaz(r1$root, a=a, b=b)))

## An error if 'extendInt' assumes "wrong zero-crossing direction":

uniroot(froot, u= 0.99, a=a, b=b, interval= c(0.1, 10), extendInt="down")

</code></pre>

<hr>
<h2 id='update'>Update and Re-fit a Model Call</h2><span id='topic+update'></span><span id='topic+update.default'></span><span id='topic+getCall'></span><span id='topic+getCall.default'></span>

<h3>Description</h3>

<p><code>update</code> will update and (by default) re-fit a model.  It does this
by extracting the call stored in the object, updating the call and (by
default) evaluating that call.  Sometimes it is useful to call
<code>update</code> with only one argument, for example if the data frame has
been corrected.
</p>
<p>&ldquo;Extracting the call&rdquo; in <code>update()</code> and similar functions
uses <code>getCall()</code> which itself is a (S3) generic function with a
default method that simply gets <code>x$call</code>.
</p>
<p>Because of this, <code>update()</code> will often work (via its default
method) on new model classes, either automatically, or by providing a
simple <code>getCall()</code> method for that class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update(object, ...)
## Default S3 method:
update(object, formula., ..., evaluate = TRUE)

getCall(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update_+3A_object">object</code>, <code id="update_+3A_x">x</code></td>
<td>
<p>An existing fit from a model function such as <code>lm</code>,
<code>glm</code> and many others.</p>
</td></tr>
<tr><td><code id="update_+3A_formula.">formula.</code></td>
<td>
<p>Changes to the formula &ndash; see <code>update.formula</code> for
details.</p>
</td></tr>
<tr><td><code id="update_+3A_...">...</code></td>
<td>
<p>Additional arguments to the call, or arguments with
changed values. Use <code>name = NULL</code> to remove the argument <code>name</code>.</p>
</td></tr>
<tr><td><code id="update_+3A_evaluate">evaluate</code></td>
<td>
<p>If true evaluate the new call else return the call.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>evaluate = TRUE</code> the fitted object, otherwise the updated call.
</p>


<h3>References</h3>

<p>Chambers, J. M. (1992)
<em>Linear models.</em>
Chapter 4 of <em>Statistical Models in S</em>
eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+update.formula">update.formula</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>oldcon &lt;- options(contrasts = c("contr.treatment", "contr.poly"))
## Annette Dobson (1990) "An Introduction to Generalized Linear Models".
## Page 9: Plant Weight Data.
ctl &lt;- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt &lt;- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group &lt;- gl(2, 10, 20, labels = c("Ctl", "Trt"))
weight &lt;- c(ctl, trt)
lm.D9 &lt;- lm(weight ~ group)
lm.D9
summary(lm.D90 &lt;- update(lm.D9, . ~ . - 1))
options(contrasts = c("contr.helmert", "contr.poly"))
update(lm.D9)
getCall(lm.D90)  # "through the origin"

options(oldcon)
</code></pre>

<hr>
<h2 id='update.formula'>Model Updating</h2><span id='topic+update.formula'></span>

<h3>Description</h3>

<p><code>update.formula</code> is used to update model formulae.
This typically involves adding or dropping terms,
but updates can be more general.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'formula'
update(old, new, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update.formula_+3A_old">old</code></td>
<td>
<p>a model formula to be updated.</p>
</td></tr>
<tr><td><code id="update.formula_+3A_new">new</code></td>
<td>
<p>a formula giving a template which specifies how to update.</p>
</td></tr>
<tr><td><code id="update.formula_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Either or both of <code>old</code> and <code>new</code> can be objects such as
length-one character vectors which can be coerced to a formula via
<code><a href="#topic+as.formula">as.formula</a></code>.
</p>
<p>The function works by first identifying the <em>left-hand side</em>
and <em>right-hand side</em> of the <code>old</code> formula.
It then examines the <code>new</code> formula and substitutes
the <em>lhs</em> of the <code>old</code> formula for any occurrence
of &lsquo;.&rsquo; on the left of <code>new</code>, and substitutes
the <em>rhs</em> of the <code>old</code> formula for any occurrence
of &lsquo;.&rsquo; on the right of <code>new</code>.  The result is then
simplified <em>via</em> <code><a href="#topic+terms.formula">terms.formula</a>(simplify = TRUE)</code>.
</p>


<h3>Value</h3>

<p>The updated formula is returned.  The environment of the result is
that of <code>old</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+terms">terms</a></code>, <code><a href="#topic+model.matrix">model.matrix</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>update(y ~ x,    ~ . + x2) #&gt; y ~ x + x2
update(y ~ x, log(.) ~ . ) #&gt; log(y) ~ x
update(. ~ u+v, res  ~ . ) #&gt; res ~ u + v
</code></pre>

<hr>
<h2 id='var.test'>F Test to Compare Two Variances</h2><span id='topic+var.test'></span><span id='topic+var.test.default'></span><span id='topic+var.test.formula'></span>

<h3>Description</h3>

<p>Performs an F test to compare the variances of two samples from normal
populations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var.test(x, ...)

## Default S3 method:
var.test(x, y, ratio = 1,
         alternative = c("two.sided", "less", "greater"),
         conf.level = 0.95, ...)

## S3 method for class 'formula'
var.test(formula, data, subset, na.action, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var.test_+3A_x">x</code>, <code id="var.test_+3A_y">y</code></td>
<td>
<p>numeric vectors of data values, or fitted linear model
objects (inheriting from class <code>"lm"</code>).</p>
</td></tr>
<tr><td><code id="var.test_+3A_ratio">ratio</code></td>
<td>
<p>the hypothesized ratio of the population variances of
<code>x</code> and <code>y</code>.</p>
</td></tr>
<tr><td><code id="var.test_+3A_alternative">alternative</code></td>
<td>
<p>a character string specifying the alternative
hypothesis, must be one of <code>"two.sided"</code> (default),
<code>"greater"</code> or <code>"less"</code>.  You can specify just the initial
letter.</p>
</td></tr>
<tr><td><code id="var.test_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level for the returned confidence
interval.</p>
</td></tr>
<tr><td><code id="var.test_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>lhs ~ rhs</code> where <code>lhs</code>
is a numeric variable giving the data values and <code>rhs</code> a factor
with two levels giving the corresponding groups.</p>
</td></tr>
<tr><td><code id="var.test_+3A_data">data</code></td>
<td>
<p>an optional matrix or data frame (or similar: see
<code><a href="#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="var.test_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="var.test_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.  Defaults to
<code>getOption("na.action")</code>.</p>
</td></tr>
<tr><td><code id="var.test_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The null hypothesis is that the ratio of the variances of the
populations from which <code>x</code> and <code>y</code> were drawn, or in the
data to which the linear models <code>x</code> and <code>y</code> were fitted, is
equal to <code>ratio</code>.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the F test statistic.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of the freedom of the F distribution of
the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>a confidence interval for the ratio of the population
variances.</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>the ratio of the sample variances of <code>x</code> and
<code>y</code>.</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>the ratio of population variances under the null.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative
hypothesis.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the character string
<code>"F test to compare two variances"</code>.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the names of the data.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+bartlett.test">bartlett.test</a></code> for testing homogeneity of variances in
more than two samples from normal distributions;
<code><a href="#topic+ansari.test">ansari.test</a></code> and <code><a href="#topic+mood.test">mood.test</a></code> for two rank
based (nonparametric) two-sample tests for difference in scale.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(50, mean = 0, sd = 2)
y &lt;- rnorm(30, mean = 1, sd = 1)
var.test(x, y)                  # Do x and y have the same variance?
var.test(lm(x ~ 1), lm(y ~ 1))  # The same.
</code></pre>

<hr>
<h2 id='varimax'>Rotation Methods for Factor Analysis</h2><span id='topic+promax'></span><span id='topic+varimax'></span>

<h3>Description</h3>

<p>These functions &lsquo;rotate&rsquo; loading matrices in factor analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>varimax(x, normalize = TRUE, eps = 1e-5)
promax(x, m = 4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="varimax_+3A_x">x</code></td>
<td>
<p>A loadings matrix, with <code class="reqn">p</code> rows and <code class="reqn">k &lt; p</code> columns</p>
</td></tr>
<tr><td><code id="varimax_+3A_m">m</code></td>
<td>
<p>The power used the target for <code>promax</code>.  Values of 2 to
4 are recommended.</p>
</td></tr>
<tr><td><code id="varimax_+3A_normalize">normalize</code></td>
<td>
<p>logical. Should Kaiser normalization be performed?
If so the rows of <code>x</code> are re-scaled to unit length before
rotation, and scaled back afterwards.</p>
</td></tr>
<tr><td><code id="varimax_+3A_eps">eps</code></td>
<td>
<p>The tolerance for stopping: the relative change in the sum
of singular values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These seek a &lsquo;rotation&rsquo; of the factors <code>x %*% T</code> that
aims to clarify the structure of the loadings matrix.  The matrix
<code>T</code> is a rotation (possibly with reflection) for <code>varimax</code>,
but a general linear transformation for <code>promax</code>, with the
variance of the factors being preserved.
</p>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>loadings</code></td>
<td>
<p>The &lsquo;rotated&rsquo; loadings matrix,
<code>x %*% rotmat</code>, of class <code>"loadings"</code>.</p>
</td></tr>
<tr><td><code>rotmat</code></td>
<td>
<p>The &lsquo;rotation&rsquo; matrix.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Hendrickson, A. E. and White, P. O. (1964).
Promax: a quick method for rotation to orthogonal oblique structure.
<em>British Journal of Statistical Psychology</em>, <b>17</b>, 65&ndash;70.
<a href="https://doi.org/10.1111/j.2044-8317.1964.tb00244.x">doi:10.1111/j.2044-8317.1964.tb00244.x</a>.
</p>
<p>Horst, P. (1965).
<em>Factor Analysis of Data Matrices</em>.
Holt, Rinehart and Winston.
Chapter 10.
</p>
<p>Kaiser, H. F. (1958).
The varimax criterion for analytic rotation in factor analysis.
<em>Psychometrika</em>, <b>23</b>, 187&ndash;200.
<a href="https://doi.org/10.1007/BF02289233">doi:10.1007/BF02289233</a>.
</p>
<p>Lawley, D. N. and Maxwell, A. E. (1971).
<em>Factor Analysis as a Statistical Method</em>, second edition.
Butterworths.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+factanal">factanal</a></code>, <code><a href="datasets.html#topic+Harman74.cor">Harman74.cor</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## varimax with normalize = TRUE is the default
fa &lt;- factanal( ~., 2, data = swiss)
varimax(loadings(fa), normalize = FALSE)
promax(loadings(fa))
</code></pre>

<hr>
<h2 id='vcov'>Calculate Variance-Covariance Matrix for a Fitted Model Object</h2><span id='topic+vcov'></span><span id='topic+vcov.aov'></span><span id='topic+vcov.lm'></span><span id='topic+vcov.glm'></span><span id='topic+vcov.summary.lm'></span><span id='topic+vcov.summary.glm'></span><span id='topic+vcov.lme'></span><span id='topic+vcov.gls'></span><span id='topic+.vcov.aliased'></span>

<h3>Description</h3>

<p>Returns the variance-covariance matrix of the main parameters of
a fitted model object.  The &ldquo;main&rdquo; parameters of model
correspond to those returned by <code><a href="#topic+coef">coef</a></code>, and typically do
not contain a nuisance scale parameter (<code><a href="#topic+sigma">sigma</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcov(object, ...)
## S3 method for class 'lm'
vcov(object, complete = TRUE, ...)
## and also for '[summary.]glm' and 'mlm'
## S3 method for class 'aov'
vcov(object, complete = FALSE, ...)

.vcov.aliased(aliased, vc, complete = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcov_+3A_object">object</code></td>
<td>

<p>a fitted model object, typically.  Sometimes also a
<code><a href="base.html#topic+summary">summary</a>()</code> object of such a fitted model.
</p>
</td></tr>
<tr><td><code id="vcov_+3A_complete">complete</code></td>
<td>
<p>for the <code>aov</code>, <code>lm</code>, <code>glm</code>, <code>mlm</code>, and where
applicable <code>summary.lm</code> etc methods: logical indicating if the
full variance-covariance matrix should be returned also in case of
an over-determined system where some coefficients are undefined and
<code><a href="#topic+coef">coef</a>(.)</code> contains <code>NA</code>s correspondingly.   When
<code>complete = TRUE</code>,  <code>vcov()</code> is compatible with
<code>coef()</code> also in this singular case.</p>
</td></tr>
<tr><td><code id="vcov_+3A_...">...</code></td>
<td>

<p>additional arguments for method functions.  For the
<code><a href="#topic+glm">glm</a></code> method this can be used to pass a
<code>dispersion</code> parameter.</p>
</td></tr>

<tr><td><code id="vcov_+3A_aliased">aliased</code></td>
<td>
<p>a <code><a href="base.html#topic+logical">logical</a></code> vector typically identical to
<code>is.na(coef(.))</code> indicating which coefficients are &lsquo;aliased&rsquo;.</p>
</td></tr>
<tr><td><code id="vcov_+3A_vc">vc</code></td>
<td>
<p>a variance-covariance matrix, typically &ldquo;incomplete&rdquo;,
i.e., with no rows and columns for aliased coefficients.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>vcov()</code> is a generic function and functions with names beginning
in <code>vcov.</code> will be methods for this function.
Classes with methods for this function include:
<code>lm</code>, <code>mlm</code>, <code>glm</code>, <code>nls</code>,
<code>summary.lm</code>, <code>summary.glm</code>,
<code>negbin</code>, <code>polr</code>, <code>rlm</code> (in package <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a>),
<code>multinom</code> (in package <a href="https://CRAN.R-project.org/package=nnet"><span class="pkg">nnet</span></a>)
<code>gls</code>, <code>lme</code> (in package <a href="https://CRAN.R-project.org/package=nlme"><span class="pkg">nlme</span></a>),
<code>coxph</code> and <code>survreg</code> (in package <a href="https://CRAN.R-project.org/package=survival"><span class="pkg">survival</span></a>).
</p>
<p>(<code>vcov()</code> methods for summary objects allow more
efficient and still encapsulated access when both
<code>summary(mod)</code> and <code>vcov(mod)</code> are needed.)
</p>
<p><code>.vcov.aliased()</code> is an auxiliary function useful for
<code>vcov</code> method implementations which have to deal with singular
model fits encoded via NA coefficients: It augments a vcov&ndash;matrix
<code>vc</code> by <code><a href="base.html#topic+NA">NA</a></code> rows and columns where needed, i.e., when
some entries of <code>aliased</code> are true and <code>vc</code> is of smaller dimension
than <code>length(aliased)</code>.
</p>


<h3>Value</h3>

<p>A matrix of the estimated covariances between the parameter estimates
in the linear or non-linear predictor of the model.  This should have
row and column names corresponding to the parameter names given by the
<code><a href="#topic+coef">coef</a></code> method.
</p>
<p>When some coefficients of the (linear) model are undetermined and
hence <code>NA</code> because of linearly dependent terms (or an
&ldquo;over specified&rdquo; model), also called
&ldquo;aliased&rdquo;, see <code><a href="#topic+alias">alias</a></code>, then since <span class="rlang"><b>R</b></span> version 3.5.0,
<code>vcov()</code> (iff <code>complete = TRUE</code>, i.e., by default for
<code>lm</code> etc, but not for <code>aov</code>) contains corresponding rows and
columns of <code>NA</code>s, wherever <code><a href="#topic+coef">coef</a>()</code> has always
contained such <code>NA</code>s.
</p>

<hr>
<h2 id='Weibull'>The Weibull Distribution</h2><span id='topic+Weibull'></span><span id='topic+dweibull'></span><span id='topic+pweibull'></span><span id='topic+qweibull'></span><span id='topic+rweibull'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the Weibull distribution with parameters <code>shape</code>
and <code>scale</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dweibull(x, shape, scale = 1, log = FALSE)
pweibull(q, shape, scale = 1, lower.tail = TRUE, log.p = FALSE)
qweibull(p, shape, scale = 1, lower.tail = TRUE, log.p = FALSE)
rweibull(n, shape, scale = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Weibull_+3A_x">x</code>, <code id="Weibull_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="Weibull_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="Weibull_+3A_n">n</code></td>
<td>
<p>number of observations. If <code>length(n) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="Weibull_+3A_shape">shape</code>, <code id="Weibull_+3A_scale">scale</code></td>
<td>
<p>shape and scale parameters, the latter defaulting to 1.</p>
</td></tr>
<tr><td><code id="Weibull_+3A_log">log</code>, <code id="Weibull_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="Weibull_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Weibull distribution with <code>shape</code> parameter <code class="reqn">a</code> and
<code>scale</code> parameter <code class="reqn">\sigma</code> has density given by
</p>
<p style="text-align: center;"><code class="reqn">f(x) = (a/\sigma) {(x/\sigma)}^{a-1} \exp (-{(x/\sigma)}^{a})</code>
</p>
<p> for <code class="reqn">x &gt; 0</code>.
The cumulative distribution function is
<code class="reqn">F(x) = 1 - \exp(-{(x/\sigma)}^a)</code>
on <code class="reqn">x &gt; 0</code>, the
mean is <code class="reqn">E(X) = \sigma \Gamma(1 + 1/a)</code>, and
the <code class="reqn">Var(X) = \sigma^2(\Gamma(1 + 2/a)-(\Gamma(1 + 1/a))^2)</code>.
</p>


<h3>Value</h3>

<p><code>dweibull</code> gives the density,
<code>pweibull</code> gives the distribution function,
<code>qweibull</code> gives the quantile function, and
<code>rweibull</code> generates random deviates.
</p>
<p>Invalid arguments will result in return value <code>NaN</code>, with a warning.
</p>
<p>The length of the result is determined by <code>n</code> for
<code>rweibull</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.  
</p>
<p>The numerical arguments other than <code>n</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.
</p>


<h3>Note</h3>

<p>The cumulative hazard <code class="reqn">H(t) = - \log(1 - F(t))</code>
is
</p>
<pre>-pweibull(t, a, b, lower = FALSE, log = TRUE)
</pre>
<p>which is just <code class="reqn">H(t) = {(t/b)}^a</code>.
</p>


<h3>Source</h3>

<p><code>[dpq]weibull</code> are calculated directly from the definitions.
<code>rweibull</code> uses inversion.
</p>


<h3>References</h3>

<p>Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
<em>Continuous Univariate Distributions</em>, volume 1, chapter 21.
Wiley, New York.
</p>


<h3>See Also</h3>

<p><a href="#topic+Distributions">Distributions</a> for other standard distributions, including
the <a href="#topic+Exponential">Exponential</a> which is a special case of the Weibull distribution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(0, rlnorm(50))
all.equal(dweibull(x, shape = 1), dexp(x))
all.equal(pweibull(x, shape = 1, scale = pi), pexp(x, rate = 1/pi))
## Cumulative hazard H():
all.equal(pweibull(x, 2.5, pi, lower.tail = FALSE, log.p = TRUE),
          -(x/pi)^2.5, tolerance = 1e-15)
all.equal(qweibull(x/11, shape = 1, scale = pi), qexp(x/11, rate = 1/pi))
</code></pre>

<hr>
<h2 id='weighted.mean'>Weighted Arithmetic Mean</h2><span id='topic+weighted.mean'></span><span id='topic+weighted.mean.default'></span>

<h3>Description</h3>

<p>Compute a weighted mean.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weighted.mean(x, w, ...)

## Default S3 method:
weighted.mean(x, w, ..., na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weighted.mean_+3A_x">x</code></td>
<td>
<p>an object containing the values whose weighted mean is to be
computed.</p>
</td></tr>
<tr><td><code id="weighted.mean_+3A_w">w</code></td>
<td>
<p>a numerical vector of weights the same length as <code>x</code> giving
the weights to use for elements of <code>x</code>.</p>
</td></tr>
<tr><td><code id="weighted.mean_+3A_...">...</code></td>
<td>
<p>arguments to be passed to or from methods.</p>
</td></tr>
<tr><td><code id="weighted.mean_+3A_na.rm">na.rm</code></td>
<td>
<p>a logical value indicating whether <code>NA</code>
values in <code>x</code> should be stripped before the computation proceeds.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function and methods can be defined for the first
argument <code>x</code>: apart from the default methods there are methods
for the date-time classes <code>"POSIXct"</code>, <code>"POSIXlt"</code>,
<code>"difftime"</code> and <code>"Date"</code>.  The default method will work for
any numeric-like object for which <code>[</code>, multiplication, division
and <code><a href="base.html#topic+sum">sum</a></code> have suitable methods, including complex vectors.
</p>
<p>If <code>w</code> is missing then all elements of <code>x</code> are given the
same weight, otherwise the weights 
are normalized to sum to one (if possible: if
their sum is zero or infinite the value is likely to be <code>NaN</code>).
</p>
<p>Missing values in <code>w</code> are not handled specially and so give a
missing value as the result.  However, zero weights <em>are</em> handled
specially and the corresponding <code>x</code> values are omitted from the
sum.
</p>


<h3>Value</h3>

<p>For the default method, a length-one numeric vector.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+mean">mean</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## GPA from Siegel 1994
wt &lt;- c(5,  5,  4,  1)/15
x &lt;- c(3.7,3.3,3.5,2.8)
xm &lt;- weighted.mean(x, wt)
</code></pre>

<hr>
<h2 id='weighted.residuals'>Compute Weighted Residuals</h2><span id='topic+weighted.residuals'></span>

<h3>Description</h3>

<p>Computed weighted residuals from a linear model fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weighted.residuals(obj, drop0 = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weighted.residuals_+3A_obj">obj</code></td>
<td>
<p><span class="rlang"><b>R</b></span> object, typically of class <code><a href="#topic+lm">lm</a></code> or
<code><a href="#topic+glm">glm</a></code>.</p>
</td></tr>
<tr><td><code id="weighted.residuals_+3A_drop0">drop0</code></td>
<td>
<p>logical.  If <code>TRUE</code>, drop all cases with
<code><a href="#topic+weights">weights</a> == 0</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Weighted residuals are based on the deviance residuals, which for
a <code><a href="#topic+lm">lm</a></code> fit are the raw residuals <code class="reqn">R_i</code>
multiplied by <code class="reqn">\sqrt{w_i}</code>, where <code class="reqn">w_i</code> are the
<code>weights</code> as specified in <code><a href="#topic+lm">lm</a></code>'s call.
</p>
<p>Dropping cases with weights zero is compatible with
<code><a href="#topic+influence">influence</a></code> and related functions.
</p>


<h3>Value</h3>

<p>Numeric vector of length <code class="reqn">n'</code>, where <code class="reqn">n'</code> is the number
of non-0 weights (<code>drop0 = TRUE</code>) or the number of
observations, otherwise.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+residuals">residuals</a></code>, <code><a href="#topic+lm.influence">lm.influence</a></code>, etc.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## following on from example(lm)

all.equal(weighted.residuals(lm.D9),
          residuals(lm.D9))
x &lt;- 1:10
w &lt;- 0:9
y &lt;- rnorm(x)
weighted.residuals(lmxy &lt;- lm(y ~ x, weights = w))
weighted.residuals(lmxy, drop0 = FALSE)
</code></pre>

<hr>
<h2 id='weights'>Extract Model Weights</h2><span id='topic+weights'></span><span id='topic+weights.default'></span>

<h3>Description</h3>

<p><code>weights</code> is a generic function which extracts fitting weights from
objects returned by modeling functions.
</p>
<p>Methods can make use of <code><a href="#topic+napredict">napredict</a></code> methods to compensate
for the omission of missing values.  The default methods does so.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weights(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weights_+3A_object">object</code></td>
<td>
<p>an object for which the extraction of model weights is
meaningful.</p>
</td></tr>
<tr><td><code id="weights_+3A_...">...</code></td>
<td>
<p>other arguments passed to methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Weights extracted from the object <code>object</code>: the default method
looks for component <code>"weights"</code> and if not <code>NULL</code> calls
<code><a href="#topic+napredict">napredict</a></code> on it.
</p>


<h3>References</h3>

<p>Chambers, J. M. and Hastie, T. J. (1992)
<em>Statistical Models in S</em>.
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weights.glm">weights.glm</a></code>
</p>

<hr>
<h2 id='wilcox.test'>Wilcoxon Rank Sum and Signed Rank Tests</h2><span id='topic+wilcox.test'></span><span id='topic+wilcox.test.default'></span><span id='topic+wilcox.test.formula'></span>

<h3>Description</h3>

<p>Performs one- and two-sample Wilcoxon tests on vectors of data; the
latter is also known as &lsquo;Mann-Whitney&rsquo; test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wilcox.test(x, ...)

## Default S3 method:
wilcox.test(x, y = NULL,
            alternative = c("two.sided", "less", "greater"),
            mu = 0, paired = FALSE, exact = NULL, correct = TRUE,
            conf.int = FALSE, conf.level = 0.95,
            tol.root = 1e-4, digits.rank = Inf, ...)

## S3 method for class 'formula'
wilcox.test(formula, data, subset, na.action = na.pass, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wilcox.test_+3A_x">x</code></td>
<td>
<p>numeric vector of data values.  Non-finite (e.g., infinite or
missing) values will be omitted.</p>
</td></tr>
<tr><td><code id="wilcox.test_+3A_y">y</code></td>
<td>
<p>an optional numeric vector of data values: as with <code>x</code>
non-finite values will be omitted.</p>
</td></tr>
<tr><td><code id="wilcox.test_+3A_alternative">alternative</code></td>
<td>
<p>a character string specifying the alternative
hypothesis, must be one of <code>"two.sided"</code> (default),
<code>"greater"</code> or <code>"less"</code>.  You can specify just the initial
letter.</p>
</td></tr>
<tr><td><code id="wilcox.test_+3A_mu">mu</code></td>
<td>
<p>a number specifying an optional parameter used to form the
null hypothesis.  See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="wilcox.test_+3A_paired">paired</code></td>
<td>
<p>a logical indicating whether you want a paired test.</p>
</td></tr>
<tr><td><code id="wilcox.test_+3A_exact">exact</code></td>
<td>
<p>a logical indicating whether an exact p-value
should be computed.</p>
</td></tr>
<tr><td><code id="wilcox.test_+3A_correct">correct</code></td>
<td>
<p>a logical indicating whether to apply continuity
correction in the normal approximation for the p-value.</p>
</td></tr>
<tr><td><code id="wilcox.test_+3A_conf.int">conf.int</code></td>
<td>
<p>a logical indicating whether a confidence interval
should be computed.</p>
</td></tr>
<tr><td><code id="wilcox.test_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level of the interval.</p>
</td></tr>
<tr><td><code id="wilcox.test_+3A_tol.root">tol.root</code></td>
<td>
<p>(when <code>conf.int</code> is true:) a positive numeric
tolerance, used in <code><a href="#topic+uniroot">uniroot</a>(*, tol=tol.root)</code> calls.</p>
</td></tr>
<tr><td><code id="wilcox.test_+3A_digits.rank">digits.rank</code></td>
<td>
<p>a number; if finite, <code><a href="base.html#topic+rank">rank</a>(<a href="base.html#topic+signif">signif</a>(r, digits.rank))</code>
will be used to compute ranks for the test statistic instead of (the
default) <code>rank(r)</code>.</p>
</td></tr>
<tr><td><code id="wilcox.test_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>lhs ~ rhs</code> where <code>lhs</code>
is a numeric variable giving the data values and <code>rhs</code> either
<code>1</code> for a one-sample or paired test or a factor
with two levels giving the corresponding groups. If <code>lhs</code> is of
class <code>"<a href="#topic+Pair">Pair</a>"</code> and <code>rhs</code> is <code>1</code>, a paired test
is done, see Examples.</p>
</td></tr>
<tr><td><code id="wilcox.test_+3A_data">data</code></td>
<td>
<p>an optional matrix or data frame (or similar: see
<code><a href="#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="wilcox.test_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="wilcox.test_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code><a href="base.html#topic+NA">NA</a></code>s.</p>
</td></tr>
<tr><td><code id="wilcox.test_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.
For the <code>formula</code> method, this includes arguments of the
default method, but not <code>paired</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The formula interface is only applicable for the 2-sample tests.
</p>
<p>If only <code>x</code> is given, or if both <code>x</code> and <code>y</code> are given
and <code>paired</code> is <code>TRUE</code>, a Wilcoxon signed rank test of the
null that the distribution of <code>x</code> (in the one sample case) or of
<code>x - y</code> (in the paired two sample case) is symmetric about
<code>mu</code> is performed.
</p>
<p>Otherwise, if both <code>x</code> and <code>y</code> are given and <code>paired</code>
is <code>FALSE</code>, a Wilcoxon rank sum test (equivalent to the
Mann-Whitney test: see the Note) is carried out.  In this case, the
null hypothesis is that the distributions of <code>x</code> and <code>y</code>
differ by a location shift of <code>mu</code> and the alternative is that
they differ by some other location shift (and the one-sided
alternative <code>"greater"</code> is that <code>x</code> is shifted to the right
of <code>y</code>).
</p>
<p>By default (if <code>exact</code> is not specified), an exact p-value
is computed if the samples contain less than 50 finite values and
there are no ties.  Otherwise, a normal approximation is used.
</p>
<p>For stability reasons, it may be advisable to use rounded data or to set
<code>digits.rank = 7</code>, say, such that determination of ties does not
depend on very small numeric differences (see the example).
</p>
<p>Optionally (if argument <code>conf.int</code> is true), a nonparametric
confidence interval and an estimator for the pseudomedian (one-sample
case) or for the difference of the location parameters <code>x-y</code> is
computed.  (The pseudomedian of a distribution <code class="reqn">F</code> is the median
of the distribution of <code class="reqn">(u+v)/2</code>, where <code class="reqn">u</code> and <code class="reqn">v</code> are
independent, each with distribution <code class="reqn">F</code>.  If <code class="reqn">F</code> is symmetric,
then the pseudomedian and median coincide.  See Hollander &amp; Wolfe
(1973), page 34.)  Note that in the two-sample case the estimator for
the difference in location parameters does <b>not</b> estimate the
difference in medians (a common misconception) but rather the median
of the difference between a sample from <code>x</code> and a sample from
<code>y</code>.
</p>
<p>If exact p-values are available, an exact confidence interval is
obtained by the algorithm described in Bauer (1972), and the
Hodges-Lehmann estimator is employed.  Otherwise, the returned
confidence interval and point estimate are based on normal
approximations.  These are continuity-corrected for the interval but
<em>not</em> the estimate (as the correction depends on the
<code>alternative</code>).
</p>
<p>With small samples it may not be possible to achieve very high
confidence interval coverages. If this happens a warning will be given
and an interval with lower coverage will be substituted.
</p>
<p>When <code>x</code> (and <code>y</code> if applicable) are valid, the function now
always returns, also in the <code>conf.int = TRUE</code> case when a
confidence interval cannot be computed, in which case the interval
boundaries and sometimes the <code>estimate</code> now contain
<code><a href="base.html#topic+NaN">NaN</a></code>.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the test statistic with a name
describing it.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the parameter(s) for the exact distribution of the
test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>the location parameter <code>mu</code>.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative
hypothesis.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the type of test applied.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the names of the data.</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>a confidence interval for the location parameter.
(Only present if argument <code>conf.int = TRUE</code>.)</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>an estimate of the location parameter.
(Only present if argument <code>conf.int = TRUE</code>.)</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>This function can use large amounts of memory and stack (and even
crash <span class="rlang"><b>R</b></span> if the stack limit is exceeded) if <code>exact = TRUE</code> and
one sample is large (several thousands or more).
</p>


<h3>Note</h3>

<p>The literature is not unanimous about the definitions of the Wilcoxon
rank sum and Mann-Whitney tests.  The two most common definitions
correspond to the sum of the ranks of the first sample with the
minimum value (<code class="reqn">m(m+1)/2</code> for a first sample of size <code class="reqn">m</code>)
subtracted or not: <span class="rlang"><b>R</b></span> subtracts.  It seems Wilcoxon's original paper
used the unadjusted sum of the ranks but subsequent tables subtracted
the minimum.
</p>
<p><span class="rlang"><b>R</b></span>'s value can also be computed as the number of all pairs
<code>(x[i], y[j])</code> for which <code>y[j]</code> is not greater than
<code>x[i]</code>, the most common definition of the Mann-Whitney test.
</p>


<h3>References</h3>

<p>David F. Bauer (1972).
Constructing confidence sets using rank statistics.
<em>Journal of the American Statistical Association</em>
<b>67</b>, 687&ndash;690.
<a href="https://doi.org/10.1080/01621459.1972.10481279">doi:10.1080/01621459.1972.10481279</a>.
</p>
<p>Myles Hollander and Douglas A. Wolfe (1973).
<em>Nonparametric Statistical Methods</em>.
New York: John Wiley &amp; Sons.
Pages 27&ndash;33 (one-sample), 68&ndash;75 (two-sample).<br />
Or second edition (1999).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+psignrank">psignrank</a></code>, <code><a href="#topic+pwilcox">pwilcox</a></code>.
</p>
<p><code><a href="coin.html#topic+LocationTests">wilcox_test</a></code> in package
<a href="https://CRAN.R-project.org/package=coin"><span class="pkg">coin</span></a> for exact, asymptotic and Monte Carlo
<em>conditional</em> p-values, including in the presence of ties.
</p>
<p><code><a href="#topic+kruskal.test">kruskal.test</a></code> for testing homogeneity in location
parameters in the case of two or more samples;
<code><a href="#topic+t.test">t.test</a></code> for an alternative under normality
assumptions [or large samples]
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)
## One-sample test.
## Hollander &amp; Wolfe (1973), 29f.
## Hamilton depression scale factor measurements in 9 patients with
##  mixed anxiety and depression, taken at the first (x) and second
##  (y) visit after initiation of a therapy (administration of a
##  tranquilizer).
x &lt;- c(1.83,  0.50,  1.62,  2.48, 1.68, 1.88, 1.55, 3.06, 1.30)
y &lt;- c(0.878, 0.647, 0.598, 2.05, 1.06, 1.29, 1.06, 3.14, 1.29)
wilcox.test(x, y, paired = TRUE, alternative = "greater")
wilcox.test(y - x, alternative = "less")    # The same.
wilcox.test(y - x, alternative = "less",
            exact = FALSE, correct = FALSE) # H&amp;W large sample
                                            # approximation

## Formula interface to one-sample and paired tests

depression &lt;- data.frame(first = x, second = y, change = y - x)
wilcox.test(change ~ 1, data = depression)
wilcox.test(Pair(first, second) ~ 1, data = depression)

## Two-sample test.
## Hollander &amp; Wolfe (1973), 69f.
## Permeability constants of the human chorioamnion (a placental
##  membrane) at term (x) and between 12 to 26 weeks gestational
##  age (y).  The alternative of interest is greater permeability
##  of the human chorioamnion for the term pregnancy.
x &lt;- c(0.80, 0.83, 1.89, 1.04, 1.45, 1.38, 1.91, 1.64, 0.73, 1.46)
y &lt;- c(1.15, 0.88, 0.90, 0.74, 1.21)
wilcox.test(x, y, alternative = "g")        # greater
wilcox.test(x, y, alternative = "greater",
            exact = FALSE, correct = FALSE) # H&amp;W large sample
                                            # approximation

wilcox.test(rnorm(10), rnorm(10, 2), conf.int = TRUE)

## Formula interface.
boxplot(Ozone ~ Month, data = airquality)
wilcox.test(Ozone ~ Month, data = airquality,
            subset = Month %in% c(5, 8))

## accuracy in ties determination via 'digits.rank':
wilcox.test( 4:2,      3:1,     paired=TRUE) # Warning:  cannot compute exact p-value with ties
wilcox.test((4:2)/10, (3:1)/10, paired=TRUE) # no ties =&gt; *no* warning
wilcox.test((4:2)/10, (3:1)/10, paired=TRUE, digits.rank = 9) # same ties as (4:2, 3:1)
</code></pre>

<hr>
<h2 id='Wilcoxon'>Distribution of the Wilcoxon Rank Sum Statistic</h2><span id='topic+Wilcoxon'></span><span id='topic+dwilcox'></span><span id='topic+pwilcox'></span><span id='topic+qwilcox'></span><span id='topic+rwilcox'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the distribution of the Wilcoxon rank sum statistic
obtained from samples with size <code>m</code> and <code>n</code>, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dwilcox(x, m, n, log = FALSE)
pwilcox(q, m, n, lower.tail = TRUE, log.p = FALSE)
qwilcox(p, m, n, lower.tail = TRUE, log.p = FALSE)
rwilcox(nn, m, n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Wilcoxon_+3A_x">x</code>, <code id="Wilcoxon_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="Wilcoxon_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="Wilcoxon_+3A_nn">nn</code></td>
<td>
<p>number of observations. If <code>length(nn) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="Wilcoxon_+3A_m">m</code>, <code id="Wilcoxon_+3A_n">n</code></td>
<td>
<p>numbers of observations in the first and second sample,
respectively.  Can be vectors of positive integers.</p>
</td></tr>
<tr><td><code id="Wilcoxon_+3A_log">log</code>, <code id="Wilcoxon_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="Wilcoxon_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This distribution is obtained as follows.  Let <code>x</code> and <code>y</code>
be two random, independent samples of size <code>m</code> and <code>n</code>.
Then the Wilcoxon rank sum statistic is the number of all pairs
<code>(x[i], y[j])</code> for which <code>y[j]</code> is not greater than
<code>x[i]</code>.  This statistic takes values between <code>0</code> and
<code>m * n</code>, and its mean and variance are <code>m * n / 2</code> and
<code>m * n * (m + n + 1) / 12</code>, respectively.
</p>
<p>If any of the first three arguments are vectors, the recycling rule is
used to do the calculations for all combinations of the three up to
the length of the longest vector.
</p>


<h3>Value</h3>

<p><code>dwilcox</code> gives the density,
<code>pwilcox</code> gives the distribution function,
<code>qwilcox</code> gives the quantile function, and
<code>rwilcox</code> generates random deviates.
</p>
<p>The length of the result is determined by <code>nn</code> for
<code>rwilcox</code>, and is the maximum of the lengths of the
numerical arguments for the other functions.
</p>
<p>The numerical arguments other than <code>nn</code> are recycled to the
length of the result.  Only the first elements of the logical
arguments are used.
</p>


<h3>Warning</h3>

<p>These functions can use large amounts of memory and stack (and even crash
<span class="rlang"><b>R</b></span> if the stack limit is exceeded and stack-checking is not in place)
if one sample is large (several thousands or more).
</p>


<h3>Note</h3>

<p>S-PLUS used a different (but equivalent) definition of the Wilcoxon
statistic: see <code><a href="#topic+wilcox.test">wilcox.test</a></code> for details.
</p>


<h3>Author(s)</h3>

<p>Kurt Hornik</p>


<h3>Source</h3>

<p>These (&quot;d&quot;,&quot;p&quot;,&quot;q&quot;) are calculated via recursion, based on <code>cwilcox(k, m, n)</code>,
the number of choices with statistic <code>k</code> from samples of size
<code>m</code> and <code>n</code>, which is itself calculated recursively and the
results cached.  Then <code>dwilcox</code> and <code>pwilcox</code> sum
appropriate values of <code>cwilcox</code>, and <code>qwilcox</code> is based on
inversion.
</p>
<p><code>rwilcox</code> generates a random permutation of ranks and evaluates
the statistic.  Note that it is based on the same C code as <code><a href="base.html#topic+sample">sample</a>()</code>,
and hence is determined by <code><a href="base.html#topic+.Random.seed">.Random.seed</a></code>, notably from
<code><a href="base.html#topic+RNGkind">RNGkind</a>(sample.kind = ..)</code> which changed with <span class="rlang"><b>R</b></span> version 3.6.0.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+wilcox.test">wilcox.test</a></code> to calculate the statistic from data, find p
values and so on.
</p>
<p><a href="#topic+Distributions">Distributions</a> for standard distributions, including
<code><a href="#topic+dsignrank">dsignrank</a></code> for the distribution of the
<em>one-sample</em> Wilcoxon signed rank statistic.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

x &lt;- -1:(4*6 + 1)
fx &lt;- dwilcox(x, 4, 6)
Fx &lt;- pwilcox(x, 4, 6)

layout(rbind(1,2), widths = 1, heights = c(3,2))
plot(x, fx, type = "h", col = "violet",
     main =  "Probabilities (density) of Wilcoxon-Statist.(n=6, m=4)")
plot(x, Fx, type = "s", col = "blue",
     main =  "Distribution of Wilcoxon-Statist.(n=6, m=4)")
abline(h = 0:1, col = "gray20", lty = 2)
layout(1) # set back

N &lt;- 200
hist(U &lt;- rwilcox(N, m = 4,n = 6), breaks = 0:25 - 1/2,
     border = "red", col = "pink", sub = paste("N =",N))
mtext("N * f(x),  f() = true \"density\"", side = 3, col = "blue")
 lines(x, N*fx, type = "h", col = "blue", lwd = 2)
points(x, N*fx, cex = 2)

## Better is a Quantile-Quantile Plot
qqplot(U, qw &lt;- qwilcox((1:N - 1/2)/N, m = 4, n = 6),
       main = paste("Q-Q-Plot of empirical and theoretical quantiles",
                     "Wilcoxon Statistic,  (m=4, n=6)", sep = "\n"))
n &lt;- as.numeric(names(print(tU &lt;- table(U))))
text(n+.2, n+.5, labels = tU, col = "red")
</code></pre>

<hr>
<h2 id='window'>Time (Series) Windows</h2><span id='topic+window'></span><span id='topic+window.default'></span><span id='topic+window.ts'></span><span id='topic+window+3C-'></span><span id='topic+window+3C-.ts'></span>

<h3>Description</h3>

<p><code>window</code> is a generic function which
extracts the subset of the object <code>x</code>
observed between the times <code>start</code> and <code>end</code>. If a
frequency is specified, the series is then re-sampled at the new
frequency.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>window(x, ...)
## S3 method for class 'ts'
window(x, ...)
## Default S3 method:
window(x, start = NULL, end = NULL,
      frequency = NULL, deltat = NULL, extend = FALSE, ts.eps = getOption("ts.eps"), ...)

window(x, ...) &lt;- value
## S3 replacement method for class 'ts'
window(x, start, end, frequency, deltat, ...) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="window_+3A_x">x</code></td>
<td>
<p>a time-series (or other object if not replacing values).</p>
</td></tr>
<tr><td><code id="window_+3A_start">start</code></td>
<td>
<p>the start time of the period of interest.</p>
</td></tr>
<tr><td><code id="window_+3A_end">end</code></td>
<td>
<p>the end time of the period of interest.</p>
</td></tr>
<tr><td><code id="window_+3A_frequency">frequency</code>, <code id="window_+3A_deltat">deltat</code></td>
<td>
<p>the new frequency can be specified by either
(or both if they are consistent).</p>
</td></tr>
<tr><td><code id="window_+3A_extend">extend</code></td>
<td>
<p>logical.  If true, the <code>start</code> and <code>end</code> values
are allowed to extend the series.  If false, attempts to extend the
series give a warning and are ignored.</p>
</td></tr>
<tr><td><code id="window_+3A_ts.eps">ts.eps</code></td>
<td>
<p>time series comparison tolerance.  Frequencies are
considered equal if their absolute difference is less than
<code>ts.eps</code> and boundaries (length-1 versions of <code>start</code> and
<code>end</code>) are checked with fuzz <code>ts.eps/frequency(x)</code>.</p>
</td></tr>
<tr><td><code id="window_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="window_+3A_value">value</code></td>
<td>
<p>replacement values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The start and end times can be specified as for <code><a href="#topic+ts">ts</a></code>. If
there is no observation at the new <code>start</code> or <code>end</code>,
the immediately following (<code>start</code>) or preceding (<code>end</code>)
observation time is used.
</p>
<p>The replacement function has a method for <code>ts</code> objects, and
is allowed to extend the series (with a warning).  There is no default
method.
</p>


<h3>Value</h3>

<p>The value depends on the method. <code>window.default</code> will return a
vector or matrix with an appropriate <code><a href="#topic+tsp">tsp</a></code> attribute.
</p>
<p><code>window.ts</code> differs from <code>window.default</code> only in
ensuring the result is a <code>ts</code> object.
</p>
<p>If <code>extend = TRUE</code> the series will be padded with <code>NA</code>s if
needed.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+time">time</a></code>, <code><a href="#topic+ts">ts</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>window(presidents, 1960, c(1969,4)) # values in the 1960's
window(presidents, deltat = 1)  # All Qtr1s
window(presidents, start = c(1945,3), deltat = 1)  # All Qtr3s
window(presidents, 1944, c(1979,2), extend = TRUE)

pres &lt;- window(presidents, 1945, c(1949,4)) # values in the 1940's
window(pres, 1945.25, 1945.50) &lt;- c(60, 70)
window(pres, 1944, 1944.75) &lt;- 0 # will generate a warning
window(pres, c(1945,4), c(1949,4), frequency = 1) &lt;- 85:89
pres
</code></pre>

<hr>
<h2 id='xtabs'>Cross Tabulation</h2><span id='topic+xtabs'></span><span id='topic+print.xtabs'></span>

<h3>Description</h3>

<p>Create a contingency table (optionally a sparse matrix) from
cross-classifying factors, usually contained in a data frame,
using a formula interface.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xtabs(formula = ~., data = parent.frame(), subset, sparse = FALSE,
      na.action, addNA = FALSE, exclude = if(!addNA) c(NA, NaN),
      drop.unused.levels = FALSE)

## S3 method for class 'xtabs'
print(x, na.print = "", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xtabs_+3A_formula">formula</code></td>
<td>
<p>a <a href="#topic+formula">formula</a> object with the cross-classifying variables
(separated by <code>+</code>) on the right hand side (or an object which
can be coerced to a formula).  Interactions are not allowed.  On the
left hand side, one may optionally give a vector or a matrix of
counts; in the latter case, the columns are interpreted as
corresponding to the levels of a variable.  This is useful if the
data have already been tabulated, see the examples below.</p>
</td></tr>
<tr><td><code id="xtabs_+3A_data">data</code></td>
<td>
<p>an optional matrix or data frame (or similar: see
<code><a href="#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="xtabs_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="xtabs_+3A_sparse">sparse</code></td>
<td>
<p>logical specifying if the result should be a
<em>sparse</em> matrix, i.e., inheriting from
<code><a href="Matrix.html#topic+sparseMatrix-class">sparseMatrix</a></code>
Only works for two factors (since there
are no higher-order sparse array classes yet).
</p>
</td></tr>
<tr><td><code id="xtabs_+3A_na.action">na.action</code></td>
<td>
<p>a <code><a href="base.html#topic+function">function</a></code> which indicates what should happen when
the data contain <code><a href="base.html#topic+NA">NA</a></code>s.  If unspecified, and
<code>addNA</code> is true, this is set to <code><a href="#topic+na.pass">na.pass</a></code>.  When it
is <code><a href="#topic+na.omit">na.omit</a></code> and <code>formula</code> has a left hand side (with
counts), <code><a href="base.html#topic+sum">sum</a>(*, na.rm = TRUE)</code> is used instead of
<code>sum(*)</code> for the counts.</p>
</td></tr>
<tr><td><code id="xtabs_+3A_addna">addNA</code></td>
<td>
<p>logical indicating if <code>NA</code>s should get a separate
level and be counted, using <code><a href="base.html#topic+addNA">addNA</a>(*, ifany=TRUE)</code> and
setting the default for <code>na.action</code> to <code>na.pass</code>.</p>
</td></tr>
<tr><td><code id="xtabs_+3A_exclude">exclude</code></td>
<td>
<p>a vector of values to be excluded when forming the
set of levels of the classifying factors.</p>
</td></tr>
<tr><td><code id="xtabs_+3A_drop.unused.levels">drop.unused.levels</code></td>
<td>
<p>a logical indicating whether to drop unused
levels in the classifying factors.  If this is <code>FALSE</code> and
there are unused levels, the table will contain zero marginals, and
a subsequent chi-squared test for independence of the factors will
not work.</p>
</td></tr>
<tr><td><code id="xtabs_+3A_x">x</code></td>
<td>
<p>an object of class <code>"xtabs"</code>.</p>
</td></tr>
<tr><td><code id="xtabs_+3A_na.print">na.print</code></td>
<td>
<p>character string (or <code>NULL</code>) indicating how
<code><a href="base.html#topic+NA">NA</a></code> are printed.  The default (<code>""</code>) does not show
<code>NA</code>s clearly, and <code>na.print = "NA"</code> maybe advisable
instead.</p>
</td></tr>
<tr><td><code id="xtabs_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There is a <code>summary</code> method for contingency table objects created
by <code>table</code> or <code>xtabs(*, sparse = FALSE)</code>, which gives basic
information and performs a chi-squared test for independence of
factors (note that the function <code><a href="#topic+chisq.test">chisq.test</a></code> currently
only handles 2-d tables).
</p>
<p>If a left hand side is given in <code>formula</code>, its entries are simply
summed over the cells corresponding to the right hand side; this also
works if the lhs does not give counts.
</p>
<p>For variables in <code>formula</code> which are factors, <code>exclude</code>
must be specified explicitly; the default exclusions will not be used.
</p>
<p>In <span class="rlang"><b>R</b></span> versions before 3.4.0, e.g., when <code>na.action = na.pass</code>,
sometimes zeroes (<code>0</code>) were returned instead of <code>NA</code>s.
</p>
<p>Note that when <code>addNA</code> is false as by default, and <code>na.action</code>
is not specified (or set to <code>NULL</code>), in effect <code>na.action =
    getOption("na.action", default=na.omit)</code> is used; see also the examples.
</p>


<h3>Value</h3>

<p>By default, when <code>sparse = FALSE</code>,
a contingency table in array representation of S3 class <code>c("xtabs",
    "table")</code>, with a <code>"call"</code> attribute storing the matched call.
</p>
<p>When <code>sparse = TRUE</code>, a sparse numeric matrix, specifically an
object of S4 class 
<code><a href="Matrix.html#topic+dgTMatrix-class">dgTMatrix</a></code> from package
<a href="https://CRAN.R-project.org/package=Matrix"><span class="pkg">Matrix</span></a>.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+table">table</a></code> for traditional cross-tabulation, and
<code><a href="base.html#topic+as.data.frame.table">as.data.frame.table</a></code> which is the inverse operation of
<code>xtabs</code> (see the <code>DF</code> example below).
</p>
<p><code><a href="Matrix.html#topic+sparseMatrix-class">sparseMatrix</a></code> on sparse
matrices in package <a href="https://CRAN.R-project.org/package=Matrix"><span class="pkg">Matrix</span></a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## 'esoph' has the frequencies of cases and controls for all levels of
## the variables 'agegp', 'alcgp', and 'tobgp'.
xtabs(cbind(ncases, ncontrols) ~ ., data = esoph)
## Output is not really helpful ... flat tables are better:
ftable(xtabs(cbind(ncases, ncontrols) ~ ., data = esoph))
## In particular if we have fewer factors ...
ftable(xtabs(cbind(ncases, ncontrols) ~ agegp, data = esoph))

## This is already a contingency table in array form.
DF &lt;- as.data.frame(UCBAdmissions)
## Now 'DF' is a data frame with a grid of the factors and the counts
## in variable 'Freq'.
DF
## Nice for taking margins ...
xtabs(Freq ~ Gender + Admit, DF)
## And for testing independence ...
summary(xtabs(Freq ~ ., DF))

## with NA's
DN &lt;- DF; DN[cbind(6:9, c(1:2,4,1))] &lt;- NA
DN # 'Freq' is missing only for (Rejected, Female, B)
tools::assertError(# 'na.fail' should fail :
     xtabs(Freq ~ Gender + Admit, DN, na.action=na.fail), verbose=TRUE)
op &lt;- options(na.action = "na.omit") # the "factory" default
(xtabs(Freq ~ Gender + Admit, DN) -&gt; xtD)
noC &lt;- function(O) `attr&lt;-`(O, "call", NULL)
ident_noC &lt;- function(x,y) identical(noC(x), noC(y))
stopifnot(exprs = {
  ident_noC(xtD, xtabs(Freq ~ Gender + Admit, DN, na.action = na.omit))
  ident_noC(xtD, xtabs(Freq ~ Gender + Admit, DN, na.action = NULL))
})

xtabs(Freq ~ Gender + Admit, DN, na.action = na.pass)
## The Female:Rejected combination has NA 'Freq' (and NA prints 'invisibly' as "")
(xtNA &lt;- xtabs(Freq ~ Gender + Admit, DN, addNA = TRUE)) # ==&gt; count NAs
## show NA's better via  na.print = ".." :
print(xtNA, na.print= "NA")


## Create a nice display for the warp break data.
warpbreaks$replicate &lt;- rep_len(1:9, 54)
ftable(xtabs(breaks ~ wool + tension + replicate, data = warpbreaks))

### ---- Sparse Examples ----

if(require("Matrix")) withAutoprint({
 ## similar to "nlme"s  'ergoStool' :
 d.ergo &lt;- data.frame(Type = paste0("T", rep(1:4, 9*4)),
                      Subj = gl(9, 4, 36*4))
 xtabs(~ Type + Subj, data = d.ergo) # 4 replicates each
 set.seed(15) # a subset of cases:
 xtabs(~ Type + Subj, data = d.ergo[sample(36, 10), ], sparse = TRUE)

 ## Hypothetical two-level setup:
 inner &lt;- factor(sample(letters[1:25], 100, replace = TRUE))
 inout &lt;- factor(sample(LETTERS[1:5], 25, replace = TRUE))
 fr &lt;- data.frame(inner = inner, outer = inout[as.integer(inner)])
 xtabs(~ inner + outer, fr, sparse = TRUE)
})
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
