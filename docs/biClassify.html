<!DOCTYPE html><html><head><title>Help for package biClassify</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {biClassify}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#KOS'><p>Function which generates feature weights, discriminant vector, and class predictions.</p></a></li>
<li><a href='#KOS_Data'><p>A list consisting of Training and Test data along with corresponding class labels.</p></a></li>
<li><a href='#LDA'><p>Linear Discriminant Analysis (LDA)</p></a></li>
<li><a href='#LDA_Data'><p>A list consisting of Training and Test data along with corresponding class labels.</p></a></li>
<li><a href='#QDA'><p>Quadratic Discriminant Analysis (QDA)</p></a></li>
<li><a href='#QDA_Data'><p>A list consisting of Training and Test data along with corresponding class labels.</p></a></li>
<li><a href='#SelectParams'><p>Generates parameters.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Binary Classification Using Extensions of Discriminant Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-12-6</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Alexander F. Lapanowski &lt;aflapan@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements methods for sample size 
  reduction within Linear and Quadratic Discriminant Analysis in Lapanowski and Gaynanova (2020) &lt;<a href="https://arxiv.org/abs/2005.03858">arXiv:2005.03858</a>&gt;.
  Also includes methods for non-linear discriminant analysis with simultaneous sparse feature selection in Lapanowski and Gaynanova (2019) PMLR 89:1704-1713. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.1), Matrix, stats, fields, MASS, datasets,
mvtnorm, expm, DAAG</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-12-06 16:44:12 UTC; aflap</td>
</tr>
<tr>
<td>Author:</td>
<td>Alexander F. Lapanowski [aut, cre],
  Irina Gaynanova [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-12-06 17:10:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='KOS'>Function which generates feature weights, discriminant vector, and class predictions.</h2><span id='topic+KOS'></span>

<h3>Description</h3>

<p>Returns a (m x 1) vector of predicted group membership (either 1 or 2) for each data point in X. Uses Data and Cat to train the classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KOS(TestData = NULL, TrainData, TrainCat, Method = "Full",
  Mode = "Automatic", m1 = NULL, m2 = NULL, Sigma = NULL,
  Gamma = NULL, Lambda = NULL, Epsilon = 1e-05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KOS_+3A_testdata">TestData</code></td>
<td>
<p>(m x p) Matrix of unlabelled data with numeric features to be classified. Cannot have missing values.</p>
</td></tr>
<tr><td><code id="KOS_+3A_traindata">TrainData</code></td>
<td>
<p>(n x p) Matrix of training data with numeric features. Cannot have missing values.</p>
</td></tr>
<tr><td><code id="KOS_+3A_traincat">TrainCat</code></td>
<td>
<p>(n x 1) Vector of class membership corresponding to Data. Values must be either 1 or 2.</p>
</td></tr>
<tr><td><code id="KOS_+3A_method">Method</code></td>
<td>
<p>A string of characters which determines which version of KOS to use. Must be either &quot;Full&quot; or &quot;Subsampled&quot;. Default is &quot;Full&quot;.</p>
</td></tr>
<tr><td><code id="KOS_+3A_mode">Mode</code></td>
<td>
<p>A string of characters which determines how the reduced sample paramters will be inputted for each method. Must be either &quot;Research&quot;, &quot;Interactive&quot;, or &quot;Automatic&quot;. Default is &quot;Automatic&quot;.</p>
</td></tr>
<tr><td><code id="KOS_+3A_m1">m1</code></td>
<td>
<p>The number of class 1 compressed samples to be generated. Must be a positive integer.</p>
</td></tr>
<tr><td><code id="KOS_+3A_m2">m2</code></td>
<td>
<p>The number of class 2 compressed samples to be generated. Must be a positive integer.</p>
</td></tr>
<tr><td><code id="KOS_+3A_sigma">Sigma</code></td>
<td>
<p>Scalar Gaussian kernel parameter. Default set to NULL and is automatically generated if user-specified value not provided. Must be &gt; 0. User-specified parameters must satisfy hierarchical ordering.</p>
</td></tr>
<tr><td><code id="KOS_+3A_gamma">Gamma</code></td>
<td>
<p>Scalar ridge parameter used in kernel optimal scoring. Default set to NULL and is automatically generated if user-specified value not provided. Must be &gt; 0. User-specified parameters must satisfy hierarchical ordering.</p>
</td></tr>
<tr><td><code id="KOS_+3A_lambda">Lambda</code></td>
<td>
<p>Scalar sparsity parameter on weight vector. Default set to NULL and is automatically generated by the function if user-specified value not provided. Must be &gt;= 0. When Lambda = 0, SparseKOS defaults to kernel optimal scoring of [Lapanowski and Gaynanova, preprint] without sparse feature selection. User-specified parameters must satisfy hierarchical ordering.</p>
</td></tr>
<tr><td><code id="KOS_+3A_epsilon">Epsilon</code></td>
<td>
<p>Numerical stability constant with default value 1e-05. Must be &gt; 0 and is typically chosen to be small.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function which handles classification. Generates feature weight vector and discriminant coefficients vector in sparse kernel optimal scoring. If a matrix X is provided, the function classifies each data point using the generated feature weight vector and discriminant vector. Will use user-supplied parameters Sigma, Gamma, and Lambda if any are given. If any are missing, the function will run SelectParams to generate the other parameters. User-specified values must satisfy hierarchical ordering.
</p>


<h3>Value</h3>

<p>A list of
</p>
<table>
<tr><td><code>Predictions</code></td>
<td>
<p> (m x 1) Vector of predicted class labels for the data points in TestData. Only included in non-null value of X is provided.</p>
</td></tr> 
<tr><td><code>Weights</code></td>
<td>
<p> (p x 1) Vector of feature weights.</p>
</td></tr> 
<tr><td><code>Dvec</code></td>
<td>
<p>(n x 1) Discrimiant coefficients vector.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Lapanowski, Alexander F., and Gaynanova, Irina. &ldquo;Sparse feature selection in kernel discriminant analysis via optimal scoring&rdquo;, Artificial Intelligence and Statistics, 2019.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
Sigma &lt;- 1.325386  #Set parameter values equal to result of SelectParam.
Gamma &lt;- 0.07531579 #Speeds up example.
Lambda &lt;- 0.002855275

TrainData &lt;- KOS_Data$TrainData
TrainCat &lt;- KOS_Data$TrainCat
TestData &lt;- KOS_Data$TestData
TestCat &lt;- KOS_Data$TestCat

KOS(TestData = TestData, 
    TrainData = TrainData, 
    TrainCat = TrainCat , 
    Sigma = Sigma , 
    Gamma = Gamma , 
    Lambda = Lambda)

</code></pre>

<hr>
<h2 id='KOS_Data'>A list consisting of Training and Test data along with corresponding class labels.</h2><span id='topic+KOS_Data'></span>

<h3>Description</h3>

<p>A list consisting of Training and Test data along with corresponding class labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KOS_Data
</code></pre>


<h3>Format</h3>

<p>A list consisting of:
</p>

<dl>
<dt>TrainData</dt><dd><p> (179 x 4) Matrix of training data features. the first two features satisfy sqrt(x_i1^2 + x_i2^2) &gt; 2/3 if the ith sample is in class 1. 
Otherwise, they satisfy sqrt(x_i1^2 + x_i2^2) &lt; 2/3 - 1/10 if the ith sample is in class 2. 
The third and fourth features are generated as independent N(0, 1/2) noise.</p>
</dd>
<dt>TestData</dt><dd><p> (94 x 4) Matrix of test data features. the first two features satisfy sqrt(x_i1^2 + x_i2^2) &gt; 2/3 if the ith sample is in class 1. 
Otherwise, they satisfy sqrt(x_i1^2 + x_i2^2) &lt; 2/3 - 1/10 if the ith sample is in class 2. 
The third and fourth features are generated as independent N(0, 1/2) noise.</p>
</dd>
<dt>CatTrain</dt><dd><p> (179 x 1) Vector of class labels for the training data.</p>
</dd>
<dt>CatTest</dt><dd><p> (94 x 1) Vector of class labels for the test data.</p>
</dd>
</dl>
<p>...
</p>


<h3>Source</h3>

<p>Simulation model 1 from [Lapanowski and Gaynanova, preprint].
</p>


<h3>References</h3>

<p>Lapanowski, Alexander F., and Gaynanova, Irina. &ldquo;Sparse Feature Selection in Kernel Discriminant Analysis via Optimal Scoring&rdquo;, preprint.
</p>

<hr>
<h2 id='LDA'>Linear Discriminant Analysis (LDA)</h2><span id='topic+LDA'></span>

<h3>Description</h3>

<p>A wrapper function for the various LDA implementations available in this package.
</p>
<p>Generates class predictions for <code>TestData</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LDA(TrainData, TrainCat, TestData, Method = "Full", Mode = "Automatic",
  m1 = NULL, m2 = NULL, m = NULL, s = NULL, gamma = 1e-05,
  type = "Rademacher")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LDA_+3A_traindata">TrainData</code></td>
<td>
<p>A (n x p) numeric matrix without missing values consisting of n training samples each with p features.</p>
</td></tr>
<tr><td><code id="LDA_+3A_traincat">TrainCat</code></td>
<td>
<p>A vector of length n consisting of group labels of the n training samples in <code>TrainData</code>. Must consist of 1s and 2s.</p>
</td></tr>
<tr><td><code id="LDA_+3A_testdata">TestData</code></td>
<td>
<p>A (m x p) numeric matrix without missing values consisting of m training samples each with p features. The number of features must equal the number of features in <code>TrainData</code>.</p>
</td></tr>
<tr><td><code id="LDA_+3A_method">Method</code></td>
<td>
<p>A string of characters which determines which version of LDA to use. Must be either &quot;Full&quot;, &quot;Compressed&quot;, &quot;Subsampled&quot;, &quot;Projected&quot;, or &quot;fastRandomFisher&quot;. Default is &quot;Full&quot;.</p>
</td></tr>
<tr><td><code id="LDA_+3A_mode">Mode</code></td>
<td>
<p>A string of characters which determines how the reduced sample paramters will be inputted for each method. Must be either &quot;Research&quot;, &quot;Interactive&quot;, or &quot;Automatic&quot;. Default is &quot;Automatic&quot;.</p>
</td></tr>
<tr><td><code id="LDA_+3A_m1">m1</code></td>
<td>
<p>The number of class 1 compressed samples to be generated. Must be a positive integer.</p>
</td></tr>
<tr><td><code id="LDA_+3A_m2">m2</code></td>
<td>
<p>The number of class 2 compressed samples to be generated. Must be a positive integer.</p>
</td></tr>
<tr><td><code id="LDA_+3A_m">m</code></td>
<td>
<p>The number of total compressed samples to be generated. Must be a positive integer.</p>
</td></tr>
<tr><td><code id="LDA_+3A_s">s</code></td>
<td>
<p>The sparsity level used in compression. Must satify 0 &lt; s &lt; 1.</p>
</td></tr>
<tr><td><code id="LDA_+3A_gamma">gamma</code></td>
<td>
<p>A numeric value for the stabilization amount gamma * I added to the covariance matrixed used in the LDA decision rule. Default amount is 1E-5. Cannot be negative.</p>
</td></tr>
<tr><td><code id="LDA_+3A_type">type</code></td>
<td>
<p>A string of characters determining the type of compression matrix used. The accepted values are <code>Rademacher</code>, <code>Gaussian</code>, and <code>Count</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function which handles all implementations of LDA.
</p>


<h3>Value</h3>

<p>A list containing 
</p>
<table>
<tr><td><code>Predictions</code></td>
<td>
<p>(m x 1) Vector of predicted class labels for the data points in <code>TestData</code>.</p>
</td></tr>
<tr><td><code>Dvec</code></td>
<td>
<p>(px1) Discriminant vector used to predict the class labels.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Lapanowski, Alexander F., and Gaynanova, Irina. &ldquo;Compressing large sample data for discriminant analysis&rdquo; arXiv preprint arXiv:2005.03858 (2020).
</p>
<p>Ye, Haishan, Yujun Li, Cheng Chen, and Zhihua Zhang. &ldquo;Fast Fisher discriminant analysis with randomized algorithms.&rdquo; Pattern Recognition 72 (2017): 82-92.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>TrainData &lt;- LDA_Data$TrainData
TrainCat &lt;- LDA_Data$TrainCat
TestData &lt;- LDA_Data$TestData
plot(TrainData[,2]~TrainData[,1], col = c("blue","orange")[as.factor(TrainCat)])

#----- Full LDA -------
LDA(TrainData = TrainData,
    TrainCat = TrainCat,
    TestData = TestData,
    Method = "Full",
    gamma = 1E-5)
  
#----- Compressed LDA -------  
 m1 &lt;- 700
 m2 &lt;- 300
 s &lt;- 0.01
 LDA(TrainData = TrainData,
     TrainCat = TrainCat,
     TestData = TestData,
     Method = "Compressed",
     Mode = "Research",
     m1 = m1,
     m2 = m2,
     s = s,
     gamma = 1E-5)
     
 LDA(TrainData = TrainData,
     TrainCat = TrainCat,
     TestData = TestData,
     Method = "Compressed",
     Mode = "Automatic",
     gamma = 1E-5)
 
 #----- Sub-sampled LDA ------
 m1 &lt;- 700
 m2 &lt;- 300
 LDA(TrainData = TrainData,
     TrainCat = TrainCat,
     TestData = TestData,
     Method = "Subsampled",
     Mode = "Research",
     m1 = m1,
     m2 = m2,
     gamma = 1E-5)
 
  LDA(TrainData = TrainData,
     TrainCat = TrainCat,
     TestData = TestData,
     Method = "Subsampled",
     Mode = "Automatic",
     gamma = 1E-5)
     
 #----- Projected LDA ------
  m1 &lt;- 700
  m2 &lt;- 300
  s &lt;- 0.01
  LDA(TrainData = TrainData,
      TrainCat = TrainCat,
      TestData = TestData,
      Method = "Projected",
      Mode = "Research",
      m1 = m1, 
      m2 = m2, 
      s = s,
      gamma = 1E-5)
      
   LDA(TrainData = TrainData,
      TrainCat = TrainCat,
      TestData = TestData,
      Method = "Projected",
      Mode = "Automatic",
      gamma = 1E-5)
      
 #----- Fast Random Fisher ------    
  m &lt;- 1000 
  s &lt;- 0.01
  LDA(TrainData = TrainData,
      TrainCat = TrainCat,
      TestData = TestData,
      Method = "fastRandomFisher",
      Mode = "Research",
      m = m, 
      s = s,
      gamma = 1E-5)
      
   LDA(TrainData = TrainData,
      TrainCat = TrainCat,
      TestData = TestData,
      Method = "fastRandomFisher",
      Mode = "Automatic",
      gamma = 1E-5)  
</code></pre>

<hr>
<h2 id='LDA_Data'>A list consisting of Training and Test data along with corresponding class labels.</h2><span id='topic+LDA_Data'></span>

<h3>Description</h3>

<p>A list consisting of Training and Test data along with corresponding class labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LDA_Data
</code></pre>


<h3>Format</h3>

<p>A list consisting of:
</p>

<dl>
<dt>TrainData</dt><dd><p> (10000 x 10) Matrix of independent normally-distributed training samples conditioned on class membership.
There are 7000 samples belonging to class 1, and 3000 samples belonging to class 2.
The class 1 mean vector is the vector of length 10 consisting only of -2. Likewise,
the class 2 mean vector is the vector of length 10 consisting only of 2.
The shared covariance matrix has (i,j) entry (0.5)^|i-j|.</p>
</dd>
<dt>TestData</dt><dd><p> (1000 x 10) Matrix of independenttest data features with the same distributions and class proportions as <code>TrainData</code></p>
</dd></dl>
<p>.
</p>
<dl>
<dt>Train</dt><dd><p> (10000 x 1) Vector of class labels for the samples in <code>TrainData</code>.</p>
</dd>
<dt>TestCat</dt><dd><p> (1000 x 1) Vector of class labels for the samples in <code>TestData</code>.</p>
</dd>
</dl>
<p>...
</p>


<h3>References</h3>

<p>Lapanowski, Alexander F., and Gaynanova, Irina. &ldquo;Compressing large-sample data for discriminant analysis&rdquo;, preprint.
</p>

<hr>
<h2 id='QDA'>Quadratic Discriminant Analysis (QDA)</h2><span id='topic+QDA'></span>

<h3>Description</h3>

<p>A wrapper function for the various QDA implementations available in this package.
</p>
<p>Generates class predictions for <code>TestData</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>QDA(TrainData, TrainCat, TestData, Method = "Full", Mode = "Automatic",
  m1 = NULL, m2 = NULL, m = NULL, s = NULL, gamma = 1e-05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="QDA_+3A_traindata">TrainData</code></td>
<td>
<p>A (n x p) numeric matrix without missing values consisting of n training samples each with p features.</p>
</td></tr>
<tr><td><code id="QDA_+3A_traincat">TrainCat</code></td>
<td>
<p>A vector of length n consisting of group labels of the n training samples in <code>TrainData</code>. Must consist of 1s and 2s.</p>
</td></tr>
<tr><td><code id="QDA_+3A_testdata">TestData</code></td>
<td>
<p>A (m x p) numeric matrix without missing values consisting of m training samples each with p features. The number of features must equal the number of features in <code>TrainData</code>.</p>
</td></tr>
<tr><td><code id="QDA_+3A_method">Method</code></td>
<td>
<p>A string of characters which determinds which version of QDA to use. Must be either &quot;Full&quot;, &quot;Compressed&quot;, or &quot;Subsampled&quot;.</p>
</td></tr>
<tr><td><code id="QDA_+3A_mode">Mode</code></td>
<td>
<p>A string of characters which determines how the reduced sample paramters will be inputted for each method. Must be either &quot;Research&quot;, &quot;Interactive&quot;, or &quot;Automatic&quot;. Default is &quot;Automatic&quot;.</p>
</td></tr>
<tr><td><code id="QDA_+3A_m1">m1</code></td>
<td>
<p>The number of class 1 compressed samples to be generated. Must be a positive integer.</p>
</td></tr>
<tr><td><code id="QDA_+3A_m2">m2</code></td>
<td>
<p>The number of class 2 compressed samples to be generated. Must be a positive integer.</p>
</td></tr>
<tr><td><code id="QDA_+3A_m">m</code></td>
<td>
<p>The number of total compressed samples to be generated. Must be a positive integer.</p>
</td></tr>
<tr><td><code id="QDA_+3A_s">s</code></td>
<td>
<p>The sparsity level used in compression. Must satify 0 &lt; s &lt; 1.</p>
</td></tr>
<tr><td><code id="QDA_+3A_gamma">gamma</code></td>
<td>
<p>A numeric value for the stabilization amount gamma * I added to the covariance matrixed used in the LDA decision rule. Default amount is 1E-5. Cannot be negative.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function which handles all implementations of LDA.
</p>


<h3>Value</h3>

<table>
<tr><td><code>Predictions</code></td>
<td>
<p>(m x 1) Vector of predicted class labels for the data points in <code>TestData</code>.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Lapanowski, Alexander F., and Gaynanova, Irina. &ldquo;Compressing large sample data for discriminant analysis&rdquo; arXiv preprint arXiv:2005.03858 (2020).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>TrainData &lt;- QDA_Data$TrainData
TrainCat &lt;- QDA_Data$TrainCat
TestData &lt;- QDA_Data$TestData
plot(TrainData[,2]~TrainData[,1], col = c("blue","orange")[as.factor(TrainCat)])

#----- Full QDA -------
QDA(TrainData = TrainData,
    TrainCat = TrainCat,
    TestData = TestData,
    Method = "Full",
    gamma = 1E-5)
  
#----- Compressed QDA -------  
 m1 &lt;- 700
 m2 &lt;- 300
 s &lt;- 0.01
 QDA(TrainData = TrainData,
     TrainCat = TrainCat,
     TestData = TestData,
     Method = "Compressed",
     Mode = "Research",
     m1 = m1,
     m2 = m2,
     s = s,
     gamma = 1E-5)
     
 QDA(TrainData = TrainData,
     TrainCat = TrainCat,
     TestData = TestData,
     Method = "Compressed",
     Mode = "Automatic",
     gamma = 1E-5)
 
 #----- Sub-sampled QDA ------
 m1 &lt;- 700
 m2 &lt;- 300
 QDA(TrainData = TrainData,
     TrainCat = TrainCat,
     TestData = TestData,
     Method = "Subsampled",
     Mode = "Research",
     m1 = m1,
     m2 = m2,
     gamma = 1E-5)
     
 QDA(TrainData = TrainData,
     TrainCat = TrainCat,
     TestData = TestData,
     Method = "Subsampled",
     Mode = "Automatic",
     gamma = 1E-5)
     
</code></pre>

<hr>
<h2 id='QDA_Data'>A list consisting of Training and Test data along with corresponding class labels.</h2><span id='topic+QDA_Data'></span>

<h3>Description</h3>

<p>A list consisting of Training and Test data along with corresponding class labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>QDA_Data
</code></pre>


<h3>Format</h3>

<p>A list consisting of:
</p>

<dl>
<dt>TrainData</dt><dd><p> (10000 x 10) Matrix of independent normally-distributed training samples conditioned on class membership.
There are 7000 samples belonging to class 1, and 3000 samples belonging to class 2.
The class 1 mean vector is the vector of length 10 consisting only of -2. Likewise,
the class 2 mean vector is the vector of length 10 consisting only of 2.
The class 1 covariance matrix has (i,j) entry (0.5)^|i-j|.
The class 2 covariance matrix has (i,j) entry (-0.5)^|i-j|.</p>
</dd>
<dt>TestData</dt><dd><p> (1000 x 10) Matrix of independenttest data features with the same distributions and class proportions as <code>TrainData</code></p>
</dd></dl>
<p>.
</p>
<dl>
<dt>Train</dt><dd><p> (10000 x 1) Vector of class labels for the samples in <code>TrainData</code>.</p>
</dd>
<dt>TestCat</dt><dd><p> (1000 x 1) Vector of class labels for the samples in <code>TestData</code>.</p>
</dd>
</dl>
<p>...
</p>


<h3>References</h3>

<p>Lapanowski, Alexander F., and Gaynanova, Irina. &ldquo;Compressing large-sample data for discriminant analysis&rdquo;, preprint.
</p>

<hr>
<h2 id='SelectParams'>Generates parameters.</h2><span id='topic+SelectParams'></span>

<h3>Description</h3>

<p>Generates parameters to be used in sparse kernel optimal scoring.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SelectParams(TrainData, TrainCat, Sigma = NULL, Gamma = NULL,
  Epsilon = 1e-05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SelectParams_+3A_traindata">TrainData</code></td>
<td>
<p>(n x p) Matrix of training data with numeric features. Cannot have missing values.</p>
</td></tr>
<tr><td><code id="SelectParams_+3A_traincat">TrainCat</code></td>
<td>
<p>(n x 1) Vector of class membership. Values must be either 1 or 2.</p>
</td></tr>
<tr><td><code id="SelectParams_+3A_sigma">Sigma</code></td>
<td>
<p>Scalar Gaussian kernel parameter. Default set to NULL and is automatically generated if user-specified value not provided. Must be &gt; 0. User-specified parameters must satisfy hierarchical ordering.</p>
</td></tr>
<tr><td><code id="SelectParams_+3A_gamma">Gamma</code></td>
<td>
<p>Scalar ridge parameter used in kernel optimal scoring. Default set to NULL and is automatically generated if user-specified value not provided. Must be &gt; 0. User-specified parameters must satisfy hierarchical ordering.</p>
</td></tr>
<tr><td><code id="SelectParams_+3A_epsilon">Epsilon</code></td>
<td>
<p>Numerical stability constant with default value 1e-05. Must be &gt; 0 and is typically chosen to be small.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generates the gaussian kernel, ridge, and sparsity parameters for use in sparse kernel optimal scoring using the methods presented in [Lapanowski and Gaynanova, preprint]. 
The Gaussian kernel parameter is generated using five-fold cross-validation of the misclassification error rate aross the .05, .1, .2, .3, .5 quantiles of squared-distances between groups. 
The ridge parameter is generated using a stabilization technique developed in Lapanowski and Gaynanova (2019).
The sparsity parameter is generated by five-fold cross-validation over a logarithmic grid of 20 values in an automatically-generated interval.
</p>


<h3>Value</h3>

<p>A list of 
</p>
<table>
<tr><td><code>Sigma</code></td>
<td>
<p> Gaussian kernel parameter.</p>
</td></tr>  
<tr><td><code>Gamma</code></td>
<td>
<p> Ridge Parameter.</p>
</td></tr>
<tr><td><code>Lambda</code></td>
<td>
<p> Sparsity parameter.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Lancewicki, Tomer. &quot;Regularization of the kernel matrix via covariance matrix shrinkage estimation.&quot; arXiv preprint arXiv:1707.06156 (2017).
</p>
<p>Lapanowski, Alexander F., and Gaynanova, Irina. &ldquo;Sparse feature selection in kernel discriminant analysis via optimal scoring&rdquo;, Artificial Intelligence and Statistics, 2019.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
Sigma &lt;- 1.325386  #Set parameter values equal to result of SelectParam.
Gamma &lt;- 0.07531579 #Speeds up example

TrainData &lt;- KOS_Data$TrainData
TrainCat &lt;- KOS_Data$TrainCat

SelectParams(TrainData = TrainData , 
             TrainCat = TrainCat, 
             Sigma = Sigma, 
             Gamma = Gamma)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
