<!DOCTYPE html><html><head><title>Help for package NlinTS</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {NlinTS}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#causality.test'><p>The Granger causality test</p></a></li>
<li><a href='#df.test'><p>Augmented Dickey_Fuller test</p></a></li>
<li><a href='#entropy_cont'><p>Continuous  entropy</p></a></li>
<li><a href='#entropy_disc'><p>Discrete Entropy</p></a></li>
<li><a href='#mi_cont'><p>Continuous  Mutual Information</p></a></li>
<li><a href='#mi_disc'><p>Discrete multivariate Mutual Information</p></a></li>
<li><a href='#mi_disc_bi'><p>Discrete bivariate Mutual Information</p></a></li>
<li><a href='#nlin_causality.test'><p>A non linear Granger causality test</p></a></li>
<li><a href='#NlinTS-package'>
<p>Models for non-linear causality detection in time series.</p></a></li>
<li><a href='#te_cont'><p>Continuous  Transfer Entropy</p></a></li>
<li><a href='#te_disc'><p>Discrete  Transfer Entropy</p></a></li>
<li><a href='#varmlp'><p>Artificial Neural Network VAR (Vector Auto-Regressive) model using a MultiLayer Perceptron, with the sigmoid activation function. The optimization algorithm is based on the stochastic gradient descent.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Models for Non Linear Causality Detection in Time Series</td>
</tr>
<tr>
<td>Version:</td>
<td>1.4.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-02-01</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Youssef Hmamouche &lt;hmamoucheyussef@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Models for non-linear time series analysis and causality detection. The main functionalities of this package consist of an implementation of the classical causality test (C.W.J.Granger 1980) &lt;<a href="https://doi.org/10.1016%2F0165-1889%2880%2990069-X">doi:10.1016/0165-1889(80)90069-X</a>&gt;,  and a non-linear version of it based on feed-forward neural networks. This package contains also an implementation of the Transfer Entropy &lt;<a href="https://doi.org/10.1103%2FPhysRevLett.85.461">doi:10.1103/PhysRevLett.85.461</a>&gt;, and the continuous Transfer Entropy using an approximation based on the k-nearest neighbors &lt;<a href="https://doi.org/10.1103%2FPhysRevE.69.066138">doi:10.1103/PhysRevE.69.066138</a>&gt;. There are also some other useful tools, like the VARNN (Vector Auto-Regressive Neural Network) prediction model, the Augmented test of stationarity, and the discrete and continuous entropy and mutual information.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GNU General Public License]</td>
</tr>
<tr>
<td>Depends:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, timeSeries, Rdpack</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++11</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Author:</td>
<td>Youssef Hmamouche [aut, cre]</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-02-01 15:52:25 UTC; youssef</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-02-02 01:20:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='causality.test'>The Granger causality test</h2><span id='topic+causality.test'></span>

<h3>Description</h3>

<p>The Granger causality test
</p>


<h3>Usage</h3>

<pre><code class='language-R'>causality.test(ts1, ts2, lag, diff = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="causality.test_+3A_ts1">ts1</code></td>
<td>
<p>Numerical dataframe containing one variable.</p>
</td></tr>
<tr><td><code id="causality.test_+3A_ts2">ts2</code></td>
<td>
<p>Numerical dataframe containing one variable.</p>
</td></tr>
<tr><td><code id="causality.test_+3A_lag">lag</code></td>
<td>
<p>The lag parameter.</p>
</td></tr>
<tr><td><code id="causality.test_+3A_diff">diff</code></td>
<td>
<p>Logical argument for the option of making data stationary before making the test.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is the classical Granger test of causality. The null hypothesis is that the second time series does not cause the first one
</p>


<h3>Value</h3>

<p>gci: the Granger causality index.
</p>
<p>Ftest:  the statistic of the test.
</p>
<p>pvalue: the p-value of the test.
</p>
<p>summary ():  shows the test results.
</p>


<h3>References</h3>


<p>Granger CWJ (1980).
&ldquo;Testing for Causality.&rdquo;
<em>Journal of Economic Dynamics and Control</em>, <b>2</b>, 329&ndash;352.
ISSN 0165-1889, doi: <a href="https://doi.org/10.1016/0165-1889(80)90069-X">10.1016/0165-1889(80)90069-X</a>.

</p>


<h3>Examples</h3>

<pre><code class='language-R'>library (timeSeries) # to extract time series
library (NlinTS)
data = LPP2005REC
model = causality.test (data[,1], data[,2], 2)
model$summary ()
</code></pre>

<hr>
<h2 id='df.test'>Augmented Dickey_Fuller test</h2><span id='topic+df.test'></span>

<h3>Description</h3>

<p>Augmented Dickey_Fuller test
</p>


<h3>Usage</h3>

<pre><code class='language-R'>df.test(ts, lag)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="df.test_+3A_ts">ts</code></td>
<td>
<p>Numerical dataframe.</p>
</td></tr>
<tr><td><code id="df.test_+3A_lag">lag</code></td>
<td>
<p>The lag parameter.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the stationarity test for a given univariate time series.
</p>


<h3>Value</h3>

<p>df: returns the value of the test.
</p>
<p>summary ():  shows the test results.
</p>


<h3>References</h3>


<p>Elliott G, Rothenberg TJ, Stock JH (1992).
&ldquo;Efficient tests for an autoregressive unit root.&rdquo;

</p>


<h3>Examples</h3>

<pre><code class='language-R'>library (timeSeries)
library (NlinTS)
#load data
data = LPP2005REC
model = df.test (data[,1], 1)
model$summary ()
</code></pre>

<hr>
<h2 id='entropy_cont'>Continuous  entropy</h2><span id='topic+entropy_cont'></span>

<h3>Description</h3>

<p>Continuous  entropy
</p>


<h3>Usage</h3>

<pre><code class='language-R'>entropy_cont(V, k = 3, log = "loge")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="entropy_cont_+3A_v">V</code></td>
<td>
<p>Interger vector.</p>
</td></tr>
<tr><td><code id="entropy_cont_+3A_k">k</code></td>
<td>
<p>Integer argument, the number of neighbors.</p>
</td></tr>
<tr><td><code id="entropy_cont_+3A_log">log</code></td>
<td>
<p>String argument in the set (&quot;log2&quot;, &quot;loge&quot;,&quot;log10&quot;), which indicates the log function to use. The loge is used by default.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the continuous entropy of a numerical vector using the Kozachenko approximation.
</p>


<h3>References</h3>


<p>Kraskov A, Stogbauer H, Grassberger P (2004).
&ldquo;Estimating mutual information.&rdquo;
<em>Phys. Rev. E</em>, <b>69</b>, 066138.
doi: <a href="https://doi.org/10.1103/PhysRevE.69.066138">10.1103/PhysRevE.69.066138</a>.

</p>


<h3>Examples</h3>

<pre><code class='language-R'>library (timeSeries)
library (NlinTS)
#load data
data = LPP2005REC
print (entropy_cont (data[,1], 3))
</code></pre>

<hr>
<h2 id='entropy_disc'>Discrete Entropy</h2><span id='topic+entropy_disc'></span>

<h3>Description</h3>

<p>Discrete Entropy
</p>


<h3>Usage</h3>

<pre><code class='language-R'>entropy_disc(V, log = "log2")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="entropy_disc_+3A_v">V</code></td>
<td>
<p>Integer vector.</p>
</td></tr>
<tr><td><code id="entropy_disc_+3A_log">log</code></td>
<td>
<p>String argument in the set (&quot;log2&quot;, &quot;loge&quot;,&quot;log10&quot;), which indicates the log function to use. The log2 is used by default.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the Shanon entropy of an integer vector.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library (NlinTS)
print (entropy_disc (c(3,2,4,4,3)))
</code></pre>

<hr>
<h2 id='mi_cont'>Continuous  Mutual Information</h2><span id='topic+mi_cont'></span>

<h3>Description</h3>

<p>Continuous  Mutual Information
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mi_cont(X, Y, k = 3, algo = "ksg1", normalize = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mi_cont_+3A_x">X</code></td>
<td>
<p>Integer vector, first time series.</p>
</td></tr>
<tr><td><code id="mi_cont_+3A_y">Y</code></td>
<td>
<p>Integer vector, the second time series.</p>
</td></tr>
<tr><td><code id="mi_cont_+3A_k">k</code></td>
<td>
<p>Integer argument, the number of neighbors.</p>
</td></tr>
<tr><td><code id="mi_cont_+3A_algo">algo</code></td>
<td>
<p>String argument specifies the algorithm use (&quot;ksg1&quot;, &quot;ksg2&quot;), as tow propositions of Kraskov estimation are provided. The first one (&quot;ksg1&quot;) is used by default.</p>
</td></tr>
<tr><td><code id="mi_cont_+3A_normalize">normalize</code></td>
<td>
<p>Logical argument (FALSE by default)  for the option of normalizing the mutual information by dividing it by the joint entropy.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the   Mutual Information between two vectors using the Kraskov estimator.
</p>


<h3>References</h3>


<p>Kraskov A, Stogbauer H, Grassberger P (2004).
&ldquo;Estimating mutual information.&rdquo;
<em>Phys. Rev. E</em>, <b>69</b>, 066138.
doi: <a href="https://doi.org/10.1103/PhysRevE.69.066138">10.1103/PhysRevE.69.066138</a>.

</p>


<h3>Examples</h3>

<pre><code class='language-R'>library (timeSeries)
library (NlinTS)
#load data
data = LPP2005REC
print (mi_cont (data[,1], data[,2], 3, 'ksg1'))
print (mi_cont (data[,1], data[,2], 3, 'ksg2'))
</code></pre>

<hr>
<h2 id='mi_disc'>Discrete multivariate Mutual Information</h2><span id='topic+mi_disc'></span>

<h3>Description</h3>

<p>Discrete multivariate Mutual Information
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mi_disc(df, log = "log2", normalize = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mi_disc_+3A_df">df</code></td>
<td>
<p>Datafame of type Integer.</p>
</td></tr>
<tr><td><code id="mi_disc_+3A_log">log</code></td>
<td>
<p>String argument in the set (&quot;log2&quot;, &quot;loge&quot;,&quot;log10&quot;), which indicates the log function to use. The log2 is used by default.</p>
</td></tr>
<tr><td><code id="mi_disc_+3A_normalize">normalize</code></td>
<td>
<p>Logical argument (FALSE by default)  for the option of normalizing the mutual information by dividing it by the joint entropy.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the Mutual Information between columns of a dataframe.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library (NlinTS)
df = data.frame (c(3,2,4,4,3), c(1,4,4,3,3))
mi = mi_disc (df)
print (mi)
</code></pre>

<hr>
<h2 id='mi_disc_bi'>Discrete bivariate Mutual Information</h2><span id='topic+mi_disc_bi'></span>

<h3>Description</h3>

<p>Discrete bivariate Mutual Information
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mi_disc_bi(X, Y, log = "log2", normalize = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mi_disc_bi_+3A_x">X</code></td>
<td>
<p>Integer vector.</p>
</td></tr>
<tr><td><code id="mi_disc_bi_+3A_y">Y</code></td>
<td>
<p>Integer vector.</p>
</td></tr>
<tr><td><code id="mi_disc_bi_+3A_log">log</code></td>
<td>
<p>String argument in the set (&quot;log2&quot;, &quot;loge&quot;,&quot;log10&quot;), which indicates the log function to use. The log2 is used by default.</p>
</td></tr>
<tr><td><code id="mi_disc_bi_+3A_normalize">normalize</code></td>
<td>
<p>Logical argument (FALSE by default)  for the option of normalizing the mutual information by dividing it by the joint entropy.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the Mutual Information between two integer vectors.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library (NlinTS)
mi = mi_disc_bi (c(3,2,4,4,3), c(1,4,4,3,3))
print (mi)
</code></pre>

<hr>
<h2 id='nlin_causality.test'>A non linear Granger causality test</h2><span id='topic+nlin_causality.test'></span>

<h3>Description</h3>

<p>A non linear Granger causality test
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlin_causality.test(
  ts1,
  ts2,
  lag,
  LayersUniv,
  LayersBiv,
  iters = 50,
  learningRate = 0.01,
  algo = "sgd",
  batch_size = 10,
  bias = TRUE,
  seed = 0,
  activationsUniv = vector(),
  activationsBiv = vector()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nlin_causality.test_+3A_ts1">ts1</code></td>
<td>
<p>Numerical series.</p>
</td></tr>
<tr><td><code id="nlin_causality.test_+3A_ts2">ts2</code></td>
<td>
<p>Numerical series.</p>
</td></tr>
<tr><td><code id="nlin_causality.test_+3A_lag">lag</code></td>
<td>
<p>The lag parameter</p>
</td></tr>
<tr><td><code id="nlin_causality.test_+3A_layersuniv">LayersUniv</code></td>
<td>
<p>Integer vector that contains the size of hidden layers of the univariate model. The length of this vector is the number of hidden layers, and the i-th element is the number of neurons in the i-th hidden layer.</p>
</td></tr>
<tr><td><code id="nlin_causality.test_+3A_layersbiv">LayersBiv</code></td>
<td>
<p>Integer vector that contains the size of hidden layers of the bivariate model. The length of this vector is the number of hidden layers, and the i-th element is the number of neurons in the i-th hidden layer.</p>
</td></tr>
<tr><td><code id="nlin_causality.test_+3A_iters">iters</code></td>
<td>
<p>The number of iterations.</p>
</td></tr>
<tr><td><code id="nlin_causality.test_+3A_learningrate">learningRate</code></td>
<td>
<p>The learning rate to use, O.1 by default, and if Adam algorithm is used, then it is the initial learning rate.</p>
</td></tr>
<tr><td><code id="nlin_causality.test_+3A_algo">algo</code></td>
<td>
<p>String argument, for the optimisation algorithm to use, in choice [&quot;sgd&quot;, &quot;adam&quot;]. By default &quot;sgd&quot; (stochastic gradient descent) is used. The algorithm 'adam' is to adapt the learning rate while using &quot;sgd&quot;.</p>
</td></tr>
<tr><td><code id="nlin_causality.test_+3A_batch_size">batch_size</code></td>
<td>
<p>Integer argument for the batch size used in the back-propagation algorithm.</p>
</td></tr>
<tr><td><code id="nlin_causality.test_+3A_bias">bias</code></td>
<td>
<p>Logical argument  for the option of using the bias in the networks.</p>
</td></tr>
<tr><td><code id="nlin_causality.test_+3A_seed">seed</code></td>
<td>
<p>Integer value for the random seed used in the random generation of the weights of the network (a value = 0 will use the clock as random generator seed).</p>
</td></tr>
<tr><td><code id="nlin_causality.test_+3A_activationsuniv">activationsUniv</code></td>
<td>
<p>String vector for the activations functions to use (in choice [&quot;sigmoid&quot;, &quot;relu&quot;, &quot;tanh&quot;]) for the univariate model. The length of this vector is the number of hidden layers plus one (the output layer). By default, the relu activation function is used in hidden layers, and the sigmoid in the last layer.</p>
</td></tr>
<tr><td><code id="nlin_causality.test_+3A_activationsbiv">activationsBiv</code></td>
<td>
<p>String vector for the activations functions to use (in choice [&quot;sigmoid&quot;, &quot;relu&quot;, &quot;tanh&quot;]) for the bivariate model. The length of this vector is the number of hidden layers plus one (the output layer). By default, the relu activation function is used in hidden layers, and the sigmoid in the last layer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A non-linear test of causality using artificial neural networks. Two MLP artificial neural networks are evaluated to perform the test, one using just the target time series (ts1), and the second using both time series. The null hypothesis of this test is that the second time series does not cause the first one.
</p>


<h3>Value</h3>

<p>gci: the Granger causality index.
</p>
<p>Ftest:  the statistic of the test.
</p>
<p>pvalue: the p-value of the test.
</p>
<p>summary ():  shows the test results.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library (timeSeries) # to extract time series
library (NlinTS)
data = LPP2005REC
model = nlin_causality.test (data[,1], data[,2], 2, c(2), c(4), 50, 0.01, "sgd", 30, TRUE, 5)
model$summary ()
</code></pre>

<hr>
<h2 id='NlinTS-package'>
Models for non-linear causality detection in time series.
</h2><span id='topic+NlinTS-package'></span><span id='topic+NlinTS'></span>

<h3>Description</h3>

<p>Globally, this package focuses on non-linear time series analysis,  especially on causality detection.
To deal with non-linear dependencies between time series, we propose an extension of the Granger causality test using feed-forward neural networks. This package includes also an implementation of the Transfer Entropy, which  can be also seen as a causality measure based on information theory. To do that, the package  includes discrete and continuous Transfer entropy using the Kraskov approximation. The NlinTS package includes also some other useful tools, like the VARNN (Vector Auto-Regressive Neural Network) model, the Augmented Dickey-Fuller test of stationarity, and the discrete and continuous entropy and mutual information.
</p>

<hr>
<h2 id='te_cont'>Continuous  Transfer Entropy</h2><span id='topic+te_cont'></span>

<h3>Description</h3>

<p>Continuous  Transfer Entropy
</p>


<h3>Usage</h3>

<pre><code class='language-R'>te_cont(X, Y, p = 1, q = 1, k = 3, normalize = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="te_cont_+3A_x">X</code></td>
<td>
<p>Integer vector, first time series.</p>
</td></tr>
<tr><td><code id="te_cont_+3A_y">Y</code></td>
<td>
<p>Integer vector, the second time series.</p>
</td></tr>
<tr><td><code id="te_cont_+3A_p">p</code></td>
<td>
<p>Integer, the lag parameter to use for the first vector, (p = 1 by default).</p>
</td></tr>
<tr><td><code id="te_cont_+3A_q">q</code></td>
<td>
<p>Integer the lag parameter to use for the first vector, (q = 1 by default).</p>
</td></tr>
<tr><td><code id="te_cont_+3A_k">k</code></td>
<td>
<p>Integer argument, the number of neighbors.</p>
</td></tr>
<tr><td><code id="te_cont_+3A_normalize">normalize</code></td>
<td>
<p>Logical argument  for the option of normalizing value of TE (transfer entropy) (FALSE by default).
This normalization is different from the discrete case, because, here the term H (X(t)| X(t-1), ..., X(t-p)) may be negative.
Consequently, we use another technique, we divide TE by H0 - H (X(t)| X(t-1), ..., X(t-p), Yt-1), ..., Y(t-q)), where H0 is the max entropy (of uniform distribution).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the continuous Transfer Entropy from the second time series to the first one using the Kraskov estimation
</p>


<h3>References</h3>


<p>Kraskov A, Stogbauer H, Grassberger P (2004).
&ldquo;Estimating mutual information.&rdquo;
<em>Phys. Rev. E</em>, <b>69</b>, 066138.
doi: <a href="https://doi.org/10.1103/PhysRevE.69.066138">10.1103/PhysRevE.69.066138</a>.

</p>


<h3>Examples</h3>

<pre><code class='language-R'>library (timeSeries)
library (NlinTS)
#load data
data = LPP2005REC
te = te_cont (data[,1], data[,2], 1, 1, 3)
print (te)
</code></pre>

<hr>
<h2 id='te_disc'>Discrete  Transfer Entropy</h2><span id='topic+te_disc'></span>

<h3>Description</h3>

<p>Discrete  Transfer Entropy
</p>


<h3>Usage</h3>

<pre><code class='language-R'>te_disc(X, Y, p = 1, q = 1, log = "log2", normalize = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="te_disc_+3A_x">X</code></td>
<td>
<p>Integer vector, first time series.</p>
</td></tr>
<tr><td><code id="te_disc_+3A_y">Y</code></td>
<td>
<p>Integer vector, the second time series.</p>
</td></tr>
<tr><td><code id="te_disc_+3A_p">p</code></td>
<td>
<p>Integer, the lag parameter to use for the first vector (p = 1 by default).</p>
</td></tr>
<tr><td><code id="te_disc_+3A_q">q</code></td>
<td>
<p>Integer, the lag parameter to use for the first vector (q = 1 by default)..</p>
</td></tr>
<tr><td><code id="te_disc_+3A_log">log</code></td>
<td>
<p>String argument in the set (&quot;log2&quot;, &quot;loge&quot;,&quot;log10&quot;), which indicates the log function to use. The log2 is used by default.</p>
</td></tr>
<tr><td><code id="te_disc_+3A_normalize">normalize</code></td>
<td>
<p>Logical argument  for the option of normalizing the value of TE (transfer entropy) (FALSE by default).
This normalization is done by deviding TE by H (X(t)| X(t-1), ..., X(t-p)), where H is the Shanon entropy.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the Transfer Entropy from the second time series to the first one.
</p>


<h3>References</h3>


<p>Schreiber T (2000).
&ldquo;Measuring Information Transfer.&rdquo;
<em>Physical Review Letters</em>, <b>85</b>(2), 461-464.
doi: <a href="https://doi.org/10.1103/PhysRevLett.85.461">10.1103/PhysRevLett.85.461</a>.

</p>


<h3>Examples</h3>

<pre><code class='language-R'>library (NlinTS)
te = te_disc (c(3,2,4,4,3), c(1,4,4,3,3), 1, 1)
print (te)
</code></pre>

<hr>
<h2 id='varmlp'>Artificial Neural Network VAR (Vector Auto-Regressive) model using a MultiLayer Perceptron, with the sigmoid activation function. The optimization algorithm is based on the stochastic gradient descent.</h2><span id='topic+varmlp'></span>

<h3>Description</h3>

<p>Artificial Neural Network VAR (Vector Auto-Regressive) model using a MultiLayer Perceptron, with the sigmoid activation function. The optimization algorithm is based on the stochastic gradient descent.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>varmlp(
  df,
  lag,
  sizeOfHLayers,
  iters = 50,
  learningRate = 0.01,
  algo = "sgd",
  batch_size = 10,
  bias = TRUE,
  seed = 5,
  activations = vector()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="varmlp_+3A_df">df</code></td>
<td>
<p>A numerical dataframe</p>
</td></tr>
<tr><td><code id="varmlp_+3A_lag">lag</code></td>
<td>
<p>The lag parameter.</p>
</td></tr>
<tr><td><code id="varmlp_+3A_sizeofhlayers">sizeOfHLayers</code></td>
<td>
<p>Integer vector that contains the size of hidden layers, where the length of this vector is the number of hidden layers, and the i-th element is the number of neurons in the i-th hidden layer.</p>
</td></tr>
<tr><td><code id="varmlp_+3A_iters">iters</code></td>
<td>
<p>The number of iterations.</p>
</td></tr>
<tr><td><code id="varmlp_+3A_learningrate">learningRate</code></td>
<td>
<p>The learning rate to use, O.1 by default, and if Adam algorithm is used, then it is the initial learning rate.</p>
</td></tr>
<tr><td><code id="varmlp_+3A_algo">algo</code></td>
<td>
<p>String argument, for the optimisation algorithm to use, in choice [&quot;sgd&quot;, &quot;adam&quot;]. By default &quot;sgd&quot; (stochastic gradient descent) is used. The algorithm 'adam' is to adapt the learning rate while using &quot;sgd&quot;.</p>
</td></tr>
<tr><td><code id="varmlp_+3A_batch_size">batch_size</code></td>
<td>
<p>Integer argument for the batch size used in the back-propagation algorithm.</p>
</td></tr>
<tr><td><code id="varmlp_+3A_bias">bias</code></td>
<td>
<p>Logical, true if the bias have to be used in the network.</p>
</td></tr>
<tr><td><code id="varmlp_+3A_seed">seed</code></td>
<td>
<p>Integer value for the seed used in the random generation of the weights of the network (a value = 0 will use the clock as random generator seed).</p>
</td></tr>
<tr><td><code id="varmlp_+3A_activations">activations</code></td>
<td>
<p>String vector for the activations functions to use (in choice [&quot;sigmoid&quot;, &quot;relu&quot;, &quot;tanh&quot;]). The length of this vector is the number of hidden layers plus one (the output layer). By default, the relu activation function is used in hidden layers, and the sigmoid in the last layer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function builds the model, and returns an object that can be used to make forecasts and can be updated from new data.
</p>


<h3>Value</h3>

<p>fit (df, iterations, batch_size):  fit/update the weights of the model from  the dataframe.
</p>
<p>forecast (df):   makes forecasts of an given dataframe. The forecasts include the forecasted row based on each previous &quot;lag&quot; rows, where the last one is the next forecasted row of  df.
</p>
<p>save (filename): save the model in a text file.
</p>
<p>load (filename): load the model from a text file.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library (timeSeries) # to extract time series
library (NlinTS)
#load data
data = LPP2005REC
# Predict the last row of the data
train_data = data[1:(nrow (data) - 1), ]
model = varmlp (train_data, 1, c(10), 50, 0.01, "sgd", 30, TRUE, 0);
predictions = model$forecast (train_data)
print (predictions[nrow (predictions),])
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
