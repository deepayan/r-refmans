<!DOCTYPE html><html><head><title>Help for package BayesLCA</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {BayesLCA}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#BayesLCA-package'>
<p>Bayesian Latent Class Analysis</p></a></li>
<li><a href='#Alzheimer'>
<p>Symptoms of Patients Suffering from Alzheimer's Syndrome</p></a></li>
<li><a href='#as.mcmc.blca.gibbs'>
<p>Converts <code>blca.gibbs</code> Objects to type <code>mcmc</code></p></a></li>
<li><a href='#blca'>
<p>Bayesian Latent Class Analysis with one of several methods</p></a></li>
<li><a href='#blca.boot'>
<p>Bayesian Latent Class Analysis via an EM Algorithm and Using Empirical Bootstrapping</p></a></li>
<li><a href='#blca.em'>
<p>Bayesian Latent Class Analysis  via an EM Algorithm</p></a></li>
<li><a href='#blca.em.sd'>
<p>Posterior Standard Deviation Estimates for Bayesian Latent Class Analysis  via an EM Algorithm</p></a></li>
<li><a href='#blca.gibbs'>
<p>Bayesian Latent Class Analysis via Gibbs Sampling</p></a></li>
<li><a href='#blca.vb'>
<p>Bayesian Latent Class Analysis  via a variational Bayes algorithm</p></a></li>
<li><a href='#data.blca'>
<p>Conveniently Format Data for Bayesian Latent Class</p></a></li>
<li><a href='#MAP'>
<p>Maximum <em>a posteriori</em> (MAP) Classification</p></a></li>
<li><a href='#plot.blca'>
<p>Plot Parameter Summaries, Density Estimates and Model Diagnostics for Bayesian Latent Class Analysis</p></a></li>
<li><a href='#print.blca'>
<p>Bayesian Latent Class Analysis</p></a></li>
<li><a href='#rlca'>
<p>Randomly Generate Binary Data with Underlying Latent Classes</p></a></li>
<li><a href='#summary.blca'>
<p>Bayesian Latent Class Analysis</p></a></li>
<li><a href='#Zscore'>
<p>Evaluating Class Membership of Binary Data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Latent Class Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>1.9</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-05-05</td>
</tr>
<tr>
<td>Depends:</td>
<td>e1071, coda</td>
</tr>
<tr>
<td>Description:</td>
<td>Bayesian Latent Class Analysis using several different
        methods.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Imports:</td>
<td>fields, nlme, MCMCpack</td>
</tr>
<tr>
<td>Author:</td>
<td>Arthur White [aut, cre] (Previous email address:
    arthur.white@ucdconnect.ie),
  Thomas Brendan Murphy [aut, ths]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Arthur White &lt;arwhite@tcd.ie&gt;</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-05-05 13:23:38 UTC; arthur</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-05-06 17:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='BayesLCA-package'>
Bayesian Latent Class Analysis
</h2><span id='topic+BayesLCA-package'></span><span id='topic+BayesLCA'></span>

<h3>Description</h3>

<p>Bayesian latent class analysis using several different methods.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> BayesLCA</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.4</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2015-04-09</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 2)</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Arthur White and Brendan Murphy
Maintainer: Arthur White &lt;arthur.white@ucdconnect.ie&gt;
</p>


<h3>References</h3>

<p>Arthur White, Thomas Brendan Murphy (2014). BayesLCA: An R Package for Bayesian Latent Class Analysis.&quot; Journal of Statistical Software, 61(13), 1-28. URL: http://www.jstatsoft.org/v61/i13/.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>type1 &lt;- c(0.8, 0.8, 0.2, 0.2)
type2 &lt;- c(0.2, 0.2, 0.8, 0.8)
x &lt;- rlca(1000, rbind(type1, type2), c(0.4,0.6))
fit.em &lt;- blca.em(x, 2)
plot(fit.em, which=1)
print(fit.em)
summary(fit.em)
data(Alzheimer)
fit.vb &lt;- blca(Alzheimer, 2, method="vb")
par(mfrow=c(3,3))
plot(fit.vb, which=3:4)
summary(fit.vb)
par(mfrow=c(1,1))
</code></pre>

<hr>
<h2 id='Alzheimer'>
Symptoms of Patients Suffering from Alzheimer's Syndrome
</h2><span id='topic+Alzheimer'></span>

<h3>Description</h3>

<p>Presence or absence of 6 symptoms of Alzheimer's disease (AD) in 240 patients diagnosed with early onset AD conducted in the Mercer Institute in St. James's Hospital, Dublin.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Alzheimer)</code></pre>


<h3>Format</h3>

<p>A binary matrix, consisting of 240 rows and 6 columns, with each row denoting an individual and each column denoting the presence/absence of one of the 6 symptoms: Hallucination, Activity, Aggression, Agitation, Diurnal and Affective.  A 1 denotes the presence of a symptom, a 0 the absence.
</p>


<h3>Source</h3>

<p>Moran M, Walsh C, Lynch A, Coen RF, Coakley D, Lawlor BA (2004) &ldquo;Syndromes of behavioural and psychological symptoms in mild Alzheimer's disease.&rdquo; <em>International Journal 
of Geriatric Psychiatry</em>, <b>19(4)</b>, 359&ndash;364. ISSN 1099-1166. doi:10.1002/gps.1091. URL 
http://dx.doi.org/10.1002/gps.1091. 
</p>
<p>Walsh C (2006) &ldquo;Latent Class Analysis Identification of Syndromes in Alzheimer's Disease: A 
Bayesian Approach.&rdquo; <em>metodoloyski zvezki - Advances in Methodology and Statistics</em>, <b>3(1)</b>, pp.147 &ndash; 162. URL mrvar.fdv.uni-lj.si/pub/mz/mz3.1/walsh.pdf
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Alzheimer)
fit2 &lt;- blca.em(Alzheimer, 2)
summary(fit2)

fit3&lt;- blca.em(Alzheimer, 3, restarts=20)
summary(fit3)

</code></pre>

<hr>
<h2 id='as.mcmc.blca.gibbs'>
Converts <code>blca.gibbs</code> Objects to type <code>mcmc</code>
</h2><span id='topic+as.mcmc.blca.gibbs'></span><span id='topic+blca2mcmc'></span>

<h3>Description</h3>

<p>Converts blca objects to mcmc objects.  This is only to be used with the Gibbs sampling method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'blca.gibbs'
as.mcmc(x, ...)
 blca2mcmc(x)
 </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.mcmc.blca.gibbs_+3A_x">x</code></td>
<td>

<p>An object of class blca.gibbs. An error is returned if this is not the case.
</p>
</td></tr>
<tr><td><code id="as.mcmc.blca.gibbs_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the <code><a href="coda.html#topic+mcmc">mcmc</a></code> function. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Whenever a Gibbs sampler is employed, it is always a good idea to ensure that parameter samples are being obtained correctly - that burn-in has been achieved, and that appropriate mixing is taking place, for example.  <code>as.mcmc.blca.gibbs</code> converts an object of class <code><a href="#topic+blca">blca</a></code> to that of <code><a href="coda.html#topic+mcmc">mcmc</a></code> to avail of the diagnostic  checks available in other R packages, particularly those in the <b>coda</b> package.
</p>


<h3>Value</h3>

<p>An <code class="reqn">N \times G*(M+1)</code> matrix of class <code><a href="coda.html#topic+mcmc">mcmc</a></code>, where N is the number of data points, M the number of columns and G the number of classes. The first G columns (labelled ClassProb 1 , ..., ClassProb G) are class membership probability samples, the next G*M columns (labelled ItemProb 1 1 , ItemProb 1 2, ..., ItemProb G 1, ..., ItemProb G M) are item response probability samples.
</p>


<h3>Note</h3>

<p>This function replaces the function <code>mcmc2blca</code>, which appeared in the original version of the package, and which is retained as an internal function for backwards compatibility reasons.
</p>


<h3>Author(s)</h3>

<p>Arthur White
</p>


<h3>See Also</h3>

<p><code><a href="#topic+blca.gibbs">blca.gibbs</a></code>,  <code><a href="coda.html#topic+geweke.diag">geweke.diag</a></code>, <code><a href="coda.html#topic+raftery.diag">raftery.diag</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Alzheimer)

## Not run:  fit.gibbs &lt;- blca.gibbs(Alzheimer, 2) 
## Not run:  raftery.diag(as.mcmc(fit.gibbs)) 

## Not run:  fit.gibbs &lt;- blca.gibbs(Alzheimer, 2, iter=50000, accept=0.1, burn.in=100) 
## Not run:  plot(as.mcmc(fit.gibbs)) 

</code></pre>

<hr>
<h2 id='blca'>
Bayesian Latent Class Analysis with one of several methods
</h2><span id='topic+blca'></span>

<h3>Description</h3>

<p>Latent class analysis (LCA) attempts to find G hidden classes in binary data X. blca utilises one of: an EM algorithm, a variational Bayes approximation, Gibbs sampling or boot-strapping techniques to find maximum <em> a posteriori</em> (MAP), standard error and density estimates of the parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>blca(X, G, method = c("em", "gibbs", "boot", "vb"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="blca_+3A_x">X</code></td>
<td>

<p>The data matrix. This may take one of several forms, see <code><a href="#topic+data.blca">data.blca</a></code>.
</p>
</td></tr>
<tr><td><code id="blca_+3A_g">G</code></td>
<td>

<p>The number of classes to run lca for.
</p>
</td></tr>
<tr><td><code id="blca_+3A_method">method</code></td>
<td>

<p>The method with which to perform lca on the data. Four methods are currently available, &quot;em&quot;, &quot;gibbs&quot;, &quot;boot&quot; or &quot;vb&quot;. Defaults to &quot;em&quot;, with a warning.
</p>
</td></tr>
<tr><td><code id="blca_+3A_...">...</code></td>
<td>

<p>Additional arguments to be passed on, depending on the method. See additional help files for details.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function calls to one of <code><a href="#topic+blca.em">blca.em</a></code>, <code><a href="#topic+blca.boot">blca.boot</a></code>, <code><a href="#topic+blca.gibbs">blca.gibbs</a></code>, <code><a href="#topic+blca.vb">blca.vb</a></code>, depending on the method specified.
</p>


<h3>Value</h3>

<p>A list of class &quot;blca&quot; is returned. All methods return the following items:
</p>
<table>
<tr><td><code>classprob</code></td>
<td>
<p>The class probabilities.</p>
</td></tr>
<tr><td><code>itemprob</code></td>
<td>
<p>The item probabilities, conditional on class membership.</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>Estimate of class membership for each unique datapoint.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>A list containing the prior values specified for the model.</p>
</td></tr>
</table>
<p>See additional help files for  details.
</p>


<h3>Note</h3>

<p>Earlier versions of this function erroneously referred to posterior standard deviations as standard errors. This also extended to some of the variable names of the returned function, which are now returned with the corrected suffix <code>blca.em.sd</code> (for standard deviation). For backwards compatability reasons, the earlier suffix <code>.se</code> has been retained as a returned argument.
</p>


<h3>Author(s)</h3>

<p>Arthur White
</p>


<h3>References</h3>

<p>Arthur White, Thomas Brendan Murphy (2014). BayesLCA: An R Package for Bayesian Latent Class Analysis.&quot; Journal of Statistical Software, 61(13), 1-28. URL: http://www.jstatsoft.org/v61/i13/.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+blca.em">blca.em</a></code>, <code><a href="#topic+blca.boot">blca.boot</a></code>, <code><a href="#topic+blca.gibbs">blca.gibbs</a></code>, <code><a href="#topic+blca.vb">blca.vb</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>type1 &lt;- c(0.8, 0.8, 0.2, 0.2)
type2 &lt;- c(0.2, 0.2, 0.8, 0.8)
x&lt;- rlca(1000, rbind(type1,type2), c(0.6,0.4))

set.seed(1)
fit &lt;- blca(x, 2) ## EM algorithm used, warning returned
print(fit)	  ## No posterior standard deviations returned
summary(fit)

set.seed(1)
fit2 &lt;- blca(x, 2, method="em", sd=TRUE) ##No warning - same fit
print(fit2) 				 ##Posterior standard deviations returned

set.seed(1)
##Variational Bayes approximation, with priors specified.
fit3 &lt;- blca(x, 2, method="vb", delta=c(5,5), alpha=2, beta=1) 
print(fit3)	##Posterior standard deviations returned also.
par(mfrow=c(3,2))			  
plot(fit3, which=3:4)
par(mfrow=c(1,1))
</code></pre>

<hr>
<h2 id='blca.boot'>
Bayesian Latent Class Analysis via an EM Algorithm and Using Empirical Bootstrapping
</h2><span id='topic+blca.boot'></span>

<h3>Description</h3>

<p>Latent class analysis (LCA) attempts to find G hidden classes in binary data X.  blca.boot repeatedly samples from X with replacement then utilises an EM algorithm to find maximum posterior (MAP) and standard error estimates of the parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>blca.boot(X, G, alpha = 1, beta = 1, delta = rep(1, G), 
	  start.vals = c("single", "across"), counts.n = NULL, 
	  fit = NULL, iter = 50, B = 100, relabel = FALSE, 
          verbose = TRUE, verbose.update = 10, small = 1e-100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="blca.boot_+3A_x">X</code></td>
<td>

<p>The data matrix. This may take one of several forms, see <code><a href="#topic+data.blca">data.blca</a></code>.
</p>
</td></tr>
<tr><td><code id="blca.boot_+3A_g">G</code></td>
<td>

<p>The number of classes to run lca for.
</p>
</td></tr>
<tr><td><code id="blca.boot_+3A_alpha">alpha</code>, <code id="blca.boot_+3A_beta">beta</code></td>
<td>

<p>The prior values for the data conditional on group membership. These may take several forms: a single value, recycled across all groups and columns, a vector of length G or M (the number of columns in the data), or finally, a <code class="reqn">G \times M</code> matrix specifying each prior value separately. Defaults to 1, i.e, a uniform prior, for each value.
</p>
</td></tr>
<tr><td><code id="blca.boot_+3A_delta">delta</code></td>
<td>

<p>Prior values for the mixture components in model.  Defaults to 1, i.e., a uniform prior.  May be single or vector valued (of length G).
</p>
</td></tr>
<tr><td><code id="blca.boot_+3A_start.vals">start.vals</code></td>
<td>

<p>Denotes how class membership is to be assigned during the initial step of the algorithm. Two character values may be chosen, &quot;single&quot;, which randomly assigns data points exclusively to one class, or &quot;across&quot;, which assigns class membership via  <code><a href="stats.html#topic+runif">runif</a></code>. Alternatively, class membership may be pre-specified, either as a vector of class membership, or as a matrix of probabilities. Defaults to &quot;single&quot;.
</p>
</td></tr>
<tr><td><code id="blca.boot_+3A_counts.n">counts.n</code></td>
<td>

<p>If data patterns have already been counted, a data matrix consisting of each unique data pattern can be supplied to the function, in addition to a vector counts.n, which supplies the corresponding number of times each pattern occurs in the data. 
</p>
</td></tr>
<tr><td><code id="blca.boot_+3A_fit">fit</code></td>
<td>

<p>Previously fitted models may be supplied in order to approximate standard error and unbiased point estimates. fit should be an object of class &quot;blca.em&quot;. Defaults to NULL if no object is supplied.
</p>
</td></tr>
<tr><td><code id="blca.boot_+3A_iter">iter</code></td>
<td>

<p>The maximum number of iterations that the algorithm runs over, for each bootstrapped sample.  Will stop earlier if the algorithm converges.
</p>
</td></tr>
<tr><td><code id="blca.boot_+3A_b">B</code></td>
<td>

<p>The number of bootstrap samples to run. Defaults to 100.
</p>
</td></tr>
<tr><td><code id="blca.boot_+3A_relabel">relabel</code></td>
<td>

<p>Logical valued. As the data is recursively sampled, it is possible that label-switching may occur with respect to parameter estimates.  If TRUE, parameter estimates are checked at each iteration, and relabeled if necessary.  Defaults to FALSE.
</p>
</td></tr>
<tr><td><code id="blca.boot_+3A_verbose">verbose</code></td>
<td>

<p>Logical valued. If TRUE, the current number of completed bootstrap samples is printed at regular intervals.
</p>
</td></tr>
<tr><td><code id="blca.boot_+3A_verbose.update">verbose.update</code></td>
<td>

<p>If <code>verbose=TRUE</code>, <code>verbose.update</code> determines the periodicity with which updates are printed.
</p>
</td></tr>
<tr><td><code id="blca.boot_+3A_small">small</code></td>
<td>

<p>To ensure numerical stability a small constant is added to certain parameter estimates. Defaults to 1e-100.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Bootstrapping methods can be used to estimate properties of a distribution's parameters, such as the standard error estimates, by constructing multiple resamples of an observed dataset, obtained by sampling with replacement from said dataset.  The multiple parameter estimates obtained from these resamples may then be analysed.  This method is implemented in blca.boot by first running blca.em over the full data set and then using the returned values of the item and class probabilities as the initial values when running the algorithm for each bootstrapped sample.  Alternatively, initial parameter estimates may be specified using the fit argument.
</p>
<p>Note that if a previously fitted model is supplied, then the prior values with which the model was fitted will be used for the sampling run, regardless of the values supplied to the prior arguments.
</p>


<h3>Value</h3>

<p>A list of class &quot;blca.boot&quot; is returned, containing:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>The initial call passed to the function.</p>
</td></tr>
<tr><td><code>itemprob</code></td>
<td>
<p>The item probabilities, conditional on class membership.</p>
</td></tr>
<tr><td><code>classprob</code></td>
<td>
<p>The class probabilities.</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>Estimate of class membership for each unique datapoint.</p>
</td></tr>
<tr><td><code>itemprob.sd</code></td>
<td>
<p>Posterior standard deviation estimates of the item probabilities.</p>
</td></tr>
<tr><td><code>classprob.sd</code></td>
<td>
<p>Posterior standard deviation estimates of the class probabilities.</p>
</td></tr>
<tr><td><code>classprob.initial</code>, <code>itemprob.initial</code></td>
<td>
<p>Initial parameter values for classprob and itemprob, used to run over each bootstrapped sample.</p>
</td></tr>
<tr><td><code>samples</code></td>
<td>
<p>A list containing the parameter estimates for each bootstrapped sample.</p>
</td></tr>
<tr><td><code>logpost</code></td>
<td>
<p>The log-posterior of the estimated model.</p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>The Bayesian Information Criterion for the estimated model.</p>
</td></tr>
<tr><td><code>AIC</code></td>
<td>
<p>Akaike's Information Criterion for the estimated model.</p>
</td></tr>
<tr><td><code>label</code></td>
<td>
<p>Logical value, indicating whether label switching has been checked for.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p> The number of times each unique datapoint point occured.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>A list containing the prior values specified for the model.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Earlier versions of this function erroneously referred to posterior standard deviations as standard errors. This also extended to arguments supplied to and returned by the function, some of which are now returned with the corrected corrected suffix <code>blca.em.sd</code> (for standard deviation). For backwards compatability reasons, the earlier suffix <code>.se</code> has been retained as a returned argument.
</p>


<h3>Author(s)</h3>

<p>Arthur White
</p>


<h3>References</h3>

<p>Wasserman, L, 22nd May 2007, <em>All of Nonparametric Statistics</em>, Springer-Verlag.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+blca.em">blca.em</a></code>, <code><a href="#topic+blca">blca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>type1 &lt;- c(0.8, 0.8, 0.2, 0.2)
type2 &lt;- c(0.2, 0.2, 0.8, 0.8)
x &lt;- rlca(1000, rbind(type1,type2), c(0.6,0.4))
fit.boot &lt;- blca.boot(x, 2)
summary(fit.boot)

## Not run: fit &lt;- blca.em(x, 2, se=FALSE)
## Not run: fit.boot &lt;- blca.boot(x, 2, fit=fit)
## Not run: fit.boot
## Not run: plot(fit.boot, which=1:4)
</code></pre>

<hr>
<h2 id='blca.em'>
Bayesian Latent Class Analysis  via an EM Algorithm
</h2><span id='topic+blca.em'></span>

<h3>Description</h3>

<p>Latent class analysis (LCA) attempts to find G hidden classes in binary data X. blca.em utilises an expectation-maximisation algorithm to find maximum <em>a posteriori</em> (map) estimates of the parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>blca.em(X, G, alpha = 1, beta = 1, delta = 1, 
	start.vals = c("single", "across"), counts.n = NULL,
	iter = 500, restarts = 5, verbose = TRUE, 
	sd = FALSE, se=sd, conv = 1e-06, small = 1e-100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="blca.em_+3A_x">X</code></td>
<td>

<p>The data matrix. This may take one of several forms, see <code><a href="#topic+data.blca">data.blca</a></code>.
</p>
</td></tr>
<tr><td><code id="blca.em_+3A_g">G</code></td>
<td>

<p>The number of classes to run lca for.
</p>
</td></tr>
<tr><td><code id="blca.em_+3A_alpha">alpha</code>, <code id="blca.em_+3A_beta">beta</code></td>
<td>

<p>The prior values for the data conditional on group membership. These may take several forms: a single value, recycled across all groups and columns, a vector of length G or M (the number of columns in the data), or finally, a <code class="reqn">G \times M</code> matrix specifying each prior value separately. Defaults to 1, i.e, a uniform prior, for each value.
</p>
</td></tr>
<tr><td><code id="blca.em_+3A_delta">delta</code></td>
<td>

<p>Prior values for the mixture components in model.  Defaults to 1, i.e., a uniform prior.  May be single or vector valued (of length G).
</p>
</td></tr>
<tr><td><code id="blca.em_+3A_start.vals">start.vals</code></td>
<td>

<p>Denotes how class membership is to be assigned during the initial step of the algorithm. Two character values may be chosen, &quot;single&quot;, which randomly assigns data points exclusively to one class, or &quot;across&quot;, which assigns class membership via  <code><a href="stats.html#topic+runif">runif</a></code>. Alternatively, class membership may be pre-specified, either as a vector of class membership, or as a matrix of probabilities. Defaults to &quot;single&quot;.
</p>
</td></tr>
<tr><td><code id="blca.em_+3A_counts.n">counts.n</code></td>
<td>

<p>If data patterns have already been counted, a data matrix consisting of each unique data pattern can be supplied to the function, in addition to a vector counts.n, which supplies the corresponding number of times each pattern occurs in the data. 
</p>
</td></tr>
<tr><td><code id="blca.em_+3A_iter">iter</code></td>
<td>

<p>The maximum number of iterations that the algorithm runs over.  Will stop early if the algorithm is deemed to converge.
</p>
</td></tr>
<tr><td><code id="blca.em_+3A_restarts">restarts</code></td>
<td>

<p><code>restarts</code> determines how many times the algorithm is run with different starting values. Parameter estimates from the run which achieved the highest log-posterior are returned. If starting values are supplied, these are used for the first run, after which random starting points are used. Defaults to 5. 
</p>
</td></tr>
<tr><td><code id="blca.em_+3A_verbose">verbose</code></td>
<td>

<p>Logical valued. If <code>TRUE</code>, the log-posterior from each run is printed.
</p>
</td></tr>
<tr><td><code id="blca.em_+3A_sd">sd</code></td>
<td>

<p>Specifies whether posterior standard deviation estimates should also be returned. If TRUE, calls to <a href="#topic+blca.em.sd">blca.em.sd</a>.
</p>
</td></tr>
<tr><td><code id="blca.em_+3A_se">se</code></td>
<td>

<p>Similarly to <code>sd</code>, specifies whether posterior standard deviation estimates should also be returned, however, its use is discouraged. Should always agree with <code>sd</code>. Retained for backwards compatability reasons. See &lsquo;Note&rsquo;.
</p>
</td></tr>
<tr><td><code id="blca.em_+3A_conv">conv</code></td>
<td>

<p>Convergence criteria, i.e., how small should the log-posterior increase become before the algorithm is deemed to have converged?  Set relative to the size of the data matrix.
</p>
</td></tr>
<tr><td><code id="blca.em_+3A_small">small</code></td>
<td>

<p>To ensure numerical stability a small constant is added to certain parameter estimates. Defaults to 1e-100.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Regardless of the form of the data supplied to <code><a href="#topic+blca.em">blca.em</a></code>, it is internally converted to be of the form <code><a href="#topic+data.blca">data.blca</a></code>.  In particular, this should be noted when supplying starting values: the object must be of either the same length or have the same number of rows as the number of <b>unique</b> observations in the dataset, as opposed to the total number.
</p>
<p>Posterior standard deviations and convergence checks are calculated using <code><a href="#topic+blca.em.sd">blca.em.sd</a></code>. 
</p>


<h3>Value</h3>

<p>A list of class &quot;blca.em&quot; is returned, containing:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>The initial call passed to the function.</p>
</td></tr>
<tr><td><code>itemprob</code></td>
<td>
<p>The item probabilities, conditional on class membership.</p>
</td></tr>
<tr><td><code>classprob</code></td>
<td>
<p>The class probabilities.</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>Estimate of class membership for each unique datapoint.</p>
</td></tr>
<tr><td><code>itemprob.sd</code></td>
<td>
<p>If returned, standard error estimates of the item probabilities.</p>
</td></tr>
<tr><td><code>classprob.sd</code></td>
<td>
<p>If returned, standard error estimates of the class probabilities.</p>
</td></tr>
<tr><td><code>logpost</code></td>
<td>
<p>The log-posterior of the estimated model.</p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>The Bayesian Information Criterion for the estimated model.</p>
</td></tr>
<tr><td><code>AIC</code></td>
<td>
<p>Akaike's Information Criterion for the estimated model.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>The number of iterations required before convergence.</p>
</td></tr>
<tr><td><code>poststore</code></td>
<td>
<p>The value of the log-posterior for each iteration.</p>
</td></tr>
<tr><td><code>eps</code></td>
<td>
<p>The value for which the algorithm was deemed to have converged at.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p> The number of times each unique datapoint point occured.</p>
</td></tr>
<tr><td><code>lpstarts</code></td>
<td>
<p> The log-posterior achieved after each of the multiple starts of the algorithm. </p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>If posterior standard deviations are calculated, then the Hessian of the model is also checked to determine whether the algorithm has converged to at least a local maximum. The convergence status is calculated by an integer value: 1 denotes acceptable convergence, 2 denotes that it converged at a saddle point, 3 that the algorithm ended before it had satisfactorily converged  and 4 denotes that at least one parameter value converged at a boundary value (i.e., a 1 or 0). 0 denotes that the algorithm converged satisfactorily but that the Hessian has not been checked.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>A list containing the prior values specified for the model.</p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p>A logical value indicating whether standard error estimates were returned. </p>
</td></tr>
</table>


<h3>Note</h3>

<p>Earlier versions of this function erroneously referred to posterior standard deviations as standard errors. This also extended to arguments supplied to and returned by the function, some of which are now returned with the corrected corrected suffix <code>blca.em.sd</code> (for standard deviation). For backwards compatability reasons, the earlier suffix <code>.se</code> has been retained as a returned argument.
</p>


<h3>Author(s)</h3>

<p>Arthur White
</p>


<h3>References</h3>

<p>Dempster AP, Laird NM, Rubin DB (1977). &ldquo;Maximum Likelihood from Incomplete Data via 
the EM Algorithm.&rdquo; Journal of the Royal Statistical Society. Series B (Methodological), <b>39(1)</b>, 
pp. 1&ndash;38. ISSN 00359246. doi:10.2307/2984875. URL http://dx.doi.org/10.2307/2984875.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+blca">blca</a></code>,<code><a href="#topic+blca.em.sd">blca.em.sd</a></code>, <code><a href="#topic+blca.boot">blca.boot</a></code>, <code><a href="#topic+blca.vb">blca.vb</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>type1 &lt;- c(0.8, 0.8, 0.2, 0.2)
type2 &lt;- c(0.2, 0.2, 0.8, 0.8)
x &lt;- rlca(1000, rbind(type1,type2), c(0.6,0.4))

fit &lt;- blca.em(x, 2)
print(fit)
fit &lt;- blca.em(x, 2, sd=TRUE) ##Returns posterior standard deviations
summary(fit)
plot(fit)

## Different starting values
fit &lt;- blca.em(x, 2, start.vals="across")
xx &lt;- data.blca(x)
fit &lt;- blca.em(xx, 2, start.vals=sample(1:2, length(xx$counts) , replace=TRUE))
</code></pre>

<hr>
<h2 id='blca.em.sd'>
Posterior Standard Deviation Estimates for Bayesian Latent Class Analysis  via an EM Algorithm
</h2><span id='topic+blca.em.sd'></span><span id='topic+blca.em.se'></span>

<h3>Description</h3>

<p>Returns posterior standard deviation estimates for point estimates returned by <code><a href="#topic+blca.em">blca.em</a></code>. These are obtained via asymptotic estmation of the Observed Information matrix. The Hessian of the log-posterior is also checked to determine whether point estimates occur at at least a local maximum.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>blca.em.sd(fit, x, counts.n = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="blca.em.sd_+3A_fit">fit</code></td>
<td>

<p>An object of class &quot;blca.em&quot;. 
</p>
</td></tr>
<tr><td><code id="blca.em.sd_+3A_x">x</code></td>
<td>

<p>A binary matrix. An object of class <code><a href="#topic+data.blca">data.blca</a></code> may also be supplied. In this case the argument <code>counts.n</code> is ignored.
</p>
</td></tr>
<tr><td><code id="blca.em.sd_+3A_counts.n">counts.n</code></td>
<td>

<p>A vector which supplies the corresponding number of times each pattern in X occurs in the data.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is primarily intended for use in conjunction with <code><a href="#topic+blca.em">blca.em</a></code>, and may be called directly by that function by setting <code>se=TRUE</code>. However it can in fact be used with any blca object. 
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>itemprob</code></td>
<td>
<p>Posterior standard deviation estimates of the item probabilities.</p>
</td></tr>
<tr><td><code>classprob</code></td>
<td>
<p>Posterior standard deviation estimates of the class probabilities.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>An integer value denoting whether point estimates occur at at least a local maximum. 1 denotes acceptable convergence, 2 denotes that it converged at a saddle point, 3 that the algorithm ended before it converged and 4 denotes that at least one parameter value converged at a boundary value.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The posterior standard deviation estimates are derived asymptotically, i.e., by inverting the information matrix of the parameters.  These values are known to be unreliable in cases where parameters estimates are close to  1 or 0, so caution is advised when checking their values.  Bootstrapping methods may provide better estimates.
</p>
<p>Computationally, the method becomes becomes unstable for values close to 1 or 0.  If the distance of any of the supplied parameter values from 0 or 1 is  &lt;1e-5, then posterior standard deviation estimates for these values are returned as 0.
</p>
<p>Earlier versions of this function erroneously referred to posterior standard deviations as standard errors.  This also extended to the function name, which has now been corrected to <code>blca.em.sd</code> (for standard deviation).  For backwards compatability reasons, the earlier function <code>blca.em.se</code> has been retained as an in internal function.
</p>


<h3>Author(s)</h3>

<p>Arthur White
</p>


<h3>See Also</h3>

<p><code><a href="#topic+blca.em">blca.em</a></code>, <code><a href="#topic+blca.boot">blca.boot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>type1 &lt;- c(0.8, 0.8, 0.2, 0.2)
type2 &lt;- c(0.2, 0.2, 0.8, 0.8)
x&lt;- rlca(1000, rbind(type1,type2), c(0.6,0.4))
dat&lt;- data.blca(x)

set.seed(1)
fit1 &lt;- blca.em(dat, 2, se=TRUE)
fit1$itemprob.sd
fit1$classprob.sd

set.seed(1)
fit2&lt;- blca.em(dat, 2, se=FALSE)
fit2.sd&lt;- blca.em.sd(fit2, dat)
fit2.sd$itemprob
fit2.sd$classprob
</code></pre>

<hr>
<h2 id='blca.gibbs'>
Bayesian Latent Class Analysis via Gibbs Sampling
</h2><span id='topic+blca.gibbs'></span>

<h3>Description</h3>

<p>Latent class analysis (LCA) attempts to find G hidden classes in binary data X. blca.gibbs performs Gibbs sampling to sample from the parameters' true distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>blca.gibbs(X, G, alpha = 1, beta = 1, delta = 1, 
	   start.vals = c("prior", "single", "across"), 
	   counts.n = NULL, iter = 5000, thin = 1, 
	   accept=thin, burn.in = 100, relabel = TRUE, 
           verbose = TRUE, verbose.update = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="blca.gibbs_+3A_x">X</code></td>
<td>

<p>The data matrix. This may take one of several forms, see <code><a href="#topic+data.blca">data.blca</a></code>.
</p>
</td></tr>
<tr><td><code id="blca.gibbs_+3A_g">G</code></td>
<td>

<p>The number of classes to run lca for.
</p>
</td></tr>
<tr><td><code id="blca.gibbs_+3A_alpha">alpha</code>, <code id="blca.gibbs_+3A_beta">beta</code></td>
<td>

<p>The prior values for the data conditional on group membership. These may take several forms: a single value, recycled across all groups and columns, a vector of length G or M (the number of columns in the data), or finally, a G x M matrix specifying each prior value separately. Defaults to 1, i.e, a uniform prior, for each value.
</p>
</td></tr>
<tr><td><code id="blca.gibbs_+3A_delta">delta</code></td>
<td>

<p>Prior values for the mixture components in model.  Defaults to 1, i.e., a uniform prior.  May be single or vector valued (of length G).
</p>
</td></tr>
<tr><td><code id="blca.gibbs_+3A_start.vals">start.vals</code></td>
<td>

<p>Denotes how class membership is to be assigned during the initial step of the algorithm. One of three character values may be chosen: &quot;prior&quot;, which samples parameter values from prior distributions, &quot;single&quot;, which randomly assigns data points exclusively to one class, or &quot;across&quot;, which assigns class membership via  <code><a href="stats.html#topic+runif">runif</a></code>. Alternatively, class membership may be pre-specified, either as a vector of class membership, or as a matrix of probabilities. Defaults to &quot;single&quot;.
</p>
</td></tr>
<tr><td><code id="blca.gibbs_+3A_counts.n">counts.n</code></td>
<td>

<p>If data patterns have already been counted, a data matrix consisting of each unique data pattern can be supplied to the function, in addition to a vector counts.n, which supplies the corresponding number of times each pattern occurs in the data. 
</p>
</td></tr>
<tr><td><code id="blca.gibbs_+3A_iter">iter</code></td>
<td>

<p>The number of iterations to run the gibbs sampler for <b>after</b> burn-in.
</p>
</td></tr>
<tr><td><code id="blca.gibbs_+3A_thin">thin</code></td>
<td>

<p>The thinning rate for samples from the distribution, in order to achieve good mixing. Should take a value greater  &gt;0 and &lt;=1. Defaults to 1.
</p>
</td></tr>
<tr><td><code id="blca.gibbs_+3A_accept">accept</code></td>
<td>

<p>Similarly to <code>accept</code>, specifies the thinning rate for samples from the distribution, in order to achieve good mixing, however, its use is discouraged. Should always agree with <code>sd</code>. Retained for backwards compatability reasons. See &lsquo;Note&rsquo;.
</p>
</td></tr>
<tr><td><code id="blca.gibbs_+3A_burn.in">burn.in</code></td>
<td>

<p>Number of iterations to run the Gibbs sampler for before beginning to store values.
</p>
</td></tr>
<tr><td><code id="blca.gibbs_+3A_relabel">relabel</code></td>
<td>

<p>Logical, indicating whether a mechanism to prevent label-switching be used or not. Defaults to TRUE.
</p>
</td></tr>
<tr><td><code id="blca.gibbs_+3A_verbose">verbose</code></td>
<td>

<p>Logical valued. If TRUE, the current number of completed samples is printed at regular intervals.
</p>
</td></tr>
<tr><td><code id="blca.gibbs_+3A_verbose.update">verbose.update</code></td>
<td>

<p>If <code>verbose=TRUE</code>, <code>verbose.update</code> determines the periodicity with which updates are printed.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The library coda provide extensive tools to diagnose and visualise MCMC chains. The generic function <code><a href="#topic+as.mcmc.blca.gibbs">as.mcmc.blca.gibbs</a></code>, makes <code>blca.gibbs</code> objects compatible with functions such as <code><a href="coda.html#topic+summary.mcmc">summary.mcmc</a></code> and <code><a href="coda.html#topic+raftery.diag">raftery.diag</a></code>.
</p>


<h3>Value</h3>

<p>A list of class &quot;blca.gibbs&quot; is returned, containing:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>The initial call passed to the function.</p>
</td></tr>
<tr><td><code>classprob</code></td>
<td>
<p>The class probabilities.</p>
</td></tr>
<tr><td><code>itemprob</code></td>
<td>
<p>The item probabilities, conditional on class membership.</p>
</td></tr>
<tr><td><code>classprob.sd</code></td>
<td>
<p>Posterior standard deviation estimates of the class probabilities.</p>
</td></tr>
<tr><td><code>itemprob.sd</code></td>
<td>
<p>Posterior standard deviation estimates of the item probabilities.</p>
</td></tr>
<tr><td><code>logpost</code></td>
<td>
<p>The log-posterior of the estimated model.</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>Estimate of class membership for each unique datapoint.</p>
</td></tr>
<tr><td><code>samples</code></td>
<td>
<p>A list containing Gibbs samples of the item and class probabilities and log-posterior.</p>
</td></tr>
<tr><td><code>DIC</code></td>
<td>
<p>The Deviance Information Criterion for the estimated model.</p>
</td></tr>
<tr><td><code>BICM</code></td>
<td>
<p>The Bayesian Information Criterion (Monte Carlo) for the estimated model.</p>
</td></tr>
<tr><td><code>AICM</code></td>
<td>
<p>Akaike's Information Criterion (Monte Carlo) for the estimated model.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p> The number of times each unique datapoint point occured.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>A list containing the prior values specified for the model.</p>
</td></tr>
<tr><td><code>thin</code></td>
<td>
<p>The acceptance rate for samples from the distribution.</p>
</td></tr>
<tr><td><code>burn.in</code></td>
<td>
<p>The number of iterations the gibbs sampler was run before beginning to store values.</p>
</td></tr>
<tr><td><code>relabel</code></td>
<td>
<p>Logical, indicating whether a mechanism to prevent label-switching was used.</p>
</td></tr>
<tr><td><code>labelstore</code></td>
<td>
<p>The stored labels during the sampling run. If relabel=TRUE, these show how labels were permuted in an attempt to avoid label-switching in the model.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Earlier versions of this function erroneously referred to posterior standard deviations as standard errors. This also extended to arguments supplied to and returned by the function, some of which are now returned with the corrected corrected suffix <code>blca.em.sd</code> (for standard deviation). For backwards compatability reasons, the earlier suffix <code>.se</code> has been retained as a returned argument.
The argument <code>thin</code> replaces <code>accept</code>, which appeared in the earliest version of the package. This is to maintain consistency with other packages, such as <b>rjags</b>.
</p>


<h3>Author(s)</h3>

<p>Arthur White
</p>


<h3>References</h3>

<p>Spiegelhalter DJ, Best NG, Carlin BP, Linde Avd (2002). &ldquo;Bayesian Measures of Model Complexity and Fit.&rdquo; Journal of the Royal Statistical Society. Series B (Statistical Methodology), 
64(4), pp. 583-639. ISSN 13697412. URL http://www.jstor.org/stable/3088806.
</p>
<p>Raftery AE, Newton MA, Satagopan JM, Krivitsky PN (2007). &ldquo;Estimating the integrated 
likelihood via posterior simulation using the harmonic mean identity.&rdquo; In Bayesian Statistics, 
pp. 1-45. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+blca">blca</a></code>, <code><a href="#topic+as.mcmc.blca.gibbs">as.mcmc.blca.gibbs</a></code>, <code><a href="coda.html#topic+raftery.diag">raftery.diag</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate a 4-dim. sample with 2 latent classes of 500 data points each.
## The probabilities for the 2 classes are given by type1 and type2.

type1 &lt;- c(0.8, 0.8, 0.2, 0.2)
type2 &lt;- c(0.2, 0.2, 0.8, 0.8)
x&lt;- rlca(1000, rbind(type1,type2), c(0.6,0.4))

## Not run: fit.gibbs&lt;-blca.gibbs(x,2, iter=1000, burn.in=10)
## Not run: summary(fit.gibbs)
## Not run: plot(fit.gibbs)
## Not run: raftery.diag(as.mcmc(fit.gibbs))


## Not run: fit.gibbs&lt;-blca.gibbs(x,2, iter=10000, burn.in=100, thin=0.5) 
## Not run: plot(fit.gibbs, which=4)
## Not run: raftery.diag(as.mcmc(fit.gibbs))

</code></pre>

<hr>
<h2 id='blca.vb'>
Bayesian Latent Class Analysis  via a variational Bayes algorithm
</h2><span id='topic+blca.vb'></span>

<h3>Description</h3>

<p>Latent class analysis (LCA) attempts to find G hidden classes in binary data X. blca.vb uses a variational EM algorithm to find the distribution which best approximates the parameters' true distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>blca.vb(X, G, alpha = 1, beta = 1, delta = 1, 
	start.vals = c("single", "across"), counts.n = NULL, 
        iter = 500, restarts = 1, verbose = TRUE, conv = 1e-06, 
	small = 1e-100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="blca.vb_+3A_x">X</code></td>
<td>

<p>The data matrix. This may take one of several forms, see <code><a href="#topic+data.blca">data.blca</a></code>.
</p>
</td></tr>
<tr><td><code id="blca.vb_+3A_g">G</code></td>
<td>

<p>The number of classes to run lca for.
</p>
</td></tr>
<tr><td><code id="blca.vb_+3A_alpha">alpha</code>, <code id="blca.vb_+3A_beta">beta</code></td>
<td>

<p>The prior values for the data conditional on group membership. These may take several forms: a single value, recycled across all groups and columns, a vector of length G or M (the number of columns in the data), or finally, a G x M matrix specifying each prior value separately. Defaults to 1, i.e, a uniform prior, for each value.
</p>
</td></tr>
<tr><td><code id="blca.vb_+3A_delta">delta</code></td>
<td>

<p>Prior values for the mixture components in model.  Defaults to 1, i.e., a uniform prior.  May be single or vector valued (of length G).
</p>
</td></tr>
<tr><td><code id="blca.vb_+3A_start.vals">start.vals</code></td>
<td>

<p>Denotes how class membership is to be assigned during the initial step of the algorithm. Two character values may be chosen, &quot;single&quot;, which randomly assigns data points exclusively to one class, or &quot;across&quot;, which assigns class membership via  <code><a href="stats.html#topic+runif">runif</a></code>. Alternatively, class membership may be pre-specified, either as a vector of class membership, or as a matrix of probabilities. Defaults to &quot;single&quot;.
</p>
</td></tr>
<tr><td><code id="blca.vb_+3A_counts.n">counts.n</code></td>
<td>

<p>If data patterns have already been counted, a data matrix consisting of each unique data pattern can be supplied to the function, in addition to a vector counts.n, which supplies the corresponding number of times each pattern occurs in the data. 
</p>
</td></tr>
<tr><td><code id="blca.vb_+3A_iter">iter</code></td>
<td>

<p>The maximum number of iterations that the algorithm runs over.  Will stop earlier if the algorithm converges.
</p>
</td></tr>
<tr><td><code id="blca.vb_+3A_restarts">restarts</code></td>
<td>

<p><code>restarts</code> determines how many times the algorithm is run with different starting values. Parameter estimates from the run which achieved the highest log-posterior are returned. If starting values are supplied, these are used for the first run, after which random starting points are used. Defaults to 1. 
</p>
</td></tr>
<tr><td><code id="blca.vb_+3A_verbose">verbose</code></td>
<td>

<p>Logical valued. If TRUE, the log-posterior from each run is printed.
</p>
</td></tr>
<tr><td><code id="blca.vb_+3A_conv">conv</code></td>
<td>

<p>Convergence criteria, i.e., how small should the log-posterior increase become before the algorithm is deemed to have converged?  Set relative to the size of the data matrix.
</p>
</td></tr>
<tr><td><code id="blca.vb_+3A_small">small</code></td>
<td>

<p>To ensure numerical stability a small constant is added to certain parameter estimates. Defaults to 1e-100.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The variational Bayes method approximates the posterior using as a product of independent distributions. Parameters are then estimated for this approximate distribution using a variational EM algorithm. This method has a tendency to underestimate parameter's variance; as such the standard error and density estimates should be interpreted with caution. 
</p>
<p>While it is worth starting the algorithm from multiple starting points, variational algorithms have less of a tendency to cpnverge at saddle point or sub-optimal local maxima.
</p>


<h3>Value</h3>

<p>A list of class &quot;blca.vb&quot; is returned, containing:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>The initial call passed to the function.</p>
</td></tr>
<tr><td><code>itemprob</code></td>
<td>
<p>The item probabilities, conditional on class membership.</p>
</td></tr>
<tr><td><code>classprob</code></td>
<td>
<p>The class probabilities.</p>
</td></tr>
<tr><td><code>itemprob.sd</code></td>
<td>
<p>Posterior standard deviation estimates of the item probabilities.</p>
</td></tr>
<tr><td><code>classprob.sd</code></td>
<td>
<p>Posterior standard deviation estimates of the class probabilities.</p>
</td></tr>
<tr><td><code>parameters</code></td>
<td>
<p>A list containing posterior parameter values for item and class probabilities, which are assumed to follow beta and Dirichlet distributions respectively.</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>Estimate of class membership for each unique datapoint.</p>
</td></tr>
<tr><td><code>LB</code></td>
<td>
<p>The lower bound estimate of the log-posterior of the estimated model.</p>
</td></tr>
<tr><td><code>lbstore</code></td>
<td>
<p>The value of the lower bound estimate for each iteration.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>The number of iterations required before convergence.</p>
</td></tr>
<tr><td><code>eps</code></td>
<td>
<p>The amount that the lower bound increased at the final iteration  of the algorithm's run. </p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p> The number of times each unique datapoint point occured.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>A list containing the prior values specified for the model.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Variational Bayes approximations, are known to often underestimate the standard errors of the parameters under investigation, so caution is advised when checking their values.
</p>
<p>Earlier versions of this function erroneously referred to posterior standard deviations as standard errors. This also extended to arguments supplied to and returned by the function, some of which are now returned with the corrected corrected suffix <code>blca.em.sd</code> (for standard deviation). For backwards compatability reasons, the earlier suffix <code>.se</code> has been retained as a returned argument.
</p>


<h3>Author(s)</h3>

<p>Arthur White
</p>


<h3>References</h3>

<p>Ormerod J, Wand M (2010). &ldquo;Explaining Variational Approximations.&rdquo; The American Statistician, 64(2), 140-153. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+blca.em">blca.em</a></code>, <code><a href="#topic+blca.gibbs">blca.gibbs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>type1 &lt;- c(0.8, 0.8, 0.2, 0.2)
type2 &lt;- c(0.2, 0.2, 0.8, 0.8)
x&lt;- rlca(1000, rbind(type1,type2), c(0.6,0.4))

fit &lt;- blca.vb(x, 2)
print(fit)
summary(fit)
par(mfrow=c(3,2))
plot(fit)
par(mfrow=c(1,1))

data(Alzheimer)
sj &lt;- blca.vb(Alzheimer, 10, delta=1/10)
sj$classprob    ##Empty Groups
</code></pre>

<hr>
<h2 id='data.blca'>
Conveniently Format Data for Bayesian Latent Class 
</h2><span id='topic+data.blca'></span>

<h3>Description</h3>

<p>Conveniently format data for use with <code><a href="#topic+blca">blca</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data.blca(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="data.blca_+3A_x">X</code></td>
<td>

<p>A data matrix intended for latent class analysis. See details.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data may take of one of two forms, either as a binary matrix, or as a matrix consisting of unique binary rows, with a column of counts. In either case <code><a href="#topic+data.blca">data.blca</a></code> will convert X into a list, with binary matrix and count vector explicitly identified.
</p>


<h3>Value</h3>

<p>A list of class data.blca, containing
</p>
<table>
<tr><td><code>counts.n</code></td>
<td>
<p>A vector of counts of each unique data entry.</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>A matrix consisting of each unique data entry.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function is used internally by <code><a href="#topic+blca">blca</a></code>, so its use is not necessary, though it will speed up computation time to supply the model with data of this form if repeated use of a function is required.
</p>


<h3>Author(s)</h3>

<p>Arthur White
</p>


<h3>See Also</h3>

<p><code><a href="#topic+blca">blca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>type1 &lt;- c(0.8, 0.8, 0.2, 0.2)
type2 &lt;- c(0.2, 0.2, 0.8, 0.8)
x&lt;- rlca(1000, rbind(type1,type2), c(0.6,0.4)) ##Only 16 unique observations possible

data.blca(x)
</code></pre>

<hr>
<h2 id='MAP'>
Maximum <em>a posteriori</em> (MAP) Classification
</h2><span id='topic+MAP'></span><span id='topic+unMAP'></span>

<h3>Description</h3>

<p><code>MAP</code> obtains maximum <em>a posteriori</em> (MAP) classifications. <code>unMAP</code> converts a classification vector into an indicator matrix.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MAP(mat, tie = c("random", "standard"))
unMAP(vec)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MAP_+3A_mat">mat</code></td>
<td>

<p>An <code class="reqn">N \times G</code> matrix, typically <code class="reqn">N</code> denotes observations from a dataset and <code class="reqn">G</code> denotes the number of underlying groups in the data. Each row is expected to contain positive entries which sum to 1, but this isn't strictly necessary.
</p>
</td></tr>
<tr><td><code id="MAP_+3A_tie">tie</code></td>
<td>

<p>May take one of two values, <code>"random"</code> or <code>"standard"</code>. Takes the value <code>"random"</code> by default. See 'Details'.
</p>
</td></tr>
<tr><td><code id="MAP_+3A_vec">vec</code></td>
<td>

<p>An vector consisting of integer entries. <code>unMAP</code> is intended to be used with a vector whose entries are classifications of dataset observations.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each row in <code>mat</code>, <code>MAP</code> assigns an indexing value identifying the entry in the row taking the highest value. In the case where multiple values in a row share a common largest value, <code>tie</code> determines how such a value is chosen. If <code>tie = "random"</code>, one of the suitable values is chosen at random; when <code>tie = "standard"</code>, the first such suitable value is selected, in common with other packages. Defaults to <code>"random"</code>. 
</p>


<h3>Value</h3>

<p><code>MAP</code> returns a classification vector. <code>unMAP</code> returns a classification matrix, with each row indicating group membership by the column entry which is non-zero (and equal to one). 
</p>


<h3>Author(s)</h3>

<p>Arthur White
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Zscore">Zscore</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##Simple example
s1&lt;- sample(1:2, 10, replace=TRUE)
unMAP(s1)
MAP(unMAP(s1))

##More to the point
data(Alzheimer)
fit&lt;- blca.em(Alzheimer, 2)
MAP(fit$Z) ## Best estimates of group membership. 

mat1&lt;- matrix(1/3, nrow=10, ncol=3) ##demonstrating the use of "tie" argument
MAP(mat1, tie="random")
MAP(mat1, tie="standard")
</code></pre>

<hr>
<h2 id='plot.blca'>
Plot Parameter Summaries, Density Estimates and Model Diagnostics for Bayesian Latent Class Analysis 
</h2><span id='topic+plot.blca'></span><span id='topic+plot.blca.boot'></span><span id='topic+plot.blca.em'></span><span id='topic+plot.blca.gibbs'></span><span id='topic+plot.blca.vb'></span>

<h3>Description</h3>

<p>Five plots are selectable: a plot summarising item and class probability, a mosaic plot representing classification uncertainty, item probability density estimates, conditional class probability density estimates, and a diagnostics plot. The default setting is for the first four plots to be displayed, with the exception of plot.blca.em, which cannot produce density plots and so only produces the first two plots by default.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'blca'
plot(x, which = 1L, main = "", col1 = heat.colors(12), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.blca_+3A_x">x</code></td>
<td>

<p>An object of class <code><a href="#topic+blca">blca</a></code>.
</p>
</td></tr>
<tr><td><code id="plot.blca_+3A_which">which</code></td>
<td>

<p>Which plots to select. May be any subset of <code>1:5</code>, with some exceptions. See &lsquo;Details&rsquo;.  
</p>
</td></tr>
<tr><td><code id="plot.blca_+3A_main">main</code></td>
<td>

<p>An overall title for the plot: see <code><a href="graphics.html#topic+title">title</a></code>.
</p>
</td></tr>
<tr><td><code id="plot.blca_+3A_col1">col1</code></td>
<td>

<p>Specifies a list of colours to be used by the heat map plot used when <code>which = 1</code>.<code><a href="graphics.html#topic+image">image</a></code>. Uses <code><a href="grDevices.html#topic+heat.colors">heat.colors</a></code> by default, but several other choices are available. See the help files of <code><a href="fields.html#topic+image.plot">image.plot</a></code>, <code><a href="graphics.html#topic+image">image</a></code> and <code><a href="grDevices.html#topic+palette">palette</a></code> for details.
</p>
</td></tr>
<tr><td><code id="plot.blca_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed onto the plotting devices. When <code>which = 1</code>, the plotting device is <code><a href="fields.html#topic+image.plot">image.plot</a></code>,  <code><a href="graphics.html#topic+mosaicplot">mosaicplot</a></code> in the case of <code>which=2</code>, and when <code>which=3:5</code>, <code><a href="graphics.html#topic+plot">plot</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Not all plots are available for some object classes. If the object is of class <code><a href="#topic+blca.em">blca.em</a></code>, density plots (<code>which = 3:4</code>) are unavailable, and a warning is returned. Similarly, diagnostic plots (<code>which = 5</code>) for <code><a href="#topic+blca.boot">blca.boot</a></code> objects are unavailable. 
</p>
<p>The available diagnostic plots differ depending on the class of the object in question. For <code><a href="#topic+blca.em">blca.em</a></code> and <code><a href="#topic+blca.vb">blca.vb</a></code> objects, the plot is intended as visual aid to check whether the respective algorithms have converged, i.e., that the log-posterior or lower bound have ceased increasing after successive iterations. The main aim of the diagnostic plot for <code><a href="#topic+blca.gibbs">blca.gibbs</a></code> objects is to visually check diagnostic measures such as mixing and burn-in, and also to assess whether label-switching has occurred, or been corrected for satisfactorily.
</p>
<p>Currently, the colors used in a plot can only be specified directly for <code>which = 1</code>. For classification uncertainty (<code>which = 2</code>) and density plots (<code>which = 3:4</code>), each group is colored by the <code><a href="grDevices.html#topic+palette">palette</a></code> function so that Group g takes color <code>palette()[g+1]</code>. For the default settings, Group 1 will then be colored red, Group 2 green, and so on.
</p>


<h3>Author(s)</h3>

<p>Arthur White
</p>


<h3>References</h3>

<p>Arthur White, Thomas Brendan Murphy (2014). BayesLCA: An R Package for Bayesian Latent Class Analysis.&quot; Journal of Statistical Software, 61(13), 1-28. URL: http://www.jstatsoft.org/v61/i13/.
</p>


<h3>See Also</h3>

<p><code><a href="fields.html#topic+image.plot">image.plot</a></code>, <code><a href="graphics.html#topic+mosaicplot">mosaicplot</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>type1 &lt;- c(0.8, 0.8, 0.2, 0.2)
type2 &lt;- c(0.2, 0.2, 0.8, 0.8)
x&lt;- rlca(1000, rbind(type1,type2), c(0.6,0.4))

fit &lt;- blca.em(x, 2)
plot(fit, which = 1:2) ## Parameter summary and classification uncertainty plots.

palette(rainbow(6)) ## Change color scheme
plot(fit, which = 2) 
palette("default") ## Restore default color scheme

fit2&lt;- blca.vb(x,2)
par(mfrow = c(3,4))
plot(fit2, which = 3) ## Approximate density plots for item probability parameters.
par(mfrow = c(1,1))
</code></pre>

<hr>
<h2 id='print.blca'>
Bayesian Latent Class Analysis
</h2><span id='topic+print.blca'></span>

<h3>Description</h3>

<p>Print a <code><a href="#topic+blca">blca</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'blca'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.blca_+3A_x">x</code></td>
<td>

<p>An object of class <code><a href="#topic+blca">blca</a></code>.
</p>
</td></tr>
<tr><td><code id="print.blca_+3A_...">...</code></td>
<td>

<p>Additional arguments to be passed onto lower-level functions at a later stage of development.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Prints parameter maximum <em>a posteriori</em> (map) and standard deviation estimates. The latter are sometimes unavailable for <code><a href="#topic+blca.em">blca.em</a></code> objects.
</p>


<h3>Value</h3>

<p>The blca object itself.
</p>


<h3>Author(s)</h3>

<p>Arthur White
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Alzheimer)
fit1&lt;- blca(Alzheimer, 2, method="em")
class(fit1)
print(fit1)
fit2&lt;- blca(Alzheimer, 2, method="em", sd=TRUE)
print(fit2) ## Standard Errors also printed

fit3&lt;- blca(Alzheimer, 2, method="vb")
print(fit3) ## Standard Errors as standard
</code></pre>

<hr>
<h2 id='rlca'>
Randomly Generate Binary Data with Underlying Latent Classes
</h2><span id='topic+rlca'></span>

<h3>Description</h3>

<p>A function which randomly generates data with respect to some underlying latent class. Data may be generated either by specifying item and class probabilities or by utilising an object previously fitted to data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rlca(n, itemprob = 0.5, classprob = 1, fit = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rlca_+3A_n">n</code></td>
<td>

<p>Number of data points to be generated.
</p>
</td></tr>
<tr><td><code id="rlca_+3A_itemprob">itemprob</code></td>
<td>

<p>The item probabilities, conditional on class membership. Defaults to 0.5.
</p>
</td></tr>
<tr><td><code id="rlca_+3A_classprob">classprob</code></td>
<td>

<p>The class probabilities. Defaults to 1, i.e., a one class model.
</p>
</td></tr>
<tr><td><code id="rlca_+3A_fit">fit</code></td>
<td>

<p>An object of class blca. If fit is supplied, data is generated using the class and item probabilities obtained. Defaults to NULL.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arthur White
</p>


<h3>See Also</h3>

<p><code><a href="#topic+data.blca">data.blca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>type1 &lt;- c(0.8, 0.8, 0.2, 0.2)
type2 &lt;- c(0.2, 0.2, 0.8, 0.8)
x&lt;- rlca(1000, rbind(type1,type2), c(0.6,0.4))

fit &lt;- blca.em(x, 2)

x2&lt;- rlca(1000, fit=fit)
fit2&lt;- blca.em(x2,2)
</code></pre>

<hr>
<h2 id='summary.blca'>
Bayesian Latent Class Analysis
</h2><span id='topic+summary.blca'></span><span id='topic+summary.blca.boot'></span><span id='topic+summary.blca.em'></span><span id='topic+summary.blca.gibbs'></span><span id='topic+summary.blca.vb'></span><span id='topic+print.summary.blca'></span>

<h3>Description</h3>

<p>Summary method for class &quot;blca&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'blca'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.blca_+3A_object">object</code></td>
<td>

<p>Object of class <code><a href="#topic+blca">blca</a></code>.
</p>
</td></tr>
<tr><td><code id="summary.blca_+3A_...">...</code></td>
<td>

<p>Additional arguments to be passed onto lower-level functions at a later stage of development.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A brief summary consisting of two parts: the prior values specified to the model,  and model diagnostics specific to the inference method used, such as information about the log-posterior (or lower bound in the case of <code>blca.vb</code>), as well the number of iterations the algorithm ran for, etc..
</p>


<h3>Author(s)</h3>

<p>Arthur White
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Alzheimer)
summary(blca.em(Alzheimer, 2))
summary(blca.vb(Alzheimer, 2, alpha=2, beta=2, delta=0.5))

## Not run: (fit.gibbs)&lt;- blca.gibbs(Alzheimer, 2, delta=2)
## Not run: summary(fit.gibbs)
</code></pre>

<hr>
<h2 id='Zscore'>
Evaluating Class Membership of Binary Data
</h2><span id='topic+Zscore'></span><span id='topic+Zscore.internal'></span>

<h3>Description</h3>

<p>For a fitted model of class <code><a href="#topic+blca">blca</a></code>, and binary data <code>X</code>, the probability of class membership for each data point is provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Zscore(X, fit = NULL, itemprob = NULL, classprob = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Zscore_+3A_x">X</code></td>
<td>

<p>A binary data matrix. <code>X</code> must have the same number of columns as the data that <code>fit</code> was applied to.
</p>
</td></tr>
<tr><td><code id="Zscore_+3A_fit">fit</code></td>
<td>

<p>An object of class <code><a href="#topic+blca">blca</a></code>.
</p>
</td></tr>
<tr><td><code id="Zscore_+3A_itemprob">itemprob</code></td>
<td>

<p>A matrix of item probabilities, conditional on class membership.
</p>
</td></tr>
<tr><td><code id="Zscore_+3A_classprob">classprob</code></td>
<td>

<p>A vector denoting class membership probability.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculation of the probability of class membership for a data point relies on two parameters, class membership and item probability. These may be supplied directly to Zscore, or alternatively, a <code><a href="#topic+blca">blca</a></code> object containing both parameters can be used instead. 
</p>


<h3>Value</h3>

<p>A matrix of equal rows to <code>X</code> and with G, the number of classes, columns, where each row is a score denoting the probability of class membership. Each row should therefore sum to 1.
</p>


<h3>Note</h3>

<p><code>Zscore.internal</code> has the same functionality as <code>Zscore</code>, but is only intended for internal use.
</p>


<h3>Author(s)</h3>

<p>Arthur White
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
type1 &lt;- c(0.8, 0.8, 0.05, 0.2)
type2 &lt;- c(0.2, 0.2, 0.05, 0.8)
x&lt;- rlca(250, rbind(type1,type2), c(0.5,0.5))

fit &lt;- blca.em(x, 2)
fit$Z ## Unique data types
Zscore(x, fit=fit) ## Whole data set
Zscore(c(0, 1, 1, 0), fit=fit) ## Not in data set
Zscore(x, itemprob=rbind(type1,type2), classprob=c(0.5,0.5))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
