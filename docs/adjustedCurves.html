<!DOCTYPE html><html><head><title>Help for package adjustedCurves</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {adjustedCurves}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#adjusted_curve_diff'>
<p>Estimate the difference between or the ratio of two Confounder-Adjusted Survival Curves or CIFs</p></a></li>
<li><a href='#adjusted_curve_test'>
<p>Test if there is a difference between two Confounder-Adjusted Survival Curves or CIFs</p></a></li>
<li><a href='#adjusted_rmst'>
<p>Estimate Confounder-Adjusted Restricted Mean Survival Times</p></a></li>
<li><a href='#adjusted_rmtl'>
<p>Estimate Confounder-Adjusted Restricted Mean Time Lost</p></a></li>
<li><a href='#adjusted_surv_quantile'>
<p>Estimate Confounder-Adjusted Survival Time Quantiles</p></a></li>
<li><a href='#adjustedcif'>
<p>Estimate Cause-Specific Confounder-Adjusted Cumulative Incidence Functions</p></a></li>
<li><a href='#adjustedCurves-package'>
<p>Confounder-Adjusted Survival Curves and Cumulative Incidence Functions</p></a></li>
<li><a href='#adjustedsurv'>
<p>Estimate Confounder-Adjusted Survival Curves</p></a></li>
<li><a href='#as_ggsurvplot_df'>
<p>Extract a <code>data.frame</code> containing the estimated survival curves from a <code>adjustedsurv</code> object</p></a></li>
<li><a href='#cif_aalen_johansen'>
<p>Group-Specific Aalen-Johansen CIFs</p></a></li>
<li><a href='#cif_aiptw'>
<p>Augmented Inverse Probability of Treatment Weighted CIFs</p></a></li>
<li><a href='#cif_aiptw_pseudo'>
<p>Augmented Inverse Probability of Treatment Weighted CIFs using Pseudo-Values</p></a></li>
<li><a href='#cif_direct'>
<p>Direct Adjusted Cumulative Incidence Functions</p></a></li>
<li><a href='#cif_direct_pseudo'>
<p>Direct Adjusted CIFs using Pseudo-Values</p></a></li>
<li><a href='#cif_iptw'>
<p>Inverse Probability of Treatment Weighted CIFs</p></a></li>
<li><a href='#cif_iptw_pseudo'>
<p>Inverse Probability of Treatment Weighted CIFs using Pseudo-Values</p></a></li>
<li><a href='#cif_matching'>
<p>Using Propensity-Score Matching to Calculate Adjusted CIFs</p></a></li>
<li><a href='#cif_tmle'>
<p>Targeted Maximum Likelihood Estimation for Continuous Time Competing Events Data</p></a></li>
<li><a href='#CSC_MI'>
<p>Cause-Specific Cox Regression with Multiple Imputation</p></a></li>
<li><a href='#FGR_MI'>
<p>Fine &amp; Gray Model with Multiple Imputation</p></a></li>
<li><a href='#models_cif_direct'>
<p>List of supported models in <code>cif_direct</code></p>
</p></a></li>
<li><a href='#models_surv_direct'>
<p>List of supported models in <code>surv_direct</code></p>
</p></a></li>
<li><a href='#plot_curve_diff'>
<p>Plot the Difference Between or the Ratio of Two Adjusted Survival Curves or CIFs</p></a></li>
<li><a href='#plot_rmst_curve'>
<p>Plot Adjusted Restricted Mean Survival Time Curves</p></a></li>
<li><a href='#plot_rmtl_curve'>
<p>Plot Adjusted Restricted Mean Time Lost Curves</p></a></li>
<li><a href='#plot.adjustedcif'>
<p>Plot Confounder-Adjusted Cumulative Incidence Functions</p></a></li>
<li><a href='#plot.adjustedsurv'>
<p>Plot Confounder-Adjusted Survival Curves</p></a></li>
<li><a href='#plot.curve_test'>
<p>Plot Method for <code>curve_test</code> Objects</p></a></li>
<li><a href='#print.curve_test'>
<p>Print Method for <code>curve_test</code> Objects</p></a></li>
<li><a href='#sim_confounded_crisk'>
<p>Simulate Competing Risks Data with Confounders</p></a></li>
<li><a href='#sim_confounded_surv'>
<p>Simulate Survival Data with Confounders</p></a></li>
<li><a href='#surv_aiptw'>
<p>Augmented Inverse Probability of Treatment Weighted Survival Curves</p></a></li>
<li><a href='#surv_aiptw_pseudo'>
<p>Augmented Inverse Probability of Treatment Weighted Survival Curves using Pseudo-Values</p></a></li>
<li><a href='#surv_direct'>
<p>Direct Adjusted Survival Curves</p></a></li>
<li><a href='#surv_direct_pseudo'>
<p>Direct Adjusted Survival Curves using Pseudo-Values</p></a></li>
<li><a href='#surv_emp_lik'>
<p>Empirical Likelihood Estimation Survival Curves</p></a></li>
<li><a href='#surv_iptw_cox'>
<p>Inverse Probability of Treatment Weighted Survival using Cox-Regression</p></a></li>
<li><a href='#surv_iptw_km'>
<p>Inverse Probability of Treatment Weighted Kaplan-Meier estimates</p></a></li>
<li><a href='#surv_iptw_pseudo'>
<p>Inverse Probability of Treatment Weighted Survival Estimates using Pseudo-Values</p></a></li>
<li><a href='#surv_iv_2SRIF'>
<p>Instrumental Variable based Survival Curve Estimation using the Two Stage Residual Inclusion method with a Frailty Term (2SRI-F)</p></a></li>
<li><a href='#surv_km'>
<p>Group-Specific Kaplan-Meier Survival Curves</p></a></li>
<li><a href='#surv_matching'>
<p>Using Propensity-Score Matching to Calculate Adjusted Survival Curves</p></a></li>
<li><a href='#surv_prox_aiptw'>
<p>Proximal Augmented Inverse Probability of Treatment Weighted Survival Curve Estimates</p></a></li>
<li><a href='#surv_prox_iptw'>
<p>Proximal Inverse Probability of Treatment Weighted Survival Curve Estimates</p></a></li>
<li><a href='#surv_strat_amato'>
<p>Adjusted Survival Curves for Categorical Confounders using the Method by Amato (1988)</p></a></li>
<li><a href='#surv_strat_cupples'>
<p>Adjusted Survival Curves for Categorical Confounders using the Method by Cupples et al. (1995)</p></a></li>
<li><a href='#surv_strat_nieto'>
<p>Adjusted Survival Curves for Categorical Confounders using the Method by Gregory (1988) and Nieto &amp; Coresh (1996)</p></a></li>
<li><a href='#surv_tmle'>
<p>Targeted Maximum Likelihood Estimation for Continuously Distributed Time-To-Event Data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Confounder-Adjusted Survival Curves and Cumulative Incidence
Functions</td>
</tr>
<tr>
<td>Version:</td>
<td>0.11.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Robin Denz &lt;robin.denz@rub.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Estimate and plot confounder-adjusted survival curves using
    either 'Direct Adjustment', 'Direct Adjustment with Pseudo-Values',
    various forms of 'Inverse Probability of Treatment Weighting', two
    forms of 'Augmented Inverse Probability of Treatment Weighting',
    'Empirical Likelihood Estimation' or 'Targeted Maximum Likelihood Estimation'.
	Also includes a significance test for the difference
    between two adjusted survival curves and the calculation of adjusted
    restricted mean survival times.  Additionally enables the user to
    estimate and plot cause-specific confounder-adjusted cumulative
    incidence functions in the competing risks setting using the same
    methods (with some exceptions).
	For details, see Denz et. al (2023) &lt;<a href="https://doi.org/10.1002%2Fsim.9681">doi:10.1002/sim.9681</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/RobinDenz1/adjustedCurves">https://github.com/RobinDenz1/adjustedCurves</a>,
<a href="https://robindenz1.github.io/adjustedCurves/">https://robindenz1.github.io/adjustedCurves/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/RobinDenz1/adjustedCurves/issues">https://github.com/RobinDenz1/adjustedCurves/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>R.utils, doParallel, doRNG, dplyr (&ge; 1.0.0), foreach, rlang</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS, Matching (&ge; 4.9), WeightIt (&ge; 0.11.0), cmprsk (&ge;
2.2), eventglm (&ge; 1.1.1), geepack (&ge; 1.3), ggplot2 (&ge;
3.4.0), knitr, mice (&ge; 3.0.0), nnet, pammtools (&ge; 0.5), pec
(&ge; 2020.11.17), prodlim (&ge; 2019.11.13), riskRegression (&ge;
2020.12.08), rmarkdown, survival (&ge; 3.0.0), testthat (&ge;
3.0.0), tidyr, ggpp (&ge; 0.4.3), vdiffr (&ge; 1.0.0), covr,
data.table, concrete (&ge; 1.0.5), numDeriv</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Contact:</td>
<td>&lt;robin.denz@rub.de&gt;</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-13 12:19:31 UTC; Robin Denz</td>
</tr>
<tr>
<td>Author:</td>
<td>Robin Denz [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-13 12:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='adjusted_curve_diff'>
Estimate the difference between or the ratio of two Confounder-Adjusted Survival Curves or CIFs
</h2><span id='topic+adjusted_curve_diff'></span><span id='topic+adjusted_curve_ratio'></span>

<h3>Description</h3>

<p>Given a previously created <code>adjustedsurv</code> or <code>adjustedcif</code> object, calculate the difference between or the ratio of two of the <code>variable</code> specific curves. Can either calculate the whole difference / ratio curve or estimates at specified points in time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjusted_curve_diff(adj, group_1=NULL, group_2=NULL,
                    times=NULL, conf_int=FALSE, conf_level=0.95,
                    use_boot=FALSE, interpolation="steps")

adjusted_curve_ratio(adj, group_1=NULL, group_2=NULL,
                     times=NULL, conf_int=FALSE, conf_level=0.95,
                     use_boot=FALSE, interpolation="steps")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adjusted_curve_diff_+3A_adj">adj</code></td>
<td>

<p>An <code>adjustedsurv</code> object created using the <code>adjustedsurv</code> function, or a <code>adjustedcif</code> object created using the <code>adjustedcif</code> function.
</p>
</td></tr>
<tr><td><code id="adjusted_curve_diff_+3A_group_1">group_1</code></td>
<td>

<p>Optional argument to get a specific difference or ratio. This argument takes a single character string specifying one of the levels of the <code>variable</code> used in the original <code>adjustedsurv</code> or <code>adjustedcif</code> function call. This group will be subtracted from. For example if <code>group_1="A"</code> and <code>group_2="B"</code> the difference <code>A - B</code> or ratio <code>A / B</code> will be used. If <code>NULL</code>, the order of the factor levels in the original <code>data</code> determines the order. If not <code>NULL</code>, the <code>group_2</code> argument also needs to be specified.
</p>
</td></tr>
<tr><td><code id="adjusted_curve_diff_+3A_group_2">group_2</code></td>
<td>

<p>Also a single character string specifying one of the levels of <code>variable</code>. This corresponds to the right side of the difference equation. See argument <code>group_1</code>.
</p>
</td></tr>
<tr><td><code id="adjusted_curve_diff_+3A_times">times</code></td>
<td>

<p>An optional numeric vector of points in time at which the difference or ratio should be estimated. If <code>NULL</code> (default) the differences or ratios are estimated for the whole curve.
</p>
</td></tr>
<tr><td><code id="adjusted_curve_diff_+3A_conf_int">conf_int</code></td>
<td>

<p>Whether standard errors, confidence intervals and p-values should be calculated. Only possible when either <code>conf_int=TRUE</code> or <code>bootstap=TRUE</code> was used in the original function call. See details for how those are estimated.
</p>
</td></tr>
<tr><td><code id="adjusted_curve_diff_+3A_conf_level">conf_level</code></td>
<td>

<p>A number specifying the confidence level of the confidence intervals.
</p>
</td></tr>
<tr><td><code id="adjusted_curve_diff_+3A_use_boot">use_boot</code></td>
<td>

<p>Whether to use the standard errors estimated using bootstrapping for the confidence interval and p-value calculation. Can only be used if <code>bootstrap=TRUE</code> was used in the original <code>adjustedsurv</code> or <code>adjustedcif</code> function call. Ignored if <code>conf_int=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="adjusted_curve_diff_+3A_interpolation">interpolation</code></td>
<td>

<p>Either <code>"steps"</code> (default) or <code>"linear"</code>. This parameter controls how interpolation is performed. If this argument is set to <code>"steps"</code>, the curves will be treated as step functions. If it is set to <code>"linear"</code>, the curves wil be treated as if there are straight lines between the point estimates instead. Points that lie between estimated points will be interpolated accordingly. Should usually be kept at <code>"steps"</code>. See Details.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong><em>Confidence Intervals &amp; P-Values</em></strong>
</p>
<p>For differences, the standard error of the difference is estimated using the pooled standard error of the two probability estimates, given by:
</p>
<p style="text-align: center;"><code class="reqn">SE_{group_1 - group_2} = \sqrt{SE_{group_1}^2 + SE_{group_2}^2}</code>
</p>

<p>Confidence intervals are then calculated using this pooled standard error and the normal approximation. The P-Values are also obtained using this standard error combined with a two-sided one-sample t-test. The null-hypothesis is that the difference is equal to 0, and the alternative hypothesis is that the difference is not equal to 0.
</p>
<p>For ratios, the confidence intervals are calculated according to the method given by Fieller (1954), assuming the probabilities to be independent. P-values are calculated using a one-sample two-sided t-test with the test-statistic of Fieller (1954).
</p>
<p>If p-values are calculated for multiple points in time simultaneously, the user should adjust those. See <code>?p.adjust</code> for more information.
</p>
<p><strong><em>Overall Difference Test</em></strong>
</p>
<p>This function does not perform a test of the overall difference between two functions. To calculate the integral of the difference in a given interval the <code><a href="#topic+plot_curve_diff">plot_curve_diff</a></code> function can be used. Additionally, to test whether that integral is equal to zero the <code><a href="#topic+adjusted_curve_test">adjusted_curve_test</a></code> function can be used. No such test is available for ratios, as it is unclear what that would entail.
</p>
<p><strong><em>More than Two Groups</em></strong>
</p>
<p>If more than two groups are present in <code>variable</code>, all other comparisons except for <code>group_1 vs. group_2</code> are ignored. If multiple comparisons are desired, the user needs to call this function multiple times and adjust the <code>group_1</code> and <code>group_2</code> arguments accordingly.
</p>
<p><strong><em>Graphical Displays</em></strong>
</p>
<p>There is no directly associated <code>plot</code> method for this function. However, this function is used internally when calling the <code><a href="#topic+plot_curve_diff">plot_curve_diff</a></code> function. In order to get a plot of the difference curve or point estimates, that function can be used.
</p>
<p><strong><em>Multiple Imputation</em></strong>
</p>
<p>This function works exactly the same way for adjusted survival curves or adjusted CIFs estimated using multiple imputation as it does without any missing values. If multiple imputation was used previously, this function simply uses the pooled estimates to calculate the differences or ratios.
</p>
<p><strong><em>Computational Details</em></strong>
</p>
<p>When estimating the difference or ratios at some point in time at which no direct point estimates are available, this function needs to interpolate the curves. The interpolation method can be controlled using the <code>interpolation</code> function. In most cases, the estimated curves are step functions and the default (<code>interpolation="steps"</code>) is therefore appropriate. However, when parametric survival models where used in the estimation process it might be preferable to use linear interpolation instead.
</p>


<h3>Value</h3>

<p>Returns a <code>data.frame</code> containing the columns <code>time</code> (the points in time where the difference or ratios were estimated) and <code>diff</code> or <code>ratio</code> (the estimated difference or ratio).
</p>
<p>If <code>conf_int=TRUE</code> was used in the function call, it additionally contains the columns <code>se</code> (the estimated standard error of the difference, not included for ratios), <code>ci_lower</code> (lower limit of the confidence interval of the difference/ratio), <code>ci_upper</code> (upper limit of the confidence interval of the difference/ratio) and <code>p_value</code> (the p-value for the mentioned test).
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>John P. Klein, Brent Logan, Mette Harhoff, and Per Kragh Andersen (2007). &quot;Analyzing Survival Curves at a Fixed Point in Time&quot;. In: Statistics in Medicine 26, pp. 4505-4519
</p>
<p>Michael Coory, Karen E. Lamb, and Michael Sorich (2014). &quot;Risk-Difference Curves can be used to Communicate Time-Dependent Effects of Adjuvant Therapies for Early Stage Cancer&quot;. In: Journal of Clinical Epidemiology 67, pp. 966-972
</p>
<p>Edgar C. Fieller (1954). &quot;Some Problems in Interval Estimation&quot;. In: Journal of the Royal Statistical Society, Series B 16.2, pp. 175-185
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot_curve_diff">plot_curve_diff</a></code>, <code><a href="#topic+adjustedsurv">adjustedsurv</a></code>, <code><a href="#topic+adjustedcif">adjustedcif</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)
library(cmprsk)

#### Simple Survival Case with adjusted survival curves ####

# simulate some data as example
set.seed(42)
sim_dat &lt;- sim_confounded_surv(n=30, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# propensity score model
ps_mod &lt;- glm(group ~ x1 + x2 + x4 + x5, data=sim_dat, family="binomial")

# use it to estimate adjusted survival curves with bootstrapping
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="iptw_km",
                        treatment_model=ps_mod,
                        conf_int=TRUE,
                        bootstrap=TRUE,
                        n_boot=10) # n_boot should be much higher in reality

# calculate the whole difference curve
adjdiff &lt;- adjusted_curve_diff(adjsurv)
adjratio &lt;- adjusted_curve_ratio(adjsurv)

# only some points in time
adjdiff &lt;- adjusted_curve_diff(adjsurv, times=c(0.2, 0.4))
adjratio &lt;- adjusted_curve_ratio(adjsurv, times=c(0.2, 0.4))

# with confidence intervals, p-values
adjdiff &lt;- adjusted_curve_diff(adjsurv, times=c(0.2, 0.4), conf_int=TRUE)
adjratio &lt;- adjusted_curve_ratio(adjsurv, times=c(0.2, 0.4), conf_int=TRUE)

# using bootstrapping
adjdiff &lt;- adjusted_curve_diff(adjsurv, times=c(0.2, 0.4), conf_int=TRUE,
                               use_boot=TRUE)

#### Competing Risks Case with adjusted CIFs ####
library(riskRegression)
library(prodlim)

# simulate some data as example
sim_dat &lt;- sim_confounded_crisk(n=41, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a cause-specific cox-regression for the outcome
csc_mod &lt;- CSC(Hist(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group,
               data=sim_dat)

# use it to calculate adjusted CIFs for cause = 1 with bootstrapping
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      method="direct",
                      outcome_model=csc_mod,
                      conf_int=TRUE,
                      bootstrap=TRUE,
                      n_boot=10,
                      cause=1)

# calculate the whole difference curve
adjdiff &lt;- adjusted_curve_diff(adjcif)
adjratio &lt;- adjusted_curve_ratio(adjcif)

# with confidence intervals
adjdiff &lt;- adjusted_curve_diff(adjcif, conf_int=TRUE)
adjratio &lt;- adjusted_curve_ratio(adjcif, conf_int=TRUE)

# only at specific points in time
adjdiff &lt;- adjusted_curve_diff(adjcif, times=c(0.2, 0.4), conf_int=TRUE)
adjratio &lt;- adjusted_curve_ratio(adjcif, times=c(0.2, 0.4), conf_int=TRUE)
</code></pre>

<hr>
<h2 id='adjusted_curve_test'>
Test if there is a difference between two Confounder-Adjusted Survival Curves or CIFs
</h2><span id='topic+adjusted_curve_test'></span>

<h3>Description</h3>

<p>This function implements a modified version of the Pepe and Flemming (1989) test for the difference between two adjusted survival curves or CIFs. In particular, the Null-Hypothesis is that the integral of the difference of the two curves in a specified time interval is equal to zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjusted_curve_test(adj, to, from=0, conf_level=0.95,
                    interpolation="steps",
                    group_1=NULL, group_2=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adjusted_curve_test_+3A_adj">adj</code></td>
<td>

<p>An <code>adjustedsurv</code> object created using the <code>adjustedsurv</code> function, or a <code>adjustedcif</code> object created using the <code>adjustedcif</code> function, with <code>bootstap=TRUE</code> in the original function call.
</p>
</td></tr>
<tr><td><code id="adjusted_curve_test_+3A_to">to</code></td>
<td>

<p>A number specifying the right side of the time interval of interest. It has to be a value of time that can be read from both of the estimated survival curves or CIFs.
</p>
</td></tr>
<tr><td><code id="adjusted_curve_test_+3A_from">from</code></td>
<td>

<p>A number specifying the left side of the time interval of interest. It has to be a value of time that can be read from both of the estimated survival curves or CIFs. It is set to 0 by default.
</p>
</td></tr>
<tr><td><code id="adjusted_curve_test_+3A_conf_level">conf_level</code></td>
<td>

<p>A number specifying the confidence level of the bootstrap confidence intervals.
</p>
</td></tr>
<tr><td><code id="adjusted_curve_test_+3A_interpolation">interpolation</code></td>
<td>

<p>Either <code>"steps"</code> (default) or <code>"linear"</code>. This parameter controls how interpolation is performed. If this argument is set to <code>"steps"</code>, the curves will be treated as step functions. If it is set to <code>"linear"</code>, the curves wil be treated as if there are straight lines between the point estimates instead. Points that lie between estimated points will be interpolated accordingly. Should usually be kept at <code>"steps"</code>. See Details.
</p>
</td></tr>
<tr><td><code id="adjusted_curve_test_+3A_group_1">group_1</code></td>
<td>

<p>Optional argument to get one specific hypothesis test. This argument takes a single character string specifying one of the levels of the <code>variable</code> used in the original <code>adjustedsurv</code> or <code>adjustedcif</code> function call. This group will be subtracted from. For example if <code>group_1="A"</code> and <code>group_2="B"</code> the difference <code>A - B</code> will be used. If <code>NULL</code>, the order of the factor levels in the original <code>data</code> determines the test order. If not <code>NULL</code>, the <code>group_2</code> argument also needs to be specified. When these arguments are used, all other potential pairwise comparisons are ignored.
</p>
</td></tr>
<tr><td><code id="adjusted_curve_test_+3A_group_2">group_2</code></td>
<td>

<p>Also a single character string specifying one of the levels of <code>variable</code>. This corresponds to the right side of the difference equation. See argument <code>group_2</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>adjustedsurv</code> and <code>adjustedcif</code> functions with <code>bootstrap=TRUE</code> draw <code>n_boot</code> bootstrap samples and estimate the adjusted curves for each one. This function uses those estimates and calculates the integral of the difference between two curves in the interval defined by <code>to</code> and <code>from</code>. If the curves are approximately equal, this quantity should be close to zero. The direct variance calculation of this is quantity is quite involved even in the non-adjusted case and has not been proposed for adjusted survival curves or adjusted CIFs yet. We can however use the distribution of the integrals over all bootstrap samples to approximate the variation. By shifting the bootstrap distribution to be centered around 0 we approximate the distribution of the integral under the Null-Hypothesis. The p-value can then be calculated by taking the proportion of cases where the absolute of the integral observed in the actual curves is smaller or equal to the shifted bootstrap values.
</p>
<p>The associated <code>print</code> and <code>summary</code> methods can be used to obtain a neat data.frame of the most important quantities. We also recommend checking the test assumptions using the <code><a href="#topic+plot.curve_test">plot</a></code> method.
</p>
<p><strong><em>Pairwise Comparisons</em></strong>
</p>
<p>When there are more than two survival curves or CIFs this function automatically performs pairwise comparisons between those. It is recommended to adjust the p-values obtained using this method for multiple testing. See <code>?p.adjust</code> for more information. If only one of the potential pairwise comparisons is of interest, the <code>group_1</code> and <code>group_2</code> arguments can be used to obtain only this specific one.
</p>
<p><strong><em>Multiple Imputation</em></strong>
</p>
<p>When the <code>adjustedsurv</code> or <code>adjustedcif</code> object was fitted using multiply imputed datasets, the tests are performed separately for each dataset. The estimates for the integral of the difference are combined using Rubins Rule. The confidence intervals for this quantity are calculated by pooling the bootstrap standard errors and recalculating the confidence interval using the normal approximation. The p-values are also pooled using a method described in Licht (2010). It is recommended to check if the pooled p-value is in agreement with the pooled confidence interval.
</p>
<p><strong><em>Graphical Displays</em></strong>
</p>
<p>To plot the curves of the differences directly, we recommend using the  <code><a href="#topic+plot_curve_diff">plot_curve_diff</a></code> function. Similar to the main plot functions, it has a lot of arguments to customize the plot. If the main goal is to check the assumptions, we recommend using the associated <code><a href="#topic+plot.curve_test">plot</a></code> method instead.
</p>
<p><strong><em>Computational Details</em></strong>
</p>
<p>Instead of relying on numerical integration, this function uses exact calculations. This is achieved by using either step-function interpolation (<code>interpolation="steps"</code>, the default) or linear interpolation (<code>interpolation="linear"</code>). In the former case, the integral is simply the sum of the area of the squares defined by the step function. In the second case, the integral is simply the sum of the area of the rectangles. Either way, there is no need for approximations. In some situations (for example when using parametric survival models with <code>method="direct"</code>), the curves are not step functions. In this case the <code>interpolation</code> argument should be set to <code>"linear"</code>.
</p>


<h3>Value</h3>

<p>Returns a <code>curve_test</code> object. If there are exactly two treatments this list contains the following object:
</p>
<table>
<tr><td><code>diff_curves</code></td>
<td>
<p>A <code>data.frame</code> containing the difference curves used for calculating the integrals.
</p>
</td></tr>
<tr><td><code>diff_intergals</code></td>
<td>
<p>A numeric vector containing the integrals of the difference for the estimated adjusted survival curves or CIFs.
</p>
</td></tr>
<tr><td><code>observed_diff_curve</code></td>
<td>
<p>The curve of the difference between the two non-bootstrapped adjusted survival curves or CIFs.
</p>
</td></tr>
<tr><td><code>observed_diff_integral</code></td>
<td>
<p>The integral of the curve in <code>observed_diff_curve</code>.
</p>
</td></tr>
<tr><td><code>integral_se</code></td>
<td>
<p>The bootstrap standard error of the difference integral.
</p>
</td></tr>
<tr><td><code>p_value</code></td>
<td>
<p>The p-value for the modified Pepe-Fleming Test. See details.
</p>
</td></tr>
<tr><td><code>n_boot</code></td>
<td>
<p>The number of bootstrap repetitions used.
</p>
</td></tr>
<tr><td><code>kind</code></td>
<td>
<p>Whether survival curves or cumulative incidence functions where used.
</p>
</td></tr>
<tr><td><code>conf_int</code></td>
<td>
<p>The percentile bootstrap confidence interval of the difference between the two curves.
</p>
</td></tr>
<tr><td><code>categorical</code></td>
<td>
<p>Whether there are more than two treatments/groups.
</p>
</td></tr>
<tr><td><code>treat_labs</code></td>
<td>
<p>The labels of all treatments/groups.
</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>The adjustment method used in the original <code>adjustedsurv</code> or <code>adjustedcif</code> object.
</p>
</td></tr>
<tr><td><code>interpolation</code></td>
<td>
<p>The interpolation method specified in the original <code>adjustedsurv</code> or <code>adjustedcif</code> object.
</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The original function call.
</p>
</td></tr>
</table>
<p>If there are more than two treatment groups the object returned is a list of these objects with one list for each pairwise comparison.
</p>
<p>If multiply imputed datasets where used, the object also includes a <code>mids_analyses</code> object, including a <code>curve_test</code> object for each imputed dataset. It also includes a <code>mids_p_values</code> object containing the separately estimated p-values.
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>Margaret Sullivan Pepe and Thomas R. Fleming (1989). &quot;Weighted Kaplan-Meier Statistics: A Class of Distance Tests for Censored Survival Data&quot;. In: Biometrics 45.2, pp. 497-507
</p>
<p>Margaret Sullivan Pepe and Thomas R. Fleming (1991). &quot;Weighted Kaplan-Meier Statistics: Large Sample and Optimality Considerations&quot;. In: Journal of the Royal Statistical Society: Series B 53.2, pp. 341-352
</p>
<p>Nicholas I. Fisher and Peter Hall (1990). &quot;On Bootstrap Hypothesis Testing&quot;. In: Australian Journal of Statistics 32.2, pp. 177-190
</p>
<p>Florent Le Borgne, Bruno Giraudeau, Anne Héléne Querard, Magali Giral, and Yohann Foucher (2016). &quot;Comparisons of the Performance of Different Statistical Tests for Time-To-Event Analysis with Confounding Factors: Practical Illustrations in Kidney Transplantation&quot;. In: Statistics in Medicine 35, pp. 1103-1116
</p>
<p>Christine Licht (2010). &quot;New Methods for Generating Significance Levels from Multiply-Imputed Data&quot;. PhD thesis. Otto-Friedrich-Universität Bamberg, Fakultät Sozial- und Wirtschaftswissenschaften
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.curve_test">plot.curve_test</a></code>, <code><a href="#topic+adjustedsurv">adjustedsurv</a></code>, <code><a href="#topic+adjustedcif">adjustedcif</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)
library(cmprsk)

#### Simple Survival Case with adjusted survival curves ####

# simulate some data as example
set.seed(42)
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a cox-regression for the outcome
cox_mod &lt;- coxph(Surv(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group,
                 data=sim_dat, x=TRUE)

# use it to estimate adjusted survival curves with bootstrapping
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="direct",
                        outcome_model=cox_mod,
                        conf_int=FALSE,
                        bootstrap=TRUE,
                        n_boot=10) # n_boot should be much higher in reality

# test the equality of both curves in the interval 0 to 1
adjtest &lt;- adjusted_curve_test(adjsurv, from=0, to=1)
print(adjtest)

#### Competing Risks Case with adjusted CIFs ####
library(riskRegression)
library(prodlim)

# simulate some data as example
sim_dat &lt;- sim_confounded_crisk(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a cause-specific cox-regression for the outcome
csc_mod &lt;- CSC(Hist(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group,
               data=sim_dat)

# use it to calculate adjusted CIFs for cause = 1 with bootstrapping
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      method="direct",
                      outcome_model=csc_mod,
                      conf_int=FALSE,
                      bootstrap=TRUE,
                      n_boot=10,
                      cause=1)

# test the equality of both curves in the interval 0 to 1
adjtest &lt;- adjusted_curve_test(adjcif, from=0, to=1)
print(adjtest)
</code></pre>

<hr>
<h2 id='adjusted_rmst'>
Estimate Confounder-Adjusted Restricted Mean Survival Times
</h2><span id='topic+adjusted_rmst'></span>

<h3>Description</h3>

<p>This function can be utilized to estimate the confounder-adjusted restricted mean survival time, given previously estimated adjusted survival curves.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjusted_rmst(adjsurv, to, from=0, conf_int=FALSE,
              conf_level=0.95, interpolation="steps",
              difference=FALSE, ratio=FALSE,
              group_1=NULL, group_2=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adjusted_rmst_+3A_adjsurv">adjsurv</code></td>
<td>

<p>An <code>adjustedsurv</code> object created using the <code>adjustedsurv</code> function.
</p>
</td></tr>
<tr><td><code id="adjusted_rmst_+3A_from">from</code></td>
<td>

<p>A single number specifying the left side of the time interval of interest. See details. Usually this should be kept at 0 (default) to estimate the standard RMST. Should only be changed if there are good reasons for it.
</p>
</td></tr>
<tr><td><code id="adjusted_rmst_+3A_to">to</code></td>
<td>

<p>One or multiple numbers specifying the right side of the time interval of interest. If a vector of numbers is supplied, the adjusted RMST will be estimated for each value of <code>to</code>. See details.
</p>
</td></tr>
<tr><td><code id="adjusted_rmst_+3A_conf_int">conf_int</code></td>
<td>

<p>Whether bootstrap estimates should be used to estimate the standard errors and confidence intervals of the RMST estimates. Can only be used if <code>bootstrap=TRUE</code> was used in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> call.
</p>
</td></tr>
<tr><td><code id="adjusted_rmst_+3A_conf_level">conf_level</code></td>
<td>

<p>A number specifying the confidence level of the bootstrap confidence intervals.
</p>
</td></tr>
<tr><td><code id="adjusted_rmst_+3A_interpolation">interpolation</code></td>
<td>

<p>Either <code>"steps"</code> (default) or <code>"linear"</code>. This parameter controls how interpolation is performed. If this argument is set to <code>"steps"</code>, the curves will be treated as step functions. If it is set to <code>"linear"</code>, the curves wil be treated as if there are straight lines between the point estimates instead. Points that lie between estimated points will be interpolated accordingly. Should usually be kept at <code>"steps"</code>. See Details.
</p>
</td></tr>
<tr><td><code id="adjusted_rmst_+3A_difference">difference</code></td>
<td>

<p>Whether to estimate the difference between two adjusted restricted mean survival times instead. When <code>conf_int=TRUE</code> is also specified, this function will also return the standard error of the difference, the associated confidence interval and a p-value. The p-value is the result of a one-sample t-test where the null-hypothesis is that the difference is equal to 0. To specify which difference should be calculated, the <code>group_1</code> and <code>group_2</code> arguments can be used. By default, the difference between the first and second level in <code>variable</code> is computed.
</p>
</td></tr>
<tr><td><code id="adjusted_rmst_+3A_ratio">ratio</code></td>
<td>

<p>Whether to estimate the ratio between two adjusted restricted mean survival times instead. When <code>conf_int=TRUE</code> is also specified, this function will also return the associated confidence interval and a p-value. The p-value is the result of a one-sample t-test where the null-hypothesis is that the ratio is equal to 1. To specify which ratio should be calculated, the <code>group_1</code> and <code>group_2</code> arguments can be used. By default, the ratio between the first and second level in <code>variable</code> is computed. The confidence interval and test-statistic is estimated using the Fieller method.
</p>
</td></tr>
<tr><td><code id="adjusted_rmst_+3A_group_1">group_1</code></td>
<td>

<p>Optional argument to get a specific difference or ratio. This argument takes a single character string specifying one of the levels of the <code>variable</code> used in the original <code>adjustedsurv</code> or <code>adjustedcif</code> function call. This group will be subtracted from. For example if <code>group_1="A"</code> and <code>group_2="B"</code> and <code>difference=TRUE</code> the difference <code>A - B</code> will be used. If <code>NULL</code>, the order of the factor levels in the original <code>data</code> determines the order. Ignored if <code>difference=FALSE</code> and <code>ratio=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="adjusted_rmst_+3A_group_2">group_2</code></td>
<td>

<p>Also a single character string specifying one of the levels of <code>variable</code>. This corresponds to the right side of the difference/ratio equation. See argument <code>group_2</code>. Ignored if <code>difference=FALSE</code> and <code>ratio=FALSE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The adjusted restricted mean survival times (RMST) are estimated by integrating the estimated adjusted survival curves in a specified interval. Let <code class="reqn">Z</code> be the grouping variable (corresponding to the <code>variable</code> argument in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function) with possible levels <code class="reqn">Z \in \{0, 1, 2, ..., k\}</code>. <code class="reqn">T</code> is defined as the time and <code class="reqn">\hat{S}_z(t)</code> denotes the estimated counterfactual survival function. The RMST is then defined as:
</p>
<p style="text-align: center;"><code class="reqn">RMST_{z}(to) = \int_{from=0}^{to} \hat{S}_z(t)dt</code>
</p>

<p>It can be interpreted as the mean survival time of individuals in group <code class="reqn">Z = z</code> in the interval [<code>from</code>, <code>to</code>]. Note however that simply subtracting the estimates from each other does not give a correct estimate of the area between the survival curves if the respective curves cross at some point. The <code><a href="#topic+adjusted_curve_test">adjusted_curve_test</a></code> function can be used to calculate the actual area between the curves instead. See <code>?adjusted_curve_test</code> for more information.
</p>
<p><strong><em>Confidence Intervals</em></strong>
</p>
<p>If the <code>adjsurv</code> object was created with <code>bootstrap=TRUE</code> in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function, bootstrap confidence intervals and standard errors for the RMSTs can be approximated by setting <code>conf_int</code> to <code>TRUE</code>. If bootstrap samples occur where the survival function is not estimated up to <code>to</code>, the bootstrap sample is discarded and not used in further calculations. Approximate variance calculations not relying on the bootstrap estimates are currently not implemented. When using <code>difference=TRUE</code> the standard error of the difference between the two RMST values is approximated by <code class="reqn">SE_{group_1 - group_2} = \sqrt{SE_{group_1}^2 + SE_{group_2}^2}</code>. When using <code>ratio=TRUE</code> the confidence intervals are calculated using the approximate formula given by Fieller (1954), assuming that the values are independent.
</p>
<p><strong><em>Multiple Imputation</em></strong>
</p>
<p>If multiple imputation was used when creating the <code>adjsurv</code> object, the analysis is carried out on all multiply imputed datasets and pooled using Rubins Rule. When bootstrapping was carried out as well, the pooled standard error over all imputed datasets is used in combination with the normal approximation to re-calculate the bootstrap confidence intervals.
</p>
<p><strong><em>Competing Risks</em></strong>
</p>
<p>This function cannot be used with <code>adjustedcif</code> objects, because the survival probability cannot be estimated in an unbiased way when competing risks are present. However, a very similar quantity, the <em>adjusted restricted mean time lost</em>, can be calculated using the <code><a href="#topic+adjusted_rmtl">adjusted_rmtl</a></code> function.
</p>
<p><strong><em>Graphical Displays</em></strong>
</p>
<p>A plot of the RMST over time (with changing values for the <code>to</code> argument) can be produced using the <code><a href="#topic+plot_rmst_curve">plot_rmst_curve</a></code> function.
</p>
<p><strong><em>Computational Details</em></strong>
</p>
<p>Instead of relying on numerical integration, this function uses exact calculations. This is achieved by using either step-function interpolation (<code>interpolation="steps"</code>, the default) or linear interpolation (<code>interpolation="linear"</code>). In the former case, the integral is simply the sum of the area of the squares defined by the step function. In the second case, the integral is simply the sum of the area of the rectangles. Either way, there is no need for approximations. In some situations (for example when using parametric survival models with <code>method="direct"</code>), the curves are not step functions. In this case the <code>interpolation</code> argument should be set to <code>"linear"</code>.
</p>


<h3>Value</h3>

<p>Returns a <code>data.frame</code> containing the columns <code>to</code> (the supplied <code>to</code> values), <code>group</code> (groups in <code>variable</code>) and <code>rmst</code> (the estimated restricted mean survival time).
</p>
<p>If <code>conf_int=TRUE</code> was used it additionally contains the columns <code>se</code> (the standard error of the restricted mean survival time), <code>ci_lower</code> (lower limit of the confidence interval), <code>ci_upper</code> (upper limit of the confidence interval) and <code>n_boot</code> (the actual number of bootstrap estimates used).
</p>
<p>If <code>difference=TRUE</code> was used, it instead returns a <code>data.frame</code> that contains the columns <code>to</code>, <code>diff</code> (the difference between the RMST values), <code>se</code> (the standard error of the difference), <code>ci_lower</code> (lower limit of the confidence interval), <code>ci_upper</code> (upper limit of the confidence interval) and <code>p_value</code> (the p-value of the one-sample t-test). The same results are presented when using <code>ratio=TRUE</code>, except that the <code>diff</code> column is named <code>ratio</code> and that there is no <code>se</code> column.
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>Sarah C. Conner, Lisa M. Sullivan, Emelia J. Benjamin, Michael P. LaValley, Sandro Galea, and Ludovic Trinquart (2019). &quot;Adjusted Restricted Mean Survival Times in Observational Studies&quot;. In: Statistics in Medicine 38, pp. 3832-3860
</p>
<p>Patrick Royston and Mahesh K. B. Parmar (2013). &quot;Restricted Mean Survival Time: An Alternative to the Hazard Ratio for the Design and Analysis of Randomized Trials with a Time-To-Event Outcome&quot;. In: BMC Medical Research Methodology 13.152
</p>
<p>Edgar C. Fieller (1954). &quot;Some Problems in Interval Estimation&quot;. In: Journal of the Royal Statistical Society, Series B 16.2, pp. 175-185
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjustedsurv">adjustedsurv</a></code>, <code><a href="#topic+plot_rmst_curve">plot_rmst_curve</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=30, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a cox-regression for the outcome
cox_mod &lt;- coxph(Surv(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group,
                 data=sim_dat, x=TRUE)

# use it to calculate adjusted survival curves with bootstrapping
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="direct",
                        outcome_model=cox_mod,
                        conf_int=FALSE,
                        bootstrap=TRUE,
                        n_boot=10) # n_boot should be much higher in reality

# calculate adjusted restricted mean survival times from 0 to 0.5
adjrmst &lt;- adjusted_rmst(adjsurv, from=0, to=0.5, conf_int=FALSE)

# calculate adjusted restricted mean survival times from 0 to 0.5
# and from 0 to 0.7 simultaneously
adjrmst &lt;- adjusted_rmst(adjsurv, from=0, to=c(0.5, 0.7), conf_int=FALSE)

# calculate adjusted restricted mean survival times from 0 to 0.5,
# including standard errors and confidence intervals
adjrmst &lt;- adjusted_rmst(adjsurv, from=0, to=0.5, conf_int=TRUE,
                         conf_level=0.95)

# calculate difference between adjusted restricted mean survival times
# from 0 to 0.5 in the two groups
adjrmst &lt;- adjusted_rmst(adjsurv, from=0, to=0.5, conf_int=FALSE,
                         difference=TRUE)

# calculate ratio between adjusted restricted mean survival times
# from 0 to 0.5 in the two groups
adjrmst &lt;- adjusted_rmst(adjsurv, from=0, to=0.5, conf_int=FALSE,
                         ratio=TRUE)
</code></pre>

<hr>
<h2 id='adjusted_rmtl'>
Estimate Confounder-Adjusted Restricted Mean Time Lost
</h2><span id='topic+adjusted_rmtl'></span>

<h3>Description</h3>

<p>This function can be utilized to estimate the confounder-adjusted restricted mean time lost (RMTL), possibly due to a specific cause, given previously estimated adjusted survival curves / CIFs created using the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> or <code><a href="#topic+adjustedcif">adjustedcif</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjusted_rmtl(adj, to, from=0, conf_int=FALSE,
              conf_level=0.95, interpolation="steps",
              difference=FALSE, ratio=FALSE,
              group_1=NULL, group_2=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adjusted_rmtl_+3A_adj">adj</code></td>
<td>

<p>An <code>adjustedsurv</code> object created using the <code>adjustedsurv</code> function or a <code>adjustedcif</code> object created using the <code>adjustedcif</code> function.
</p>
</td></tr>
<tr><td><code id="adjusted_rmtl_+3A_from">from</code></td>
<td>

<p>A single number specifying the left side of the time interval of interest. See details. Usually this should be kept at 0 (default) to estimate the standard RMTL. Should only be changed if there are good reasons for it.
</p>
</td></tr>
<tr><td><code id="adjusted_rmtl_+3A_to">to</code></td>
<td>

<p>One or more numbers specifying the right side of the time interval of interest. If a vector of numbers is supplied, the adjusted RMTL will be estimated for each value of <code>to</code>. See details.
</p>
</td></tr>
<tr><td><code id="adjusted_rmtl_+3A_conf_int">conf_int</code></td>
<td>

<p>Whether bootstrap estimates should be used to estimate the standard errors and confidence intervals of the RMST estimates. Can only be used if <code>bootstrap=TRUE</code> was used in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> or <code><a href="#topic+adjustedcif">adjustedcif</a></code> call.
</p>
</td></tr>
<tr><td><code id="adjusted_rmtl_+3A_conf_level">conf_level</code></td>
<td>

<p>A number specifying the confidence level of the bootstrap confidence intervals.
</p>
</td></tr>
<tr><td><code id="adjusted_rmtl_+3A_interpolation">interpolation</code></td>
<td>

<p>Either <code>"steps"</code> (default) or <code>"linear"</code>. This parameter controls how interpolation is performed. If this argument is set to <code>"steps"</code>, the curves will be treated as step functions. If it is set to <code>"linear"</code>, the curves wil be treated as if there are straight lines between the point estimates instead. Points that lie between estimated points will be interpolated accordingly. Should usually be kept at <code>"steps"</code>. See Details.
</p>
</td></tr>
<tr><td><code id="adjusted_rmtl_+3A_difference">difference</code></td>
<td>

<p>Whether to estimate the difference between two adjusted RMTLs instead. When <code>conf_int=TRUE</code> is also specified, this function will also return the standard error of the difference, the associated confidence interval and a p-value. The p-value is the result of a one-sample t-test where the null-hypothesis is that the difference is equal to 0. To specify which difference should be estimated, the <code>group_1</code> and <code>group_2</code> arguments can be used. By default, the difference between the first and second level in <code>variable</code> is computed.
</p>
</td></tr>
<tr><td><code id="adjusted_rmtl_+3A_ratio">ratio</code></td>
<td>

<p>Whether to estimate the ratio between two RMTLs instead. When <code>conf_int=TRUE</code> is also specified, this function will also return the associated confidence interval and a p-value. The p-value is the result of a one-sample t-test where the null-hypothesis is that the ratio is equal to 1. To specify which ratio should be calculated, the <code>group_1</code> and <code>group_2</code> arguments can be used. By default, the ratio between the first and second level in <code>variable</code> is computed. The confidence interval and test-statistic is estimated using the Fieller method.
</p>
</td></tr>
<tr><td><code id="adjusted_rmtl_+3A_group_1">group_1</code></td>
<td>

<p>Optional argument to get a specific difference or ratio. This argument takes a single character string specifying one of the levels of the <code>variable</code> used in the original <code>adjustedsurv</code> or <code>adjustedcif</code> function call. This group will be subtracted from. For example if <code>group_1="A"</code> and <code>group_2="B"</code> and <code>difference=TRUE</code> the difference <code>A - B</code> will be used. If <code>NULL</code>, the order of the factor levels in the original <code>data</code> determines the order. Ignored if <code>difference=FALSE</code> and <code>ratio=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="adjusted_rmtl_+3A_group_2">group_2</code></td>
<td>

<p>Also a single character string specifying one of the levels of <code>variable</code>. This corresponds to the right side of the difference / ratio equation. See argument <code>group_2</code>. Ignored if <code>difference=FALSE</code> and <code>ratio=FALSE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cause-specific adjusted restricted mean time lost (RMTL) is calculated by integrating the estimated adjusted cause-specific CIF in a specified interval. Let <code class="reqn">Z</code> be the grouping variable (corresponding to the <code>variable</code> argument in the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function) with possible levels <code class="reqn">Z \in \{0, 1, 2, ..., k\}</code>. <code class="reqn">T</code> is defined as the time and <code class="reqn">\hat{F}_z^d(t)</code> denotes the estimated counterfactual CIF for <code>cause</code> <code class="reqn">d</code>. The RMTL is then defined as:
</p>
<p style="text-align: center;"><code class="reqn">RMTL_{z}^d(to) = \int_{from=0}^{to} \hat{F}_z^d(t)dt</code>
</p>

<p>It can be interpreted as the mean time it takes an individual to succumb to the event of interest in group <code class="reqn">Z = z</code> in the interval [0, <code>to</code>]. . More information on the method itself can be found in the references. Note however that simply subtracting the estimates from each other does not give a correct estimate of the area between the CIFs if the respective curves cross at some point. The <code><a href="#topic+adjusted_curve_test">adjusted_curve_test</a></code> function can be used to calculate the actual area between the curves instead. See <code>?adjusted_curve_test</code> for more information.
</p>
<p>If an <code>adjustedsurv</code> object is supplied in the <code>adj</code> argument, the CIF is calculated from the adjusted survival curves using the simple transformation: <code class="reqn">\hat{F}_{z}(t) = 1 - \hat{S}_z(t)</code>. All further calculations are identical.
</p>
<p><strong><em>Confidence Intervals</em></strong>
</p>
<p>If the <code>adj</code> object was created with <code>bootstrap=TRUE</code> in the corresponding function, bootstrap confidence intervals and standard errors for the RMTLs can be approximated by setting <code>conf_int</code> to <code>TRUE</code>. If bootstrap samples occur where the CIF is not estimated up to <code>to</code>, the bootstrap sample is discarded and not used in further calculations. Approximate variance calculations not relying on the bootstrap estimates are currently not implemented. When using <code>difference=TRUE</code> the standard error of the difference between the two RMST values is approximated by <code class="reqn">SE_{group_1 - group_2} = \sqrt{SE_{group_1}^2 + SE_{group_2}^2}</code>. When using <code>ratio=TRUE</code> the confidence intervals are calculated using the approximate formula given by Fieller (1954), assuming that the values are independent.
</p>
<p><strong><em>Multiple Imputation</em></strong>
</p>
<p>If multiple imputation was used when creating the <code>adj</code> object, the analysis is carried out on all multiply imputed datasets and pooled using Rubins Rule. When bootstrapping was carried out as well, the pooled standard error over all imputed datasets is used in combination with the normal approximation to re-calculate the bootstrap confidence intervals.
</p>
<p><strong><em>Graphical Displays</em></strong>
</p>
<p>A plot of the RMTL over time (with changing values for the <code>to</code> argument) can be produced using the <code><a href="#topic+plot_rmtl_curve">plot_rmtl_curve</a></code> function.
</p>
<p><strong><em>Computational Details</em></strong>
</p>
<p>Instead of relying on numerical integration, this function uses exact calculations. This is achieved by using either step-function interpolation (<code>interpolation="steps"</code>, the default) or linear interpolation (<code>interpolation="linear"</code>). In the former case, the integral is simply the sum of the area of the squares defined by the step function. In the second case, the integral is simply the sum of the area of the rectangles. Either way, there is no need for approximations. In some situations (for example when using parametric models with <code>method="direct"</code>), the curves are not step functions. In this case the <code>interpolation</code> argument should be set to <code>"linear"</code>.
</p>


<h3>Value</h3>

<p>Returns a <code>data.frame</code> containing the columns <code>group</code> (groups in <code>variable</code>) and <code>rmtl</code> (the estimated restricted mean time lost).
</p>
<p>If <code>conf_int=TRUE</code> was used it additionally contains the columns <code>to</code> (the supplied <code>to</code> values), <code>se</code> (the standard error of the restricted mean time lost), <code>ci_lower</code> (lower limit of the confidence interval), <code>ci_upper</code> (upper limit of the confidence interval) and <code>n_boot</code> (the actual number of bootstrap estimates used).
</p>
<p>If <code>difference=TRUE</code> was used, it instead returns a <code>data.frame</code> that contains the columns <code>to</code>, <code>diff</code> (the difference between the RMTL values), <code>se</code> (the standard error of the difference), <code>ci_lower</code> (lower limit of the confidence interval), <code>ci_upper</code> (upper limit of the confidence interval) and <code>p_value</code> (the p-value of the one-sample t-test). The same results are presented when using <code>ratio=TRUE</code>, except that the <code>diff</code> column is named <code>ratio</code> and that there is no <code>se</code> column.
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>Sarah C. Conner and Ludovic Trunquart (2021). &quot;Estimation and Modeling of the Restricted Mean Time Lost in the Presence of Competing Risks&quot;. In: Statistics in Medicine
</p>
<p>Edgar C. Fieller (1954). &quot;Some Problems in Interval Estimation&quot;. In: Journal of the Royal Statistical Society, Series B 16.2, pp. 175-185
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjustedcif">adjustedcif</a></code>, <code><a href="#topic+adjustedsurv">adjustedsurv</a></code>, <code><a href="#topic+plot_rmtl_curve">plot_rmtl_curve</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)

###### when using single-event survival data

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a cox-regression for the outcome
cox_mod &lt;- coxph(Surv(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group,
                 data=sim_dat, x=TRUE)

# use it to calculate adjusted survival curves with bootstrapping
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="direct",
                        outcome_model=cox_mod,
                        conf_int=FALSE,
                        bootstrap=TRUE,
                        n_boot=10) # n_boot should be much higher in reality

# calculate adjusted restricted mean survival times from 0 to 1
adjrmst &lt;- adjusted_rmst(adjsurv, from=0, to=1, conf_int=FALSE)

# calculate adjusted restricted mean survival times from 0 to 0.5
# and from 0 to 1 simulatenously
adjrmst &lt;- adjusted_rmst(adjsurv, from=0, to=c(0.5, 1), conf_int=FALSE)

# calculate adjusted restricted mean time lost estimates from 0 to 1,
# including standard errors and confidence intervals
adjrmst &lt;- adjusted_rmst(adjsurv, from=0, to=1, conf_int=TRUE,
                         conf_level=0.95)

# calculate difference in adjusted restricted mean survival times from 0 to 1
adjrmst &lt;- adjusted_rmst(adjsurv, from=0, to=1, conf_int=FALSE,
                         difference=TRUE)

###### when using data with competing-risks

library(riskRegression)
library(prodlim)

# simulate some data as example
set.seed(42)
sim_dat &lt;- sim_confounded_crisk(n=50)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a cause-specific cox-regression model for the outcome
csc_mod &lt;- CSC(Hist(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group,
               data=sim_dat)

# calculate confounder-adjusted cause-specific CIFs for cause = 1
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      method="direct",
                      outcome_model=csc_mod,
                      conf_int=FALSE,
                      bootstrap=TRUE,
                      n_boot=10,
                      cause=1)

# calculate adjusted restricted mean time lost estimates from 0 to 1
# including standard errors and confidence intervals
adjrmtl &lt;- adjusted_rmtl(adjcif, from=0, to=1, conf_int=TRUE)

# calculate ratio of adjusted restricted mean time lost estimates from 0 to 1
# including confidence interval and p-value
adjrmtl &lt;- adjusted_rmtl(adjcif, from=0, to=1, conf_int=TRUE, ratio=TRUE)
</code></pre>

<hr>
<h2 id='adjusted_surv_quantile'>
Estimate Confounder-Adjusted Survival Time Quantiles
</h2><span id='topic+adjusted_surv_quantile'></span>

<h3>Description</h3>

<p>This function can be utilized to estimate confounder-adjusted survival time quantiles, including the median survival time, given previously estimated adjusted survival curves.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjusted_surv_quantile(adjsurv, p=0.5, conf_int=FALSE,
                       use_boot=FALSE, interpolation="steps")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adjusted_surv_quantile_+3A_adjsurv">adjsurv</code></td>
<td>

<p>An <code>adjustedsurv</code> object created using the <code>adjustedsurv</code> function.
</p>
</td></tr>
<tr><td><code id="adjusted_surv_quantile_+3A_p">p</code></td>
<td>

<p>The quantile of interest. To calculate the median survival time, set this parameter to 0.5 (default). Multiple values in form of a numeric vector are allowed.
</p>
</td></tr>
<tr><td><code id="adjusted_surv_quantile_+3A_conf_int">conf_int</code></td>
<td>

<p>Whether to calculate confidence intervals or not. Those are calculated in the same way as the quantiles but using the confidence limit curves. This requires either that <code>conf_int=TRUE</code> or <code>bootstrap=TRUE</code> was used in the original <code>adjustedsurv</code> function call. Since this directly uses the previously estimated intervals, the same confidence level used in the original <code>adjustedsurv</code> call is used here.
</p>
</td></tr>
<tr><td><code id="adjusted_surv_quantile_+3A_use_boot">use_boot</code></td>
<td>

<p>Whether to use the bootstrap confidence interval estimates of the survival curves to estimate the confidence intervals of the survival time quantiles or not. Can only be used when <code>bootstrap=TRUE</code> was used in the original <code>adjustedsurv</code> function call. Ignored if <code>conf_int=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="adjusted_surv_quantile_+3A_interpolation">interpolation</code></td>
<td>

<p>Either <code>"steps"</code> (default) or <code>"linear"</code>. This parameter controls how interpolation is performed. If this argument is set to <code>"steps"</code>, the curves will be treated as step functions. If it is set to <code>"linear"</code>, the curves wil be treated as if there are straight lines between the point estimates instead. Points that lie between estimated points will be interpolated accordingly.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The median survival time is simply the time when half the patients are expected to be alive. The chance of surviving beyond that time is 50 percent. In general, any quantile can be calculated this way. Those can be read directly from the respective survival curve by drawing a straight line from the desired quantile <code>p</code> on the Y-Axis and reading the X-Axis value where this line intersects with the survival curve. The adjusted survival time quantile for group <code class="reqn">z</code> (corresponding to the <code>variable</code> argument in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function) is formally defined as:
</p>
<p style="text-align: center;"><code class="reqn">\hat{Q}_z(p) = min\left(t | \hat{S}_z(t) \leq p\right)</code>
</p>

<p>where <code class="reqn">\hat{S}_z(t)</code> is the estimated counterfactual survival function for <code class="reqn">z</code>.
</p>
<p>If the survival probability never drops below <code>p</code>, the survival time quantile cannot be calculated. This also applies to the confidence interval estimation. This function calculates this quantity automatically. When multiple imputation was used in the original function call, the survival time quantiles are read off the final pooled survival curves directly.
</p>


<h3>Value</h3>

<p>Returns a <code>data.frame</code> containing the columns <code>p</code> (the quantiles from the original function call), <code>group</code> (groups in <code>variable</code>) and <code>q_surv</code> (the survival time quantile).
</p>
<p>If <code>conf_int=TRUE</code> was used it also includes the confidence limits in the <code>ci_lower</code> and <code>ci_upper</code> columns.
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>Omer Ben-Aharon, Racheli Magnezi, Moshe Leshno, and Daniel A. Goldstein (2019). &quot;Median Survival or Mean Survival: Which Measure is the Most Appropriate for Patients, Physicians, and Policymakers?&quot; In: The Oncologist 24, pp. 1469-1478
</p>
<p>Zhongxue Chen and Guoyi Zhang (2016). &quot;Comparing Survival Curves based on Medians&quot;. In: BMC Medical Research Methodology 16.33
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjustedsurv">adjustedsurv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)

# simulate some data as example
set.seed(42)
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a cox-regression for the outcome
cox_mod &lt;- coxph(Surv(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group,
                 data=sim_dat, x=TRUE)

# use it to calculate adjusted survival curves with bootstrapping
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="direct",
                        outcome_model=cox_mod,
                        conf_int=TRUE,
                        bootstrap=FALSE)

# calculate adjusted median survival times
adjusted_surv_quantile(adjsurv)

# calculate other quantiles + confidence intervals
adjusted_surv_quantile(adjsurv, conf_int=TRUE, p=c(0.2, 0.4))
</code></pre>

<hr>
<h2 id='adjustedcif'>
Estimate Cause-Specific Confounder-Adjusted Cumulative Incidence Functions
</h2><span id='topic+adjustedcif'></span>

<h3>Description</h3>

<p>This is one of two main functions of this R-Package. It allows the user to estimate cause-specific confounder-adjusted cumulative incidence functions in the presence of competing events using a variety of different methods. Some of these methods require additional packages to be installed and, depending on the specified method, there might be additional required arguments in the function call. More information is available on the documentation page of the respective <code>cif_method</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjustedcif(data, variable, ev_time, event, cause,
            method, conf_int=FALSE, conf_level=0.95,
            times=NULL, bootstrap=FALSE, n_boot=500,
            n_cores=1, na.action=options()$na.action,
            clean_data=TRUE, iso_reg=FALSE,
            force_bounds=FALSE, mi_extrapolation=FALSE,
            ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adjustedcif_+3A_data">data</code></td>
<td>

<p>A <code>data.frame</code> object containing the needed time-to-event data in standard format. Can also be a <code>mids</code> object created with the <span class="pkg">mice</span> package. See details for how this works.
</p>
</td></tr>
<tr><td><code id="adjustedcif_+3A_variable">variable</code></td>
<td>

<p>A character string specifying the variable by which the cumulative incidence functions should be grouped. Must be a valid column name of <code>data</code>. The variable specified should needs to be a factor variable.
</p>
</td></tr>
<tr><td><code id="adjustedcif_+3A_ev_time">ev_time</code></td>
<td>

<p>A character string specifying the variable indicating the time-to-event or time-to-censoring. Must be a valid column name of <code>data</code>.
</p>
</td></tr>
<tr><td><code id="adjustedcif_+3A_event">event</code></td>
<td>

<p>A character string specifying the numeric event indicator. The censoring indicator should be coded as 0 and all other events of interest as 1, 2, etc. Must be a valid column name of <code>data</code>.
</p>
</td></tr>
<tr><td><code id="adjustedcif_+3A_cause">cause</code></td>
<td>

<p>The cause of interest for which the cumulative incidence functions should be estimated. Should be a number that appears in the <code>event</code> column of <code>data</code>.
</p>
</td></tr>
<tr><td><code id="adjustedcif_+3A_method">method</code></td>
<td>

<p>A character string specifying the adjustment method to use. Case sensitive. See details.
</p>
</td></tr>
<tr><td><code id="adjustedcif_+3A_conf_int">conf_int</code></td>
<td>

<p>A logical variable, indicating whether the asymptotic variances and confidence intervals of the cumulative incidence should be estimated. Not available for all methods. More information can be found in the documentation of each method. For an alternative way to get confidence intervals, see the <code>bootstrap</code> argument.
</p>
</td></tr>
<tr><td><code id="adjustedcif_+3A_conf_level">conf_level</code></td>
<td>

<p>A number specifying the confidence level of asymptotic and/or bootstrap confidence intervals.
</p>
</td></tr>
<tr><td><code id="adjustedcif_+3A_times">times</code></td>
<td>

<p>A numeric vector of time points at which the cumulative incidences should be estimated or <code>NULL</code>. If <code>NULL</code> the cumulative incidence is estimated at all points in time at which any event occurred in the pooled sample.
</p>
</td></tr>
<tr><td><code id="adjustedcif_+3A_bootstrap">bootstrap</code></td>
<td>

<p>A logical variable indicating whether bootstrapping should be performed or not. In bootstrapping, a number of simple random samples with replacement of size <code>nrow(data)</code> are drawn from <code>data</code>. For each sample the calculations are repeated and used to estimate standard errors and confidence intervals. This can be used to obtain confidence intervals when asymptotic variance calculations are not available.
</p>
</td></tr>
<tr><td><code id="adjustedcif_+3A_n_boot">n_boot</code></td>
<td>

<p>Number of bootstrap replications to perform. Ignored if <code>bootstrap</code> is <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="adjustedcif_+3A_n_cores">n_cores</code></td>
<td>

<p>The number of cores to use when calculating bootstrap estimates. Ignored if <code>bootstrap=FALSE</code>. Is set to 1 by default, resulting in single threaded processing. Internally uses the <span class="pkg">doParallel</span> package if <code>n_cores &gt; 1</code>. In that case it also uses the <span class="pkg">doRNG</span> package to make the results replicable. See <code>?doRNG</code> and <code>?doParallel</code> for more details. Using multiple cores will speed up the calculation considerably in most cases.
</p>
</td></tr>
<tr><td><code id="adjustedcif_+3A_na.action">na.action</code></td>
<td>

<p>How missing values should be handled. Can be one of: na.fail, na.omit, na.pass or na.exclude. Also accepts strings of the function names. See <code>?na.action</code> for more details. By default it uses the na.action which is set in the global options by the respective user.
</p>
</td></tr>
<tr><td><code id="adjustedcif_+3A_clean_data">clean_data</code></td>
<td>

<p>If <code>TRUE</code> all columns which are not needed for the estimation are removed from <code>data</code> before any further calculations are performed. This ensures that calls to <code>na.omit</code> (see argument <code>na.action</code>) do not remove rows which are fully observed in respect to relevant columns due to missing values in irrelevant columns. Set to <code>FALSE</code> to skip this step. Usually this argument can be ignored. When using non-standard outcome models however it should be set to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="adjustedcif_+3A_iso_reg">iso_reg</code></td>
<td>

<p>Either <code>TRUE</code> or <code>FALSE</code> (default), controlling whether isotonic regression is performed on the resulting failure probability estimates. This can be used to ensure that the CIFs are non-decreasing. Since only a few methods may exhibit this problem, this argument is only relevant for some methods (see method specific documentation).
</p>
</td></tr>
<tr><td><code id="adjustedcif_+3A_force_bounds">force_bounds</code></td>
<td>

<p>Either <code>TRUE</code> or <code>FALSE</code> (default), controlling whether the resulting failure probability estimates should be forced to lie between 0 and 1. If <code>TRUE</code> and there are values higher than 1, they are simply set to 1. Values lower than 0 are similarly set to 0. Since only a few methods may exhibit this problem, this argument is only relevant for some methods (see method specific documentation).
</p>
</td></tr>
<tr><td><code id="adjustedcif_+3A_mi_extrapolation">mi_extrapolation</code></td>
<td>

<p>Whether to allow extrapolation due to imputed survival times or not. This argument is only relevant when using multiply imputed <code>data</code> with missing covariates in <code>variable</code>, <code>ev_time</code> or <code>event</code>. Depending on the algorithm used to obtain the imputed datasets, it may be possible that one or more imputed datasets contain survival times in a group that are larger than the maximum observed survival time in that group. This may lead to unwanted extrapolation (e.g. the survival curves extending further than they should). By keeping this argument at <code>FALSE</code>, these times are removed from the output. If set to <code>TRUE</code>, all available estimates will be used.
</p>
</td></tr>
<tr><td><code id="adjustedcif_+3A_...">...</code></td>
<td>

<p>Further arguments passed to the respective <code>cif_method</code>. For example when using <code>method="direct"</code> all further arguments are passed to the <code>cif_direct</code> function. See details.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The primary purpose of the <code>adjustedcif</code> function is to provide a convenient way to estimate confounder-adjusted cumulative incidence functions using any of the methods provided in the literature. A <code><a href="#topic+plot.adjustedcif">plot</a></code> method is provided to graphically display the estimated cumulative incidence functions as well. Currently the following methods can be used:
</p>

<ul>
<li><p>&quot;<a href="#topic+cif_direct">direct</a>&quot;: Direct Standardization based on a model describing the outcome mechanism (<code><a href="riskRegression.html#topic+CSC">CSC</a></code>, <code><a href="riskRegression.html#topic+FGR">FGR</a></code>, ..).
</p>
</li>
<li><p>&quot;<a href="#topic+cif_direct_pseudo">direct_pseudo</a>&quot;: Direct Standardization based on Pseudo-Values.
</p>
</li>
<li><p>&quot;<a href="#topic+cif_iptw">iptw</a>&quot;: A weighted Aalen-Johansen estimator.
</p>
</li>
<li><p>&quot;<a href="#topic+cif_iptw_pseudo">iptw_pseudo</a>&quot;: A weighted estimator based on Pseudo-Values.
</p>
</li>
<li><p>&quot;<a href="#topic+cif_matching">matching</a>&quot;: Using Propensity Score Matching to estimate the adjusted CIFs.
</p>
</li>
<li><p>&quot;<a href="#topic+cif_aiptw">aiptw</a>&quot;: An Augmented Inverse Probability of Treatment Weighting estimator.
</p>
</li>
<li><p>&quot;<a href="#topic+cif_aiptw_pseudo">aiptw_pseudo</a>&quot;: An Augmented Inverse Probability of Treatment Weighting estimator using Pseudo-Values.
</p>
</li>
<li><p>&quot;<a href="#topic+cif_tmle">tmle</a>&quot;: Targeted Maximum Likelihood Estimation for continuous time time-to-event data with competing events.
</p>
</li>
<li><p>&quot;<a href="#topic+cif_aalen_johansen">aalen_johansen</a>&quot;: A simple stratified Aalen-Johansen estimator without any form of adjustment.
</p>
</li></ul>

<p>A short description of each method is contained in the documentation of the respective <code>cif_method</code> function. A concise overview of the supported functionality of each method can be found in the associated vignette (<code>vignette(topic="method_overview", package="adjustedCurves")</code>). For more detailed descriptions the cited literature in the respective documentation pages can be used. The documentation for <code>method="direct"</code> for example can be accessed using <code>?cif_direct</code>.
</p>
<p><strong><em>Required &amp; Optional Arguments</em></strong>
</p>
<p>Every method requires the specification of the <code>data</code>, <code>variable</code>, <code>ev_time</code>, <code>event</code>, <code>cause</code> and <code>method</code> arguments. All other arguments mentioned on this page are optional and work for all methods. Depending on the method used, other arguments are required as well. Those can be found on the top of the help page of the respective method. The help pages also list additional optional arguments.
</p>
<p><strong><em>Confidence Intervals</em></strong>
</p>
<p>For most methods approximations for the asymptotic variance of point estimates of the CIF have been proposed in the literature. Where available, those can be estimated and added to the output object using <code>conf_int=TRUE</code>. It is however recommended to use bootstrapping to estimate the variance instead, which can be done by setting <code>bootstrap=TRUE</code>. The <code>n_boot</code> argument is set to 500 by default. This number was chosen because it worked well in simulations but it does not guarantee convergence in practice. Users are recommended to inspect the bootstrapped estimates and adjust the number of replications accordingly. To allow faster bootstrapping the user can choose to run the function on multiple CPU cores in parallel using the <code>n_cores</code> argument.
</p>
<p><strong><em>Missing Data</em></strong>
</p>
<p>There are two ways to deal with missing data using this function. The first is using the <code>na.action</code> argument. It simply calls the respective <code>na.action</code> function on the data before doing any further processing. By using <code>na.action="na.omit"</code> for example, only rows with complete data are kept for the analysis.
</p>
<p>Alternatively, this function also supports the use of multiple imputation via the <span class="pkg">mice</span> package. Instead of supplying a single data.frame, the user should create a <code>mids</code> object using the <code>mice</code> function and directly pass this to the <code>data</code> argument. When methods are used which rely on previously estimated treatment or outcome models such as <code>"direct"</code> or <code>"aiptw"</code>, the user is required to supply a <code>mira</code> object instead of a single model. In other words: the models have to be fit on every imputed dataset before supplying them to this function. See <code>?mice</code> and the associated documentation for more information on how to use multiple imputation. When using <code>bootstrap=TRUE</code> and multiple imputation, the bootstrapping is performed on every imputed dataset separately. Cumulative Incidences are simply averaged across the imputed datasets according to Rubins Rule. Confidence intervals are calculated by first averaging the standard errors over all imputed datasets and afterwards using this pooled value to obtain a new confidence interval with the normal approximation.
</p>
<p><strong><em>Competing Risks</em></strong>
</p>
<p>This function is meant to be used for data containing multiple competing risks. If the data does not contain competing-events, it is recommended to use the <code>adjustedsurv</code> function instead. It does not estimate the CIF directly, but the CIF can be calculated from the survival using CIF = 1 - <code class="reqn">S(t)</code>. This can be done automatically in the <code>plot.adjustedsurv</code> function using <code>cif=TRUE</code>.
</p>
<p><strong><em>Graphical Displays</em></strong>
</p>
<p>A general plot of the estimated adjusted CIFs can be obtained using the associated <code><a href="#topic+plot.adjustedcif">plot</a></code> method. In addition, a plot of the difference between two estimated adjusted CIFs can be produced using the <code><a href="#topic+plot_curve_diff">plot_curve_diff</a></code> function.
</p>


<h3>Value</h3>

<p>Returns an <code>adjustedcif</code> object containing the following objects:
</p>
<table>
<tr><td><code>adjcif</code></td>
<td>
<p>A <code>data.frame</code> of estimated cumulative incidences for <code>cause</code> at some points in time for each level of <code>variable</code>. Depending on the arguments used also includes standard errors and confidence intervals.
</p>
</td></tr>
<tr><td><code>method</code></td>
<td>

<p>The method used to adjust the CIFs.
</p>
</td></tr>
<tr><td><code>categorical</code></td>
<td>
<p>Whether there are more than 2 groups in <code>variable</code>.
</p>
</td></tr>
<tr><td><code>call</code></td>
<td>

<p>The original function call.
</p>
</td></tr>
</table>
<p>When the argument <code>bootstrap</code> is set to <code>TRUE</code> it additionally contains the following objects:
</p>
<table>
<tr><td><code>boot_data</code></td>
<td>
<p>The adjusted CIFs estimated in each bootstrap sample.
</p>
</td></tr>
<tr><td><code>boot_adjcif</code></td>
<td>
<p>The mean CIFs of all bootstrap samples and corresponding standard errors and percentile confidence intervals.
</p>
</td></tr>
</table>
<p>When multiple imputation was used, the function additionally contains a <code>mids_analyses</code> object, containing the <code>adjustedcif</code> objects for each imputed dataset.
</p>
<p>Some method specific objects might also be contained in the output.
</p>


<h3>Author(s)</h3>

<p>The function itself was written by Robin Denz, but some <code>cif_method</code> functions include wrappers for functions written by other people. More information can be found in the respective <code>cif_method</code> documentation.
</p>


<h3>References</h3>

<p>Robin Denz, Renate Klaaßen-Mielke, and Nina Timmesfeld (2023). &quot;A Comparison of Different Methods to Adjust Survival Curves for Confounders&quot;. In: Statistics in Medicine 42.10, pp. 1461-1479
</p>
<p>More relevant literature can be found in the respective <code>cif_method</code> documentation.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.adjustedcif">plot.adjustedcif</a></code>, <code><a href="#topic+adjusted_rmtl">adjusted_rmtl</a></code>, <code><a href="#topic+plot_curve_diff">plot_curve_diff</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)
library(riskRegression)

set.seed(42)

# simulate some example data
sim_dat &lt;- sim_confounded_crisk(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# treatment assignment model
glm_mod &lt;- glm(group ~ x2 + x3 + x5 + x6, data=sim_dat, family="binomial")

# outcome model
cox_mod &lt;- CSC(Hist(time, event) ~ x1 + x2 + x4 + x5 + group, data=sim_dat)

# using direct adjustment with asymptotic confidence intervals for cause 1
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="direct",
                      outcome_model=cox_mod,
                      conf_int=TRUE,
                      bootstrap=FALSE)

# using IPTW with asymptotic confidence intervals for cause 2
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      method="iptw",
                      cause=2,
                      treatment_model=glm_mod,
                      conf_int=TRUE,
                      bootstrap=FALSE)

# using AIPTW with asymptotic confidence intervals for cause 1
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="aiptw",
                      outcome_model=cox_mod,
                      treatment_model=glm_mod,
                      conf_int=TRUE,
                      bootstrap=FALSE)

# using direct adjustment at custom points in time
custom_times &lt;- c(0.001, 0.1, 0.2, 0.6, 1.1)
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="direct",
                      outcome_model=cox_mod,
                      conf_int=TRUE,
                      bootstrap=FALSE,
                      times=custom_times)

# using bootstrapping with direct adjustment
# NOTE: In practice the number of bootstrap replications should be
#       greater than 10. This is only shown here for convenience.
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="direct",
                      outcome_model=cox_mod,
                      conf_int=TRUE,
                      bootstrap=TRUE,
                      n_boot=10)

# not run because those are too slow

# using bootstrapping with direct adjustment, run in parallel
# on two cores
library(foreach)
library(parallel)
library(doRNG)

adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="direct",
                      outcome_model=cox_mod,
                      conf_int=TRUE,
                      bootstrap=TRUE,
                      n_boot=4,
                      n_cores=2)

# using multiple imputation
library(mice)
library(WeightIt)

# simulate some data as example
sim_dat &lt;- sim_confounded_crisk(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# introduce random missingness in x1 as example
# NOTE: This is only done as an example, in reality you would
#       already have missing data, not introduce it yourself.
sim_dat$x1 &lt;- ifelse(runif(n=50) &lt; 0.5, sim_dat$x1, NA)

# perform multiple imputation
mids &lt;- mice::mice(data=sim_dat, method="pmm", m=2, printFlag=FALSE)

# IPTW Pseudo using WeightIt on imputed data, for cause = 1
adj &lt;- adjustedcif(data=mids,
                   variable="group",
                   ev_time="time",
                   event="event",
                   method="iptw_pseudo",
                   cause=1,
                   treatment_model=group ~ x1 + x2 + x5 + x6,
                   weight_method="ps")
plot(adj, force_bounds=TRUE, iso_reg=TRUE)

# More specific examples can be found in the documentation of each
# respective cif_method. See ?cif_ + "method" for more information.

</code></pre>

<hr>
<h2 id='adjustedCurves-package'>
Confounder-Adjusted Survival Curves and Cumulative Incidence Functions
</h2><span id='topic+adjustedCurves-package'></span>

<h3>Description</h3>

<p><strong><em>What is this package about?</em></strong>
</p>
<p>This package aims to unite all available adjustments methods for estimate confounder-adjusted survival curves and cause-specific confounder-adjusted cumulative incidence functions under one consistent framework. We try to make the usage of these methods and the calculation of associated statistics as easy as possible for the user, while still providing substantial functionality.
</p>
<p><strong><em>What exactly are adjusted survival curves / adjusted cumulative incidence functions?</em></strong>
</p>
<p>It is well known that confounding is a serious problem when analyzing data from non-randomized studies. This is also true when estimating survival curves or cumulative incidence functions. The aim is to estimate the population averaged survival probability or cumulative incidence for some group <code class="reqn">z</code>, which would have been observed if every individual would have been assigned to group <code class="reqn">z</code>. For example, the formal definition for the causal survival curve is:
</p>
<p style="text-align: center;"><code class="reqn">S_{z}(t) = E(I(T_z &gt; t))</code>
</p>

<p>where <code class="reqn">T_z</code> is the survival time that would have been observed if treatment <code class="reqn">z</code> was actually administered. See Denz et al. (2023) or Cai and Van der Laan (2020) for more details
</p>
<p><strong><em>What features are included in this package?</em></strong>
</p>
<p>This package includes 16 methods to estimate confounder-adjusted survival curves (single event) and 8 methods to estimate confounder adjusted cumulative incidence functions (possibly with multiple competing events). It provides <code>plot</code> functions to easily produce highly customizable and publication-ready graphics. It also allows the user to easily calculate relevant statistics, such as confidence intervals, p-values, and adjusted restricted mean survival time estimates. Multiple Imputation is directly supported.
</p>
<p><strong><em>What does a typical workflow using this package look like?</em></strong>
</p>
<p>The design of this package is based on the design of the <span class="pkg">WeightIt</span> package. It includes two main functions: <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> and <code><a href="#topic+adjustedcif">adjustedcif</a></code>. Every implemented adjustment method has their own documentation page including a small description, code examples, and relevant literature references. The typical workflow using this package is as follows (1) estimate confounder-adjusted curves (survival curves or CIFs) using either <code>adjustedsurv</code> or <code>adjustedcif</code>, (2) plot those using the S3 <code>plot</code> method and sometimes (3) calculate further statistics using <code>adjusted_rmst</code>, <code>adjusted_rmtl</code> or <code>adjusted_curve_test</code>.
</p>
<p><strong><em>When should I use <code>adjustedsurv</code> and when <code>adjustedcif</code>?</em></strong>
</p>
<p>With standard time-to-event data where only one type of event is possible both the confounder-adjusted survival curves and the confounder-adjusted cumulative incidence function can be estimated using the <code>adjustedsurv</code> function. While the adjustedsurv function only estimates the survival, the CIF can simply be calculated by <code class="reqn">1 - S(t)</code>. This transformation from survival curves to CIFs is directly implemented in the <code>plot</code> function (argument <code>cif</code>).
</p>
<p>When competing risks are present, the cause-specific confounder-adjusted survival curves can not be estimated in an unbiased way (see for example Satagopan et al. (2004) for an explanation). The cause-specific confounder-adjusted cumulative incidence functions however can be estimated using the <code>adjustedcif</code> function.
</p>
<p><strong><em>What features are missing from this package?</em></strong>
</p>
<p>In a former version, this package included two targeted maximum likelihood based methods for the estimation of adjusted survival curves and one for the estimation of adjusted cumulative incidence functions based on discrete-time data. These methods have been removed because the <span class="pkg">survtmle</span> package has been removed from CRAN and there is currently no other available implementation of these estimators on CRAN. However, since version 0.10.2 this package contains an implementation of targeted maximum likelihood estimation for continuous time-to-event data (a wrapper function for the <span class="pkg">concrete</span> package).
</p>
<p>This package also currently does not support time-varying treatments or covariates. It also does not support left-censoring, interval-censoring or left-truncation. These features may be added in future releases.
</p>
<p><strong><em>What if the variable of interest is continuous?</em></strong>
</p>
<p>All methods in this package are designed strictly for categorical variables of interest. If the variable of interest is continuous the user has to manually categorize the variable and save it as a factor first. This is, however, generally discouraged because artificial categorization may lead to bias or misleading results. To face this issue we have developed the <code>contsurvplot</code> R package which implements multiple plotting methods to visualize the (causal) effect of a continuous variable when analyzing survival data (Denz &amp; Timmesfeld 2023).
</p>
<p><strong><em>Where can I get more information?</em></strong>
</p>
<p>The documentation pages contain a lot of information, relevant examples and literature references. Additional examples can be found in the vignette of this package, which can be accessed using <code>vignette(topic="introduction", package="adjustedCurves")</code> or in the main paper associated with this package (Denz et al. 2023). We are also working on a separate article on this package that is going to be published in a peer-reviewed journal.
</p>
<p><strong><em>I want to suggest a new feature / I want to report a bug. Where can I do this?</em></strong>
</p>
<p>Bug reports, suggestions and feature requests are highly welcome. Please file an issue on the official github page or contact the author directly using the supplied e-mail address.
</p>


<h3>Author(s)</h3>

<p>Robin Denz, &lt;robin.denz@rub.de&gt;
</p>


<h3>References</h3>

<p>Robin Denz, Renate Klaaßen-Mielke, and Nina Timmesfeld (2023). &quot;A Comparison of Different Methods to Adjust Survival Curves for Confounders&quot;. In: Statistics in Medicine 42.10, pp. 1461-1479.
</p>
<p>Robin Denz, Nina Timmesfeld (2023). &quot;Visualizing the (Causal) Effect of a Continuous Variable on a Time-To-Event Outcome&quot;. In: Epidemiology 34.5, pp. 652-660.
</p>
<p>Weixin Cai and Mark J. van der Laan (2020). &quot;One-Step Targeted Maximum Likelihood Estimation for Time-To-Event Outcomes&quot;. In: Biometrics 76, pp. 722-733
</p>
<p>J. M. Satagopan, L. Ben-Porat, M. Berwick, M. Robson, D. Kutler, and A. D. Auerbach (2004). &quot;A Note on Competing Risks in Survival Data Analysis&quot;. In: British Journal of Cancer 91, pp. 1229-1235.
</p>

<hr>
<h2 id='adjustedsurv'>
Estimate Confounder-Adjusted Survival Curves
</h2><span id='topic+adjustedsurv'></span>

<h3>Description</h3>

<p>This is one of the two main functions of this R-Package. It allows the user to estimate confounder-adjusted survival curves using a variety of different methods. Some of these methods require additional packages to be installed and, depending on the specified method, there might be additional required arguments in the function call. More information is available on the documentation page of the respective <code>surv_method</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjustedsurv(data, variable, ev_time, event, method,
             conf_int=FALSE, conf_level=0.95, times=NULL,
             bootstrap=FALSE, n_boot=500,
             n_cores=1, na.action=options()$na.action,
             clean_data=TRUE, iso_reg=FALSE,
             force_bounds=FALSE, mi_extrapolation=FALSE,
             ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adjustedsurv_+3A_data">data</code></td>
<td>

<p>A <code>data.frame</code> object containing the needed time-to-event data in standard format. Ideally, this data set should only contain required variables. Can also be a <code>mids</code> object created with the <span class="pkg">mice</span> package. See details for how this works.
</p>
</td></tr>
<tr><td><code id="adjustedsurv_+3A_variable">variable</code></td>
<td>

<p>A character string specifying the variable by which the survival curves should be grouped. Must be a valid column name of <code>data</code>. The variable specified needs to be a factor variable.
</p>
</td></tr>
<tr><td><code id="adjustedsurv_+3A_ev_time">ev_time</code></td>
<td>

<p>A character string specifying the variable indicating the time-to-event or time-to-censoring. Must be a valid column name of <code>data</code>.
</p>
</td></tr>
<tr><td><code id="adjustedsurv_+3A_event">event</code></td>
<td>

<p>A character string specifying the binary event indicator. Must be a valid column name of <code>data</code>.
</p>
</td></tr>
<tr><td><code id="adjustedsurv_+3A_method">method</code></td>
<td>

<p>A character string specifying the adjustment method to use. Case sensitive. See details.
</p>
</td></tr>
<tr><td><code id="adjustedsurv_+3A_conf_int">conf_int</code></td>
<td>

<p>A logical variable, indicating whether the asymptotic variances and confidence intervals of the survival probabilities should be estimated. Not available for all methods. More information can be found in the documentation of each method. For an alternative way to get confidence intervals, see the <code>bootstrap</code> argument.
</p>
</td></tr>
<tr><td><code id="adjustedsurv_+3A_conf_level">conf_level</code></td>
<td>

<p>A number specifying the confidence level of asymptotic and/or bootstrap confidence intervals.
</p>
</td></tr>
<tr><td><code id="adjustedsurv_+3A_times">times</code></td>
<td>

<p>A numeric vector of time points at which the survival probability should be estimated or <code>NULL</code> (default). If <code>NULL</code> the survival probability is estimated at all points in time at which an event occurred in the pooled sample.
</p>
</td></tr>
<tr><td><code id="adjustedsurv_+3A_bootstrap">bootstrap</code></td>
<td>

<p>A logical variable indicating whether bootstrapping should be performed or not. In bootstrapping, a number of simple random samples with replacement of size <code>nrow(data)</code> are drawn from <code>data</code>. For each sample the calculations are repeated and used to estimate standard errors and confidence intervals. This can be used to obtain confidence intervals when asymptotic variance calculations are not available.
</p>
</td></tr>
<tr><td><code id="adjustedsurv_+3A_n_boot">n_boot</code></td>
<td>

<p>Number of bootstrap replications to perform. Ignored if <code>bootstrap</code> is <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="adjustedsurv_+3A_n_cores">n_cores</code></td>
<td>

<p>The number of cores to use when calculating bootstrap estimates. Ignored if <code>bootstrap=FALSE</code>. Is set to 1 by default, resulting in single threaded processing. Internally uses the <span class="pkg">doParallel</span> package if <code>n_cores &gt; 1</code>. In that case it also uses the <span class="pkg">doRNG</span> package to make the results replicable. See <code>?doRNG</code> and <code>?doParallel</code> for more details. Using multiple cores will speed up the calculation considerably in most cases.
</p>
</td></tr>
<tr><td><code id="adjustedsurv_+3A_na.action">na.action</code></td>
<td>

<p>How missing values should be handled. Can be one of: na.fail, na.omit, na.pass or na.exclude. Also accepts strings of the function names. See <code>?na.action</code> for more details. By default it uses the na.action which is set in the global options by the respective user. Ignored if multiple imputation is used.
</p>
</td></tr>
<tr><td><code id="adjustedsurv_+3A_clean_data">clean_data</code></td>
<td>

<p>If <code>TRUE</code> all columns which are not needed for the estimation are removed from <code>data</code> before any further calculations are performed. This ensures that calls to <code>na.omit</code> (see argument <code>na.action</code>) do not remove rows which are fully observed in respect to relevant columns due to missing values in irrelevant columns. Set to <code>FALSE</code> to skip this step. Usually this argument can be ignored. When using non-standard outcome models however it should be set to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="adjustedsurv_+3A_iso_reg">iso_reg</code></td>
<td>

<p>Either <code>TRUE</code> or <code>FALSE</code> (default), controlling whether isotonic regression is performed on the resulting survival probability estimates. This can be used to ensure that the survival curves are non-increasing. Since only a few methods may have this problem, this argument is only relevant for some methods (see method specific documentation).
</p>
</td></tr>
<tr><td><code id="adjustedsurv_+3A_force_bounds">force_bounds</code></td>
<td>

<p>Either <code>TRUE</code> or <code>FALSE</code> (default), controlling whether the resulting survival probability estimates should be forced to lie between 0 and 1. If <code>TRUE</code> and there are values higher than 1, they are simply set to 1. Values lower than 0 are similarly set to 0. Since only a few methods may have this problem, this argument is only relevant for some methods (see method specific documentation).
</p>
</td></tr>
<tr><td><code id="adjustedsurv_+3A_mi_extrapolation">mi_extrapolation</code></td>
<td>

<p>Whether to allow extrapolation due to imputed survival times or not. This argument is only relevant when using multiply imputed <code>data</code> with missing covariates in <code>variable</code>, <code>ev_time</code> or <code>event</code>. Depending on the algorithm used to obtain the imputed datasets, it may be possible that one or more imputed datasets contain survival times in a group that are larger than the maximum observed survival time in that group. This may lead to unwanted extrapolation (e.g. the survival curves extending further than they should). By keeping this argument at <code>FALSE</code>, these times are removed from the output. If set to <code>TRUE</code>, all available estimates will be used.
</p>
</td></tr>
<tr><td><code id="adjustedsurv_+3A_...">...</code></td>
<td>

<p>Further arguments passed to the respective <code>surv_method</code>. For example when using <code>method="direct"</code> all further arguments are passed to the <code>surv_direct</code> function. See details.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The primary purpose of the <code>adjustedsurv</code> function is to provide a convenient way to estimate confounder-adjusted survival curves using any of the methods provided in the literature. A <code><a href="#topic+plot.adjustedsurv">plot</a></code> method is provided to graphically display the estimated survival curves as well. Currently the following methods can be used:
</p>

<ul>
<li><p>&quot;<a href="#topic+surv_direct">direct</a>&quot;: Direct Standardization based on a previously fit model (Cox-Regression, ...).
</p>
</li>
<li><p>&quot;<a href="#topic+surv_direct_pseudo">direct_pseudo</a>&quot;: Direct Standardization based on Pseudo-Values.
</p>
</li>
<li><p>&quot;<a href="#topic+surv_iptw_km">iptw_km</a>&quot;: A weighted Kaplan-Meier estimator.
</p>
</li>
<li><p>&quot;<a href="#topic+surv_iptw_cox">iptw_cox</a>&quot;: A weighted estimator based on a stratified weighted Cox-Regression model.
</p>
</li>
<li><p>&quot;<a href="#topic+surv_iptw_pseudo">iptw_pseudo</a>&quot;: A weighted estimator based on Pseudo-Values.
</p>
</li>
<li><p>&quot;<a href="#topic+surv_matching">matching</a>&quot;: Using Propensity Score Matching to estimate the adjusted survival curves.
</p>
</li>
<li><p>&quot;<a href="#topic+surv_emp_lik">emp_lik</a>&quot;: An Empirical Likelihood based estimator.
</p>
</li>
<li><p>&quot;<a href="#topic+surv_aiptw">aiptw</a>&quot;: An Augmented Inverse Probability of Treatment Weighting estimator.
</p>
</li>
<li><p>&quot;<a href="#topic+surv_aiptw_pseudo">aiptw_pseudo</a>&quot;: An Augmented Inverse Probability of Treatment Weighting estimator using Pseudo-Values.
</p>
</li>
<li><p>&quot;<a href="#topic+surv_tmle">tmle</a>&quot;: Targeted Maximum Likelihood Estimation for continuously distributed time-to-event data.
</p>
</li>
<li><p>&quot;<a href="#topic+surv_strat_amato">strat_amato</a>&quot;: A method based on stratification and weighting by Amato (1988).
</p>
</li>
<li><p>&quot;<a href="#topic+surv_strat_nieto">strat_nieto</a>&quot;: A method based on stratification and weighting by Gregory (1988) and Nieto &amp; Coresh (1996).
</p>
</li>
<li><p>&quot;<a href="#topic+surv_strat_cupples">strat_cupples</a>&quot;: A method based on stratification and weighting by Cupples et al. (1995).
</p>
</li>
<li><p>&quot;<a href="#topic+surv_iv_2SRIF">iv_2SRIF</a>&quot;: An instrumental variable method based on two stage residual inclusion with a frailty term.
</p>
</li>
<li><p>&quot;<a href="#topic+surv_prox_iptw">prox_iptw</a>&quot;: Proximal causal inference based inverse probability of treatment weighting.
</p>
</li>
<li><p>&quot;<a href="#topic+surv_prox_aiptw">prox_aiptw</a>&quot;: Proximal causal inference based augmented inverse probability of treatment weighting.
</p>
</li>
<li><p>&quot;<a href="#topic+surv_km">km</a>&quot;: A simple stratified Kaplan-Meier estimator without any form of adjustment.
</p>
</li></ul>

<p>A short description of each method is contained in the documentation of the respective <code>surv_method</code> function. A concise overview of the supported functionality of each method can be found in the associated vignette (<code>vignette(topic="method_overview", package="adjustedCurves")</code>). For more detailed descriptions the cited literature in the respective documentation pages can be used. The documentation for <code>method="direct"</code> for example can be accessed using <code>?surv_direct</code>.
</p>
<p><strong><em>Required &amp; Optional Arguments</em></strong>
</p>
<p>Every method requires the specification of the <code>data</code>, <code>variable</code>, <code>ev_time</code>, <code>event</code> and <code>method</code> arguments. All other arguments mentioned on this page are optional and work for all methods. Depending on the method used, other arguments are required as well. Those can be found on the top of the help page of the respective method. The help pages also list additional optional arguments.
</p>
<p><strong><em>Confidence Intervals</em></strong>
</p>
<p>For most methods approximations for the asymptotic variance of point estimates of the survival function have been proposed in the literature. Where available, those can be estimated and added to the output object using <code>conf_int=TRUE</code>. It is however recommended to use bootstrapping to estimate the variance instead, which can be done by setting <code>bootstrap=TRUE</code>. The <code>n_boot</code> argument is set to 500 by default. This number was chosen because it worked well in simulations but it does not guarantee convergence in practice. Users are recommended to inspect the bootstrapped estimates and adjust the number of replications accordingly. To allow faster bootstrapping the user can choose to run the function on multiple CPU cores in parallel using the <code>n_cores</code> argument.
</p>
<p><strong><em>Missing Data</em></strong>
</p>
<p>There are two ways to deal with missing data using this function. The first is using the <code>na.action</code> argument. It simply calls the respective <code>na.action</code> function on the data before doing any further processing. By using <code>na.action="na.omit"</code> for example, only rows with complete data are kept for the analysis.
</p>
<p>Alternatively, this function also supports the use of multiple imputation via the <span class="pkg">mice</span> package. Instead of supplying a single data.frame, the user should create a <code>mids</code> object using the <code>mice</code> function and directly pass this to the <code>data</code> argument. When methods are used which rely on previously estimated treatment assignment or outcome models such as <code>"direct"</code> or <code>"aiptw"</code>, the user is required to supply a <code>mira</code> object instead of a single model. In other words: the models have to be fit on every imputed dataset before supplying them to this function. See <code>?mice</code> and the associated documentation for more information on how to use multiple imputation. When using <code>bootstrap=TRUE</code> and multiple imputation, the bootstrapping is performed on every imputed dataset separately. Survival probabilities are simply averaged across the imputed datasets according to Rubins Rule. Confidence intervals are calculated by first averaging the standard errors over all imputed datasets and afterwards using this pooled value to obtain a new confidence interval with the normal approximation.
</p>
<p><strong><em>Competing Risks</em></strong>
</p>
<p>If the data contains competing-risks, this function cannot be used. It is however possible to estimate confounder-adjusted cause-specific cumulative incidence functions using the <code>adjustedcif</code> function.
</p>
<p><strong><em>Graphical Displays</em></strong>
</p>
<p>A general plot of the estimated adjusted survival curves can be obtained using the associated <code><a href="#topic+plot.adjustedsurv">plot</a></code> method. In addition, a plot of the difference between two estimated adjusted survival curves can be produced using the <code><a href="#topic+plot_curve_diff">plot_curve_diff</a></code> function.
</p>


<h3>Value</h3>

<p>Returns an <code>adjustedsurv</code> object containing the following objects:
</p>
<table>
<tr><td><code>adjsurv</code></td>
<td>
<p>A <code>data.frame</code> of estimated adjusted survival probabilities for some points in time for each level of <code>variable</code>. Depending on the arguments used also includes standard errors and confidence intervals.
</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>The <code>data.frame</code> used in the original function call.
</p>
</td></tr>
<tr><td><code>method</code></td>
<td>

<p>The method used to adjust the survival curves.
</p>
</td></tr>
<tr><td><code>categorical</code></td>
<td>
<p>Whether there are more than 2 groups in <code>variable</code>.
</p>
</td></tr>
<tr><td><code>call</code></td>
<td>

<p>The original function call.
</p>
</td></tr>
</table>
<p>When the argument <code>bootstrap</code> is set to <code>TRUE</code>, it additionally contains the following objects:
</p>
<table>
<tr><td><code>boot_data</code></td>
<td>
<p>The adjusted survival curves estimated in each bootstrap sample.
</p>
</td></tr>
<tr><td><code>boot_adjsurv</code></td>
<td>
<p>The mean adjusted survival curves of all bootstrap samples and corresponding standard errors and percentile confidence intervals.
</p>
</td></tr>
</table>
<p>When multiple imputation was used, the function additionally contains a <code>mids_analyses</code> object, containing the <code>adjustedsurv</code> objects for each imputed dataset.
</p>
<p>Some method specific objects might also be contained in the output.
</p>


<h3>Author(s)</h3>

<p>The function itself was written by Robin Denz, but some <code>surv_method</code> functions include wrappers for functions written by other people. More information can be found in the respective <code>surv_method</code> documentation.
</p>


<h3>References</h3>

<p>Robin Denz, Renate Klaaßen-Mielke, and Nina Timmesfeld (2023). &quot;A Comparison of Different Methods to Adjust Survival Curves for Confounders&quot;. In: Statistics in Medicine 42.10, pp. 1461-1479
</p>
<p>Other relevant literature can be found in the respective <code>surv_method</code> documentation.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.adjustedsurv">plot.adjustedsurv</a></code>, <code><a href="#topic+adjusted_rmst">adjusted_rmst</a></code>, <code><a href="#topic+adjusted_rmtl">adjusted_rmtl</a></code>, <code><a href="#topic+adjusted_surv_quantile">adjusted_surv_quantile</a></code>, <code><a href="#topic+adjusted_curve_diff">adjusted_curve_diff</a></code>, <code><a href="#topic+adjusted_curve_test">adjusted_curve_test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)

set.seed(42)

# simulate some example data
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# treatment assignment model
glm_mod &lt;- glm(group ~ x2 + x3 + x5 + x6, data=sim_dat, family="binomial")

# outcome model
cox_mod &lt;- coxph(Surv(time, event) ~ x1 + x2 + x4 + x5 + group,
                 data=sim_dat, x=TRUE)

# using direct adjustment with asymptotic confidence intervals
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="direct",
                        outcome_model=cox_mod,
                        conf_int=TRUE,
                        bootstrap=FALSE)

# using IPTW Kaplan-Meier with asymptotic confidence intervals
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="iptw_km",
                        treatment_model=glm_mod,
                        conf_int=TRUE,
                        bootstrap=FALSE)

# using AIPTW with asymptotic confidence intervals
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="aiptw",
                        outcome_model=cox_mod,
                        treatment_model=glm_mod,
                        conf_int=TRUE,
                        bootstrap=FALSE)

# using direct adjustment at custom points in time
custom_times &lt;- c(0.001, 0.1, 0.2, 0.6, 1.1)
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="direct",
                        outcome_model=cox_mod,
                        conf_int=TRUE,
                        bootstrap=FALSE,
                        times=custom_times)

# using bootstrapping with direct adjustment
# NOTE: n_boot should be much higher than 10 in reality, only used
#       here as a fast example
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="direct",
                        outcome_model=cox_mod,
                        conf_int=TRUE,
                        bootstrap=TRUE,
                        n_boot=10)

# not run because those are too slow

# using bootstrapping with direct adjustment, run in parallel
# on two cores
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="direct",
                        outcome_model=cox_mod,
                        conf_int=TRUE,
                        bootstrap=TRUE,
                        n_boot=4,
                        n_cores=2)

# using multiple imputation
library(mice)
library(WeightIt)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# introduce random missingness in x1 as example
# NOTE: This is only done as an example, in reality you would
#       already have missing data, not introduce it yourself.
sim_dat$x1 &lt;- ifelse(runif(n=50) &lt; 0.5, sim_dat$x1, NA)

# perform multiple imputation
mids &lt;- mice::mice(data=sim_dat, method="pmm", m=2, printFlag=FALSE)

# IPTW KM using WeightIt on imputed data
adj &lt;- adjustedsurv(data=mids,
                    variable="group",
                    ev_time="time",
                    event="event",
                    method="iptw_km",
                    treatment_model=group ~ x1 + x2 + x5 + x6,
                    weight_method="ps")
plot(adj)

# More specific examples can be found in the documentation of each
# respective surv_method. See ?surv_ + "method" for more information.

</code></pre>

<hr>
<h2 id='as_ggsurvplot_df'>
Extract a <code>data.frame</code> containing the estimated survival curves from a <code>adjustedsurv</code> object
</h2><span id='topic+as_ggsurvplot_df'></span>

<h3>Description</h3>

<p>A small convenience function to extract the most important quantities from an <code>adjustedsurv</code> object. The resulting <code>data.frame</code> is structured according to the format required by the <code>ggsurvplot_df</code> function of the <span class="pkg">survminer</span> package, making it easy to use the <code>ggsurvplot_df</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_ggsurvplot_df(adjsurv)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_ggsurvplot_df_+3A_adjsurv">adjsurv</code></td>
<td>

<p>An object of class <code>adjustedsurv</code> created by the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code>data.frame</code> containing the required information, extracted from the <code>adjustedsurv</code> object.
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjustedsurv">adjustedsurv</a></code>, <code><a href="#topic+plot.adjustedsurv">plot.adjustedsurv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)

set.seed(42)

# simulate some example data
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# treatment assignment model
glm_mod &lt;- glm(group ~ x2 + x3 + x5 + x6, data=sim_dat, family="binomial")

# estimate some adjusted survival curves
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="iptw_km",
                        treatment_model=glm_mod,
                        conf_int=TRUE,
                        bootstrap=FALSE)

# extract info
df &lt;- as_ggsurvplot_df(adjsurv)

# not run here to avoid dependency on survminer
if (interactive()) {
# plot using survminer, requires the 'survminer' package
ggsurvplot_df(df)
}
</code></pre>

<hr>
<h2 id='cif_aalen_johansen'>
Group-Specific Aalen-Johansen CIFs
</h2><span id='topic+cif_aalen_johansen'></span>

<h3>Description</h3>

<p>This page explains the details of estimating standard Aalen-Johansen cumulative incidence functions, stratified by the group variable (<code>method="aalen_johansen"</code> in the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function). All regular arguments of the <code>adjustedcif</code> function can be used. Further arguments specific to this method are listed below.
</p>
<p>NO adjustment for any confounders are made. This function is included only for reference and should not be used when confounder adjusted CIFs are desired.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="cif_aalen_johansen_+3A_...">...</code></td>
<td>

<p>Further arguments passed to <code><a href="cmprsk.html#topic+cuminc">cuminc</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> NO adjustments are made. This is just a stratified Aalen-Johansen estimator.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Any number of levels in <code>variable</code> are allowed. Must be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are available.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the the <span class="pkg">cmprsk</span> package.
</p>
</li></ul>

<p>This function is just a convenient wrapper around the <code><a href="cmprsk.html#topic+cuminc">cuminc</a></code> function. See <code>?cuminc</code> or the cited literature for more details.
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedsurv</code> function:
</p>

<ul>
<li> <p><code>cuminc_object</code>: The object returned by the <code>cuminc</code> function.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>The wrapper function was written by Robin Denz, the <code>cuminc</code> function (which this wrapper is build around) was written by other people. See <code>?cuminc</code> for more details.
</p>


<h3>References</h3>

<p>Odd O. Aalen and Søren Johansen (1978). &quot;An Empirical Transition Matrix for Non-Homogeneous Markov Chains Based on Censored Observations&quot;. In: Scandinavian Journal of Statistics 5.3, pp. 141-150
</p>


<h3>See Also</h3>

<p><code><a href="cmprsk.html#topic+cuminc">cuminc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(cmprsk)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_crisk(n=50, max_t=5)
sim_dat$group &lt;- as.factor(sim_dat$group)

# calculate un-adjusted aalen-johansen estimates
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="aalen_johansen")

# plot the curves
plot(adjcif)
</code></pre>

<hr>
<h2 id='cif_aiptw'>
Augmented Inverse Probability of Treatment Weighted CIFs
</h2><span id='topic+cif_aiptw'></span>

<h3>Description</h3>

<p>This page explains the details of estimating augmented inverse probability of treatment weighted cumulative incidence functions for competing risks data (<code>method="aiptw"</code> in the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function). All regular arguments of the <code>adjustedcif</code> function can be used. Additionally, the <code>outcome_model</code> argument and the <code>treatment_model</code> argument have to be specified in the <code>adjustedcif</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="cif_aiptw_+3A_outcome_model">outcome_model</code></td>
<td>

<p>[<strong>required</strong>] Must be a <code>CauseSpecificCox</code> model object created using the <code><a href="riskRegression.html#topic+CSC">CSC</a></code> function, modeling the time-to-event mechanism. See details and examples.
</p>
</td></tr>
<tr><td><code id="cif_aiptw_+3A_treatment_model">treatment_model</code></td>
<td>

<p>[<strong>required</strong>] Must be a <code>glm</code> model object with <code>variable</code> as response variable. See details and examples.
</p>
</td></tr>
<tr><td><code id="cif_aiptw_+3A_censoring_model">censoring_model</code></td>
<td>

<p>Must be a <code>coxph</code> model object, modeling the censoring mechanism or <code>NULL</code>. If <code>NULL</code> (default) independent censoring is assumed. See details and examples.
</p>
</td></tr>
<tr><td><code id="cif_aiptw_+3A_verbose">verbose</code></td>
<td>

<p>Whether to print estimation information of the <code>ate</code> function in the <span class="pkg">riskRegression</span> package. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="cif_aiptw_+3A_...">...</code></td>
<td>

<p>Further arguments passed to <code><a href="riskRegression.html#topic+ate">ate</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Requires both a treatment assignment model (<code><a href="stats.html#topic+glm">glm</a></code>) and a outcome model (<code><a href="riskRegression.html#topic+CSC">CSC</a></code>). Also allows, but does not rely on, an additional model describing the censoring mechanism (a <code><a href="survival.html#topic+coxph">coxph</a></code> object).
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Currently only two groups in <code>variable</code> are allowed. Must still be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are available.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are not guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are not guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the <span class="pkg">riskRegression</span> package.
</p>
</li></ul>

<p>Instead of only modeling the outcome mechanism or the treatment assignment mechanism, both kind of models are required to use this method. If either of those models are correctly specified, unbiased estimates will be obtained. Can also be used to adjust for dependent censoring using a Cox-Regression model. An obvious advantage of this method is it's doubly robust property. This however comes at the price of some efficiency. It is also possible that some estimates fall outside the 0 and 1 probability bounds, particularly if the time is near 0 or the maximal observed event time. There is also no guarantee that the estimated CIFs will be monotonically increasing. For more information on the methods the user is referred to the literature listed in the references.
</p>
<p>This function is basically just a wrapper around the <code><a href="riskRegression.html#topic+ate">ate</a></code> function from the <span class="pkg">riskRegression</span> package. Additional arguments may be passed to that function using the <code>...</code> syntax. It is however recommended to use <code><a href="riskRegression.html#topic+ate">ate</a></code> directly in these cases.
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedcif</code> function:
</p>

<ul>
<li> <p><code>ate_object</code>: The object returned by the <code>ate</code> function.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>The wrapper function was written by Robin Denz, the <code>ate</code> function (which this wrapper is build around) was written by other people. See <code>?ate</code> for more details.
</p>


<h3>References</h3>

<p>James M. Robins and Andrea Rotnitzky (1992). &quot;Recovery of Information and Adjustment for Dependent Censoring Using Surrogate Markers&quot;. In: AIDS Epidemiology: Methodological Issues. Ed. by Nicholas P. Jewell, Klaus Dietz, and Vernon T. Farewell. New York: Springer Science + Business Media, pp. 297-331
</p>
<p>Alan E. Hubbard, Mark J. van der Laan, and James M. Robins (2000). &quot;Nonparametric Locally Efficient Estimation of the Treatment Specific Survival Distribution with Right Censored Data and Covariates in Observational Studies&quot;. In: Statistical Models in Epidemiology, the Environment, and Clinical Trials. Ed. by M. Elizabeth Halloran and Donald Berry. New York: Springer Science + Business Media, pp. 135-177
</p>
<p>Brice Maxime Hugues Ozenne, Thomas Harder Scheike, and Laila Staerk (2020). &quot;On the Estimation of Average Treatment Effects with Right-Censored Time to Event Outcome and Competing Risks&quot;. In: Biometrical Journal 62, pp. 751-763
</p>


<h3>See Also</h3>

<p><code><a href="riskRegression.html#topic+ate">ate</a></code>, <code><a href="riskRegression.html#topic+CSC">CSC</a></code>, <code><a href="survival.html#topic+coxph">coxph</a></code>, <code><a href="stats.html#topic+glm">glm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)
library(riskRegression)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_crisk(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a cause-specific cox-regression for the outcome
cox_mod &lt;- CSC(Hist(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group,
               data=sim_dat)

# estimate a treatment assignment model
glm_mod &lt;- glm(group ~ x1 + x3 + x5 + x6, data=sim_dat, family="binomial")

# use it to calculate adjusted survival curves
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="aiptw",
                      outcome_model=cox_mod,
                      treatment_model=glm_mod,
                      conf_int=FALSE)

# plot the curves
plot(adjcif)
</code></pre>

<hr>
<h2 id='cif_aiptw_pseudo'>
Augmented Inverse Probability of Treatment Weighted CIFs using Pseudo-Values
</h2><span id='topic+cif_aiptw_pseudo'></span>

<h3>Description</h3>

<p>This page explains the details of estimating augmented inverse probability of treatment weighted CIFs using pseudo-values in a competing risks setting (<code>method="aiptw_pseudo"</code> in the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function). All regular arguments of the <code>adjustedcif</code> function can be used. Additionally, the <code>outcome_vars</code> argument and the <code>treatment_model</code> argument have to be specified in the <code>adjustedcif</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="cif_aiptw_pseudo_+3A_outcome_vars">outcome_vars</code></td>
<td>

<p>[<strong>required</strong>] A character vector of column names specifying variables to be used when modeling the outcome mechanism using <code>geese</code>. See details and examples.
</p>
</td></tr>
<tr><td><code id="cif_aiptw_pseudo_+3A_treatment_model">treatment_model</code></td>
<td>

<p>[<strong>required</strong>] Must be a <code>glm</code> or <code>multinom</code> model object with <code>variable</code> as response variable. See details and examples.
</p>
</td></tr>
<tr><td><code id="cif_aiptw_pseudo_+3A_type_time">type_time</code></td>
<td>

<p>A character string specifying how the time should be modeled. Possible values are <code>"factor"</code> (modeling each point in time as a separate variable, the default), <code>"bs"</code> (modeling time using B-Splines) or <code>"ns"</code> (modeling time using natural splines).
</p>
</td></tr>
<tr><td><code id="cif_aiptw_pseudo_+3A_spline_df">spline_df</code></td>
<td>

<p>The number of degrees of freedom used for the natural-spline or B-spline function. Defaults to 5. Ignored if <code>type_time="factor"</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Requires a treatment assignment model (<code><a href="stats.html#topic+glm">glm</a></code> or <code><a href="mgcv.html#topic+multinom">multinom</a></code>) and a character vector of variable names used to model the outcome mechanism (internally uses <code><a href="geepack.html#topic+geese">geese</a></code>). In contrast to the <code><a href="#topic+cif_aiptw">&quot;aiptw&quot;</a></code> method this function does not allow for dependent censoring.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Any number of levels in <code>variable</code> are allowed. Must be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are available.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are not guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are not guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the <span class="pkg">geepack</span> and <span class="pkg">prodlim</span> packages.
</p>
</li></ul>

<p>Instead of only modeling the outcome mechanism or the treatment assignment mechanism, both kind of models are required to use this method. If either of those models are correctly specified, unbiased estimates will be obtained. In contrast to the <code><a href="#topic+cif_aiptw">&quot;aiptw&quot;</a></code> method, the &quot;aiptw_pseudo&quot; method uses a generalized estimation equation (geese) approach to model the outcome mechanism. The model is fit in the same way as described in the <code><a href="#topic+cif_direct_pseudo">&quot;direct_pseudo&quot;</a></code> method. Those Direct Standardization based estimates are then transformed using the previously estimated propensity score. This results in the doubly-robust property of the method. More information on this particular method can be found in the original article by Wang (2018). The original article only deals with survival probabilities without competing risks, but the only difference to the CIF estimation with competing risks is the calculation of the pseudo-values. More information on Pseudo-Values is available in Andersen et al. (2017) and Andersen and Perme (2010).
</p>
<p>When estimating the <code>geese</code> model the <code>ev_time</code> variable is used as a factor by default. This results in one coefficient being estimated for each unique point in time, which can be very slow computationally if there are a lot of unique points in time and/or the dataset has many rows. In these cases it is recommended to use <code>type_time="bs"</code> or <code>type_time="ns"</code>, which results in the <code>ev_time</code> being modeled using B-Splines or Natural Splines. Simulation studies indicate that there is little difference in the estimates when an appropriately large number of <code>spline_df</code> is used.
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedcif</code> function:
</p>

<ul>
<li> <p><code>pseudo_values</code>: The matrix of estimated pseudo-values.
</p>
</li>
<li> <p><code>geese_model</code>: The geese model used to make the predictions.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jixian Wang supplied the R source code used in the original article, which was used by Robin Denz to create a generalized version of this method with additional functionality and improved performance.
</p>


<h3>References</h3>

<p>Jixian Wang (2018). &quot;A Simple, Doubly Robust, Efficient Estimator for Survival Functions Using Pseudo Observations&quot;. In: Pharmaceutical Statistics 17.38-48
</p>
<p>James M. Robins and Andrea Rotnitzky (1992). &quot;Recovery of Information and Adjustment for Dependent Censoring Using Surrogate Markers&quot;. In: AIDS Epidemiology: Methodological Issues. Ed. by Nicholas P. Jewell, Klaus Dietz, and Vernon T. Farewell. New York: Springer Science + Business Media, pp. 297-331
</p>
<p>Per Kragh Andersen, Elisavet Syriopoulou, and Erik T. Parner (2017). &quot;Causal Inference in Survival Analysis using Pseudo-Observations&quot;. In: Statistics in Medicine 36, pp. 2669-2681
</p>
<p>Per Kragh Andersen and Maja Pohar Perme (2010). &quot;Pseudo-Observations in Survival Analysis&quot;. In: Statistical Methods in Medical Research 19, pp. 71-99
</p>
<p>Aris Perperoglou, Willi Sauerbrei, Michal Abrahamowicz, and Matthias Schmid (2019). &quot;A Review of Spline Function Procedures in R&quot;. in: BMC Medical Research Methodology 19.46, pp. 1-16
</p>


<h3>See Also</h3>

<p><code><a href="geepack.html#topic+geese">geese</a></code>, <code><a href="prodlim.html#topic+jackknife">jackknife</a></code>, <code><a href="splines.html#topic+ns">ns</a></code>, <code><a href="splines.html#topic+bs">bs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_crisk(n=30, max_t=5)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a treatment assignment model
glm_mod &lt;- glm(group ~ x1 + x3 + x5 + x6, data=sim_dat, family="binomial")

# use it + pseudo values + geese model to calculate adjusted CIFs
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="aiptw_pseudo",
                      outcome_vars=c("x1", "x2", "x3", "x4", "x5", "x6"),
                      treatment_model=glm_mod,
                      conf_int=FALSE,
                      force_bounds=TRUE,
                      iso_reg=TRUE)

# plot the curves
plot(adjcif)
</code></pre>

<hr>
<h2 id='cif_direct'>
Direct Adjusted Cumulative Incidence Functions
</h2><span id='topic+cif_direct'></span>

<h3>Description</h3>

<p>This page explains the details of estimating confounder-adjusted CIFs using a previously fit model to describe the outcome mechanism in a competing risks setting (<code>method="direct"</code> in the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function). All regular arguments of the <code>adjustedcif</code> function can be used. Additionally, the <code>outcome_model</code> argument has to be specified in the <code>adjustedcif</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="cif_direct_+3A_outcome_model">outcome_model</code></td>
<td>

<p>[<strong>required</strong>] Must be a previously fit model object including <code>variable</code> as independent variable. Apart from the classic <code>CauseSpecificCox</code> model this function also supports a variety of other models, such as the Fine &amp; Gray model (<code><a href="riskRegression.html#topic+FGR">FGR</a></code>). See <code><a href="#topic+models_cif_direct">models_cif_direct</a></code> for a list of supported model objects and some more details.
</p>
</td></tr>
<tr><td><code id="cif_direct_+3A_verbose">verbose</code></td>
<td>

<p>Whether to print estimation information of the <code>ate</code> function in the <span class="pkg">riskRegression</span> package. Defaults to <code>FALSE</code>. Ignored if a <code>outcome_model</code> is not a <code>CauseSpecificCox</code> model.
</p>
</td></tr>
<tr><td><code id="cif_direct_+3A_predict_fun">predict_fun</code></td>
<td>

<p>A function which should be used to calculate the predicted cause-specific cumulative incidences given covariates and some points in time. This argument only needs to be specified if the kind of model supplied in the <code>outcome_model</code> is not directly supported. See <code><a href="#topic+models_cif_direct">models_cif_direct</a></code> for more information. Defaults to <code>NULL</code>.
</p>
</td></tr>
<tr><td><code id="cif_direct_+3A_...">...</code></td>
<td>

<p>Further arguments passed to <code><a href="riskRegression.html#topic+ate">ate</a></code> when a <code>CauseSpecificCox</code> model is supplied in the <code>outcome_model</code> argument. Otherwise arguments are passed to the respective <code>predict</code> function. See <code><a href="#topic+models_cif_direct">models_cif_direct</a></code> for more details.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Requires a model describing the outcome mechanism. Both Cause-Specific-Cox models (<code><a href="riskRegression.html#topic+CSC">CSC</a></code>) and Fine &amp; Gray models (<code><a href="riskRegression.html#topic+FGR">FGR</a></code>) are supported, as well as other models. See <code><a href="#topic+models_cif_direct">models_cif_direct</a></code> for a full list.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Any number of levels in <code>variable</code> are allowed. Must be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Asymptotic variance calculations are only available if the <code>outcome_model</code> is a <code>CauseSpecificCox</code> model. The <code><a href="riskRegression.html#topic+ate">ate</a></code> function is used for the calculation in that case. Bootstrap confidence intervals can however be calculated with all supported models. See <code>?adjustedcif</code> for more information on bootstrapping.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the <span class="pkg">riskRegression</span> package. Depending on <code>outcome_model</code> other packages might be needed. See <code><a href="#topic+models_cif_direct">models_cif_direct</a></code> for more details.
</p>
</li></ul>

<p>This method works by executing the following steps: (1) First a model is fitted which describes the outcome mechanism (time-to-event). Next (2) multiple copies of the original dataset are created, one for each possible level of the <code>variable</code> of interest. (3) The <code>variable</code> is then set to one level for all observations in each dataset. (4) The model is used to predict the CIF at some points in time T for each observation in all dataset copies. (5) Those estimated probabilities are averaged for each dataset at each point in time, resulting in adjusted CIFs for all levels of the group variable at the specified points in time.
</p>
<p>In the literature this method is sometimes called &quot;Direct Standardization&quot;, &quot;Corrected Group-Prognosis&quot;, &quot;G-Computation&quot; or &quot;G-Formula&quot;. If the model in step (1) is &quot;correct&quot;&quot; this method will produce unbiased estimates of the counterfactual cumulative incidences. A model can be called a &quot;correct&quot; model in this context if it can be used to produce unbiased estimates of the true (but unknown) individual CIFs given covariates. When used properly this is one of the most efficient methods. Theoretically any type of model could be used. The most popular ones are <code><a href="riskRegression.html#topic+CSC">CSC</a></code> models and <code><a href="riskRegression.html#topic+FGR">FGR</a></code> models, but a variety of others models is also supported. More information can be found in the literature listed in the references.
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedcif</code> function:
</p>

<ul>
<li> <p><code>ate_object</code>: The object returned by the <code>ate</code> function.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>The function itself was written by Robin Denz. When using <code>CauseSpecificCox</code> models however, this function is just a wrapper around the <code><a href="riskRegression.html#topic+ate">ate</a></code> function, which was written by other people. See <code>?ate</code> for more information.
</p>


<h3>References</h3>

<p>Xu Zhang and Mei-Jie Zhang (2011). &quot;SAS Macros for Estimation of Direct Adjusted Cumulative Incidence Curves Under Proportional Subdistribution Hazards Models&quot;. In: Computer Methods and Programs in Biomedicine 101.1, pp. 87-93
</p>
<p>Brice Maxime Hugues Ozenne, Thomas Harder Scheike, and Laila Staerk (2020). &quot;On the Estimation of Average Treatment Effects with Right-Censored Time to Event Outcome and Competing Risks&quot;. In: Biometrical Journal 62, pp. 751-763
</p>


<h3>See Also</h3>

<p><code><a href="#topic+models_cif_direct">models_cif_direct</a></code>, <code><a href="riskRegression.html#topic+ate">ate</a></code>, <code><a href="riskRegression.html#topic+CSC">CSC</a></code>, <code><a href="riskRegression.html#topic+FGR">FGR</a></code>, <code><a href="#topic+CSC_MI">CSC_MI</a></code>, <code><a href="#topic+FGR_MI">FGR_MI</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)
library(riskRegression)
library(prodlim)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_crisk(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a cause-specific cox-regression for the outcome
cox_mod &lt;- CSC(Hist(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group,
                 data=sim_dat)

# use it to calculate adjusted CIFs
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="direct",
                      outcome_model=cox_mod,
                      conf_int=FALSE)

# plot the curves
plot(adjcif)

# estimate a Fine &amp; Gray model for the outcome instead
fgr_mod &lt;- FGR(Hist(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group,
               data=sim_dat, cause=1)

# use it to calculate adjusted CIFs
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="direct",
                      outcome_model=fgr_mod,
                      conf_int=FALSE)

# plot the curves
plot(adjcif)

# not run because it would be too slow

## using multiple imputation
library(mice)

# introduce random missingness in x1 as example
# NOTE: This is only done as an example, in reality you would
#       already have missing data, not introduce it yourself.
sim_dat$x1 &lt;- ifelse(runif(n=50) &lt; 0.5, sim_dat$x1, NA)

# perform multiple imputation
mids &lt;- mice::mice(data=sim_dat, method="pmm", m=5, printFlag=FALSE)

# fit model for each imputed dataset, using the CSC_MI helper function
mira &lt;- CSC_MI(mids, Hist(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group)

# calculate adjusted CIFs on imputed data
adj &lt;- adjustedcif(data=mids,
                   variable="group",
                   ev_time="time",
                   event="event",
                   method="direct",
                   cause=1,
                   outcome_model=mira)
plot(adj)

</code></pre>

<hr>
<h2 id='cif_direct_pseudo'>
Direct Adjusted CIFs using Pseudo-Values
</h2><span id='topic+cif_direct_pseudo'></span>

<h3>Description</h3>

<p>This page explains the details of estimating direct adjusted cumulative incidence functions using pseudo-values in a competing risks setting (<code>method="direct_pseudo"</code> in the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function). All regular arguments of the <code>adjustedcif</code> function can be used. Additionally, the <code>outcome_vars</code> argument has to be specified in the <code>adjustedcif</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="cif_direct_pseudo_+3A_outcome_vars">outcome_vars</code></td>
<td>

<p>[<strong>required</strong>] A character vector of column names specifying variables to be used when modeling the outcome mechanism. See details and examples.
</p>
</td></tr>
<tr><td><code id="cif_direct_pseudo_+3A_type_time">type_time</code></td>
<td>

<p>A character string specifying how the time should be modeled. Possible values are <code>"factor"</code> (modeling each point in time as a separate variable, the default), <code>"bs"</code> (modeling time using B-Splines) or <code>"ns"</code> (modeling time using natural splines).
</p>
</td></tr>
<tr><td><code id="cif_direct_pseudo_+3A_spline_df">spline_df</code></td>
<td>

<p>The number of degrees of freedom used for the natural-spline or B-spline function. Defaults to 5. Ignored if <code>type_time="factor"</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Requires a character vector of variable names used to model the outcome mechanism (internally uses <code><a href="geepack.html#topic+geese">geese</a></code>).
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Any number of levels in <code>variable</code> are allowed. Must be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are not available. Bootstrapping can still be used to estimate the confidence intervals (see <code>?adjustedcif</code>).
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are not guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are not guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the <span class="pkg">geepack</span> and <span class="pkg">prodlim</span> packages.
</p>
</li></ul>

<p>This method works by executing the following steps: (1) First Pseudo-Values for the cause-specific cumulative incidence function are estimated for each observation in the dataset and some points in time T. Afterwards (2) a new dataset is created in which every individual observation has multiple rows, one for each point in time of interest. (3) This dataset is used to fit a generalized estimating equations (geese) model, using the Pseudo-Values as independent variable. Next (4) multiple copies of the new dataset are created, one for each possible level of the <code>variable</code> of interest. (5) The <code>variable</code> is then set to one level for all observations in each dataset. (5) The <code><a href="geepack.html#topic+geese">geese</a></code> model is used to predict the CIF at some points in time T for each observation in all dataset copies. (6) Those estimated probabilities are averaged for each dataset at each point in time, resulting in adjusted CIFs for all levels of the group variable at the specified points in time.
</p>
<p>It is essentially the same procedure as described in <code><a href="#topic+cif_direct">&quot;direct&quot;</a></code>. The only difference is that instead of relying on a <code><a href="riskRegression.html#topic+CSC">CSC</a></code> model, this method uses Pseudo-Values and a geese model.
</p>
<p>When estimating the <code>geese</code> model the <code>ev_time</code> variable is used as a factor by default. This results in one coefficient being estimated for each unique point in time, which can be very slow computationally if there are a lot of unique points in time and/or the dataset has many rows. In these cases it is recommended to use <code>type_time="bs"</code> or <code>type_time="ns"</code>, which results in the <code>ev_time</code> being modeled using B-Splines or Natural Splines. Simulation studies indicate that there is little difference in the estimates when an appropriately large number of <code>spline_df</code> is used.
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedcif</code> function:
</p>

<ul>
<li> <p><code>pseudo_values</code>: The matrix of estimated pseudo-values.
</p>
</li>
<li> <p><code>geese_model</code>: The geese model used to make the predictions.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>Per Kragh Andersen, Elisavet Syriopoulou, and Erik T. Parner (2017). &quot;Causal Inference in Survival Analysis using Pseudo-Observations&quot;. In: Statistics in Medicine 36, pp. 2669-2681
</p>
<p>Per Kragh Andersen and Maja Pohar Perme (2010). &quot;Pseudo-Observations in Survival Analysis&quot;. In: Statistical Methods in Medical Research 19, pp. 71-99
</p>
<p>Aris Perperoglou, Willi Sauerbrei, Michal Abrahamowicz, and Matthias Schmid (2019). &quot;A Review of Spline Function Procedures in R&quot;. in: BMC Medical Research Methodology 19.46, pp. 1-16
</p>


<h3>See Also</h3>

<p><code><a href="geepack.html#topic+geese">geese</a></code>, <code><a href="prodlim.html#topic+jackknife">jackknife</a></code>, <code><a href="splines.html#topic+ns">ns</a></code>, <code><a href="splines.html#topic+bs">bs</a></code>, <code><a href="riskRegression.html#topic+CSC">CSC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_crisk(n=30, max_t=1.3)
sim_dat$group &lt;- as.factor(sim_dat$group)

# calculate adjusted CIFs, with time as factor
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="direct_pseudo",
                      outcome_vars=c("x1", "x2", "x3", "x4", "x5", "x6"),
                      type_time="factor",
                      force_bounds=TRUE,
                      iso_reg=TRUE)
plot(adjcif)

# with time modelled as B-Spline using 5 degrees of freedom
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="direct_pseudo",
                      outcome_vars=c("x1", "x2", "x3", "x4", "x5", "x6"),
                      type_time="bs",
                      spline_df=5,
                      force_bounds=TRUE,
                      iso_reg=TRUE)

# plot the curves
plot(adjcif)
</code></pre>

<hr>
<h2 id='cif_iptw'>
Inverse Probability of Treatment Weighted CIFs
</h2><span id='topic+cif_iptw'></span>

<h3>Description</h3>

<p>This page explains the details of estimating inverse probability of treatment weighted cumulative incidence functions in a competing risks setting (<code>method="iptw"</code> in the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function). All regular arguments of the <code>adjustedcif</code> function can be used. Additionally, the <code>treatment_model</code> argument has to be specified in the <code>adjustedcif</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="cif_iptw_+3A_treatment_model">treatment_model</code></td>
<td>

<p>[<strong>required</strong>] Must be a <code>glm</code> or <code>multinom</code> model object with <code>variable</code> as response variable.
</p>
</td></tr>
<tr><td><code id="cif_iptw_+3A_censoring_model">censoring_model</code></td>
<td>

<p>Either <code>NULL</code> (default) to make no adjustments for dependent censoring, or a <code>coxph</code> object. See <code>?ate</code> for more details.
</p>
</td></tr>
<tr><td><code id="cif_iptw_+3A_verbose">verbose</code></td>
<td>

<p>Whether to print estimation information of the <code>ate</code> function in the <span class="pkg">riskRegression</span> package. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="cif_iptw_+3A_...">...</code></td>
<td>

<p>Further arguments passed to <code><a href="riskRegression.html#topic+ate">ate</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Requires a model describing the treatment assignment mechanism. This must be either a <code><a href="stats.html#topic+glm">glm</a></code> or a <code><a href="mgcv.html#topic+multinom">multinom</a></code> object.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Any number of levels in <code>variable</code> are allowed. Must be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are available.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the <span class="pkg">riskRegression</span> package
</p>
</li></ul>

<p>This method works by modeling the treatment assignment mechanism. Adjusted CIFs are calculated by first estimating appropriate case-weights for each observation in <code>data</code>. Those weights are used in a weighted version of the Aalen-Johansen estimator. If the weights are correctly estimated the resulting estimates will be unbiased. A more detailed description can be found in Neumann et al. (2016) and Choi et al. (2019). By utilizing another set of weights, this function can also correct the estimates for covariate-dependent censoring (Ozenne et al. 2020). Asymptotic variance calculations are based on the efficient influence curve.
</p>
<p>Internally, this function simply calls the <code><a href="riskRegression.html#topic+ate">ate</a></code> function with appropriate arguments. The three-dot syntax can be used to pass further arguments to that function. It is however recommended to use the <code><a href="riskRegression.html#topic+ate">ate</a></code> function directly when specific settings are required.
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedcif</code> function:
</p>

<ul>
<li> <p><code>ate_object</code>: The object returned by the <code>ate</code> function.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>The wrapper function was written by Robin Denz, the <code><a href="riskRegression.html#topic+ate">ate</a></code> function itself was written by other people. See <code>?ate</code> for more information.
</p>


<h3>References</h3>

<p>Anke Neumann and Cécile Billionnet (2016). &quot;Covariate Adjustment of Cumulative Incidence Functions for Competing Risks Data Using Inverse Probability of Treatment Weighting&quot;. In: Computer Methods and Programs in Biomedicine 129, pp. 63-70
</p>
<p>Sangbum Choi, Chaewon Kim, Hua Zhong, Eun-Seok Ryu, and Sung Won Han (2019). &quot;Adjusted-Crude-Incidence Analysis of Multiple Treatments and Unbalanced Samples on Competing Risks&quot;. In: Statistics and Its Inference 12, pp. 423-437
</p>
<p>Brice Maxime Hugues Ozenne, Thomas Harder Scheike, and Laila Stærk (2020). &quot;On the Estimation of Average Treatment Effects with Right-Censored Time to Event Outcome and Competing Risks&quot;. In: Biometrical Journal 62, pp. 751-763
</p>


<h3>See Also</h3>

<p><code><a href="riskRegression.html#topic+ate">ate</a></code>, <code><a href="stats.html#topic+glm">glm</a></code>, <code><a href="nnet.html#topic+multinom">multinom</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_crisk(n=50, max_t=5)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a treatment assignment model
glm_mod &lt;- glm(group ~ x1 + x3 + x5 + x6, data=sim_dat, family="binomial")

# use it to calculate adjusted CIFs
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="iptw",
                      treatment_model=glm_mod)
plot(adjcif)
</code></pre>

<hr>
<h2 id='cif_iptw_pseudo'>
Inverse Probability of Treatment Weighted CIFs using Pseudo-Values
</h2><span id='topic+cif_iptw_pseudo'></span>

<h3>Description</h3>

<p>This page explains the details of estimating inverse probability of treatment weighted cumulative incidence functions using Pseudo-Values in a competing risks setting (<code>method="iptw_pseudo"</code> in the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function). All regular arguments of the <code>adjustedcif</code> function can be used. Additionally, the <code>treatment_model</code> argument has to be specified in the <code>adjustedcif</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="cif_iptw_pseudo_+3A_treatment_model">treatment_model</code></td>
<td>

<p>[<strong>required</strong>] Must be either a model object with <code>variable</code> as response variable, a vector of weights or a formula which can be passed to <code>WeightIt</code>.
</p>
</td></tr>
<tr><td><code id="cif_iptw_pseudo_+3A_weight_method">weight_method</code></td>
<td>

<p>Method used in <code>WeightIt</code> function call. Ignored if <code>treatment_model</code> is not a formula object. Defaults to <code>"ps"</code>.
</p>
</td></tr>
<tr><td><code id="cif_iptw_pseudo_+3A_stabilize">stabilize</code></td>
<td>

<p>Whether to stabilize the weights or not. Is set to <code>FALSE</code> by default. Stabilizing weights ensures that the sum of all weights is equal to the original sample size. It has no effect on point estimates, only on the asymptotic variance calculations and confidence intervals.
</p>
</td></tr>
<tr><td><code id="cif_iptw_pseudo_+3A_trim">trim</code></td>
<td>

<p>Can be either <code>FALSE</code> (default) or a numeric value at which to trim the weights. If <code>FALSE</code>, weights are used as calculated or supplied. If a numeric value is supplied, all weights that are bigger than <code>trim</code> are set to <code>trim</code> before the analysis is carried out. Useful when some weights are extremely large.
</p>
</td></tr>
<tr><td><code id="cif_iptw_pseudo_+3A_se_method">se_method</code></td>
<td>

<p>One of <code>"miller"</code>, <code>"galloway"</code>, <code>"cochrane"</code> and <code>"Hmisc"</code>. Specifies which kind of standard error to calculate. Defaults to <code>"cochrane"</code>. See details.
</p>
</td></tr>
<tr><td><code id="cif_iptw_pseudo_+3A_...">...</code></td>
<td>

<p>Further arguments passed to <code><a href="WeightIt.html#topic+weightit">weightit</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Requires a model describing the treatment assignment mechanism. This must be either a <code><a href="stats.html#topic+glm">glm</a></code> or <code><a href="mgcv.html#topic+multinom">multinom</a></code> object. Alternatively, weights can be supplied directly or estimated using <code>WeightIt</code>
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Any number of levels in <code>variable</code> are allowed. Must be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are available.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are not guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are not guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the <span class="pkg">prodlim</span> package. The <span class="pkg">WeightIt</span> package is also required if <code>treatment_model</code> is a formula object.
</p>
</li></ul>

<p>This method works by modeling the treatment assignment mechanism. Adjusted CIFs are calculated by first estimating appropriate case-weights for each observation in <code>data</code>. This can be done using inverse probability of treatment weights using the propensity score (usually estimated using a logistic regression model) or by some other method (see <code><a href="WeightIt.html#topic+weightit">weightit</a></code>). Pseudo-Values of the cause-specific CIF are then calculated for every observation in <code>data</code> at some points in time <code class="reqn">T</code>. Since Pseudo-Values bypass the problem of censoring, a simple weighted average of the Pseudo-Values can be taken for every <code class="reqn">T</code>. See Andersen et al. (2017) for more details on this method and Andersen and Perme (2010) for more information on Pseudo-Values in general.
</p>
<p>The standard error of this estimator can be approximated by calculation a weighted version of the standard error estimator. Interestingly, no exact method exists in the weighted case. Four approximations are implemented which can be chosen using the <code>se_method</code> argument. The equations for <code>"miller"</code>, <code>"galloway"</code> and <code>"cochrane"</code> are described and compared in Gatz and Smith (1995). <code>"Hmisc"</code> is the standard equation with a weight term added, as specified in the <span class="pkg">Hmisc</span> package, and should only be used with stabilized weights (<code>stabilize=TRUE</code>). It is generally recommended to use bootstrap estimates instead.
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedcif</code> function:
</p>

<ul>
<li> <p><code>pseudo_values</code>: The matrix of estimated pseudo-values.
</p>
</li>
<li> <p><code>weights</code>: The final weights used in the analysis.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>Per Kragh Andersen, Elisavet Syriopoulou, and Erik T. Parner (2017). &quot;Causal Inference in Survival Analysis using Pseudo-Observations&quot;. In: Statistics in Medicine 36, pp. 2669-2681
</p>
<p>Per Kragh Andersen and Maja Pohar Perme (2010). &quot;Pseudo-Observations in Survival Analysis&quot;. In: Statistical Methods in Medical Research 19, pp. 71-99
</p>
<p>Donald F. Gatz and Luther Smith (1995). &quot;The Standard Error of a Weighted Mean Concentration - I: Bootstrapping Vs Other Methods&quot;. In: Atmospheric Environment 29.11, pp. 1185-1193
</p>
<p>William G. Cochran (1977). Sampling Techniques. Vol. 3. New York: Wiley
</p>
<p>J. N. Galloway, G. E. Likens, and M. E. Hawley (1984). &quot;Acid Precipitation: Natural Versus Anthropogenic Components&quot;. In: Science 226, pp. 829-831
</p>
<p>J. M. Miller (1977). A Statistical Evaluation of the U.S. Precipitation Chemistry Network. Precipitation Scavenging (edited by Semonin R. G. and Beadle R. W.) pp. 639-659. Available as CONF 74100 from National Technical Information Service, U.S. Dept. of Commerce, Springfiel, VA.
</p>


<h3>See Also</h3>

<p><code><a href="WeightIt.html#topic+weightit">weightit</a></code>, <code><a href="prodlim.html#topic+prodlim">prodlim</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_crisk(n=50, max_t=5)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a treatment assignment model
glm_mod &lt;- glm(group ~ x1 + x3 + x5 + x6, data=sim_dat, family="binomial")

# use it to calculate adjusted CIFs
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="iptw_pseudo",
                      treatment_model=glm_mod)
plot(adjcif, force_bounds=TRUE, iso_reg=TRUE)

# Alternatively, use custom weights
# In this example we use weights calculated using the propensity score,
# which is equal to using the glm model directly in the function
ps_score &lt;- glm_mod$fitted.values
weights &lt;- ifelse(sim_dat$group==1, 1/ps_score, 1/(1-ps_score))

adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="iptw_pseudo",
                      treatment_model=weights)
plot(adjcif, force_bounds=TRUE, iso_reg=TRUE)

# And a third alternative: use the WeightIt package
# here an example with equal results to the ones above:
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="iptw_pseudo",
                      treatment_model=group ~ x1 + x3 + x5 + x6,
                      weight_method="ps")
plot(adjcif, force_bounds=TRUE, iso_reg=TRUE)

# here an example using Entropy Balancing Weighting:
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="iptw_pseudo",
                      treatment_model=group ~ x1 + x3 + x5 + x6,
                      weight_method="ebal")
plot(adjcif, force_bounds=TRUE, iso_reg=TRUE)
</code></pre>

<hr>
<h2 id='cif_matching'>
Using Propensity-Score Matching to Calculate Adjusted CIFs
</h2><span id='topic+cif_matching'></span>

<h3>Description</h3>

<p>This page explains the details of estimating adjusted cumulative incidence functions using propensity-score matching in a competing risks setting (<code>method="matching"</code> in the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function). All regular arguments of the <code>adjustedcif</code> function can be used. Additionally, the <code>treatment_model</code> argument has to be specified in the <code>adjustedcif</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="cif_matching_+3A_treatment_model">treatment_model</code></td>
<td>

<p>[<strong>required</strong>] Must be either a model object with <code>variable</code> as response variable or a vector of previously estimated propensity scores.
</p>
</td></tr>
<tr><td><code id="cif_matching_+3A_gtol">gtol</code></td>
<td>

<p>Tolerance at which estimated treatment assignment probabilities are truncated. Every propensity score bigger than 1 - <code>gtol</code> is set to 1 - <code>gtol</code> and every propensity score smaller than <code>gtol</code> is set to <code>gtol</code>. Useful when there are extreme propensity scores close to 0 or 1. Defaults to 0.001,
</p>
</td></tr>
<tr><td><code id="cif_matching_+3A_...">...</code></td>
<td>

<p>Further arguments passed to the <code>Match</code> function of the <span class="pkg">Matching</span> Package.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Requires a model describing the treatment assignment mechanism. This must be either a <code><a href="stats.html#topic+glm">glm</a></code> object or a vector of propensity scores.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Only two groups in <code>variable</code> are allowed. Must be a factor variable with exactly two levels.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are currently not available. Bootstrapping can still be used to estimate the confidence intervals (see <code>?adjustedcif</code>).
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on both the <span class="pkg">Matching</span> and the <span class="pkg">cmprsk</span> packages.
</p>
</li></ul>

<p>Using the estimated propensity score, the individual observations in the dataset are matched to each other creating a new dataset in which the covariate distributions are balanced in respect to the two groups defined by <code>variable</code>. A simple Aalen-Johansen estimator is then used to calculate the confounder-adjusted CIFs. This corresponds to the method described in Austin &amp; Fine (2019). Details on the algorithm used for matching can be found in the documentation of the <span class="pkg">Matching</span> package.
</p>
<p>Simulation results showed that this specific implementation of this method is the least efficient method contained in this R-Package. While it does produce unbiased estimates, the variation in these estimates is very high. We strongly suggest using one of the other methods implemented here.
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedcif</code> function:
</p>

<ul>
<li> <p><code>match_object</code>: The object creates using the <code>Match</code> function.
</p>
</li>
<li> <p><code>cuminc_object</code>: The <code>cuminc</code> object fit on the matched data.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>Peter C. Austin and Jason P. Fine (2019). &quot;Propensity-Score Matching with Competing Risks in Survival Analysis&quot;. In: Statistics in Medicine 38, pp. 751-777
</p>


<h3>See Also</h3>

<p><code><a href="Matching.html#topic+Match">Match</a></code>, <code><a href="cmprsk.html#topic+cuminc">cuminc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)
library(Matching)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_crisk(n=50, max_t=5)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate treatment assignment model
glm_mod &lt;- glm(group ~ x1 + x2 + x4 + x6, data=sim_dat, family="binomial")

# calculate adjusted CIFs
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="matching",
                      treatment_model=glm_mod)
plot(adjcif)

# Alternatively, supply the propensity score directly
# Here we use the logistic regression to calculate it, so we get
# exactly the same result. The propensity score can be calculated in
# any other way in practice, allowing flexibility
ps_score &lt;- glm_mod$fitted.values

adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      cause=1,
                      method="matching",
                      treatment_model=ps_score)

# plot the curves
plot(adjcif)
</code></pre>

<hr>
<h2 id='cif_tmle'>
Targeted Maximum Likelihood Estimation for Continuous Time Competing Events Data
</h2><span id='topic+cif_tmle'></span>

<h3>Description</h3>

<p>This page explains the details of estimating causal cause-specific cumulative incidence functions in a competing risks setting with targeted maximum likelihood estimation (<code>method="tmle"</code> in the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function). All regular arguments of the <code>adjustedcif</code> function can be used. Additionally, the <code>outcome_model</code> argument and the <code>treatment_model</code> argument have to be specified in the <code>adjustedcif</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="cif_tmle_+3A_outcome_model">outcome_model</code></td>
<td>

<p>[<strong>required</strong>] Should be a list containing at least one Cox model formula for the desired cause. In those Cox model formula, the time variable is always called <code>time</code> and the status variable should always be called <code>status</code>. For example, to use just one Cox model for <code>cause = 1</code> including all variables of the dataset as independent variables the user can use <code>list(Surv(time, status==1) ~ .)</code>. If <code>cause = 2</code> is of interest it should be <code>list(Surv(time, status==2) ~ .)</code> instead. All Cox models defined in that list are fitted to the data and the ensemble of these models is used to provide the initial predictions for the conditional hazard. See details and the documentation of the <span class="pkg">concrete</span> package for more information.
</p>
</td></tr>
<tr><td><code id="cif_tmle_+3A_treatment_model">treatment_model</code></td>
<td>

<p>[<strong>required</strong>] A character vector specifying which <span class="pkg">SuperLearner</span> libraries should be used to obtain an estimate of the propensity score. For example, <code>c("SL.glm", "SL.glmnet")</code> could be used. See <code>?SuperLearner</code> for more details.
</p>
</td></tr>
<tr><td><code id="cif_tmle_+3A_censoring_model">censoring_model</code></td>
<td>

<p>Either <code>NULL</code> (default) to make no adjustments for dependent censoring, or a list of Cox models as described in the <code>outcome_model</code> argument. The only difference between this and the <code>outcome_model</code> argument is that <code>status==0</code> should be used in the Cox formulas. See below or <code>?formatArguments</code> in the <span class="pkg">concrete</span> package for more details.
</p>
</td></tr>
<tr><td><code id="cif_tmle_+3A_cv_args">cv_args</code></td>
<td>

<p>A list of arguments specifying how exactly cross-validation should be performed. Internally passed to the <code>CVArg</code> argument of the <code>formatArguments</code> function in the <span class="pkg">concrete</span> package.
</p>
</td></tr>
<tr><td><code id="cif_tmle_+3A_max_update_iter">max_update_iter</code></td>
<td>

<p>A single positive integer specifying the maximum iterations performed to obtain the estimates. Defaults to 500. Internally passed to the <code>MaxUpdateIter</code> argument of the <code>formatArguments</code> function in the <span class="pkg">concrete</span> package.
</p>
</td></tr>
<tr><td><code id="cif_tmle_+3A_one_step_eps">one_step_eps</code></td>
<td>

<p>A single positive number specifying the step size of the tmle updates. Defaults to 0.1. Internally passed to the <code>OneStepEps</code> argument of the <code>formatArguments</code> function in the <span class="pkg">concrete</span> package.
</p>
</td></tr>
<tr><td><code id="cif_tmle_+3A_min_nuisance">min_nuisance</code></td>
<td>

<p>A single number between 0 and 1 used for truncating the g-related denominator of the clever covariate. Defaults to <code>5/sqrt(nrow(data))/log(nrow(data))</code>. Internally passed to the <code>MinNuisance</code> argument of the <code>formatArguments</code> function in the <span class="pkg">concrete</span> package.
</p>
</td></tr>
<tr><td><code id="cif_tmle_+3A_verbose">verbose</code></td>
<td>

<p>Whether to print estimation information of the <code>doConcrete</code> function in the <span class="pkg">concrete</span> package. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="cif_tmle_+3A_return_models">return_models</code></td>
<td>

<p>Whether to add the estimated models for the outcome, treatment, and censoring mechanism to the output object. Defaults to <code>TRUE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Requires a model describing the treatment assignment mechanism and the outcome mechanism, also allows a model for the censoring mechanism. See details and the <span class="pkg">concrete</span> package.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> This function currently only allows two levels in <code>variable</code>.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are available.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the <span class="pkg">concrete</span> package, the <span class="pkg">data.table</span> package and all of their respective dependencies.
</p>
</li></ul>

<p><strong><em>What it does:</em></strong>
</p>
<p>This function implements Targeted Maximum Likelihood Estimation (TMLE) for continuously distributed competing-events data as described in Rytgaard et al. (2023) and Rytgaard and van der Laan (2022). The TMLE method is similar to the AIPTW methods included in this package. It also relies on both an outcome model and a treatment model (with an additional optional censoring model) to obtain the counterfactual failure probability estimates. In contrast to the AIPTW methods, however, the estimator uses an iterative approach to obtain the estimates where each update targets the entire cumulative incidence function. As a consequence, the resulting estimates are guaranteed to lie in the 0/1 probability bounds and are also guaranteed to be non-decreasing over time. Simulation studies and theoretical results indicate a good performance of this method in terms of bias and standard errors. See the cited literature for more detailed and more rigorous explanations of the method.
</p>
<p>Instead of relying on a single model to obtain the propensity score or the initial conditional hazards estimates, this estimator relies on the <span class="pkg">SuperLearner</span> framework in conjunction with cross-validation to do this. How cross-validation should be performed may be controlled with the <code>cv_args</code> argument. The resulting models can be inspected from the output object if <code>return_models</code> is set to <code>TRUE</code>.
</p>
<p><strong><em>The Implementation:</em></strong>
</p>
<p>Internally, this function simply calls multiple functions of the <span class="pkg">concrete</span> package in correct order with appropriate arguments. This wrapper function is limited in the sense that it does not allow dynamic interventions or time-varying variables, which are supported by the <span class="pkg">concrete</span> package. It is recommended to use the <span class="pkg">concrete</span> package directly when the user wants to use these features or other specific settings are required.
</p>
<p><strong><em>Speed Considerations:</em></strong>
</p>
<p>This method is very computationally expensive. For medium to large datasets and when considering many different points in time, it will usually take a very long time to execute. If speed is important, we recommend using other methods. Alternatively, user may adjust the <code>times</code> arguments to target fewer points in time.
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedcif</code> function:
</p>

<ul>
<li> <p><code>concrete_object</code>: The object returned by the <code>doConcrete</code> function.
</p>
</li></ul>



<h3>Note</h3>

<p>A previous version of this package (&lt;= 0.9.1) included a function with the same name, which was removed in version 0.10.0. The old version implemented a TMLE estimator that was only applicable to discrete-time survival data based on the <span class="pkg">survtmle</span> package, which was removed from CRAN. The new version implements a different estimator. Code using this method for version &lt;= 0.9.1 does NOT work with versions 0.10.2 or higher.
</p>


<h3>Author(s)</h3>

<p>The wrapper function was written by Robin Denz, but the real estimation functions are all contained in the <span class="pkg">concrete</span> package, which was written by David Chen. See <code>?doConcrete</code> for more information.
</p>


<h3>References</h3>

<p>Helene C. W. Rytgaard and Mark J. van der Laan (2023). &quot;Targeted Maximum Likelihood Estimation for Causal Inference in Survival and Competing Risks Analysis&quot;. In: Lifetime Data Analysis
</p>
<p>Helene C. W. Rytgaard and Mark J. van der Laan (2023). &quot;One-Step Targeted Maximum Likelihood Estimation for Targeting Cause-Specific Absolute Risks and Survival Curves&quot;. In: Biometrika
</p>
<p>Helene C. W. Rytgaard, Frank Eriksson and Mark J. van der Laan (2023). &quot;Estimation of Time-Specific Intervention Effects on Continuously Distributed Time-To-Event Outcomes by Targeted Maximum Likelihood Estimation&quot;. In: Biometrics
</p>
<p>David Chen, Helene C. W. Rytgaard and Edwin Fong and Jens M. Tarp and Maya L. Petersen and Mark J. van der Laan and Thomas A. Gerds (2023). &quot;concrete: An R Package for Continuous-Time, Competing Risks Targeted Maximum Likelihood Estimation&quot;. Available at &lt;https://github.com/imbroglio-dc/concrete&gt; or on CRAN
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjustedcif">adjustedcif</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)

data &lt;- sim_confounded_crisk(n=100)
data$group &lt;- factor(data$group)

# for a single point in time using only one model for both
# the treatment mechanism and outcome mechanism
out &lt;- adjustedcif(data=data,
                   variable="group",
                   ev_time="time",
                   event="event",
                   cause=1,
                   treatment_model=c("SL.glm"),
                   outcome_model=list(Surv(time, status==1) ~ .),
                   times=c(0.5),
                   conf_int=TRUE,
                   method="tmle")

## using multiple models for both the treatment assignment and
## outcome mechanism
out &lt;- adjustedcif(data=data,
                   variable="group",
                   ev_time="time",
                   event="event",
                   cause=1,
                   treatment_model=c("SL.glm", "SL.mean"),
                   outcome_model=list(Surv(time, status==1) ~ x1 + x3,
                                      Surv(time, status==1) ~ x2 + x4 + x5),
                   times=c(0.5),
                   conf_int=TRUE,
                   method="tmle")

## with corrections for covariate dependent censoring
out &lt;- adjustedcif(data=data,
                   variable="group",
                   ev_time="time",
                   event="event",
                   cause=1,
                   treatment_model=c("SL.glm", "SL.mean"),
                   outcome_model=list(Surv(time, status==1) ~ x1 + x3,
                                      Surv(time, status==1) ~ x2 + x4 + x5),
                   censoring_model=list(Surv(time, status==0) ~ x6 + x1),
                   times=c(0.5),
                   conf_int=TRUE,
                   method="tmle")
</code></pre>

<hr>
<h2 id='CSC_MI'>
Cause-Specific Cox Regression with Multiple Imputation
</h2><span id='topic+CSC_MI'></span>

<h3>Description</h3>

<p>This function can be utilized to perform Cause-Specific Cox Regression on multiply imputed datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CSC_MI(mids, formula, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CSC_MI_+3A_mids">mids</code></td>
<td>

<p>A <code>mids</code> object created using the <code><a href="mice.html#topic+mice">mice</a></code> function. This replaces the <code>data</code> argument in the original function call.
</p>
</td></tr>
<tr><td><code id="CSC_MI_+3A_formula">formula</code></td>
<td>

<p>A formula object passed to the <code><a href="riskRegression.html#topic+CSC">CSC</a></code> function in the <span class="pkg">riskRegression</span> package.
</p>
</td></tr>
<tr><td><code id="CSC_MI_+3A_...">...</code></td>
<td>

<p>Other arguments which should be passed to the <code><a href="riskRegression.html#topic+CSC">CSC</a></code> function in the <span class="pkg">riskRegression</span> package.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A small convenience function to perform CSC regression on multiply imputed data. It is simply a wrapper around the <code><a href="riskRegression.html#topic+CSC">CSC</a></code> function from the <span class="pkg">riskRegression</span> package, because the usual use of <code>with</code> is not supported directly. It returns a <code>mira</code> object, which can be passed to the <code>outcome_model</code> argument inside of the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function when needed. No <code>pool</code> method or other functionality is available.
</p>


<h3>Value</h3>

<p>A <code>mira</code> object containing the CSC regression for every imputed dataset.
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjustedsurv">adjustedsurv</a></code>, <code><a href="riskRegression.html#topic+CSC">CSC</a></code>, <code><a href="mice.html#topic+mice">mice</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># not run because it would be too slow

library(adjustedCurves)
library(survival)
library(riskRegression)
library(mice)

# simulate some data as example
sim_dat &lt;- sim_confounded_crisk(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# introduce random missingness in x1 as example
sim_dat$x1 &lt;- ifelse(runif(n=50) &lt; 0.5, sim_dat$x1, NA)

# perform multiple imputation
mids &lt;- mice::mice(data=sim_dat, method="pmm", m=5)

# use the function
csc_mods &lt;- CSC_MI(mids=mids,
                   formula=Hist(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group
                   )

</code></pre>

<hr>
<h2 id='FGR_MI'>
Fine &amp; Gray Model with Multiple Imputation
</h2><span id='topic+FGR_MI'></span>

<h3>Description</h3>

<p>This function can be utilized to calculate Fine &amp; Gray models for multiply imputed datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FGR_MI(mids, formula, cause=1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FGR_MI_+3A_mids">mids</code></td>
<td>

<p>A <code>mids</code> object created using the <code><a href="mice.html#topic+mice">mice</a></code> function. This replaces the <code>data</code> argument in the original function call.
</p>
</td></tr>
<tr><td><code id="FGR_MI_+3A_formula">formula</code></td>
<td>

<p>A formula object passed to the <code><a href="riskRegression.html#topic+FGR">FGR</a></code> function in the <span class="pkg">riskRegression</span> package.
</p>
</td></tr>
<tr><td><code id="FGR_MI_+3A_cause">cause</code></td>
<td>

<p>The failure type of interest. Defaults to 1.
</p>
</td></tr>
<tr><td><code id="FGR_MI_+3A_...">...</code></td>
<td>

<p>Other arguments which should be passed to the <code><a href="riskRegression.html#topic+FGR">FGR</a></code> function in the <span class="pkg">riskRegression</span> package.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A small convenience function to calculate Fine &amp; Gray models for multiply imputed data. It is simply a wrapper around the <code><a href="riskRegression.html#topic+FGR">FGR</a></code> function from the <span class="pkg">riskRegression</span> package, because the usual use of <code>with</code> is not supported directly. It returns a <code>mira</code> object, which can be passed to the <code>outcome_model</code> argument inside of the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function when needed. No <code>pool</code> method or other functionality is available.
</p>


<h3>Value</h3>

<p>A <code>mira</code> object containing the FGR regression for every imputed dataset.
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjustedsurv">adjustedsurv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># not run because it would be too slow

library(adjustedCurves)
library(survival)
library(riskRegression)
library(mice)
library(prodlim)

# simulate some data as example
sim_dat &lt;- sim_confounded_crisk(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# introduce random missingness in x1 as example
sim_dat$x1 &lt;- ifelse(runif(n=50) &lt; 0.5, sim_dat$x1, NA)

# perform multiple imputation
mids &lt;- mice::mice(data=sim_dat, method="pmm", m=5, printFlag=FALSE)

# use the function
fgr_mods &lt;- FGR_MI(mids=mids,
                   formula=Hist(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group,
                   cause=1)

</code></pre>

<hr>
<h2 id='models_cif_direct'>
List of supported models in <code>cif_direct</code>
</h2><span id='topic+models_cif_direct'></span>

<h3>Description</h3>

<p>Supported models for the <code>outcome_model</code> argument when using <code>method="direct"</code> in the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function.
</p>


<h3>Details</h3>

<p>The following models are directly supported in the <code>outcome_model</code> in the <code><a href="#topic+cif_direct">cif_direct</a></code> function. The first letter in parentheses after the object name is a group indicator. Below the list there are more information for each group.
</p>

<ul>
<li> <p><code><a href="riskRegression.html#topic+CSC">CSC</a></code> [<strong>A</strong>, Required Packages: <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code><a href="riskRegression.html#topic+FGR">FGR</a></code> [<strong>B</strong>, Required Packages: <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code><a href="riskRegression.html#topic+riskRegression">riskRegression</a></code> [<strong>B</strong>, Required Packages: <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code><a href="prodlim.html#topic+prodlim">prodlim</a></code> [<strong>B</strong>, Required Packages: <span class="pkg">prodlim</span>, <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code>rfsrc</code> [<strong>B</strong>, Required Packages: <span class="pkg">randomForestSRC</span>, <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code>ARR</code> [<strong>B</strong>, Required Packages: <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code>fit_hal</code> [<strong>B</strong>, Required Packages: <span class="pkg">hal9001</span>, <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code>fastCrr</code> [<strong>C</strong>, Required Packages: <span class="pkg">fastcmprsk</span>]
</p>
</li>
<li> <p><code><a href="timereg.html#topic+comp.risk">comp.risk</a></code> [<strong>C</strong>, Required Packages: <span class="pkg">timereg</span>]
</p>
</li>
<li><p> Any model with a fitting S3 prediction method or a valid <code>predict_fun</code> can be used as well. See below.
</p>
</li></ul>

<p><strong>Group A:</strong> The direct adjusted cumulative incidences are estimated directly using the <code><a href="riskRegression.html#topic+ate">ate</a></code> function. Additional arguments supplied using the <code>...</code> syntax are passed to the <code><a href="riskRegression.html#topic+ate">ate</a></code> function. <br />
<strong>Group B:</strong> The <code><a href="riskRegression.html#topic+predictRisk">predictRisk</a></code> function is used to obtain predicted cumulative incidences, which are then used in the G-Computation step. Additional arguments supplied using the <code>...</code> syntax are passed to the <code><a href="riskRegression.html#topic+predictRisk">predictRisk</a></code> function.<br />
<strong>Group C:</strong> Custom code is used to do the estimation. Additional arguments supplied using the <code>...</code> syntax are currently not supported.<br />
</p>
<p>It is sometimes possible to use models even if they are not listed here. There are two ways to make this work. The first one is to use the models S3 <code>predict</code> method. This works if the <code>predict</code> function contains the arguments <code>object</code>, <code>newdata</code>, <code>times</code> and <code>cause</code> and returns a matrix of predicted cause-specific cumulative incidences. The matrix should be of size <code>nrow(data) * length(times)</code>, where each row corresponds to a row in the original dataset and each column to one point in time. The matrix should contain the cause-specific cumulative incidences predicted by the model given covariates. If no such <code>predict</code> method exists the only option left is to write your own function which produces the output described above and supply this function to the <code>predict_fun</code> argument.
</p>
<p>If you think that some important models are missing from this list, please file an issue on the official github page with a specific feature request (URL can be found in the DESCRIPTION file) or contact the package maintainer directly using the given e-mail address.
</p>


<h3>Note</h3>

<p>When using outcome models which are not directly supported (either through the default predict method or a custom <code>predict_fun</code>) it might be necessary to set the <code>clean_data</code> argument of the <code>adjustedcif</code> function to <code>FALSE</code>.
</p>

<hr>
<h2 id='models_surv_direct'>
List of supported models in <code>surv_direct</code>
</h2><span id='topic+models_surv_direct'></span>

<h3>Description</h3>

<p>Supported models for the <code>outcome_model</code> argument when using <code>method="direct"</code> in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function.
</p>


<h3>Details</h3>

<p>The following models are directly supported in the <code>outcome_model</code> in the <code><a href="#topic+surv_direct">surv_direct</a></code> function. The first letter in parentheses after the object name is a group indicator. Below the list there are more information for each group.
</p>

<ul>
<li> <p><code><a href="survival.html#topic+coxph">coxph</a></code> [<strong>A</strong>, Required Packages: <span class="pkg">survival</span>, <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code><a href="rms.html#topic+cph">cph</a></code> [<strong>A</strong>, Required Packages: <span class="pkg">rms</span>, <span class="pkg">survival</span>, <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code><a href="timereg.html#topic+aalen">aalen</a></code> [<strong>B</strong>, Required Packages: <span class="pkg">timereg</span>, <span class="pkg">pec</span>]
</p>
</li>
<li> <p><code><a href="timereg.html#topic+cox.aalen">cox.aalen</a></code> [<strong>B</strong>, Required Packages: <span class="pkg">timereg</span>, <span class="pkg">pec</span>]
</p>
</li>
<li> <p><code><a href="riskRegression.html#topic+selectCox">selectCox</a></code> [<strong>B</strong>, Required Packages: <span class="pkg">riskRegression</span>, <span class="pkg">pec</span>]
</p>
</li>
<li> <p><code><a href="pec.html#topic+pecCforest">pecCforest</a></code> [<strong>B</strong>, Required Packages: <span class="pkg">pec</span>]
</p>
</li>
<li> <p><code><a href="pec.html#topic+pecRpart">pecRpart</a></code> [<strong>B</strong>, Required Packages: <span class="pkg">pec</span>, Bootstrapping not allowed.]
</p>
</li>
<li> <p><code><a href="riskRegression.html#topic+riskRegression">riskRegression</a></code> [<strong>C</strong>, Required Packages: <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code><a href="prodlim.html#topic+prodlim">prodlim</a></code> [<strong>C</strong>, Required Packages: <span class="pkg">prodlim</span>, <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code><a href="rms.html#topic+psm">psm</a></code> [<strong>C</strong>, Required Packages: <span class="pkg">rms</span>, <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code>flexsurvreg</code> [<strong>C</strong>, Required Packages: <span class="pkg">flexsurv</span>, <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code>flexsurvspline</code> [<strong>C</strong>, Required Packages: <span class="pkg">flexsurv</span>, <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code><a href="ranger.html#topic+ranger">ranger</a></code> [<strong>C</strong>, Required Packages: <span class="pkg">ranger</span>, <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code>rfsrc</code> [<strong>C</strong>, Required Packages: <span class="pkg">randomForestSRC</span>, <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code>ARR</code> [<strong>C</strong>, Required Packages: <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code><a href="riskRegression.html#topic+penalizedS3">penalizedS3</a></code> [<strong>C</strong>, Required Packages: <span class="pkg">penalized</span>, <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code>gbm</code> [<strong>C</strong>, Required Packages: <span class="pkg">gbm</span>, <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code>fit_hal</code> [<strong>C</strong>, Required Packages: <span class="pkg">hal9001</span>, <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code>fitSmoothHazard</code> [<strong>C</strong>, Required Packages: <span class="pkg">casebase</span>, <span class="pkg">riskRegression</span>]
</p>
</li>
<li> <p><code><a href="stats.html#topic+glm">glm</a></code> [<strong>D</strong>, Required Packages: <span class="pkg">stats</span>, <span class="pkg">pec</span>]
</p>
</li>
<li> <p><code><a href="rms.html#topic+ols">ols</a></code> [<strong>D</strong>, Required Packages: <span class="pkg">rms</span>, <span class="pkg">pec</span>]
</p>
</li>
<li> <p><code>randomForest</code> [<strong>D</strong>, Required Packages: <span class="pkg">randomForest</span>, <span class="pkg">pec</span>]
</p>
</li>
<li> <p><code>mexhaz</code> [<strong>E</strong>, Required Packages: <span class="pkg">mexhaz</span>]
</p>
</li>
<li><p> Any model with a fitting S3 prediction method or a valid <code>predict_fun</code> can be used as well. See below.
</p>
</li></ul>

<p><strong>Group A:</strong> The direct adjusted survival probabilities are estimated directly using the <code><a href="riskRegression.html#topic+ate">ate</a></code> function. Additional arguments supplied using the <code>...</code> syntax are passed to the <code><a href="riskRegression.html#topic+ate">ate</a></code> function. Note that <code>Surv()</code> calls required to fit the model should be made inside the formula, not saved elsewhere. <br />
<strong>Group B:</strong> Predicted survival probabilities are obtained using the <code><a href="pec.html#topic+predictSurvProb">predictSurvProb</a></code> function. The G-Computation is carried out using those. Additional arguments supplied using the <code>...</code> syntax are passed to the <code><a href="pec.html#topic+predictSurvProb">predictSurvProb</a></code> function. <br />
<strong>Group C:</strong> The <code><a href="riskRegression.html#topic+predictRisk">predictRisk</a></code> function is used to obtain predicted cumulative incidences, which are then transformed to survival probabilities. Additional arguments supplied using the <code>...</code> syntax are passed to the <code><a href="riskRegression.html#topic+predictRisk">predictRisk</a></code> function.<br />
<strong>Group D:</strong> These models are only allowed if there is no censoring. Predicted survival probabilities are obtained using the <code>predictProb</code> function from the <span class="pkg">pec</span> package. Additional arguments supplied using the <code>...</code> syntax are passed to the <code>predictProb</code> function.<br />
<strong>Group E:</strong> Custom code is used to obtain predicted survival probabilities. Additional arguments are not used.
</p>
<p>It is sometimes possible to use models even if they are not listed here. There are two ways to make this work. The first one is to use the models S3 <code>predict</code> method. This works if the <code>predict</code> function contains the arguments <code>object</code>, <code>newdata</code> and <code>times</code> and returns a matrix of predicted survival probabilities. The matrix should be of size <code>nrow(data) * length(times)</code>, where each row corresponds to a row in the original dataset and each column to one point in time. The matrix should contain the survival probabilities predicted by the model given covariates. If no such <code>predict</code> method exists the only option left is to write your own function which produces the output described above and supply this function to the <code>predict_fun</code> argument.
</p>
<p>If you think that some important models are missing from this list, please file an issue on the official github page with a specific feature request (URL can be found in the DESCRIPTION file) or contact the package maintainer directly using the given e-mail address.
</p>


<h3>Note</h3>

<p>When using outcome models which are not directly supported (either through the default predict method or a custom <code>predict_fun</code>) it might be necessary to set the <code>clean_data</code> argument of the <code>adjustedsurv</code> function to <code>FALSE</code>.
</p>

<hr>
<h2 id='plot_curve_diff'>
Plot the Difference Between or the Ratio of Two Adjusted Survival Curves or CIFs
</h2><span id='topic+plot_curve_diff'></span><span id='topic+plot_curve_ratio'></span>

<h3>Description</h3>

<p>A function to graphically display the difference or ratio between two confounder-adjusted survival curves which where previously estimated using the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function or between two confounder-adjusted CIFs which where previously estimated using the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function. The user can customize the plot using a variety of options. Internally it uses the <span class="pkg">ggplot2</span> package, so additional not implemented features can be added using the standard <code>ggplot2</code> syntax.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_curve_diff(x, group_1=NULL, group_2=NULL,
                conf_int=FALSE, conf_level=0.95, type="steps",
                times=NULL, max_t=Inf, use_boot=FALSE,
                size=0.7, color="black", linetype="solid",
                alpha=1, conf_int_alpha=0.4,
                points_ci_size=NULL, points_ci_width=NULL,
                xlab="Time", ylab=NULL, title=NULL,
                subtitle=NULL, gg_theme=ggplot2::theme_classic(),
                line_at_ref=TRUE, line_at_ref_size=0.7,
                line_at_ref_color="grey", line_at_ref_linetype="dashed",
                line_at_ref_alpha=1,
                loess_smoother=FALSE, loess_span=0.75,
                loess_color=color, loess_size=size,
                loess_linetype="dashed", loess_alpha=alpha,
                test=NULL, integral_from=0, integral_to=NULL,
                p_value=FALSE, integral=FALSE,
                interval=FALSE, text_pos_x="left",
                text_pos_y="bottom", text_size=3.5,
                text_family="serif", text_fontface="italic",
                text_color="black", text_alpha=1,
                text_digits=3, text_format_p=TRUE,
                fill_area=FALSE, area_color="blue", area_alpha=0.4,
                fill_only_interval=TRUE,
                ...)

plot_curve_ratio(x, group_1=NULL, group_2=NULL, conf_int=FALSE,
                 conf_level=0.95, type="steps", times=NULL,
                 max_t=Inf, use_boot=FALSE, size=0.7, color="black",
                 linetype="solid", alpha=1,
                 conf_int_alpha=0.4, xlab="Time", ylab=NULL,
                 title=NULL, subtitle=NULL,
                 gg_theme=ggplot2::theme_classic(),
                 line_at_ref=TRUE, line_at_ref_size=0.7,
                 line_at_ref_color="grey",
                 line_at_ref_linetype="dashed",
                 line_at_ref_alpha=1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_curve_diff_+3A_x">x</code></td>
<td>

<p>An <code>adjustedsurv</code> object created using the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function or an <code>adjustedcif</code> object created using the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_group_1">group_1</code></td>
<td>

<p>A single character string specifying one of the levels of the <code>variable</code> used in the original <code>adjustedsurv</code> or <code>adjustedcif</code> function call. This group will be subtracted from. For example if <code>group_1="A"</code> and <code>group_2="B"</code> the plotted curve will correspond to the survival probability (or CIF) of <code>A</code> minus the survival probability (or CIF) of <code>B</code> over time. If <code>NULL</code>, this will default to the first level of <code>variable</code>. Similarly if one plots ratios instead, the ratio would be calculated as <code>A / B</code>.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_group_2">group_2</code></td>
<td>

<p>Also a single character string specifying one of the levels of <code>variable</code>. This corresponds to the right side of the difference equation. See argument <code>group_2</code>. If <code>NULL</code>, this will default to the second level of <code>variable</code>.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_conf_int">conf_int</code></td>
<td>

<p>A logical variable indicating whether the confidence intervals should be drawn. This only works when <code>conf_int=TRUE</code> or <code>bootstrap=TRUE</code> was used in the original <code>adjustedsurv</code> or <code>adjustedcif</code> function call.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_conf_level">conf_level</code></td>
<td>

<p>The confidence level that should be used when calculating the confidence intervals. Ignored if <code>conf_int=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_type">type</code></td>
<td>

<p>Must be one of <code>"steps"</code> (drawing the difference/ratio as a step function), <code>"lines"</code> (drawing the difference/ratio using linear interpolation), <code>"points"</code> (drawing points only) or <code>"none"</code> (drawing nothing, useful when only the smoothed difference is of interest). It defaults to <code>"steps"</code>. For ratios, only the <code>"steps"</code> and <code>"lines"</code> options are available.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_times">times</code></td>
<td>

<p>An optional numeric vector of points in time at which the difference or ratio should be estimated. If <code>NULL</code> (default) the differences / ratios are estimated for the whole curve. This only affects the plot and has no effect on the <code>integral</code> or <code>p_value</code> if those are also specified.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_max_t">max_t</code></td>
<td>

<p>A number indicating the latest time to which the curve should be extended to.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_use_boot">use_boot</code></td>
<td>

<p>Whether to use the bootstrapped estimates to calculate the confidence intervals or not. Can only be used if <code>bootstrap=TRUE</code> was used in the original <code>adjustedsurv</code> or <code>adjustedcif</code> function call. Ignored if <code>conf_int=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_size">size</code></td>
<td>

<p>A number controlling the thickness of the curve.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_color">color</code></td>
<td>

<p>A string specifying the color of the curve.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_linetype">linetype</code></td>
<td>

<p>A string specifying the linetype of the curve.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_alpha">alpha</code></td>
<td>

<p>A number controlling the transparency level of the curves.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_conf_int_alpha">conf_int_alpha</code></td>
<td>

<p>A number indicating the level of transparency that should be used when drawing the confidence regions.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_points_ci_size">points_ci_size</code></td>
<td>

<p>Only used when <code>type="points"</code>. Controls the size of the error bars.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_points_ci_width">points_ci_width</code></td>
<td>

<p>Only used when <code>type="points"</code>. Controls the width of the error bars.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_xlab">xlab</code></td>
<td>

<p>A character string to be used as the X-Axis label of the plot. Defaults to &quot;Time&quot;.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_ylab">ylab</code></td>
<td>

<p>A character string to be used as the Y-Axis label of the plot. By default (<code>NULL</code>) uses the equation used to calculate the differences / ratios, based on the names supplied in <code>group_1</code> and <code>group_2</code>.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_title">title</code></td>
<td>

<p>A character string to be used as the title of the plot. Set to <code>NULL</code> if no title should be used.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_subtitle">subtitle</code></td>
<td>

<p>A character string to be used as the subtitle of the plot. Set to <code>NULL</code> if no subtitle should be used.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_gg_theme">gg_theme</code></td>
<td>

<p>A <code>ggplot2</code> theme object which will be used for the plot.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_line_at_ref">line_at_ref</code></td>
<td>

<p>Whether to draw a horizontal line at y = 0 for differences or at y = 1 for ratios or not.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_line_at_ref_size">line_at_ref_size</code></td>
<td>

<p>The size of the line drawn at the reference value. Ignored if <code>line_at_ref=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_line_at_ref_color">line_at_ref_color</code></td>
<td>

<p>The color of the line drawn at the reference value. Ignored if <code>line_at_ref=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_line_at_ref_linetype">line_at_ref_linetype</code></td>
<td>

<p>The linetype of the line drawn at the reference value. Ignored if <code>line_at_ref=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_line_at_ref_alpha">line_at_ref_alpha</code></td>
<td>

<p>The transparency level of the line drawn at the reference value. Ignored if <code>line_at_ref=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_loess_smoother">loess_smoother</code></td>
<td>

<p>Whether to draw a LOESS smoother through the difference curves.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_loess_span">loess_span</code></td>
<td>

<p>The span of the LOESS smoother. Ignored if <code>loess_smoother=FALSE</code>. See <code>stat_smooth</code> in the <code>ggplot2</code> package, <code>method="loess"</code> for more details.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_loess_color">loess_color</code></td>
<td>

<p>The color of the LOESS smoother line. Ignored if <code>loess_smoother=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_loess_size">loess_size</code></td>
<td>

<p>The size of the LOESS smoother line. Ignored if <code>loess_smoother=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_loess_linetype">loess_linetype</code></td>
<td>

<p>The linetype of the LOESS smoother line. Ignored if <code>loess_smoother=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_loess_alpha">loess_alpha</code></td>
<td>

<p>The transparency level of the LOESS smoother line. Ignored if <code>loess_smoother=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_test">test</code></td>
<td>

<p>An optional <code>curve_test</code> object created using the <code>adjusted_curve_test</code> function. If supplied it can be used to add a p-value and the integral statistic to the plot. Alternatively, the needed arguments below can be specified to obtain the values needed for the test. See below. Set to <code>NULL</code> (default) to ignore this.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_integral_from">integral_from</code></td>
<td>

<p>A number specifying the left limit of the integral. When <code>p_value=TRUE</code> and <code>test=NULL</code>, this argument will be passed to the <code>from</code> argument in the <code>adjusted_curve_test</code> function to perform the test.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_integral_to">integral_to</code></td>
<td>

<p>A number specifying the right limit of the integral. When <code>p_value=TRUE</code> and <code>test=NULL</code>, this argument will be passed to the <code>to</code> argument in the <code>adjusted_curve_test</code> function to perform the test.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_p_value">p_value</code></td>
<td>

<p>Whether to add a p-value to the plot or not. This requires either that the user supplies a previously created <code>curve_test</code> object to the <code>test</code> argument, or that the required arguments to call this function are supplied (at least <code>integral_to</code>). Either way it only works if <code>bootstrap=TRUE</code> was used in the original <code>adjustedsurv</code> or <code>adjustedcif</code> function call.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_integral">integral</code></td>
<td>

<p>Whether to add the integral of the difference in the interval [<code>from</code>, <code>to</code>] to the plot or not. This requires either that the user supplies a previously created <code>curve_test</code> object to the <code>test</code> argument, or that the required arguments to call this function are supplied (at least <code>integral_to</code>).
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_interval">interval</code></td>
<td>

<p>Whether to add the interval in which the integral was calculated to the plot as well.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_text_pos_x">text_pos_x</code></td>
<td>

<p>X position of the text. Can be either <code>"left"</code> (default), <code>"middle"</code>, <code>"right"</code> or a number specifying the exact position.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_text_pos_y">text_pos_y</code></td>
<td>

<p>Y position of the text. Can be either <code>"bottom"</code> (default), <code>"middle"</code>, <code>"top"</code> or a number specifying the exact position.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_text_digits">text_digits</code></td>
<td>

<p>The number of digits to which the p-value and the integral of the difference should be rounded to.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_text_size">text_size</code></td>
<td>

<p>The size of the text.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_text_family">text_family</code></td>
<td>

<p>The family of the text. Defaults to <code>"serif"</code>.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_text_fontface">text_fontface</code></td>
<td>

<p>The fontface of the text. Defaults to <code>"italic"</code>.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_text_color">text_color</code></td>
<td>

<p>The color of the text. Defaults to <code>"black"</code>.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_text_alpha">text_alpha</code></td>
<td>

<p>The transparency level of the text.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_text_format_p">text_format_p</code></td>
<td>

<p>Whether to format p-values smaller than 0.01 to &lt; 0.01.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_fill_area">fill_area</code></td>
<td>

<p>Whether to add color to the area between 0 and the difference.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_area_color">area_color</code></td>
<td>

<p>The color used to fill in the area between 0 and the difference when using <code>fill_area=TRUE</code>. Ignored otherwise.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_area_alpha">area_alpha</code></td>
<td>

<p>The transparency level used to fill in the area between 0 and the difference when using <code>fill_area=TRUE</code>. Ignored otherwise.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_fill_only_interval">fill_only_interval</code></td>
<td>

<p>Whether only the area corresponding to the interval defined by <code>integral_from</code> and <code>integral_to</code> should be filled. Only used when <code>fill_area=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="plot_curve_diff_+3A_...">...</code></td>
<td>

<p>Currently not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function allows the easy creation of difference / ratio curves. The syntax is exactly the same for both adjusted survival curves and adjusted CIFs. Similarly, the syntax is the same for ratios and for difference curves, although not all options of the difference curve function are available for the ratio curve function. By default it calculates the estimates up to the last point where estimates for both the <code>group_1</code> curve and the <code>group_2</code> curve are available.
</p>
<p>It currently does not support plotting multiple curves at once, which could be useful when there are more than two treatment groups in <code>variable</code>. If the user is interested in this, we recommend calling this function multiple times with the desired comparisons and concatenating the individual plots into one plot afterwards using a suitable function such as <code>par</code> or <code>ggarrange</code>.
</p>
<p>More information on how the differences or ratios and their confidence intervals are calculated can be found in the documentation of the <code><a href="#topic+adjusted_curve_diff">adjusted_curve_diff</a></code> function. More information on how the overall p-value and the integral are calculated for differences can be found in the <code><a href="#topic+adjusted_curve_test">adjusted_curve_test</a></code> function.
</p>


<h3>Value</h3>

<p>Returns a <code>ggplot2</code> object.
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>Michael Coory, Karen E. Lamb, and Michael Sorich (2014). &quot;Risk-Difference Curves can be used to Communicate Time-Dependent Effects of Adjuvant Therapies for Early Stage Cancer&quot;. In: Journal of Clinical Epidemiology 67, pp. 966-972
</p>
<p>Lihui Zhao, Lu Tian, Hajime Uno, Scott D. Solomon, Marc A. Pfeffer, Jerald S. Schindler, and L. J. Wei (2012). &quot;Utilizing the Integrated Difference of Two Survival Functions to Quantify the Treatment Contrast for Designing, Monitoring and Analyzing a Comparative Clinical Study&quot;. In: Clinical Trials 9.5, pp. 570-577
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjusted_curve_diff">adjusted_curve_diff</a></code>, <code><a href="#topic+adjusted_curve_test">adjusted_curve_test</a></code>, <code><a href="#topic+adjustedsurv">adjustedsurv</a></code>, <code><a href="#topic+adjustedcif">adjustedcif</a></code>, <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>, <code><a href="pammtools.html#topic+geom_stepribbon">geom_stepribbon</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)
library(ggplot2)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a cox-regression for the outcome
cox_mod &lt;- coxph(Surv(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group,
                 data=sim_dat, x=TRUE)


# use it to calculate adjusted survival curves with bootstrapping
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="direct",
                        outcome_model=cox_mod,
                        conf_int=TRUE,
                        bootstrap=TRUE,
                        n_boot=15) # should be much bigger in reality

# plot the difference with default values
plot_curve_diff(adjsurv)

# plot the ratio with default values
plot_curve_ratio(adjsurv)

# plot with reversed differences
plot_curve_diff(adjsurv, group_1="1", group_2="0")

# plot with confidence intervals
plot_curve_diff(adjsurv, conf_int=TRUE)
plot_curve_ratio(adjsurv, conf_int=TRUE)

# plot using lines instead
plot_curve_diff(adjsurv, conf_int=TRUE, type="lines")

# plot using points instead
plot_curve_diff(adjsurv, conf_int=TRUE, type="points")

# plot using an additional loess smoother
plot_curve_diff(adjsurv, loess_smoother=TRUE)

# plot without the line at reference
plot_curve_diff(adjsurv, line_at_ref=FALSE)
plot_curve_ratio(adjsurv, line_at_ref=FALSE)

# plot with some custom parameters
plot_curve_diff(adjsurv, conf_int=TRUE, color="blue", linetype="dotted",
                alpha=0.8, line_at_ref_size=1.1, line_at_ref_color="red",
                loess_smoother=TRUE, loess_span=0.55)

# adding a p-value for a difference test in the interval [0, 0.75]
plot_curve_diff(adjsurv, conf_int=TRUE, p_value=TRUE, integral_from=0,
                integral_to=0.75, integral=TRUE)

# adding a p-value for a difference test in the interval [0, 0.75],
# and also showing that integral visually in the plot
plot_curve_diff(adjsurv, conf_int=FALSE, p_value=TRUE, integral_from=0,
                integral_to=0.75, integral=TRUE, fill_area=TRUE,
                interval=TRUE)
</code></pre>

<hr>
<h2 id='plot_rmst_curve'>
Plot Adjusted Restricted Mean Survival Time Curves
</h2><span id='topic+plot_rmst_curve'></span>

<h3>Description</h3>

<p>A function to graphically display the Restricted Mean Survival Time (RMST) over time, using confounder-adjusted survival curves which where previously estimated using the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function. As the other plot functions in this package, it internally uses the <span class="pkg">ggplot2</span> package and allows a variety of options.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_rmst_curve(adjsurv, times=NULL, conf_int=FALSE,
                conf_level=0.95, interpolation="steps",
                max_t=Inf, color=TRUE, linetype=FALSE,
                facet=FALSE, size=1, alpha=1, xlab="Time",
                ylab="RMST", title=NULL, subtitle=NULL,
                legend.title="Group", legend.position="right",
                gg_theme=ggplot2::theme_classic(),
                custom_colors=NULL, custom_linetypes=NULL,
                conf_int_alpha=0.4, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_rmst_curve_+3A_adjsurv">adjsurv</code></td>
<td>

<p>An <code>adjustedsurv</code> object created using the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_times">times</code></td>
<td>

<p>A vector of points in time, passed to the <code>to</code> argument of the <code><a href="#topic+adjusted_rmst">adjusted_rmst</a></code> function or <code>NULL</code> (default). If <code>NULL</code>, the adjusted RMST is estimated at all points at which an event occurred. Otherwise it is estimated at <code>times</code>.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_conf_int">conf_int</code></td>
<td>

<p>A logical variable indicating whether the bootstrap confidence intervals should be drawn.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_interpolation">interpolation</code></td>
<td>

<p>Corresponds to the argument of the same name in the <code><a href="#topic+adjusted_rmst">adjusted_rmst</a></code> function.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_conf_level">conf_level</code></td>
<td>

<p>Corresponds to the argument of the same name in the <code><a href="#topic+adjusted_rmst">adjusted_rmst</a></code> function.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_max_t">max_t</code></td>
<td>

<p>A number indicating the latest survival time which is to be plotted.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_color">color</code></td>
<td>

<p>A logical variable indicating whether the curves should be colored differently. The <code>custom_colors</code> argument can be used to directly specify which colors to use. Set to <code>FALSE</code> to keep the plot black and white.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_linetype">linetype</code></td>
<td>

<p>A logical variable indicating whether the curves should have different linetypes. The <code>custom_linetypes</code> argument can be used to directly specify which linetypes to use. Set to <code>FALSE</code> to keep all lines solid.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_facet">facet</code></td>
<td>

<p>A logical variable indicating whether the curves should be in different facets.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_size">size</code></td>
<td>

<p>A number controlling the thickness of the RMST curves.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_alpha">alpha</code></td>
<td>

<p>A number controlling the transparency level of the RMST curves.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_xlab">xlab</code></td>
<td>

<p>A character string to be used as the X-Axis label of the plot.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_ylab">ylab</code></td>
<td>

<p>A character string to be used as the Y-Axis label of the plot.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_title">title</code></td>
<td>

<p>A character string to be used as the title of the plot. Set to <code>NULL</code> if no title should be used.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_subtitle">subtitle</code></td>
<td>

<p>A character string to be used as the subtitle of the plot. Set to <code>NULL</code> if no subtitle should be used.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_legend.title">legend.title</code></td>
<td>

<p>A character string to be used as the title of the legend. Set to <code>NULL</code> if no legend should be included.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_legend.position">legend.position</code></td>
<td>

<p>A character string specifying the position of the legend. Ignored if <code>legend_title=NULL</code>.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_gg_theme">gg_theme</code></td>
<td>

<p>A <code>ggplot2</code> theme object which will be used for the plot.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_custom_colors">custom_colors</code></td>
<td>

<p>A (named) vector to specify the colors of each adjusted RMST curve and possibly its confidence region. Set to <code>NULL</code> to use the <code>ggplot2</code> default values. Ignored if <code>color=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_custom_linetypes">custom_linetypes</code></td>
<td>

<p>A (named) vector to specify the linetype of each adjusted RMST curve. Set to <code>NULL</code> to use the <code>ggplot2</code> default values. Ignored if <code>color=FALSE</code>. Ignored if <code>linetype=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_conf_int_alpha">conf_int_alpha</code></td>
<td>

<p>A number indicating the level of transparency that should be used when drawing the confidence regions.
</p>
</td></tr>
<tr><td><code id="plot_rmst_curve_+3A_...">...</code></td>
<td>

<p>Currently not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function simply calls the <code><a href="#topic+adjusted_rmst">adjusted_rmst</a></code> for a range of <code>to</code> values, getting adjusted RMST estimates over the whole range of the survival curves. Those estimates are then plotted as a curve with the adjusted RMST replacing the survival probability on the Y-Axis. For a brief description on the RMST and how it is calculated in this package, see the documentation of the <code><a href="#topic+adjusted_rmst">adjusted_rmst</a></code> function. Literature describing the RMST Curve Plots in more detail is given in the references section.
</p>
<p>The RMST curve can only be created for adjusted survival curves. A similar graphic for the adjusted CIFs can be created by utilizing the adjusted Restricted Mean Time Lost (RMTL). The calculation of that statistic is implemented in the <code><a href="#topic+adjusted_rmtl">adjusted_rmtl</a></code> function and the associated curve can be created using the <code><a href="#topic+plot_rmtl_curve">plot_rmtl_curve</a></code> function.
</p>
<p>If confidence intervals are specified and there are many points in time in <code>times</code>, this function might get very slow. It will be even slower if multiple imputation was also used when creating the <code>adjustedsurv</code> object.
</p>


<h3>Value</h3>

<p>Returns a <code>ggplot2</code> object.
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>Lihui Zhao, Brian Claggett, Lu Tian, Hajime Uno, Marc A. Pfeffer, Scott D. Solomon, Lorenzo Trippa, and L. J. Wei (2016). &quot;On the Restricted Mean Survival Time Curve in Survival Analysis&quot;. In: Biometrics 72.1, pp. 215-221
</p>
<p>Jason J. Z. Liao, Frank Liu, and Wen-Chi Wu (2020). &quot;Dynamic RMST Curves for Survival Analysis in Clinical Trials&quot;. In: BMC Medical Research Methodology 20.218
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjustedsurv">adjustedsurv</a></code>, <code><a href="#topic+adjusted_rmst">adjusted_rmst</a></code>, <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)
library(ggplot2)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a cox-regression for the outcome
cox_mod &lt;- coxph(Surv(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group,
                 data=sim_dat, x=TRUE)


# use it to calculate adjusted survival curves with bootstrapping
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="direct",
                        outcome_model=cox_mod,
                        conf_int=TRUE,
                        bootstrap=TRUE,
                        n_boot=15) # should be much bigger in reality

# plot the curves with default values
plot_rmst_curve(adjsurv)

# plot with confidence intervals
plot_rmst_curve(adjsurv, conf_int=TRUE)

# plot with some custom options
plot_rmst_curve(adjsurv, max_t=0.5, linetype=TRUE,
                custom_colors=c("green", "blue"))
</code></pre>

<hr>
<h2 id='plot_rmtl_curve'>
Plot Adjusted Restricted Mean Time Lost Curves
</h2><span id='topic+plot_rmtl_curve'></span>

<h3>Description</h3>

<p>A function to graphically display the Restricted Mean Time Lost (RMTL) over time, using confounder-adjusted survival curves which were previously estimated using the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function, or cause-specific confounder-adjusted CIFs which were previously estimated using the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function. As the other plot functions in this package, it internally uses the <span class="pkg">ggplot2</span> package and allows a variety of options.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_rmtl_curve(adj, times=NULL, conf_int=FALSE,
                conf_level=0.95, interpolation="steps",
                max_t=Inf, color=TRUE, linetype=FALSE,
                facet=FALSE, size=1, alpha=1, xlab="Time",
                ylab="RMTL", title=NULL, subtitle=NULL,
                legend.title="Group", legend.position="right",
                gg_theme=ggplot2::theme_classic(),
                custom_colors=NULL, custom_linetypes=NULL,
                conf_int_alpha=0.4, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_rmtl_curve_+3A_adj">adj</code></td>
<td>

<p>An <code>adjustedsurv</code> object created using the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function, or an <code>adjustedcif</code> object created using the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_times">times</code></td>
<td>

<p>A vector of points in time, passed to the <code>to</code> argument of the <code><a href="#topic+adjusted_rmtl">adjusted_rmtl</a></code> function or <code>NULL</code> (default). If <code>NULL</code>, the adjusted RMTL is estimated at all points at which an event occurred. Otherwise it is estimated at <code>times</code>.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_conf_int">conf_int</code></td>
<td>

<p>A logical variable indicating whether the bootstrap confidence intervals should be drawn.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_conf_level">conf_level</code></td>
<td>

<p>Corresponds to the argument of the same name in the <code><a href="#topic+adjusted_rmtl">adjusted_rmtl</a></code> function.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_interpolation">interpolation</code></td>
<td>

<p>Corresponds to the argument of the same name in the <code><a href="#topic+adjusted_rmtl">adjusted_rmtl</a></code> function.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_max_t">max_t</code></td>
<td>

<p>A number indicating the latest survival time which is to be plotted.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_color">color</code></td>
<td>

<p>A logical variable indicating whether the curves should be colored differently. The <code>custom_colors</code> argument can be used to directly specify which colors to use. Set to <code>FALSE</code> to keep the plot black and white.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_linetype">linetype</code></td>
<td>

<p>A logical variable indicating whether the curves should have different linetypes. The <code>custom_linetypes</code> argument can be used to directly specify which linetypes to use. Set to <code>FALSE</code> to keep all lines solid.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_facet">facet</code></td>
<td>

<p>A logical variable indicating whether the curves should be in different facets.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_size">size</code></td>
<td>

<p>A number controlling the thickness of the RMTL curves.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_alpha">alpha</code></td>
<td>

<p>A number controlling the transparency level of the RMTL curves.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_xlab">xlab</code></td>
<td>

<p>A character string to be used as the X-Axis label of the plot.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_ylab">ylab</code></td>
<td>

<p>A character string to be used as the Y-Axis label of the plot.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_title">title</code></td>
<td>

<p>A character string to be used as the title of the plot. Set to <code>NULL</code> if no title should be used.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_subtitle">subtitle</code></td>
<td>

<p>A character string to be used as the subtitle of the plot. Set to <code>NULL</code> if no subtitle should be used.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_legend.title">legend.title</code></td>
<td>

<p>A character string to be used as the title of the legend. Set to <code>NULL</code> if no legend should be included.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_legend.position">legend.position</code></td>
<td>

<p>A character string specifying the position of the legend. Ignored if <code>legend_title=NULL</code>.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_gg_theme">gg_theme</code></td>
<td>

<p>A <code>ggplot2</code> theme object which will be used for the plot.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_custom_colors">custom_colors</code></td>
<td>

<p>A (named) vector to specify the colors of each adjusted RMTL curve and possibly its confidence region. Set to <code>NULL</code> to use the <code>ggplot2</code> default values. Ignored if <code>color=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_custom_linetypes">custom_linetypes</code></td>
<td>

<p>A (named) vector to specify the linetype of each adjusted RMTL curve. Set to <code>NULL</code> to use the <code>ggplot2</code> default values. Ignored if <code>color=FALSE</code>. Ignored if <code>linetype=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_conf_int_alpha">conf_int_alpha</code></td>
<td>

<p>A number indicating the level of transparency that should be used when drawing the confidence regions.
</p>
</td></tr>
<tr><td><code id="plot_rmtl_curve_+3A_...">...</code></td>
<td>

<p>Currently not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function simply calls the <code><a href="#topic+adjusted_rmtl">adjusted_rmtl</a></code> for a range of <code>to</code> values, getting adjusted RMTL estimates over the whole range of the survival curves or CIFs. Those estimates are then plotted as a curve with the adjusted RMTL replacing the survival probability or the failure probability on the Y-Axis. For a brief description on the RMTL and how it is calculated in this package, see the documentation of the <code><a href="#topic+adjusted_rmtl">adjusted_rmtl</a></code> function. Literature describing the RMTL Curve Plots in more detail is given in the references section.
</p>
<p>If confidence intervals are specified and there are many points in time in <code>times</code>, this function might get very slow. It will be even slower if multiple imputation was also used when creating the <code>adjustedsurv</code> or <code>adjustedcif</code> object.
</p>


<h3>Value</h3>

<p>Returns a <code>ggplot2</code> object.
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>Lihui Zhao, Brian Claggett, Lu Tian, Hajime Uno, Marc A. Pfeffer, Scott D. Solomon, Lorenzo Trippa, and L. J. Wei (2016). &quot;On the Restricted Mean Survival Time Curve in Survival Analysis&quot;. In: Biometrics 72.1, pp. 215-221
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjustedsurv">adjustedsurv</a></code>, <code><a href="#topic+adjustedcif">adjustedcif</a></code>, <code><a href="#topic+adjusted_rmtl">adjusted_rmtl</a></code>, <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)
library(ggplot2)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a cox-regression for the outcome
cox_mod &lt;- coxph(Surv(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group,
                 data=sim_dat, x=TRUE)


# use it to calculate adjusted survival curves with bootstrapping
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="direct",
                        outcome_model=cox_mod,
                        conf_int=TRUE,
                        bootstrap=TRUE,
                        n_boot=15) # should be much bigger in reality

# plot the curves with default values
plot_rmtl_curve(adjsurv)

# plot with confidence intervals
plot_rmtl_curve(adjsurv, conf_int=TRUE)

# plot with some custom options
plot_rmtl_curve(adjsurv, max_t=0.5, linetype=TRUE,
                custom_colors=c("green", "blue"))
</code></pre>

<hr>
<h2 id='plot.adjustedcif'>
Plot Confounder-Adjusted Cumulative Incidence Functions
</h2><span id='topic+plot.adjustedcif'></span>

<h3>Description</h3>

<p>A function to graphically display confounder-adjusted cumulative incidence functions which where previously estimated using the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function. The user can customize the plot using a variety of options. Internally it uses the <span class="pkg">ggplot2</span> package, so additional not implemented features can be added using the standard <span class="pkg">ggplot2</span> syntax. This function also includes the option to use isotonic regression on the CIFs, which is of benefit if the estimated curves are not monotone.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'adjustedcif'
plot(x, conf_int=FALSE, max_t=Inf,
     iso_reg=FALSE, force_bounds=FALSE,
     use_boot=FALSE, color=TRUE,
     linetype=FALSE, facet=FALSE,
     line_size=1, line_alpha=1, xlab="Time",
     ylab="Adjusted Cumulative Incidence",
     title=NULL, subtitle=NULL, legend.title="Group",
     legend.position="right",
     gg_theme=ggplot2::theme_classic(),
     ylim=NULL, custom_colors=NULL,
     custom_linetypes=NULL,
     single_color=NULL, single_linetype=NULL,
     conf_int_alpha=0.4, steps=TRUE,
     censoring_ind="none",
     censoring_ind_size=0.5,
     censoring_ind_alpha=1,
     censoring_ind_shape=17,
     censoring_ind_width=NULL,
     ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.adjustedcif_+3A_x">x</code></td>
<td>

<p>An <code>adjustedcif</code> object created using the <code><a href="#topic+adjustedcif">adjustedcif</a></code> function.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_conf_int">conf_int</code></td>
<td>

<p>A logical variable indicating whether the confidence intervals should be drawn.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_max_t">max_t</code></td>
<td>

<p>A number indicating the latest event time which is to be plotted.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_iso_reg">iso_reg</code></td>
<td>

<p>A logical variable indicating whether the estimates should be monotonized using isotonic regression. See details.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_force_bounds">force_bounds</code></td>
<td>

<p>A logical variable indicating whether the 0 and 1 bounds of the CIFs should be forced in the plot. See details.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_use_boot">use_boot</code></td>
<td>

<p>A logical variable denoting whether the bootstrapped estimates should be used for the curves and their confidence intervals. Can only be used if they were calculated. See <code><a href="#topic+adjustedcif">adjustedcif</a></code>.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_color">color</code></td>
<td>

<p>A logical variable indicating whether the curves should be colored differently. The <code>custom_colors</code> argument can be used to directly specify which colors to use. Alternatively the <code>single_color</code> argument can be used if everything should have the same color.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_linetype">linetype</code></td>
<td>

<p>A logical variable indicating whether the curves should have different linetypes. The <code>custom_linetypes</code> argument can be used to directly specify which linetypes to use. Alternatively the <code>single_linetype</code> argument can be used if all curves should have the same linetype.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_facet">facet</code></td>
<td>

<p>A logical variable indicating whether the curves should be in different facets.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_line_size">line_size</code></td>
<td>

<p>A number controlling the thickness of the curves.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_line_alpha">line_alpha</code></td>
<td>

<p>A number controlling the transparency level of the curves.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_xlab">xlab</code></td>
<td>

<p>A character string to be used as the X-Axis label of the plot.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_ylab">ylab</code></td>
<td>

<p>A character string to be used as the Y-Axis label of the plot.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_title">title</code></td>
<td>

<p>A character string to be used as the title of the plot. Set to <code>NULL</code> (default) if no title should be used.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_subtitle">subtitle</code></td>
<td>

<p>A character string to be used as the subtitle of the plot. Set to <code>NULL</code> (default) if no subtitle should be used.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_legend.title">legend.title</code></td>
<td>

<p>A character string to be used as the title of the legend. Set to <code>NULL</code> if no legend should be included.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_legend.position">legend.position</code></td>
<td>

<p>A character string specifying the position of the legend. Ignored if <code>legend_title=NULL</code>.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_gg_theme">gg_theme</code></td>
<td>

<p>A <code>ggplot2</code> theme object which will be used for the plot.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_ylim">ylim</code></td>
<td>

<p>A numeric vector of length two, specifying the limits of the Y-Axis. Set to <code>NULL</code> to use the <code>ggplot2</code> default values.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_custom_colors">custom_colors</code></td>
<td>

<p>A (named) vector to specify the colors of each CIF and possibly its confidence region. Set to <code>NULL</code> to use the <code>ggplot2</code> default values. Ignored if <code>color=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_custom_linetypes">custom_linetypes</code></td>
<td>

<p>A (named) vector to specify the linetype of each CIF. Set to <code>NULL</code> to use the <code>ggplot2</code> default values. Ignored if <code>linetype=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_single_color">single_color</code></td>
<td>

<p>A single color to use for every curve, irrespective of group status. If <code>color</code> is specified as well this argument will override it, but also generate a warning. Set to <code>NULL</code> (default) to ignore this argument.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_single_linetype">single_linetype</code></td>
<td>

<p>A single linetype to use for every curve, irrespective of group status. If <code>linetype</code> is specified as well this argument will override it, but also generate a warning. Set to <code>NULL</code> (default) to ignore this argument.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_conf_int_alpha">conf_int_alpha</code></td>
<td>

<p>A number indicating the level of transparency that should be used when drawing the confidence regions.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_steps">steps</code></td>
<td>

<p>A logical variable indicating whether the CIFs should be plotted as a step function or using straight lines. Straight lines should not be used with a simple Aalen-Joahnsen estimator. It is recommended to only use straight lines when a sufficiently fine grid of time points was used in the estimation step.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_censoring_ind">censoring_ind</code></td>
<td>

<p>What kind of indicator to plot for censored observations on the CIFs. Must be one of <code>"none"</code> (plotting no indicators at all, the default), <code>"lines"</code> (plotting small vertical lines) and <code>"points"</code> (plotting points). Those will be affected by <code>linetype</code> and <code>color</code> as well. Observations who failed due to a competing event are not considered as censored here.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_censoring_ind_size">censoring_ind_size</code></td>
<td>

<p>A numeric value specifying the size of the censoring indicators. Ignored if <code>censoring_ind="none"</code>.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_censoring_ind_alpha">censoring_ind_alpha</code></td>
<td>

<p>A numeric value specifying the alpha level of the censoring indicators. Ignored if <code>censoring_ind="none"</code>.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_censoring_ind_shape">censoring_ind_shape</code></td>
<td>

<p>A numeric value specifying the shape of the censoring indicators when using <code>censoring_ind="points"</code>. Ignored otherwise. For available shapes see <code>?geom_point</code>.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_censoring_ind_width">censoring_ind_width</code></td>
<td>

<p>A numeric value specifying the width of the censoring indicators. Ignored unless <code>censoring_ind="lines"</code>. By default (<code>censoring_ind_width=NULL</code>) the width of the censoring indicators is equal to 5 percent of the plot height.
</p>
</td></tr>
<tr><td><code id="plot.adjustedcif_+3A_...">...</code></td>
<td>

<p>Currently not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When using certain methods there is no guarantee that the resulting estimated CIFs are monotonically increasing. This is unfortunate since we know that it has to be the case. Isotonic regression can be used to fix this problem by ensuring that the CIFs are actually monotonically increasing everywhere, while also being as close to the observations as possible. Westling et al. (2020) showed mathematically that this usually does not add any systematic bias to the estimates. More information on the method can be found in Robertson et al. (1988). This adjustment can be done using this function by setting <code>iso_reg</code> to <code>TRUE</code>.
</p>
<p>Similarly, some methods can produce estimates that lie outside the theoretical 0 and 1 bounds of probability. By setting <code>force_bounds</code> to <code>TRUE</code> these estimates are manually set to either 0 or 1 (whichever is closer).
</p>


<h3>Value</h3>

<p>Returns a <code>ggplot2</code> object.
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>Ted Westling, Mark J. van der Laan, and Marco Carone (2020). &quot;Correcting an Estimator of a Multivariate Monotone Function with Isotonic Regression&quot;. In: Electronic Journal of Statistics 14, pp. 3032-3069
</p>
<p>Tim Robertson, F. T. Wright, and R. L. Dykstra (1988). Order Restricted Statistical Inference. Hoboken: John Wiley &amp; Sons
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjustedcif">adjustedcif</a></code>, <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>, <code><a href="pammtools.html#topic+geom_stepribbon">geom_stepribbon</a></code>, <code><a href="stats.html#topic+isoreg">isoreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(riskRegression)
library(prodlim)
library(survival)
library(ggplot2)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_crisk(n=50)
sim_dat$group &lt;- as.factor(sim_dat$group)

# calculate a Cause-Specific-Cox model
cox_mod &lt;- CSC(Hist(time, event) ~ x1 + x3 + x5 + group,
               data=sim_dat)

# use it to calculate adjusted CIFs with bootstrapping (for cause = 1)
adjcif &lt;- adjustedcif(data=sim_dat,
                      variable="group",
                      ev_time="time",
                      event="event",
                      method="direct",
                      outcome_model=cox_mod,
                      conf_int=TRUE,
                      bootstrap=TRUE,
                      n_boot=15, # should be much bigger in reality
                      cause=1)

# plot the curves with default values
plot(adjcif)

# plot after applying isotonic regression
plot(adjcif, iso_reg=TRUE)

# plot with confidence intervals estimated using asymptotic variances
plot(adjcif, conf_int=TRUE)

# plot with confidence intervals estimated using bootstrapping
plot(adjcif, conf_int=TRUE, use_boot=TRUE)

# plot with different linetypes only
plot(adjcif, linetype=TRUE, color=FALSE, facet=FALSE)

# plot with different facets only
plot(adjcif, linetype=FALSE, color=FALSE, facet=TRUE)

# plot with different linetypes and different colors
plot(adjcif, linetype=TRUE, color=TRUE, facet=FALSE)

# plot with some custom characteristics
plot(adjcif, legend.position="bottom", linetype=TRUE,
     custom_colors=c("green", "blue"), legend.title="Custom",
     title="Custom Plot", conf_int=TRUE, linesize=0.5)

# adding further ggplot2 elements
plot(adjcif) + theme_bw()
</code></pre>

<hr>
<h2 id='plot.adjustedsurv'>
Plot Confounder-Adjusted Survival Curves
</h2><span id='topic+plot.adjustedsurv'></span>

<h3>Description</h3>

<p>A function to graphically display confounder-adjusted survival curves which where previously estimated using the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function. The user can customize the plot using a variety of options. Internally it uses the <span class="pkg">ggplot2</span> package, so additional not implemented features can be added using the standard <code>ggplot2</code> syntax. This function also includes the option to use isotonic regression on the survival curves, which is of benefit if the estimated curves are not monotone.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'adjustedsurv'
plot(x, conf_int=FALSE, max_t=Inf,
     iso_reg=FALSE, force_bounds=FALSE,
     use_boot=FALSE, cif=FALSE, color=TRUE,
     linetype=FALSE, facet=FALSE,
     line_size=1, line_alpha=1, xlab="Time",
     ylab="Adjusted Survival Probability",
     title=NULL, subtitle=NULL, legend.title="Group",
     legend.position="right",
     gg_theme=ggplot2::theme_classic(),
     ylim=NULL, custom_colors=NULL,
     custom_linetypes=NULL,
     single_color=NULL, single_linetype=NULL,
     conf_int_alpha=0.4, steps=TRUE,
     median_surv_lines=FALSE,
     median_surv_size=0.5,
     median_surv_linetype="dashed",
     median_surv_color="black",
     median_surv_alpha=1,
     median_surv_quantile=0.5,
     censoring_ind="none",
     censoring_ind_size=0.5,
     censoring_ind_alpha=1,
     censoring_ind_shape=17,
     censoring_ind_width=NULL,
     ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.adjustedsurv_+3A_x">x</code></td>
<td>

<p>An <code>adjustedsurv</code> object created using the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_conf_int">conf_int</code></td>
<td>

<p>A logical variable indicating whether the confidence intervals should be drawn.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_max_t">max_t</code></td>
<td>

<p>A number indicating the latest survival time which is to be plotted.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_iso_reg">iso_reg</code></td>
<td>

<p>A logical variable indicating whether the estimates should be monotonized using isotonic regression. See details.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_force_bounds">force_bounds</code></td>
<td>

<p>A logical variable indicating whether the 0 and 1 bounds of the survival probabilities should be forced in the plot. See details.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_use_boot">use_boot</code></td>
<td>

<p>A logical variable denoting whether the bootstrapped estimates should be used for the curves and their confidence intervals. Can only be used if they were calculated. See <code><a href="#topic+adjustedsurv">adjustedsurv</a></code>.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_cif">cif</code></td>
<td>

<p>If <code>TRUE</code> the cumulative incidence functions are drawn instead of the survival curves. Those are calculated by taking 1 - the adjusted survival probability. If <code>FALSE</code> (default) the usual survival curves are shown.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_color">color</code></td>
<td>

<p>A logical variable indicating whether the curves should be colored differently. The <code>custom_colors</code> argument can be used to directly specify which colors to use. Alternatively the <code>single_color</code> argument can be used if everything should have the same color.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_linetype">linetype</code></td>
<td>

<p>A logical variable indicating whether the curves should have different linetypes. The <code>custom_linetypes</code> argument can be used to directly specify which linetypes to use. Alternatively the <code>single_linetype</code> argument can be used if all curves should have the same linetype.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_facet">facet</code></td>
<td>

<p>A logical variable indicating whether the curves should be in different facets.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_line_size">line_size</code></td>
<td>

<p>A number controlling the thickness of the survival curves.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_line_alpha">line_alpha</code></td>
<td>

<p>A number controlling the transparency level of the survival curves.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_xlab">xlab</code></td>
<td>

<p>A character string to be used as the X-Axis label of the plot.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_ylab">ylab</code></td>
<td>

<p>A character string to be used as the Y-Axis label of the plot.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_title">title</code></td>
<td>

<p>A character string to be used as the title of the plot. Set to <code>NULL</code> if no title should be used.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_subtitle">subtitle</code></td>
<td>

<p>A character string to be used as the subtitle of the plot. Set to <code>NULL</code> if no subtitle should be used.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_legend.title">legend.title</code></td>
<td>

<p>A character string to be used as the title of the legend. Set to <code>NULL</code> if no legend should be included.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_legend.position">legend.position</code></td>
<td>

<p>A character string specifying the position of the legend. Ignored if <code>legend_title=NULL</code>.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_gg_theme">gg_theme</code></td>
<td>

<p>A <code>ggplot2</code> theme object which will be used for the plot.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_ylim">ylim</code></td>
<td>

<p>A numeric vector of length two, specifying the limits of the Y-Axis. Set to <code>NULL</code> to use the <code>ggplot2</code> default values.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_custom_colors">custom_colors</code></td>
<td>

<p>A (named) vector to specify the colors of each adjusted survival curve and possibly its confidence region. Set to <code>NULL</code> to use the <code>ggplot2</code> default values. Ignored if <code>color=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_custom_linetypes">custom_linetypes</code></td>
<td>

<p>A (named) vector to specify the linetype of each adjusted survival curve. Set to <code>NULL</code> to use the <code>ggplot2</code> default values. Ignored if <code>color=FALSE</code>. Ignored if <code>linetype=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_single_color">single_color</code></td>
<td>

<p>A single color to use for every survival curve, irrespective of group status. If <code>color</code> is specified as well this argument will override it, but also generate a warning. Set to <code>NULL</code> (default) to ignore this argument.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_single_linetype">single_linetype</code></td>
<td>

<p>A single linetype to use for every survival curve, irrespective of group status. If <code>linetype</code> is specified as well this argument will override it, but also generate a warning. Set to <code>NULL</code> (default) to ignore this argument.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_conf_int_alpha">conf_int_alpha</code></td>
<td>

<p>A number indicating the level of transparency that should be used when drawing the confidence regions.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_steps">steps</code></td>
<td>

<p>A logical variable indicating whether the survival curves should be plotted as a step function or using straight lines. Straight lines should not be used with a simple Kaplan-Meier estimator. It is recommended to only use straight lines when a sufficiently fine grid of time points was used in the estimation step.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_median_surv_lines">median_surv_lines</code></td>
<td>

<p>Whether to draw indicator lines for the median survival times, which makes it easier to read those off the curves. Survival curves with undefined median survival times receive no lines.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_median_surv_size">median_surv_size</code></td>
<td>

<p>The size of the median survival indicator lines. Ignored if <code>median_surv_lines=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_median_surv_linetype">median_surv_linetype</code></td>
<td>

<p>The linetype of the median survival indicator lines. Ignored if <code>median_surv_lines=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_median_surv_color">median_surv_color</code></td>
<td>

<p>The color of the median survival indicator lines. Ignored if <code>median_surv_lines=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_median_surv_alpha">median_surv_alpha</code></td>
<td>

<p>The transparency level of the median survival indicator lines. Ignored if <code>median_surv_lines=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_median_surv_quantile">median_surv_quantile</code></td>
<td>

<p>The survival quantile which should be drawn. To draw the median survival time, set this parameter to 0.5 (default).
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_censoring_ind">censoring_ind</code></td>
<td>

<p>What kind of indicator to plot for censored observations on the survival curves. Must be one of <code>"none"</code> (plotting no indicators at all, the default), <code>"lines"</code> (plotting small vertical lines) and <code>"points"</code> (plotting points). Those will be affected by <code>linetype</code> and <code>color</code> as well.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_censoring_ind_size">censoring_ind_size</code></td>
<td>

<p>A numeric value specifying the size of the censoring indicators. Ignored if <code>censoring_ind="none"</code>.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_censoring_ind_alpha">censoring_ind_alpha</code></td>
<td>

<p>A numeric value specifying the alpha level of the censoring indicators. Ignored if <code>censoring_ind="none"</code>.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_censoring_ind_shape">censoring_ind_shape</code></td>
<td>

<p>A numeric value specifying the shape of the censoring indicators when using <code>censoring_ind="points"</code>. Ignored otherwise. For available shapes see <code>?geom_point</code>.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_censoring_ind_width">censoring_ind_width</code></td>
<td>

<p>A numeric value specifying the width of the censoring indicators. Ignored unless <code>censoring_ind="lines"</code>. By default (<code>censoring_ind_width=NULL</code>) the width of the censoring indicators is equal to 5 percent of the plot height.
</p>
</td></tr>
<tr><td><code id="plot.adjustedsurv_+3A_...">...</code></td>
<td>

<p>Currently not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When using certain methods there is no guarantee that the resulting estimated survival curves are monotonically decreasing. This is unfortunate since we know that it has to be the case. Isotonic regression can be used to fix this problem by ensuring that the survival curves are actually monotonically decreasing everywhere, while also being as close to the observations as possible. Westling et al. (2020) showed mathematically that this does not add any systematic bias to the estimates. More information on the method can be found in Robertson et al. (1988). This adjustment can be done using this function by setting <code>iso_reg</code> to <code>TRUE</code>.
</p>
<p>Similarly, some methods can produce estimates that lie outside the theoretical 0 and 1 bounds of probability. By setting <code>force_bounds</code> to <code>TRUE</code> these estimates are manually set to either 0 or 1 (whichever is closer).
</p>
<p>There is currently no option to add risk tables to the plot, because there is no way to meaningfully adjust those for confounders.
</p>
<p>If you prefer using the <code>ggsurvplot</code> syntax, you can also use the <code><a href="#topic+as_ggsurvplot_df">as_ggsurvplot_df</a></code> function to extract a <code>data.frame</code> from the <code>adjustedsurv</code> object, which can be used directly to call the <code>ggsurvplot_df</code> function from the <span class="pkg">survminer</span> package.
</p>


<h3>Value</h3>

<p>Returns a <code>ggplot2</code> object.
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>Robin Denz, Renate Klaaßen-Mielke, and Nina Timmesfeld (2023). &quot;A Comparison of Different Methods to Adjust Survival Curves for Confounders&quot;. In: Statistics in Medicine 42.10, pp. 1461-1479
</p>
<p>Ted Westling, Mark J. van der Laan, and Marco Carone (2020). &quot;Correcting an Estimator of a Multivariate Monotone Function with Isotonic Regression&quot;. In: Electronic Journal of Statistics 14, pp. 3032-3069
</p>
<p>Tim Robertson, F. T. Wright, and R. L. Dykstra (1988). Order Restricted Statistical Inference. Hoboken: John Wiley &amp; Sons
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjustedsurv">adjustedsurv</a></code>, <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>, <code><a href="pammtools.html#topic+geom_stepribbon">geom_stepribbon</a></code>, <code><a href="stats.html#topic+isoreg">isoreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)
library(ggplot2)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a cox-regression for the outcome
cox_mod &lt;- coxph(Surv(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group,
                 data=sim_dat, x=TRUE)


# use it to calculate adjusted survival curves with bootstrapping
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="direct",
                        outcome_model=cox_mod,
                        conf_int=TRUE,
                        bootstrap=TRUE,
                        n_boot=15) # should be much bigger in reality

# plot the curves with default values
plot(adjsurv)

# plot after applying isotonic regression
plot(adjsurv, iso_reg=TRUE)

# plot with confidence intervals estimated using asymptotic variances
plot(adjsurv, conf_int=TRUE)

# plot with confidence intervals estimated using bootstrapping
plot(adjsurv, conf_int=TRUE, use_boot=TRUE)

# plot with different linetypes only
plot(adjsurv, linetype=TRUE, color=FALSE, facet=FALSE)

# plot with different facets only
plot(adjsurv, linetype=FALSE, color=FALSE, facet=TRUE)

# plot with different linetypes and different colors
plot(adjsurv, linetype=TRUE, color=TRUE, facet=FALSE)

# plot with median survival indicator lines
plot(adjsurv, median_surv_lines=TRUE)

# plot with small lines indicating where observations were censored
plot(adjsurv, censoring_ind="lines")

# plot with points indicating where observations were censored
plot(adjsurv, censoring_ind="points", censoring_ind_size=4)

# plot with some custom characteristics
plot(adjsurv, legend.position="bottom", linetype=TRUE,
     custom_colors=c("green", "blue"), legend.title="Custom",
     title="Custom Plot", conf_int=TRUE, linesize=0.5,
     median_surv_lines=TRUE, censoring_ind="lines")

# adding further ggplot2 elements
plot(adjsurv) + theme_bw()
</code></pre>

<hr>
<h2 id='plot.curve_test'>
Plot Method for <code>curve_test</code> Objects
</h2><span id='topic+plot.curve_test'></span>

<h3>Description</h3>

<p>Produces either a spaghetti-plot of the bootstrapped difference curves (<code>type="curves"</code>) or a kernel-density plot of the shifted bootstrap distribution of the difference curve integrals (<code>type="integral"</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'curve_test'
plot(x, type="curves", xlab=NULL,
     ylab=NULL, title=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.curve_test_+3A_x">x</code></td>
<td>

<p>An object of class <code>curve_test</code> created by the <code>adjusted_curve_test</code> function.
</p>
</td></tr>
<tr><td><code id="plot.curve_test_+3A_type">type</code></td>
<td>

<p>Either <code>"curves"</code> or <code>"integral"</code>, specifying what should be plotted.
</p>
</td></tr>
<tr><td><code id="plot.curve_test_+3A_xlab">xlab</code></td>
<td>

<p>The label of the X-Axis. Set to <code>NULL</code> to use default label.
</p>
</td></tr>
<tr><td><code id="plot.curve_test_+3A_ylab">ylab</code></td>
<td>

<p>The label of the Y-Axis. Set to <code>NULL</code> to use default label.
</p>
</td></tr>
<tr><td><code id="plot.curve_test_+3A_title">title</code></td>
<td>

<p>The title of the plot. Set to <code>NULL</code> to use no title.
</p>
</td></tr>
<tr><td><code id="plot.curve_test_+3A_...">...</code></td>
<td>

<p>Currently not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When using <code>type="curves"</code> the black curve shows the observed curve of the difference. When using <code>type="integral"</code> the red line shows the observed integral of the curve of the difference.
</p>
<p>Both graphics can be used to check if the assumptions of the test hold. The bootstrap-shifted distribution of the integral of the difference should approximately be normally distributed. If the kernel-density estimate shown with <code>type="integral"</code> is clearly not normally distributed, the estimated p-value might be wrong. Similarly, if the curves of the differences do not vary randomly around the black line when using <code>type="curves"</code>, the estimated p-value might be wrong. You could also try to rerun the <code>adjustedsurv</code> or <code>adjustedcif</code> function with a bigger number in <code>n_boot</code>.
</p>


<h3>Value</h3>

<p>Returns a <code>ggplot2</code> object.
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjusted_curve_test">adjusted_curve_test</a></code>, <code><a href="#topic+adjustedsurv">adjustedsurv</a></code>, <code><a href="#topic+adjusted_rmst">adjusted_rmst</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See ?adjusted_curve_test
</code></pre>

<hr>
<h2 id='print.curve_test'>
Print Method for <code>curve_test</code> Objects
</h2><span id='topic+print.curve_test'></span>

<h3>Description</h3>

<p>Prints some important parts of the output object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'curve_test'
print(x, digits=4, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.curve_test_+3A_x">x</code></td>
<td>

<p>An object of class <code>curve_test</code> created by the <code>adjusted_curve_test</code> function.
</p>
</td></tr>
<tr><td><code id="print.curve_test_+3A_digits">digits</code></td>
<td>

<p>How many digits to use when rounding the results.
</p>
</td></tr>
<tr><td><code id="print.curve_test_+3A_...">...</code></td>
<td>

<p>Currently not used.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Silently returns the <code>data.frame</code> which can be seen when calling the function. <code>ABC</code> is an abbreviation for &quot;area between the curves&quot;.
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjusted_curve_test">adjusted_curve_test</a></code>, <code><a href="#topic+adjustedsurv">adjustedsurv</a></code>, <code><a href="#topic+adjustedcif">adjustedcif</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See ?adjusted_curve_diff
</code></pre>

<hr>
<h2 id='sim_confounded_crisk'>
Simulate Competing Risks Data with Confounders
</h2><span id='topic+sim_confounded_crisk'></span>

<h3>Description</h3>

<p>A function to simulate time-to-event data with multiple competing causes of failure and one or multiple confounders. The user can specify both the relationship between the covariates and the cause-specific survival time and the relationship between the covariates and the treatment assignment probability. Random censoring based on a custom function may also be introduced. Can be used for simulation studies or to showcase the usage of the adjusted CIF methodology presented in this package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim_confounded_crisk(n=500, lcovars=NULL, outcome_betas=NULL,
                     group_beta=c(1, 0), gamma=c(1.8, 1.8),
                     lambda=c(2, 2), treatment_betas=NULL,
                     intercept=-0.5, gtol=0.001,
                     cens_fun=function(n){stats::rweibull(n, 1, 2)},
                     cens_args=list(), max_t=1.7)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim_confounded_crisk_+3A_n">n</code></td>
<td>

<p>An integer specifying the sample size of the simulated data set.
</p>
</td></tr>
<tr><td><code id="sim_confounded_crisk_+3A_lcovars">lcovars</code></td>
<td>

<p>A named list to specify covariates. Each list element should be a vector containing information on the desired covariate distribution. See details.
</p>
</td></tr>
<tr><td><code id="sim_confounded_crisk_+3A_outcome_betas">outcome_betas</code></td>
<td>

<p>A list of numeric vectors of beta coefficients for the cause-specific time-to-event outcome. The list has to be of the same length as the      <code>lcovars</code> list and every entry of it has to be a numeric vector with one entry for each cause of failure. See details.
</p>
</td></tr>
<tr><td><code id="sim_confounded_crisk_+3A_group_beta">group_beta</code></td>
<td>

<p>A numeric vector containing specifying the beta coefficients of the grouping variable on the cause-specific survival time. Should contain one entry for every cause of failure.
</p>
</td></tr>
<tr><td><code id="sim_confounded_crisk_+3A_gamma">gamma</code></td>
<td>

<p>A numeric parameter for the simulation of the survival time using a weibull distribution. See details.
</p>
</td></tr>
<tr><td><code id="sim_confounded_crisk_+3A_lambda">lambda</code></td>
<td>

<p>A numeric parameter for the simulation of the survival time using a weibull distribution. See details.
</p>
</td></tr>
<tr><td><code id="sim_confounded_crisk_+3A_treatment_betas">treatment_betas</code></td>
<td>

<p>A named numeric vector of beta coefficients for the treatment assignment model.
</p>
</td></tr>
<tr><td><code id="sim_confounded_crisk_+3A_intercept">intercept</code></td>
<td>

<p>The intercept of the treatment assignment model.
</p>
</td></tr>
<tr><td><code id="sim_confounded_crisk_+3A_gtol">gtol</code></td>
<td>

<p>Tolerance at which estimated treatment assignment probabilities are truncated.
</p>
</td></tr>
<tr><td><code id="sim_confounded_crisk_+3A_cens_fun">cens_fun</code></td>
<td>

<p>A function to generate censoring times. The function needs to take at least one argument called <code>n</code>. Additional arguments are allowed and can be supplied using the <code>cens_args</code> argument.
</p>
</td></tr>
<tr><td><code id="sim_confounded_crisk_+3A_cens_args">cens_args</code></td>
<td>

<p>A list of named arguments passed to <code>cens_fun</code>.
</p>
</td></tr>
<tr><td><code id="sim_confounded_crisk_+3A_max_t">max_t</code></td>
<td>

<p>A number denoting the maximum follow-up time. Every event time bigger than this threshold are censored. In contrast to the single event survival simulation, this value actually has to be supplied as it is used in the numerical inversion step. Theoretically <code>Inf</code> can be used, but this might not work in practice.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The simulation of the confounded competing risks data has five main steps: (1) Generation of covariates, (2) Assigning the treatment variable, (3) Generating a cause-specific survival time (4) Generating the corresponding cause of failure and (5) Introducing censoring.
</p>
<p>First, covariates are generated by taking independent <code>n</code> random samples from the distributions defined in <code>lcovars</code>.
</p>
<p>In the second step the generated covariates are used to estimate the probability of receiving treatment (the propensity score) for each simulated person in the dataset. This is done using a logistic regression model, using the values in <code>treatment_betas</code> as coefficients and <code>interecept</code> as the intercept. By changing the intercept, the user can vary the proportion of cases that end up in each treatment group on average. The estimated probabilities are then used to generate the treatment variable (&quot;group&quot;), making the treatment assignment dependent on the covariates.
</p>
<p>Next, survival times are generated based on the method described in Beyersman et al. (2009) using the causal coefficients defined in <code>outcome_betas</code> and <code>group_beta</code>. After a survival time has been generated a corresponding cause of failure is drawn from a multinomial distribution with probabilities defined by the all cause hazard and the cause-specific hazards. More details can be found in the cited literature. Both the independently generated covariates and the covariate-dependent treatment variable are used in this step. This introduces confounding.
</p>
<p>Independent right-censoring is introduced by taking <code>n</code> independent random draws from some distribution defined by <code>cens_fun</code> and censoring every individual whose censoring time is smaller than its simulated survival time. The whole process is based on work from Chatton et al. (2020).
</p>
<p>Currently only supports binary treatments and does not allow dependent censoring.
</p>


<h3>Value</h3>

<p>Returns a <code>data.frame</code> object containing the simulated covariates, the event indicator (&quot;event&quot;), the survival/censoring time (&quot;time&quot;) and the group variable (&quot;group&quot;).
</p>


<h3>Author(s)</h3>

<p>The code for step (3) and (4) described in the details was taken from the <span class="pkg">survsim</span> R-Package, written by David Morina Soler (with slight modifications). The rest of the function was written by Robin Denz.
</p>


<h3>References</h3>

<p>Jan Beyersmann, Arélien Latouche, Anika Buchholz, and Martin Schumacher (2009). &quot;Simulating Competing Risks Data in Survival Analysis&quot;. In: Statistics in Medicine 28, pp. 956-971
</p>
<p>D. Morina and A. Navarro (2017). &quot;Competing Risks Simulation with the survsim R Package&quot;. In: Communications in Statistics: Simulation and Computation 46.7, pp. 5712-5722
</p>
<p>Arthur Chatton, Florent Le Borgne, Clémence Leyrat, and Yohann Foucher (2020). G-Computation and Inverse Probability Weighting for Time-To-Event Outcomes: A Comparative Study. arXiv:2006.16859v1
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)

set.seed(42)

# simulate data with default values
sim_dat &lt;- sim_confounded_crisk(n=10)

# set group betas to 0
sim_dat &lt;- sim_confounded_crisk(n=10, group_beta=c(0, 0))

# set some custom values
outcome_betas &lt;- list(c(0.03, 0.4),
                      c(1.1, 0.8),
                      c(0, 0),
                      c(-0.2, -0.4),
                      c(log(1.3), log(1.3)/3),
                      c(0, 0))

treatment_betas &lt;- c(x1=0, x2=log(3), x3=log(1.2),
                     x4=0, x5=log(1.1), x6=log(1.4))

lcovars &lt;- list(x1=c("rbinom", 1, 0.3),
                x2=c("rbinom", 1, 0.7),
                x3=c("rbinom", 1, 0.5),
                x4=c("rnorm", 0, 1),
                x5=c("rnorm", 0, 1.1),
                x6=c("rnorm", 0, 0.9))

sim_dat &lt;- sim_confounded_crisk(n=10,
                                treatment_betas=treatment_betas,
                                outcome_betas=outcome_betas,
                                lcovars=lcovars)
</code></pre>

<hr>
<h2 id='sim_confounded_surv'>
Simulate Survival Data with Confounders
</h2><span id='topic+sim_confounded_surv'></span>

<h3>Description</h3>

<p>A function to simulate time-to-event data with one or multiple confounders. The user can specify both the relationship between the covariates and the survival time and the relationship between the covariates and the treatment assignment probability. Random censoring based on a custom function may also be introduced. Can be used for simulation studies or to showcase the usage of the adjusted survival curve methodology presented in this package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim_confounded_surv(n=500, lcovars=NULL, outcome_betas=NULL,
                    group_beta=-1, surv_dist="weibull",
                    gamma=1.8, lambda=2, treatment_betas=NULL,
                    intercept=-0.5, gtol=0.001,
                    cens_fun=function(n){stats::rweibull(n, 1, 2)},
                    cens_args=list(), max_t=Inf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim_confounded_surv_+3A_n">n</code></td>
<td>

<p>An integer specifying the sample size of the simulated data set.
</p>
</td></tr>
<tr><td><code id="sim_confounded_surv_+3A_lcovars">lcovars</code></td>
<td>

<p>A named list to specify covariates. Each list element should be a vector containing information on the desired covariate distribution. See details.
</p>
</td></tr>
<tr><td><code id="sim_confounded_surv_+3A_outcome_betas">outcome_betas</code></td>
<td>

<p>A named numeric vector of beta coefficients for the time-to-event outcome.
</p>
</td></tr>
<tr><td><code id="sim_confounded_surv_+3A_group_beta">group_beta</code></td>
<td>

<p>A number specifying the beta coefficient of the grouping variable on the survival time.
</p>
</td></tr>
<tr><td><code id="sim_confounded_surv_+3A_surv_dist">surv_dist</code></td>
<td>

<p>A character string denoting the distribution used in the simulation of the survival time. See details.
</p>
</td></tr>
<tr><td><code id="sim_confounded_surv_+3A_gamma">gamma</code></td>
<td>

<p>A numeric parameter for the simulation of the survival time. See details.
</p>
</td></tr>
<tr><td><code id="sim_confounded_surv_+3A_lambda">lambda</code></td>
<td>

<p>A numeric parameter for the simulation of the survival time. See details.
</p>
</td></tr>
<tr><td><code id="sim_confounded_surv_+3A_treatment_betas">treatment_betas</code></td>
<td>

<p>A named numeric vector of beta coefficients for the treatment assignment model.
</p>
</td></tr>
<tr><td><code id="sim_confounded_surv_+3A_intercept">intercept</code></td>
<td>

<p>The intercept of the treatment assignment model.
</p>
</td></tr>
<tr><td><code id="sim_confounded_surv_+3A_gtol">gtol</code></td>
<td>

<p>Tolerance at which estimated treatment assignment probabilities are truncated.
</p>
</td></tr>
<tr><td><code id="sim_confounded_surv_+3A_cens_fun">cens_fun</code></td>
<td>

<p>A function to generate censoring times or <code>NULL</code>. If <code>NULL</code>, no censoring is introduced.
</p>
</td></tr>
<tr><td><code id="sim_confounded_surv_+3A_cens_args">cens_args</code></td>
<td>

<p>Arguments passed to <code>cens_fun</code>. Ignored if <code>cens_fun=NULL</code>.
</p>
</td></tr>
<tr><td><code id="sim_confounded_surv_+3A_max_t">max_t</code></td>
<td>

<p>A number denoting the maximum follow-up time. Every event time bigger than this threshold are censored.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The simulation of the confounded survival data has four main steps: (1) Generation of covariates, (2) Assigning the treatment variable, (3) Generating survival times and (4) introducing censoring.
</p>
<p>First, covariates are generated by taking independent <code>n</code> random samples from the distributions defined in <code>lcovars</code>.
</p>
<p>In the second step the generated covariates are used to estimate the probability of receiving treatment (the propensity score) for each simulated person in the dataset. This is done using a logistic regression model, using the values in <code>treatment_betas</code> as coefficients and <code>interecept</code> as the intercept. By changing the intercept, the user can vary the proportion of cases that end up in each treatment group on average. The estimated probabilities are then used to generate the treatment variable (&quot;group&quot;), making the treatment assignment dependent on the covariates.
</p>
<p>Next, survival times are generated based on the method described in Bender et al. (2005) using the causal coefficients defined in <code>outcome_betas</code> and <code>group_beta</code>. Both the independently generated covariates and the covariate-dependent treatment variable are used in this step. This introduces confounding.
</p>
<p>Independent right-censoring is introduced by taking <code>n</code> independent random draws from some distribution defined by <code>cens_fun</code> and censoring every individual whose censoring time is smaller than its simulated survival time. The whole process is based on work from Chatton et al. (2020).
</p>
<p>Currently only supports binary treatments and does not allow dependent censoring.
</p>


<h3>Value</h3>

<p>Returns a <code>data.frame</code> object containing the simulated covariates, the event indicator (&quot;event&quot;), the survival/censoring time (&quot;time&quot;) and the group variable (&quot;group&quot;).
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>Ralf Bender, Thomas Augustin, and Maria Blettner (2005). &quot;Generating Survival Times to Simulate Cox Proportional Hazards Models&quot;. In: Statistics in Medicine 24.11, pp. 1713-1723
</p>
<p>Arthur Chatton, Florent Le Borgne, Clémence Leyrat, and Yohann Foucher (2020). G-Computation and Inverse Probability Weighting for Time-To-Event Outcomes: A Comparative Study. arXiv:2006.16859v1
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)

set.seed(42)

# simulate data with default values
sim_dat &lt;- sim_confounded_surv(n=10)

# simulate data with some new values
lcovars &lt;- list(x1=c("rnorm", 1, 2),
                x2=c("rnorm", 3, 4),
                x3=c("runif", 1, 2))
treatment_betas &lt;- c(x1=0.2, x2=0.6, x3=-0.9)
outcome_betas &lt;- c(x1=1.1, x2=0, x3=-0.3)

sim_dat &lt;- sim_confounded_surv(n=10, lcovars=lcovars,
                               treatment_betas=treatment_betas,
                               outcome_betas=outcome_betas)
</code></pre>

<hr>
<h2 id='surv_aiptw'>
Augmented Inverse Probability of Treatment Weighted Survival Curves
</h2><span id='topic+surv_aiptw'></span>

<h3>Description</h3>

<p>This page explains the details of estimating augmented inverse probability of treatment weighted survival curves for single event time-to-event data (<code>method="aiptw"</code> in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function). All regular arguments of the <code>adjustedsurv</code> function can be used. Additionally, the <code>outcome_model</code> argument and the <code>treatment_model</code> argument have to be specified in the <code>adjustedsurv</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv_aiptw_+3A_outcome_model">outcome_model</code></td>
<td>

<p>[<strong>required</strong>] Must be a <code>coxph</code> model object, modeling the time-to-event mechanism. See details and examples.
</p>
</td></tr>
<tr><td><code id="surv_aiptw_+3A_treatment_model">treatment_model</code></td>
<td>

<p>[<strong>required</strong>] Must be a <code>glm</code> model object with <code>variable</code> as response variable. See details and examples.
</p>
</td></tr>
<tr><td><code id="surv_aiptw_+3A_censoring_model">censoring_model</code></td>
<td>

<p>Must be a <code>coxph</code> model object, modeling the censoring mechanism or <code>NULL</code>. If <code>NULL</code> (default) independent censoring is assumed. See details and examples.
</p>
</td></tr>
<tr><td><code id="surv_aiptw_+3A_verbose">verbose</code></td>
<td>

<p>Whether to print estimation information of the <code><a href="riskRegression.html#topic+ate">ate</a></code> function in the <span class="pkg">riskRegression</span> package. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="surv_aiptw_+3A_...">...</code></td>
<td>

<p>Further arguments passed to <code><a href="riskRegression.html#topic+ate">ate</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Requires both a treatment assignment model (<code><a href="stats.html#topic+glm">glm</a></code>) and a outcome model (<code><a href="survival.html#topic+coxph">coxph</a></code>). Also allows, but does not rely on, an additional model describing the censoring mechanism (also a <code><a href="survival.html#topic+coxph">coxph</a></code> object).
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Currently only two groups in <code>variable</code> are allowed. Must still be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are available.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are not guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are not guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the <span class="pkg">riskRegression</span> package.
</p>
</li></ul>

<p>Instead of only modeling the outcome mechanism or the treatment assignment mechanism, both kind of models are required to use this method. If either of those models are correctly specified, unbiased estimates will be obtained. Can also be used to adjust for dependent censoring using another Cox-Regression model. An obvious advantage of this method is it's doubly robust property. This however comes at the price of some efficiency. It is also possible that some estimates fall outside the 0 and 1 probability bounds, particularly if the time is near 0 or the maximal observed event time. There is also no guarantee that the estimated survival curves will be monotonically decreasing. For more information on the methods the user is referred to the literature listed in the references.
</p>
<p>This function is basically just a wrapper around the <code><a href="riskRegression.html#topic+ate">ate</a></code> function from the <span class="pkg">riskRegression</span> package. Additional arguments may be passed to that function using the <code>...</code> syntax. It is however recommended to use <code><a href="riskRegression.html#topic+ate">ate</a></code> directly in these cases.
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedsurv</code> function:
</p>

<ul>
<li> <p><code>ate_object</code>: The object returned by the <code>ate</code> function.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>The wrapper function was written by Robin Denz, the <code>ate</code> function (which this wrapper is build around) was written by other people. See <code>?ate</code> for more details.
</p>


<h3>References</h3>

<p>James M. Robins and Andrea Rotnitzky (1992). &quot;Recovery of Information and Adjustment for Dependent Censoring Using Surrogate Markers&quot;. In: AIDS Epidemiology: Methodological Issues. Ed. by Nicholas P. Jewell, Klaus Dietz, and Vernon T. Farewell. New York: Springer Science + Business Media, pp. 297-331
</p>
<p>Alan E. Hubbard, Mark J. van der Laan, and James M. Robins (2000). &quot;Nonparametric Locally Efficient Estimation of the Treatment Specific Survival Distribution with Right Censored Data and Covariates in Observational Studies&quot;. In: Statistical Models in Epidemiology, the Environment, and Clinical Trials. Ed. by M. Elizabeth Halloran and Donald Berry. New York: Springer Science + Business Media, pp. 135-177
</p>
<p>Min Zhang and Douglas E. Schaubel (2012). &quot;Contrasting Treatment-Specific Survival Using Double-Robust Estimators&quot;. In: Statistics in Medicine 31.30, pp. 4255-4268
</p>
<p>Xiaofei Bai, Anastasios A. Tsiatis, and Sean M. O’Brien (2013). &quot;Doubly-Robust Estimators of Treatment-Specific Survival Distributions in Observational Studies with Stratified Sampling&quot;. In: Biometrics 69, pp. 830–839
</p>
<p>Brice Maxime Hugues Ozenne, Thomas Harder Scheike, and Laila Staerk (2020). &quot;On the Estimation of Average Treatment Effects with Right-Censored Time to Event Outcome and Competing Risks&quot;. In: Biometrical Journal 62, pp. 751–763
</p>


<h3>See Also</h3>

<p><code><a href="riskRegression.html#topic+ate">ate</a></code>, <code><a href="survival.html#topic+coxph">coxph</a></code>, <code><a href="stats.html#topic+glm">glm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(riskRegression)
library(survival)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a cox-regression for the outcome
cox_mod &lt;- coxph(Surv(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group,
                 data=sim_dat, x=TRUE)

# estimate a treatment assignment model
glm_mod &lt;- glm(group ~ x1 + x3 + x5 + x6, data=sim_dat, family="binomial")

# use it to calculate adjusted survival curves
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="aiptw",
                        outcome_model=cox_mod,
                        treatment_model=glm_mod,
                        conf_int=FALSE)

# plot the curves
plot(adjsurv)
</code></pre>

<hr>
<h2 id='surv_aiptw_pseudo'>
Augmented Inverse Probability of Treatment Weighted Survival Curves using Pseudo-Values
</h2><span id='topic+surv_aiptw_pseudo'></span>

<h3>Description</h3>

<p>This page explains the details of estimating augmented inverse probability of treatment weighted survival curves using Pseudo-Values for single event time-to-event data (<code>method="aiptw_pseudo"</code> in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function). All regular arguments of the <code>adjustedsurv</code> function can be used. Additionally, the <code>outcome_vars</code> argument and the <code>treatment_model</code> argument have to be specified in the <code>adjustedsurv</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv_aiptw_pseudo_+3A_outcome_vars">outcome_vars</code></td>
<td>

<p>[<strong>required</strong>] A character vector of column names specifying variables to be used when modeling the outcome mechanism using <code><a href="geepack.html#topic+geese">geese</a></code>. See details and examples.
</p>
</td></tr>
<tr><td><code id="surv_aiptw_pseudo_+3A_treatment_model">treatment_model</code></td>
<td>

<p>[<strong>required</strong>] Must be a <code>glm</code> or <code>multinom</code> model object with <code>variable</code> as response variable. Alternatively you can supply a numeric vector of propensity scores directly. See details and examples.
</p>
</td></tr>
<tr><td><code id="surv_aiptw_pseudo_+3A_type_time">type_time</code></td>
<td>

<p>A character string specifying how the time should be modeled. Possible values are <code>"factor"</code> (modeling each point in time as a separate variable, the default), <code>"bs"</code> (modeling time using B-Splines) or <code>"ns"</code> (modeling time using natural splines).
</p>
</td></tr>
<tr><td><code id="surv_aiptw_pseudo_+3A_spline_df">spline_df</code></td>
<td>

<p>The number of degrees of freedom used for the natural-spline or B-spline function. Ignored if <code>type_time="factor"</code>. Defaults to 5.
</p>
</td></tr>
<tr><td><code id="surv_aiptw_pseudo_+3A_censoring_vars">censoring_vars</code></td>
<td>

<p>An optional character vector specifying variables in <code>data</code>. Those are used in the calculation of inverse probability of censoring weighted pseudo observations. See <code>?pseudo_aareg</code> for more information. Set to <code>NULL</code> (default) to use standard pseudo-values without corrections for dependent censoring instead.
</p>
</td></tr>
<tr><td><code id="surv_aiptw_pseudo_+3A_ipcw_method">ipcw_method</code></td>
<td>

<p>The specific method used in the calculation of inverse probability of censoring weighted pseudo observations. Can be either <code>"binder"</code> (default) or <code>"hajek"</code>. See <code>?pseudo_aareg</code> for more information. Ignored if <code>censoring_vars=NULL</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Requires a treatment assignment model (<code><a href="stats.html#topic+glm">glm</a></code> or <code><a href="nnet.html#topic+multinom">multinom</a></code>) and a character vector of variable names used to model the outcome mechanism (internally uses <code><a href="geepack.html#topic+geese">geese</a></code>). Covariate-Dependent censoring can be corrected for using inverse probability of censoring weighted pseudo-values (Binder et al. 2014)
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Any number of levels in <code>variable</code> are allowed. Must be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are available.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are not guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are not guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the <span class="pkg">geepack</span> and <span class="pkg">prodlim</span> packages. Additionally requires the <span class="pkg">eventglm</span> package if <code>censoring_vars</code> is specified.
</p>
</li></ul>

<p>Instead of only modeling the outcome mechanism or the treatment assignment mechanism, both kind of models are required to use this method. If either of those models are correctly specified, unbiased estimates will be obtained. In contrast to the <code><a href="#topic+surv_aiptw">&quot;aiptw&quot;</a></code> method, the &quot;aiptw_pseudo&quot; method uses a generalized estimation equation (geese) approach to model the outcome mechanism. The model is fit in the same way as described in the <code><a href="#topic+surv_direct_pseudo">&quot;direct_pseudo&quot;</a></code> method. Those Direct Standardization based estimates are then transformed using the previously estimated propensity score. This results in the doubly-robust property of the method. More information on this particular method can be found in the original article by Wang (2018), more information on Pseudo-Values is available in Andersen et al. (2017) and Andersen and Perme (2010).
</p>
<p>When estimating the <code>geese</code> model the <code>ev_time</code> variable is used as a factor by default. This results in one coefficient being estimated for each unique point in time, which can be very slow computationally if there are a lot of unique points in time and/or the dataset has many rows. In these cases it is recommended to use <code>type_time="bs"</code> or <code>type_time="ns"</code>, which results in the <code>ev_time</code> being modeled using B-Splines or Natural Splines. Simulation studies indicate that there is little difference in the estimates when an appropriately large number of <code>spline_df</code> is used.
</p>
<p>Additionally, covariate-dependent censoring can be accounted for by using inverse probability of censoring weighted pseudo-values (Binder et al. 2014) instead of regular pseudo-values (specified using the <code>censoring_vars</code> and <code>ipcw_method</code> arguments).
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedsurv</code> function:
</p>

<ul>
<li> <p><code>pseudo_values</code>: The matrix of estimated pseudo-values.
</p>
</li>
<li> <p><code>geese_model</code>: The geese model used to make the predictions.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Jixian Wang supplied the R source code used in the original article, which was used by Robin Denz to create a generalized version of this method with additional functionality and improved performance.
</p>


<h3>References</h3>

<p>Jixian Wang (2018). &quot;A Simple, Doubly Robust, Efficient Estimator for Survival Functions Using Pseudo Observations&quot;. In: Pharmaceutical Statistics 17.38-48
</p>
<p>James M. Robins and Andrea Rotnitzky (1992). &quot;Recovery of Information and Adjustment for Dependent Censoring Using Surrogate Markers&quot;. In: AIDS Epidemiology: Methodological Issues. Ed. by Nicholas P. Jewell, Klaus Dietz, and Vernon T. Farewell. New York: Springer Science + Business Media, pp. 297-331
</p>
<p>Per Kragh Andersen, Elisavet Syriopoulou, and Erik T. Parner (2017). &quot;Causal Inference in Survival Analysis using Pseudo-Observations&quot;. In: Statistics in Medicine 36, pp. 2669-2681
</p>
<p>Per Kragh Andersen and Maja Pohar Perme (2010). &quot;Pseudo-Observations in Survival Analysis&quot;. In: Statistical Methods in Medical Research 19, pp. 71-99
</p>
<p>Aris Perperoglou, Willi Sauerbrei, Michal Abrahamowicz, and Matthias Schmid (2019). &quot;A Review of Spline Function Procedures in R&quot;. in: BMC Medical Research Methodology 19.46, pp. 1-16
</p>
<p>Nadine Binder, Thomas A. Gerds, and Per Kragh Andersen (2014). &quot;Pseudo- Observations for Competing Risks with Covariate Dependent Censoring&quot;. In: Lifetime Data Analysis 20, pp. 303-315
</p>


<h3>See Also</h3>

<p><code><a href="geepack.html#topic+geese">geese</a></code>, <code><a href="prodlim.html#topic+jackknife">jackknife</a></code>, <code><a href="splines.html#topic+ns">ns</a></code>, <code><a href="splines.html#topic+bs">bs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a treatment assignment model
glm_mod &lt;- glm(group ~ x1 + x3 + x5 + x6, data=sim_dat, family="binomial")

# use it + pseudo values + geese model to calculate adjusted survival curves
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="aiptw_pseudo",
                        outcome_vars=c("x1", "x2", "x3", "x4", "x5", "x6"),
                        treatment_model=glm_mod,
                        conf_int=TRUE)

# plot the curves
plot(adjsurv, conf_int=TRUE)
</code></pre>

<hr>
<h2 id='surv_direct'>
Direct Adjusted Survival Curves
</h2><span id='topic+surv_direct'></span>

<h3>Description</h3>

<p>This page explains the details of estimating confounder-adjusted survival curves using a previously fit Cox-Regression model for single event time-to-event data using Direct Standardization (<code>method="direct"</code> in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function). All regular arguments of the <code>adjustedsurv</code> function can be used. Additionally, the <code>outcome_model</code> argument has to be specified in the <code>adjustedsurv</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv_direct_+3A_outcome_model">outcome_model</code></td>
<td>

<p>[<strong>required</strong>] Must be a previously fit model object including <code>variable</code> as independent variable. Apart from the classic <code>coxph</code> model this function also supports a variety of other models. See <code><a href="#topic+models_surv_direct">models_surv_direct</a></code> for a list of supported model objects and some more details.
</p>
</td></tr>
<tr><td><code id="surv_direct_+3A_verbose">verbose</code></td>
<td>

<p>Whether to print estimation information of the <code>ate</code> function in the <span class="pkg">riskRegression</span> package. Ignored if <code>outcome_model</code> is not a <code>coxph</code> object. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="surv_direct_+3A_predict_fun">predict_fun</code></td>
<td>

<p>A function which should be used to calculate the predicted survival probabilities given covariates and some points in time. This argument only needs to be specified if the kind of model supplied in the <code>outcome_model</code> is not directly supported. See <code><a href="#topic+models_surv_direct">models_surv_direct</a></code> for more information. Defaults to <code>NULL</code>.
</p>
</td></tr>
<tr><td><code id="surv_direct_+3A_...">...</code></td>
<td>

<p>Further arguments passed to <code>ate</code> if <code>outcome_model</code> is a <code>coxph</code> object. Otherwise the additional arguments are passed to the respective <code>predict</code> method. See <code><a href="#topic+models_surv_direct">models_surv_direct</a></code> for more information.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Requires a model describing the outcome mechanism. See <code><a href="#topic+models_surv_direct">models_surv_direct</a></code> for a list of supported model objects and some more details.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Any number of levels in <code>variable</code> are allowed. Must be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are available only if <code>outcome_model</code> is a <code>coxph</code> object. The <code><a href="riskRegression.html#topic+ate">ate</a></code> function is used for the calculation in that case. Bootstrap confidence intervals can however be calculated with all supported models. See <code>?adjustedsurv</code> for more information on bootstrapping.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the <span class="pkg">riskRegression</span> package. Depending on <code>outcome_model</code> other packages might be needed. See <code><a href="#topic+models_surv_direct">models_surv_direct</a></code> for more details.
</p>
</li></ul>

<p>This method works by executing the following steps: (1) First a model is fitted which describes the outcome mechanism (time-to-event). Next (2) multiple copies of the original dataset are created, one for each possible level of the <code>variable</code> of interest. (3) The <code>variable</code> is then set to one level for all observations in each dataset. (4) The model is used to predict the survival probabilities at some points in time T for each observation in all dataset copies. (5) Those estimated probabilities are averaged for each dataset at each point in time, resulting in adjusted survival probabilities for all levels of the group variable at the specified points in time.
</p>
<p>In the literature this method is sometimes called &quot;Direct Standardization&quot;, &quot;Corrected Group-Prognosis&quot;, &quot;G-Computation&quot; or &quot;G-Formula&quot;. If the model in step (1) is &quot;correct&quot;&quot; this method will produce unbiased estimates of the counterfactual survival curves. A model can be called a &quot;correct&quot; model in this context if it can be used to produce unbiased estimates of the true (but unknown) individual survival probabilities given covariates. When used properly this is one of the most efficient methods. More information can be found in the literature listed in the references. The most popular model for describing the outcome mechanism in a time-to-event context is the Cox-regression model (<code><a href="survival.html#topic+coxph">coxph</a></code>). This function however also supports a variety of other models.
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedsurv</code> function:
</p>

<ul>
<li> <p><code>ate_object</code>: The object returned by the <code>ate</code> function.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>The function itself was written by Robin Denz. When using <code>coxph</code> models however, this function is just a wrapper around the <code><a href="riskRegression.html#topic+ate">ate</a></code> function, which was written by other people. See <code>?ate</code> for more information.
</p>


<h3>References</h3>

<p>I-Ming Chang, Rebecca Gelman, and Marcello Pagano (1982). &quot;Corrected Group Prognostic Curves and Summary Statistics&quot;. In: Journal of Chronic Diseases 35, pp. 669-674
</p>
<p>Robert W. Makuch (1982). &quot;Adjusted Survival Curve Estimation Using Covariates&quot;. In: Journal of Chronic Diseases 35.6, pp. 437-443
</p>
<p>Xu Zhang, Fausto R. Loberiza, John P. Klein, and Mei-Jie Zhang (2007). &quot;A SAS Macro for Estimation of Direct Adjusted Survival Curves Based on a Stratified Cox Regression Model&quot;. In: Computer Methods and Programs in Biomedicine 88, pp. 95-101
</p>


<h3>See Also</h3>

<p><code><a href="#topic+models_surv_direct">models_surv_direct</a></code>, <code><a href="riskRegression.html#topic+ate">ate</a></code>, <code><a href="survival.html#topic+coxph">coxph</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)
library(riskRegression)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a cox-regression for the outcome
cox_mod &lt;- coxph(Surv(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group,
                 data=sim_dat, x=TRUE)

# use it to calculate adjusted survival curves
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="direct",
                        outcome_model=cox_mod,
                        conf_int=FALSE)

# plot the curves
plot(adjsurv)

# not run to avoid dependency on flexsurv and mice too slow
if (interactive()) {
## using a flexsurv() model, this requires the 'fleysurv' package
mod_flexsurvreg &lt;- flexsurvreg(Surv(time, event) ~ group + x1 + x2 + x5 + x6,
                               data=sim_dat, dist="gengamma")

# using it to calculate the adjusted survival curves
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="direct",
                        outcome_model=mod_flexsurvreg,
                        conf_int=FALSE)

# plot using steps=FALSE to draw them as smooth functions, since
# they were estimated using a parametric model
plot(adjsurv, steps=FALSE)

## using multiple imputation
library(mice)

# introduce random missingness in x1 as example
# NOTE: This is only done as an example, in reality you would
#       already have missing data, not introduce it yourself.
sim_dat$x1 &lt;- ifelse(runif(n=50) &lt; 0.5, sim_dat$x1, NA)

# perform multiple imputation
mids &lt;- mice::mice(data=sim_dat, method="pmm", m=5, printFlag=FALSE)

# fit model for each imputed dataset
mira &lt;- with(mids, coxph(Surv(time, event) ~ x1 + x2 + x3 + x4 + x5 + x6 + group,
                         x=TRUE))

# calculate adjusted survival curves on imputed data
adj &lt;- adjustedsurv(data=mids,
                    variable="group",
                    ev_time="time",
                    event="event",
                    method="direct",
                    outcome_model=mira)
plot(adj)
}
</code></pre>

<hr>
<h2 id='surv_direct_pseudo'>
Direct Adjusted Survival Curves using Pseudo-Values
</h2><span id='topic+surv_direct_pseudo'></span>

<h3>Description</h3>

<p>This page explains the details of estimating direct adjusted survival curves using pseudo-values for single event time-to-event data (<code>method="direct_pseudo"</code> in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function). All regular arguments of the <code>adjustedsurv</code> function can be used. Additionally, the <code>outcome_vars</code> argument has to be specified in the <code>adjustedsurv</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv_direct_pseudo_+3A_outcome_vars">outcome_vars</code></td>
<td>

<p>[<strong>required</strong>] A character vector of column names specifying variables to be used when modeling the outcome mechanism. See details and examples.
</p>
</td></tr>
<tr><td><code id="surv_direct_pseudo_+3A_type_time">type_time</code></td>
<td>

<p>A character string specifying how the time should be modeled. Possible values are <code>"factor"</code> (modeling each point in time as a separate variable, the default), <code>"bs"</code> (modeling time using B-Splines) or <code>"ns"</code> (modeling time using natural splines).
</p>
</td></tr>
<tr><td><code id="surv_direct_pseudo_+3A_spline_df">spline_df</code></td>
<td>

<p>The number of degrees of freedom used for the natural-spline or B-spline function. Ignored if <code>type_time="factor"</code>. Defaults to 5.
</p>
</td></tr>
<tr><td><code id="surv_direct_pseudo_+3A_censoring_vars">censoring_vars</code></td>
<td>

<p>An optional character vector specifying variables in <code>data</code>. Those are used in the calculation of inverse probability of censoring weighted pseudo observations. See <code>?pseudo_aareg</code> for more information. Set to <code>NULL</code> (default) to use standard pseudo-values without corrections for dependent censoring instead.
</p>
</td></tr>
<tr><td><code id="surv_direct_pseudo_+3A_ipcw_method">ipcw_method</code></td>
<td>

<p>The specific method used in the calculation of inverse probability of censoring weighted pseudo observations. Can be either <code>"binder"</code> (default) or <code>"hajek"</code>. See <code>?pseudo_aareg</code> for more information. Ignored if <code>censoring_vars=NULL</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Requires a character vector of variable names used to model the outcome mechanism (internally uses <code><a href="geepack.html#topic+geese">geese</a></code>).
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Any number of levels in <code>variable</code> are allowed. Must be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are not available. Bootstrapping can still be used to estimate the confidence intervals (see <code>?adjustedsurv</code>).
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are not guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are not guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the <span class="pkg">geepack</span> and <span class="pkg">prodlim</span> packages. Additionally requires the <span class="pkg">eventglm</span> package if <code>censoring_vars</code> is specified.
</p>
</li></ul>

<p>This method works by executing the following steps: (1) First Pseudo-Values for the survival probabilities are estimated for each observation in the dataset and some points in time T. Afterwards (2) a new dataset is created in which every individual observation has multiple rows, one for each point in time of interest. (3) This dataset is used to fit a generalized estimating equations (geese) model, using the Pseudo-Values as independent variable. Next (4) multiple copies of the new dataset are created, one for each possible level of the <code>variable</code> of interest. (5) The <code>variable</code> is then set to one level for all observations in each dataset. (5) The <code>geese</code> model is used to predict the survival probabilities at some points in time T for each observation in all dataset copies. (6) Those estimated probabilities are averaged for each dataset at each point in time, resulting in adjusted survival probabilities for all levels of the group variable at the specified points in time.
</p>
<p>It is essentially the same procedure as described in <code><a href="#topic+surv_direct">&quot;direct&quot;</a></code>. The only difference is that instead of relying on a <code>coxph</code> model, this method uses Pseudo-Values and a geese model. This can be useful if the data does not conform to some assumptions needed to use the Cox-Regression model (for example the proportional hazards assumption).
</p>
<p>When estimating the <code>geese</code> model the <code>ev_time</code> variable is used as a factor by default. This results in one coefficient being estimated for each unique point in time, which can be very slow computationally if there are a lot of unique points in time and/or the dataset has many rows. In these cases it is recommended to use <code>type_time="bs"</code> or <code>type_time="ns"</code>, which results in the <code>ev_time</code> being modeled using B-Splines or Natural Splines. Simulation studies indicate that there is little difference in the estimates when an appropriately large number of <code>spline_df</code> is used.
</p>
<p>Additionally, covariate-dependent censoring can be accounted for by using inverse probability of censoring weighted pseudo-values (Binder et al. 2014) instead of regular pseudo-values (specified using the <code>censoring_vars</code> and <code>ipcw_method</code> arguments).
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedsurv</code> function:
</p>

<ul>
<li> <p><code>pseudo_values</code>: The matrix of estimated pseudo-values.
</p>
</li>
<li> <p><code>geese_model</code>: The geese model used to make the predictions.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>Per Kragh Andersen, Elisavet Syriopoulou, and Erik T. Parner (2017). &quot;Causal Inference in Survival Analysis using Pseudo-Observations&quot;. In: Statistics in Medicine 36, pp. 2669-2681
</p>
<p>Per Kragh Andersen and Maja Pohar Perme (2010). &quot;Pseudo-Observations in Survival Analysis&quot;. In: Statistical Methods in Medical Research 19, pp. 71-99
</p>
<p>Aris Perperoglou, Willi Sauerbrei, Michal Abrahamowicz, and Matthias Schmid (2019). &quot;A Review of Spline Function Procedures in R&quot;. in: BMC Medical Research Methodology 19.46, pp. 1-16
</p>
<p>Nadine Binder, Thomas A. Gerds, and Per Kragh Andersen (2014). &quot;Pseudo-Observations for Competing Risks with Covariate Dependent Censoring&quot;. In: Lifetime Data Analysis 20, pp. 303-315
</p>


<h3>See Also</h3>

<p><code><a href="geepack.html#topic+geese">geese</a></code>, <code><a href="prodlim.html#topic+jackknife">jackknife</a></code>, <code><a href="splines.html#topic+ns">ns</a></code>, <code><a href="splines.html#topic+bs">bs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(geepack)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# calculate adjusted survival curves, with time as factor
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="direct_pseudo",
                        outcome_vars=c("x1", "x2", "x3", "x4", "x5", "x6"),
                        type_time="factor",
                        force_bounds=TRUE,
                        iso_reg=TRUE)

# with time modelled as B-Spline using 5 degrees of freedom
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="direct_pseudo",
                        outcome_vars=c("x1", "x2", "x3", "x4", "x5", "x6"),
                        type_time="bs",
                        spline_df=5,
                        force_bounds=TRUE,
                        iso_reg=TRUE)

# plot the curves
plot(adjsurv)
</code></pre>

<hr>
<h2 id='surv_emp_lik'>
Empirical Likelihood Estimation Survival Curves
</h2><span id='topic+surv_emp_lik'></span>

<h3>Description</h3>

<p>This page explains the details of estimating adjusted survival curves using the empirical likelihood estimation methodology introduced by Wang et al. (2019) for single event time-to-event data (<code>method="emp_lik"</code> in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function). All regular arguments of the <code>adjustedsurv</code> function can be used. Additionally, the <code>treatment_vars</code> argument has to be specified in the <code>adjustedsurv</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv_emp_lik_+3A_treatment_vars">treatment_vars</code></td>
<td>

<p>[<strong>required</strong>] A character vector of column names specifying variables to be used as covariates in the empirical likelihood estimation. See details and examples.
</p>
</td></tr>
<tr><td><code id="surv_emp_lik_+3A_moment">moment</code></td>
<td>

<p>A character string specifying which moment to adjust for. Can be either <code>"first"</code> (default) or <code>"second"</code>.
</p>
</td></tr>
<tr><td><code id="surv_emp_lik_+3A_standardize">standardize</code></td>
<td>

<p>A logical variable indicating whether the <code>treatment_vars</code> variables should be standardized. Defaults to <code>FALSE</code>. See details.
</p>
</td></tr>
<tr><td><code id="surv_emp_lik_+3A_gtol">gtol</code></td>
<td>

<p>A number specifying the tolerance for the weights. Is basically only used to avoid division by 0 errors in cases where the weights are estimated to be 0. Defaults to 0.00001.
</p>
</td></tr>
<tr><td><code id="surv_emp_lik_+3A_max_iter">max_iter</code></td>
<td>

<p>Maximum number of iterations allowed in the newton-raphson algorithm. Set to 100 by default which is more than enough in most cases.
</p>
</td></tr>
<tr><td><code id="surv_emp_lik_+3A_newton_tol">newton_tol</code></td>
<td>

<p>Tolerance used in the newton-raphson algorithm. Set to 1.0e-06 by default which is more than enough in most cases.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Requires a character vector of variable names used to balance the distribution of covariates (treatment assignment mechanism)
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust (see details).
</p>
</li>
<li><p><strong>Categorical groups:</strong> Only binary treatments are allowed. The column specified by <code>variable</code> must be a factor variable with exactly two levels.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are not available. Bootstrapping can still be used to estimate the confidence intervals (see <code>?adjustedsurv</code>).
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer survival times.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the <span class="pkg">MASS</span> package. While code from the <span class="pkg">adjKMtest</span> package is used internally (see &lt;https://github.com/kimihua1995/adjKMtest&gt;) it is not necessary to install this package. The code is directly included in this R-Package. If you use this method, please cite the paper by Wang et al. (2019).
</p>
</li></ul>

<p>A non-parametric likelihood based method which does not require the researcher to assume that the data was generated by any known family of distributions. This method works by forcing the moments of the covariates to be equal between treatment groups, through the maximization of a constrained likelihood function. The resulting equality of the distributions removes the bias created by the confounders. This method was proposed by Wang et al. (2019). Since the exact form of both mechanisms are left unspecified, it is more robust to model misspecification than IPTW or direct adjustment.
</p>
<p>The underlying method is theoretically doubly-robust as shown by Wang et al. (2019), but the specific implementation of this method implemented in this package is not as demonstrated in Denz et al. (2022). For example, if some confounder has a quadratic effect on the treatment-assignment but it is only passed to this function as a linear predictor (e.g. without squaring it) this method will produce asymptotically biased estimates.
</p>


<h3>Value</h3>

<p>Adds no additional objects to the output of the <code>adjustedsurv</code> function.
</p>


<h3>Author(s)</h3>

<p>All functions used for the estimation were written by:
</p>
<p>Fangfang Bai, PhD
School of Statistics, University of International Business and Economics, Beijing, China.
</p>
<p>and
</p>
<p>Xiaofei Wang, PhD
Department of Biostatistics and Bioinformatics, Duke University, Durham, NC, USA.
</p>
<p>Robin Denz only performed small changes to that code (documented with code-comments in the source code) and wrote the wrapper function.
</p>


<h3>References</h3>

<p>Xiaofei Wang, Fangfang Bai, Herbert Pang, and Stephen L. George (2019). &quot;Bias-Adjusted Kaplan-Meier Survival Curves for Marginal Treatment Effect in Observational Studies&quot;. In: Journal of Biopharmaceutical Statistics 29.4, pp. 592-605
</p>
<p>Art B. Owen (2001). Empirical Likelihood. Boca Raton: CRC Press
</p>
<p>Robin Denz, Renate Klaaßen-Mielke, and Nina Timmesfeld (2023). &quot;A Comparison of Different Methods to Adjust Survival Curves for Confounders&quot;. In: Statistics in Medicine 42.10, pp. 1461-1479
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjustedsurv">adjustedsurv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# calculate adjusted survival curves
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="emp_lik",
                        treatment_vars=c("x1", "x2", "x3", "x4", "x5", "x6"),
                        moment="first")

# plot the curves
plot(adjsurv)
</code></pre>

<hr>
<h2 id='surv_iptw_cox'>
Inverse Probability of Treatment Weighted Survival using Cox-Regression
</h2><span id='topic+surv_iptw_cox'></span>

<h3>Description</h3>

<p>This page explains the details of estimating inverse probability of treatment weighted survival curves using a weighted univariate cox-regression for single event time-to-event data (<code>method="iptw_cox"</code> in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function). All regular arguments of the <code>adjustedsurv</code> function can be used. Additionally, the <code>treatment_model</code> argument has to be specified in the <code>adjustedsurv</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv_iptw_cox_+3A_treatment_model">treatment_model</code></td>
<td>

<p>[<strong>required</strong>] Must be either a model object with <code>variable</code> as response variable, a vector of weights or a formula which can be passed to <code>WeightIt</code>.
</p>
</td></tr>
<tr><td><code id="surv_iptw_cox_+3A_weight_method">weight_method</code></td>
<td>

<p>Method used in <code>WeightIt</code> function call. Ignored if <code>treatment_model</code> is not a formula object. Defaults to <code>"ps"</code>.
</p>
</td></tr>
<tr><td><code id="surv_iptw_cox_+3A_stabilize">stabilize</code></td>
<td>

<p>Whether to stabilize the weights or not. Is set to <code>FALSE</code> by default. Stabilizing weights ensures that the sum of all weights is equal to the original sample size. It has no effect on point estimates, only on the asymptotic variance calculations and confidence intervals.
</p>
</td></tr>
<tr><td><code id="surv_iptw_cox_+3A_trim">trim</code></td>
<td>

<p>Can be either <code>FALSE</code> (default) or a numeric value at which to trim the weights. If <code>FALSE</code>, weights are used as calculated or supplied. If a numeric value is supplied, all weights that are bigger than <code>trim</code> are set to <code>trim</code> before the analysis is carried out. Useful when some weights are extremely large.
</p>
</td></tr>
<tr><td><code id="surv_iptw_cox_+3A_...">...</code></td>
<td>

<p>Further arguments passed to <code><a href="WeightIt.html#topic+weightit">weightit</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Requires a model describing the treatment assignment mechanism. This must be either a <code><a href="stats.html#topic+glm">glm</a></code> or <code><a href="mgcv.html#topic+multinom">multinom</a></code> object.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Any number of levels in <code>variable</code> are allowed. Must be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are not available.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the <span class="pkg">survival</span> package. Additionally, the <span class="pkg">WeightIt</span> package is required if <code>treatment_model</code> is a formula object.
</p>
</li></ul>

<p>This method works by modeling the treatment assignment mechanism. Adjusted survival curves are calculated by first estimating appropriate case-weights for each observation in <code>data</code>. This can be done using inverse probability of treatment weights using the propensity score (usually estimated using a logistic regression model) or by some other method (see <code>?weightit</code>). Those estimates are then used to fit a weighted Cox-Regression model, stratified by <code>variable</code>. Survival Curves based on this model are estimated using the method implemented in the <code>survfit.coxph</code> function. More information can be found in the literature listed under &quot;references&quot;. The only difference to the <code><a href="#topic+surv_iptw_km">iptw_km</a></code> method is a slightly different weighting approach.
</p>
<p>By default this method uses a a robust sandwich-type variance estimator (<code>robust=TRUE</code> in the <code>coxph</code> function call) to calculate the standard error used in the construction of confidence intervals. This estimator has been shown to be biased by Austin (2016). Coupled with stabilized weights however (<code>stabilize=TRUE</code>) this gives conservative estimates of the variance and confidence intervals (Xu et al. 2010). It is still recommended to use bootstrap confidence intervals instead. This can be done by setting <code>bootstrap=TRUE</code> in the <code>adjustedsurv</code> function call.
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedsurv</code> function:
</p>

<ul>
<li> <p><code>cox_model</code>: The stratified and weighted <code>coxph</code> model.
</p>
</li>
<li> <p><code>survfit</code>: The <code>survfit</code> object created using the <code>cox_model</code> object.
</p>
</li>
<li> <p><code>weights</code>: The final weights used in the analysis.
</p>
</li></ul>

<p>Returns a <code>list</code> object containing a <code>data.frame</code> with the estimated adjusted survival probabilities for some points in time for each level of <code>variable</code>, the weighted <code>coxph</code> model, the weighted <code>survfit</code> object and the weights used in the analysis.
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>Stephen R. Cole and Miguel A. Hernán (2004). &quot;Adjusted Survival Curves with Inverse Probability Weights&quot;. In: Computer Methods and Programs in Biomedicine 2003.75, pp. 45-49
</p>
<p>Peter C. Austin (2016). &quot;Variance Estimation when Using Inverse Probability of Treatment Weighting (IPTW) with Survival Analysis&quot;. In: Statistics in Medicine 35, pp. 5642-5655
</p>
<p>Stanley Xu, Colleen Ross and Marsha A. Raebel, Susan Shetterly, Christopher Blanchette, and David Smith (2010). &quot;Use of Stabilized Inverse Propensity Scores as Weights to Directly Estimate Relative Risk and Its Confidence Intervals&quot;. In: Value in Health 13.2, pp. 273-277
</p>


<h3>See Also</h3>

<p><code><a href="WeightIt.html#topic+weightit">weightit</a></code>, <code><a href="survival.html#topic+coxph">coxph</a></code>, <code><a href="survival.html#topic+survfit.coxph">survfit.coxph</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a treatment assignment model
glm_mod &lt;- glm(group ~ x1 + x3 + x5 + x6, data=sim_dat, family="binomial")

# use it to calculate adjusted survival curves
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="iptw_cox",
                        treatment_model=glm_mod)

# Alternatively, use custom weights
# In this example we use weights calculated using the propensity score,
# which is equal to using the glm model directly in the function
ps_score &lt;- glm_mod$fitted.values
weights &lt;- ifelse(sim_dat$group==1, 1/ps_score, 1/(1-ps_score))

adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="iptw_cox",
                        treatment_model=weights)

# And a third alternative: use the WeightIt package
# here an example with equal results to the ones above:
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="iptw_cox",
                        treatment_model=group ~ x1 + x3 + x5 + x6,
                        weight_method="ps")

# here an example using Entropy Balancing Weighting:
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="iptw_cox",
                        treatment_model=group ~ x1 + x3 + x5 + x6,
                        weight_method="ebal")
</code></pre>

<hr>
<h2 id='surv_iptw_km'>
Inverse Probability of Treatment Weighted Kaplan-Meier estimates
</h2><span id='topic+surv_iptw_km'></span>

<h3>Description</h3>

<p>This page explains the details of estimating inverse probability of treatment weighted survival curves using a weighted version of the Kaplan-Meier estimator for single event time-to-event data (<code>method="iptw_km"</code> in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function). All regular arguments of the <code>adjustedsurv</code> function can be used. Additionally, the <code>treatment_model</code> argument has to be specified in the <code>adjustedsurv</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv_iptw_km_+3A_treatment_model">treatment_model</code></td>
<td>

<p>[<strong>required</strong>] Must be either a model object with <code>variable</code> as response variable, a vector of weights or a formula which can be passed to <code>WeightIt</code>.
</p>
</td></tr>
<tr><td><code id="surv_iptw_km_+3A_weight_method">weight_method</code></td>
<td>

<p>Method used in <code>WeightIt</code> function call. Ignored if <code>treatment_model</code> is not a formula object. Defaults to <code>"ps"</code>.
</p>
</td></tr>
<tr><td><code id="surv_iptw_km_+3A_stabilize">stabilize</code></td>
<td>

<p>Whether to stabilize the weights or not. Is set to <code>FALSE</code> by default. Stabilizing weights ensures that the sum of all weights is equal to the original sample size. It has no effect on point estimates, only on the asymptotic variance calculations and confidence intervals.
</p>
</td></tr>
<tr><td><code id="surv_iptw_km_+3A_trim">trim</code></td>
<td>

<p>Can be either <code>FALSE</code> (default) or a numeric value at which to trim the weights. If <code>FALSE</code>, weights are used as calculated or supplied. If a numeric value is supplied, all weights that are bigger than <code>trim</code> are set to <code>trim</code> before the analysis is carried out. Useful when some weights are extremely large.
</p>
</td></tr>
<tr><td><code id="surv_iptw_km_+3A_...">...</code></td>
<td>

<p>Further arguments passed to <code><a href="WeightIt.html#topic+weightit">weightit</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Requires a model describing the treatment assignment mechanism. This must be either a <code><a href="stats.html#topic+glm">glm</a></code> or <code><a href="mgcv.html#topic+multinom">multinom</a></code> object.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Any number of levels in <code>variable</code> are allowed. Must be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are available.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method does not depend on other packages directly. However the <span class="pkg">WeightIt</span> package is required if <code>treatment_model</code> is a formula object.
</p>
</li></ul>

<p>This method works by modeling the treatment assignment mechanism. Adjusted survival curves are calculated by first estimating appropriate case-weights for each observation in <code>data</code>. This can be done using inverse probability of treatment weights using the propensity score (usually estimated using a logistic regression model) or by some other method (see <code>?weightit</code>). Those weights are used in a weighted version of the Kaplan-Meier estimator proposed by Xie and Liu (2005). If the weights are correctly estimated the resulting estimates will be unbiased. The only difference to the <code><a href="#topic+surv_iptw_cox">iptw_cox</a></code> method is a slightly different weighting approach.
</p>
<p>Asymptotic variances are calculated using the equations given in Xie and Liu (2005). It is also recommended to use stabilized weights by using <code>stabilize=TRUE</code> (the default value). More information can be found in the cited literature.
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedsurv</code> function:
</p>

<ul>
<li> <p><code>weights</code>: The final weights used in the analysis.
</p>
</li>
<li> <p><code>n_at_risk</code>: A <code>data.frame</code> containing the weighted number at risk and weighted number of events used in the calculations at each point in time for both groups.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>Jun Xie and Chaofeng Liu (2005). &quot;Adjusted Kaplan-Meier Estimator and Log- Rank Test with Inverse Probability of Treatment Weighting for Survival Data&quot;. In: Statistics in Medicine 24, pp. 3089-3110
</p>
<p>Stanley Xu, Colleen Ross and Marsha A. Raebel, Susan Shetterly, Christopher Blanchette, and David Smith (2010). &quot;Use of Stabilized Inverse Propensity Scores as Weights to Directly Estimate Relative Risk and Its Confidence Intervals&quot;. In: Value in Health 13.2, pp. 273-277
</p>


<h3>See Also</h3>

<p><code><a href="WeightIt.html#topic+weightit">weightit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a treatment assignment model
glm_mod &lt;- glm(group ~ x1 + x3 + x5 + x6, data=sim_dat, family="binomial")

# use it to calculate adjusted survival curves
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="iptw_km",
                        treatment_model=glm_mod)

# Alternatively, use custom weights
# In this example we use weights calculated using the propensity score,
# which is equal to using the glm model directly in the function
ps_score &lt;- glm_mod$fitted.values
weights &lt;- ifelse(sim_dat$group==1, 1/ps_score, 1/(1-ps_score))

adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="iptw_km",
                        treatment_model=weights)

# And a third alternative: use the WeightIt package
# here an example with equal results to the ones above:
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="iptw_km",
                        treatment_model=group ~ x1 + x3 + x5 + x6,
                        weight_method="ps")

# here an example using Entropy Balancing Weighting:
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="iptw_km",
                        treatment_model=group ~ x1 + x3 + x5 + x6,
                        weight_method="ebal")
</code></pre>

<hr>
<h2 id='surv_iptw_pseudo'>
Inverse Probability of Treatment Weighted Survival Estimates using Pseudo-Values
</h2><span id='topic+surv_iptw_pseudo'></span>

<h3>Description</h3>

<p>This page explains the details of estimating inverse probability of treatment weighted survival curves using Pseudo-Values for single event time-to-event data (<code>method="iptw_pseudo"</code> in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function). All regular arguments of the <code>adjustedsurv</code> function can be used. Additionally, the <code>treatment_model</code> argument has to be specified in the <code>adjustedsurv</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv_iptw_pseudo_+3A_treatment_model">treatment_model</code></td>
<td>

<p>[<strong>required</strong>] Must be either a model object with <code>variable</code> as response variable, a vector of weights or a formula which can be passed to <code>WeightIt</code>.
</p>
</td></tr>
<tr><td><code id="surv_iptw_pseudo_+3A_weight_method">weight_method</code></td>
<td>

<p>Method used in <code>WeightIt</code> function call. Ignored if <code>treatment_model</code> is not a formula object. Defaults to <code>"ps"</code>.
</p>
</td></tr>
<tr><td><code id="surv_iptw_pseudo_+3A_stabilize">stabilize</code></td>
<td>

<p>Whether to stabilize the weights or not. Is set to <code>FALSE</code> by default. Stabilizing weights ensures that the sum of all weights is equal to the original sample size. It has no effect on point estimates, only on the asymptotic variance calculations and confidence intervals.
</p>
</td></tr>
<tr><td><code id="surv_iptw_pseudo_+3A_trim">trim</code></td>
<td>

<p>Can be either <code>FALSE</code> (default) or a numeric value at which to trim the weights. If <code>FALSE</code>, weights are used as calculated or supplied. If a numeric value is supplied, all weights that are bigger than <code>trim</code> are set to <code>trim</code> before the analysis is carried out. Useful when some weights are extremely large.
</p>
</td></tr>
<tr><td><code id="surv_iptw_pseudo_+3A_se_method">se_method</code></td>
<td>

<p>One of <code>"miller"</code>, <code>"galloway"</code>, <code>"cochrane"</code> and <code>"Hmisc"</code>. Specifies which kind of standard error to calculate. Defaults to <code>"cochrane"</code>. See details.
</p>
</td></tr>
<tr><td><code id="surv_iptw_pseudo_+3A_censoring_vars">censoring_vars</code></td>
<td>

<p>An optional character vector specifying variables in <code>data</code>. Those are used in the calculation of inverse probability of censoring weighted pseudo observations. See <code>?pseudo_aareg</code> for more information. Set to <code>NULL</code> (default) to use standard pseudo-values without corrections for dependent censoring instead.
</p>
</td></tr>
<tr><td><code id="surv_iptw_pseudo_+3A_ipcw_method">ipcw_method</code></td>
<td>

<p>The specific method used in the calculation of inverse probability of censoring weighted pseudo observations. Can be either <code>"binder"</code> (default) or <code>"hajek"</code>. See <code>?pseudo_aareg</code> for more information. Ignored if <code>censoring_vars=NULL</code>.
</p>
</td></tr>
<tr><td><code id="surv_iptw_pseudo_+3A_...">...</code></td>
<td>

<p>Further arguments passed to <code><a href="WeightIt.html#topic+weightit">weightit</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Requires a model describing the treatment assignment mechanism. This must be either a <code><a href="stats.html#topic+glm">glm</a></code> or <code><a href="mgcv.html#topic+multinom">multinom</a></code> object.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Any number of levels in <code>variable</code> are allowed. Must be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are available.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are not guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are not guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the <span class="pkg">prodlim</span> package. The <span class="pkg">WeightIt</span> package is also required if <code>treatment_model</code> is a formula object. Additionally requires the <span class="pkg">eventglm</span> package if <code>censoring_vars</code> is specified.
</p>
</li></ul>

<p>This method works by modeling the treatment assignment mechanism. Adjusted survival curves are calculated by first estimating appropriate case-weights for each observation in <code>data</code>. This can be done using inverse probability of treatment weights using the propensity score (usually estimated using a logistic regression model) or by some other method (see <code>?weightit</code>). Pseudo-Values are then calculated for every observation in <code>data</code> at some points in time <code class="reqn">T</code>. Since Pseudo-Values bypass the problem of censoring, a simple weighted average of the Pseudo-Values can be taken for every <code class="reqn">T</code>. See Andersen et al. (2017) for more details on this method and Andersen and Perme (2010) for more information on Pseudo-Values in general.
</p>
<p>The standard error of this estimator can be approximated by calculation a weighted version of the standard error estimator. Interestingly, no exact method exists in the weighted case. Four approximations are implemented which can be chosen using the <code>se_method</code> argument. The equations for <code>"miller"</code>, <code>"galloway"</code> and <code>"cochrane"</code> are described and compared in Gatz and Smith (1995). <code>"Hmisc"</code> is the standard equation with a weight term added, as specified in the <span class="pkg">Hmisc</span> package, and should only be used with stabilized weights (<code>stabilize=TRUE</code>). It is generally recommended to use bootstrap estimates instead.
</p>
<p>Additionally, covariate-dependent censoring can be accounted for by using inverse probability of censoring weighted pseudo-values (Binder et al. 2014) instead of regular pseudo-values (specified using the <code>censoring_vars</code> and <code>ipcw_method</code> arguments).
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedsurv</code> function:
</p>

<ul>
<li> <p><code>pseudo_values</code>: The matrix of estimated pseudo-values.
</p>
</li>
<li> <p><code>weights</code>: The final weights used in the analysis.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>Per Kragh Andersen, Elisavet Syriopoulou, and Erik T. Parner (2017). &quot;Causal Inference in Survival Analysis using Pseudo-Observations&quot;. In: Statistics in Medicine 36, pp. 2669-2681
</p>
<p>Per Kragh Andersen and Maja Pohar Perme (2010). &quot;Pseudo-Observations in Survival Analysis&quot;. In: Statistical Methods in Medical Research 19, pp. 71-99
</p>
<p>Donald F. Gatz and Luther Smith (1995). &quot;The Standard Error of a Weighted Mean Concentration - I: Bootstrapping Vs Other Methods&quot;. In: Atmospheric Environment 29.11, pp. 1185-1193
</p>
<p>William G. Cochran (1977). Sampling Techniques. Vol. 3. New York: Wiley
</p>
<p>J. N. Galloway, G. E. Likens, and M. E. Hawley (1984). &quot;Acid Precipitation: Natural Versus Anthropogenic Components&quot;. In: Science 226, pp. 829-831
</p>
<p>J. M. Miller (1977). A Statistical Evaluation of the U.S. Precipitation Chemistry Network. Precipitation Scavenging (edited by Semonin R. G. and Beadle R. W.) pp. 639-659. Available as CONF 74100 from National Technical Information Service, U.S. Dept. of Commerce, Springfiel, VA
</p>
<p>Nadine Binder, Thomas A. Gerds, and Per Kragh Andersen (2014). &quot;Pseudo-Observations for Competing Risks with Covariate Dependent Censoring&quot;. In: Lifetime Data Analysis 20, pp. 303-315
</p>


<h3>See Also</h3>

<p><code><a href="WeightIt.html#topic+weightit">weightit</a></code>, <code><a href="prodlim.html#topic+prodlim">prodlim</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate a treatment assignment model
glm_mod &lt;- glm(group ~ x1 + x3 + x5 + x6, data=sim_dat, family="binomial")

# use it to calculate adjusted survival curves
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="iptw_km",
                        treatment_model=glm_mod,
                        force_bounds=TRUE,
                        iso_reg=TRUE)

# Alternatively, use custom weights
# In this example we use weights calculated using the propensity score,
# which is equal to using the glm model directly in the function
ps_score &lt;- glm_mod$fitted.values
weights &lt;- ifelse(sim_dat$group==1, 1/ps_score, 1/(1-ps_score))

adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="iptw_km",
                        treatment_model=weights,
                        force_bounds=TRUE,
                        iso_reg=TRUE)

# And a third alternative: use the WeightIt package
# here an example with equal results to the ones above:
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="iptw_km",
                        treatment_model=group ~ x1 + x3 + x5 + x6,
                        weight_method="ps",
                        force_bounds=TRUE,
                        iso_reg=TRUE)

# here an example using Entropy Balancing Weighting:
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="iptw_km",
                        treatment_model=group ~ x1 + x3 + x5 + x6,
                        weight_method="ebal",
                        force_bounds=TRUE,
                        iso_reg=TRUE)
</code></pre>

<hr>
<h2 id='surv_iv_2SRIF'>
Instrumental Variable based Survival Curve Estimation using the Two Stage Residual Inclusion method with a Frailty Term (2SRI-F)
</h2><span id='topic+surv_iv_2SRIF'></span>

<h3>Description</h3>

<p>This page explains the details of estimating adjusted survival curves using the 2SRI-F instrumental variable method introduced by Martinez-Camblor et al. (2021) for single event time-to-event data (<code>method="iv_2SRIF"</code> in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function). All regular arguments of the <code>adjustedsurv</code> function can be used. Additionally, the <code>adjust_vars</code> argument and the <code>instrument</code> argument have to be specified in the <code>adjustedsurv</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv_iv_2SRIF_+3A_adjust_vars">adjust_vars</code></td>
<td>

<p>[<strong>required</strong>] A character vector of column names specifying observed variables to be used as covariates in both the linear regression model and the Cox model. Set to <code>NULL</code> to use no additional observed covariates. See details and examples.
</p>
</td></tr>
<tr><td><code id="surv_iv_2SRIF_+3A_instrument">instrument</code></td>
<td>

<p>[<strong>required</strong>] A single character string specifying the instrumental variable. This variable should be numeric and fulfill all conditions required to be called a instrumental variable. See details and references for more information.
</p>
</td></tr>
<tr><td><code id="surv_iv_2SRIF_+3A_frailty_dist">frailty_dist</code></td>
<td>

<p>A single character string specifying the distribution that should be used for the frailty term (internally passed to the <code>distribution</code> argument of the <code>frailty</code> function). Defaults to <code>"gaussian"</code> and should usually be kept at this value.
</p>
</td></tr>
<tr><td><code id="surv_iv_2SRIF_+3A_return_models">return_models</code></td>
<td>

<p>Either <code>TRUE</code> (default) or <code>FALSE</code>, indicating whether the output object should also contain the two models used for the estimation of the suvival curves.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Allows adjustment for observed confounders through the Cox model and adjustment for unmeasured confounders using the instrumental variable approach.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Only binary treatments are allowed. The column specified by <code>variable</code> must be a factor variable with exactly two levels.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are not available. Bootstrapping can still be used to estimate the confidence intervals (see <code>?adjustedsurv</code>).
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer survival times.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> None.
</p>
</li></ul>

<p>An <code>instrument</code> is a variable that directly influences only the <code>variable</code> of interest and has no effect on the survival probability or any known confounders. Essentially, it is a variable that determines whether a person receives a specific level of the <code>variable</code>, without influencing anything else. Through some clever calculations, such variables may be used to adjust for confounding factors. A nice non-technical explanation of the general methodology of instrumental variables is given by Iwashyna and Kennedy (2013). More specific explanations of the method used here are given in Martinez-Camblor et al. (2021).
</p>
<p>Unlike all other methods included in this package, the instrumental variable approach allows adjustment for unmeasured confounding, while simultaneously allowing adjustment for observed confounders. This is the main advantage of this method. It is, however, only usable if there exists a variable that can be reasonably used as an instrumental variable. Conditions for what constitutes such a variable and more details on the estimation process can be found in the original article by Martinez-Camblor et al. (2021).
</p>
<p>Note that this method does not target the average treatment effect (ATE), but instead targets the local average treatment effect (LATE), also sometimes called complier average causal effect (CACE) as described in Lee et al. (2023). This is the main reason this estimator was not included in the simulation study performed by the author of this package (Denz et al. 2022). It is therefore currently not easy to judge how well this method performs in comparison with other methods.
</p>


<h3>Value</h3>

<p>If <code>return_models</code> is set to <code>TRUE</code>, it adds the following objects to the <code>adjustedsurv</code> function:
</p>

<ul>
<li> <p><code>lm_mod</code>: The linear regression model used to obtain the residuals in the first stage of the estimation process.
</p>
</li>
<li> <p><code>cox_mod</code>: The Cox model used in the second stage of the estimation process.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Robin Denz re-factored and adapted code from the online supplementary of the original article by Martinez-Camblor et al. (2021) to implement this method.
</p>


<h3>References</h3>

<p>Pablo Martínez-Camblor, Todd A. MacKenzie, Douglas O. Staiger, Phillip P. Goodney, and Jamer O'Malley (2021). &quot;Summarizing Causal Differences in Survival Curves in the Presence of Unmeasured Confounding&quot;. In: The International Journal of Biostatistics 17.2, pp. 223-240
</p>
<p>Youjin Lee, Edward H. Kennedy, and Nandita Mitra (2023). &quot;Doubly Robust Nonparametric Instrumental Variable Estimators for Survival Outcomes&quot;. In: Biostatistics 24.2, pp. 518-537
</p>
<p>Robin Denz, Renate Klaaßen-Mielke, and Nina Timmesfeld (2023). &quot;A Comparison of Different Methods to Adjust Survival Curves for Confounders&quot;. In: Statistics in Medicine 42.10, pp. 1461-1479
</p>
<p>Theodore J. Iwashyna and Edward H. Kennedy (2013). &quot;Instrumental Variable Analyses: Exploiting Natural Randomness to Understand Causal Mechanisms&quot;. In: Annals of the American Thoracic Society 10.3, pp. 255-260
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjustedsurv">adjustedsurv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)

set.seed(42)

## This example has been taken from the online supplement of the original
## article by Martinez-Camblor et al. (2021)

# generate some data
n &lt;- 1000
t &lt;- seq(0, 10, 0.01)
bu &lt;- log(2)
hr &lt;- 2
v &lt;- 2
a &lt;- 1

U &lt;- stats::rnorm(n)
Z &lt;- stats::rnorm(n)
W &lt;- stats::rnorm(n)
e &lt;- stats::rnorm(n)

X0 &lt;- (U + Z + a*W + (v - a^2)^0.5*e &gt;= 0)
L0 &lt;- Z + bu*U
L1 &lt;- log(hr) + Z + bu*U
T &lt;- stats::rexp(n, 0.005)

T0 &lt;- T/exp(L0)
T1 &lt;- T/exp(L1)

censor &lt;- stats::rexp(n, 0.05)
time1 &lt;- pmin(ifelse(X0==1,T1,T0), censor)
status1 &lt;- 1-(censor==time1)
time &lt;- pmin(time1, 10)
status &lt;- ifelse(time1 &gt; 10, 0, status1)

dt &lt;- as.data.frame(cbind(time, status, X0, Z, W))
dt$X0 &lt;- factor(dt$X0)

# calculate adjusted survival curves
adjsurv &lt;- adjustedsurv(data=dt,
                        variable="X0",
                        ev_time="time",
                        event="status",
                        method="iv_2SRIF",
                        adjust_vars="Z",
                        instrument="W")

# plot the curves
plot(adjsurv)
</code></pre>

<hr>
<h2 id='surv_km'>
Group-Specific Kaplan-Meier Survival Curves
</h2><span id='topic+surv_km'></span>

<h3>Description</h3>

<p>This page explains the details of estimating group-specific Kaplan-Meier curves for single event time-to-event data (<code>method="km"</code> in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function). All regular arguments of the <code>adjustedsurv</code> function can be used. Further arguments specific to this method are listed below.
</p>
<p>Calculates standard Kaplan-Meier survival curves, stratified by the group variable.
NO adjustment for any confounders is made. This function is included only for reference and should not be used when confounder adjusted survival curves are desired.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv_km_+3A_conf_type">conf_type</code></td>
<td>

<p>The type of confidence interval that should be calculated. Has to be a character string, passed to the <code>conf.type</code> argument in the <code>survfit</code> function. Defaults to <code>"log"</code>, which is also the default in <code>survfit</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> NO adjustments are made. This is just a stratified Kaplan-Meier estimator.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Any number of levels in <code>variable</code> are allowed. Must be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are available.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the the <span class="pkg">survival</span> package.
</p>
</li></ul>



<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedsurv</code> function:
</p>

<ul>
<li> <p><code>survfit_object</code>: The <code>survfit</code> object used to calculate the Kaplan-Meier curves.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>The wrapper function was written by Robin Denz, the <code>survfit</code> function (which this wrapper is build around) was written by other people. See <code>?survfit</code> for more details.
</p>


<h3>References</h3>

<p>E. L. Kaplan and Paul Meier (1958). &quot;Nonparametric Estimation from Incomplete Observations&quot;. In: Journal of the American Statistical Association 53.282, pp. 457-481
</p>


<h3>See Also</h3>

<p><code><a href="survival.html#topic+survfit">survfit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# calculate un-adjusted kaplan-meier survival curves
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="km")

# plot the curves
plot(adjsurv)
</code></pre>

<hr>
<h2 id='surv_matching'>
Using Propensity-Score Matching to Calculate Adjusted Survival Curves
</h2><span id='topic+surv_matching'></span>

<h3>Description</h3>

<p>This page explains the details of estimating adjusted survival curves using propensity-score matching for single event time-to-event data (<code>method="matching"</code> in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function). All regular arguments of the <code>adjustedsurv</code> function can be used. Additionally, the <code>treatment_model</code> argument has to be specified in the <code>adjustedsurv</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv_matching_+3A_treatment_model">treatment_model</code></td>
<td>

<p>[<strong>required</strong>] Must be either a model object with <code>variable</code> as response variable or a vector of previously estimated propensity scores.
</p>
</td></tr>
<tr><td><code id="surv_matching_+3A_gtol">gtol</code></td>
<td>

<p>Tolerance at which estimated treatment assignment probabilities are truncated. Every propensity score bigger than 1 - <code>gtol</code> is set to 1 - <code>gtol</code> and every propensity score smaller than <code>gtol</code> is set to <code>gtol</code>. Useful when there are extreme propensity scores close to 0 or 1. Defaults to 0.001.
</p>
</td></tr>
<tr><td><code id="surv_matching_+3A_...">...</code></td>
<td>

<p>Further arguments passed to the <code><a href="Matching.html#topic+Match">Match</a></code> function of the <span class="pkg">Matching</span> Package.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Requires a model describing the treatment assignment mechanism. This must be either a <code><a href="stats.html#topic+glm">glm</a></code> object.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Only two groups in <code>variable</code> are allowed. Must be a factor variable with exactly two levels.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are currently not available. Bootstrap confidence intervals can however be calculated with all supported models. See <code>?adjustedsurv</code> for more information on bootstrapping.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the <span class="pkg">Matching</span> package.
</p>
</li></ul>

<p>Using the estimated propensity score, the individual observations in the dataset are matched to each other creating a new dataset in which the covariate distributions are balanced in respect to the two groups defined by <code>variable</code>. A simple Kaplan-Meier estimator is then used to calculate the confounder-adjusted survival curves. This corresponds to the method described in Austin (2014). Details on the algorithm used for matching can be found in the documentation of the <span class="pkg">Matching</span> package.
</p>
<p>We choose not to implement other matching based estimators (see Winnett &amp; Sasieni (2002), Galimberti et al. (2002) and Austin (2020)) because of the wide range of matching algorithms and parameters. Trying to automate the matching process in a function like this would, in our opinion, disrupt the workflow of the user while also encouraging suboptimal practices. We however included this simple version of a matching estimator as a reference and to raise the awareness that using matching is a valid method to obtain adjusted survival curves.
</p>
<p>Simulation studies have shown that this particular method as implemented here is significantly less efficient than other methods included in this R-Package. While it does produce unbiased estimates, the variation in these estimates is very high. We suggest using one of the other available methods.
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedsurv</code> function:
</p>

<ul>
<li> <p><code>match_object</code>: The object creates using the <code>Match</code> function.
</p>
</li>
<li> <p><code>survfit_object</code>: The <code>survfit</code> object fit on the matched data.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>Peter C. Austin (2014). &quot;The Use of Propensity Score Methods with Survival or Time-To-Event Outcomes: Reporting Measures of Effect Similar to those Used in Randomized Experiments&quot;. In: Statistics in Medicine 33, pp. 1242-1258
</p>
<p>Angela Winnett and Peter Sasieni (2002). &quot;Adjusted Nelson-Aalen Estimates with Retrospective Matching&quot;. In: Journal of the American Statistical Association 97.457, pp. 245-256
</p>
<p>Stefania Galimberti, Peter Sasieni, and Maria Grazia Valsecchi (2002). &quot;A Weighted Kaplan-Meier Estimator for Matched Data with Application to the Comparison of Chemotherapy and Bone-Marrow Transplant in Leukaemia&quot;. In: Statistics in Medicine 21, pp. 3847-3864
</p>
<p>Peter C. Austin, Neal Thomas, and Donald B. Rubin (2020). &quot;Covariate-Adjusted Survival Analyses in Propensity-Score Matched Samples: Imputing Potential Time- To-Event Outcomes&quot;. In: Statistical Methods in Medical Research 29.3, pp. 728-751
</p>


<h3>See Also</h3>

<p><code><a href="Matching.html#topic+Match">Match</a></code>, <code><a href="survival.html#topic+survfit">survfit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(Matching)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# estimate treatment assignment model
glm_mod &lt;- glm(group ~ x1 + x2 + x4 + x6, data=sim_dat, family="binomial")

# calculate adjusted survival curves
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="matching",
                        treatment_model=glm_mod)

# Alternatively, supply the propensity score directly
# Here we use the logistic regression to calculate it, so we get
# exactly the same result. The propensity score can be calculated in
# any other way in practice, allowing flexibility
ps_score &lt;- glm_mod$fitted.values

adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="matching",
                        treatment_model=ps_score)

# plot the curves
plot(adjsurv)
</code></pre>

<hr>
<h2 id='surv_prox_aiptw'>
Proximal Augmented Inverse Probability of Treatment Weighted Survival Curve Estimates
</h2><span id='topic+surv_prox_aiptw'></span>

<h3>Description</h3>

<p>This page explains the details of estimating augmented inverse probability of treatment weighted survival curves using a proximal causal inference based method for single event time-to-event data (<code>method="prox_aiptw"</code> as described by Ying et al. (2022) in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function). All regular arguments of the <code>adjustedsurv</code> function can be used. Additionally, the <code>treatment_proxy</code>, <code>outcome_proxy</code> and <code>adjust_vars</code> arguments have to be specified in the <code>adjustedsurv</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv_prox_aiptw_+3A_adjust_vars">adjust_vars</code></td>
<td>

<p>[<strong>required</strong>] A character vector specifying names of variables in <code>data</code>. These variables may consist of observed confounders. At least one variable has to be named. Can be numeric, character or factor variables. Corresponds to <code class="reqn">X</code> (type 1 proxy) in the article by Ying et al. (2022).
</p>
</td></tr>
<tr><td><code id="surv_prox_aiptw_+3A_treatment_proxy">treatment_proxy</code></td>
<td>

<p>[<strong>required</strong>] A single character string specifying the (numeric) variable that should be used as a treatment proxy. Corresponds to <code class="reqn">Z</code> (type 3 proxy) in the article by Ying et al. (2022).
</p>
</td></tr>
<tr><td><code id="surv_prox_aiptw_+3A_outcome_proxy">outcome_proxy</code></td>
<td>

<p>[<strong>required</strong>] A single character string specifying the (numeric) variable that should be used as a outcome proxy. Corresponds to <code class="reqn">W</code> (type 2 proxy) in the article by Ying et al. (2022).
</p>
</td></tr>
<tr><td><code id="surv_prox_aiptw_+3A_optim_method">optim_method</code></td>
<td>

<p>A single character string passed to the <code>method</code> argument of the <code>optim</code> function, used internally when fitting the q-confounding bridge function and the h-confounding bridge function. Defaults to <code>"BFGS"</code>. To pass additional argument to the internal <code>optim</code> call, see argument <code>optim_control</code>.
</p>
</td></tr>
<tr><td><code id="surv_prox_aiptw_+3A_optim_control">optim_control</code></td>
<td>

<p>A list of named arguments passed to the <code>control</code> argument of the <code>optim</code> function, used internally when fitting the q-confounding bridge function and the h-confounding bridge function. Set to <code>list()</code> to not pass any additional argument (default).
</p>
</td></tr>
<tr><td><code id="surv_prox_aiptw_+3A_return_fit">return_fit</code></td>
<td>

<p>Whether to add intermediate results, such as the q-confounding bridge function to the output object. Defaults to <code>TRUE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Uses the proximal causal inference framework to adjust for measured and unmeasured confounding through the use of both the q-confounding bridge function and the h-confounding bridge function, which is essentially augmented inverse probability of treatment weighting, but using proxies.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are Doubly-Robust in the sense that only one of the bridge functions has to be correctly specified to achieve unbiased estimates, given that the other relevant assumptions hold.
</p>
</li>
<li><p><strong>Categorical groups:</strong><code>variable</code> may only contain two groups. Must be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are available.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are not guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> Depends on <span class="pkg">numDeriv</span> and the dependencies of that package.
</p>
</li></ul>

<p>This method is based on the proximal causal inference framework, first introduced by Miao et al. (2018) and later extended to allow for estimation of counterfactual survival curves by Ying et al. (2022). It allows the estimation of the treatment-specific counterfactual survival curve under unmeasured confounding, when the true data-generation mechanism has a particular structure. In particular, there must be an observed variable (contained in the dataset) that is a potential cause of the treatment (<code>variable</code>) and also unrelated to the time-to-event endpoint, except through measured confounders (<code>adjust_vars</code>) and a particular known but unmeasured confounder. This type of variable is called a <code>treatment_proxy</code>. Secondly, there must be another observed variable that directly or indirectly causes the outcome, but is unrelated to the treatment expect through measured confounders and the same known but unmeasured confounder as mentioned earlier. This type of variable is called an <code>outcome_proxy</code>. A better explanation is given by Zivich et al. (2023). More information on the underlying assumptions can be found in the papers listed in the references.
</p>
<p>Ying et al. (2022) proposed two methods to utilize this kind of structure for the estimation of the counterfactual survival curve. The one implemented here relies on estimating the q-confounding bridge and the h-confounding bridge using parametric models. This essentially means that it uses both the treatment-assignment mechanism and the outcome-mechanism to adjust for confounding, similar to a regular augmented inverse probability weighted estimator.
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedsurv</code> function:
</p>

<ul>
<li> <p><code>noncensor_cumhaz</code>: The estimated cumulative hazard function.
</p>
</li>
<li> <p><code>noncensor_cumhaz_IF</code>: The influence function based on the estimated cumulative hazard function.
</p>
</li>
<li> <p><code>q_bridge</code>: A <code>list</code> containing results from fitting the q-confounding bridge function.
</p>
</li>
<li> <p><code>h_bridge</code>: A <code>list</code> containing results from fitting the h-confounding bridge function.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Andrew Ying wrote all of the low-level estimation functions used to actually obtain the relevant values. Robin Denz wrote some wrapper functions around those to include this method in this package.
</p>


<h3>References</h3>

<p>Andrew Ying, Yifan Cui and Eric J. Tchetgen Tchetgen (2022). &quot;Proximal Causal Inference for Marginal Counterfactual Survival Curves&quot;. arXiv:2204.13144
</p>
<p>Wang Miao, Zhi Geng and Eric J. Tchetgen Tchetgen (2018). &quot;Identifying Causal Effects with Proxy Variables of an Unmeasured Confounder&quot;. In: Biometrika 105.4, pp. 987-993.
</p>
<p>Paul N. Zivich, Stephen R. Cole, Jessie K. Edwards, Grace E. Mulholland, Bonnie E. Shook-Sa and Eric J. Tchetgen Tchetgen (2023). &quot;Introducing Proximal Causal Inference for Epidemiologists&quot;. In: American Journal of Epidemiology 192.7, pp. 1224-1227.
</p>
<p>Eric J. Tchetgen Tchetgen, Andrew Ying, Yifan Cui, Xu Shi and Wang Miao (2020). &quot;An Introduction to Proximal Causal Learning&quot;. arXiv:2009.10982
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(numDeriv)
library(adjustedCurves)

#### generate some example data that fufill all assumptions ####
# code was taken from the github repository associated with the original
# paper by Ying et al. (2022): https://github.com/andrewyyp/Proximal_MSF

# simulation parameters
para_set &lt;- list(mu_X = 1.1,
                 sigma_X = 0.75,
                 mu_U = 1.1,
                 sigma_U = 0.75,
                 alpha_A = c(0.3, 0.4, -0.6),
                 mu_Z = c(-0.2, -0.3, 0.65),
                 sigma_Z = 0.5,
                 mu_W = c(-0.6, 0.4, 0.65),
                 sigma_W = 0.5,
                 mu_T0 = c(0.1, 0.6, 0.25, 0.5),
                 mu_C = 0.2,
                 admin_C = 2
)

# small function to obtain the required data
data_gen &lt;- function(N, para_set, a = NULL) {
  # generate X, U
  X &lt;- para_set$mu_X + rnorm(N, 0, para_set$sigma_X)
  U &lt;- para_set$mu_U + rnorm(N, 0, para_set$sigma_U)
  X &lt;- pmax(X, 0)
  U &lt;- pmax(U, 0)

  if (is.null(a)) {
    # generate A
    prop_score_0 &lt;- 1/(1 + exp(-cbind(1, X, U) %*% para_set$alpha_A))
    A &lt;- rbinom(N, 1, prop_score_0)
  } else {
    A &lt;- rep(a, N)
  }


  # generate Z
  Z &lt;- cbind(1, X, U) %*% para_set$mu_Z + rnorm(N, 0, para_set$sigma_Z)

  # generate W
  W &lt;- cbind(1, X, U) %*% para_set$mu_W + rnorm(N, 0, para_set$sigma_W)


  #generate Y
  T0 &lt;- rexp(N, rate = cbind(1, A, X, U) %*% para_set$mu_T0)

  C &lt;- rexp(N, rate = para_set$mu_C)
  C &lt;- pmin(C, para_set$admin_C)
  if (is.null(a)) {
    df &lt;- data.frame(X, U, A, Z, W, T0 = pmin(T0, C), Delta = (T0 &lt;= C))
  } else {
    df &lt;- data.frame(X, U, A, Z, W, T0 = T0, Delta = rep(1, N))
  }
  return(df)
}

#### Simple example ####

set.seed(4356)
# NOTE: increase N to get more stable estimates, kept low here to pass
#       speed requirements set by CRAN
data &lt;- data_gen(N=50, para_set=para_set)
data$A &lt;- factor(data$A)

adj &lt;- adjustedsurv(data=data,
                    variable="A",
                    ev_time="T0",
                    event="Delta",
                    method="prox_aiptw",
                    adjust_vars="X",
                    treatment_proxy="Z",
                    outcome_proxy="W",
                    conf_int=TRUE)
plot(adj, iso_reg=TRUE)
</code></pre>

<hr>
<h2 id='surv_prox_iptw'>
Proximal Inverse Probability of Treatment Weighted Survival Curve Estimates
</h2><span id='topic+surv_prox_iptw'></span>

<h3>Description</h3>

<p>This page explains the details of estimating inverse probability of treatment weighted survival curves using a proximal causal inference based method for single event time-to-event data (<code>method="prox_iptw"</code> as described by Ying et al. (2022) in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function). All regular arguments of the <code>adjustedsurv</code> function can be used. Additionally, the <code>treatment_proxy</code>, <code>outcome_proxy</code> and <code>adjust_vars</code> arguments have to be specified in the <code>adjustedsurv</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv_prox_iptw_+3A_adjust_vars">adjust_vars</code></td>
<td>

<p>[<strong>required</strong>] A character vector specifying names of variables in <code>data</code>. These variables may consist of observed confounders. At least one variable has to be named. Corresponds to <code class="reqn">X</code> (type 1 proxy) in the article by Ying et al. (2022).
</p>
</td></tr>
<tr><td><code id="surv_prox_iptw_+3A_treatment_proxy">treatment_proxy</code></td>
<td>

<p>[<strong>required</strong>] A single character string specifying the variable that should be used as a treatment proxy. Corresponds to <code class="reqn">Z</code> (type 3 proxy) in the article by Ying et al. (2022).
</p>
</td></tr>
<tr><td><code id="surv_prox_iptw_+3A_outcome_proxy">outcome_proxy</code></td>
<td>

<p>[<strong>required</strong>] A single character string specifying the variable that should be used as a outcome proxy. Corresponds to <code class="reqn">W</code> (type 2 proxy) in the article by Ying et al. (2022).
</p>
</td></tr>
<tr><td><code id="surv_prox_iptw_+3A_optim_method">optim_method</code></td>
<td>

<p>A single character string passed to the <code>method</code> argument of the <code>optim</code> function, used internally when fitting the q-confounding bridge function. Defaults to <code>"BFGS"</code>. To pass additional argument to the internal <code>optim</code> call, see argument <code>optim_control</code>.
</p>
</td></tr>
<tr><td><code id="surv_prox_iptw_+3A_optim_control">optim_control</code></td>
<td>

<p>A list of named arguments passed to the <code>control</code> argument of the <code>optim</code> function, used internally when fitting the q-confounding bridge function. Set to <code>list()</code> to not pass any additional argument (default).
</p>
</td></tr>
<tr><td><code id="surv_prox_iptw_+3A_return_fit">return_fit</code></td>
<td>

<p>Whether to add intermediate results, such as the q-confounding bridge function to the output object. Defaults to <code>TRUE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Uses the proximal causal inference framework to adjust for measured and unmeasured confounding through the q-confounding bridge function, which is essentially inverse probability of treatment weighting, but using proxies.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong><code>variable</code> may only contain two groups. Must be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are available.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are not guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> Depends on <span class="pkg">numDeriv</span> and the dependencies of that package.
</p>
</li></ul>

<p>This method is based on the proximal causal inference framework, first introduced by Miao et al. (2018) and later extended to allow for estimation of counterfactual survival curves by Ying et al. (2022). It allows the estimation of the treatment-specific counterfactual survival curve under unmeasured confounding, when the true data-generation mechanism has a particular structure. In particular, there must be an observed variable (contained in the dataset) that is a potential cause of the treatment (<code>variable</code>) and also unrelated to the time-to-event endpoint, except through measured confounders (<code>adjust_vars</code>) and a particular known but unmeasured confounder. This type of variable is called a <code>treatment_proxy</code>. Secondly, there must be another observed variable that directly or indirectly causes the outcome, but is unrelated to the treatment expect through measured confounders and the same known but unmeasured confounder as mentioned earlier. This type of variable is called an <code>outcome_proxy</code>. A better explanation is given by Zivich et al. (2023). More information on the underlying assumptions can be found in the papers listed in the references.
</p>
<p>Ying et al. (2022) proposed two methods to utilize this kind of structure for the estimation of the counterfactual survival curve. The one implemented here relies on estimating the q-confounding bridge using a parametric model. This essentially means that it uses the treatment-assignment mechanism to adjust for confounding, similar to a regular inverse probability weighted estimator.
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedsurv</code> function:
</p>

<ul>
<li> <p><code>noncensor_cumhaz</code>: The estimated cumulative hazard function.
</p>
</li>
<li> <p><code>noncensor_cumhaz_IF</code>: The influence function based on the estimated cumulative hazard function.
</p>
</li>
<li> <p><code>q_bridge</code>: A <code>list</code> containing results from fitting the q-confounding bridge function.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Andrew Ying wrote all of the low-level estimation functions used to actually obtain the relevant values. Robin Denz wrote some wrapper functions around those to include this method in this package.
</p>


<h3>References</h3>

<p>Andrew Ying, Yifan Cui and Eric J. Tchetgen Tchetgen (2022). &quot;Proximal Causal Inference for Marginal Counterfactual Survival Curves&quot;. arXiv:2204.13144
</p>
<p>Wang Miao, Zhi Geng and Eric J. Tchetgen Tchetgen (2018). &quot;Identifying Causal Effects with Proxy Variables of an Unmeasured Confounder&quot;. In: Biometrika 105.4, pp. 987-993.
</p>
<p>Paul N. Zivich, Stephen R. Cole, Jessie K. Edwards, Grace E. Mulholland, Bonnie E. Shook-Sa and Eric J. Tchetgen Tchetgen (2023). &quot;Introducing Proximal Causal Inference for Epidemiologists&quot;. In: American Journal of Epidemiology 192.7, pp. 1224-1227.
</p>
<p>Eric J. Tchetgen Tchetgen, Andrew Ying, Yifan Cui, Xu Shi and Wang Miao (2020). &quot;An Introduction to Proximal Causal Learning&quot;. arXiv:2009.10982
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(numDeriv)
library(adjustedCurves)

#### generate some example data that fufill all assumptions ####
# code was taken from the github repository associated with the original
# paper by Ying et al. (2022): https://github.com/andrewyyp/Proximal_MSF

# simulation parameters
para_set &lt;- list(mu_X = 1.1,
                 sigma_X = 0.75,
                 mu_U = 1.1,
                 sigma_U = 0.75,
                 alpha_A = c(0.3, 0.4, -0.6),
                 mu_Z = c(-0.2, -0.3, 0.65),
                 sigma_Z = 0.5,
                 mu_W = c(-0.6, 0.4, 0.65),
                 sigma_W = 0.5,
                 mu_T0 = c(0.1, 0.6, 0.25, 0.5),
                 mu_C = 0.2,
                 admin_C = 2
)

# small function to obtain the required data
data_gen &lt;- function(N, para_set, a = NULL) {
  # generate X, U
  X &lt;- para_set$mu_X + rnorm(N, 0, para_set$sigma_X)
  U &lt;- para_set$mu_U + rnorm(N, 0, para_set$sigma_U)
  X &lt;- pmax(X, 0)
  U &lt;- pmax(U, 0)

  if (is.null(a)) {
    # generate A
    prop_score_0 &lt;- 1/(1 + exp(-cbind(1, X, U) %*% para_set$alpha_A))
    A &lt;- rbinom(N, 1, prop_score_0)
  } else {
    A &lt;- rep(a, N)
  }


  # generate Z
  Z &lt;- cbind(1, X, U) %*% para_set$mu_Z + rnorm(N, 0, para_set$sigma_Z)

  # generate W
  W &lt;- cbind(1, X, U) %*% para_set$mu_W + rnorm(N, 0, para_set$sigma_W)


  #generate Y
  T0 &lt;- rexp(N, rate = cbind(1, A, X, U) %*% para_set$mu_T0)

  C &lt;- rexp(N, rate = para_set$mu_C)
  C &lt;- pmin(C, para_set$admin_C)
  if (is.null(a)) {
    df &lt;- data.frame(X, U, A, Z, W, T0 = pmin(T0, C), Delta = (T0 &lt;= C))
  } else {
    df &lt;- data.frame(X, U, A, Z, W, T0 = T0, Delta = rep(1, N))
  }
  return(df)
}

#### Simple example ####

set.seed(4356)
data &lt;- data_gen(N=300, para_set=para_set)
data$A &lt;- factor(data$A)

adj &lt;- adjustedsurv(data=data,
                    variable="A",
                    ev_time="T0",
                    event="Delta",
                    method="prox_iptw",
                    adjust_vars="X",
                    treatment_proxy="Z",
                    outcome_proxy="W",
                    conf_int=TRUE)
plot(adj, iso_reg=TRUE)
</code></pre>

<hr>
<h2 id='surv_strat_amato'>
Adjusted Survival Curves for Categorical Confounders using the Method by Amato (1988)
</h2><span id='topic+surv_strat_amato'></span>

<h3>Description</h3>

<p>This page explains the details of estimating confounder-adjusted survival curves using a weighted average of stratified Kaplan-Meier estimates using the method described in Amato (1988) (<code>method="strat_amato"</code> in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function). All regular arguments of the <code>adjustedsurv</code> function can be used. Additionally, the <code>adjust_vars</code> argument has to be specified in the <code>adjustedsurv</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv_strat_amato_+3A_adjust_vars">adjust_vars</code></td>
<td>

<p>[<strong>required</strong>] A single string or character vector specifying column names in data for which the survival curves should be adjusted for. The variables specified can be integers, factors or characters. Only categorical variables can be used with this method. See details.
</p>
</td></tr>
<tr><td><code id="surv_strat_amato_+3A_reference">reference</code></td>
<td>

<p>A <code>data.frame</code> to be used as a reference population when weighting the survival curves or <code>NULL</code> (default). If <code>NULL</code> the survival curves are weighted in reference to the full sample supplied using <code>data</code>, regardless of the <code>variable</code> level. If a <code>data.frame</code> is supplied it needs to include all variables specified in <code>adjust_vars</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> The survival curves are adjusted by calculating a weighted version of the Kaplan-Meier estimator, based on stratification on covariates. This only works for categorical confounders. See below for more information.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Any number of levels in <code>variable</code> are allowed. Must be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are not available. Bootstrap confidence intervals can however be calculated with all supported models. See <code>?adjustedsurv</code> for more information on bootstrapping.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method has no dependencies.
</p>
</li></ul>

<p>This is one of the older adjustment methods described in the literature. It only works for categorical confounders. If adjustments for continuous confounders are desired, the user needs to explicitly categorize the continuous confounders. It is recommended to use one of the other methods implemented in this package in that case. The method works exactly as described in Amato (1988). The number of people at risk and the number of events in each stratum at each point in time is reweighted and combined into a single estimate for each treatment. The reference data used to calculate the weights is the pooled sample (<code>data</code>) by default, but external reference data can be supplied. A more detailed description can be found in the original article.
</p>
<p>If a character vector is supplied in the <code>adjust_vars</code> argument, every possible combination of the variables specified in <code>adjust_vars</code> will be used as strata. This might lead to problems when there are strata with very little data in them. In contrast to other stratification based methods however, this method allows the estimation of adjusted survival curves up to the last point in time at which there is at least one individual at risk in the pooled sample.
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedsurv</code> function:
</p>

<ul>
<li> <p><code>Pjs</code>: The weights used for each stratum.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>David A. Amato (1988). &quot;A Generalized Kaplan-Meier Estimator for Heterogenous Populations&quot;. In: Communications in Statistics: Theory and Methods 17.1, pp. 263-286
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjustedsurv">adjustedsurv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# adjust survival curves for some categorical confounders
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="strat_amato",
                        adjust_vars=c("x1", "x3"),
                        conf_int=FALSE)

# plot the curves
plot(adjsurv)
</code></pre>

<hr>
<h2 id='surv_strat_cupples'>
Adjusted Survival Curves for Categorical Confounders using the Method by Cupples et al. (1995)
</h2><span id='topic+surv_strat_cupples'></span>

<h3>Description</h3>

<p>This page explains the details of estimating confounder-adjusted survival curves using a weighted average of stratified Kaplan-Meier estimates using the method described in Cupples et al. (1995) (<code>method="strat_cupples"</code> in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function). All regular arguments of the <code>adjustedsurv</code> function can be used. Additionally, the <code>adjust_vars</code> argument has to be specified in the <code>adjustedsurv</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv_strat_cupples_+3A_adjust_vars">adjust_vars</code></td>
<td>

<p>[<strong>required</strong>] A single string or character vector specifying column names in data for which the survival curves should be adjusted for. The variables specified can be integers, factors or characters. Only categorical variables can be used with this method. See details.
</p>
</td></tr>
<tr><td><code id="surv_strat_cupples_+3A_reference">reference</code></td>
<td>

<p>A <code>data.frame</code> to be used as a reference population when weighting the survival curves or <code>NULL</code> (default). If <code>NULL</code> the survival curves are weighted in reference to the full sample supplied using <code>data</code>, regardless of the <code>variable</code> level. If a <code>data.frame</code> is supplied it needs to include all variables specified in <code>adjust_vars</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> The survival curves are adjusted by taking a weighted average of stratified Kaplan-Meier estimates. This only works for categorical confounders. See below for more information.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Any number of levels in <code>variable</code> are allowed. Must be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are not available. Bootstrap confidence intervals can however be calculated with all supported models. See <code>?adjustedsurv</code> for more information on bootstrapping.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the <span class="pkg">survival</span> package.
</p>
</li></ul>

<p>This is one of the older adjustment methods described in the literature. It only works for categorical confounders. If adjustments for continuous confounders are desired, the user needs to explicitly categorize the continuous confounders. It is recommended to use one of the other methods implemented in this package in that case. The method works exactly as described in Cupples et al. (1995). First, stratified Kaplan-Meier estimates for each possible combination of all supplied variables (<code>variable</code> + <code>adjust_vars</code>) are calculated. If for example a dichotomous <code>variable</code> with the levels &quot;Treatment&quot; and &quot;Control&quot; is supplied in conjunction with a single dichotomous confounder &quot;Sex&quot; with the levels &quot;male&quot; and &quot;female&quot;, this method would calculate four Kaplan-Meier curves (Treatment + male, Treatment + female, Control + male, Control + female). Next a simple weighted average of these survival curves is taken per level in <code>variable</code>, where the weights are the number of occurrences of each confounder level in the reference data. The reference data is the pooled sample by default, but external reference data can be used. A more detailed description can be found in the original article.
</p>
<p>If a character vector is supplied in the <code>adjust_vars</code> argument, the Kaplan-Meier estimates are created for each combination of all supplied variables. If the sample size is small and/or there are many levels in these variables, the estimates can become unstable or undefined. Because it is a weighted average of Kaplan-Meier curves, estimates for this method are only defined for points in time with a valid Kaplan-Meier estimate in all strata. Continuing the example from above, if the Kaplan-Meier curve of the strata &quot;Treatment + male&quot; only extends to t = 100, it will be impossible to estimate the adjusted survival curve for t &gt; 100 using this method.
</p>


<h3>Value</h3>

<p>Adds no additional objects to the output of the <code>adjustedsurv</code> function.
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>A. Kramar and C. Com-Nougué (1990). &quot;Estimation des courbes de survie ajustées&quot;. In: Revue d Épidémiologie et de Santé Publique 38.2, pp. 149-152
</p>
<p>L. Adrienne Cupples, David R. Gragnon, Ratna Ramaswamy, and Ralph D’Agostino (1995). &quot;Age-Adjusted Survival Curves with Application in the Framingham Study&quot;. In: Statistics in Medicine 14, pp. 1731-1744
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjustedsurv">adjustedsurv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# adjust survival curves for some categorical confounders
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="strat_cupples",
                        adjust_vars=c("x1", "x3"),
                        conf_int=FALSE)

# plot the curves
plot(adjsurv)
</code></pre>

<hr>
<h2 id='surv_strat_nieto'>
Adjusted Survival Curves for Categorical Confounders using the Method by Gregory (1988) and Nieto &amp; Coresh (1996)
</h2><span id='topic+surv_strat_nieto'></span>

<h3>Description</h3>

<p>This page explains the details of estimating confounder-adjusted survival curves using a weighted average of stratified Kaplan-Meier estimates using the method described in Gregory (1988) and Nieto &amp; Coresh (1996) (<code>method="strat_gregory_nieto"</code> in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function). All regular arguments of the <code>adjustedsurv</code> function can be used. Additionally, the <code>adjust_vars</code> argument has to be specified in the <code>adjustedsurv</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv_strat_nieto_+3A_adjust_vars">adjust_vars</code></td>
<td>

<p>[<strong>required</strong>] A single string or character vector specifying column names in data for which the survival curves should be adjusted for. The variables specified can be integers, factors or characters. Only categorical variables can be used with this method. See details.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> The survival curves are adjusted by taking a weighted average of stratified Kaplan-Meier estimates. This only works for categorical confounders. See below for more information.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are not Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> Any number of levels in <code>variable</code> are allowed. Must be a factor variable.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are available. The estimator for the variance can be found in the appendix of Nieto &amp; Coresh (1996).
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method has no additional dependencies.
</p>
</li></ul>

<p>This is one of the older adjustment methods described in the literature. It only works for categorical confounders. If adjustments for continuous confounders are desired, the user needs to explicitly categorize the continuous confounders. It is recommended to use one of the other methods implemented in this package in that case. The method works exactly as described in Gregory (1988). Similarly to the method described in <a href="#topic+surv_strat_cupples">strat_cupples</a>, Kaplan-Meier estimates are calculated for each strata and a weighted average is taken. The only difference is a slightly different weighting scheme. Weights are calculated using the pooled sample (<code>data</code>). In contrast to other stratification based methods, external reference data is not allowed. A more detailed description can be found in the original article.
</p>
<p>If a character vector is supplied in the <code>adjust_vars</code> argument, the Kaplan-Meier estimates are created for each combination of all supplied variables. If the sample size is small and/or there are many levels in these variables, the estimates can become unstable or undefined. Because it is a weighted average of Kaplan-Meier curves, estimates for this method are only defined for points in time with a valid Kaplan-Meier estimate in all strata. For example, if the Kaplan-Meier curve of the strata &quot;Treatment + male&quot; only extends to t = 100, it will be impossible to estimate the adjusted survival curve for t &gt; 100 using this method.
</p>
<p>Nieto &amp; Coresh (1996) proposed a very similar method. The only major difference is that Nieto &amp; Coresh (1996) used the control group as reference population, which results in a different causal estimand. Using the method by Nieto &amp; Coresh (1996) with the full <code>data</code> as reference population as described in Gregory (1988) produces exactly the same results. Nieto &amp; Coresh (1996) seemed to be unaware of the method by Gregory (1988), as they did not mention it in their article. In contrast to Gregory (1988) they however also proposed an approximate estimator of the variance, which is implemented here. Their formulation of this estimator also allows the use of time-dependent covariates and left-truncated data. This is however not implemented here.
</p>


<h3>Value</h3>

<p>Adds no additional objects to the output of the <code>adjustedsurv</code> function.
</p>


<h3>Author(s)</h3>

<p>Robin Denz
</p>


<h3>References</h3>

<p>W. M. Gregory (1988). &quot;Adjusting Survival Curves for Imbalances in Prognostic Factors&quot;. In: British Journal of Cancer 58, pp. 202-204
</p>
<p>F. Javier Nieto and Josef Coresh (1996). &quot;Adjusting Survival Curves for Confounders: A Review and a New Method&quot;. In: American Journal of Epidemiology 143.10, pp. 1059-1068
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjustedsurv">adjustedsurv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)
library(survival)

set.seed(42)

# simulate some data as example
sim_dat &lt;- sim_confounded_surv(n=50, max_t=1.2)
sim_dat$group &lt;- as.factor(sim_dat$group)

# adjust survival curves for some categorical confounders
adjsurv &lt;- adjustedsurv(data=sim_dat,
                        variable="group",
                        ev_time="time",
                        event="event",
                        method="strat_nieto",
                        adjust_vars=c("x1", "x3"),
                        conf_int=FALSE)

# plot the curves
plot(adjsurv)
</code></pre>

<hr>
<h2 id='surv_tmle'>
Targeted Maximum Likelihood Estimation for Continuously Distributed Time-To-Event Data
</h2><span id='topic+surv_tmle'></span>

<h3>Description</h3>

<p>This page explains the details of estimating causal survival curves in a competing risks setting with targeted maximum likelihood estimation (<code>method="tmle"</code> in the <code><a href="#topic+adjustedsurv">adjustedsurv</a></code> function). All regular arguments of the <code>adjustedsurv</code> function can be used. Additionally, the <code>outcome_model</code> argument and the <code>treatment_model</code> argument have to be specified in the <code>adjustedsurv</code> call. Further arguments specific to this method are listed below.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="surv_tmle_+3A_outcome_model">outcome_model</code></td>
<td>

<p>[<strong>required</strong>] Should be a list containing at least one Cox model formula. In those Cox model formula, the time variable is always called <code>time</code> and the status variable should always be called <code>status</code>. For example, to use just one Cox model including all variables of the dataset as independent variables the user can use <code>list(Surv(time, status) ~ .)</code>. All Cox models defined in that list are fitted to the data and the ensemble of these models is used to provide the initial predictions for the conditional hazard. See details and the documentation of the <span class="pkg">concrete</span> package for more information.
</p>
</td></tr>
<tr><td><code id="surv_tmle_+3A_treatment_model">treatment_model</code></td>
<td>

<p>[<strong>required</strong>] A character vector specifying which <span class="pkg">SuperLearner</span> libraries should be used to obtain an estimate of the propensity score. For example, <code>c("SL.glm", "SL.glmnet")</code> could be used. See <code>?SuperLearner</code> for more details.
</p>
</td></tr>
<tr><td><code id="surv_tmle_+3A_censoring_model">censoring_model</code></td>
<td>

<p>Either <code>NULL</code> (default) to make no adjustments for dependent censoring, or a list of Cox models as described in the <code>outcome_model</code> argument. The only difference between this and the <code>outcome_model</code> argument is that <code>status==0</code> should be used in the Cox formulas. See below or <code>?formatArguments</code> in the <span class="pkg">concrete</span> package for more details.
</p>
</td></tr>
<tr><td><code id="surv_tmle_+3A_cv_args">cv_args</code></td>
<td>

<p>A list of arguments specifying how exactly cross-validation should be performed. Internally passed to the <code>CVArg</code> argument of the <code>formatArguments</code> function in the <span class="pkg">concrete</span> package.
</p>
</td></tr>
<tr><td><code id="surv_tmle_+3A_max_update_iter">max_update_iter</code></td>
<td>

<p>A single positive integer specifying the maximum iterations performed to obtain the estimates. Defaults to 500. Internally passed to the <code>MaxUpdateIter</code> argument of the <code>formatArguments</code> function in the <span class="pkg">concrete</span> package.
</p>
</td></tr>
<tr><td><code id="surv_tmle_+3A_one_step_eps">one_step_eps</code></td>
<td>

<p>A single positive number specifying the step size of the tmle updates. Defaults to 0.1. Internally passed to the <code>OneStepEps</code> argument of the <code>formatArguments</code> function in the <span class="pkg">concrete</span> package.
</p>
</td></tr>
<tr><td><code id="surv_tmle_+3A_min_nuisance">min_nuisance</code></td>
<td>

<p>A single number between 0 and 1 used for truncating the g-related denominator of the clever covariate. Defaults to <code>5/sqrt(nrow(data))/log(nrow(data))</code>. Internally passed to the <code>MinNuisance</code> argument of the <code>formatArguments</code> function in the <span class="pkg">concrete</span> package.
</p>
</td></tr>
<tr><td><code id="surv_tmle_+3A_verbose">verbose</code></td>
<td>

<p>Whether to print estimation information of the <code>doConcrete</code> function in the <span class="pkg">concrete</span> package. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="surv_tmle_+3A_return_models">return_models</code></td>
<td>

<p>Whether to add the estimated models for the outcome, treatment, and censoring mechanism to the output object. Defaults to <code>TRUE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p><strong>Type of Adjustment:</strong> Requires a model describing the treatment assignment mechanism and the outcome mechanism, also allows a model for the censoring mechanism. See details and the <span class="pkg">concrete</span> package.
</p>
</li>
<li><p><strong>Doubly-Robust:</strong> Estimates are Doubly-Robust.
</p>
</li>
<li><p><strong>Categorical groups:</strong> This function currently only allows two levels in <code>variable</code>.
</p>
</li>
<li><p><strong>Approximate Variance:</strong> Calculations to approximate the variance and confidence intervals are available.
</p>
</li>
<li><p><strong>Allowed Time Values:</strong> Allows both continuous and integer time.
</p>
</li>
<li><p><strong>Bounded Estimates:</strong> Estimates are guaranteed to be bounded in the 0 to 1 probability range.
</p>
</li>
<li><p><strong>Monotone Function:</strong> Estimates are guaranteed to be monotone.
</p>
</li>
<li><p><strong>Dependencies:</strong> This method relies on the <span class="pkg">concrete</span> package, the <span class="pkg">data.table</span> package and all of their respective dependencies.
</p>
</li></ul>

<p><strong><em>What it does:</em></strong>
</p>
<p>This function implements Targeted Maximum Likelihood Estimation (TMLE) for continuously distributed time-to-event data as described in Rytgaard et al. (2023) and Rytgaard and van der Laan (2022). The TMLE method is similar to the AIPTW methods included in this package. It also relies on both an outcome model and a treatment model (with an additional optional censoring model) to obtain the counterfactual survival probability estimates. In contrast to the AIPTW methods, however, the estimator uses an iterative approach to obtain the estimates where each update targets the entire survival curve. As a consequence, the resulting estimates are guaranteed to lie in the 0/1 probability bounds and are also guaranteed to be non-increasing over time. Simulation studies and theoretical results indicate a good performance of this method in terms of bias and standard errors. See the cited literature for more detailed and more rigorous explanations of the method.
</p>
<p>Instead of relying on a single model to obtain the propensity score or the initial conditional hazards estimates, this estimator relies on the <span class="pkg">SuperLearner</span> framework in conjunction with cross-validation to do this. How cross-validation should be performed may be controlled with the <code>cv_args</code> argument. The resulting models can be inspected from the output object if <code>return_models</code> is set to <code>TRUE</code>.
</p>
<p><strong><em>The Implementation:</em></strong>
</p>
<p>Internally, this function simply calls multiple functions of the <span class="pkg">concrete</span> package in correct order with appropriate arguments. This wrapper function is limited in the sense that it does not allow dynamic interventions or time-varying variables, which are supported by the <span class="pkg">concrete</span> package. It is recommended to use the <span class="pkg">concrete</span> package directly when the user wants to use these features or other specific settings are required.
</p>
<p><strong><em>Speed Considerations:</em></strong>
</p>
<p>This method is very computationally expensive. For medium to large datasets and when considering many different points in time, it will usually take a very long time to execute. If speed is important, we recommend using other methods. Alternatively, user may adjust the <code>times</code> arguments to target fewer points in time.
</p>


<h3>Value</h3>

<p>Adds the following additional objects to the output of the <code>adjustedsurv</code> function:
</p>

<ul>
<li> <p><code>concrete_object</code>: The object returned by the <code>doConcrete</code> function.
</p>
</li></ul>



<h3>Note</h3>

<p>A previous version of this package (&lt;= 0.9.1) included a function with the same name, which was removed in version 0.10.0. The old version implemented a TMLE estimator that was only applicable to discrete-time survival data based on the <span class="pkg">survtmle</span> package, which was removed from CRAN. The new version implements a different estimator. Code using this method for version &lt;= 0.9.1 does NOT work with versions 0.10.2 or higher.
</p>


<h3>Author(s)</h3>

<p>The wrapper function was written by Robin Denz, but the real estimation functions are all contained in the <span class="pkg">concrete</span> package, which was written by David Chen. See <code>?doConcrete</code> for more information.
</p>


<h3>References</h3>

<p>Helene C. W. Rytgaard and Mark J. van der Laan (2023). &quot;Targeted Maximum Likelihood Estimation for Causal Inference in Survival and Competing Risks Analysis&quot;. In: Lifetime Data Analysis
</p>
<p>Helene C. W. Rytgaard and Mark J. van der Laan (2023). &quot;One-Step Targeted Maximum Likelihood Estimation for Targeting Cause-Specific Absolute Risks and Survival Curves&quot;. In: Biometrika
</p>
<p>Helene C. W. Rytgaard, Frank Eriksson and Mark J. van der Laan (2023). &quot;Estimation of Time-Specific Intervention Effects on Continuously Distributed Time-To-Event Outcomes by Targeted Maximum Likelihood Estimation&quot;. In: Biometrics
</p>
<p>David Chen, Helene C. W. Rytgaard and Edwin Fong and Jens M. Tarp and Maya L. Petersen and Mark J. van der Laan and Thomas A. Gerds (2023). &quot;concrete: An R Package for Continuous-Time, Competing Risks Targeted Maximum Likelihood Estimation&quot;. Available at &lt;https://github.com/imbroglio-dc/concrete&gt; or on CRAN
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adjustedcif">adjustedcif</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(adjustedCurves)

data &lt;- sim_confounded_surv(n=100)
data$group &lt;- factor(data$group)

# for a single point in time using only one model for both
# the treatment mechanism and outcome mechanism
out &lt;- adjustedsurv(data=data,
                    variable="group",
                    ev_time="time",
                    event="event",
                    treatment_model=c("SL.glm"),
                    outcome_model=list(Surv(time, status) ~ .),
                    times=c(0.5),
                    conf_int=TRUE,
                    method="tmle")

## using multiple models for both the treatment assignment and
## outcome mechanism
out &lt;- adjustedsurv(data=data,
                    variable="group",
                    ev_time="time",
                    event="event",
                    treatment_model=c("SL.glm", "SL.mean"),
                    outcome_model=list(Surv(time, status) ~ x1 + x3,
                                       Surv(time, status) ~ x2 + x4 + x5),
                    times=c(0.5),
                    conf_int=TRUE,
                    method="tmle")

## with corrections for covariate dependent censoring
out &lt;- adjustedsurv(data=data,
                    variable="group",
                    ev_time="time",
                    event="event",
                    treatment_model=c("SL.glm", "SL.mean"),
                    outcome_model=list(Surv(time, status) ~ x1 + x3,
                                       Surv(time, status) ~ x2 + x4 + x5),
                    censoring_model=list(Surv(time, status==0) ~ x6 + x1),
                    times=c(0.5),
                    conf_int=TRUE,
                    method="tmle")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
