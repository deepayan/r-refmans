<!DOCTYPE html><html><head><title>Help for package laGP</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {laGP}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#aGP'>
<p>Localized Approximate GP Regression For Many Predictive Locations</p></a></li>
<li><a href='#alcGP'>
<p>Improvement statistics for sequential or local design</p></a></li>
<li><a href='#blhs'>
<p>Bootstrapped block Latin hypercube subsampling</p></a></li>
<li><a href='#darg'>
<p>Generate Priors for GP correlation</p></a></li>
<li><a href='#deleteGP'>
<p>Delete C-side Gaussian Process Objects</p></a></li>
<li><a href='#discrep.est'>
<p>Estimate Discrepancy in Calibration Model</p></a></li>
<li><a href='#distance'>
<p>Calculate the squared Euclidean distance between pairs of points</p></a></li>
<li><a href='#fcalib'>
<p>Objective function for performing large scale computer model</p>
calibration via optimization</a></li>
<li><a href='#laGP'>
<p>Localized Approximate GP Prediction At a Single Input Location</p></a></li>
<li><a href='#llikGP'>
<p>Calculate a GP log likelihood</p></a></li>
<li><a href='#mleGP'>
<p>Inference for GP correlation parameters</p></a></li>
<li><a href='#newGP'>
<p>Create A New GP Object</p></a></li>
<li><a href='#optim.auglag'>
<p>Optimize an objective function under</p>
multiple blackbox constraints</a></li>
<li><a href='#predGP'>
<p>GP Prediction/Kriging</p></a></li>
<li><a href='#randLine'>
<p>Generate two-dimensional random paths</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Local Approximate Gaussian Process Regression</td>
</tr>
<tr>
<td>Version:</td>
<td>1.5-9</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-03-13</td>
</tr>
<tr>
<td>Author:</td>
<td>Robert B. Gramacy &lt;rbg@vt.edu&gt;, Furong Sun &lt;furongs@vt.edu&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.14)</td>
</tr>
<tr>
<td>Imports:</td>
<td>tgp, parallel</td>
</tr>
<tr>
<td>Suggests:</td>
<td>mvtnorm, MASS, interp, lhs, crs, DiceOptim</td>
</tr>
<tr>
<td>Description:</td>
<td>Performs approximate GP regression for large computer experiments and spatial datasets.  The approximation is based on finding small local designs for prediction (independently) at particular inputs. OpenMP and SNOW parallelization are supported for prediction over a vast out-of-sample testing set; GPU acceleration is also supported for an important subroutine.  OpenMP and GPU features may require special compilation.  An interface to lower-level (full) GP inference and prediction is provided. Wrapper routines for blackbox optimization under mixed equality and inequality constraints via an augmented Lagrangian scheme, and for large scale computer model calibration, are also provided.  For details and tutorial, see Gramacy (2016 &lt;<a href="https://doi.org/10.18637%2Fjss.v072.i01">doi:10.18637/jss.v072.i01</a>&gt;.</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Robert B. Gramacy  &lt;rbg@vt.edu&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-2">LGPL-2</a> | <a href="https://www.r-project.org/Licenses/LGPL-2.1">LGPL-2.1</a> | <a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a> [expanded from: LGPL]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://bobby.gramacy.com/r_packages/laGP/">https://bobby.gramacy.com/r_packages/laGP/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-03-13 16:57:56 UTC; bobby</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-03-14 08:30:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='aGP'>
Localized Approximate GP Regression For Many Predictive Locations
</h2><span id='topic+aGP'></span><span id='topic+aGP.R'></span><span id='topic+aGPsep.R'></span><span id='topic+aGPsep'></span><span id='topic+aGP.parallel'></span><span id='topic+aGP.seq'></span>

<h3>Description</h3>

<p>Facilitates localized Gaussian process inference and prediction at a large 
set of predictive locations, by (essentially) calling <code><a href="#topic+laGP">laGP</a></code>
at each location, and returning the moments of the predictive
equations, and indices into the design, thus obtained
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aGP(X, Z, XX, start = 6, end = 50, d = NULL, g = 1/10000,
    method = c("alc", "alcray", "mspe", "nn", "fish"), Xi.ret = TRUE,
    close = min((1000+end)*if(method[1] == "alcray") 10 else 1, nrow(X)), 
    numrays = ncol(X), num.gpus = 0, gpu.threads = num.gpus, 
    omp.threads = if (num.gpus &gt; 0) 0 else 1, 
    nn.gpu = if (num.gpus &gt; 0) nrow(XX) else 0, verb = 1)
aGP.parallel(cls, XX, chunks = length(cls), X, Z, start = 6, end = 50, 
    d = NULL, g = 1/10000, method = c("alc", "alcray", "mspe", "nn", "fish"), 
    Xi.ret = TRUE, 
    close = min((1000+end)*if(method[1] == "alcray") 10 else 1, nrow(X)),
    numrays = ncol(X), num.gpus = 0, gpu.threads = num.gpus, 
    omp.threads = if (num.gpus &gt; 0) 0 else 1,
    nn.gpu = if (num.gpus &gt; 0) nrow(XX) else 0, verb = 1)
aGP.R(X, Z, XX, start = 6, end = 50, d = NULL, g = 1/10000,
    method = c("alc", "alcray", "mspe", "nn", "fish"), Xi.ret = TRUE,
    close = min((1000+end) *if(method[1] == "alcray") 10 else 1, nrow(X)),
    numrays = ncol(X), laGP=laGP.R, verb = 1)
aGPsep(X, Z, XX, start = 6, end = 50, d = NULL, g = 1/10000,
    method = c("alc", "alcray", "nn"), Xi.ret = TRUE,
    close = min((1000+end)*if(method[1] == "alcray") 10 else 1, nrow(X)),
    numrays = ncol(X),  omp.threads = 1, verb = 1)
aGPsep.R(X, Z, XX, start = 6, end = 50, d = NULL, g = 1/10000,
    method = c("alc", "alcray", "nn"), Xi.ret = TRUE,
    close = min((1000+end)*if(method[1] == "alcray") 10 else 1, nrow(X)),
    numrays = ncol(X), laGPsep=laGPsep.R, verb = 1)
aGP.seq(X, Z, XX, d, methods=rep("alc", 2), M=NULL, ncalib=0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aGP_+3A_x">X</code></td>
<td>
<p>a <code>matrix</code> or <code>data.frame</code> containing
the full (large) design matrix of input locations
</p>
</td></tr>
<tr><td><code id="aGP_+3A_z">Z</code></td>
<td>

<p>a vector of responses/dependent values with <code>length(Z) = nrow(X)</code>
</p>
</td></tr>
<tr><td><code id="aGP_+3A_xx">XX</code></td>
<td>

<p>a <code>matrix</code> or <code>data.frame</code> of out-of-sample
predictive locations with <code>ncol(XX) = ncol(X)</code>; <code>aGP</code> calls <code>laGP</code> for
each row of <code>XX</code> as a value of <code>Xref</code>, independently
</p>
</td></tr>
<tr><td><code id="aGP_+3A_start">start</code></td>
<td>

<p>the number of Nearest Neighbor (NN) locations to start each
independent call to <code>laGP</code> with; must have <code>start &gt;= 6</code>
</p>
</td></tr>
<tr><td><code id="aGP_+3A_end">end</code></td>
<td>

<p>the total size of the local designs; <code>start &lt; end</code>
</p>
</td></tr>
<tr><td><code id="aGP_+3A_d">d</code></td>
<td>

<p>a prior or initial setting for the lengthscale
parameter in a Gaussian correlation function; a (default)
<code>NULL</code> value triggers a sensible regularization (prior) and
initial setting to be generated via <code><a href="#topic+darg">darg</a></code>;
a scalar specifies an initial value, causing <code><a href="#topic+darg">darg</a></code>
to only generate the prior; otherwise,
a list or partial list matching the output
of <code><a href="#topic+darg">darg</a></code> can be used to specify a custom prior.  In the
case of a partial list, only the missing entries will be
generated. Note that a default/generated list specifies MLE/MAP
inference for this parameter.  When specifying initial values, a
vector of length <code>nrow(XX)</code> can be provided, giving a
different initial value for each predictive location. With 
<code>aGPsep</code>, the starting values can be an 
<code>ncol(X)</code>-by-<code>nrow(XX)</code> <code>matrix</code> or an <code>ncol(X)</code> vector
</p>
</td></tr>
<tr><td><code id="aGP_+3A_g">g</code></td>
<td>

<p>a prior or initial setting for the nugget parameter; a 
<code>NULL</code> value causes a sensible regularization (prior) and
initial setting to be generated via <code><a href="#topic+garg">garg</a></code>; a scalar
(default <code>g = 1/10000</code>) specifies an initial value, causing <code><a href="#topic+garg">garg</a></code>
to only generate the prior; otherwise, a
list or partial list matching the output of <code><a href="#topic+garg">garg</a></code> can be used to
specify a custom prior.  In the case of a partial list, only the
missing entries will be generated. Note that a default/generated list
specifies <em>no</em> inference for this parameter; i.e., it is fixed
at its starting or default value, which may be appropriate for
emulating deterministic computer code output. In such situations a 
value much smaller than the default may work even better (i.e., 
yield better out-of-sample predictive performance).  
The default was chosen conservatively.  When specifying non-default initial
values, a vector of length <code>nrow(XX)</code> can be provided, giving a different
initial value for each predictive location
</p>
</td></tr>
<tr><td><code id="aGP_+3A_method">method</code></td>
<td>

<p>specifies the method by which <code>end-start</code> candidates from
<code>X</code> are chosen in order to predict at each row <code>XX</code> independently. 
In brief, ALC (<code>"alc"</code>, default) minimizes predictive variance; 
ALCRAY (<code>"alcray")</code> executes a thrifty search focused on rays emanating
from the reference location(s); MSPE
(<code>"mspe"</code>) augments ALC with extra derivative information to
minimize mean-squared prediction error (requires extra computation);
NN (<code>"nn"</code>) uses nearest neighbor; and (<code>"fish"</code>) uses
the expected Fisher information - essentially <code>1/G</code> from
Gramacy &amp; Apley (2015) - which is global heuristic, i.e., not
localized to each row of <code>XX</code></p>
</td></tr>
<tr><td><code id="aGP_+3A_methods">methods</code></td>
<td>
<p> for <code>aGP.seq</code> this is a vectorized <code>method</code> argument,
containing a list of valid methods to perform in sequence.  When <code>methods = FALSE</code>
a call to <code>M</code> is invoked instead; see below for
more details </p>
</td></tr>
<tr><td><code id="aGP_+3A_xi.ret">Xi.ret</code></td>
<td>

<p>a scalar logical indicating whether or not a <code>matrix</code> of indices
into <code>X</code>, describing the chosen sub-design for each of the
predictive locations in <code>XX</code>, should be returned on
output</p>
</td></tr>
<tr><td><code id="aGP_+3A_close">close</code></td>
<td>

<p>a non-negative integer  <code>end &lt; close &lt;= nrow(X)</code>
specifying the number of NNs
(to each row <code>XX</code>) in <code>X</code> to consider when
searching for the sub-design; <code>close = 0</code> specifies all. 
For <code>method="alcray"</code> this
specifies the scope used to snap ray-based solutions back to
elements of <code>X</code>, otherwise there are no restrictions on that 
search
</p>
</td></tr>
<tr><td><code id="aGP_+3A_numrays">numrays</code></td>
<td>
<p> a scalar integer indicating the number of rays for each
greedy search; only relevant when <code>method="alcray"</code>.  More rays
leads to a more thorough, but more computationally intensive search </p>
</td></tr>
<tr><td><code id="aGP_+3A_lagp">laGP</code></td>
<td>

<p>applicable only to the <span class="rlang"><b>R</b></span>-version <code>aGP.R</code>, this is a
function providing the local design implementation to be used.
Either <code><a href="#topic+laGP">laGP</a></code> or <code><a href="#topic+laGP.R">laGP.R</a></code> can be
provided, or a bespoke routine providing similar outputs
</p>
</td></tr>
<tr><td><code id="aGP_+3A_lagpsep">laGPsep</code></td>
<td>

<p>applicable only to the <span class="rlang"><b>R</b></span>-version <code>aGPsep.R</code>, this is a
function providing the local design implementation to be used.
Either <code><a href="#topic+laGPsep">laGPsep</a></code> or <code><a href="#topic+laGPsep.R">laGPsep.R</a></code> can be
provided, or a bespoke routine providing similar outputs
</p>
</td></tr>
<tr><td><code id="aGP_+3A_num.gpus">num.gpus</code></td>
<td>

<p>applicable only to the C-version <code>aGP</code>, this is a scalar 
positive integer indicating the number of GPUs available for calculating 
ALC (see <code><a href="#topic+alcGP">alcGP</a></code>); the package must be compiled for CUDA support; 
see README/INSTALL in the package source for more details
</p>
</td></tr>
<tr><td><code id="aGP_+3A_gpu.threads">gpu.threads</code></td>
<td>
<p> applicable only to the C-version <code>aGP</code>; this 
is a scalar positive integer indicating the number of SMP (i.e., CPU)
threads queuing ALC jobs on a GPU; the package must be compiled for CUDA support.
If <code>gpu.threads &gt;= 2</code> then the package must <em>also</em>
be compiled for OpenMP support; see README/INSTALL in the package source for
more details.  We recommend setting <code>gpu.threads</code> to up to two-times
the sum of the number of GPU devices and CPU cores.
Only <code>method = "alc"</code> is supported when using CUDA.   If the
sum of <code>omp.threads</code> and <code>gpu.threads</code> is bigger than the
max allowed by your system, then that max is used instead (giving 
<code>gpu.threads</code> preference) 
</p>
</td></tr>
<tr><td><code id="aGP_+3A_omp.threads">omp.threads</code></td>
<td>
<p> applicable only to the C-version <code>aGP</code>;
this is a scalar positive integer indicating the number
of threads to use for SMP parallel processing; the package must be 
compiled for OpenMP support; see README/INSTALL in the package source for
more details.  For most Intel-based machines, we recommend setting
<code>omp.threads</code> to up to two-times the number of hyperthreaded cores.  When
using GPUs (<code>num.gpu &gt; 0</code>), a good default is <code>omp.threads=0</code>,
otherwise load balancing could be required; see <code>nn.gpu</code> below. If the
sum of <code>omp.threads</code> and <code>gpu.threads</code> is bigger than the
max allowed by your system, then that max is used instead (giving 
<code>gpu.threads</code> preference)  </p>
</td></tr>
<tr><td><code id="aGP_+3A_nn.gpu">nn.gpu</code></td>
<td>
<p> a scalar non-negative integer between <code>0</code> and 
<code>nrow(XX)</code> indicating the number of predictive locations 
utilizing GPU ALC calculations.  Note this argument is only useful when
both <code>gpu.threads</code> and <code>omp.threads</code> are non-zero, whereby
it acts as a load balancing mechanism </p>
</td></tr>
<tr><td><code id="aGP_+3A_verb">verb</code></td>
<td>

<p>a non-negative integer specifying the verbosity level; <code>verb = 0</code>
is quiet, and larger values cause more progress information to be
printed to the screen.  The value <code>min(0,verb-1)</code> is provided
to each <code>laGP</code> call</p>
</td></tr>
<tr><td><code id="aGP_+3A_cls">cls</code></td>
<td>
 
<p>a cluster object created by <code><a href="parallel.html#topic+makeCluster">makeCluster</a></code> 
from the <span class="pkg">parallel</span> or <span class="pkg">snow</span> packages
</p>
</td></tr>
<tr><td><code id="aGP_+3A_chunks">chunks</code></td>
<td>

<p>a scalar integer indicating the number of chunks to break <code>XX</code> into
for <span class="pkg">parallel</span> evaluation on cluster <code>cls</code>.  
Usually <code>chunks = length(cl)</code> is appropriate.  
However specifying more chunks can be useful when the nodes of
the cluster are not homogeneous</p>
</td></tr>
<tr><td><code id="aGP_+3A_m">M</code></td>
<td>
<p> an optional function taking two matrix inputs, of <code>ncol(X)-ncalib</code>
and <code>ncalib</code> columns respectively, which is applied in lieu of 
<code>aGP</code>.  This can be useful for calibration where the computer model
is cheap, i.e., does not require emulation; more details below
</p>
</td></tr>
<tr><td><code id="aGP_+3A_ncalib">ncalib</code></td>
<td>
<p> an integer between 1 and <code>ncol(X)</code> indicating how to
partition <code>X</code> and <code>XX</code> inputs into the two matrices required for <code>M</code>
</p>
</td></tr>
<tr><td><code id="aGP_+3A_...">...</code></td>
<td>
<p> other arguments passed from <code>aGP.sep</code> to <code>aGP</code> </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function invokes <code><a href="#topic+laGP">laGP</a></code> with argument <code>Xref
  = XX[i,]</code> for each <code>i=1:nrow(XX)</code>, building up local designs,
inferring local correlation parameters, and
obtaining predictive locations independently for each location.  For
more details see <code><a href="#topic+laGP">laGP</a></code>.
</p>
<p>The function <code>aGP.R</code> is a prototype <span class="rlang"><b>R</b></span>-only version for
debugging and transparency purposes.  It is slower than
<code>aGP</code>, which is primarily in C.  However it may be
useful for developing new programs that involve similar subroutines.
Note that <code>aGP.R</code> may provide different output than <code>aGP</code>
due to differing library subroutines deployed in <span class="rlang"><b>R</b></span> and C.
</p>
<p>The function <code>aGP.parallel</code> allows <code>aGP</code> to be called on segments
of the <code>XX</code> matrix distributed to a cluster created by <span class="pkg">parallel</span>.
It breaks <code>XX</code> into <code>chunks</code> which are sent to <code>aGP</code> 
workers pointed to by the entries of <code>cls</code>.  The <code>aGP.parallel</code> function
collects the outputs from each chunk before returning an object
almost identical to what would have been returned from a single <code>aGP</code>
call.  On a single (SMP) node, this represents is a poor-man's version of
the OpenMP version described below.  On multiple nodes both can be used.
</p>
<p>If compiled with OpenMP flags, the independent calls to 
<code><a href="#topic+laGP">laGP</a></code> will be
farmed out to threads allowing them to proceed in parallel - obtaining
nearly linear speed-ups.  At this time <code>aGP.R</code> does not
facilitate parallel computation, although a future version may exploit
the <span class="pkg">parallel</span> functionality for clustered parallel execution.
</p>
<p>If <code>num.gpus &gt; 0</code> then the ALC part of the independent 
calculations performed by each thread will be offloaded to a GPU.  
If both <code>gpu.threads &gt;= 1</code> and <code>omp.threads &gt;= 1</code>, 
some of the ALC calculations will be done on the GPUs, and some 
on the CPUs.  In our own experimentation we have not found this
to lead to large speedups relative to <code>omp.threads = 0</code> when
using GPUs.  For more details, see Gramacy, Niemi, &amp; Weiss (2014).
</p>
<p>The <code>aGP.sep</code> function is provided primarily for use in calibration
exercises, see Gramacy, et al. (2015).  It automates a sequence of
<code>aGP</code> calls, each with a potentially different method, 
successively feeding the previous estimate of local lengthscale (<code>d</code>)
in as an initial set of values for the next call.  It also allows the
use of <code>aGP</code> to be bypassed, feeding the inputs into a user-supplied
<code>M</code> function instead.  This feature is enabled when 
<code>methods = FALSE</code>.  The <code>M</code> function takes two matrices
(same number of rows) as inputs, where the first <code>ncol(X) - ncalib</code> 
columns represent
&ldquo;field data&rdquo; inputs shared by the physical and computer model
(in the calibration context), and the remaining <code>ncalib</code> are 
the extra tuning or calibration parameters required to evalue the computer
model.  For examples illustrating <code>aGP.seq</code> please see the
documentation file for <code><a href="#topic+discrep.est">discrep.est</a></code> and <code>demo("calib")</code>
</p>


<h3>Value</h3>

<p>The output is a <code>list</code> with the following components.
</p>
<table>
<tr><td><code>mean</code></td>
<td>
<p>a vector of predictive means of length <code>nrow(XX)</code></p>
</td></tr>
<tr><td><code>var</code></td>
<td>
<p>a vector of predictive variances of length
<code>nrow(Xref)</code></p>
</td></tr>
<tr><td><code>llik</code></td>
<td>
<p>a vector indicating the log likelihood/posterior
probability of the data/parameter(s) under the chosen sub-design for
each predictive location in <code>XX</code>; provided up to an additive constant</p>
</td></tr>
<tr><td><code>time</code></td>
<td>
<p>a scalar giving the passage of wall-clock time elapsed
for (substantive parts of) the calculation</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a copy of the <code>method</code> argument</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>a full-list version of the <code>d</code> argument, possibly completed by <code>darg</code></p>
</td></tr>
<tr><td><code>g</code></td>
<td>
<p>a full-list version of the <code>g</code> argument, possibly
completed by <code>garg</code></p>
</td></tr>
<tr><td><code>mle</code></td>
<td>
<p>if <code>d$mle</code> and/or <code>g$mle</code> are <code>TRUE</code>, then
<code>mle</code> is a <code>data.frame</code> containing the values found for
these parameters, and the number of required iterations, for each
predictive location in <code>XX</code> </p>
</td></tr>
<tr><td><code>Xi</code></td>
<td>
<p>when <code>Xi.ret = TRUE</code>, this field contains a <code>matrix</code> of
indices of length <code>end</code> into <code>X</code> indicating the sub-design
chosen for each predictive location in <code>XX</code></p>
</td></tr>
<tr><td><code>close</code></td>
<td>
<p> a copy of the input argument </p>
</td></tr>
</table>
<p>The <code>aGP.seq</code> function only returns the output from the final <code>aGP</code> call.
When <code>methods = FALSE</code> and <code>M</code> is supplied, the returned object
is a data frame with a <code>mean</code> column indicating the output of the computer
model run, and a <code>var</code> column, which at this time is zero
</p>


<h3>Note</h3>

<p><code>aGPsep</code> provides the same functionality as <code>aGP</code> but deploys
a separable covariance function.  Criteria (<code>method</code>s) EFI and
MSPE are not supported by <code>aGPsep</code> at this time.
</p>
<p>Note that using <code>method="NN"</code> gives the same result as specifying
<code>start=end</code>, however at some extra computational expense.
</p>
<p>At this time, this function provides no facility to find local designs
for the subset of predictive locations <code>XX</code> jointly, i.e.,
providing a matrix <code>Xref</code> to <code><a href="#topic+laGP">laGP</a></code>.  See <code><a href="#topic+laGP">laGP</a></code>
for more details/support for this alternative.
</p>
<p>The use of OpenMP threads with <code>aGPsep</code> is not as efficient as with
<code>aGP</code> when calculating MLEs with respect to the lengthscale (i.e.,
<code>d=list(mle=TRUE, ...)</code>).  The reason is that the <code>lbfgsb</code> C
entry point uses static variables, and is therefore not thread safe.  
To circumvent this problem, an OpenMP <code>critical</code> pragma is used,
which can create a small bottle neck
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Gramacy, R. B. (2020) <em>Surrogates: Gaussian Process Modeling,
Design and Optimization for the Applied Sciences</em>. Boca Raton,
Florida: Chapman Hall/CRC.  (See Chapter 9.)
<a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p>R.B. Gramacy (2016). <em><span class="pkg">laGP</span>: Large-Scale Spatial Modeling via 
Local Approximate Gaussian Processes in <span class="rlang"><b>R</b></span>.</em>, Journal of Statistical 
Software, 72(1), 1-46; <a href="https://doi.org/10.18637/jss.v072.i01">doi:10.18637/jss.v072.i01</a> 
or see <code>vignette("laGP")</code>
</p>
<p>R.B. Gramacy and D.W. Apley (2015).
<em>Local Gaussian process approximation for large computer
experiments.</em> Journal of Computational and Graphical Statistics, 
24(2), pp. 561-678; preprint on arXiv:1303.0383; 
<a href="https://arxiv.org/abs/1303.0383">https://arxiv.org/abs/1303.0383</a>
</p>
<p>R.B. Gramacy, J. Niemi, R.M. Weiss (2014). 
<em>Massively parallel approximate Gaussian process regression.</em>
SIAM/ASA Journal on Uncertainty Quantification, 2(1), pp. 568-584;
preprint on arXiv:1310.5182;
<a href="https://arxiv.org/abs/1310.5182">https://arxiv.org/abs/1310.5182</a>
</p>
<p>R.B. Gramacy and B. Haaland (2016).
<em>Speeding up neighborhood search in local Gaussian process prediction.</em>
Technometrics, 58(3), pp. 294-303;
preprint on arXiv:1409.0074 
<a href="https://arxiv.org/abs/1409.0074">https://arxiv.org/abs/1409.0074</a>
</p>


<h3>See Also</h3>

<p><code>vignette("laGP")</code>, 
<code><a href="#topic+laGP">laGP</a></code>, <code><a href="#topic+alcGP">alcGP</a></code>, <code><a href="#topic+mspeGP">mspeGP</a></code>,  <code><a href="#topic+alcrayGP">alcrayGP</a></code>,
<code><a href="parallel.html#topic+makeCluster">makeCluster</a></code>, <code><a href="parallel.html#topic+clusterApply">clusterApply</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## first, a "computer experiment"

## Simple 2-d test function used in Gramacy &amp; Apley (2014);
## thanks to Lee, Gramacy, Taddy, and others who have used it before
f2d &lt;- function(x, y=NULL)
  {
    if(is.null(y)){
      if(!is.matrix(x) &amp;&amp; !is.data.frame(x)) x &lt;- matrix(x, ncol=2)
      y &lt;- x[,2]; x &lt;- x[,1]
    }
    g &lt;- function(z)
      return(exp(-(z-1)^2) + exp(-0.8*(z+1)^2) - 0.05*sin(8*(z+0.1)))
    z &lt;- -g(x)*g(y)
  }

## build up a design with N=~40K locations
x &lt;- seq(-2, 2, by=0.02)
X &lt;- expand.grid(x, x)
Z &lt;- f2d(X)

## predictive grid with NN=400 locations,
## change NN to 10K (length=100) to mimic setup in Gramacy &amp; Apley (2014)
## the low NN set here is for fast CRAN checks
xx &lt;- seq(-1.975, 1.975, length=10)
XX &lt;- expand.grid(xx, xx)
ZZ &lt;- f2d(XX)

## get the predictive equations, first based on Nearest Neighbor
out &lt;- aGP(X, Z, XX, method="nn", verb=0)
## RMSE
sqrt(mean((out$mean - ZZ)^2))

## Not run: 
## refine with ALC
out2 &lt;- aGP(X, Z, XX, method="alc", d=out$mle$d)
## RMSE
sqrt(mean((out2$mean - ZZ)^2))

## visualize the results
par(mfrow=c(1,3))
image(xx, xx, matrix(out2$mean, nrow=length(xx)), col=heat.colors(128),
      xlab="x1", ylab="x2", main="predictive mean")
image(xx, xx, matrix(out2$mean-ZZ, nrow=length(xx)), col=heat.colors(128),
      xlab="x1", ylab="x2", main="bias")
image(xx, xx, matrix(sqrt(out2$var), nrow=length(xx)), col=heat.colors(128),
      xlab="x1", ylab="x2", main="sd")

## refine with MSPE
out3 &lt;- aGP(X, Z, XX, method="mspe", d=out2$mle$d)
## RMSE
sqrt(mean((out3$mean - ZZ)^2))

## End(Not run)

## version with ALC-ray which is much faster than the ones not
## run above
out2r &lt;- aGP(X, Z, XX, method="alcray", d=out$mle$d, verb=0)
sqrt(mean((out2r$mean - ZZ)^2))

## a simple example with estimated nugget
if(require("MASS")) {

  ## motorcycle data and predictive locations
  X &lt;- matrix(mcycle[,1], ncol=1)
  Z &lt;- mcycle[,2]
  XX &lt;- matrix(seq(min(X), max(X), length=100), ncol=1)

  ## first stage
  out &lt;- aGP(X=X, Z=Z, XX=XX, end=30, g=list(mle=TRUE), verb=0) 
  
  ## plot smoothed versions of the estimated parameters
  par(mfrow=c(2,1))
  df &lt;- data.frame(y=log(out$mle$d), XX)
  lo &lt;- loess(y~., data=df, span=0.25)
  plot(XX, log(out$mle$d), type="l")
  lines(XX, lo$fitted, col=2)
  dfnug &lt;- data.frame(y=log(out$mle$g), XX)
  lonug &lt;- loess(y~., data=dfnug, span=0.25)
  plot(XX, log(out$mle$g), type="l")
  lines(XX, lonug$fitted, col=2)

  ## second stage design
  out2 &lt;- aGP(X=X, Z=Z, XX=XX, end=30, verb=0,
		  d=list(start=exp(lo$fitted), mle=FALSE),
		  g=list(start=exp(lonug$fitted)))
  
  ## plot the estimated surface
  par(mfrow=c(1,1))
  plot(X,Z)
  df &lt;- 20
  s2 &lt;- out2$var*(df-2)/df
  q1 &lt;- qt(0.05, df)*sqrt(s2) + out2$mean
  q2 &lt;- qt(0.95, df)*sqrt(s2) + out2$mean
  lines(XX, out2$mean)
  lines(XX, q1, col=1, lty=2)
  lines(XX, q2, col=1, lty=2)
}

## compare to the single-GP result provided in the mleGP documentation
</code></pre>

<hr>
<h2 id='alcGP'>
Improvement statistics for sequential or local design
</h2><span id='topic+alcGP'></span><span id='topic+alcGPsep'></span><span id='topic+alcrayGP'></span><span id='topic+alcrayGPsep'></span><span id='topic+mspeGP'></span><span id='topic+fishGP'></span><span id='topic+ieciGP'></span><span id='topic+ieciGPsep'></span><span id='topic+alcoptGP'></span><span id='topic+dalcGP'></span><span id='topic+alcoptGPsep'></span><span id='topic+dalcGPsep'></span>

<h3>Description</h3>

<p>Calculate the active learning Cohn (ALC) statistic, mean-squared predictive
error (MSPE) or expected Fisher information (fish) for a Gaussian
process (GP) predictor relative to a set of reference locations, towards
sequential design or local search for Gaussian process regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>alcGP(gpi, Xcand, Xref = Xcand, parallel = c("none", "omp", "gpu"), 
      verb = 0)
alcGPsep(gpsepi, Xcand, Xref = Xcand, parallel = c("none", "omp", "gpu"), 
      verb = 0)
alcrayGP(gpi, Xref, Xstart, Xend, verb = 0)
alcrayGPsep(gpsepi, Xref, Xstart, Xend, verb = 0)
ieciGP(gpi, Xcand, fmin, Xref = Xcand, w = NULL, nonug = FALSE, verb = 0)
ieciGPsep(gpsepi, Xcand, fmin, Xref = Xcand, w = NULL, nonug = FALSE, verb = 0)
mspeGP(gpi, Xcand, Xref = Xcand, fi = TRUE, verb = 0)
fishGP(gpi, Xcand)
alcoptGP(gpi, Xref, start, lower, upper, maxit = 100, verb = 0)
alcoptGPsep(gpsepi, Xref, start, lower, upper, maxit = 100, verb = 0)
dalcGP(gpi, Xcand, Xref = Xcand, verb = 0)
dalcGPsep(gpsepi, Xcand, Xref = Xcand, verb = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="alcGP_+3A_gpi">gpi</code></td>
<td>

<p>a C-side GP object identifier (positive integer);
e.g., as returned by <code><a href="#topic+newGP">newGP</a></code>
</p>
</td></tr>
<tr><td><code id="alcGP_+3A_gpsepi">gpsepi</code></td>
<td>

<p>a C-side separable GP object identifier (positive integer);
e.g., as returned by <code><a href="#topic+newGPsep">newGPsep</a></code>
</p>
</td></tr>
<tr><td><code id="alcGP_+3A_xcand">Xcand</code></td>
<td>

<p>a <code>matrix</code> or <code>data.frame</code> containing
a design of candidate predictive locations at which the ALC
(or other) criteria is (are) evaluated.  In the context of
<code><a href="#topic+laGP">laGP</a></code>, these are the possible locations for adding
into the current local design
</p>
</td></tr>
<tr><td><code id="alcGP_+3A_fmin">fmin</code></td>
<td>
<p> for <code>ieci*</code> only: a scalar value indicating the 
value of the best minimum found so far.  This is usually set to 
the minimum of the <code>Z</code>-values stored in the <code>gpi</code> or 
<code>gpsepi</code> reference (for deterministic/low nugget settings),
or otherwise the predicted mean value at the <code>X</code> locations  
</p>
</td></tr>
<tr><td><code id="alcGP_+3A_xref">Xref</code></td>
<td>

<p>a <code>matrix</code> or <code>data.frame</code> containing a design of
reference locations for ALC or MSPE.  I.e., these are the locations
at which the reduction in variance, or mean squared predictive error,
are calculated.  In the context of <code><a href="#topic+laGP">laGP</a></code>, this is
the single location, or set of reference locations,
around which a local design (for accurate prediction) is sought.  
For <code>alcrayGP</code> and <code>alcrayGPsep</code> the 
<code>matrix</code> may only have one row, i.e., one reference location
</p>
</td></tr>
<tr><td><code id="alcGP_+3A_parallel">parallel</code></td>
<td>

<p>a switch indicating if any parallel calculation of 
the criteria (<code>method</code>) is desired.    
For <code>parallel = "omp"</code>, the package must be compiled with OpenMP flags; 
for <code>parallel = "gpu"</code>, the package must be compiled with CUDA
flags (only the ALC criteria is supported on the GPU); see README/INSTALL
in the package source for more details
</p>
</td></tr>
<tr><td><code id="alcGP_+3A_xstart">Xstart</code></td>
<td>

<p>a <code>1</code>-by-<code>ncol(Xref)</code> starting location for a search along
a ray between <code>Xstart</code> and <code>Xend</code>
</p>
</td></tr>
<tr><td><code id="alcGP_+3A_xend">Xend</code></td>
<td>

<p>a <code>1</code>-by-<code>ncol(Xref)</code> ending location for a search along
a ray between <code>Xstart</code> and <code>Xend</code>
</p>
</td></tr>
<tr><td><code id="alcGP_+3A_fi">fi</code></td>
<td>

<p>a scalar logical indicating if the expected Fisher information portion
of the expression (MSPE is essentially <code>ALC + c(x)*EFI</code>) should be
calculated (<code>TRUE</code>) or set to zero (<code>FALSE</code>).  This flag is mostly
for error checking against the other functions, <code>alcGP</code> and <code>fishGP</code>,
since the constituent parts are separately available via those functions
</p>
</td></tr>
<tr><td><code id="alcGP_+3A_w">w</code></td>
<td>
<p> weights on the reference locations <code>Xref</code> for IECI calculations;
IECI, which stands for Integrated Expected Conditional Improvement, is not fully documented at this 
time.  See Gramacy &amp; Lee (2010) for more details.</p>
</td></tr>
<tr><td><code id="alcGP_+3A_nonug">nonug</code></td>
<td>
<p> a scalar logical indicating if a (nonzero) nugget should be used in the predictive
equations behind IECI calculations; this allows the user to toggle improvement via predictive
mean uncertainty versus full predictive uncertainty. The latter (default <code>nonug = FALSE</code>) 
is the standard approach, but the former may work better (citation forthcoming)</p>
</td></tr>
<tr><td><code id="alcGP_+3A_verb">verb</code></td>
<td>

<p>a non-negative integer specifying the verbosity level; <code>verb = 0</code>
is quiet, and larger values cause more progress information to be
printed to the screen
</p>
</td></tr>
<tr><td><code id="alcGP_+3A_start">start</code></td>
<td>
<p> initial values to the derivative-based search via <code>"L-BFGS-B"</code> 
within <code>alcoptGP</code> and <code>alcoptGPsep</code>; a nearest neighbor often
represents a sensible initialization </p>
</td></tr>
<tr><td><code id="alcGP_+3A_lower">lower</code>, <code id="alcGP_+3A_upper">upper</code></td>
<td>
<p> bounds on the derivative-based search via <code>"L-BFGS-B"</code>
within <code>alcoptGP</code> and <code>alcoptGPsep</code></p>
</td></tr>
<tr><td><code id="alcGP_+3A_maxit">maxit</code></td>
<td>
<p> the maximum number of iterations (default <code>maxit=100</code>) in <code>"L-BFGS-B"</code>
search within <code>alcoptGP</code> and <code>alcoptGPsep</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The best way to see how these functions are used in the context of local
approximation is to inspect the code in the <code><a href="#topic+laGP.R">laGP.R</a></code> function.
</p>
<p>Otherwise they are pretty self-explanatory.  They evaluate the ALC, MSPE,
and EFI quantities outlined in Gramacy &amp; Apley (2015).  ALC is originally
due to Seo, et al. (2000).  The ray-based search is described by Gramacy &amp; Haaland (2015).
</p>
<p>MSPE and EFI calculations are not supported for separable GP models, i.e.,
there are no <code>mspeGPsep</code> or <code>fishGPsep</code> functions.
</p>
<p><code>alcrayGP</code> and <code>alcrayGPsep</code> allow only one reference location 
(<code>nrow(Xref) = 1</code>).  <code>alcoptGP</code> and <code>alcoptGPsep</code> allow multiple 
reference locations. These optimize a continuous ALC analog in its natural logarithm 
using the starting locations, bounding boxes and (stored) GP provided by <code>gpi</code> or <code>gpisep</code>, 
and finally snaps the solution back to the candidate grid.  For details, see
Sun, et al. (2017).
</p>
<p>Note that <code>ieciGP</code> and <code>ieciGPsep</code>, which are for optimization via 
integrated expected conditional improvement (Gramacy &amp; Lee, 2011) are 
&ldquo;alpha&rdquo; functionality and are not fully documented at this time.
</p>


<h3>Value</h3>

<p>Except for <code>alcoptGP</code>, <code>alcoptGPsep</code>, <code>dalcGP</code>, and <code>dalcGPsep</code>, a vector of length <code>nrow(Xcand)</code> is returned 
filled with values corresponding to the desired statistic
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the best set of parameters/input configuration found on optimization</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>the optimized objective value corresponding to output <code>par</code></p>
</td></tr>
<tr><td><code>its</code></td>
<td>
<p>a two-element integer vector giving the number of calls to the object function and the gradient respectively.</p>
</td></tr>
<tr><td><code>msg</code></td>
<td>
<p>a character string giving any additional information returned by the optimizer, or <code>NULL</code></p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>An integer code. <code>0</code> indicates successful completion. For the other error codes,
see the documentation for <code><a href="stats.html#topic+optim">optim</a></code></p>
</td></tr>
<tr><td><code>alcs</code></td>
<td>
<p>reduced predictive variance averaged over the reference locations</p>
</td></tr>
<tr><td><code>dalcs</code></td>
<td>
<p>the derivative of <code>alcs</code> with respect to the new location</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a> and Furong Sun <a href="mailto:furongs@vt.edu">furongs@vt.edu</a>
</p>


<h3>References</h3>

 
<p>Gramacy, R. B. (2020) <em>Surrogates: Gaussian Process Modeling,
Design and Optimization for the Applied Sciences</em>. Boca Raton,
Florida: Chapman Hall/CRC.  (See Chapter 9.)
<a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p>F. Sun, R.B. Gramacy, B. Haaland, E. Lawrence, and A. Walker (2019).
<em>Emulating satellite drag from large simulation experiments</em>,
SIAM/ASA Journal on Uncertainty Quantification, 7(2), pp. 720-759;
preprint on arXiv:1712.00182;
<a href="https://arxiv.org/abs/1712.00182">https://arxiv.org/abs/1712.00182</a>
</p>
<p>R.B. Gramacy (2016). <em><span class="pkg">laGP</span>: Large-Scale Spatial Modeling via 
Local Approximate Gaussian Processes in <span class="rlang"><b>R</b></span></em>, Journal of Statistical 
Software, 72(1), 1-46; <a href="https://doi.org/10.18637/jss.v072.i01">doi:10.18637/jss.v072.i01</a> 
or see <code>vignette("laGP")</code>
</p>
<p>R.B. Gramacy and B. Haaland (2016).
<em>Speeding up neighborhood search in local Gaussian process prediction</em>,
Technometrics, 58(3), pp. 294-303;
preprint on arXiv:1409.0074; 
<a href="https://arxiv.org/abs/1409.0074">https://arxiv.org/abs/1409.0074</a>
</p>
<p>R.B. Gramacy and D.W. Apley (2015).
<em>Local Gaussian process approximation for large computer
experiments</em>, Journal of Computational and Graphical Statistics, 
24(2), pp. 561-678; 
preprint on arXiv:1303.0383;
<a href="https://arxiv.org/abs/1303.0383">https://arxiv.org/abs/1303.0383</a>
</p>
<p>R.B. Gramacy, J. Niemi, R.M. Weiss (2014).
<em>Massively parallel approximate Gaussian process regression</em>,
SIAM/ASA Journal on Uncertainty Quantification, 2(1), pp. 568-584;
preprint on arXiv:1310.5182;
<a href="https://arxiv.org/abs/1310.5182">https://arxiv.org/abs/1310.5182</a>
</p>
<p>R.B. Gramacy, H.K.H. Lee (2011). 
<em>Optimization under unknown constraints</em>, Valencia discussion paper, in Bayesian Statistics 9. 
Oxford University Press; 
preprint on arXiv:1004.4027; 
<a href="https://arxiv.org/abs/1004.4027">https://arxiv.org/abs/1004.4027</a>
</p>
<p>S. Seo, M., Wallat, T. Graepel, K. Obermayer (2000).
<em>Gaussian Process Regression: Active Data Selection and Test Point Rejection</em>,
In Proceedings of the International Joint Conference on Neural Networks, 
vol. III, 241-246. IEEE
</p>


<h3>See Also</h3>

<p><code><a href="#topic+laGP">laGP</a></code>, <code><a href="#topic+aGP">aGP</a></code>, <code><a href="#topic+predGP">predGP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## this follows the example in predGP, but only evaluates 
## information statistics documented here

## Simple 2-d test function used in Gramacy &amp; Apley (2015);
## thanks to Lee, Gramacy, Taddy, and others who have used it before
f2d &lt;- function(x, y=NULL)
  {
    if(is.null(y)) {
      if(!is.matrix(x) &amp;&amp; !is.data.frame(x)) x &lt;- matrix(x, ncol=2)
      y &lt;- x[,2]; x &lt;- x[,1]
    }
    g &lt;- function(z)
      return(exp(-(z-1)^2) + exp(-0.8*(z+1)^2) - 0.05*sin(8*(z+0.1)))
    z &lt;- -g(x)*g(y)
  }

## design with N=441
x &lt;- seq(-2, 2, length=11)
X &lt;- expand.grid(x, x)
Z &lt;- f2d(X)

## fit a GP
gpi &lt;- newGP(X, Z, d=0.35, g=1/1000, dK=TRUE)

## predictive grid with NN=400
xx &lt;- seq(-1.9, 1.9, length=20)
XX &lt;- expand.grid(xx, xx)

## predict
alc &lt;- alcGP(gpi, XX)
mspe &lt;- mspeGP(gpi, XX)
fish &lt;- fishGP(gpi, XX)

## visualize the result
par(mfrow=c(1,3))
image(xx, xx, matrix(sqrt(alc), nrow=length(xx)), col=heat.colors(128),
      xlab="x1", ylab="x2", main="sqrt ALC")
image(xx, xx, matrix(sqrt(mspe), nrow=length(xx)), col=heat.colors(128),
      xlab="x1", ylab="x2", main="sqrt MSPE")
image(xx, xx, matrix(log(fish), nrow=length(xx)), col=heat.colors(128),
      xlab="x1", ylab="x2", main="log fish")

## clean up
deleteGP(gpi)


## 
## Illustrating some of the other functions in a sequential design context, 
## using X and XX above
## 

## new, much bigger design
x &lt;- seq(-2, 2, by=0.02)
X &lt;- expand.grid(x, x)
Z &lt;- f2d(X)

## first build a local design of size 25, see laGP documentation
out &lt;- laGP.R(XX, start=6, end=25, X, Z, method="alc", close=10000)

## extract that design and fit GP
XC &lt;- X[out$Xi,] ## inputs
ZC &lt;- Z[out$Xi]  ## outputs
gpi &lt;- newGP(XC, ZC, d=out$mle$d, g=out$g$start)

## calculate the ideal "next" location via continuous ALC optimization
alco &lt;- alcoptGP(gpi=gpi, Xref=XX, start=c(0,0), lower=range(x)[1], upper=range(x)[2])

## alco$par is the "new" location; calculate distances between candidates (remaining
## unchosen X locations) and this solution
Xcan &lt;- X[-out$Xi,]
D &lt;- distance(Xcan, matrix(alco$par, ncol=ncol(Xcan))) 

## snap the new location back to the candidate set
lab &lt;- which.min(D) 
xnew &lt;- Xcan[lab,] 
## add xnew to the local design, remove it from Xcan, and repeat

## evaluate the derivative at this new location
dalc &lt;- dalcGP(gpi=gpi, Xcand=matrix(xnew, nrow=1), Xref=XX)

## clean up
deleteGP(gpi)
</code></pre>

<hr>
<h2 id='blhs'>
Bootstrapped block Latin hypercube subsampling
</h2><span id='topic+blhs'></span><span id='topic+blhs.loop'></span>

<h3>Description</h3>

<p>Provides bootstrapped block Latin hypercube subsampling under a given 
data set to aid in consistent estimation of a global separable lengthscale 
parameter
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  blhs(y, X, m)
  blhs.loop(y, X, m, K, da, g = 1e-3, maxit = 100, verb = 0, plot.it = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="blhs_+3A_y">y</code></td>
<td>
<p> a vector of responses/dependent values with <code>length(y) = nrow(X)</code></p>
</td></tr>
<tr><td><code id="blhs_+3A_x">X</code></td>
<td>
<p> a <code>matrix</code> or <code>data.frame</code> containing the full (large) design matrix of input locations</p>
</td></tr>
<tr><td><code id="blhs_+3A_m">m</code></td>
<td>
<p> a positive scalar integer giving the number of divisions on each coordinate of input space defining the block structure </p>
</td></tr>
<tr><td><code id="blhs_+3A_k">K</code></td>
<td>
<p> a positive scalar integer specifying the number of Bootstrap replicates desired </p>
</td></tr>
<tr><td><code id="blhs_+3A_da">da</code></td>
<td>
<p> a lengthscale prior, say as generated by <code><a href="#topic+darg">darg</a></code> </p>
</td></tr>
<tr><td><code id="blhs_+3A_g">g</code></td>
<td>
<p> a positive scalar giving the fixed nugget value of the nugget parameter; by default <code>g = 1e-3</code></p>
</td></tr>
<tr><td><code id="blhs_+3A_maxit">maxit</code></td>
<td>
<p> a positive scalar integer giving the maximum number of iterations for MLE calculations via <code>"L-BFGS-B"</code>; 
see <code><a href="#topic+mleGPsep">mleGPsep</a></code> for more details</p>
</td></tr>
<tr><td><code id="blhs_+3A_verb">verb</code></td>
<td>
<p> a non-negative integer specifying the verbosity level; <code>verb = 0</code> (by default) is quiet, 
and larger values cause more progress information to be printed to the screen</p>
</td></tr>
<tr><td><code id="blhs_+3A_plot.it">plot.it</code></td>
<td>
 <p><code>plot.it = FALSE</code> by default; if <code>plot.it = TRUE</code>, then each of the <code>K</code> 
lengthscale estimates from bootstrap iterations will be shown 
via <code><a href="graphics.html#topic+boxplot">boxplot</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Bootstrapped block Latin hypercube subsampling (BLHS) yields a global lengthscale estimator 
which is asymptotically consistent with the MLE calculated on the full data set. However, since it
works on data subsets, it comes at a much reduced computational cost.  Intuitively, the BLHS 
guarantees a good mix of short and long pairwise distances. A single bootstrap LH subsample 
may be obtained by dividing each dimension of the input space equally into <code>m</code> 
intervals, yielding <code class="reqn">m^d</code> mutually exclusive hypercubes.  It is easy to show 
that the average number of observations in each hypercube is <code class="reqn">Nm^{-d}</code> 
if there are <code class="reqn">N</code> samples in the original design. From each of these hypercubes, 
<code>m</code> <code>blocks</code> are randomly selected following the LH paradigm, i.e., so that 
only one interval is chosen from each of the <code>m</code> segments. The average number of 
observations in the subsample, combining the <code>m</code> randomly selected blocks, 
is <code class="reqn">Nm^{-d+1}</code>. 
</p>
<p>Ensuring a subsample size of at least <code>one</code> requires having <code class="reqn">m\leq N^{\frac{1}{d-1}}</code>, 
thereby linking the parameter <code>m</code> to computational effort.  Smaller <code>m</code> is preferred so long 
as GP inference on data of that size remains tractable.  Since the blocks follow 
an LH structure, the resulting sub-design inherits the usual LHS properties, 
e.g., retaining marginal properties like univariate stratification modulo features present in the original, 
large <code>N</code>, design. 
</p>
<p>For more details, see Liu (2014), Zhao, et al. (2017) and Sun, et al. (2019).
</p>
<p><code>blhs</code> returns the subsampled input space and the corresponding responses. 
</p>
<p><code>blhs.loop</code> returns the median of the <code>K</code> lengthscale maximum likelihood estimates, the subsampled data size to which
that corresponds, and the subsampled data, including the input space and the responses, from the bootstrap iterations
</p>


<h3>Value</h3>

<p><code>blhs</code> returns
</p>
<table>
<tr><td><code>xs</code></td>
<td>
<p>the subsampled input space</p>
</td></tr>
<tr><td><code>ys</code></td>
<td>
<p>the subsampled responses, <code>length(ys) = nrow(xs)</code></p>
</td></tr>
</table>
<p><code>blhs.loop</code> returns
</p>
<table>
<tr><td><code>that</code></td>
<td>
<p>the lengthscale estimate (median), <code>length(that) = ncol(X)</code></p>
</td></tr>
<tr><td><code>ly</code></td>
<td>
<p>the subsampled data size (median)</p>
</td></tr>
<tr><td><code>xm</code></td>
<td>
<p>the subsampled input space (median)</p>
</td></tr>
<tr><td><code>ym</code></td>
<td>
<p>the subsampled responses (median)</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This implementation assums that <code>X</code> has been coded to the unit cube (<code class="reqn">[0,1]^p</code>),
where <code>p = ncol(X)</code>.
</p>
<p><code>X</code> should be relatively homogeneous. A space-filling design (input) <code>X</code>
is ideal, but not required
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a> and Furong Sun <a href="mailto:furongs@vt.edu">furongs@vt.edu</a>
</p>


<h3>References</h3>

<p>Gramacy, R. B. (2020) <em>Surrogates: Gaussian Process Modeling,
Design and Optimization for the Applied Sciences</em>. Boca Raton,
Florida: Chapman Hall/CRC.  (See Chapter 9.)
<a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p>F. Sun, R.B. Gramacy, B. Haaland, E. Lawrence, and A. Walker (2019).
<em>Emulating satellite drag from large simulation experiments</em>,
SIAM/ASA Journal on Uncertainty Quantification, 7(2), pp. 720-759;
preprint on arXiv:1712.00182;
<a href="https://arxiv.org/abs/1712.00182">https://arxiv.org/abs/1712.00182</a>
</p>
<p>Y. Zhao, Y. Hung, and Y. Amemiya (2017).
<em>Efficient Gaussian Process Modeling using Experimental Design-Based Subagging</em>,
Statistica Sinica, to appear;
</p>
<p>Yufan Liu (2014)
<em>Recent Advances in Computer Experiment Modeling</em>.
Ph.D. Thesis at Rutgers, The State University of New Jersey.
<a href="https://dx.doi.org/doi:10.7282/T38G8J1H">https://dx.doi.org/doi:10.7282/T38G8J1H</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  # input space based on latin-hypercube sampling (not required)
  # two dimensional example with N=216 sized sample
  if(require(lhs)) { X &lt;- randomLHS(216, 2)  
  } else { X &lt;- matrix(runif(216*2), ncol=2) }
  # pseudo responses, not important for visualizing design
  Y &lt;- runif(216) 
  
  ## BLHS sample with m=6 divisions in each coordinate
  sub &lt;- blhs(y=Y, X=X, m=6)
  Xsub &lt;- sub$xs # the bootstrapped subsample
  
  # visualization
  plot(X, xaxt="n", yaxt="n", xlim=c(0,1), ylim=c(0,1), xlab="factor 1", 
    ylab="factor 2", col="cyan", main="BLHS")
  b &lt;- seq(0, 1, by=1/6)
  abline(h=b, v=b, col="black", lty=2)
  axis(1, at=seq (0, 1, by=1/6), cex.axis=0.8, 
    labels=expression(0, 1/6, 2/6, 3/6, 4/6, 5/6, 1))
  axis(2, at=seq (0, 1, by=1/6), cex.axis=0.8, 
    labels=expression(0, 1/6, 2/6, 3/6, 4/6, 5/6, 1), las=1)
  points(Xsub, col="red", pch=19, cex=1.25)
  
  ## Comparing global lengthscale MLE based on BLHS and random subsampling
  ## Not run: 
    # famous borehole function
    borehole &lt;- function(x){
      rw &lt;- x[1] * (0.15 - 0.05) + 0.05
      r &lt;-  x[2] * (50000 - 100) + 100
      Tu &lt;- x[3] * (115600 - 63070) + 63070
      Tl &lt;- x[5] * (116 - 63.1) + 63.1
      Hu &lt;- x[4] * (1110 - 990) + 990
      Hl &lt;- x[6] * (820 - 700) + 700
      L &lt;-  x[7] * (1680 - 1120) + 1120
      Kw &lt;- x[8] * (12045 - 9855) + 9855
      m1 &lt;- 2 * pi * Tu * (Hu - Hl)
      m2 &lt;- log(r / rw)
      m3 &lt;- 1 + 2*L*Tu/(m2*rw^2*Kw) + Tu/Tl
      return(m1/m2/m3)
    }
    
    N &lt;- 100000                   # number of observations
    if(require(lhs)) { xt &lt;- randomLHS(N, 8)   # input space
    } else { xt &lt;- matrix(runif(N*8), ncol=8) }
    yt &lt;- apply(xt, 1, borehole)  # response
    colnames(xt) &lt;- c("rw", "r", "Tu", "Tl", "Hu", "Hl", "L", "Kw")

    ## prior on the GP lengthscale parameter
    da &lt;- darg(list(mle=TRUE, max=100), xt)

    ## make space for two sets of boxplots
    par(mfrow=c(1,2))
    
    # BLHS calculating with visualization of the K MLE lengthscale estimates
    K &lt;- 10  # number of Bootstrap samples; Sun, et al (2017) uses K &lt;- 31
    sub_blhs &lt;- blhs.loop(y=yt, X=xt, K=K, m=2, da=da, maxit=200, plot.it=TRUE)
  
    # a random subsampling analog for comparison
    sn &lt;- sub_blhs$ly # extract a size that is consistent with the BLHS
    that.rand &lt;- matrix(NA, ncol=8, nrow=K)
    for(i in 1:K){
      sub &lt;- sample(1:nrow(xt), sn)
      gpsepi &lt;- newGPsep(xt[sub,], yt[sub], d=da$start, g=1e-3, dK=TRUE)
      mle &lt;- mleGPsep(gpsepi, tmin=da$min, tmax=10*da$max, ab=da$ab, maxit=200)
      deleteGPsep(gpsepi)
      that.rand[i,] &lt;- mle$d
    }

    ## put random boxplots next to BLHS ones
    boxplot(that.rand, xlab="input", ylab="theta-hat", col=2, 
      main="random subsampling")
  
## End(Not run)
</code></pre>

<hr>
<h2 id='darg'>
Generate Priors for GP correlation
</h2><span id='topic+darg'></span><span id='topic+garg'></span>

<h3>Description</h3>

<p>Generate empirical Bayes regularization (priors) and choose initial values
and ranges for (isotropic) lengthscale and nugget parameters to a Gaussian
correlation function for a GP regression model </p>


<h3>Usage</h3>

<pre><code class='language-R'>darg(d, X, samp.size = 1000)
garg(g, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="darg_+3A_d">d</code></td>
<td>

<p>can be <code>NULL</code>, or a scalar indicating an initial value
or a partial <code>list</code> whose format matches the one described
in the Value section below
</p>
</td></tr>
<tr><td><code id="darg_+3A_g">g</code></td>
<td>

<p>can be <code>NULL</code>, or a scalar indicating an initial value
or a partial <code>list</code> whose format matches the one described
in the Value section below
</p>
</td></tr>
<tr><td><code id="darg_+3A_x">X</code></td>
<td>

<p>a <code>matrix</code> or <code>data.frame</code> containing
the full (large) design matrix of input locations
</p>
</td></tr>
<tr><td><code id="darg_+3A_y">y</code></td>
<td>

<p>a vector of responses/dependent values
</p>
</td></tr>
<tr><td><code id="darg_+3A_samp.size">samp.size</code></td>
<td>

<p>a scalar integer indicating a subset size of <code>X</code> to use
for calculations; this is important for very large <code>X</code> matrices
since the calculations are quadratic in <code>nrow(X)</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions use aspects of the data, either <code>X</code> or <code>y</code>,
to form weakly informative default priors and choose initial values
for a lengthscale and nugget parameter.  This is useful since the
likelihood can sometimes be very flat, and even with proper priors
inference can be sensitive to the specification of those priors
and any initial search values.  The focus here is on avoiding pathologies
while otherwise remaining true to the spirit of MLE calculation.  
</p>
<p><code>darg</code> output specifies MLE inference (<code>out$mle = TRUE</code>)
by default, whereas <code>garg</code> instead fixes the nugget at the starting value,
which may be sensible for emulating deterministic computer simulation data; 
when <code>out$mle =  FALSE</code> the calculated range outputs <code>c(out$min, out$max)</code> 
are set to dummy values that are ignored in other parts of the <span class="pkg">laGP</span> package.
</p>
<p><code>darg</code> calculates a Gaussian distance matrix between all pairs of
<code>X</code> rows, or a subsample of rows of size <code>samp.size</code>.  From
those distances it chooses the range and start values from the range
of (non-zero) distances and the <code>0.1</code> quantile, respectively.
The Gamma prior values have a shape of <code>out$a = 3/2</code> and a rate
<code>out$b</code> chosen by the incomplete Gamma inverse function to put
<code>0.95</code> probability below <code>out$max</code>. 
</p>
<p><code>garg</code> is similar except that it works with <code>(y- mean(y))^2</code>
instead of the pairwise distances of <code>darg</code>.  The only difference
is that the starting value is chosen as the 2.5% quantile.
</p>


<h3>Value</h3>

<p>Both functions return a list containing the following entries.  If the
input object (<code>d</code> or <code>g</code>) specifies one of the values then
that value is copied to the same list entry on output.  See the
Details section for how these values are calculated
</p>
<table>
<tr><td><code>mle</code></td>
<td>
<p> by default, <code>TRUE</code> for <code>darg</code> and <code>FALSE</code>
for <code>garg</code> </p>
</td></tr>
<tr><td><code>start</code></td>
<td>
<p> starting value chosen from the quantiles of
<code>distance(X)</code> or <code>(y - mean(y))^2</code></p>
</td></tr>
<tr><td><code>min</code></td>
<td>
<p> minimum value in allowable range for the parameter - for
future inference purposes </p>
</td></tr>
<tr><td><code>max</code></td>
<td>
<p> maximum value in allowable range for the parameter - for
future inference purposes </p>
</td></tr>
<tr><td><code>ab</code></td>
<td>
<p> shape and rate parameters specifying a Gamma prior for the parameter</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>See Also</h3>

<p><code>vignette("laGP")</code>, 
<code><a href="#topic+laGP">laGP</a></code>, <code><a href="#topic+aGP">aGP</a></code>,
<code><a href="#topic+mleGP">mleGP</a></code>, <code><a href="#topic+distance">distance</a></code>, <code><a href="#topic+llikGP">llikGP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## motorcycle data
if(require("MASS")) {
  X &lt;- matrix(mcycle[,1], ncol=1)
  Z &lt;- mcycle[,2]

  ## get darg and garg
  darg(NULL, X)
  garg(list(mle=TRUE), Z)
}
</code></pre>

<hr>
<h2 id='deleteGP'>
Delete C-side Gaussian Process Objects
</h2><span id='topic+deleteGP'></span><span id='topic+deleteGPs'></span><span id='topic+deleteGPsep'></span><span id='topic+deleteGPseps'></span>

<h3>Description</h3>

<p>Frees memory allocated by a particular C-side Gaussian process
object, or all GP objects currently allocated
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deleteGP(gpi)
deleteGPsep(gpsepi)
deleteGPs()
deleteGPseps()
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deleteGP_+3A_gpi">gpi</code></td>
<td>

<p>a scalar positive integer specifying an allocated isotropic GP object
</p>
</td></tr>
<tr><td><code id="deleteGP_+3A_gpsepi">gpsepi</code></td>
<td>
<p> similar to <code>gpi</code> but indicating a separable GP object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Any function calling <code><a href="#topic+newGP">newGP</a></code> or <code><a href="#topic+newGPsep">newGPsep</a></code>
will require destruction
via these functions or there will be a memory leak
</p>


<h3>Value</h3>

<p>Nothing is returned
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>See Also</h3>

<p><code>vignette("laGP")</code>, 
<code><a href="#topic+newGP">newGP</a></code>, <code><a href="#topic+predGP">predGP</a></code>, <code><a href="#topic+mleGP">mleGP</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## see examples for newGP, predGP, or mleGP
</code></pre>

<hr>
<h2 id='discrep.est'>
Estimate Discrepancy in Calibration Model
</h2><span id='topic+discrep.est'></span>

<h3>Description</h3>

<p>Estimates the Gaussian process discrepancy/bias and/or noise term in
a modularized calibration of a computer model (emulator) to field data,
and returns the log likelihood or posterior probability
</p>


<h3>Usage</h3>

<pre><code class='language-R'>discrep.est(X, Y, Yhat, d, g, bias = TRUE, clean = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="discrep.est_+3A_x">X</code></td>
<td>

<p>a <code>matrix</code> or <code>data.frame</code> containing
a design matrix of input locations for field data sites.  Any columns
of <code>X</code> without at least three unique input settings are dropped
in a pre-processing step
</p>
</td></tr>
<tr><td><code id="discrep.est_+3A_y">Y</code></td>
<td>

<p>a vector of values with <code>length(Y) = ncol(X)</code>
containing the response from field data observations
at <code>X</code>.  A <code>Y</code>-vector with <code>length(Y) = k*ncol(X)</code>,
for positive integer <code>k</code>, can be supplied in which case
the multiple code <code>Y</code>-values will be treated as replicates
at the <code>X</code>-values
</p>
</td></tr>
<tr><td><code id="discrep.est_+3A_yhat">Yhat</code></td>
<td>

<p>a vector with <code>length(Yhat) = length(Y)</code> containing
predictions at <code>X</code> from an emulator of a computer simulation
</p>
</td></tr>
<tr><td><code id="discrep.est_+3A_d">d</code></td>
<td>

<p>a prior or initial setting for the (single/isotropic) lengthscale
parameter in a Gaussian correlation function; a (default)
<code>NULL</code> value triggers a sensible regularization (prior) and
initial setting to be generated via <code><a href="#topic+darg">darg</a></code>;
a scalar specifies an initial value, causing <code><a href="#topic+darg">darg</a></code>
to only generate the prior; otherwise,
a list or partial list matching the output
of <code><a href="#topic+darg">darg</a></code> can be used to specify a custom prior.  In the
case of a partial list, the only the missing entries will be
generated. Note that a default/generated list specifies MLE/MAP
inference for this parameter.  When specifying initial values, a
vector of length <code>nrow(XX)</code> can be provided, giving a
different initial value for each predictive location.
</p>
</td></tr>
<tr><td><code id="discrep.est_+3A_g">g</code></td>
<td>

<p>a prior or initial setting for the nugget parameter; a 
<code>NULL</code> value causes a sensible regularization (prior) and
initial setting to be generated via <code><a href="#topic+garg">garg</a></code>; a scalar
(default <code>g = 1/1000</code>) specifies an initial value, causing <code><a href="#topic+garg">garg</a></code>
to only generate the prior; otherwise, a
list or partial list matching the output of <code><a href="#topic+garg">garg</a></code> can be used to
specify a custom prior.  In the case of a partial list, only the
missing entries will be generated. Note that a default/generated list
specifies <em>no</em> inference for this parameter; i.e., it is fixed
at its starting value, which may be appropriate for emulating 
deterministic computer code output
</p>
</td></tr>
<tr><td><code id="discrep.est_+3A_bias">bias</code></td>
<td>

<p>a scalar logical indicating if a (isotropic) 
GP discrepancy should be estimated (<code>TRUE</code>)
or a Gaussian noise term only (<code>FALSE</code>)
</p>
</td></tr>
<tr><td><code id="discrep.est_+3A_clean">clean</code></td>
<td>

<p>a scalar logical indicating if the C-side GP object should be freed before
returning.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimates an isotropic Gaussian correlation Gaussian process (GP) discrepancy
term for the difference between a computer model output (<code>Yhat</code>) and
field data observations (<code>Y</code>) at locations <code>X</code>. The computer model
predictions would typically come from a GP emulation from simulation data,
possibly via <code><a href="#topic+aGP">aGP</a></code> if the computer experiment is large.
</p>
<p>This function is used primarily as a subroutine by <code><a href="#topic+fcalib">fcalib</a></code> which
defines an objective function for optimization in order to solve the
calibration problem via the method described by Gramacy, et al. (2015),
designed for large computer experiments.  However, once calibration is
performed this function can be useful for making comparisons to other methods.
Examples are provided in the <code><a href="#topic+fcalib">fcalib</a></code> documentation.
</p>
<p>When <code>bias=FALSE</code> no discrepancy is estimated; only a zero-mean 
Gaussian error distribution is assumed
</p>


<h3>Value</h3>

<p>The output object is comprised of the output of <code>jmleGP</code>, applied to a GP
object built with responses <code>Y - Yhat</code>.  That object is augmented with
a log likelihood, in <code>$ll</code>, and with a GP index <code>$gpi</code> when
<code>clean=FALSE</code>.  When <code>bias = FALSE</code> the output object retains the
same form as above, except with dummy zero-values since calling <code>jmleGP</code> is not
required
</p>


<h3>Note</h3>

<p>Note that in principle a separable correlation function could be used 
(e.g, via <code><a href="#topic+newGPsep">newGPsep</a></code> and <code><a href="#topic+mleGPsep">mleGPsep</a></code>), however this is not implemented at this time
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Gramacy, R. B. (2020) <em>Surrogates: Gaussian Process Modeling,
Design and Optimization for the Applied Sciences</em>. Boca Raton,
Florida: Chapman Hall/CRC.  (See Chapter 8.)
<a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p>R.B. Gramacy (2016). <em><span class="pkg">laGP</span>: Large-Scale Spatial Modeling via 
Local Approximate Gaussian Processes in <span class="rlang"><b>R</b></span>.</em>, Journal of Statistical 
Software, 72(1), 1-46; <a href="https://doi.org/10.18637/jss.v072.i01">doi:10.18637/jss.v072.i01</a> 
or see <code>vignette("laGP")</code>
</p>
<p>R.B. Gramacy, D. Bingham, JP. Holloway, M.J. Grosskopf, C.C. Kuranz, E. Rutter, 
M. Trantham, P.R. Drake (2015). <em>Calibrating a large computer 
experiment simulating radiative shock hydrodynamics.</em>  
Annals of Applied Statistics, 9(3) 1141-1168; preprint on arXiv:1410.3293 
<a href="https://arxiv.org/abs/1410.3293">https://arxiv.org/abs/1410.3293</a>
</p>
<p>F. Liu, M. Bayarri and J. Berger (2009). 
<em>Modularization in Bayesian analysis, with emphasis on analysis of computer 
models.</em> Bayesian Analysis, 4(1) 119-150.
</p>


<h3>See Also</h3>

<p><code>vignette("laGP")</code>,
<code><a href="#topic+jmleGP">jmleGP</a></code>, <code><a href="#topic+newGP">newGP</a></code>, <code><a href="#topic+aGP.seq">aGP.seq</a></code>, <code><a href="#topic+fcalib">fcalib</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## the example here combines aGP.seq and discrep.est functions; 
## it is comprised of snippets from demo("calib"), which contains
## code from the Calibration Section of vignette("laGP")

## Here we generate calibration data using a true calibration
## parameter, u, and then evaluate log posterior probabilities
## and out-of-sample RMSEs for that u value;  the fcalib 
## documentation repeats this with a single call to fcalib rather 
## than first aGP.seq and then discrep.est

## begin data-generation code identical to aGP.seq, discrep.est, fcalib
## example sections and demo("calib")

## M: computer model test functon used in Goh et al, 2013 (Technometrics)
## an elaboration of one from Bastos and O'Hagan, 2009 (Technometrics) 
M &lt;- function(x,u) 
  {
    x &lt;- as.matrix(x)
    u &lt;- as.matrix(u)
    out &lt;- (1-exp(-1/(2*x[,2]))) 
    out &lt;- out * (1000*u[,1]*x[,1]^3+1900*x[,1]^2+2092*x[,1]+60) 
    out &lt;- out / (100*u[,2]*x[,1]^3+500*x[,1]^2+4*x[,1]+20)  
    return(out)
  }
  
## bias: discrepancy function from Goh et al, 2013 
bias &lt;- function(x) 
  {
    x&lt;-as.matrix(x)   
    out&lt;- 2*(10*x[,1]^2+4*x[,2]^2) / (50*x[,1]*x[,2]+10)
    return(out)
  }

## beta.prior: marginal beta prior for u, 
## defaults to a mode at 1/2 in hypercube
beta.prior &lt;- function(u, a=2, b=2, log=TRUE)
{
  if(length(a) == 1) a &lt;- rep(a, length(u))
  else if(length(a) != length(u)) stop("length(a) must be 1 or length(u)")
  if(length(b) == 1) b &lt;- rep(b, length(u))
  else if(length(b) != length(u)) stop("length(b) must be 1 or length(u)")
  if(log) return(sum(dbeta(u, a, b, log=TRUE)))
  else return(prod(dbeta(u, a, b, log=FALSE)))
}

## tgp for LHS sampling
library(tgp)
rect &lt;- matrix(rep(0:1, 4), ncol=2, byrow=2)

## training and testing inputs
ny &lt;- 50; nny &lt;- 1000  
X &lt;- lhs(ny, rect[1:2,])    ## computer model train
XX &lt;- lhs(nny, rect[1:2,],) ## test

## true (but unknown) setting of the calibration parameter
## for the computer model
u &lt;- c(0.2, 0.1)
Zu &lt;- M(X, matrix(u, nrow=1)) 
ZZu &lt;- M(XX, matrix(u, nrow=1)) 

## field data response, biased and replicated
sd &lt;- 0.5
## Y &lt;- computer output + bias + noise
reps &lt;- 2 ## example from paper uses reps &lt;- 10
Y &lt;- rep(Zu,reps) + rep(bias(X),reps) + rnorm(reps*length(Zu), sd=sd) 
YYtrue &lt;- ZZu + bias(XX) 
## variations: remove the bias or change 2 to 1 to remove replicates

## computer model design
nz &lt;- 10000
XT &lt;- lhs(nz, rect)
nth &lt;- 1 ## number of threads to use in emulation, demo uses 8

## augment with physical model design points 
## with various u settings
XT2 &lt;- matrix(NA, nrow=10*ny, ncol=4)
for(i in 1:10) {
  I &lt;- ((i-1)*ny+1):(ny*i)
  XT2[I,1:2] &lt;- X
}
XT2[,3:4] &lt;- lhs(10*ny, rect[3:4,])
XT &lt;- rbind(XT, XT2)

## evaluate the computer model
Z &lt;- M(XT[,1:2], XT[,3:4])

## flag indicating if estimating bias/discrepancy or not
bias.est &lt;- TRUE
## two passes of ALC with MLE calculations for aGP.seq
methods &lt;- rep("alcray", 2) ## demo uses rep("alc", 2)

## set up priors
da &lt;- d &lt;- darg(NULL, XT)
g &lt;- garg(list(mle=TRUE), Y) 

## end identical data generation code

## now calculate log posterior and do out-of-sample RMSE calculation
## for true calibration parameter value (u).  You could repeat
## this for an estimate value from demo("calib"), for example
## u.hat &lt;- c(0.8236673, 0.1406989)

## first log posterior

## emulate at field-data locations Xu
Xu &lt;- cbind(X, matrix(rep(u, ny), ncol=2, byrow=TRUE))
ehat.u &lt;- aGP.seq(XT, Z, Xu, da, methods, ncalib=2, omp.threads=nth, verb=0)

## estimate discrepancy from the residual
cmle.u &lt;- discrep.est(X, Y, ehat.u$mean, d, g, bias.est, FALSE)
cmle.u$ll &lt;- cmle.u$ll + beta.prior(u)
print(cmle.u$ll)
## compare to same calculation with u.hat above

## now RMSE
## Not run: 
## predictive design with true calibration parameter
XXu &lt;- cbind(XX, matrix(rep(u, nny), ncol=2, byrow=TRUE))

## emulate at predictive design
ehat.oos.u &lt;- aGP.seq(XT, Z, XXu, da, methods, ncalib=2, 
  omp.threads=nth, verb=0)

## predict via discrepency
YYm.pred.u &lt;- predGP(cmle.u$gp, XX)

## add in emulation
YY.pred.u &lt;- YYm.pred.u$mean + ehat.oos.u$mean

## calculate RMSE
rmse.u &lt;- sqrt(mean((YY.pred.u - YYtrue)^2))
print(rmse.u)
## compare to same calculation with u.hat above

## clean up
deleteGP(cmle.u$gp)

## End(Not run)
</code></pre>

<hr>
<h2 id='distance'>
Calculate the squared Euclidean distance between pairs of points
</h2><span id='topic+distance'></span>

<h3>Description</h3>

<p>Calculate the squared Euclidean distance between pairs of points
and return a distance matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distance(X1, X2 = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distance_+3A_x1">X1</code></td>
<td>

<p>a <code>matrix</code> or <code>data.frame</code> containing real-valued
numbers
</p>
</td></tr>
<tr><td><code id="distance_+3A_x2">X2</code></td>
<td>

<p>an optional <code>matrix</code> or <code>data.frame</code> containing real-valued
numbers; must have <code>ncol(X2) = ncol(X1)</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>X2 = NULL</code> distances between <code>X1</code> and itself are
calculated, resulting in an <code>nrow(X1)</code>-by-<code>nrow(X1)</code> distance 
matrix.  Otherwise the result is <code>nrow(X1)</code>-by-<code>nrow(X2)</code> and
contains distances between <code>X1</code> and <code>X2</code>.
</p>
<p>Calling <code>distance(X)</code> is the same as <code>distance(X,X)</code>
</p>


<h3>Value</h3>

<p>The output is a <code>matrix</code>, whose dimensions are described in the Details
section above
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+darg">darg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- seq(-2, 2, length=11)
X &lt;- as.matrix(expand.grid(x, x))
## predictive grid with NN=400
xx &lt;- seq(-1.9, 1.9, length=20)
XX &lt;- as.matrix(expand.grid(xx, xx))

D &lt;- distance(X)
DD &lt;- distance(X, XX)
</code></pre>

<hr>
<h2 id='fcalib'>
Objective function for performing large scale computer model
calibration via optimization
</h2><span id='topic+fcalib'></span>

<h3>Description</h3>

<p>Defines an objective function for performing blackbox optimization towards
solving a modularized calibration of large computer model simulation to field
data </p>


<h3>Usage</h3>

<pre><code class='language-R'>fcalib(u, XU, Z, X, Y, da, d, g, uprior = NULL, methods = rep("alc", 2), 
  M = NULL, bias = TRUE, omp.threads = 1, save.global = FALSE, verb = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fcalib_+3A_u">u</code></td>
<td>

<p>a vector of length <code>ncol(XU) - ncol(X)</code> containing a setting
of the calibration parameter
</p>
</td></tr>
<tr><td><code id="fcalib_+3A_xu">XU</code></td>
<td>

<p>a <code>matrix</code> or <code>data.frame</code> containing
the full (large) design matrix of input locations to a computer
simulator whose final <code>ncol(XU) - ncol(X)</code> columns
contain settings of a calibration or tuning parameter like
<code>u</code>
</p>
</td></tr>
<tr><td><code id="fcalib_+3A_z">Z</code></td>
<td>

<p>a vector of responses/dependent values with <code>length(Z) = ncol(XU)</code>
of computer model outputs at <code>XU</code>
</p>
</td></tr>
<tr><td><code id="fcalib_+3A_x">X</code></td>
<td>

<p>a <code>matrix</code> or <code>data.frame</code> containing
the full (large) design matrix of input locations
</p>
</td></tr>
<tr><td><code id="fcalib_+3A_y">Y</code></td>
<td>

<p>a vector of values with <code>length(Y) = ncol(X)</code>
containing the response from field data observations
at <code>X</code>.  A <code>Y</code>-vector with <code>length(Y) = k*ncol(X)</code>,
for positive integer <code>k</code>, can be supplied in which case
the multiple <code>Y</code>-values will be treated as replicates
at the <code>X</code>-values
</p>
</td></tr>
<tr><td><code id="fcalib_+3A_da">da</code></td>
<td>

<p>for emulating <code>Z</code> at <code>XU</code>: 
a prior or initial setting for the (single/isotropic) lengthscale
parameter in a Gaussian correlation function; a (default)
<code>NULL</code> value triggers a sensible regularization (prior) and
initial setting to be generated via <code><a href="#topic+darg">darg</a></code>;
a scalar specifies an initial value, causing <code><a href="#topic+darg">darg</a></code>
to only generate the prior; otherwise,
a list or partial list matching the output
of <code><a href="#topic+darg">darg</a></code> can be used to specify a custom prior.  In the
case of a partial list, the only the missing entries will be
generated. Note that a default/generated list specifies MLE/MAP
inference for this parameter.  When specifying initial values, a
vector of length <code>nrow(XX)</code> can be provided, giving a
different initial value for each predictive location
</p>
</td></tr>
<tr><td><code id="fcalib_+3A_d">d</code></td>
<td>

<p>for the discrepancy between emulations <code>Yhat</code> at <code>X</code>, based
on <code>Z</code> at <code>XU</code>, and the oputs <code>Y</code> observed at <code>X</code>.
Otherwise, same description as <code>da</code> above 
</p>
</td></tr>
<tr><td><code id="fcalib_+3A_g">g</code></td>
<td>

<p>for the nugget in the GP model for the discrepancy between emulation
<code>Yhat</code> at <code>X</code>, based
on <code>Z</code> at <code>XU</code>, and the outputs <code>Y</code> observed at <code>X</code>:
a prior or initial setting for the nugget parameter; a 
<code>NULL</code> value causes a sensible regularization (prior) and
initial setting to be generated via <code><a href="#topic+garg">garg</a></code>; a scalar (default
<code>g = 1/1000</code>) specifies an initial value, causing <code><a href="#topic+garg">garg</a></code>
to only generate the prior; otherwise, a list or partial list matching the
output of <code><a href="#topic+garg">garg</a></code> can be used to specify a custom prior.  In
the case of a partial list, only the missing entries will be
generated. Note that a default/generated list specifies <em>no</em>
inference for this parameter; i.e., it is fixed at its starting value,
which may be appropriate for emulating deterministic computer code output.
At this time, estimating a nugget for the computer model emulator is not
supported by <code>fcalib</code> </p>
</td></tr>
<tr><td><code id="fcalib_+3A_uprior">uprior</code></td>
<td>

<p>an optional function taking <code>u</code> arguments which returns a log
prior density value for the calibration parameter.
</p>
</td></tr>
<tr><td><code id="fcalib_+3A_methods">methods</code></td>
<td>

<p>a sequence of local search methods to be deployed when emulating 
<code>Z</code> at <code>XU</code> via <code><a href="#topic+aGP">aGP</a></code>;  see <code><a href="#topic+aGP.seq">aGP.seq</a></code> for more
details; provide <code>methods = FALSE</code> to use the computer model <code>M</code>
directly
</p>
</td></tr>
<tr><td><code id="fcalib_+3A_m">M</code></td>
<td>

<p>a computer model &ldquo;simulation&rdquo; function taking two matrices as
inputs, to be used in lieu of emulation; see <code><a href="#topic+aGP.seq">aGP.seq</a></code> for mode details
</p>
</td></tr>
<tr><td><code id="fcalib_+3A_bias">bias</code></td>
<td>

<p>a scalar logical indicating whether a GP discrepancy or bias term should
be estimated via <code><a href="#topic+discrep.est">discrep.est</a></code>, as opposed to 
only a Gaussian (zero-mean) variance;
see <code><a href="#topic+discrep.est">discrep.est</a></code> for more details
</p>
</td></tr>
<tr><td><code id="fcalib_+3A_omp.threads">omp.threads</code></td>
<td>

<p>a scalar positive integer indicating the number
of threads to use for SMP parallel processing; see <code><a href="#topic+aGP">aGP</a></code> for more details
</p>
</td></tr>
<tr><td><code id="fcalib_+3A_save.global">save.global</code></td>
<td>

<p>an environment, e.g., <code>.GlobalEnv</code> if each evaluation of <code>fcalib</code>, say as
called by a wrapper or optimization routine, should be saved.  The variable
used in that environment will be <code>fcalib.save</code>.  Otherwise <code>save.global = FALSE</code>
will skip saving the information</p>
</td></tr>
<tr><td><code id="fcalib_+3A_verb">verb</code></td>
<td>

<p>a non-negative integer specifying the verbosity level; <code>verb = 0</code>
is quiet, whereas a larger value causes each evaluation to be printed
to the screen</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Gramacy, et al. (2015) defined an objective function which, when optimized,
returns a setting of calibration parameters under a setup akin to
the modularized calibration method of Liu, et al., (2009).  The <code>fcalib</code>
function returns a log density (likelihood or posterior probability) value 
obtained by performing emulation at a set of inputs <code>X</code> augmented
with a value of the calibration parameter, <code>u</code>.  The emulator
is trained on <code>XU</code> and <code>Z</code>, presumed to be very large 
relative to the size of the field data set <code>X</code> and <code>Y</code>,
necessitating the use of approximate methods like <code><a href="#topic+aGP">aGP</a></code>, 
via <code><a href="#topic+aGP.seq">aGP.seq</a></code>.  The
emulated values, call them <code>Yhat</code> are fed along with <code>X</code> and 
<code>Y</code> into the <code><a href="#topic+discrep.est">discrep.est</a></code> function, whose
likelihood or posterior calculation serves as a measure of merit for
the value <code>u</code>.
</p>
<p>The <code>fcalib</code> function is deterministic but, as Gramacy, et al. (2015)
described, can result is a rugged objective surface for optimizing,
meaning that conventional methods, like those in <code><a href="stats.html#topic+optim">optim</a></code>
are unlikely to work well.  They instead recommend using a blackbox
derivative-free method, like NOMAD (Le Digabel, 2011).  In our example
below we use the implementation in the <span class="pkg">crs</span> package, which provides
an <span class="rlang"><b>R</b></span> wrapper around the underlying C library.
</p>
<p>Note that while <code>fcalib</code> automates a call first to <code><a href="#topic+aGP.seq">aGP.seq</a></code>
and then to <code><a href="#topic+discrep.est">discrep.est</a></code>, it does not return enough
information to complete, say, an out-of-sample prediction exercise like
the one demonstrated in the <code><a href="#topic+discrep.est">discrep.est</a></code> documentation.
Therefore, after <code><a href="#topic+fcalib">fcalib</a></code> is used in an optimization to
find the best setting of the calibration parameter, <code>u</code>, 
those functions must then be used in post-processing to complete a
prediction exercise.  See <code>demo("calib")</code> or <code>vignette("laGP")</code>
for more details
</p>


<h3>Value</h3>

<p>Returns a scalar measuring the negative log likelihood or posterior density
of the calibration parameter <code>u</code> given the other inputs, for
the purpose of optimization over <code>u</code>
</p>


<h3>Note</h3>

<p>Note that in principle a separable correlation function could be used 
(e.g, via <code><a href="#topic+newGPsep">newGPsep</a></code> and <code><a href="#topic+mleGPsep">mleGPsep</a></code>), 
however this is not implemented at this time
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Gramacy, R. B. (2020) <em>Surrogates: Gaussian Process Modeling,
Design and Optimization for the Applied Sciences</em>. Boca Raton,
Florida: Chapman Hall/CRC.  (See Chapter 8.)
<a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p>R.B. Gramacy (2016). <em><span class="pkg">laGP</span>: Large-Scale Spatial Modeling via 
Local Approximate Gaussian Processes in <span class="rlang"><b>R</b></span>.</em>, Journal of Statistical 
Software, 72(1), 1-46; <a href="https://doi.org/10.18637/jss.v072.i01">doi:10.18637/jss.v072.i01</a> 
or see <code>vignette("laGP")</code>
</p>
<p>R.B. Gramacy, D. Bingham, JP. Holloway, M.J. Grosskopf, C.C. Kuranz, E. Rutter, 
M. Trantham, and P.R. Drake (2015). <em>Calibrating a large computer 
experiment simulating radiative shock hydrodynamics.</em> 
Annals of Applied Statistics, 9(3) 1141-1168; preprint on arXiv:1410.3293 
<a href="https://arxiv.org/abs/1410.3293">https://arxiv.org/abs/1410.3293</a>
</p>
<p>F. Liu, M. Bayarri, and J. Berger (2009). 
<em>Modularization in Bayesian analysis, with emphasis on analysis of computer models.</em> 
Bayesian Analysis, 4(1) 119-150.
</p>
<p>S. Le Digabel (2011). 
<em>Algorithm 909: NOMAD: Nonlinear Optimization with the MADS algorithm</em>.
ACM Transactions on Mathematical Software, 37, 4, 44:1-44:15.
</p>
<p>J.S. Racine, Z. and Nie (2012). <span class="pkg">crs</span>: 
<em>Categorical regression splines</em>. <span class="rlang"><b>R</b></span> package version 0.15-18.
</p>


<h3>See Also</h3>

<p><code>vignette("laGP")</code>, 
<code><a href="#topic+jmleGP">jmleGP</a></code>, <code><a href="#topic+newGP">newGP</a></code>, <code><a href="#topic+aGP.seq">aGP.seq</a></code>, <code><a href="#topic+discrep.est">discrep.est</a></code>,
<code><a href="crs.html#topic+snomadr">snomadr</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## the example here illustrates how fcalib combines aGP.seq and 
## discrep.est functions, duplicating the example in the discrep.est
## documentation file.  It is comprised of snippets from demo("calib"), 
## which contains code from the Calibration Section of vignette("laGP")

## Here we generate calibration data using a true calibration
## parameter, u, and then evaluate log posterior probabilities; 
## the discrep.est documentation repeats this with by first calling
## aGP.seq and then discrep.est.  The answers should be identical, however
## note that a call first to example("fcalib") and then 
## example("discrep.est") will generate two random data sets, causing
## the results not to match

## begin data-generation code identical to aGP.seq, discrep.est, fcalib
## example sections and demo("calib")

## M: computer model test function used in Goh et al, 2013 (Technometrics)
## an elaboration of one from Bastos and O'Hagan, 2009 (Technometrics) 
M &lt;- function(x,u) 
  {
    x &lt;- as.matrix(x)
    u &lt;- as.matrix(u)
    out &lt;- (1-exp(-1/(2*x[,2]))) 
    out &lt;- out * (1000*u[,1]*x[,1]^3+1900*x[,1]^2+2092*x[,1]+60) 
    out &lt;- out / (100*u[,2]*x[,1]^3+500*x[,1]^2+4*x[,1]+20)  
    return(out)
  }
  
## bias: discrepancy function from Goh et al, 2013 
bias &lt;- function(x) 
  {
    x&lt;-as.matrix(x)   
    out&lt;- 2*(10*x[,1]^2+4*x[,2]^2) / (50*x[,1]*x[,2]+10)
    return(out)
  }

## beta.prior: marginal beta prior for u, 
## defaults to a mode at 1/2 in hypercube
beta.prior &lt;- function(u, a=2, b=2, log=TRUE)
{
  if(length(a) == 1) a &lt;- rep(a, length(u))
  else if(length(a) != length(u)) stop("length(a) must be 1 or length(u)")
  if(length(b) == 1) b &lt;- rep(b, length(u))
  else if(length(b) != length(u)) stop("length(b) must be 1 or length(u)")
  if(log) return(sum(dbeta(u, a, b, log=TRUE)))
  else return(prod(dbeta(u, a, b, log=FALSE)))
}

## tgp for LHS sampling
library(tgp)
rect &lt;- matrix(rep(0:1, 4), ncol=2, byrow=2)

## training inputs
ny &lt;- 50; 
X &lt;- lhs(ny, rect[1:2,])    ## computer model train

## true (but unknown) setting of the calibration parameter
## for the computer model
u &lt;- c(0.2, 0.1)
Zu &lt;- M(X, matrix(u, nrow=1)) 

## field data response, biased and replicated
sd &lt;- 0.5
## Y &lt;- computer output + bias + noise
reps &lt;- 2 ## example from paper uses reps &lt;- 10
Y &lt;- rep(Zu,reps) + rep(bias(X),reps) + rnorm(reps*length(Zu), sd=sd) 
## variations: remove the bias or change 2 to 1 to remove replicates

## computer model design
nz &lt;- 10000
XU &lt;- lhs(nz, rect)
nth &lt;- 1 ## number of threads to use in emulation, demo uses 8

## augment with physical model design points 
## with various u settings
XU2 &lt;- matrix(NA, nrow=10*ny, ncol=4)
for(i in 1:10) {
  I &lt;- ((i-1)*ny+1):(ny*i)
  XU2[I,1:2] &lt;- X
}
XU2[,3:4] &lt;- lhs(10*ny, rect[3:4,])
XU &lt;- rbind(XU, XU2)

## evaluate the computer model
Z &lt;- M(XU[,1:2], XU[,3:4])

## flag indicating if estimating bias/discrepancy or not
bias.est &lt;- TRUE
## two passes of ALC with MLE calculations for aGP.seq
methods &lt;- rep("alcray", 2) ## demo uses rep("alc", 2)

## set up priors
da &lt;- d &lt;- darg(NULL, XU)
g &lt;- garg(list(mle=TRUE), Y) 

## end identical data generation code

## now calculate log posterior for true calibration parameter 
## value (u).  You could repeat this for an estimate value 
## from demo("calib"), for example u.hat &lt;- c(0.8236673, 0.1406989)

fcalib(u, XU, Z, X, Y, da, d, g, beta.prior, methods, M, bias.est, nth)
</code></pre>

<hr>
<h2 id='laGP'>
Localized Approximate GP Prediction At a Single Input Location
</h2><span id='topic+laGP'></span><span id='topic+laGP.R'></span><span id='topic+laGPsep.R'></span><span id='topic+laGPsep'></span>

<h3>Description</h3>

<p>Build a sub-design of <code>X</code> of size <code>end</code>, and infer parameters,
for approximate Gaussian process prediction at reference location(s) 
<code>Xref</code>. Return the moments of those predictive equations, and indices
into the local design
</p>


<h3>Usage</h3>

<pre><code class='language-R'>laGP(Xref, start, end, X, Z, d = NULL, g = 1/10000,
     method = c("alc", "alcopt", "alcray", "mspe", "nn", "fish"), Xi.ret = TRUE,
     close = min((1000+end)*if(method[1] %in% c("alcray", "alcopt")) 10 else 1, nrow(X)), 
     alc.gpu = FALSE, numstart = if(method[1] == "alcray") ncol(X) else 1, 
     rect = NULL, lite = TRUE, verb = 0)
laGP.R(Xref, start, end, X, Z, d = NULL, g = 1/10000,
     method = c("alc", "alcopt", "alcray", "mspe", "nn", "fish"), 
     Xi.ret = TRUE, pall = FALSE, 
     close = min((1000+end)*if(method[1] %in% c("alcray", "alcopt")) 10 else 1, nrow(X)),
     parallel = c("none", "omp", "gpu"), 
     numstart = if(method[1] == "alcray") ncol(X) else 1, 
     rect = NULL, lite = TRUE, verb = 0)
laGPsep(Xref, start, end, X, Z, d = NULL, g = 1/10000,
     method = c("alc", "alcopt", "alcray", "nn"), Xi.ret = TRUE, 
     close = min((1000+end)*if(method[1] %in% c("alcray", "alcopt")) 10 else 1, nrow(X)), 
     alc.gpu = FALSE, numstart = if(method[1] == "alcray") ncol(X) else 1, 
     rect = NULL, lite = TRUE, verb=0)
laGPsep.R(Xref, start, end, X, Z, d = NULL, g = 1/10000,
     method = c("alc", "alcopt", "alcray", "nn"), 
     Xi.ret = TRUE, pall = FALSE, 
     close = min((1000+end)*if(method[1] %in% c("alcray", "alcopt")) 10 else 1, nrow(X)),
     parallel = c("none", "omp", "gpu"), 
     numstart = if(method[1] == "alcray") ncol(X) else 1, 
     rect = NULL, lite = TRUE, verb = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="laGP_+3A_xref">Xref</code></td>
<td>

<p>a vector of length <code>ncol(X)</code> containing a single reference
location; or a <code>matrix</code> with <code>ncol(Xref) = ncol(X)</code> containing
multiple reference locations (unless <code>method = "alcray"</code>) 
for simultaneous sub-design and prediction
</p>
</td></tr>
<tr><td><code id="laGP_+3A_start">start</code></td>
<td>

<p>the number of Nearest Neighbor (NN) locations for initialization; must
specify <code>start &gt;= 6</code>
</p>
</td></tr>
<tr><td><code id="laGP_+3A_end">end</code></td>
<td>

<p>the total size of the local designs; must have <code>start &lt; end</code>
</p>
</td></tr>
<tr><td><code id="laGP_+3A_x">X</code></td>
<td>

<p>a <code>matrix</code> or <code>data.frame</code> containing
the full (large) design matrix of input locations
</p>
</td></tr>
<tr><td><code id="laGP_+3A_z">Z</code></td>
<td>

<p>a vector of responses/dependent values with <code>length(Z) = nrow(X)</code>
</p>
</td></tr>
<tr><td><code id="laGP_+3A_d">d</code></td>
<td>

<p>a prior or initial setting for the lengthscale
parameter for a Gaussian correlation function; a (default)
<code>NULL</code> value causes a sensible regularization (prior) and
initial setting to be generated via <code><a href="#topic+darg">darg</a></code>;
a scalar specifies an initial value, causing <code><a href="#topic+darg">darg</a></code>
to only generate the prior; otherwise, a list or partial list matching the output
of <code><a href="#topic+darg">darg</a></code> can be used to specify a custom prior.  In the
case of a partial list, the only the missing entries will be
generated. Note that a default/generated list specifies MLE/MAP
inference for this parameter. With 
<code>laGPsep</code>, the starting values can be an
<code>ncol(X)</code>-by-<code>nrow(XX)</code> <code>matrix</code> or <code>ncol(X)</code> vector
</p>
</td></tr>
<tr><td><code id="laGP_+3A_g">g</code></td>
<td>

<p>a prior or initial setting for the nugget parameter; a 
<code>NULL</code> value causes a sensible regularization (prior) and
initial setting to be generated via <code><a href="#topic+garg">garg</a></code>; a scalar
(default <code>g = 1/10000</code>) specifies an initial value, 
causing <code><a href="#topic+garg">garg</a></code> to only generate the prior; otherwise, a
list or partial list matching the output of <code><a href="#topic+garg">garg</a></code> 
can be used to specify a custom prior.  In the case of a partial list,
only the missing entries will be generated. Note that a 
default/generated list specifies <em>no</em> inference for this
parameter; i.e., it is fixed at its starting or default value, 
which may be appropriate for emulating 
deterministic computer code output.  In such situations a 
value much smaller than the default may work even better (i.e., 
yield better out-of-sample predictive performance).  
The default was chosen conservatively
</p>
</td></tr>
<tr><td><code id="laGP_+3A_method">method</code></td>
<td>

<p>Specifies the method by which <code>end-start</code> candidates from
<code>X</code> are chosen in order to predict at <code>Xref</code>. In brief, ALC
(<code>"alc"</code>, default) minimizes predictive variance; ALCRAY 
(<code>"alcray")</code>) executes a thrifty ALC-based search focused on 
rays emanating from the reference location [must have <code>nrow(Xref) = 1</code>]; 
ALCOPT (<code>"alcopt"</code>) optimizes a continuous ALC analog via derivatives to
and snaps the solution back to the candidate grid; 
MSPE (<code>"mspe"</code>) augments ALC 
with extra derivative information to minimize mean-squared prediction 
error (requires extra computation);
NN (<code>"nn"</code>) uses nearest neighbor; and EFI (<code>"fish"</code>) uses
the expected Fisher information - essentially <code>1/G</code> from
Gramacy &amp; Apley (2015) - which is global heuristic, i.e., not
localized to <code>Xref</code>.  
</p>
</td></tr>
<tr><td><code id="laGP_+3A_xi.ret">Xi.ret</code></td>
<td>

<p>A scalar logical indicating whether or not a vector of indices
into <code>X</code>, specifying the chosen sub-design, should be returned on
output
</p>
</td></tr>
<tr><td><code id="laGP_+3A_pall">pall</code></td>
<td>
<p>a scalar logical (for <code>laGP.R</code> only) offering the
ability to obtain predictions after every update (for progress
indication and debugging), rather than after just the last update</p>
</td></tr>
<tr><td><code id="laGP_+3A_close">close</code></td>
<td>

<p>a non-negative integer <code>end &lt; close &lt;= nrow(X)</code> 
specifying the number of NNs (to <code>Xref</code>) in 
<code>X</code> to consider when searching for elements of the sub-design; 
<code>close = 0</code> specifies all.  For <code>method="alcray"</code> and 
<code>method="alcopt"</code>, this argument specifies the scope used to snap 
solutions obtained via analog continuous searches back to elements of <code>X</code>, 
otherwise there are no restrictions on those searches.  Since these
approximate searches are cheaper, they can afford a larger 
&ldquo;snapping scope&rdquo; hence the larger default</p>
</td></tr>
<tr><td><code id="laGP_+3A_alc.gpu">alc.gpu</code></td>
<td>

<p>a scalar <code>logical</code> indicating if a GPU should be used to
parallelize the evaluation of the ALC criteria (<code>method = "alc"</code>).
Requires the package be compiled with CUDA flags; see README/INSTALL 
in the package source for more details; currently only available 
for <code>nrow(Xref) == 1</code> 
via <code>laGP</code>, not <code>laGPsep</code> or the <code>.R</code> variants,
and only supports off-loading ALC calculation to the GPU
</p>
</td></tr>
<tr><td><code id="laGP_+3A_parallel">parallel</code></td>
<td>

<p>a switch indicating if any parallel calculation of 
the criteria is desired.  Currently parallelization at this level is only 
provided for option <code>method = "alc"</code>). 
For <code>parallel = "omp"</code>, the package must be compiled with OMP flags;
for <code>parallel = "gpu"</code>, the package must be compiled with CUDA
flags see README/INSTALL
in the package source for more details; currently only available 
via <code>laGP.R</code>
</p>
</td></tr>
<tr><td><code id="laGP_+3A_numstart">numstart</code></td>
<td>
<p> a scalar integer indicating the number of rays for each
greedy search when <code>method="alcray"</code> or the number of restarts
when <code>method="alcopt"</code>.  More rays or restarts
leads to a more thorough, but more computational intensive search.
This argument is not involved in other methods </p>
</td></tr>
<tr><td><code id="laGP_+3A_rect">rect</code></td>
<td>

<p>an optional <code>2</code>-by-<code>ncol(X)</code> <code>matrix</code> describing a bounding
rectangle for <code>X</code> that is used by the <code>"alcray"</code> method.  
If not specified, the rectangle is calculated from <code>range</code> applied
to the columns of <code>X</code>
</p>
</td></tr>
<tr><td><code id="laGP_+3A_lite">lite</code></td>
<td>
<p> Similar to the <code><a href="#topic+predGP">predGP</a></code> option of the same name,
this argument specifies whether (<code>TRUE</code>, the default) or not (<code>FALSE</code>) to return
a full covariance structure is returned, as opposed the diagonal only.  A full covariance
structure requires more computation and more storage.  This option is
only relevant when <code>nrow(Xref) &gt; 1</code> </p>
</td></tr>
<tr><td><code id="laGP_+3A_verb">verb</code></td>
<td>

<p>a non-negative integer specifying the verbosity level; <code>verb = 0</code>
is quiet, and larger values cause more progress information to be
printed to the screen
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A sub-design of <code>X</code> of size <code>end</code> is built-up according to
the criteria prescribed by the <code>method</code> and then used to predict at
<code>Xref</code>.  The first <code>start</code> locations are NNs in order to
initialize the first GP, via <code><a href="#topic+newGP">newGP</a></code> or <code><a href="#topic+newGPsep">newGPsep</a></code>, 
and thereby initialize the
search.  Each subsequent addition is found via the chosen criterion
(<code>method</code>), and the GP fit is updated via <code><a href="#topic+updateGP">updateGP</a></code>
or <code><a href="#topic+updateGPsep">updateGPsep</a></code>
</p>
<p>The runtime is cubic in <code>end</code>, although
the multiplicative &ldquo;constant&rdquo; out front depends on the
<code>method</code> chosen, the size of the design <code>X</code>, and
<code>close</code>.  The <code>"alcray"</code> method has a smaller constant
since it does not search over all candidates exhaustively.
</p>
<p>After building the sub-design, local MLE/MAP lengthscale (and/or
nugget) parameters are estimated, depending on the <code>d</code> and
<code>g</code> arguments supplied.  This is facilitated by calls to
<code><a href="#topic+mleGP">mleGP</a></code> or <code><a href="#topic+jmleGP">jmleGP</a></code>.
</p>
<p>Finally <code><a href="#topic+predGP">predGP</a></code> is called on the resulting local GP
model, and the parameters of the resulting Student-t distribution(s)
are returned.  Unless <code>Xi.ret = FALSE</code>, the indices of the
local design are also returned.
</p>
<p><code>laGP.R</code> and <code>laGPsep.R</code> are a prototype <span class="rlang"><b>R</b></span>-only version for
debugging and transparency purposes.  They are slower than
<code>laGP</code> and <code>laGPsep</code>, which are primarily in C, and may not
provide identical output in all cases due to differing library implementations
used as subroutines; see note below for an example.  <code>laGP.R</code> and other
<code>.R</code> functions in the package may be useful for developing new programs
that involve similar subroutines. The current version of <code>laGP.R</code>
allows OpenMP and/or GPU parallelization of the criteria (<code>method</code>) if
the package is compiled with the appropriate flags.  See README/INSTALL in
the package source for more information.  For algorithmic details, see
Gramacy, Niemi, &amp; Weiss (2014) </p>


<h3>Value</h3>

<p>The output is a <code>list</code> with the following components.
</p>
<table>
<tr><td><code>mean</code></td>
<td>
<p>a vector of predictive means of length <code>nrow(Xref)</code></p>
</td></tr>
<tr><td><code>s2</code></td>
<td>
<p>a vector of Student-t scale parameters of length
<code>nrow(Xref)</code></p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>a Student-t degrees of freedom scalar (applies to all
<code>Xref</code>)</p>
</td></tr>
<tr><td><code>llik</code></td>
<td>
<p>a scalar indicating the maximized log likelihood or log posterior
probability of the data/parameter(s) under the chosen sub-design;
provided up to an additive constant</p>
</td></tr>
<tr><td><code>time</code></td>
<td>
<p>a scalar giving the passage of wall-clock time elapsed
for (substantive parts of) the calculation</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a copy of the <code>method</code> argument</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>a full-list version of the <code>d</code> argument, possibly completed by <code>darg</code></p>
</td></tr>
<tr><td><code>g</code></td>
<td>
<p>a full-list version of the <code>g</code> argument, possibly
completed by <code>garg</code></p>
</td></tr>
<tr><td><code>mle</code></td>
<td>
<p>if <code>d$mle</code> and/or <code>g$mle</code> are <code>TRUE</code>, then
<code>mle</code> is a <code>data.frame</code> containing the values found for
these parameters, and the number of required iterations</p>
</td></tr>
<tr><td><code>Xi</code></td>
<td>
<p>when <code>Xi.ret = TRUE</code>, this field contains a vector of
indices of length <code>end</code> into <code>X</code> indicating the sub-design chosen</p>
</td></tr>
<tr><td><code>close</code></td>
<td>
<p>a copy of the input argument</p>
</td></tr>
</table>


<h3>Note</h3>

<p><code>laGPsep</code> provides the same functionality as <code>laGP</code> but deploys
a separable covariance function.  However criteria (<code>method</code>s) EFI and
MSPE are not supported.  This is considered &ldquo;beta&rdquo; functionality
at this time.
</p>
<p>Note that using <code>method="NN"</code> gives the same result as specifying
<code>start=end</code>, however at some extra computational expense.
</p>
<p>Handling multiple reference locations
(<code>nrow(Xref) &gt; 1</code>) is &ldquo;beta&rdquo; functionality.  In this case
the initial <code>start</code> locations are chosen by applying NN to the
average distances to all <code>Xref</code> locations.  Using 
<code>method="alcopt"</code> causes exhaustive search to be approximated by
a continuous analog via closed form derivatives.  
See <code><a href="#topic+alcoptGP">alcoptGP</a></code> for more details.   Although the approximation
provided has a spirit similar to <code>method="alcray"</code>, in that 
both methods are intended to offer a thrifty alternative, 
<code>method="alcray"</code> is not applicable when <code>nrow(Xref) &gt; 1</code>.
</p>
<p>Differences between the C <code>qsort</code> function and <span class="rlang"><b>R</b></span>'s
<code><a href="base.html#topic+order">order</a></code> function may cause chosen designs returned from
<code>laGP</code> and <code>laGP.R</code> (code and <code>laGPsep</code> and <code>laGPsep.R</code>)
to differ when multiple <code>X</code> values are equidistant to <code>Xref</code>
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a> and Furong Sun <a href="mailto:furongs@vt.edu">furongs@vt.edu</a>
</p>


<h3>References</h3>

<p>Gramacy, R. B. (2020) <em>Surrogates: Gaussian Process Modeling,
Design and Optimization for the Applied Sciences</em>. Boca Raton,
Florida: Chapman Hall/CRC.  (See Chapter 9.)
<a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p>F. Sun, R.B. Gramacy, B. Haaland, E. Lawrence, and A. Walker (2019).
<em>Emulating satellite drag from large simulation experiments</em>,
SIAM/ASA Journal on Uncertainty Quantification, 7(2), pp. 720-759;
preprint on arXiv:1712.00182;
<a href="https://arxiv.org/abs/1712.00182">https://arxiv.org/abs/1712.00182</a>
</p>
<p>R.B. Gramacy (2016). <em><span class="pkg">laGP</span>: Large-Scale Spatial Modeling via 
Local Approximate Gaussian Processes in <span class="rlang"><b>R</b></span>.</em>, Journal of Statistical 
Software, 72(1), 1-46; <a href="https://doi.org/10.18637/jss.v072.i01">doi:10.18637/jss.v072.i01</a> 
or see <code>vignette("laGP")</code>
</p>
<p>R.B. Gramacy and B. Haaland (2016).
<em>Speeding up neighborhood search in local Gaussian process prediction.</em>
Technometrics, 58(3), pp. 294-303;
preprint on arXiv:1409.0074 
<a href="https://arxiv.org/abs/1409.0074">https://arxiv.org/abs/1409.0074</a>
</p>
<p>R.B. Gramacy and D.W. Apley (2015).
<em>Local Gaussian process approximation for large computer
experiments.</em> Journal of Computational and Graphical Statistics, 
24(2), pp. 561-678; preprint on arXiv:1303.0383;
<a href="https://arxiv.org/abs/1303.0383">https://arxiv.org/abs/1303.0383</a>
</p>
<p>R.B. Gramacy, J. Niemi, R.M. Weiss (2014).
<em>Massively parallel approximate Gaussian process regression.</em>
SIAM/ASA Journal on Uncertainty Quantification, 2(1), pp. 568-584;
preprint on arXiv:1310.5182;
<a href="https://arxiv.org/abs/1310.5182">https://arxiv.org/abs/1310.5182</a>
</p>


<h3>See Also</h3>

<p><code>vignette("laGP")</code>, 
<code><a href="#topic+aGP">aGP</a></code>, <code><a href="#topic+newGP">newGP</a></code>, <code><a href="#topic+updateGP">updateGP</a></code>,
<code><a href="#topic+predGP">predGP</a></code>, <code><a href="#topic+mleGP">mleGP</a></code>, <code><a href="#topic+jmleGP">jmleGP</a></code>,
<code><a href="#topic+alcGP">alcGP</a></code>, <code><a href="#topic+mspeGP">mspeGP</a></code>, <code><a href="#topic+alcrayGP">alcrayGP</a></code>,
<code><a href="#topic+randLine">randLine</a></code> ## path-based local prediction via <code>laGP</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## examining a particular laGP call from the larger analysis provided
## in the aGP documentation

## A simple 2-d test function used in Gramacy &amp; Apley (2014);
## thanks to Lee, Gramacy, Taddy, and others who have used it before
f2d &lt;- function(x, y=NULL)
  {
    if(is.null(y)) {
      if(!is.matrix(x) &amp;&amp; !is.data.frame(x)) x &lt;- matrix(x, ncol=2)
      y &lt;- x[,2]; x &lt;- x[,1]
    }
    g &lt;- function(z)
      return(exp(-(z-1)^2) + exp(-0.8*(z+1)^2) - 0.05*sin(8*(z+0.1)))
    z &lt;- -g(x)*g(y)
  }

## build up a design with N=~40K locations
x &lt;- seq(-2, 2, by=0.02)
X &lt;- as.matrix(expand.grid(x, x))
Z &lt;- f2d(X)

## optional first pass of nearest neighbor
Xref &lt;- matrix(c(-1.725, 1.725), nrow=TRUE)
out &lt;- laGP(Xref, 6, 50, X, Z, method="nn")

## second pass via ALC, ALCOPT, MSPE, and ALC-ray respectively,
## conditioned on MLE d-values found above
out2 &lt;- laGP(Xref, 6, 50, X, Z, d=out$mle$d)
# out2.alcopt &lt;- laGP(Xref, 6, 50, X, Z, d=out2$mle$d, method="alcopt")
out2.mspe &lt;- laGP(Xref, 6, 50, X, Z, d=out2$mle$d, method="mspe")
out2.alcray &lt;- laGP(Xref, 6, 50, X, Z, d=out2$mle$d, method="alcray")

## look at the different designs
plot(rbind(X[out2$Xi,], X[out2.mspe$Xi,]), type="n",
     xlab="x1", ylab="x2", main="comparing local designs")
points(Xref[1], Xref[2], col=2, cex=0.5)
text(X[out2$Xi,], labels=1:50, cex=0.6)
# text(X[out2.alcopt$Xi,], labels=1:50, cex=0.6, col="forestgreen")
text(X[out2.mspe$Xi,], labels=1:50, cex=0.6, col="blue")
text(X[out2.alcray$Xi,], labels=1:50, cex=0.6, col="red")
legend("right", c("ALC", "ALCOPT", "MSPE", "ALCRAY"),
       text.col=c("black", "forestgreen", "blue", "red"), bty="n")

## compare computational time
c(nn=out$time, alc=out2$time, # alcopt=out2.alcopt$time, 
  mspe=out2.mspe$time, alcray=out2.alcray$time)

## Not run: 
  ## Joint path sampling: a comparison between ALC-ex, ALC-opt and NN

  ## defining a predictive path
  wx &lt;- seq(-0.85, 0.45, length=100)
  W &lt;- cbind(wx-0.75, wx^3+0.51)

  ## three comparators from Sun, et al. (2017)
  ## larger-than-default "close" argument to capture locations nearby path
  p.alc &lt;- laGPsep(W, 6, 100, X, Z, close=10000, lite=FALSE)
  p.alcopt &lt;- laGPsep(W, 6, 100, X, Z, method="alcopt", lite=FALSE)
  ## note that close=10*(1000+end) would be the default for method = "alcopt"
  p.nn &lt;- laGPsep(W, 6, 100, X, Z, method="nn", close=10000, lite=FALSE)

  ## time comparison
  c(alc=p.alc$time, alcopt=p.alcopt$time, nn=p.nn$time)

  ## visualization
  plot(W, type="l", xlab="x1", ylab="x2", xlim=c(-2.25,0), ylim=c(-0.75,1.25), lwd=2)
  points(X[p.alc$Xi,], col=2, cex=0.6)
  lines(W[,1]+0.25, W[,2]-0.25, lwd=2)
  points(X[p.nn$Xi,1]+0.25, X[p.nn$Xi,2]-0.25, pch=22, col=3, cex=0.6)
  lines(W[,1]-0.25, W[,2]+0.25, lwd=2) 
  points(X[p.alcopt$Xi,1]-0.25, X[p.alcopt$Xi,2]+0.25, pch=23, col=4, cex=0.6)
  legend("bottomright", c("ALC-opt", "ALC-ex", "NN"), pch=c(22, 21, 23), col=c(4,2,3))

## End(Not run)
</code></pre>

<hr>
<h2 id='llikGP'>
Calculate a GP log likelihood
</h2><span id='topic+llikGP'></span><span id='topic+llikGPsep'></span>

<h3>Description</h3>

<p>Calculate a Gaussian process (GP) log likelihood or posterior
probability with reference to a C-side GP object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llikGP(gpi, dab = c(0, 0), gab = c(0, 0))
llikGPsep(gpsepi, dab = c(0, 0), gab = c(0, 0))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="llikGP_+3A_gpi">gpi</code></td>
<td>

<p>a C-side GP object identifier (positive integer);
e.g., as returned by <code><a href="#topic+newGP">newGP</a></code></p>
</td></tr>
<tr><td><code id="llikGP_+3A_gpsepi">gpsepi</code></td>
<td>
<p> similar to <code>gpi</code> but indicating a separable GP object</p>
</td></tr>
<tr><td><code id="llikGP_+3A_dab">dab</code></td>
<td>

<p><code>ab</code> for the lengthscale parameter, see Details
</p>
</td></tr>
<tr><td><code id="llikGP_+3A_gab">gab</code></td>
<td>

<p><code>ab</code> for the nugget parameter, see Details
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An &ldquo;<code>ab</code>&rdquo; parameter is a non-negative 2-vector describing 
shape and rate parameters to a Gamma prior; a zero-setting for
either value results in no-prior being used in which case a log likelihood
is returned.  If both <code>ab</code> parameters are specified, then the value
returned can be interpreted as a log posterior density.  See <code><a href="#topic+darg">darg</a></code>
for more information about <code>ab</code>
</p>


<h3>Value</h3>

<p>A real-valued scalar is returned.
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mleGP">mleGP</a></code>, <code><a href="#topic+darg">darg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## partly following the example in mleGP
if(require("MASS")) {

  ## motorcycle data and predictive locations
  X &lt;- matrix(mcycle[,1], ncol=1)
  Z &lt;- mcycle[,2]

  ## get sensible ranges
  d &lt;- darg(NULL, X)
  g &lt;- garg(list(mle=TRUE), Z)
  
  ## initialize the model
  gpi &lt;- newGP(X, Z, d=d$start, g=g$start)

  ## calculate log likelihood
  llikGP(gpi)
  ## calculate posterior probability
  llikGP(gpi, d$ab, g$ab)

  ## clean up
  deleteGP(gpi)
}
</code></pre>

<hr>
<h2 id='mleGP'>
Inference for GP correlation parameters
</h2><span id='topic+mleGP'></span><span id='topic+mleGPsep.R'></span><span id='topic+mleGPsep'></span><span id='topic+jmleGP'></span><span id='topic+jmleGP.R'></span><span id='topic+jmleGPsep'></span><span id='topic+jmleGPsep.R'></span>

<h3>Description</h3>

<p>Maximum likelihood/a posteriori inference for (isotropic and separable)
Gaussian lengthscale and nugget parameters, marginally or jointly, for 
Gaussian process regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mleGP(gpi, param = c("d", "g"), tmin=sqrt(.Machine$double.eps), 
  tmax = -1, ab = c(0, 0), verb = 0)
mleGPsep(gpsepi, param=c("d", "g", "both"), tmin=rep(sqrt(.Machine$double.eps), 2),
  tmax=c(-1,1), ab=rep(0,4), maxit=100, verb=0)
mleGPsep.R(gpsepi, param=c("d", "g"), tmin=sqrt(.Machine$double.eps), 
  tmax=-1, ab=c(0,0), maxit=100, verb=0)
jmleGP(gpi, drange=c(sqrt(.Machine$double.eps),10), 
  grange=c(sqrt(.Machine$double.eps), 1), dab=c(0,0), gab=c(0,0), verb=0)
jmleGP.R(gpi, N=100, drange=c(sqrt(.Machine$double.eps),10), 
  grange=c(sqrt(.Machine$double.eps), 1), dab=c(0,0), gab=c(0,0), verb=0)
jmleGPsep(gpsepi, drange=c(sqrt(.Machine$double.eps),10), 
  grange=c(sqrt(.Machine$double.eps), 1), dab=c(0,0), gab=c(0,0), 
  maxit=100, verb=0)
jmleGPsep.R(gpsepi, N=100, drange=c(sqrt(.Machine$double.eps),10), 
  grange=c(sqrt(.Machine$double.eps), 1), dab=c(0,0), gab=c(0,0), 
  maxit=100, mleGPsep=mleGPsep.R, verb=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mleGP_+3A_gpi">gpi</code></td>
<td>

<p>a C-side GP object identifier (positive integer);
e.g., as returned by <code><a href="#topic+newGP">newGP</a></code>
</p>
</td></tr>
<tr><td><code id="mleGP_+3A_gpsepi">gpsepi</code></td>
<td>
<p> similar to <code>gpi</code> but indicating a separable GP object,
as returned by <code><a href="#topic+newGPsep">newGPsep</a></code></p>
</td></tr>
<tr><td><code id="mleGP_+3A_n">N</code></td>
<td>
<p> for <code>jmleGP.R</code>, the maximum number of times the pair of margins
should be iterated over before determining failed convergence; note
that (at this time) <code>jmleGP</code> uses a hard-coded <code>N=100</code> in
its C implementation</p>
</td></tr>
<tr><td><code id="mleGP_+3A_param">param</code></td>
<td>

<p>for <code>mleGP</code>, indicating whether to work on the lengthscale
(<code>d</code>) or nugget (<code>g</code>) margin
</p>
</td></tr>
<tr><td><code id="mleGP_+3A_tmin">tmin</code></td>
<td>

<p>for <code>mleGP</code>, smallest value considered for the parameter (<code>param</code>)
</p>
</td></tr>
<tr><td><code id="mleGP_+3A_tmax">tmax</code></td>
<td>

<p>for <code>mleGP</code>, largest value considered for the parameter (<code>param</code>); a setting of <code>-1</code> for lengthscales, the default, causes
<code>ncol(X)^2</code> to be used
</p>
</td></tr>
<tr><td><code id="mleGP_+3A_drange">drange</code></td>
<td>
<p> for <code>jmleGP</code>, these are <code>c(tmin, tmax)</code>
values for the lengthscale parameter; the default values are
reasonable for 1-d inputs in the unit interval </p>
</td></tr>
<tr><td><code id="mleGP_+3A_grange">grange</code></td>
<td>
<p> for <code>jmleGP</code>, these are <code>c(tmin, tmax)</code>
values for the nugget parameter; the default values are reasonable
for responses with a range of one </p>
</td></tr>
<tr><td><code id="mleGP_+3A_ab">ab</code></td>
<td>

<p>for <code>mleGP</code>, a non-negative 2-vector describing shape and rate parameters to a
Gamma prior for the parameter (<code>param</code>); a zero-setting for
either value results in no-prior being used (MLE inference); otherwise
MAP inference is performed
</p>
</td></tr>
<tr><td><code id="mleGP_+3A_maxit">maxit</code></td>
<td>
<p> for <code>mleGPsep</code> this is passed as <code>control=list(trace=maxit)</code>
to <code><a href="stats.html#topic+optim">optim</a></code>'s L-BFGS-B method for optimizing
the likelihood/posterior of a separable GP representation; this argument is
not used for isotropic GP versions, nor for optimizing the nugget </p>
</td></tr>
<tr><td><code id="mleGP_+3A_dab">dab</code></td>
<td>
<p> for <code>jmleGP</code>, this is <code>ab</code> for the lengthscale
parameter </p>
</td></tr>
<tr><td><code id="mleGP_+3A_gab">gab</code></td>
<td>
<p> for <code>jmleGP</code>, this is <code>ab</code> for the nugget
parameter </p>
</td></tr>
<tr><td><code id="mleGP_+3A_mlegpsep">mleGPsep</code></td>
<td>
<p> function for internal MLE calculation of the separable
lengthscale parameter; one of either <code>mleGPsep.R</code> based on 
<code>method="L-BFGS-B"</code> using <code><a href="stats.html#topic+optim">optim</a></code>; 
or <code>mleGPsep</code> using the <code>C</code> entry point <code>lbfgsb</code>.  
Both options use a <code>C</code> backend for the nugget </p>
</td></tr>
<tr><td><code id="mleGP_+3A_verb">verb</code></td>
<td>

<p>a verbosity argument indicating how much information about the optimization
steps should be printed to the screen; <code>verb &lt;= 0</code> is quiet; for
<code>jmleGP</code>, a <code>verb - 1</code> value is passed to the <code>mleGP</code> or 
<code>mleGPsep</code> subroutine(s)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>mleGP</code> and <code>mleGPsep</code> perform marginal (or profile) inference
for the specified <code>param</code>, either the lengthscale or the nugget.  
<code>mleGPsep</code> can perform simultaneous lengthscale and nugget inference via
a common gradient with <code>param = "both"</code>.  More details are provided below.
</p>
<p>For the lengthscale, <code>mleGP</code> uses a Newton-like scheme with analytic first 
and second derivatives (more below) to find the scalar parameter for the isotropic
Gaussian correlation function, with hard-coded 100-max iterations threshold and a
<code>sqrt(.Machine$double.eps)</code> tolerance for determining convergence;
<code>mleGPsep.R</code> uses L-BFGS-B via <code><a href="stats.html#topic+optim">optim</a></code> for the 
vectorized parameter of the separable Gaussian correlation, with a user-supplied
maximum number of iterations (<code>maxit</code>) passed to <code><a href="stats.html#topic+optim">optim</a></code>.  
When <code>maxit</code> is reached the output <code>conv = 1</code> is returned, 
subsequent identical calls to <code>mleGPsep.R</code> can be used to continue with
further iterations.  <code>mleGPsep</code> is similar, but uses the <code>C</code> 
entry point <code>lbfgsb</code>.
</p>
<p>For the nugget, both <code>mleGP</code> and <code>mleGPsep</code> utilize a (nearly 
identical) Newton-like scheme leveraging first and second derivatives.
</p>
<p><code>jmleGP</code> and <code>jmleGPsep</code> provide joint inference by iterating
over the marginals of lengthscale and nugget.  The <code>jmleGP.R</code> function
is an <span class="rlang"><b>R</b></span>-only wrapper around
<code>mleGP</code> (which is primarily in C), whereas <code>jmleGP</code> is
primarily in C but with reduced output and 
with hard-coded <code>N=100</code>.  The same is true for <code>jmleGPsep</code>.
</p>
<p><code>mleGPsep</code> provides a <code>param = "both"</code> alternative to <code>jmleGPsep</code> 
leveraging a common gradient. It can be helpful to supply a larger <code>maxit</code> 
argument compared to <code>jmleGPsep</code> as the latter may do up to 100 outer
iterations, cycling over lengthscale and nugget.  <code>mleGPsep</code> usually requires
many fewer total iterations, unless one of the lengthscale or nugget is 
already converged. In anticipation of <code>param = "both"</code> the 
<code>mleGPsep</code> function has longer default values for its bounds and prior 
arguments.  These longer arguments are 
ignored when <code>param != "both"</code>.  At this time <code>mleGP</code> does not have a 
<code>param = "both"</code> option.
</p>
<p>All methods are initialized at the value of the parameter(s) currently
stored by the C-side object referenced by <code>gpi</code> or <code>gpsepi</code>.  It is
<em>highly recommended</em> that sensible range values (<code>tmin</code>, <code>tmax</code>
or <code>drange</code>, <code>grange</code>) be provided.  The defaults provided are
too loose for most applications.  As illustrated in the examples below,
the <code><a href="#topic+darg">darg</a></code> and <code><a href="#topic+garg">garg</a></code> functions can be used
to set appropriate ranges from the distributions of inputs and output
data respectively.
</p>
<p>The Newton-like method implemented for the isotropic lengthscale and for the
nugget offers very fast convergence to local maxima, but sometimes it fails
to converge (for any of the usual reasons).  The implementation
detects this, and in such cases it invokes a <code>Brent_fmin</code> call instead -
this is the method behind the <code><a href="stats.html#topic+optimize">optimize</a></code> function.
</p>
<p>Note that the <code>gpi</code> or <code>gpsepi</code> object(s) must have been allocated with
<code>dK=TRUE</code>; alternatively, one can call <code>buildKGP</code> or <code>buildKGPsep</code>
- however, this is not in the NAMESPACE at this time
</p>


<h3>Value</h3>

<p>A self-explanatory <code><a href="base.html#topic+data.frame">data.frame</a></code> is returned containing the
values inferred and the number of iterations used.  The
<code>jmleGP.R</code> and <code>jmleGPsep.R</code> functions will also show progress details (the values
obtained after each iteration over the marginals).
</p>
<p>However, the most important &ldquo;output&rdquo; is the modified GP object
which retains the setting of the parameters reported on output as a 
side effect.
</p>
<p><code>mleGPsep</code> and <code>jmleGPsep</code> provide an output field/column
called <code>conv</code> indicating convergence (when 0), or alternately 
a value agreeing with a non-convergence code provided on 
output by <code><a href="stats.html#topic+optim">optim</a></code>
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>For standard GP inference, refer to any graduate text, e.g., Rasmussen
&amp; Williams <em>Gaussian Processes for Machine Learning</em>, or
</p>
<p>Gramacy, R. B. (2020) <em>Surrogates: Gaussian Process Modeling,
Design and Optimization for the Applied Sciences</em>. Boca Raton,
Florida: Chapman Hall/CRC.  (See Chapter 5.)
<a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>


<h3>See Also</h3>

<p><code>vignette("laGP")</code>, 
<code><a href="#topic+newGP">newGP</a></code>, <code><a href="#topic+laGP">laGP</a></code>, <code><a href="#topic+llikGP">llikGP</a></code>, <code><a href="stats.html#topic+optimize">optimize</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## a simple example with estimated nugget
if(require("MASS")) {

  ## motorcycle data and predictive locations
  X &lt;- matrix(mcycle[,1], ncol=1)
  Z &lt;- mcycle[,2]

  ## get sensible ranges
  d &lt;- darg(NULL, X)
  g &lt;- garg(list(mle=TRUE), Z)

  ## initialize the model
  gpi &lt;- newGP(X, Z, d=d$start, g=g$start, dK=TRUE)

  ## separate marginal inference (not necessary - for illustration only)
  print(mleGP(gpi, "d", d$min, d$max))
  print(mleGP(gpi, "g", g$min, g$max))

  ## joint inference (could skip straight to here)
  print(jmleGP(gpi, drange=c(d$min, d$max), grange=c(g$min, g$max)))

  ## plot the resulting predictive surface
  N &lt;- 100
  XX &lt;- matrix(seq(min(X), max(X), length=N), ncol=1)
  p &lt;- predGP(gpi, XX, lite=TRUE)
  plot(X, Z, main="stationary GP fit to motorcycle data")
  lines(XX, p$mean, lwd=2)
  lines(XX, p$mean+1.96*sqrt(p$s2*p$df/(p$df-2)), col=2, lty=2)
  lines(XX, p$mean-1.96*sqrt(p$s2*p$df/(p$df-2)), col=2, lty=2)

  ## clean up
  deleteGP(gpi)
}

## 
## with a separable correlation function 
##

## 2D Example: GoldPrice Function, mimicking GP_fit package
f &lt;- function(x) 
{
  x1 &lt;- 4*x[,1] - 2
  x2 &lt;- 4*x[,2] - 2;
  t1 &lt;- 1 + (x1 + x2 + 1)^2*(19 - 14*x1 + 3*x1^2 - 14*x2 + 6*x1*x2 + 3*x2^2);
  t2 &lt;- 30 + (2*x1 -3*x2)^2*(18 - 32*x1 + 12*x1^2 + 48*x2 - 36*x1*x2 + 27*x2^2);
  y &lt;- t1*t2;
  return(y)
}

## build design
library(tgp)
n &lt;- 50 ## change to 100 or 1000 for more interesting demo
B &lt;- rbind(c(0,1), c(0,1))
X &lt;- dopt.gp(n, Xcand=lhs(10*n, B))$XX
## this differs from GP_fit in that we use the log response
Y &lt;- log(f(X))

## get sensible ranges
d &lt;- darg(NULL, X)
g &lt;- garg(list(mle=TRUE), Y)

## build GP and jointly optimize via profile mehtods
gpisep &lt;- newGPsep(X, Y, d=rep(d$start, 2), g=g$start, dK=TRUE)
jmleGPsep(gpisep, drange=c(d$min, d$max), grange=c(g$min, g$max))

## clean up
deleteGPsep(gpisep)

## alternatively, we can optimize via a combined gradient
gpisep &lt;- newGPsep(X, Y, d=rep(d$start, 2), g=g$start, dK=TRUE)
mleGPsep(gpisep, param="both", tmin=c(d$min, g$min), tmax=c(d$max, g$max))
deleteGPsep(gpisep)
</code></pre>

<hr>
<h2 id='newGP'>
Create A New GP Object
</h2><span id='topic+newGP'></span><span id='topic+updateGP'></span><span id='topic+newGPsep'></span><span id='topic+updateGPsep'></span>

<h3>Description</h3>

<p>Build a Gaussian process C-side object based on the <code>X</code>-<code>Z</code>
data and parameters provided, and augment those objects with new
data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>newGP(X, Z, d, g, dK = FALSE)
newGPsep(X, Z, d, g, dK = FALSE)
updateGP(gpi, X, Z, verb = 0)
updateGPsep(gpsepi, X, Z, verb = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="newGP_+3A_x">X</code></td>
<td>

<p>a <code>matrix</code> or <code>data.frame</code> containing
the full (large) design matrix of input locations
</p>
</td></tr>
<tr><td><code id="newGP_+3A_z">Z</code></td>
<td>

<p>a vector of responses/dependent values with <code>length(Z) = nrow(X)</code>
</p>
</td></tr>
<tr><td><code id="newGP_+3A_d">d</code></td>
<td>

<p>a positive scalar lengthscale parameter for an isotropic Gaussian
correlation function (<code>newGP</code>); or a vector for a separable
version (<code>newGPsep</code>) 
</p>
</td></tr>
<tr><td><code id="newGP_+3A_g">g</code></td>
<td>

<p>a positive scalar nugget parameter
</p>
</td></tr>
<tr><td><code id="newGP_+3A_dk">dK</code></td>
<td>

<p>a scalar logical indicating whether or not derivative information
(for the lengthscale) should be maintained by the GP object; 
this is required for calculating MLEs/MAPs of the lengthscale 
parameter(s) via <code><a href="#topic+mleGP">mleGP</a></code> and <code><a href="#topic+jmleGP">jmleGP</a></code>
</p>
</td></tr>
<tr><td><code id="newGP_+3A_gpi">gpi</code></td>
<td>

<p>a C-side GP object identifier (positive integer); e.g., as returned by <code>newGP</code>
</p>
</td></tr>
<tr><td><code id="newGP_+3A_gpsepi">gpsepi</code></td>
<td>
<p> similar to <code>gpi</code> but indicating a separable GP object,
as returned by <code><a href="#topic+newGPsep">newGPsep</a></code></p>
</td></tr>
<tr><td><code id="newGP_+3A_verb">verb</code></td>
<td>

<p>a non-negative integer indicating the verbosity level.  A positive value
causes progress statements to be printed to the screen for each
update of <code>i in 1:nrow(X)</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>newGP</code> allocates a new GP object on the C-side and returns its
unique integer identifier (<code>gpi</code>), taking time which is cubic on
<code>nrow(X)</code>;  allocated GP objects must (eventually) be destroyed
with <code><a href="#topic+deleteGP">deleteGP</a></code> or <code><a href="#topic+deleteGPs">deleteGPs</a></code> or memory will leak.
The same applies for <code>newGPsep</code>, except deploying a separable 
correlation with limited feature set; see <code><a href="#topic+deleteGPsep">deleteGPsep</a></code> and
<code><a href="#topic+deleteGPseps">deleteGPseps</a></code>
</p>
<p><code>updateGP</code> takes <code>gpi</code> identifier as
input and augments that GP with new data.  A sequence of updates is
performed, for each <code>i in 1:nrow(X)</code>, each taking time which is
quadratic in the number of data points. 
<code>updateGP</code> also updates any statistics needed in order to quickly
search for new local design candidates via <code><a href="#topic+laGP">laGP</a></code>.
The same applies to <code>updateGPsep</code> on <code>gpsepi</code> objects
</p>


<h3>Value</h3>

<p><code>newGP</code> and <code>newGPsep</code> create 
a unique GP indicator (<code>gpi</code> or <code>gpsepi</code>) referencing a C-side object;
<code>updateGP</code> and <code>updateGPsep</code> do not return anything, but yields a
modified C-side object as a side effect </p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>For standard GP inference, refer to any graduate text, e.g., Rasmussen
&amp; Williams <em>Gaussian Processes for Machine Learning</em>, or
</p>
<p>Gramacy, R. B. (2020) <em>Surrogates: Gaussian Process Modeling,
Design and Optimization for the Applied Sciences</em>. Boca Raton,
Florida: Chapman Hall/CRC.  (See Chapter 6.)
<a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p>For efficient updates of GPs, see: 
</p>
<p>R.B. Gramacy and D.W. Apley (2015).
<em>Local Gaussian process approximation for large computer
experiments.</em> Journal of Computational and Graphical Statistics, 
24(2), pp. 561-678; preprint on arXiv:1303.0383;
<a href="https://arxiv.org/abs/1303.0383">https://arxiv.org/abs/1303.0383</a>
</p>


<h3>See Also</h3>

<p><code>vignette("laGP")</code>, 
<code><a href="#topic+deleteGP">deleteGP</a></code>, <code><a href="#topic+mleGP">mleGP</a></code>, <code><a href="#topic+predGP">predGP</a></code>, <code><a href="#topic+laGP">laGP</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## for more examples, see predGP and mleGP docs

## simple sine data
X &lt;- matrix(seq(0,2*pi,length=7), ncol=1)
Z &lt;- sin(X)

## new GP fit
gpi &lt;- newGP(X, Z, 2, 0.000001)

## make predictions
XX &lt;- matrix(seq(-1,2*pi+1, length=499), ncol=ncol(X))
p &lt;- predGP(gpi, XX)

## sample from the predictive distribution
if(require(mvtnorm)) {
  N &lt;- 100
  ZZ &lt;- rmvt(N, p$Sigma, p$df) 
  ZZ &lt;- ZZ + t(matrix(rep(p$mean, N), ncol=N))
  matplot(XX, t(ZZ), col="gray", lwd=0.5, lty=1, type="l", 
         xlab="x", ylab="f-hat(x)", bty="n")
  points(X, Z, pch=19, cex=2)
}

## update with four more points
X2 &lt;- matrix(c(pi/2, 3*pi/2, -0.5, 2*pi+0.5), ncol=1)
Z2 &lt;- sin(X2)
updateGP(gpi, X2, Z2)

## make a new set of predictions
p2 &lt;- predGP(gpi, XX)
if(require(mvtnorm)) {
  ZZ &lt;- rmvt(N, p2$Sigma, p2$df) 
  ZZ &lt;- ZZ + t(matrix(rep(p2$mean, N), ncol=N))
  matplot(XX, t(ZZ), col="gray", lwd=0.5, lty=1, type="l", 
         xlab="x", ylab="f-hat(x)", bty="n")
  points(X, Z, pch=19, cex=2)
  points(X2, Z2, pch=19, cex=2, col=2)
}

## clean up
deleteGP(gpi)
</code></pre>

<hr>
<h2 id='optim.auglag'>
Optimize an objective function under
multiple blackbox constraints
</h2><span id='topic+optim.auglag'></span><span id='topic+optim.efi'></span>

<h3>Description</h3>

<p>Uses a surrogate modeled augmented Lagrangian (AL) system to optimize
an objective function (blackbox or known and linear) 
under unknown multiple (blackbox) constraints via
expected improvement (EI) and variations; a comparator based
on EI with constraints is also provided
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim.auglag(fn, B, fhat = FALSE, equal = FALSE, ethresh = 1e-2, 
  slack = FALSE, cknown = NULL, start = 10, end = 100, 
  Xstart = NULL, sep = TRUE, ab = c(3/2, 8), lambda = 1, rho = NULL, 
  urate = 10, ncandf = function(t) { t }, dg.start = c(0.1, 1e-06), 
  dlim = sqrt(ncol(B)) * c(1/100, 10), Bscale = 1, ey.tol = 1e-2, 
  N = 1000, plotprog = FALSE, verb = 2, ...)
optim.efi(fn, B, fhat = FALSE, cknown = NULL, start = 10, end = 100, 
  Xstart = NULL, sep = TRUE, ab = c(3/2,8), urate = 10, 
  ncandf = function(t) { t }, dg.start = c(0.1, 1e-6), 
  dlim = sqrt(ncol(B))*c(1/100,10), Bscale = 1, plotprog = FALSE, 
  verb = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optim.auglag_+3A_fn">fn</code></td>
<td>

<p>function of an input (<code>x</code>), facilitating vectorization on a 
<code>matrix</code> <code>X</code> thereof,  returning a <code>list</code> 
with elements <code>$obj</code> containing the (scalar) objective value and <code>$c</code> 
containing a vector of evaluations of the (multiple) constraint function at <code>x</code>.
The <code>fn</code> function must take a <code>known.only</code> argument which is explained
in the note below; it need not act on that argument
</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_b">B</code></td>
<td>

<p>2-column <code>matrix</code> describing the bounding box.  The number of rows
of the <code>matrix</code> determines the input dimension 
(<code>length(x)</code> in <code>fn(x)</code>); the first column gives
lower bounds and the second gives upper bounds
</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_fhat">fhat</code></td>
<td>
<p> a scalar logical indicating if the objective function should
be modeled with a GP surrogate.  The default of <code>FALSE</code> assumes a known
linear objective scaled by <code>Bscale</code>.  Using <code>TRUE</code> is an &ldquo;alpha&rdquo;
feature at this time</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_equal">equal</code></td>
<td>
<p> an optional vector containing zeros and ones, whose length equals the number of
constraints, specifying which should be treated as equality constraints (<code>0</code>) and 
which as inequality (<code>1</code>) </p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_ethresh">ethresh</code></td>
<td>
<p> a threshold used for equality constraints to determine validity for 
progress measures; ignored if there are no equality constraints </p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_slack">slack</code></td>
<td>
<p> A scalar logical indicating if slack variables, and thus exact EI 
calculations should be used.  The default of <code>slack = FALSE</code> results in Monte 
Carlo EI approximation.  
One can optionally specify <code>slack = 2</code> to get the <code>slack = TRUE</code> behavior,
with a second-stage L-BFGS-B optimization of the EI acquisition applied at the end,
starting from the best value found on the random search grid </p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_cknown">cknown</code></td>
<td>
<p> A optional positive integer vector specifying which of the constraint
values returned by <code>fn</code> should be treated as &ldquo;known&rdquo;, i.e., not modeled
with Gaussian processes</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_start">start</code></td>
<td>

<p>positive integer giving the number of random starting locations before 
sequential design (for optimization) is performed; <code>start &gt;= 6</code> is
recommended unless <code>Xstart</code> is non-<code>NULL</code>; in the current version
the starting locations come from a space-filling design via <code><a href="tgp.html#topic+dopt.gp">dopt.gp</a></code>
</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_end">end</code></td>
<td>

<p>positive integer giving the total number of evaluations/trials in the 
optimization; must have <code>end &gt; start</code>
</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_xstart">Xstart</code></td>
<td>

<p>optional matrix of starting design locations in lieu of, or in addition to,
<code>start</code> random ones;  we recommend <code>nrow(Xstart) + start &gt;= 6</code>; also must
have <code>ncol(Xstart) = nrow(B)</code>
</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_sep">sep</code></td>
<td>

<p>The default <code>sep = TRUE</code> uses separable GPs (i.e., via <code><a href="#topic+newGPsep">newGPsep</a></code>, etc.)
to model the constraints and objective; otherwise the isotropic GPs are used
</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_ab">ab</code></td>
<td>

<p>prior parameters; see <code><a href="#topic+darg">darg</a></code> describing the prior used on the
lengthscale parameter during emulation(s) for the constraints
</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_lambda">lambda</code></td>
<td>

<p><code>m</code>-dimensional initial Lagrange multiplier parameter for <code>m</code>-constraints
</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_rho">rho</code></td>
<td>

<p>positive scalar initial quadratic penalty parameter in the augmented Lagrangian; the default setting of <code>rho = NULL</code> causes an automatic starting value to be chosen; see rejoinder to Gramacy, et al. (2016) or supplementary material to Picheny, et al. (2016)
</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_urate">urate</code></td>
<td>

<p>positive integer indicating  how many optimization trials should pass before
each MLE/MAP update is performed for GP correlation lengthscale 
parameter(s) 
</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_ncandf">ncandf</code></td>
<td>

<p>function taking a single integer indicating the optimization trial number <code>t</code>, where 
<code>start &lt; t &lt;= end</code>, and returning the number of search candidates (e.g., for
expected improvement calculations) at round <code>t</code>; the default setting
allows the number of candidates to grow linearly with <code>t</code>
</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_dg.start">dg.start</code></td>
<td>

<p>2-vector giving starting values for the lengthscale and nugget parameters
of the GP surrogate model(s) for constraints
</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_dlim">dlim</code></td>
<td>

<p>2-vector giving bounds for the lengthscale parameter(s) under MLE/MAP inference,
thereby augmenting the prior specification in <code>ab</code>
</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_bscale">Bscale</code></td>
<td>

<p>scalar indicating the relationship between the sum of the inputs, <code>sum(x)</code>, 
to <code>fn</code> and the output <code>fn(x)$obj</code>; note that at this time only linear
objectives are fully supported by the code - more details below
</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_ey.tol">ey.tol</code></td>
<td>

<p>a scalar proportion indicating how many of the EIs
at <code>ncandf(t)</code> candidate locations must be non-zero to &ldquo;trust&rdquo;
that metric to guide search, reducing to an EY-based search instead 
[choosing that proportion to be one forces EY-based search]
</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_n">N</code></td>
<td>

<p>positive scalar integer indicating the number of Monte Carlo samples to be
used for calculating EI and EY
</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_plotprog">plotprog</code></td>
<td>

<p><code>logical</code> indicating if progress plots should be made after each inner iteration;
the plots show three panels tracking the best valid objective, the EI or EY surface
over the first two input variables (requires <code><a href="interp.html#topic+interp">interp</a></code>, 
and the parameters of the lengthscale(s) of the GP(s) respectively.  When 
<code>plotprog = TRUE</code> the <code><a href="tgp.html#topic+interp.loess">interp.loess</a></code> function is used to
aid in creating surface plots, however this does not work well with fewer than 
fifteen points.  You may also provide a function as an argument, having similar
arguments/formals as <code><a href="tgp.html#topic+interp.loess">interp.loess</a></code>.  For example, we use
<code><a href="interp.html#topic+interp">interp</a></code> below, which would have been the default if not 
for licensing incompatibilities
</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_verb">verb</code></td>
<td>

<p>a non-negative integer indicating the verbosity level; the larger the value the
more that is printed to the screen
</p>
</td></tr>
<tr><td><code id="optim.auglag_+3A_...">...</code></td>
<td>
<p> additional arguments passed to <code>fn</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>These subroutines support a suite of
methods used to optimize challenging constrained problems
from Gramacy, et al. (2016); and from Picheny, et al., (2016) see references below.  
</p>
<p>Those schemes hybridize Gaussian process based surrogate modeling and expected
improvement (EI; Jones, et., al, 2008) with the additive penalty method (APM)
implemented by the augmented Lagrangian (AL, e.g., Nocedal &amp; Wright, 2006).
The goal is to minimize a (possibly known) linear objective function <code>f(x)</code> under
multiple, unknown (blackbox) constraint functions satisfying <code>c(x) &lt;= 0</code>,
which is vector-valued.  The solution here emulates the components of <code>c</code>
with Gaussian process surrogates, and guides optimization by searching the
posterior mean surface, or the EI of, the following composite objective
</p>
<p style="text-align: center;"><code class="reqn">
    Y(x) = f(x) + \lambda^\top Y_c(x) + \frac{1}{2\rho} \sum_{i=1}^m 
\max(0, Y_{c_i}(x))^2,
</code>
</p>
  
<p>where <code class="reqn">\lambda</code> and <code class="reqn">\rho</code> follow updating equations that
guarantee convergence to a valid solution minimizing the objective.  For more
details, see Gramacy, et al. (2016).
</p>
<p>A slack variable implementation that allows for exact EI calculations and can 
accommodate equality constraints, and mixed (equality and inequality) constraints,
is also provided.  For further details, see Picheny, et al. (2016).
</p>
<p>The example below illustrates a variation on the toy problem considered in both papers,
which bases sampling on EI.  For examples making used of equality constraints, 
following the Picheny, et al (2016) papers; see the demos listed in the 
&ldquo;See Also&rdquo; section below.
</p>
<p>Although it is off by default, these functions allow an unknown objective to
be modeled (<code>fhat = TRUE</code>), rather than assuming a known linear one.  For examples see
<code>demo("ALfhat")</code> and <code>demo("GSBP")</code> which illustrate the AL and comparators
in inequality and mixed constraints setups, respectively.
</p>
<p>The <code>optim.efi</code> function is provided as a comparator.  This method uses
the same underlying GP models to with the hybrid EI and probability of satisfying
the constraints heuristic from Schonlau, et al., (1998).  See <code>demo("GSBP")</code>
and <code>demo("LAH")</code> for <code>optim.efi</code> examples and comparisons between 
the original AL, the slack variable enhancement(s) on mixed constraint
problems with known and blackbox objectives, respectively
</p>


<h3>Value</h3>

<p>The output is a <code>list</code> summarizing the progress of the evaluations of the
blackbox under optimization
</p>
<table>
<tr><td><code>prog</code></td>
<td>
<p> vector giving the best valid (<code>c(x) &lt; 0</code>) value 
of the objective over the trials </p>
</td></tr>
<tr><td><code>obj</code></td>
<td>
<p> vector giving the value of the objective for the input under consideration 
at each trial </p>
</td></tr>
<tr><td><code>X</code></td>
<td>
 <p><code>matrix</code> giving the input values at which the blackbox function was
evaluated </p>
</td></tr>
<tr><td><code>C</code></td>
<td>
 <p><code>matrix</code> giving the value of the constraint function for the input 
under consideration at each trial</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
 <p><code>matrix</code> of lengthscale values obtained at the final update of the 
GP emulator for each constraint</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p> if <code>fhat = TRUE</code> then this is a <code>matrix</code> of lengthscale values
for the objective obtained at the final update of the GP emulator</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p> a <code>matrix</code> containing <code>lambda</code> vectors used in each &ldquo;outer
loop&rdquo; AL iteration </p>
</td></tr>
<tr><td><code>rho</code></td>
<td>
<p> a vector of <code>rho</code> values used in each &ldquo;outer loop&rdquo; AL iteration </p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function is under active development, especially the newest features
including separable GP surrogate modeling, surrogate modeling of a
blackbox objective, and the use of slack variables for exact EI calculations and
the support if equality constraints. Also note that, compared with earlier versions, it is now
required to augment your blackbox function (<code>fn</code>) with an argument named
<code>known.only</code>. This allows the user to specify if a potentially different
object 
(with a subset of the outputs, those that are &ldquo;known&rdquo;) gets returned in
certain circumstances. For example, the objective is treated as known in many of our
examples. When a non-null <code>cknown</code> object is used, the <code>known.only</code>
flag can be used to return only the outputs which are known.
</p>
<p>Older versions of this function provided an argument called <code>nomax</code>.
The NoMax feature is no longer supported
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Gramacy, R. B. (2020) <em>Surrogates: Gaussian Process Modeling,
Design and Optimization for the Applied Sciences</em>. Boca Raton,
Florida: Chapman Hall/CRC.  (See Chapter 7.)
<a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p>Picheny, V., Gramacy, R.B., Wild, S.M., Le Digabel, S. (2016). 
&ldquo;Bayesian optimization under mixed constraints
with a slack-variable augmented Lagrangian&rdquo;. Preprint available on arXiv:1605.09466;
<a href="https://arxiv.org/abs/1605.09466">https://arxiv.org/abs/1605.09466</a>
</p>
<p>Gramacy, R.B, Gray, G.A, Lee, H.K.H, Le Digabel, S., Ranjan P., Wells, G., Wild, S.M. (2016)
&ldquo;Modeling an Augmented Lagrangian for Improved
Blackbox Constrained Optimization&rdquo;, <em>Technometrics</em> (with discussion), 
58(1), 1-11. Preprint available on arXiv:1403.4890;
<a href="https://arxiv.org/abs/1403.4890">https://arxiv.org/abs/1403.4890</a>
</p>
<p>Jones, D., Schonlau, M., and Welch, W. J. (1998). 
&ldquo;Efficient Global Optimization of Expensive Black Box Functions.&rdquo; 
<em>Journal of Global Optimization</em>, 13, 455-492.
</p>
<p>Schonlau, M., Jones, D.R., and Welch, W. J. (1998). &ldquo;Global Versus Local Search in
Constrained Optimization of Computer Models.&rdquo; In <em>New Developments and Applications
in Experimental Design</em>, vol. 34, 11-25. Institute of Mathematical Statistics.
</p>
<p>Nocedal, J. and Wright, S.J. (2006). <em>Numerical Optimization.</em>
2nd ed. Springer.
</p>


<h3>See Also</h3>

<p><code>vignette("laGP")</code>, <code>demo("ALfhat")</code> for blackbox objective,
<code>demo("GSBP")</code> for a mixed constraints problem with blackbox objective,
<code>demo("LAH")</code> for mix constraints with known objective,
<code><a href="tgp.html#topic+optim.step.tgp">optim.step.tgp</a></code> for unconstrained optimization;
<code>optim</code> with <code>method="L-BFGS-B"</code> for box constraints, or
<code>optim</code> with <code>method="SANN"</code> for simulated annealing
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## this example assumes a known linear objective; further examples
## are in the optim.auglag demo

## a test function returning linear objective evaluations and 
## non-linear constraints
aimprob &lt;- function(X, known.only = FALSE)
{
  if(is.null(nrow(X))) X &lt;- matrix(X, nrow=1)
  f &lt;- rowSums(X)
  if(known.only) return(list(obj=f))
  c1 &lt;- 1.5-X[,1]-2*X[,2]-0.5*sin(2*pi*(X[,1]^2-2*X[,2]))
  c2 &lt;- rowSums(X^2)-1.5
  return(list(obj=f, c=cbind(c1,c2)))
}

## set bounding rectangle for adaptive sampling
B &lt;- matrix(c(rep(0,2),rep(1,2)),ncol=2)

## optimization (primarily) by EI, change 25 to 100 for
## 99% chance of finding the global optimum with value 0.6
if(require("interp")) { ## for plotprog=interp
  out &lt;- optim.auglag(aimprob, B, end=25, plotprog=interp)
} else {
  out &lt;- optim.auglag(aimprob, B, end=25)
}

## using the slack variable implementation which is a little slower
## but more precise; slack=2 augments random search with L-BFGS-B
  
out2 &lt;- optim.auglag(aimprob, B, end=25, slack=TRUE)
## Not run: 
out3 &lt;- optim.auglag(aimprob, B, end=25, slack=2)

## End(Not run)

## for more slack examples and comparison to optim.efi on problems
## involving equality and mixed (equality and inequality) constraints,
## see demo("ALfhat"), demo("GSBP") and demo("LAH")

## for comparison, here is a version that uses simulated annealing
## with the Additive Penalty Method (APM) for constraints
## Not run: 
aimprob.apm &lt;- function(x, B=matrix(c(rep(0,2),rep(1,2)),ncol=2))
{ 
  ## check bounding box
  for(i in 1:length(x)) {
    if(x[i] &lt; B[i,1] || x[i] &gt; B[i,2]) return(Inf)
  }

  ## evaluate objective and constraints
  f &lt;- sum(x)
  c1 &lt;- 1.5-x[1]-2*x[2]-0.5*sin(2*pi*(x[1]^2-2*x[2]))
  c2 &lt;- x[1]^2+x[2]^2-1.5

  ## return APM composite
  return(f + abs(c1) + abs(c2))
}

## use SA; specify control=list(maxit=100), say, to control max 
## number of iterations; does not easily facilitate plotting progress
out4 &lt;- optim(runif(2), aimprob.apm, method="SANN") 
## check the final value, which typically does not satisfy both
## constraints
aimprob(out4$par)

## End(Not run)

## for a version with a modeled objective see demo("ALfhat")
</code></pre>

<hr>
<h2 id='predGP'>
GP Prediction/Kriging
</h2><span id='topic+predGP'></span><span id='topic+predGPsep'></span>

<h3>Description</h3>

<p>Perform Gaussian processes prediction (under isotropic or separable formulation)
at new <code>XX</code> locations using a GP object stored on the C-side
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predGP(gpi, XX, lite = FALSE, nonug = FALSE)
predGPsep(gpsepi, XX, lite = FALSE, nonug = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predGP_+3A_gpi">gpi</code></td>
<td>

<p>a C-side GP object identifier (positive integer);
e.g., as returned by <code><a href="#topic+newGP">newGP</a></code>
</p>
</td></tr>
<tr><td><code id="predGP_+3A_gpsepi">gpsepi</code></td>
<td>
<p> similar to <code>gpi</code> but indicating a separable GP object,
as returned by <code><a href="#topic+newGPsep">newGPsep</a></code></p>
</td></tr>
<tr><td><code id="predGP_+3A_xx">XX</code></td>
<td>

<p>a <code>matrix</code> or <code>data.frame</code> containing
a design of predictive locations
</p>
</td></tr>
<tr><td><code id="predGP_+3A_lite">lite</code></td>
<td>

<p>a scalar logical indicating whether (<code>lite = FALSE</code>, default) or not
(<code>lite = TRUE</code>) a full predictive covariance matrix should be
returned, as would be required for plotting random sample paths,
but substantially increasing computation time if only point-prediction
is required
</p>
</td></tr>
<tr><td><code id="predGP_+3A_nonug">nonug</code></td>
<td>
<p> a scalar logical indicating if a (nonzero) nugget should be used in the predictive
equations; this allows the user to toggle between visualizations of uncertainty due just
to the mean, and a full quantification of predictive uncertainty. The latter (default <code>nonug = FALSE</code>) 
is the standard approach, but the former may work better in some sequential design contexts.  See, e.g.,
<code><a href="#topic+ieciGP">ieciGP</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns the parameters of Student-t predictive equations.  By
default, these include a full predictive covariance matrix between all
<code>XX</code> locations.  However, this can be slow when <code>nrow(XX)</code>
is large, so a <code>lite</code> options is provided, which only returns the
diagonal of that matrix.
</p>
<p>GP prediction is sometimes called &ldquo;kriging&rdquo;, especially in
the spatial statistics literature.  So this function could
also be described as returning evaluations of the &ldquo;kriging equations&rdquo;

</p>


<h3>Value</h3>

<p>The output is a <code>list</code> with the following components.
</p>
<table>
<tr><td><code>mean</code></td>
<td>
<p>a vector of predictive means of length <code>nrow(Xref)</code></p>
</td></tr>
<tr><td><code>Sigma</code></td>
<td>
<p> covariance matrix 
for a multivariate Student-t distribution; alternately
if <code>lite = TRUE</code>,
then a field <code>s2</code> contains the diagonal of this matrix</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>a Student-t degrees of freedom scalar (applies to all
<code>XX</code>)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>For standard GP prediction, refer to any graduate text, e.g., Rasmussen
&amp; Williams <em>Gaussian Processes for Machine Learning</em>, or
</p>
<p>Gramacy, R. B. (2020) <em>Surrogates: Gaussian Process Modeling,
Design and Optimization for the Applied Sciences</em>. Boca Raton,
Florida: Chapman Hall/CRC.  (See Chapter 5.)
<a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>


<h3>See Also</h3>

<p><code>vignette("laGP")</code>, 
<code><a href="#topic+newGP">newGP</a></code>, <code><a href="#topic+mleGP">mleGP</a></code>, <code><a href="#topic+jmleGP">jmleGP</a></code>, 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## a "computer experiment" -- a much smaller version than the one shown
## in the aGP docs

## Simple 2-d test function used in Gramacy &amp; Apley (2015);
## thanks to Lee, Gramacy, Taddy, and others who have used it before
f2d &lt;- function(x, y=NULL)
  {
    if(is.null(y)) {
      if(!is.matrix(x) &amp;&amp; !is.data.frame(x)) x &lt;- matrix(x, ncol=2)
      y &lt;- x[,2]; x &lt;- x[,1]
    }
    g &lt;- function(z)
      return(exp(-(z-1)^2) + exp(-0.8*(z+1)^2) - 0.05*sin(8*(z+0.1)))
    z &lt;- -g(x)*g(y)
  }

## design with N=441
x &lt;- seq(-2, 2, length=11)
X &lt;- expand.grid(x, x)
Z &lt;- f2d(X)

## fit a GP
gpi &lt;- newGP(X, Z, d=0.35, g=1/1000)

## predictive grid with NN=400
xx &lt;- seq(-1.9, 1.9, length=20)
XX &lt;- expand.grid(xx, xx)
ZZ &lt;- f2d(XX)

## predict
p &lt;- predGP(gpi, XX, lite=TRUE)
## RMSE: compare to similar experiment in aGP docs
sqrt(mean((p$mean - ZZ)^2))

## visualize the result
par(mfrow=c(1,2))
image(xx, xx, matrix(p$mean, nrow=length(xx)), col=heat.colors(128),
      xlab="x1", ylab="x2", main="predictive mean")
image(xx, xx, matrix(p$mean-ZZ, nrow=length(xx)), col=heat.colors(128),
      xlab="x1", ylab="x2", main="bas")

## clean up
deleteGP(gpi)

## see the newGP and mleGP docs for examples using lite = FALSE for
## sampling from the joint predictive distribution
</code></pre>

<hr>
<h2 id='randLine'>
Generate two-dimensional random paths
</h2><span id='topic+randLine'></span>

<h3>Description</h3>

<p>Generate two-dimensional random paths (one-dimensional manifolds in 2d)
comprising of different randomly chosen line types: linear, quadratic,
cubic, exponential, and natural logarithm. If the input dimensionality 
is higher than 2, then a line in two randomly chosen input coordinates 
is generated </p>


<h3>Usage</h3>

<pre><code class='language-R'>  randLine(a, D, N, smin, res)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="randLine_+3A_a">a</code></td>
<td>
<p> a fixed two-element vector denoting the range of the bounding box (lower bound and upper bound) of all input coordinates </p>
</td></tr>
<tr><td><code id="randLine_+3A_d">D</code></td>
<td>
<p> a scalar denoting the dimensionality of input space </p>
</td></tr>
<tr><td><code id="randLine_+3A_n">N</code></td>
<td>
<p> a scalar denoting the desired total number of random lines </p>
</td></tr>
<tr><td><code id="randLine_+3A_smin">smin</code></td>
<td>
<p> a scalar denoting the minimum absolute scaling constant, i.e., the length of the shortest line that could be generated </p>
</td></tr>
<tr><td><code id="randLine_+3A_res">res</code></td>
<td>
<p> a scalar denoting the number of data points, i.e., the resolution on the random path </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This two-dimensional random line generating function produces different
types of <code>2d</code> random paths, including linear, quadratic, cubic,
exponential, and natural logarithm.
</p>
<p>First, one of these line types is chosen uniformly at random. The line is
then drawn, via a collection of discrete points, from the origin according 
to the arguments, e.g., resolution and length, provided by the user. The
discrete set of coordinates are then shifted and scaled, uniformly at
random, into the specified 2d rectangle, e.g., <code class="reqn">[-2,2]^2</code>, with
the restriction that at least half of the points comprising the line lie
within the rectangle.
</p>
<p>For a quick visualization, see Figure 15 in Sun, et al. (2017). Figure 7 
in the same manuscript illustrates the application of this function in
out-of-sample prediction using <code><a href="#topic+laGPsep">laGPsep</a></code>, in <code>2d</code> and <code>4d</code>, respectively.
</p>
<p><code>randLine</code> returns different types of random paths and the indices of
the randomly selected pair, i.e., subset, of input coordinates (when <code>D &gt; 2</code>).
</p>


<h3>Value</h3>

<p><code>randLine</code> returns a <code>list</code> of <code>list</code>s.  The outer list is of
length six, representing each of the five possible line types (linear, quadratic, 
cubic, exponential, and natural logarithm), with the sixth entry providing the 
randomly chosen input dimensions.
</p>
<p>The inner <code>list</code>s are comprised of <code class="reqn">res \times 2</code>
<code>data.frame</code>s, the number of which span <code>N</code> samples across all
inner <code>list</code>s.
</p>


<h3>Note</h3>

<p>Users should scale each coordinate of global input space to the same coded
range, e.g., <code class="reqn">[-2,2]^D</code>, in order to avoid computational burden
caused by passing global input space argument. Users may convert back to the 
natural units when necessary.
</p>


<h3>Author(s)</h3>

<p>Furong Sun <a href="mailto:furongs@vt.edu">furongs@vt.edu</a> and Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>F. Sun, R.B. Gramacy, B. Haaland, E. Lawrence, and A. Walker (2019).
<em>Emulating satellite drag from large simulation experiments</em>,
SIAM/ASA Journal on Uncertainty Quantification, 7(2), pp. 720-759;
preprint on arXiv:1712.00182;
<a href="https://arxiv.org/abs/1712.00182">https://arxiv.org/abs/1712.00182</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+laGPsep">laGPsep</a></code>, <code><a href="#topic+aGPsep">aGPsep</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## 1. visualization of the randomly generated paths

## generate the paths
D &lt;- 4
a &lt;- c(-2, 2)
N &lt;- 30
smin &lt;- 0.1
res &lt;- 100
line.set &lt;- randLine(a=a, D=D, N=N, smin=smin, res=res)
  
## the indices of the randomly selected pair of input coordinates
d &lt;- line.set$d

## visualization

## first create an empty plot
par(mar=c(5, 4, 6, 2) + 0.1)
plot(0, xlim=a, ylim=a, type="l", xlab=paste("factor ", d[1], sep=""), 
     ylab=paste("factor ", d[2], sep=""), main="2d random paths", 
     cex.lab=1.5, cex.main=2)
abline(h=(a[1]+a[2])/2, v=(a[1]+a[2])/2, lty=2)

## merge each path type together
W &lt;- unlist(list(line.set$lin, line.set$qua, line.set$cub, line.set$ep, line.set$ln), 
  recursive=FALSE)

## calculate colors to retain 
n &lt;- unlist(lapply(line.set, length)[-6])
cols &lt;- rep(c("orange", "blue", "forestgreen", "magenta", "cornflowerblue"), n)

## plot randomly generated paths with a centering dot in red at the midway point
for(i in 1:N){
  lines(W[[i]][,1], W[[i]][,2], col=cols[i])
  points(W[[i]][res/2,1], W[[i]][res/2,2], col=2, pch=20)
}

## add legend
legend("top", legend=c("lin", "qua", "cub", "exp", "log"), cex=1.5, bty="n",
       xpd=TRUE, horiz=TRUE, inset=c(0, -0.085), lty=rep(1, 5), lwd=rep(1, 5),
       col=c("orange", "blue", "forestgreen", "magenta", "cornflowerblue"))

## 2. use the random paths for out-of-sample prediction via laGPsep

## test function (same 2d function as in other examples package)
## (ignoring 4d nature of path generation above)
f2d &lt;- function(x, y=NULL){
  if(is.null(y)){
     if(!is.matrix(x) &amp;&amp; !is.data.frame(x)) x &lt;- matrix(x, ncol=2)
     y &lt;- x[,2]; x &lt;- x[,1]
  }
  g &lt;- function(z)
  return(exp(-(z-1)^2) + exp(-0.8*(z+1)^2) - 0.05*sin(8*(z+0.1)))
  z &lt;- -g(x)*g(y)
}
    
## generate training data using 2d input space
x &lt;- seq(a[1], a[2], by=0.02)
X &lt;- as.matrix(expand.grid(x, x))
Y &lt;- f2d(X)

## example of joint path calculation folowed by RMSE calculation
## on the first random path
WW &lt;- W[[sample(1:N, 1)]]
WY &lt;- f2d(WW)

## exhaustive search via ``joint" ALC
j.exh &lt;- laGPsep(WW, 6, 100, X, Y, method="alcopt", close=10000, lite=FALSE)
sqrt(mean((WY - j.exh$mean)^2)) ## RMSE

## repeat for all thirty path elements (way too slow for checking) and other
## local design choices and visualize RMSE distribution(s) side-by-side
## Not run:    
  ## pre-allocate to save RMSE
  rmse.exh &lt;- rmse.opt &lt;- rmse.nn &lt;- rmse.pw &lt;- rmse.pwnn &lt;- rep(NA, N)
  for(t in 1:N){
     
    WW &lt;- W[[t]]
    WY &lt;- f2d(WW)
       
    ## joint local design exhaustive search via ALC
    j.exh &lt;- laGPsep(WW, 6, 100, X, Y, method="alc", close=10000, lite=FALSE)
    rmse.exh[t] &lt;- sqrt(mean((WY - j.exh$mean)^2))
     
    ## joint local design gradient-based search via ALC
    j.opt &lt;- laGPsep(WW, 6, 100, X, Y, method="alcopt", close=10000, lite=FALSE)
    rmse.opt[t] &lt;- sqrt(mean((WY - j.opt$mean)^2))
    
    ## joint local design exhaustive search via NN
    j.nn &lt;- laGPsep(WW, 6, 100, X, Y, method="nn", close=10000, lite=FALSE)
    rmse.nn[t] &lt;- sqrt(mean((WY - j.nn$mean)^2))
     
    ## pointwise local design via ALC
    pw &lt;- aGPsep(X, Y, WW, start=6, end=50, d=list(max=20), method="alc", verb=0)
    rmse.pw[t] &lt;- sqrt(mean((WY - pw$mean)^2))
     
    ## pointwise local design via NN
    pw.nn &lt;- aGPsep(X, Y, WW, start=6, end=50, d=list(max=20), method="nn", verb=0)   
    rmse.pwnn[t] &lt;- sqrt(mean((WY - pw.nn$mean)^2))
     
    ## progress meter
    print(t)
  }
  
  ## justify the y range
  ylim_RMSE &lt;- log(range(rmse.exh, rmse.opt, rmse.nn, rmse.pw, rmse.pwnn))
     
  ## plot the distribution of RMSE output
  boxplot(log(rmse.exh), log(rmse.opt), log(rmse.nn), log(rmse.pw), log(rmse.pwnn),
          xaxt='n', xlab="", ylab="log(RMSE)", ylim=ylim_RMSE, main="")
  axis(1, at=1:5, labels=c("ALC-ex", "ALC-opt", "NN", "ALC-pw", "NN-pw"), las=1)

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
