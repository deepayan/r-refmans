<!DOCTYPE html><html><head><title>Help for package Rdimtools</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Rdimtools}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#aux.gensamples'><p>Generate model-based samples</p></a></li>
<li><a href='#aux.graphnbd'><p>Construct Nearest-Neighborhood Graph</p></a></li>
<li><a href='#aux.kernelcov'><p>Build a centered kernel matrix K</p></a></li>
<li><a href='#aux.pkgstat'><p>Show the number of functions for <span class="pkg">Rdimtools</span>.</p></a></li>
<li><a href='#aux.preprocess'><p>Preprocessing the data</p></a></li>
<li><a href='#aux.shortestpath'><p>Find shortest path using Floyd-Warshall algorithm</p></a></li>
<li><a href='#do.adr'><p>Adaptive Dimension Reduction</p></a></li>
<li><a href='#do.ammc'><p>Adaptive Maximum Margin Criterion</p></a></li>
<li><a href='#do.anmm'><p>Average Neighborhood Margin Maximization</p></a></li>
<li><a href='#do.asi'><p>Adaptive Subspace Iteration</p></a></li>
<li><a href='#do.bmds'><p>Bayesian Multidimensional Scaling</p></a></li>
<li><a href='#do.bpca'><p>Bayesian Principal Component Analysis</p></a></li>
<li><a href='#do.cca'><p>Canonical Correlation Analysis</p></a></li>
<li><a href='#do.cge'><p>Constrained Graph Embedding</p></a></li>
<li><a href='#do.cisomap'><p>Conformal Isometric Feature Mapping</p></a></li>
<li><a href='#do.cnpe'><p>Complete Neighborhood Preserving Embedding</p></a></li>
<li><a href='#do.crca'><p>Curvilinear Component Analysis</p></a></li>
<li><a href='#do.crda'><p>Curvilinear Distance Analysis</p></a></li>
<li><a href='#do.crp'><p>Collaborative Representation-based Projection</p></a></li>
<li><a href='#do.cscore'><p>Constraint Score</p></a></li>
<li><a href='#do.cscoreg'><p>Constraint Score using Spectral Graph</p></a></li>
<li><a href='#do.dagdne'><p>Double-Adjacency Graphs-based Discriminant Neighborhood Embedding</p></a></li>
<li><a href='#do.disr'><p>Diversity-Induced Self-Representation</p></a></li>
<li><a href='#do.dm'><p>Diffusion Maps</p></a></li>
<li><a href='#do.dne'><p>Discriminant Neighborhood Embedding</p></a></li>
<li><a href='#do.dppca'><p>Dual Probabilistic Principal Component Analysis</p></a></li>
<li><a href='#do.dspp'><p>Discriminative Sparsity Preserving Projection</p></a></li>
<li><a href='#do.dve'><p>Distinguishing Variance Embedding</p></a></li>
<li><a href='#do.elde'><p>Exponential Local Discriminant Embedding</p></a></li>
<li><a href='#do.elpp2'><p>Enhanced Locality Preserving Projection (2013)</p></a></li>
<li><a href='#do.enet'><p>Elastic Net Regularization</p></a></li>
<li><a href='#do.eslpp'><p>Extended Supervised Locality Preserving Projection</p></a></li>
<li><a href='#do.extlpp'><p>Extended Locality Preserving Projection</p></a></li>
<li><a href='#do.fa'><p>Exploratory Factor Analysis</p></a></li>
<li><a href='#do.fastmap'><p>FastMap</p></a></li>
<li><a href='#do.fosmod'><p>Forward Orthogonal Search by Maximizing the Overall Dependency</p></a></li>
<li><a href='#do.fscore'><p>Fisher Score</p></a></li>
<li><a href='#do.fssem'><p>Feature Subset Selection using Expectation-Maximization</p></a></li>
<li><a href='#do.hydra'><p>Hyperbolic Distance Recovery and Approximation</p></a></li>
<li><a href='#do.ica'><p>Independent Component Analysis</p></a></li>
<li><a href='#do.idmap'><p>Interactive Document Map</p></a></li>
<li><a href='#do.iltsa'><p>Improved Local Tangent Space Alignment</p></a></li>
<li><a href='#do.isomap'><p>Isometric Feature Mapping</p></a></li>
<li><a href='#do.isoproj'><p>Isometric Projection</p></a></li>
<li><a href='#do.ispe'><p>Isometric Stochastic Proximity Embedding</p></a></li>
<li><a href='#do.keca'><p>Kernel Entropy Component Analysis</p></a></li>
<li><a href='#do.klde'><p>Kernel Local Discriminant Embedding</p></a></li>
<li><a href='#do.klfda'><p>Kernel Local Fisher Discriminant Analysis</p></a></li>
<li><a href='#do.klsda'><p>Kernel Locality Sensitive Discriminant Analysis</p></a></li>
<li><a href='#do.kmfa'><p>Kernel Marginal Fisher Analysis</p></a></li>
<li><a href='#do.kmmc'><p>Kernel Maximum Margin Criterion</p></a></li>
<li><a href='#do.kmvp'><p>Kernel-Weighted Maximum Variance Projection</p></a></li>
<li><a href='#do.kpca'><p>Kernel Principal Component Analysis</p></a></li>
<li><a href='#do.kqmi'><p>Kernel Quadratic Mutual Information</p></a></li>
<li><a href='#do.ksda'><p>Kernel Semi-Supervised Discriminant Analysis</p></a></li>
<li><a href='#do.kudp'><p>Kernel-Weighted Unsupervised Discriminant Projection</p></a></li>
<li><a href='#do.lamp'><p>Local Affine Multidimensional Projection</p></a></li>
<li><a href='#do.lapeig'><p>Laplacian Eigenmaps</p></a></li>
<li><a href='#do.lasso'><p>Least Absolute Shrinkage and Selection Operator</p></a></li>
<li><a href='#do.lda'><p>Linear Discriminant Analysis</p></a></li>
<li><a href='#do.ldakm'><p>Combination of LDA and K-means</p></a></li>
<li><a href='#do.lde'><p>Local Discriminant Embedding</p></a></li>
<li><a href='#do.ldp'><p>Locally Discriminating Projection</p></a></li>
<li><a href='#do.lea'><p>Locally Linear Embedded Eigenspace Analysis</p></a></li>
<li><a href='#do.lfda'><p>Local Fisher Discriminant Analysis</p></a></li>
<li><a href='#do.lisomap'><p>Landmark Isometric Feature Mapping</p></a></li>
<li><a href='#do.lle'><p>Locally Linear Embedding</p></a></li>
<li><a href='#do.llle'><p>Local Linear Laplacian Eigenmaps</p></a></li>
<li><a href='#do.llp'><p>Local Learning Projections</p></a></li>
<li><a href='#do.lltsa'><p>Linear Local Tangent Space Alignment</p></a></li>
<li><a href='#do.lmds'><p>Landmark Multidimensional Scaling</p></a></li>
<li><a href='#do.lpca2006'><p>Locally Principal Component Analysis by Yang et al. (2006)</p></a></li>
<li><a href='#do.lpe'><p>Locality Pursuit Embedding</p></a></li>
<li><a href='#do.lpfda'><p>Locality Preserving Fisher Discriminant Analysis</p></a></li>
<li><a href='#do.lpmip'><p>Locality-Preserved Maximum Information Projection</p></a></li>
<li><a href='#do.lpp'><p>Locality Preserving Projection</p></a></li>
<li><a href='#do.lqmi'><p>Linear Quadratic Mutual Information</p></a></li>
<li><a href='#do.lscore'><p>Laplacian Score</p></a></li>
<li><a href='#do.lsda'><p>Locality Sensitive Discriminant Analysis</p></a></li>
<li><a href='#do.lsdf'><p>Locality Sensitive Discriminant Feature</p></a></li>
<li><a href='#do.lsir'><p>Localized Sliced Inverse Regression</p></a></li>
<li><a href='#do.lsls'><p>Locality Sensitive Laplacian Score</p></a></li>
<li><a href='#do.lspe'><p>Locality and Similarity Preserving Embedding</p></a></li>
<li><a href='#do.lspp'><p>Local Similarity Preserving Projection</p></a></li>
<li><a href='#do.ltsa'><p>Local Tangent Space Alignment</p></a></li>
<li><a href='#do.mcfs'><p>Multi-Cluster Feature Selection</p></a></li>
<li><a href='#do.mds'><p>(Classical) Multidimensional Scaling</p></a></li>
<li><a href='#do.mfa'><p>Marginal Fisher Analysis</p></a></li>
<li><a href='#do.mifs'><p>Mutual Information for Selecting Features</p></a></li>
<li><a href='#do.mlie'><p>Maximal Local Interclass Embedding</p></a></li>
<li><a href='#do.mmc'><p>Maximum Margin Criterion</p></a></li>
<li><a href='#do.mmds'><p>Metric Multidimensional Scaling</p></a></li>
<li><a href='#do.mmp'><p>Maximum Margin Projection</p></a></li>
<li><a href='#do.mmsd'><p>Multiple Maximum Scatter Difference</p></a></li>
<li><a href='#do.modp'><p>Modified Orthogonal Discriminant Projection</p></a></li>
<li><a href='#do.msd'><p>Maximum Scatter Difference</p></a></li>
<li><a href='#do.mve'><p>Minimum Volume Embedding</p></a></li>
<li><a href='#do.mvp'><p>Maximum Variance Projection</p></a></li>
<li><a href='#do.mvu'><p>Maximum Variance Unfolding / Semidefinite Embedding</p></a></li>
<li><a href='#do.nnp'><p>Nearest Neighbor Projection</p></a></li>
<li><a href='#do.nolpp'><p>Nonnegative Orthogonal Locality Preserving Projection</p></a></li>
<li><a href='#do.nonpp'><p>Nonnegative Orthogonal Neighborhood Preserving Projections</p></a></li>
<li><a href='#do.npca'><p>Nonnegative Principal Component Analysis</p></a></li>
<li><a href='#do.npe'><p>Neighborhood Preserving Embedding</p></a></li>
<li><a href='#do.nrsr'><p>Non-convex Regularized Self-Representation</p></a></li>
<li><a href='#do.odp'><p>Orthogonal Discriminant Projection</p></a></li>
<li><a href='#do.olda'><p>Orthogonal Linear Discriminant Analysis</p></a></li>
<li><a href='#do.olpp'><p>Orthogonal Locality Preserving Projection</p></a></li>
<li><a href='#do.onpp'><p>Orthogonal Neighborhood Preserving Projections</p></a></li>
<li><a href='#do.opls'><p>Orthogonal Partial Least Squares</p></a></li>
<li><a href='#do.pca'><p>Principal Component Analysis</p></a></li>
<li><a href='#do.pfa'><p>Principal Feature Analysis</p></a></li>
<li><a href='#do.pflpp'><p>Parameter-Free Locality Preserving Projection</p></a></li>
<li><a href='#do.phate'><p>Potential of Heat Diffusion for Affinity-based Transition Embedding</p></a></li>
<li><a href='#do.plp'><p>Piecewise Laplacian-based Projection (PLP)</p></a></li>
<li><a href='#do.pls'><p>Partial Least Squares</p></a></li>
<li><a href='#do.ppca'><p>Probabilistic Principal Component Analysis</p></a></li>
<li><a href='#do.procrustes'><p>Feature Selection using PCA and Procrustes Analysis</p></a></li>
<li><a href='#do.ree'><p>Robust Euclidean Embedding</p></a></li>
<li><a href='#do.rlda'><p>Regularized Linear Discriminant Analysis</p></a></li>
<li><a href='#do.rndproj'><p>Random Projection</p></a></li>
<li><a href='#do.rpca'><p>Robust Principal Component Analysis</p></a></li>
<li><a href='#do.rpcag'><p>Robust Principal Component Analysis via Geometric Median</p></a></li>
<li><a href='#do.rsir'><p>Regularized Sliced Inverse Regression</p></a></li>
<li><a href='#do.rsr'><p>Regularized Self-Representation</p></a></li>
<li><a href='#do.sammc'><p>Semi-Supervised Adaptive Maximum Margin Criterion</p></a></li>
<li><a href='#do.sammon'><p>Sammon Mapping</p></a></li>
<li><a href='#do.save'><p>Sliced Average Variance Estimation</p></a></li>
<li><a href='#do.sda'><p>Semi-Supervised Discriminant Analysis</p></a></li>
<li><a href='#do.sdlpp'><p>Sample-Dependent Locality Preserving Projection</p></a></li>
<li><a href='#do.sir'><p>Sliced Inverse Regression</p></a></li>
<li><a href='#do.slpe'><p>Supervised Locality Pursuit Embedding</p></a></li>
<li><a href='#do.slpp'><p>Supervised Locality Preserving Projection</p></a></li>
<li><a href='#do.sne'><p>Stochastic Neighbor Embedding</p></a></li>
<li><a href='#do.spc'><p>Supervised Principal Component Analysis</p></a></li>
<li><a href='#do.spca'><p>Sparse Principal Component Analysis</p></a></li>
<li><a href='#do.spe'><p>Stochastic Proximity Embedding</p></a></li>
<li><a href='#do.specs'><p>Supervised Spectral Feature Selection</p></a></li>
<li><a href='#do.specu'><p>Unsupervised Spectral Feature Selection</p></a></li>
<li><a href='#do.splapeig'><p>Supervised Laplacian Eigenmaps</p></a></li>
<li><a href='#do.spmds'><p>Spectral Multidimensional Scaling</p></a></li>
<li><a href='#do.spp'><p>Sparsity Preserving Projection</p></a></li>
<li><a href='#do.spufs'><p>Structure Preserving Unsupervised Feature Selection</p></a></li>
<li><a href='#do.ssldp'><p>Semi-Supervised Locally Discriminant Projection</p></a></li>
<li><a href='#do.tsne'><p>t-distributed Stochastic Neighbor Embedding</p></a></li>
<li><a href='#do.udfs'><p>Unsupervised Discriminative Features Selection</p></a></li>
<li><a href='#do.udp'><p>Unsupervised Discriminant Projection</p></a></li>
<li><a href='#do.ugfs'><p>Unsupervised Graph-based Feature Selection</p></a></li>
<li><a href='#do.ulda'><p>Uncorrelated Linear Discriminant Analysis</p></a></li>
<li><a href='#do.uwdfs'><p>Uncorrelated Worst-Case Discriminative Feature Selection</p></a></li>
<li><a href='#do.wdfs'><p>Worst-Case Discriminative Feature Selection</p></a></li>
<li><a href='#est.boxcount'><p>Box-counting Dimension</p></a></li>
<li><a href='#est.clustering'><p>Intrinsic Dimension Estimation via Clustering</p></a></li>
<li><a href='#est.correlation'><p>Correlation Dimension</p></a></li>
<li><a href='#est.danco'><p>Intrinsic Dimensionality Estimation with DANCo</p></a></li>
<li><a href='#est.gdistnn'><p>Intrinsic Dimension Estimation based on Manifold Assumption and Graph Distance</p></a></li>
<li><a href='#est.incisingball'><p>Intrinsic Dimension Estimation with Incising Ball</p></a></li>
<li><a href='#est.made'><p>Manifold-Adaptive Dimension Estimation</p></a></li>
<li><a href='#est.mindkl'><p>MiNDkl</p></a></li>
<li><a href='#est.mindml'><p>MINDml</p></a></li>
<li><a href='#est.mle1'><p>Maximum Likelihood Esimation with Poisson Process</p></a></li>
<li><a href='#est.mle2'><p>Maximum Likelihood Esimation with Poisson Process and Bias Correction</p></a></li>
<li><a href='#est.nearneighbor1'><p>Intrinsic Dimension Estimation with Near-Neighbor Information</p></a></li>
<li><a href='#est.nearneighbor2'><p>Near-Neighbor Information with Bias Correction</p></a></li>
<li><a href='#est.packing'><p>Intrinsic Dimension Estimation using Packing Numbers</p></a></li>
<li><a href='#est.pcathr'><p>PCA Thresholding with Accumulated Variance</p></a></li>
<li><a href='#est.twonn'><p>Intrinsic Dimension Estimation by a Minimal Neighborhood Information</p></a></li>
<li><a href='#est.Ustat'><p>ID Estimation with Convergence Rate of U-statistic on Manifold</p></a></li>
<li><a href='#iris'><p>Load Iris data</p></a></li>
<li><a href='#oos.linproj'><p>OOS : Linear Projection</p></a></li>
<li><a href='#usps'><p>Load USPS handwritten digits data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Dimension Reduction and Estimation Methods</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.2</td>
</tr>
<tr>
<td>Description:</td>
<td>We provide linear and nonlinear dimension reduction techniques.
	Intrinsic dimension estimation methods for exploratory analysis are also provided.
	For more details on the package, see the paper by You and Shung (2022) &lt;<a href="https://doi.org/10.1016%2Fj.simpa.2022.100414">doi:10.1016/j.simpa.2022.100414</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>ADMM, CVXR (&ge; 1.0), MASS, RANN, Rcpp (&ge; 0.12.15), RcppDE,
Rdpack, RSpectra, graphics, maotai (&ge; 0.2.4), mclustcomp,
stats, utils</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, RcppDist, maotai</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.2</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.kisungyou.com/Rdimtools/">https://www.kisungyou.com/Rdimtools/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/kisungyou/Rdimtools/issues">https://github.com/kisungyou/Rdimtools/issues</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-12-15 17:48:29 UTC; kisung</td>
</tr>
<tr>
<td>Author:</td>
<td>Kisung You <a href="https://orcid.org/0000-0002-8584-459X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Changhee Suh [ctb],
  Dennis Shung [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kisung You &lt;kisungyou@outlook.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-12-15 18:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='aux.gensamples'>Generate model-based samples</h2><span id='topic+aux.gensamples'></span>

<h3>Description</h3>

<p>It generates samples from predefined shapes, set by <code>dname</code> parameter.
Also incorporated a functionality to add white noise with degree <code>noise</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aux.gensamples(
  n = 496,
  noise = 0.01,
  dname = c("swiss", "crown", "helix", "saddle", "ribbon", "bswiss", "cswiss",
    "twinpeaks", "sinusoid", "mobius", "R12in72"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aux.gensamples_+3A_n">n</code></td>
<td>
<p>the number of points to be generated.</p>
</td></tr>
<tr><td><code id="aux.gensamples_+3A_noise">noise</code></td>
<td>
<p>level of additive white noise.</p>
</td></tr>
<tr><td><code id="aux.gensamples_+3A_dname">dname</code></td>
<td>
<p>name of a predefined shape. Should be one of </p>

<dl>
<dt><code>"swiss"</code></dt><dd><p>swiss roll</p>
</dd>
<dt><code>"crown"</code></dt><dd><p>crown</p>
</dd>
<dt><code>"helix"</code></dt><dd><p>helix</p>
</dd>
<dt><code>"saddle"</code></dt><dd><p>manifold near saddle point</p>
</dd>
<dt><code>"ribbon"</code></dt><dd><p>ribbon</p>
</dd>
<dt><code>"bswiss"</code></dt><dd><p>broken swiss</p>
</dd>
<dt><code>"cswiss"</code></dt><dd><p>cut swiss</p>
</dd>
<dt><code>"twinpeaks"</code></dt><dd><p>two peaks</p>
</dd>
<dt><code>"sinusoid"</code></dt><dd><p>sinusoid on the circle</p>
</dd>
<dt><code>"mobius"</code></dt><dd><p>mobius strip embedded in <code class="reqn">\mathbf{R}^3</code></p>
</dd>
<dt><code>"R12in72"</code></dt><dd><p>12-dimensional manifold in <code class="reqn">\mathbf{R}^{12}</code></p>
</dd>
</dl>
</td></tr>
<tr><td><code id="aux.gensamples_+3A_...">...</code></td>
<td>
<p>extra parameters for the followings #' </p>

<table>
<tr>
 <td style="text-align: left;">
parameter </td><td style="text-align: left;"> dname </td><td style="text-align: left;"> description </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>ntwist</code> </td><td style="text-align: left;"> <code>"mobius"</code> </td><td style="text-align: left;"> number of twists
</td>
</tr>

</table>
</td></tr>
</table>


<h3>Value</h3>

<p>an <code class="reqn">(n\times p)</code> matrix of generated data by row. For all methods other than <code>"R12in72"</code>, it returns a matrix with <code class="reqn">p=3</code>.
</p>


<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Hein M, Audibert J (2005).
&ldquo;Intrinsic Dimensionality Estimation of Submanifolds in $R^d$.&rdquo;
In <em>Proceedings of the 22nd International Conference on Machine Learning</em>, 289&ndash;296.
</p>
<p>van der Maaten L (2009).
&ldquo;Learning a Parametric Embedding by Preserving Local Structure.&rdquo;
<em>Proceedings of AI-STATS</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generating toy example datasets
set.seed(100)
dat.swiss = aux.gensamples(50, dname="swiss")
dat.crown = aux.gensamples(50, dname="crown")
dat.helix = aux.gensamples(50, dname="helix")


</code></pre>

<hr>
<h2 id='aux.graphnbd'>Construct Nearest-Neighborhood Graph</h2><span id='topic+aux.graphnbd'></span>

<h3>Description</h3>

<p>Given data, it first computes pairwise distance (<code>method</code>) using one of measures
defined from <code><a href="stats.html#topic+dist">dist</a></code> function. Then, <code>type</code> controls how nearest neighborhood
graph should be constructed. Finally, <code>symmetric</code> parameter controls how
nearest neighborhood graph should be symmetrized.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aux.graphnbd(
  data,
  method = "euclidean",
  type = c("proportion", 0.1),
  symmetric = "union",
  pval = 2
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aux.graphnbd_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> data matrix.</p>
</td></tr>
<tr><td><code id="aux.graphnbd_+3A_method">method</code></td>
<td>
<p>type of distance to be used. See also <code><a href="stats.html#topic+dist">dist</a></code>.</p>
</td></tr>
<tr><td><code id="aux.graphnbd_+3A_type">type</code></td>
<td>
<p>a defining pattern of neighborhood criterion. One of </p>

<dl>
<dt>c(&quot;knn&quot;, k)</dt><dd><p>knn with <code>k</code> a positive integer.</p>
</dd>
<dt>c(&quot;enn&quot;, radius)</dt><dd><p>enn with a positive radius.</p>
</dd>
<dt>c(&quot;proportion&quot;, ratio)</dt><dd><p>takes an <code>ratio</code> in (0,1) portion of edges to be connected.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="aux.graphnbd_+3A_symmetric">symmetric</code></td>
<td>
<p>either &ldquo;intersect&rdquo; or &ldquo;union&rdquo; for symmetrization, or &ldquo;asymmetric&rdquo;.</p>
</td></tr>
<tr><td><code id="aux.graphnbd_+3A_pval">pval</code></td>
<td>
<p>a <code class="reqn">p</code>-norm option for Minkowski distance.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>mask</dt><dd><p>a binary matrix of indicating existence of an edge for each element.</p>
</dd>
<dt>dist</dt><dd><p>corresponding distance matrix. <code>-Inf</code> is returned for non-connecting edges.</p>
</dd>
</dl>



<h3>Nearest Neighbor(NN) search</h3>

<p>Our package supports three ways of defining nearest neighborhood. First is
<em>knn</em>, which finds <code>k</code> nearest points and flag them as neighbors.
Second is <em>enn</em> - epsilon nearest neighbor - that connects all the
data poinst within a certain radius. Finally, <em>proportion</em> flag is to
connect proportion-amount of data points sequentially from the nearest to farthest.
</p>


<h3>Symmetrization</h3>

<p>In many graph setting, it starts from dealing with undirected graphs.
NN search, however, does not necessarily guarantee if symmetric connectivity
would appear or not. There are two easy options for symmetrization;
<code>intersect</code> for connecting two nodes if both of them are
nearest neighbors of each other and <code>union</code> for only either of them to be present.
</p>


<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Generate data
set.seed(100)
X = aux.gensamples(n=100)

## Test three different types of neighborhood connectivity
nn1 = aux.graphnbd(X,type=c("knn",20))         # knn with k=20
nn2 = aux.graphnbd(X,type=c("enn",1))          # enn with radius = 1
nn3 = aux.graphnbd(X,type=c("proportion",0.4)) # connecting 40% of edges

## Visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3), pty="s")
image(nn1$mask); title("knn with k=20")
image(nn2$mask); title("enn with radius=1")
image(nn3$mask); title("proportion of ratio=0.4")
par(opar)


</code></pre>

<hr>
<h2 id='aux.kernelcov'>Build a centered kernel matrix K</h2><span id='topic+aux.kernelcov'></span>

<h3>Description</h3>

<p>From the celebrated Mercer's Theorem, we know that for a mapping <code class="reqn">\phi</code>, there exists
a kernel function - or, symmetric bilinear form, <code class="reqn">K</code> such that </p>
<p style="text-align: center;"><code class="reqn">K(x,y) = &lt;\phi(x),\phi(y)&gt;</code>
</p>
<p> where <code class="reqn">&lt;,&gt;</code> is
standard inner product. <code>aux.kernelcov</code> is a collection of 20 such positive definite kernel functions, as
well as centering of such kernel since covariance requires a mean to be subtracted and
a set of transformed values <code class="reqn">\phi(x_i),i=1,2,\dots,n</code> are not centered after transformation.
Since some kernels require parameters - up to 2, its usage will be listed in arguments section.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aux.kernelcov(X, ktype)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aux.kernelcov_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> data matrix</p>
</td></tr>
<tr><td><code id="aux.kernelcov_+3A_ktype">ktype</code></td>
<td>
<p>a vector containing the type of kernel and parameters involved. Below the usage is
consistent with description
</p>

<dl>
<dt>linear</dt><dd><p><code>c("linear",c)</code></p>
</dd>
<dt>polynomial</dt><dd><p><code>c("polynomial",c,d)</code></p>
</dd>
<dt>gaussian</dt><dd><p><code>c("gaussian",c)</code></p>
</dd>
<dt>laplacian</dt><dd><p><code>c("laplacian",c)</code></p>
</dd>
<dt>anova</dt><dd><p><code>c("anova",c,d)</code></p>
</dd>
<dt>sigmoid</dt><dd><p><code>c("sigmoid",a,b)</code></p>
</dd>
<dt>rational quadratic</dt><dd><p><code>c("rq",c)</code></p>
</dd>
<dt>multiquadric</dt><dd><p><code>c("mq",c)</code></p>
</dd>
<dt>inverse quadric</dt><dd><p><code>c("iq",c)</code></p>
</dd>
<dt>inverse multiquadric</dt><dd><p><code>c("imq",c)</code></p>
</dd>
<dt>circular</dt><dd><p><code>c("circular",c)</code></p>
</dd>
<dt>spherical</dt><dd><p><code>c("spherical",c)</code></p>
</dd>
<dt>power/triangular</dt><dd><p><code>c("power",d)</code></p>
</dd>
<dt>log</dt><dd><p><code>c("log",d)</code></p>
</dd>
<dt>spline</dt><dd><p><code>c("spline")</code></p>
</dd>
<dt>Cauchy</dt><dd><p><code>c("cauchy",c)</code></p>
</dd>
<dt>Chi-squared</dt><dd><p><code>c("chisq")</code></p>
</dd>
<dt>histogram intersection</dt><dd><p><code>c("histintx")</code></p>
</dd>
<dt>generalized histogram intersection</dt><dd><p><code>c("ghistintx",c,d)</code></p>
</dd>
<dt>generalized Student-t</dt><dd><p><code>c("t",d)</code></p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>There are 20 kernels supported. Belows are the kernels when given two vectors <code class="reqn">x,y</code>, <code class="reqn">K(x,y)</code>
</p>

<dl>
<dt>linear</dt><dd><p><code class="reqn">=&lt;x,y&gt;+c</code></p>
</dd>
<dt>polynomial</dt><dd><p><code class="reqn">=(&lt;x,y&gt;+c)^d</code></p>
</dd>
<dt>gaussian</dt><dd><p><code class="reqn">=exp(-c\|x-y\|^2)</code>, <code class="reqn">c&gt;0</code></p>
</dd>
<dt>laplacian</dt><dd><p><code class="reqn">=exp(-c\|x-y\|)</code>, <code class="reqn">c&gt;0</code></p>
</dd>
<dt>anova</dt><dd><p><code class="reqn">=\sum_k exp(-c(x_k-y_k)^2)^d</code>, <code class="reqn">c&gt;0,d\ge 1</code></p>
</dd>
<dt>sigmoid</dt><dd><p><code class="reqn">=tanh(a&lt;x,y&gt;+b)</code></p>
</dd>
<dt>rational quadratic</dt><dd><p><code class="reqn">=1-(\|x-y\|^2)/(\|x-y\|^2+c)</code></p>
</dd>
<dt>multiquadric</dt><dd><p><code class="reqn">=\sqrt{\|x-y\|^2 + c^2}</code></p>
</dd>
<dt>inverse quadric</dt><dd><p><code class="reqn">=1/(\|x-y\|^2+c^2)</code></p>
</dd>
<dt>inverse multiquadric</dt><dd><p><code class="reqn">=1/\sqrt{\|x-y\|^2+c^2}</code></p>
</dd>
<dt>circular</dt><dd><p><code class="reqn">=
\frac{2}{\pi} arccos(-\frac{\|x-y\|}{c}) - \frac{2}{\pi} \frac{\|x-y\|}{c}\sqrt{1-(\|x-y\|/c)^2}
</code>, <code class="reqn">c&gt;0</code></p>
</dd>
<dt>spherical</dt><dd><p><code class="reqn">=
1-1.5\frac{\|x-y\|}{c}+0.5(\|x-y\|/c)^3
</code>, <code class="reqn">c&gt;0</code></p>
</dd>
<dt>power/triangular</dt><dd><p><code class="reqn">=-\|x-y\|^d</code>, <code class="reqn">d\ge 1</code></p>
</dd>
<dt>log</dt><dd><p><code class="reqn">=-\log (\|x-y\|^d+1)</code></p>
</dd>
<dt>spline</dt><dd><p><code class="reqn">=
\prod_i (
1+x_i y_i(1+min(x_i,y_i)) - \frac{x_i + y_i}{2} min(x_i,y_i)^2
+ \frac{min(x_i,y_i)^3}{3}
)
</code></p>
</dd>
<dt>Cauchy</dt><dd><p><code class="reqn">=\frac{c^2}{c^2+\|x-y\|^2}</code></p>
</dd>
<dt>Chi-squared</dt><dd><p><code class="reqn">=\sum_i \frac{2x_i y_i}{x_i+y_i}</code></p>
</dd>
<dt>histogram intersection</dt><dd><p><code class="reqn">=\sum_i min(x_i,y_i)</code></p>
</dd>
<dt>generalized histogram intersection</dt><dd><p><code class="reqn">=sum_i min(
|x_i|^c,|y_i|^d
)</code></p>
</dd>
<dt>generalized Student-t</dt><dd><p><code class="reqn">=1/(1+\|x-y\|^d)</code>, <code class="reqn">d\ge 1</code></p>
</dd>
</dl>



<h3>Value</h3>

<p>a named list containing </p>

<dl>
<dt>K</dt><dd><p>a <code class="reqn">(p\times p)</code> kernelizd gram matrix.</p>
</dd>
<dt>Kcenter</dt><dd><p>a <code class="reqn">(p\times p)</code> centered version of <code>K</code>.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Hofmann, T., Scholkopf, B., and Smola, A.J. (2008) <em>Kernel methods in
machine learning</em>. arXiv:math/0701907.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate a toy data
set.seed(100)
X = aux.gensamples(n=100)

## compute a few kernels
Klin = aux.kernelcov(X, ktype=c("linear",0))
Kgau = aux.kernelcov(X, ktype=c("gaussian",1))
Klap = aux.kernelcov(X, ktype=c("laplacian",1))

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3), pty="s")
image(Klin$K, main="kernel=linear")
image(Kgau$K, main="kernel=gaussian")
image(Klap$K, main="kernel=laplacian")
par(opar)



</code></pre>

<hr>
<h2 id='aux.pkgstat'>Show the number of functions for <span class="pkg">Rdimtools</span>.</h2><span id='topic+aux.pkgstat'></span>

<h3>Description</h3>

<p>This function is mainly used for tracking progress for this package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aux.pkgstat()
</code></pre>


<h3>Examples</h3>

<pre><code class='language-R'>## run with following command
aux.pkgstat()

</code></pre>

<hr>
<h2 id='aux.preprocess'>Preprocessing the data</h2><span id='topic+aux.preprocess'></span>

<h3>Description</h3>

<p><code>aux.preprocess</code> can perform one of following operations; <code>"center"</code>, <code>"scale"</code>,
<code>"cscale"</code>, <code>"decorrelate"</code> and <code>"whiten"</code>. See below for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aux.preprocess(
  data,
  type = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aux.preprocess_+3A_data">data</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="aux.preprocess_+3A_type">type</code></td>
<td>
<p>one of <code>"center"</code>, <code>"scale"</code>, <code>"cscale"</code>, <code>"decorrelate"</code> or <code>"whiten"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>named list containing:
</p>

<dl>
<dt>pX</dt><dd><p>an <code class="reqn">(n\times p)</code> matrix after preprocessing in accordance with <code>type</code> parameter</p>
</dd>
<dt>info</dt><dd><p>a list containing </p>

<ul>
<li> <p><code>type:</code> name of preprocessing procedure.
</p>
</li>
<li> <p><code>mean:</code> a mean vector of length <code class="reqn">p</code>.
</p>
</li>
<li> <p><code>multiplier:</code> a <code class="reqn">(p\times p)</code> matrix or 1 for &quot;center&quot;.</p>
</li></ul>
</dd>
</dl>



<h3>Operations</h3>

<p>we have following operations,
</p>

<dl>
<dt><code>"center"</code></dt><dd><p>subtracts mean of each column so that every variable has mean <code class="reqn">0</code>.</p>
</dd>
<dt><code>"scale"</code></dt><dd><p>turns each column corresponding to variable have variance <code class="reqn">1</code>.</p>
</dd>
<dt><code>"cscale"</code></dt><dd><p>combines <code>"center"</code> and <code>"scale"</code>.</p>
</dd>
<dt><code>"decorrelate"</code></dt><dd><p><code>"center"</code> and sets its covariance term having diagonal entries only.</p>
</dd>
<dt><code>"whiten"</code></dt><dd><p><code>"decorrelate"</code> and sets all diagonal elements be <code class="reqn">1</code>.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Generate data
set.seed(100)
X = aux.gensamples(n=200)

## 5 types of preprocessing
X_center = aux.preprocess(X)
X_scale  = aux.preprocess(X,type="scale")
X_cscale = aux.preprocess(X,type="cscale")
X_decorr = aux.preprocess(X,type="decorrelate")
X_whiten = aux.preprocess(X,type="whiten")


</code></pre>

<hr>
<h2 id='aux.shortestpath'>Find shortest path using Floyd-Warshall algorithm</h2><span id='topic+aux.shortestpath'></span>

<h3>Description</h3>

<p>This is a fast implementation of Floyd-Warshall algorithm to find the
shortest path in a pairwise sense using 'RcppArmadillo'. A logical input
is also accepted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aux.shortestpath(dist)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aux.shortestpath_+3A_dist">dist</code></td>
<td>
<p>either an <code class="reqn">(n\times n)</code> matrix or a <code>dist</code> class object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an <code class="reqn">(n\times n)</code> matrix containing pairwise shortest path.
</p>


<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Floyd, R.W. (1962) <em>Algorithm 97: Shortest Path</em>. Commincations of the ACMS, Vol.5(6):345.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate a toy data
X = aux.gensamples(n=10)

## Find knn graph with k=5
Xgraph = aux.graphnbd(X,type=c("knn",5))

## Separately use binarized and real distance matrices
W1 = aux.shortestpath(Xgraph$mask)
W2 = aux.shortestpath(Xgraph$dist)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2), pty="s")
image(W1, main="from binarized")
image(W2, main="from Euclidean distance")
par(opar)


</code></pre>

<hr>
<h2 id='do.adr'>Adaptive Dimension Reduction</h2><span id='topic+do.adr'></span>

<h3>Description</h3>

<p>Adaptive Dimension Reduction (Ding et al. 2002) iteratively finds the best subspace to perform data clustering. It can be regarded as
one of remedies for clustering in high dimensional space. Eigenvectors of a between-cluster scatter matrix are used
as basis of projection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.adr(X, ndim = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.adr_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.adr_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.adr_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>maxiter</dt><dd><p>maximum number of iterations (default: 100).</p>
</dd>
<dt>abstol</dt><dd><p>absolute tolerance stopping criterion (default: 1e-8).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Ding C, Xiaofeng He, Hongyuan Zha, Simon HD (2002).
&ldquo;Adaptive Dimension Reduction for Clustering High Dimensional Data.&rdquo;
In <em>Proceedings 2002 IEEE International Conference on Data Mining</em>, 147&ndash;154.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.ldakm">do.ldakm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## compare ADR with other methods
outADR = do.adr(X)
outPCA = do.pca(X)
outLDA = do.lda(X, label)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(outADR$Y, col=label, pch=19, main="ADR")
plot(outPCA$Y, col=label, pch=19, main="PCA")
plot(outLDA$Y, col=label, pch=19, main="LDA")
par(opar)


</code></pre>

<hr>
<h2 id='do.ammc'>Adaptive Maximum Margin Criterion</h2><span id='topic+do.ammc'></span>

<h3>Description</h3>

<p>Adaptive Maximum Margin Criterion (AMMC) is a supervised linear dimension reduction method.
The method uses different weights to characterize the different contributions of the
training samples embedded in MMC framework. With the choice of  <code>a=0</code>, <code>b=0</code>, and
<code>lambda=1</code>, it is identical to standard MMC method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.ammc(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  a = 1,
  b = 1,
  lambda = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.ammc_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.ammc_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.ammc_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.ammc_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.ammc_+3A_a">a</code></td>
<td>
<p>tuning parameter for between-class weight in <code class="reqn">[0,\infty)</code>.</p>
</td></tr>
<tr><td><code id="do.ammc_+3A_b">b</code></td>
<td>
<p>tuning parameter for within-class weight in <code class="reqn">[0,\infty)</code>.</p>
</td></tr>
<tr><td><code id="do.ammc_+3A_lambda">lambda</code></td>
<td>
<p>balance parameter for between-class and within-class scatter matrices in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Lu J, Tan Y (2011).
&ldquo;Adaptive Maximum Margin Criterion for Image Classification.&rdquo;
In <em>2011 IEEE International Conference on Multimedia and Expo</em>, 1&ndash;6.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.mmc">do.mmc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## try different lambda values
out1 = do.ammc(X, label, lambda=0.1)
out2 = do.ammc(X, label, lambda=1)
out3 = do.ammc(X, label, lambda=10)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="AMMC::lambda=0.1", pch=19, cex=0.5, col=label)
plot(out2$Y, main="AMMC::lambda=1",   pch=19, cex=0.5, col=label)
plot(out3$Y, main="AMMC::lambda=10",  pch=19, cex=0.5, col=label)
par(opar)

</code></pre>

<hr>
<h2 id='do.anmm'>Average Neighborhood Margin Maximization</h2><span id='topic+do.anmm'></span>

<h3>Description</h3>

<p>Average Neighborhood Margin Maximization (ANMM) is a supervised method
for feature extraction. It aims to find a projection mapping in the following manner;
for each data point, the algorithm tries to pull the neighboring points in the
same class while pushing neighboring points of different classes far away. It is known
that ANMM does suffer less from small sample size problem, which is bottleneck for LDA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.anmm(
  X,
  label,
  ndim = 2,
  preprocess = c("null", "center", "scale", "cscale", "decorrelate", "whiten"),
  No = ceiling(nrow(X)/10),
  Ne = ceiling(nrow(X)/10)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.anmm_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.anmm_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.anmm_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.anmm_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.anmm_+3A_no">No</code></td>
<td>
<p>neighborhood size for same-class data points; either a constant number or
a vector of length-<code class="reqn">n</code> can be provided, as long as the values reside in <code class="reqn">[2,n]</code>.</p>
</td></tr>
<tr><td><code id="do.anmm_+3A_ne">Ne</code></td>
<td>
<p>neighborhood size for different-class data points; either a constant number or
a vector of length-<code class="reqn">n</code> can be provided, as long as the values reside in <code class="reqn">[2,n]</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Wang F, Zhang C (2007).
&ldquo;Feature Extraction by Maximizing the Average Neighborhood Margin.&rdquo;
In <em>2007 IEEE Conference on Computer Vision and Pattern Recognition</em>, 1&ndash;8.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## perform ANMM on different choices of neighborhood size
out1 = do.anmm(X, label, No=6, Ne=6)
out2 = do.anmm(X, label, No=2, Ne=10)
out3 = do.anmm(X, label, No=10,Ne=2)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="(No,Ne)=(6,6)",  pch=19, cex=0.5, col=label)
plot(out2$Y, main="(No,Ne)=(2,10)", pch=19, cex=0.5, col=label)
plot(out3$Y, main="(No,Ne)=(10,2)", pch=19, cex=0.5, col=label)
par(opar)

</code></pre>

<hr>
<h2 id='do.asi'>Adaptive Subspace Iteration</h2><span id='topic+do.asi'></span>

<h3>Description</h3>

<p>Adaptive Subspace Iteration (ASI) iteratively finds the best subspace to perform data clustering. It can be regarded as
one of remedies for clustering in high dimensional space. Eigenvectors of a within-cluster scatter matrix are used
as basis of projection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.asi(X, ndim = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.asi_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.asi_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.asi_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>maxiter</dt><dd><p>maximum number of iterations (default: 100).</p>
</dd>
<dt>abstol</dt><dd><p>absolute tolerance stopping criterion (default: 1e-8).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Li T, Ma S, Ogihara M (2004).
&ldquo;Document Clustering via Adaptive Subspace Iteration.&rdquo;
In <em>Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, 218.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.ldakm">do.ldakm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
data(iris, package="Rdimtools")
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## compare ASI with other methods
outASI = do.asi(X)
outPCA = do.pca(X)
outLDA = do.lda(X, label)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(outASI$Y, pch=19, col=label, main="ASI")
plot(outPCA$Y, pch=19, col=label, main="PCA")
plot(outLDA$Y, pch=19, col=label, main="LDA")
par(opar)


</code></pre>

<hr>
<h2 id='do.bmds'>Bayesian Multidimensional Scaling</h2><span id='topic+do.bmds'></span>

<h3>Description</h3>

<p>A Bayesian formulation of classical Multidimensional Scaling is presented.
Even though this method is based on MCMC sampling, we only return maximum a posterior (MAP) estimate
that maximizes the posterior distribution. Due to its nature without any special tuning,
increasing <code>mc.iter</code> requires much computation. A note on the method is that
this algorithm does not return an explicit form of projection matrix so it's
classified in our package as a nonlinear method. Also, automatic dimension selection is not supported
for simplicity as well as consistency with other methods in the package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.bmds(
  X,
  ndim = 2,
  par.a = 5,
  par.alpha = 0.5,
  par.step = 1,
  mc.iter = 50,
  print.progress = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.bmds_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.bmds_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.bmds_+3A_par.a">par.a</code></td>
<td>
<p>hyperparameter for conjugate prior on variance term, i.e., <code class="reqn">\sigma^2 \sim IG(a,b)</code>. Note that <code class="reqn">b</code> is chosen appropriately as in paper.</p>
</td></tr>
<tr><td><code id="do.bmds_+3A_par.alpha">par.alpha</code></td>
<td>
<p>hyperparameter for conjugate prior on diagonal term, i.e., <code class="reqn">\lambda_j \sim IG(\alpha, \beta_j)</code>. Note that <code class="reqn">\beta_j</code> is chosen appropriately as in paper.</p>
</td></tr>
<tr><td><code id="do.bmds_+3A_par.step">par.step</code></td>
<td>
<p>stepsize for random-walk, which is standard deviation of Gaussian proposal.</p>
</td></tr>
<tr><td><code id="do.bmds_+3A_mc.iter">mc.iter</code></td>
<td>
<p>the number of MCMC iterations.</p>
</td></tr>
<tr><td><code id="do.bmds_+3A_print.progress">print.progress</code></td>
<td>
<p>a logical; <code>TRUE</code> to show iterations, <code>FALSE</code> otherwise (default: <code>FALSE</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Oh M, Raftery AE (2001).
&ldquo;Bayesian Multidimensional Scaling and Choice of Dimension.&rdquo;
<em>Journal of the American Statistical Association</em>, <b>96</b>(455), 1031&ndash;1044.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## compare with other methods
outBMD &lt;- do.bmds(X, ndim=2)
outPCA &lt;- do.pca(X, ndim=2)
outLDA &lt;- do.lda(X, label, ndim=2)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(outBMD$Y, pch=19, col=label, main="Bayesian MDS")
plot(outPCA$Y, pch=19, col=label, main="PCA")
plot(outLDA$Y, pch=19, col=label, main="LDA")
par(opar)


</code></pre>

<hr>
<h2 id='do.bpca'>Bayesian Principal Component Analysis</h2><span id='topic+do.bpca'></span>

<h3>Description</h3>

<p>Bayesian PCA (BPCA) is a further variant of PCA in that it imposes prior and encodes
basis selection mechanism. Even though the model is fully Bayesian, <code>do.bpca</code>
faithfully follows the original paper by Bishop in that it only returns the mode value
of posterior as an estimate, in conjunction with ARD-motivated prior as well as
consideration of variance to be estimated. Unlike PPCA, it uses full basis and returns
relative weight for each base in that the smaller <code class="reqn">\alpha</code> value is, the more likely
corresponding column vector of <code>mp.W</code> to be selected as potential basis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.bpca(X, ndim = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.bpca_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.bpca_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.bpca_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>maxiter</dt><dd><p>maximum number of iterations (default: 100).</p>
</dd>
<dt>reltol</dt><dd><p>relative tolerance stopping criterion (default: 1e-4).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>mp.itercount</dt><dd><p>the number of iterations taken for EM algorithm to converge.</p>
</dd>
<dt>mp.sigma2</dt><dd><p>estimated <code class="reqn">\sigma^2</code> value via EM algorithm.</p>
</dd>
<dt>mp.alpha</dt><dd><p>length-<code>ndim-1</code> vector of relative weight for each base in <code>mp.W</code>.</p>
</dd>
<dt>mp.W</dt><dd><p>an <code class="reqn">(ndim\times ndim-1)</code> matrix from EM update.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Bishop C (1999).
&ldquo;Bayesian PCA.&rdquo;
In <em>Advances in Neural Information Processing Systems</em>, volume 11, 382&ndash;388.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.pca">do.pca</a></code>, <code><a href="#topic+do.ppca">do.ppca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## use iris dataset
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
lab   = as.factor(iris[subid,5])

## compare BPCA with others
out1  &lt;- do.bpca(X, ndim=2)
out2  &lt;- do.pca(X,  ndim=2)
out3  &lt;- do.lda(X, lab, ndim=2)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=lab, pch=19, cex=0.8, main="Bayesian PCA")
plot(out2$Y, col=lab, pch=19, cex=0.8, main="PCA")
plot(out3$Y, col=lab, pch=19, cex=0.8, main="LDA")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='do.cca'>Canonical Correlation Analysis</h2><span id='topic+do.cca'></span>

<h3>Description</h3>

<p>Canonical Correlation Analysis (CCA) is similar to Partial Least Squares (PLS), except for one objective; while PLS focuses
on maximizing covariance, CCA maximizes the correlation. This difference sometimes incurs quite distinct results compared to PLS.
For algorithm aspects, we used recursive gram-schmidt orthogonalization in conjunction with extracting projection vectors under
eigen-decomposition formulation, as the problem dimension matters only up to original dimensionality.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.cca(data1, data2, ndim = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.cca_+3A_data1">data1</code></td>
<td>
<p>an <code class="reqn">(n\times N)</code> data matrix whose rows are observations</p>
</td></tr>
<tr><td><code id="do.cca_+3A_data2">data2</code></td>
<td>
<p>an <code class="reqn">(n\times M)</code> data matrix whose rows are observations</p>
</td></tr>
<tr><td><code id="do.cca_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y1</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix of projected observations from <code>data1</code>.</p>
</dd>
<dt>Y2</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix of projected observations from <code>data2</code>.</p>
</dd>
<dt>projection1</dt><dd><p>a <code class="reqn">(N\times ndim)</code> whose columns are loadings for <code>data1</code>.</p>
</dd>
<dt>projection2</dt><dd><p>a <code class="reqn">(M\times ndim)</code> whose columns are loadings for <code>data2</code>.</p>
</dd>
<dt>trfinfo1</dt><dd><p>a list containing information for out-of-sample prediction for <code>data1</code>.</p>
</dd>
<dt>trfinfo2</dt><dd><p>a list containing information for out-of-sample prediction for <code>data2</code>.</p>
</dd>
<dt>eigvals</dt><dd><p>a vector of eigenvalues for iterative decomposition.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Hotelling H (1936).
&ldquo;RELATIONS BETWEEN TWO SETS OF VARIATES.&rdquo;
<em>Biometrika</em>, <b>28</b>(3-4), 321&ndash;377.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.pls">do.pls</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate 2 normal data matrices
set.seed(100)
mat1 = matrix(rnorm(100*12),nrow=100)+10 # 12-dim normal
mat2 = matrix(rnorm(100*6), nrow=100)-10 # 6-dim normal

## project onto 2 dimensional space for each data
output = do.cca(mat1, mat2, ndim=2)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(output$Y1, main="proj(mat1)")
plot(output$Y2, main="proj(mat2)")
par(opar)

</code></pre>

<hr>
<h2 id='do.cge'>Constrained Graph Embedding</h2><span id='topic+do.cge'></span>

<h3>Description</h3>

<p>Constrained Graph Embedding (CGE) is a semi-supervised embedding method that incorporates
partially available label information into the graph structure that find embeddings
consistent with the labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.cge(
  X,
  label,
  ndim = 2,
  type = c("proportion", 0.1),
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.cge_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations</p>
</td></tr>
<tr><td><code id="do.cge_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels. It should contain <code>NA</code> elements for missing label.</p>
</td></tr>
<tr><td><code id="do.cge_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.cge_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.cge_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is <code>"null"</code>. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>He X, Ji M, Bao H (2009).
&ldquo;Graph Embedding with Constraints.&rdquo;
In <em>IJCAI</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris data
data(iris)
X     = as.matrix(iris[,2:4])
label = as.integer(iris[,5])
lcols = as.factor(label)

## copy a label and let 10% of elements be missing
nlabel = length(label)
nmissing = round(nlabel*0.10)
label_missing = label
label_missing[sample(1:nlabel, nmissing)]=NA

## try different neighborhood sizes
out1 = do.cge(X, label_missing, type=c("proportion",0.10))
out2 = do.cge(X, label_missing, type=c("proportion",0.25))
out3 = do.cge(X, label_missing, type=c("proportion",0.50))

## visualize
opar = par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="10% connected",  pch=19, col=lcols)
plot(out2$Y, main="25% connected", pch=19, col=lcols)
plot(out3$Y, main="50% connected", pch=19, col=lcols)
par(opar)

</code></pre>

<hr>
<h2 id='do.cisomap'>Conformal Isometric Feature Mapping</h2><span id='topic+do.cisomap'></span>

<h3>Description</h3>

<p>Conformal Isomap(C-Isomap) is a variant of a celebrated method of Isomap. It aims at, rather than
preserving full isometry, maintaining infinitestimal angles - conformality - in that
it alters geodesic distance to reflect scale information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.cisomap(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  symmetric = c("union", "intersect", "asymmetric"),
  weight = TRUE,
  preprocess = c("center", "scale", "cscale", "whiten", "decorrelate")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.cisomap_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.cisomap_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.cisomap_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.cisomap_+3A_symmetric">symmetric</code></td>
<td>
<p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.cisomap_+3A_weight">weight</code></td>
<td>
<p><code>TRUE</code> to perform Isomap on weighted graph, or <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="do.cisomap_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Silva VD, Tenenbaum JB (2003).
&ldquo;Global Versus Local Methods in Nonlinear Dimensionality Reduction.&rdquo;
In Becker S, Thrun S, Obermayer K (eds.), <em>Advances in Neural Information Processing Systems 15</em>, 721&ndash;728.
MIT Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate data
set.seed(100)
X &lt;- aux.gensamples(dname="cswiss",n=100)

## 1. original Isomap
output1 &lt;- do.isomap(X,ndim=2)

## 2. C-Isomap
output2 &lt;- do.cisomap(X,ndim=2)

## 3. C-Isomap on a binarized graph
output3 &lt;- do.cisomap(X,ndim=2,weight=FALSE)

## Visualize three different projections
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(output1$Y, main="Isomap")
plot(output2$Y, main="C-Isomap")
plot(output3$Y, main="Binarized C-Isomap")
par(opar)


</code></pre>

<hr>
<h2 id='do.cnpe'>Complete Neighborhood Preserving Embedding</h2><span id='topic+do.cnpe'></span>

<h3>Description</h3>

<p>One of drawbacks of Neighborhood Preserving Embedding (NPE) is the small-sample-size problem
under high-dimensionality of original data, where singular matrices to be decomposed suffer from
rank deficiency. Instead of applying PCA as a preprocessing step, Complete NPE (CNPE) transforms the
singular generalized eigensystem computation of NPE into two eigenvalue decomposition problems.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.cnpe(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.cnpe_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.cnpe_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.cnpe_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.cnpe_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Wang Y, Wu Y (2010).
&ldquo;Complete Neighborhood Preserving Embedding for Face Recognition.&rdquo;
<em>Pattern Recognition</em>, <b>43</b>(3), 1008&ndash;1015.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate data of 3 types with clear difference
dt1  = aux.gensamples(n=20)-50
dt2  = aux.gensamples(n=20)
dt3  = aux.gensamples(n=20)+50
lab  = rep(1:3, each=20)

## merge the data
X      = rbind(dt1,dt2,dt3)

## try different numbers for neighborhood size
out1 = do.cnpe(X, type=c("proportion",0.10))
out2 = do.cnpe(X, type=c("proportion",0.25))
out3 = do.cnpe(X, type=c("proportion",0.50))

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=lab, pch=19, main="CNPE::10% connected")
plot(out2$Y, col=lab, pch=19, main="CNPE::25% connected")
plot(out3$Y, col=lab, pch=19, main="CNPE::50% connected")
par(opar)


</code></pre>

<hr>
<h2 id='do.crca'>Curvilinear Component Analysis</h2><span id='topic+do.crca'></span>

<h3>Description</h3>

<p>Curvilinear Component Analysis (CRCA) is a type of self-organizing algorithms for
manifold learning. Like MDS, it aims at minimizing a cost function (<em>Stress</em>)
based on pairwise proximity. Parameter <code>lambda</code> is a heaviside function for
penalizing distance pair of embedded data, and <code>alpha</code> controls learning rate
similar to that of subgradient method in that at each iteration <code class="reqn">t</code> the gradient is
weighted by <code class="reqn">\alpha /t</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.crca(X, ndim = 2, lambda = 1, alpha = 1, maxiter = 1000, tolerance = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.crca_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.crca_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.crca_+3A_lambda">lambda</code></td>
<td>
<p>threshold value.</p>
</td></tr>
<tr><td><code id="do.crca_+3A_alpha">alpha</code></td>
<td>
<p>initial value for updating.</p>
</td></tr>
<tr><td><code id="do.crca_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations allowed.</p>
</td></tr>
<tr><td><code id="do.crca_+3A_tolerance">tolerance</code></td>
<td>
<p>stopping criterion for maximum absolute discrepancy between two distance matrices.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>niter</dt><dd><p>the number of iterations until convergence.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Demartines P, Herault J (1997).
&ldquo;Curvilinear Component Analysis: A Self-Organizing Neural Network for Nonlinear Mapping of Data Sets.&rdquo;
<em>IEEE Transactions on Neural Networks</em>, <b>8</b>(1), 148&ndash;154.
</p>
<p>Hérault J, Jausions-Picaud C, Guérin-Dugué A (1999).
&ldquo;Curvilinear Component Analysis for High-Dimensional Data Representation: I. Theoretical Aspects and Practical Use in the Presence of Noise.&rdquo;
In Goos G, Hartmanis J, van Leeuwen J, Mira J, Sánchez-Andrés JV (eds.), <em>Engineering Applications of Bio-Inspired Artificial Neural Networks</em>, volume 1607, 625&ndash;634.
Springer Berlin Heidelberg, Berlin, Heidelberg.
ISBN 978-3-540-66068-2 978-3-540-48772-2.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.crda">do.crda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## different initial learning rates
out1 &lt;- do.crca(X,alpha=1)
out2 &lt;- do.crca(X,alpha=5)
out3 &lt;- do.crca(X,alpha=10)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=label, pch=19, main="alpha=1.0")
plot(out2$Y, col=label, pch=19, main="alpha=5.0")
plot(out3$Y, col=label, pch=19, main="alpha=10.0")
par(opar)

</code></pre>

<hr>
<h2 id='do.crda'>Curvilinear Distance Analysis</h2><span id='topic+do.crda'></span>

<h3>Description</h3>

<p>Curvilinear Distance Analysis (CRDA) is a variant of Curvilinear Component Analysis in that
the input pairwise distance is altered by curvilinear distance on a data manifold.
Like in Isomap, it first generates <em>neighborhood graph</em> and finds <em>shortest path</em> on
a constructed graph so that the shortest-path length plays as an approximate geodesic distance on
nonlinear manifolds.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.crda(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  symmetric = "union",
  weight = TRUE,
  lambda = 1,
  alpha = 1,
  maxiter = 1000,
  tolerance = 1e-06
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.crda_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.crda_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.crda_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.crda_+3A_symmetric">symmetric</code></td>
<td>
<p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.crda_+3A_weight">weight</code></td>
<td>
<p><code>TRUE</code> to perform CRDA on weighted graph, or <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="do.crda_+3A_lambda">lambda</code></td>
<td>
<p>threshold value.</p>
</td></tr>
<tr><td><code id="do.crda_+3A_alpha">alpha</code></td>
<td>
<p>initial value for updating.</p>
</td></tr>
<tr><td><code id="do.crda_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations allowed.</p>
</td></tr>
<tr><td><code id="do.crda_+3A_tolerance">tolerance</code></td>
<td>
<p>stopping criterion for maximum absolute discrepancy between two distance matrices.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>niter</dt><dd><p>the number of iterations until convergence.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Lee JA, Lendasse A, Verleysen M (2002).
&ldquo;Curvilinear Distance Analysis versus Isomap.&rdquo;
In <em>ESANN</em>.
</p>
<p>Lee JA, Lendasse A, Verleysen M (2004).
&ldquo;Nonlinear Projection with Curvilinear Distances: Isomap versus Curvilinear Distance Analysis.&rdquo;
<em>Neurocomputing</em>, <b>57</b>, 49&ndash;76.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.isomap">do.isomap</a></code>, <code><a href="#topic+do.crca">do.crca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## different settings of connectivity
out1 &lt;- do.crda(X, type=c("proportion",0.10))
out2 &lt;- do.crda(X, type=c("proportion",0.25))
out3 &lt;- do.crda(X, type=c("proportion",0.50))

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=label, pch=19, main="10% connected")
plot(out2$Y, col=label, pch=19, main="25% connected")
plot(out3$Y, col=label, pch=19, main="50% connected")
par(opar)

</code></pre>

<hr>
<h2 id='do.crp'>Collaborative Representation-based Projection</h2><span id='topic+do.crp'></span>

<h3>Description</h3>

<p>Collaborative Representation-based Projection (CRP) is an unsupervised linear
dimension reduction method. Its embedding is based on <code class="reqn">\ell</code>_2 graph construction,
similar to that of SPP where sparsity constraint is imposed via <code class="reqn">\ell_1</code> optimization problem.
Note that though it may be way faster, rank deficiency can pose a great deal of problems,
especially when the dataset is large.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.crp(
  X,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  lambda = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.crp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations</p>
</td></tr>
<tr><td><code id="do.crp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.crp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.crp_+3A_lambda">lambda</code></td>
<td>
<p>regularization parameter for constructing <code class="reqn">\ell_2</code> graph.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Yang W, Wang Z, Sun C (2015).
&ldquo;A Collaborative Representation Based Projections Method for Feature Extraction.&rdquo;
<em>Pattern Recognition</em>, <b>48</b>(1), 20&ndash;27.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.spp">do.spp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris dataset
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
lab   = as.factor(iris[subid,5])

## test different regularization parameters
out1 &lt;- do.crp(X,ndim=2,lambda=0.1)
out2 &lt;- do.crp(X,ndim=2,lambda=1)
out3 &lt;- do.crp(X,ndim=2,lambda=10)

# visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=lab, pch=19, main="CRP::lambda=0.1")
plot(out2$Y, col=lab, pch=19, main="CRP::lambda=1")
plot(out3$Y, col=lab, pch=19, main="CRP::lambda=10")
par(opar)

</code></pre>

<hr>
<h2 id='do.cscore'>Constraint Score</h2><span id='topic+do.cscore'></span>

<h3>Description</h3>

<p>Constraint Score (Zhang et al. 2008) is a filter-type algorithm for feature selection using pairwise constraints.
It first marks all pairwise constraints as same- and different-cluster and
construct a feature score for both constraints. It takes ratio or difference of
feature score vectors and selects the indices with smallest values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.cscore(X, label, ndim = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.cscore_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.cscore_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of class labels.</p>
</td></tr>
<tr><td><code id="do.cscore_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension (default: 2).</p>
</td></tr>
<tr><td><code id="do.cscore_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>preprocess</dt><dd><p>an additional option for preprocessing the data.
See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details (default: <code>"null"</code>).</p>
</dd>
<dt>score</dt><dd><p>type of score measures from two score vectors of same- and different-class pairwise constraints; <code>"ratio"</code> (default) and <code>"difference"</code> method. See the paper from the reference for more details.</p>
</dd>
<dt>lambda</dt><dd><p>a penalty value for different-class pairwise constraints. Only valid for <code>"difference"</code> scoring method. (default: 0.5).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>cscore</dt><dd><p>a length-<code class="reqn">p</code> vector of constraint scores. Indices with smallest values are selected.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Zhang D, Chen S, Zhou Z (2008).
&ldquo;Constraint Score: A New Filter Method for Feature Selection with Pairwise Constraints.&rdquo;
<em>Pattern Recognition</em>, <b>41</b>(5), 1440&ndash;1451.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.cscoreg">do.cscoreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
## it is known that feature 3 and 4 are more important.
data(iris)
iris.dat = as.matrix(iris[,1:4])
iris.lab = as.factor(iris[,5])

## try different strategy
out1 = do.cscore(iris.dat, iris.lab, score="ratio")
out2 = do.cscore(iris.dat, iris.lab, score="difference", lambda=0)
out3 = do.cscore(iris.dat, iris.lab, score="difference", lambda=0.5)
out4 = do.cscore(iris.dat, iris.lab, score="difference", lambda=1)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,2))
plot(out1$Y, col=iris.lab, main="ratio")
plot(out2$Y, col=iris.lab, main="diff/lambda=0")
plot(out3$Y, col=iris.lab, main="diff/lambda=0.5")
plot(out4$Y, col=iris.lab, main="diff/lambda=1")
par(opar)


</code></pre>

<hr>
<h2 id='do.cscoreg'>Constraint Score using Spectral Graph</h2><span id='topic+do.cscoreg'></span>

<h3>Description</h3>

<p>Constraint Score is a filter-type algorithm for feature selection using pairwise constraints.
It first marks all pairwise constraints as same- and different-cluster and
construct a feature score for both constraints. It takes ratio or difference of
feature score vectors and selects the indices with smallest values. Graph laplacian is constructed
for approximated nonlinear manifold structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.cscoreg(X, label, ndim = 2, score = c("ratio", "difference"), lambda = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.cscoreg_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.cscoreg_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of class labels.</p>
</td></tr>
<tr><td><code id="do.cscoreg_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.cscoreg_+3A_score">score</code></td>
<td>
<p>type of score measures from two score vectors of same- and different-class pairwise constraints; <code>"ratio"</code> and <code>"difference"</code> method. See the paper from the reference for more details.</p>
</td></tr>
<tr><td><code id="do.cscoreg_+3A_lambda">lambda</code></td>
<td>
<p>a penalty value for different-class pairwise constraints. Only valid for <code>"difference"</code> scoring method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>cscore</dt><dd><p>a length-<code class="reqn">p</code> vector of constraint scores. Indices with smallest values are selected.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zhang D, Chen S, Zhou Z (2008).
&ldquo;Constraint Score: A New Filter Method for Feature Selection with Pairwise Constraints.&rdquo;
<em>Pattern Recognition</em>, <b>41</b>(5), 1440&ndash;1451.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.cscore">do.cscore</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
## it is known that feature 3 and 4 are more important.
data(iris)
set.seed(100)
subid    = sample(1:150,50)
iris.dat = as.matrix(iris[subid,1:4])
iris.lab = as.factor(iris[subid,5])

## try different strategy
out1 = do.cscoreg(iris.dat, iris.lab, score="ratio")
out2 = do.cscoreg(iris.dat, iris.lab, score="difference", lambda=0)
out3 = do.cscoreg(iris.dat, iris.lab, score="difference", lambda=0.5)
out4 = do.cscoreg(iris.dat, iris.lab, score="difference", lambda=1)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,2))
plot(out1$Y, pch=19, col=iris.lab, main="ratio")
plot(out2$Y, pch=19, col=iris.lab, main="diff/lambda=0")
plot(out3$Y, pch=19, col=iris.lab, main="diff/lambda=0.5")
plot(out4$Y, pch=19, col=iris.lab, main="diff/lambda=1")
par(opar)


</code></pre>

<hr>
<h2 id='do.dagdne'>Double-Adjacency Graphs-based Discriminant Neighborhood Embedding</h2><span id='topic+do.dagdne'></span>

<h3>Description</h3>

<p>Doublue Adjacency Graphs-based Discriminant Neighborhood Embedding (DAG-DNE) is a
variant of DNE. As its name suggests, it introduces two adjacency graphs for
homogeneous and heterogeneous samples accordaing to their labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.dagdne(
  X,
  label,
  ndim = 2,
  numk = max(ceiling(nrow(X)/10), 2),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.dagdne_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.dagdne_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.dagdne_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.dagdne_+3A_numk">numk</code></td>
<td>
<p>the number of neighboring points for k-nn graph construction.</p>
</td></tr>
<tr><td><code id="do.dagdne_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Ding C, Zhang L (2015).
&ldquo;Double Adjacency Graphs-Based Discriminant Neighborhood Embedding.&rdquo;
<em>Pattern Recognition</em>, <b>48</b>(5), 1734&ndash;1742.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.dne">do.dne</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## try different numbers for neighborhood size
out1 = do.dagdne(X, label, numk=5)
out2 = do.dagdne(X, label, numk=10)
out3 = do.dagdne(X, label, numk=20)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="nbd size=5", col=label, pch=19)
plot(out2$Y, main="nbd size=10",col=label, pch=19)
plot(out3$Y, main="nbd size=20",col=label, pch=19)
par(opar)

</code></pre>

<hr>
<h2 id='do.disr'>Diversity-Induced Self-Representation</h2><span id='topic+do.disr'></span>

<h3>Description</h3>

<p>Diversity-Induced Self-Representation (DISR) is a feature selection method that aims at
ranking features by both representativeness and diversity. Self-representation controlled by
<code>lbd1</code> lets the most representative features to be selected, while <code>lbd2</code> penalizes
the degree of inter-feature similarity to enhance diversity from the chosen features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.disr(
  X,
  ndim = 2,
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate"),
  lbd1 = 1,
  lbd2 = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.disr_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.disr_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.disr_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.disr_+3A_lbd1">lbd1</code></td>
<td>
<p>nonnegative number to control the degree of regularization of the self-representation.</p>
</td></tr>
<tr><td><code id="do.disr_+3A_lbd2">lbd2</code></td>
<td>
<p>nonnegative number to control the degree of feature diversity. <code>lbd2=1</code> gives equal weight to self-representation and diversity.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Liu Y, Liu K, Zhang C, Wang J, Wang X (2017).
&ldquo;Unsupervised Feature Selection via Diversity-Induced Self-Representation.&rdquo;
<em>Neurocomputing</em>, <b>219</b>, 350&ndash;363.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.rsr">do.rsr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

#### try different lbd combinations
out1 = do.disr(X, lbd1=1, lbd2=1)
out2 = do.disr(X, lbd1=1, lbd2=5)
out3 = do.disr(X, lbd1=5, lbd2=1)
out4 = do.disr(X, lbd1=5, lbd2=5)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,2))
plot(out1$Y, main="(lbd1,lbd2)=(1,1)", col=label, pch=19)
plot(out2$Y, main="(lbd1,lbd2)=(1,5)", col=label, pch=19)
plot(out3$Y, main="(lbd1,lbd2)=(5,1)", col=label, pch=19)
plot(out4$Y, main="(lbd1,lbd2)=(5,5)", col=label, pch=19)
par(opar)


</code></pre>

<hr>
<h2 id='do.dm'>Diffusion Maps</h2><span id='topic+do.dm'></span>

<h3>Description</h3>

<p><code>do.dm</code> discovers low-dimensional manifold structure embedded in high-dimensional
data space using Diffusion Maps (DM). It exploits diffusion process and distances in data space to find
equivalent representations in low-dimensional space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.dm(
  X,
  ndim = 2,
  preprocess = c("null", "center", "scale", "cscale", "decorrelate", "whiten"),
  bandwidth = 1,
  timescale = 1,
  multiscale = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.dm_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.dm_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.dm_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.dm_+3A_bandwidth">bandwidth</code></td>
<td>
<p>a scaling parameter for diffusion kernel. Default is 1 and should be a nonnegative real number.</p>
</td></tr>
<tr><td><code id="do.dm_+3A_timescale">timescale</code></td>
<td>
<p>a target scale whose value represents behavior of heat kernels at time <em>t</em>. Default is 1 and should be a positive real number.</p>
</td></tr>
<tr><td><code id="do.dm_+3A_multiscale">multiscale</code></td>
<td>
<p>logical; <code>FALSE</code> is to use the fixed <code>timescale</code> value, <code>TRUE</code> to ignore the given value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>eigvals</dt><dd><p>a vector of eigenvalues for Markov transition matrix.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Nadler B, Lafon S, Coifman RR, Kevrekidis IG (2005).
&ldquo;Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators.&rdquo;
In <em>Proceedings of the 18th International Conference on Neural Information Processing Systems</em>,  NIPS'05, 955&ndash;962.
</p>
<p>Coifman RR, Lafon S (2006).
&ldquo;Diffusion Maps.&rdquo;
<em>Applied and Computational Harmonic Analysis</em>, <b>21</b>(1), 5&ndash;30.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## compare different bandwidths
out1 &lt;- do.dm(X,bandwidth=10)
out2 &lt;- do.dm(X,bandwidth=100)
out3 &lt;- do.dm(X,bandwidth=1000)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="DM::bandwidth=10")
plot(out2$Y, pch=19, col=label, main="DM::bandwidth=100")
plot(out3$Y, pch=19, col=label, main="DM::bandwidth=1000")
par(opar)


</code></pre>

<hr>
<h2 id='do.dne'>Discriminant Neighborhood Embedding</h2><span id='topic+do.dne'></span>

<h3>Description</h3>

<p>Discriminant Neighborhood Embedding (DNE) is a supervised subspace learning method.
DNE tries to move multi-class data points in high-dimensional space in accordance with
local intra-class attraction and inter-class repulsion.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.dne(
  X,
  label,
  ndim = 2,
  numk = max(ceiling(nrow(X)/10), 2),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.dne_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.dne_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.dne_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.dne_+3A_numk">numk</code></td>
<td>
<p>the number of neighboring points for k-nn graph construction.</p>
</td></tr>
<tr><td><code id="do.dne_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zhang W, Xue X, Lu H, Guo Y (2006).
&ldquo;Discriminant Neighborhood Embedding for Classification.&rdquo;
<em>Pattern Recognition</em>, <b>39</b>(11), 2240&ndash;2243.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## try different numbers for neighborhood size
out1 = do.dne(X, label, numk=5)
out2 = do.dne(X, label, numk=10)
out3 = do.dne(X, label, numk=20)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="DNE::nbd size=5",  col=label, pch=19)
plot(out2$Y, main="DNE::nbd size=10", col=label, pch=19)
plot(out3$Y, main="DNE::nbd size=20", col=label, pch=19)
par(opar)

</code></pre>

<hr>
<h2 id='do.dppca'>Dual Probabilistic Principal Component Analysis</h2><span id='topic+do.dppca'></span>

<h3>Description</h3>

<p>Dual view of PPCA optimizes the latent variables directly from a simple
Bayesian approach to model the noise using the multivariate Gaussian distribution
of zero mean and spherical covariance <code class="reqn">\beta^{-1} I</code>. When <code class="reqn">\beta</code> is too small,
the algorithm automatically returns an error and provides a guideline for minimal
value that enables successful computation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.dppca(X, ndim = 2, beta = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.dppca_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.dppca_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension (default: 2).</p>
</td></tr>
<tr><td><code id="do.dppca_+3A_beta">beta</code></td>
<td>
<p>the degree for modeling the level of noise (default: 1).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Lawrence N (2005).
&ldquo;Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models.&rdquo;
<em>Journal of Machine Learning Research</em>, <b>6</b>(60), 1783-1816.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.ppca">do.ppca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
X     = as.matrix(iris[,1:4])
lab   = as.factor(iris[,5])

## compare difference choices of 'beta'
embed1 &lt;- do.dppca(X, beta=0.2)
embed2 &lt;- do.dppca(X, beta=1)
embed3 &lt;- do.dppca(X, beta=5)

## Visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3), pty="s")
plot(embed1$Y , col=lab, pch=19, main="beta=0.2")
plot(embed2$Y , col=lab, pch=19, main="beta=1")
plot(embed3$Y , col=lab, pch=19, main="beta=5")
par(opar)


</code></pre>

<hr>
<h2 id='do.dspp'>Discriminative Sparsity Preserving Projection</h2><span id='topic+do.dspp'></span>

<h3>Description</h3>

<p>Discriminative Sparsity Preserving Projection (DSPP) is a supervised dimension reduction method
that employs sparse representation model to adaptively build both intrinsic adjacency graph and
penalty graph. It follows an integration of global within-class structure into manifold learning
under exploiting discriminative nature provided from label information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.dspp(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  lambda = 1,
  rho = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.dspp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.dspp_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.dspp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.dspp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.dspp_+3A_lambda">lambda</code></td>
<td>
<p>regularization parameter for constructing sparsely weighted network.</p>
</td></tr>
<tr><td><code id="do.dspp_+3A_rho">rho</code></td>
<td>
<p>a parameter for balancing the local and global contribution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Gao Q, Huang Y, Zhang H, Hong X, Li K, Wang Y (2015).
&ldquo;Discriminative Sparsity Preserving Projections for Image Recognition.&rdquo;
<em>Pattern Recognition</em>, <b>48</b>(8), 2543&ndash;2553.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## try different rho values
out1 &lt;- do.dspp(X, label, ndim=2, rho=0.01)
out2 &lt;- do.dspp(X, label, ndim=2, rho=0.1)
out3 &lt;- do.dspp(X, label, ndim=2, rho=1)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="rho=0.01", col=label, pch=19)
plot(out2$Y, main="rho=0.1",  col=label, pch=19)
plot(out3$Y, main="rho=1",    col=label, pch=19)
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='do.dve'>Distinguishing Variance Embedding</h2><span id='topic+do.dve'></span>

<h3>Description</h3>

<p>Distinguishing Variance Embedding (DVE) is an unsupervised nonlinear manifold learning method.
It can be considered as a balancing method between Maximum Variance Unfolding and Laplacian
Eigenmaps. The algorithm unfolds the data by maximizing the global variance subject to the
locality-preserving constraint. Instead of defining certain kernel, it applies local scaling scheme
in that it automatically computes adaptive neighborhood-based kernel bandwidth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.dve(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  preprocess = c("null", "center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.dve_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.dve_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.dve_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.dve_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Wang Q, Li J (2009).
&ldquo;Combining Local and Global Information for Nonlinear Dimensionality Reduction.&rdquo;
<em>Neurocomputing</em>, <b>72</b>(10-12), 2235&ndash;2241.
</p>
<p>Qinggang W, Jianwei L, Xuchu W (2010).
&ldquo;Distinguishing Variance Embedding.&rdquo;
<em>Image and Vision Computing</em>, <b>28</b>(6), 872&ndash;880.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate swiss-roll dataset of size 100
set.seed(100)
X &lt;- aux.gensamples(dname="crown", n=100)

## try different nbd size
out1 &lt;- do.dve(X, type=c("proportion",0.5))
out2 &lt;- do.dve(X, type=c("proportion",0.7))
out3 &lt;- do.dve(X, type=c("proportion",0.9))

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="50% connected")
plot(out2$Y, main="70% connected")
plot(out3$Y, main="90% connected")
par(opar)


</code></pre>

<hr>
<h2 id='do.elde'>Exponential Local Discriminant Embedding</h2><span id='topic+do.elde'></span>

<h3>Description</h3>

<p>Local Discriminant Embedding (LDE) suffers from a small-sample-size problem where
scatter matrix may suffer from rank deficiency. Exponential LDE (ELDE) provides
not only a remedy for the problem using matrix exponential, but also a flexible
framework to transform original data into a new space via distance diffusion mapping
similar to kernel-based nonlinear mapping.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.elde(
  X,
  label,
  ndim = 2,
  t = 1,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  k1 = max(ceiling(nrow(X)/10), 2),
  k2 = max(ceiling(nrow(X)/10), 2)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.elde_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.elde_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.elde_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.elde_+3A_t">t</code></td>
<td>
<p>kernel bandwidth in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
<tr><td><code id="do.elde_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.elde_+3A_k1">k1</code></td>
<td>
<p>the number of same-class neighboring points (homogeneous neighbors).</p>
</td></tr>
<tr><td><code id="do.elde_+3A_k2">k2</code></td>
<td>
<p>the number of different-class neighboring points (heterogeneous neighbors).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Dornaika F, Bosaghzadeh A (2013).
&ldquo;Exponential Local Discriminant Embedding and Its Application to Face Recognition.&rdquo;
<em>IEEE Transactions on Cybernetics</em>, <b>43</b>(3), 921&ndash;934.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.lde">do.lde</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate data of 3 types with difference
set.seed(100)
dt1  = aux.gensamples(n=20)-50
dt2  = aux.gensamples(n=20)
dt3  = aux.gensamples(n=20)+50

## merge the data and create a label correspondingly
X      = rbind(dt1,dt2,dt3)
label  = rep(1:3, each=20)

## try different kernel bandwidth
out1 = do.elde(X, label, t=1)
out2 = do.elde(X, label, t=10)
out3 = do.elde(X, label, t=100)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="ELDE::bandwidth=1")
plot(out2$Y, pch=19, col=label, main="ELDE::bandwidth=10")
plot(out3$Y, pch=19, col=label, main="ELDE::bandwidth=100")
par(opar)

</code></pre>

<hr>
<h2 id='do.elpp2'>Enhanced Locality Preserving Projection (2013)</h2><span id='topic+do.elpp2'></span>

<h3>Description</h3>

<p>Enhanced Locality Preserving Projection proposed in 2013 (ELPP2) is built upon
a parameter-free philosophy from PFLPP. It further aims to exclude its projection
to be uncorrelated in the sense that the scatter matrix is placed in a generalized eigenvalue problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.elpp2(
  X,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.elpp2_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations</p>
</td></tr>
<tr><td><code id="do.elpp2_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.elpp2_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Dornaika F, Assoum A (2013).
&ldquo;Enhanced and Parameterless Locality Preserving Projections for Face Recognition.&rdquo;
<em>Neurocomputing</em>, <b>99</b>, 448&ndash;457.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.pflpp">do.pflpp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
lab   = as.factor(iris[subid,5])

## compare with PCA and PFLPP
out1 = do.pca(X, ndim=2)
out2 = do.pflpp(X, ndim=2)
out3 = do.elpp2(X, ndim=2)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=lab, main="PCA")
plot(out2$Y, pch=19, col=lab, main="Parameter-Free LPP")
plot(out3$Y, pch=19, col=lab, main="Enhanced LPP (2013)")
par(opar)

</code></pre>

<hr>
<h2 id='do.enet'>Elastic Net Regularization</h2><span id='topic+do.enet'></span>

<h3>Description</h3>

<p>Elastic Net is a regularized regression method by solving
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_{\beta} ~ \frac{1}{2}\|X\beta-y\|_2^2 + \lambda_1 \|\beta \|_1 + \lambda_2 \|\beta \|_2^2</code>
</p>

<p>where <code class="reqn">y</code> iis <code>response</code> variable in our method. The method can be used in feature selection like LASSO.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.enet(X, response, ndim = 2, lambda1 = 1, lambda2 = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.enet_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.enet_+3A_response">response</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of response variable.</p>
</td></tr>
<tr><td><code id="do.enet_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.enet_+3A_lambda1">lambda1</code></td>
<td>
<p><code class="reqn">\ell_1</code> regularization parameter in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
<tr><td><code id="do.enet_+3A_lambda2">lambda2</code></td>
<td>
<p><code class="reqn">\ell_2</code> regularization parameter in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zou H, Hastie T (2005).
&ldquo;Regularization and Variable Selection via the Elastic Net.&rdquo;
<em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, <b>67</b>(2), 301&ndash;320.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate swiss roll with auxiliary dimensions
## it follows reference example from LSIR paper.
set.seed(100)
n = 123
theta = runif(n)
h     = runif(n)
t     = (1+2*theta)*(3*pi/2)
X     = array(0,c(n,10))
X[,1] = t*cos(t)
X[,2] = 21*h
X[,3] = t*sin(t)
X[,4:10] = matrix(runif(7*n), nrow=n)

## corresponding response vector
y = sin(5*pi*theta)+(runif(n)*sqrt(0.1))

## try different regularization parameters
out1 = do.enet(X, y, lambda1=0.01)
out2 = do.enet(X, y, lambda1=1)
out3 = do.enet(X, y, lambda1=100)

## extract embeddings
Y1 = out1$Y; Y2 = out2$Y; Y3 = out3$Y

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(Y1, pch=19, main="ENET::lambda1=0.01")
plot(Y2, pch=19, main="ENET::lambda1=1")
plot(Y3, pch=19, main="ENET::lambda1=100")
par(opar)


</code></pre>

<hr>
<h2 id='do.eslpp'>Extended Supervised Locality Preserving Projection</h2><span id='topic+do.eslpp'></span>

<h3>Description</h3>

<p>Extended LPP and Supervised LPP are two variants of the celebrated Locality Preserving Projection (LPP) algorithm for dimension
reduction. Their combination, Extended Supervised LPP, is a combination of two algorithmic novelties in one that
it reflects discriminant information with realistic distance measure via Z-score function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.eslpp(
  X,
  label,
  ndim = 2,
  numk = max(ceiling(nrow(X)/10), 2),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.eslpp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.eslpp_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.eslpp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.eslpp_+3A_numk">numk</code></td>
<td>
<p>the number of neighboring points for k-nn graph construction.</p>
</td></tr>
<tr><td><code id="do.eslpp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zheng Z, Yang F, Tan W, Jia J, Yang J (2007).
&ldquo;Gabor Feature-Based Face Recognition Using Supervised Locality Preserving Projection.&rdquo;
<em>Signal Processing</em>, <b>87</b>(10), 2473&ndash;2483.
</p>
<p>Shikkenawis G, Mitra SK (2012).
&ldquo;Improving the Locality Preserving Projection for Dimensionality Reduction.&rdquo;
In <em>2012 Third International Conference on Emerging Applications of Information Technology</em>, 161&ndash;164.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.lpp">do.lpp</a></code>, <code><a href="#topic+do.slpp">do.slpp</a></code>, <code><a href="#topic+do.extlpp">do.extlpp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate data of 2 types with clear difference
set.seed(100)
diff = 50
dt1  = aux.gensamples(n=50)-diff;
dt2  = aux.gensamples(n=50)+diff;

## merge the data and create a label correspondingly
Y      = rbind(dt1,dt2)
label  = rep(1:2, each=50)

## compare LPP, SLPP and ESLPP
outLPP   &lt;- do.lpp(Y)
outSLPP  &lt;- do.slpp(Y, label)
outESLPP &lt;- do.eslpp(Y, label)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(outLPP$Y,   col=label, pch=19, main="LPP")
plot(outSLPP$Y,  col=label, pch=19, main="SLPP")
plot(outESLPP$Y, col=label, pch=19, main="ESLPP")
par(opar)

</code></pre>

<hr>
<h2 id='do.extlpp'>Extended Locality Preserving Projection</h2><span id='topic+do.extlpp'></span>

<h3>Description</h3>

<p>Extended Locality Preserving Projection (EXTLPP) is an unsupervised
dimension reduction algorithm with a bit of flavor in adopting
discriminative idea by nature. It raises a question on the data points
at <em>moderate</em> distance in that a Z-shaped function is introduced in
defining similarity derived from Euclidean distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.extlpp(
  X,
  ndim = 2,
  numk = max(ceiling(nrow(X)/10), 2),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.extlpp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.extlpp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.extlpp_+3A_numk">numk</code></td>
<td>
<p>the number of neighboring points for k-nn graph construction.</p>
</td></tr>
<tr><td><code id="do.extlpp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Shikkenawis G, Mitra SK (2012).
&ldquo;Improving the Locality Preserving Projection for Dimensionality Reduction.&rdquo;
In <em>2012 Third International Conference on Emerging Applications of Information Technology</em>, 161&ndash;164.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.lpp">do.lpp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate data
set.seed(100)
X &lt;- aux.gensamples(n=75)

## run Extended LPP with different neighborhood graph
out1 &lt;- do.extlpp(X, numk=5)
out2 &lt;- do.extlpp(X, numk=10)
out3 &lt;- do.extlpp(X, numk=25)

## Visualize three different projections
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="EXTLPP::k=5")
plot(out2$Y, main="EXTLPP::k=10")
plot(out3$Y, main="EXTLPP::k=25")
par(opar)

</code></pre>

<hr>
<h2 id='do.fa'>Exploratory Factor Analysis</h2><span id='topic+do.fa'></span>

<h3>Description</h3>

<p><code>do.fa</code> is an optimization-based implementation of a popular technique for Exploratory Data Analysis.
It is closely related to principal component analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.fa(X, ndim = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.fa_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.fa_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued number of loading variables, or target dimension.</p>
</td></tr>
<tr><td><code id="do.fa_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>maxiter</dt><dd><p>maximum number of iterations (default: 10).</p>
</dd>
<dt>tolerance</dt><dd><p>stopping criterion in a Frobenius norm (default: 1e-8).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>loadings</dt><dd><p>a <code class="reqn">(p\times ndim)</code> matrix whose rows are extracted loading factors.</p>
</dd>
<dt>noise</dt><dd><p>a length-<code class="reqn">p</code> vector of estimated noise.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Spearman C (1904).
&ldquo;&quot;General Intelligence,&quot; Objectively Determined and Measured.&rdquo;
<em>The American Journal of Psychology</em>, <b>15</b>(2), 201.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
lab   = as.factor(iris[subid,5])

## compare with PCA and MDS
out1 &lt;- do.fa(X, ndim=2)
out2 &lt;- do.mds(X, ndim=2)
out3 &lt;- do.pca(X, ndim=2)

## visualize three different projections
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=lab, main="Factor Analysis")
plot(out2$Y, pch=19, col=lab, main="MDS")
plot(out3$Y, pch=19, col=lab, main="PCA")
par(opar)


</code></pre>

<hr>
<h2 id='do.fastmap'>FastMap</h2><span id='topic+do.fastmap'></span>

<h3>Description</h3>

<p><code>do.fastmap</code> is an implementation of <em>FastMap</em> algorithm. Though
it shares similarities with MDS, it is innately a nonlinear method that makes an iterative update
for the projection information using pairwise distance information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.fastmap(
  X,
  ndim = 2,
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.fastmap_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.fastmap_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.fastmap_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Faloutsos C, Lin K (1995).
&ldquo;FastMap: A Fast Algorithm for Indexing, Data-Mining and Visualization of Traditional and Multimedia Datasets.&rdquo;
In <em>Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data - SIGMOD '95</em>, 163&ndash;174.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## let's compare with other methods
out1 &lt;- do.pca(X, ndim=2)      # PCA
out2 &lt;- do.mds(X, ndim=2)      # Classical MDS
out3 &lt;- do.fastmap(X, ndim=2)  # FastMap

## visualize
opar = par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="PCA")
plot(out2$Y, pch=19, col=label, main="MDS")
plot(out3$Y, pch=19, col=label, main="FastMap")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='do.fosmod'>Forward Orthogonal Search by Maximizing the Overall Dependency</h2><span id='topic+do.fosmod'></span>

<h3>Description</h3>

<p>The FOS-MOD algorithm (Wei and Billings 2007)
is an unsupervised algorithm that selects a desired number of features in
a forward manner by ranking the features using the squared correlation
coefficient and sequential orthogonalization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.fosmod(X, ndim = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.fosmod_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.fosmod_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension (default: 2).</p>
</td></tr>
<tr><td><code id="do.fosmod_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>preprocess</dt><dd><p>an additional option for preprocessing the data.
See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details (default: <code>"center"</code>).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Wei H, Billings S (2007).
&ldquo;Feature Subset Selection and Ranking for Data Dimensionality Reduction.&rdquo;
<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <b>29</b>(1), 162&ndash;166.
ISSN 0162-8828.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
## it is known that feature 3 and 4 are more important.
data(iris)
set.seed(100)
subid    &lt;- sample(1:150, 50)
iris.dat &lt;- as.matrix(iris[subid,1:4])
iris.lab &lt;- as.factor(iris[subid,5])

## compare with other methods
out1 = do.fosmod(iris.dat)
out2 = do.lscore(iris.dat)
out3 = do.fscore(iris.dat, iris.lab)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=iris.lab, main="FOS-MOD")
plot(out2$Y, pch=19, col=iris.lab, main="Laplacian Score")
plot(out3$Y, pch=19, col=iris.lab, main="Fisher Score")
par(opar)


</code></pre>

<hr>
<h2 id='do.fscore'>Fisher Score</h2><span id='topic+do.fscore'></span>

<h3>Description</h3>

<p>Fisher Score (Fisher 1936) is a supervised linear feature extraction method. For each
feature/variable, it computes Fisher score, a ratio of between-class variance to within-class variance.
The algorithm selects variables with largest Fisher scores and returns an indicator projection matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.fscore(X, label, ndim = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.fscore_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.fscore_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.fscore_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.fscore_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>preprocess</dt><dd><p>an additional option for preprocessing the data.
Default is <code>"null"</code>. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Fisher RA (1936).
&ldquo;THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS.&rdquo;
<em>Annals of Eugenics</em>, <b>7</b>(2), 179&ndash;188.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
## it is known that feature 3 and 4 are more important.
data(iris)
set.seed(100)
subid    = sample(1:150,50)
iris.dat = as.matrix(iris[subid,1:4])
iris.lab = as.factor(iris[subid,5])

## compare Fisher score with LDA
out1 = do.lda(iris.dat, iris.lab)
out2 = do.fscore(iris.dat, iris.lab)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(out1$Y, pch=19, col=iris.lab, main="LDA")
plot(out2$Y, pch=19, col=iris.lab, main="Fisher Score")
par(opar)


</code></pre>

<hr>
<h2 id='do.fssem'>Feature Subset Selection using Expectation-Maximization</h2><span id='topic+do.fssem'></span>

<h3>Description</h3>

<p>Feature Subset Selection using Expectation-Maximization (FSSEM) takes a wrapper approach to feature selection problem.
It iterates over optimizing the selection of variables by incrementally including each variable that adds the most
significant amount of scatter separability from a labeling obtained by Gaussian mixture model. This method is
quite computation intensive as it pertains to multiple fitting of GMM. Setting smaller <code>max.k</code> for each round of
EM algorithm as well as target dimension <code>ndim</code> would ease the burden.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.fssem(
  X,
  ndim = 2,
  max.k = 10,
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.fssem_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.fssem_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.fssem_+3A_max.k">max.k</code></td>
<td>
<p>maximum number of clusters for GMM fitting with EM algorithms.</p>
</td></tr>
<tr><td><code id="do.fssem_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Dy JG, Brodley CE (2004).
&ldquo;Feature Selection for Unsupervised Learning.&rdquo;
<em>J. Mach. Learn. Res.</em>, <b>5</b>, 845&ndash;889.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## run FSSEM with IRIS dataset - select 2 of 4 variables
data(iris)
irismat = as.matrix(iris[,2:4])

## select 50 observations for CRAN-purpose small example
id50 = sample(1:nrow(irismat), 50)
sel.dat = irismat[id50,]
sel.lab = as.factor(iris[id50,5])

## run and visualize
out0 = do.fssem(sel.dat, ndim=2, max.k=3)
opar = par(no.readonly=TRUE)
plot(out0$Y, main="small run", col=sel.lab, pch=19)
par(opar)

## Not run: 
## NOT-FOR-CRAN example; run at your machine !
## try different maximum number of clusters
out3 = do.fssem(irismat, ndim=2, max.k=3)
out6 = do.fssem(irismat, ndim=2, max.k=6)
out9 = do.fssem(irismat, ndim=2, max.k=9)

## visualize
cols = as.factor(iris[,5])
opar = par(no.readonly=TRUE)
par(mfrow=c(3,1))
plot(out3$Y, main="max k=3", col=cols)
plot(out6$Y, main="max k=6", col=cols)
plot(out9$Y, main="max k=9", col=cols)
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='do.hydra'>Hyperbolic Distance Recovery and Approximation</h2><span id='topic+do.hydra'></span>

<h3>Description</h3>

<p>Hyperbolic Distance Recovery and Approximation, also known as <code>hydra</code> in short,
implements embedding of distance-based data into hyperbolic space represented as the Poincare disk,
which is interior of a hypersphere.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.hydra(X, ndim = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.hydra_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.hydra_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension (default: 2).</p>
</td></tr>
<tr><td><code id="do.hydra_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>kappa</dt><dd><p>embedding curvature, which is a nonnegative number (default: 1).</p>
</dd>
<dt>iso.adjust</dt><dd><p>perform isotropic adjustment. If <code>ndim=2</code>, default is <code>FALSE</code>. Otherwise, <code>TRUE</code> is used as default.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations in the Poincare disk.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Keller-Ressel M, Nargang S (2020).
&ldquo;Hydra: A Method for Strain-Minimizing Hyperbolic Embedding of Network- and Distance-Based Data.&rdquo;
<em>Journal of Complex Networks</em>, <b>8</b>(1), cnaa002.
ISSN 2051-1329.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
X     = as.matrix(iris[,1:4])
lab   = as.factor(iris[,5])

## multiple runs with varying curvatures
embed1 &lt;- do.hydra(X, kappa=0.1)
embed2 &lt;- do.hydra(X, kappa=1)
embed3 &lt;- do.hydra(X, kappa=10)

## Visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3), pty="s")
plot(embed1$Y , col=lab, pch=19, main="kappa=0.1")
plot(embed2$Y , col=lab, pch=19, main="kappa=1")
plot(embed3$Y , col=lab, pch=19, main="kappa=10")
par(opar)


</code></pre>

<hr>
<h2 id='do.ica'>Independent Component Analysis</h2><span id='topic+do.ica'></span>

<h3>Description</h3>

<p><code>do.ica</code> is an R implementation of FastICA algorithm, which aims at
finding weight vectors that maximize a measure of non-Gaussianity of projected data.
FastICA is initiated with pre-whitening of the data. Single and multiple component
extraction are both supported. For more detailed information on ICA and FastICA algorithm,
see this <a href="https://en.wikipedia.org/wiki/FastICA">Wikipedia</a> page.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.ica(
  X,
  ndim = 2,
  type = "logcosh",
  tpar = 1,
  sym = FALSE,
  tol = 1e-06,
  redundancy = TRUE,
  maxiter = 100
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.ica_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.ica_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.ica_+3A_type">type</code></td>
<td>
<p>nonquadratic function, one of <code>"logcosh"</code>,<code>"exp"</code>, or <code>"poly"</code> be chosen.</p>
</td></tr>
<tr><td><code id="do.ica_+3A_tpar">tpar</code></td>
<td>
<p>a numeric parameter for <code>logcosh</code> and <code>exp</code> parameters that should be close to 1.</p>
</td></tr>
<tr><td><code id="do.ica_+3A_sym">sym</code></td>
<td>
<p>a logical value; <code>FALSE</code> for not using symmetric decorrelation, <code>TRUE</code> otherwise.</p>
</td></tr>
<tr><td><code id="do.ica_+3A_tol">tol</code></td>
<td>
<p>stopping criterion for iterative update.</p>
</td></tr>
<tr><td><code id="do.ica_+3A_redundancy">redundancy</code></td>
<td>
<p>a logical value; <code>TRUE</code> for removing <code>NA</code> values after prewhitening, <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="do.ica_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations allowed.
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>In most of ICA literature, we have </p>
<p style="text-align: center;"><code class="reqn">S = X*W</code>
</p>
<p> where <code class="reqn">W</code> is an unmixing matrix for
the given data <code class="reqn">X</code>. In order to preserve consistency throughout our package, we changed
the notation; <code class="reqn">Y</code> a projected matrix for <code class="reqn">S</code>, and <code>projection</code> for unmixing matrix <code class="reqn">W</code>.
</p>


<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Hyvarinen A, Karhunen J, Oja E (2001).
<em>Independent Component Analysis</em>.
J. Wiley, New York.
ISBN 978-0-471-40540-5.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris dataset
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
lab   = as.factor(iris[subid,5])

## 1. use logcosh function for transformation
output1 &lt;- do.ica(X,ndim=2,type="logcosh")

## 2. use exponential function for transformation
output2 &lt;- do.ica(X,ndim=2,type="exp")

## 3. use polynomial function for transformation
output3 &lt;- do.ica(X,ndim=2,type="poly")

## Visualize three different projections
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(output1$Y, col=lab, pch=19, main="ICA::logcosh")
plot(output2$Y, col=lab, pch=19, main="ICA::exp")
plot(output3$Y, col=lab, pch=19, main="ICA::poly")
par(opar)

</code></pre>

<hr>
<h2 id='do.idmap'>Interactive Document Map</h2><span id='topic+do.idmap'></span>

<h3>Description</h3>

<p>Interactive Document Map originates from text analysis to generate maps of documents by placing
similar documents in the same neighborhood. After defining pairwise distance with cosine similarity,
authors asserted to use either <code>NNP</code> or <code>FastMap</code> as an engine behind.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.idmap(
  X,
  ndim = 2,
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate"),
  engine = c("NNP", "FastMap")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.idmap_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.idmap_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.idmap_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.idmap_+3A_engine">engine</code></td>
<td>
<p>either <code>NNP</code> or <code>FastMap</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>References</h3>

<p>Minghim R, Paulovich FV, de Andrade Lopes A (2006).
&ldquo;Content-Based Text Mapping Using Multi-Dimensional Projections for Exploration of Document Collections.&rdquo;
In Erbacher RF, Roberts JC, Gröhn MT, Börner K (eds.), <em>Visualization and Data Analysis</em>, 60600S.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.nnp">do.nnp</a></code>, <code><a href="#topic+do.fastmap">do.fastmap</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
lab   = as.factor(iris[subid,5])

## let's compare with other methods
out1 &lt;- do.pca(X, ndim=2)
out2 &lt;- do.lda(X, ndim=2, label=lab)
out3 &lt;- do.idmap(X, ndim=2, engine="NNP")

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=lab, main="PCA")
plot(out2$Y, pch=19, col=lab, main="LDA")
plot(out3$Y, pch=19, col=lab, main="IDMAP")
par(opar)


</code></pre>

<hr>
<h2 id='do.iltsa'>Improved Local Tangent Space Alignment</h2><span id='topic+do.iltsa'></span>

<h3>Description</h3>

<p>Conventional LTSA method relies on PCA for approximating local tangent spaces.
Improved LTSA (ILTSA) provides a remedy that can efficiently recover the geometric
structure of data manifolds even when data are sparse or non-uniformly distributed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.iltsa(
  X,
  ndim = 2,
  type = c("proportion", 0.25),
  symmetric = c("union", "intersect", "asymmetric"),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  t = 10
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.iltsa_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.iltsa_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.iltsa_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.iltsa_+3A_symmetric">symmetric</code></td>
<td>
<p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.iltsa_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.iltsa_+3A_t">t</code></td>
<td>
<p>heat kernel bandwidth parameter in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zhang P, Qiao H, Zhang B (2011).
&ldquo;An Improved Local Tangent Space Alignment Method for Manifold Learning.&rdquo;
<em>Pattern Recognition Letters</em>, <b>32</b>(2), 181&ndash;189.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## try different bandwidth size
out1 &lt;- do.iltsa(X, t=1)
out2 &lt;- do.iltsa(X, t=10)
out3 &lt;- do.iltsa(X, t=100)

## Visualize two comparisons
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="ILTSA::t=1")
plot(out2$Y, pch=19, col=label, main="ILTSA::t=10")
plot(out3$Y, pch=19, col=label, main="ILTSA::t=100")
par(opar)


</code></pre>

<hr>
<h2 id='do.isomap'>Isometric Feature Mapping</h2><span id='topic+do.isomap'></span>

<h3>Description</h3>

<p><code>do.isomap</code> is an efficient implementation of a well-known <em>Isomap</em> method
by Tenenbaum et al (2000). Its novelty comes from applying classical multidimensional
scaling on nonlinear manifold, which is approximated as a graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.isomap(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  symmetric = c("union", "intersect", "asymmetric"),
  weight = FALSE,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.isomap_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.isomap_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.isomap_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.isomap_+3A_symmetric">symmetric</code></td>
<td>
<p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.isomap_+3A_weight">weight</code></td>
<td>
<p><code>TRUE</code> to perform Isomap on weighted graph, or <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="do.isomap_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Silva VD, Tenenbaum JB (2003).
&ldquo;Global Versus Local Methods in Nonlinear Dimensionality Reduction.&rdquo;
In Becker S, Thrun S, Obermayer K (eds.), <em>Advances in Neural Information Processing Systems 15</em>, 721&ndash;728.
MIT Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate data
set.seed(100)
X &lt;- aux.gensamples(n=123)

## 1. connecting 10% of data for graph construction.
output1 &lt;- do.isomap(X,ndim=2,type=c("proportion",0.10),weight=FALSE)

## 2. constructing 25%-connected graph
output2 &lt;- do.isomap(X,ndim=2,type=c("proportion",0.25),weight=FALSE)

## 3. constructing 25%-connected with binarization
output3 &lt;- do.isomap(X,ndim=2,type=c("proportion",0.50),weight=FALSE)

## Visualize three different projections
opar = par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(output1$Y, main="10%")
plot(output2$Y, main="25%")
plot(output3$Y, main="25%+Binary")
par(opar)


</code></pre>

<hr>
<h2 id='do.isoproj'>Isometric Projection</h2><span id='topic+do.isoproj'></span>

<h3>Description</h3>

<p>Isometric Projection is a linear dimensionality reduction algorithm that exploits
geodesic distance in original data dimension and mimicks the behavior in the target dimension.
Embedded manifold is approximated by graph construction as of ISOMAP. Since it involves
singular value decomposition and guesses intrinsic dimension by the number of positive singular values
from the decomposition of data matrix, it automatically corrects the target dimension accordingly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.isoproj(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  symmetric = c("union", "intersect", "asymmetric"),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.isoproj_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.isoproj_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.isoproj_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.isoproj_+3A_symmetric">symmetric</code></td>
<td>
<p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.isoproj_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix of projected observations as rows.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are loadings.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Cai D, He X, Han J (2007).
&ldquo;Isometric Projection.&rdquo;
In <em>Proceedings of the 22Nd National Conference on Artificial Intelligence - Volume 1</em>,  AAAI'07, 528&ndash;533.
ISBN 978-1-57735-323-2.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris dataset
data(iris)
set.seed(100)
subid &lt;- sample(1:150, 50)
X     &lt;- as.matrix(iris[subid,1:4])
lab   &lt;- as.factor(iris[subid,5])

## try different connectivity levels
output1 &lt;- do.isoproj(X,ndim=2,type=c("proportion",0.50))
output2 &lt;- do.isoproj(X,ndim=2,type=c("proportion",0.70))
output3 &lt;- do.isoproj(X,ndim=2,type=c("proportion",0.90))

## visualize two different projections
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(output1$Y, main="50%", col=lab, pch=19)
plot(output2$Y, main="70%", col=lab, pch=19)
plot(output3$Y, main="90%", col=lab, pch=19)
par(opar)

</code></pre>

<hr>
<h2 id='do.ispe'>Isometric Stochastic Proximity Embedding</h2><span id='topic+do.ispe'></span>

<h3>Description</h3>

<p>The isometric SPE (ISPE) adopts the idea of approximating geodesic distance on embedded manifold
when two data points are close enough. It introduces the concept of <code>cutoff</code> where the learning process
is only applied to the pair of data points whose original proximity is small enough to be considered as
mutually local whose distance should be close to geodesic distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.ispe(
  X,
  ndim = 2,
  proximity = function(x) {
     dist(x, method = "euclidean")
 },
  C = 50,
  S = 50,
  lambda = 1,
  drate = 0.9,
  cutoff = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.ispe_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.ispe_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.ispe_+3A_proximity">proximity</code></td>
<td>
<p>a function for constructing proximity matrix from original data dimension.</p>
</td></tr>
<tr><td><code id="do.ispe_+3A_c">C</code></td>
<td>
<p>the number of cycles to be run; after each cycle, learning parameter</p>
</td></tr>
<tr><td><code id="do.ispe_+3A_s">S</code></td>
<td>
<p>the number of updates for each cycle.</p>
</td></tr>
<tr><td><code id="do.ispe_+3A_lambda">lambda</code></td>
<td>
<p>initial learning parameter.</p>
</td></tr>
<tr><td><code id="do.ispe_+3A_drate">drate</code></td>
<td>
<p>multiplier for <code>lambda</code> at each cycle; should be a positive real number in <code class="reqn">(0,1).</code></p>
</td></tr>
<tr><td><code id="do.ispe_+3A_cutoff">cutoff</code></td>
<td>
<p>cutoff threshold value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Agrafiotis DK, Xu H (2002).
&ldquo;A Self-Organizing Principle for Learning Nonlinear Manifolds.&rdquo;
<em>Proceedings of the National Academy of Sciences</em>, <b>99</b>(25), 15869&ndash;15872.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## compare with original SPE
outSPE &lt;- do.spe(X, ndim=2)
out1 &lt;- do.ispe(X, ndim=2, cutoff=0.5)
out2 &lt;- do.ispe(X, ndim=2, cutoff=5)
out3 &lt;- do.ispe(X, ndim=2, cutoff=50)

## Visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,2))
plot(outSPE$Y, pch=19, col=label, main="SPE")
plot(out1$Y,   pch=19, col=label, main="ISPE::cutoff=0.5")
plot(out2$Y,   pch=19, col=label, main="ISPE::cutoff=5")
plot(out3$Y,   pch=19, col=label, main="ISPE::cutoff=50")
par(opar)

</code></pre>

<hr>
<h2 id='do.keca'>Kernel Entropy Component Analysis</h2><span id='topic+do.keca'></span>

<h3>Description</h3>

<p>Kernel Entropy Component Analysis(KECA) is a kernel method of dimensionality reduction.
Unlike Kernel PCA(<code><a href="#topic+do.kpca">do.kpca</a></code>), it utilizes eigenbasis of kernel matrix <code class="reqn">K</code>
in accordance with indices of largest Renyi quadratic entropy in which entropy for
<code class="reqn">j</code>-th eigenpair is defined to be <code class="reqn">\sqrt{\lambda_j}e_j^T 1_n</code>, where <code class="reqn">e_j</code> is
<code class="reqn">j</code>-th eigenvector of an uncentered kernel matrix <code class="reqn">K</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.keca(
  X,
  ndim = 2,
  kernel = c("gaussian", 1),
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.keca_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.keca_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.keca_+3A_kernel">kernel</code></td>
<td>
<p>a vector containing name of a kernel and corresponding parameters. See also <code><a href="#topic+aux.kernelcov">aux.kernelcov</a></code> for complete description of Kernel Trick.</p>
</td></tr>
<tr><td><code id="do.keca_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>entropy</dt><dd><p>a length-<code>ndim</code> vector of estimated entropy values.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Jenssen R (2010).
&ldquo;Kernel Entropy Component Analysis.&rdquo;
<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <b>32</b>(5), 847&ndash;860.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+aux.kernelcov">aux.kernelcov</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## 1. standard KECA with gaussian kernel
output1 &lt;- do.keca(X,ndim=2)

## 2. gaussian kernel with large bandwidth
output2 &lt;- do.keca(X,ndim=2,kernel=c("gaussian",5))

## 3. use laplacian kernel
output3 &lt;- do.keca(X,ndim=2,kernel=c("laplacian",1))

## Visualize three different projections
opar = par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(output1$Y, pch=19, col=label, main="Gaussian kernel")
plot(output2$Y, pch=19, col=label, main="Gaussian, sigma=5")
plot(output3$Y, pch=19, col=label, main="Laplacian kernel")
par(opar)


</code></pre>

<hr>
<h2 id='do.klde'>Kernel Local Discriminant Embedding</h2><span id='topic+do.klde'></span>

<h3>Description</h3>

<p>Kernel Local Discriminant Embedding (KLDE) is a variant of Local Discriminant Embedding in that
it aims to preserve inter- and intra-class neighborhood information in a nonlinear manner using
kernel trick. <em>Note</em> that the combination of kernel matrix and its eigendecomposition
often suffers from lacking numerical rank. For such case, our algorithm returns a warning message and
algorithm stops working any further due to its innate limitations of constructing weight matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.klde(
  X,
  label,
  ndim = 2,
  t = 1,
  numk = max(ceiling(nrow(X)/10), 2),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  ktype = c("gaussian", 1),
  kcentering = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.klde_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.klde_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.klde_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.klde_+3A_t">t</code></td>
<td>
<p>kernel bandwidth in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
<tr><td><code id="do.klde_+3A_numk">numk</code></td>
<td>
<p>the number of neighboring points for k-nn graph construction.</p>
</td></tr>
<tr><td><code id="do.klde_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.klde_+3A_ktype">ktype</code></td>
<td>
<p>a vector containing name of a kernel and corresponding parameters. See also <code><a href="#topic+aux.kernelcov">aux.kernelcov</a></code> for complete description of Kernel Trick.</p>
</td></tr>
<tr><td><code id="do.klde_+3A_kcentering">kcentering</code></td>
<td>
<p>a logical; <code>TRUE</code> to use centered Kernel matrix, <code>FALSE</code> otherwise.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Hwann-Tzong Chen, Huang-Wei Chang, Tyng-Luh Liu (2005).
&ldquo;Local Discriminant Embedding and Its Variants.&rdquo;
In <em>2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em>, volume 2, 846&ndash;853.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate data of 2 types with clear difference
set.seed(100)
diff = 25
dt1  = aux.gensamples(n=50)-diff;
dt2  = aux.gensamples(n=50)+diff;

## merge the data and create a label correspondingly
X      = rbind(dt1,dt2)
label  = rep(1:2, each=50)

## try different neighborhood size
out1 &lt;- do.klde(X, label, numk=5)
out2 &lt;- do.klde(X, label, numk=10)
out3 &lt;- do.klde(X, label, numk=20)

## visualize
opar = par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=label, pch=19, main="k=5")
plot(out2$Y, col=label, pch=19, main="k=10")
plot(out3$Y, col=label, pch=19, main="k=20")
par(opar)


</code></pre>

<hr>
<h2 id='do.klfda'>Kernel Local Fisher Discriminant Analysis</h2><span id='topic+do.klfda'></span>

<h3>Description</h3>

<p>Kernel LFDA is a nonlinear extension of LFDA method using kernel trick. It applies conventional kernel method
to extend excavation of hidden patterns in a more flexible manner in tradeoff of computational load. For simplicity,
only the gaussian kernel parametrized by its bandwidth <code>t</code> is supported.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.klfda(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  type = c("proportion", 0.1),
  symmetric = c("union", "intersect", "asymmetric"),
  localscaling = TRUE,
  t = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.klfda_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.klfda_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.klfda_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.klfda_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.klfda_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.klfda_+3A_symmetric">symmetric</code></td>
<td>
<p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.klfda_+3A_localscaling">localscaling</code></td>
<td>
<p><code>TRUE</code> to use local scaling method for construction affinity matrix, <code>FALSE</code> for binary affinity.</p>
</td></tr>
<tr><td><code id="do.klfda_+3A_t">t</code></td>
<td>
<p>bandwidth parameter for heat kernel in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Sugiyama M (2006).
&ldquo;Local Fisher Discriminant Analysis for Supervised Dimensionality Reduction.&rdquo;
In <em>Proceedings of the 23rd International Conference on Machine Learning</em>, 905&ndash;912.
</p>
<p>Zelnik-manor L, Perona P (2005).
&ldquo;Self-Tuning Spectral Clustering.&rdquo;
In Saul LK, Weiss Y, Bottou L (eds.), <em>Advances in Neural Information Processing Systems 17</em>, 1601&ndash;1608.
MIT Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.lfda">do.lfda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate 3 different groups of data X and label vector
set.seed(100)
x1 = matrix(rnorm(4*10), nrow=10)-20
x2 = matrix(rnorm(4*10), nrow=10)
x3 = matrix(rnorm(4*10), nrow=10)+20
X     = rbind(x1, x2, x3)
label = rep(1:3, each=10)

## try different affinity matrices
out1 = do.klfda(X, label, t=0.1)
out2 = do.klfda(X, label, t=1)
out3 = do.klfda(X, label, t=10)

## visualize
opar = par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="bandwidth=0.1")
plot(out2$Y, pch=19, col=label, main="bandwidth=1")
plot(out3$Y, pch=19, col=label, main="bandwidth=10")
par(opar)


</code></pre>

<hr>
<h2 id='do.klsda'>Kernel Locality Sensitive Discriminant Analysis</h2><span id='topic+do.klsda'></span>

<h3>Description</h3>

<p>Kernel LSDA (KLSDA) is a nonlinear extension of LFDA method using kernel trick. It applies conventional kernel method
to extend excavation of hidden patterns in a more flexible manner in tradeoff of computational load. For simplicity,
only the gaussian kernel parametrized by its bandwidth <code>t</code> is supported.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.klsda(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "whiten", "decorrelate"),
  alpha = 0.5,
  k1 = max(ceiling(nrow(X)/10), 2),
  k2 = max(ceiling(nrow(X)/10), 2),
  t = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.klsda_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.klsda_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.klsda_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.klsda_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.klsda_+3A_alpha">alpha</code></td>
<td>
<p>balancing parameter for between- and within-class scatter in <code class="reqn">[0,1]</code>.</p>
</td></tr>
<tr><td><code id="do.klsda_+3A_k1">k1</code></td>
<td>
<p>the number of same-class neighboring points (homogeneous neighbors).</p>
</td></tr>
<tr><td><code id="do.klsda_+3A_k2">k2</code></td>
<td>
<p>the number of different-class neighboring points (heterogeneous neighbors).</p>
</td></tr>
<tr><td><code id="do.klsda_+3A_t">t</code></td>
<td>
<p>bandwidth parameter for heat kernel in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Cai D, He X, Zhou K, Han J, Bao H (2007).
&ldquo;Locality Sensitive Discriminant Analysis.&rdquo;
In <em>Proceedings of the 20th International Joint Conference on Artifical Intelligence</em>,  IJCAI'07, 708&ndash;713.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate 3 different groups of data X and label vector
x1 = matrix(rnorm(4*10), nrow=10)-50
x2 = matrix(rnorm(4*10), nrow=10)
x3 = matrix(rnorm(4*10), nrow=10)+50
X     = rbind(x1, x2, x3)
label = rep(1:3, each=10)

## try different kernel bandwidths
out1 = do.klsda(X, label, t=0.1)
out2 = do.klsda(X, label, t=1)
out3 = do.klsda(X, label, t=10)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=label, pch=19, main="bandwidth=0.1")
plot(out2$Y, col=label, pch=19, main="bandwidth=1")
plot(out3$Y, col=label, pch=19, main="bandwidth=10")
par(opar)

</code></pre>

<hr>
<h2 id='do.kmfa'>Kernel Marginal Fisher Analysis</h2><span id='topic+do.kmfa'></span>

<h3>Description</h3>

<p>Kernel Marginal Fisher Analysis (KMFA) is a nonlinear variant of MFA using kernel tricks.
For simplicity, we only enabled a heat kernel of a form
</p>
<p style="text-align: center;"><code class="reqn">k(x_i,x_j)=\exp(-d(x_i,x_j)^2/2*t^2)</code>
</p>

<p>where <code class="reqn">t</code> is a bandwidth parameter. Note that the method is far sensitive to the choice of <code class="reqn">t</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.kmfa(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  k1 = max(ceiling(nrow(X)/10), 2),
  k2 = max(ceiling(nrow(X)/10), 2),
  t = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.kmfa_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.kmfa_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.kmfa_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.kmfa_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.kmfa_+3A_k1">k1</code></td>
<td>
<p>the number of same-class neighboring points (homogeneous neighbors).</p>
</td></tr>
<tr><td><code id="do.kmfa_+3A_k2">k2</code></td>
<td>
<p>the number of different-class neighboring points (heterogeneous neighbors).</p>
</td></tr>
<tr><td><code id="do.kmfa_+3A_t">t</code></td>
<td>
<p>bandwidth parameter for heat kernel in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Yan S, Xu D, Zhang B, Zhang H, Yang Q, Lin S (2007).
&ldquo;Graph Embedding and Extensions: A General Framework for Dimensionality Reduction.&rdquo;
<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <b>29</b>(1), 40&ndash;51.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate data of 3 types with clear difference
set.seed(100)
dt1  = aux.gensamples(n=20)-100
dt2  = aux.gensamples(n=20)
dt3  = aux.gensamples(n=20)+100

## merge the data and create a label correspondingly
X      = rbind(dt1,dt2,dt3)
label  = rep(1:3, each=20)

## try different numbers for neighborhood size
out1 = do.kmfa(X, label, k1=10, k2=10, t=0.001)
out2 = do.kmfa(X, label, k1=10, k2=10, t=0.01)
out3 = do.kmfa(X, label, k1=10, k2=10, t=0.1)

## visualize
opar = par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="bandwidth=0.001")
plot(out2$Y, pch=19, col=label, main="bandwidth=0.01")
plot(out3$Y, pch=19, col=label, main="bandwidth=0.1")
par(opar)

</code></pre>

<hr>
<h2 id='do.kmmc'>Kernel Maximum Margin Criterion</h2><span id='topic+do.kmmc'></span>

<h3>Description</h3>

<p>Kernel Maximum Margin Criterion (KMMC) is a nonlinear variant of MMC method using kernel trick.
For computational simplicity, only the gaussian kernel is used with bandwidth parameter <code>t</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.kmmc(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "decorrelate", "whiten"),
  t = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.kmmc_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.kmmc_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.kmmc_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.kmmc_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.kmmc_+3A_t">t</code></td>
<td>
<p>bandwidth parameter for heat kernel in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Li H, Jiang T, Zhang K (2006).
&ldquo;Efficient and Robust Feature Extraction by Maximum Margin Criterion.&rdquo;
<em>IEEE Transactions on Neural Networks</em>, <b>17</b>(1), 157&ndash;165.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.mmc">do.mmc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,100)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## perform MVP with different preprocessings
out1 = do.kmmc(X, label, t=0.1)
out2 = do.kmmc(X, label, t=1.0)
out3 = do.kmmc(X, label, t=10.0)

## visualize
opar = par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="bandwidth=0.1")
plot(out2$Y, pch=19, col=label, main="bandwidth=1")
plot(out3$Y, pch=19, col=label, main="bandwidth=10.0")
par(opar)


</code></pre>

<hr>
<h2 id='do.kmvp'>Kernel-Weighted Maximum Variance Projection</h2><span id='topic+do.kmvp'></span>

<h3>Description</h3>

<p>Kernel-Weighted Maximum Variance Projection (KMVP) is a generalization of
Maximum Variance Projection (MVP). Even though its name contains <em>kernel</em>, it is
not related to kernel trick well known in the machine learning community. Rather, it
generalizes the binary penalization on class discrepancy,
</p>
<p style="text-align: center;"><code class="reqn">S_{ij} = \exp(-\|x_i-x_j\|^2/t) \quad\textrm{if}\quad C_i \ne C_j</code>
</p>

<p>where <code class="reqn">x_i</code> is an <code class="reqn">i</code>-th data point and <code class="reqn">t</code> a kernel bandwidth (<code>bandwidth</code>). <b>Note</b> that
when the bandwidth value is too small, it might suffer from numerical instability and rank deficiency due to its formulation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.kmvp(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  bandwidth = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.kmvp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.kmvp_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.kmvp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.kmvp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.kmvp_+3A_bandwidth">bandwidth</code></td>
<td>
<p>bandwidth parameter for heat kernel as the equation above.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zhang T (2007).
&ldquo;Maximum Variance Projections for Face Recognition.&rdquo;
<em>Optical Engineering</em>, <b>46</b>(6), 067206.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.mvp">do.mvp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## perform KMVP with different bandwidths
out1 = do.kmvp(X, label, bandwidth=0.1)
out2 = do.kmvp(X, label, bandwidth=1)
out3 = do.kmvp(X, label, bandwidth=10)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="bandwidth=0.1", col=label, pch=19)
plot(out2$Y, main="bandwidth=1",   col=label, pch=19)
plot(out3$Y, main="bandwidth=10",  col=label, pch=19)
par(opar)

</code></pre>

<hr>
<h2 id='do.kpca'>Kernel Principal Component Analysis</h2><span id='topic+do.kpca'></span>

<h3>Description</h3>

<p>Kernel principal component analysis (KPCA/Kernel PCA) is a nonlinear extension of classical
PCA using techniques called <a href="https://en.wikipedia.org/wiki/Kernel_method">kernel trick</a>,
a common method of introducing nonlinearity by transforming, usually, covariance structure or
other gram-type estimate to make it flexible in Reproducing Kernel Hilbert Space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.kpca(
  X,
  ndim = 2,
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate"),
  kernel = c("gaussian", 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.kpca_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.kpca_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.kpca_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.kpca_+3A_kernel">kernel</code></td>
<td>
<p>a vector containing name of a kernel and corresponding parameters. See also <code><a href="#topic+aux.kernelcov">aux.kernelcov</a></code> for complete description of Kernel Trick.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>vars</dt><dd><p>variances of projected data / eigenvalues from kernelized covariance matrix.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Schölkopf B, Smola A, Müller K (1997).
&ldquo;Kernel Principal Component Analysis.&rdquo;
In Goos G, Hartmanis J, van Leeuwen J, Gerstner W, Germond A, Hasler M, Nicoud J (eds.), <em>Artificial Neural Networks — ICANN'97</em>, volume 1327, 583&ndash;588.
Springer Berlin Heidelberg, Berlin, Heidelberg.
ISBN 978-3-540-63631-1 978-3-540-69620-9.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+aux.kernelcov">aux.kernelcov</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## try out different settings
output1 &lt;- do.kpca(X)                         # default setting
output2 &lt;- do.kpca(X,kernel=c("gaussian",5))  # gaussian kernel with large bandwidth
output3 &lt;- do.kpca(X,kernel=c("laplacian",1)) # laplacian kernel

## visualize three different projections
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(output1$Y, col=label, pch=19, main="Gaussian kernel")
plot(output2$Y, col=label, pch=19, main="Gaussian kernel with sigma=5")
plot(output3$Y, col=label, pch=19, main="Laplacian kernel")
par(opar)


</code></pre>

<hr>
<h2 id='do.kqmi'>Kernel Quadratic Mutual Information</h2><span id='topic+do.kqmi'></span>

<h3>Description</h3>

<p>Kernel Quadratic Mutual Information (KQMI) is a supervised linear dimension reduction method.
Quadratic Mutual Information is an efficient nonparametric estimation method for Mutual Information
for class labels not requiring class priors. The method re-states the estimation procedure in terms of
kernel objective in the graph embedding framework.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.kqmi(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "whiten", "decorrelate"),
  t = 10
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.kqmi_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.kqmi_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.kqmi_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.kqmi_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.kqmi_+3A_t">t</code></td>
<td>
<p>bandwidth parameter for heat kernel in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Bouzas D, Arvanitopoulos N, Tefas A (2015).
&ldquo;Graph Embedded Nonparametric Mutual Information for Supervised Dimensionality Reduction.&rdquo;
<em>IEEE Transactions on Neural Networks and Learning Systems</em>, <b>26</b>(5), 951&ndash;963.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.lqmi">do.lqmi</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## generate 3 different groups of data X and label vector
x1 = matrix(rnorm(4*10), nrow=10)-20
x2 = matrix(rnorm(4*10), nrow=10)
x3 = matrix(rnorm(4*10), nrow=10)+20
X  = rbind(x1, x2, x3)
label = c(rep(1,10), rep(2,10), rep(3,10))

## try different kernel bandwidths
out1 = do.kqmi(X, label, t=0.01)
out2 = do.kqmi(X, label, t=1)
out3 = do.kqmi(X, label, t=100)

## visualize
opar = par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=label, main="KQMI::t=0.01")
plot(out2$Y, col=label, main="KQMI::t=1")
plot(out3$Y, col=label, main="KQMI::t=100")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='do.ksda'>Kernel Semi-Supervised Discriminant Analysis</h2><span id='topic+do.ksda'></span>

<h3>Description</h3>

<p>Kernel Semi-Supervised Discriminant Analysis (KSDA) is a nonlinear variant of
SDA (<code><a href="#topic+do.sda">do.sda</a></code>). For simplicity, we enabled heat/gaussian kernel only.
Note that this method is <em>quite</em> sensitive to choices of
parameters, <code>alpha</code>, <code>beta</code>, and <code>t</code>. Especially when data
are well separated in the original space, it may lead to unsatisfactory results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.ksda(
  X,
  label,
  ndim = 2,
  type = c("proportion", 0.1),
  alpha = 1,
  beta = 1,
  t = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.ksda_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.ksda_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.ksda_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.ksda_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.ksda_+3A_alpha">alpha</code></td>
<td>
<p>balancing parameter between model complexity and empirical loss.</p>
</td></tr>
<tr><td><code id="do.ksda_+3A_beta">beta</code></td>
<td>
<p>Tikhonov regularization parameter.</p>
</td></tr>
<tr><td><code id="do.ksda_+3A_t">t</code></td>
<td>
<p>bandwidth parameter for heat kernel.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Cai D, He X, Han J (2007).
&ldquo;Semi-Supervised Discriminant Analysis.&rdquo;
In <em>2007 IEEE 11th International Conference on Computer Vision</em>, 1&ndash;7.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.sda">do.sda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate data of 3 types with clear difference
set.seed(100)
dt1  = aux.gensamples(n=20)-100
dt2  = aux.gensamples(n=20)
dt3  = aux.gensamples(n=20)+100

## merge the data and create a label correspondingly
X      = rbind(dt1,dt2,dt3)
label  = rep(1:3, each=20)

## copy a label and let 10% of elements be missing
nlabel = length(label)
nmissing = round(nlabel*0.10)
label_missing = label
label_missing[sample(1:nlabel, nmissing)]=NA

## compare true case with missing-label case
out1 = do.ksda(X, label, beta=0, t=0.1)
out2 = do.ksda(X, label_missing, beta=0, t=0.1)

## visualize
opar = par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(out1$Y, col=label, main="true projection")
plot(out2$Y, col=label, main="20% missing labels")
par(opar)

</code></pre>

<hr>
<h2 id='do.kudp'>Kernel-Weighted Unsupervised Discriminant Projection</h2><span id='topic+do.kudp'></span>

<h3>Description</h3>

<p>Kernel-Weighted Unsupervised Discriminant Projection (KUDP) is a generalization of UDP where
proximity is given by weighted values via heat kernel,
</p>
<p style="text-align: center;"><code class="reqn">K_{i,j} = \exp(-\|x_i-x_j\|^2/bandwidth)</code>
</p>

<p>whence UDP uses binary connectivity. If <code>bandwidth</code> is <code class="reqn">+\infty</code>, it becomes
a standard UDP problem. Like UDP, it also performs PCA preprocessing for rank-deficient case.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.kudp(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  bandwidth = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.kudp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.kudp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.kudp_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.kudp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.kudp_+3A_bandwidth">bandwidth</code></td>
<td>
<p>bandwidth parameter for heat kernel as the equation above.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>interimdim</dt><dd><p>the number of PCA target dimension used in preprocessing.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Yang J, Zhang D, Yang J, Niu B (2007).
&ldquo;Globally Maximizing, Locally Minimizing: Unsupervised Discriminant Projection with Applications to Face and Palm Biometrics.&rdquo;
<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <b>29</b>(4), 650&ndash;664.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.udp">do.udp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris dataset
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
lab   = as.factor(iris[subid,5])

## use different kernel bandwidth
out1 &lt;- do.kudp(X, bandwidth=0.1)
out2 &lt;- do.kudp(X, bandwidth=10)
out3 &lt;- do.kudp(X, bandwidth=1000)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=lab, pch=19, main="bandwidth=0.1")
plot(out2$Y, col=lab, pch=19, main="bandwidth=10")
plot(out3$Y, col=lab, pch=19, main="bandwidth=1000")
par(opar)

</code></pre>

<hr>
<h2 id='do.lamp'>Local Affine Multidimensional Projection</h2><span id='topic+do.lamp'></span>

<h3>Description</h3>

<p>Local Affine Mulditimensional Projection (<em>LAMP</em>) can be considered as
a nonlinear method even though each datum is projected using locally estimated
affine mapping. It first finds a low-dimensional embedding for control points
and then locates the rest data using affine mapping. We use <code class="reqn">\sqrt{n}</code> number
of data as controls and Stochastic Neighborhood Embedding is applied as an
initial projection of control set. Note that this belongs to the method for
visualization so projection onto <code class="reqn">\mathbf{R}^2</code> is suggested for use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lamp(X, ndim = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lamp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lamp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Joia P, Paulovich FV, Coimbra D, Cuminato JA, Nonato LG (2011).
&ldquo;Local Affine Multidimensional Projection.&rdquo;
<em>IEEE Transactions on Visualization and Computer Graphics</em>, <b>17</b>(12), 2563&ndash;2571.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.sne">do.sne</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## let's compare with PCA
out1 &lt;- do.pca(X, ndim=2)      # PCA
out2 &lt;- do.lamp(X, ndim=2)     # LAMP

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(out1$Y, pch=19, col=label, main="PCA")
plot(out2$Y, pch=19, col=label, main="LAMP")
par(opar)


</code></pre>

<hr>
<h2 id='do.lapeig'>Laplacian Eigenmaps</h2><span id='topic+do.lapeig'></span>

<h3>Description</h3>

<p><code>do.lapeig</code> performs Laplacian Eigenmaps (LE) to discover low-dimensional
manifold embedded in high-dimensional data space using graph laplacians. This
is a classic algorithm employing spectral graph theory.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lapeig(X, ndim = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lapeig_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lapeig_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lapeig_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>kernelscale</dt><dd><p>kernel scale parameter. Default value is 1.0.</p>
</dd>
<dt>preprocess</dt><dd><p>an additional option for preprocessing the data.
Default is <code>"null"</code>. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</dd>
<dt>symmetric</dt><dd><p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</dd>
<dt>type</dt><dd><p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</dd>
<dt>weighted</dt><dd><p>a logical; <code>TRUE</code> for weighted graph laplacian and <code>FALSE</code> for
combinatorial laplacian where connectivity is represented as 1 or 0 only.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>eigvals</dt><dd><p>a vector of eigenvalues for laplacian matrix.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Belkin M, Niyogi P (2003).
&ldquo;Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.&rdquo;
<em>Neural Computation</em>, <b>15</b>(6), 1373&ndash;1396.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
lab   = as.factor(iris[subid,5])

## try different levels of connectivity
out1 &lt;- do.lapeig(X, type=c("proportion",0.5), weighted=FALSE)
out2 &lt;- do.lapeig(X, type=c("proportion",0.10), weighted=FALSE)
out3 &lt;- do.lapeig(X, type=c("proportion",0.25), weighted=FALSE)

## Visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=lab, main="5% connected")
plot(out2$Y, pch=19, col=lab, main="10% connected")
plot(out3$Y, pch=19, col=lab, main="25% connected")
par(opar)


</code></pre>

<hr>
<h2 id='do.lasso'>Least Absolute Shrinkage and Selection Operator</h2><span id='topic+do.lasso'></span>

<h3>Description</h3>

<p>LASSO is a popular regularization scheme in linear regression in pursuit of sparsity in coefficient vector
that has been widely used. The method can be used in feature selection in that given the regularization parameter,
it first solves the problem and takes indices of estimated coefficients with the largest magnitude as
meaningful features by solving
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_{\beta} ~ \frac{1}{2}\|X\beta-y\|_2^2 + \lambda \|\beta\|_1</code>
</p>

<p>where <code class="reqn">y</code> is <code>response</code> in our method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lasso(X, response, ndim = 2, lambda = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lasso_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lasso_+3A_response">response</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of response variable.</p>
</td></tr>
<tr><td><code id="do.lasso_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lasso_+3A_lambda">lambda</code></td>
<td>
<p>sparsity regularization parameter in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Tibshirani R (1996).
&ldquo;Regression Shrinkage and Selection via the Lasso.&rdquo;
<em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, <b>58</b>(1), 267&ndash;288.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate swiss roll with auxiliary dimensions
## it follows reference example from LSIR paper.
set.seed(1)
n = 123
theta = runif(n)
h     = runif(n)
t     = (1+2*theta)*(3*pi/2)
X     = array(0,c(n,10))
X[,1] = t*cos(t)
X[,2] = 21*h
X[,3] = t*sin(t)
X[,4:10] = matrix(runif(7*n), nrow=n)

## corresponding response vector
y = sin(5*pi*theta)+(runif(n)*sqrt(0.1))

## try different regularization parameters
out1 = do.lasso(X, y, lambda=0.1)
out2 = do.lasso(X, y, lambda=1)
out3 = do.lasso(X, y, lambda=10)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="LASSO::lambda=0.1")
plot(out2$Y, main="LASSO::lambda=1")
plot(out3$Y, main="LASSO::lambda=10")
par(opar)


</code></pre>

<hr>
<h2 id='do.lda'>Linear Discriminant Analysis</h2><span id='topic+do.lda'></span>

<h3>Description</h3>

<p>Linear Discriminant Analysis (LDA) originally aims to find a set of features
that best separate groups of data. Since we need <em>label</em> information,
LDA belongs to a class of supervised methods of performing classification.
However, since it is based on finding <em>suitable</em> projections, it can still
be used to do dimension reduction. We support both binary and multiple-class cases.
Note that the target dimension <code>ndim</code> should be <em>less than or equal to</em> <code>K-1</code>,
where <code>K</code> is the number of classes, or <code>K=length(unique(label))</code>. Our code
automatically gives bounds on user's choice to correspond to what theory has shown. See
the comments section for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lda(X, label, ndim = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lda_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lda_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.lda_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Limit of Target Dimension Selection</h3>

<p>In unsupervised algorithms, selection of <code>ndim</code> is arbitrary as long as
the target dimension is lower-dimensional than original data dimension, i.e., <code>ndim &lt; p</code>.
In LDA, it is <em>not allowed</em>. Suppose we have <code>K</code> classes, then its formulation on
<code class="reqn">S_B</code>, between-group variance, has maximum rank of <code>K-1</code>. Therefore, the maximal
subspace can only be spanned by at most <code>K-1</code> orthogonal vectors.
</p>


<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Fisher RA (1936).
&ldquo;THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS.&rdquo;
<em>Annals of Eugenics</em>, <b>7</b>(2), 179&ndash;188.
</p>
<p>Fukunaga K (1990).
<em>Introduction to Statistical Pattern Recognition</em>,  Computer Science and Scientific Computing, 2nd ed edition.
Academic Press, Boston.
ISBN 978-0-12-269851-4.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris dataset
data(iris)
X     = as.matrix(iris[,1:4])
lab   = as.factor(iris[,5])

## compare with PCA
outLDA = do.lda(X, lab, ndim=2)
outPCA = do.pca(X, ndim=2)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(outLDA$Y, col=lab, pch=19, main="LDA")
plot(outPCA$Y, col=lab, pch=19, main="PCA")
par(opar)


</code></pre>

<hr>
<h2 id='do.ldakm'>Combination of LDA and K-means</h2><span id='topic+do.ldakm'></span>

<h3>Description</h3>

<p><code>do.ldakm</code> is an unsupervised subspace discovery method that combines linear discriminant analysis (LDA) and K-means algorithm.
It tries to build an adaptive framework that selects the most discriminative subspace. It iteratively applies two methods in that
the clustering process is integrated with the subspace selection, and continuously updates its discrimative basis. From its formulation
with respect to generalized eigenvalue problem, it can be considered as generalization of Adaptive Subspace Iteration (ASI) and Adaptive Dimension Reduction (ADR).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.ldakm(
  X,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  maxiter = 10,
  abstol = 0.001
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.ldakm_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.ldakm_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.ldakm_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.ldakm_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations allowed.</p>
</td></tr>
<tr><td><code id="do.ldakm_+3A_abstol">abstol</code></td>
<td>
<p>stopping criterion for incremental change in projection matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Ding C, Li T (2007).
&ldquo;Adaptive Dimension Reduction Using Discriminant Analysis and K-Means Clustering.&rdquo;
In <em>Proceedings of the 24th International Conference on Machine Learning</em>, 521&ndash;528.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.asi">do.asi</a></code>, <code><a href="#topic+do.adr">do.adr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris dataset
data(iris)
set.seed(100)
subid &lt;- sample(1:150, 50)
X     &lt;- as.matrix(iris[subid,1:4])
lab   &lt;- as.factor(iris[subid,5])

## try different tolerance level
out1 = do.ldakm(X, abstol=1e-2)
out2 = do.ldakm(X, abstol=1e-3)
out3 = do.ldakm(X, abstol=1e-4)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=lab, main="LDA-KM::tol=1e-2")
plot(out2$Y, pch=19, col=lab, main="LDA-KM::tol=1e-3")
plot(out3$Y, pch=19, col=lab, main="LDA-KM::tol=1e-4")
par(opar)

</code></pre>

<hr>
<h2 id='do.lde'>Local Discriminant Embedding</h2><span id='topic+do.lde'></span>

<h3>Description</h3>

<p>Local Discriminant Embedding (LDE) is a supervised algorithm that learns
the embedding for the submanifold of each class. Its idea is to same-class
data points maintain their original neighborhood information while
segregating different-class data distinct from each other.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lde(
  X,
  label,
  ndim = 2,
  t = 1,
  numk = max(ceiling(nrow(X)/10), 2),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lde_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.lde_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.lde_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lde_+3A_t">t</code></td>
<td>
<p>kernel bandwidth in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
<tr><td><code id="do.lde_+3A_numk">numk</code></td>
<td>
<p>the number of neighboring points for k-nn graph construction.</p>
</td></tr>
<tr><td><code id="do.lde_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Hwann-Tzong Chen, Huang-Wei Chang, Tyng-Luh Liu (2005).
&ldquo;Local Discriminant Embedding and Its Variants.&rdquo;
In <em>2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em>, volume 2, 846&ndash;853.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate data of 2 types with clear difference
set.seed(100)
diff = 15
dt1  = aux.gensamples(n=50)-diff;
dt2  = aux.gensamples(n=50)+diff;

## merge the data and create a label correspondingly
X      = rbind(dt1,dt2)
label  = rep(1:2, each=50)

## try different neighborhood size
out1 &lt;- do.lde(X, label, numk=5)
out2 &lt;- do.lde(X, label, numk=10)
out3 &lt;- do.lde(X, label, numk=25)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="LDE::k=5")
plot(out2$Y, pch=19, col=label, main="LDE::k=10")
plot(out3$Y, pch=19, col=label, main="LDE::k=25")
par(opar)

</code></pre>

<hr>
<h2 id='do.ldp'>Locally Discriminating Projection</h2><span id='topic+do.ldp'></span>

<h3>Description</h3>

<p>Locally Discriminating Projection (LDP) is a supervised linear dimension reduction method.
It utilizes both label/class information and local neighborhood information to discover
the intrinsic structure of the data. It can be considered as an extension
of LPP in a supervised manner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.ldp(
  X,
  label,
  ndim = 2,
  type = c("proportion", 0.1),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  beta = 10
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.ldp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.ldp_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.ldp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.ldp_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.ldp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.ldp_+3A_beta">beta</code></td>
<td>
<p>bandwidth parameter for heat kernel in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zhao H, Sun S, Jing Z, Yang J (2006).
&ldquo;Local Structure Based Supervised Feature Extraction.&rdquo;
<em>Pattern Recognition</em>, <b>39</b>(8), 1546&ndash;1550.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate data of 3 types with clear difference
dt1  = aux.gensamples(n=20)-100
dt2  = aux.gensamples(n=20)
dt3  = aux.gensamples(n=20)+100

## merge the data and create a label correspondingly
X      = rbind(dt1,dt2,dt3)
label  = rep(1:3, each=20)

## try different neighborhood sizes
out1 = do.ldp(X, label, type=c("proportion",0.10))
out2 = do.ldp(X, label, type=c("proportion",0.25))
out3 = do.ldp(X, label, type=c("proportion",0.50))

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=label, pch=19, main="10% connectivity")
plot(out2$Y, col=label, pch=19, main="25% connectivity")
plot(out3$Y, col=label, pch=19, main="50% connectivity")
par(opar)

</code></pre>

<hr>
<h2 id='do.lea'>Locally Linear Embedded Eigenspace Analysis</h2><span id='topic+do.lea'></span>

<h3>Description</h3>

<p>Locally Linear Embedding (LLE) is a powerful nonlinear manifold learning method. This method,
Locally Linear Embedded Eigenspace Analysis - LEA, in short - is a linear approximation to LLE,
similar to Neighborhood Preserving Embedding. In our implementation, the choice of weight binarization
is removed in order to respect original work. For 1-dimensional projection, which is rarely performed,
authors provided a detour for rank correcting mechanism but it is omitted for practical reason.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lea(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  symmetric = c("union", "intersect", "asymmetric"),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lea_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lea_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lea_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lea_+3A_symmetric">symmetric</code></td>
<td>
<p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lea_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Fu Y, Huang TS (2005).
&ldquo;Locally Linear Embedded Eigenspace Analysis.&rdquo;
<em>IFP-TR, UIUC</em>, <b>2005</b>, 2&ndash;05.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.npe">do.npe</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## use iris dataset
data(iris)
set.seed(100)
subid &lt;- sample(1:150, 50)
X     &lt;- as.matrix(iris[subid,1:4])
lab   &lt;- as.factor(iris[subid,5])

## compare LEA with LLE and another approximation NPE
out1 &lt;- do.lle(X, ndim=2)
out2 &lt;- do.npe(X, ndim=2)
out3 &lt;- do.lea(X, ndim=2)

## visual comparison
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=lab, main="LLE")
plot(out2$Y, pch=19, col=lab, main="NPE")
plot(out3$Y, pch=19, col=lab, main="LEA")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='do.lfda'>Local Fisher Discriminant Analysis</h2><span id='topic+do.lfda'></span>

<h3>Description</h3>

<p>Local Fisher Discriminant Analysis (LFDA) is a linear dimension reduction method for
supervised case, i.e., labels are given. It reflects <em>local</em> information to overcome
undesired results of traditional Fisher Discriminant Analysis which results in a poor mapping
when samples in a single class form form several separate clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lfda(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  type = c("proportion", 0.1),
  symmetric = c("union", "intersect", "asymmetric"),
  localscaling = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lfda_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lfda_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.lfda_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lfda_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lfda_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lfda_+3A_symmetric">symmetric</code></td>
<td>
<p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lfda_+3A_localscaling">localscaling</code></td>
<td>
<p><code>TRUE</code> to use local scaling method for construction affinity matrix, <code>FALSE</code> for binary affinity.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Sugiyama M (2006).
&ldquo;Local Fisher Discriminant Analysis for Supervised Dimensionality Reduction.&rdquo;
In <em>Proceedings of the 23rd International Conference on Machine Learning</em>, 905&ndash;912.
</p>
<p>Zelnik-manor L, Perona P (2005).
&ldquo;Self-Tuning Spectral Clustering.&rdquo;
In Saul LK, Weiss Y, Bottou L (eds.), <em>Advances in Neural Information Processing Systems 17</em>, 1601&ndash;1608.
MIT Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate 3 different groups of data X and label vector
x1 = matrix(rnorm(4*10), nrow=10)-20
x2 = matrix(rnorm(4*10), nrow=10)
x3 = matrix(rnorm(4*10), nrow=10)+20
X     = rbind(x1, x2, x3)
label = rep(1:3, each=10)

## try different affinity matrices
out1 = do.lfda(X, label)
out2 = do.lfda(X, label, localscaling=FALSE)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(out1$Y, col=label, main="binary affinity matrix")
plot(out2$Y, col=label, main="local scaling affinity")
par(opar)

</code></pre>

<hr>
<h2 id='do.lisomap'>Landmark Isometric Feature Mapping</h2><span id='topic+do.lisomap'></span>

<h3>Description</h3>

<p>Landmark Isomap is a variant of Isomap in that
it first finds a low-dimensional embedding using a small portion of given dataset
and graft the others in a manner to preserve as much pairwise distance from
all the other data points to landmark points as possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lisomap(
  X,
  ndim = 2,
  ltype = c("random", "MaxMin"),
  npoints = max(nrow(X)/5, ndim + 1),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  type = c("proportion", 0.1),
  symmetric = c("union", "intersect", "asymmetric"),
  weight = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lisomap_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lisomap_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lisomap_+3A_ltype">ltype</code></td>
<td>
<p>on how to select landmark points, either <code>"random"</code> or <code>"MaxMin"</code>.</p>
</td></tr>
<tr><td><code id="do.lisomap_+3A_npoints">npoints</code></td>
<td>
<p>the number of landmark points to be drawn.</p>
</td></tr>
<tr><td><code id="do.lisomap_+3A_preprocess">preprocess</code></td>
<td>
<p>an option for preprocessing the data. Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lisomap_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lisomap_+3A_symmetric">symmetric</code></td>
<td>
<p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lisomap_+3A_weight">weight</code></td>
<td>
<p><code>TRUE</code> to perform Landmark Isomap on weighted graph, or <code>FALSE</code> otherwise.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Silva VD, Tenenbaum JB (2003).
&ldquo;Global Versus Local Methods in Nonlinear Dimensionality Reduction.&rdquo;
In Becker S, Thrun S, Obermayer K (eds.), <em>Advances in Neural Information Processing Systems 15</em>, 721&ndash;728.
MIT Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.isomap">do.isomap</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
data(iris)
X   &lt;- as.matrix(iris[,1:4])
lab &lt;- as.factor(iris[,5])

## use different number of data points as landmarks
output1 &lt;- do.lisomap(X, npoints=10, type=c("proportion",0.25))
output2 &lt;- do.lisomap(X, npoints=25, type=c("proportion",0.25))
output3 &lt;- do.lisomap(X, npoints=50, type=c("proportion",0.25))

## visualize three different projections
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(output1$Y, pch=19, col=lab, main="10 landmarks")
plot(output2$Y, pch=19, col=lab, main="25 landmarks")
plot(output3$Y, pch=19, col=lab, main="50 landmarks")
par(opar)


</code></pre>

<hr>
<h2 id='do.lle'>Locally Linear Embedding</h2><span id='topic+do.lle'></span>

<h3>Description</h3>

<p>Locally-Linear Embedding (LLE) was introduced approximately at the same time as Isomap.
Its idea was motivated to describe entire data manifold by making a chain of local patches
in that low-dimensional embedding should resemble the connectivity pattern of patches.
<code>do.lle</code> also provides an automatic choice of regularization parameter based on an
optimality criterion suggested by authors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lle(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  symmetric = "union",
  weight = TRUE,
  preprocess = c("null", "center", "scale", "cscale", "decorrelate", "whiten"),
  regtype = FALSE,
  regparam = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lle_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lle_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lle_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lle_+3A_symmetric">symmetric</code></td>
<td>
<p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>.
See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lle_+3A_weight">weight</code></td>
<td>
<p><code>TRUE</code> to perform LLE on weighted graph, or <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="do.lle_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lle_+3A_regtype">regtype</code></td>
<td>
<p><code>TRUE</code> for automatic regularization parameter selection, <code>FALSE</code> otherwise as default.</p>
</td></tr>
<tr><td><code id="do.lle_+3A_regparam">regparam</code></td>
<td>
<p>regularization parameter.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>eigvals</dt><dd><p>a vector of eigenvalues from computation of embedding matrix.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Roweis ST (2000).
&ldquo;Nonlinear Dimensionality Reduction by Locally Linear Embedding.&rdquo;
<em>Science</em>, <b>290</b>(5500), 2323&ndash;2326.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate swiss-roll data
set.seed(100)
X = aux.gensamples(n=100)

## 1. connecting 10% of data for graph construction.
output1 &lt;- do.lle(X,ndim=2,type=c("proportion",0.10))

## 2. constructing 20%-connected graph
output2 &lt;- do.lle(X,ndim=2,type=c("proportion",0.20))

## 3. constructing 50%-connected with bigger regularization parameter
output3 &lt;- do.lle(X,ndim=2,type=c("proportion",0.5),regparam=10)

## Visualize three different projections
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(output1$Y, main="5%")
plot(output2$Y, main="10%")
plot(output3$Y, main="50%+Binary")
par(opar)


</code></pre>

<hr>
<h2 id='do.llle'>Local Linear Laplacian Eigenmaps</h2><span id='topic+do.llle'></span>

<h3>Description</h3>

<p>Local Linear Laplacian Eigenmaps is an unsupervised manifold learning method as an
extension of Local Linear Embedding (<code><a href="#topic+do.lle">do.lle</a></code>). It is claimed to be
more robust to local structure and noises. It involves the concept of
artificial neighborhood in constructing the adjacency graph for reconstruction of
the approximated manifold.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.llle(
  X,
  ndim = 2,
  preprocess = c("null", "center", "scale", "cscale", "decorrelate", "whiten"),
  K = round(nrow(X)/2),
  P = max(round(nrow(X)/4), 2),
  bandwidth = 0.2
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.llle_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.llle_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.llle_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is <code>"null"</code>. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.llle_+3A_k">K</code></td>
<td>
<p>size of near neighborhood for each data point.</p>
</td></tr>
<tr><td><code id="do.llle_+3A_p">P</code></td>
<td>
<p>size of artifical neighborhood.</p>
</td></tr>
<tr><td><code id="do.llle_+3A_bandwidth">bandwidth</code></td>
<td>
<p>scale parameter for Gaussian kernel. It should be in <code class="reqn">(0,1)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Liu F, Zhang W, Gu S (2016).
&ldquo;Local Linear Laplacian Eigenmaps: A Direct Extension of LLE.&rdquo;
<em>Pattern Recognition Letters</em>, <b>75</b>, 30&ndash;35.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.lle">do.lle</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## use iris data
data(iris)
X     = as.matrix(iris[,1:4])
label = as.integer(iris$Species)

# see the effect bandwidth
out1 = do.llle(X, bandwidth=0.1, P=20)
out2 = do.llle(X, bandwidth=0.5, P=20)
out3 = do.llle(X, bandwidth=0.9, P=20)

# visualize the results
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=label, main="bandwidth=0.1")
plot(out2$Y, col=label, main="bandwidth=0.5")
plot(out3$Y, col=label, main="bandwidth=0.9")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='do.llp'>Local Learning Projections</h2><span id='topic+do.llp'></span>

<h3>Description</h3>

<p>While Principal Component Analysis (PCA) aims at minimizing global estimation error, Local Learning
Projection (LLP) approach tries to find the projection with the minimal <em>local</em>
estimation error in the sense that each projected datum can be well represented
based on ones neighbors. For the kernel part, we only enabled to use
a gaussian kernel as suggested from the original paper. The parameter <code>lambda</code>
controls possible rank-deficiency of kernel matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.llp(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  symmetric = c("union", "intersect", "asymmetric"),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  t = 1,
  lambda = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.llp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations</p>
</td></tr>
<tr><td><code id="do.llp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.llp_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.llp_+3A_symmetric">symmetric</code></td>
<td>
<p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>.
See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.llp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.llp_+3A_t">t</code></td>
<td>
<p>bandwidth for heat kernel in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
<tr><td><code id="do.llp_+3A_lambda">lambda</code></td>
<td>
<p>regularization parameter for kernel matrix in <code class="reqn">[0,\infty)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>References</h3>

<p>Wu M, Yu K, Yu S, Schölkopf B (2007).
&ldquo;Local Learning Projections.&rdquo;
In <em>Proceedings of the 24th International Conference on Machine Learning</em>, 1039&ndash;1046.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate data
set.seed(100)
X &lt;- aux.gensamples(n=100, dname="crown")

## test different lambda - regularization - values
out1 &lt;- do.llp(X,ndim=2,lambda=0.1)
out2 &lt;- do.llp(X,ndim=2,lambda=1)
out3 &lt;- do.llp(X,ndim=2,lambda=10)

# visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, main="lambda=0.1")
plot(out2$Y, pch=19, main="lambda=1")
plot(out3$Y, pch=19, main="lambda=10")
par(opar)


</code></pre>

<hr>
<h2 id='do.lltsa'>Linear Local Tangent Space Alignment</h2><span id='topic+do.lltsa'></span>

<h3>Description</h3>

<p>Linear Local Tangent Space Alignment (LLTSA) is a linear variant of the
celebrated LTSA method. It uses the tangent space in the neighborhood for each data point
to represent the local geometry. Alignment of those local tangent spaces in the low-dimensional space
returns an explicit mapping from the high-dimensional space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lltsa(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  symmetric = c("union", "intersect", "asymmetric"),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lltsa_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lltsa_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lltsa_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lltsa_+3A_symmetric">symmetric</code></td>
<td>
<p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lltsa_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zhang T, Yang J, Zhao D, Ge X (2007).
&ldquo;Linear Local Tangent Space Alignment and Application to Face Recognition.&rdquo;
<em>Neurocomputing</em>, <b>70</b>(7-9), 1547&ndash;1553.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.ltsa">do.ltsa</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris dataset
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
lab   = as.factor(iris[subid,5])

## try different neighborhood size
out1 &lt;- do.lltsa(X, type=c("proportion",0.25))
out2 &lt;- do.lltsa(X, type=c("proportion",0.50))
out3 &lt;- do.lltsa(X, type=c("proportion",0.75))

## Visualize three different projections
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=lab, pch=19, main="LLTSA::25% connected")
plot(out2$Y, col=lab, pch=19, main="LLTSA::50% connected")
plot(out3$Y, col=lab, pch=19, main="LLTSA::75% connected")
par(opar)

</code></pre>

<hr>
<h2 id='do.lmds'>Landmark Multidimensional Scaling</h2><span id='topic+do.lmds'></span>

<h3>Description</h3>

<p>Landmark MDS is a variant of Classical Multidimensional Scaling in that
it first finds a low-dimensional embedding using a small portion of given dataset
and graft the others in a manner to preserve as much pairwise distance from
all the other data points to landmark points as possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lmds(X, ndim = 2, npoints = max(nrow(X)/5, ndim + 1))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lmds_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lmds_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lmds_+3A_npoints">npoints</code></td>
<td>
<p>the number of landmark points to be drawn.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Silva VD, Tenenbaum JB (2002).
&ldquo;Global Versus Local Methods in Nonlinear Dimensionality Reduction.&rdquo;
In Thrun S, Obermayer K (eds.), <em>Advances in Neural Information Processing Systems 15</em>, 705&ndash;712.
MIT Press, Cambridge, MA.
</p>
<p>Lee S, Choi S (2009).
&ldquo;Landmark MDS Ensemble.&rdquo;
<em>Pattern Recognition</em>, <b>42</b>(9), 2045&ndash;2053.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.mds">do.mds</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
data(iris)
X     = as.matrix(iris[,1:4])
lab   = as.factor(iris[,5])

## use 10% and 25% of the data and compare with full MDS
output1 &lt;- do.lmds(X, ndim=2, npoints=round(nrow(X)*0.10))
output2 &lt;- do.lmds(X, ndim=2, npoints=round(nrow(X)*0.25))
output3 &lt;- do.mds(X, ndim=2)

## vsualization
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(output1$Y, pch=19, col=lab, main="10% random points")
plot(output2$Y, pch=19, col=lab, main="25% random points")
plot(output3$Y, pch=19, col=lab, main="original MDS")
par(opar)


</code></pre>

<hr>
<h2 id='do.lpca2006'>Locally Principal Component Analysis by Yang et al. (2006)</h2><span id='topic+do.lpca2006'></span>

<h3>Description</h3>

<p>Locally Principal Component Analysis (LPCA) is an unsupervised linear dimension reduction method.
It focuses on the information brought by local neighborhood structure and seeks the corresponding
structure, which may contain useful information for revealing discriminative information of the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lpca2006(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lpca2006_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lpca2006_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lpca2006_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lpca2006_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Yang J, Zhang D, Yang J (2006).
&ldquo;Locally Principal Component Learning for Face Representation and Recognition.&rdquo;
<em>Neurocomputing</em>, <b>69</b>(13-15), 1697&ndash;1701.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris dataset
data(iris)
set.seed(100)
subid = sample(1:150,100)
X     = as.matrix(iris[subid,1:4])
lab   = as.factor(iris[subid,5])

## try different neighborhood size
out1 &lt;- do.lpca2006(X, ndim=2, type=c("proportion",0.25))
out2 &lt;- do.lpca2006(X, ndim=2, type=c("proportion",0.50))
out3 &lt;- do.lpca2006(X, ndim=2, type=c("proportion",0.75))

## Visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=lab, main="LPCA2006::25% connected")
plot(out2$Y, pch=19, col=lab, main="LPCA2006::50% connected")
plot(out3$Y, pch=19, col=lab, main="LPCA2006::75% connected")
par(opar)


</code></pre>

<hr>
<h2 id='do.lpe'>Locality Pursuit Embedding</h2><span id='topic+do.lpe'></span>

<h3>Description</h3>

<p>Locality Pursuit Embedding (LPE) is an unsupervised linear dimension reduction method.
It aims at preserving local structure by solving a variational problem that models
the local geometrical structure by the Euclidean distances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lpe(
  X,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  numk = max(ceiling(nrow(X)/10), 2)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lpe_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lpe_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lpe_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lpe_+3A_numk">numk</code></td>
<td>
<p>size of <code class="reqn">k</code>-nn neighborhood in original dimensional space.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Min W, Lu K, He X (2004).
&ldquo;Locality Pursuit Embedding.&rdquo;
<em>Pattern Recognition</em>, <b>37</b>(4), 781&ndash;788.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate swiss roll with auxiliary dimensions
set.seed(100)
n     = 100
theta = runif(n)
h     = runif(n)
t     = (1+2*theta)*(3*pi/2)
X     = array(0,c(n,10))
X[,1] = t*cos(t)
X[,2] = 21*h
X[,3] = t*sin(t)
X[,4:10] = matrix(runif(7*n), nrow=n)

## try with different neighborhood sizes
out1 = do.lpe(X, numk=5)
out2 = do.lpe(X, numk=10)
out3 = do.lpe(X, numk=25)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="LPE::numk=5")
plot(out2$Y, main="LPE::numk=10")
plot(out3$Y, main="LPE::numk=25")
par(opar)


</code></pre>

<hr>
<h2 id='do.lpfda'>Locality Preserving Fisher Discriminant Analysis</h2><span id='topic+do.lpfda'></span>

<h3>Description</h3>

<p>Locality Preserving Fisher Discriminant Analysis (LPFDA) is a supervised variant of LPP.
It can also be seemed as an improved version of LDA where the locality structure of the data
is preserved. The algorithm aims at getting a subspace projection matrix by solving a generalized
eigenvalue problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lpfda(
  X,
  label,
  ndim = 2,
  type = c("proportion", 0.1),
  preprocess = c("center", "scale", "cscale", "whiten", "decorrelate"),
  t = 10
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lpfda_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lpfda_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.lpfda_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lpfda_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lpfda_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lpfda_+3A_t">t</code></td>
<td>
<p>bandwidth parameter for heat kernel in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zhao X, Tian X (2009).
&ldquo;Locality Preserving Fisher Discriminant Analysis for Face Recognition.&rdquo;
In Huang D, Jo K, Lee H, Kang H, Bevilacqua V (eds.), <em>Emerging Intelligent Computing Technology and Applications</em>, 261&ndash;269.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate data of 3 types with clear difference
set.seed(100)
dt1  = aux.gensamples(n=20)-50
dt2  = aux.gensamples(n=20)
dt3  = aux.gensamples(n=20)+50

## merge the data and create a label correspondingly
X      = rbind(dt1,dt2,dt3)
label  = rep(1:3, each=20)

## try different proportion of connected edges
out1 = do.lpfda(X, label, type=c("proportion",0.10))
out2 = do.lpfda(X, label, type=c("proportion",0.25))
out3 = do.lpfda(X, label, type=c("proportion",0.50))

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="10% connectivity")
plot(out2$Y, pch=19, col=label, main="25% connectivity")
plot(out3$Y, pch=19, col=label, main="50% connectivity")
par(opar)

</code></pre>

<hr>
<h2 id='do.lpmip'>Locality-Preserved Maximum Information Projection</h2><span id='topic+do.lpmip'></span>

<h3>Description</h3>

<p>Locality-Preserved Maximum Information Projection (LPMIP) is an unsupervised linear dimension reduction method
to identify the underlying manifold structure by learning both the within- and between-locality information. The
parameter <code>alpha</code> is balancing the tradeoff between two and the flexibility of this model enables an interpretation
of it as a generalized extension of LPP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lpmip(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate"),
  sigma = 10,
  alpha = 0.5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lpmip_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lpmip_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lpmip_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lpmip_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lpmip_+3A_sigma">sigma</code></td>
<td>
<p>bandwidth parameter for heat kernel in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
<tr><td><code id="do.lpmip_+3A_alpha">alpha</code></td>
<td>
<p>balancing parameter between two locality information in <code class="reqn">[0,1]</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Haixian Wang, Sibao Chen, Zilan Hu, Wenming Zheng (2008).
&ldquo;Locality-Preserved Maximum Information Projection.&rdquo;
<em>IEEE Transactions on Neural Networks</em>, <b>19</b>(4), 571&ndash;585.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris dataset
data(iris)
set.seed(100)
subid &lt;- sample(1:150, 50)
X     &lt;- as.matrix(iris[subid,1:4])
lab   &lt;- as.factor(iris[subid,5])

## try different neighborhood size
out1 &lt;- do.lpmip(X, ndim=2, type=c("proportion",0.10))
out2 &lt;- do.lpmip(X, ndim=2, type=c("proportion",0.25))
out3 &lt;- do.lpmip(X, ndim=2, type=c("proportion",0.50))

## Visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=lab, main="10% connected")
plot(out2$Y, pch=19, col=lab, main="25% connected")
plot(out3$Y, pch=19, col=lab, main="50% connected")
par(opar)

</code></pre>

<hr>
<h2 id='do.lpp'>Locality Preserving Projection</h2><span id='topic+do.lpp'></span>

<h3>Description</h3>

<p><code>do.lpp</code> is a linear approximation to Laplacian Eigenmaps. More precisely,
it aims at finding a linear approximation to the eigenfunctions of the Laplace-Beltrami
operator on the graph-approximated data manifold.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lpp(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  symmetric = c("union", "intersect", "asymmetric"),
  preprocess = c("center", "scale", "cscale", "whiten", "decorrelate"),
  t = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lpp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations</p>
</td></tr>
<tr><td><code id="do.lpp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lpp_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lpp_+3A_symmetric">symmetric</code></td>
<td>
<p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>.
See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lpp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is <code>"center"</code>. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lpp_+3A_t">t</code></td>
<td>
<p>bandwidth for heat kernel in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>He X (2005).
<em>Locality Preserving Projections</em>.
PhD Thesis, University of Chicago, Chicago, IL, USA.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris dataset
data(iris)
set.seed(100)
subid &lt;- sample(1:150, 50)
X     &lt;- as.matrix(iris[subid,1:4])
lab   &lt;- as.factor(iris[subid,5])

## try different kernel bandwidths
out1 &lt;- do.lpp(X, t=0.1)
out2 &lt;- do.lpp(X, t=1)
out3 &lt;- do.lpp(X, t=10)

## Visualize three different projections
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=lab, pch=19, main="LPP::bandwidth=0.1")
plot(out2$Y, col=lab, pch=19, main="LPP::bandwidth=1")
plot(out3$Y, col=lab, pch=19, main="LPP::bandwidth=10")
par(opar)

</code></pre>

<hr>
<h2 id='do.lqmi'>Linear Quadratic Mutual Information</h2><span id='topic+do.lqmi'></span>

<h3>Description</h3>

<p>Linear Quadratic Mutual Information (LQMI) is a supervised linear dimension reduction method.
Quadratic Mutual Information is an efficient nonparametric estimation method for Mutual Information
for class labels not requiring class priors. For the KQMI formulation, LQMI is a linear equivalent.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lqmi(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "whiten", "decorrelate")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lqmi_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lqmi_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.lqmi_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lqmi_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Bouzas D, Arvanitopoulos N, Tefas A (2015).
&ldquo;Graph Embedded Nonparametric Mutual Information for Supervised Dimensionality Reduction.&rdquo;
<em>IEEE Transactions on Neural Networks and Learning Systems</em>, <b>26</b>(5), 951&ndash;963.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.kqmi">do.kqmi</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## compare against LDA
out1 = do.lda(X, label)
out2 = do.lqmi(X, label)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(out1$Y, col=label, main="LDA projection")
plot(out2$Y, col=label, main="LQMI projection")
par(opar)

</code></pre>

<hr>
<h2 id='do.lscore'>Laplacian Score</h2><span id='topic+do.lscore'></span>

<h3>Description</h3>

<p>Laplacian Score (He et al. 2005) is an unsupervised linear feature extraction method. For each
feature/variable, it computes Laplacian score based on an observation that data from the
same class are often close to each other. Its power of locality preserving property is used, and
the algorithm selects variables with smallest scores.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lscore(X, ndim = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lscore_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lscore_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension (default: 2).</p>
</td></tr>
<tr><td><code id="do.lscore_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>preprocess</dt><dd><p>an additional option for preprocessing the data.
See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details (default: <code>"null"</code>).</p>
</dd>
<dt>type</dt><dd><p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details (default: <code>c("proportion",0.1)</code>).</p>
</dd>
<dt>t</dt><dd><p>bandwidth parameter for heat kernel in <code class="reqn">(0,\infty)</code> (default: <code>1</code>).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>lscore</dt><dd><p>a length-<code class="reqn">p</code> vector of laplacian scores. Indices with smallest values are selected.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>He X, Cai D, Niyogi P (2005).
&ldquo;Laplacian Score for Feature Selection.&rdquo;
In <em>Proceedings of the 18th International Conference on Neural Information Processing Systems</em>,  NIPS'05, 507&ndash;514.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
## it is known that feature 3 and 4 are more important.
data(iris)
set.seed(100)
subid    &lt;- sample(1:150, 50)
iris.dat &lt;- as.matrix(iris[subid,1:4])
iris.lab &lt;- as.factor(iris[subid,5])

## try different kernel bandwidth
out1 = do.lscore(iris.dat, t=0.1)
out2 = do.lscore(iris.dat, t=1)
out3 = do.lscore(iris.dat, t=10)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=iris.lab, main="bandwidth=0.1")
plot(out2$Y, pch=19, col=iris.lab, main="bandwidth=1")
plot(out3$Y, pch=19, col=iris.lab, main="bandwidth=10")
par(opar)


</code></pre>

<hr>
<h2 id='do.lsda'>Locality Sensitive Discriminant Analysis</h2><span id='topic+do.lsda'></span>

<h3>Description</h3>

<p>Locality Sensitive Discriminant Analysis (LSDA) is a supervised linear method.
It aims at finding a projection which maximizes the margin between data points from different classes
at each local area in which the nearby points with the same label are close to each other while
the nearby points with different labels are far apart.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lsda(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "whiten", "decorrelate"),
  alpha = 0.5,
  k1 = max(ceiling(nrow(X)/10), 2),
  k2 = max(ceiling(nrow(X)/10), 2)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lsda_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lsda_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.lsda_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lsda_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lsda_+3A_alpha">alpha</code></td>
<td>
<p>balancing parameter for between- and within-class scatter in <code class="reqn">[0,1]</code>.</p>
</td></tr>
<tr><td><code id="do.lsda_+3A_k1">k1</code></td>
<td>
<p>the number of same-class neighboring points (homogeneous neighbors).</p>
</td></tr>
<tr><td><code id="do.lsda_+3A_k2">k2</code></td>
<td>
<p>the number of different-class neighboring points (heterogeneous neighbors).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Cai D, He X, Zhou K, Han J, Bao H (2007).
&ldquo;Locality Sensitive Discriminant Analysis.&rdquo;
In <em>Proceedings of the 20th International Joint Conference on Artifical Intelligence</em>,  IJCAI'07, 708&ndash;713.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## create a data matrix with clear difference
x1 = matrix(rnorm(4*10), nrow=10)-20
x2 = matrix(rnorm(4*10), nrow=10)
x3 = matrix(rnorm(4*10), nrow=10)+20
X  = rbind(x1, x2, x3)
label = c(rep(1,10), rep(2,10), rep(3,10))

## try different affinity matrices
out1 = do.lsda(X, label, k1=2, k2=2)
out2 = do.lsda(X, label, k1=5, k2=5)
out3 = do.lsda(X, label, k1=10, k2=10)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=label, main="nbd size 2")
plot(out2$Y, col=label, main="nbd size 5")
plot(out3$Y, col=label, main="nbd size 10")
par(opar)

</code></pre>

<hr>
<h2 id='do.lsdf'>Locality Sensitive Discriminant Feature</h2><span id='topic+do.lsdf'></span>

<h3>Description</h3>

<p>Locality Sensitive Discriminant Feature (LSDF) is a semi-supervised feature selection method.
It utilizes both labeled and unlabeled data points in that labeled points are used to maximize
the margin between data opints from different classes, while labeled ones are used to discover
the geometrical structure of the data space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lsdf(
  X,
  label,
  ndim = 2,
  type = c("proportion", 0.1),
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate"),
  gamma = 100
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lsdf_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lsdf_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels. It should contain <code>NA</code> elements for missing label.</p>
</td></tr>
<tr><td><code id="do.lsdf_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lsdf_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lsdf_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lsdf_+3A_gamma">gamma</code></td>
<td>
<p>within-class weight parameter for same-class data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Cai D, He X, Zhou K, Han J, Bao H (2007).
&ldquo;Locality Sensitive Discriminant Analysis.&rdquo;
In <em>Proceedings of the 20th International Joint Conference on Artifical Intelligence</em>,  IJCAI'07, 708&ndash;713.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate data of 3 types with clear difference
set.seed(100)
dt1  = aux.gensamples(n=20)-50
dt2  = aux.gensamples(n=20)
dt3  = aux.gensamples(n=20)+50

## merge the data and create a label correspondingly
X      = rbind(dt1,dt2,dt3)
label  = rep(1:3, each=20)

## copy a label and let 20% of elements be missing
nlabel = length(label)
nmissing = round(nlabel*0.20)
label_missing = label
label_missing[sample(1:nlabel, nmissing)]=NA

## try different neighborhood sizes
out1 = do.lsdf(X, label_missing, type=c("proportion",0.10))
out2 = do.lsdf(X, label_missing, type=c("proportion",0.25))
out3 = do.lsdf(X, label_missing, type=c("proportion",0.50))

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="10% connectivity")
plot(out2$Y, pch=19, col=label, main="25% connectivity")
plot(out3$Y, pch=19, col=label, main="50% connectivity")
par(opar)

</code></pre>

<hr>
<h2 id='do.lsir'>Localized Sliced Inverse Regression</h2><span id='topic+do.lsir'></span>

<h3>Description</h3>

<p>Localized SIR (SIR) is an extension of celebrated SIR method. As its name suggests,
the <em>locality</em> concept is brought in that for each slice, only local data points
are considered in order to discover intrinsic structure of the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lsir(
  X,
  response,
  ndim = 2,
  h = max(2, round(nrow(X)/5)),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  ycenter = FALSE,
  numk = max(2, round(nrow(X)/10)),
  tau = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lsir_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lsir_+3A_response">response</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of response variable.</p>
</td></tr>
<tr><td><code id="do.lsir_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lsir_+3A_h">h</code></td>
<td>
<p>the number of slices to divide the range of response vector.</p>
</td></tr>
<tr><td><code id="do.lsir_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lsir_+3A_ycenter">ycenter</code></td>
<td>
<p>a logical; <code>TRUE</code> to center the response variable, <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="do.lsir_+3A_numk">numk</code></td>
<td>
<p>size of determining neighborhood via <code class="reqn">k</code>-nearest neighbor selection.</p>
</td></tr>
<tr><td><code id="do.lsir_+3A_tau">tau</code></td>
<td>
<p>regularization parameter for adjusting rank-deficient scatter matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Wu Q, Liang F, Mukherjee S (2010).
&ldquo;Localized Sliced Inverse Regression.&rdquo;
<em>Journal of Computational and Graphical Statistics</em>, <b>19</b>(4), 843&ndash;860.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.sir">do.sir</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate swiss roll with auxiliary dimensions
## it follows reference example from LSIR paper.
set.seed(100)
n     = 123
theta = runif(n)
h     = runif(n)
t     = (1+2*theta)*(3*pi/2)
X     = array(0,c(n,10))
X[,1] = t*cos(t)
X[,2] = 21*h
X[,3] = t*sin(t)
X[,4:10] = matrix(runif(7*n), nrow=n)

## corresponding response vector
y = sin(5*pi*theta)+(runif(n)*sqrt(0.1))

## try different number of neighborhoods
out1 = do.lsir(X, y, numk=5)
out2 = do.lsir(X, y, numk=10)
out3 = do.lsir(X, y, numk=25)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="LSIR::nbd size=5")
plot(out2$Y, main="LSIR::nbd size=10")
plot(out3$Y, main="LSIR::nbd size=25")
par(opar)

</code></pre>

<hr>
<h2 id='do.lsls'>Locality Sensitive Laplacian Score</h2><span id='topic+do.lsls'></span>

<h3>Description</h3>

<p>Locality Sensitive Laplacian Score (LSLS) is a supervised linear feature extraction method that combines
a feature selection framework of laplacian score where the graph laplacian is adjusted as in the
scheme of LSDA. The adjustment is taken via decomposed affinity matrices which are separately constructed
using the provided class label information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lsls(
  X,
  label,
  ndim = 2,
  alpha = 0.5,
  k = 5,
  preprocess = c("null", "center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lsls_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lsls_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.lsls_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lsls_+3A_alpha">alpha</code></td>
<td>
<p>a weight factor; should be a real number in <code class="reqn">[0,1]</code>.</p>
</td></tr>
<tr><td><code id="do.lsls_+3A_k">k</code></td>
<td>
<p>an integer; the size of a neighborhood.</p>
</td></tr>
<tr><td><code id="do.lsls_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Liao B, Jiang Y, Liang W, Zhu W, Cai L, Cao Z (2014).
&ldquo;Gene Selection Using Locality Sensitive Laplacian Score.&rdquo;
<em>IEEE/ACM Transactions on Computational Biology and Bioinformatics</em>, <b>11</b>(6), 1146&ndash;1156.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.lsda">do.lsda</a></code>, <code><a href="#topic+do.lscore">do.lscore</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
## it is known that feature 3 and 4 are more important.
data(iris)
set.seed(100)
subid    = sample(1:150,50)
iris.dat = as.matrix(iris[subid,1:4])
iris.lab = as.factor(iris[subid,5])

## compare different neighborhood sizes
out1 = do.lsls(iris.dat, iris.lab, k=3)
out2 = do.lsls(iris.dat, iris.lab, k=6)
out3 = do.lsls(iris.dat, iris.lab, k=9)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=iris.lab, pch=19, main="LSLS::k=3")
plot(out2$Y, col=iris.lab, pch=19, main="LSLS::k=6")
plot(out3$Y, col=iris.lab, pch=19, main="LSLS::k=9")
par(opar)


</code></pre>

<hr>
<h2 id='do.lspe'>Locality and Similarity Preserving Embedding</h2><span id='topic+do.lspe'></span>

<h3>Description</h3>

<p>Locality and Similarity Preserving Embedding (LSPE) is a feature selection method based on Neighborhood Preserving Embedding (<code><a href="#topic+do.npe">do.npe</a></code>) and
Sparsity Preserving Projection (<code><a href="#topic+do.spp">do.spp</a></code>) by first building a neighborhood graph and
then mapping the locality structure to reconstruct coefficients such that data similarity is preserved.
Use of <code class="reqn">\ell_{2,1}</code> norm boosts to impose column-sparsity that enables feature selection procedure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lspe(
  X,
  ndim = 2,
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate"),
  alpha = 1,
  beta = 1,
  bandwidth = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lspe_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.lspe_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lspe_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.lspe_+3A_alpha">alpha</code></td>
<td>
<p>nonnegative number to control <code class="reqn">\ell_{2,1}</code> norm of projection.</p>
</td></tr>
<tr><td><code id="do.lspe_+3A_beta">beta</code></td>
<td>
<p>nonnegative number to control the degree of local similarity.</p>
</td></tr>
<tr><td><code id="do.lspe_+3A_bandwidth">bandwidth</code></td>
<td>
<p>positive number for Gaussian kernel bandwidth to define similarity.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Fang X, Xu Y, Li X, Fan Z, Liu H, Chen Y (2014).
&ldquo;Locality and Similarity Preserving Embedding for Feature Selection.&rdquo;
<em>Neurocomputing</em>, <b>128</b>, 304&ndash;315.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.rsr">do.rsr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#### generate R12in72 dataset
set.seed(100)
X = aux.gensamples(n=50, dname="R12in72")

#### try different bandwidth values
out1 = do.lspe(X, bandwidth=0.1)
out2 = do.lspe(X, bandwidth=1)
out3 = do.lspe(X, bandwidth=10)

#### visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="LSPE::bandwidth=0.1")
plot(out2$Y, main="LSPE::bandwidth=1")
plot(out3$Y, main="LSPE::bandwidth=10")
par(opar)


</code></pre>

<hr>
<h2 id='do.lspp'>Local Similarity Preserving Projection</h2><span id='topic+do.lspp'></span>

<h3>Description</h3>

<p>Local Similarity Preserving Projection (LSPP) is a variant of LPP in that
it employs a sample-dependent graph generation process as of <code><a href="#topic+do.sdlpp">do.sdlpp</a></code>.
LSPP takes advantage of labeling information to correct local similarity weight
in order to make intra-class weight larger than inter-class weight. It uses
PCA preprocessing as suggested from the original work.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.lspp(
  X,
  label,
  ndim = 2,
  t = 1,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.lspp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.lspp_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.lspp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.lspp_+3A_t">t</code></td>
<td>
<p>kernel bandwidth in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
<tr><td><code id="do.lspp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Huang P, Gao G (2015).
&ldquo;Local Similarity Preserving Projections for Face Recognition.&rdquo;
<em>AEU - International Journal of Electronics and Communications</em>, <b>69</b>(11), 1724&ndash;1732.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.sdlpp">do.sdlpp</a></code>, <code><a href="#topic+do.lpp">do.lpp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate data of 2 types with clear difference
diff = 15
dt1  = aux.gensamples(n=50)-diff;
dt2  = aux.gensamples(n=50)+diff;

## merge the data and create a label correspondingly
Y      = rbind(dt1,dt2)
label  = rep(1:2, each=50)

## compare with PCA
out1 &lt;- do.pca(Y, ndim=2)
out2 &lt;- do.slpp(Y, label, ndim=2)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(out1$Y, col=label, pch=19, main="PCA")
plot(out2$Y, col=label, pch=19, main="LSPP")
par(opar)

</code></pre>

<hr>
<h2 id='do.ltsa'>Local Tangent Space Alignment</h2><span id='topic+do.ltsa'></span>

<h3>Description</h3>

<p>Local Tangent Space Alignment, or LTSA in short, is a nonlinear dimensionality reduction method
that mimicks the behavior of low-dimensional manifold embedded in high-dimensional space.
Similar to LLE, LTSA computes tangent space using nearest neighbors of a given data point, and
a multiple of tangent spaces are gathered to to find an embedding that aligns the tangent spaces
in target dimensional space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.ltsa(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  symmetric = c("union", "intersect", "asymmetric"),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.ltsa_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.ltsa_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.ltsa_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.ltsa_+3A_symmetric">symmetric</code></td>
<td>
<p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.ltsa_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>eigvals</dt><dd><p>a vector of eigenvalues from the final decomposition.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zhang T, Yang J, Zhao D, Ge X (2007).
&ldquo;Linear Local Tangent Space Alignment and Application to Face Recognition.&rdquo;
<em>Neurocomputing</em>, <b>70</b>(7-9), 1547&ndash;1553.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate data
set.seed(100)
X &lt;- aux.gensamples(dname="cswiss",n=100)

## 1. use 10%-connected graph
output1 &lt;- do.ltsa(X,ndim=2)

## 2. use 25%-connected graph
output2 &lt;- do.ltsa(X,ndim=2,type=c("proportion",0.25))

## 3. use 50%-connected graph
output3 &lt;- do.ltsa(X,ndim=2,type=c("proportion",0.50))

## Visualize three different projections
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(output1$Y, main="10%")
plot(output2$Y, main="25%")
plot(output3$Y, main="50%")
par(opar)


</code></pre>

<hr>
<h2 id='do.mcfs'>Multi-Cluster Feature Selection</h2><span id='topic+do.mcfs'></span>

<h3>Description</h3>

<p>Multi-Cluster Feature Selection (MCFS) is an unsupervised feature selection method. Based on
a multi-cluster assumption, it aims at finding meaningful features using sparse reconstruction of
spectral basis using LASSO.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.mcfs(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate"),
  K = max(round(nrow(X)/5), 2),
  lambda = 1,
  t = 10
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.mcfs_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.mcfs_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.mcfs_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.mcfs_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.mcfs_+3A_k">K</code></td>
<td>
<p>assumed number of clusters in the original dataset.</p>
</td></tr>
<tr><td><code id="do.mcfs_+3A_lambda">lambda</code></td>
<td>
<p><code class="reqn">\ell_1</code> regularization parameter in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
<tr><td><code id="do.mcfs_+3A_t">t</code></td>
<td>
<p>bandwidth parameter for heat kernel in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Cai D, Zhang C, He X (2010).
&ldquo;Unsupervised Feature Selection for Multi-Cluster Data.&rdquo;
In <em>Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 333&ndash;342.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate data of 3 types with clear difference
dt1  = aux.gensamples(n=20)-100
dt2  = aux.gensamples(n=20)
dt3  = aux.gensamples(n=20)+100

## merge the data and create a label correspondingly
X      = rbind(dt1,dt2,dt3)
label  = rep(1:3, each=20)

## try different regularization parameters
out1 = do.mcfs(X, lambda=0.01)
out2 = do.mcfs(X, lambda=0.1)
out3 = do.mcfs(X, lambda=1)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="lambda=0.01")
plot(out2$Y, pch=19, col=label, main="lambda=0.1")
plot(out3$Y, pch=19, col=label, main="lambda=1")
par(opar)

</code></pre>

<hr>
<h2 id='do.mds'>(Classical) Multidimensional Scaling</h2><span id='topic+do.mds'></span>

<h3>Description</h3>

<p><code>do.mds</code> performs a classical Multidimensional Scaling (MDS) using
<code>Rcpp</code> and <code>RcppArmadillo</code> package to achieve faster performance than
<code><a href="stats.html#topic+cmdscale">cmdscale</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.mds(X, ndim = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.mds_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.mds_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.mds_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>preprocess</dt><dd><p>an additional option for preprocessing the data.
Default is <code>"null"</code>. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Kruskal JB (1964).
&ldquo;Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis.&rdquo;
<em>Psychometrika</em>, <b>29</b>(1), 1&ndash;27.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
lab   = as.factor(iris[subid,5])

## compare with PCA
Rmds &lt;- do.mds(X, ndim=2)
Rpca &lt;- do.pca(X, ndim=2)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(Rmds$Y, pch=19, col=lab, main="MDS")
plot(Rpca$Y, pch=19, col=lab, main="PCA")
par(opar)


</code></pre>

<hr>
<h2 id='do.mfa'>Marginal Fisher Analysis</h2><span id='topic+do.mfa'></span>

<h3>Description</h3>

<p>Marginal Fisher Analysis (MFA) is a supervised linear dimension reduction method.
The intrinsic graph characterizes the intraclass compactness and connects each data point with
its neighboring pionts of the same class, while the penalty graph connects the marginal points
and characterizes the interclass separability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.mfa(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  k1 = max(ceiling(nrow(X)/10), 2),
  k2 = max(ceiling(nrow(X)/10), 2)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.mfa_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.mfa_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.mfa_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.mfa_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.mfa_+3A_k1">k1</code></td>
<td>
<p>the number of same-class neighboring points (homogeneous neighbors).</p>
</td></tr>
<tr><td><code id="do.mfa_+3A_k2">k2</code></td>
<td>
<p>the number of different-class neighboring points (heterogeneous neighbors).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Yan S, Xu D, Zhang B, Zhang H, Yang Q, Lin S (2007).
&ldquo;Graph Embedding and Extensions: A General Framework for Dimensionality Reduction.&rdquo;
<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <b>29</b>(1), 40&ndash;51.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate data of 3 types with clear difference
dt1  = aux.gensamples(n=20)-100
dt2  = aux.gensamples(n=20)
dt3  = aux.gensamples(n=20)+100

## merge the data and create a label correspondingly
X      = rbind(dt1,dt2,dt3)
label  = rep(1:3, each=20)

## try different numbers for neighborhood size
out1 = do.mfa(X, label, k1=5, k2=5)
out2 = do.mfa(X, label, k1=10,k2=10)
out3 = do.mfa(X, label, k1=25,k2=25)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="MFA::nbd size=5")
plot(out2$Y, main="MFA::nbd size=10")
plot(out3$Y, main="MFA::nbd size=25")
par(opar)

</code></pre>

<hr>
<h2 id='do.mifs'>Mutual Information for Selecting Features</h2><span id='topic+do.mifs'></span>

<h3>Description</h3>

<p>MIFS is a supervised feature selection that iteratively increases the subset of variables by choosing maximally informative feature based on the mutual information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.mifs(
  X,
  label,
  ndim = 2,
  beta = 0.75,
  discretize = c("default", "histogram"),
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.mifs_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.mifs_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of class labels.</p>
</td></tr>
<tr><td><code id="do.mifs_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.mifs_+3A_beta">beta</code></td>
<td>
<p>penalty for relative importance of mutual information between the candidate and already-chosen features in iterations. Author proposes to use a value in <code class="reqn">(0.5,1)</code>.</p>
</td></tr>
<tr><td><code id="do.mifs_+3A_discretize">discretize</code></td>
<td>
<p>the method for each variable to be discretized. The paper proposes <code>"default"</code> method to use 10 bins while <code>"histogram"</code> uses automatic discretization via Sturges' method.</p>
</td></tr>
<tr><td><code id="do.mifs_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data. Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Battiti R (1994).
&ldquo;Using Mutual Information for Selecting Features in Supervised Neural Net Learning.&rdquo;
<em>IEEE Transactions on Neural Networks</em>, <b>5</b>(4), 537&ndash;550.
ISSN 10459227.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
## it is known that feature 3 and 4 are more important.
data(iris)
iris.dat = as.matrix(iris[,1:4])
iris.lab = as.factor(iris[,5])

## try different beta values
out1 = do.mifs(iris.dat, iris.lab, beta=0)
out2 = do.mifs(iris.dat, iris.lab, beta=0.5)
out3 = do.mifs(iris.dat, iris.lab, beta=1)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=iris.lab, main="beta=0")
plot(out2$Y, pch=19, col=iris.lab, main="beta=0.5")
plot(out3$Y, pch=19, col=iris.lab, main="beta=1")
par(opar)



</code></pre>

<hr>
<h2 id='do.mlie'>Maximal Local Interclass Embedding</h2><span id='topic+do.mlie'></span>

<h3>Description</h3>

<p>Maximal Local Interclass Embedding (MLIE) is a linear supervised method that
the local interclass graph and the intrinsic graph are constructed to find a set of
projections that maximize the local interclass scatter and the local
intraclass compactness at the same time. It can be deemed an extended version of MFA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.mlie(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  k1 = max(ceiling(nrow(X)/10), 2),
  k2 = max(ceiling(nrow(X)/10), 2)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.mlie_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.mlie_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.mlie_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.mlie_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.mlie_+3A_k1">k1</code></td>
<td>
<p>the number of same-class neighboring points (homogeneous neighbors).</p>
</td></tr>
<tr><td><code id="do.mlie_+3A_k2">k2</code></td>
<td>
<p>the number of different-class neighboring points (heterogeneous neighbors).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>References</h3>

<p>Lai Z, Zhao C, Chen Y, Jin Z (2011).
&ldquo;Maximal Local Interclass Embedding with Application to Face Recognition.&rdquo;
<em>Machine Vision and Applications</em>, <b>22</b>(4), 619&ndash;627.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.mfa">do.mfa</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## generate data of 3 types with clear difference
set.seed(100)
diff = 100
dt1  = aux.gensamples(n=20)-diff
dt2  = aux.gensamples(n=20)
dt3  = aux.gensamples(n=20)+diff

## merge the data and create a label correspondingly
X      = rbind(dt1,dt2,dt3)
label  = rep(1:3, each=20)

## try different numbers for neighborhood size
out1 = do.mlie(X, label, k1=5, k2=5)
out2 = do.mlie(X, label, k1=10,k2=10)
out3 = do.mlie(X, label, k1=25,k2=25)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="MLIE::nbd size=5")
plot(out2$Y, main="MLIE::nbd size=10")
plot(out3$Y, main="MLIE::nbd size=25")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='do.mmc'>Maximum Margin Criterion</h2><span id='topic+do.mmc'></span>

<h3>Description</h3>

<p>Maximum Margin Criterion (MMC) is a linear supervised dimension reduction method that
maximizes average margin between classes. The cost function is defined as
</p>
<p style="text-align: center;"><code class="reqn">trace(S_b - S_w)</code>
</p>

<p>where <code class="reqn">S_b</code> is an overall variance of class mean vectors, and <code class="reqn">S_w</code> refers to
spread of every class. Note that Principal Component Analysis (PCA) maximizes
total scatter, <code class="reqn">S_t = S_b + S_w</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.mmc(X, label, ndim = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.mmc_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.mmc_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.mmc_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Li H, Jiang T, Zhang K (2006).
&ldquo;Efficient and Robust Feature Extraction by Maximum Margin Criterion.&rdquo;
<em>IEEE Transactions on Neural Networks</em>, <b>17</b>(1), 157&ndash;165.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
data(iris, package="Rdimtools")
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## compare MMC with other methods
outMMC = do.mmc(X, label)
outMVP = do.mvp(X, label)
outPCA = do.pca(X)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(outMMC$Y, pch=19, col=label, main="MMC")
plot(outMVP$Y, pch=19, col=label, main="MVP")
plot(outPCA$Y, pch=19, col=label, main="PCA")
par(opar)


</code></pre>

<hr>
<h2 id='do.mmds'>Metric Multidimensional Scaling</h2><span id='topic+do.mmds'></span>

<h3>Description</h3>

<p>Metric MDS is a nonlinear method that is solved iteratively. We adopt a
well-known SMACOF algorithm for updates with uniform weights over all pairwise
distances after initializing the low-dimensional configuration via classical MDS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.mmds(X, ndim = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.mmds_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.mmds_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension (default: 2).</p>
</td></tr>
<tr><td><code id="do.mmds_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>maxiter</dt><dd><p>maximum number of iterations for metric MDS updates (default: 100).</p>
</dd>
<dt>abstol</dt><dd><p>stopping criterion for metric MDS iterations (default: 1e-8).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Leeuw JD, Barra IJR, Brodeau F, Romier G, (eds BVC (1977).
&ldquo;Applications of Convex Analysis to Multidimensional Scaling.&rdquo;
In <em>Recent Developments in Statistics</em>, 133&ndash;146.
</p>
<p>Borg I, Groenen PJF (2010).
<em>Modern Multidimensional Scaling: Theory and Applications</em>.
Springer New York, New York, NY.
ISBN 978-1-4419-2046-1 978-0-387-28981-6.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
X     = as.matrix(iris[,1:4])
lab   = as.factor(iris[,5])

## compare with other methods
pca2d &lt;- do.pca(X, ndim=2)
cmd2d &lt;- do.mds(X, ndim=2)
mmd2d &lt;- do.mmds(X, ndim=2)

## Visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(pca2d$Y, col=lab, pch=19, main="PCA")
plot(cmd2d$Y, col=lab, pch=19, main="Classical MDS")
plot(mmd2d$Y, col=lab, pch=19, main="Metric MDS")
par(opar)


</code></pre>

<hr>
<h2 id='do.mmp'>Maximum Margin Projection</h2><span id='topic+do.mmp'></span>

<h3>Description</h3>

<p>Maximum Margin Projection (MMP) is a supervised linear method that maximizes the margin
between positive and negative examples at each local neighborhood based on
same- and different-class neighborhoods depending on class labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.mmp(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  numk = max(ceiling(nrow(X)/10), 2),
  alpha = 0.5,
  gamma = 50
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.mmp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.mmp_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.mmp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.mmp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.mmp_+3A_numk">numk</code></td>
<td>
<p>the number of neighboring points.</p>
</td></tr>
<tr><td><code id="do.mmp_+3A_alpha">alpha</code></td>
<td>
<p>balancing parameter in <code class="reqn">[0,1]</code>.</p>
</td></tr>
<tr><td><code id="do.mmp_+3A_gamma">gamma</code></td>
<td>
<p>weight for same-label data points with large magnitude.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Xiaofei He, Deng Cai, Jiawei Han (2008).
&ldquo;Learning a Maximum Margin Subspace for Image Retrieval.&rdquo;
<em>IEEE Transactions on Knowledge and Data Engineering</em>, <b>20</b>(2), 189&ndash;201.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate data of 3 types with clear difference
dt1  = aux.gensamples(n=20)-100
dt2  = aux.gensamples(n=20)
dt3  = aux.gensamples(n=20)+100

## merge the data and create a label correspondingly
X      = rbind(dt1,dt2,dt3)
label  = rep(1:3, each=20)

## copy a label and let 20% of elements be missing
nlabel = length(label)
nmissing = round(nlabel*0.20)
label_missing = label
label_missing[sample(1:nlabel, nmissing)]=NA

## compare with PCA case for full-label case
## for missing label case from MMP computation
out1 = do.pca(X, ndim=2)
out2 = do.mmp(X, label_missing, numk=10)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(out1$Y, col=label, main="PCA projection")
plot(out2$Y, col=label, main="20% missing labels")
par(opar)

</code></pre>

<hr>
<h2 id='do.mmsd'>Multiple Maximum Scatter Difference</h2><span id='topic+do.mmsd'></span>

<h3>Description</h3>

<p>Multiple Maximum Scatter Difference (MMSD) is a supervised linear dimension reduction method.
It is a variant of MSD in that discriminant vectors are orthonormal. Similar to MSD, it also does not suffer from
rank deficiency issue of scatter matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.mmsd(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "whiten", "decorrelate"),
  C = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.mmsd_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.mmsd_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.mmsd_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.mmsd_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.mmsd_+3A_c">C</code></td>
<td>
<p>nonnegative balancing parameter for intra- and inter-class scatter.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Fengxi Song, Zhang D, Dayong Mei, Zhongwei Guo (2007).
&ldquo;A Multiple Maximum Scatter Difference Discriminant Criterion for Facial Feature Extraction.&rdquo;
<em>IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)</em>, <b>37</b>(6), 1599&ndash;1606.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate data of 3 types with clear difference
set.seed(100)
dt1  = aux.gensamples(n=20)-50
dt2  = aux.gensamples(n=20)
dt3  = aux.gensamples(n=20)+50

## merge the data and create a label correspondingly
X      = rbind(dt1,dt2,dt3)
label  = rep(1:3, each=20)

## try different balancing parameter
out1 = do.mmsd(X, label, C=0.01)
out2 = do.mmsd(X, label, C=1)
out3 = do.mmsd(X, label, C=100)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="MMSD::C=0.01")
plot(out2$Y, pch=19, col=label, main="MMSD::C=1")
plot(out3$Y, pch=19, col=label, main="MMSD::C=100")
par(opar)

</code></pre>

<hr>
<h2 id='do.modp'>Modified Orthogonal Discriminant Projection</h2><span id='topic+do.modp'></span>

<h3>Description</h3>

<p>Modified Orthogonal Discriminant Projection (MODP) is a variant of Orthogonal Discriminant Projection (ODP).
Authors argue the assumption in modeling ODP's mechanism to reflect distance and class labeling seem unsound.
They propose a modified method to explore the intrinsic structure of original data and enhance
the classification ability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.modp(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  type = c("proportion", 0.1),
  symmetric = c("union", "intersect", "asymmetric"),
  alpha = 0.5,
  beta = 10
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.modp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.modp_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.modp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.modp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.modp_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.modp_+3A_symmetric">symmetric</code></td>
<td>
<p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.modp_+3A_alpha">alpha</code></td>
<td>
<p>balancing parameter of non-local and local scatter in <code class="reqn">[0,1]</code>.</p>
</td></tr>
<tr><td><code id="do.modp_+3A_beta">beta</code></td>
<td>
<p>scaling control parameter for distant pairs of data in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>References</h3>

<p>Zhang S, Lei Y, Wu Y, Yang J (2011).
&ldquo;Modified Orthogonal Discriminant Projection for Classification.&rdquo;
<em>Neurocomputing</em>, <b>74</b>(17), 3690&ndash;3694.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate 3 different groups of data X and label vector
x1 = matrix(rnorm(4*10), nrow=10)-20
x2 = matrix(rnorm(4*10), nrow=10)
x3 = matrix(rnorm(4*10), nrow=10)+20
X     = rbind(x1, x2, x3)
label = rep(1:3, each=10)

## try different beta (scaling control) parameter
out1 = do.modp(X, label, beta=1)
out2 = do.modp(X, label, beta=10)
out3 = do.modp(X, label, beta=100)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="MODP::beta=1")
plot(out2$Y, main="MODP::beta=10")
plot(out3$Y, main="MODP::beta=100")
par(opar)

</code></pre>

<hr>
<h2 id='do.msd'>Maximum Scatter Difference</h2><span id='topic+do.msd'></span>

<h3>Description</h3>

<p>Maximum Scatter Difference (MSD) is a supervised linear dimension reduction method.
The basic idea of MSD is to use <em>additive</em> cost function rather than <em>multiplicative</em>
trace ratio criterion that was adopted by LDA. Due to such formulation, it can neglect sample-sample-size
problem from rank-deficiency of between-class variance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.msd(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "whiten", "decorrelate"),
  C = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.msd_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.msd_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.msd_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.msd_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.msd_+3A_c">C</code></td>
<td>
<p>nonnegative balancing parameter for intra- and inter-class variance.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Song F, Zhang D, Chen Q, Wang J (2007).
&ldquo;Face Recognition Based on a Novel Linear Discriminant Criterion.&rdquo;
<em>Pattern Analysis and Applications</em>, <b>10</b>(3), 165&ndash;174.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate data of 3 types with clear difference
set.seed(100)
dt1  = aux.gensamples(n=20)-50
dt2  = aux.gensamples(n=20)
dt3  = aux.gensamples(n=20)+50

## merge the data and create a label correspondingly
X      = rbind(dt1,dt2,dt3)
label  = rep(1:3, each=20)

## try different balancing parameter
out1 = do.msd(X, label, C=0.01)
out2 = do.msd(X, label, C=1)
out3 = do.msd(X, label, C=100)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="MSD::C=0.01")
plot(out2$Y, pch=19, col=label, main="MSD::C=1")
plot(out3$Y, pch=19, col=label, main="MSD::C=100")
par(opar)

</code></pre>

<hr>
<h2 id='do.mve'>Minimum Volume Embedding</h2><span id='topic+do.mve'></span>

<h3>Description</h3>

<p>Minimum Volume Embedding (MVE) is a nonlinear dimension reduction
algorithm that exploits semidefinite programming (SDP), like MVU/SDE.
Whereas MVU aims at stretching through all direction by maximizing
<code class="reqn">\sum \lambda_i</code>, MVE only opts for unrolling the top eigenspectrum
and chooses to shrink left-over spectral dimension. For ease of use,
unlike kernel PCA, we only made use of Gaussian kernel for MVE.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.mve(
  X,
  ndim = 2,
  knn = ceiling(nrow(X)/10),
  kwidth = 1,
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate"),
  tol = 1e-04,
  maxiter = 10
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.mve_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.mve_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.mve_+3A_knn">knn</code></td>
<td>
<p>size of <code class="reqn">k</code>-nn neighborhood.</p>
</td></tr>
<tr><td><code id="do.mve_+3A_kwidth">kwidth</code></td>
<td>
<p>bandwidth for Gaussian kernel.</p>
</td></tr>
<tr><td><code id="do.mve_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.mve_+3A_tol">tol</code></td>
<td>
<p>stopping criterion for incremental change.</p>
</td></tr>
<tr><td><code id="do.mve_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations allowed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Shaw B, Jebara T (2007).
&ldquo;Minimum Volume Embedding.&rdquo;
In Meila M, Shen X (eds.), <em>Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics March 21-24, 2007, San Juan, Puerto Rico</em>, 460&ndash;467.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.mvu">do.mvu</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## use a small subset of iris data
set.seed(100)
id  = sample(1:150, 50)
X   = as.matrix(iris[id,1:4])
lab = as.factor(iris[id,5])

## try different connectivity levels
output1 &lt;- do.mve(X, knn=5)
output2 &lt;- do.mve(X, knn=10)
output3 &lt;- do.mve(X, knn=20)

## Visualize two comparisons
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(output1$Y, main="knn:k=5",  pch=19, col=lab)
plot(output2$Y, main="knn:k=10", pch=19, col=lab)
plot(output3$Y, main="knn:k=20", pch=19, col=lab)
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='do.mvp'>Maximum Variance Projection</h2><span id='topic+do.mvp'></span>

<h3>Description</h3>

<p>Maximum Variance Projection (MVP) is a supervised method based on linear discriminant analysis (LDA).
In addition to classical LDA, it further aims at preserving local information by capturing
the local geometry of the manifold via the following proximity coding,
</p>
<p style="text-align: center;"><code class="reqn">S_{ij} = 1\quad\textrm{if}\quad C_i \ne C_j\quad\textrm{and} = 0 \quad\textrm{otherwise}</code>
</p>
<p>,
where <code class="reqn">C_i</code> is the label of an <code class="reqn">i</code>-th data point.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.mvp(X, label, ndim = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.mvp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.mvp_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.mvp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zhang T (2007).
&ldquo;Maximum Variance Projections for Face Recognition.&rdquo;
<em>Optical Engineering</em>, <b>46</b>(6), 067206.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## perform MVP and compare with others
outMVP = do.mvp(X, label)
outPCA = do.pca(X)
outLDA = do.lda(X, label)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(outMVP$Y, col=label, pch=19, main="MVP")
plot(outPCA$Y, col=label, pch=19, main="PCA")
plot(outLDA$Y, col=label, pch=19, main="LDA")
par(opar)


</code></pre>

<hr>
<h2 id='do.mvu'>Maximum Variance Unfolding / Semidefinite Embedding</h2><span id='topic+do.mvu'></span><span id='topic+do.sde'></span>

<h3>Description</h3>

<p>The method of Maximum Variance Unfolding(MVU), also known as Semidefinite Embedding(SDE) is, as its names suggest,
to exploit semidefinite programming in performing nonlinear dimensionality reduction by <em>unfolding</em>
neighborhood graph constructed in the original high-dimensional space. Its unfolding generates a gram
matrix <code class="reqn">K</code> in that we can choose from either directly finding embeddings (<code>"spectral"</code>) or
use again Kernel PCA technique (<code>"kpca"</code>) to find low-dimensional representations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.mvu(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  preprocess = c("null", "center", "scale", "cscale", "decorrelate", "whiten"),
  projtype = c("spectral", "kpca")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.mvu_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.mvu_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.mvu_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.mvu_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.mvu_+3A_projtype">projtype</code></td>
<td>
<p>type of method for projection; either <code>"spectral"</code> or <code>"kpca"</code> used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Weinberger KQ, Saul LK (2006).
&ldquo;Unsupervised Learning of Image Manifolds by Semidefinite Programming.&rdquo;
<em>International Journal of Computer Vision</em>, <b>70</b>(1), 77&ndash;90.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use a small subset of iris data
set.seed(100)
id  = sample(1:150, 50)
X   = as.matrix(iris[id,1:4])
lab = as.factor(iris[id,5])

## try different connectivity levels
output1 &lt;- do.mvu(X, type=c("proportion", 0.10))
output2 &lt;- do.mvu(X, type=c("proportion", 0.25))
output3 &lt;- do.mvu(X, type=c("proportion", 0.50))

## visualize three different projections
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(output1$Y, main="10% connected", pch=19, col=lab)
plot(output2$Y, main="25% connected", pch=19, col=lab)
plot(output3$Y, main="50% connected", pch=19, col=lab)
par(opar)


</code></pre>

<hr>
<h2 id='do.nnp'>Nearest Neighbor Projection</h2><span id='topic+do.nnp'></span>

<h3>Description</h3>

<p>Nearest Neighbor Projection is an iterative method for visualizing high-dimensional dataset
in that a data is sequentially located in the low-dimensional space by maintaining
the triangular distance spread of target data with its two nearest neighbors in the high-dimensional space.
We extended the original method to be applied for arbitrarily low-dimensional space. Due the generalization,
we opted for a global optimization method of <em>Differential Evolution</em> (<code><a href="RcppDE.html#topic+DEoptim">DEoptim</a></code>) within in that it may add computational burden to certain degrees.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.nnp(
  X,
  ndim = 2,
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.nnp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.nnp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.nnp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Tejada E, Minghim R, Nonato LG (2003).
&ldquo;On Improved Projection Techniques to Support Visual Exploration of Multidimensional Data Sets.&rdquo;
<em>Information Visualization</em>, <b>2</b>(4), 218&ndash;231.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## let's compare with other methods
out1 &lt;- do.nnp(X, ndim=2)      # NNP
out2 &lt;- do.pca(X, ndim=2)      # PCA
out3 &lt;- do.dm(X, ndim=2)     # Diffusion Maps

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="NNP")
plot(out2$Y, pch=19, col=label, main="PCA")
plot(out3$Y, pch=19, col=label, main="Diffusion Maps")
par(opar)


</code></pre>

<hr>
<h2 id='do.nolpp'>Nonnegative Orthogonal Locality Preserving Projection</h2><span id='topic+do.nolpp'></span>

<h3>Description</h3>

<p>Nonnegative Orthogonal Locality Preserving Projection (NOLPP) is a variant of OLPP where
projection vectors - or, basis for learned subspace - contain no negative values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.nolpp(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  preprocess = c("null", "center", "scale", "cscale", "decorrelate", "whiten"),
  t = 1,
  maxiter = 1000,
  reltol = 1e-05
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.nolpp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.nolpp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.nolpp_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.nolpp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.nolpp_+3A_t">t</code></td>
<td>
<p>kernel bandwidth in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
<tr><td><code id="do.nolpp_+3A_maxiter">maxiter</code></td>
<td>
<p>number of maximum iteraions allowed.</p>
</td></tr>
<tr><td><code id="do.nolpp_+3A_reltol">reltol</code></td>
<td>
<p>stopping criterion for incremental relative error.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zafeiriou S, Laskaris N (2010).
&ldquo;Nonnegative Embeddings and Projections for Dimensionality Reduction and Information Visualization.&rdquo;
In <em>2010 20th International Conference on Pattern Recognition</em>, 726&ndash;729.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.olpp">do.olpp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## use different kernel bandwidths with 20% connectivity
out1 = do.nolpp(X, type=c("proportion",0.5), t=0.01)
out2 = do.nolpp(X, type=c("proportion",0.5), t=0.1)
out3 = do.nolpp(X, type=c("proportion",0.5), t=1)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=label, main="NOLPP::t=0.01")
plot(out2$Y, col=label, main="NOLPP::t=0.1")
plot(out3$Y, col=label, main="NOLPP::t=1")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='do.nonpp'>Nonnegative Orthogonal Neighborhood Preserving Projections</h2><span id='topic+do.nonpp'></span>

<h3>Description</h3>

<p>Nonnegative Orthogonal Neighborhood Preserving Projections (NONPP) is a variant of ONPP where
projection vectors - or, basis for learned subspace - contain no negative values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.nonpp(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  preprocess = c("null", "center", "decorrelate", "whiten"),
  maxiter = 1000,
  reltol = 1e-05
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.nonpp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.nonpp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.nonpp_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.nonpp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot; and other options of &quot;decorrelate&quot; and &quot;whiten&quot;
are supported. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.nonpp_+3A_maxiter">maxiter</code></td>
<td>
<p>number of maximum iteraions allowed.</p>
</td></tr>
<tr><td><code id="do.nonpp_+3A_reltol">reltol</code></td>
<td>
<p>stopping criterion for incremental relative error.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zafeiriou S, Laskaris N (2010).
&ldquo;Nonnegative Embeddings and Projections for Dimensionality Reduction and Information Visualization.&rdquo;
In <em>2010 20th International Conference on Pattern Recognition</em>, 726&ndash;729.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.onpp">do.onpp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## use different levels of connectivity
out1 = do.nonpp(X, type=c("proportion",0.1))
out2 = do.nonpp(X, type=c("proportion",0.2))
out3 = do.nonpp(X, type=c("proportion",0.5))

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=label, main="NONPP::10% connected")
plot(out2$Y, col=label, main="NONPP::20% connected")
plot(out3$Y, col=label, main="NONPP::50% connected")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='do.npca'>Nonnegative Principal Component Analysis</h2><span id='topic+do.npca'></span>

<h3>Description</h3>

<p>Nonnegative Principal Component Analysis (NPCA) is a variant of PCA where
projection vectors - or, basis for learned subspace - contain no negative values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.npca(X, ndim = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.npca_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.npca_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.npca_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>maxiter</dt><dd><p>maximum number of iterations (default: 100).</p>
</dd>
<dt>reltol</dt><dd><p>relative tolerance stopping criterion (default: 1e-4).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zafeiriou S, Laskaris N (2010).
&ldquo;Nonnegative Embeddings and Projections for Dimensionality Reduction and Information Visualization.&rdquo;
In <em>2010 20th International Conference on Pattern Recognition</em>, 726&ndash;729.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.pca">do.pca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## use iris data
data(iris, package="Rdimtools")
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4]) + 50
label = as.factor(iris[subid,5])

## run NCPA and compare with others
outNPC = do.npca(X)
outPCA = do.pca(X)
outMVP = do.mvp(X, label)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(outNPC$Y, pch=19, col=label, main="NPCA")
plot(outPCA$Y, pch=19, col=label, main="PCA")
plot(outMVP$Y, pch=19, col=label, main="MVP")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='do.npe'>Neighborhood Preserving Embedding</h2><span id='topic+do.npe'></span>

<h3>Description</h3>

<p><code>do.npe</code> performs a linear dimensionality reduction using Neighborhood Preserving
Embedding (NPE) proposed by He et al (2005). It can be regarded as a linear approximation
to Locally Linear Embedding (LLE). Like LLE, it is possible for the weight matrix being rank deficient.
If <code>regtype</code> is set to <code>TRUE</code> with a proper value of <code>regparam</code>, it will
perform Tikhonov regularization as designated. When regularization is needed
with <code>regtype</code> parameter to be <code>FALSE</code>, it will automatically find a suitable
regularization parameter and put penalty for stable computation. See also
<code><a href="#topic+do.lle">do.lle</a></code> for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.npe(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  symmetric = "union",
  weight = TRUE,
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate"),
  regtype = FALSE,
  regparam = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.npe_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.npe_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.npe_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.npe_+3A_symmetric">symmetric</code></td>
<td>
<p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.npe_+3A_weight">weight</code></td>
<td>
<p><code>TRUE</code> to perform NPE on weighted graph, or <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="do.npe_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.npe_+3A_regtype">regtype</code></td>
<td>
<p><code>FALSE</code> for not applying automatic Tikhonov Regularization,
or <code>TRUE</code> otherwise.</p>
</td></tr>
<tr><td><code id="do.npe_+3A_regparam">regparam</code></td>
<td>
<p>a positive real number for Regularization. Default value is 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>eigval</dt><dd><p>a vector of eigenvalues corresponding to basis expansion in an ascending order.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>He X, Cai D, Yan S, Zhang H (2005).
&ldquo;Neighborhood Preserving Embedding.&rdquo;
In <em>Proceedings of the Tenth IEEE International Conference on Computer Vision - Volume 2</em>,  ICCV '05, 1208&ndash;1213.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## use different settings for connectivity
output1 = do.npe(X, ndim=2, type=c("proportion",0.10))
output2 = do.npe(X, ndim=2, type=c("proportion",0.25))
output3 = do.npe(X, ndim=2, type=c("proportion",0.50))

## visualize three different projections
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(output1$Y, pch=19, col=label, main="NPE::10% connected")
plot(output2$Y, pch=19, col=label, main="NPE::25% connected")
plot(output3$Y, pch=19, col=label, main="NPE::50% connected")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='do.nrsr'>Non-convex Regularized Self-Representation</h2><span id='topic+do.nrsr'></span>

<h3>Description</h3>

<p>In the standard, convex RSR problem (<code><a href="#topic+do.rsr">do.rsr</a></code>), row-sparsity for self-representation is
acquired using matrix <code class="reqn">\ell_{2,1}</code> norm, i.e, <code class="reqn">\|W\|_{2,1} = \sum \|W_{i:}\|_2</code>. Its non-convex
extension aims at achieving higher-level of sparsity using arbitrarily chosen <code class="reqn">\|W\|_{2,l}</code> norm for
<code class="reqn">l\in (0,1)</code> and this exploits Iteratively Reweighted Least Squares (IRLS) algorithm for computation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.nrsr(
  X,
  ndim = 2,
  expl = 0.5,
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate"),
  lbd = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.nrsr_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.nrsr_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.nrsr_+3A_expl">expl</code></td>
<td>
<p>an exponent in <code class="reqn">\ell_{2,l}</code> norm for sparsity. Must be in <code class="reqn">(0,1)</code>, or <code class="reqn">l=1</code> reduces to RSR problem.</p>
</td></tr>
<tr><td><code id="do.nrsr_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.nrsr_+3A_lbd">lbd</code></td>
<td>
<p>nonnegative number to control the degree of self-representation by imposing row-sparsity.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zhu P, Zhu W, Wang W, Zuo W, Hu Q (2017).
&ldquo;Non-Convex Regularized Self-Representation for Unsupervised Feature Selection.&rdquo;
<em>Image and Vision Computing</em>, <b>60</b>, 22&ndash;29.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.rsr">do.rsr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

#### try different exponents for regularization
out1 = do.nrsr(X, expl=0.01)
out2 = do.nrsr(X, expl=0.1)
out3 = do.nrsr(X, expl=0.5)

#### visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="NRSR::expl=0.01")
plot(out2$Y, pch=19, col=label, main="NRSR::expl=0.1")
plot(out3$Y, pch=19, col=label, main="NRSR::expl=0.5")
par(opar)


</code></pre>

<hr>
<h2 id='do.odp'>Orthogonal Discriminant Projection</h2><span id='topic+do.odp'></span>

<h3>Description</h3>

<p>Orthogonal Discriminant Projection (ODP) is a linear dimension reduction method with label information, i.e., <em>supervised</em>.
The method maximizes weighted difference between local and non-local scatter while local information is also preserved by
constructing a neighborhood graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.odp(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  type = c("proportion", 0.1),
  symmetric = c("union", "intersect", "asymmetric"),
  alpha = 0.5,
  beta = 10
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.odp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.odp_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.odp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.odp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.odp_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.odp_+3A_symmetric">symmetric</code></td>
<td>
<p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.odp_+3A_alpha">alpha</code></td>
<td>
<p>balancing parameter of non-local and local scatter in <code class="reqn">[0,1]</code>.</p>
</td></tr>
<tr><td><code id="do.odp_+3A_beta">beta</code></td>
<td>
<p>scaling control parameter for distant pairs of data in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>References</h3>

<p>Li B, Wang C, Huang D (2009).
&ldquo;Supervised Feature Extraction Based on Orthogonal Discriminant Projection.&rdquo;
<em>Neurocomputing</em>, <b>73</b>(1-3), 191&ndash;196.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## try different beta (scaling control) parameter
out1 = do.odp(X, label, beta=1)
out2 = do.odp(X, label, beta=10)
out3 = do.odp(X, label, beta=100)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=label, pch=19, main="ODP::beta=1")
plot(out2$Y, col=label, pch=19, main="ODP::beta=10")
plot(out3$Y, col=label, pch=19, main="ODP::beta=100")
par(opar)

</code></pre>

<hr>
<h2 id='do.olda'>Orthogonal Linear Discriminant Analysis</h2><span id='topic+do.olda'></span>

<h3>Description</h3>

<p>Orthogonal LDA (OLDA) is an extension of classical LDA where the discriminant vectors are
orthogonal to each other.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.olda(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "whiten", "decorrelate")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.olda_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.olda_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.olda_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.olda_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Ye J (2005).
&ldquo;Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems.&rdquo;
<em>J. Mach. Learn. Res.</em>, <b>6</b>, 483&ndash;502.
ISSN 1532-4435.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## compare with LDA
out1 = do.lda(X, label)
out2 = do.olda(X, label)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(out1$Y, pch=19, col=label, main="LDA")
plot(out2$Y, pch=19, col=label, main="Orthogonal LDA")
par(opar)

</code></pre>

<hr>
<h2 id='do.olpp'>Orthogonal Locality Preserving Projection</h2><span id='topic+do.olpp'></span>

<h3>Description</h3>

<p>Orthogonal Locality Preserving Projection (OLPP) is a variant of <code>do.lpp</code>, which
extracts orthogonal basis functions to reconstruct the data in a more intuitive fashion.
It adopts PCA as preprocessing step and uses only one eigenvector at each iteration in that
it might incur warning messages for solving near-singular system of linear equations. Current
implementation may not return an orthogonal projection matrix as of the paper. We plan to
fix this issue in the near future.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.olpp(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  symmetric = c("union", "intersect"),
  t = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.olpp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations</p>
</td></tr>
<tr><td><code id="do.olpp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.olpp_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.olpp_+3A_symmetric">symmetric</code></td>
<td>
<p>either <code>"intersect"</code> or <code>"union"</code> is supported. Default is <code>"union"</code>.
See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.olpp_+3A_t">t</code></td>
<td>
<p>bandwidth for heat kernel in <code class="reqn">(0,\infty)</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Cai D, He X, Han J, Zhang H (2006).
&ldquo;Orthogonal Laplacianfaces for Face Recognition.&rdquo;
<em>IEEE Transactions on Image Processing</em>, <b>15</b>(11), 3608&ndash;3614.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.lpp">do.lpp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

##  connecting 10% and 25% of data for graph construction each.
output1 &lt;- do.olpp(X,ndim=2,type=c("proportion",0.10))
output2 &lt;- do.olpp(X,ndim=2,type=c("proportion",0.25))

## Visualize
#  In theory, it should show two separated groups of data
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(output1$Y, col=label, pch=19, main="OLPP::10% connected")
plot(output2$Y, col=label, pch=19, main="OLPP::25% connected")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='do.onpp'>Orthogonal Neighborhood Preserving Projections</h2><span id='topic+do.onpp'></span>

<h3>Description</h3>

<p>Orthogonal Neighborhood Preserving Projection (ONPP) is an unsupervised linear dimension reduction method.
It constructs a weighted data graph from LLE method. Also, it develops LPP method by preserving
the structure of local neighborhoods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.onpp(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.onpp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.onpp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.onpp_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.onpp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Kokiopoulou E, Saad Y (2007).
&ldquo;Orthogonal Neighborhood Preserving Projections: A Projection-Based Dimensionality Reduction Technique.&rdquo;
<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <b>29</b>(12), 2143&ndash;2156.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## try different numbers for neighborhood size
out1 = do.onpp(X, type=c("proportion",0.10))
out2 = do.onpp(X, type=c("proportion",0.25))
out3 = do.onpp(X, type=c("proportion",0.50))

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="ONPP::10% connectivity")
plot(out2$Y, pch=19, col=label, main="ONPP::25% connectivity")
plot(out3$Y, pch=19, col=label, main="ONPP::50% connectivity")
par(opar)

</code></pre>

<hr>
<h2 id='do.opls'>Orthogonal Partial Least Squares</h2><span id='topic+do.opls'></span>

<h3>Description</h3>

<p>Also known as multilinear regression or semipenalized CCA, Orthogonal Partial Least Squares (OPLS)
was first used to perform multilinear ordinary least squares. In its usage, unlike PLS or CCA,
OPLS does not rely on projected variance of response -or, <code>data2</code>. Instead, it exploits projected
variance of input - covariance of <code>data1</code> and relates it under cross-covariance setting. Therefore,
OPLS only returns projection information of <code>data1</code>, just like any other unsupervised methods in our package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.opls(data1, data2, ndim = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.opls_+3A_data1">data1</code></td>
<td>
<p>an <code class="reqn">(n\times N)</code> data matrix whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.opls_+3A_data2">data2</code></td>
<td>
<p>an <code class="reqn">(n\times M)</code> data matrix whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.opls_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix of projected observations from <code>data1</code>.</p>
</dd>
<dt>projection</dt><dd><p>an <code class="reqn">(N\times ndim)</code> whose columns are loadings for <code>data1</code>.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction for <code>data1</code>.</p>
</dd>
<dt>eigvals</dt><dd><p>a vector of eigenvalues for iterative decomposition.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Barker M, Rayens W (2003).
&ldquo;Partial Least Squares for Discrimination.&rdquo;
<em>Journal of Chemometrics</em>, <b>17</b>(3), 166&ndash;173.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.pls">do.pls</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate 2 normal data matrices
mat1 = matrix(rnorm(100*12),nrow=100)+10 # 12-dim normal
mat2 = matrix(rnorm(100*6), nrow=100)-10 # 6-dim normal

## compare OPLS and PLS
res_opls = do.opls(mat1, mat2, ndim=2)
res_pls  = do.pls(mat1, mat2, ndim=2)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(res_opls$Y, cex=0.5, main="OPLS result")
plot(res_pls$Y1, cex=0.5, main="PLS result")
par(opar)

</code></pre>

<hr>
<h2 id='do.pca'>Principal Component Analysis</h2><span id='topic+do.pca'></span>

<h3>Description</h3>

<p><code>do.pca</code> performs a classical principal component analysis (Pearson 1901) using
<code>RcppArmadillo</code> package for faster and efficient computation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.pca(X, ndim = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.pca_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.pca_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.pca_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>cor</dt><dd><p>mode of eigendecomposition. <code>FALSE</code> for decomposing covariance matrix (default),
and <code>TRUE</code> for correlation matrix.</p>
</dd>
<dt>preprocess</dt><dd><p>an additional option for preprocessing the data.
Default is <code>"center"</code>. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>vars</dt><dd><p>a vector containing variances of projected data onto principal components.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Pearson K (1901).
&ldquo;LIII. On Lines and Planes of Closest Fit to Systems of Points in Space.&rdquo;
<em>Philosophical Magazine Series 6</em>, <b>2</b>(11), 559&ndash;572.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
lab   = as.factor(iris[subid,5])

## try covariance &amp; correlation decomposition
out1 &lt;- do.pca(X, ndim=2, cor=FALSE)
out2 &lt;- do.pca(X, ndim=2, cor=TRUE)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(out1$Y, col=lab, pch=19, main="correlation decomposition")
plot(out2$Y, col=lab, pch=19, main="covariance decomposition")
par(opar)


</code></pre>

<hr>
<h2 id='do.pfa'>Principal Feature Analysis</h2><span id='topic+do.pfa'></span>

<h3>Description</h3>

<p>Principal Feature Analysis (Lu et al. 2007)
adopts an idea from the celebrated PCA for unsupervised feature selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.pfa(X, ndim = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.pfa_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.pfa_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension (default: 2).</p>
</td></tr>
<tr><td><code id="do.pfa_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>cor</dt><dd><p>mode of eigendecomposition. <code>FALSE</code> for decomposing the
empirical covariance matrix and <code>TRUE</code> uses the correlation matrix
(default: <code>FALSE</code>).</p>
</dd>
<dt>preprocess</dt><dd><p>an additional option for preprocessing the data.
See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details (default: <code>"center"</code>).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>References</h3>

<p>Lu Y, Cohen I, Zhou XS, Tian Q (2007).
&ldquo;Feature Selection Using Principal Feature Analysis.&rdquo;
In <em>Proceedings of the 15th International Conference on Multimedia  - MULTIMEDIA '07</em>, 301.
ISBN 978-1-59593-702-5.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
## it is known that feature 3 and 4 are more important.
data(iris)
set.seed(100)
subid    &lt;- sample(1:150, 50)
iris.dat &lt;- as.matrix(iris[subid,1:4])
iris.lab &lt;- as.factor(iris[subid,5])

## compare with other methods
out1 = do.pfa(iris.dat)
out2 = do.lscore(iris.dat)
out3 = do.fscore(iris.dat, iris.lab)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=iris.lab, main="Principal Feature Analysis")
plot(out2$Y, pch=19, col=iris.lab, main="Laplacian Score")
plot(out3$Y, pch=19, col=iris.lab, main="Fisher Score")
par(opar)


</code></pre>

<hr>
<h2 id='do.pflpp'>Parameter-Free Locality Preserving Projection</h2><span id='topic+do.pflpp'></span>

<h3>Description</h3>

<p>Conventional LPP is known to suffer from sensitivity upon choice of parameters, especially
in building neighborhood information. Parameter-Free LPP (PFLPP) takes an alternative step
to use normalized Pearson correlation, taking an average of such similarity as a threshold
to decide which points are neighbors of a given datum.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.pflpp(
  X,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "whiten", "decorrelate")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.pflpp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations</p>
</td></tr>
<tr><td><code id="do.pflpp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.pflpp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Dornaika F, Assoum A (2013).
&ldquo;Enhanced and Parameterless Locality Preserving Projections for Face Recognition.&rdquo;
<em>Neurocomputing</em>, <b>99</b>, 448&ndash;457.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## compare with PCA
out1 = do.pca(X, ndim=2)
out2 = do.pflpp(X, ndim=2)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(out1$Y, pch=19, col=label, main="PCA")
plot(out2$Y, pch=19, col=label, main="Parameter-Free LPP")
par(opar)

</code></pre>

<hr>
<h2 id='do.phate'>Potential of Heat Diffusion for Affinity-based Transition Embedding</h2><span id='topic+do.phate'></span>

<h3>Description</h3>

<p>PHATE is a nonlinear method that is specifically targeted at visualizing
high-dimensional data by embedding it on 2- or 3-dimensional space. We offer
a native implementation of PHATE solely in R/C++ without interface to python module.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.phate(
  X,
  ndim = 2,
  k = 5,
  alpha = 10,
  dtype = c("sqrt", "log"),
  smacof = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.phate_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.phate_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension (default: 2).</p>
</td></tr>
<tr><td><code id="do.phate_+3A_k">k</code></td>
<td>
<p>size of nearest neighborhood (default: 5).</p>
</td></tr>
<tr><td><code id="do.phate_+3A_alpha">alpha</code></td>
<td>
<p>decay parameter for Gaussian kernel exponent (default: 10).</p>
</td></tr>
<tr><td><code id="do.phate_+3A_dtype">dtype</code></td>
<td>
<p>type of potential distance transformation; <code>"log"</code> or <code>"sqrt"</code> (default: <code>"sqrt"</code>).</p>
</td></tr>
<tr><td><code id="do.phate_+3A_smacof">smacof</code></td>
<td>
<p>a logical; <code>TRUE</code> to use SMACOF for Metric MDS or <code>FALSE</code> to use Classical MDS (default: <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="do.phate_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>maxiter</dt><dd><p>maximum number of iterations (default: 100).</p>
</dd>
<dt>abstol</dt><dd><p>absolute stopping criterion for metric MDS iterations (default: 1e-8).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>References</h3>

<p>Moon KR, van Dijk D, Wang Z, Gigante S, Burkhardt DB, Chen WS, Yim K, van den Elzen A, Hirn MJ, Coifman RR, Ivanova NB, Wolf G, Krishnaswamy S (2019).
&ldquo;Visualizing Structure and Transitions in High-Dimensional Biological Data.&rdquo;
<em>Nature Biotechnology</em>, <b>37</b>(12), 1482&ndash;1492.
ISSN 1087-0156, 1546-1696.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
X     = as.matrix(iris[,1:4])
lab   = as.factor(iris[,5])

## compare different neighborhood sizes.
pca2d &lt;- do.pca(X, ndim=2)
phk01 &lt;- do.phate(X, ndim=2, k=2)
phk02 &lt;- do.phate(X, ndim=2, k=5)
phk03 &lt;- do.phate(X, ndim=2, k=7)

## Visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(2,2))
plot(pca2d$Y, col=lab, pch=19, main="PCA")
plot(phk01$Y, col=lab, pch=19, main="PHATE:k=2")
plot(phk02$Y, col=lab, pch=19, main="PHATE:k=5")
plot(phk03$Y, col=lab, pch=19, main="PHATE:k=7")
par(opar)


</code></pre>

<hr>
<h2 id='do.plp'>Piecewise Laplacian-based Projection (PLP)</h2><span id='topic+do.plp'></span>

<h3>Description</h3>

<p><code>do.plp</code> is an implementation of Piecewise Laplacian-based Projection (PLP) that
adopts two-stage reduction scheme with local approximation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.plp(X, ndim = 2, type = c("proportion", 0.2))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.plp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.plp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.plp_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>First step is to select <code class="reqn">\sqrt{n}</code> number of control points using <code class="reqn">k</code>-means algorithm.
After selecting control points that play similar roles as representatives of the entire data points,
it performs classical multidimensional scaling.
</p>
<p>For the rest of the data other than control points,
Laplacian Eigenmaps (<code><a href="#topic+do.lapeig">do.lapeig</a></code>) is then applied to high-dimensional data points
lying in neighborhoods of each control point. Embedded low-dimensional local manifold is then
aligned to match their coordinates as of their counterparts from classical MDS.
</p>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Notes</h3>

<p><em>Random Control Points</em> : The performance of embedding using PLP heavily relies on
selection of control points, which is contingent on the performance of <code class="reqn">k</code>-means
clustering.
</p>
<p><em>User Interruption</em> : PLP is actually an interactive algorithm that a user should be able to intervene
intermittently. Such functionality is, however, sacrificed in this version.
</p>


<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Paulovich FV, Eler DM, Poco J, Botha CP, Minghim R, Nonato LG (2011).
&ldquo;Piece Wise Laplacian-Based Projection for Interactive Data Exploration and Organization.&rdquo;
<em>Computer Graphics Forum</em>, <b>30</b>(3), 1091&ndash;1100.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## use iris data
data(iris)
X     = as.matrix(iris[,1:4])
label = as.integer(iris$Species)

## try with 3 levels of connectivity
out1 = do.plp(X, type=c("proportion", 0.1))
out2 = do.plp(X, type=c("proportion", 0.2))
out3 = do.plp(X, type=c("proportion", 0.5))

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=label, main="PLP::10% connected")
plot(out2$Y, col=label, main="PLP::20% connected")
plot(out3$Y, col=label, main="PLP::50% connected")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='do.pls'>Partial Least Squares</h2><span id='topic+do.pls'></span>

<h3>Description</h3>

<p>Given two data sets, Partial Least Squares (PLS) aims at maximizing cross-covariance of latent variables for each data matrix,
therefore it can be considered as supervised methods. As we have two input matrices, <code>do.pls</code> generates two sets of
outputs. Though it is widely used for regression problem, we used it in dimension reduction setting. For
algorithm aspects, we used recursive gram-schmidt orthogonalization in conjunction with extracting projection vectors under
eigen-decomposition formulation, as the problem dimension matters only up to original dimensionality.
For more details, see <a href="https://en.wikipedia.org/wiki/Partial_least_squares_regression">Wikipedia entry</a> on PLS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.pls(data1, data2, ndim = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.pls_+3A_data1">data1</code></td>
<td>
<p>an <code class="reqn">(n\times N)</code> data matrix whose rows are observations</p>
</td></tr>
<tr><td><code id="do.pls_+3A_data2">data2</code></td>
<td>
<p>an <code class="reqn">(n\times M)</code> data matrix whose rows are observations</p>
</td></tr>
<tr><td><code id="do.pls_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y1</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix of projected observations from <code>data1</code>.</p>
</dd>
<dt>Y2</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix of projected observations from <code>data2</code>.</p>
</dd>
<dt>projection1</dt><dd><p>an <code class="reqn">(N\times ndim)</code> whose columns are loadings for <code>data1</code>.</p>
</dd>
<dt>projection2</dt><dd><p>an <code class="reqn">(M\times ndim)</code> whose columns are loadings for <code>data2</code>.</p>
</dd>
<dt>trfinfo1</dt><dd><p>a list containing information for out-of-sample prediction for <code>data1</code>.</p>
</dd>
<dt>trfinfo2</dt><dd><p>a list containing information for out-of-sample prediction for <code>data2</code>.</p>
</dd>
<dt>eigvals</dt><dd><p>a vector of eigenvalues for iterative decomposition.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Wold H (1975).
&ldquo;Path Models with Latent Variables: The NIPALS Approach.&rdquo;
In <em>Quantitative Sociology</em>, 307&ndash;357.
Elsevier.
ISBN 978-0-12-103950-9.
</p>
<p>Rosipal R, Krämer N (2006).
&ldquo;Overview and Recent Advances in Partial Least Squares.&rdquo;
In Saunders C, Grobelnik M, Gunn S, Shawe-Taylor J (eds.), <em>Subspace, Latent Structure and Feature Selection: Statistical and Optimization Perspectives Workshop, SLSFS 2005, Bohinj, Slovenia, February 23-25, 2005, Revised Selected Papers</em>, 34&ndash;51.
Springer Berlin Heidelberg, Berlin, Heidelberg.
ISBN 978-3-540-34138-3.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.cca">do.cca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate 2 normal data matrices
mat1 = matrix(rnorm(100*12),nrow=100)+10 # 12-dim normal
mat2 = matrix(rnorm(100*6), nrow=100)-10 # 6-dim normal

## project onto 2 dimensional space for each data
output = do.pls(mat1, mat2, ndim=2)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(output$Y1, main="proj(mat1)")
plot(output$Y2, main="proj(mat2)")
par(opar)

</code></pre>

<hr>
<h2 id='do.ppca'>Probabilistic Principal Component Analysis</h2><span id='topic+do.ppca'></span>

<h3>Description</h3>

<p>Probabilistic PCA (PPCA) is a probabilistic framework to explain the well-known PCA model. Using
the conjugacy of normal model, we compute MLE for values explicitly derived in the paper. Note that
unlike PCA where loadings are directly used for projection, PPCA uses <code class="reqn">WM^{-1}</code> as projection matrix,
as it is relevant to the error model. Also, for high-dimensional problem, it is possible that MLE can have
negative values if sample covariance given the data is rank-deficient.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.ppca(X, ndim = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.ppca_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.ppca_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>mle.sigma2</dt><dd><p>MLE for <code class="reqn">\sigma^2</code>.</p>
</dd>
<dt>mle.W</dt><dd><p>MLE of a <code class="reqn">(p\times ndim)</code> mapping from latent to observation in column major.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Tipping ME, Bishop CM (1999).
&ldquo;Probabilistic Principal Component Analysis.&rdquo;
<em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, <b>61</b>(3), 611&ndash;622.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.pca">do.pca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## Compare PCA and PPCA
PCA  &lt;- do.pca(X, ndim=2)
PPCA &lt;- do.ppca(X, ndim=2)

## Visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(PCA$Y,  pch=19, col=label, main="PCA")
plot(PPCA$Y, pch=19, col=label, main="PPCA")
par(opar)


</code></pre>

<hr>
<h2 id='do.procrustes'>Feature Selection using PCA and Procrustes Analysis</h2><span id='topic+do.procrustes'></span>

<h3>Description</h3>

<p><code>do.procrustes</code> selects a set of features that best aligns PCA's coordinates in the embedded low dimension.
It iteratively selects each variable that minimizes Procrustes distance between configurations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.procrustes(X, ndim = 2, intdim = (ndim - 1), cor = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.procrustes_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.procrustes_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.procrustes_+3A_intdim">intdim</code></td>
<td>
<p>intrinsic dimension of PCA to be applied. It should be smaller than <code>ndim</code>.</p>
</td></tr>
<tr><td><code id="do.procrustes_+3A_cor">cor</code></td>
<td>
<p>mode of eigendecomposition. <code>FALSE</code> for decomposing covariance, and <code>TRUE</code> for correlation matrix in PCA.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Krzanowski WJ (1987).
&ldquo;Selection of Variables to Preserve Multivariate Data Structure, Using Principal Components.&rdquo;
<em>Applied Statistics</em>, <b>36</b>(1), 22.
ISSN 00359254.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
## it is known that feature 3 and 4 are more important.
data(iris)
iris.dat = as.matrix(iris[,1:4])
iris.lab = as.factor(iris[,5])

## try different strategy
out1 = do.procrustes(iris.dat, cor=TRUE)
out2 = do.procrustes(iris.dat, cor=FALSE)
out3 = do.mifs(iris.dat, iris.lab, beta=0)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1, 3))
plot(out1$Y, pch=19, col=iris.lab, main="PCA with Covariance")
plot(out2$Y, pch=19, col=iris.lab, main="PCA with Correlation")
plot(out3$Y, pch=19, col=iris.lab, main="MIFS")
par(opar)


</code></pre>

<hr>
<h2 id='do.ree'>Robust Euclidean Embedding</h2><span id='topic+do.ree'></span>

<h3>Description</h3>

<p>Robust Euclidean Embedding (REE) is an embedding procedure exploiting
robustness of <code class="reqn">\ell_1</code> cost function. In our implementation, we adopted
a generalized version with weight matrix to be applied as well. Its original
paper introduced a subgradient algorithm to overcome memory-intensive nature of
original semidefinite programming formulation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.ree(
  X,
  ndim = 2,
  W = NA,
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate"),
  initc = 1,
  dmethod = c("euclidean", "maximum", "manhattan", "canberra", "binary", "minkowski"),
  maxiter = 100,
  abstol = 0.001
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.ree_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.ree_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.ree_+3A_w">W</code></td>
<td>
<p>an <code class="reqn">(n\times n)</code> weight matrix. Default is uniform weight of 1s.</p>
</td></tr>
<tr><td><code id="do.ree_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.ree_+3A_initc">initc</code></td>
<td>
<p>initial <code>c</code> value for subgradient iterating stepsize, <code class="reqn">c/\sqrt{i}</code>.</p>
</td></tr>
<tr><td><code id="do.ree_+3A_dmethod">dmethod</code></td>
<td>
<p>a type of distance measure. See <code><a href="stats.html#topic+dist">dist</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.ree_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations for subgradient descent method.</p>
</td></tr>
<tr><td><code id="do.ree_+3A_abstol">abstol</code></td>
<td>
<p>stopping criterion for subgradient descent method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>niter</dt><dd><p>the number of iterations taken til convergence. </p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Cayton L, Dasgupta S (2006).
&ldquo;Robust Euclidean Embedding.&rdquo;
In <em>Proceedings of the 23rd International Conference on Machine Learning</em>,  ICML '06, 169&ndash;176.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## try different distance method
output1 &lt;- do.ree(X, maxiter=50, dmethod="euclidean")
output2 &lt;- do.ree(X, maxiter=50, dmethod="maximum")
output3 &lt;- do.ree(X, maxiter=50, dmethod="canberra")

## visualize three different projections
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(output1$Y, col=label, pch=19, main="dmethod-euclidean")
plot(output2$Y, col=label, pch=19, main="dmethod-maximum")
plot(output3$Y, col=label, pch=19, main="dmethod-canberra")
par(opar)


</code></pre>

<hr>
<h2 id='do.rlda'>Regularized Linear Discriminant Analysis</h2><span id='topic+do.rlda'></span>

<h3>Description</h3>

<p>In small sample case, Linear Discriminant Analysis (LDA) may suffer from
rank deficiency issue. Applied mathematics has used Tikhonov regularization -
also known as <code class="reqn">\ell_2</code> regularization/shrinkage - to adjust linear operator.
Regularized Linear Discriminant Analysis (RLDA) adopts such idea to stabilize
eigendecomposition in LDA formulation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.rlda(X, label, ndim = 2, alpha = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.rlda_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.rlda_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.rlda_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.rlda_+3A_alpha">alpha</code></td>
<td>
<p>Tikhonow regularization parameter.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Friedman JH (1989).
&ldquo;Regularized Discriminant Analysis.&rdquo;
<em>Journal of the American Statistical Association</em>, <b>84</b>(405), 165.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## try different regularization parameters
out1 &lt;- do.rlda(X, label, alpha=0.001)
out2 &lt;- do.rlda(X, label, alpha=0.01)
out3 &lt;- do.rlda(X, label, alpha=100)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="RLDA::alpha=0.1")
plot(out2$Y, pch=19, col=label, main="RLDA::alpha=1")
plot(out3$Y, pch=19, col=label, main="RLDA::alpha=10")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='do.rndproj'>Random Projection</h2><span id='topic+do.rndproj'></span>

<h3>Description</h3>

<p><code>do.rndproj</code> is a linear dimensionality reduction method based on
random projection technique, featured by the celebrated Johnson–Lindenstrauss lemma.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.rndproj(
  X,
  ndim = 2,
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate"),
  type = c("gaussian", "achlioptas", "sparse"),
  s = max(sqrt(ncol(X)), 3)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.rndproj_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.rndproj_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.rndproj_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.rndproj_+3A_type">type</code></td>
<td>
<p>a type of random projection, one of &quot;gaussian&quot;,&quot;achlioptas&quot; or &quot;sparse&quot;.</p>
</td></tr>
<tr><td><code id="do.rndproj_+3A_s">s</code></td>
<td>
<p>a tuning parameter for determining values in projection matrix. While default
is to use <code class="reqn">max(log \sqrt{p},3)</code>, it is required for <code class="reqn">s \ge 3</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Johnson-Lindenstrauss(JL) lemma states that given <code class="reqn">0 &lt; \epsilon &lt; 1</code>, for a set
<code class="reqn">X</code> of <code class="reqn">m</code> points in <code class="reqn">R^N</code> and a number <code class="reqn">n &gt; 8log(m)/\epsilon^2</code>,
there is a linear map <code class="reqn">f:R^N</code> to R^n such that
</p>
<p style="text-align: center;"><code class="reqn">(1-\epsilon)|u-v|^2 \le |f(u)-f(v)|^2 \le (1+\epsilon)|u-v|^2</code>
</p>

<p>for all <code class="reqn">u,v</code> in <code class="reqn">X</code>.
</p>
<p>Three types of random projections are supported for an <code>(p-by-ndim)</code> projection matrix <code class="reqn">R</code>.
</p>

<ol>
<li><p> Conventional approach is to use normalized Gaussian random vectors sampled from unit sphere <code class="reqn">S^{p-1}</code>.
</p>
</li>
<li><p> Achlioptas suggested to employ a sparse approach using samples from <code class="reqn">\sqrt{3}(1,0,-1)</code> with probability <code class="reqn">(1/6,4/6,1/6)</code>.
</p>
</li>
<li><p> Li et al proposed to  sample from <code class="reqn">\sqrt{s}(1,0,-1)</code>
with probability <code class="reqn">(1/2s,1-1/s,1/2s)</code> for <code class="reqn">s\ge 3</code>
to incorporate sparsity while attaining speedup with little loss in accuracy. While
the original suggsetion from the authors is to use <code class="reqn">\sqrt{p}</code> or <code class="reqn">p/log(p)</code>
for <code class="reqn">s</code>, any user-supported <code class="reqn">s \ge 3</code> is allowed.
</p>
</li></ol>



<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>epsilon</dt><dd><p>an estimated error <code class="reqn">\epsilon</code> in accordance with JL lemma.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>References</h3>

<p>Johnson WB, Lindenstrauss J (1984).
&ldquo;Extensions of Lipschitz Mappings into a Hilbert Space.&rdquo;
In Beals R, Beck A, Bellow A, Hajian A (eds.), <em>Contemporary Mathematics</em>, volume 26, 189&ndash;206.
American Mathematical Society, Providence, Rhode Island.
ISBN 978-0-8218-5030-5 978-0-8218-7611-4.
</p>
<p>Achlioptas D (2003).
&ldquo;Database-Friendly Random Projections: Johnson-Lindenstrauss with Binary Coins.&rdquo;
<em>Journal of Computer and System Sciences</em>, <b>66</b>(4), 671&ndash;687.
</p>
<p>Li P, Hastie TJ, Church KW (2006).
&ldquo;Very Sparse Random Projections.&rdquo;
In <em>Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>,  KDD '06, 287&ndash;296.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## 1. Gaussian projection
output1 &lt;- do.rndproj(X,ndim=2)

## 2. Achlioptas projection
output2 &lt;- do.rndproj(X,ndim=2,type="achlioptas")

## 3. Sparse projection
output3 &lt;- do.rndproj(X,type="sparse")

## Visualize three different projections
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(output1$Y, pch=19, col=label, main="RNDPROJ::Gaussian")
plot(output2$Y, pch=19, col=label, main="RNDPROJ::Arclioptas")
plot(output3$Y, pch=19, col=label, main="RNDPROJ::Sparse")
par(opar)

</code></pre>

<hr>
<h2 id='do.rpca'>Robust Principal Component Analysis</h2><span id='topic+do.rpca'></span>

<h3>Description</h3>

<p>Robust PCA (RPCA) is not like other methods in this package as finding explicit low-dimensional embedding with reduced number of columns.
Rather, it is more of a decomposition method of data matrix <code class="reqn">X</code>, possibly noisy, into low-rank and sparse matrices by
solving the following,
</p>
<p style="text-align: center;"><code class="reqn">\textrm{minimize}\quad \|L\|_* + \lambda \|S\|_1 \quad{s.t.} L+S=X</code>
</p>

<p>where <code class="reqn">L</code> is a low-rank matrix, <code class="reqn">S</code> is a sparse matrix and <code class="reqn">\|\cdot\|_*</code> denotes nuclear norm, i.e., sum of singular values. Therefore,
it should be considered as <em>preprocessing</em> procedure of denoising. Note that after RPCA is applied, <code class="reqn">L</code> should be used
as kind of a new data matrix for any manifold learning scheme to be applied.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.rpca(X, mu = 1, lambda = sqrt(1/(max(dim(X)))), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.rpca_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or whose rows are observations and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.rpca_+3A_mu">mu</code></td>
<td>
<p>an augmented Lagrangian parameter</p>
</td></tr>
<tr><td><code id="do.rpca_+3A_lambda">lambda</code></td>
<td>
<p>parameter for the sparsity term <code class="reqn">\|S\|_1</code>. Default value is given accordingly to the referred paper.</p>
</td></tr>
<tr><td><code id="do.rpca_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>maxiter</dt><dd><p>maximum number of iterations (default: 100).</p>
</dd>
<dt>abstol</dt><dd><p>absolute tolerance stopping criterion (default: 1e-8).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>L</dt><dd><p>an <code class="reqn">(n\times p)</code> low-rank matrix.</p>
</dd>
<dt>S</dt><dd><p>an <code class="reqn">(n\times p)</code> sparse matrix.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Candès EJ, Li X, Ma Y, Wright J (2011).
&ldquo;Robust Principal Component Analysis?&rdquo;
<em>Journal of the ACM</em>, <b>58</b>(3), 1&ndash;37.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data and add some noise
data(iris, package="Rdimtools")
set.seed(100)
subid = sample(1:150,50)
noise = 0.2
X = as.matrix(iris[subid,1:4])
X = X + matrix(noise*rnorm(length(X)), nrow=nrow(X))
lab = as.factor(iris[subid,5])

## try different regularization parameters
rpca1 = do.rpca(X, lambda=0.1)
rpca2 = do.rpca(X, lambda=1)
rpca3 = do.rpca(X, lambda=10)

## apply identical PCA methods
Y1 = do.pca(rpca1$L, ndim=2)$Y
Y2 = do.pca(rpca2$L, ndim=2)$Y
Y3 = do.pca(rpca3$L, ndim=2)$Y

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(Y1, pch=19, col=lab, main="RPCA+PCA::lambda=0.1")
plot(Y2, pch=19, col=lab, main="RPCA+PCA::lambda=1")
plot(Y3, pch=19, col=lab, main="RPCA+PCA::lambda=10")
par(opar)


</code></pre>

<hr>
<h2 id='do.rpcag'>Robust Principal Component Analysis via Geometric Median</h2><span id='topic+do.rpcag'></span>

<h3>Description</h3>

<p>This function robustifies the traditional PCA via an idea of geometric median.
To describe, the given data is first split into <code>k</code> subsets for each sample
covariance is attained. According to the paper, the median covariance is computed
under Frobenius norm and projection is extracted from the largest eigenvectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.rpcag(
  X,
  ndim = 2,
  k = 5,
  preprocess = c("center", "scale", "cscale", "whiten", "decorrelate")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.rpcag_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.rpcag_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.rpcag_+3A_k">k</code></td>
<td>
<p>the number of subsets for <code>X</code> to be divided.</p>
</td></tr>
<tr><td><code id="do.rpcag_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Minsker S (2015).
&ldquo;Geometric Median and Robust Estimation in Banach Spaces.&rdquo;
<em>Bernoulli</em>, <b>21</b>(4), 2308&ndash;2335.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris data
data(iris)
X     = as.matrix(iris[,1:4])
label = as.integer(iris$Species)

## try different numbers for subsets
out1 = do.rpcag(X, ndim=2, k=2)
out2 = do.rpcag(X, ndim=2, k=5)
out3 = do.rpcag(X, ndim=2, k=10)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=label, main="RPCAG::k=2")
plot(out2$Y, col=label, main="RPCAG::k=5")
plot(out3$Y, col=label, main="RPCAG::k=10")
par(opar)

</code></pre>

<hr>
<h2 id='do.rsir'>Regularized Sliced Inverse Regression</h2><span id='topic+do.rsir'></span>

<h3>Description</h3>

<p>One of possible drawbacks in SIR method is that for high-dimensional data, it might suffer from
rank deficiency of scatter/covariance matrix. Instead of naive matrix inversion, several have
proposed regularization schemes that reflect several ideas from various incumbent methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.rsir(
  X,
  response,
  ndim = 2,
  h = max(2, round(nrow(X)/5)),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  regmethod = c("Ridge", "Tikhonov", "PCA", "PCARidge", "PCATikhonov"),
  tau = 1,
  numpc = ndim
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.rsir_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.rsir_+3A_response">response</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of response variable.</p>
</td></tr>
<tr><td><code id="do.rsir_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.rsir_+3A_h">h</code></td>
<td>
<p>the number of slices to divide the range of response vector.</p>
</td></tr>
<tr><td><code id="do.rsir_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.rsir_+3A_regmethod">regmethod</code></td>
<td>
<p>type of regularization scheme to be used.</p>
</td></tr>
<tr><td><code id="do.rsir_+3A_tau">tau</code></td>
<td>
<p>regularization parameter for adjusting rank-deficient scatter matrix.</p>
</td></tr>
<tr><td><code id="do.rsir_+3A_numpc">numpc</code></td>
<td>
<p>number of principal components to be used in intermediate dimension reduction scheme.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Chiaromonte F, Martinelli J (2002).
&ldquo;Dimension Reduction Strategies for Analyzing Global Gene Expression Data with a Response.&rdquo;
<em>Mathematical Biosciences</em>, <b>176</b>(1), 123&ndash;144.
ISSN 0025-5564.
</p>
<p>Zhong W, Zeng P, Ma P, Liu JS, Zhu Y (2005).
&ldquo;RSIR: Regularized Sliced Inverse Regression for Motif Discovery.&rdquo;
<em>Bioinformatics</em>, <b>21</b>(22), 4169&ndash;4175.
</p>
<p>Bernard-Michel C, Gardes L, Girard S (2009).
&ldquo;Gaussian Regularized Sliced Inverse Regression.&rdquo;
<em>Statistics and Computing</em>, <b>19</b>(1), 85&ndash;98.
</p>
<p>Bernard-Michel C, Douté S, Fauvel M, Gardes L, Girard S (2009).
&ldquo;Retrieval of Mars Surface Physical Properties from OMEGA Hyperspectral Images Using Regularized Sliced Inverse Regression.&rdquo;
<em>Journal of Geophysical Research</em>, <b>114</b>(E6).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.sir">do.sir</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate swiss roll with auxiliary dimensions
## it follows reference example from LSIR paper.
set.seed(100)
n     = 50
theta = runif(n)
h     = runif(n)
t     = (1+2*theta)*(3*pi/2)
X     = array(0,c(n,10))
X[,1] = t*cos(t)
X[,2] = 21*h
X[,3] = t*sin(t)
X[,4:10] = matrix(runif(7*n), nrow=n)

## corresponding response vector
y = sin(5*pi*theta)+(runif(n)*sqrt(0.1))

## try with different regularization methods
## use default number of slices
out1 = do.rsir(X, y, regmethod="Ridge")
out2 = do.rsir(X, y, regmethod="Tikhonov")
outsir = do.sir(X, y)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y,   main="RSIR::Ridge")
plot(out2$Y,   main="RSIR::Tikhonov")
plot(outsir$Y, main="standard SIR")
par(opar)

</code></pre>

<hr>
<h2 id='do.rsr'>Regularized Self-Representation</h2><span id='topic+do.rsr'></span>

<h3>Description</h3>

<p>Given a data matrix <code class="reqn">X</code> where observations are stacked in a row-wise manner,
Regularized Self-Representation (RSR) aims at finding a solution to following optimization problem
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}~ \|X-XW\|_{2,1} + \lambda \| W \|_{2,1}</code>
</p>

<p>where <code class="reqn">\|W\|_{2,1} = \sum_{i=1}^{m} \|W_{i:} \|_2</code> is an <code class="reqn">\ell_{2,1}</code> norm that imposes
row-wise sparsity constraint.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.rsr(X, ndim = 2, lbd = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.rsr_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.rsr_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.rsr_+3A_lbd">lbd</code></td>
<td>
<p>nonnegative number to control the degree of self-representation by imposing row-sparsity.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zhu P, Zuo W, Zhang L, Hu Q, Shiu SC (2015).
&ldquo;Unsupervised Feature Selection by Regularized Self-Representation.&rdquo;
<em>Pattern Recognition</em>, <b>48</b>(2), 438&ndash;446.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

#### try different lbd combinations
out1 = do.rsr(X, lbd=0.1)
out2 = do.rsr(X, lbd=1)
out3 = do.rsr(X, lbd=10)

#### visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="RSR::lbd=0.1")
plot(out2$Y, pch=19, col=label, main="RSR::lbd=1")
plot(out3$Y, pch=19, col=label, main="RSR::lbd=10")
par(opar)


</code></pre>

<hr>
<h2 id='do.sammc'>Semi-Supervised Adaptive Maximum Margin Criterion</h2><span id='topic+do.sammc'></span>

<h3>Description</h3>

<p>Semi-Supervised Adaptive Maximum Margin Criterion (SAMMC) is a semi-supervised variant of
AMMC by making use of both labeled and unlabeled data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.sammc(
  X,
  label,
  ndim = 2,
  type = c("proportion", 0.1),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  a = 1,
  b = 1,
  lambda = 1,
  beta = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.sammc_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.sammc_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.sammc_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.sammc_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.sammc_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.sammc_+3A_a">a</code></td>
<td>
<p>tuning parameter for between-class weight in <code class="reqn">[0,\infty)</code>.</p>
</td></tr>
<tr><td><code id="do.sammc_+3A_b">b</code></td>
<td>
<p>tuning parameter for within-class weight in <code class="reqn">[0,\infty)</code>.</p>
</td></tr>
<tr><td><code id="do.sammc_+3A_lambda">lambda</code></td>
<td>
<p>balance parameter for between-class and within-class scatter matrices in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
<tr><td><code id="do.sammc_+3A_beta">beta</code></td>
<td>
<p>balance parameter for within-class scatter of the labeled data and consistency of the whole data in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Lu J, Tan Y (2011).
&ldquo;Adaptive Maximum Margin Criterion for Image Classification.&rdquo;
In <em>2011 IEEE International Conference on Multimedia and Expo</em>, 1&ndash;6.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.mmc">do.mmc</a></code>, <code><a href="#topic+do.ammc">do.ammc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate data of 3 types with clear difference
set.seed(100)
dt1  = aux.gensamples(n=33)-50
dt2  = aux.gensamples(n=33)
dt3  = aux.gensamples(n=33)+50

## merge the data and create a label correspondingly
X      = rbind(dt1,dt2,dt3)
label  = rep(1:3, each=33)

## copy a label and let 20% of elements be missing
nlabel = length(label)
nmissing = round(nlabel*0.20)
label_missing = label
label_missing[sample(1:nlabel, nmissing)]=NA

## try different balancing
out1 = do.sammc(X, label_missing, beta=0.1)
out2 = do.sammc(X, label_missing, beta=1)
out3 = do.sammc(X, label_missing, beta=10)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="SAMMC::beta=0.1")
plot(out2$Y, pch=19, col=label, main="SAMMC::beta=1")
plot(out3$Y, pch=19, col=label, main="SAMMC::beta=10")
par(opar)

</code></pre>

<hr>
<h2 id='do.sammon'>Sammon Mapping</h2><span id='topic+do.sammon'></span>

<h3>Description</h3>

<p><code>do.sammon</code> is an implementation for Sammon mapping, one of the earliest
dimension reduction techniques that aims to find low-dimensional embedding
that preserves pairwise distance structure in high-dimensional data space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.sammon(
  X,
  ndim = 2,
  preprocess = c("null", "center", "scale", "cscale", "decorrelate", "whiten"),
  initialize = c("pca", "random")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.sammon_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.sammon_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.sammon_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.sammon_+3A_initialize">initialize</code></td>
<td>
<p><code>"random"</code> or <code>"pca"</code>; the former performs
fast random projection (see also <code><a href="#topic+do.rndproj">do.rndproj</a></code>) and the latter
performs standard PCA (see also <code><a href="#topic+do.pca">do.pca</a></code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Sammon, J.W. (1969) <em>A Nonlinear Mapping for Data Structure Analysis</em>.
IEEE Transactions on Computers, C-18 5:401-409.
</p>
<p>Sammon JW (1969).
&ldquo;A Nonlinear Mapping for Data Structure Analysis.&rdquo;
<em>IEEE Transactions on Computers</em>, <b>C-18</b>(5), 401&ndash;409.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
X     = as.matrix(iris[,1:4])
label = as.factor(iris$Species)

## compare two initialization
out1 = do.sammon(X,ndim=2)                   # random projection
out2 = do.sammon(X,ndim=2,initialize="pca")  # pca as initialization

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(out1$Y, pch=19, col=label, main="out1:rndproj")
plot(out2$Y, pch=19, col=label, main="out2:pca")
par(opar)


</code></pre>

<hr>
<h2 id='do.save'>Sliced Average Variance Estimation</h2><span id='topic+do.save'></span>

<h3>Description</h3>

<p>Sliced Average Variance Estimation (SAVE) is a supervised linear dimension reduction method.
It is based on sufficiency principle with respect to central subspace concept under the
linerity and constant covariance conditions. For more details, see the reference paper.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.save(
  X,
  response,
  ndim = 2,
  h = max(2, round(nrow(X)/5)),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.save_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.save_+3A_response">response</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of response variable.</p>
</td></tr>
<tr><td><code id="do.save_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.save_+3A_h">h</code></td>
<td>
<p>the number of slices to divide the range of response vector.</p>
</td></tr>
<tr><td><code id="do.save_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Dennis Cook R (2000).
&ldquo;Save: A Method for Dimension Reduction and Graphics in Regression.&rdquo;
<em>Communications in Statistics - Theory and Methods</em>, <b>29</b>(9-10), 2109&ndash;2121.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.sir">do.sir</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate swiss roll with auxiliary dimensions
## it follows reference example from LSIR paper.
set.seed(100)
n = 50
theta = runif(n)
h     = runif(n)
t     = (1+2*theta)*(3*pi/2)
X     = array(0,c(n,10))
X[,1] = t*cos(t)
X[,2] = 21*h
X[,3] = t*sin(t)
X[,4:10] = matrix(runif(7*n), nrow=n)

## corresponding response vector
y = sin(5*pi*theta)+(runif(n)*sqrt(0.1))

## try with different numbers of slices
out1 = do.save(X, y, h=2)
out2 = do.save(X, y, h=5)
out3 = do.save(X, y, h=10)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="SAVE::2 slices")
plot(out2$Y, main="SAVE::5 slices")
plot(out3$Y, main="SAVE::10 slices")
par(opar)

</code></pre>

<hr>
<h2 id='do.sda'>Semi-Supervised Discriminant Analysis</h2><span id='topic+do.sda'></span>

<h3>Description</h3>

<p>Semi-Supervised Discriminant Analysis (SDA) is a linear dimension reduction method
when label is partially missing, i.e., semi-supervised. The labeled data
points are used to maximize the separability between classes while
the unlabeled ones to estimate the intrinsic structure of the data.
Regularization in case of rank-deficient case is also supported via an <code class="reqn">\ell_2</code>
scheme via <code>beta</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.sda(X, label, ndim = 2, type = c("proportion", 0.1), alpha = 1, beta = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.sda_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.sda_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.sda_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.sda_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.sda_+3A_alpha">alpha</code></td>
<td>
<p>balancing parameter between model complexity and empirical loss.</p>
</td></tr>
<tr><td><code id="do.sda_+3A_beta">beta</code></td>
<td>
<p>Tikhonov regularization parameter.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Cai D, He X, Han J (2007).
&ldquo;Semi-Supervised Discriminant Analysis.&rdquo;
In <em>2007 IEEE 11th International Conference on Computer Vision</em>, 1&ndash;7.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris data
data(iris)
X     = as.matrix(iris[,1:4])
label = as.integer(iris$Species)

## copy a label and let 20% of elements be missing
nlabel = length(label)
nmissing = round(nlabel*0.20)
label_missing = label
label_missing[sample(1:nlabel, nmissing)]=NA

## compare true case with missing-label case
out1 = do.sda(X, label)
out2 = do.sda(X, label_missing)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(out1$Y, col=label, main="true projection")
plot(out2$Y, col=label, main="20% missing labels")
par(opar)

</code></pre>

<hr>
<h2 id='do.sdlpp'>Sample-Dependent Locality Preserving Projection</h2><span id='topic+do.sdlpp'></span>

<h3>Description</h3>

<p>Many variants of Locality Preserving Projection are contingent on
graph construction schemes in that they sometimes return a range of
heterogeneous results when parameters are controlled to cover a wide range of values.
This algorithm takes an approach called <em>sample-dependent construction</em> of
graph connectivity in that it tries to discover intrinsic structures of data
solely based on data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.sdlpp(
  X,
  ndim = 2,
  t = 1,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.sdlpp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.sdlpp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.sdlpp_+3A_t">t</code></td>
<td>
<p>kernel bandwidth in <code class="reqn">(0,\infty)</code>.</p>
</td></tr>
<tr><td><code id="do.sdlpp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Yang B, Chen S (2010).
&ldquo;Sample-Dependent Graph Construction with Application to Dimensionality Reduction.&rdquo;
<em>Neurocomputing</em>, <b>74</b>(1-3), 301&ndash;314.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.lpp">do.lpp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## compare with PCA
out1 &lt;- do.pca(X,ndim=2)
out2 &lt;- do.sdlpp(X, t=0.01)
out3 &lt;- do.sdlpp(X, t=10)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="PCA")
plot(out2$Y, pch=19, col=label, main="SDLPP::t=1")
plot(out3$Y, pch=19, col=label, main="SDLPP::t=10")
par(opar)

</code></pre>

<hr>
<h2 id='do.sir'>Sliced Inverse Regression</h2><span id='topic+do.sir'></span>

<h3>Description</h3>

<p>Sliced Inverse Regression (SIR) is a supervised linear dimension reduction technique.
Unlike engineering-driven methods, SIR takes a concept of <em>central subspace</em>, where
conditional independence after projection is guaranteed. It first divides the range of
response variable. Projection vectors are extracted where projected data best explains response variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.sir(
  X,
  response,
  ndim = 2,
  h = max(2, round(nrow(X)/5)),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.sir_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.sir_+3A_response">response</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of response variable.</p>
</td></tr>
<tr><td><code id="do.sir_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.sir_+3A_h">h</code></td>
<td>
<p>the number of slices to divide the range of response vector.</p>
</td></tr>
<tr><td><code id="do.sir_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Li K (1991).
&ldquo;Sliced Inverse Regression for Dimension Reduction.&rdquo;
<em>Journal of the American Statistical Association</em>, <b>86</b>(414), 316.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate swiss roll with auxiliary dimensions
## it follows reference example from LSIR paper.
set.seed(100)
n = 50
theta = runif(n)
h     = runif(n)
t     = (1+2*theta)*(3*pi/2)
X     = array(0,c(n,10))
X[,1] = t*cos(t)
X[,2] = 21*h
X[,3] = t*sin(t)
X[,4:10] = matrix(runif(7*n), nrow=n)

## corresponding response vector
y = sin(5*pi*theta)+(runif(n)*sqrt(0.1))

## try with different numbers of slices
out1 = do.sir(X, y, h=2)
out2 = do.sir(X, y, h=5)
out3 = do.sir(X, y, h=10)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="SIR::2 slices")
plot(out2$Y, main="SIR::5 slices")
plot(out3$Y, main="SIR::10 slices")
par(opar)

</code></pre>

<hr>
<h2 id='do.slpe'>Supervised Locality Pursuit Embedding</h2><span id='topic+do.slpe'></span>

<h3>Description</h3>

<p>Supervised Locality Pursuit Embedding (SLPE) is a supervised extension of LPE
that uses class labels of data points in order to enhance discriminating power in
its mapping into a low dimensional space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.slpe(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.slpe_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.slpe_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.slpe_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.slpe_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zheng Z, Yang J (2006).
&ldquo;Supervised Locality Pursuit Embedding for Pattern Classification.&rdquo;
<em>Image and Vision Computing</em>, <b>24</b>(8), 819&ndash;826.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.lpe">do.lpe</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## compare SLPE with SLPP
out1 &lt;- do.slpp(X, label)
out2 &lt;- do.slpe(X, label)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(out1$Y, pch=19, col=label, main="SLPP")
plot(out2$Y, pch=19, col=label, main="SLPE")
par(opar)

</code></pre>

<hr>
<h2 id='do.slpp'>Supervised Locality Preserving Projection</h2><span id='topic+do.slpp'></span>

<h3>Description</h3>

<p>As its names suggests, Supervised Locality Preserving Projection (SLPP) is a variant of LPP
in that it replaces neighborhood network construction schematic with class information in that
if two nodes belong to the same class, it assigns weight of 1, i.e., <code class="reqn">S_{ij}=1</code> if <code class="reqn">x_i</code> and
<code class="reqn">x_j</code> have same class labelings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.slpp(X, label, ndim = 2, preprocess = c("center", "decorrelate", "whiten"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.slpp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="do.slpp_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.slpp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.slpp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot; and other options of &quot;decorrelate&quot; and &quot;whiten&quot;
are supported. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zheng Z, Yang F, Tan W, Jia J, Yang J (2007).
&ldquo;Gabor Feature-Based Face Recognition Using Supervised Locality Preserving Projection.&rdquo;
<em>Signal Processing</em>, <b>87</b>(10), 2473&ndash;2483.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.lpp">do.lpp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## compare SLPP with LPP
outLPP  &lt;- do.lpp(X)
outSLPP &lt;- do.slpp(X, label)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(outLPP$Y,  pch=19, col=label, main="LPP")
plot(outSLPP$Y, pch=19, col=label, main="SLPP")
par(opar)

</code></pre>

<hr>
<h2 id='do.sne'>Stochastic Neighbor Embedding</h2><span id='topic+do.sne'></span>

<h3>Description</h3>

<p>Stochastic Neighbor Embedding (SNE) is a probabilistic approach to mimick distributional
description in high-dimensional - possible, nonlinear - subspace on low-dimensional target space.
<code>do.sne</code> fully adopts algorithm details in an original paper by Hinton and Roweis (2002).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.sne(
  X,
  ndim = 2,
  perplexity = 30,
  eta = 0.05,
  maxiter = 2000,
  jitter = 0.3,
  jitterdecay = 0.99,
  momentum = 0.5,
  pca = TRUE,
  pcascale = FALSE,
  symmetric = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.sne_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.sne_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.sne_+3A_perplexity">perplexity</code></td>
<td>
<p>desired level of perplexity; ranging [5,50].</p>
</td></tr>
<tr><td><code id="do.sne_+3A_eta">eta</code></td>
<td>
<p>learning parameter.</p>
</td></tr>
<tr><td><code id="do.sne_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code id="do.sne_+3A_jitter">jitter</code></td>
<td>
<p>level of white noise added at the beginning.</p>
</td></tr>
<tr><td><code id="do.sne_+3A_jitterdecay">jitterdecay</code></td>
<td>
<p>decay parameter in <code class="reqn">(0,1)</code>. The closer to 0, the faster artificial noise decays.</p>
</td></tr>
<tr><td><code id="do.sne_+3A_momentum">momentum</code></td>
<td>
<p>level of acceleration in learning.</p>
</td></tr>
<tr><td><code id="do.sne_+3A_pca">pca</code></td>
<td>
<p>whether to use PCA as preliminary step; <code>TRUE</code> for using it, <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="do.sne_+3A_pcascale">pcascale</code></td>
<td>
<p>a logical; <code>FALSE</code> for using Covariance, <code>TRUE</code> for using Correlation matrix. See also <code><a href="#topic+do.pca">do.pca</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.sne_+3A_symmetric">symmetric</code></td>
<td>
<p>a logical; <code>FALSE</code> to solve it naively, and <code>TRUE</code> to adopt symmetrization scheme.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>vars</dt><dd><p>a vector containing betas used in perplexity matching.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Hinton GE, Roweis ST (2003).
&ldquo;Stochastic Neighbor Embedding.&rdquo;
In Becker S, Thrun S, Obermayer K (eds.), <em>Advances in Neural Information Processing Systems 15</em>, 857&ndash;864.
MIT Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## try different perplexity values
out1 &lt;- do.sne(X, perplexity=5)
out2 &lt;- do.sne(X, perplexity=25)
out3 &lt;- do.sne(X, perplexity=50)

## Visualize two comparisons
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="perplexity=5")
plot(out2$Y, pch=19, col=label, main="perplexity=25")
plot(out3$Y, pch=19, col=label, main="perplexity=50")
par(opar)


</code></pre>

<hr>
<h2 id='do.spc'>Supervised Principal Component Analysis</h2><span id='topic+do.spc'></span>

<h3>Description</h3>

<p>Unlike original principal component analysis (<code><a href="#topic+do.pca">do.pca</a></code>), this algorithm implements
a supervised version using response information for feature selection. For each feature/column,
its normalized association with <code>response</code> variable is computed and the features with
large magnitude beyond <code>threshold</code> are selected. From the selected submatrix,
regular PCA is applied for dimension reduction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.spc(
  X,
  response,
  ndim = 2,
  preprocess = c("center", "whiten", "decorrelate"),
  threshold = 0.1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.spc_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.spc_+3A_response">response</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of response variable.</p>
</td></tr>
<tr><td><code id="do.spc_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.spc_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is <code>center</code>. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.spc_+3A_threshold">threshold</code></td>
<td>
<p>a threshold value to cut off normalized association between covariates and response.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Bair E, Hastie T, Paul D, Tibshirani R (2006).
&ldquo;Prediction by Supervised Principal Components.&rdquo;
<em>Journal of the American Statistical Association</em>, <b>101</b>(473), 119&ndash;137.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate swiss roll with auxiliary dimensions
## it follows reference example from LSIR paper.
set.seed(100)
n = 100
theta = runif(n)
h     = runif(n)
t     = (1+2*theta)*(3*pi/2)
X     = array(0,c(n,10))
X[,1] = t*cos(t)
X[,2] = 21*h
X[,3] = t*sin(t)
X[,4:10] = matrix(runif(7*n), nrow=n)

## corresponding response vector
y = sin(5*pi*theta)+(runif(n)*sqrt(0.1))

## try different threshold values
out1 = do.spc(X, y, threshold=2)
out2 = do.spc(X, y, threshold=5)
out3 = do.spc(X, y, threshold=10)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="SPC::threshold=2")
plot(out2$Y, main="SPC::threshold=5")
plot(out3$Y, main="SPC::threshold=10")
par(opar)

</code></pre>

<hr>
<h2 id='do.spca'>Sparse Principal Component Analysis</h2><span id='topic+do.spca'></span>

<h3>Description</h3>

<p>Sparse PCA (<code>do.spca</code>) is a variant of PCA in that each loading - or, principal
component - should be sparse. Instead of using generic optimization package,
we opt for formulating a problem as semidefinite relaxation and utilizing ADMM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.spca(X, ndim = 2, mu = 1, rho = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.spca_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.spca_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.spca_+3A_mu">mu</code></td>
<td>
<p>an augmented Lagrangian parameter.</p>
</td></tr>
<tr><td><code id="do.spca_+3A_rho">rho</code></td>
<td>
<p>a regularization parameter for sparsity.</p>
</td></tr>
<tr><td><code id="do.spca_+3A_...">...</code></td>
<td>
<p>extra parameters including </p>

<dl>
<dt>maxiter</dt><dd><p>maximum number of iterations (default: 100).</p>
</dd>
<dt>abstol</dt><dd><p>absolute tolerance stopping criterion (default: 1e-8).</p>
</dd>
<dt>reltol</dt><dd><p>relative tolerance stopping criterion (default: 1e-4).</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zou H, Hastie T, Tibshirani R (2006).
&ldquo;Sparse Principal Component Analysis.&rdquo;
<em>Journal of Computational and Graphical Statistics</em>, <b>15</b>(2), 265&ndash;286.
</p>
<p>d'Aspremont A, El Ghaoui L, Jordan MI, Lanckriet GRG (2007).
&ldquo;A Direct Formulation for Sparse PCA Using Semidefinite Programming.&rdquo;
<em>SIAM Review</em>, <b>49</b>(3), 434&ndash;448.
</p>
<p>Ma S (2013).
&ldquo;Alternating Direction Method of Multipliers for Sparse Principal Component Analysis.&rdquo;
<em>Journal of the Operations Research Society of China</em>, <b>1</b>(2), 253&ndash;274.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.pca">do.pca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
data(iris, package="Rdimtools")
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
lab   = as.factor(iris[subid,5])

## try different regularization parameters for sparsity
out1 &lt;- do.spca(X,ndim=2,rho=0.01)
out2 &lt;- do.spca(X,ndim=2,rho=1)
out3 &lt;- do.spca(X,ndim=2,rho=100)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=lab, pch=19, main="SPCA::rho=0.01")
plot(out2$Y, col=lab, pch=19, main="SPCA::rho=1")
plot(out3$Y, col=lab, pch=19, main="SPCA::rho=100")
par(opar)


</code></pre>

<hr>
<h2 id='do.spe'>Stochastic Proximity Embedding</h2><span id='topic+do.spe'></span>

<h3>Description</h3>

<p>One of drawbacks for Multidimensional Scaling or Sammon mapping is that
they have quadratic computational complexity with respect to the number of data.
Stochastic Proximity Embedding (SPE) adopts stochastic update rule in that
its computational speed is much improved. It performs <code>C</code> number of cycles,
where for each cycle, it randomly selects two data points and updates their
locations correspondingly <code>S</code> times. After each cycle, learning parameter <code class="reqn">\lambda</code>
is multiplied by <code>drate</code>, becoming smaller in magnitude.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.spe(
  X,
  ndim = 2,
  proximity = function(x) {
     dist(x, method = "euclidean")
 },
  C = 50,
  S = 50,
  lambda = 1,
  drate = 0.9
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.spe_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.spe_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.spe_+3A_proximity">proximity</code></td>
<td>
<p>a function for constructing proximity matrix from original data dimension.</p>
</td></tr>
<tr><td><code id="do.spe_+3A_c">C</code></td>
<td>
<p>the number of cycles to be run; after each cycle, learning parameter</p>
</td></tr>
<tr><td><code id="do.spe_+3A_s">S</code></td>
<td>
<p>the number of updates for each cycle.</p>
</td></tr>
<tr><td><code id="do.spe_+3A_lambda">lambda</code></td>
<td>
<p>initial learning parameter.</p>
</td></tr>
<tr><td><code id="do.spe_+3A_drate">drate</code></td>
<td>
<p>multiplier for <code>lambda</code> at each cycle; should be a positive real number in <code class="reqn">(0,1).</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Agrafiotis DK (2003).
&ldquo;Stochastic Proximity Embedding.&rdquo;
<em>Journal of Computational Chemistry</em>, <b>24</b>(10), 1215&ndash;1221.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
X     = as.matrix(iris[,1:4])
label = as.factor(iris$Species)

## compare with mds using 2 distance metrics
outM &lt;- do.mds(X, ndim=2)
out1 &lt;- do.spe(X, ndim=2)
out2 &lt;- do.spe(X, ndim=2, proximity=function(x){dist(x, method="manhattan")})

## Visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(outM$Y, pch=19, col=label, main="MDS")
plot(out1$Y, pch=19, col=label, main="SPE with L2 norm")
plot(out2$Y, pch=19, col=label, main="SPE with L1 norm")
par(opar)


</code></pre>

<hr>
<h2 id='do.specs'>Supervised Spectral Feature Selection</h2><span id='topic+do.specs'></span>

<h3>Description</h3>

<p>SPEC algorithm selects features from the data via spectral graph approach.
Three types of ranking methods that appeared in the paper are available where
the graph laplacian is built via class label information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.specs(
  X,
  label,
  ndim = 2,
  ranking = c("method1", "method2", "method3"),
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.specs_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.specs_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of class labels.</p>
</td></tr>
<tr><td><code id="do.specs_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.specs_+3A_ranking">ranking</code></td>
<td>
<p>types of feature scoring method. See the paper in the reference for more details.</p>
</td></tr>
<tr><td><code id="do.specs_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data. Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>sscore</dt><dd><p>a length-<code class="reqn">p</code> vector of spectral feature scores.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zhao Z, Liu H (2007).
&ldquo;Spectral Feature Selection for Supervised and Unsupervised Learning.&rdquo;
In <em>Proceedings of the 24th International Conference on Machine Learning - ICML '07</em>, 1151&ndash;1157.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.specu">do.specu</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
## it is known that feature 3 and 4 are more important.
data(iris)
set.seed(100)
subid = sample(1:150, 50)
iris.dat = as.matrix(iris[subid,1:4])
iris.lab = as.factor(iris[subid,5])

## try different ranking methods
out1 = do.specs(iris.dat, iris.lab, ranking="method1")
out2 = do.specs(iris.dat, iris.lab, ranking="method2")
out3 = do.specs(iris.dat, iris.lab, ranking="method3")

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=iris.lab, main="SPECS::method1")
plot(out2$Y, pch=19, col=iris.lab, main="SPECS::method2")
plot(out3$Y, pch=19, col=iris.lab, main="SPECS::method3")
par(opar)


</code></pre>

<hr>
<h2 id='do.specu'>Unsupervised Spectral Feature Selection</h2><span id='topic+do.specu'></span>

<h3>Description</h3>

<p>SPEC algorithm selects features from the data via spectral graph approach.
Three types of ranking methods that appeared in the paper are available where
the graph laplacian is built via RBF kernel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.specu(
  X,
  ndim = 2,
  sigma = 1,
  ranking = c("method1", "method2", "method3"),
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.specu_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.specu_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.specu_+3A_sigma">sigma</code></td>
<td>
<p>bandwidth parameter for RBK kernel of type <code class="reqn">S_{i,j} = \exp(-\|x_i - x_j \|^2 / 2\sigma^2 )</code>.</p>
</td></tr>
<tr><td><code id="do.specu_+3A_ranking">ranking</code></td>
<td>
<p>types of feature scoring method. See the paper in the reference for more details.</p>
</td></tr>
<tr><td><code id="do.specu_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data. Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>sscore</dt><dd><p>a length-<code class="reqn">p</code> vector of spectral feature scores.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zhao Z, Liu H (2007).
&ldquo;Spectral Feature Selection for Supervised and Unsupervised Learning.&rdquo;
In <em>Proceedings of the 24th International Conference on Machine Learning - ICML '07</em>, 1151&ndash;1157.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.specs">do.specs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
## it is known that feature 3 and 4 are more important.
data(iris)
set.seed(100)
subid    = sample(1:150,50)
iris.dat = as.matrix(iris[subid,1:4])
iris.lab = as.factor(iris[subid,5])


## try different ranking methods
mysig = 6
out1  = do.specu(iris.dat, sigma=mysig, ranking="method1")
out2  = do.specu(iris.dat, sigma=mysig, ranking="method2")
out3  = do.specu(iris.dat, sigma=mysig, ranking="method3")

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=iris.lab, main="SPECU::method1")
plot(out2$Y, pch=19, col=iris.lab, main="SPECU::method2")
plot(out3$Y, pch=19, col=iris.lab, main="SPECU::method3")
par(opar)


</code></pre>

<hr>
<h2 id='do.splapeig'>Supervised Laplacian Eigenmaps</h2><span id='topic+do.splapeig'></span>

<h3>Description</h3>

<p>Supervised Laplacian Eigenmaps (SPLAPEIG) is a supervised variant of Laplacian Eigenmaps.
Instead of setting up explicit neighborhood, it utilizes an adaptive threshold strategy
to define neighbors for both within- and between-class neighborhood. It then builds affinity
matrices for each information and solves generalized eigenvalue problem. This algorithm
may be quite sensitive in the choice of <code>beta</code> value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.splapeig(
  X,
  label,
  ndim = 2,
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate"),
  beta = 1,
  gamma = 0.5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.splapeig_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.splapeig_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.splapeig_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.splapeig_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.splapeig_+3A_beta">beta</code></td>
<td>
<p>bandwidth parameter for heat kernel in <code class="reqn">[0,\infty)</code>.</p>
</td></tr>
<tr><td><code id="do.splapeig_+3A_gamma">gamma</code></td>
<td>
<p>a balancing parameter in <code class="reqn">[0,1]</code> between within- and between-class information.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Raducanu B, Dornaika F (2012).
&ldquo;A Supervised Non-Linear Dimensionality Reduction Approach for Manifold Learning.&rdquo;
<em>Pattern Recognition</em>, <b>45</b>(6), 2432&ndash;2444.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.lapeig">do.lapeig</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
X     = as.matrix(iris[,1:4])
label = as.factor(iris[,5])

## try different balancing parameters with beta=50
out1 = do.splapeig(X, label, beta=50, gamma=0.3); Y1=out1$Y
out2 = do.splapeig(X, label, beta=50, gamma=0.6); Y2=out2$Y
out3 = do.splapeig(X, label, beta=50, gamma=0.9); Y3=out3$Y

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(Y1, pch=19, col=label, main="gamma=0.3")
plot(Y2, pch=19, col=label, main="gamma=0.6")
plot(Y3, pch=19, col=label, main="gamma=0.9")
par(opar)


</code></pre>

<hr>
<h2 id='do.spmds'>Spectral Multidimensional Scaling</h2><span id='topic+do.spmds'></span>

<h3>Description</h3>

<p><code>do.spmds</code> transfers the classical multidimensional scaling problem into
the data spectral domain using Laplace-Beltrami operator. Its flexibility
to use subsamples and spectral interpolation of non-reference data enables relatively
efficient computation for large-scale data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.spmds(
  X,
  ndim = 2,
  neigs = max(2, nrow(X)/10),
  ratio = 0.1,
  preprocess = c("null", "center", "scale", "cscale", "decorrelate", "whiten"),
  type = c("proportion", 0.1),
  symmetric = c("union", "intersect", "asymmetric")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.spmds_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.spmds_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.spmds_+3A_neigs">neigs</code></td>
<td>
<p>number of eigenvectors to be used as <em>spectral dimension</em>.</p>
</td></tr>
<tr><td><code id="do.spmds_+3A_ratio">ratio</code></td>
<td>
<p>percentage of subsamples as reference points.</p>
</td></tr>
<tr><td><code id="do.spmds_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is <code>"null"</code>. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.spmds_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.spmds_+3A_symmetric">symmetric</code></td>
<td>
<p>one of <code>"intersect"</code>, <code>"union"</code> or <code>"asymmetric"</code> is supported. Default is <code>"union"</code>. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Aflalo Y, Kimmel R (2013).
&ldquo;Spectral Multidimensional Scaling.&rdquo;
<em>Proceedings of the National Academy of Sciences</em>, <b>110</b>(45), 18052&ndash;18057.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Replicate the numerical example from the paper
#  Data Preparation
set.seed(100)
dim.true  = 3     # true dimension
dim.embed = 100   # embedding space (high-d)
npoints   = 1000  # number of samples to be generated

v     = matrix(runif(dim.embed*dim.true),ncol=dim.embed)
coeff = matrix(runif(dim.true*npoints),  ncol=dim.true)
X     = coeff%*%v

# see the effect of neighborhood size
out1  = do.spmds(X, neigs=100, type=c("proportion",0.10))
out2  = do.spmds(X, neigs=100, type=c("proportion",0.25))
out3  = do.spmds(X, neigs=100, type=c("proportion",0.50))

# visualize the results
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, main="10% neighborhood")
plot(out2$Y, main="25% neighborhood")
plot(out3$Y, main="50% neighborhood")
par(opar)

## End(Not run)

</code></pre>

<hr>
<h2 id='do.spp'>Sparsity Preserving Projection</h2><span id='topic+do.spp'></span>

<h3>Description</h3>

<p>Sparsity Preserving Projection (SPP) is an unsupervised linear dimension reduction technique.
It aims to preserve high-dimensional structure in a sparse manner to find projections
that keeps such sparsely-connected pattern in the low-dimensional space. Note that
we used <span class="pkg">CVXR</span> for convenient computation, which may lead to slower execution
once used for large dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.spp(
  X,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten"),
  reltol = 1e-04
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.spp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations</p>
</td></tr>
<tr><td><code id="do.spp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.spp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.spp_+3A_reltol">reltol</code></td>
<td>
<p>tolerance level for stable computation of sparse reconstruction weights.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Qiao L, Chen S, Tan X (2010).
&ldquo;Sparsity Preserving Projections with Applications to Face Recognition.&rdquo;
<em>Pattern Recognition</em>, <b>43</b>(1), 331&ndash;341.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## test different tolerance levels
out1 &lt;- do.spp(X,ndim=2,reltol=0.001)
out2 &lt;- do.spp(X,ndim=2,reltol=0.01)
out3 &lt;- do.spp(X,ndim=2,reltol=0.1)

# visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="SPP::reltol=.001")
plot(out2$Y, pch=19, col=label, main="SPP::reltol=.01")
plot(out3$Y, pch=19, col=label, main="SPP::reltol=.1")
par(opar)

## End(Not run)
</code></pre>

<hr>
<h2 id='do.spufs'>Structure Preserving Unsupervised Feature Selection</h2><span id='topic+do.spufs'></span>

<h3>Description</h3>

<p>This unsupervised feature selection method is based on self-expression model, which means that
the cost function involves difference in self-representation. It does not explicitly
require learning the clusterings and different features are weighted individually
based on their relative importance. The cost function involves two penalties,
sparsity and preservation of local structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.spufs(
  X,
  ndim = 2,
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate"),
  alpha = 1,
  beta = 1,
  bandwidth = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.spufs_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.spufs_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.spufs_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.spufs_+3A_alpha">alpha</code></td>
<td>
<p>nonnegative number to control sparsity in rows of matrix of representation coefficients.</p>
</td></tr>
<tr><td><code id="do.spufs_+3A_beta">beta</code></td>
<td>
<p>nonnegative number to control the degree of local-structure preservation.</p>
</td></tr>
<tr><td><code id="do.spufs_+3A_bandwidth">bandwidth</code></td>
<td>
<p>positive number for Gaussian kernel bandwidth to define similarity.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Lu Q, Li X, Dong Y (2018).
&ldquo;Structure Preserving Unsupervised Feature Selection.&rdquo;
<em>Neurocomputing</em>, <b>301</b>, 36&ndash;45.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

#### try different bandwidth values
out1 = do.spufs(X, bandwidth=0.1)
out2 = do.spufs(X, bandwidth=1)
out3 = do.spufs(X, bandwidth=10)

#### visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="SPUFS::bandwidth=0.1")
plot(out2$Y, pch=19, col=label, main="SPUFS::bandwidth=1")
plot(out3$Y, pch=19, col=label, main="SPUFS::bandwidth=10")
par(opar)

</code></pre>

<hr>
<h2 id='do.ssldp'>Semi-Supervised Locally Discriminant Projection</h2><span id='topic+do.ssldp'></span>

<h3>Description</h3>

<p>Semi-Supervised Locally Discriminant Projection (SSLDP) is a semi-supervised
extension of LDP. It utilizes unlabeled data to overcome the small-sample-size problem
under the situation where labeled data have the small number. Using two information,
it both constructs the within- and between-class weight matrices incorporating the
neighborhood information of the data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.ssldp(
  X,
  label,
  ndim = 2,
  type = c("proportion", 0.1),
  preprocess = c("center", "scale", "cscale", "whiten", "decorrelate"),
  beta = 0.5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.ssldp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.ssldp_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.ssldp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.ssldp_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.ssldp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.ssldp_+3A_beta">beta</code></td>
<td>
<p>balancing parameter for intra- and inter-class information in <code class="reqn">[0,1]</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Zhang S, Lei Y, Wu Y (2011).
&ldquo;Semi-Supervised Locally Discriminant Projection for Classification and Recognition.&rdquo;
<em>Knowledge-Based Systems</em>, <b>24</b>(2), 341&ndash;346.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris data
data(iris)
X     = as.matrix(iris[,1:4])
label = as.integer(iris$Species)

## copy a label and let 10% of elements be missing
nlabel = length(label)
nmissing = round(nlabel*0.10)
label_missing = label
label_missing[sample(1:nlabel, nmissing)]=NA

## compute with 3 different levels of 'beta' values
out1 = do.ssldp(X, label_missing, beta=0.1)
out2 = do.ssldp(X, label_missing, beta=0.5)
out3 = do.ssldp(X, label_missing, beta=0.9)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=label, main="SSLDP::beta=0.1")
plot(out2$Y, col=label, main="SSLDP::beta=0.5")
plot(out3$Y, col=label, main="SSLDP::beta=0.9")
par(opar)

</code></pre>

<hr>
<h2 id='do.tsne'>t-distributed Stochastic Neighbor Embedding</h2><span id='topic+do.tsne'></span>

<h3>Description</h3>

<p><code class="reqn">t</code>-distributed Stochastic Neighbor Embedding (t-SNE) is a variant of Stochastic Neighbor Embedding (SNE)
that mimicks patterns of probability distributinos over pairs of high-dimensional objects on low-dimesional
target embedding space by minimizing Kullback-Leibler divergence. While conventional SNE uses gaussian
distributions to measure similarity, t-SNE, as its name suggests, exploits a heavy-tailed Student t-distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.tsne(
  X,
  ndim = 2,
  perplexity = 30,
  eta = 0.05,
  maxiter = 2000,
  jitter = 0.3,
  jitterdecay = 0.99,
  momentum = 0.5,
  pca = TRUE,
  pcascale = FALSE,
  symmetric = FALSE,
  BHuse = TRUE,
  BHtheta = 0.25
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.tsne_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.tsne_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.tsne_+3A_perplexity">perplexity</code></td>
<td>
<p>desired level of perplexity; ranging [5,50].</p>
</td></tr>
<tr><td><code id="do.tsne_+3A_eta">eta</code></td>
<td>
<p>learning parameter.</p>
</td></tr>
<tr><td><code id="do.tsne_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code id="do.tsne_+3A_jitter">jitter</code></td>
<td>
<p>level of white noise added at the beginning.</p>
</td></tr>
<tr><td><code id="do.tsne_+3A_jitterdecay">jitterdecay</code></td>
<td>
<p>decay parameter in (0,1). The closer to 0, the faster artificial noise decays.</p>
</td></tr>
<tr><td><code id="do.tsne_+3A_momentum">momentum</code></td>
<td>
<p>level of acceleration in learning.</p>
</td></tr>
<tr><td><code id="do.tsne_+3A_pca">pca</code></td>
<td>
<p>whether to use PCA as preliminary step; <code>TRUE</code> for using it, <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="do.tsne_+3A_pcascale">pcascale</code></td>
<td>
<p>a logical; <code>FALSE</code> for using Covariance, <code>TRUE</code> for using Correlation matrix. See also <code><a href="#topic+do.pca">do.pca</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.tsne_+3A_symmetric">symmetric</code></td>
<td>
<p>a logical; <code>FALSE</code> to solve it naively, and <code>TRUE</code> to adopt symmetrization scheme.</p>
</td></tr>
<tr><td><code id="do.tsne_+3A_bhuse">BHuse</code></td>
<td>
<p>a logical; <code>TRUE</code> to use Barnes-Hut approximation. See <code><a href="Rtsne.html#topic+Rtsne">Rtsne</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.tsne_+3A_bhtheta">BHtheta</code></td>
<td>
<p>speed-accuracy tradeoff. If set as 0.0, it reduces to exact t-SNE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named <code>Rdimtools</code> S3 object containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>algorithm</dt><dd><p>name of the algorithm.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>van der Maaten L, Hinton G (2008).
&ldquo;Visualizing Data Using T-SNE.&rdquo;
<em>The Journal of Machine Learning Research</em>, <b>9</b>(2579-2605), 85.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.sne">do.sne</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
lab   = as.factor(iris[subid,5])

## compare different perplexity
out1 &lt;- do.tsne(X, ndim=2, perplexity=5)
out2 &lt;- do.tsne(X, ndim=2, perplexity=10)
out3 &lt;- do.tsne(X, ndim=2, perplexity=15)

## Visualize three different projections
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=lab, main="tSNE::perplexity=5")
plot(out2$Y, pch=19, col=lab, main="tSNE::perplexity=10")
plot(out3$Y, pch=19, col=lab, main="tSNE::perplexity=15")
par(opar)


</code></pre>

<hr>
<h2 id='do.udfs'>Unsupervised Discriminative Features Selection</h2><span id='topic+do.udfs'></span>

<h3>Description</h3>

<p>Though it may sound weird, this method aims at finding discriminative features
under the unsupervised learning framework. It assumes that the class label
could be predicted by a linear classifier and iteratively updates its
discriminative nature while attaining row-sparsity scores for selecting features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.udfs(
  X,
  ndim = 2,
  lbd = 1,
  gamma = 1,
  k = 5,
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.udfs_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.udfs_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.udfs_+3A_lbd">lbd</code></td>
<td>
<p>regularization parameter for local Gram matrix to be invertible.</p>
</td></tr>
<tr><td><code id="do.udfs_+3A_gamma">gamma</code></td>
<td>
<p>regularization parameter for row-sparsity via <code class="reqn">\ell_{2,1}</code> norm.</p>
</td></tr>
<tr><td><code id="do.udfs_+3A_k">k</code></td>
<td>
<p>size of nearest neighborhood for each data point.</p>
</td></tr>
<tr><td><code id="do.udfs_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Yang Y, Shen HT, Ma Z, Huang Z, Zhou X (2011).
&ldquo;L2,1-Norm Regularized Discriminative Feature Selection for Unsupervised Learning.&rdquo;
In <em>Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume Volume Two</em>,  IJCAI'11, 1589&ndash;1594.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## use iris data
data(iris)
set.seed(100)
subid = sample(1:150, 50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

#### try different neighborhood size
out1 = do.udfs(X, k=5)
out2 = do.udfs(X, k=10)
out3 = do.udfs(X, k=25)

#### visualize
opar = par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=label, main="UDFS::k=5")
plot(out2$Y, pch=19, col=label, main="UDFS::k=10")
plot(out3$Y, pch=19, col=label, main="UDFS::k=25")
par(opar)

</code></pre>

<hr>
<h2 id='do.udp'>Unsupervised Discriminant Projection</h2><span id='topic+do.udp'></span>

<h3>Description</h3>

<p>Unsupervised Discriminant Projection (UDP) aims finding projection that balances local and global scatter.
Even though the name contains the word <em>Discriminant</em>, this algorithm is <em>unsupervised</em>. The
term there reflects its algorithmic tactic to discriminate distance points not in the neighborhood of each data point.
It performs PCA as intermittent preprocessing for rank singularity issue. Authors clearly mentioned that it is inspired
by Locality Preserving Projection, which minimizes the local scatter only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.udp(
  X,
  ndim = 2,
  type = c("proportion", 0.1),
  preprocess = c("center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.udp_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.udp_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.udp_+3A_type">type</code></td>
<td>
<p>a vector of neighborhood graph construction. Following types are supported;
<code>c("knn",k)</code>, <code>c("enn",radius)</code>, and <code>c("proportion",ratio)</code>.
Default is <code>c("proportion",0.1)</code>, connecting about 1/10 of nearest data points
among all data points. See also <code><a href="#topic+aux.graphnbd">aux.graphnbd</a></code> for more details.</p>
</td></tr>
<tr><td><code id="do.udp_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
<dt>interimdim</dt><dd><p>the number of PCA target dimension used in preprocessing.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Yang J, Zhang D, Yang J, Niu B (2007).
&ldquo;Globally Maximizing, Locally Minimizing: Unsupervised Discriminant Projection with Applications to Face and Palm Biometrics.&rdquo;
<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <b>29</b>(4), 650&ndash;664.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.lpp">do.lpp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## use different connectivity level
out1 &lt;- do.udp(X, type=c("proportion",0.05))
out2 &lt;- do.udp(X, type=c("proportion",0.10))
out3 &lt;- do.udp(X, type=c("proportion",0.25))

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, col=label, pch=19, main="connectivity 5%")
plot(out2$Y, col=label, pch=19, main="connectivity 10%")
plot(out3$Y, col=label, pch=19, main="connectivity 25%")
par(opar)

</code></pre>

<hr>
<h2 id='do.ugfs'>Unsupervised Graph-based Feature Selection</h2><span id='topic+do.ugfs'></span>

<h3>Description</h3>

<p>UGFS is an unsupervised feature selection method with two parameters <code>nbdk</code> and <code>varthr</code> that it constructs
an affinity graph using local variance computation and scores variables based on PageRank algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.ugfs(
  X,
  ndim = 2,
  nbdk = 5,
  varthr = 2,
  preprocess = c("null", "center", "scale", "cscale", "whiten", "decorrelate")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.ugfs_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.ugfs_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.ugfs_+3A_nbdk">nbdk</code></td>
<td>
<p>the size of neighborhood for local variance computation.</p>
</td></tr>
<tr><td><code id="do.ugfs_+3A_varthr">varthr</code></td>
<td>
<p>threshold value for affinity graph construction. If too small so that the graph of variables is not constructed, it returns an error.</p>
</td></tr>
<tr><td><code id="do.ugfs_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data. Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>prscore</dt><dd><p>a length-<code class="reqn">p</code> vector of score computed from PageRank algorithm. Indices with largest values are selected.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Henni K, Mezghani N, Gouin-Vallerand C (2018).
&ldquo;Unsupervised Graph-Based Feature Selection via Subspace and Pagerank Centrality.&rdquo;
<em>Expert Systems with Applications</em>, <b>114</b>, 46&ndash;53.
ISSN 09574174.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
## it is known that feature 3 and 4 are more important.
data(iris)
iris.dat &lt;- as.matrix(iris[,1:4])
iris.lab &lt;- as.factor(iris[,5])

## try multiple thresholding values
out1 = do.ugfs(iris.dat, nbdk=10, varthr=0.5)
out2 = do.ugfs(iris.dat, nbdk=10, varthr=5.0)
out3 = do.ugfs(iris.dat, nbdk=10, varthr=9.5)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=iris.lab, main="bandwidth=0.1")
plot(out2$Y, pch=19, col=iris.lab, main="bandwidth=1")
plot(out3$Y, pch=19, col=iris.lab, main="bandwidth=10")
par(opar)


</code></pre>

<hr>
<h2 id='do.ulda'>Uncorrelated Linear Discriminant Analysis</h2><span id='topic+do.ulda'></span>

<h3>Description</h3>

<p>Uncorrelated LDA (Jin et al. 2001) is an extension of LDA by using the uncorrelated discriminant transformation
and Kahrunen-Loeve expansion of the basis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.ulda(
  X,
  label,
  ndim = 2,
  preprocess = c("center", "scale", "cscale", "whiten", "decorrelate")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.ulda_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.ulda_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.ulda_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.ulda_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;center&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Jin Z, Yang J, Hu Z, Lou Z (2001).
&ldquo;Face Recognition Based on the Uncorrelated Discriminant Transformation.&rdquo;
<em>Pattern Recognition</em>, <b>34</b>(7), 1405&ndash;1416.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.lda">do.lda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load iris data
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
label = as.factor(iris[subid,5])

## compare with LDA
out1 = do.lda(X, label)
out2 = do.ulda(X, label)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(out1$Y, pch=19, col=label, main="LDA")
plot(out2$Y, pch=19, col=label, main="Uncorrelated LDA")
par(opar)

</code></pre>

<hr>
<h2 id='do.uwdfs'>Uncorrelated Worst-Case Discriminative Feature Selection</h2><span id='topic+do.uwdfs'></span>

<h3>Description</h3>

<p>Built upon <code>do.wdfs</code>, this method selects features step-by-step to opt out the redundant sets
by iteratively update feature scores via scaling by the correlation between target and previously chosen variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.uwdfs(
  X,
  label,
  ndim = 2,
  preprocess = c("null", "center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.uwdfs_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.uwdfs_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.uwdfs_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.uwdfs_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Liao S, Gao Q, Nie F, Liu Y, Zhang X (2019).
&ldquo;Worst-Case Discriminative Feature Selection.&rdquo;
In <em>Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</em>, 2973&ndash;2979.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.wdfs">do.wdfs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
## it is known that feature 3 and 4 are more important.
data(iris)
set.seed(100)
subid    = sample(1:150,50)
iris.dat = as.matrix(iris[subid,1:4])
iris.lab = as.factor(iris[subid,5])

## compare with other algorithms
out1 = do.lda(iris.dat, iris.lab)
out2 = do.wdfs(iris.dat, iris.lab)
out3 = do.uwdfs(iris.dat, iris.lab)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=iris.lab, main="LDA")
plot(out2$Y, pch=19, col=iris.lab, main="WDFS")
plot(out3$Y, pch=19, col=iris.lab, main="UWDFS")
par(opar)


</code></pre>

<hr>
<h2 id='do.wdfs'>Worst-Case Discriminative Feature Selection</h2><span id='topic+do.wdfs'></span>

<h3>Description</h3>

<p>As a supervised feature selection method, WDFS searches over all pairs of
between-class and within-class scatters and chooses the highest-scoring features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>do.wdfs(
  X,
  label,
  ndim = 2,
  preprocess = c("null", "center", "scale", "cscale", "decorrelate", "whiten")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="do.wdfs_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations
and columns represent independent variables.</p>
</td></tr>
<tr><td><code id="do.wdfs_+3A_label">label</code></td>
<td>
<p>a length-<code class="reqn">n</code> vector of data class labels.</p>
</td></tr>
<tr><td><code id="do.wdfs_+3A_ndim">ndim</code></td>
<td>
<p>an integer-valued target dimension.</p>
</td></tr>
<tr><td><code id="do.wdfs_+3A_preprocess">preprocess</code></td>
<td>
<p>an additional option for preprocessing the data.
Default is &quot;null&quot;. See also <code><a href="#topic+aux.preprocess">aux.preprocess</a></code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing
</p>

<dl>
<dt>Y</dt><dd><p>an <code class="reqn">(n\times ndim)</code> matrix whose rows are embedded observations.</p>
</dd>
<dt>featidx</dt><dd><p>a length-<code class="reqn">ndim</code> vector of indices with highest scores.</p>
</dd>
<dt>trfinfo</dt><dd><p>a list containing information for out-of-sample prediction.</p>
</dd>
<dt>projection</dt><dd><p>a <code class="reqn">(p\times ndim)</code> whose columns are basis for projection.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Liao S, Gao Q, Nie F, Liu Y, Zhang X (2019).
&ldquo;Worst-Case Discriminative Feature Selection.&rdquo;
In <em>Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</em>, 2973&ndash;2979.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use iris data
## it is known that feature 3 and 4 are more important.
data(iris)
set.seed(100)
subid    = sample(1:150,50)
iris.dat = as.matrix(iris[subid,1:4])
iris.lab = as.factor(iris[subid,5])

## compare with other algorithms
out1 = do.lda(iris.dat, iris.lab)
out2 = do.fscore(iris.dat, iris.lab)
out3 = do.wdfs(iris.dat, iris.lab)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(out1$Y, pch=19, col=iris.lab, main="LDA")
plot(out2$Y, pch=19, col=iris.lab, main="FSCORE")
plot(out3$Y, pch=19, col=iris.lab, main="WDFS")
par(opar)


</code></pre>

<hr>
<h2 id='est.boxcount'>Box-counting Dimension</h2><span id='topic+est.boxcount'></span>

<h3>Description</h3>

<p>Box-counting dimension, also known as Minkowski-Bouligand dimension, is a popular way of figuring out
the fractal dimension of a set in a Euclidean space. Its idea is to measure the number of boxes
required to cover the set repeatedly by decreasing the length of each side of a box. It is defined as
</p>
<p style="text-align: center;"><code class="reqn">dim(S) = \lim \frac{\log N(r)}{\log (1/r)}</code>
</p>
<p> as <code class="reqn">r\rightarrow 0</code>, where <code class="reqn">N(r)</code> is
the number of boxes counted to cover a given set for each corresponding <code class="reqn">r</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>est.boxcount(X, nlevel = 50, cut = c(0.1, 0.9))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="est.boxcount_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="est.boxcount_+3A_nlevel">nlevel</code></td>
<td>
<p>the number of <code>r</code> (radius) to be tested.</p>
</td></tr>
<tr><td><code id="est.boxcount_+3A_cut">cut</code></td>
<td>
<p>a vector of ratios for computing estimated dimension in <code class="reqn">(0,1)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing containing </p>

<dl>
<dt>estdim</dt><dd><p>estimated dimension using <code>cut</code> ratios.</p>
</dd>
<dt>r</dt><dd><p>a vector of radius used.</p>
</dd>
<dt>Nr</dt><dd><p>a vector of boxes counted for each corresponding <code>r</code>.</p>
</dd>
</dl>



<h3>Determining the dimension</h3>

<p>Even though we could use arbitrary <code>cut</code> to compute estimated dimension, it is also possible to
use visual inspection. According to the theory, if the function returns an <code>output</code>, we can plot
<code>plot(log(1/output$r),log(output$Nr))</code> and use the linear slope in the middle as desired dimension of data.
</p>


<h3>Automatic choice of <code class="reqn">r</code></h3>

<p>The least value for radius <code class="reqn">r</code> must have non-degenerate counts, while the maximal value should be the
maximum distance among all pairs of data points across all coordinates. <code>nlevel</code> controls the number of interim points
in a log-equidistant manner.
</p>


<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Hentschel HGE, Procaccia I (1983).
&ldquo;The Infinite Number of Generalized Dimensions of Fractals and Strange Attractors.&rdquo;
<em>Physica D: Nonlinear Phenomena</em>, <b>8</b>(3), 435&ndash;444.
</p>
<p>Ott E (2002).
<em>Chaos in Dynamical Systems</em>, 2nd ed edition.
Cambridge University Press, Cambridge, U.K. ; New York.
ISBN 978-0-521-81196-5 978-0-521-01084-9.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+est.correlation">est.correlation</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate three different dataset
X1 = aux.gensamples(dname="swiss")
X2 = aux.gensamples(dname="ribbon")
X3 = aux.gensamples(dname="twinpeaks")

## compute boxcount dimension
out1 = est.boxcount(X1)
out2 = est.boxcount(X2)
out3 = est.boxcount(X3)

## visually verify : all should have approximate slope of 2.
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(log(1/out1$r), log(out1$Nr), main="swiss roll")
plot(log(1/out2$r), log(out2$Nr), main="ribbon")
plot(log(1/out3$r), log(out3$Nr), main="twinpeaks")
par(opar)


</code></pre>

<hr>
<h2 id='est.clustering'>Intrinsic Dimension Estimation via Clustering</h2><span id='topic+est.clustering'></span>

<h3>Description</h3>

<p>Instead of directly using neighborhood information, <code>est.clustering</code> adopts hierarchical
neighborhood information using <code><a href="stats.html#topic+hclust">hclust</a></code> by recursively merging leafs
over the range of radii.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>est.clustering(X, kmin = round(sqrt(nrow(X))))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="est.clustering_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="est.clustering_+3A_kmin">kmin</code></td>
<td>
<p>minimal number of neighborhood size to search over.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing containing </p>

<dl>
<dt>estdim</dt><dd><p>estimated intrinsic dimension.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Eriksson B, Crovella M (2012).
&ldquo;Estimating Intrinsic Dimension via Clustering.&rdquo;
In <em>2012 IEEE Statistical Signal Processing Workshop (SSP)</em>, 760&ndash;763.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## create 'swiss' roll dataset
X = aux.gensamples(dname="swiss")

## try different k values
out1 = est.clustering(X, kmin=5)
out2 = est.clustering(X, kmin=25)
out3 = est.clustering(X, kmin=50)

## print the results
line1 = paste0("* est.clustering : kmin=5  gives ",round(out1$estdim,2))
line2 = paste0("* est.clustering : kmin=25 gives ",round(out2$estdim,2))
line3 = paste0("* est.clustering : kmin=50 gives ",round(out3$estdim,2))
cat(paste0(line1,"\n",line2,"\n",line3))


</code></pre>

<hr>
<h2 id='est.correlation'>Correlation Dimension</h2><span id='topic+est.correlation'></span>

<h3>Description</h3>

<p>Correlation dimension is a measure of determining the dimension of a given set. It is
often referred to as a type of fractal dimension.  Its mechanism is somewhat similar to
that of box-counting dimension, but has the advantage of being intuitive as well as
efficient in terms of computation with some robustness contingent on the lack of availability for large dataset.
</p>
<p style="text-align: center;"><code class="reqn">dim(S) = \lim \frac{\log C(r)}{\log r}</code>
</p>
<p> as <code class="reqn">r\rightarrow 0</code>, where
<code class="reqn">C(r)=\lim (2/(N-1)*N)\sum_i^N \sum_{j=i+1}^N I(\|x_i-x_j\|\le r)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>est.correlation(X, nlevel = 50, method = c("lm", "cut"), cut = c(0.1, 0.9))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="est.correlation_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="est.correlation_+3A_nlevel">nlevel</code></td>
<td>
<p>the number of <code>r</code> (radius) to be tested.</p>
</td></tr>
<tr><td><code id="est.correlation_+3A_method">method</code></td>
<td>
<p>method to estimate the intrinsic dimension; <code>"lm"</code> for fitting a linear model for
the entire grid of values, and <code>"cut"</code> to trim extreme points. <code>"cut"</code> method is more robust.</p>
</td></tr>
<tr><td><code id="est.correlation_+3A_cut">cut</code></td>
<td>
<p>a vector of ratios for computing estimated dimension in <code class="reqn">(0,1)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing containing </p>

<dl>
<dt>estdim</dt><dd><p>estimated dimension using <code>cut</code> values.</p>
</dd>
<dt>r</dt><dd><p>a vector of radius used.</p>
</dd>
<dt>Cr</dt><dd><p>a vector of <code class="reqn">C(r)</code> as decribed above.</p>
</dd>
</dl>



<h3>Determining the dimension</h3>

<p>Even though we could use arbitrary <code>cut</code> to compute estimated dimension, it is also possible to
use visual inspection. According to the theory, if the function returns an <code>output</code>, we can plot
<code>plot(log(output$r), log(output$Cr))</code> and use the linear slope in the middle as desired dimension of data.
</p>


<h3>Automatic choice of <code class="reqn">r</code></h3>

<p>The least value for radius <code class="reqn">r</code> must have non-degenerate counts, while the maximal value should be the
maximum distance among all pairs of data points across all coordinates. <code>nlevel</code> controls the number of interim points
in a log-equidistant manner.
</p>


<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Grassberger P, Procaccia I (1983).
&ldquo;Measuring the Strangeness of Strange Attractors.&rdquo;
<em>Physica D: Nonlinear Phenomena</em>, <b>9</b>(1-2), 189&ndash;208.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+est.boxcount">est.boxcount</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate three different dataset
set.seed(1)
X1 = aux.gensamples(dname="swiss")
X2 = aux.gensamples(dname="ribbon")
X3 = aux.gensamples(dname="twinpeaks")

## compute
out1 = est.correlation(X1)
out2 = est.correlation(X2)
out3 = est.correlation(X3)

## visually verify : all should have approximate slope of 2.
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
plot(log(out1$r), log(out1$Cr), main="swiss roll")
plot(log(out2$r), log(out2$Cr), main="ribbon")
plot(log(out3$r), log(out3$Cr), main="twinpeaks")
par(opar)



</code></pre>

<hr>
<h2 id='est.danco'>Intrinsic Dimensionality Estimation with DANCo</h2><span id='topic+est.danco'></span>

<h3>Description</h3>

<p>DANCo exploits the balanced information of both the normalized nearest neighbor distances
as well as the angles of data pairs in the neighboring points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>est.danco(X, k = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="est.danco_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="est.danco_+3A_k">k</code></td>
<td>
<p>the neighborhood size used for estimating local intrinsic dimension.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing containing </p>

<dl>
<dt>estdim</dt><dd><p>estimated dimension via the method.</p>
</dd>
</dl>



<h3>References</h3>

<p>Ceruti C, Bassis S, Rozza A, Lombardi G, Casiraghi E, Campadelli P (2014).
&ldquo;DANCo: An Intrinsic Dimensionality Estimator Exploiting Angle and Norm Concentration.&rdquo;
<em>Pattern Recognition</em>, <b>47</b>(8), 2569&ndash;2581.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## create 3 datasets of intrinsic dimension 2.
X1 = aux.gensamples(n=50, dname="swiss")
X2 = aux.gensamples(n=50, dname="ribbon")
X3 = aux.gensamples(n=50, dname="saddle")

## acquire an estimate for intrinsic dimension
out1 = est.danco(X1, k=10)
out2 = est.danco(X2, k=10)
out3 = est.danco(X3, k=10)

## print the results
line1 = paste0("* est.danco : 'swiss'  estiamte is ",round(out1$estdim,2))
line2 = paste0("* est.danco : 'ribbon' estiamte is ",round(out2$estdim,2))
line3 = paste0("* est.danco : 'saddle' estiamte is ",round(out3$estdim,2))
cat(paste0(line1,"\n",line2,"\n",line3))


</code></pre>

<hr>
<h2 id='est.gdistnn'>Intrinsic Dimension Estimation based on Manifold Assumption and Graph Distance</h2><span id='topic+est.gdistnn'></span>

<h3>Description</h3>

<p>As the name suggests, this function assumes that the data is sampled from the manifold in that
graph representing the underlying manifold is first estimated via <code class="reqn">k</code>-nn. Then graph distance
is employed as an approximation of geodesic distance to locally estimate intrinsic dimension.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>est.gdistnn(X, k = 5, k1 = 3, k2 = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="est.gdistnn_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="est.gdistnn_+3A_k">k</code></td>
<td>
<p>the neighborhood size used for constructing a graph. We suggest it to be large enough to build a connected graph.</p>
</td></tr>
<tr><td><code id="est.gdistnn_+3A_k1">k1</code></td>
<td>
<p>local neighborhood parameter (smaller radius) for graph distance.</p>
</td></tr>
<tr><td><code id="est.gdistnn_+3A_k2">k2</code></td>
<td>
<p>local neighborhood parameter (larger radius)  for graph distance.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing containing </p>

<dl>
<dt>estdim</dt><dd><p>the global estimated dimension, which is averaged local dimension.</p>
</dd>
<dt>estloc</dt><dd><p>a length-<code class="reqn">n</code> vector of locally estimated dimension at each point.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>He J, Ding L, Jiang L, Li Z, Hu Q (2014).
&ldquo;Intrinsic Dimensionality Estimation Based on Manifold Assumption.&rdquo;
<em>Journal of Visual Communication and Image Representation</em>, <b>25</b>(5), 740&ndash;747.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## create 3 datasets of intrinsic dimension 2.
X1 = aux.gensamples(dname="swiss")
X2 = aux.gensamples(dname="ribbon")
X3 = aux.gensamples(dname="saddle")

## acquire an estimate for intrinsic dimension
out1 = est.gdistnn(X1, k=10)
out2 = est.gdistnn(X2, k=10)
out3 = est.gdistnn(X3, k=10)

## print the results
sprintf("* est.gdistnn : estimated dimension for 'swiss'  data is %.2f.",out1$estdim)
sprintf("* est.gdistnn : estimated dimension for 'ribbon' data is %.2f.",out2$estdim)
sprintf("* est.gdistnn : estimated dimension for 'saddle' data is %.2f.",out3$estdim)

line1 = paste0("* est.gdistnn : 'swiss'  estiamte is ",round(out1$estdim,2))
line2 = paste0("* est.gdistnn : 'ribbon' estiamte is ",round(out2$estdim,2))
line3 = paste0("* est.gdistnn : 'saddle' estiamte is ",round(out3$estdim,2))
cat(paste0(line1,"\n",line2,"\n",line3))

## compare with local-dimension estimate
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,3))
hist(out1$estloc, main="Result-'Swiss'", xlab="local dimension")
abline(v=out1$estdim, lwd=3, col="red")
hist(out2$estloc, main="Result-'Ribbon'", xlab="local dimension")
abline(v=out2$estdim, lwd=3, col="red")
hist(out3$estloc, main="Result-'Saddle'", xlab="local dimension")
abline(v=out2$estdim, lwd=3, col="red")
par(opar)


</code></pre>

<hr>
<h2 id='est.incisingball'>Intrinsic Dimension Estimation with Incising Ball</h2><span id='topic+est.incisingball'></span>

<h3>Description</h3>

<p>Incising ball methods exploits the exponential relationship of the number of samples
contained in a ball and the radius of the incising ball.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>est.incisingball(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="est.incisingball_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing containing </p>

<dl>
<dt>estdim</dt><dd><p>estimated intrinsic dimension.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Fan M, Qiao H, Zhang B (2009).
&ldquo;Intrinsic Dimension Estimation of Manifolds by Incising Balls.&rdquo;
<em>Pattern Recognition</em>, <b>42</b>(5), 780&ndash;787.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## create an example data with intrinsic dimension 2
X = cbind(aux.gensamples(dname="swiss"),aux.gensamples(dname="swiss"))

## acquire an estimate for intrinsic dimension
output = est.incisingball(X)
sprintf("* est.incisingball : estimated dimension is %d.",output$estdim)


</code></pre>

<hr>
<h2 id='est.made'>Manifold-Adaptive Dimension Estimation</h2><span id='topic+est.made'></span>

<h3>Description</h3>

<p><code>do.made</code> first aims at finding local dimesion estimates using nearest neighbor techniques based on
the first-order approximation of the probability mass function and then combines them to get a single global estimate. Due to the rate of convergence of such
estimate to be independent of assumed dimensionality, authors claim this method to be
<em>manifold-adaptive</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>est.made(
  X,
  k = round(sqrt(ncol(X))),
  maxdim = min(ncol(X), 15),
  combine = c("mean", "median", "vote")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="est.made_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="est.made_+3A_k">k</code></td>
<td>
<p>size of neighborhood for analysis.</p>
</td></tr>
<tr><td><code id="est.made_+3A_maxdim">maxdim</code></td>
<td>
<p>maximum possible dimension allowed for the algorithm to investigate.</p>
</td></tr>
<tr><td><code id="est.made_+3A_combine">combine</code></td>
<td>
<p>method to aggregate local estimates for a single global estimate.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing containing </p>

<dl>
<dt>estdim</dt><dd><p>estimated global intrinsic dimension.</p>
</dd>
<dt>estloc</dt><dd><p>a length-<code class="reqn">n</code> vector estimated dimension at each point.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Farahmand AM, Szepesvári C, Audibert J (2007).
&ldquo;Manifold-Adaptive Dimension Estimation.&rdquo;
In <em>ICML</em>, volume 227 of <em>ACM International Conference Proceeding Series</em>, 265&ndash;272.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## create a data set of intrinsic dimension 2.
X = aux.gensamples(dname="swiss")

## compare effect of 3 combining scheme
out1 = est.made(X, combine="mean")
out2 = est.made(X, combine="median")
out3 = est.made(X, combine="vote")

## print the results
line1 = paste0("* est.made : 'mean'   estiamte is ",round(out1$estdim,2))
line2 = paste0("* est.made : 'median' estiamte is ",round(out2$estdim,2))
line3 = paste0("* est.made : 'vote'   estiamte is ",round(out3$estdim,2))
cat(paste0(line1,"\n",line2,"\n",line3))


</code></pre>

<hr>
<h2 id='est.mindkl'>MiNDkl</h2><span id='topic+est.mindkl'></span>

<h3>Description</h3>

<p>It is a minimum neighbor distance estimator of the intrinsic dimension based on Kullback Leibler divergence estimator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>est.mindkl(X, k = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="est.mindkl_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="est.mindkl_+3A_k">k</code></td>
<td>
<p>the neighborhood size for defining locality.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing containing </p>

<dl>
<dt>estdim</dt><dd><p>the global estimated dimension.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Lombardi G, Rozza A, Ceruti C, Casiraghi E, Campadelli P (2011).
&ldquo;Minimum Neighbor Distance Estimators of Intrinsic Dimension.&rdquo;
In Gunopulos D, Hofmann T, Malerba D, Vazirgiannis M (eds.), <em>Machine Learning and Knowledge Discovery in Databases</em>, volume 6912, 374&ndash;389.
Springer Berlin Heidelberg, Berlin, Heidelberg.
ISBN 978-3-642-23782-9 978-3-642-23783-6.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+est.mindml">est.mindml</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## create 3 datasets of intrinsic dimension 2.
X1 = aux.gensamples(dname="swiss")
X2 = aux.gensamples(dname="ribbon")
X3 = aux.gensamples(dname="saddle")

## acquire an estimate for intrinsic dimension
out1 = est.mindkl(X1, k=5)
out2 = est.mindkl(X2, k=5)
out3 = est.mindkl(X3, k=5)

## print the results
line1 = paste0("* est.mindkl : 'swiss'  estiamte is ",round(out1$estdim,2))
line2 = paste0("* est.mindkl : 'ribbon' estiamte is ",round(out2$estdim,2))
line3 = paste0("* est.mindkl : 'saddle' estiamte is ",round(out3$estdim,2))
cat(paste0(line1,"\n",line2,"\n",line3))


</code></pre>

<hr>
<h2 id='est.mindml'>MINDml</h2><span id='topic+est.mindml'></span>

<h3>Description</h3>

<p>It is a minimum neighbor distance estimator of the intrinsic dimension based on Maximum Likelihood principle.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>est.mindml(X, k = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="est.mindml_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="est.mindml_+3A_k">k</code></td>
<td>
<p>the neighborhood size for defining locality.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing containing </p>

<dl>
<dt>estdim</dt><dd><p>the global estimated dimension.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Lombardi G, Rozza A, Ceruti C, Casiraghi E, Campadelli P (2011).
&ldquo;Minimum Neighbor Distance Estimators of Intrinsic Dimension.&rdquo;
In Gunopulos D, Hofmann T, Malerba D, Vazirgiannis M (eds.), <em>Machine Learning and Knowledge Discovery in Databases</em>, volume 6912, 374&ndash;389.
Springer Berlin Heidelberg, Berlin, Heidelberg.
ISBN 978-3-642-23782-9 978-3-642-23783-6.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+est.mindkl">est.mindkl</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## create 3 datasets of intrinsic dimension 2.
set.seed(100)
X1 = aux.gensamples(dname="swiss")
X2 = aux.gensamples(dname="ribbon")
X3 = aux.gensamples(dname="saddle")

## acquire an estimate for intrinsic dimension
out1 = est.mindml(X1, k=10)
out2 = est.mindml(X2, k=10)
out3 = est.mindml(X3, k=10)

## print the results
line1 = paste0("* est.mindml : 'swiss'  estiamte is ",round(out1$estdim,2))
line2 = paste0("* est.mindml : 'ribbon' estiamte is ",round(out2$estdim,2))
line3 = paste0("* est.mindml : 'saddle' estiamte is ",round(out3$estdim,2))
cat(paste0(line1,"\n",line2,"\n",line3))


</code></pre>

<hr>
<h2 id='est.mle1'>Maximum Likelihood Esimation with Poisson Process</h2><span id='topic+est.mle1'></span>

<h3>Description</h3>

<p>Assuming the density in a hypersphere is constant, authors proposed to build
a likelihood structure based on modeling local spread of information via Poisson Process.
<code>est.mle1</code> requires two parameters that model the reasonable range of neighborhood size
to reflect inhomogeneity of distribution across data points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>est.mle1(X, k1 = 10, k2 = 20)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="est.mle1_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="est.mle1_+3A_k1">k1</code></td>
<td>
<p>minimum neighborhood size, larger than 1.</p>
</td></tr>
<tr><td><code id="est.mle1_+3A_k2">k2</code></td>
<td>
<p>maximum neighborhood size, smaller than <code class="reqn">n</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing containing </p>

<dl>
<dt>estdim</dt><dd><p>estimated intrinsic dimension.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Levina E, Bickel PJ (2005).
&ldquo;Maximum Likelihood Estimation of Intrinsic Dimension.&rdquo;
In Saul LK, Weiss Y, Bottou L (eds.), <em>Advances in Neural Information Processing Systems 17</em>, 777&ndash;784.
MIT Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## create example data sets with intrinsic dimension 2
X1 = aux.gensamples(dname="swiss")
X2 = aux.gensamples(dname="ribbon")
X3 = aux.gensamples(dname="saddle")

## acquire an estimate for intrinsic dimension
out1 = est.mle1(X1)
out2 = est.mle1(X2)
out3 = est.mle1(X3)

## print the estimates
line1 = paste0("* est.mle1 : 'swiss'  estiamte is ",round(out1$estdim,2))
line2 = paste0("* est.mle1 : 'ribbon' estiamte is ",round(out2$estdim,2))
line3 = paste0("* est.mle1 : 'saddle' estiamte is ",round(out3$estdim,2))
cat(paste0(line1,"\n",line2,"\n",line3))


</code></pre>

<hr>
<h2 id='est.mle2'>Maximum Likelihood Esimation with Poisson Process and Bias Correction</h2><span id='topic+est.mle2'></span>

<h3>Description</h3>

<p>Authors argue that the approach proposed in <code><a href="#topic+est.mle1">est.mle1</a></code> is
empirically bias-prone in that the averaging of sample statistics over
all data points is taken to be a harmonic manner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>est.mle2(X, k1 = 10, k2 = 20)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="est.mle2_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="est.mle2_+3A_k1">k1</code></td>
<td>
<p>minimum neighborhood size, larger than 1.</p>
</td></tr>
<tr><td><code id="est.mle2_+3A_k2">k2</code></td>
<td>
<p>maximum neighborhood size, smaller than <code class="reqn">n</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing containing </p>

<dl>
<dt>estdim</dt><dd><p>estimated intrinsic dimension.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>MacKay DJC, Ghahramani Z (2005).
&ldquo;Comments on 'Maximum Likelihood Estimation of Intrinsic Dimension' by E. Levina and P. Bickel (2004).&rdquo;
https://www.inference.org.uk/mackay/dimension/.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## create example data sets with intrinsic dimension 2
X1 = aux.gensamples(dname="swiss")
X2 = aux.gensamples(dname="ribbon")
X3 = aux.gensamples(dname="saddle")

## acquire an estimate for intrinsic dimension
out1 = est.mle2(X1)
out2 = est.mle2(X2)
out3 = est.mle2(X3)

line1 = paste0("* est.mle2 : dimension of 'swiss'  data is ",round(out1$estdim,2))
line2 = paste0("* est.mle2 : dimension of 'ribbon' data is ",round(out2$estdim,2))
line3 = paste0("* est.mle2 : dimension of 'saddle' data is ",round(out3$estdim,2))
cat(paste0(line1,"\n",line2,"\n",line3))


</code></pre>

<hr>
<h2 id='est.nearneighbor1'>Intrinsic Dimension Estimation with Near-Neighbor Information</h2><span id='topic+est.nearneighbor1'></span>

<h3>Description</h3>

<p>Based on an assumption of data points being locally uniformly distributed,
<code>est.nearneighbor1</code> estimates the intrinsic dimension based on the
local distance information in an iterative manner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>est.nearneighbor1(X, K = max(2, round(ncol(X)/5)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="est.nearneighbor1_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="est.nearneighbor1_+3A_k">K</code></td>
<td>
<p>maximum neighborhood size, smaller than <code class="reqn">p</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing containing </p>

<dl>
<dt>estdim</dt><dd><p>estimated intrinsic dimension.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Pettis KW, Bailey TA, Jain AK, Dubes RC (1979).
&ldquo;An Intrinsic Dimensionality Estimator from Near-Neighbor Information.&rdquo;
<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <b>PAMI-1</b>(1), 25&ndash;37.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## create an example data with intrinsic dimension 2
X = cbind(aux.gensamples(dname="swiss"),aux.gensamples(dname="swiss"))

## acquire an estimate for intrinsic dimension
output = est.nearneighbor1(X)
sprintf("* est.nearneighbor1 : estimated dimension is %.2f.",output$estdim)


</code></pre>

<hr>
<h2 id='est.nearneighbor2'>Near-Neighbor Information with Bias Correction</h2><span id='topic+est.nearneighbor2'></span>

<h3>Description</h3>

<p>Though similar to <code><a href="#topic+est.nearneighbor1">est.nearneighbor1</a></code>, authors of the reference
argued that there exists innate bias in the method and proposed a non-iterative algorithm
to reflect local distance information under a range of neighborhood sizes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>est.nearneighbor2(X, kmin = 2, kmax = max(3, round(ncol(X)/2)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="est.nearneighbor2_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="est.nearneighbor2_+3A_kmin">kmin</code></td>
<td>
<p>minimum neighborhood size, larger than 1.</p>
</td></tr>
<tr><td><code id="est.nearneighbor2_+3A_kmax">kmax</code></td>
<td>
<p>maximum neighborhood size, smaller than <code class="reqn">p</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing containing </p>

<dl>
<dt>estdim</dt><dd><p>estimated intrinsic dimension.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Verveer PJ, Duin RPW (1995).
&ldquo;An Evaluation of Intrinsic Dimensionality Estimators.&rdquo;
<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <b>17</b>(1), 81&ndash;86.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## create an example data with intrinsic dimension 2
X = cbind(aux.gensamples(dname="swiss"),aux.gensamples(dname="swiss"))

## acquire an estimate for intrinsic dimension
output = est.nearneighbor2(X)
sprintf("* est.nearneighbor2 : estimated dimension is %.2f.",output$estdim)


</code></pre>

<hr>
<h2 id='est.packing'>Intrinsic Dimension Estimation using Packing Numbers</h2><span id='topic+est.packing'></span>

<h3>Description</h3>

<p>Instead of covering numbers which are expensive to compute in many fractal-based methods,
<code>est.packing</code> exploits packing numbers as a proxy to describe spatial density. Since
it involves random permutation of the dataset at each iteration, every run might have
different results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>est.packing(X, eps = 0.01)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="est.packing_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="est.packing_+3A_eps">eps</code></td>
<td>
<p>small positive number for stopping threshold.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing containing </p>

<dl>
<dt>estdim</dt><dd><p>estimated intrinsic dimension.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Kégl B (2002).
&ldquo;Intrinsic Dimension Estimation Using Packing Numbers.&rdquo;
In <em>Proceedings of the 15th International Conference on Neural Information Processing Systems</em>,  NIPS'02, 697&ndash;704.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## create 'swiss' roll dataset
X = aux.gensamples(dname="swiss")

## try different eps values
out1 = est.packing(X, eps=0.1)
out2 = est.packing(X, eps=0.01)
out3 = est.packing(X, eps=0.001)

## print the results
line1 = paste0("* est.packing : eps=0.1   gives ",round(out1$estdim,2))
line2 = paste0("* est.packing : eps=0.01  gives ",round(out2$estdim,2))
line3 = paste0("* est.packing : eps=0.001 gives ",round(out3$estdim,2))
cat(paste0(line1,"\n",line2,"\n",line3))


</code></pre>

<hr>
<h2 id='est.pcathr'>PCA Thresholding with Accumulated Variance</h2><span id='topic+est.pcathr'></span>

<h3>Description</h3>

<p>Principal Component Analysis exploits sample covariance matrix whose
eigenvectors and eigenvalues are principal components and projected
variance, correspondingly. Given <code>varratio</code>, it thresholds the
accumulated variance and selects the estimated dimension. Note that other than
linear submanifold case, the naive selection scheme from this algorithm
lacks flexibility in discovering intrinsic dimension.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>est.pcathr(X, varratio = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="est.pcathr_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="est.pcathr_+3A_varratio">varratio</code></td>
<td>
<p>target explainability for accumulated variance in <code class="reqn">(0,1)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing containing </p>

<dl>
<dt>estdim</dt><dd><p>estimated dimension according to <code>varratio</code>.</p>
</dd>
<dt>values</dt><dd><p>eigenvalues of sample covariance matrix.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>See Also</h3>

<p><code><a href="#topic+do.pca">do.pca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate 3-dimensional normal data
X = matrix(rnorm(100*3), nrow=100)

## replicate 3 times with translations
Y = cbind(X-10,X,X+10)

## use PCA thresholding estimation with 95% variance explainability
## desired return is for dimension 3.
output   = est.pcathr(Y)
pmessage = paste("* estimated dimension is ",output$estdim, sep="")
print(pmessage)

## use screeplot
opar &lt;- par(no.readonly=TRUE)
plot(output$values, main="scree plot", type="b")
par(opar)

</code></pre>

<hr>
<h2 id='est.twonn'>Intrinsic Dimension Estimation by a Minimal Neighborhood Information</h2><span id='topic+est.twonn'></span>

<h3>Description</h3>

<p>Unlike many intrinsic dimension (ID) estimation methods, <code>est.twonn</code> only requires
two nearest datapoints from a target point and their distances. This extremely minimal approach
is claimed to redue the effects of curvature and density variation across different locations
in an underlying manifold.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>est.twonn(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="est.twonn_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing containing </p>

<dl>
<dt>estdim</dt><dd><p>estimated intrinsic dimension.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Facco E, d'Errico M, Rodriguez A, Laio A (2017).
&ldquo;Estimating the Intrinsic Dimension of Datasets by a Minimal Neighborhood Information.&rdquo;
<em>Scientific Reports</em>, <b>7</b>(1).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## create 3 datasets of intrinsic dimension 2.
X1 = aux.gensamples(dname="swiss")
X2 = aux.gensamples(dname="ribbon")
X3 = aux.gensamples(dname="saddle")

## acquire an estimate for intrinsic dimension
out1 = est.twonn(X1)
out2 = est.twonn(X2)
out3 = est.twonn(X3)

## print the results
line1 = paste0("* est.twonn : 'swiss'  gives ",round(out1$estdim,2))
line2 = paste0("* est.twonn : 'ribbon' gives ",round(out2$estdim,2))
line3 = paste0("* est.twonn : 'saddle' gives ",round(out3$estdim,2))
cat(paste0(line1,"\n",line2,"\n",line3))


</code></pre>

<hr>
<h2 id='est.Ustat'>ID Estimation with Convergence Rate of U-statistic on Manifold</h2><span id='topic+est.Ustat'></span>

<h3>Description</h3>

<p><code class="reqn">U</code>-statistic is built upon theoretical arguments with the language of
smooth manifold. The convergence rate of the statistic is achieved as a proxy
for the estimated dimension by, at least partially, considering
the scale and influence of extrinsic curvature. The method returns <em>integer</em> valued
estimate in that there is no need for rounding the result for practical usage.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>est.Ustat(X, maxdim = min(ncol(X), 15))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="est.Ustat_+3A_x">X</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix or data frame whose rows are observations.</p>
</td></tr>
<tr><td><code id="est.Ustat_+3A_maxdim">maxdim</code></td>
<td>
<p>maximum possible dimension allowed for the algorithm to investigate.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list containing containing </p>

<dl>
<dt>estdim</dt><dd><p>estimated intrinsic dimension.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>References</h3>

<p>Hein M, Audibert J (2005).
&ldquo;Intrinsic Dimensionality Estimation of Submanifolds in $R^d$.&rdquo;
In <em>Proceedings of the 22nd International Conference on Machine Learning</em>, 289&ndash;296.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## create 3 datasets of intrinsic dimension 2.
X1 = aux.gensamples(dname="swiss")
X2 = aux.gensamples(dname="ribbon")
X3 = aux.gensamples(dname="saddle")

## acquire an estimate for intrinsic dimension
out1 = est.Ustat(X1)
out2 = est.Ustat(X2)
out3 = est.Ustat(X3)

## print the results
line1 = paste0("* est.Ustat : 'swiss'  gives ",round(out1$estdim,2))
line2 = paste0("* est.Ustat : 'ribbon' gives ",round(out2$estdim,2))
line3 = paste0("* est.Ustat : 'saddle' gives ",round(out3$estdim,2))
cat(paste0(line1,"\n",line2,"\n",line3))


</code></pre>

<hr>
<h2 id='iris'>Load Iris data</h2><span id='topic+iris'></span>

<h3>Description</h3>

<p>This is the identical dataset as original <code>iris</code> data where numeric values of
<code>Sepal.Length</code>, <code>Sepal.Width</code>, <code>Petal.Length</code>, <code>Petal.Width</code>
measured in centimeters are given for 50 flowers from each of 3 species of iris.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(iris)
</code></pre>


<h3>Format</h3>

<p>a data.frame containing</p>

<dl>
<dt>Sepal.Length</dt><dd><p>sepal length</p>
</dd>
<dt>Sepal.Width</dt><dd><p>sepal width</p>
</dd>
<dt>Petal.Length</dt><dd><p>petal length</p>
</dd>
<dt>Petal.Width</dt><dd><p>petal width</p>
</dd>
<dt>Species</dt><dd><p>(factor) one of 'setosa','versicolor', and 'virginica'.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
# load the data
data(iris)

# visualize
opar &lt;- par(no.readonly=TRUE)
plot(iris[,1:4])
par(opar)


</code></pre>

<hr>
<h2 id='oos.linproj'>OOS : Linear Projection</h2><span id='topic+oos.linproj'></span>

<h3>Description</h3>

<p>The simplest way of out-of-sample extension might be linear regression even though the original embedding
is not the linear type by solving
</p>
<p style="text-align: center;"><code class="reqn">\textrm{min}_{\beta} \|X_{old} \beta - Y_{old}\|_2^2</code>
</p>
<p> and use the estimate <code class="reqn">\hat{beta}</code> to acquire
</p>
<p style="text-align: center;"><code class="reqn">Y_{new} = X_{new} \hat{\beta}</code>
</p>
<p>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>oos.linproj(Xold, Yold, Xnew)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="oos.linproj_+3A_xold">Xold</code></td>
<td>
<p>an <code class="reqn">(n\times p)</code> matrix of data in original high-dimensional space.</p>
</td></tr>
<tr><td><code id="oos.linproj_+3A_yold">Yold</code></td>
<td>
<p>an <code class="reqn">(n\times ndim)</code> matrix of data in reduced-dimensional space.</p>
</td></tr>
<tr><td><code id="oos.linproj_+3A_xnew">Xnew</code></td>
<td>
<p>an <code class="reqn">(m\times p)</code> matrix for out-of-sample extension.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an <code class="reqn">(m\times ndim)</code> matrix whose rows are embedded observations.
</p>


<h3>Author(s)</h3>

<p>Kisung You
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate sample data and separate them
data(iris, package="Rdimtools")
X   = as.matrix(iris[,1:4])
lab = as.factor(as.vector(iris[,5]))
ids = sample(1:150, 30)

Xold = X[setdiff(1:150,ids),]  # 80% of data for training
Xnew = X[ids,]                 # 20% of data for testing

## run PCA for train data &amp; use the info for prediction
training = do.pca(Xold,ndim=2)
Yold     = training$Y
Ynew     = Xnew%*%training$projection
Yplab    = lab[ids]

## perform out-of-sample prediction
Yoos  = oos.linproj(Xold, Yold, Xnew)

## visualize
opar &lt;- par(no.readonly=TRUE)
par(mfrow=c(1,2))
plot(Ynew, pch=19, col=Yplab, main="true prediction")
plot(Yoos, pch=19, col=Yplab, main="OOS prediction")
par(opar)


</code></pre>

<hr>
<h2 id='usps'>Load USPS handwritten digits data</h2><span id='topic+usps'></span>

<h3>Description</h3>

<p>The well-known USPS handwritten digits from &quot;0&quot; to &quot;9&quot;. Though the original version
of each digit is given as a <code class="reqn">16\times 16</code> matrix of grayscale image, it is
convention to vectorize it. For each digit, 1100 examples are given.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(usps)
</code></pre>


<h3>Format</h3>

<p>a named list containing</p>

<dl>
<dt>data</dt><dd><p>an <code class="reqn">(11000\times 256)</code> matrix where each row is a number.</p>
</dd>
<dt>label</dt><dd><p>(factor) a length-<code class="reqn">11000</code> class label in <code class="reqn">0,1,\ldots,9</code>.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
# load the data
data(usps)

# visualize
opar &lt;- par(no.readonly=TRUE, mfrow=c(1,3), pty="s")
image(t(matrix(usps$data[4400,],nrow=16)[16:1,])) # last of digit 4
image(t(matrix(usps$data[9900,],nrow=16)[16:1,])) # last of digit 9
image(t(matrix(usps$data[6600,],nrow=16)[16:1,])) # last of digit 6
par(opar)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
