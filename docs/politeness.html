<!DOCTYPE html><html><head><title>Help for package politeness</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {politeness}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bowl_offers'><p>Purchase offers for bowl</p></a></li>
<li><a href='#cleanpunct'><p>Cleaning weird encodings</p></a></li>
<li><a href='#cleantext'><p>Clean Text</p></a></li>
<li><a href='#ctxpand'><p>Contraction Expander</p></a></li>
<li><a href='#dictWrap'><p>Dictionary Wrapper</p></a></li>
<li><a href='#feature_table'><p>Table of Politeness Features</p></a></li>
<li><a href='#findPoliteTexts'><p>Find polite text</p></a></li>
<li><a href='#foldset'><p>Fold Assignment for Cross-Validation</p></a></li>
<li><a href='#getTokenSets'><p>Extracting Tokens from Natural Language</p></a></li>
<li><a href='#noLeadDash'><p>Cleaning leading punctuation</p></a></li>
<li><a href='#phone_offers'><p>#' Positive Emotions List</p>
#'
#' Positive words.
#'
#' @format A list of 2006 positively-valenced words
#'
&quot;positive_list&quot;</a></li>
<li><a href='#polite_train'><p>Pre-Trained Politeness</p></a></li>
<li><a href='#politeness'><p>Politeness Features</p></a></li>
<li><a href='#politenessDNM'><p>Politeness Features</p></a></li>
<li><a href='#politenessModel'><p>Pre-Trained Politeness Classifier</p></a></li>
<li><a href='#politenessPlot'><p>Politeness plot</p></a></li>
<li><a href='#politenessProjection'><p>Politeness projection</p></a></li>
<li><a href='#receptive_model'><p>A pre-trained model for detecting conversational receptiveness.</p>
Estimated with glmnet using annotated data from a previous paper.
Primarily for use within the receptiveness() function.</a></li>
<li><a href='#receptive_names'><p>This is the list of variables to be extracted for the receptiveness algorithm</p>
For internal use only, within the receptiveness() function.</a></li>
<li><a href='#receptive_polite'><p>Pre-Trained Receptiveness Data</p></a></li>
<li><a href='#receptive_train'><p>Pre-Trained Receptiveness Data</p></a></li>
<li><a href='#receptiveness'><p>Conversational Receptiveness</p></a></li>
<li><a href='#slogodds'><p>Variance-Weighted Log Odds</p></a></li>
<li><a href='#spacyParser'><p>Spacy Parser</p></a></li>
<li><a href='#textcounter'><p>Text Counter</p></a></li>
<li><a href='#uk2us'><p>UK to US Conversion dictionary</p></a></li>
<li><a href='#usWords'><p>UK to US conversion</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Detecting Politeness Features in Text</td>
</tr>
<tr>
<td>Version:</td>
<td>0.9.3</td>
</tr>
<tr>
<td>Author:</td>
<td>Mike Yeomans, Alejandro Kantor, Dustin Tingley</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Mike Yeomans &lt;mk.yeomans@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Detecting markers of politeness in English natural language. This package allows researchers to easily visualize and quantify politeness between groups of documents. This package combines prior research on the linguistic markers of politeness. We thank the Spencer Foundation, the Hewlett Foundation, and Harvard's Institute for Quantitative Social Science for support.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>tm, quanteda, ggplot2, parallel, spacyr, textir, glmnet,
data.table, stringr, stringi</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-12 12:48:34 UTC; myeomans</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-12 13:13:26 UTC</td>
</tr>
</table>
<hr>
<h2 id='bowl_offers'>Purchase offers for bowl</h2><span id='topic+bowl_offers'></span>

<h3>Description</h3>

<p>A dataset containing the purchase offer message and a
label indicating if the writer was assigned to be warm (1) or tough (0)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bowl_offers
</code></pre>


<h3>Format</h3>

<p>A data frame with 70 rows and 2 variables:
</p>

<dl>
<dt>message</dt><dd><p>character of purchase offer message</p>
</dd>
<dt>condition</dt><dd><p>binary label indicating if message is warm or tough</p>
</dd>
</dl>



<h3>Source</h3>

<p>Jeong, M., Minson, J., Yeomans, M. &amp; Gino, F. (2019).
</p>
<p>&quot;Communicating Warmth in Distributed Negotiations is Surprisingly Ineffective.&quot; Study 3.
</p>
<p>Study 3. <a href="https://osf.io/t7sd6/">https://osf.io/t7sd6/</a>
</p>

<hr>
<h2 id='cleanpunct'>Cleaning weird encodings</h2><span id='topic+cleanpunct'></span>

<h3>Description</h3>

<p>Handles curly quotes, umlauts, etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cleanpunct(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cleanpunct_+3A_text">text</code></td>
<td>
<p>character Vector of strings to clean.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>character Vector of clean strings.
</p>

<hr>
<h2 id='cleantext'>Clean Text</h2><span id='topic+cleantext'></span>

<h3>Description</h3>

<p>Basic text cleaning
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cleantext(text, language = "english", stop.words = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cleantext_+3A_text">text</code></td>
<td>
<p>character text to be cleaned</p>
</td></tr>
<tr><td><code id="cleantext_+3A_language">language</code></td>
<td>
<p>string. Default &quot;english&quot;.</p>
</td></tr>
<tr><td><code id="cleantext_+3A_stop.words">stop.words</code></td>
<td>
<p>logical. Default TRUE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector
</p>

<hr>
<h2 id='ctxpand'>Contraction Expander</h2><span id='topic+ctxpand'></span>

<h3>Description</h3>

<p>Expands Contractions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ctxpand(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ctxpand_+3A_text">text</code></td>
<td>
<p>a character vector of texts.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector
</p>

<hr>
<h2 id='dictWrap'>Dictionary Wrapper</h2><span id='topic+dictWrap'></span>

<h3>Description</h3>

<p>background function to load
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dictWrap(text, dict = NULL, binary = FALSE, num_mc_cores = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dictWrap_+3A_text">text</code></td>
<td>
<p>a character vector of texts.</p>
</td></tr>
<tr><td><code id="dictWrap_+3A_dict">dict</code></td>
<td>
<p>a dictionary class object (see <a href="quanteda.html#topic+dictionary">dictionary</a>) containing dictionaries for six of the politeness features</p>
</td></tr>
<tr><td><code id="dictWrap_+3A_binary">binary</code></td>
<td>
<p>return the prevalence (percent of words) or the presence (yes/no) of a feature in each text?</p>
</td></tr>
<tr><td><code id="dictWrap_+3A_num_mc_cores">num_mc_cores</code></td>
<td>
<p>integer Number of cores for parallelization. Default is 1.</p>
</td></tr>
<tr><td><code id="dictWrap_+3A_...">...</code></td>
<td>
<p>arguments passes onto the <code>quanteda:dfm</code> function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix with six columns (one for each feature) and a row for every text entered into the function.
</p>

<hr>
<h2 id='feature_table'>Table of Politeness Features</h2><span id='topic+feature_table'></span>

<h3>Description</h3>

<p>This table describes all the text features extracted in this package. See vignette for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>feature_table
</code></pre>


<h3>Format</h3>

<p>A data.frame with information about the politeness features.
</p>

<hr>
<h2 id='findPoliteTexts'>Find polite text</h2><span id='topic+findPoliteTexts'></span>

<h3>Description</h3>

<p>Finds examples of most or least polite text in a corpus
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findPoliteTexts(
  text,
  df_polite,
  covar,
  type = c("most", "least", "both"),
  num_docs = 5L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findPoliteTexts_+3A_text">text</code></td>
<td>
<p>a character vector of texts.</p>
</td></tr>
<tr><td><code id="findPoliteTexts_+3A_df_polite">df_polite</code></td>
<td>
<p>a data.frame with politeness features, as outputed by <code><a href="#topic+politeness">politeness</a></code>, used to train model.</p>
</td></tr>
<tr><td><code id="findPoliteTexts_+3A_covar">covar</code></td>
<td>
<p>a vector of politeness labels, or other covariate.</p>
</td></tr>
<tr><td><code id="findPoliteTexts_+3A_type">type</code></td>
<td>
<p>a string indicating if function should return the most or least polite texts or both. If <code>length &gt; 1</code> only first value is used.</p>
</td></tr>
<tr><td><code id="findPoliteTexts_+3A_num_docs">num_docs</code></td>
<td>
<p>integer of number of documents to be returned. Default is 5.</p>
</td></tr>
<tr><td><code id="findPoliteTexts_+3A_...">...</code></td>
<td>
<p>additional parameters to be passed to <code>politenessProjection</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function returns a data.frame ranked by (more or least) politeness.
If <code>type == 'most'</code>, the <code>num_docs</code> most polite texts will be returned.
If <code>type == 'least'</code>, the <code>num_docs</code> least polite texts will be returned.
If <code>type == 'both'</code>, both most and least polite text will be returned.
if <code>num_docs</code> is even, half will be most and half least polite else half + 1 will be most polite.
</p>
<p><code>df_polite</code> must have the same number of rows as the <code>length(text)</code> and <code>length(covar)</code>.
</p>


<h3>Value</h3>

<p>data.frame with texts ranked by (more or least) politeness. See details for more information.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("phone_offers")
polite.data&lt;-politeness(phone_offers$message, parser="none",drop_blank=FALSE)

findPoliteTexts(phone_offers$message,
                polite.data,
                phone_offers$condition,
                type = "most",
                num_docs = 5)

findPoliteTexts(phone_offers$message,
                polite.data,
                phone_offers$condition,
                type = "least",
                num_docs = 10)

</code></pre>

<hr>
<h2 id='foldset'>Fold Assignment for Cross-Validation</h2><span id='topic+foldset'></span>

<h3>Description</h3>

<p>background function to load
</p>


<h3>Usage</h3>

<pre><code class='language-R'>foldset(sizer, nfold, balance = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="foldset_+3A_sizer">sizer</code></td>
<td>
<p>number of observations in dataset.</p>
</td></tr>
<tr><td><code id="foldset_+3A_nfold">nfold</code></td>
<td>
<p>number of outer folds needed.</p>
</td></tr>
<tr><td><code id="foldset_+3A_balance">balance</code></td>
<td>
<p>Optional vector of a categorical covariate to stratify fold assignment</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of fold IDs
</p>

<hr>
<h2 id='getTokenSets'>Extracting Tokens from Natural Language</h2><span id='topic+getTokenSets'></span>

<h3>Description</h3>

<p>Return tokens (words or POS tags) from natural language.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getTokenSets(text, parser = c("none", "spacy"), num_mc_cores = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getTokenSets_+3A_text">text</code></td>
<td>
<p>a character vector of texts.</p>
</td></tr>
<tr><td><code id="getTokenSets_+3A_parser">parser</code></td>
<td>
<p>character Name of dependency parser to use.</p>
</td></tr>
<tr><td><code id="getTokenSets_+3A_num_mc_cores">num_mc_cores</code></td>
<td>
<p>integer Number of cores for parallelization. Default is 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of compiled POS-tagged items.
</p>

<hr>
<h2 id='noLeadDash'>Cleaning leading punctuation</h2><span id='topic+noLeadDash'></span>

<h3>Description</h3>

<p>Handles interruption dashes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>noLeadDash(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="noLeadDash_+3A_text">text</code></td>
<td>
<p>character Vector of strings to clean.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>character Vector of clean strings.
</p>

<hr>
<h2 id='phone_offers'>#' Positive Emotions List
#'
#' Positive words.
#'
#' @format A list of 2006 positively-valenced words
#'
&quot;positive_list&quot;</h2><span id='topic+phone_offers'></span>

<h3>Description</h3>

<p>#' Negative Emotions List
#'
#' Negative words.
#'
#' @format A list of 4783 negatively-valenced words
#'
&quot;negative_list&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>phone_offers
</code></pre>


<h3>Format</h3>

<p>A data frame with 355 rows and 2 variables:
</p>

<dl>
<dt>message</dt><dd><p>character of purchase offer message</p>
</dd>
<dt>condition</dt><dd><p>binary label indicating if message is warm or tough</p>
</dd>
</dl>



<h3>Details</h3>

<p>#' Hedge Words List
#'
#' Hedges
#'
#' @format A list of 72 hedging words.
#'
&quot;hedge_list&quot;
</p>
<p>#' Feature Dictionaries
#'
#' Six dictionary-like features for the detector: Negations; Pauses; Swearing; Pronouns; Formal Titles; and Informal Titles.
#'
#' @format A list of six <code>quanteda::dictionary</code> objects
&quot;polite_dicts&quot;
Purchase offers for phone
</p>
<p>A dataset containing the purchase offer message and a
label indicating if the writer was assigned to be warm (1) or tough (0)
</p>


<h3>Source</h3>

<p>Jeong, M., Minson, J., Yeomans, M. &amp; Gino, F. (2019).
</p>
<p>&quot;Communicating Warmth in Distributed Negotiations is Surprisingly Ineffective.&quot;
</p>
<p>Study 1. <a href="https://osf.io/t7sd6/">https://osf.io/t7sd6/</a>
</p>

<hr>
<h2 id='polite_train'>Pre-Trained Politeness</h2><span id='topic+polite_train'></span>

<h3>Description</h3>

<p>A dataset to train a model for detecting politeness.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>polite_train
</code></pre>


<h3>Format</h3>

<p>list of two objects. x contains pre-calculated politeness features for each document. y contains standardized human annotations for politeness.
</p>


<h3>Source</h3>

<p>Danescu-Niculescu-Mizil, C., Sudhof, M., Jurafsky, D., Leskovec, J. &amp; Potts, C. (2013). A computational approach to politeness with application to social factors. Proc. 51st ACL, 250-259.
</p>

<hr>
<h2 id='politeness'>Politeness Features</h2><span id='topic+politeness'></span>

<h3>Description</h3>

<p>Detects linguistic markers of politeness in natural language.
This function is the workhorse of the <code>politeness</code> package, taking an N-length vector of text documents and returning an N-row data.frame of feature counts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>politeness(
  text,
  parser = c("none", "spacy"),
  metric = c("count", "binary", "average"),
  drop_blank = FALSE,
  uk_english = FALSE,
  num_mc_cores = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="politeness_+3A_text">text</code></td>
<td>
<p>character A vector of texts, each of which will be tallied for politeness features.</p>
</td></tr>
<tr><td><code id="politeness_+3A_parser">parser</code></td>
<td>
<p>character Name of dependency parser to use (see details). Without a dependency parser, some features will be approximated, while others cannot be calculated at all.</p>
</td></tr>
<tr><td><code id="politeness_+3A_metric">metric</code></td>
<td>
<p>character What metric to return? Raw feature count totals, Binary presence/absence of features, or feature counts per 100 words. Default is &quot;count&quot;.</p>
</td></tr>
<tr><td><code id="politeness_+3A_drop_blank">drop_blank</code></td>
<td>
<p>logical Should features that were not found in any text be removed from the data.frame? Default is FALSE</p>
</td></tr>
<tr><td><code id="politeness_+3A_uk_english">uk_english</code></td>
<td>
<p>logical Does the text contain any British English spelling? Including variants (e.g. Canadian). Default is FALSE</p>
</td></tr>
<tr><td><code id="politeness_+3A_num_mc_cores">num_mc_cores</code></td>
<td>
<p>integer Number of cores for parallelization. Default is 1, but we encourage users to try parallel::detectCores() if possible.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Some politeness features depend on part-of-speech tagged sentences (e.g. &quot;bare commands&quot; are a particular verb class).
To include these features in the analysis, a POS tagger must be initialized beforehand - we currently support SpaCy which must
be installed separately in Python (see example for implementation).
</p>


<h3>Value</h3>

<p>a data.frame of politeness features, with one row for every item in 'text'. Possible politeness features are listed in <code><a href="#topic+feature_table">feature_table</a></code>
</p>


<h3>References</h3>

<p>Brown, P., &amp; Levinson, S. C. (1987). Politeness: Some universals in language usage (Vol. 4). Cambridge university press.
</p>
<p>Danescu-Niculescu-Mizil, C., Sudhof, M., Jurafsky, D., Leskovec, J., &amp; Potts, C. (2013). A computational approach to politeness with application to social factors. arXiv preprint arXiv:1306.6078.
</p>
<p>Voigt, R., Camp, N. P., Prabhakaran, V., Hamilton, W. L., ... &amp; Eberhardt, J. L. (2017). Language from police body camera footage shows racial disparities in officer respect. Proceedings of the National Academy of Sciences, 201702413.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("phone_offers")

politeness(phone_offers$message, parser="none",drop_blank=FALSE)

colMeans(politeness(phone_offers$message, parser="none", metric="binary", drop_blank=FALSE))
colMeans(politeness(phone_offers$message, parser="none", metric="count", drop_blank=FALSE))

dim(politeness(phone_offers$message, parser="none",drop_blank=FALSE))
dim(politeness(phone_offers$message, parser="none",drop_blank=TRUE))

## Not run: 
# Detect multiple cores automatically for parallel processing
politeness(phone_offers$message, num_mc_cores=parallel::detectCores())

# Connect to SpaCy installation for part-of-speech features
install.packages("spacyr")
spacyr::spacy_initialize(python_executable = PYTHON_PATH)
politeness(phone_offers$message, parser="spacy",drop_blank=FALSE)


## End(Not run)




</code></pre>

<hr>
<h2 id='politenessDNM'>Politeness Features</h2><span id='topic+politenessDNM'></span>

<h3>Description</h3>

<p>Detects linguistic markers of politeness in natural language.
This function emulates the original features of the Danescu-Niculescu-Mizil Politeness paper. This primarily exists to contrast with the full feature set in the main package, and is not recommended otherwise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>politenessDNM(text, uk_english = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="politenessDNM_+3A_text">text</code></td>
<td>
<p>character A vector of texts, each of which will be tallied for politeness features.</p>
</td></tr>
<tr><td><code id="politenessDNM_+3A_uk_english">uk_english</code></td>
<td>
<p>logical Does the text contain any British English spelling? Including variants (e.g. Canadian). Default is FALSE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame of politeness features, with one row for every item in 'text'. The original names are used where possible.
</p>


<h3>References</h3>

<p>Danescu-Niculescu-Mizil, C., Sudhof, M., Jurafsky, D., Leskovec, J., &amp; Potts, C. (2013). A computational approach to politeness with application to social factors. arXiv preprint arXiv:1306.6078.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
# Connect to SpaCy installation for part-of-speech features
install.packages("spacyr")
spacyr::spacy_initialize(python_executable = PYTHON_PATH)
data("phone_offers")

politeness(phone_offers$message)


## End(Not run)


</code></pre>

<hr>
<h2 id='politenessModel'>Pre-Trained Politeness Classifier</h2><span id='topic+politenessModel'></span>

<h3>Description</h3>

<p>Pre-trained model to detect politeness based on data from Danescu-Niculescu-Mizil et al. (2013)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>politenessModel(texts, num_mc_cores = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="politenessModel_+3A_texts">texts</code></td>
<td>
<p>character A vector of texts, each of which will be given a politeness score.</p>
</td></tr>
<tr><td><code id="politenessModel_+3A_num_mc_cores">num_mc_cores</code></td>
<td>
<p>integer Number of cores for parallelization.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a wrapper around a pre-trained model of &quot;politeness&quot; for all the data from the 2013 DNM et al paper.
This model requires grammar parsing via SpaCy. Please see <code><a href="spacyr.html#topic+spacyr">spacyr</a></code> for details on installation.
</p>


<h3>Value</h3>

<p>a vector with receptiveness scores
</p>


<h3>References</h3>

<p>Danescu-Niculescu-Mizil, C., Sudhof, M., Jurafsky, D., Leskovec, J. &amp; Potts, C. (2013). A computational approach to politeness with application to social factors. Proc. 51st ACL, 250-259.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

## Not run: 
data("phone_offers")

politenessModel(phone_offers$message)


## End(Not run)

</code></pre>

<hr>
<h2 id='politenessPlot'>Politeness plot</h2><span id='topic+politenessPlot'></span>

<h3>Description</h3>

<p>Plots the prevalence of politeness features in documents, divided by a binary covariate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>politenessPlot(
  df_polite,
  split = NULL,
  split_levels = NULL,
  split_name = NULL,
  split_cols = c("firebrick", "navy"),
  top_title = "",
  drop_blank = 0.05,
  middle_out = 0.5,
  features = NULL,
  ordered = FALSE,
  CI = 0.68
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="politenessPlot_+3A_df_polite">df_polite</code></td>
<td>
<p>a data.frame with politeness features calculated from a document set, as output by <code><a href="#topic+politeness">politeness</a></code>.</p>
</td></tr>
<tr><td><code id="politenessPlot_+3A_split">split</code></td>
<td>
<p>a vector of covariate values. must have a length equal to the number of documents included in <code>df_polite</code>. No NA values allowed.</p>
</td></tr>
<tr><td><code id="politenessPlot_+3A_split_levels">split_levels</code></td>
<td>
<p>character vector of length 2 default NULL. Labels for covariate levels for legend. If NULL, this will be inferred from <code>split</code>.</p>
</td></tr>
<tr><td><code id="politenessPlot_+3A_split_name">split_name</code></td>
<td>
<p>character default NULL. Name of the covariate for legend.</p>
</td></tr>
<tr><td><code id="politenessPlot_+3A_split_cols">split_cols</code></td>
<td>
<p>character vector of length 2. Name of colors to use.</p>
</td></tr>
<tr><td><code id="politenessPlot_+3A_top_title">top_title</code></td>
<td>
<p>character default &quot;&quot;. Title of plot.</p>
</td></tr>
<tr><td><code id="politenessPlot_+3A_drop_blank">drop_blank</code></td>
<td>
<p>Features less prevalent than this in the sample value are excluded from the plot. To include all features, set to <code>0</code></p>
</td></tr>
<tr><td><code id="politenessPlot_+3A_middle_out">middle_out</code></td>
<td>
<p>Features less distinctive than this value (measured by p-value of t-test) are excluded. Defaults to 1 (i.e. include all).</p>
</td></tr>
<tr><td><code id="politenessPlot_+3A_features">features</code></td>
<td>
<p>character vector of feature names. If NULL all will be included.</p>
</td></tr>
<tr><td><code id="politenessPlot_+3A_ordered">ordered</code></td>
<td>
<p>logical should features be ordered according to features param? default is FALSE.</p>
</td></tr>
<tr><td><code id="politenessPlot_+3A_ci">CI</code></td>
<td>
<p>Coverage of error bars. Defaults to 0.68 (i.e. standard error).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Length of <code>split</code> must be the same as number of rows of <code>df_polite</code>. Typically <code>split</code> should be a two-category variable. However, if a continuous covariate is given, then the top and bottom terciles of that distribution are treated as the two categories (while dropping data from the middle tercile).
</p>


<h3>Value</h3>

<p>a ggplot of the prevalence of politeness features, conditional on <code>split</code>. Features are sorted by variance-weighted log odds ratio.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("phone_offers")

polite.data&lt;-politeness(phone_offers$message, parser="none", drop_blank=FALSE)

politeness::politenessPlot(polite.data,
                           split=phone_offers$condition,
                           split_levels = c("Tough","Warm"),
                           split_name = "Condition",
                           top_title = "Average Feature Counts")


politeness::politenessPlot(polite.data,
                           split=phone_offers$condition,
                           split_levels = c("Tough","Warm"),
                           split_name = "Condition",
                           top_title = "Average Feature Counts",
                           features=c("Positive.Emotion","Hedges","Negation"))


polite.data&lt;-politeness(phone_offers$message, parser="none", metric="binary", drop_blank=FALSE)

politeness::politenessPlot(polite.data,
                           split=phone_offers$condition,
                           split_levels = c("Tough","Warm"),
                           split_name = "Condition",
                           top_title = "Binary Feature Use")

</code></pre>

<hr>
<h2 id='politenessProjection'>Politeness projection</h2><span id='topic+politenessProjection'></span>

<h3>Description</h3>

<p>Training and projecting a regression model using politeness features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>politenessProjection(
  df_polite_train,
  covar = NULL,
  df_polite_test = NULL,
  classifier = c("glmnet", "mnir"),
  cv_folds = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="politenessProjection_+3A_df_polite_train">df_polite_train</code></td>
<td>
<p>a data.frame with politeness features as outputed by <code><a href="#topic+politeness">politeness</a></code> used to train model.</p>
</td></tr>
<tr><td><code id="politenessProjection_+3A_covar">covar</code></td>
<td>
<p>a vector of politeness labels, or other covariate.</p>
</td></tr>
<tr><td><code id="politenessProjection_+3A_df_polite_test">df_polite_test</code></td>
<td>
<p>optional data.frame with politeness features as outputed by <code><a href="#topic+politeness">politeness</a></code> used for out-of-sample fitting. Must have same feature set as polite_train (most easily achieved by setting <code>dropblank=FALSE</code> in both calls to <code>politeness</code>).</p>
</td></tr>
<tr><td><code id="politenessProjection_+3A_classifier">classifier</code></td>
<td>
<p>name of classification algorithm. Defaults to &quot;glmnet&quot; (see <code>glmnet</code>) but &quot;mnir&quot; (see <code>mnlm</code>) is also available.</p>
</td></tr>
<tr><td><code id="politenessProjection_+3A_cv_folds">cv_folds</code></td>
<td>
<p>Number of outer folds for projection of training data. Default is NULL (i.e. no nested cross-validation). However, positive values are highly recommended (e.g. 10) for in-sample accuracy estimation.</p>
</td></tr>
<tr><td><code id="politenessProjection_+3A_...">...</code></td>
<td>
<p>additional parameters to be passed to the classification algorithm.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>List:
</p>

<ul>
<li><p> train_proj projection of politeness model within training set.
</p>
</li>
<li><p> test_proj projection of politeness model onto test set (i.e. out-of-sample).
</p>
</li>
<li><p> train_coef coefficients from the trained model.
</p>
</li></ul>



<h3>Value</h3>

<p>List of df_polite_train and df_polite_test with projection. See details.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("phone_offers")
data("bowl_offers")

polite.data&lt;-politeness(phone_offers$message, parser="none",drop_blank=FALSE)

polite.holdout&lt;-politeness(bowl_offers$message, parser="none",drop_blank=FALSE)

project&lt;-politenessProjection(polite.data,
                              phone_offers$condition,
                              polite.holdout)

# Difference in average politeness across conditions in the new sample.

mean(project$test_proj[bowl_offers$condition==1])
mean(project$test_proj[bowl_offers$condition==0])

</code></pre>

<hr>
<h2 id='receptive_model'>A pre-trained model for detecting conversational receptiveness.
Estimated with glmnet using annotated data from a previous paper.
Primarily for use within the receptiveness() function.</h2><span id='topic+receptive_model'></span>

<h3>Description</h3>

<p>A pre-trained model for detecting conversational receptiveness.
Estimated with glmnet using annotated data from a previous paper.
Primarily for use within the receptiveness() function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>receptive_model
</code></pre>


<h3>Format</h3>

<p>A fitted glmnet model
</p>


<h3>Source</h3>

<p>Minson, J., Yeomans, M., Collins, H. &amp; Dorison, C.
</p>
<p>&quot;Conversational Receptiveness: Improving Engagement with Opposing Views&quot;
</p>

<hr>
<h2 id='receptive_names'>This is the list of variables to be extracted for the receptiveness algorithm
For internal use only, within the receptiveness() function.</h2><span id='topic+receptive_names'></span>

<h3>Description</h3>

<p>This is the list of variables to be extracted for the receptiveness algorithm
For internal use only, within the receptiveness() function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>receptive_names
</code></pre>


<h3>Format</h3>

<p>Character vector containing variable names
</p>


<h3>Source</h3>

<p>Minson, J., Yeomans, M., Collins, H. &amp; Dorison, C.
</p>
<p>&quot;Conversational Receptiveness: Improving Engagement with Opposing Views&quot;
</p>

<hr>
<h2 id='receptive_polite'>Pre-Trained Receptiveness Data</h2><span id='topic+receptive_polite'></span>

<h3>Description</h3>

<p>A dataset to train a model for detecting conversational receptiveness.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>receptive_polite
</code></pre>


<h3>Format</h3>

<p>Pre-calculated politeness features for the receptive_train dataset
</p>

<hr>
<h2 id='receptive_train'>Pre-Trained Receptiveness Data</h2><span id='topic+receptive_train'></span>

<h3>Description</h3>

<p>A dataset to train a model for detecting conversational receptiveness.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>receptive_train
</code></pre>


<h3>Format</h3>

<p>A data frame with 2860 rows and 2 variables:
</p>

<dl>
<dt>text</dt><dd><p>character written response about policy disagreement</p>
</dd>
<dt>receptive</dt><dd><p>numeric standardized average of annotator ratings for &quot;receptiveness&quot;</p>
</dd>
</dl>

<p>Primarily for use within the receptiveness() function. The data was compiled from
Studies 1 and 4 of the original paper, as well as an unpublished study with a
very similar design, in which text responses were rated by disagreeing others.
</p>


<h3>Source</h3>

<p>Yeomans, M., Minson, J., Collins, H., Chen, F. &amp; Gino, F. (2020).
</p>
<p>&quot;Conversational Receptiveness: Improving Engagement with Opposing Views&quot;
</p>
<p><a href="https://osf.io/2n59b/">https://osf.io/2n59b/</a>
</p>

<hr>
<h2 id='receptiveness'>Conversational Receptiveness</h2><span id='topic+receptiveness'></span>

<h3>Description</h3>

<p>Pre-trained model to detect conversational receptiveness
</p>


<h3>Usage</h3>

<pre><code class='language-R'>receptiveness(texts, num_mc_cores = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="receptiveness_+3A_texts">texts</code></td>
<td>
<p>character A vector of texts, each of which will be tallied for politeness features.</p>
</td></tr>
<tr><td><code id="receptiveness_+3A_num_mc_cores">num_mc_cores</code></td>
<td>
<p>integer Number of cores for parallelization.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a wrapper around a pre-trained model of &quot;conversational receptiveness&quot;.
The model trained from Study 1 of that paper can be applied to new text with a single function.
This model requires grammar parsing via SpaCy. Please see <code><a href="spacyr.html#topic+spacyr">spacyr</a></code> for details on installation.
</p>


<h3>Value</h3>

<p>a vector with receptiveness scores
</p>


<h3>References</h3>

<p>Yeomans, M., Minson, J., Collins, H., Chen, F. &amp; Gino, F. (2020). Conversational Receptiveness: Improving Engagement with Opposing Views. OBHDP.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

## Not run: 
data("phone_offers")

receptiveness(phone_offers$message)


## End(Not run)

</code></pre>

<hr>
<h2 id='slogodds'>Variance-Weighted Log Odds</h2><span id='topic+slogodds'></span>

<h3>Description</h3>

<p>background function to load
</p>


<h3>Usage</h3>

<pre><code class='language-R'>slogodds(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="slogodds_+3A_x">x</code></td>
<td>
<p>prevalence in one sample</p>
</td></tr>
<tr><td><code id="slogodds_+3A_y">y</code></td>
<td>
<p>prevalence in another sample</p>
</td></tr>
</table>


<h3>Value</h3>

<p>variance-weighted log odds ratio of prevalence across samples
</p>

<hr>
<h2 id='spacyParser'>Spacy Parser</h2><span id='topic+spacyParser'></span>

<h3>Description</h3>

<p>Return POS tags from natural language.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spacyParser(txt)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spacyParser_+3A_txt">txt</code></td>
<td>
<p>a character vector of texts.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of compiled POS-tagged items.
</p>

<hr>
<h2 id='textcounter'>Text Counter</h2><span id='topic+textcounter'></span>

<h3>Description</h3>

<p>Counts total prevalence of a set of items in each of a set of texts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textcounter(
  counted,
  texts,
  words = FALSE,
  fixed = TRUE,
  start = FALSE,
  num_mc_cores = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textcounter_+3A_counted">counted</code></td>
<td>
<p>character vector of items to search for in the texts.</p>
</td></tr>
<tr><td><code id="textcounter_+3A_texts">texts</code></td>
<td>
<p>character vector of to-be-searched text.</p>
</td></tr>
<tr><td><code id="textcounter_+3A_words">words</code></td>
<td>
<p>logical. Default FALSE. Does <code>counted</code> contain words, or sequences of chracters?</p>
</td></tr>
<tr><td><code id="textcounter_+3A_fixed">fixed</code></td>
<td>
<p>logical. Default TRUE. Use literal characters instead of regular expressions?</p>
</td></tr>
<tr><td><code id="textcounter_+3A_start">start</code></td>
<td>
<p>logical. Default FALSE. Does <code>counted</code> only look at the start of a sentence?</p>
</td></tr>
<tr><td><code id="textcounter_+3A_num_mc_cores">num_mc_cores</code></td>
<td>
<p>integer Number of cores for parallelization. Default is 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector as long as <code>texts</code> indicating total frequencies of <code>counted</code> items.
</p>

<hr>
<h2 id='uk2us'>UK to US Conversion dictionary</h2><span id='topic+uk2us'></span>

<h3>Description</h3>

<p>For internal use only. This dataset contains a quanteda dictionary for converting UK words to US words. The models in this package were all trained on US English.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>uk2us
</code></pre>


<h3>Format</h3>

<p>A quanteda dictionary with named entries. Names are the US version, and entries are the UK version.
</p>


<h3>Source</h3>

<p>Borrowed from the quanteda.dictionaries package on github (from user kbenoit)
</p>

<hr>
<h2 id='usWords'>UK to US conversion</h2><span id='topic+usWords'></span>

<h3>Description</h3>

<p>background function to load.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>usWords(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="usWords_+3A_text">text</code></td>
<td>
<p>character Vector of strings to convert to US spelling.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>character Vector of Americanized strings.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
