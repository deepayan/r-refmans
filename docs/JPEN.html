<!DOCTYPE html><html><head><title>Help for package JPEN</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {JPEN}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#f.K.fold'><p> Subset the data into K fold, training and test data.</p></a></li>
<li><a href='#jpen'><p>JPEN Estimate of covariance matrix</p></a></li>
<li><a href='#JPEN-package'>
<p>Covariance and Inverse Covariance Matrix Estimation Using Joint Penalty</p></a></li>
<li><a href='#jpen.inv'><p>JPEN estimate of inverse cov matrix</p></a></li>
<li><a href='#jpen.inv.tune'><p>Tuning parameter Selection for inverse covariance matrix estimation based on minimization of Gaussian log-likelihood.</p></a></li>
<li><a href='#jpen.tune'><p>Tuning parameter selection based on minimization of 5 fold mean square error.</p></a></li>
<li><a href='#lamvec'><p>returns a vector of values of lambda for given value of gamma</p></a></li>
<li><a href='#tr'><p>Trace of matrix</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Covariance and Inverse Covariance Matrix Estimation Using Joint
Penalty</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2015-08-20</td>
</tr>
<tr>
<td>Author:</td>
<td>Ashwini Maurya </td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ashwini Maurya &lt;mauryaas@msu.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>A Joint PENalty Estimation of Covariance and Inverse Covariance Matrices.</td>
</tr>
<tr>
<td>Depends:</td>
<td>mvtnorm(&ge; 1.0-2), stats(&ge; 2.15.0),</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2015-09-06 23:34:02 UTC; STT User</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2015-09-16 10:05:02</td>
</tr>
</table>
<hr>
<h2 id='f.K.fold'> Subset the data into K fold, training and test data.
</h2><span id='topic+f.K.fold'></span>

<h3>Description</h3>

<p>K-fold subsetting.</p>


<h3>Usage</h3>

<pre><code class='language-R'>f.K.fold(Nobs, K = 5)</code></pre>


<h3>Arguments</h3>

  <table>
<tr><td><code id="f.K.fold_+3A_nobs">Nobs</code></td>
<td>
<p>n is number of observations</p>
</td></tr>
<tr><td><code id="f.K.fold_+3A_k">K</code></td>
<td>
<p>K is number of folds, typically 5 fold.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>K-fold subset of observations into training and test data. </p>


<h3>Value</h3>

<p>Returns the index for K-fold training and test data subsets.</p>


<h3>Author(s)</h3>

<p>Ashwini Maurya, Email: mauryaas@msu.edu
</p>


<h3>References</h3>

<p>A Well Conditioned and Sparse Estimate of Covariance and Inverse Covariance Matrix Using Joint Penalty. Submitted.
http://arxiv.org/pdf/1412.7907v2.pdf
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n=100;K=5;cv=f.K.fold(n,K);
</code></pre>

<hr>
<h2 id='jpen'>JPEN Estimate of covariance matrix
</h2><span id='topic+jpen'></span>

<h3>Description</h3>

<p>Estimate of covariance Matrix using Joint Penalty Method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jpen(S,  gam, lam=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jpen_+3A_s">S</code></td>
<td>
<p> Sample covariance matrix.
</p>
</td></tr>
<tr><td><code id="jpen_+3A_gam">gam</code></td>
<td>
<p>Tuning parameter gamma. gam is non-negative.
</p>
</td></tr>
<tr><td><code id="jpen_+3A_lam">lam</code></td>
<td>
<p>Tuning parameter lambda. lam is non-negative.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function returns an estimate of covariance matrix using Joint Penalty method.</p>


<h3>Value</h3>

<p>Estimate of Covariance Matrix.</p>


<h3>Author(s)</h3>

<p>Ashwini Maurya, Email: mauryaas@msu.edu
</p>


<h3>References</h3>

<p>A Well Conditioned and Sparse Estimate of Covariance and Inverse Covariance Matrix Using Joint Penalty. Submitted.
http://arxiv.org/pdf/1412.7907v2.pdf
</p>


<h3>See Also</h3>

<p>jpen.tune, jpen.inv</p>


<h3>Examples</h3>

<pre><code class='language-R'>p=10;n=100;
Sig=diag(p);
y=rmvnorm(n,mean=rep(0,p),sigma=Sig);
gam=1.0;S=var(y);
lam=2/p;
Sighat=jpen(S,gam,lam);
</code></pre>

<hr>
<h2 id='JPEN-package'>
Covariance and Inverse Covariance Matrix Estimation Using Joint Penalty
</h2><span id='topic+JPEN-package'></span><span id='topic+JPEN'></span>

<h3>Description</h3>

<p>A Joint PENalty Estimation of Covariance and Inverse Covariance Matrices.</p>


<h3>Details</h3>

<p>The DESCRIPTION file:
</p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> JPEN</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Covariance and Inverse Covariance Matrix Estimation Using Joint Penalty</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2015-08-20</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Ashwini Maurya </td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Ashwini Maurya &lt;mauryaas@msu.edu&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> A Joint PENalty Estimation of Covariance and Inverse Covariance Matrices.</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> mvtnorm(&gt;= 1.0-2), stats(&gt;= 2.15.0),</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Index of help topics:
</p>
<pre>
JPEN-package            Covariance and Inverse Covariance Matrix
                        Estimation Using Joint Penalty
f.K.fold                Subset the data into K fold, training and test
                        data.
jpen                    JPEN Estimate of covariance matrix
jpen.inv                JPEN estimate of inverse cov matrix
jpen.inv.tune           Tuning parameter Selection for inverse
                        covariance matrix estimation based on
                        minimization of Gaussian log-likelihood.
jpen.tune               Tuning parameter selection based on
                        minimization of 5 fold mean square error.
lamvec                  returns a vector of values of lambda for given
                        value of gamma
tr                      Trace of matrix
</pre>


<h3>Author(s)</h3>

<p>Ashwini Maurya, Email: mauryaas@msu.edu.
Ashwini Maurya 
Maintainer: Ashwini Maurya &lt;mauryaas@msu.edu&gt;
</p>


<h3>References</h3>

<p>A Well Conditioned and Sparse Estimate of Covariance and Inverse Covariance Matrix Using Joint Penalty. Submitted.
http://arxiv.org/pdf/1412.7907v2.pdf
</p>


<h3>See Also</h3>

<p>jpen,jpen.inv</p>

<hr>
<h2 id='jpen.inv'>JPEN estimate of inverse cov matrix</h2><span id='topic+jpen.inv'></span>

<h3>Description</h3>

<p>A well conditioned and sparse estimate of inverse covariance matrix using Joint Penalty</p>


<h3>Usage</h3>

<pre><code class='language-R'>jpen.inv(S, gam, lam=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jpen.inv_+3A_s">S</code></td>
<td>
<p>Sample cov matrix or a positive definite estimate based on covariance matrix.</p>
</td></tr>
<tr><td><code id="jpen.inv_+3A_gam">gam</code></td>
<td>
<p>gam is tuning parameter for eigenvalues shrinkage.</p>
</td></tr>
<tr><td><code id="jpen.inv_+3A_lam">lam</code></td>
<td>
<p>lam is tuning parameter for sparsity.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimates a well conditioned and sparse inverse covariance matrix using Joint Penalty. If input matrix is singular or nearly singular, a JPEN estimate of covariance matrix is used in place of S.</p>


<h3>Value</h3>

<p>Returns a well conditioned and positive inverse covariance matrix.
</p>


<h3>Author(s)</h3>

<p>Ashwini Maurya, Email: mauryaas@msu.edu.
</p>


<h3>References</h3>

<p>A Well Conditioned and Sparse Estimate of Covariance and Inverse Covariance Matrix Using Joint Penalty. Submitted.
http://arxiv.org/pdf/1412.7907v2.pdf
</p>


<h3>See Also</h3>

<p>jpen,jpen.tune,jpen.inv.tune</p>


<h3>Examples</h3>

<pre><code class='language-R'>p=10;n=100;
Sig=diag(p);
y=rmvnorm(n,mean=rep(0,p),sigma=Sig);
S=var(y);
gam=1.0;
lam=2*max(abs(S[col(S)!=row(S)]))/p;
Omghat=jpen.inv(var(y),gam,lam);
</code></pre>

<hr>
<h2 id='jpen.inv.tune'>Tuning parameter Selection for inverse covariance matrix estimation based on minimization of Gaussian log-likelihood.</h2><span id='topic+jpen.inv.tune'></span>

<h3>Description</h3>

<p> Returns optimal values of tuning parameters lambda and gamma</p>


<h3>Usage</h3>

<pre><code class='language-R'>jpen.inv.tune(Ytr, gama, lambda=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jpen.inv.tune_+3A_ytr">Ytr</code></td>
<td>

<p>Ytr is matrix of observations.</p>
</td></tr>
<tr><td><code id="jpen.inv.tune_+3A_gama">gama</code></td>
<td>

<p>A  vector of gamma values.</p>
</td></tr>
<tr><td><code id="jpen.inv.tune_+3A_lambda">lambda</code></td>
<td>

<p>Optional vector of values of lambda. If optional, the algorithm automatically calculates 10 values of lambda for each gamma and finds the optimal values of 
(lambda,gamma) that minimizes the negative of Gaussian likelihood function using K-fold cross validation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns the value of optimal tuning parameters. The function uses  K-fold cross validation to select the best tuning parameter from among a set of of values of lambda and gamma.
</p>


<h3>Value</h3>

<p>Returns the optimal values of lambda and gamma.
</p>


<h3>Author(s)</h3>

<p>Ashwini Maurya, Email: mauryaas@msu.edu.
</p>


<h3>References</h3>

<p>A Well Conditioned and Sparse Estimate of Covariance and Inverse Covariance Matrix Using Joint Penalty. Submitted.
http://arxiv.org/pdf/1412.7907v2.pdf
</p>


<h3>See Also</h3>

<p>jpen</p>


<h3>Examples</h3>

<pre><code class='language-R'>p=10;n=100;
Sig=diag(p);
y=rmvnorm(n,mean=rep(0,p),sigma=Sig);
gama=c(0.5,1.0);
opt=jpen.inv.tune(var(y),gama);
</code></pre>

<hr>
<h2 id='jpen.tune'>Tuning parameter selection based on minimization of 5 fold mean square error. 
</h2><span id='topic+jpen.tune'></span>

<h3>Description</h3>

<p> Returns optimal values of tuning parameters lambda and gamma which minimizes the K-fold crossvalidation error on
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jpen.tune(Ytr, gama, lambda=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jpen.tune_+3A_ytr">Ytr</code></td>
<td>

<p>Ytr is matrix of observations.
</p>
</td></tr>
<tr><td><code id="jpen.tune_+3A_gama">gama</code></td>
<td>

<p>gama is vector of gamma values. gamma is non-negative.</p>
</td></tr> 
<tr><td><code id="jpen.tune_+3A_lambda">lambda</code></td>
<td>

<p>lambda is vector of lambda values. lambda is non-negative. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns the value of optimal tuning parameters. The function uses  K-fold cross validation to select the best tuning parameter from among a set of of values of lambda and gamma.
</p>


<h3>Value</h3>

<p>Returns the optimal values of lambda and gamma.
</p>


<h3>Author(s)</h3>

<p>Ashwini Maurya, Email: mauryaas@msu.edu.
</p>


<h3>References</h3>

<p>A Well Conditioned and Sparse Estimate of Covariance and Inverse Covariance Matrix Using Joint Penalty. Submitted.
http://arxiv.org/pdf/1412.7907v2.pdf
</p>


<h3>See Also</h3>

<p>jpen</p>


<h3>Examples</h3>

<pre><code class='language-R'>p=10;n=100;
Sig=diag(p);
y=rmvnorm(n,mean=rep(0,p),sigma=Sig);
gama=c(0.5,1.0);
opt=jpen.tune(Ytr=y,gama);
</code></pre>

<hr>
<h2 id='lamvec'>returns a vector of values of lambda for given value of gamma </h2><span id='topic+lamvec'></span>

<h3>Description</h3>

<p>returns 10 values of lambda for each gamma.</p>


<h3>Usage</h3>

<pre><code class='language-R'>lamvec(c, gam, p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lamvec_+3A_c">c</code></td>
<td>

<p>c is absolute maximum of off-diagonal entries of sample covariance matrix.
</p>
</td></tr>
<tr><td><code id="lamvec_+3A_gam">gam</code></td>
<td>

<p>gamma is a non-negative constant. 
</p>
</td></tr>
<tr><td><code id="lamvec_+3A_p">p</code></td>
<td>

<p>p is number of rows/columns of matrix.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The lamvec function retuns a 10 values of lambda for each value of gamma. A larger value of lambda yields sparse estimate but need not be positive definite, however at least one combination of (lambda, gamma) will yield a positive definite solution. If two different combination of (lambda, gamma) yeilds same cross validation error, a larger values of lambda will be selected which results in more sparse solution.</p>


<h3>Value</h3>

<p>A vector of values of lambda for each combination of gama. By choosing c as the maximum of off-diagonal elements of sample covariance matrix, the largest value of lambda yields an estimate which diagonal matrix with elements proportional to the diagonal elements of sample covariance matrix.</p>


<h3>Author(s)</h3>

<p> Ashwini Maurya, Email: mauryaas@msu.edu
</p>


<h3>References</h3>

<p>A Well Conditioned and Sparse Estimate of Covariance and Inverse Covariance Matrix Using Joint Penalty. Submitted.
http://arxiv.org/pdf/1412.7907v2.pdf
</p>


<h3>See Also</h3>

<p>jpen, jpen.inv, jpen.tune, jpen.tune.inv</p>


<h3>Examples</h3>

<pre><code class='language-R'>p=10;n=100;Sig=diag(p);
y=rmvnorm(n,mean=rep(0,p),sigma=Sig);
gam=c(0.5);
S=var(y);
c=max(abs(S[row(S)!=col(S)]));
lambda=lamvec(c,gam,p);
</code></pre>

<hr>
<h2 id='tr'>Trace of matrix
</h2><span id='topic+tr'></span>

<h3>Description</h3>

<p>Returns the trace of a matrix</p>


<h3>Usage</h3>

<pre><code class='language-R'>tr(A)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tr_+3A_a">A</code></td>
<td>

<p>A is the input matrix.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns the trace (sum of diagonal elements )of input matrix).</p>


<h3>Value</h3>

<p>Trace of input matrix.</p>


<h3>Author(s)</h3>

<p>Ashwini Maurya, Email: mauryaas@msu.edu
</p>


<h3>References</h3>

<p>A Well Conditioned and Sparse Estimate of Covariance and Inverse Covariance Matrix Using Joint Penalty. Submitted.
http://arxiv.org/pdf/1412.7907v2.pdf
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p=10;n=100;Sig=diag(p);
y=rmvnorm(n,mean=rep(0,p),sigma=Sig);
S=var(y);
tr(S);
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
