<!DOCTYPE html><html><head><title>Help for package logicDT</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {logicDT}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bestBoostingIter'><p>Get the best number of boosting iterations</p></a></li>
<li><a href='#calcAUC'><p>Fast computation of the AUC w.r.t. to the ROC</p></a></li>
<li><a href='#calcBrier'><p>Calculate the Brier score</p></a></li>
<li><a href='#calcDev'><p>Calculate the deviance</p></a></li>
<li><a href='#calcMis'><p>Calculate the misclassification rate</p></a></li>
<li><a href='#calcMSE'><p>Calculate the MSE</p></a></li>
<li><a href='#calcNCE'><p>Calculate the normalized cross entropy</p></a></li>
<li><a href='#calcNRMSE'><p>Calculate the NRMSE</p></a></li>
<li><a href='#cooling.schedule'><p>Define the cooling schedule for simulated annealing</p></a></li>
<li><a href='#cv.prune'><p>Optimal pruning via cross-validation</p></a></li>
<li><a href='#fit4plModel'><p>Fitting 4pL models</p></a></li>
<li><a href='#fitLinearBoostingModel'><p>Linear models based on boosted models</p></a></li>
<li><a href='#fitLinearLogicModel'><p>Linear models based on logic terms</p></a></li>
<li><a href='#fitLinearModel'><p>Fitting linear models</p></a></li>
<li><a href='#get.ideal.penalty'><p>Tuning the LASSO regularization parameter</p></a></li>
<li><a href='#getDesignMatrix'><p>Design matrix for the set of conjunctions</p></a></li>
<li><a href='#gxe.test'><p>Gene-environment interaction test</p></a></li>
<li><a href='#gxe.test.boosting'><p>Gene-environment (GxE) interaction test based on boosted linear models</p></a></li>
<li><a href='#importance.test.boosting'><p>Term importance test based on boosted linear models</p></a></li>
<li><a href='#logicDT'><p>Fitting logic decision trees</p></a></li>
<li><a href='#logicDT.bagging'><p>Fitting bagged logicDT models</p></a></li>
<li><a href='#logicDT.boosting'><p>Fitting boosted logicDT models</p></a></li>
<li><a href='#partial.predict'><p>Partial prediction for boosted models</p></a></li>
<li><a href='#plot.logicDT'><p>Plot a logic decision tree</p></a></li>
<li><a href='#plot.vim'><p>Plot calculated VIMs</p></a></li>
<li><a href='#predict.4pl'><p>Prediction for 4pL models</p></a></li>
<li><a href='#predict.linear'><p>Prediction for linear models</p></a></li>
<li><a href='#predict.linear.logic'><p>Prediction for <code>linear.logic</code> models</p></a></li>
<li><a href='#predict.logicDT'><p>Prediction for logicDT models</p></a></li>
<li><a href='#prune'><p>Post-pruning using a fixed complexity penalty</p></a></li>
<li><a href='#prune.path'><p>Pruning path of a logic decision tree</p></a></li>
<li><a href='#refitTrees'><p>Refit the logic decision trees</p></a></li>
<li><a href='#splitSNPs'><p>Split biallelic SNPs into binary variables</p></a></li>
<li><a href='#tree.control'><p>Control parameters for fitting decision trees</p></a></li>
<li><a href='#vim'><p>Variable Importance Measures (VIMs)</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Identifying Interactions Between Binary Predictors</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.4</td>
</tr>
<tr>
<td>Description:</td>
<td>A statistical learning method that tries to find the best set
  of predictors and interactions between predictors for modeling binary or
  quantitative response data in a decision tree. Several search algorithms
  and ensembling techniques are implemented allowing for finetuning the
  method to the specific problem. Interactions with quantitative
  covariables can be properly taken into account by fitting local
  regression models. Moreover, a variable importance measure for assessing
  marginal and interaction effects is provided. Implements the
  procedures proposed by Lau et al. (2024, &lt;<a href="https://doi.org/10.1007%2Fs10994-023-06488-6">doi:10.1007/s10994-023-06488-6</a>&gt;).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>glmnet, graphics, stats, utils</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-19 12:47:24 UTC; Lau</td>
</tr>
<tr>
<td>Author:</td>
<td>Michael Lau <a href="https://orcid.org/0000-0002-5327-8351"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michael Lau &lt;michael.lau@hhu.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-19 13:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bestBoostingIter'>Get the best number of boosting iterations</h2><span id='topic+bestBoostingIter'></span>

<h3>Description</h3>

<p>This function can be used to compute the ideal number of boosting iterations
for the fitted <code>logic.boosted</code> model using independent validation data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bestBoostingIter(model, X, y, Z = NULL, consec.iter = 5, scoring_rule = "auc")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bestBoostingIter_+3A_model">model</code></td>
<td>
<p>Fitted <code>logic.boosted</code> model</p>
</td></tr>
<tr><td><code id="bestBoostingIter_+3A_x">X</code></td>
<td>
<p>Matrix or data frame of binary validation input data.
This object should correspond to the binary matrix for fitting the model.</p>
</td></tr>
<tr><td><code id="bestBoostingIter_+3A_y">y</code></td>
<td>
<p>Validation response vector. 0-1 coding for binary outcomes.</p>
</td></tr>
<tr><td><code id="bestBoostingIter_+3A_z">Z</code></td>
<td>
<p>Optional quantitative covariables supplied as a matrix or
data frame. Only used (and required) if the model was fitted using them.</p>
</td></tr>
<tr><td><code id="bestBoostingIter_+3A_consec.iter">consec.iter</code></td>
<td>
<p>Number of consecutive boosting iterations that do not
increase the validation performance for determining the ideal number of
iterations</p>
</td></tr>
<tr><td><code id="bestBoostingIter_+3A_scoring_rule">scoring_rule</code></td>
<td>
<p>Scoring rule computing the validation performance.
This can either be <code>"auc"</code> for the area under the receiver
operating characteristic curve (default for binary reponses),
<code>"deviance"</code> for the deviance, &quot;nce&quot; for the normalized cross entropy
or <code>"brier"</code> for the Brier score.
For regression purposes, the MSE (mean squared error) is
automatically chosen.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the model performance (on the validation data) cannot be increased for
<code>consec.iter</code> consecutive boosting iterations, the last iteration
which increased the validation performance induces the ideal number of
boosting iterations.
</p>


<h3>Value</h3>

<p>The ideal number of boosting iterations
</p>

<hr>
<h2 id='calcAUC'>Fast computation of the AUC w.r.t. to the ROC</h2><span id='topic+calcAUC'></span>

<h3>Description</h3>

<p>This function computes the area under the receiver operating
characteristic curve.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calcAUC(preds, y, fast = TRUE, sorted = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calcAUC_+3A_preds">preds</code></td>
<td>
<p>Numeric vector of predicted scores</p>
</td></tr>
<tr><td><code id="calcAUC_+3A_y">y</code></td>
<td>
<p>True binary outcomes coded as 0 or 1. Must be an integer
vector.</p>
</td></tr>
<tr><td><code id="calcAUC_+3A_fast">fast</code></td>
<td>
<p>Shall the computation be as fast as possible?</p>
</td></tr>
<tr><td><code id="calcAUC_+3A_sorted">sorted</code></td>
<td>
<p>Are the predicted scores already sorted
increasingly? If so, this can slightly speed up the computation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The AUC between 0 and 1
</p>

<hr>
<h2 id='calcBrier'>Calculate the Brier score</h2><span id='topic+calcBrier'></span>

<h3>Description</h3>

<p>Computation of the Brier score, i.e., the mean squared
error for risk estimates in a binary classification
problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calcBrier(preds, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calcBrier_+3A_preds">preds</code></td>
<td>
<p>Numeric vector of predictions</p>
</td></tr>
<tr><td><code id="calcBrier_+3A_y">y</code></td>
<td>
<p>True outcomes</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The Brier score
</p>

<hr>
<h2 id='calcDev'>Calculate the deviance</h2><span id='topic+calcDev'></span>

<h3>Description</h3>

<p>Computation of the deviance, i.e., two times the negative
log likelihood for risk estimates in a binary classification
problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calcDev(preds, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calcDev_+3A_preds">preds</code></td>
<td>
<p>Numeric vector of predictions</p>
</td></tr>
<tr><td><code id="calcDev_+3A_y">y</code></td>
<td>
<p>True outcomes</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The deviance
</p>

<hr>
<h2 id='calcMis'>Calculate the misclassification rate</h2><span id='topic+calcMis'></span>

<h3>Description</h3>

<p>Computation of the misclassification rate for risk
estimates in a binary classification problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calcMis(preds, y, cutoff = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calcMis_+3A_preds">preds</code></td>
<td>
<p>Numeric vector of predictions</p>
</td></tr>
<tr><td><code id="calcMis_+3A_y">y</code></td>
<td>
<p>True outcomes</p>
</td></tr>
<tr><td><code id="calcMis_+3A_cutoff">cutoff</code></td>
<td>
<p>Classification cutoff. By default,
scores above 50
otherwise.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The misclassification rate
</p>

<hr>
<h2 id='calcMSE'>Calculate the MSE</h2><span id='topic+calcMSE'></span>

<h3>Description</h3>

<p>Computation of the mean squared error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calcMSE(preds, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calcMSE_+3A_preds">preds</code></td>
<td>
<p>Numeric vector of predictions</p>
</td></tr>
<tr><td><code id="calcMSE_+3A_y">y</code></td>
<td>
<p>True outcomes</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The MSE
</p>

<hr>
<h2 id='calcNCE'>Calculate the normalized cross entropy</h2><span id='topic+calcNCE'></span>

<h3>Description</h3>

<p>This function computes the normalized cross entropy (NCE)
which is given by
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{NCE} = \frac{\frac{1}{N} \sum_{i=1}^{N}
y_i \cdot \log(p_i) + (1-y_i) \cdot \log(1-p_i)}{
p \cdot \log(p) + (1-p) \cdot \log(1-p)}</code>
</p>

<p>where (for <code class="reqn">i \in \lbrace 1,\ldots,N \rbrace</code>)
<code class="reqn">y_i \in \lbrace 0,1 \rbrace</code> are the true classes,
<code class="reqn">p_i</code> are the risk/probability predictions and
<code class="reqn">p = \frac{1}{N} \sum_{i=1}^{N} y_i</code> is total unrestricted
empirical risk estimate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calcNCE(preds, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calcNCE_+3A_preds">preds</code></td>
<td>
<p>Numeric vector of risk estimates</p>
</td></tr>
<tr><td><code id="calcNCE_+3A_y">y</code></td>
<td>
<p>Vector of true binary outcomes</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Smaller values towards zero are generally prefered.
A NCE of one or above would indicate that the used model
yields comparable or worse predictions than the naive mean
model.
</p>


<h3>Value</h3>

<p>The normalized cross entropy
</p>


<h3>References</h3>


<ul>
<li><p> He, X., Pan, J., Jin, O., Xu, T., Liu, B., Xu, T.,
Shi, Y., Atallah, A., Herbrich, R., Bowers, S., Candela, J. Q.
(2014). Practical Lessons from Predicting Clicks on Ads at
Facebook. Proceedings of the Eighth International Workshop on
Data Mining for Online Advertising 1-9.
doi: <a href="https://doi.org/10.1145/2648584.2648589">10.1145/2648584.2648589</a>
</p>
</li></ul>


<hr>
<h2 id='calcNRMSE'>Calculate the NRMSE</h2><span id='topic+calcNRMSE'></span>

<h3>Description</h3>

<p>Computation of the normalized root mean squared error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calcNRMSE(preds, y, type = "sd")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calcNRMSE_+3A_preds">preds</code></td>
<td>
<p>Numeric vector of predictions</p>
</td></tr>
<tr><td><code id="calcNRMSE_+3A_y">y</code></td>
<td>
<p>True outcomes</p>
</td></tr>
<tr><td><code id="calcNRMSE_+3A_type">type</code></td>
<td>
<p><code>"sd"</code> uses the standard deviation of <code>y</code> for
normalization. <code>"range"</code> uses the whole span of <code>y</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The NRMSE
</p>

<hr>
<h2 id='cooling.schedule'>Define the cooling schedule for simulated annealing</h2><span id='topic+cooling.schedule'></span>

<h3>Description</h3>

<p>This function should be used to configure a search
with simulated annealing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cooling.schedule(
  type = "adaptive",
  start_temp = 1,
  end_temp = -1,
  lambda = 0.01,
  total_iter = 2e+05,
  markov_iter = 1000,
  markov_leave_frac = 1,
  acc_type = "probabilistic",
  frozen_def = "acc",
  frozen_acc_frac = 0.01,
  frozen_markov_count = 5,
  frozen_markov_mode = "total",
  start_temp_steps = 10000,
  start_acc_ratio = 0.95,
  auto_start_temp = TRUE,
  remember_models = TRUE,
  print_iter = 1000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cooling.schedule_+3A_type">type</code></td>
<td>
<p>Type of cooling schedule. <code>"adaptive"</code>
(default) or <code>"geometric"</code></p>
</td></tr>
<tr><td><code id="cooling.schedule_+3A_start_temp">start_temp</code></td>
<td>
<p>Start temperature on a log10 scale.
Only used if <code>auto_start_temp = FALSE</code>.</p>
</td></tr>
<tr><td><code id="cooling.schedule_+3A_end_temp">end_temp</code></td>
<td>
<p>End temperature on a log10 scale.
Only used if <code>type = "geometric"</code>.</p>
</td></tr>
<tr><td><code id="cooling.schedule_+3A_lambda">lambda</code></td>
<td>
<p>Cooling parameter for the adaptive
schedule. Values between 0.01 and 0.1 are recommended
such that in total, several hundred thousand
iterations are performed. Lower values lead to a more
fine search with more iterations while higher values
lead to a more coarse search with less total
iterations.</p>
</td></tr>
<tr><td><code id="cooling.schedule_+3A_total_iter">total_iter</code></td>
<td>
<p>Total number of iterations that
should be performed. Only used for the geometric
cooling schedule.</p>
</td></tr>
<tr><td><code id="cooling.schedule_+3A_markov_iter">markov_iter</code></td>
<td>
<p>Number of iterations for each
Markov chain. The standard value does not need to be
tuned, since the temperature steps and number of
iterations per chain act complementary to each other,
i.e., less iterations can be compensated by smaller
temperature steps.</p>
</td></tr>
<tr><td><code id="cooling.schedule_+3A_markov_leave_frac">markov_leave_frac</code></td>
<td>
<p>Fraction of accepted moves
leading to an early temperature reduction. This is
primarily used at (too) high temperatures lowering
the temperature if essentially a random walk is
performed. E.g., a value of 0.5 together with
<code>markov_iter = 1000</code> means that the chain will
be left if <code class="reqn">0.5 \cdot 1000 = 500</code> states were
accepted in a single chain.</p>
</td></tr>
<tr><td><code id="cooling.schedule_+3A_acc_type">acc_type</code></td>
<td>
<p>Type of acceptance function. The
standard <code>"probabilistic"</code> uses the conventional
function
<code class="reqn">\exp((\mathrm{Score}_\mathrm{old} -
\mathrm{Score}_\mathrm{new})/t)</code>
for calculating the acceptance probability.
<code>"deterministic"</code> accepts the new state, if and only
if
<code class="reqn">\mathrm{Score}_\mathrm{new} -
\mathrm{Score}_\mathrm{old} &lt; t</code>.</p>
</td></tr>
<tr><td><code id="cooling.schedule_+3A_frozen_def">frozen_def</code></td>
<td>
<p>How to define a frozen chain.
<code>"acc"</code> means that if less than
<code class="reqn">\texttt{frozen\_acc\_frac} \cdot
\texttt{markov\_iter}</code> states with different scores
were accepted in a single chain, this chain is
marked as frozen. <code>"sd"</code> declares a chain as frozen if the
corresponding score standard deviation is zero.
Several frozen chains indicate that the search is finished.</p>
</td></tr>
<tr><td><code id="cooling.schedule_+3A_frozen_acc_frac">frozen_acc_frac</code></td>
<td>
<p>If <code>frozen_def = "acc"</code>, this parameter
determines the fraction of iterations that define a frozen chain.</p>
</td></tr>
<tr><td><code id="cooling.schedule_+3A_frozen_markov_count">frozen_markov_count</code></td>
<td>
<p>Number of frozen chains that
need to be observed for finishing the search.</p>
</td></tr>
<tr><td><code id="cooling.schedule_+3A_frozen_markov_mode">frozen_markov_mode</code></td>
<td>
<p>Do the frozen chains
have to occur consecutively (<code>"consecutive"</code>)
or is the total number of frozen chains
relevant (<code>"total"</code>)?</p>
</td></tr>
<tr><td><code id="cooling.schedule_+3A_start_temp_steps">start_temp_steps</code></td>
<td>
<p>Number of iterations that should be used for
estimating the ideal start temperature if <code>auto_start_temp =
TRUE</code> is set.</p>
</td></tr>
<tr><td><code id="cooling.schedule_+3A_start_acc_ratio">start_acc_ratio</code></td>
<td>
<p>Acceptance ratio that
should be achieved with the automatically
configured start temperature.</p>
</td></tr>
<tr><td><code id="cooling.schedule_+3A_auto_start_temp">auto_start_temp</code></td>
<td>
<p>Should the start
temperature be configured automatically?
<code>TRUE</code> or <code>FALSE</code></p>
</td></tr>
<tr><td><code id="cooling.schedule_+3A_remember_models">remember_models</code></td>
<td>
<p>Should already evaluated
models be saved in a 2-dimensional hash table
to prevent fitting the same trees multiple
times?</p>
</td></tr>
<tr><td><code id="cooling.schedule_+3A_print_iter">print_iter</code></td>
<td>
<p>Number of iterations after which
a progress report shall be printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>type = "adapative"</code> (default)
automatically choses the temperature steps by using the
standard deviation of the scores in a Markov chain
together with the current temperature to
evaluate if equilibrium is achieved. If the standard
deviation is small or the temperature is high,
equilibrium can be assumed leading
to a strong temperature reduction. Otherwise, the
temperature is only merely lowered.
The parameter <code>lambda</code> is essential to control
how fast the schedule will be executed and, thus,
how many total iterations will be performed.
</p>
<p><code>type = "geometric"</code> is the conventional
approach which requires more finetuning. Here,
temperatures are uniformly lowered on a log10 scale.
Thus, a start and an end temperature have to be
supplied.
</p>


<h3>Value</h3>

<p>An object of class <code>cooling.schedule</code>
which is a list of all necessary cooling parameters.
</p>

<hr>
<h2 id='cv.prune'>Optimal pruning via cross-validation</h2><span id='topic+cv.prune'></span>

<h3>Description</h3>

<p>Using a fitted <code><a href="#topic+logicDT">logicDT</a></code> model, its logic decision tree can be
optimally (post-)pruned utilizing k-fold cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.prune(
  model,
  nfolds = 10,
  scoring_rule = "deviance",
  choose = "1se",
  simplify = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.prune_+3A_model">model</code></td>
<td>
<p>A fitted <code>logicDT</code> model</p>
</td></tr>
<tr><td><code id="cv.prune_+3A_nfolds">nfolds</code></td>
<td>
<p>Number of cross-validation folds</p>
</td></tr>
<tr><td><code id="cv.prune_+3A_scoring_rule">scoring_rule</code></td>
<td>
<p>The scoring rule for evaluating the cross-validation
error and its standard error. For classification tasks, <code>"deviance"</code>
or <code>"Brier"</code> should be used.</p>
</td></tr>
<tr><td><code id="cv.prune_+3A_choose">choose</code></td>
<td>
<p>Model selection scheme. If the model that minimizes the
cross-validation error should be chosen, <code>choose = "min"</code> should be
set. Otherwise, <code>choose = "1se"</code> leads to simplest model in the range
of one standard error of the minimizing model.</p>
</td></tr>
<tr><td><code id="cv.prune_+3A_simplify">simplify</code></td>
<td>
<p>Should the pruned model be simplified with regard to the
input terms, i.e., should terms that are no longer in the tree contained
be removed from the model?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Similar to Breiman et al. (1984), we implement post-pruning by first
computing the optimal pruning path and then using cross-validation for
identifying the best generalizing model.
</p>
<p>In order to handle continuous covariables with fitted regression models in
each leaf, similar to the likelihood-ratio splitting criterion in
<code><a href="#topic+logicDT">logicDT</a></code>, we propose using the log-likelihood as the impurity
criterion in this case for computing the pruning path.
In particular, for each node <code class="reqn">t</code>, the weighted node impurity
<code class="reqn">p(t)i(t)</code> has to be calculated and the inequality
</p>
<p style="text-align: center;"><code class="reqn">\Delta i(s,t) := i(t) - p(t_L | t)i(t_L) - p(t_R | t)i(t_R) \geq 0</code>
</p>

<p>has to be fulfilled for each possible split <code class="reqn">s</code> splitting <code class="reqn">t</code> into
two subnodes <code class="reqn">t_L</code> and <code class="reqn">t_R</code>. Here, <code class="reqn">i(t)</code> describes the
impurity of a node <code class="reqn">t</code>, <code class="reqn">p(t)</code> the proportion of data points falling
into <code class="reqn">t</code>, and <code class="reqn">p(t' | t)</code> the proportion of data points falling
from <code class="reqn">t</code> into <code class="reqn">t'</code>.
Since the regression models are fitted using maximum likelihood, the
maximum likelihood criterion fulfills this property and can also be seen as
an extension of the entropy impurity criterion in the case of classification
or an extension of the MSE impurity criterion in the case of regression.
</p>
<p>The default model selection is done by choosing the most parsimonious model
that yields a cross-validation error in the range of
<code class="reqn">\mathrm{CV}_{\min} + \mathrm{SE}_{\min}</code>
for the minimal cross-validation error <code class="reqn">\mathrm{CV}_{\min}</code> and its
corresponding standard error <code class="reqn">\mathrm{SE}_{\min}</code>.
For a more robust standard error estimation, the scores are calculated per
training observation such that the AUC is no longer an appropriate choice
and the deviance or the Brier score should be used in the case of
classification.
</p>


<h3>Value</h3>

<p>A list containing
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>The new <code>logicDT</code> model containing the optimally
pruned tree</p>
</td></tr>
<tr><td><code>cv.res</code></td>
<td>
<p>A data frame containing the penalties, the
cross-validation scores and the corresponding standard errors</p>
</td></tr>
<tr><td><code>best.beta</code></td>
<td>
<p>The ideal penalty value</p>
</td></tr>
</table>


<h3>References</h3>


<ul>
<li><p> Breiman, L., Friedman, J., Stone, C. J. &amp; Olshen, R. A. (1984).
Classification and Regression Trees. CRC Press.
doi: <a href="https://doi.org/10.1201/9781315139470">10.1201/9781315139470</a>
</p>
</li></ul>


<hr>
<h2 id='fit4plModel'>Fitting 4pL models</h2><span id='topic+fit4plModel'></span>

<h3>Description</h3>

<p>Method for fitting four parameter logistic models.
In the fashion of this package, only binary and quantitative
outcomes are supported.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit4plModel(y, Z)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit4plModel_+3A_y">y</code></td>
<td>
<p>Response vector. 0-1 coding for binary outcomes,
otherwise conventional regression is performed.</p>
</td></tr>
<tr><td><code id="fit4plModel_+3A_z">Z</code></td>
<td>
<p>Numeric vector of (univariate) input samples.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>4pL models are non-linear regression models of the shape
</p>
<p style="text-align: center;"><code class="reqn">Y = f(x, b, c, d, e) + \varepsilon =
  c + \frac{d-c}{1+\exp(b \cdot (x-e))} + \varepsilon</code>
</p>

<p>with <code class="reqn">\varepsilon</code> being a random error term.
</p>


<h3>Value</h3>

<p>An object of class <code>"4pl"</code> which contains a numeric
vector of the fitted parameters b, c, d, and e.
</p>

<hr>
<h2 id='fitLinearBoostingModel'>Linear models based on boosted models</h2><span id='topic+fitLinearBoostingModel'></span>

<h3>Description</h3>

<p>This function uses a fitted <code>logic.boosted</code> model for fitting
a linear or logistic (depending on the type of outcome) regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fitLinearBoostingModel(model, n.iter, type = "standard", s = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitLinearBoostingModel_+3A_model">model</code></td>
<td>
<p>Fitted <code>logic.boosted</code> model</p>
</td></tr>
<tr><td><code id="fitLinearBoostingModel_+3A_n.iter">n.iter</code></td>
<td>
<p>Number of boosting iterations to be used</p>
</td></tr>
<tr><td><code id="fitLinearBoostingModel_+3A_type">type</code></td>
<td>
<p>Type of linear model to be fitted. Either <code>"standard"</code>
(without regularization), <code>"lasso"</code> (LASSO) or <code>"cv.lasso"</code>
(LASSO with cross-validation for automatically configuring the complexity
penalty).</p>
</td></tr>
<tr><td><code id="fitLinearBoostingModel_+3A_s">s</code></td>
<td>
<p>Regularization parameter. Only used if <code>type = "lasso"</code> is
set.</p>
</td></tr>
<tr><td><code id="fitLinearBoostingModel_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to <code>glmnet</code> or <code>cv.glmnet</code>
if the corresponding model type was chosen.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In this procedure, the logic terms are extracted from the individual
<code>logicDT</code> models and the set of unique terms are used as predictors
in a regression model. For incorporating a continuous covariable
the covariable itself as well as products of the covariable with the
extracted logic terms are included as predictors in the regression model.
</p>
<p>For more details on the possible types of linear models, see
<code><a href="#topic+fitLinearLogicModel">fitLinearLogicModel</a></code>.
</p>


<h3>Value</h3>

<p>A <code>linear.logic</code> model. This is a list containing
the logic terms used as predictors in the model and the fitted <code>glm</code>
model.
</p>

<hr>
<h2 id='fitLinearLogicModel'>Linear models based on logic terms</h2><span id='topic+fitLinearLogicModel'></span>

<h3>Description</h3>

<p>This function fits a linear or logistic regression model (based on the
type of outcome) using the supplied logic terms, e.g., <code>$disj</code> from
a fitted <code>logicDT</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fitLinearLogicModel(
  X,
  y,
  Z = NULL,
  disj,
  Z.interactions = TRUE,
  type = "standard",
  s = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitLinearLogicModel_+3A_x">X</code></td>
<td>
<p>Matrix or data frame of binary input data.
This object should correspond to the binary matrix for fitting the model.</p>
</td></tr>
<tr><td><code id="fitLinearLogicModel_+3A_y">y</code></td>
<td>
<p>Response vector. 0-1 coding for binary outcomes.</p>
</td></tr>
<tr><td><code id="fitLinearLogicModel_+3A_z">Z</code></td>
<td>
<p>Optional quantitative covariables supplied as a matrix or
data frame. Only used (and required) if the model was fitted using them.</p>
</td></tr>
<tr><td><code id="fitLinearLogicModel_+3A_disj">disj</code></td>
<td>
<p>Integer matrix of logic terms. As in <code><a href="#topic+logicDT">logicDT</a></code>,
each row corresponds to a term/conjunction. Negative values indicate
negations. The absolute values of an entry correspond to the predictor
index in <code>X</code>.</p>
</td></tr>
<tr><td><code id="fitLinearLogicModel_+3A_z.interactions">Z.interactions</code></td>
<td>
<p>Shall interactions with the continuous covariable
<code>Z</code> be taken into account by including products of the terms with
<code>Z</code>?</p>
</td></tr>
<tr><td><code id="fitLinearLogicModel_+3A_type">type</code></td>
<td>
<p>Type of linear model to be fitted. Either <code>"standard"</code>
(without regularization), <code>"lasso"</code> (LASSO) or <code>"cv.lasso"</code>
(LASSO with cross-validation for automatically configuring the complexity
penalty).</p>
</td></tr>
<tr><td><code id="fitLinearLogicModel_+3A_s">s</code></td>
<td>
<p>Regularization parameter. Only used if <code>type = "lasso"</code> is
set.</p>
</td></tr>
<tr><td><code id="fitLinearLogicModel_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to <code>glmnet</code> or <code>cv.glmnet</code>
if the corresponding model type was chosen.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For creating sparse final models, the LASSO can be used for shrinking
unnecessary term coefficients down to zero (<code>type = "lasso"</code>).
If the complexity penalty <code>s</code> shall be automatically tuned,
cross-validation can be employed (<code>type = "cv.lasso"</code>).
However, since other hyperparameters also have to be tuned when fitting
a linear boosting model such as the complexity penalty for restricting
the number of variables in the terms, manually tuning the LASSO penalty
together with the other hyperparameters is recommended.
For every hyperparameter setting of the boosting itself, the best
corresponding LASSO penalty <code>s</code> can be identified by, e.g., choosing
the <code>s</code> that minimizes the validation data error.
Thus, this hyperparameter does not have to be explicitly tuned via a grid
search but is induced by the setting of the other hyperparameters.
For finding the ideal value of <code>s</code> using independent validation data,
the function <code><a href="#topic+get.ideal.penalty">get.ideal.penalty</a></code> can be used.
</p>


<h3>Value</h3>

<p>A <code>linear.logic</code> model. This is a list containing
the logic terms used as predictors in the model and the fitted <code>glm</code>
model.
</p>


<h3>References</h3>


<ul>
<li><p> Tibshirani, R. (1996).
Regression Shrinkage and Selection via the Lasso. Journal of the Royal
Statistical Society. Series B (Methodological), 58(1), 267–288.
doi: <a href="https://doi.org/10.1111/j.2517-6161.1996.tb02080.x">10.1111/j.2517-6161.1996.tb02080.x</a>
</p>
</li>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of statistical software, 33(1), 1–22.
doi: <a href="https://doi.org/10.18637/jss.v033.i01">10.18637/jss.v033.i01</a>
</p>
</li></ul>


<hr>
<h2 id='fitLinearModel'>Fitting linear models</h2><span id='topic+fitLinearModel'></span>

<h3>Description</h3>

<p>Method for fitting linear models.
In the fashion of this package, only binary and quantitative
outcomes are supported.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fitLinearModel(y, Z, logistic = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitLinearModel_+3A_y">y</code></td>
<td>
<p>Response vector. 0-1 coding for binary outcomes,
otherwise conventional regression is performed.</p>
</td></tr>
<tr><td><code id="fitLinearModel_+3A_z">Z</code></td>
<td>
<p>Numeric vector of (univariate) input samples.</p>
</td></tr>
<tr><td><code id="fitLinearModel_+3A_logistic">logistic</code></td>
<td>
<p>Logical indicating whether, in the case of a binary
outcome, a logistic regression model should be fitted
(<code>TRUE</code>) or a LDA model should be fitted (<code>FALSE</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For binary outcomes, predictions are cut at 0 or 1 for generating
proper probability estimates.
</p>


<h3>Value</h3>

<p>An object of class <code>"linear"</code> which contains a numeric
vector of the fitted parameters b and c.
</p>

<hr>
<h2 id='get.ideal.penalty'>Tuning the LASSO regularization parameter</h2><span id='topic+get.ideal.penalty'></span>

<h3>Description</h3>

<p>This function takes a fitted <code>linear.logic</code> model and independent
validation data as input for finding the ideal LASSO complexity penalty
<code>s</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get.ideal.penalty(
  model,
  X,
  y,
  Z = NULL,
  scoring_rule = "deviance",
  choose = "min"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get.ideal.penalty_+3A_model">model</code></td>
<td>
<p>A fitted <code>linear.logic</code> model (i.e., a model created via
<code><a href="#topic+fitLinearLogicModel">fitLinearLogicModel</a></code> or <code><a href="#topic+fitLinearBoostingModel">fitLinearBoostingModel</a></code>)</p>
</td></tr>
<tr><td><code id="get.ideal.penalty_+3A_x">X</code></td>
<td>
<p>Matrix or data frame of binary input data.
This object should correspond to the binary matrix for fitting the model.</p>
</td></tr>
<tr><td><code id="get.ideal.penalty_+3A_y">y</code></td>
<td>
<p>Response vector. 0-1 coding for binary outcomes.</p>
</td></tr>
<tr><td><code id="get.ideal.penalty_+3A_z">Z</code></td>
<td>
<p>Optional quantitative covariables supplied as a matrix or
data frame. Only used (and required) if the model was fitted using them.</p>
</td></tr>
<tr><td><code id="get.ideal.penalty_+3A_scoring_rule">scoring_rule</code></td>
<td>
<p>The scoring rule for evaluating the validation
error and its standard error. For classification tasks, <code>"deviance"</code>
or <code>"Brier"</code> should be used.</p>
</td></tr>
<tr><td><code id="get.ideal.penalty_+3A_choose">choose</code></td>
<td>
<p>Model selection scheme. If the model that minimizes the
validation error should be chosen, <code>choose = "min"</code> should be
set. Otherwise, <code>choose = "1se"</code> leads to simplest model in the range
of one standard error of the minimizing model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing
</p>
<table>
<tr><td><code>val.res</code></td>
<td>
<p>A data frame containing the penalties, the
validation scores and the corresponding standard errors</p>
</td></tr>
<tr><td><code>best.s</code></td>
<td>
<p>The ideal penalty value</p>
</td></tr>
</table>

<hr>
<h2 id='getDesignMatrix'>Design matrix for the set of conjunctions</h2><span id='topic+getDesignMatrix'></span>

<h3>Description</h3>

<p>Transform the original predictor matrix X into the conjunction design matrix
which contains for each conjunction a corresponding column.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getDesignMatrix(X, disj)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getDesignMatrix_+3A_x">X</code></td>
<td>
<p>The original (binary) predictor matrix. This has to be of type
<code>integer</code>.</p>
</td></tr>
<tr><td><code id="getDesignMatrix_+3A_disj">disj</code></td>
<td>
<p>The conjunction matrix which can, e.g., be extracted from a
fitted <code>logicDT</code> model via <code>$disj</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The transformed design matrix.
</p>

<hr>
<h2 id='gxe.test'>Gene-environment interaction test</h2><span id='topic+gxe.test'></span>

<h3>Description</h3>

<p>Using a fitted <code>logicDT</code> model, a general GxE interaction
test can be performed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gxe.test(model, X, y, Z, perm.test = TRUE, n.perm = 10000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gxe.test_+3A_model">model</code></td>
<td>
<p>A fitted <code>logicDT</code> model with 4pL models in its leaves.</p>
</td></tr>
<tr><td><code id="gxe.test_+3A_x">X</code></td>
<td>
<p>Binary predictor data for testing the interaction effect.
This can be equal to the training data.</p>
</td></tr>
<tr><td><code id="gxe.test_+3A_y">y</code></td>
<td>
<p>Response vector for testing the interaction effect.
This can be equal to the training data.</p>
</td></tr>
<tr><td><code id="gxe.test_+3A_z">Z</code></td>
<td>
<p>Quantitative covariable for testing the interaction effect.
This can be equal to the training data.</p>
</td></tr>
<tr><td><code id="gxe.test_+3A_perm.test">perm.test</code></td>
<td>
<p>Should additionally permutation testing be performed?
Useful if likelihood ratio test asymptotics cannot be justified.</p>
</td></tr>
<tr><td><code id="gxe.test_+3A_n.perm">n.perm</code></td>
<td>
<p>Number of random permutations for permutation testing</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The testing is done by fitting one shared 4pL model
for all tree branches with different offsets, i.e., allowing main effects
of SNPs. This shared model is compared to the individual 4pL models fitted
in the <code><a href="#topic+logicDT">logicDT</a></code> procedure using a likelihood ratio test which
is asymptotically <code class="reqn">\chi^2</code> distributed. The degrees of freedom are
equal to the difference in model parameters.
For regression tasks, alternatively, a F-test can be utilized.
</p>
<p>The shared 4pL model is given by
</p>
<p style="text-align: center;"><code class="reqn">Y = \tilde{f}(x, z, b, c, d, e, \beta_1, \ldots, \beta_{G-1})
  + \varepsilon = c + \frac{d-c}{1+\exp(b \cdot (x-e))}
  + \sum_{g=1}^{G-1} \beta_g \cdot 1(z = g) + \varepsilon</code>
</p>

<p>with <code class="reqn">z \in \lbrace 1, \ldots, G \rbrace</code> being a grouping variable,
<code class="reqn">\beta_1, \ldots, \beta_{G-1}</code> being the offsets for the different
groups, and <code class="reqn">\varepsilon</code> being a random error term.
Note that the last group <code class="reqn">G</code> does not have an offset parameter, since
the model is calibrated such that the curve without any <code class="reqn">\beta</code>'s
fits to the last group.
</p>
<p>The likelihood ratio test statistic is given by
</p>
<p style="text-align: center;"><code class="reqn">\Lambda = -2(\ell_{\mathrm{shared}} - \ell_{\mathrm{full}})</code>
</p>

<p>for the log likelihoods of the shared and full 4pL models, respectively.
In the regression case, the test statistic can be calculated as
</p>
<p style="text-align: center;"><code class="reqn">\Lambda = N(\log(\mathrm{RSS}_{\mathrm{shared}}) -
  \log(\mathrm{RSS}_{\mathrm{full}}))</code>
</p>

<p>with <code class="reqn">\mathrm{RSS}</code> being the residual sum of squares for the
respective model.
</p>
<p>For regression tasks, the alternative F test statistic is given by
</p>
<p style="text-align: center;"><code class="reqn">f = \frac{\frac{1}{\mathrm{df}_1}(\mathrm{RSS}_{\mathrm{shared}} -
  \mathrm{RSS}_{\mathrm{full}})}
  {\frac{1}{\mathrm{df}_2} \mathrm{RSS}_{\mathrm{full}}}</code>
</p>

<p>with
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{df}_1 = \mathrm{Difference\ in\ the\ number\ of\ model\
parameters} = 3 \cdot n_{\mathrm{scenarios}} - 3,</code>
</p>

<p style="text-align: center;"><code class="reqn">\mathrm{df}_2 = \mathrm{Degrees\ of\ freedom\ of\ the\ full\ model}
= N - 4 \cdot n_{\mathrm{scenarios}},</code>
</p>

<p>and <code class="reqn">n_{\mathrm{scenarios}}</code> being the number of identified predictor
scenarios/groups by <code><a href="#topic+logicDT">logicDT</a></code>.
</p>
<p>Alternatively, if linear models were fitted in the supplied <code>logicDT</code>
model, shared linear models can be used to test for a GxE interaction.
For continuous outcomes, the shared linear model is given by
</p>
<p style="text-align: center;"><code class="reqn">Y = \tilde{f}(x, z, \alpha, \beta_1, \ldots, \beta_{G})
  + \varepsilon = \alpha \cdot x
  + \sum_{g=1}^{G} \beta_g \cdot 1(z = g) + \varepsilon.</code>
</p>

<p>For binary outcomes, LDA (linear discriminant analysis) models are fitted.
In contrast to the 4pL-based test for binary outcomes, varying offsets for
the individual groups are injected to the linear predictor instead of to
the probability (response) scale.
</p>
<p>If only few samples are available and the asymptotics of likelihood ratio
tests cannot be justified, alternatively, a permutation test approach
can be employed by setting <code>perm.test = TRUE</code> and specifying an
appropriate number of random permutations via <code>n.perm</code>.
For this approach, computed likelihoods of the shared and (paired) full
likelihood groups are randomly interchanged approximating the null
distribution of equal likelihoods. A p-value can be computed by determining
the fraction of more extreme null samples compared to the original
likelihood ratio test statistic, i.e., using the fraction of higher
likelihood ratios in the null distribution than the original likelihood
ratio.
</p>


<h3>Value</h3>

<p>A list containing
</p>
<table>
<tr><td><code>p.chisq</code></td>
<td>
<p>The p-value of the chi-squared test statistic.</p>
</td></tr>
<tr><td><code>p.f</code></td>
<td>
<p>The p-value of the F test statistic.</p>
</td></tr>
<tr><td><code>p.perm</code></td>
<td>
<p>The p-value of the optional permutation test.</p>
</td></tr>
<tr><td><code>ll.shared</code></td>
<td>
<p>Log likelihood of the shared parameters 4pL model.</p>
</td></tr>
<tr><td><code>ll.full</code></td>
<td>
<p>Log likelihood of the full <code>logicDT</code> model.</p>
</td></tr>
<tr><td><code>rss.shared</code></td>
<td>
<p>Residual sum of squares of the shared parameters
4pL model.</p>
</td></tr>
<tr><td><code>rss.full</code></td>
<td>
<p>Residual sum of squares of the full <code>logicDT</code>
model.</p>
</td></tr>
</table>

<hr>
<h2 id='gxe.test.boosting'>Gene-environment (GxE) interaction test based on boosted linear models</h2><span id='topic+gxe.test.boosting'></span>

<h3>Description</h3>

<p>This function takes a fitted <code>linear.logic</code> model and independent test
data as input for testing if there is a general GxE interaction.
This hypothesis test is based on a likelihood-ratio test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gxe.test.boosting(model, X, y, Z)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gxe.test.boosting_+3A_model">model</code></td>
<td>
<p>A fitted <code>linear.logic</code> model (i.e., a model created via
<code><a href="#topic+fitLinearLogicModel">fitLinearLogicModel</a></code> or <code><a href="#topic+fitLinearBoostingModel">fitLinearBoostingModel</a></code>)</p>
</td></tr>
<tr><td><code id="gxe.test.boosting_+3A_x">X</code></td>
<td>
<p>Matrix or data frame of binary input data.
This object should correspond to the binary matrix for fitting the model.</p>
</td></tr>
<tr><td><code id="gxe.test.boosting_+3A_y">y</code></td>
<td>
<p>Response vector. 0-1 coding for binary outcomes.</p>
</td></tr>
<tr><td><code id="gxe.test.boosting_+3A_z">Z</code></td>
<td>
<p>Quantitative covariable supplied as a matrix or data frame</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In detail, the null hypothesis
</p>
<p style="text-align: center;"><code class="reqn">H_0: \delta_1 = \ldots = \delta_B = 0</code>
</p>

<p>using the supplied linear model
</p>
<p style="text-align: center;"><code class="reqn">g(E[Y]) = \beta_0 + \sum_{i=1}^B \beta_i \cdot 1[C_i] + \delta_0 \cdot E
+ \sum_{i=1}^B \delta_i \cdot 1[C_i] \cdot E</code>
</p>

<p>is tested.
</p>


<h3>Value</h3>

<p>A list containing
</p>
<table>
<tr><td><code>Deviance</code></td>
<td>
<p>The deviance used for performing the
likelihood-ratio test</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>The p-value of the test</p>
</td></tr>
</table>

<hr>
<h2 id='importance.test.boosting'>Term importance test based on boosted linear models</h2><span id='topic+importance.test.boosting'></span>

<h3>Description</h3>

<p>This function takes a fitted <code>linear.logic</code> model and independent test
data as input for testing if the included terms are influential with respect
to the outcome.
This hypothesis test is based on a likelihood-ratio test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>importance.test.boosting(model, X, y, Z, Z.interactions = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="importance.test.boosting_+3A_model">model</code></td>
<td>
<p>A fitted <code>linear.logic</code> model (i.e., a model created via
<code><a href="#topic+fitLinearLogicModel">fitLinearLogicModel</a></code> or <code><a href="#topic+fitLinearBoostingModel">fitLinearBoostingModel</a></code>)</p>
</td></tr>
<tr><td><code id="importance.test.boosting_+3A_x">X</code></td>
<td>
<p>Matrix or data frame of binary input data.
This object should correspond to the binary matrix for fitting the model.</p>
</td></tr>
<tr><td><code id="importance.test.boosting_+3A_y">y</code></td>
<td>
<p>Response vector. 0-1 coding for binary outcomes.</p>
</td></tr>
<tr><td><code id="importance.test.boosting_+3A_z">Z</code></td>
<td>
<p>Optional quantitative covariables supplied as a matrix or
data frame. Only used (and required) if the model was fitted using them.</p>
</td></tr>
<tr><td><code id="importance.test.boosting_+3A_z.interactions">Z.interactions</code></td>
<td>
<p>A Boolean value determining whether interactions with
quantitative covaraible <code>Z</code> shall be taken into account</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In detail, the null hypotheses
</p>
<p style="text-align: center;"><code class="reqn">H_0: \beta_j = \delta_j = 0</code>
</p>

<p>using the linear model
</p>
<p style="text-align: center;"><code class="reqn">g(E[Y]) = \beta_0 + \sum_{i=1}^B \beta_i \cdot 1[C_i] + \delta_0 \cdot E
+ \sum_{i=1}^B \delta_i \cdot 1[C_i] \cdot E</code>
</p>

<p>are tested for each <code class="reqn">j \in \lbrace 1,\ldots,B \rbrace</code>
if <code>Z.interactions</code> is set to <code>TRUE</code>.
Otherwise, the null hypotheses
</p>
<p style="text-align: center;"><code class="reqn">H_0: \beta_j = 0</code>
</p>

<p>using the linear model
</p>
<p style="text-align: center;"><code class="reqn">g(E[Y]) = \beta_0 + \sum_{i=1}^B \beta_i \cdot 1[C_i] + \delta_0 \cdot E</code>
</p>

<p>are tested.
</p>


<h3>Value</h3>

<p>A data frame consisting of three columns,
</p>
<table>
<tr><td><code>var</code></td>
<td>
<p>The tested term,</p>
</td></tr>
<tr><td><code>vim</code></td>
<td>
<p>The associated variable importance, and</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>The corresponding p-value for testing if the term
is influential.</p>
</td></tr>
</table>

<hr>
<h2 id='logicDT'>Fitting logic decision trees</h2><span id='topic+logicDT'></span><span id='topic+logicDT.default'></span><span id='topic+logicDT.formula'></span>

<h3>Description</h3>

<p>Main function for fitting logicDT models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
logicDT(
  X,
  y,
  max_vars = 3,
  max_conj = 3,
  Z = NULL,
  search_algo = "sa",
  cooling_schedule = cooling.schedule(),
  scoring_rule = "auc",
  tree_control = tree.control(),
  gamma = 0,
  simplify = "vars",
  val_method = "none",
  val_frac = 0.5,
  val_reps = 10,
  allow_conj_removal = TRUE,
  conjsize = 1,
  randomize_greedy = FALSE,
  greedy_mod = TRUE,
  greedy_rem = FALSE,
  max_gen = 10000,
  gp_sigma = 0.15,
  gp_fs_interval = 1,
  ...
)

## S3 method for class 'formula'
logicDT(formula, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logicDT_+3A_x">X</code></td>
<td>
<p>Matrix or data frame of binary predictors coded as 0 or 1.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_y">y</code></td>
<td>
<p>Response vector. 0-1 coding for binary responses.
Otherwise, a regression task is assumed.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_max_vars">max_vars</code></td>
<td>
<p>Maximum number of predictors in the set of predictors.
For the set <code class="reqn">[X_1 \land X_2^c, X_1 \land X_3]</code>, this parameter
is equal to 4.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_max_conj">max_conj</code></td>
<td>
<p>Maximum number of conjunctions/input variables for the
decision trees. For the set <code class="reqn">[X_1 \land X_2^c, X_1 \land X_3]</code>, this
parameter is equal to 2.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_z">Z</code></td>
<td>
<p>Optional matrix or data frame of quantitative/continuous
covariables. Multiple covariables allowed for splitting the trees.
If leaf regression models (such as four parameter logistic models) shall
be fitted, only the first given covariable is used.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_search_algo">search_algo</code></td>
<td>
<p>Search algorithm for guiding the global search.
This can either be <code>"sa"</code> for simulated annealing, <code>"greedy"</code>
for a greedy search or <code>"gp"</code> for genetic programming.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_cooling_schedule">cooling_schedule</code></td>
<td>
<p>Cooling schedule parameters if simulated
annealing is used. The required object should be created via
the function <code><a href="#topic+cooling.schedule">cooling.schedule</a></code>.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_scoring_rule">scoring_rule</code></td>
<td>
<p>Scoring rule for guiding the global search.
This can either be <code>"auc"</code> for the area under the receiver
operating characteristic curve (default for binary reponses),
<code>"deviance"</code> for the deviance, <code>"nce"</code> for the normalized cross
entropy or <code>"brier"</code> for the Brier score.
For regression purposes, the MSE (mean squared error) is
automatically chosen.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_tree_control">tree_control</code></td>
<td>
<p>Parameters controlling the fitting of
decision trees. This should be configured via the
function <code><a href="#topic+tree.control">tree.control</a></code>.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_gamma">gamma</code></td>
<td>
<p>Complexity penalty added to the score.
If <code class="reqn">\texttt{gamma} &gt; 0</code> is given, <code class="reqn">\texttt{gamma} \cdot ||m||_0</code>
is added to the score with <code class="reqn">||m||_0</code> being the total number of
variables contained in the current model <code class="reqn">m</code>.
The main purpose of this penalty is for fitting logicDT stumps
in conjunction with boosting. For regular logicDT models or bagged
logicDT models, instead, the model complexity parameters <code>max_vars</code>
and <code>max_conj</code> should be tuned.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_simplify">simplify</code></td>
<td>
<p>Should the final fitted model be simplified?
This means, that unnecessary terms as a whole (<code>"conj"</code>) will
be removed if they cannot improve the score.
<code>simplify = "vars"</code> additionally tries to prune individual
conjunctions by removing unnecessary variables in those.
<code>simplify = "none"</code> will not modify the final model.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_val_method">val_method</code></td>
<td>
<p>Inner validation method. <code>"rv"</code> leads to a
repeated validation where <code>val_reps</code> times the original data
set is divided into <code class="reqn">\texttt{val\_frac} \cdot 100\%</code> validation
data and <code class="reqn">(1-\texttt{val\_frac}) \cdot 100\%</code> training data.
<code>"bootstrap"</code> draws bootstrap samples and uses the out-of-bag
data as validation data. <code>"cv"</code> employs cross-validation with
<code>val_reps</code> folds.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_val_frac">val_frac</code></td>
<td>
<p>Only used if <code>val_method = "rv"</code>. See description
of <code>val_method</code>.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_val_reps">val_reps</code></td>
<td>
<p>Number of inner validation partitionings.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_allow_conj_removal">allow_conj_removal</code></td>
<td>
<p>Should it be allowed to remove
complete terms/conjunctions in the search?
If a model with the specified exact number of terms is desired,
this should be set to <code>FALSE</code>. If extensive hyperparameter
optimizations are feasible, <code>allow_conj_removal = FALSE</code> with a
proper search over <code>max_vars</code> and <code>max_conj</code> is advised for
fitting single models. For bagging or boosting with a greedy search,
<code>allow_conj_removal = TRUE</code> together with a small number for
<code>max_vars = max_conj</code> is recommended, e.g., 2 or 3.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_conjsize">conjsize</code></td>
<td>
<p>The minimum of training samples that have to
belong to a conjunction. This parameters prevents including
unnecessarily complex conjunctions that rarely occur.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_randomize_greedy">randomize_greedy</code></td>
<td>
<p>Should the greedy search be randomized
by only considering <code class="reqn">\sqrt{\mathrm{Neighbour\ states}}</code>
neighbors at each iteration, similar to random forests.
Speeds up the greedy search but can lead to inferior results.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_greedy_mod">greedy_mod</code></td>
<td>
<p>Should modifications of conjunctions be
considered in a greedy search?
<code>greedy_mod = FALSE</code> speeds up the greedy search but can
lead to inferior results.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_greedy_rem">greedy_rem</code></td>
<td>
<p>Should the removal of conjunctions be
considered in a greedy search?
<code>greedy_rem = FALSE</code> speeds up the greedy search but can
lead to inferior results.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_max_gen">max_gen</code></td>
<td>
<p>Maximum number of generations for genetic
programming.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_gp_sigma">gp_sigma</code></td>
<td>
<p>Parameter <code class="reqn">\sigma</code> for fitness sharing in
genetic programming. Very small values (e.g., 0.001) are
recommended leading to only penalizing models which yield
the exact same score.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_gp_fs_interval">gp_fs_interval</code></td>
<td>
<p>Interval for fitness sharing in
genetic programming. The fitness calculation can be
computationally expensive if many models exist in one
generation. <code>gp_fs_interval = 10</code> leads to performing
fitness sharing only every 10th generation.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code><a href="#topic+logicDT.default">logicDT.default</a></code></p>
</td></tr>
<tr><td><code id="logicDT_+3A_formula">formula</code></td>
<td>
<p>An object of type <code>formula</code> describing the
model to be fitted.</p>
</td></tr>
<tr><td><code id="logicDT_+3A_data">data</code></td>
<td>
<p>A data frame containing the data for the corresponding
<code>formula</code> object. Must also contain quantitative covariables
if they should be included as well.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>logicDT is a method for finding response-associated interactions between
binary predictors. A global search for the best set of predictors and
interactions between predictors is performed trying to find the global
optimal decision trees. On the one hand, this can be seen as a variable
selection. On the other hand, Boolean conjunctions between binary predictors
can be identified as impactful which is particularly useful if the
corresponding marginal effects are negligible due to the greedy fashion of
choosing splits in decision trees.
</p>
<p>Three search algorithms are implemented:
</p>

<ul>
<li><p> Simulated annealing. An exhaustive stochastic optimization procedure.
Recommended for single models (without [outer] bagging or boosting).
</p>
</li>
<li><p> Greedy search. A very fast search always looking for the best
possible improvement. Recommended for ensemble models.
</p>
</li>
<li><p> Genetic programming. A more or less intensive search holding
several competetive models at each generation. Niche method
which is only recommended if multiple (simple) models do
explain the variation in the response.
</p>
</li></ul>

<p>Furthermore, the option of a so-called &quot;inner validation&quot; is available.
Here, the search is guided using several train-validation-splits and
the average of the validation performance. This approach is
computationally expensive but can lead to more robust single models.
</p>
<p>For minimizing the computation time, two-dimensional hash tables
are used saving evaluated models. This is irrelevant for the greedy
search but can heavily improve the fitting times when employing
a search with simulated annealing or genetic programming, especially
when choosing an inner validation.
</p>


<h3>Value</h3>

<p>An object of class <code>logicDT</code>. This is a list
containing
</p>
<table>
<tr><td><code>disj</code></td>
<td>
<p>A matrix of the identified set of predictors
and conjunctions of predictors. Each row corresponds to one term.
Each entry corresponds to the column index in <code>X</code>.
Negative values indicate negations. Missing values mean that the
term does not contain any more variables.</p>
</td></tr>
<tr><td><code>real_disj</code></td>
<td>
<p>Human readable form of <code>disj</code>.
Here, variable names are directly depicted.</p>
</td></tr>
<tr><td><code>score</code></td>
<td>
<p>Score of the best model. Smaller values
are prefered.</p>
</td></tr>
<tr><td><code>pet</code></td>
<td>
<p>Decision tree fitted on the best set of
input terms. This is a list containing the pointer to the
<code>C</code> representation of the tree and <code>R</code> representations of the
tree structure such as the splits and predictions.</p>
</td></tr>
<tr><td><code>ensemble</code></td>
<td>
<p>List of decision trees. Only relevant
if inner validation was used.</p>
</td></tr>
<tr><td><code>total_iter</code></td>
<td>
<p>The total number of search
iterations, i.e., tested configurations by fitting a tree
(ensemble) and evaluating it.</p>
</td></tr>
<tr><td><code>prevented_evals</code></td>
<td>
<p>The number of prevented tree
fittings by using the two-dimensional hash table.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>Supplied parameters of the functional call
to <code><a href="#topic+logicDT">logicDT</a></code>.</p>
</td></tr>
</table>


<h3>Saving and Loading</h3>

<p>logicDT models can be saved and loaded using <code>save(...)</code> and
<code>load(...)</code>. The internal <code>C</code> structures will not be saved
but rebuilt from the <code>R</code> representations if necessary.
</p>


<h3>References</h3>


<ul>
<li><p> Lau, M., Schikowski, T. &amp; Schwender, H. (2024).
logicDT: A procedure for identifying response-associated
interactions between binary predictors. Machine Learning 113(2):933–992.
doi: <a href="https://doi.org/10.1007/s10994-023-06488-6">10.1007/s10994-023-06488-6</a>
</p>
</li>
<li><p> Breiman, L., Friedman, J., Stone, C. J. &amp; Olshen, R. A. (1984).
Classification and Regression Trees. CRC Press.
doi: <a href="https://doi.org/10.1201/9781315139470">10.1201/9781315139470</a>
</p>
</li>
<li><p> Kirkpatrick, S., Gelatt C. D. &amp; Vecchi M. P. (1983).
Optimization by Simulated Annealing. Science 220(4598):671–680.
doi: <a href="https://doi.org/10.1126/science.220.4598.671">10.1126/science.220.4598.671</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Generate toy data
set.seed(123)
maf &lt;- 0.25
n.snps &lt;- 50
N &lt;- 2000
X &lt;- matrix(sample(0:2, n.snps * N, replace = TRUE,
                   prob = c((1-maf)^2, 1-(1-maf)^2-maf^2, maf^2)),
            ncol = n.snps)
colnames(X) &lt;- paste("SNP", 1:n.snps, sep="")
X &lt;- splitSNPs(X)
Z &lt;- matrix(rnorm(N, 20, 10), ncol = 1)
colnames(Z) &lt;- "E"
Z[Z &lt; 0] &lt;- 0
y &lt;- -0.75 + log(2) * (X[,"SNP1D"] != 0) +
  log(4) * Z/20 * (X[,"SNP2D"] != 0 &amp; X[,"SNP3D"] == 0) +
  rnorm(N, 0, 1)


# Fit and evaluate single logicDT model
model &lt;- logicDT(X[1:(N/2),], y[1:(N/2)],
                 Z = Z[1:(N/2),,drop=FALSE],
                 max_vars = 3, max_conj = 2,
                 search_algo = "sa",
                 tree_control = tree.control(
                   nodesize = floor(0.05 * nrow(X)/2)
                 ),
                 simplify = "vars",
                 allow_conj_removal = FALSE,
                 conjsize = floor(0.05 * nrow(X)/2))
calcNRMSE(predict(model, X[(N/2+1):N,],
                  Z = Z[(N/2+1):N,,drop=FALSE]), y[(N/2+1):N])
plot(model)
print(model)

# Fit and evaluate bagged logicDT model
model.bagged &lt;- logicDT.bagging(X[1:(N/2),], y[1:(N/2)],
                                Z = Z[1:(N/2),,drop=FALSE],
                                bagging.iter = 50,
                                max_vars = 3, max_conj = 3,
                                search_algo = "greedy",
                                tree_control = tree.control(
                                  nodesize = floor(0.05 * nrow(X)/2)
                                ),
                                simplify = "vars",
                                conjsize = floor(0.05 * nrow(X)/2))
calcNRMSE(predict(model.bagged, X[(N/2+1):N,],
                  Z = Z[(N/2+1):N,,drop=FALSE]), y[(N/2+1):N])
print(model.bagged)

# Fit and evaluate boosted logicDT model
model.boosted &lt;- logicDT.boosting(X[1:(N/2),], y[1:(N/2)],
                                  Z = Z[1:(N/2),,drop=FALSE],
                                  boosting.iter = 50,
                                  learning.rate = 0.01,
                                  subsample.frac = 0.75,
                                  replace = FALSE,
                                  max_vars = 3, max_conj = 3,
                                  search_algo = "greedy",
                                  tree_control = tree.control(
                                    nodesize = floor(0.05 * nrow(X)/2)
                                  ),
                                  simplify = "vars",
                                  conjsize = floor(0.05 * nrow(X)/2))
calcNRMSE(predict(model.boosted, X[(N/2+1):N,],
                  Z = Z[(N/2+1):N,,drop=FALSE]), y[(N/2+1):N])
print(model.boosted)

# Calculate VIMs (variable importance measures)
vims &lt;- vim(model.bagged)
plot(vims)
print(vims)

# Single greedy model
model &lt;- logicDT(X[1:(N/2),], y[1:(N/2)],
                 Z = Z[1:(N/2),,drop=FALSE],
                 max_vars = 3, max_conj = 2,
                 search_algo = "greedy",
                 tree_control = tree.control(
                   nodesize = floor(0.05 * nrow(X)/2)
                 ),
                 simplify = "vars",
                 allow_conj_removal = FALSE,
                 conjsize = floor(0.05 * nrow(X)/2))
calcNRMSE(predict(model, X[(N/2+1):N,],
                  Z = Z[(N/2+1):N,,drop=FALSE]), y[(N/2+1):N])
plot(model)
print(model)
</code></pre>

<hr>
<h2 id='logicDT.bagging'>Fitting bagged logicDT models</h2><span id='topic+logicDT.bagging'></span><span id='topic+logicDT.bagging.default'></span><span id='topic+logicDT.bagging.formula'></span>

<h3>Description</h3>

<p>Function for fitting bagged logicDT models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
logicDT.bagging(X, y, Z = NULL, bagging.iter = 500, ...)

## S3 method for class 'formula'
logicDT.bagging(formula, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logicDT.bagging_+3A_x">X</code></td>
<td>
<p>Matrix or data frame of binary predictors coded as 0 or 1.</p>
</td></tr>
<tr><td><code id="logicDT.bagging_+3A_y">y</code></td>
<td>
<p>Response vector. 0-1 coding for binary responses.
Otherwise, a regression task is assumed.</p>
</td></tr>
<tr><td><code id="logicDT.bagging_+3A_z">Z</code></td>
<td>
<p>Optional matrix or data frame of quantitative/continuous
covariables. Multiple covariables allowed for splitting the trees.
If leaf regression models (such as four parameter logistic models) shall
be fitted, only the first given covariable is used.</p>
</td></tr>
<tr><td><code id="logicDT.bagging_+3A_bagging.iter">bagging.iter</code></td>
<td>
<p>Number of bagging iterations</p>
</td></tr>
<tr><td><code id="logicDT.bagging_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code>logicDT</code></p>
</td></tr>
<tr><td><code id="logicDT.bagging_+3A_formula">formula</code></td>
<td>
<p>An object of type <code>formula</code> describing the
model to be fitted.</p>
</td></tr>
<tr><td><code id="logicDT.bagging_+3A_data">data</code></td>
<td>
<p>A data frame containing the data for the corresponding
<code>formula</code> object. Must also contain quantitative covariables
if they should be included as well.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Details on single logicDT models can be found in <code><a href="#topic+logicDT">logicDT</a></code>.
</p>


<h3>Value</h3>

<p>An object of class <code>logic.bagged</code>. This is a list
containing
</p>
<table>
<tr><td><code>models</code></td>
<td>
<p>A list of fitted <code>logicDT</code> models</p>
</td></tr>
<tr><td><code>bags</code></td>
<td>
<p>A list of observation indices which were
used to train each model</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>Supplied parameters of the functional call
to <code><a href="#topic+logicDT.bagging">logicDT.bagging</a></code>.</p>
</td></tr>
</table>

<hr>
<h2 id='logicDT.boosting'>Fitting boosted logicDT models</h2><span id='topic+logicDT.boosting'></span><span id='topic+logicDT.boosting.default'></span><span id='topic+logicDT.boosting.formula'></span>

<h3>Description</h3>

<p>Function for fitting gradient boosted logicDT models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
logicDT.boosting(
  X,
  y,
  Z = NULL,
  boosting.iter = 500,
  learning.rate = 0.01,
  subsample.frac = 1,
  replace = TRUE,
  line.search = "min",
  ...
)

## S3 method for class 'formula'
logicDT.boosting(formula, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logicDT.boosting_+3A_x">X</code></td>
<td>
<p>Matrix or data frame of binary predictors coded as 0 or 1.</p>
</td></tr>
<tr><td><code id="logicDT.boosting_+3A_y">y</code></td>
<td>
<p>Response vector. 0-1 coding for binary responses.
Otherwise, a regression task is assumed.</p>
</td></tr>
<tr><td><code id="logicDT.boosting_+3A_z">Z</code></td>
<td>
<p>Optional matrix or data frame of quantitative/continuous
covariables. Multiple covariables allowed for splitting the trees.
If leaf regression models (such as four parameter logistic models) shall
be fitted, only the first given covariable is used.</p>
</td></tr>
<tr><td><code id="logicDT.boosting_+3A_boosting.iter">boosting.iter</code></td>
<td>
<p>Number of boosting iterations</p>
</td></tr>
<tr><td><code id="logicDT.boosting_+3A_learning.rate">learning.rate</code></td>
<td>
<p>Learning rate for boosted models.
Values between 0.001 and 0.1 are recommended.</p>
</td></tr>
<tr><td><code id="logicDT.boosting_+3A_subsample.frac">subsample.frac</code></td>
<td>
<p>Subsample fraction for each
boosting iteration. E.g., 0.5 means that are random draw
of 50
is used in each iteration.</p>
</td></tr>
<tr><td><code id="logicDT.boosting_+3A_replace">replace</code></td>
<td>
<p>Should the random draws with subsample.frac
in boosted models be performed with or without
replacement? <code>TRUE</code> or <code>FALSE</code></p>
</td></tr>
<tr><td><code id="logicDT.boosting_+3A_line.search">line.search</code></td>
<td>
<p>Type of line search for gradient boosting.
<code>"min"</code> performs a real minimization while <code>"binary"</code> performs
a loose binary search for a boosting coefficient that
just reduces the score.</p>
</td></tr>
<tr><td><code id="logicDT.boosting_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code>logicDT</code></p>
</td></tr>
<tr><td><code id="logicDT.boosting_+3A_formula">formula</code></td>
<td>
<p>An object of type <code>formula</code> describing the
model to be fitted.</p>
</td></tr>
<tr><td><code id="logicDT.boosting_+3A_data">data</code></td>
<td>
<p>A data frame containing the data for the corresponding
<code>formula</code> object. Must also contain quantitative covariables
if they should be included as well.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Details on single logicDT models can be found in <code><a href="#topic+logicDT">logicDT</a></code>.
</p>


<h3>Value</h3>

<p>An object of class <code>logic.boosted</code>. This is a list
containing
</p>
<table>
<tr><td><code>models</code></td>
<td>
<p>A list of fitted <code>logicDT</code> models</p>
</td></tr>
<tr><td><code>rho</code></td>
<td>
<p>A vector of boosting coefficient corresponding
to each model</p>
</td></tr>
<tr><td><code>initialModel</code></td>
<td>
<p>Initial model which is usually the
observed mean</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>Supplied parameters of the functional call
to <code><a href="#topic+logicDT.boosting">logicDT.boosting</a></code>.</p>
</td></tr>
</table>


<h3>References</h3>


<ul>
<li><p> Lau, M., Schikowski, T. &amp; Schwender, H. (2024).
logicDT: A procedure for identifying response-associated
interactions between binary predictors. Machine Learning 113(2):933–992.
doi: <a href="https://doi.org/10.1007/s10994-023-06488-6">10.1007/s10994-023-06488-6</a>
</p>
</li>
<li><p> Friedman, J. H. (2001).
Greedy Function Approximation: A Gradient Boosting Machine.
The Annals of Statistics, 29(5), 1189–1232.
doi: <a href="https://doi.org/10.1214/aos/1013203451">10.1214/aos/1013203451</a>
</p>
</li></ul>


<hr>
<h2 id='partial.predict'>Partial prediction for boosted models</h2><span id='topic+partial.predict'></span>

<h3>Description</h3>

<p>Alternative prediction function for <code>logic.boosted</code> models
using up to <code>n.iter</code> boosting iterations.
An array of predictions for every number of boosting iterations
up to <code>n.iter</code> is returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>partial.predict(model, X, Z = NULL, n.iter = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="partial.predict_+3A_model">model</code></td>
<td>
<p>Fitted <code>logic.boosted</code> model</p>
</td></tr>
<tr><td><code id="partial.predict_+3A_x">X</code></td>
<td>
<p>Matrix or data frame of binary input data.
This object should correspond to the binary matrix for fitting the model.</p>
</td></tr>
<tr><td><code id="partial.predict_+3A_z">Z</code></td>
<td>
<p>Optional quantitative covariables supplied as a matrix or
data frame. Only used (and required) if the model was fitted using them.</p>
</td></tr>
<tr><td><code id="partial.predict_+3A_n.iter">n.iter</code></td>
<td>
<p>Maximum number of boosting iterations for prediction</p>
</td></tr>
<tr><td><code id="partial.predict_+3A_...">...</code></td>
<td>
<p>Parameters supplied to <code><a href="#topic+predict.logicDT">predict.logicDT</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The main purpose of this function is to retrieve the optimal number of
boosting iterations (early stopping) using a validation data set and to
restrict future predictions on this number of iterations.
</p>


<h3>Value</h3>

<p>An array of dimension <code>(N, n.iter)</code> containing the partial
predictions
</p>

<hr>
<h2 id='plot.logicDT'>Plot a logic decision tree</h2><span id='topic+plot.logicDT'></span><span id='topic+fancy.plot'></span>

<h3>Description</h3>

<p>This function plots a logicDT model on the active graphics device.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fancy.plot(x, cdot = FALSE, ...)

## S3 method for class 'logicDT'
plot(
  x,
  fancy = TRUE,
  x_scaler = 0.5,
  margin_scaler = 0.2,
  cex = 1,
  cdot = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.logicDT_+3A_x">x</code></td>
<td>
<p>An object of the class <code>logicDT</code></p>
</td></tr>
<tr><td><code id="plot.logicDT_+3A_cdot">cdot</code></td>
<td>
<p>Should a centered dot be used instead of a logical and
for depicting interactions?</p>
</td></tr>
<tr><td><code id="plot.logicDT_+3A_...">...</code></td>
<td>
<p>Arguments passed to fancy plotting function</p>
</td></tr>
<tr><td><code id="plot.logicDT_+3A_fancy">fancy</code></td>
<td>
<p>Should the fancy mode be used for plotting? Default is
<code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="plot.logicDT_+3A_x_scaler">x_scaler</code></td>
<td>
<p>Scaling factor on the horizontal axis for deeper trees,
i.e., <code>x_scaler = 0.5</code> means that the horizontal distance
between two adjacent nodes is halved for every vertical level.</p>
</td></tr>
<tr><td><code id="plot.logicDT_+3A_margin_scaler">margin_scaler</code></td>
<td>
<p>Margin factor. Smaller values lead to smaller
margins.</p>
</td></tr>
<tr><td><code id="plot.logicDT_+3A_cex">cex</code></td>
<td>
<p>Scaling factor for the plotted text elements.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are two plotting modes:
</p>

<ul>
<li> <p><code>fancy = FALSE</code> which draws a tree with direct edges between
the nodes. Leaves are represented by their prediction value which is
obtained by the (observed) conditional mean.
</p>
</li>
<li> <p><code>fancy = TRUE</code> plots a tree similar to those in the <code>rpart</code>
(Therneau and Atkinson, 2019) and <code>splinetree</code> (Neufeld and Heggeseth,
2019) <code>R</code> packages. The trees are drawn in an angular manner and
if leaf regression models were fitted, appropriate plots of the fitted
curves are depicted in the leaves. Otherwise, the usual prediction values
are shown.
</p>
</li></ul>



<h3>Value</h3>

<p>No return value, called for side effects
</p>


<h3>References</h3>


<ul>
<li><p> Therneau, T. &amp; Atkinson, B. (2019). rpart: Recursive Partitioning
and Regression Trees. <a href="https://CRAN.R-project.org/package=rpart">https://CRAN.R-project.org/package=rpart</a>
</p>
</li>
<li><p> Neufeld, A. &amp; Heggeseth, B. (2019). splinetree: Longitudinal
Regression Trees and Forests.
<a href="https://CRAN.R-project.org/package=splinetree">https://CRAN.R-project.org/package=splinetree</a>
</p>
</li></ul>


<hr>
<h2 id='plot.vim'>Plot calculated VIMs</h2><span id='topic+plot.vim'></span>

<h3>Description</h3>

<p>This function plots variable importance measures yielded by the function
<code><a href="#topic+vim">vim</a></code> in a dotchart.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'vim'
plot(x, p = 10, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.vim_+3A_x">x</code></td>
<td>
<p>An object of the class <code>vim</code></p>
</td></tr>
<tr><td><code id="plot.vim_+3A_p">p</code></td>
<td>
<p>The number of most important terms which will be included
in the plot. A value of 0 leads to plotting all terms.</p>
</td></tr>
<tr><td><code id="plot.vim_+3A_...">...</code></td>
<td>
<p>Ignored additional parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, called for side effects
</p>

<hr>
<h2 id='predict.4pl'>Prediction for 4pL models</h2><span id='topic+predict.4pl'></span>

<h3>Description</h3>

<p>Use new input data and a fitted four parameter logistic
model to predict corresponding outcomes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class ''4pl''
predict(object, Z, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.4pl_+3A_object">object</code></td>
<td>
<p>Fitted <code>4pl</code> model</p>
</td></tr>
<tr><td><code id="predict.4pl_+3A_z">Z</code></td>
<td>
<p>Numeric vector of new input samples</p>
</td></tr>
<tr><td><code id="predict.4pl_+3A_...">...</code></td>
<td>
<p>Ignored additional parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of predictions. For binary outcomes,
this is a vector with estimates for
<code class="reqn">P(Y=1 \mid X = x)</code>.
</p>

<hr>
<h2 id='predict.linear'>Prediction for linear models</h2><span id='topic+predict.linear'></span>

<h3>Description</h3>

<p>Use new input data and a fitted linear
model to predict corresponding outcomes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'linear'
predict(object, Z, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.linear_+3A_object">object</code></td>
<td>
<p>Fitted <code>linear</code> model</p>
</td></tr>
<tr><td><code id="predict.linear_+3A_z">Z</code></td>
<td>
<p>Numeric vector of new input samples</p>
</td></tr>
<tr><td><code id="predict.linear_+3A_...">...</code></td>
<td>
<p>Ignored additional parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For binary outcomes, predictions are cut at 0 or 1 for generating
proper probability estimates.
</p>


<h3>Value</h3>

<p>A numeric vector of predictions. For binary outcomes,
this is a vector with estimates for
<code class="reqn">P(Y=1 \mid X = x)</code>.
</p>

<hr>
<h2 id='predict.linear.logic'>Prediction for <code>linear.logic</code> models</h2><span id='topic+predict.linear.logic'></span>

<h3>Description</h3>

<p>Use new input data and a fitted <code>linear.logic</code> model to
predict corresponding outcomes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'linear.logic'
predict(object, X, Z = NULL, s = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.linear.logic_+3A_object">object</code></td>
<td>
<p>Fitted <code>linear.logic</code> model</p>
</td></tr>
<tr><td><code id="predict.linear.logic_+3A_x">X</code></td>
<td>
<p>Matrix or data frame of binary input data.
This object should correspond to the binary matrix for fitting the model.</p>
</td></tr>
<tr><td><code id="predict.linear.logic_+3A_z">Z</code></td>
<td>
<p>Optional quantitative covariables supplied as a matrix or
data frame. Only used (and required) if the model was fitted using them.</p>
</td></tr>
<tr><td><code id="predict.linear.logic_+3A_s">s</code></td>
<td>
<p>Regularization parameter. Only used if <code>type = "lasso"</code> or
<code>type = "cv.lasso"</code> was set. Only useful if the penalty saved in
<code>object$s</code> should be overwritten.</p>
</td></tr>
<tr><td><code id="predict.linear.logic_+3A_...">...</code></td>
<td>
<p>Ignored additional parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of predictions. For binary outcomes,
this is a vector with estimates for
<code class="reqn">P(Y=1 \mid X = x)</code>.
</p>

<hr>
<h2 id='predict.logicDT'>Prediction for logicDT models</h2><span id='topic+predict.logicDT'></span><span id='topic+predict.logic.bagged'></span><span id='topic+predict.logic.boosted'></span><span id='topic+predict.genetic.logicDT'></span>

<h3>Description</h3>

<p>Supply new input data for predicting the outcome with a fitted
logicDT model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'logic.bagged'
predict(object, X, Z = NULL, type = "prob", ...)

## S3 method for class 'logic.boosted'
predict(object, X, Z = NULL, type = "prob", ...)

## S3 method for class 'logicDT'
predict(
  object,
  X,
  Z = NULL,
  type = "prob",
  ensemble = FALSE,
  leaves = "4pl",
  ...
)

## S3 method for class 'genetic.logicDT'
predict(
  object,
  X,
  Z = NULL,
  models = "best",
  n_models = 10,
  ensemble = NULL,
  leaves = "4pl",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.logicDT_+3A_object">object</code></td>
<td>
<p>Fitted <code>logicDT</code> model. Usually a product of a call
to <code><a href="#topic+logicDT">logicDT</a></code>.</p>
</td></tr>
<tr><td><code id="predict.logicDT_+3A_x">X</code></td>
<td>
<p>Matrix or data frame of binary input data. This
object should correspond to the binary matrix for fitting
the model.</p>
</td></tr>
<tr><td><code id="predict.logicDT_+3A_z">Z</code></td>
<td>
<p>Optional quantitative covariables supplied as a
matrix or data frame. Only used (and required) if the
model was fitted using them.</p>
</td></tr>
<tr><td><code id="predict.logicDT_+3A_type">type</code></td>
<td>
<p>Prediction type. This can either be <code>"prob"</code> for
probability estimates or <code>"class"</code> for (hard)
classification of binary responses. Ignored for regression.</p>
</td></tr>
<tr><td><code id="predict.logicDT_+3A_...">...</code></td>
<td>
<p>Parameters supplied to <code><a href="#topic+predict.logicDT">predict.logicDT</a></code></p>
</td></tr>
<tr><td><code id="predict.logicDT_+3A_ensemble">ensemble</code></td>
<td>
<p>If the model was fitted using the inner
validation approach, shall the prediction be constructed
using the final validated ensemble (<code>TRUE</code>) or using the
single final tree (<code>FALSE</code>)?</p>
</td></tr>
<tr><td><code id="predict.logicDT_+3A_leaves">leaves</code></td>
<td>
<p>If leaf regression models (such as four parameter logistic
models) were fitted, shall these models be used for the prediction
(<code>"4pl"</code>) or shall the constant leaf means be used
(<code>"constant"</code>)?</p>
</td></tr>
<tr><td><code id="predict.logicDT_+3A_models">models</code></td>
<td>
<p>Which logicDT models fitted via
genetic programming shall be used for prediction?
<code>"best"</code> leads to the single best model in the final
generation, <code>"all"</code> uses the average over the final
generation and <code>"n_models"</code> uses the <code>n_models</code> best models.</p>
</td></tr>
<tr><td><code id="predict.logicDT_+3A_n_models">n_models</code></td>
<td>
<p>How many models shall be used if
<code>models = "n_models"</code> and genetic programming was employed?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of predictions. For binary outcomes,
this is a vector with estimates for <code class="reqn">P(Y=1 \mid X = x)</code>.
</p>

<hr>
<h2 id='prune'>Post-pruning using a fixed complexity penalty</h2><span id='topic+prune'></span>

<h3>Description</h3>

<p>Using a fitted <code><a href="#topic+logicDT">logicDT</a></code> model and a fixed complexity penalty
<code>alpha</code>, its logic decision tree can be (post-)pruned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prune(model, alpha, simplify = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prune_+3A_model">model</code></td>
<td>
<p>A fitted <code>logicDT</code> model</p>
</td></tr>
<tr><td><code id="prune_+3A_alpha">alpha</code></td>
<td>
<p>A fixed complexity penalty value. This value should be
determined out-of-sample, e.g., performing hyperparameter optimization
on independent validation data.</p>
</td></tr>
<tr><td><code id="prune_+3A_simplify">simplify</code></td>
<td>
<p>Should the pruned model be simplified with regard to the
input terms, i.e., should terms that are no longer in the tree contained
be removed from the model?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Similar to Breiman et al. (1984), we implement post-pruning by first
computing the optimal pruning path and then choosing the tree that is
pruned according to the specified complexity penalty.
</p>
<p>If no validation data is available or if the tree shall be automatically
optimally pruned, <code><a href="#topic+cv.prune">cv.prune</a></code> should be used instead which
employs k-fold cross-validation for finding the best complexity penalty
value.
</p>


<h3>Value</h3>

<p>The new <code>logicDT</code> model containing the pruned tree
</p>

<hr>
<h2 id='prune.path'>Pruning path of a logic decision tree</h2><span id='topic+prune.path'></span>

<h3>Description</h3>

<p>Using a single fitted logic decision tree, the cost-complexity pruning path
containing the ideal subtree for a certain complexity penalty can be
computed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prune.path(pet, y, Z)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prune.path_+3A_pet">pet</code></td>
<td>
<p>A fitted logic decision tree. This can be extracted from a
<code><a href="#topic+logicDT">logicDT</a></code> model, e.g., using <code>model$pet</code>.</p>
</td></tr>
<tr><td><code id="prune.path_+3A_y">y</code></td>
<td>
<p>Training outcomes for potentially refitting regression models in
the leaves. This can be extracted from a <code><a href="#topic+logicDT">logicDT</a></code> model,
e.g., using <code>model$y</code>.</p>
</td></tr>
<tr><td><code id="prune.path_+3A_z">Z</code></td>
<td>
<p>Continuous training predictors for potentially refitting regression
models in the leaves. This can be extracted from a <code><a href="#topic+logicDT">logicDT</a></code>
model, e.g., using <code>model$Z</code>. If no continuous covariable was used in
fitting the model, <code>Z = model$Z = NULL</code> should be specified.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is mainly a helper function for <code><a href="#topic+cv.prune">cv.prune</a></code> and should only
be used by the user if manual pruning is preferred.
More details are given in <code><a href="#topic+cv.prune">cv.prune</a></code>.
</p>


<h3>Value</h3>

<p>Two lists. The first contains the sequence of complexity penalties
<code class="reqn">alpha</code>. The second list contains the corresponding logic decision
trees which can then be substituted in an already fitted
<code><a href="#topic+logicDT">logicDT</a></code> model, e.g., using
<code>model$pet &lt;- result[[2]][[i]]</code> where <code>result</code> is the returned
object from this function and <code>i</code> is the chosen tree index.
</p>

<hr>
<h2 id='refitTrees'>Refit the logic decision trees</h2><span id='topic+refitTrees'></span>

<h3>Description</h3>

<p>Newly fit the decision trees in the <code>logicDT</code> model using
the supplied tree control parameters.
This is especially useful if, e.g., the model was initially trained
without utilizing a continuous covariable or fitting linear models and
now 4pL model shall be fitted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>refitTrees(model, tree_control)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="refitTrees_+3A_model">model</code></td>
<td>
<p>A fitted <code>logicDT</code> model</p>
</td></tr>
<tr><td><code id="refitTrees_+3A_tree_control">tree_control</code></td>
<td>
<p>Tree control parameters. This object should be
constructed using the function <code><a href="#topic+tree.control">tree.control</a></code>.
Alternatively, the old <code>tree_control</code> from <code>model</code> can be
modified and specified here.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>logicDT</code> model with newly fitted trees
</p>

<hr>
<h2 id='splitSNPs'>Split biallelic SNPs into binary variables</h2><span id='topic+splitSNPs'></span>

<h3>Description</h3>

<p>This function takes a matrix or data frame of SNPs coded as
0, 1, 2 or 1, 2, 3 and returns a data frame with twice as many
columns. SNPs are splitted into dominant and recessive modes,
i.e., for a <code class="reqn">\mathrm{SNP} \in \lbrace 0,1,2 \rbrace</code>, two variables
<code class="reqn">\mathrm{SNP}_D = (\mathrm{SNP} \neq 0)</code> and
<code class="reqn">\mathrm{SNP}_R = (\mathrm{SNP} = 2)</code> are generated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>splitSNPs(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="splitSNPs_+3A_data">data</code></td>
<td>
<p>A matrix or data frame only consisting of SNPs to
be splitted</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame of the splitted SNPs
</p>

<hr>
<h2 id='tree.control'>Control parameters for fitting decision trees</h2><span id='topic+tree.control'></span>

<h3>Description</h3>

<p>Configure the fitting process of individual decision trees.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tree.control(
  nodesize = 10,
  split_criterion = "gini",
  alpha = 0.05,
  cp = 0.001,
  smoothing = "none",
  mtry = "none",
  covariable = "final_4pl"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tree.control_+3A_nodesize">nodesize</code></td>
<td>
<p>Minimum number of samples contained in a
terminal node. This parameter ensures that enough samples
are available for performing predictions which includes
fitting regression models such as 4pL models.</p>
</td></tr>
<tr><td><code id="tree.control_+3A_split_criterion">split_criterion</code></td>
<td>
<p>Splitting criterion for deciding
when and how to split. The default is <code>"gini"</code>/<code>"mse"</code> which
utilizes the Gini splitting criterion for binary risk
estimation tasks and the mean squared error as impurity
measure in regression tasks. Alternatively, <code>"4pl"</code> can be
used if a quantitative covariable is supplied and
the parameter <code>covariable</code> is chosen such that 4pL
model fitting is enabled, i.e., <code>covariable = "final_4pl"</code>
or <code>covariable = "full_4pl"</code>.
A fast modeling alternative is given by <code>"linear"</code> which also
requires the parameter <code>covariable</code> to be properly
chosen, i.e., <code>covariable = "final_linear"</code>
or <code>covariable = "full_linear"</code>.</p>
</td></tr>
<tr><td><code id="tree.control_+3A_alpha">alpha</code></td>
<td>
<p>Significance threshold for the likelihood ratio
tests when using <code>split_criterion = "4pl"</code> or <code>"linear"</code>.
Only splits that achieve a p-value smaller than <code>alpha</code> are eligible.</p>
</td></tr>
<tr><td><code id="tree.control_+3A_cp">cp</code></td>
<td>
<p>Complexity parameter. This parameter determines
by which amount the impurity has to be reduced to further
split a node. Here, the total tree impurity is considered.
See details for a specific formula. Only used if
<code>split_criterion = "gini"</code> or <code>"mse"</code>.</p>
</td></tr>
<tr><td><code id="tree.control_+3A_smoothing">smoothing</code></td>
<td>
<p>Shall the leaf predictions for risk
estimation be smoothed? <code>"laplace"</code> yields Laplace smoothing.
The default is <code>"none"</code> which does not employ smoothing.</p>
</td></tr>
<tr><td><code id="tree.control_+3A_mtry">mtry</code></td>
<td>
<p>Shall the tree fitting process be randomized
as in random forests? Currently, only <code>"sqrt"</code> for using
<code class="reqn">\sqrt{p}</code> random predictors at each node for splitting
and <code>"none"</code> (default) for fitting conventional decision trees
are supported.</p>
</td></tr>
<tr><td><code id="tree.control_+3A_covariable">covariable</code></td>
<td>
<p>How shall optional quantitative covariables
be handled? <code>"constant"</code> ignores them. Alternatively,
they can be considered as splitting variables (<code>"_split"</code>),
used for fitting 4pL models in each leaf (<code>"_4pl"</code>), or used
for fitting linear models in each leaf (<code>"_linear"</code>). If either
splitting or model fitting is chosen, one should state if this
should be handled over the whole search (<code>"full_"</code>,
computationally expensive) or just the final trees
(<code>"final_"</code>). Thus, <code>"final_4pl"</code> would lead to fitting
4pL models in each leaf but only for the final tree fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the Gini or MSE splitting criterion,
if any considered split <code class="reqn">s</code> leads to
</p>
<p style="text-align: center;"><code class="reqn">P(t) \cdot \Delta I(s,t) &gt; \texttt{cp}</code>
</p>

<p>for a node <code class="reqn">t</code>, the empirical node probability
<code class="reqn">P(t)</code> and the impurity reduction <code class="reqn">\Delta I(s,t)</code>,
then the node is further splitted. If not, the node is
declared as a leaf.
For continuous outcomes, <code>cp</code> will be scaled by the
empirical variance of <code>y</code> to ensure the right scaling,
i.e., <code>cp &lt;- cp * var(y)</code>. Since the impurity measure
for continuous outcomes is the mean squared error, this can
be interpreted as controlling the minimum reduction of the
normalized mean squared error (NRMSE to the power of two).
</p>
<p>If one chooses the 4pL or linear splitting criterion, likelihood
ratio tests testing the alternative of better fitting individual
models are employed. The corresponding test statistic
asymptotically follows a <code class="reqn">\chi^2</code> distribution where
the degrees of freedom are given by the difference in the
number of model parameters, i.e., leading to
<code class="reqn">2 \cdot 4 - 4 = 4</code> degrees of freedom in the case of 4pL
models and to <code class="reqn">2 \cdot 2 - 2 = 2</code> degrees of freedom in
the case of linear models.
</p>
<p>For binary outcomes, choosing to fit linear models for evaluating
the splits or for modeling the leaves actually leads to fitting
LDA (linear discriminant analysis) models.
</p>


<h3>Value</h3>

<p>An object of class <code>tree.control</code> which is a list
of all necessary tree parameters.
</p>

<hr>
<h2 id='vim'>Variable Importance Measures (VIMs)</h2><span id='topic+vim'></span>

<h3>Description</h3>

<p>Calculate variable importance measures (VIMs) based on different
approaches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vim(
  model,
  scoring_rule = "auc",
  vim_type = "logic",
  adjust = TRUE,
  interaction_order = 3,
  nodesize = NULL,
  alpha = 0.05,
  X_oob = NULL,
  y_oob = NULL,
  Z_oob = NULL,
  leaves = "4pl",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vim_+3A_model">model</code></td>
<td>
<p>The fitted <code>logicDT</code> or <code>logic.bagged</code>
model</p>
</td></tr>
<tr><td><code id="vim_+3A_scoring_rule">scoring_rule</code></td>
<td>
<p>The scoring rule for assessing the model
performance. As in <code><a href="#topic+logicDT">logicDT</a></code>, <code>"auc"</code>, <code>"nce"</code>,
<code>"deviance"</code> and <code>"brier"</code> are possible for binary outcomes.
For regression, the mean squared error is used.</p>
</td></tr>
<tr><td><code id="vim_+3A_vim_type">vim_type</code></td>
<td>
<p>The type of VIM to be calculated. This can
either be <code>"logic"</code>, <code>"remove"</code> or
<code>"permutation"</code>. See below for details.</p>
</td></tr>
<tr><td><code id="vim_+3A_adjust">adjust</code></td>
<td>
<p>Shall adjusted interaction VIMs be additionally
(to the VIMs of identified terms) computed? See below for
details.</p>
</td></tr>
<tr><td><code id="vim_+3A_interaction_order">interaction_order</code></td>
<td>
<p>If <code>adjust = TRUE</code>, up to which
interaction order shall adjusted interaction VIMs be
computed?</p>
</td></tr>
<tr><td><code id="vim_+3A_nodesize">nodesize</code></td>
<td>
<p>If <code>adjust = TRUE</code>, how many observations
need to be discriminated by an interaction in order to being
considered? Similar to <code>conjsize</code> in <code><a href="#topic+logicDT">logicDT</a></code>
and <code>nodesize</code> in <code><a href="#topic+tree.control">tree.control</a></code>.</p>
</td></tr>
<tr><td><code id="vim_+3A_alpha">alpha</code></td>
<td>
<p>If <code>adjust = TRUE</code>, a further adjustment can be
performed trying to identify the specific conjunctions responsible
for the interaction of the considered binary predictors.
<code>alpha</code> specifies the significance level for statistical tests
testing the alternative of a difference in the response for specific
conjunctions. <code>alpha = 0</code> leads to no further adjustment.
See below for details.</p>
</td></tr>
<tr><td><code id="vim_+3A_x_oob">X_oob</code></td>
<td>
<p>The predictor data which should be used for
calculating the VIMs.
Preferably some type of validation
data independent of the training data.</p>
</td></tr>
<tr><td><code id="vim_+3A_y_oob">y_oob</code></td>
<td>
<p>The outcome data for computing the VIMs.
Preferably some type of validation
data independent of the training data.</p>
</td></tr>
<tr><td><code id="vim_+3A_z_oob">Z_oob</code></td>
<td>
<p>The optional covariable data for computing the
VIMs.
Preferably some type of validation
data independent of the training data.</p>
</td></tr>
<tr><td><code id="vim_+3A_leaves">leaves</code></td>
<td>
<p>The prediction mode if regression models (such as 4pL models)
were fitted in the leaves. As in <code><a href="#topic+predict.logicDT">predict.logicDT</a></code>,
<code>"4pl"</code> and <code>"constant"</code> are the possible settings.</p>
</td></tr>
<tr><td><code id="vim_+3A_...">...</code></td>
<td>
<p>Parameters passed to the different VIM type functions.
For <code>vim_type = "logic"</code>, the argument <code>average</code> can
be specified as <code>"before"</code> or <code>"after"</code>. For
<code>vim_type = "permutation"</code>, <code>n.perm</code> can be set to
the number of random permutations.
For <code>vim_type = "remove"</code>, <code>empty.model</code> can be specified
as either <code>"none"</code> ignoring empty models with all predictive
terms removed or <code>"mean"</code> using the response mean as prediction
in the case of an empty model.
See below for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Three different VIM methods are implemented:
</p>

<ul>
<li><p> Permutation VIMs: Random permutations of the respective
identified logic terms
</p>
</li>
<li><p> Removal VIMs: Removing single logic terms
</p>
</li>
<li><p> Logic VIMs: Prediction with both possible outcomes
of a logic term
</p>
</li></ul>

<p>Details on the calculation of these VIMs are given below.
</p>
<p>By variable importance, importance of identified logic terms
is meant. These terms can be single predictors or
conjunctions between predictors in the spirit of this software package.
</p>


<h3>Value</h3>

<p>A data frame with two columns:
</p>
<table>
<tr><td><code>var</code></td>
<td>
<p>Short descriptions of the terms for which the
importance was measured. For example <code>-X1^X2</code> for
<code class="reqn">X_1^c \land X_2</code>.</p>
</td></tr>
<tr><td><code>vim</code></td>
<td>
<p>The actual calculated VIM values.</p>
</td></tr>
</table>
<p>The rows of such a data frame are sorted decreasingly by the VIM values.
</p>


<h3>Permutation VIMs (Breiman &amp; Cutler, 2003)</h3>

<p>Permutation VIMs are computed by comparing the the model's
performance using the original data and data with random
permutations of single terms.
</p>


<h3>Removal VIMs</h3>

<p>Removal VIMs are constructed by removing specific logic
terms from the set of predictors, refitting the decision
tree and comparing the performance to the original model.
Thus, this approach requires that at least two terms were
found by the algorithm. Therefore, no VIM will be
calculated if <code>empty.model = "none"</code> was specified.
Alternatively, <code>empty.model = "mean"</code> can be set to
use the constant mean response model for approximating
the empty model.
</p>


<h3>Logic VIMs (Lau et al., 2024)</h3>

<p>Logic VIMs use the fact that Boolean conjunctions are
Boolean variables themselves and therefore are equal to
0 or 1. To compute the VIM for a specific term,
predictions are performed once for this term fixed to
0 and once for this term fixed to 1. Then, the arithmetic
mean of these two (risk or regression) predictions is
used for calculating the performance. This performance
is then compared to the original one as in the other
VIM approaches (<code>average = "before"</code>). Alternatively,
predictions for each fixed 0-1 scenario of the considered
term can be performed leading to individual performances
which then are averaged and compared to the original
performance (<code>average = "after"</code>).
</p>


<h3>Validation</h3>

<p>Validation data sets which
were not used in the fitting of the model are prefered
preventing an overfitting of the VIMs themselves.
These should be specified by the <code>_oob</code> arguments,
if neither bagging nor inner validation was used for fitting
the model.
</p>


<h3>Bagging</h3>

<p>For the bagging version, out-of-bag (OOB) data are naturally
used for the calculation of VIMs.
</p>


<h3>VIM Adjustment for Interactions (Lau et al., 2024)</h3>

<p>Since decision trees can naturally include interactions
between single predictors (especially when strong marginal
effects are present as well), logicDT models might, e.g.,
include the single input variables <code class="reqn">X_1</code> and <code class="reqn">X_2</code> but
not their interaction <code class="reqn">X_1 \land X_2</code> although an interaction
effect is present. We, therefore, developed and implemented an
adjustment approach for calculating VIMs for such
unidentified interactions nonetheless.
For predictors <code class="reqn">X_{i_1}, \ldots, X_{i_k} =: Z</code>, this interaction
importance is given by
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{VIM}(X_{i_1} \land \ldots \land X_{i_k}) =
\mathrm{VIM}(X_{i_1}, \ldots, X_{i_k} \mid X \setminus Z) -
\sum_{\lbrace j_1, \ldots, j_l \rbrace {\subset \atop \neq}
\lbrace i_1, \ldots, i_k \rbrace}
\mathrm{VIM}(X_{j_1} \land \ldots \land X_{j_l} \mid X \setminus Z)</code>
</p>

<p>and can basically be applied to all black-box models.
By <code class="reqn">\mathrm{VIM}(A \mid X \setminus Z)</code>, the VIM of <code class="reqn">A</code>
considering the predictor set excluding the variables in <code class="reqn">Z</code>
is meant, i.e., the improvement of additionally considering <code class="reqn">A</code>
while regarding only the predictors in <code class="reqn">X \setminus Z</code>.
The proposed interaction VIM can be recursively calculated through
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{VIM}(X_{i_1} \land X_{i_2}) =
\mathrm{VIM}(X_{i_1}, X_{i_2} \mid X \setminus Z) -
\mathrm{VIM}(X_{i_1} \mid X \setminus Z) -
\mathrm{VIM}(X_{i_2} \mid X \setminus Z)</code>
</p>

<p>for <code class="reqn">Z = X_{i_1}, X_{i_2}</code>.
This leads to the relationship
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{VIM}(X_{i_1} \land \ldots \land X_{i_k}) =
\sum_{\lbrace j_1, \ldots, j_l \rbrace \subseteq \lbrace i_1, \ldots, i_k \rbrace}
(-1)^{k-l} \cdot \mathrm{VIM}(X_{j_1}, \ldots, X_{j_l} \mid X \setminus Z).</code>
</p>



<h3>Identification of Specific Conjunctions (Lau et al., 2024)</h3>

<p>The aforementioned VIM adjustment approach only captures the importance
of a general definition of interactions, i.e., it just considers
the question whether some variables do interact in any way.
Since logicDT is aimed at identifying specific conjunctions (and also assigns
them VIMs if they were identified by <code><a href="#topic+logicDT">logicDT</a></code>), a further
adjustment approach is implemented which tries to identify the specific
conjunction leading to an interaction effect.
The idea of this method is to consider the response for each possible
scenario of the interacting variables, e.g., for <code class="reqn">X_1 \land (X_2^c \land X_3)</code>
where the second term <code class="reqn">X_2^c \land X_3</code> was identified by <code><a href="#topic+logicDT">logicDT</a></code>
and, thus, two interacting terms are regarded,
the <code class="reqn">2^2 = 4</code> possible scenarios
<code class="reqn">\lbrace (i, j) \mid i, j \in \lbrace 0, 1 \rbrace \rbrace</code>
are considered. For each setting, the corresponding response is compared with
outcome values of the complementary set. For continuous outcomes, a two sample
t-test (with Welch correction for potentially unequal variances) is performed
comparing the means between these two groups. For binary outcomes, Fisher's exact
test is performed testing different underlying case probabilities.
If at least one test rejects the null hypothesis of equal outcomes (without adjusting
for multiple testing), the combination with the lowest p-value is chosen as the
explanatory term for the interaction effect. For example, if the most significant
deviation results from <code class="reqn">X_1 = 0</code> and <code class="reqn">(X_2^c \land X_3) = 1</code> from the example
above, the term <code class="reqn">X_1^c \land (X_2^c \land X_3)</code> is chosen.
</p>


<h3>References</h3>


<ul>
<li><p> Lau, M., Schikowski, T. &amp; Schwender, H. (2024).
logicDT: A procedure for identifying response-associated
interactions between binary predictors. Machine Learning 113(2):933–992.
doi: <a href="https://doi.org/10.1007/s10994-023-06488-6">10.1007/s10994-023-06488-6</a>
</p>
</li>
<li><p> Breiman, L. (2001). Random Forests. Machine Learning 45(1):5-32.
doi: <a href="https://doi.org/10.1023/A:1010933404324">10.1023/A:1010933404324</a>
</p>
</li>
<li><p> Breiman, L. &amp; Cutler, A. (2003). Manual on Setting Up, Using,
and Understanding Random Forests V4.0. University of California,
Berkeley, Department of Statistics.
<a href="https://www.stat.berkeley.edu/~breiman/Using_random_forests_v4.0.pdf">https://www.stat.berkeley.edu/~breiman/Using_random_forests_v4.0.pdf</a>
</p>
</li></ul>


</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
