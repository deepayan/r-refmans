<!DOCTYPE html><html><head><title>Help for package energy</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {energy}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#centering distance matrices'><p> Double centering and U-centering</p></a></li>
<li><a href='#dcorT'><p> Distance Correlation t-Test</p></a></li>
<li><a href='#dcov.test'><p> Distance Covariance Test and Distance Correlation test</p></a></li>
<li><a href='#dcov2d'><p>Fast dCor and dCov for bivariate data only</p></a></li>
<li><a href='#dcovU_stats'><p>Unbiased distance covariance statistics</p></a></li>
<li><a href='#disco'><p> distance components (DISCO)</p></a></li>
<li><a href='#distance correlation'><p> Distance Correlation and Covariance Statistics</p></a></li>
<li><a href='#Distance Matrix'><p> Distance Matrices</p></a></li>
<li><a href='#edist'><p>E-distance</p></a></li>
<li><a href='#energy-deprecated'><p> Deprecated Functions</p></a></li>
<li><a href='#energy-package'>
<p>E-statistics: Multivariate Inference via the Energy of Data</p></a></li>
<li><a href='#energy.hclust'><p> Hierarchical Clustering by Minimum (Energy) E-distance</p></a></li>
<li><a href='#eqdist.etest'><p>Multisample E-statistic (Energy) Test of Equal Distributions</p></a></li>
<li><a href='#EVnormal'><p>Eigenvalues for the energy Test of Univariate Normality</p></a></li>
<li><a href='#indep.etest'><p> Energy Statistic Test of Independence</p></a></li>
<li><a href='#indep.test'><p> Energy-tests of Independence</p></a></li>
<li><a href='#kgroups'>
<p>K-Groups Clustering</p></a></li>
<li><a href='#mutual independence'><p> Energy Test of Mutual Independence</p></a></li>
<li><a href='#mvI.test'><p> Energy Statistic Test of Independence</p></a></li>
<li><a href='#mvnorm.test'><p>E-statistic (Energy) Test of Multivariate Normality</p></a></li>
<li><a href='#normal.test'><p>Energy Test of Univariate Normality</p></a></li>
<li><a href='#pdcor'>
<p>Partial distance correlation and covariance</p></a></li>
<li><a href='#Poisson Tests'><p> Goodness-of-Fit Tests for Poisson Distribution</p></a></li>
<li><a href='#sortrank'><p> Sort, order and rank a vector</p></a></li>
<li><a href='#U_product'><p> Inner product in the Hilbert space of U-centered</p>
distance matrices</a></li>
<li><a href='#Unbiased distance covariance'><p>Unbiased dcov and bias-corrected dcor statistics</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>E-Statistics: Multivariate Inference via the Energy of Data</td>
</tr>
<tr>
<td>Version:</td>
<td>1.7-11</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-12-21</td>
</tr>
<tr>
<td>Description:</td>
<td>E-statistics (energy) tests and statistics for multivariate and univariate inference,
             including distance correlation, one-sample, two-sample, and multi-sample tests for
             comparing multivariate distributions, are implemented. Measuring and testing
             multivariate independence based on distance correlation, partial distance correlation,
             multivariate goodness-of-fit tests, k-groups and hierarchical clustering based on energy 
             distance, testing for multivariate normality, distance components (disco) for non-parametric 
             analysis of structured data, and other energy statistics/methods are implemented.</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.6), stats, boot, gsl</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS, CompQuadForm</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1)</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mariarizzo/energy">https://github.com/mariarizzo/energy</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-12-21 16:56:06 UTC; maria</td>
</tr>
<tr>
<td>Author:</td>
<td>Maria Rizzo [aut, cre],
  Gabor Szekely [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Maria Rizzo &lt;mrizzo@bgsu.edu&gt;</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-12-22 09:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='centering+20distance+20matrices'> Double centering and U-centering </h2><span id='topic+Ucenter'></span><span id='topic+Dcenter'></span><span id='topic+U_center'></span><span id='topic+D_center'></span>

<h3>Description</h3>

<p>Stand-alone double centering and U-centering functions
that are applied in unbiased distance covariance, bias
corrected distance correlation, and partial distance correlation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Dcenter(x)
Ucenter(x)
U_center(Dx)
D_center(Dx)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="centering+2B20distance+2B20matrices_+3A_x">x</code></td>
<td>
<p> dist object or data matrix</p>
</td></tr>
<tr><td><code id="centering+2B20distance+2B20matrices_+3A_dx">Dx</code></td>
<td>
<p> distance or dissimilarity matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In <code>Dcenter</code> and <code>Ucenter</code>, <code>x</code> must be
a <code>dist</code> object or a data matrix. Both functions return
a doubly centered distance matrix.
</p>
<p>Note that <code>pdcor</code>, etc. functions include the
centering operations (in C), so that these stand alone versions
of centering functions are not needed except in case one
wants to compute just a double-centered or U-centered matrix.
</p>
<p><code>U_center</code> is the Rcpp export of the cpp function.
<code>D_center</code> is the Rcpp export of the cpp function.
</p>


<h3>Value</h3>

<p>All functions return a square symmetric matrix.
</p>
<p><code>Dcenter</code> returns a matrix
</p>
<p style="text-align: center;"><code class="reqn">A_{ij}=a_{ij} - \bar a_{i.} - \bar a_{.j} + \bar a_{..}</code>
</p>

<p>as in classical multidimensional scaling. <code>Ucenter</code>
returns a matrix
</p>
<p style="text-align: center;"><code class="reqn">\tilde A_{ij}=a_{ij} - \frac{a_{i.}}{n-2}
 - \frac{a_{.j}}{n-2} + \frac{a_{..}}{(n-1)(n-2)},\quad i \neq j,</code>
</p>

<p>with zero diagonal,
and this is the double centering applied in <code>pdcov</code> and
<code>pdcor</code> as well as the unbiased dCov and bias corrected
dCor statistics.
</p>


<h3>Note</h3>

<p>The c++ versions <code>D_center</code> and <code>U_center</code> should typically
be faster. R versions are retained for historical reasons.
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Szekely, G.J. and Rizzo, M.L. (2014),
Partial Distance Correlation with Methods for Dissimilarities,
<em>Annals of Statistics</em>, Vol. 42, No. 6, pp. 2382-2412.
<br /> <a href="https://projecteuclid.org/euclid.aos/1413810731">https://projecteuclid.org/euclid.aos/1413810731</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x &lt;- iris[1:10, 1:4]
 dx &lt;- dist(x)
 Dx &lt;- as.matrix(dx)
 M &lt;- U_center(Dx)

 all.equal(M, U_center(M))     #idempotence
 all.equal(M, D_center(M))     #invariance
</code></pre>

<hr>
<h2 id='dcorT'> Distance Correlation t-Test</h2><span id='topic+dcorT.test'></span><span id='topic+dcorT'></span>

<h3>Description</h3>

<p>Distance correlation t-test of multivariate independence for high dimension.</p>


<h3>Usage</h3>

<pre><code class='language-R'>dcorT.test(x, y)
dcorT(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dcorT_+3A_x">x</code></td>
<td>
<p> data or distances of first sample</p>
</td></tr>
<tr><td><code id="dcorT_+3A_y">y</code></td>
<td>
<p> data or distances of second sample</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>dcorT.test</code> performs a nonparametric t-test of
multivariate independence in high dimension (dimension is close to
or larger than sample size). As dimension goes to infinity, the
asymptotic distribution of the test statistic is approximately Student t with <code class="reqn">n(n-3)/2-1</code> degrees of freedom and for <code class="reqn">n \geq 10</code> the statistic is approximately distributed as standard normal.
</p>
<p>The sample sizes (number of rows) of the two samples must
agree, and samples must not contain missing values.
</p>
<p>The t statistic (dcorT) is a transformation of a bias corrected 
version of distance correlation (see SR 2013 for details).
</p>
<p>Large values (upper tail) of the dcorT statistic are significant.
</p>


<h3>Value</h3>

<p><code>dcorT</code> returns the dcor t statistic, and
<code>dcorT.test</code> returns a list with class <code>htest</code> containing
</p>
<table>
<tr><td><code>method</code></td>
<td>
<p> description of test</p>
</td></tr>
<tr><td><code>statistic</code></td>
<td>
<p> observed value of the test statistic</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p> degrees of freedom</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p> (bias corrected) squared dCor(x,y)</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p> p-value of the t-test</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p> description of data</p>
</td></tr>
</table>


<h3>Note</h3>

<p><code>dcor.t</code> and <code>dcor.ttest</code> are deprecated.
</p>


<h3>Author(s)</h3>

<p>Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Szekely, G.J. and Rizzo, M.L. (2013). The distance correlation t-test of  independence in high dimension. <em>Journal of Multivariate Analysis</em>,  Volume 117, pp. 193-213. <br />
<a href="https://doi.org/10.1016/j.jmva.2013.02.012">doi:10.1016/j.jmva.2013.02.012</a>
</p>
<p>Szekely, G.J., Rizzo, M.L., and Bakirov, N.K. (2007),
Measuring and Testing Dependence by Correlation of Distances,
<em>Annals of Statistics</em>, Vol. 35 No. 6, pp. 2769-2794.
<br /> <a href="https://doi.org/10.1214/009053607000000505">doi:10.1214/009053607000000505</a>
</p>
<p>Szekely, G.J. and Rizzo, M.L. (2009),
Brownian Distance Covariance,
<em>Annals of Applied Statistics</em>,
Vol. 3, No. 4, 1236-1265.
<br /> <a href="https://doi.org/10.1214/09-AOAS312">doi:10.1214/09-AOAS312</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bcdcor">bcdcor</a></code> <code><a href="#topic+dcov.test">dcov.test</a></code> <code><a href="#topic+dcor">dcor</a></code> <code><a href="#topic+DCOR">DCOR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x &lt;- matrix(rnorm(100), 10, 10)
 y &lt;- matrix(runif(100), 10, 10)
 dcorT(x, y)
 dcorT.test(x, y)
</code></pre>

<hr>
<h2 id='dcov.test'> Distance Covariance Test and Distance Correlation test</h2><span id='topic+distance+20covariance'></span><span id='topic+dcov.test'></span><span id='topic+dcor.test'></span>

<h3>Description</h3>

<p>Distance covariance test and distance correlation test of multivariate independence.
Distance covariance and distance correlation are
multivariate measures of dependence.</p>


<h3>Usage</h3>

<pre><code class='language-R'>dcov.test(x, y, index = 1.0, R = NULL)
dcor.test(x, y, index = 1.0, R)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dcov.test_+3A_x">x</code></td>
<td>
<p> data or distances of first sample</p>
</td></tr>
<tr><td><code id="dcov.test_+3A_y">y</code></td>
<td>
<p> data or distances of second sample</p>
</td></tr>
<tr><td><code id="dcov.test_+3A_r">R</code></td>
<td>
<p> number of replicates</p>
</td></tr>
<tr><td><code id="dcov.test_+3A_index">index</code></td>
<td>
<p> exponent on Euclidean distance, in (0,2]</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>dcov.test</code> and <code>dcor.test</code> are nonparametric
tests of multivariate independence. The test decision is
obtained via permutation bootstrap, with <code>R</code> replicates.
</p>
<p>The sample sizes (number of rows) of the two samples must
agree, and samples must not contain missing values. 
</p>
<p>The <code>index</code> is an optional exponent on Euclidean distance.
Valid exponents for energy are in (0, 2) excluding 2. 
</p>
<p>Argument types supported are 
numeric data matrix, data.frame, or tibble, with observations in rows;
numeric vector; ordered or unordered factors. In case of unordered factors
a 0-1 distance matrix is computed.
</p>
<p>Optionally pre-computed distances can be input as class &quot;dist&quot; objects or as distance matrices. 
For data types of arguments,
distance matrices are computed internally. 
</p>
<p>The <code>dcov</code> test statistic is
<code class="reqn">n \mathcal V_n^2</code> where
<code class="reqn">\mathcal V_n(x,y)</code> = dcov(x,y),
which is based on interpoint Euclidean distances
<code class="reqn">\|x_{i}-x_{j}\|</code>. The <code>index</code>
is an optional exponent on Euclidean distance.
</p>
<p>Similarly, the <code>dcor</code> test statistic is based on the normalized
coefficient, the distance correlation. (See the manual page for <code>dcor</code>.)
</p>
<p>Distance correlation is a new measure of dependence between random
vectors introduced by Szekely, Rizzo, and Bakirov (2007).
For all distributions with finite first moments, distance
correlation <code class="reqn">\mathcal R</code> generalizes the idea of correlation in two
fundamental ways:
</p>
<p>(1) <code class="reqn">\mathcal R(X,Y)</code> is defined for <code class="reqn">X</code> and <code class="reqn">Y</code> in arbitrary dimension.
</p>
<p>(2) <code class="reqn">\mathcal R(X,Y)=0</code> characterizes independence of <code class="reqn">X</code> and
<code class="reqn">Y</code>.
</p>
<p>Characterization (2) also holds for powers of Euclidean distance <code class="reqn">\|x_i-x_j\|^s</code>, where <code class="reqn">0&lt;s&lt;2</code>, but (2) does not hold when <code class="reqn">s=2</code>.
</p>
<p>Distance correlation satisfies <code class="reqn">0 \le \mathcal R \le 1</code>, and
<code class="reqn">\mathcal R = 0</code> only if <code class="reqn">X</code> and <code class="reqn">Y</code> are independent. Distance
covariance <code class="reqn">\mathcal V</code> provides a new approach to the problem of
testing the joint independence of random vectors. The formal
definitions of the population coefficients <code class="reqn">\mathcal V</code> and
<code class="reqn">\mathcal R</code> are given in (SRB 2007). The definitions of the
empirical coefficients are given in the energy
<code><a href="#topic+dcov">dcov</a></code> topic.
</p>
<p>For all values of the index in (0,2), under independence
the asymptotic distribution of <code class="reqn">n\mathcal V_n^2</code>
is a quadratic form of centered Gaussian random variables,
with coefficients that depend on the distributions of <code class="reqn">X</code> and <code class="reqn">Y</code>. For the general problem of testing independence when the distributions of <code class="reqn">X</code> and <code class="reqn">Y</code> are unknown, the test based on <code class="reqn">n\mathcal V^2_n</code> can be implemented as a permutation test. See (SRB 2007) for
theoretical properties of the test, including statistical consistency.
</p>


<h3>Value</h3>

<p><code>dcov.test</code> or <code>dcor.test</code> returns a list with class <code>htest</code> containing
</p>
<table>
<tr><td><code>method</code></td>
<td>
<p> description of test</p>
</td></tr>
<tr><td><code>statistic</code></td>
<td>
<p> observed value of the test statistic</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p> dCov(x,y) or dCor(x,y)</p>
</td></tr>
<tr><td><code>estimates</code></td>
<td>
<p> a vector: [dCov(x,y), dCor(x,y), dVar(x), dVar(y)]</p>
</td></tr>
<tr><td><code>condition</code></td>
<td>
<p> logical, permutation test applied</p>
</td></tr>
<tr><td><code>replicates</code></td>
<td>
<p> replicates of the test statistic</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p> approximate p-value of the test</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p> sample size</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p> description of data</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For the dcov test of independence,
the distance covariance test statistic is the V-statistic
<code class="reqn">\mathrm{n\, dCov^2} = n \mathcal{V}_n^2</code> (not dCov). 
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Szekely, G.J., Rizzo, M.L., and Bakirov, N.K. (2007),
Measuring and Testing Dependence by Correlation of Distances,
<em>Annals of Statistics</em>, Vol. 35 No. 6, pp. 2769-2794.
<br /> <a href="https://doi.org/10.1214/009053607000000505">doi:10.1214/009053607000000505</a>
</p>
<p>Szekely, G.J. and Rizzo, M.L. (2009),
Brownian Distance Covariance,
<em>Annals of Applied Statistics</em>,
Vol. 3, No. 4, 1236-1265.
<br /> <a href="https://doi.org/10.1214/09-AOAS312">doi:10.1214/09-AOAS312</a>
</p>
<p>Szekely, G.J. and Rizzo, M.L. (2009),
Rejoinder: Brownian Distance Covariance,
<em>Annals of Applied Statistics</em>, Vol. 3, No. 4, 1303-1308.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dcov">dcov</a> </code> <code><a href="#topic+dcor">dcor</a> </code> 
<code><a href="#topic+pdcov.test">pdcov.test</a></code> <code><a href="#topic+pdcor.test">pdcor.test</a></code> 
<code><a href="#topic+dcor.ttest">dcor.ttest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x &lt;- iris[1:50, 1:4]
 y &lt;- iris[51:100, 1:4]
 set.seed(1)
 dcor.test(dist(x), dist(y), R=199)
 set.seed(1)
 dcov.test(x, y, R=199)
</code></pre>

<hr>
<h2 id='dcov2d'>Fast dCor and dCov for bivariate data only</h2><span id='topic+dcor2d'></span><span id='topic+dcov2d'></span>

<h3>Description</h3>

<p>For bivariate data only, these are fast O(n log n) implementations of distance
correlation and distance covariance statistics. The U-statistic for dcov^2 is unbiased; 
the V-statistic is the original definition in SRB 2007. These algorithms do not
store the distance matrices, so they are suitable for large samples. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dcor2d(x, y, type = c("V", "U"))
dcov2d(x, y, type = c("V", "U"), all.stats = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dcov2d_+3A_x">x</code></td>
<td>
<p> numeric vector</p>
</td></tr>
<tr><td><code id="dcov2d_+3A_y">y</code></td>
<td>
<p> numeric vector</p>
</td></tr>
<tr><td><code id="dcov2d_+3A_type">type</code></td>
<td>
<p> &quot;V&quot; or &quot;U&quot;, for V- or U-statistics</p>
</td></tr>
<tr><td><code id="dcov2d_+3A_all.stats">all.stats</code></td>
<td>
<p> logical</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The unbiased (squared) dcov is documented in <code>dcovU</code>, for multivariate data in arbitrary, not necessarily equal dimensions. <code>dcov2d</code> and <code>dcor2d</code> provide a faster O(n log n) algorithm for bivariate (x, y) only (X and Y are real-valued random vectors). The O(n log n) algorithm was proposed by Huo and Szekely (2016). The algorithm is faster above a certain sample size n. It does not store the distance matrix so the sample size can be very large. 
</p>


<h3>Value</h3>

<p>By default, <code>dcov2d</code> returns the V-statistic <code class="reqn">V_n = dCov_n^2(x, y)</code>, and if type=&quot;U&quot;, it returns the U-statistic, unbiased for <code class="reqn">dCov^2(X, Y)</code>. The argument all.stats=TRUE is used internally when the function is called from <code>dcor2d</code>. 
</p>
<p>By default, <code>dcor2d</code> returns <code class="reqn">dCor_n^2(x, y)</code>, and if type=&quot;U&quot;, it returns a bias-corrected estimator of squared dcor equivalent to <code>bcdcor</code>.
</p>
<p>These functions do not store the distance matrices so they are helpful when sample size is large and the data is bivariate. 
</p>


<h3>Note</h3>

<p>The U-statistic <code class="reqn">U_n</code> can be negative in the lower tail so 
the square root of the U-statistic is not applied.
Similarly, <code>dcor2d(x, y, "U")</code> is bias-corrected and can be
negative in the lower tail, so we do not take the
square root. The original definitions of dCov and dCor 
(SRB2007, SR2009) were based on V-statistics, which are non-negative,
and defined using the square root of V-statistics.
</p>
<p>It has been suggested that instead of taking the square root of the U-statistic, one could take the root of <code class="reqn">|U_n|</code> before applying the sign, but that introduces more bias than the original dCor, and should never be used. 
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Huo, X. and Szekely, G.J. (2016). Fast computing for 
distance covariance. Technometrics, 58(4), 435-447.
</p>
<p>Szekely, G.J. and Rizzo, M.L. (2014),
Partial Distance Correlation with Methods for Dissimilarities.
<em>Annals of Statistics</em>, Vol. 42 No. 6, 2382-2412.
</p>
<p>Szekely, G.J., Rizzo, M.L., and Bakirov, N.K. (2007),
Measuring and Testing Dependence by Correlation of Distances,
<em>Annals of Statistics</em>, Vol. 35 No. 6, pp. 2769-2794.
<br /> <a href="https://doi.org/10.1214/009053607000000505">doi:10.1214/009053607000000505</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dcov">dcov</a></code> <code><a href="#topic+dcov.test">dcov.test</a></code>  <code><a href="#topic+dcor">dcor</a></code> <code><a href="#topic+dcor.test">dcor.test</a></code> (multivariate statistics and permutation test) 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
    ## these are equivalent, but 2d is faster for n &gt; 50
    n &lt;- 100
    x &lt;- rnorm(100)
    y &lt;- rnorm(100)
    all.equal(dcov(x, y)^2, dcov2d(x, y), check.attributes = FALSE)
    all.equal(bcdcor(x, y), dcor2d(x, y, "U"), check.attributes = FALSE)

    x &lt;- rlnorm(400)
    y &lt;- rexp(400)
    dcov.test(x, y, R=199)    #permutation test
    dcor.test(x, y, R=199)
      
</code></pre>

<hr>
<h2 id='dcovU_stats'>Unbiased distance covariance statistics</h2><span id='topic+dcovU_stats'></span>

<h3>Description</h3>

<p>This function computes unbiased estimators of squared distance
covariance, distance variance, and a bias-corrected estimator of
(squared) distance correlation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dcovU_stats(Dx, Dy)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dcovU_stats_+3A_dx">Dx</code></td>
<td>
<p> distance matrix of first sample</p>
</td></tr>
<tr><td><code id="dcovU_stats_+3A_dy">Dy</code></td>
<td>
<p> distance matrix of second sample</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The unbiased (squared) dcov is inner product definition of
dCov, in the Hilbert space of U-centered distance matrices.
</p>
<p>The sample sizes (number of rows) of the two samples must
agree, and samples must not contain missing values. The
arguments must be square symmetric matrices. 
</p>


<h3>Value</h3>

<p><code>dcovU_stats</code> returns a vector of the components of bias-corrected
dcor: [dCovU, bcdcor, dVarXU, dVarYU].
</p>


<h3>Note</h3>

<p>Unbiased distance covariance (SR2014) corresponds to the biased
(original) <code class="reqn">\mathrm{dCov^2}</code>. Since <code>dcovU</code> is an
unbiased statistic, it is signed and we do not take the square root.
For the original distance covariance test of independence (SRB2007,
SR2009), the distance covariance test statistic is the V-statistic
<code class="reqn">\mathrm{n\, dCov^2} = n \mathcal{V}_n^2</code> (not dCov).
Similarly, <code>bcdcor</code> is bias-corrected, so we do not take the
square root as with dCor.
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Szekely, G.J. and Rizzo, M.L. (2014),
Partial Distance Correlation with Methods for Dissimilarities.
<em>Annals of Statistics</em>, Vol. 42 No. 6, 2382-2412.
</p>
<p>Szekely, G.J., Rizzo, M.L., and Bakirov, N.K. (2007),
Measuring and Testing Dependence by Correlation of Distances,
<em>Annals of Statistics</em>, Vol. 35 No. 6, pp. 2769-2794.
<br /> <a href="https://doi.org/10.1214/009053607000000505">doi:10.1214/009053607000000505</a>
</p>
<p>Szekely, G.J. and Rizzo, M.L. (2009),
Brownian Distance Covariance,
<em>Annals of Applied Statistics</em>,
Vol. 3, No. 4, 1236-1265.
<br /> <a href="https://doi.org/10.1214/09-AOAS312">doi:10.1214/09-AOAS312</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x &lt;- iris[1:50, 1:4]
 y &lt;- iris[51:100, 1:4]
 Dx &lt;- as.matrix(dist(x))
 Dy &lt;- as.matrix(dist(y))
 dcovU_stats(Dx, Dy)
 </code></pre>

<hr>
<h2 id='disco'> distance components (DISCO)</h2><span id='topic+disco'></span><span id='topic+disco.between'></span><span id='topic+print.disco'></span>

<h3>Description</h3>

<p>E-statistics DIStance COmponents and tests, analogous to variance components
and anova.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>disco(x, factors, distance, index=1.0, R, method=c("disco","discoB","discoF"))
disco.between(x, factors, distance, index=1.0, R)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="disco_+3A_x">x</code></td>
<td>
<p> data matrix or distance matrix or dist object</p>
</td></tr>
<tr><td><code id="disco_+3A_factors">factors</code></td>
<td>
<p> matrix or data frame of factor labels or integers (not design matrix)</p>
</td></tr>
<tr><td><code id="disco_+3A_distance">distance</code></td>
<td>
<p> logical, TRUE if x is distance matrix</p>
</td></tr>
<tr><td><code id="disco_+3A_index">index</code></td>
<td>
<p> exponent on Euclidean distance in (0,2]</p>
</td></tr>
<tr><td><code id="disco_+3A_r">R</code></td>
<td>
<p> number of replicates for a permutation test</p>
</td></tr>
<tr><td><code id="disco_+3A_method">method</code></td>
<td>
<p> test statistic </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>disco</code> calculates the distance components decomposition of
total dispersion and if R &gt; 0 tests for significance using the test statistic
disco &quot;F&quot; ratio (default <code>method="disco"</code>),
or using the between component statistic (<code>method="discoB"</code>),
each implemented by permutation test.
</p>
<p>If <code>x</code> is a <code>dist</code> object, argument <code>distance</code> is
ignored. If <code>x</code> is a distance matrix, set <code>distance=TRUE</code>. 
</p>
<p>In the current release <code>disco</code> computes the decomposition for one-way models
only.
</p>


<h3>Value</h3>

<p>When <code>method="discoF"</code>, <code>disco</code> returns a list similar to the
return value from <code>anova.lm</code>, and the <code>print.disco</code> method is
provided to format the output into a similar table. Details:
</p>
<p><code>disco</code> returns a class <code>disco</code> object, which is a list containing
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>call</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>method</p>
</td></tr>
<tr><td><code>statistic</code></td>
<td>
<p>vector of observed statistics</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>vector of p-values</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>number of factors</p>
</td></tr>
<tr><td><code>N</code></td>
<td>
<p>number of observations</p>
</td></tr>
<tr><td><code>between</code></td>
<td>
<p>between-sample distance components</p>
</td></tr>
<tr><td><code>withins</code></td>
<td>
<p>one-way within-sample distance components</p>
</td></tr>
<tr><td><code>within</code></td>
<td>
<p>within-sample distance component</p>
</td></tr>
<tr><td><code>total</code></td>
<td>
<p>total dispersion</p>
</td></tr>
<tr><td><code>Df.trt</code></td>
<td>
<p>degrees of freedom for treatments</p>
</td></tr>
<tr><td><code>Df.e</code></td>
<td>
<p>degrees of freedom for error</p>
</td></tr>
<tr><td><code>index</code></td>
<td>
<p>index (exponent on distance)</p>
</td></tr>
<tr><td><code>factor.names</code></td>
<td>
<p>factor names</p>
</td></tr>
<tr><td><code>factor.levels</code></td>
<td>
<p>factor levels</p>
</td></tr>
<tr><td><code>sample.sizes</code></td>
<td>
<p>sample sizes</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>
<p>matrix containing decomposition</p>
</td></tr>
</table>
<p>When <code>method="discoB"</code>, <code>disco</code> passes the arguments to
<code>disco.between</code>, which returns a class <code>htest</code> object.
</p>
<p><code>disco.between</code> returns a class <code>htest</code> object, where the test
statistic is the between-sample statistic (proportional to the numerator of the F ratio
of the <code>disco</code> test.
</p>


<h3>Note</h3>

<p>The current version does all calculations via matrix arithmetic and
boot function. Support for more general additive models
and a formula interface is under development.
</p>
<p><code>disco</code> methods have been added to the cluster distance summary
function <code>edist</code>, and energy tests for equality of distribution
(see <code>eqdist.etest</code>).
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and Gabor J. Szekely
</p>


<h3>References</h3>

<p>M. L. Rizzo and G. J. Szekely (2010).
DISCO Analysis: A Nonparametric Extension of
Analysis of Variance, Annals of Applied Statistics,
Vol. 4, No. 2, 1034-1055.
<br /> <a href="https://doi.org/10.1214/09-AOAS245">doi:10.1214/09-AOAS245</a>
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+edist">edist</a> </code>
<code> <a href="#topic+eqdist.e">eqdist.e</a> </code>
<code> <a href="#topic+eqdist.etest">eqdist.etest</a> </code>
<code> <a href="#topic+ksample.e">ksample.e</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>      ## warpbreaks one-way decompositions
      data(warpbreaks)
      attach(warpbreaks)
      disco(breaks, factors=wool, R=99)
      
      ## warpbreaks two-way wool+tension
      disco(breaks, factors=data.frame(wool, tension), R=0)

      ## warpbreaks two-way wool*tension
      disco(breaks, factors=data.frame(wool, tension, wool:tension), R=0)

      ## When index=2 for univariate data, we get ANOVA decomposition
      disco(breaks, factors=tension, index=2.0, R=99)
      aov(breaks ~ tension)

      ## Multivariate response
      ## Example on producing plastic film from Krzanowski (1998, p. 381)
      tear &lt;- c(6.5, 6.2, 5.8, 6.5, 6.5, 6.9, 7.2, 6.9, 6.1, 6.3,
                6.7, 6.6, 7.2, 7.1, 6.8, 7.1, 7.0, 7.2, 7.5, 7.6)
      gloss &lt;- c(9.5, 9.9, 9.6, 9.6, 9.2, 9.1, 10.0, 9.9, 9.5, 9.4,
                 9.1, 9.3, 8.3, 8.4, 8.5, 9.2, 8.8, 9.7, 10.1, 9.2)
      opacity &lt;- c(4.4, 6.4, 3.0, 4.1, 0.8, 5.7, 2.0, 3.9, 1.9, 5.7,
                   2.8, 4.1, 3.8, 1.6, 3.4, 8.4, 5.2, 6.9, 2.7, 1.9)
      Y &lt;- cbind(tear, gloss, opacity)
      rate &lt;- factor(gl(2,10), labels=c("Low", "High"))

	    ## test for equal distributions by rate
      disco(Y, factors=rate, R=99)
	    disco(Y, factors=rate, R=99, method="discoB")

      ## Just extract the decomposition table
      disco(Y, factors=rate, R=0)$stats

	    ## Compare eqdist.e methods for rate
	    ## disco between stat is half of original when sample sizes equal
	    eqdist.e(Y, sizes=c(10, 10), method="original")
	    eqdist.e(Y, sizes=c(10, 10), method="discoB")

      ## The between-sample distance component
      disco.between(Y, factors=rate, R=0)
</code></pre>

<hr>
<h2 id='distance+20correlation'> Distance Correlation and Covariance Statistics</h2><span id='topic+dcor'></span><span id='topic+dcov'></span>

<h3>Description</h3>

<p>Computes distance covariance and distance correlation statistics,
which are multivariate measures of dependence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dcov(x, y, index = 1.0)
dcor(x, y, index = 1.0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distance+2B20correlation_+3A_x">x</code></td>
<td>
<p> data or distances of first sample</p>
</td></tr>
<tr><td><code id="distance+2B20correlation_+3A_y">y</code></td>
<td>
<p> data or distances of second sample</p>
</td></tr>
<tr><td><code id="distance+2B20correlation_+3A_index">index</code></td>
<td>
<p> exponent on Euclidean distance, in (0,2]</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>dcov</code> and <code>dcor</code> compute distance
covariance and distance correlation statistics.
</p>
<p>The sample sizes (number of rows) of the two samples must
agree, and samples must not contain missing values. 
</p>
<p>The <code>index</code> is an optional exponent on Euclidean distance.
Valid exponents for energy are in (0, 2) excluding 2. 
</p>
<p>Argument types supported are 
numeric data matrix, data.frame, or tibble, with observations in rows;
numeric vector; ordered or unordered factors. In case of unordered factors
a 0-1 distance matrix is computed.
</p>
<p>Optionally pre-computed distances can be input as class &quot;dist&quot; objects or as distance matrices. 
For data types of arguments, distance matrices are computed internally. 
</p>
<p>Distance correlation is a new measure of dependence between random
vectors introduced by Szekely, Rizzo, and Bakirov (2007).
For all distributions with finite first moments, distance
correlation <code class="reqn">\mathcal R</code> generalizes the idea of correlation in two
fundamental ways:
(1) <code class="reqn">\mathcal R(X,Y)</code> is defined for <code class="reqn">X</code> and <code class="reqn">Y</code> in arbitrary dimension.
(2) <code class="reqn">\mathcal R(X,Y)=0</code> characterizes independence of <code class="reqn">X</code> and
<code class="reqn">Y</code>.
</p>
<p>Distance correlation satisfies <code class="reqn">0 \le \mathcal R \le 1</code>, and
<code class="reqn">\mathcal R = 0</code> only if <code class="reqn">X</code> and <code class="reqn">Y</code> are independent. Distance
covariance <code class="reqn">\mathcal V</code> provides a new approach to the problem of
testing the joint independence of random vectors. The formal
definitions of the population coefficients <code class="reqn">\mathcal V</code> and
<code class="reqn">\mathcal R</code> are given in (SRB 2007). The definitions of the
empirical coefficients are as follows.
</p>
<p>The empirical distance covariance <code class="reqn">\mathcal{V}_n(\mathbf{X,Y})</code>
with index 1 is
the nonnegative number defined by
</p>
<p style="text-align: center;"><code class="reqn">
 \mathcal{V}^2_n (\mathbf{X,Y}) = \frac{1}{n^2} \sum_{k,\,l=1}^n
 A_{kl}B_{kl}
 </code>
</p>

<p>where <code class="reqn">A_{kl}</code> and <code class="reqn">B_{kl}</code> are
</p>
<p style="text-align: center;"><code class="reqn">
A_{kl} = a_{kl}-\bar a_{k.}- \bar a_{.l} + \bar a_{..}
</code>
</p>

<p style="text-align: center;"><code class="reqn">
 B_{kl} = b_{kl}-\bar b_{k.}- \bar b_{.l} + \bar b_{..}.
 </code>
</p>

<p>Here
</p>
<p style="text-align: center;"><code class="reqn">
a_{kl} = \|X_k - X_l\|_p, \quad b_{kl} = \|Y_k - Y_l\|_q, \quad
k,l=1,\dots,n,
</code>
</p>

<p>and the subscript <code>.</code> denotes that the mean is computed for the
index that it replaces.  Similarly,
<code class="reqn">\mathcal{V}_n(\mathbf{X})</code> is the nonnegative number defined by
</p>
<p style="text-align: center;"><code class="reqn">
 \mathcal{V}^2_n (\mathbf{X}) = \mathcal{V}^2_n (\mathbf{X,X}) =
 \frac{1}{n^2} \sum_{k,\,l=1}^n
 A_{kl}^2.
 </code>
</p>

<p>The empirical distance correlation <code class="reqn">\mathcal{R}_n(\mathbf{X,Y})</code> is
the square root of
</p>
<p style="text-align: center;"><code class="reqn">
  \mathcal{R}^2_n(\mathbf{X,Y})=
 \frac {\mathcal{V}^2_n(\mathbf{X,Y})}
 {\sqrt{ \mathcal{V}^2_n (\mathbf{X}) \mathcal{V}^2_n(\mathbf{Y})}}.
</code>
</p>

<p>See <code><a href="#topic+dcov.test">dcov.test</a></code> for a test of multivariate independence
based on the distance covariance statistic. 
</p>


<h3>Value</h3>

<p><code>dcov</code> returns the sample distance covariance and
<code>dcor</code> returns the sample distance correlation.
</p>


<h3>Note</h3>

<p>Note that it is inefficient to compute dCor by:
</p>
<p>square root of
<code>dcov(x,y)/sqrt(dcov(x,x)*dcov(y,y))</code>
</p>
<p>because the individual
calls to <code>dcov</code> involve unnecessary repetition of calculations.
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Szekely, G.J., Rizzo, M.L., and Bakirov, N.K. (2007),
Measuring and Testing Dependence by Correlation of Distances,
<em>Annals of Statistics</em>, Vol. 35 No. 6, pp. 2769-2794.
<br /> <a href="https://doi.org/10.1214/009053607000000505">doi:10.1214/009053607000000505</a>
</p>
<p>Szekely, G.J. and Rizzo, M.L. (2009),
Brownian Distance Covariance,
<em>Annals of Applied Statistics</em>,
Vol. 3, No. 4, 1236-1265.
<br /> <a href="https://doi.org/10.1214/09-AOAS312">doi:10.1214/09-AOAS312</a>
</p>
<p>Szekely, G.J. and Rizzo, M.L. (2009),
Rejoinder: Brownian Distance Covariance,
<em>Annals of Applied Statistics</em>, Vol. 3, No. 4, 1303-1308.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dcov2d">dcov2d</a></code> <code><a href="#topic+dcor2d">dcor2d</a></code> 
<code><a href="#topic+bcdcor">bcdcor</a></code>  <code><a href="#topic+dcovU">dcovU</a></code>  <code><a href="#topic+pdcor">pdcor</a></code>
<code><a href="#topic+dcov.test">dcov.test</a></code> <code><a href="#topic+dcor.test">dcor.test</a></code>  <code><a href="#topic+pdcor.test">pdcor.test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x &lt;- iris[1:50, 1:4]
 y &lt;- iris[51:100, 1:4]
 dcov(x, y)
 dcov(dist(x), dist(y))  #same thing
</code></pre>

<hr>
<h2 id='Distance+20Matrix'> Distance Matrices </h2><span id='topic+is.dmatrix'></span><span id='topic+calc_dist'></span>

<h3>Description</h3>

<p>Utilities for working with distance matrices. 
<code>is.dmatrix</code> is a utility that checks whether the argument is a distance or dissimilarity matrix; is it square symmetric, non-negative, with zero diagonal? <code>calc_dist</code> computes a distance matrix directly from a data matrix. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.dmatrix(x, tol = 100 * .Machine$double.eps)
calc_dist(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Distance+2B20Matrix_+3A_x">x</code></td>
<td>
<p> numeric matrix</p>
</td></tr>
<tr><td><code id="Distance+2B20Matrix_+3A_tol">tol</code></td>
<td>
<p> tolerance for checking required conditions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Energy functions work with the distance matrices of samples. The <code>is.dmatrix</code> function is used internally when converting arguments to distance matrices. The default <code>tol</code> is the same as default tolerance of <code>isSymmetric</code>.
</p>
<p><code>calc_dist</code> is an exported Rcpp function that returns a Euclidean distance matrix from the input data matrix. 
</p>


<h3>Value</h3>

<p><code>is.dmatrix</code> returns TRUE if (within tolerance) <code>x</code> is a distance/dissimilarity matrix; otherwise FALSE. It will return FALSE if <code>x</code> is a class <code>dist</code> object.
</p>
<p><code>calc_dist</code> returns the Euclidean distance matrix for the data matrix <code>x</code>, which has observations in rows. 
</p>


<h3>Note</h3>

<p>In practice, if <code>dist(x)</code> is not yet computed, <code>calc_dist(x)</code> will be faster than <code>as.matrix(dist(x))</code>. 
</p>
<p>On working with non-Euclidean dissimilarities, see the references.
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a>
</p>


<h3>References</h3>

<p>Szekely, G.J. and Rizzo, M.L. (2014),
Partial Distance Correlation with Methods for Dissimilarities.
<em>Annals of Statistics</em>, Vol. 42 No. 6, 2382-2412.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(rnorm(20), 10, 2)
D &lt;- calc_dist(x)
is.dmatrix(D)
is.dmatrix(cov(x))
</code></pre>

<hr>
<h2 id='edist'>E-distance</h2><span id='topic+edist'></span>

<h3>Description</h3>

<p>Returns the E-distances (energy statistics) between clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> edist(x, sizes, distance = FALSE, ix = 1:sum(sizes), alpha = 1,
        method = c("cluster","discoB"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="edist_+3A_x">x</code></td>
<td>
<p> data matrix of pooled sample or Euclidean distances</p>
</td></tr>
<tr><td><code id="edist_+3A_sizes">sizes</code></td>
<td>
<p> vector of sample sizes</p>
</td></tr>
<tr><td><code id="edist_+3A_distance">distance</code></td>
<td>
<p> logical: if TRUE, x is a distance matrix</p>
</td></tr>
<tr><td><code id="edist_+3A_ix">ix</code></td>
<td>
<p> a permutation of the row indices of x </p>
</td></tr>
<tr><td><code id="edist_+3A_alpha">alpha</code></td>
<td>
<p> distance exponent in (0,2]</p>
</td></tr>
<tr><td><code id="edist_+3A_method">method</code></td>
<td>
<p> how to weight the statistics </p>
</td></tr>
</table>


<h3>Details</h3>

<p>A vector containing the pairwise two-sample multivariate
<code class="reqn">\mathcal{E}</code>-statistics for comparing clusters or samples is returned.
The e-distance between clusters is computed from the original pooled data,
stacked in matrix <code>x</code> where each row is a multivariate observation, or
from the distance matrix <code>x</code> of the original data, or distance object
returned by <code>dist</code>. The first <code>sizes[1]</code> rows of the original data
matrix are the first sample, the next <code>sizes[2]</code> rows are the second
sample, etc. The permutation vector <code>ix</code> may be used to obtain
e-distances corresponding to a clustering solution at a given level in
the hierarchy.
</p>
<p>The default method <code>cluster</code> summarizes the e-distances between
clusters in a table.
The e-distance between two clusters <code class="reqn">C_i, C_j</code>
of size <code class="reqn">n_i, n_j</code>
proposed by Szekely and Rizzo (2005)
is the e-distance <code class="reqn">e(C_i,C_j)</code>, defined by
</p>
<p style="text-align: center;"><code class="reqn">e(C_i,C_j)=\frac{n_i n_j}{n_i+n_j}[2M_{ij}-M_{ii}-M_{jj}],
  </code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">M_{ij}=\frac{1}{n_i n_j}\sum_{p=1}^{n_i} \sum_{q=1}^{n_j}
     \|X_{ip}-X_{jq}\|^\alpha,</code>
</p>

<p><code class="reqn">\|\cdot\|</code> denotes Euclidean norm, <code class="reqn">\alpha=</code>
<code>alpha</code>, and <code class="reqn">X_{ip}</code> denotes the p-th observation in the i-th cluster.  The
exponent <code>alpha</code> should be in the interval (0,2].
</p>
<p>The coefficient <code class="reqn">\frac{n_i n_j}{n_i+n_j}</code>
is one-half of the harmonic mean of the sample sizes. The
<code>discoB</code> method is related but with
different ways of summarizing the pairwise differences between samples.
The <code>disco</code> methods apply the coefficient
<code class="reqn">\frac{n_i n_j}{2N}</code> where N is the total number
of observations. This weights each (i,j) statistic by sample size
relative to N. See the <code>disco</code> topic for more details.
</p>


<h3>Value</h3>

<p>A object of class <code>dist</code> containing the lower triangle of the
e-distance matrix of cluster distances corresponding to the permutation
of indices <code>ix</code> is returned. The <code>method</code> attribute of the
distance object is assigned a value of type, index.
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Szekely, G. J. and Rizzo, M. L. (2005) Hierarchical Clustering
via Joint Between-Within Distances: Extending Ward's Minimum
Variance Method, <em>Journal of Classification</em> 22(2) 151-183.
<br /> <a href="https://doi.org/10.1007/s00357-005-0012-9">doi:10.1007/s00357-005-0012-9</a>
</p>
<p>M. L. Rizzo and G. J. Szekely (2010).
DISCO Analysis: A Nonparametric Extension of
Analysis of Variance, Annals of Applied Statistics,
Vol. 4, No. 2, 1034-1055.
<br /> <a href="https://doi.org/10.1214/09-AOAS245">doi:10.1214/09-AOAS245</a>
</p>
<p>Szekely, G. J. and Rizzo, M. L. (2004) Testing for Equal
Distributions in High Dimension, InterStat, November (5).
</p>
<p>Szekely, G. J. (2000) Technical Report 03-05,
<code class="reqn">\mathcal{E}</code>-statistics: Energy of
Statistical Samples, Department of Mathematics and Statistics,
Bowling Green State University.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+energy.hclust">energy.hclust</a></code>
<code><a href="#topic+eqdist.etest">eqdist.etest</a></code>
<code><a href="#topic+ksample.e">ksample.e</a></code>
<code><a href="#topic+disco">disco</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>     ## compute cluster e-distances for 3 samples of iris data
     data(iris)
     edist(iris[,1:4], c(50,50,50))
    
     ## pairwise disco statistics
     edist(iris[,1:4], c(50,50,50), method="discoB")  

     ## compute e-distances from a distance object
     data(iris)
     edist(dist(iris[,1:4]), c(50, 50, 50), distance=TRUE, alpha = 1)

     ## compute e-distances from a distance matrix
     data(iris)
     d &lt;- as.matrix(dist(iris[,1:4]))
     edist(d, c(50, 50, 50), distance=TRUE, alpha = 1)

 </code></pre>

<hr>
<h2 id='energy-deprecated'> Deprecated Functions</h2><span id='topic+dcor.ttest'></span><span id='topic+dcor.t'></span><span id='topic+DCOR'></span>

<h3>Description</h3>

<p> These deprecated functions have been replaced by revised functions and will be removed in future releases of the energy package.</p>


<h3>Usage</h3>

<pre><code class='language-R'>dcor.ttest(x, y, distance=FALSE)
dcor.t(x, y, distance=FALSE)
DCOR(x, y, index=1.0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="energy-deprecated_+3A_x">x</code></td>
<td>
<p> data or distances of first sample</p>
</td></tr>
<tr><td><code id="energy-deprecated_+3A_y">y</code></td>
<td>
<p> data or distances of second sample</p>
</td></tr>
<tr><td><code id="energy-deprecated_+3A_distance">distance</code></td>
<td>
<p> logical: TRUE if x and y are distances</p>
</td></tr>
<tr><td><code id="energy-deprecated_+3A_index">index</code></td>
<td>
<p> exponent on Euclidean distance in (0, 2)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>* dcor.t has been replaced by <code>dcorT</code>. See <a href="#topic+dcorT">dcorT</a> for details.
* dcor.ttest has been replaced by <code>dcorT.test</code>. See <a href="#topic+dcorT.test">dcorT.test</a> for details. 
* DCOR is an R version replaced by faster compiled code.
</p>

<hr>
<h2 id='energy-package'>
E-statistics: Multivariate Inference via the Energy of Data
</h2><span id='topic+energy-package'></span><span id='topic+energy'></span>

<h3>Description</h3>

<p>Description: E-statistics (energy) tests and statistics for multivariate and univariate inference,
including distance correlation, one-sample, two-sample, and multi-sample tests for
comparing multivariate distributions, are implemented. Measuring and testing
multivariate independence based on distance correlation, partial distance
correlation, multivariate goodness-of-fit tests, clustering based on energy distance,
testing for multivariate normality, distance components (disco) for non-parametric
analysis of structured data, and other energy statistics/methods are implemented.
</p>


<h3>Author(s)</h3>

<p>Maria L. Rizzo and Gabor J. Szekely
</p>


<h3>References</h3>

<p>G. J. Szekely and M. L. Rizzo (2013). Energy statistics:
A class of statistics based on distances, <em>Journal of
Statistical Planning and Inference</em>,
<a href="https://doi.org/10.1016/j.jspi.2013.03.018">doi:10.1016/j.jspi.2013.03.018</a>
</p>
<p>M. L. Rizzo and G. J. Szekely (2016). Energy Distance,
<em>WIRES Computational Statistics</em>, Wiley, Volume 8 Issue 1, 27-38.
Available online Dec., 2015, <a href="https://doi.org/10.1002/wics.1375">doi:10.1002/wics.1375</a>.
</p>
<p>G. J. Szekely and M. L. Rizzo (2017). The Energy of Data. 
<em>The Annual Review of Statistics and Its Application</em> 
4:447-79.  <a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-060116-054026">https://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-060116-054026</a>
</p>
<p>G. J. Szekely and M. L. Rizzo (2023). <em>The Energy of Data and Distance Correlation</em>. Chapman &amp; Hall/CRC Monographs on Statistics and Applied Probability. ISBN 9781482242744. 
<a href="https://www.routledge.com/The-Energy-of-Data-and-Distance-Correlation/Szekely-Rizzo/p/book/9781482242744">https://www.routledge.com/The-Energy-of-Data-and-Distance-Correlation/Szekely-Rizzo/p/book/9781482242744</a>.
</p>

<hr>
<h2 id='energy.hclust'> Hierarchical Clustering by Minimum (Energy) E-distance </h2><span id='topic+energy.hclust'></span>

<h3>Description</h3>

<p>Performs hierarchical clustering by minimum (energy) E-distance method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    energy.hclust(dst, alpha = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="energy.hclust_+3A_dst">dst</code></td>
<td>
<p><code>dist</code> object</p>
</td></tr>
<tr><td><code id="energy.hclust_+3A_alpha">alpha</code></td>
<td>
<p>distance exponent</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Dissimilarities are <code class="reqn">d(x,y) = \|x-y\|^\alpha</code>,
where the exponent <code class="reqn">\alpha</code> is in the interval (0,2].
This function performs agglomerative hierarchical clustering.
Initially, each of the n singletons is a cluster. At each of n-1 steps, the
procedure merges the pair of clusters with minimum e-distance.
The e-distance between two clusters <code class="reqn">C_i, C_j</code> of sizes <code class="reqn">n_i, n_j</code> 
is given by
</p>
<p style="text-align: center;"><code class="reqn">e(C_i, C_j)=\frac{n_i n_j}{n_i+n_j}[2M_{ij}-M_{ii}-M_{jj}],
    </code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">M_{ij}=\frac{1}{n_i n_j}\sum_{p=1}^{n_i} \sum_{q=1}^{n_j}
       \|X_{ip}-X_{jq}\|^\alpha,</code>
</p>

<p><code class="reqn">\|\cdot\|</code> denotes Euclidean norm, and <code class="reqn">X_{ip}</code> denotes the p-th observation in the i-th cluster.
</p>
<p>The return value is an object of class <code>hclust</code>, so <code>hclust</code>
methods such as print or plot methods, <code>plclust</code>, and <code>cutree</code>
are available. See the documentation for <code>hclust</code>.
</p>
<p>The e-distance measures both the heterogeneity between clusters and the
homogeneity within clusters. <code class="reqn">\mathcal E</code>-clustering
(<code class="reqn">\alpha=1</code>) is particularly effective in
high dimension, and is more effective than some standard hierarchical
methods when clusters have equal means (see example below).
For other advantages see the references.
</p>
<p><code>edist</code> computes the energy distances for the result (or any partition)
and returns the cluster distances in a <code>dist</code> object. See the <code>edist</code>
examples.
</p>


<h3>Value</h3>

<p>An object of class <code>hclust</code> which describes the tree produced by
the clustering process. The object is a list with components:
</p>
<table>
<tr><td><code>merge:</code></td>
<td>
<p> an n-1 by 2 matrix, where row i of <code>merge</code> describes the
merging of clusters at step i of the clustering. If an element j in the
row is negative, then observation -j was merged at this
stage. If j is positive then the merge was with the cluster
formed at the (earlier) stage j of the algorithm.</p>
</td></tr>
<tr><td><code>height:</code></td>
<td>
<p>the clustering height: a vector of n-1 non-decreasing
real numbers (the e-distance between merging clusters)</p>
</td></tr>
<tr><td><code>order:</code></td>
<td>
<p> a vector giving a permutation of the indices of
original observations suitable for plotting, in the sense that a
cluster plot using this ordering and matrix <code>merge</code> will not have
crossings of the branches.</p>
</td></tr>
<tr><td><code>labels:</code></td>
<td>
<p> labels for each of the objects being clustered.</p>
</td></tr>
<tr><td><code>call:</code></td>
<td>
<p> the call which produced the result.</p>
</td></tr>
<tr><td><code>method:</code></td>
<td>
<p> the cluster method that has been used (e-distance).</p>
</td></tr>
<tr><td><code>dist.method:</code></td>
<td>
<p> the distance that has been used to create <code>dst</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Currently <code>stats::hclust</code> implements Ward's method by <code>method="ward.D2"</code>,
which applies the squared distances. That method was previously <code>"ward"</code>. 
Because both <code>hclust</code> and energy use the same type of Lance-Williams recursive formula to update cluster distances, now with the additional option <code>method="ward.D"</code> in <code>hclust</code>, the
energy distance method is easily implemented by <code>hclust</code>. (Some &quot;Ward&quot; algorithms do not use Lance-Williams, however). Energy clustering (with <code>alpha=1</code>) and &quot;ward.D&quot; now return the same result, except that the cluster heights of energy hierarchical clustering with <code>alpha=1</code> are two times the heights from <code>hclust</code>. In order to ensure compatibility with hclust methods, <code>energy.hclust</code> now passes arguments through to <code>hclust</code> after possibly applying the optional exponent to distance.
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Szekely, G. J. and Rizzo, M. L. (2005) Hierarchical Clustering
via Joint Between-Within Distances: Extending Ward's Minimum
Variance Method, <em>Journal of Classification</em> 22(2) 151-183.
<br /> <a href="https://doi.org/10.1007/s00357-005-0012-9">doi:10.1007/s00357-005-0012-9</a>
</p>
<p>Szekely, G. J. and Rizzo, M. L. (2004) Testing for Equal
Distributions in High Dimension, <em>InterStat</em>, November (5).
</p>
<p>Szekely, G. J. (2000) Technical Report 03-05:
<code class="reqn">\mathcal{E}</code>-statistics: Energy of
Statistical Samples, Department of Mathematics and Statistics, Bowling
Green State University.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+edist">edist</a></code> <code><a href="#topic+ksample.e">ksample.e</a></code> <code><a href="#topic+eqdist.etest">eqdist.etest</a></code> <code>hclust</code></p>


<h3>Examples</h3>

<pre><code class='language-R'>   ## Not run: 
   library(cluster)
   data(animals)
   plot(energy.hclust(dist(animals)))

   data(USArrests)
   ecl &lt;- energy.hclust(dist(USArrests))
   print(ecl)
   plot(ecl)
   cutree(ecl, k=3)
   cutree(ecl, h=150)

   ## compare performance of e-clustering, Ward's method, group average method
   ## when sampled populations have equal means: n=200, d=5, two groups
   z &lt;- rbind(matrix(rnorm(1000), nrow=200), matrix(rnorm(1000, 0, 5), nrow=200))
   g &lt;- c(rep(1, 200), rep(2, 200))
   d &lt;- dist(z)
   e &lt;- energy.hclust(d)
   a &lt;- hclust(d, method="average")
   w &lt;- hclust(d^2, method="ward.D2")
   list("E" = table(cutree(e, k=2) == g), "Ward" = table(cutree(w, k=2) == g),
    "Avg" = table(cutree(a, k=2) == g))
  
## End(Not run)
 </code></pre>

<hr>
<h2 id='eqdist.etest'>Multisample E-statistic (Energy) Test of Equal Distributions</h2><span id='topic+eqdist.etest'></span><span id='topic+eqdist.e'></span><span id='topic+ksample.e'></span>

<h3>Description</h3>

<p>Performs the nonparametric multisample E-statistic (energy) test
for equality of multivariate distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eqdist.etest(x, sizes, distance = FALSE,
    method=c("original","discoB","discoF"), R)
eqdist.e(x, sizes, distance = FALSE,
    method=c("original","discoB","discoF"))
ksample.e(x, sizes, distance = FALSE,
    method=c("original","discoB","discoF"), ix = 1:sum(sizes))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="eqdist.etest_+3A_x">x</code></td>
<td>
<p> data matrix of pooled sample</p>
</td></tr>
<tr><td><code id="eqdist.etest_+3A_sizes">sizes</code></td>
<td>
<p> vector of sample sizes</p>
</td></tr>
<tr><td><code id="eqdist.etest_+3A_distance">distance</code></td>
<td>
<p>logical: if TRUE, first argument is a distance matrix</p>
</td></tr>
<tr><td><code id="eqdist.etest_+3A_method">method</code></td>
<td>
<p> use original (default) or distance components (discoB, discoF)</p>
</td></tr>
<tr><td><code id="eqdist.etest_+3A_r">R</code></td>
<td>
<p> number of bootstrap replicates </p>
</td></tr>
<tr><td><code id="eqdist.etest_+3A_ix">ix</code></td>
<td>
<p> a permutation of the row indices of x </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The k-sample multivariate <code class="reqn">\mathcal{E}</code>-test of equal distributions
is performed. The statistic is computed from the original
pooled samples, stacked in matrix <code>x</code> where each row
is a multivariate observation, or the corresponding distance matrix. The
first <code>sizes[1]</code> rows of <code>x</code> are the first sample, the next
<code>sizes[2]</code> rows of <code>x</code> are the second sample, etc.
</p>
<p>The test is implemented by nonparametric bootstrap, an approximate
permutation test with <code>R</code> replicates.
</p>
<p>The function <code>eqdist.e</code> returns the test statistic only; it simply
passes the arguments through to <code>eqdist.etest</code> with <code>R = 0</code>.
</p>
<p>The k-sample multivariate <code class="reqn">\mathcal{E}</code>-statistic for testing equal distributions
is returned. The statistic is computed from the original pooled samples, stacked in
matrix <code>x</code> where each row is a multivariate observation, or from the distance
matrix <code>x</code> of the original data. The
first <code>sizes[1]</code> rows of <code>x</code> are the first sample, the next
<code>sizes[2]</code> rows of <code>x</code> are the second sample, etc.
</p>
<p>The two-sample <code class="reqn">\mathcal{E}</code>-statistic proposed by
Szekely and Rizzo (2004)
is the e-distance <code class="reqn">e(S_i,S_j)</code>, defined for two samples <code class="reqn">S_i, S_j</code>
of size <code class="reqn">n_i, n_j</code> by
</p>
<p style="text-align: center;"><code class="reqn">e(S_i,S_j)=\frac{n_i n_j}{n_i+n_j}[2M_{ij}-M_{ii}-M_{jj}],
    </code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">M_{ij}=\frac{1}{n_i n_j}\sum_{p=1}^{n_i} \sum_{q=1}^{n_j}
       \|X_{ip}-X_{jq}\|,</code>
</p>

<p><code class="reqn">\|\cdot\|</code> denotes Euclidean norm, and <code class="reqn">X_{ip}</code> denotes the p-th observation in the i-th sample.
</p>
<p>The original (default method) k-sample
<code class="reqn">\mathcal{E}</code>-statistic is defined by summing the pairwise e-distances over
all <code class="reqn">k(k-1)/2</code> pairs
of samples:
</p>
<p style="text-align: center;"><code class="reqn">\mathcal{E}=\sum_{1 \leq i &lt; j \leq k} e(S_i,S_j).
    </code>
</p>

<p>Large values of <code class="reqn">\mathcal{E}</code> are significant.
</p>
<p>The <code>discoB</code> method computes the between-sample disco statistic.
For a one-way analysis, it is related to the original statistic as follows.
In the above equation, the weights <code class="reqn">\frac{n_i n_j}{n_i+n_j}</code>
are replaced with
</p>
<p style="text-align: center;"><code class="reqn">\frac{n_i + n_j}{2N}\frac{n_i n_j}{n_i+n_j} =
    \frac{n_i n_j}{2N}</code>
</p>

<p>where N is the total number of observations: <code class="reqn">N=n_1+...+n_k</code>.
</p>
<p>The <code>discoF</code> method is based on the disco F ratio, while the <code>discoB</code>
method is based on the between sample component.
</p>
<p>Also see <code>disco</code> and <code>disco.between</code> functions.
</p>


<h3>Value</h3>

<p>A list with class <code>htest</code> containing
</p>
<table>
<tr><td><code>method</code></td>
<td>
<p>description of test</p>
</td></tr>
<tr><td><code>statistic</code></td>
<td>
<p>observed value of the test statistic</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>approximate p-value of the test</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>description of data</p>
</td></tr>
</table>
<p><code>eqdist.e</code> returns test statistic only.
</p>


<h3>Note</h3>

<p>The pairwise e-distances between samples can be conveniently
computed by the <code>edist</code> function, which returns a <code>dist</code> object.
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Szekely, G. J. and Rizzo, M. L. (2004) Testing for Equal
Distributions in High Dimension, <em>InterStat</em>, November (5).
</p>
<p>M. L. Rizzo and G. J. Szekely (2010).
DISCO Analysis: A Nonparametric Extension of
Analysis of Variance, Annals of Applied Statistics,
Vol. 4, No. 2, 1034-1055.
<br /> <a href="https://doi.org/10.1214/09-AOAS245">doi:10.1214/09-AOAS245</a>
</p>
<p>Szekely, G. J. (2000) Technical Report 03-05:
<code class="reqn">\mathcal{E}</code>-statistics: Energy of
Statistical Samples, Department of Mathematics and Statistics, Bowling
Green State University.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ksample.e">ksample.e</a></code>,
<code><a href="#topic+edist">edist</a></code>,
<code><a href="#topic+disco">disco</a></code>,
<code><a href="#topic+disco.between">disco.between</a></code>,
<code><a href="#topic+energy.hclust">energy.hclust</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> data(iris)

 ## test if the 3 varieties of iris data (d=4) have equal distributions
 eqdist.etest(iris[,1:4], c(50,50,50), R = 199)

 ## example that uses method="disco"
  x &lt;- matrix(rnorm(100), nrow=20)
  y &lt;- matrix(rnorm(100), nrow=20)
  X &lt;- rbind(x, y)
  d &lt;- dist(X)

  # should match edist default statistic
  set.seed(1234)
  eqdist.etest(d, sizes=c(20, 20), distance=TRUE, R = 199)

  # comparison with edist
  edist(d, sizes=c(20, 10), distance=TRUE)

  # for comparison
  g &lt;- as.factor(rep(1:2, c(20, 20)))
  set.seed(1234)
  disco(d, factors=g, distance=TRUE, R=199)

  # should match statistic in edist method="discoB", above
  set.seed(1234)
  disco.between(d, factors=g, distance=TRUE, R=199)
</code></pre>

<hr>
<h2 id='EVnormal'>Eigenvalues for the energy Test of Univariate Normality</h2><span id='topic+EVnormal'></span><span id='topic+eigenvalues'></span>

<h3>Description</h3>

<p>Pre-computed eigenvalues corresponding to the asymptotic sampling
distribution of the energy test statistic for univariate
normality, under the null hypothesis. Four Cases are computed:
</p>

<ol>
<li><p> Simple hypothesis, known parameters.
</p>
</li>
<li><p> Estimated mean, known variance.
</p>
</li>
<li><p> Known mean, estimated variance.
</p>
</li>
<li><p> Composite hypothesis, estimated parameters.
</p>
</li></ol>

<p>Case 4 eigenvalues are used in the test function <code>normal.test</code>
when <code>method=="limit"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(EVnormal)</code></pre>


<h3>Format</h3>

<p>Numeric matrix with 125 rows and 5 columns;
column 1 is the index, and columns 2-5 are 
the eigenvalues of Cases 1-4.</p>


<h3>Source</h3>

<p>Computed</p>


<h3>References</h3>

<p>Szekely, G. J. and Rizzo, M. L. (2005) A New Test for
Multivariate Normality, <em>Journal of Multivariate Analysis</em>,
93/1, 58-80,
<a href="https://doi.org/10.1016/j.jmva.2003.12.002">doi:10.1016/j.jmva.2003.12.002</a>.
</p>

<hr>
<h2 id='indep.etest'> Energy Statistic Test of Independence</h2><span id='topic+indep.e'></span><span id='topic+indep.etest'></span>

<h3>Description</h3>

<p>Defunct: use <code>indep.test</code> with <code>method = mvI</code>.
Computes a multivariate nonparametric E-statistic and test of independence.</p>


<h3>Usage</h3>

<pre><code class='language-R'>indep.e(x, y)
indep.etest(x, y, R)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="indep.etest_+3A_x">x</code></td>
<td>
<p> matrix: first sample, observations in rows</p>
</td></tr>
<tr><td><code id="indep.etest_+3A_y">y</code></td>
<td>
<p> matrix: second sample, observations in rows</p>
</td></tr>
<tr><td><code id="indep.etest_+3A_r">R</code></td>
<td>
<p> number of replicates</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the coefficient <code class="reqn">\mathcal I</code> and performs a nonparametric
<code class="reqn">\mathcal E</code>-test of independence. The test decision is obtained via
bootstrap, with <code>R</code> replicates.
The sample sizes (number of rows) of the two samples must agree, and
samples must not contain missing values. The statistic
<code class="reqn">\mathcal E = n \mathcal I^2</code> is a ratio of V-statistics based
on interpoint distances <code class="reqn">\|x_{i}-y_{j}\|</code>.
See the reference below for details.
</p>


<h3>Value</h3>

<p>The sample coefficient <code class="reqn">\mathcal I</code> is returned by <code>indep.e</code>.
The function <code>indep.etest</code> returns a list with class
<code>htest</code> containing
</p>
<table>
<tr><td><code>method</code></td>
<td>
<p>description of test</p>
</td></tr>
<tr><td><code>statistic</code></td>
<td>
<p>observed value of the coefficient <code class="reqn">\mathcal I</code></p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>approximate p-value of the test</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>description of data</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Bakirov, N.K., Rizzo, M.L., and Szekely, G.J. (2006), A Multivariate
Nonparametric Test of Independence, <em>Journal of Multivariate Analysis</em>
93/1, 58-80
</p>

<hr>
<h2 id='indep.test'> Energy-tests of Independence</h2><span id='topic+indep.test'></span>

<h3>Description</h3>

<p>Computes a multivariate nonparametric test of independence.
The default method implements the distance covariance test
<code><a href="#topic+dcov.test">dcov.test</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>indep.test(x, y, method = c("dcov","mvI"), index = 1, R)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="indep.test_+3A_x">x</code></td>
<td>
<p> matrix: first sample, observations in rows</p>
</td></tr>
<tr><td><code id="indep.test_+3A_y">y</code></td>
<td>
<p> matrix: second sample, observations in rows</p>
</td></tr>
<tr><td><code id="indep.test_+3A_method">method</code></td>
<td>
<p> a character string giving the name of the test</p>
</td></tr>
<tr><td><code id="indep.test_+3A_index">index</code></td>
<td>
<p> exponent on Euclidean distances</p>
</td></tr>
<tr><td><code id="indep.test_+3A_r">R</code></td>
<td>
<p> number of replicates</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>indep.test</code> with the default <code>method = "dcov"</code> computes
the distance
covariance test of independence. <code>index</code> is an exponent on
the Euclidean distances. Valid choices for <code>index</code> are in (0,2],
with default value 1 (Euclidean distance). The arguments are passed
to the <code>dcov.test</code> function. See the help topic <code><a href="#topic+dcov.test">dcov.test</a></code> for
the description and documentation and also see the references below.
</p>
<p><code>indep.test</code> with <code>method = "mvI"</code>
computes the coefficient <code class="reqn">\mathcal I_n</code> and performs a nonparametric
<code class="reqn">\mathcal E</code>-test of independence. The arguments are passed to
<code>mvI.test</code>. The
<code>index</code> argument is ignored (<code>index = 1</code> is applied).
See the help topic <code><a href="#topic+mvI.test">mvI.test</a></code> and also
see the reference (2006) below for details.
</p>
<p>The test decision is obtained via
bootstrap, with <code>R</code> replicates.
The sample sizes (number of rows) of the two samples must agree, and
samples must not contain missing values.
</p>
<p>These energy tests of independence are based on related theoretical
results, but different test statistics.
The <code>dcov</code> method is faster than <code>mvI</code> method by
approximately a factor of O(n).
</p>


<h3>Value</h3>

<p><code>indep.test</code> returns a list with class
<code>htest</code> containing
</p>
<table>
<tr><td><code>method</code></td>
<td>
<p>description of test</p>
</td></tr>
<tr><td><code>statistic</code></td>
<td>
<p>observed value of the
test statistic <code class="reqn">n \mathcal V_n^2</code>
or <code class="reqn">n \mathcal I_n^2</code></p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
 <p><code class="reqn">\mathcal V_n</code> or <code class="reqn">\mathcal I_n</code></p>
</td></tr>
<tr><td><code>estimates</code></td>
<td>
<p> a vector [dCov(x,y), dCor(x,y), dVar(x), dVar(y)]
(method dcov)</p>
</td></tr>
<tr><td><code>replicates</code></td>
<td>
<p> replicates of the test statistic</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>approximate p-value of the test</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>description of data</p>
</td></tr>
</table>


<h3>Note</h3>

<p>As of energy-1.1-0,
<code>indep.etest</code> is deprecated and replaced by <code>indep.test</code>, which
has methods for two different energy tests of independence.  <code>indep.test</code> applies
the distance covariance test (see <code>dcov.test</code>) by default (<code>method = "dcov"</code>).
The original <code>indep.etest</code> applied the independence coefficient
<code class="reqn">\mathcal I_n</code>, which is now obtained by <code>method = "mvI"</code>.
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Szekely, G.J. and Rizzo, M.L. (2009),
Brownian Distance Covariance,
<em>Annals of Applied Statistics</em>, Vol. 3 No. 4, pp.
1236-1265. (Also see discussion and rejoinder.)
<br /> <a href="https://doi.org/10.1214/09-AOAS312">doi:10.1214/09-AOAS312</a>
</p>
<p>Szekely, G.J., Rizzo, M.L., and Bakirov, N.K. (2007),
Measuring and Testing Dependence by Correlation of Distances,
<em>Annals of Statistics</em>, Vol. 35 No. 6, pp. 2769-2794.
<br /> <a href="https://doi.org/10.1214/009053607000000505">doi:10.1214/009053607000000505</a>
</p>
<p>Bakirov, N.K., Rizzo, M.L., and Szekely, G.J. (2006), A Multivariate
Nonparametric Test of Independence, <em>Journal of Multivariate Analysis</em>
93/1, 58-80, <br />
<a href="https://doi.org/10.1016/j.jmva.2005.10.005">doi:10.1016/j.jmva.2005.10.005</a>
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+dcov.test">dcov.test</a> </code>
<code> <a href="#topic+mvI.test">mvI.test</a> </code>
<code> <a href="#topic+dcov">dcov</a> </code>
<code> <a href="#topic+mvI">mvI</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 ## independent multivariate data
 x &lt;- matrix(rnorm(60), nrow=20, ncol=3)
 y &lt;- matrix(rnorm(40), nrow=20, ncol=2)
 indep.test(x, y, method = "dcov", R = 99)
 indep.test(x, y, method = "mvI", R = 99)

 ## dependent multivariate data
 if (require(MASS)) {
   Sigma &lt;- matrix(c(1, .1, 0, 0 , 1, 0, 0 ,.1, 1), 3, 3)
   x &lt;- mvrnorm(30, c(0, 0, 0), diag(3))
   y &lt;- mvrnorm(30, c(0, 0, 0), Sigma) * x
   indep.test(x, y, R = 99)    #dcov method
   indep.test(x, y, method = "mvI", R = 99)
    }
  
</code></pre>

<hr>
<h2 id='kgroups'>
K-Groups Clustering
</h2><span id='topic+kgroups'></span>

<h3>Description</h3>

<p>Perform k-groups clustering by energy distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kgroups(x, k, iter.max = 10, nstart = 1, cluster = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kgroups_+3A_x">x</code></td>
<td>
<p>Data frame or data matrix or distance object</p>
</td></tr>
<tr><td><code id="kgroups_+3A_k">k</code></td>
<td>
<p>number of clusters</p>
</td></tr>
<tr><td><code id="kgroups_+3A_iter.max">iter.max</code></td>
<td>
<p>maximum number of iterations</p>
</td></tr>
<tr><td><code id="kgroups_+3A_nstart">nstart</code></td>
<td>
<p>number of restarts</p>
</td></tr>
<tr><td><code id="kgroups_+3A_cluster">cluster</code></td>
<td>
<p>initial clustering vector</p>
</td></tr>
</table>


<h3>Details</h3>

<p>K-groups is based on the multisample energy distance for comparing distributions.
Based on the disco decomposition of total dispersion (a Gini type mean distance) the objective function should either maximize the total between cluster energy distance, or equivalently, minimize the total within cluster energy distance. It is more computationally efficient to minimize within distances, and that makes it possible to use a modified version of the Hartigan-Wong algorithm (1979) to implement K-groups clustering.
</p>
<p>The within cluster Gini mean distance is
</p>
<p style="text-align: center;"><code class="reqn">G(C_j) = \frac{1}{n_j^2} \sum_{i,m=1}^{n_j} |x_{i,j} - x_{m,j}|</code>
</p>

<p>and the K-groups within cluster distance is
</p>
<p style="text-align: center;"><code class="reqn">W_j = \frac{n_j}{2}G(C_j) = \frac{1}{2 n_j} \sum_{i,m=1}^{n_j} |x_{i,j} - x_{m,j}.</code>
</p>

<p>If z is the data matrix for cluster <code class="reqn">C_j</code>, then <code class="reqn">W_j</code> could be computed as
<code>sum(dist(z)) / nrow(z)</code>.
</p>
<p>If cluster is not NULL, the clusters are initialized by this vector (can be a factor or integer vector). Otherwise clusters are initialized with random labels in k approximately equal size clusters.
</p>
<p>If <code>x</code> is not a distance object (class(x) == &quot;dist&quot;) then <code>x</code> is converted to a data matrix for analysis. 
</p>
<p>Run up to <code>iter.max</code> complete passes through the data set until a local min is reached. If <code>nstart &gt; 1</code>, on second and later starts, clusters are initialized at random, and the best result is returned. 
</p>


<h3>Value</h3>

<p>An object of class <code>kgroups</code> containing the components
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the function call</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>vector of cluster indices</p>
</td></tr>
<tr><td><code>sizes</code></td>
<td>
<p>cluster sizes</p>
</td></tr>
<tr><td><code>within</code></td>
<td>
<p>vector of Gini within cluster distances</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>sum of within cluster distances</p>
</td></tr>
<tr><td><code>count</code></td>
<td>
<p>number of moves</p>
</td></tr>
<tr><td><code>iterations</code></td>
<td>
<p>number of iterations</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>number of clusters</p>
</td></tr>
</table>
<p><code>cluster</code> is a vector containing the group labels, 1 to k. <code>print.kgroups</code>
prints some of the components of the kgroups object.
</p>
<p>Expect that count is 0 if the algorithm converged to a local min (that is, 0 moves happened on the last iteration). If iterations equals iter.max and count is positive, then the algorithm did not converge to a local min. 
</p>


<h3>Author(s)</h3>

<p>Maria Rizzo and Songzi Li
</p>


<h3>References</h3>

<p>Li, Songzi (2015).
&quot;K-groups: A Generalization of K-means by Energy Distance.&quot;
Ph.D. thesis, Bowling Green State University.
</p>
<p>Li, S. and Rizzo, M. L. (2017).
&quot;K-groups: A Generalization of K-means Clustering&quot;.
ArXiv e-print 1711.04359. https://arxiv.org/abs/1711.04359
</p>
<p>Szekely, G. J., and M. L. Rizzo. &quot;Testing for equal distributions in high dimension.&quot; InterStat 5, no. 16.10 (2004).
</p>
<p>Rizzo, M. L., and G. J. Szekely. &quot;Disco analysis: A nonparametric extension of analysis of variance.&quot; The Annals of Applied Statistics (2010): 1034-1055.
</p>
<p>Hartigan, J. A. and Wong, M. A. (1979). &quot;Algorithm AS 136: A K-means clustering algorithm.&quot; Applied Statistics, 28, 100-108. doi: 10.2307/2346830.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  x &lt;- as.matrix(iris[ ,1:4])
  set.seed(123)
  kg &lt;- kgroups(x, k = 3, iter.max = 5, nstart = 2)
  kg
  fitted(kg)
  
  
    d &lt;- dist(x)
    set.seed(123)
    kg &lt;- kgroups(d, k = 3, iter.max = 5, nstart = 2)
    kg
    
    kg$cluster
  
    fitted(kg)
    fitted(kg, method = "groups")
    
</code></pre>

<hr>
<h2 id='mutual+20independence'> Energy Test of Mutual Independence</h2><span id='topic+mutualIndep.test'></span>

<h3>Description</h3>

<p>The test statistic is the sum of d-1 bias-corrected squared dcor statistics where the number of variables is d. Implementation is by permuation test. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mutualIndep.test(x, R)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mutual+2B20independence_+3A_x">x</code></td>
<td>
<p> data matrix or data frame</p>
</td></tr>
<tr><td><code id="mutual+2B20independence_+3A_r">R</code></td>
<td>
<p> number of permutation replicates</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A population coefficient for mutual independence of d random variables, <code class="reqn">d \geq 2</code>,  is
</p>
<p style="text-align: center;"><code class="reqn">
  \sum_{k=1}^{d-1} \mathcal R^2(X_k, [X_{k+1},\dots,X_d]).
</code>
</p>

<p>which is non-negative and equals zero iff mutual independence holds. 
For example, if d=4 the population coefficient is 
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal R^2(X_1, [X_2,X_3,X_4]) +
\mathcal R^2(X_2, [X_3,X_4]) +
\mathcal R^2(X_3, X_4),
</code>
</p>

<p>A permutation test is implemented based on the corresponding sample coefficient. 
To test mutual independence of </p>
<p style="text-align: center;"><code class="reqn">X_1,\dots,X_d</code>
</p>
<p> the test statistic is the sum of the d-1 
statistics (bias-corrected <code class="reqn">dcor^2</code> statistics):
</p>
<p style="text-align: center;"><code class="reqn">\sum_{k=1}^{d-1} \mathcal R_n^*(X_k, [X_{k+1},\dots,X_d])</code>
</p>
<p>. 
</p>


<h3>Value</h3>

<p><code>mutualIndep.test</code> returns an object of class <code>power.htest</code>.
</p>


<h3>Note</h3>

<p>See Szekely and Rizzo (2014) for details on unbiased <code class="reqn">dCov^2</code> and bias-corrected <code class="reqn">dCor^2</code> (<code>bcdcor</code>) statistics.
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Szekely, G.J., Rizzo, M.L., and Bakirov, N.K. (2007),
Measuring and Testing Dependence by Correlation of Distances,
<em>Annals of Statistics</em>, Vol. 35 No. 6, pp. 2769-2794.
<br /> <a href="https://doi.org/10.1214/009053607000000505">doi:10.1214/009053607000000505</a>
</p>
<p>Szekely, G.J. and Rizzo, M.L. (2014),
Partial Distance Correlation with Methods for Dissimilarities.
<em>Annals of Statistics</em>, Vol. 42 No. 6, 2382-2412.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bcdcor">bcdcor</a></code>, <code><a href="#topic+dcovU_stats">dcovU_stats</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(rnorm(100), nrow=20, ncol=5)
mutualIndep.test(x, 199)
</code></pre>

<hr>
<h2 id='mvI.test'> Energy Statistic Test of Independence</h2><span id='topic+mvI.test'></span><span id='topic+mvI'></span>

<h3>Description</h3>

<p>Computes the multivariate nonparametric E-statistic and test of independence
based on independence coefficient <code class="reqn">\mathcal I_n</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>    mvI.test(x, y, R)
    mvI(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mvI.test_+3A_x">x</code></td>
<td>
<p> matrix: first sample, observations in rows</p>
</td></tr>
<tr><td><code id="mvI.test_+3A_y">y</code></td>
<td>
<p> matrix: second sample, observations in rows</p>
</td></tr>
<tr><td><code id="mvI.test_+3A_r">R</code></td>
<td>
<p> number of replicates</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the coefficient <code class="reqn">\mathcal I</code> and performs a nonparametric
<code class="reqn">\mathcal E</code>-test of independence. The test decision is obtained via
bootstrap, with <code>R</code> replicates.
The sample sizes (number of rows) of the two samples must agree, and
samples must not contain missing values. The statistic
<code class="reqn">\mathcal E = n \mathcal I^2</code> is a ratio of V-statistics based
on interpoint distances <code class="reqn">\|x_{i}-y_{j}\|</code>.
See the reference below for details.
</p>


<h3>Value</h3>

<p><code>mvI</code> returns the statistic. <code>mvI.test</code> returns
a list with class
<code>htest</code> containing
</p>
<table>
<tr><td><code>method</code></td>
<td>
<p> description of test</p>
</td></tr>
<tr><td><code>statistic</code></td>
<td>
<p> observed value of the test statistic <code class="reqn">n\mathcal I_n^2</code></p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
 <p><code class="reqn">\mathcal I_n</code></p>
</td></tr>
<tr><td><code>replicates</code></td>
<td>
<p> replicates of the test statistic</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p> approximate p-value of the test</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p> description of data</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Historically this is the first energy test of independence. The
distance covariance test  <code><a href="#topic+dcov.test">dcov.test</a></code>, distance correlation
<code><a href="#topic+dcor">dcor</a></code>, and related methods are more recent (2007,2009).
The distance covariance test is faster and has different properties than
<code>mvI.test</code>. Both methods are based on a population independence coefficient
that characterizes independence and both tests are statistically consistent.
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Bakirov, N.K., Rizzo, M.L., and Szekely, G.J. (2006), A Multivariate
Nonparametric Test of Independence, <em>Journal of Multivariate Analysis</em>
93/1, 58-80, <br />
<a href="https://doi.org/10.1016/j.jmva.2005.10.005">doi:10.1016/j.jmva.2005.10.005</a>
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+indep.test">indep.test</a> </code>
<code> <a href="#topic+mvI.test">mvI.test</a> </code>
<code> <a href="#topic+dcov.test">dcov.test</a> </code>
<code> <a href="#topic+dcov">dcov</a> </code>
</p>

<hr>
<h2 id='mvnorm.test'>E-statistic (Energy) Test of Multivariate Normality</h2><span id='topic+mvnorm.test'></span><span id='topic+mvnorm.etest'></span><span id='topic+mvnorm.e'></span>

<h3>Description</h3>

<p>Performs the E-statistic (energy) test of multivariate or univariate normality.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mvnorm.test(x, R)
mvnorm.etest(x, R)
mvnorm.e(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mvnorm.test_+3A_x">x</code></td>
<td>
<p> data matrix of multivariate sample, or univariate data vector</p>
</td></tr>
<tr><td><code id="mvnorm.test_+3A_r">R</code></td>
<td>
<p> number of bootstrap replicates </p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>x</code> is a matrix, each row is a multivariate observation. The
data will be standardized to zero mean and identity covariance matrix
using the sample mean vector and sample covariance matrix. If <code>x</code>
is a vector, <code>mvnorm.e</code> returns the univariate statistic 
<code>normal.e(x)</code>.
If the data contains missing values or the sample covariance matrix is
singular, <code>mvnorm.e</code> returns NA. 
</p>
<p>The <code class="reqn">\mathcal{E}</code>-test of multivariate normality was proposed
and implemented by Szekely and Rizzo (2005). The test statistic for
d-variate normality is given by
</p>
<p style="text-align: center;"><code class="reqn">\mathcal{E} = n (\frac{2}{n} \sum_{i=1}^n E\|y_i-Z\| -
 E\|Z-Z'\| - \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n \|y_i-y_j\|),
 </code>
</p>

<p>where <code class="reqn">y_1,\ldots,y_n</code> is the standardized sample,
<code class="reqn">Z, Z'</code> are iid standard d-variate normal, and
<code class="reqn">\| \cdot \|</code> denotes Euclidean norm.
</p>
<p>The <code class="reqn">\mathcal{E}</code>-test of multivariate (univariate) normality
is implemented by parametric bootstrap with <code>R</code> replicates.
</p>


<h3>Value</h3>

<p>The value of the <code class="reqn">\mathcal{E}</code>-statistic for multivariate
normality is returned by <code>mvnorm.e</code>.
</p>
<p><code>mvnorm.test</code> returns a list with class <code>htest</code> containing
</p>
<table>
<tr><td><code>method</code></td>
<td>
<p>description of test</p>
</td></tr>
<tr><td><code>statistic</code></td>
<td>
<p>observed value of the test statistic</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>approximate p-value of the test</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>description of data</p>
</td></tr>
</table>
<p><code>mvnorm.etest</code> is replaced by <code>mvnorm.test</code>. 
</p>


<h3>Note</h3>

<p>If the data is univariate, the test statistic is formally
the same as the multivariate case, but a more efficient computational
formula is applied in <code><a href="#topic+normal.e">normal.e</a></code>.
</p>
<p><code><a href="#topic+normal.test">normal.test</a></code> also provides an optional method for the
test based on the asymptotic sampling distribution of the test
statistic. 
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Szekely, G. J. and Rizzo, M. L. (2005) A New Test for
Multivariate Normality, <em>Journal of Multivariate Analysis</em>,
93/1, 58-80,
<a href="https://doi.org/10.1016/j.jmva.2003.12.002">doi:10.1016/j.jmva.2003.12.002</a>.
</p>
<p>Mori, T. F., Szekely, G. J. and Rizzo, M. L. &quot;On energy tests of normality.&quot; Journal of Statistical Planning and Inference 213 (2021): 1-15. 
</p>
<p>Rizzo, M. L. (2002). A New Rotation Invariant Goodness-of-Fit Test, Ph.D. dissertation, Bowling Green State University.
</p>
<p>Szekely, G. J. (1989) Potential and Kinetic Energy in Statistics,
Lecture Notes, Budapest Institute of Technology (Technical University).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+normal.test">normal.test</a></code> for the energy test of univariate
normality and <code><a href="#topic+normal.e">normal.e</a></code> for the statistic.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> ## compute normality test statistic for iris Setosa data
 data(iris)
 mvnorm.e(iris[1:50, 1:4])

 ## test if the iris Setosa data has multivariate normal distribution
 mvnorm.test(iris[1:50,1:4], R = 199)
</code></pre>

<hr>
<h2 id='normal.test'>Energy Test of Univariate Normality</h2><span id='topic+normal.test'></span><span id='topic+normal.e'></span>

<h3>Description</h3>

<p>Performs the energy test of univariate normality
for the composite hypothesis Case 4, estimated parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normal.test(x, method=c("mc","limit"), R)
normal.e(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normal.test_+3A_x">x</code></td>
<td>
<p> univariate data vector</p>
</td></tr>
<tr><td><code id="normal.test_+3A_method">method</code></td>
<td>
<p> method for p-value</p>
</td></tr>
<tr><td><code id="normal.test_+3A_r">R</code></td>
<td>
<p> number of replications if Monte Carlo method</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>method="mc"</code> this test function applies the parametric
bootstrap method implemented in <code><a href="#topic+mvnorm.test">mvnorm.test</a></code>. 
</p>
<p>If <code>method="limit"</code>, the p-value of the test is computed from 
the asymptotic distribution of the test statistic under the null 
hypothesis. The asymptotic 
distribution is a quadratic form of centered Gaussian random variables, 
which has the form
</p>
<p style="text-align: center;"><code class="reqn">\sum_{k=1}^\infty \lambda_k Z_k^2,</code>
</p>

<p>where <code class="reqn">\lambda_k</code> are positive constants (eigenvalues) and
<code class="reqn">Z_k</code> are iid standard normal variables. Eigenvalues are
pre-computed and stored internally. 
A p-value is computed using Imhof's method as implemented in the
<span class="pkg">CompQuadForm</span> package. 
</p>
<p>Note that the &quot;limit&quot; method is intended for moderately large
samples because it applies the asymptotic distribution.  
</p>
<p>The energy test of normality was proposed
and implemented by Szekely and Rizzo (2005). 
See <code><a href="#topic+mvnorm.test">mvnorm.test</a></code>
for more details. 
</p>


<h3>Value</h3>

<p><code>normal.e</code> returns the energy goodness-of-fit statistic for
a univariate sample. 
</p>
<p><code>normal.test</code> returns a list with class <code>htest</code> containing
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>observed value of the test statistic</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>p-value of the test</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>sample estimates: mean, sd</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>description of data</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Szekely, G. J. and Rizzo, M. L. (2005) A New Test for
Multivariate Normality, <em>Journal of Multivariate Analysis</em>,
93/1, 58-80,
<a href="https://doi.org/10.1016/j.jmva.2003.12.002">doi:10.1016/j.jmva.2003.12.002</a>.
</p>
<p>Mori, T. F., Szekely, G. J. and Rizzo, M. L. &quot;On energy tests of normality.&quot; Journal of Statistical Planning and Inference 213 (2021): 1-15. 
</p>
<p>Rizzo, M. L. (2002). A New Rotation Invariant Goodness-of-Fit Test,
Ph.D. dissertation, Bowling Green State University.
</p>
<p>J. P. Imhof (1961). Computing the Distribution of Quadratic Forms in 
Normal Variables, <em>Biometrika</em>, Volume 48, Issue 3/4,
419-426.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mvnorm.test">mvnorm.test</a></code> and <code><a href="#topic+mvnorm.e">mvnorm.e</a></code> for the
energy test of multivariate normality and the test statistic
for multivariate samples.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  x &lt;- iris[1:50, 1]
  normal.e(x)
  normal.test(x, R=199)
  normal.test(x, method="limit")
</code></pre>

<hr>
<h2 id='pdcor'>
Partial distance correlation and covariance
</h2><span id='topic+pdcor'></span><span id='topic+pdcov'></span><span id='topic+pdcor.test'></span><span id='topic+pdcov.test'></span>

<h3>Description</h3>

<p>Partial distance correlation pdcor, pdcov, and tests.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  pdcov.test(x, y, z, R)
  pdcor.test(x, y, z, R)
  pdcor(x, y, z)
  pdcov(x, y, z)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pdcor_+3A_x">x</code></td>
<td>
<p> data or dist object of first sample</p>
</td></tr>
<tr><td><code id="pdcor_+3A_y">y</code></td>
<td>
<p> data or dist object of second sample</p>
</td></tr>
<tr><td><code id="pdcor_+3A_z">z</code></td>
<td>
<p> data or dist object of third sample</p>
</td></tr>
<tr><td><code id="pdcor_+3A_r">R</code></td>
<td>
<p> replicates for permutation test</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>pdcor(x, y, z)</code> and <code>pdcov(x, y, z)</code> compute the partial distance
correlation and partial distance covariance, respectively,
of x and y removing z.
</p>
<p>A test for zero partial distance correlation (or zero partial distance covariance) is implemented in <code>pdcor.test</code>, and <code>pdcov.test</code>.
</p>
<p>Argument types supported are numeric data matrix, data.frame, tibble, numeric vector, class &quot;dist&quot; object, or factor. For unordered factors a 0-1 distance matrix is computed. 
</p>


<h3>Value</h3>

<p>Each test returns an object of class <code>htest</code>.
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Szekely, G.J. and Rizzo, M.L. (2014),
Partial Distance Correlation with Methods for Dissimilarities.
<em>Annals of Statistics</em>, Vol. 42 No. 6, 2382-2412.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  n = 30
  R &lt;- 199

  ## mutually independent standard normal vectors
  x &lt;- rnorm(n)
  y &lt;- rnorm(n)
  z &lt;- rnorm(n)

  pdcor(x, y, z)
  pdcov(x, y, z)
  set.seed(1)
  pdcov.test(x, y, z, R=R)
  set.seed(1)
  pdcor.test(x, y, z, R=R)


  if (require(MASS)) {
    p = 4
    mu &lt;- rep(0, p)
    Sigma &lt;- diag(p)
  
    ## linear dependence
    y &lt;- mvrnorm(n, mu, Sigma) + x
    print(pdcov.test(x, y, z, R=R))
  
    ## non-linear dependence
    y &lt;- mvrnorm(n, mu, Sigma) * x
    print(pdcov.test(x, y, z, R=R))
    }
  
</code></pre>

<hr>
<h2 id='Poisson+20Tests'> Goodness-of-Fit Tests for Poisson Distribution</h2><span id='topic+poisson.tests'></span><span id='topic+poisson.e'></span><span id='topic+poisson.etest'></span><span id='topic+poisson.m'></span><span id='topic+poisson.mtest'></span>

<h3>Description</h3>

<p>Performs the mean distance goodness-of-fit test and the energy goodness-of-fit test of Poisson distribution with unknown parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>poisson.e(x)
poisson.m(x)
poisson.etest(x, R)
poisson.mtest(x, R)
poisson.tests(x, R, test="all")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Poisson+2B20Tests_+3A_x">x</code></td>
<td>
<p> vector of nonnegative integers, the sample data </p>
</td></tr>
<tr><td><code id="Poisson+2B20Tests_+3A_r">R</code></td>
<td>
<p> number of bootstrap replicates </p>
</td></tr>
<tr><td><code id="Poisson+2B20Tests_+3A_test">test</code></td>
<td>
<p> name of test(s) </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Two distance-based tests of Poissonity are applied in <code>poisson.tests</code>, &quot;M&quot; and &quot;E&quot;. The default is to 
do all tests and return results in a data frame. 
Valid choices for <code>test</code> are &quot;M&quot;, &quot;E&quot;, or &quot;all&quot; with 
default &quot;all&quot;. 
</p>
<p>If &quot;all&quot; tests, all tests are performed by a single parametric bootstrap computing all test statistics on each sample. 
</p>
<p>The &quot;M&quot; choice is two tests, one based on a Cramer-von Mises  distance and the other an Anderson-Darling distance. The &quot;E&quot; choice is the energy goodness-of-fit test. 
</p>
<p><code>R</code> must be a positive integer for a test. If <code>R</code> is missing or 0, a warning is printed but test statistics are computed (without testing). 
</p>
<p>The mean distance test of Poissonity (M-test) is based on the result that the sequence
of expected values E|X-j|, j=0,1,2,... characterizes the distribution of
the random  variable X. As an application of this characterization one can
get an estimator <code class="reqn">\hat F(j)</code> of the CDF. The test statistic
(see <code><a href="#topic+poisson.m">poisson.m</a></code>) is a Cramer-von Mises type of distance, with
M-estimates replacing the usual EDF estimates of the CDF:
</p>
<p style="text-align: center;"><code class="reqn">M_n = n\sum_{j=0}^\infty (\hat F(j) - F(j\;; \hat \lambda))^2
 f(j\;; \hat \lambda).</code>
</p>
 
<p>In <code>poisson.tests</code>, an Anderson-Darling type of weight is also applied when <code>test="M"</code> or <code>test="all"</code>.
</p>
<p>The tests are implemented by parametric bootstrap with
<code>R</code> replicates.
</p>
<p>An energy goodness-of-fit test (E) is based on the test statistic
</p>
<p style="text-align: center;"><code class="reqn">Q_n = n (\frac{2}{n} \sum_{i=1}^n E|x_i - X| - E|X-X'| - \frac{1}{n^2} \sum_{i,j=1}^n |x_i - x_j|,
</code>
</p>

<p>where X and X' are iid with the hypothesized null distribution. For a test of H: X ~ Poisson(<code class="reqn">\lambda</code>), we can express E|X-X'| in terms of Bessel functions, and E|x_i - X| in terms of the CDF of Poisson(<code class="reqn">\lambda</code>).
</p>
<p>If test==&quot;all&quot; or not specified, all tests are run with a single parametric bootstrap. <code>poisson.mtest</code> implements only the Poisson M-test with Cramer-von Mises type distance. <code>poisson.etest</code> implements only the Poisson energy test.
</p>


<h3>Value</h3>

<p>The functions <code>poisson.m</code> and <code>poisson.e</code> return the test statistics. The function
<code>poisson.mtest</code> or <code>poisson.etest</code> return an <code>htest</code> object containing
</p>
<table>
<tr><td><code>method</code></td>
<td>
<p>Description of test</p>
</td></tr>
<tr><td><code>statistic</code></td>
<td>
<p>observed value of the test statistic</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>approximate p-value of the test</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>replicates R</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>sample mean</p>
</td></tr>
</table>
<p><code>poisson.tests</code> returns &quot;M-CvM test&quot;, &quot;M-AD test&quot; and &quot;Energy test&quot; results in a data frame with columns 
</p>
<table>
<tr><td><code>estimate</code></td>
<td>
<p>sample mean</p>
</td></tr>
<tr><td><code>statistic</code></td>
<td>
<p>observed value of the test statistic</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>approximate p-value of the test</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>Description of test</p>
</td></tr>
</table>
<p>which can be coerced to a <code>tibble</code>.
</p>


<h3>Note</h3>

<p>The running time of the M test is much faster than the E-test.</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Szekely, G. J. and Rizzo, M. L. (2004) Mean Distance Test of Poisson Distribution, <em>Statistics and Probability Letters</em>,
67/3, 241-247. <a href="https://doi.org/10.1016/j.spl.2004.01.005">doi:10.1016/j.spl.2004.01.005</a>.
</p>
<p>Szekely, G. J. and Rizzo, M. L. (2005) A New Test for
Multivariate Normality, <em>Journal of Multivariate Analysis</em>,
93/1, 58-80,
<a href="https://doi.org/10.1016/j.jmva.2003.12.002">doi:10.1016/j.jmva.2003.12.002</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x &lt;- rpois(50, 2)
 poisson.m(x)
 poisson.e(x)
 
 poisson.etest(x, R=199)
 poisson.mtest(x, R=199)
 poisson.tests(x, R=199)
 
</code></pre>

<hr>
<h2 id='sortrank'> Sort, order and rank a vector </h2><span id='topic+sortrank'></span>

<h3>Description</h3>

<p>A utility that returns a list with the components
equivalent to sort(x), order(x), rank(x, ties.method = &quot;first&quot;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sortrank(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sortrank_+3A_x">x</code></td>
<td>
<p> vector compatible with sort(x)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This utility exists to save a little time on large vectors when two or all three of the sort(), order(), rank() results are required. In case of ties, the ranks component matches <code>rank(x, ties.method = "first")</code>. 
</p>


<h3>Value</h3>

<p>A list with components 
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>the sorted input vector x</p>
</td></tr>
<tr><td><code>ix</code></td>
<td>
<p>the permutation = order(x) which rearranges x into ascending order</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>the ranks of x</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function was benchmarked faster than the combined calls to <code>sort</code> and <code>rank</code>. 
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a>
</p>


<h3>References</h3>

<p>See <code><a href="base.html#topic+sort">sort</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sortrank(rnorm(5))
</code></pre>

<hr>
<h2 id='U_product'> Inner product in the Hilbert space of U-centered
distance matrices</h2><span id='topic+U_product'></span>

<h3>Description</h3>

<p>Stand-alone function to compute the inner product in the
Hilbert space of U-centered distance matrices, as in the definition of
partial distance covariance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>U_product(U, V)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="U_product_+3A_u">U</code></td>
<td>
<p> U-centered distance matrix</p>
</td></tr>
<tr><td><code id="U_product_+3A_v">V</code></td>
<td>
<p> U-centered distance matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that <code>pdcor</code>, etc. functions include the centering and
projection operations, so that these stand alone versions are not
needed except in case one wants to check the internal computations.
</p>
<p>Exported from U_product.cpp.
</p>


<h3>Value</h3>

<p><code>U_product</code> returns the inner product, a scalar.
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Szekely, G.J. and Rizzo, M.L. (2014),
Partial Distance Correlation with Methods for Dissimilarities,
<em>Annals of Statistics</em>, Vol. 42, No. 6, pp. 2382-2412.
<br /> <a href="https://projecteuclid.org/euclid.aos/1413810731">https://projecteuclid.org/euclid.aos/1413810731</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x &lt;- iris[1:10, 1:4]
 y &lt;- iris[11:20, 1:4]
 M1 &lt;- as.matrix(dist(x))
 M2 &lt;- as.matrix(dist(y))
 U &lt;- U_center(M1)
 V &lt;- U_center(M2)

 U_product(U, V)
 dcovU_stats(M1, M2)
 </code></pre>

<hr>
<h2 id='Unbiased+20distance+20covariance'>Unbiased dcov and bias-corrected dcor statistics</h2><span id='topic+bcdcor'></span><span id='topic+dcovU'></span>

<h3>Description</h3>

<p>These functions compute unbiased estimators of squared distance
covariance and a bias-corrected estimator of
(squared) distance correlation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bcdcor(x, y)
dcovU(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Unbiased+2B20distance+2B20covariance_+3A_x">x</code></td>
<td>
<p> data or dist object of first sample</p>
</td></tr>
<tr><td><code id="Unbiased+2B20distance+2B20covariance_+3A_y">y</code></td>
<td>
<p> data or dist object of second sample</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The unbiased (squared) dcov is inner product definition of
dCov, in the Hilbert space of U-centered distance matrices.
</p>
<p>The sample sizes (number of rows) of the two samples must
agree, and samples must not contain missing values. 
</p>
<p>Argument types supported are 
numeric data matrix, data.frame, or tibble, with observations in rows;
numeric vector; ordered or unordered factors. In case of unordered factors
a 0-1 distance matrix is computed.
</p>


<h3>Value</h3>

<p><code>dcovU</code> returns the unbiased estimator of squared dcov.
<code>bcdcor</code> returns a bias-corrected estimator of squared dcor.
</p>


<h3>Note</h3>

<p>Unbiased distance covariance (SR2014) corresponds to the biased
(original) <code class="reqn">\mathrm{dCov^2}</code>. Since <code>dcovU</code> is an
unbiased statistic, it is signed and we do not take the square root.
For the original distance covariance test of independence (SRB2007,
SR2009), the distance covariance test statistic is the V-statistic
<code class="reqn">\mathrm{n\, dCov^2} = n \mathcal{V}_n^2</code> (not dCov).
Similarly, <code>bcdcor</code> is bias-corrected, so we do not take the
square root as with dCor.
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Szekely, G.J. and Rizzo, M.L. (2014),
Partial Distance Correlation with Methods for Dissimilarities.
<em>Annals of Statistics</em>, Vol. 42 No. 6, 2382-2412.
</p>
<p>Szekely, G.J., Rizzo, M.L., and Bakirov, N.K. (2007),
Measuring and Testing Dependence by Correlation of Distances,
<em>Annals of Statistics</em>, Vol. 35 No. 6, pp. 2769-2794.
<br /> <a href="https://doi.org/10.1214/009053607000000505">doi:10.1214/009053607000000505</a>
</p>
<p>Szekely, G.J. and Rizzo, M.L. (2009),
Brownian Distance Covariance,
<em>Annals of Applied Statistics</em>,
Vol. 3, No. 4, 1236-1265.
<br /> <a href="https://doi.org/10.1214/09-AOAS312">doi:10.1214/09-AOAS312</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x &lt;- iris[1:50, 1:4]
 y &lt;- iris[51:100, 1:4]
 dcovU(x, y)
 bcdcor(x, y)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
