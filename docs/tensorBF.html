<!DOCTYPE html><html lang="en"><head><title>Help for package tensorBF</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tensorBF}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#getDefaultOpts'><p>A function for generating a default set of parameters for Bayesian Tensor Factorization methods</p></a></li>
<li><a href='#normFiberCentering'><p>Preprocessing: fiber Centering</p></a></li>
<li><a href='#normSlabScaling'><p>Preprocessing: Slab Scaling</p></a></li>
<li><a href='#plotTensorBF'><p>Plot Tensor Components</p></a></li>
<li><a href='#predictTensorBF'><p>Predict Missing Values using the Bayesian tensor factorization model</p></a></li>
<li><a href='#reconstructTensorBF'><p>Reconstruct the data based on posterior samples</p></a></li>
<li><a href='#tensorBF'><p>Bayesian Factorization of a Tensor</p></a></li>
<li><a href='#undoFiberCentering'><p>Postprocessing: Undo fiber Centering</p></a></li>
<li><a href='#undoSlabScaling'><p>Postprocessing: Undo Slab Scaling</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Tensor Factorization</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-10-01</td>
</tr>
<tr>
<td>Author:</td>
<td>Suleiman A Khan [aut, cre],
  Muhammad Ammad-ud-din [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Suleiman A Khan &lt;khan.suleiman@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Bayesian Tensor Factorization for decomposition of tensor data sets using the trilinear CANDECOMP/PARAFAC (CP) factorization, with automatic component selection. The complete data analysis pipeline is provided, including functions and recommendations for data normalization and model definition, as well as missing value prediction and model visualization. The method performs factorization for three-way tensor datasets and the inference is implemented with Gibbs sampling.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Imports:</td>
<td>tensor, methods</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.0</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-10-02 07:20:14 UTC; skhan</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-10-02 18:10:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='getDefaultOpts'>A function for generating a default set of parameters for Bayesian Tensor Factorization methods</h2><span id='topic+getDefaultOpts'></span>

<h3>Description</h3>

<p><code>getDefaultOpts</code> returns the default choices for model parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getDefaultOpts(method = "CP")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getDefaultOpts_+3A_method">method</code></td>
<td>
<p>the factorization method for which options are required.
Currently only &quot;CP&quot; (default) is supported.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function returns options for defining the model's high-level structure
(sparsity priors), the hyperparameters, and the uninformative
priors. We recommend keeping these as provided.
</p>


<h3>Value</h3>

<p>A list with the following model options:
</p>
<table role = "presentation">
<tr><td><code>ARDX</code></td>
<td>
<p>TRUE: use elementwise ARD prior for X, resulting in sparse X's.
FALSE: use guassian prior for a dense X (default).</p>
</td></tr>
<tr><td><code>ARDW</code></td>
<td>
<p>TRUE: use elementwise ARD prior for W, resulting in sparse W's (default).
FALSE: use guassian prior for a dense W.</p>
</td></tr>
<tr><td><code>ARDU</code></td>
<td>
<p>TRUE: use elementwise ARD prior for U, resulting in sparse U's.
FALSE: use guassian prior for a dense U (default).</p>
</td></tr>
<tr><td><code>iter.burnin</code></td>
<td>
<p>The number of burn-in samples (default 5000).</p>
</td></tr>
<tr><td><code>iter.sampling</code></td>
<td>
<p>The number of saved posterior samples (default 50).</p>
</td></tr>
<tr><td><code>iter.thinning</code></td>
<td>
<p>The thinning factor to use in saving posterior samples (default 10).</p>
</td></tr>
<tr><td><code>prior.alpha_0t</code></td>
<td>
<p>The shape parameter for residual noise (tau's) prior (default 1).</p>
</td></tr>
<tr><td><code>prior.beta_0t</code></td>
<td>
<p>The rate parameter for residual noise (tau's) prior (default 1).</p>
</td></tr>
<tr><td><code>prior.alpha_0</code></td>
<td>
<p>The shape parameter for the ARD precisions (default 1e-3).</p>
</td></tr>
<tr><td><code>prior.beta_0</code></td>
<td>
<p>The rate parameter for the ARD precisions (default 1e-3).</p>
</td></tr>
<tr><td><code>prior.betaW1</code></td>
<td>
<p>Bernoulli prior for component activiations, prior.betaW1 &lt; prior.betaW2: sparsity inducing (default: 1).</p>
</td></tr>
<tr><td><code>prior.betaW2</code></td>
<td>
<p>Bernoulli prior for component activation, (default: 1).</p>
</td></tr>
<tr><td><code>init.tau</code></td>
<td>
<p>The initial value for noise precision (default 1e3).</p>
</td></tr>
<tr><td><code>verbose</code></td>
<td>
<p>The verbosity level. 0=no printing, 1=moderate printing,
2=maximal printing (default 1).</p>
</td></tr>
<tr><td><code>checkConvergence</code></td>
<td>
<p>Check for the convergence of the data reconstruction,
based on the Geweke diagnostic (default TRUE).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>#To run the algorithm with other values:
opts &lt;- getDefaultOpts()
opts$ARDW &lt;- FALSE #Switch off Feature-level Sparsity on W's
 ## Not run: res &lt;- tensorBF(Y=Y,opts=opts)
</code></pre>

<hr>
<h2 id='normFiberCentering'>Preprocessing: fiber Centering</h2><span id='topic+normFiberCentering'></span>

<h3>Description</h3>

<p><code>normFiberCentering</code> center the fibers of the <code class="reqn">o^{th}</code> mode of the tensor to zero mean.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normFiberCentering(Y, o)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="normFiberCentering_+3A_y">Y</code></td>
<td>
<p>the tensor data. See function <code><a href="#topic+tensorBF">tensorBF</a></code> for
details.</p>
</td></tr>
<tr><td><code id="normFiberCentering_+3A_o">o</code></td>
<td>
<p>the <code class="reqn">o^{th}</code> (default: 1) mode of the tensor in which the fibers are to be centered to zero mean.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing the following elements:
</p>
<table role = "presentation">
<tr><td><code>data</code></td>
<td>
<p>The data after performing the required centering operation.</p>
</td></tr>
<tr><td><code>pre</code></td>
<td>
<p>The centering values used for preprocessing.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Kolda, Tamara G., and Brett W. Bader. &quot;Tensor decompositions and applications.&quot; SIAM review 51.3 (2009): 455-500.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Data generation
K &lt;- 3
X &lt;- matrix(rnorm(20*K),20,K)
W &lt;- matrix(rnorm(30*K),30,K)
U &lt;- matrix(rnorm(3*K),3,K)
Y = 0
for(k in 1:K) Y &lt;- Y + outer(outer(X[,k],W[,k]),U[,k])
 Y &lt;- Y + array(rnorm(20*30*3),dim=c(20,30,3))

#center the fibers in first mode of tensor Y
res &lt;- normFiberCentering(Y=Y,o=1)
dim(res$data) #the centered data
</code></pre>

<hr>
<h2 id='normSlabScaling'>Preprocessing: Slab Scaling</h2><span id='topic+normSlabScaling'></span>

<h3>Description</h3>

<p><code>normSlabScaling</code> scales the slabs of the <code class="reqn">o^{th}</code> mode of the tensor to unit variance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normSlabScaling(Y, o = 2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="normSlabScaling_+3A_y">Y</code></td>
<td>
<p>the tensor data. See function <code><a href="#topic+tensorBF">tensorBF</a></code> for
details.</p>
</td></tr>
<tr><td><code id="normSlabScaling_+3A_o">o</code></td>
<td>
<p>the <code class="reqn">o^{th}</code> (default: 2) mode of the tensor in which the slabs are to be scaled to unit variance.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing the following elements:
</p>
<table role = "presentation">
<tr><td><code>data</code></td>
<td>
<p>The data after performing the required scaling operation.</p>
</td></tr>
<tr><td><code>pre</code></td>
<td>
<p>The scale's used for preprocessing.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Kolda, Tamara G., and Brett W. Bader. &quot;Tensor decompositions and applications.&quot; SIAM review 51.3 (2009): 455-500.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Data generation
K &lt;- 3
X &lt;- matrix(rnorm(20*K),20,K)
W &lt;- matrix(rnorm(30*K),30,K)
U &lt;- matrix(rnorm(3*K),3,K)
Y = 0
for(k in 1:K) Y &lt;- Y + outer(outer(X[,k],W[,k]),U[,k])
 Y &lt;- Y + array(rnorm(20*30*3),dim=c(20,30,3))

#scale the slabs in second mode of tensor Y
res &lt;- normSlabScaling(Y=Y,o=2)
dim(res$data) #the scaled data
</code></pre>

<hr>
<h2 id='plotTensorBF'>Plot Tensor Components</h2><span id='topic+plotTensorBF'></span>

<h3>Description</h3>

<p><code>plotTensorBF</code> shows the heatmap of components inferred by <code><a href="#topic+tensorBF">tensorBF</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotTensorBF(res, Y = NULL, k = 1, modesOnAxis = c(1, 2, 3),
  nTopFeatures = c(5, 15, 3), margins = c(4, 4, 4, 12), cex.axis = 1,
  cols = colorRampPalette(c("blue", "white", "red"))(101), key = TRUE,
  plimit = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotTensorBF_+3A_res">res</code></td>
<td>
<p>The learned tensorBF model.</p>
</td></tr>
<tr><td><code id="plotTensorBF_+3A_y">Y</code></td>
<td>
<p>The original input data to be plotted. If specified NULL,
the function plots the data reconstruction using <code><a href="#topic+reconstructTensorBF">reconstructTensorBF</a></code> (default: NULL).</p>
</td></tr>
<tr><td><code id="plotTensorBF_+3A_k">k</code></td>
<td>
<p>the component number to visualize (default: 1).</p>
</td></tr>
<tr><td><code id="plotTensorBF_+3A_modesonaxis">modesOnAxis</code></td>
<td>
<p>which mode to plot on each axis c(Yaxis,Xaxis,lateral). Defaults to c(1,2,3).</p>
</td></tr>
<tr><td><code id="plotTensorBF_+3A_ntopfeatures">nTopFeatures</code></td>
<td>
<p>The number of most relevant features to show for the data space
visualizations in each of the modes. Defaults to c(5,15,3) for displaying top 10 features
of <code class="reqn">1^{st}</code> mode, 20 of <code class="reqn">2^{nd}</code> mode and 5 of <code class="reqn">3^{rd}</code> mode.</p>
</td></tr>
<tr><td><code id="plotTensorBF_+3A_margins">margins</code></td>
<td>
<p>numeric vector of length 4 containing the margins (see par(mar= *))</p>
</td></tr>
<tr><td><code id="plotTensorBF_+3A_cex.axis">cex.axis</code></td>
<td>
<p>positive numbers, used as cex.axis (default: 1)</p>
</td></tr>
<tr><td><code id="plotTensorBF_+3A_cols">cols</code></td>
<td>
<p>colors used for the image. Defaults to a blue-white-red color scale.</p>
</td></tr>
<tr><td><code id="plotTensorBF_+3A_key">key</code></td>
<td>
<p>logical indicating whether a color-key should be drawn.</p>
</td></tr>
<tr><td><code id="plotTensorBF_+3A_plimit">plimit</code></td>
<td>
<p>(optional) numerical number indicating the maximum absolute value to be plotted in the heatmap.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>#Data generation
K &lt;- 3
X &lt;- matrix(rnorm(20*K),20,K)
W &lt;- matrix(rnorm(30*K),30,K)
U &lt;- matrix(rnorm(3*K),3,K)
Y = 0
for(k in 1:K) Y &lt;- Y + outer(outer(X[,k],W[,k]),U[,k])
 Y &lt;- Y + array(rnorm(20*30*3,0,0.25),dim=c(20,30,3))

#Run the method with default options
## Not run: res1 &lt;- tensorBF(Y)
## Not run: plotTensorBF(res = res1,Y=Y,k=1)
</code></pre>

<hr>
<h2 id='predictTensorBF'>Predict Missing Values using the Bayesian tensor factorization model</h2><span id='topic+predictTensorBF'></span>

<h3>Description</h3>

<p><code>predictTensorBF</code> predicts the missing values in the data <code>Y</code> using the learned model <code>res</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predictTensorBF(Y, res)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predictTensorBF_+3A_y">Y</code></td>
<td>
<p>is a 3-mode tensor containing missing values as NA's. See function <code><a href="#topic+tensorBF">tensorBF</a></code> for details.</p>
</td></tr>
<tr><td><code id="predictTensorBF_+3A_res">res</code></td>
<td>
<p>the model object returned by the function <code><a href="#topic+tensorBF">tensorBF</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the original data <code>Y</code> contained missing values (NA's),
this function predicts them using the model. The predictions are
returned in the un-normalized space if <code>res$pre</code> contains appropriate
preprocessing information.
</p>


<h3>Value</h3>

<p>A tensor of the same size as <code>Y</code> containing predicted values in place of NA's.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Data generation
## Not run: K &lt;- 2
## Not run: X &lt;- matrix(rnorm(20*K),20,K)
## Not run: W &lt;- matrix(rnorm(30*K),30,K)
## Not run: U &lt;- matrix(rnorm(3*K),3,K)
## Not run: Y = 0
## Not run: for(k in 1:K) Y &lt;- Y + outer(outer(X[,k],W[,k]),U[,k])
## Not run:  Y &lt;- Y + array(rnorm(20*30*3,0,0.25),dim=c(20,30,3))

#insert missing values
## Not run: m.inds = sample(prod(dim(Y)),100)
## Not run: Yobs = Y[m.inds]
## Not run: Y[m.inds] = NA

#Run the method with default options and predict missing values
## Not run: res &lt;- tensorBF(Y)
## Not run: pred = predictTensorBF(Y=Y,res=res)
## Not run: plot(Yobs,pred[m.inds],xlab="obs",ylab="pred",main=round(cor(Yobs,pred[m.inds]),2))
</code></pre>

<hr>
<h2 id='reconstructTensorBF'>Reconstruct the data based on posterior samples</h2><span id='topic+reconstructTensorBF'></span>

<h3>Description</h3>

<p><code>reconstructTensorBF</code> returns the reconstruction of the data based on
posterior samples of a given run. The function reconstructs the tensor for
each posterior sample and then computes the expected value.
The reconstruction is returned in the un-normalized space if <code>res$pre</code>
contains appropriate preprocessing information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reconstructTensorBF(res)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="reconstructTensorBF_+3A_res">res</code></td>
<td>
<p>The model object from function <code><a href="#topic+tensorBF">tensorBF</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The reconstructed data, a tensor of the size equivalent to the
data on which the model was run.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Data generation
K &lt;- 3
X &lt;- matrix(rnorm(20*K),20,K)
W &lt;- matrix(rnorm(30*K),30,K)
U &lt;- matrix(rnorm(3*K),3,K)
Y = 0
for(k in 1:K) Y &lt;- Y + outer(outer(X[,k],W[,k]),U[,k])
 Y &lt;- Y + array(rnorm(20*30*3,0,0.25),dim=c(20,30,3))

#Run the method with default options and reconstruct the model's representation of the tensor
## Not run: res &lt;- tensorBF(Y)
## Not run: recon = reconstructTensorBF(res)
## Not run: inds = sample(prod(dim(Y)),100)
## Not run: plot(Y[inds],recon[inds],xlab="obs",ylab="recon",main=round(cor(Y[inds],recon[inds]),2))
</code></pre>

<hr>
<h2 id='tensorBF'>Bayesian Factorization of a Tensor</h2><span id='topic+tensorBF'></span>

<h3>Description</h3>

<p><code>tensorBF</code> implements the Bayesian factorization of a tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tensorBF(Y, method = "CP", K = NULL, opts = NULL,
  fiberCentering = NULL, slabScaling = NULL, noiseProp = c(0.5, 0.5))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tensorBF_+3A_y">Y</code></td>
<td>
<p>is a three-mode tensor to be factorized.</p>
</td></tr>
<tr><td><code id="tensorBF_+3A_method">method</code></td>
<td>
<p>the factorization method. Currently only &quot;CP&quot; (default) is supported.</p>
</td></tr>
<tr><td><code id="tensorBF_+3A_k">K</code></td>
<td>
<p>The number of components (i.e. latent variables or factors). Recommended to be
set somewhat higher than the expected component number, so that the method
can determine the model complexity by prunning excessive components
(default: 20% of the sum of lower two dimensions).
High values result in high CPU time.
</p>
<p>NOTE: Adjust parameter noiseProp if sufficiently
large values of K do not lead to a model with pruned components.</p>
</td></tr>
<tr><td><code id="tensorBF_+3A_opts">opts</code></td>
<td>
<p>List of model options; see function <code><a href="#topic+getDefaultOpts">getDefaultOpts</a></code> for details and default.</p>
</td></tr>
<tr><td><code id="tensorBF_+3A_fibercentering">fiberCentering</code></td>
<td>
<p>the mode for which fibers are to be centered at zero (default = NULL).
Fiber is analogous to a vector in a particular mode.
Fiber centering and Slab scaling are the recommended normalizations for a tensor.
For details see the provided normalization functions and the references therein.</p>
</td></tr>
<tr><td><code id="tensorBF_+3A_slabscaling">slabScaling</code></td>
<td>
<p>the mode for which slabs are to be scaled to unit variance (default = NULL).
Slab is analogous to a matrix in a particular mode.
Alternativly, you can preprocess the data using the provided normalization functions.</p>
</td></tr>
<tr><td><code id="tensorBF_+3A_noiseprop">noiseProp</code></td>
<td>
<p>c(prop,conf); sets an informative noise prior for tensorBF.
The model sets the noise prior such that the expected proportion of
variance explained by noise is defined by this parameter. It is recommended when
the standard prior from <code><a href="#topic+getDefaultOpts">getDefaultOpts</a></code> seems to overfit the model
by not prunning any component with high initial K. Use NULL to switch off
informative noise prior.
</p>
<p>- prop defines the proportion of total variance to be explained by noise (between 0.1 and 0.9),
</p>
<p>- conf defines the confidence in the prior (between 0.1 and 10).
</p>
<p>We suggest a default value of c(0.5,0.5) for real data sets.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Bayesian Tensor Factorization performs tri-linear (CP) factorization of a tensor.
The method automatically identifies the number of components,
given K is initialized to a large enough value, see arguments.
Missing values are supported and should be set as NA's in the data.
They will not affect the model parameters, and can be predicted
with function <code><a href="#topic+predictTensorBF">predictTensorBF</a></code>, based on the observed values.
</p>


<h3>Value</h3>

<p>A list containing model parameters.
For key parameters, the final posterior sample ordered w.r.t. component variance is provided to aid in
initial checks; all the posterior samples should be used for model
analysis. The list elements are:
</p>
<table role = "presentation">
<tr><td><code>K</code></td>
<td>
<p>The number of learned components. If this value is not less then the input argument K, the model should be rerun with a larger K or use the noiseProp parameter.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a matrix of <code class="reqn">N \times K</code> dimensions, containing the last Gibbs sample of the first-mode latent variables.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>a matrix of <code class="reqn">D \times K</code> dimensions, containing the last Gibbs sample of the second-mode latent variables.</p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p>a matrix of <code class="reqn">L \times K</code> dimensions, containing the last Gibbs sample of the third-mode latent variables.</p>
</td></tr>
<tr><td><code>tau</code></td>
<td>
<p>The last sample of noise precision.</p>
</td></tr>
</table>
<p>and the following elements:
</p>
<table role = "presentation">
<tr><td><code>posterior</code></td>
<td>
<p>the posterior samples of model parameters (X,U,W,Z,tau).</p>
</td></tr>
<tr><td><code>cost</code></td>
<td>
<p>The likelihood of all the posterior samples.</p>
</td></tr>
<tr><td><code>opts</code></td>
<td>
<p>The options used to run the model.</p>
</td></tr>
<tr><td><code>conv</code></td>
<td>
<p>An estimate of the convergence of the model, based on reconstruction
of data using the Geweke diagnostic. Values significantly above 0.05 occur when
model has not converged and should therefore be rerun with a higher value of iter.burnin in <code><a href="#topic+getDefaultOpts">getDefaultOpts</a></code>.</p>
</td></tr>
<tr><td><code>pre</code></td>
<td>
<p>A list of centering and scaling values used to transform the data, if any. Else an empty list.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>#Data generation
K &lt;- 2
X &lt;- matrix(rnorm(20*K),20,K)
W &lt;- matrix(rnorm(25*K),25,K)
U &lt;- matrix(rnorm(3*K),3,K)
Y = 0
for(k in 1:K) Y &lt;- Y + outer(outer(X[,k],W[,k]),U[,k])
 Y &lt;- Y + array(rnorm(20*25*3,0,0.25),dim=c(20,25,3))

#Run the method with default options
## Not run: res2 &lt;- tensorBF(Y=Y)

#Run the method with K=3 and iterations=1000
## Not run: opts &lt;- getDefaultOpts(); opts$iter.burnin = 1000
## Not run: res1 &lt;- tensorBF(Y=Y,K=3,opts=opts)

#Vary the user defined expected proportion of noise variance
#explained. c(0.2,1) represents 0.2 as the noise proportion
#and confidence of 1
## Not run: res3 &lt;- tensorBF(Y=Y,noiseProp=c(0.2,1))

</code></pre>

<hr>
<h2 id='undoFiberCentering'>Postprocessing: Undo fiber Centering</h2><span id='topic+undoFiberCentering'></span>

<h3>Description</h3>

<p><code>undoFiberCentering</code> reverts the fiber's of the <code class="reqn">o^{th}</code> mode to undo the centering effect.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>undoFiberCentering(Yn, pre)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="undoFiberCentering_+3A_yn">Yn</code></td>
<td>
<p>the normalized tensor data. This can be, for example, the output of <code><a href="#topic+reconstructTensorBF">reconstructTensorBF</a></code>.</p>
</td></tr>
<tr><td><code id="undoFiberCentering_+3A_pre">pre</code></td>
<td>
<p>The centering parameters used for preprocessing in the format as produced by <code><a href="#topic+normFiberCentering">normFiberCentering</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The data tensor after reversing the centering operation.
</p>


<h3>References</h3>

<p>Kolda, Tamara G., and Brett W. Bader. &quot;Tensor decompositions and applications.&quot; SIAM review 51.3 (2009): 455-500.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Given tensor Y
## Not run: Ycentered &lt;- normFiberCentering(Y=Y,o=1)
## Not run: Yuncentered &lt;- undoFiberCentering(Ycentered$data,Ycentered$pre)
</code></pre>

<hr>
<h2 id='undoSlabScaling'>Postprocessing: Undo Slab Scaling</h2><span id='topic+undoSlabScaling'></span>

<h3>Description</h3>

<p><code>undoSlabScaling</code> reverts the slabs of the <code class="reqn">o^{th}</code> mode to undo the scaling effect.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>undoSlabScaling(Yn, pre)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="undoSlabScaling_+3A_yn">Yn</code></td>
<td>
<p>the normalized tensor data. This can be, for example, the output of <code><a href="#topic+reconstructTensorBF">reconstructTensorBF</a></code>.</p>
</td></tr>
<tr><td><code id="undoSlabScaling_+3A_pre">pre</code></td>
<td>
<p>The scaling values and mode used for preprocessing in the format as produced by <code><a href="#topic+normSlabScaling">normSlabScaling</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The data tensor after reversing the scaling operation.
</p>


<h3>References</h3>

<p>Kolda, Tamara G., and Brett W. Bader. &quot;Tensor decompositions and applications.&quot; SIAM review 51.3 (2009): 455-500.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Given tensor Y
## Not run: Yscaled &lt;- normSlabScaling(Y=Y,o=2)
## Not run: Yunscaled &lt;- undoSlabScaling(Yscaled$data,Yscaled$pre)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
