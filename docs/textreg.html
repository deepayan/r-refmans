<!DOCTYPE html><html><head><title>Help for package textreg</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {textreg}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bathtub'><p>Sample of cleaned OSHA accident summaries.</p></a></li>
<li><a href='#build.corpus'><p>Build a corpus that can be used in the textreg call.</p></a></li>
<li><a href='#calc.loss'><p>Calculate total loss of model (Squared hinge loss).</p></a></li>
<li><a href='#clean.text'><p>Clean text and get it ready for textreg.</p></a></li>
<li><a href='#cluster.phrases'><p>Cluster phrases based on similarity of appearance.</p></a></li>
<li><a href='#convert.tm.to.character'><p>Convert tm corpus to vector of strings.</p></a></li>
<li><a href='#cpp_build.corpus'><p>Driver function for the C++ function.</p></a></li>
<li><a href='#cpp_textreg'><p>Driver function for the C++ function.</p></a></li>
<li><a href='#dirtyBathtub'><p>Sample of raw-text OSHA accident summaries.</p></a></li>
<li><a href='#find.CV.C'><p>K-fold cross-validation to determine optimal tuning parameter</p></a></li>
<li><a href='#find.threshold.C'><p>Conduct permutation test on labeling to get null distribution of regularization parameter.</p></a></li>
<li><a href='#grab.fragments'><p>Grab all fragments in a corpus with given phrase.</p></a></li>
<li><a href='#is.fragment.sample'><p>Is object a fragment.sample object?</p></a></li>
<li><a href='#is.textreg.corpus'><p>Is object a textreg.corpus object?</p></a></li>
<li><a href='#is.textreg.result'><p>Is object a textreg.result object?</p></a></li>
<li><a href='#list.table.chart'><p>Graphic showing multiple word lists side-by-side.</p></a></li>
<li><a href='#make_search_phrases'><p>Convert phrases to appropriate search string.</p></a></li>
<li><a href='#make.appearance.matrix'><p>Make phrase appearance matrix from textreg result.</p></a></li>
<li><a href='#make.count.table'><p>Count number of times documents have a given phrase.</p></a></li>
<li><a href='#make.CV.chart'><p>Plot K-fold cross validation curves</p></a></li>
<li><a href='#make.list.table'><p>Collate multiple regression runs.</p></a></li>
<li><a href='#make.path.matrix'><p>Generate matrix describing gradient descent path of textreg.</p></a></li>
<li><a href='#make.phrase.correlation.chart'><p>Generate visualization of phrase overlap.</p></a></li>
<li><a href='#make.phrase.matrix'><p>Make a table of where phrases appear in a corpus</p></a></li>
<li><a href='#make.similarity.matrix'><p>Calculate similarity matrix for set of phrases.</p></a></li>
<li><a href='#path.matrix.chart'><p>Plot optimization path of textreg.</p></a></li>
<li><a href='#phrase.count'><p>Count phrase appearance.</p></a></li>
<li><a href='#phrase.matrix'><p>Make matrix of where phrases appear in corpus.</p></a></li>
<li><a href='#phrases'><p>Get the phrases from the textreg.result object?</p></a></li>
<li><a href='#plot.textreg.result'><p>Plot the sequence of features as they are introduced with the textreg gradient descent</p>
program.</a></li>
<li><a href='#predict.textreg.result'><p>Predict labeling with the selected phrases.</p></a></li>
<li><a href='#print.fragment.sample'><p>Pretty print results of phrase sampling object.</p></a></li>
<li><a href='#print.textreg.corpus'><p>Pretty print textreg corpus object</p></a></li>
<li><a href='#print.textreg.result'><p>Pretty print results of textreg regression.</p></a></li>
<li><a href='#reformat.textreg.model'><p>Clean up output from textreg.</p></a></li>
<li><a href='#sample.fragments'><p>Sample fragments of text to contextualize a phrase.</p></a></li>
<li><a href='#save.corpus.to.files'><p>Save corpus to text (and RData) file.</p></a></li>
<li><a href='#stem.corpus'><p>Step corpus with annotation.</p></a></li>
<li><a href='#testCorpora'><p>Some small, fake test corpora.</p></a></li>
<li><a href='#textreg'><p>Sparse regression of labeling vector onto all phrases in a corpus.</p></a></li>
<li><a href='#textreg-package'><p>Sparse regression package for text that allows for multiple word phrases.</p></a></li>
<li><a href='#tm_gregexpr'><p>Call gregexpr on the content of a tm Corpus.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>n-Gram Text Regression, aka Concise Comparative Summarization</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-09-01</td>
</tr>
<tr>
<td>Author:</td>
<td>Luke Miratrix</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Luke Miratrix &lt;lmiratrix@stat.harvard.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Function for sparse regression on raw text, regressing a labeling
    vector onto a feature space consisting of all possible phrases.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10),</td>
</tr>
<tr>
<td>Imports:</td>
<td>tm (&ge; 0.7), NLP (&ge; 0.1-10), Rcpp (&ge; 0.12.9), stats,
graphics, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>corrplot, knitr, SnowballC (&ge; 0.5.1), xtable, testthat,
plyr, rmarkdown</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.0</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-10-04 10:07:20 UTC; lmiratrix</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-10-04 11:00:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='bathtub'>Sample of cleaned OSHA accident summaries.</h2><span id='topic+bathtub'></span>

<h3>Description</h3>

<p>bathtub consists of several accident reports plus a labeling with a +1 for any report
that had been tagged as related to METHELYNE CHLORIDE.
</p>


<h3>Format</h3>

<p>Corpus object from the <code>tm</code> package.  Has a meta info of the METHELYNE CHLORIDE labeling called &quot;meth.chl&quot;</p>


<h3>See Also</h3>

<p>Other bathtub: <code><a href="#topic+dirtyBathtub">dirtyBathtub</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library( tm )
data( bathtub )
meta( bathtub, "meth.chl" )
</code></pre>

<hr>
<h2 id='build.corpus'>Build a corpus that can be used in the textreg call.</h2><span id='topic+build.corpus'></span>

<h3>Description</h3>

<p>Pre-building a corpus allows for calling multiple textregs without doing a lot 
of initial data processing (e.g., if you want to explore different ban lists or
regularization parameters)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build.corpus(corpus, labeling, banned = NULL, verbosity = 1,
  token.type = "word")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build.corpus_+3A_corpus">corpus</code></td>
<td>
<p>A list of strings or a corpus from the <code>tm</code> package.</p>
</td></tr>
<tr><td><code id="build.corpus_+3A_labeling">labeling</code></td>
<td>
<p>A vector of +1/-1 or TRUE/FALSE indicating which documents are considered relevant 
and which are baseline.  The +1/-1 can contain 0 whcih means drop the document.</p>
</td></tr>
<tr><td><code id="build.corpus_+3A_banned">banned</code></td>
<td>
<p>List of words that should be dropped from consideration.</p>
</td></tr>
<tr><td><code id="build.corpus_+3A_verbosity">verbosity</code></td>
<td>
<p>Level of output.  0 is no printed output.</p>
</td></tr>
<tr><td><code id="build.corpus_+3A_token.type">token.type</code></td>
<td>
<p>&quot;word&quot; or &quot;character&quot; as tokens.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the bathtub vignette for more complete discussion of this method and the options 
you might pass to it.
</p>
<p>A textreg.corpus object is not a <code>tm</code>-style corpus.  In particular, all text
pre-processing, etc., to text should be done to the data <em>before</em> building the
textreg.corpus object.
</p>


<h3>Value</h3>

<p>A <code><a href="#topic+textreg.corpus">textreg.corpus</a></code> object.
</p>


<h3>Note</h3>

<p>Unfortunately, the process of seperating out the textreg call and the build.corpus
call is not quite as clean as one would hope.  The build.corpus call moves the text into
the C++ memory, but the way the search tree is built for the regression it is hard to salvage
it across runs and so this is of limited use.  In particular, the labeling and banned words
cannot be easily changed.   Future versions of the package would ideally remedy this.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( testCorpora )
textreg( testCorpora$testI$corpus, testCorpora$testI$labelI, c(), C=1, verbosity=1 )
</code></pre>

<hr>
<h2 id='calc.loss'>Calculate total loss of model (Squared hinge loss).</h2><span id='topic+calc.loss'></span>

<h3>Description</h3>

<p>Calculate the loss for a model in predicting the -1/+1 labeling.
If new text and labeling given, then calc loss on the new text and labeling.
This can be useful for cross validation and train-test splits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc.loss(model.blob, new.text = NULL, new.labeling = NULL,
  loss = c("square.hinge", "square", "hinge"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc.loss_+3A_model.blob">model.blob</code></td>
<td>
<p>The model returned from <code><a href="#topic+textreg">textreg</a></code></p>
</td></tr>
<tr><td><code id="calc.loss_+3A_new.text">new.text</code></td>
<td>
<p>New text (string or tm Corpus) to predict labeling for</p>
</td></tr>
<tr><td><code id="calc.loss_+3A_new.labeling">new.labeling</code></td>
<td>
<p>Labeling to go with new text.</p>
</td></tr>
<tr><td><code id="calc.loss_+3A_loss">loss</code></td>
<td>
<p>Type of loss to calc for.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Three numbers: total loss, loss from prediction, loss from penalty term
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( testCorpora )
res = textreg( c( "", "", "A", "A" ), c( -1, -1, 1, 1 ), C=1, Lq=1,
          convergence.threshold=0.00000001, verbosity=0 )
calc.loss( res )
calc.loss( res, new.text=c("A B C A"), new.labeling=c(1) )
</code></pre>

<hr>
<h2 id='clean.text'>Clean text and get it ready for textreg.</h2><span id='topic+clean.text'></span>

<h3>Description</h3>

<p>Changes multiline documents to single line.  Strips extra whitespace and punctuation.
Changes digits to 'X's.  Non-alpha characters converted to spaces.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clean.text(bigcorp)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clean.text_+3A_bigcorp">bigcorp</code></td>
<td>
<p>A tm Corpus object.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library( tm )
txt = c( "thhis s! and bonkus  4:33pm and Jan 3, 2015. ", 
         "   big    space\n     dawg-ness?")
a &lt;- clean.text( VCorpus( VectorSource( txt ) ) )
a[[1]]
</code></pre>

<hr>
<h2 id='cluster.phrases'>Cluster phrases based on similarity of appearance.</h2><span id='topic+cluster.phrases'></span>

<h3>Description</h3>

<p>Cluster phrases based on similarity of their appearance in the positive documents.
Can also plot this if so desired.
</p>
<p>Uses hclust() with the &ldquo;ward.D&rdquo; method on 1-S with S from make.similarity.matrix
</p>
<p>Warning: for 'negative weight' phrases this method does not do well since it ignores
negative documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster.phrases(result, num.groups = 5, plot = TRUE, yaxt = "n",
  ylab = "", sub = "", main = "Association of Phrases", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster.phrases_+3A_result">result</code></td>
<td>
<p>A similarity matrix from make.similarity.matrix call or an textreg.result object</p>
</td></tr>
<tr><td><code id="cluster.phrases_+3A_num.groups">num.groups</code></td>
<td>
<p>Number of groups to box.</p>
</td></tr>
<tr><td><code id="cluster.phrases_+3A_plot">plot</code></td>
<td>
<p>Actually plot clustering or just calculate it.</p>
</td></tr>
<tr><td><code id="cluster.phrases_+3A_yaxt">yaxt</code></td>
<td>
<p>Whether to include a y-axis</p>
</td></tr>
<tr><td><code id="cluster.phrases_+3A_ylab">ylab</code></td>
<td>
<p>Label for y-axis</p>
</td></tr>
<tr><td><code id="cluster.phrases_+3A_sub">sub</code></td>
<td>
<p>Subtitle for plot</p>
</td></tr>
<tr><td><code id="cluster.phrases_+3A_main">main</code></td>
<td>
<p>Title of plot.</p>
</td></tr>
<tr><td><code id="cluster.phrases_+3A_...">...</code></td>
<td>
<p>Extra arguments to pass to the plot command.  See par.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Phrase Vizualization: <code><a href="#topic+make.appearance.matrix">make.appearance.matrix</a></code>,
<code><a href="#topic+make.phrase.correlation.chart">make.phrase.correlation.chart</a></code>,
<code><a href="#topic+make.similarity.matrix">make.similarity.matrix</a></code>
</p>

<hr>
<h2 id='convert.tm.to.character'>Convert tm corpus to vector of strings.</h2><span id='topic+convert.tm.to.character'></span>

<h3>Description</h3>

<p>A utility function useful for testing and some dirty hacks.
This is because the tm package doesn't leave vector corpora of 
strings alone anymore.  
</p>
<p>and so sometimes you need to convert your tm object to a string vector
for various reasons, the 
main one being handing it to the C++ method.  It is ugly,
but so it goes.
</p>
<p>It is therefore a possibly better decision to pass a filename to a plain-text file
to the textreg call to be loaded by C++ directly.
See <code><a href="#topic+textreg">textreg</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert.tm.to.character(corpus)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convert.tm.to.character_+3A_corpus">corpus</code></td>
<td>
<p>The tm corpus to convert.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of character.
</p>

<hr>
<h2 id='cpp_build.corpus'>Driver function for the C++ function.</h2><span id='topic+cpp_build.corpus'></span>

<h3>Description</h3>

<p>Given a labeling and a corpus, create a corpus object for use in textreg.
Generally you should use the buildCorpus method, not this method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cpp_build.corpus(corpus, labeling, banned = c(), params)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cpp_build.corpus_+3A_corpus">corpus</code></td>
<td>
<p>A list of strings or a corpus from the <code>tm</code> package.</p>
</td></tr>
<tr><td><code id="cpp_build.corpus_+3A_labeling">labeling</code></td>
<td>
<p>A vector of +1/-1 or TRUE/FALSE indicating which documents are considered relevant and
which are baseline.  The +1/-1 can contain 0 whcih means drop the document.</p>
</td></tr>
<tr><td><code id="cpp_build.corpus_+3A_banned">banned</code></td>
<td>
<p>List of words that should be dropped from consideration.</p>
</td></tr>
<tr><td><code id="cpp_build.corpus_+3A_params">params</code></td>
<td>
<p>List of parameters to pass to the call.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Warning: do not call directly.  Use textreg instead
</p>


<h3>See Also</h3>

<p>textreg, find_C_threshold
</p>

<hr>
<h2 id='cpp_textreg'>Driver function for the C++ function.</h2><span id='topic+cpp_textreg'></span>

<h3>Description</h3>

<p>Given a labeling and a corpus, find phrases that predict this labeling.
Generally you should use the textreg method, not this method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cpp_textreg(corpus, params)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cpp_textreg_+3A_corpus">corpus</code></td>
<td>
<p>A list of strings or a corpus from the <code>tm</code> package.</p>
</td></tr>
<tr><td><code id="cpp_textreg_+3A_params">params</code></td>
<td>
<p>List of parameters to pass to the call.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Warning: do not call directly.  Use textreg instead
</p>


<h3>See Also</h3>

<p>textreg, find_C_threshold
</p>

<hr>
<h2 id='dirtyBathtub'>Sample of raw-text OSHA accident summaries.</h2><span id='topic+dirtyBathtub'></span>

<h3>Description</h3>

<p>dirtyBathtub consists of the (more) raw data from which the <code>bathtub</code> dataset is derived.
</p>


<h3>Format</h3>

<p>Dataframe.  Has a meta info of the METHELYNE CHLORIDE labeling, plus 100s of other labels.</p>


<h3>See Also</h3>

<p>Other bathtub: <code><a href="#topic+bathtub">bathtub</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( dirtyBathtub )
table( dirtyBathtub$fatality )
</code></pre>

<hr>
<h2 id='find.CV.C'>K-fold cross-validation to determine optimal tuning parameter</h2><span id='topic+find.CV.C'></span>

<h3>Description</h3>

<p>Given a corpus, divide into K-folds and do test-train spilts averaged over
the folds.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find.CV.C(corpus, labeling, banned, K = 5, length.out = 10,
  max_C = NULL, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find.CV.C_+3A_corpus">corpus</code></td>
<td>
<p>The text</p>
</td></tr>
<tr><td><code id="find.CV.C_+3A_labeling">labeling</code></td>
<td>
<p>The labeling</p>
</td></tr>
<tr><td><code id="find.CV.C_+3A_banned">banned</code></td>
<td>
<p>The words to drop.</p>
</td></tr>
<tr><td><code id="find.CV.C_+3A_k">K</code></td>
<td>
<p>Number of folds for K-fold cross-validation</p>
</td></tr>
<tr><td><code id="find.CV.C_+3A_length.out">length.out</code></td>
<td>
<p>number of values of C to examine from 0 to max_C.</p>
</td></tr>
<tr><td><code id="find.CV.C_+3A_max_c">max_C</code></td>
<td>
<p>upper bound for tuning parameter; if NULL, sets max_C to threshold C</p>
</td></tr>
<tr><td><code id="find.CV.C_+3A_verbose">verbose</code></td>
<td>
<p>Print progress</p>
</td></tr>
<tr><td><code id="find.CV.C_+3A_...">...</code></td>
<td>
<p>parameters to be passed to the original textreg() function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Increments tuning parameter, performs K-fold cross-validation on each C giving a profile
of predictive power for different C.
</p>


<h3>Value</h3>

<p>a dataframe containing the mean/standard error of out-of-sample predictions under K-Fold Cross-validation
</p>


<h3>See Also</h3>

<p>make.CV.chart
</p>

<hr>
<h2 id='find.threshold.C'>Conduct permutation test on labeling to get null distribution of regularization parameter.</h2><span id='topic+find.threshold.C'></span>

<h3>Description</h3>

<p>First determines what regularization will give null model on labeling.  Then permutes labeling
repeatidly, recording what regularization will give null model for permuted labeling.
This allows for permutation-style inference on the relationship of the labeling to the text, and
allows for appropriate selection of the tuning parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find.threshold.C(corpus, labeling, banned = NULL, R = 0,
  objective.function = 2, a = 1, verbosity = 0,
  step.verbosity = verbosity, positive.only = FALSE,
  binary.features = FALSE, no.regularization = FALSE,
  positive.weight = 1, Lq = 2, min.support = 1, min.pattern = 1,
  max.pattern = 100, gap = 0, token.type = "word",
  convergence.threshold = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find.threshold.C_+3A_corpus">corpus</code></td>
<td>
<p>A list of strings or a corpus from the <code>tm</code> package.</p>
</td></tr>
<tr><td><code id="find.threshold.C_+3A_labeling">labeling</code></td>
<td>
<p>A vector of +1/-1 or TRUE/FALSE indicating which documents are considered relevant and
which are baseline.  The +1/-1 can contain 0 whcih means drop the document.</p>
</td></tr>
<tr><td><code id="find.threshold.C_+3A_banned">banned</code></td>
<td>
<p>List of words that should be dropped from consideration.</p>
</td></tr>
<tr><td><code id="find.threshold.C_+3A_r">R</code></td>
<td>
<p>Number of times to scramble labling.  0 means use given labeling and find single C value.</p>
</td></tr>
<tr><td><code id="find.threshold.C_+3A_objective.function">objective.function</code></td>
<td>
<p>2 is hinge loss.  0 is something.  1 is something else.</p>
</td></tr>
<tr><td><code id="find.threshold.C_+3A_a">a</code></td>
<td>
<p>What percent of regularization should be L1 loss (a=1) vs L2 loss (a=0)</p>
</td></tr>
<tr><td><code id="find.threshold.C_+3A_verbosity">verbosity</code></td>
<td>
<p>Level of output.  0 is no printed output.</p>
</td></tr>
<tr><td><code id="find.threshold.C_+3A_step.verbosity">step.verbosity</code></td>
<td>
<p>Level of output for line searches.  0 is no printed output.</p>
</td></tr>
<tr><td><code id="find.threshold.C_+3A_positive.only">positive.only</code></td>
<td>
<p>Disallow negative features if true</p>
</td></tr>
<tr><td><code id="find.threshold.C_+3A_binary.features">binary.features</code></td>
<td>
<p>Just code presence/absence of a feature in a document rather than count of feature in document.</p>
</td></tr>
<tr><td><code id="find.threshold.C_+3A_no.regularization">no.regularization</code></td>
<td>
<p>Do not renormalize the features at all.  (Lq will be ignored.)</p>
</td></tr>
<tr><td><code id="find.threshold.C_+3A_positive.weight">positive.weight</code></td>
<td>
<p>Scale weight pf all positively marked documents by this value.  (1, i.e., no scaling) is default)   NOT FULLY IMPLEMENTED</p>
</td></tr>
<tr><td><code id="find.threshold.C_+3A_lq">Lq</code></td>
<td>
<p>Rescaling to put on the features (2 is standard).  Can be from 1 up.  Values above 10 invoke an infinity-norm.</p>
</td></tr>
<tr><td><code id="find.threshold.C_+3A_min.support">min.support</code></td>
<td>
<p>Only consider phrases that appear this many times or more.</p>
</td></tr>
<tr><td><code id="find.threshold.C_+3A_min.pattern">min.pattern</code></td>
<td>
<p>Only consider phrases this long or longer</p>
</td></tr>
<tr><td><code id="find.threshold.C_+3A_max.pattern">max.pattern</code></td>
<td>
<p>Only consider phrases this short or shorter</p>
</td></tr>
<tr><td><code id="find.threshold.C_+3A_gap">gap</code></td>
<td>
<p>Allow phrases that have wildcard words in them.  Number is how many wildcards in a row.</p>
</td></tr>
<tr><td><code id="find.threshold.C_+3A_token.type">token.type</code></td>
<td>
<p>&quot;word&quot; or &quot;character&quot; as tokens.</p>
</td></tr>
<tr><td><code id="find.threshold.C_+3A_convergence.threshold">convergence.threshold</code></td>
<td>
<p>How to decide if descent has converged.  (Will go for three steps at this threshold to check for flatness.)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Important: use the same parameter values as used with the original textreg call!
</p>


<h3>Value</h3>

<p>A list of numbers (the Cs) R+1 long.  The first number is always the C used for the _passed_ labeling.  The remainder are shuffles.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( testCorpora )
find.threshold.C( testCorpora$testI$corpus, testCorpora$testI$labelI, c(), R=5, verbosity=1 )
</code></pre>

<hr>
<h2 id='grab.fragments'>Grab all fragments in a corpus with given phrase.</h2><span id='topic+grab.fragments'></span>

<h3>Description</h3>

<p>Search corpus for passed phrase, using some wildcard notation.  Return snippits
of text containing this phrase, with a specified number of characters before and
after.  This gives context for phrases in documents.
</p>
<p>Use like this <code>frags = grab.fragments( "israel", bigcorp )</code>
</p>
<p>Can take phrases such as 'appl+' which means any word starting with &quot;appl.&quot;  
Can also take phrases such as &quot;big * city&quot; which consist of any three-word phrase with &quot;big&quot; 
as the first word and &quot;city&quot; as the third word.
</p>
<p>If a pattern matches overlapping phrases, it will return the first but not 
the second.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>grab.fragments(phrase, corp, char.before = 80,
  char.after = char.before, cap.phrase = TRUE, clean = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grab.fragments_+3A_phrase">phrase</code></td>
<td>
<p>Phrase to find in corpus</p>
</td></tr>
<tr><td><code id="grab.fragments_+3A_corp">corp</code></td>
<td>
<p>is a tm corpus</p>
</td></tr>
<tr><td><code id="grab.fragments_+3A_char.before">char.before</code></td>
<td>
<p>Number of characters of document to pull before phrase to give 
context.</p>
</td></tr>
<tr><td><code id="grab.fragments_+3A_char.after">char.after</code></td>
<td>
<p>As above, but trailing characters.  Defaults to char.before value.</p>
</td></tr>
<tr><td><code id="grab.fragments_+3A_cap.phrase">cap.phrase</code></td>
<td>
<p>TRUE if the phrase should be put in ALL CAPS.  False if left alone.</p>
</td></tr>
<tr><td><code id="grab.fragments_+3A_clean">clean</code></td>
<td>
<p>True means drop all documents without phrase from list. False means leave
NULLs in the list.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>fragments in corp that have given phrase.List of lists.  First list is 
len(corp) long
with NULL values for documents without phrase, and lists
of phrases for those documents with the phrase
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library( tm )
docs = c( "987654321 test 123456789", "987654321 test test word 123456789", 
       "test at start", "a test b", "this is a test", "without the t-word",
       "a test for you and a test for me" )
corpus &lt;- VCorpus(VectorSource(docs))
grab.fragments( "test *", corpus, char.before=4, char.after=4 )
</code></pre>

<hr>
<h2 id='is.fragment.sample'>Is object a fragment.sample object?</h2><span id='topic+is.fragment.sample'></span><span id='topic+fragment.sample'></span>

<h3>Description</h3>

<p>Is object a fragment.sample object?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.fragment.sample(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.fragment.sample_+3A_x">x</code></td>
<td>
<p>the object to check.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other sample.fragments: <code><a href="#topic+print.fragment.sample">print.fragment.sample</a></code>,
<code><a href="#topic+sample.fragments">sample.fragments</a></code>
</p>

<hr>
<h2 id='is.textreg.corpus'>Is object a textreg.corpus object?</h2><span id='topic+is.textreg.corpus'></span><span id='topic+textreg.corpus'></span>

<h3>Description</h3>

<p>Is object a textreg.corpus object?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.textreg.corpus(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.textreg.corpus_+3A_x">x</code></td>
<td>
<p>the object to check.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other textreg.corpus: <code><a href="#topic+print.textreg.corpus">print.textreg.corpus</a></code>
</p>

<hr>
<h2 id='is.textreg.result'>Is object a textreg.result object?</h2><span id='topic+is.textreg.result'></span><span id='topic+textreg.result'></span>

<h3>Description</h3>

<p>Is object a textreg.result object?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.textreg.result(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.textreg.result_+3A_x">x</code></td>
<td>
<p>the object to check.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other textreg.result: <code><a href="#topic+phrases">phrases</a></code>,
<code><a href="#topic+print.textreg.result">print.textreg.result</a></code>,
<code><a href="#topic+reformat.textreg.model">reformat.textreg.model</a></code>
</p>

<hr>
<h2 id='list.table.chart'>Graphic showing multiple word lists side-by-side.</h2><span id='topic+list.table.chart'></span>

<h3>Description</h3>

<p>This method basically makes a visual plot of a list table (which you call first).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list.table.chart(model.list, M = 100, linespace = 4, ytick = NULL,
  dates = NULL, main = paste("Word Appearance for ", attr(model.list,
  "topic"), "\n(Method: ", attr(model.list, "method"), ")", sep = ""),
  xlab = "Model", mar = c(3, 5, 2.5, 0.1), xaxt = "y",
  color.breaks = NULL, color.ramp = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="list.table.chart_+3A_model.list">model.list</code></td>
<td>
<p>Matrix (or data.frame) from the make.list.table call.</p>
</td></tr>
<tr><td><code id="list.table.chart_+3A_m">M</code></td>
<td>
<p>is the max number of words to show in chart</p>
</td></tr>
<tr><td><code id="list.table.chart_+3A_linespace">linespace</code></td>
<td>
<p>Where to space</p>
</td></tr>
<tr><td><code id="list.table.chart_+3A_ytick">ytick</code></td>
<td>
<p>Put y tick marks</p>
</td></tr>
<tr><td><code id="list.table.chart_+3A_dates">dates</code></td>
<td>
<p>Dates to put on bottom</p>
</td></tr>
<tr><td><code id="list.table.chart_+3A_main">main</code></td>
<td>
<p>Main title</p>
</td></tr>
<tr><td><code id="list.table.chart_+3A_xlab">xlab</code></td>
<td>
<p>Label for x-axis</p>
</td></tr>
<tr><td><code id="list.table.chart_+3A_mar">mar</code></td>
<td>
<p>Margin of plot (see par)</p>
</td></tr>
<tr><td><code id="list.table.chart_+3A_xaxt">xaxt</code></td>
<td>
<p>Plot an x-axis (see par)</p>
</td></tr>
<tr><td><code id="list.table.chart_+3A_color.breaks">color.breaks</code></td>
<td>
<p>Cut-points (like on a histogram) defining the different color levels.</p>
</td></tr>
<tr><td><code id="list.table.chart_+3A_color.ramp">color.ramp</code></td>
<td>
<p>List of colors to use from lowest value (potentially negative weights) to highest.  If both color.breaks and color.ramp passed, color.breaks is list one longer than color.ramp.</p>
</td></tr>
<tr><td><code id="list.table.chart_+3A_...">...</code></td>
<td>
<p>Extra arguments for the core image() call that plots the word weights.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>make.list.table
</p>

<hr>
<h2 id='make_search_phrases'>Convert phrases to appropriate search string.</h2><span id='topic+make_search_phrases'></span>

<h3>Description</h3>

<p>Will change, e.g., &quot;test * pig+&quot; to appropriate regular 
expression to find in the text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_search_phrases(phrases)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_search_phrases_+3A_phrases">phrases</code></td>
<td>
<p>List of strings denoting the phrases to be
searched for.</p>
</td></tr>
</table>

<hr>
<h2 id='make.appearance.matrix'>Make phrase appearance matrix from textreg result.</h2><span id='topic+make.appearance.matrix'></span>

<h3>Description</h3>

<p>Make matrix of which phrases appear in which of the positively
marked documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.appearance.matrix(result)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.appearance.matrix_+3A_result">result</code></td>
<td>
<p>An textreg.result object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Very similar to phrase.matrix, except this looks only at positively 
marked documents and just returns 1 or 0 on whether any document has
a phrase, rather than giving counts.
This is used by the clustering vizualizations and make.similarity.matrix.
</p>


<h3>Value</h3>

<p>A $n X p$ matrix for $n$ documents and $p$ phrases in the result object.  Each entry is a 0/1 value 
indicating presence of the given phrase in the given document.
</p>


<h3>See Also</h3>

<p>make.similarity.matrix
</p>
<p>phrase.matrix
</p>
<p>Other Phrase Vizualization: <code><a href="#topic+cluster.phrases">cluster.phrases</a></code>,
<code><a href="#topic+make.phrase.correlation.chart">make.phrase.correlation.chart</a></code>,
<code><a href="#topic+make.similarity.matrix">make.similarity.matrix</a></code>
</p>

<hr>
<h2 id='make.count.table'>Count number of times documents have a given phrase.</h2><span id='topic+make.count.table'></span>

<h3>Description</h3>

<p>Given a list of phrases, count how many documents they appear in
and subdivide by positive and negative appearance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.count.table(phrases, labeling, corpus)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.count.table_+3A_phrases">phrases</code></td>
<td>
<p>List of strings</p>
</td></tr>
<tr><td><code id="make.count.table_+3A_labeling">labeling</code></td>
<td>
<p>Vector of +1/0/-1 labels</p>
</td></tr>
<tr><td><code id="make.count.table_+3A_corpus">corpus</code></td>
<td>
<p>A corpus object from tm package</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method does not consider multiple counts of phrases within documents.
Phrases can have wildcards and stemming notation.  See <code><a href="#topic+grab.fragments">grab.fragments</a></code>.
</p>


<h3>Value</h3>

<p>a dataframe of statistics.  per.pos is the percent of the 
documents with the phrase that are positively labeled.  per.tag is
the percent of the positively labeled documents that have the 
phrase.
</p>


<h3>See Also</h3>

<p>grab.fragments
</p>
<p>Other textregCounting: <code><a href="#topic+make.phrase.matrix">make.phrase.matrix</a></code>,
<code><a href="#topic+phrase.count">phrase.count</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library( tm )
data( bathtub )
lbl = meta( bathtub )$meth.chl
make.count.table( c("bathtub","strip+", "vapor *"), lbl, bathtub )
</code></pre>

<hr>
<h2 id='make.CV.chart'>Plot K-fold cross validation curves</h2><span id='topic+make.CV.chart'></span>

<h3>Description</h3>

<p>Make a loess curve with loess() to predict the test error for different values of C by interpolating the passed evaluated
points on the tbl dataframe.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.CV.chart(tbl, plot = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.CV.chart_+3A_tbl">tbl</code></td>
<td>
<p>Table from find.CV.C</p>
</td></tr>
<tr><td><code id="make.CV.chart_+3A_plot">plot</code></td>
<td>
<p>TRUE means plot the chart.  False means do not, but return the optimal C</p>
</td></tr>
<tr><td><code id="make.CV.chart_+3A_...">...</code></td>
<td>
<p>Parameters to the plot function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Then plot the test error with SE bars for the cross validation.  Also calculate the spot that is 1 SE above the minimum.
Fits the points with loess lines so, in principle, few actually evaluated points are needed in evaluating the function.  
All a bit ad hoc and worthy of improvement.
</p>
<p>Not particularly well implemented.
</p>


<h3>Value</h3>

<p>invisible list of the minimum C value and the estimated test error for both the minimum
and the predicted C corresponding to 1 SE above the minimum estimate.
</p>


<h3>See Also</h3>

<p>find.CV.C
</p>

<hr>
<h2 id='make.list.table'>Collate multiple regression runs.</h2><span id='topic+make.list.table'></span>

<h3>Description</h3>

<p>This method makes a table of several regression runs side by side.
The table has rows being phrases and the
columns being the regression runs.  A number is usually the 
weight found for that word at that window.
If multiple runs have the same phrase, row will
have multiple entries.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.list.table(result.list, model.names = names(result.list), M = 100,
  topic = "Summary Collection", method = c("rank", "weight", "count",
  "word"), annotate = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.list.table_+3A_result.list">result.list</code></td>
<td>
<p>List of mix of textreg.result objects and dataframes with two columns of
&quot;word&quot; and &quot;weight&quot;.  (The latter is for merging lists from other regression packages.)</p>
</td></tr>
<tr><td><code id="make.list.table_+3A_model.names">model.names</code></td>
<td>
<p>Names of the textreg.result objects</p>
</td></tr>
<tr><td><code id="make.list.table_+3A_m">M</code></td>
<td>
<p>maximum number of words to keep</p>
</td></tr>
<tr><td><code id="make.list.table_+3A_topic">topic</code></td>
<td>
<p>String A name for the topic</p>
</td></tr>
<tr><td><code id="make.list.table_+3A_method">method</code></td>
<td>
<p>Different ways to sort the phrases.  'word' means make a list of words.</p>
</td></tr>
<tr><td><code id="make.list.table_+3A_annotate">annotate</code></td>
<td>
<p>Add summary statistics to table such as phrase counts, etc.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Method will also order rows based on calculated importance of phrases.
Multiple ways of ordering are possible, via the <code>method</code> argument.
</p>
<p>Finally, the table can be annotated with descriptive statistics of the phrases.
</p>
<p>Warning: this method DOES NOT flip negative weight words (so negative weight usually look
less imporant in the ordering).
</p>
<p>See the bathtub vignette for an example of this method.
</p>


<h3>Value</h3>

<p>If annotate = true, a dataframe with each column corresponding to an textreg.result 
object (and possibly extra columns about phrases).  Otherwise a matrix of the word scores.
</p>

<hr>
<h2 id='make.path.matrix'>Generate matrix describing gradient descent path of textreg.</h2><span id='topic+make.path.matrix'></span>

<h3>Description</h3>

<p>Generate a matrix of the sequence of features as they are introduced with the textreg gradient descent 
program along with their coefficients with each step of the descent.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.path.matrix(res)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.path.matrix_+3A_res">res</code></td>
<td>
<p>A textreg.result object.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other plot.path.matrix: <code><a href="#topic+path.matrix.chart">path.matrix.chart</a></code>,
<code><a href="#topic+plot.textreg.result">plot.textreg.result</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( testCorpora )
testI = testCorpora$testI
res = textreg( testI$corpus, testI$labelI, c("frog","goat","bat"), C=2, verbosity=0 )	
make.path.matrix( res )
</code></pre>

<hr>
<h2 id='make.phrase.correlation.chart'>Generate visualization of phrase overlap.</h2><span id='topic+make.phrase.correlation.chart'></span>

<h3>Description</h3>

<p>Make simple chart showing which phrases have substantial overlap with other phrases.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.phrase.correlation.chart(result, count = FALSE, num.groups = 5,
  use.corrplot = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.phrase.correlation.chart_+3A_result">result</code></td>
<td>
<p>textreg.result object or a similarity matrix from a make.similarity.matrix call.</p>
</td></tr>
<tr><td><code id="make.phrase.correlation.chart_+3A_count">count</code></td>
<td>
<p>Display counts rather than similarity scores.</p>
</td></tr>
<tr><td><code id="make.phrase.correlation.chart_+3A_num.groups">num.groups</code></td>
<td>
<p>Number of groups to box.</p>
</td></tr>
<tr><td><code id="make.phrase.correlation.chart_+3A_use.corrplot">use.corrplot</code></td>
<td>
<p>Use the corrplot package of Taiyun Wei (will need to install it).</p>
</td></tr>
<tr><td><code id="make.phrase.correlation.chart_+3A_...">...</code></td>
<td>
<p>Extra arguments to pass to the image() plotting command.  See par.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Phrase Vizualization: <code><a href="#topic+cluster.phrases">cluster.phrases</a></code>,
<code><a href="#topic+make.appearance.matrix">make.appearance.matrix</a></code>,
<code><a href="#topic+make.similarity.matrix">make.similarity.matrix</a></code>
</p>

<hr>
<h2 id='make.phrase.matrix'>Make a table of where phrases appear in a corpus</h2><span id='topic+make.phrase.matrix'></span>

<h3>Description</h3>

<p>Generate a n by p phrase count matrix, with n being number of documents
and p being number of phrases:
\tabularrrrrr
0 \tab 0 \tab 0 \tab 0 \tab 0 \cr
1 \tab 6 \tab 2 \tab 0 \tab 0 \cr
8 \tab 0 \tab 0 \tab 0 \tab 0
This is the phrase equivilent of a document-term matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.phrase.matrix(phrase_list, corpus)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.phrase.matrix_+3A_phrase_list">phrase_list</code></td>
<td>
<p>List of strings</p>
</td></tr>
<tr><td><code id="make.phrase.matrix_+3A_corpus">corpus</code></td>
<td>
<p>A corpus object from tm package</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a n X p matrix, n being number of documents, p being number of phrases.
</p>


<h3>See Also</h3>

<p>Other textregCounting: <code><a href="#topic+make.count.table">make.count.table</a></code>,
<code><a href="#topic+phrase.count">phrase.count</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library( tm )
data( bathtub )
lbl = meta( bathtub )$meth.chl
head( make.phrase.matrix( c("bathtub","strip+", "vapor *"), bathtub ) )
</code></pre>

<hr>
<h2 id='make.similarity.matrix'>Calculate similarity matrix for set of phrases.</h2><span id='topic+make.similarity.matrix'></span>

<h3>Description</h3>

<p>First get phrase appearance pattern on positive labeling 
(if not directly passed)
and then calculate similarity matrix of how they are similar
to each other.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.similarity.matrix(result)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.similarity.matrix_+3A_result">result</code></td>
<td>
<p>An textreg.result object or a matrix from make.appearance.matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Warning: for 'negative weight' phrases this method does not do well since it ignores
negative documents.
</p>


<h3>See Also</h3>

<p>Other Phrase Vizualization: <code><a href="#topic+cluster.phrases">cluster.phrases</a></code>,
<code><a href="#topic+make.appearance.matrix">make.appearance.matrix</a></code>,
<code><a href="#topic+make.phrase.correlation.chart">make.phrase.correlation.chart</a></code>
</p>

<hr>
<h2 id='path.matrix.chart'>Plot optimization path of textreg.</h2><span id='topic+path.matrix.chart'></span>

<h3>Description</h3>

<p>Plot the sequence of features as they are introduced with the textreg gradient descent 
program.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>path.matrix.chart(path.matrix, xlab = "step", ylab = "beta",
  bty = "n", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="path.matrix.chart_+3A_path.matrix">path.matrix</code></td>
<td>
<p>Either a textreg.result object or a matrix from the make.path.matrix call.</p>
</td></tr>
<tr><td><code id="path.matrix.chart_+3A_xlab">xlab</code></td>
<td>
<p>Label for x axis</p>
</td></tr>
<tr><td><code id="path.matrix.chart_+3A_ylab">ylab</code></td>
<td>
<p>Label for y axis</p>
</td></tr>
<tr><td><code id="path.matrix.chart_+3A_bty">bty</code></td>
<td>
<p>Box for plot</p>
</td></tr>
<tr><td><code id="path.matrix.chart_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to the matplot() command.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other plot.path.matrix: <code><a href="#topic+make.path.matrix">make.path.matrix</a></code>,
<code><a href="#topic+plot.textreg.result">plot.textreg.result</a></code>
</p>

<hr>
<h2 id='phrase.count'>Count phrase appearance.</h2><span id='topic+phrase.count'></span>

<h3>Description</h3>

<p>Count number of times a _single_ phrase appears in the corpus
</p>


<h3>Usage</h3>

<pre><code class='language-R'>phrase.count(phrase, corp)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="phrase.count_+3A_phrase">phrase</code></td>
<td>
<p>A string</p>
</td></tr>
<tr><td><code id="phrase.count_+3A_corp">corp</code></td>
<td>
<p>A corpus object from tm package</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other textregCounting: <code><a href="#topic+make.count.table">make.count.table</a></code>,
<code><a href="#topic+make.phrase.matrix">make.phrase.matrix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library( tm )
data( bathtub )
phrase.count( "bathtub", bathtub )
</code></pre>

<hr>
<h2 id='phrase.matrix'>Make matrix of where phrases appear in corpus.</h2><span id='topic+phrase.matrix'></span>

<h3>Description</h3>

<p>Construct a $n X p$ matrix of appearances for selected phrases out of textreg object.
$n$ is the number of documents, $p$ is the number of phrases selected in the result object &lsquo;rules.&rsquo;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>phrase.matrix(rules, n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="phrase.matrix_+3A_rules">rules</code></td>
<td>
<p>Either a textreg.result object or the rules list from such an object.</p>
</td></tr>
<tr><td><code id="phrase.matrix_+3A_n">n</code></td>
<td>
<p>(Optional) If giving a rules list, the number of documents in corpus.</p>
</td></tr>
</table>

<hr>
<h2 id='phrases'>Get the phrases from the textreg.result object?</h2><span id='topic+phrases'></span>

<h3>Description</h3>

<p>Get the phrases from the textreg.result object?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>phrases(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="phrases_+3A_x">x</code></td>
<td>
<p>the object to check.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other textreg.result: <code><a href="#topic+is.textreg.result">is.textreg.result</a></code>,
<code><a href="#topic+print.textreg.result">print.textreg.result</a></code>,
<code><a href="#topic+reformat.textreg.model">reformat.textreg.model</a></code>
</p>

<hr>
<h2 id='plot.textreg.result'>Plot the sequence of features as they are introduced with the textreg gradient descent 
program.</h2><span id='topic+plot.textreg.result'></span>

<h3>Description</h3>

<p>Simply calls path.matrix.chart.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'textreg.result'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.textreg.result_+3A_x">x</code></td>
<td>
<p>A textreg.result object.</p>
</td></tr>
<tr><td><code id="plot.textreg.result_+3A_...">...</code></td>
<td>
<p>Parameters to be passed to path.matrix.chart.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>path.matrix.chart
</p>
<p>Other plot.path.matrix: <code><a href="#topic+make.path.matrix">make.path.matrix</a></code>,
<code><a href="#topic+path.matrix.chart">path.matrix.chart</a></code>
</p>

<hr>
<h2 id='predict.textreg.result'>Predict labeling with the selected phrases.</h2><span id='topic+predict.textreg.result'></span>

<h3>Description</h3>

<p>Given raw text and a textreg model, predict the labeling by counting appearance of 
relevant phrases in text and then multiplying these counts by the beta vector
associated with the textreg object.  Just like linear regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'textreg.result'
predict(object, new.text = NULL,
  return.matrix = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.textreg.result_+3A_object">object</code></td>
<td>
<p>A textreg.result object</p>
</td></tr>
<tr><td><code id="predict.textreg.result_+3A_new.text">new.text</code></td>
<td>
<p>If you want to predict for new text, pass it along.</p>
</td></tr>
<tr><td><code id="predict.textreg.result_+3A_return.matrix">return.matrix</code></td>
<td>
<p>TRUE means hand back the phrase appearance pattern matrix.</p>
</td></tr>
<tr><td><code id="predict.textreg.result_+3A_...">...</code></td>
<td>
<p>Nothing can be passed extra.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of predictions (numbers).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>res = textreg( c( "", "", "A", "A" ), c( -1, -1, 1, 1 ), 
      C=1, Lq=1, convergence.threshold=0.00000001, verbosity=0 )
predict( res )
predict( res, new.text=c("A B C A") )
</code></pre>

<hr>
<h2 id='print.fragment.sample'>Pretty print results of phrase sampling object.</h2><span id='topic+print.fragment.sample'></span>

<h3>Description</h3>

<p>Pretty print results of phrase sampling object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fragment.sample'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.fragment.sample_+3A_x">x</code></td>
<td>
<p>A fragment.sample object.</p>
</td></tr>
<tr><td><code id="print.fragment.sample_+3A_...">...</code></td>
<td>
<p>No extra options passed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other sample.fragments: <code><a href="#topic+is.fragment.sample">is.fragment.sample</a></code>,
<code><a href="#topic+sample.fragments">sample.fragments</a></code>
</p>

<hr>
<h2 id='print.textreg.corpus'>Pretty print textreg corpus object</h2><span id='topic+print.textreg.corpus'></span>

<h3>Description</h3>

<p>Pretty print textreg corpus object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'textreg.corpus'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.textreg.corpus_+3A_x">x</code></td>
<td>
<p>A textreg.corpus object.</p>
</td></tr>
<tr><td><code id="print.textreg.corpus_+3A_...">...</code></td>
<td>
<p>No extra options passed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other textreg.corpus: <code><a href="#topic+is.textreg.corpus">is.textreg.corpus</a></code>
</p>

<hr>
<h2 id='print.textreg.result'>Pretty print results of textreg regression.</h2><span id='topic+print.textreg.result'></span>

<h3>Description</h3>

<p>You can also reformat an textreg.result to get simpler diagnostics via <code><a href="#topic+reformat.textreg.model">reformat.textreg.model</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'textreg.result'
print(x, simple = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.textreg.result_+3A_x">x</code></td>
<td>
<p>A textreg.result object.</p>
</td></tr>
<tr><td><code id="print.textreg.result_+3A_simple">simple</code></td>
<td>
<p>TRUE means print out simpler results.  False includes some ugly detail.</p>
</td></tr>
<tr><td><code id="print.textreg.result_+3A_...">...</code></td>
<td>
<p>No extra options passed.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>reformat.textreg.model
</p>
<p>Other textreg.result: <code><a href="#topic+is.textreg.result">is.textreg.result</a></code>,
<code><a href="#topic+phrases">phrases</a></code>,
<code><a href="#topic+reformat.textreg.model">reformat.textreg.model</a></code>
</p>

<hr>
<h2 id='reformat.textreg.model'>Clean up output from textreg.</h2><span id='topic+reformat.textreg.model'></span>

<h3>Description</h3>

<p>Calculate some useful statistics (percents, etc) and return as dataframe.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reformat.textreg.model(model, short = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reformat.textreg.model_+3A_model">model</code></td>
<td>
<p>The model returned from <code><a href="#topic+textreg">textreg</a></code></p>
</td></tr>
<tr><td><code id="reformat.textreg.model_+3A_short">short</code></td>
<td>
<p>True if the output should be abbrviated for easy consumption.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Dataframe with statistics on the terms in the model
</p>


<h3>See Also</h3>

<p>Other textreg.result: <code><a href="#topic+is.textreg.result">is.textreg.result</a></code>,
<code><a href="#topic+phrases">phrases</a></code>, <code><a href="#topic+print.textreg.result">print.textreg.result</a></code>
</p>

<hr>
<h2 id='sample.fragments'>Sample fragments of text to contextualize a phrase.</h2><span id='topic+sample.fragments'></span>

<h3>Description</h3>

<p>Take a phrase, a labeling and a corpus and return text fragments
containing that phrase.
</p>
<p>Grab all phrases and then give sample of N from positive class
and N from negative class.  Sampling is to first sample from documents
and then sample a random phrase from each of those documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample.fragments(phrases, labeling, corp, N = 10, char.before = 80,
  char.after = char.before, metainfo = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample.fragments_+3A_phrases">phrases</code></td>
<td>
<p>Phrases to examine (a list of strings)</p>
</td></tr>
<tr><td><code id="sample.fragments_+3A_labeling">labeling</code></td>
<td>
<p>&ndash; a vector of the same length as the corpus</p>
</td></tr>
<tr><td><code id="sample.fragments_+3A_corp">corp</code></td>
<td>
<p>Corpus object (tm package Corpus object)</p>
</td></tr>
<tr><td><code id="sample.fragments_+3A_n">N</code></td>
<td>
<p>size of sample to make.</p>
</td></tr>
<tr><td><code id="sample.fragments_+3A_char.before">char.before</code></td>
<td>
<p>Number of characters of document to pull before phrase to give 
context.</p>
</td></tr>
<tr><td><code id="sample.fragments_+3A_char.after">char.after</code></td>
<td>
<p>As above, but trailing characters.  Defaults to char.before value.</p>
</td></tr>
<tr><td><code id="sample.fragments_+3A_metainfo">metainfo</code></td>
<td>
<p>&ndash; extra string to add to the printout for clarity if many such
printouts are being generated.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other sample.fragments: <code><a href="#topic+is.fragment.sample">is.fragment.sample</a></code>,
<code><a href="#topic+print.fragment.sample">print.fragment.sample</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library( tm )
data( bathtub )
sample.fragments( "bathtub", meta(bathtub)$meth.chl, bathtub )
</code></pre>

<hr>
<h2 id='save.corpus.to.files'>Save corpus to text (and RData) file.</h2><span id='topic+save.corpus.to.files'></span>

<h3>Description</h3>

<p>Small utility to save a corpus to a text file (and RData file) for ease of use.
</p>
<p>It is possibly recommended to pass a filename to the C++ function <code><a href="#topic+textreg">textreg</a></code>
rather than the entire corpus for
large text since I believe it will otherwise copy over everything due to the coder's (my) poor
understanding of how RCpp converts objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>save.corpus.to.files(bigcorp, filename = "corpus")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="save.corpus.to.files_+3A_bigcorp">bigcorp</code></td>
<td>
<p>A tm Corpus object.</p>
</td></tr>
<tr><td><code id="save.corpus.to.files_+3A_filename">filename</code></td>
<td>
<p>The first part of the filename.  A rda and txt extension will be appended to the two generated files.</p>
</td></tr>
</table>

<hr>
<h2 id='stem.corpus'>Step corpus with annotation.</h2><span id='topic+stem.corpus'></span>

<h3>Description</h3>

<p>Given a <code>tm</code>-package VCorpus of original text, 
returns a VCorpus of stemmed text with '+' appended to all stemmed words.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stem.corpus(corpus, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stem.corpus_+3A_corpus">corpus</code></td>
<td>
<p>Original text</p>
</td></tr>
<tr><td><code id="stem.corpus_+3A_verbose">verbose</code></td>
<td>
<p>True means print out text progress bar so you can watch progress.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is non-optimized code that is expensive to run.
First the stemmer chops words.  Then this method passes through and adds a &quot;+&quot;
to all chopped words, and builds a list of stems.
Finally, the method passes through and adds a &quot;+&quot; to all stems found without a
suffix.
</p>
<p>So, e.g., goblins and goblin will both be transformed to &quot;goblin+&quot;.
</p>
<p>Adding the '+' makes stemmed text more readible.
</p>
<p>Code based on code from Kevin Wu, UC Berkeley Undergrad Thesis 2014.
</p>
<p>Requires, via the tm package, the SnowballC package.
</p>
<p>Warning: Do not use this on a <code><a href="#topic+textreg.corpus">textreg.corpus</a></code> object.  Do to text before
building the <code><a href="#topic+textreg.corpus">textreg.corpus</a></code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
library( tm )
texts &lt;- c("texting goblins the dagger", "text these goblins", 
            "texting 3 goblins appl daggers goblining gobble")
corpus &lt;- VCorpus(VectorSource(texts))
stemmed_corpus&lt;-stem.corpus(corpus, verbose=FALSE)
inspect( stemmed_corpus[[2]] )

</code></pre>

<hr>
<h2 id='testCorpora'>Some small, fake test corpora.</h2><span id='topic+testCorpora'></span>

<h3>Description</h3>

<p>A list of several fake documents along with some labeling schemes primarily used by the unit testing code.
Also used in some examples.
</p>


<h3>Format</h3>

<p>A list of dataframes</p>

<hr>
<h2 id='textreg'>Sparse regression of labeling vector onto all phrases in a corpus.</h2><span id='topic+textreg'></span>

<h3>Description</h3>

<p>Given a labeling and a corpus, find phrases that predict this labeling.  This function
calls a C++ function that builds a tree of phrases and searches it using greedy coordinate
descent to solve the optimization problem associated with the associated sparse regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textreg(corpus, labeling, banned = NULL, objective.function = 2,
  C = 1, a = 1, maxIter = 40, verbosity = 1,
  step.verbosity = verbosity, positive.only = FALSE,
  binary.features = FALSE, no.regularization = FALSE,
  positive.weight = 1, Lq = 2, min.support = 1, min.pattern = 1,
  max.pattern = 100, gap = 0, token.type = "word",
  convergence.threshold = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textreg_+3A_corpus">corpus</code></td>
<td>
<p>A list of strings or a corpus from the <code>tm</code> package.</p>
</td></tr>
<tr><td><code id="textreg_+3A_labeling">labeling</code></td>
<td>
<p>A vector of +1/-1 or TRUE/FALSE indicating which documents are considered relevant and
which are baseline.  The +1/-1 can contain 0 whcih means drop the document.</p>
</td></tr>
<tr><td><code id="textreg_+3A_banned">banned</code></td>
<td>
<p>List of words that should be dropped from consideration.</p>
</td></tr>
<tr><td><code id="textreg_+3A_objective.function">objective.function</code></td>
<td>
<p>2 is hinge loss.  0 is something.  1 is something else.</p>
</td></tr>
<tr><td><code id="textreg_+3A_c">C</code></td>
<td>
<p>The regularization term.  0 is no regularization.</p>
</td></tr>
<tr><td><code id="textreg_+3A_a">a</code></td>
<td>
<p>What percent of regularization should be L1 loss (a=1) vs L2 loss (a=0)</p>
</td></tr>
<tr><td><code id="textreg_+3A_maxiter">maxIter</code></td>
<td>
<p>Number of gradient descent steps to take (not including intercept adjustments)</p>
</td></tr>
<tr><td><code id="textreg_+3A_verbosity">verbosity</code></td>
<td>
<p>Level of output.  0 is no printed output.</p>
</td></tr>
<tr><td><code id="textreg_+3A_step.verbosity">step.verbosity</code></td>
<td>
<p>Level of output for line searches.  0 is no printed output.</p>
</td></tr>
<tr><td><code id="textreg_+3A_positive.only">positive.only</code></td>
<td>
<p>Disallow negative features if true</p>
</td></tr>
<tr><td><code id="textreg_+3A_binary.features">binary.features</code></td>
<td>
<p>Just code presence/absence of a feature in a document rather than count of feature in document.</p>
</td></tr>
<tr><td><code id="textreg_+3A_no.regularization">no.regularization</code></td>
<td>
<p>Do not renormalize the features at all.  (Lq will be ignored.)</p>
</td></tr>
<tr><td><code id="textreg_+3A_positive.weight">positive.weight</code></td>
<td>
<p>Scale weight pf all positively marked documents by this value.  (1, i.e., no scaling) is default)   NOT FULLY IMPLEMENTED</p>
</td></tr>
<tr><td><code id="textreg_+3A_lq">Lq</code></td>
<td>
<p>Rescaling to put on the features (2 is standard).  Can be from 1 up.  Values above 10 invoke an infinity-norm.</p>
</td></tr>
<tr><td><code id="textreg_+3A_min.support">min.support</code></td>
<td>
<p>Only consider phrases that appear this many times or more.</p>
</td></tr>
<tr><td><code id="textreg_+3A_min.pattern">min.pattern</code></td>
<td>
<p>Only consider phrases this long or longer</p>
</td></tr>
<tr><td><code id="textreg_+3A_max.pattern">max.pattern</code></td>
<td>
<p>Only consider phrases this short or shorter</p>
</td></tr>
<tr><td><code id="textreg_+3A_gap">gap</code></td>
<td>
<p>Allow phrases that have wildcard words in them.  Number is how many wildcards in a row.</p>
</td></tr>
<tr><td><code id="textreg_+3A_token.type">token.type</code></td>
<td>
<p>&quot;word&quot; or &quot;character&quot; as tokens.</p>
</td></tr>
<tr><td><code id="textreg_+3A_convergence.threshold">convergence.threshold</code></td>
<td>
<p>How to decide if descent has converged.  (Will go for three steps at this threshold to check for flatness.)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the bathtub vignette for more complete discussion of this method and the options 
you might pass to it.
</p>


<h3>Value</h3>

<p>A <code><a href="#topic+textreg.result">textreg.result</a></code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data( testCorpora )
textreg( testCorpora$testI$corpus, testCorpora$testI$labelI, c(), C=1, verbosity=1 )
</code></pre>

<hr>
<h2 id='textreg-package'>Sparse regression package for text that allows for multiple word phrases.</h2><span id='topic+textreg-package'></span>

<h3>Description</h3>

<p>Built on Georgiana Ifrim's work, but allowing for regularization of phrases, this 
package does sparse regression using greedy coordinate descent.
In a nutshell, the textreg package allows for regressing a vector of +1/-1 labels onto raw text. 
The textreg package takes care of converting the text to all of the possible related features, allowing
you to think of the more organic statement of regressing onto &ldquo;text&rdquo; in some broad sense.
</p>


<h3>Details</h3>

<p>Implementation-wise, it is a wrapper for a modified version of the C++ code written by Georgiana Ifrim to do this regression.
It is also designed to (somewhat) integrate with the tm package, a commonly used R package for dealing with text.
</p>
<p>One warning: this package uses tm, but does need to generate vectors of character strings to pass to the textreg call, which can be quite expensive.  
You can also pass a filename to the textreg call instead, which allows one to avoid loading a large corpus into memory and then copying it over.
You can use a prior build.corpus command before textreg to mitigate this cost, but it is an imperfect method.
</p>
<p>The n-gram package is documented, but it is research code, meaning gaps and errors are possible; the author would appreciate notification of anything that is out of order.
</p>
<p>The primary method in this package is the regression call 'textreg()'. This method takes a corpus and a labeling vector and returns a textreg.result
object that contains the final regression result along with diagnostic information that can be of use.
</p>
<p>Start by reading the &ldquo;bathtub&rdquo; vignette, which walks through most of the functionality of this package.
</p>
<p>Special thanks and acknowledgements to Pavel Logacev, who found some subtle bugs on the windows platform and gave excellent advice in general.
Also thanks to Kevin Wu, who wrote earlier versions of the stemming and cross-validation code.  And Georgiana Ifrim, of course, for the earlier
version of the C++ code.
</p>


<h3>References</h3>

<p>Ifrim, G., Bakir, G., &amp; Weikum, G. (2008). Fast logistic regression for text categorization with variable-length n-grams. 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 354-362.
</p>
<p>Ifrim, G., &amp; Wiuf, C. (2011). Bounded coordinate-descent for biological sequence classification in high dimensional predictor space. 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 708-716.
</p>
<p>Jia, J., Miratrix, L., Yu, B., Gawalt, B., Ghaoui, El, L., Barnesmoore, L., &amp; Clavier, S. (2014). Concise Comparative Summaries (CCS) of Large Text Corpora with a Human Experiment. The Annals of Applied Statistics, 8(1), 499-529.
</p>
<p>Miratrix, L., &amp; Ackerman, R. (2014). A method for conducting text-based sparse feature selection for interpretability of selected phrases.
</p>

<hr>
<h2 id='tm_gregexpr'>Call gregexpr on the content of a tm Corpus.</h2><span id='topic+tm_gregexpr'></span>

<h3>Description</h3>

<p>Pull out content of a tm corpus and call gregexpr on that content represented
as a list of character strings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tm_gregexpr(pattern, corpus, ignore.case = FALSE, perl = FALSE,
  fixed = FALSE, useBytes = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tm_gregexpr_+3A_pattern">pattern</code></td>
<td>
<p>See gregexpr</p>
</td></tr>
<tr><td><code id="tm_gregexpr_+3A_corpus">corpus</code></td>
<td>
<p>Either a character vector or tm Corpus object.</p>
</td></tr>
<tr><td><code id="tm_gregexpr_+3A_ignore.case">ignore.case</code></td>
<td>
<p>See gregexpr</p>
</td></tr>
<tr><td><code id="tm_gregexpr_+3A_perl">perl</code></td>
<td>
<p>See gregexpr</p>
</td></tr>
<tr><td><code id="tm_gregexpr_+3A_fixed">fixed</code></td>
<td>
<p>See gregexpr</p>
</td></tr>
<tr><td><code id="tm_gregexpr_+3A_usebytes">useBytes</code></td>
<td>
<p>See gregexpr</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If 'corpus' is already a character vector, it just calls
gregexpr with no fuss (or warning).
</p>


<h3>Value</h3>

<p>This method gives results exactly as if <code><a href="base.html#topic+gregexpr">gregexpr</a></code> were called on the Corpus 
represented as a list of strings.
</p>
<p>See gregexpr.
</p>


<h3>See Also</h3>

<p>gregexpr
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
