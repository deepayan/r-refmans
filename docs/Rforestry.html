<!DOCTYPE html><html lang="en"><head><title>Help for package Rforestry</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Rforestry}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#addTrees'><p>addTrees-forestry</p></a></li>
<li><a href='#autoforestry'><p>autoforestry-forestry</p></a></li>
<li><a href='#autohonestRF'><p>Honest Random Forest</p></a></li>
<li><a href='#compute_lp-forestry'><p>compute lp distances</p></a></li>
<li><a href='#CppToR_translator'><p>Cpp to R translator</p></a></li>
<li><a href='#forest_checker'><p>Checks if forestry object has valid pointer for C++ object.</p></a></li>
<li><a href='#forestry'><p>forestry</p></a></li>
<li><a href='#forestry-class'><p>forestry class</p></a></li>
<li><a href='#getOOB-forestry'><p>getOOB-forestry</p></a></li>
<li><a href='#getOOBpreds-forestry'><p>getOOBpreds-forestry</p></a></li>
<li><a href='#getVI'><p>getVI-forestry</p></a></li>
<li><a href='#honestRF'><p>Honest Random Forest</p></a></li>
<li><a href='#impute_features'><p>Feature imputation using random forests neigborhoods</p></a></li>
<li><a href='#loadForestry'><p>load RF</p></a></li>
<li><a href='#make_savable'><p>make_savable</p></a></li>
<li><a href='#multilayer-forestry'><p>Multilayer forestry</p></a></li>
<li><a href='#plot-forestry'><p>visualize a tree</p></a></li>
<li><a href='#predict-forestry'><p>predict-forestry</p></a></li>
<li><a href='#predict-multilayer-forestry'><p>predict-multilayer-forestry</p></a></li>
<li><a href='#preprocess_testing'><p>preprocess_testing</p></a></li>
<li><a href='#preprocess_training'><p>preprocess_training</p></a></li>
<li><a href='#relinkCPP_prt'><p>relink CPP ptr</p></a></li>
<li><a href='#saveForestry'><p>save RF</p></a></li>
<li><a href='#testing_data_checker-forestry'><p>Test data check</p></a></li>
<li><a href='#training_data_checker'><p>Training data check</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Random Forests, Linear Trees, and Gradient Boosting for
Inference and Interpretability</td>
</tr>
<tr>
<td>Version:</td>
<td>0.11.1.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Theo Saarinen &lt;theo_s@berkeley.edu&gt;</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/forestry-labs/Rforestry/issues">https://github.com/forestry-labs/Rforestry/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/forestry-labs/Rforestry">https://github.com/forestry-labs/Rforestry</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Provides fast implementations of Random Forests, 
    Gradient Boosting, and Linear Random Forests, with an emphasis on inference 
    and interpretability. Additionally contains methods for variable 
    importance, out-of-bag prediction, regression monotonicity, and
    several methods for missing data imputation.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a> | file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.9), parallel, methods, visNetwork, glmnet (&ge;
4.1), grDevices, onehot</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, RcppThread</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown, mvtnorm</td>
</tr>
<tr>
<td>Collate:</td>
<td>'R_preprocessing.R' 'RcppExports.R' 'forestry.R'
'backwards_compatible.R' 'compute_rf_lp.R'
'neighborhood_imputation.R' 'plottree.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-03-14 14:37:59 UTC; edwardliu</td>
</tr>
<tr>
<td>Author:</td>
<td>Sören Künzel [aut],
  Theo Saarinen [aut, cre],
  Simon Walter [aut],
  Sam Antonyan [aut],
  Edward Liu [aut],
  Allen Tang [aut],
  Jasjeet Sekhon [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-03-15 23:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='addTrees'>addTrees-forestry</h2><span id='topic+addTrees'></span>

<h3>Description</h3>

<p>Add more trees to the existing forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addTrees(object, ntree)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="addTrees_+3A_object">object</code></td>
<td>
<p>A 'forestry' object.</p>
</td></tr>
<tr><td><code id="addTrees_+3A_ntree">ntree</code></td>
<td>
<p>Number of new trees to add</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 'forestry' object
</p>

<hr>
<h2 id='autoforestry'>autoforestry-forestry</h2><span id='topic+autoforestry'></span>

<h3>Description</h3>

<p>autoforestry-forestry
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autoforestry(
  x,
  y,
  sampsize = as.integer(nrow(x) * 0.75),
  num_iter = 1024,
  eta = 2,
  verbose = FALSE,
  seed = 24750371,
  nthread = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="autoforestry_+3A_x">x</code></td>
<td>
<p>A data frame of all training predictors.</p>
</td></tr>
<tr><td><code id="autoforestry_+3A_y">y</code></td>
<td>
<p>A vector of all training responses.</p>
</td></tr>
<tr><td><code id="autoforestry_+3A_sampsize">sampsize</code></td>
<td>
<p>The size of total samples to draw for the training data.</p>
</td></tr>
<tr><td><code id="autoforestry_+3A_num_iter">num_iter</code></td>
<td>
<p>Maximum iterations/epochs per configuration. Default is 1024.</p>
</td></tr>
<tr><td><code id="autoforestry_+3A_eta">eta</code></td>
<td>
<p>Downsampling rate. Default value is 2.</p>
</td></tr>
<tr><td><code id="autoforestry_+3A_verbose">verbose</code></td>
<td>
<p>if tuning process in verbose mode</p>
</td></tr>
<tr><td><code id="autoforestry_+3A_seed">seed</code></td>
<td>
<p>random seed</p>
</td></tr>
<tr><td><code id="autoforestry_+3A_nthread">nthread</code></td>
<td>
<p>Number of threads to train and predict theforest. The default
number is 0 which represents using all cores.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 'forestry' object
</p>

<hr>
<h2 id='autohonestRF'>Honest Random Forest</h2><span id='topic+autohonestRF'></span>

<h3>Description</h3>

<p>This function is deprecated and only exists for backwards
backwards compatibility. The function you want to use is 'autoforestry'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autohonestRF(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="autohonestRF_+3A_...">...</code></td>
<td>
<p>parameters which are passed directly to 'autoforestry'</p>
</td></tr>
</table>

<hr>
<h2 id='compute_lp-forestry'>compute lp distances</h2><span id='topic+compute_lp-forestry'></span><span id='topic+compute_lp'></span>

<h3>Description</h3>

<p>return lp ditances of selected test observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_lp(object, feature.new, feature, p)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compute_lp-forestry_+3A_object">object</code></td>
<td>
<p>A 'forestry' object.</p>
</td></tr>
<tr><td><code id="compute_lp-forestry_+3A_feature.new">feature.new</code></td>
<td>
<p>A data frame of testing predictors.</p>
</td></tr>
<tr><td><code id="compute_lp-forestry_+3A_feature">feature</code></td>
<td>
<p>A string denoting the dimension for computing lp distances.</p>
</td></tr>
<tr><td><code id="compute_lp-forestry_+3A_p">p</code></td>
<td>
<p>A positive real number determining the norm p-norm used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector lp distances.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Set seed for reproductivity
set.seed(292313)

# Use Iris Data
test_idx &lt;- sample(nrow(iris), 11)
x_train &lt;- iris[-test_idx, -1]
y_train &lt;- iris[-test_idx, 1]
x_test &lt;- iris[test_idx, -1]

rf &lt;- forestry(x = x_train, y = y_train)
predict(rf, x_test)

# Compute the l2 distances in the "Petal.Length" dimension
distances_2 &lt;- compute_lp(object = rf,
                          feature.new = x_test,
                          feature = "Petal.Length",
                          p = 2)
</code></pre>

<hr>
<h2 id='CppToR_translator'>Cpp to R translator</h2><span id='topic+CppToR_translator'></span>

<h3>Description</h3>

<p>Add more trees to the existing forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CppToR_translator(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CppToR_translator_+3A_object">object</code></td>
<td>
<p>external CPP pointer that should be translated from Cpp to an R
object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of lists. Each sublist contains the information to span a
tree.
</p>

<hr>
<h2 id='forest_checker'>Checks if forestry object has valid pointer for C++ object.</h2><span id='topic+forest_checker'></span>

<h3>Description</h3>

<p>Checks if forestry object has valid pointer for C++ object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>forest_checker(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="forest_checker_+3A_object">object</code></td>
<td>
<p>a forestry object</p>
</td></tr>
</table>

<hr>
<h2 id='forestry'>forestry</h2><span id='topic+forestry'></span>

<h3>Description</h3>

<p>forestry
</p>


<h3>Usage</h3>

<pre><code class='language-R'>forestry(
  x,
  y,
  ntree = 500,
  replace = TRUE,
  sampsize = if (replace) nrow(x) else ceiling(0.632 * nrow(x)),
  sample.fraction = NULL,
  mtry = max(floor(ncol(x)/3), 1),
  nodesizeSpl = 3,
  nodesizeAvg = 3,
  nodesizeStrictSpl = 1,
  nodesizeStrictAvg = 1,
  minSplitGain = 0,
  maxDepth = round(nrow(x)/2) + 1,
  interactionDepth = maxDepth,
  interactionVariables = numeric(0),
  featureWeights = NULL,
  deepFeatureWeights = NULL,
  observationWeights = NULL,
  splitratio = 1,
  seed = as.integer(runif(1) * 1000),
  verbose = FALSE,
  nthread = 0,
  splitrule = "variance",
  middleSplit = FALSE,
  maxObs = length(y),
  linear = FALSE,
  linFeats = 0:(ncol(x) - 1),
  monotonicConstraints = rep(0, ncol(x)),
  overfitPenalty = 1,
  doubleTree = FALSE,
  reuseforestry = NULL,
  savable = TRUE,
  saveable = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="forestry_+3A_x">x</code></td>
<td>
<p>A data frame of all training predictors.</p>
</td></tr>
<tr><td><code id="forestry_+3A_y">y</code></td>
<td>
<p>A vector of all training responses.</p>
</td></tr>
<tr><td><code id="forestry_+3A_ntree">ntree</code></td>
<td>
<p>The number of trees to grow in the forest. The default value is
500.</p>
</td></tr>
<tr><td><code id="forestry_+3A_replace">replace</code></td>
<td>
<p>An indicator of whether sampling of training data is with
replacement. The default value is TRUE.</p>
</td></tr>
<tr><td><code id="forestry_+3A_sampsize">sampsize</code></td>
<td>
<p>The size of total samples to draw for the training data. If
sampling with replacement, the default value is the length of the training
data. If samplying without replacement, the default value is two-third of
the length of the training data.</p>
</td></tr>
<tr><td><code id="forestry_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>if this is given, then sampsize is ignored and set to
be round(length(y) * sample.fraction). It must be a real number between 0
and 1</p>
</td></tr>
<tr><td><code id="forestry_+3A_mtry">mtry</code></td>
<td>
<p>The number of variables randomly selected at each split point.
The default value is set to be one third of total number of features of the
training data.</p>
</td></tr>
<tr><td><code id="forestry_+3A_nodesizespl">nodesizeSpl</code></td>
<td>
<p>Minimum observations contained in terminal nodes. The
default value is 3.</p>
</td></tr>
<tr><td><code id="forestry_+3A_nodesizeavg">nodesizeAvg</code></td>
<td>
<p>Minimum size of terminal nodes for averaging dataset. The
default value is 3.</p>
</td></tr>
<tr><td><code id="forestry_+3A_nodesizestrictspl">nodesizeStrictSpl</code></td>
<td>
<p>Minimum observations to follow strictly in terminal
nodes. The default value is 1.</p>
</td></tr>
<tr><td><code id="forestry_+3A_nodesizestrictavg">nodesizeStrictAvg</code></td>
<td>
<p>Minimum size of terminal nodes for averaging dataset
to follow strictly. The default value is 1.</p>
</td></tr>
<tr><td><code id="forestry_+3A_minsplitgain">minSplitGain</code></td>
<td>
<p>Minimum loss reduction to split a node further in a tree.</p>
</td></tr>
<tr><td><code id="forestry_+3A_maxdepth">maxDepth</code></td>
<td>
<p>Maximum depth of a tree. The default value is 99.</p>
</td></tr>
<tr><td><code id="forestry_+3A_interactiondepth">interactionDepth</code></td>
<td>
<p>All splits at or above interaction depth must be on variables
that are not weighting variables (as provided by the interactionVariables argument)</p>
</td></tr>
<tr><td><code id="forestry_+3A_interactionvariables">interactionVariables</code></td>
<td>
<p>Indices of weighting variables.</p>
</td></tr>
<tr><td><code id="forestry_+3A_featureweights">featureWeights</code></td>
<td>
<p>(optional) vector of sampling probablities/weights for
each feature used when subsampling mtry features at each node above or at
interactionDepth. The default is to use uniform probabilities.</p>
</td></tr>
<tr><td><code id="forestry_+3A_deepfeatureweights">deepFeatureWeights</code></td>
<td>
<p>used in place of featureWeights for splits below interactionDepth.</p>
</td></tr>
<tr><td><code id="forestry_+3A_observationweights">observationWeights</code></td>
<td>
<p>These denote the weights for each training observation
which determines how likely the observation is to be selected in each bootstrap
sample. This option is not allowed when sampling is done without replacement.</p>
</td></tr>
<tr><td><code id="forestry_+3A_splitratio">splitratio</code></td>
<td>
<p>Proportion of the training data used as the splitting
dataset. It is a ratio between 0 and 1. If the ratio is 1, then essentially
splitting dataset becomes the total entire sampled set and the averaging
dataset is empty. If the ratio is 0, then the splitting data set is empty
and all the data is used for the averaging data set (This is not a good
usage however since there will be no data available for splitting).</p>
</td></tr>
<tr><td><code id="forestry_+3A_seed">seed</code></td>
<td>
<p>random seed</p>
</td></tr>
<tr><td><code id="forestry_+3A_verbose">verbose</code></td>
<td>
<p>if training process in verbose mode</p>
</td></tr>
<tr><td><code id="forestry_+3A_nthread">nthread</code></td>
<td>
<p>Number of threads to train and predict the forest. The default
number is 0 which represents using all cores.</p>
</td></tr>
<tr><td><code id="forestry_+3A_splitrule">splitrule</code></td>
<td>
<p>only variance is implemented at this point and it contains
specifies the loss function according to which the splits of random forest
should be made</p>
</td></tr>
<tr><td><code id="forestry_+3A_middlesplit">middleSplit</code></td>
<td>
<p>if the split value is taking the average of two feature
values. If false, it will take a point based on a uniform distribution
between two feature values. (Default = FALSE)</p>
</td></tr>
<tr><td><code id="forestry_+3A_maxobs">maxObs</code></td>
<td>
<p>The max number of observations to split on</p>
</td></tr>
<tr><td><code id="forestry_+3A_linear">linear</code></td>
<td>
<p>Fit the model with a ridge regression or not</p>
</td></tr>
<tr><td><code id="forestry_+3A_linfeats">linFeats</code></td>
<td>
<p>Specify which features to split linearly on when using
linear (defaults to use all numerical features)</p>
</td></tr>
<tr><td><code id="forestry_+3A_monotonicconstraints">monotonicConstraints</code></td>
<td>
<p>Specifies monotonic relationships between the
continuous features and the outcome. Supplied as a vector of length p with
entries in 1,0,-1 which 1 indicating an increasing monotonic relationship,
-1 indicating a decreasing monotonic relationship, and 0 indicating no
relationship. Constraints supplied for categorical will be ignored.</p>
</td></tr>
<tr><td><code id="forestry_+3A_overfitpenalty">overfitPenalty</code></td>
<td>
<p>Value to determine how much to penalize magnitude of
coefficients in ridge regression</p>
</td></tr>
<tr><td><code id="forestry_+3A_doubletree">doubleTree</code></td>
<td>
<p>if the number of tree is doubled as averaging and splitting
data can be exchanged to create decorrelated trees. (Default = FALSE)</p>
</td></tr>
<tr><td><code id="forestry_+3A_reuseforestry">reuseforestry</code></td>
<td>
<p>pass in an 'forestry' object which will recycle the
dataframe the old object created. It will save some space working on the
same dataset.</p>
</td></tr>
<tr><td><code id="forestry_+3A_savable">savable</code></td>
<td>
<p>If TRUE, then RF is created in such a way that it can be
saved and loaded using save(...) and load(...). Setting it to TRUE
(default) will, however, take longer and it will use more memory. When
training many RF, it makes a lot of sense to set this to FALSE to save
time and memory.</p>
</td></tr>
<tr><td><code id="forestry_+3A_saveable">saveable</code></td>
<td>
<p>deprecated. Do not use.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 'forestry' object.
</p>


<h3>Note</h3>

<p>Treatment of missing data
</p>
<p>When training the forest, if a splitting feature is missing for an
observation, we assign that observation to the child node which has
an average y closer to the observed y of the observation with the
missing feature, and record how many observations with missingness
went to each child.
</p>
<p>At predict time, if there were missing observations in a node at
training time, we randomly assign an observation with a missing
feature to a child node with probability proportional to the number
of observations with a missing splitting variable that went to each
child at training time. If there was no missingness at training
time, we assign to the child nodes with probability proportional to
the number of observations in each child node.
</p>
<p>This procedure is a generalization of the usual recommended
approach to missingness for forests&mdash;i.e., at each point add a
decision to send the NAs to the left, right or to split on NA
versus no NA. This usual recommendation is heuristically equivalent
to adding an indicator for each feature plus a recoding of each
missing variable where the missigness is the maximum and then the
minimum observed value. This recommendation, however, allows the
method to pickup time effects for when variables are missing
because of the indicator. We, therefore, do not allow splitting on
NAs. This should increase MSE in training but hopefully allows for
better learning of universal relationships. Importantly, it is
straightforward to show that our approach is weakly dominant in
expected MSE to the always left or right approach. We should also
note that almost no software package actually implements even the
usual recommended approach&mdash;e.g., ranger does not.
</p>
<p>In version 0.8.2.09, the procedure for identifying the best variable to split
on when there is missing training data was modified. Previously candidate
variables were evaluated by computing the MSE taken over all observations,
including those for which the splitting variable was missing. In the current
implementation we only use observations for which the splitting variable is
not missing. The previous approach was biased towards splitting on variables
with missingness because observations with a missing splitting variable are
assigned to the leaf that minimized the MSE.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(292315)
library(Rforestry)
test_idx &lt;- sample(nrow(iris), 3)
x_train &lt;- iris[-test_idx, -1]
y_train &lt;- iris[-test_idx, 1]
x_test &lt;- iris[test_idx, -1]

rf &lt;- forestry(x = x_train, y = y_train)
weights = predict(rf, x_test, aggregation = "weightMatrix")$weightMatrix

weights %*% y_train
predict(rf, x_test)

set.seed(49)
library(Rforestry)

n &lt;- c(100)
a &lt;- rnorm(n)
b &lt;- rnorm(n)
c &lt;- rnorm(n)
y &lt;- 4*a + 5.5*b - .78*c
x &lt;- data.frame(a,b,c)

forest &lt;- forestry(
          x,
          y,
          ntree = 10,
          replace = TRUE,
          nodesizeStrictSpl = 5,
          nodesizeStrictAvg = 5,
          linear = TRUE
          )

predict(forest, x)
</code></pre>

<hr>
<h2 id='forestry-class'>forestry class</h2><span id='topic+forestry-class'></span>

<h3>Description</h3>

<p>'honestRF' class only exists for backwards compatibility reasons
</p>

<hr>
<h2 id='getOOB-forestry'>getOOB-forestry</h2><span id='topic+getOOB-forestry'></span><span id='topic+getOOB'></span><span id='topic+getOOB+2Cforestry-method'></span>

<h3>Description</h3>

<p>Calculate the out-of-bag error of a given forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getOOB(object, noWarning)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getOOB-forestry_+3A_object">object</code></td>
<td>
<p>A 'forestry' object.</p>
</td></tr>
<tr><td><code id="getOOB-forestry_+3A_nowarning">noWarning</code></td>
<td>
<p>flag to not display warnings</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The OOB error of the forest.
</p>

<hr>
<h2 id='getOOBpreds-forestry'>getOOBpreds-forestry</h2><span id='topic+getOOBpreds-forestry'></span><span id='topic+getOOBpreds'></span>

<h3>Description</h3>

<p>Calculate the out-of-bag predictions of a given forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getOOBpreds(object, noWarning)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getOOBpreds-forestry_+3A_object">object</code></td>
<td>
<p>A trained model object of class &quot;forestry&quot;.</p>
</td></tr>
<tr><td><code id="getOOBpreds-forestry_+3A_nowarning">noWarning</code></td>
<td>
<p>Flag to not display warnings.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The vector of all training observations, with their out of bag
predictions. Note each observation is out of bag for different trees, and so
the predictions will be more or less stable based on the observation. Some
observations may not be out of bag for any trees, and here the predictions
are returned as NA.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+forestry">forestry</a></code>
</p>

<hr>
<h2 id='getVI'>getVI-forestry</h2><span id='topic+getVI'></span>

<h3>Description</h3>

<p>Calculate increase in OOB for each shuffled feature for forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getVI(object, noWarning)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getVI_+3A_object">object</code></td>
<td>
<p>A 'forestry' object.</p>
</td></tr>
<tr><td><code id="getVI_+3A_nowarning">noWarning</code></td>
<td>
<p>flag to not display warnings</p>
</td></tr>
</table>


<h3>Note</h3>

<p>No seed is passed to this function so it is
not possible in the current implementation to replicate the vector
permutations used when measuring feature importance.
</p>

<hr>
<h2 id='honestRF'>Honest Random Forest</h2><span id='topic+honestRF'></span>

<h3>Description</h3>

<p>This function is deprecated and only exists for backwards
backwards compatibility. The function you want to use is 'forestry'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>honestRF(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="honestRF_+3A_...">...</code></td>
<td>
<p>parameters which are passed directly to 'forestry'</p>
</td></tr>
</table>

<hr>
<h2 id='impute_features'>Feature imputation using random forests neigborhoods</h2><span id='topic+impute_features'></span>

<h3>Description</h3>

<p>This function uses the neighborhoods implied by a random forest
to impute missing features. The neighbors of a data point are all the
training points assigned to the same leaf in at least one tree in the
forest. The weight of each neighbor is the fraction of trees in the forest
for which it was assigned to the same leaf. We impute a missing features
for a point by computing the average, using neighborhoods weights, for all
of the point's neighbors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>impute_features(
  object,
  feature.new,
  seed = round(runif(1) * 10000),
  use_mean_imputation_fallback = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="impute_features_+3A_object">object</code></td>
<td>
<p>an object of class 'forestry'</p>
</td></tr>
<tr><td><code id="impute_features_+3A_feature.new">feature.new</code></td>
<td>
<p>the feature data.frame we will impute</p>
</td></tr>
<tr><td><code id="impute_features_+3A_seed">seed</code></td>
<td>
<p>a random seed passed to the predict method of forestry</p>
</td></tr>
<tr><td><code id="impute_features_+3A_use_mean_imputation_fallback">use_mean_imputation_fallback</code></td>
<td>
<p>if TRUE, mean imputation (for numeric
variables) and mode imputation (for factor variables) is used for missing
features for which all neighbors also had the corresponding feature
missing; if FALSE these missing features remain as NAs in the data frame
returned by 'impute_features'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame that is feature.new with imputed missing values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>iris_with_missing &lt;- iris
idx_miss_factor &lt;- sample(nrow(iris), 25, replace = TRUE)
iris_with_missing[idx_miss_factor, 5] &lt;- NA
idx_miss_numeric &lt;- sample(nrow(iris), 25, replace = TRUE)
iris_with_missing[idx_miss_numeric, 3] &lt;- NA

x &lt;- iris_with_missing[,-1]
y &lt;- iris_with_missing[, 1]

forest &lt;- forestry(x, y, ntree = 500, seed = 2)
imputed_x &lt;- impute_features(forest, x, seed = 2)
</code></pre>

<hr>
<h2 id='loadForestry'>load RF</h2><span id='topic+loadForestry'></span>

<h3>Description</h3>

<p>This wrapper function checks the forestry object, makes it
saveable if needed, and then saves it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loadForestry(filename)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="loadForestry_+3A_filename">filename</code></td>
<td>
<p>a filename in which to store the 'forestry' object</p>
</td></tr>
</table>

<hr>
<h2 id='make_savable'>make_savable</h2><span id='topic+make_savable'></span><span id='topic+make_savable+2Cforestry-method'></span>

<h3>Description</h3>

<p>When a 'foresty' object is saved and then reloaded the Cpp
pointers for the data set and the Cpp forest have to be reconstructed
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_savable(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="make_savable_+3A_object">object</code></td>
<td>
<p>an object of class 'forestry'</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of lists. Each sublist contains the information to span a
tree.
</p>


<h3>Note</h3>

<p>'make_savable' does not translate all of the private member variables
of the C++ forestry object so when the forest is reconstructed with
'relinkCPP_prt' some attributes are lost. For example, 'nthreads' will be
reset to zero. This makes it impossible to disable threading when
predicting for forests loaded from disk.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(323652639)
x &lt;- iris[, -1]
y &lt;- iris[, 1]
forest &lt;- forestry(x, y, ntree = 3)
y_pred_before &lt;- predict(forest, x)

forest &lt;- make_savable(forest)
saveForestry(forest, file = "forest.Rda")
rm(forest)

forest &lt;- loadForestry("forest.Rda")

y_pred_after &lt;- predict(forest, x)
testthat::expect_equal(y_pred_before, y_pred_after, tolerance = 0.000001)
file.remove("forest.Rda")
</code></pre>

<hr>
<h2 id='multilayer-forestry'>Multilayer forestry</h2><span id='topic+multilayer-forestry'></span><span id='topic+multilayerForestry'></span>

<h3>Description</h3>

<p>Construct a gradient boosted random forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multilayerForestry(
  x,
  y,
  ntree = 500,
  nrounds = 1,
  eta = 0.3,
  replace = FALSE,
  sampsize = nrow(x),
  sample.fraction = NULL,
  mtry = ncol(x),
  nodesizeSpl = 3,
  nodesizeAvg = 3,
  nodesizeStrictSpl = max(round(nrow(x)/128), 1),
  nodesizeStrictAvg = max(round(nrow(x)/128), 1),
  minSplitGain = 0,
  maxDepth = 99,
  splitratio = 1,
  seed = as.integer(runif(1) * 1000),
  verbose = FALSE,
  nthread = 0,
  splitrule = "variance",
  middleSplit = TRUE,
  maxObs = length(y),
  linear = FALSE,
  linFeats = 0:(ncol(x) - 1),
  monotonicConstraints = rep(0, ncol(x)),
  featureWeights = rep(1, ncol(x)),
  deepFeatureWeights = featureWeights,
  observationWeights = NULL,
  overfitPenalty = 1,
  doubleTree = FALSE,
  reuseforestry = NULL,
  savable = TRUE,
  saveable = saveable
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="multilayer-forestry_+3A_x">x</code></td>
<td>
<p>A data frame of all training predictors.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_y">y</code></td>
<td>
<p>A vector of all training responses.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_ntree">ntree</code></td>
<td>
<p>The number of trees to grow in the forest. The default value is
500.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_nrounds">nrounds</code></td>
<td>
<p>Number of iterations used for gradient boosting.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_eta">eta</code></td>
<td>
<p>Step size shrinkage used in gradient boosting update.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_replace">replace</code></td>
<td>
<p>An indicator of whether sampling of training data is with
replacement. The default value is TRUE.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_sampsize">sampsize</code></td>
<td>
<p>The size of total samples to draw for the training data. If
sampling with replacement, the default value is the length of the training
data. If samplying without replacement, the default value is two-third of
the length of the training data.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>if this is given, then sampsize is ignored and set to
be round(length(y) * sample.fraction). It must be a real number between 0
and 1</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_mtry">mtry</code></td>
<td>
<p>The number of variables randomly selected at each split point.
The default value is set to be one third of total number of features of the
training data.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_nodesizespl">nodesizeSpl</code></td>
<td>
<p>Minimum observations contained in terminal nodes. The
default value is 3.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_nodesizeavg">nodesizeAvg</code></td>
<td>
<p>Minimum size of terminal nodes for averaging dataset. The
default value is 3.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_nodesizestrictspl">nodesizeStrictSpl</code></td>
<td>
<p>Minimum observations to follow strictly in terminal
nodes. The default value is 1.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_nodesizestrictavg">nodesizeStrictAvg</code></td>
<td>
<p>Minimum size of terminal nodes for averaging dataset
to follow strictly. The default value is 1.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_minsplitgain">minSplitGain</code></td>
<td>
<p>Minimum loss reduction to split a node further in a tree.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_maxdepth">maxDepth</code></td>
<td>
<p>Maximum depth of a tree. The default value is 99.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_splitratio">splitratio</code></td>
<td>
<p>Proportion of the training data used as the splitting
dataset. It is a ratio between 0 and 1. If the ratio is 1, then essentially
splitting dataset becomes the total entire sampled set and the averaging
dataset is empty. If the ratio is 0, then the splitting data set is empty
and all the data is used for the averaging data set (This is not a good
usage however since there will be no data available for splitting).</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_seed">seed</code></td>
<td>
<p>random seed</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_verbose">verbose</code></td>
<td>
<p>if training process in verbose mode</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_nthread">nthread</code></td>
<td>
<p>Number of threads to train and predict the forest. The default
number is 0 which represents using all cores.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_splitrule">splitrule</code></td>
<td>
<p>only variance is implemented at this point and it contains
specifies the loss function according to which the splits of random forest
should be made</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_middlesplit">middleSplit</code></td>
<td>
<p>if the split value is taking the average of two feature
values. If false, it will take a point based on a uniform distribution
between two feature values. (Default = FALSE)</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_maxobs">maxObs</code></td>
<td>
<p>The max number of observations to split on</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_linear">linear</code></td>
<td>
<p>Fit the model with a ridge regression or not</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_linfeats">linFeats</code></td>
<td>
<p>Specify which features to split linearly on when using
linear (defaults to use all numerical features)</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_monotonicconstraints">monotonicConstraints</code></td>
<td>
<p>Specifies monotonic relationships between the
continuous features and the outcome. Supplied as a vector of length p with
entries in 1,0,-1 which 1 indicating an increasing monotonic relationship,
-1 indicating a decreasing monotonic relationship, and 0 indicating no
relationship. Constraints supplied for categorical will be ignored.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_featureweights">featureWeights</code></td>
<td>
<p>weights used when subsampling features for nodes above or at interactionDepth.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_deepfeatureweights">deepFeatureWeights</code></td>
<td>
<p>weights used when subsampling features for nodes below interactionDepth.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_observationweights">observationWeights</code></td>
<td>
<p>These denote the weights for each training observation
which determines how likely the observation is to be selected in each bootstrap
sample. This option is not allowed when sampling is done without replacement.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_overfitpenalty">overfitPenalty</code></td>
<td>
<p>Value to determine how much to penalize magnitude of
coefficients in ridge regression</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_doubletree">doubleTree</code></td>
<td>
<p>if the number of tree is doubled as averaging and splitting
data can be exchanged to create decorrelated trees. (Default = FALSE)</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_reuseforestry">reuseforestry</code></td>
<td>
<p>pass in an 'forestry' object which will recycle the
dataframe the old object created. It will save some space working on the
same dataset.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_savable">savable</code></td>
<td>
<p>If TRUE, then RF is created in such a way that it can be
saved and loaded using save(...) and load(...). Setting it to TRUE
(default) will, however, take longer and it will use more memory. When
training many RF, it makes a lot of sense to set this to FALSE to save
time and memory.</p>
</td></tr>
<tr><td><code id="multilayer-forestry_+3A_saveable">saveable</code></td>
<td>
<p>deprecated. Do not use.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 'multilayerForestry' object.
</p>

<hr>
<h2 id='plot-forestry'>visualize a tree</h2><span id='topic+plot-forestry'></span><span id='topic+plot.forestry'></span>

<h3>Description</h3>

<p>plots a tree in the forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'forestry'
plot(x, tree.id = 1, print.meta_dta = FALSE, beta.char.len = 30, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot-forestry_+3A_x">x</code></td>
<td>
<p>A forestry x.</p>
</td></tr>
<tr><td><code id="plot-forestry_+3A_tree.id">tree.id</code></td>
<td>
<p>Specifies the tree number that should be visulaized.</p>
</td></tr>
<tr><td><code id="plot-forestry_+3A_print.meta_dta">print.meta_dta</code></td>
<td>
<p>Should the data for the plot be printed?</p>
</td></tr>
<tr><td><code id="plot-forestry_+3A_beta.char.len">beta.char.len</code></td>
<td>
<p>The length of the beta values in leaf node
representation.</p>
</td></tr>
<tr><td><code id="plot-forestry_+3A_...">...</code></td>
<td>
<p>additional arguments that are not used.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(292315)
rf &lt;- forestry(x = iris[,-1],
               y = iris[, 1])

plot(x = rf)
plot(x = rf, tree.id = 2)
plot(x = rf, tree.id = 500)

ridge_rf &lt;- forestry(
  x = iris[,-1],
  y = iris[, 1],
  replace = FALSE,
  nodesizeStrictSpl = 10,
  mtry = 4,
  ntree = 10,
  minSplitGain = .004,
  linear = TRUE,
  overfitPenalty = 1.65,
  linFeats = 1:2)

plot(x = ridge_rf)
plot(x = ridge_rf, tree.id = 2)
plot(x = ridge_rf, tree.id = 10)

</code></pre>

<hr>
<h2 id='predict-forestry'>predict-forestry</h2><span id='topic+predict-forestry'></span><span id='topic+predict.forestry'></span>

<h3>Description</h3>

<p>Return the prediction from the forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'forestry'
predict(
  object,
  feature.new,
  aggregation = "average",
  seed = as.integer(runif(1) * 10000),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict-forestry_+3A_object">object</code></td>
<td>
<p>A 'forestry' object.</p>
</td></tr>
<tr><td><code id="predict-forestry_+3A_feature.new">feature.new</code></td>
<td>
<p>A data frame of testing predictors.</p>
</td></tr>
<tr><td><code id="predict-forestry_+3A_aggregation">aggregation</code></td>
<td>
<p>How the individual tree predictions are aggregated:
'average' returns the mean of all trees in the forest; 'weightMatrix'
returns a list consisting of &quot;weightMatrix&quot;, the adaptive nearest neighbor
weights used to construct the predictions; &quot;terminalNodes&quot;, a matrix where
the ith entry of the jth column is the index of the leaf node to which the
ith observation is assigned in the jth tree; and &quot;sparse&quot;, a matrix
where the ith entry in the jth column is 1 if the ith observation in
feature.new is assigned to the jth leaf and 0 otherwise. In each tree the
leaves are indexed using a depth first ordering, and, in the &quot;sparse&quot;
representation, the first leaf in the second tree has column index one more than
the number of leaves in the first tree and so on. So, for example, if the
first tree has 5 leaves, the sixth column of the &quot;sparse&quot; matrix corresponds
to the first leaf in the second tree.</p>
</td></tr>
<tr><td><code id="predict-forestry_+3A_seed">seed</code></td>
<td>
<p>random seed</p>
</td></tr>
<tr><td><code id="predict-forestry_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predicted responses.
</p>

<hr>
<h2 id='predict-multilayer-forestry'>predict-multilayer-forestry</h2><span id='topic+predict-multilayer-forestry'></span><span id='topic+predict.multilayerForestry'></span>

<h3>Description</h3>

<p>Return the prediction from the forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'multilayerForestry'
predict(
  object,
  feature.new,
  aggregation = "average",
  seed = as.integer(runif(1) * 10000),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict-multilayer-forestry_+3A_object">object</code></td>
<td>
<p>A 'multilayerForestry' object.</p>
</td></tr>
<tr><td><code id="predict-multilayer-forestry_+3A_feature.new">feature.new</code></td>
<td>
<p>A data frame of testing predictors.</p>
</td></tr>
<tr><td><code id="predict-multilayer-forestry_+3A_aggregation">aggregation</code></td>
<td>
<p>How shall the leaf be aggregated. The default is to return
the mean of the leave 'average'. Other options are 'weightMatrix'.</p>
</td></tr>
<tr><td><code id="predict-multilayer-forestry_+3A_seed">seed</code></td>
<td>
<p>random seed</p>
</td></tr>
<tr><td><code id="predict-multilayer-forestry_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predicted responses.
</p>

<hr>
<h2 id='preprocess_testing'>preprocess_testing</h2><span id='topic+preprocess_testing'></span>

<h3>Description</h3>

<p>Perform preprocessing for the testing data, including converting
data to dataframe, and testing if the columns are consistent with the
training data and encoding categorical data into numerical representation
in the same way as training data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>preprocess_testing(x, categoricalFeatureCols, categoricalFeatureMapping)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="preprocess_testing_+3A_x">x</code></td>
<td>
<p>A data frame of all training predictors.</p>
</td></tr>
<tr><td><code id="preprocess_testing_+3A_categoricalfeaturecols">categoricalFeatureCols</code></td>
<td>
<p>A list of index for all categorical data. Used
for trees to detect categorical columns.</p>
</td></tr>
<tr><td><code id="preprocess_testing_+3A_categoricalfeaturemapping">categoricalFeatureMapping</code></td>
<td>
<p>A list of encoding details for each
categorical column, including all unique factor values and their
corresponding numeric representation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A preprocessed training dataaset x
</p>

<hr>
<h2 id='preprocess_training'>preprocess_training</h2><span id='topic+preprocess_training'></span>

<h3>Description</h3>

<p>Perform preprocessing for the training data, including
converting data to dataframe, and encoding categorical data into numerical
representation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>preprocess_training(x, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="preprocess_training_+3A_x">x</code></td>
<td>
<p>A data frame of all training predictors.</p>
</td></tr>
<tr><td><code id="preprocess_training_+3A_y">y</code></td>
<td>
<p>A vector of all training responses.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of two datasets along with necessary information that encoding
the preprocessing.
</p>

<hr>
<h2 id='relinkCPP_prt'>relink CPP ptr</h2><span id='topic+relinkCPP_prt'></span>

<h3>Description</h3>

<p>When a 'foresty' object is saved and then reloaded the Cpp
pointers for the data set and the Cpp forest have to be reconstructed
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relinkCPP_prt(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="relinkCPP_prt_+3A_object">object</code></td>
<td>
<p>an object of class 'forestry' or class 'multilayerForestry'</p>
</td></tr>
</table>

<hr>
<h2 id='saveForestry'>save RF</h2><span id='topic+saveForestry'></span>

<h3>Description</h3>

<p>This wrapper function checks the forestry object, makes it
saveable if needed, and then saves it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>saveForestry(object, filename, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="saveForestry_+3A_object">object</code></td>
<td>
<p>an object of class 'forestry'</p>
</td></tr>
<tr><td><code id="saveForestry_+3A_filename">filename</code></td>
<td>
<p>a filename in which to store the 'forestry' object</p>
</td></tr>
<tr><td><code id="saveForestry_+3A_...">...</code></td>
<td>
<p>additional arguments useful for specifying compression type and level</p>
</td></tr>
</table>

<hr>
<h2 id='testing_data_checker-forestry'>Test data check</h2><span id='topic+testing_data_checker-forestry'></span><span id='topic+testing_data_checker'></span>

<h3>Description</h3>

<p>Check the testing data to do prediction
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testing_data_checker(object, feature.new, hasNas)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="testing_data_checker-forestry_+3A_object">object</code></td>
<td>
<p>A forestry object.</p>
</td></tr>
<tr><td><code id="testing_data_checker-forestry_+3A_feature.new">feature.new</code></td>
<td>
<p>A data frame of testing predictors.</p>
</td></tr>
<tr><td><code id="testing_data_checker-forestry_+3A_hasnas">hasNas</code></td>
<td>
<p>TRUE if the there were NAs in the training data FALSE otherwise.</p>
</td></tr>
</table>

<hr>
<h2 id='training_data_checker'>Training data check</h2><span id='topic+training_data_checker'></span>

<h3>Description</h3>

<p>Check the input to forestry constructor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>training_data_checker(
  x,
  y,
  ntree,
  replace,
  sampsize,
  mtry,
  nodesizeSpl,
  nodesizeAvg,
  nodesizeStrictSpl,
  nodesizeStrictAvg,
  minSplitGain,
  maxDepth,
  interactionDepth,
  splitratio,
  nthread,
  middleSplit,
  doubleTree,
  linFeats,
  monotonicConstraints,
  featureWeights,
  deepFeatureWeights,
  observationWeights,
  linear,
  hasNas
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="training_data_checker_+3A_x">x</code></td>
<td>
<p>A data frame of all training predictors.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_y">y</code></td>
<td>
<p>A vector of all training responses.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_ntree">ntree</code></td>
<td>
<p>The number of trees to grow in the forest. The default value is
500.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_replace">replace</code></td>
<td>
<p>An indicator of whether sampling of training data is with
replacement. The default value is TRUE.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_sampsize">sampsize</code></td>
<td>
<p>The size of total samples to draw for the training data. If
sampling with replacement, the default value is the length of the training
data. If samplying without replacement, the default value is two-third of
the length of the training data.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_mtry">mtry</code></td>
<td>
<p>The number of variables randomly selected at each split point.
The default value is set to be one third of total number of features of the
training data.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_nodesizespl">nodesizeSpl</code></td>
<td>
<p>Minimum observations contained in terminal nodes. The
default value is 3.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_nodesizeavg">nodesizeAvg</code></td>
<td>
<p>Minimum size of terminal nodes for averaging dataset. The
default value is 3.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_nodesizestrictspl">nodesizeStrictSpl</code></td>
<td>
<p>Minimum observations to follow strictly in terminal
nodes. The default value is 1.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_nodesizestrictavg">nodesizeStrictAvg</code></td>
<td>
<p>Minimum size of terminal nodes for averaging dataset
to follow strictly. The default value is 1.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_minsplitgain">minSplitGain</code></td>
<td>
<p>Minimum loss reduction to split a node further in a tree.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_maxdepth">maxDepth</code></td>
<td>
<p>Maximum depth of a tree. The default value is 99.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_interactiondepth">interactionDepth</code></td>
<td>
<p>All splits at or above interaction depth must be on variables
that are not weighting variables (as provided by the interactionVariables argument)</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_splitratio">splitratio</code></td>
<td>
<p>Proportion of the training data used as the splitting
dataset. It is a ratio between 0 and 1. If the ratio is 1, then essentially
splitting dataset becomes the total entire sampled set and the averaging
dataset is empty. If the ratio is 0, then the splitting data set is empty
and all the data is used for the averaging data set (This is not a good
usage however since there will be no data available for splitting).</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_nthread">nthread</code></td>
<td>
<p>Number of threads to train and predict the forest. The default
number is 0 which represents using all cores.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_middlesplit">middleSplit</code></td>
<td>
<p>if the split value is taking the average of two feature
values. If false, it will take a point based on a uniform distribution
between two feature values. (Default = FALSE)</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_doubletree">doubleTree</code></td>
<td>
<p>if the number of tree is doubled as averaging and splitting
data can be exchanged to create decorrelated trees. (Default = FALSE)</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_linfeats">linFeats</code></td>
<td>
<p>Specify which features to split linearly on when using
linear (defaults to use all numerical features)</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_monotonicconstraints">monotonicConstraints</code></td>
<td>
<p>Specifies monotonic relationships between the
continuous features and the outcome. Supplied as a vector of length p with
entries in 1,0,-1 which 1 indicating an increasing monotonic relationship,
-1 indicating a decreasing monotonic relationship, and 0 indicating no
relationship. Constraints supplied for categorical will be ignored.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_featureweights">featureWeights</code></td>
<td>
<p>weights used when subsampling features for nodes above or at interactionDepth.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_deepfeatureweights">deepFeatureWeights</code></td>
<td>
<p>weights used when subsampling features for nodes below interactionDepth.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_observationweights">observationWeights</code></td>
<td>
<p>These denote the weights for each training observation
which determines how likely the observation is to be selected in each bootstrap
sample. This option is not allowed when sampling is done without replacement.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_linear">linear</code></td>
<td>
<p>Fit the model with a ridge regression or not</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_hasnas">hasNas</code></td>
<td>
<p>indicates if there is any missingness in x.</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
