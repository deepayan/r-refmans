<!DOCTYPE html><html><head><title>Help for package Rforestry</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Rforestry}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#addTrees'><p>addTrees-forestry</p></a></li>
<li><a href='#compute_lp-forestry'><p>compute lp distances</p></a></li>
<li><a href='#CppToR_translator'><p>Cpp to R translator</p></a></li>
<li><a href='#forest_checker'><p>Checks if forestry object has valid pointer for C++ object.</p></a></li>
<li><a href='#forestry'><p>forestry</p></a></li>
<li><a href='#forestry-class'><p>forestry class</p></a></li>
<li><a href='#getOOB-forestry'><p>getOOB-forestry</p></a></li>
<li><a href='#getOOBpreds-forestry'><p>getOOBpreds-forestry</p></a></li>
<li><a href='#getVI'><p>getVI-forestry</p></a></li>
<li><a href='#honestRF'><p>Honest Random Forest</p></a></li>
<li><a href='#impute_features'><p>Feature imputation using random forests neighborhoods</p></a></li>
<li><a href='#loadForestry'><p>load RF</p></a></li>
<li><a href='#make_savable'><p>make_savable</p></a></li>
<li><a href='#plot-forestry'><p>visualize a tree</p></a></li>
<li><a href='#predict-forestry'><p>predict-forestry</p></a></li>
<li><a href='#predictInfo'><p>predictInfo-forestry</p></a></li>
<li><a href='#preprocess_testing'><p>preprocess_testing</p></a></li>
<li><a href='#preprocess_training'><p>preprocess_training</p></a></li>
<li><a href='#relinkCPP_prt'><p>relink CPP ptr</p></a></li>
<li><a href='#saveForestry'><p>save RF</p></a></li>
<li><a href='#scale_center'><p>scale_center</p></a></li>
<li><a href='#testing_data_checker-forestry'><p>Test data check</p></a></li>
<li><a href='#training_data_checker'><p>Training data check</p></a></li>
<li><a href='#unscale_uncenter'><p>unscale_uncenter</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Random Forests, Linear Trees, and Gradient Boosting for
Inference and Interpretability</td>
</tr>
<tr>
<td>Version:</td>
<td>0.10.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Theo Saarinen &lt;theo_s@berkeley.edu&gt;</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/forestry-labs/Rforestry/issues">https://github.com/forestry-labs/Rforestry/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/forestry-labs/Rforestry">https://github.com/forestry-labs/Rforestry</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Provides fast implementations of Honest Random Forests, 
    Gradient Boosting, and Linear Random Forests, with an emphasis on inference 
    and interpretability. Additionally contains methods for variable 
    importance, out-of-bag prediction, regression monotonicity, and
    several methods for missing data imputation. Soren R. Kunzel, 
    Theo F. Saarinen, Edward W. Liu, Jasjeet S. Sekhon (2019) &lt;<a href="https://arxiv.org/abs/1906.06463">arXiv:1906.06463</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.9), parallel, methods, visNetwork, glmnet (&ge;
4.1), grDevices, onehot, pROC</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, RcppThread</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown, mvtnorm</td>
</tr>
<tr>
<td>Collate:</td>
<td>'R_preprocessing.R' 'RcppExports.R' 'forestry.R'
'backwards_compatible.R' 'compute_rf_lp.R'
'neighborhood_imputation.R' 'plottree.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-03-24 14:55:23 UTC; theosaa</td>
</tr>
<tr>
<td>Author:</td>
<td>Sören Künzel [aut],
  Theo Saarinen [aut, cre],
  Simon Walter [aut],
  Sam Antonyan [aut],
  Edward Liu [aut],
  Allen Tang [aut],
  Jasjeet Sekhon [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-03-25 00:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='addTrees'>addTrees-forestry</h2><span id='topic+addTrees'></span>

<h3>Description</h3>

<p>Add more trees to the existing forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addTrees(object, ntree)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addTrees_+3A_object">object</code></td>
<td>
<p>A 'forestry' object.</p>
</td></tr>
<tr><td><code id="addTrees_+3A_ntree">ntree</code></td>
<td>
<p>Number of new trees to add</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 'forestry' object
</p>

<hr>
<h2 id='compute_lp-forestry'>compute lp distances</h2><span id='topic+compute_lp-forestry'></span><span id='topic+compute_lp'></span>

<h3>Description</h3>

<p>Return the L_p norm distances of selected test observations
relative to the training observations which the forest was trained on.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_lp(
  object,
  newdata,
  feature,
  p,
  scale = FALSE,
  aggregation = "average",
  trainingIdx = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_lp-forestry_+3A_object">object</code></td>
<td>
<p>A 'forestry' object.</p>
</td></tr>
<tr><td><code id="compute_lp-forestry_+3A_newdata">newdata</code></td>
<td>
<p>A data frame of test predictors.</p>
</td></tr>
<tr><td><code id="compute_lp-forestry_+3A_feature">feature</code></td>
<td>
<p>A string denoting the dimension for computing lp distances.</p>
</td></tr>
<tr><td><code id="compute_lp-forestry_+3A_p">p</code></td>
<td>
<p>A positive real number determining the norm p-norm used.</p>
</td></tr>
<tr><td><code id="compute_lp-forestry_+3A_scale">scale</code></td>
<td>
<p>A boolean indicating whether or not we want to center + scale
the features (based on the mean and sd of the training data) before calculating
the L_p norm. This is useful for computing the detachment index, but can be
less useful when we need to interpret the L_p distances.</p>
</td></tr>
<tr><td><code id="compute_lp-forestry_+3A_aggregation">aggregation</code></td>
<td>
<p>The aggregation used when the weightMatrix is calculated.
This can be useful for calculating the lp distances on observations in
the training data. This must be one of 'average', 'oob', or 'doubleOOB'.
When newdata has fewer rows than the training data, one must also pass
the vector of training indices corresponding to the indices of the observations
in the original data set. Default is 'average'.</p>
</td></tr>
<tr><td><code id="compute_lp-forestry_+3A_trainingidx">trainingIdx</code></td>
<td>
<p>This is an optional parameter that must be set when
aggregation is set to 'oob' or 'doubleOOB' and the newdata is not the same
size as the training data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of the lp distances.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Set seed for reproductivity
set.seed(292313)

# Use Iris Data
test_idx &lt;- sample(nrow(iris), 11)
x_train &lt;- iris[-test_idx, -1]
y_train &lt;- iris[-test_idx, 1]
x_test &lt;- iris[test_idx, -1]

rf &lt;- forestry(x = x_train, y = y_train,nthread = 2)
predict(rf, x_test)

# Compute the l2 distances in the "Petal.Length" dimension
distances_2 &lt;- compute_lp(object = rf,
                          newdata = x_test,
                          feature = "Petal.Length",
                          p = 2)
</code></pre>

<hr>
<h2 id='CppToR_translator'>Cpp to R translator</h2><span id='topic+CppToR_translator'></span>

<h3>Description</h3>

<p>Add more trees to the existing forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CppToR_translator(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CppToR_translator_+3A_object">object</code></td>
<td>
<p>external CPP pointer that should be translated from Cpp to an R
object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of lists. Each sublist contains the information to span a
tree.
</p>

<hr>
<h2 id='forest_checker'>Checks if forestry object has valid pointer for C++ object.</h2><span id='topic+forest_checker'></span>

<h3>Description</h3>

<p>Checks if forestry object has valid pointer for C++ object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>forest_checker(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="forest_checker_+3A_object">object</code></td>
<td>
<p>a forestry object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A message if the forest does not have a valid C++ pointer.
</p>

<hr>
<h2 id='forestry'>forestry</h2><span id='topic+forestry'></span>

<h3>Description</h3>

<p>forestry
</p>


<h3>Usage</h3>

<pre><code class='language-R'>forestry(
  x,
  y,
  ntree = 500,
  replace = TRUE,
  sampsize = if (replace) nrow(x) else ceiling(0.632 * nrow(x)),
  sample.fraction = NULL,
  mtry = max(floor(ncol(x)/3), 1),
  nodesizeSpl = 5,
  nodesizeAvg = 5,
  nodesizeStrictSpl = 1,
  nodesizeStrictAvg = 1,
  minSplitGain = 0,
  maxDepth = round(nrow(x)/2) + 1,
  interactionDepth = maxDepth,
  interactionVariables = numeric(0),
  featureWeights = NULL,
  deepFeatureWeights = NULL,
  observationWeights = NULL,
  customSplitSample = NULL,
  customAvgSample = NULL,
  customExcludeSample = NULL,
  splitratio = 1,
  OOBhonest = FALSE,
  doubleBootstrap = if (OOBhonest) TRUE else FALSE,
  seed = as.integer(runif(1) * 1000),
  verbose = FALSE,
  nthread = 0,
  splitrule = "variance",
  middleSplit = FALSE,
  maxObs = length(y),
  linear = FALSE,
  linFeats = 0:(ncol(x) - 1),
  monotonicConstraints = rep(0, ncol(x)),
  groups = NULL,
  minTreesPerFold = 0,
  foldSize = 1,
  monotoneAvg = FALSE,
  overfitPenalty = 1,
  scale = TRUE,
  doubleTree = FALSE,
  naDirection = FALSE,
  reuseforestry = NULL,
  savable = TRUE,
  saveable = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="forestry_+3A_x">x</code></td>
<td>
<p>A data frame of all training predictors.</p>
</td></tr>
<tr><td><code id="forestry_+3A_y">y</code></td>
<td>
<p>A vector of all training responses.</p>
</td></tr>
<tr><td><code id="forestry_+3A_ntree">ntree</code></td>
<td>
<p>The number of trees to grow in the forest. The default value is
500.</p>
</td></tr>
<tr><td><code id="forestry_+3A_replace">replace</code></td>
<td>
<p>An indicator of whether sampling of training data is with
replacement. The default value is TRUE.</p>
</td></tr>
<tr><td><code id="forestry_+3A_sampsize">sampsize</code></td>
<td>
<p>The size of total samples to draw for the training data. If
sampling with replacement, the default value is the length of the training
data. If sampling without replacement, the default value is two-thirds of
the length of the training data.</p>
</td></tr>
<tr><td><code id="forestry_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>If this is given, then sampsize is ignored and set to
be round(length(y) * sample.fraction). It must be a real number between 0 and 1</p>
</td></tr>
<tr><td><code id="forestry_+3A_mtry">mtry</code></td>
<td>
<p>The number of variables randomly selected at each split point.
The default value is set to be one-third of the total number of features of the training data.</p>
</td></tr>
<tr><td><code id="forestry_+3A_nodesizespl">nodesizeSpl</code></td>
<td>
<p>Minimum observations contained in terminal nodes.
The default value is 5.</p>
</td></tr>
<tr><td><code id="forestry_+3A_nodesizeavg">nodesizeAvg</code></td>
<td>
<p>Minimum size of terminal nodes for averaging dataset.
The default value is 5.</p>
</td></tr>
<tr><td><code id="forestry_+3A_nodesizestrictspl">nodesizeStrictSpl</code></td>
<td>
<p>Minimum observations to follow strictly in terminal nodes.
The default value is 1.</p>
</td></tr>
<tr><td><code id="forestry_+3A_nodesizestrictavg">nodesizeStrictAvg</code></td>
<td>
<p>The minimum size of terminal nodes for averaging data set to follow when predicting.
No splits are allowed that result in nodes with observations less than this parameter.
This parameter enforces overlap of the averaging data set with the splitting set when training.
When using honesty, splits that leave less than nodesizeStrictAvg averaging
observations in either child node will be rejected, ensuring every leaf node
also has at least nodesizeStrictAvg averaging observations. The default value is 1.</p>
</td></tr>
<tr><td><code id="forestry_+3A_minsplitgain">minSplitGain</code></td>
<td>
<p>Minimum loss reduction to split a node further in a tree.</p>
</td></tr>
<tr><td><code id="forestry_+3A_maxdepth">maxDepth</code></td>
<td>
<p>Maximum depth of a tree. The default value is 99.</p>
</td></tr>
<tr><td><code id="forestry_+3A_interactiondepth">interactionDepth</code></td>
<td>
<p>All splits at or above interaction depth must be on
variables that are not weighting variables (as provided by the interactionVariables argument).</p>
</td></tr>
<tr><td><code id="forestry_+3A_interactionvariables">interactionVariables</code></td>
<td>
<p>Indices of weighting variables.</p>
</td></tr>
<tr><td><code id="forestry_+3A_featureweights">featureWeights</code></td>
<td>
<p>(optional) vector of sampling probabilities/weights for each
feature used when subsampling mtry features at each node above or at interactionDepth.
The default is to use uniform probabilities.</p>
</td></tr>
<tr><td><code id="forestry_+3A_deepfeatureweights">deepFeatureWeights</code></td>
<td>
<p>Used in place of featureWeights for splits below interactionDepth.</p>
</td></tr>
<tr><td><code id="forestry_+3A_observationweights">observationWeights</code></td>
<td>
<p>Denotes the weights for each training observation
that determine how likely the observation is to be selected in each bootstrap sample.
This option is not allowed when sampling is done without replacement.</p>
</td></tr>
<tr><td><code id="forestry_+3A_customsplitsample">customSplitSample</code></td>
<td>
<p>List of vectors for user-defined splitting observations per tree. The vector at
index i contains the indices of the sampled splitting observations, with replacement allowed, for tree i.
This feature overrides other sampling parameters and must be set in conjunction with customAvgSample.</p>
</td></tr>
<tr><td><code id="forestry_+3A_customavgsample">customAvgSample</code></td>
<td>
<p>List of vectors for user-defined averaging observations per tree. The vector at
index i contains the indices of the sampled splitting observations, with replacement allowed, for tree i.
This feature overrides other sampling parameters and must be set in conjunction with customSplitSample.</p>
</td></tr>
<tr><td><code id="forestry_+3A_customexcludesample">customExcludeSample</code></td>
<td>
<p>An optional list of vectors for user-defined excluded observations per tree. The vector at
index i contains the indices of the excluded observations for tree i. An observation is considered excluded if it does
not appear in the splitting or averaging set and has been explicitly withheld from being sampled for a tree.
Excluded observations are not considered out-of-bag, so when we call predict with aggregation = &quot;oob&quot;,
when we predict for an observation, we will only use the predictions of trees in which the
observation was in the customSplitSample (and neither in the customAvgSample nor the customExcludeSample).
This parameter is optional even when customSplitSample and customAvgSample are set.
It is also optional at the tree level, so can have fewer than ntree entries. When given fewer than
ntree entries, for example K, the entries will be applied to the first K trees in the forest and
the remaining trees will have no excludedSamples.</p>
</td></tr>
<tr><td><code id="forestry_+3A_splitratio">splitratio</code></td>
<td>
<p>Proportion of the training data used as the splitting dataset.
It is a ratio between 0 and 1. If the ratio is 1 (the default), then the splitting
set uses the entire data, as does the averaging set&mdash;i.e., the standard Breiman RF setup.
If the ratio is 0, then the splitting data set is empty, and the entire dataset is used
for the averaging set (This is not a good usage, however, since there will be no data available for splitting).</p>
</td></tr>
<tr><td><code id="forestry_+3A_oobhonest">OOBhonest</code></td>
<td>
<p>In this version of honesty, the out-of-bag observations for each tree
are used as the honest (averaging) set. This setting also changes how predictions
are constructed. When predicting for observations that are out-of-sample
(using predict(..., aggregation = &quot;average&quot;)), all the trees in the forest
are used to construct predictions. When predicting for an observation that was in-sample (using
predict(..., aggregation = &quot;oob&quot;)), only the trees for which that observation
was not in the averaging set are used to construct the prediction for that observation.
aggregation=&quot;oob&quot; (out-of-bag) ensures that the outcome value for an observation
is never used to construct predictions for a given observation even when it is in sample.
This property does not hold in standard honesty, which relies on an asymptotic
subsampling argument. By default, when OOBhonest = TRUE, the out-of-bag observations
for each tree are resamples with replacement to be used for the honest (averaging)
set. This results in a third set of observations that are left out of both
the splitting and averaging set, we call these the double out-of-bag (doubleOOB)
observations. In order to get the predictions of only the trees in which each
observation fell into this doubleOOB set, one can run predict(... , aggregation = &quot;doubleOOB&quot;).
In order to not do this second bootstrap sample, the doubleBootstrap flag can
be set to FALSE.</p>
</td></tr>
<tr><td><code id="forestry_+3A_doublebootstrap">doubleBootstrap</code></td>
<td>
<p>The doubleBootstrap flag provides the option to resample
with replacement from the out-of-bag observations set for each tree to construct
the averaging set when using OOBhonest. If this is FALSE, the out-of-bag observations
are used as the averaging set. By default this option is TRUE when running OOBhonest = TRUE.
This option increases diversity across trees.</p>
</td></tr>
<tr><td><code id="forestry_+3A_seed">seed</code></td>
<td>
<p>random seed</p>
</td></tr>
<tr><td><code id="forestry_+3A_verbose">verbose</code></td>
<td>
<p>Indicator to train the forest in verbose mode</p>
</td></tr>
<tr><td><code id="forestry_+3A_nthread">nthread</code></td>
<td>
<p>Number of threads to train and predict the forest. The default
number is 0 which represents using all cores.</p>
</td></tr>
<tr><td><code id="forestry_+3A_splitrule">splitrule</code></td>
<td>
<p>Only variance is implemented at this point and it
specifies the loss function according to which the splits of random forest
should be made.</p>
</td></tr>
<tr><td><code id="forestry_+3A_middlesplit">middleSplit</code></td>
<td>
<p>Indicator of whether the split value is takes the average of two feature
values. If FALSE, it will take a point based on a uniform distribution
between two feature values. (Default = FALSE)</p>
</td></tr>
<tr><td><code id="forestry_+3A_maxobs">maxObs</code></td>
<td>
<p>The max number of observations to split on.</p>
</td></tr>
<tr><td><code id="forestry_+3A_linear">linear</code></td>
<td>
<p>Indicator that enables Ridge penalized splits and linear aggregation
functions in the leaf nodes. This is recommended for data with linear outcomes.
For implementation details, see: https://arxiv.org/abs/1906.06463. Default is FALSE.</p>
</td></tr>
<tr><td><code id="forestry_+3A_linfeats">linFeats</code></td>
<td>
<p>A vector containing the indices of which features to split
linearly on when using linear penalized splits (defaults to use all numerical features).</p>
</td></tr>
<tr><td><code id="forestry_+3A_monotonicconstraints">monotonicConstraints</code></td>
<td>
<p>Specifies monotonic relationships between the continuous
features and the outcome. Supplied as a vector of length p with entries in
1,0,-1 which 1 indicating an increasing monotonic relationship, -1 indicating
a decreasing monotonic relationship, and 0 indicating no constraint.
Constraints supplied for categorical variable will be ignored.</p>
</td></tr>
<tr><td><code id="forestry_+3A_groups">groups</code></td>
<td>
<p>A vector of factors specifying the group membership of each training observation.
these groups are used in the aggregation when doing out of bag predictions in
order to predict with only trees where the entire group was not used for aggregation.
This allows the user to specify custom subgroups which will be used to create
predictions which do not use any data from a common group to make predictions for
any observation in the group. This can be used to create general custom
resampling schemes, and provide predictions consistent with the Out-of-Group set.</p>
</td></tr>
<tr><td><code id="forestry_+3A_mintreesperfold">minTreesPerFold</code></td>
<td>
<p>The number of trees which we make sure have been created leaving
out each fold (each fold is a set of randomly selected groups).
This is 0 by default, so we will not give any special treatment to
the groups when sampling observations, however if this is set to a positive integer, we
modify the bootstrap sampling scheme to ensure that exactly that many trees
have each group left out. We do this by, for each fold, creating minTreesPerFold
trees which are built on observations sampled from the set of training observations
which are not in a group in the current fold. The folds form a random partition of
all of the possible groups, each of size foldSize. This means we create at
least # folds * minTreesPerFold trees for the forest.
If ntree &gt; # folds * minTreesPerFold, we create
max(# folds * minTreesPerFold, ntree) total trees, in which at least minTreesPerFold
are created leaving out each fold.</p>
</td></tr>
<tr><td><code id="forestry_+3A_foldsize">foldSize</code></td>
<td>
<p>The number of groups that are selected randomly for each fold to be
left out when using minTreesPerFold. When minTreesPerFold is set and foldSize is
set, all possible groups will be partitioned into folds, each containing foldSize unique groups
(if foldSize doesn't evenly divide the number of groups, a single fold will be smaller,
as it will contain the remaining groups). Then minTreesPerFold are grown with each
entire fold of groups left out.</p>
</td></tr>
<tr><td><code id="forestry_+3A_monotoneavg">monotoneAvg</code></td>
<td>
<p>This is a boolean flag that indicates whether or not monotonic
constraints should be enforced on the averaging set in addition to the splitting set.
This flag is meaningless unless both honesty and monotonic constraints are in use.
The default is FALSE.</p>
</td></tr>
<tr><td><code id="forestry_+3A_overfitpenalty">overfitPenalty</code></td>
<td>
<p>Value to determine how much to penalize the magnitude
of coefficients in ridge regression when using linear splits.</p>
</td></tr>
<tr><td><code id="forestry_+3A_scale">scale</code></td>
<td>
<p>A parameter which indicates whether or not we want to scale and center
the covariates and outcome before doing the regression. This can help with
stability, so by default is TRUE.</p>
</td></tr>
<tr><td><code id="forestry_+3A_doubletree">doubleTree</code></td>
<td>
<p>if the number of tree is doubled as averaging and splitting
data can be exchanged to create decorrelated trees. (Default = FALSE)</p>
</td></tr>
<tr><td><code id="forestry_+3A_nadirection">naDirection</code></td>
<td>
<p>Sets a default direction for missing values in each split
node during training. It test placing all missing values to the left and
right, then selects the direction that minimizes loss. If no missing values
exist, then a default direction is randomly selected in proportion to the
distribution of observations on the left and right. (Default = FALSE)</p>
</td></tr>
<tr><td><code id="forestry_+3A_reuseforestry">reuseforestry</code></td>
<td>
<p>Pass in an 'forestry' object which will recycle the
dataframe the old object created. It will save some space working on the
same data set.</p>
</td></tr>
<tr><td><code id="forestry_+3A_savable">savable</code></td>
<td>
<p>If TRUE, then RF is created in such a way that it can be
saved and loaded using save(...) and load(...). However, setting it to TRUE
(default) will take longer and use more memory. When
training many RF, it makes sense to set this to FALSE to save time and memory.</p>
</td></tr>
<tr><td><code id="forestry_+3A_saveable">saveable</code></td>
<td>
<p>deprecated. Do not use.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 'forestry' object.
</p>


<h3>Note</h3>

<p>Treatment of Missing Data
</p>
<p>In version 0.9.0.34, we have modified the handling of missing data. Instead of
the greedy approach used in previous iterations, we now test any potential
split by putting all NA's to the right, and all NA's to the left, and taking
the choice which gives the best MSE for the split. Under this version of handling
the potential splits, we will still respect monotonic constraints. So if we put all
NA's to either side, and the resulting leaf nodes have means which violate
the monotone constraints, the split will be rejected.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(292315)
library(Rforestry)
test_idx &lt;- sample(nrow(iris), 3)
x_train &lt;- iris[-test_idx, -1]
y_train &lt;- iris[-test_idx, 1]
x_test &lt;- iris[test_idx, -1]

rf &lt;- forestry(x = x_train, y = y_train, nthread = 2)
predict(rf, x_test)

set.seed(49)
library(Rforestry)

n &lt;- c(100)
a &lt;- rnorm(n)
b &lt;- rnorm(n)
c &lt;- rnorm(n)
y &lt;- 4*a + 5.5*b - .78*c
x &lt;- data.frame(a,b,c)

forest &lt;- forestry(
          x,
          y,
          ntree = 10,
          replace = TRUE,
          nodesizeStrictSpl = 5,
          nodesizeStrictAvg = 5,
          nthread = 2,
          linear = TRUE
          )

predict(forest, x)
</code></pre>

<hr>
<h2 id='forestry-class'>forestry class</h2><span id='topic+forestry-class'></span>

<h3>Description</h3>

<p>'honestRF' class only exists for backwards compatibility reasons
</p>

<hr>
<h2 id='getOOB-forestry'>getOOB-forestry</h2><span id='topic+getOOB-forestry'></span><span id='topic+getOOB'></span><span id='topic+getOOB+2Cforestry-method'></span>

<h3>Description</h3>

<p>Calculate the out-of-bag error of a given forest. This is done
by using the out-of-bag predictions for each observation, and calculating the
MSE over the entire forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getOOB(object, noWarning)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getOOB-forestry_+3A_object">object</code></td>
<td>
<p>A 'forestry' object.</p>
</td></tr>
<tr><td><code id="getOOB-forestry_+3A_nowarning">noWarning</code></td>
<td>
<p>flag to not display warnings</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The OOB error of the forest.
</p>

<hr>
<h2 id='getOOBpreds-forestry'>getOOBpreds-forestry</h2><span id='topic+getOOBpreds-forestry'></span><span id='topic+getOOBpreds'></span>

<h3>Description</h3>

<p>Calculate the out-of-bag predictions of a given forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getOOBpreds(object, newdata = NULL, doubleOOB = FALSE, noWarning = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getOOBpreds-forestry_+3A_object">object</code></td>
<td>
<p>A trained model object of class &quot;forestry&quot;.</p>
</td></tr>
<tr><td><code id="getOOBpreds-forestry_+3A_newdata">newdata</code></td>
<td>
<p>A possible new data frame on which to run out of bag
predictions. If this is not NULL, we assume that the indices of
newdata are the same as the indices of the training set, and will use
these to find which trees the observation is considered in/out of bag for.</p>
</td></tr>
<tr><td><code id="getOOBpreds-forestry_+3A_doubleoob">doubleOOB</code></td>
<td>
<p>A flag specifying whether or not we should use the double OOB
set for the OOB predictions. This is the set of observations for each tree which
were in neither the averaging set nor the splitting set. Note that the forest
must have been trained with doubleBootstrap = TRUE for this to be used. Default
is FALSE.</p>
</td></tr>
<tr><td><code id="getOOBpreds-forestry_+3A_nowarning">noWarning</code></td>
<td>
<p>Flag to not display warnings.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The vector of all training observations, with their out of bag
predictions. Note each observation is out of bag for different trees, and so
the predictions will be more or less stable based on the observation. Some
observations may not be out of bag for any trees, and here the predictions
are returned as NA.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+forestry">forestry</a></code>
</p>

<hr>
<h2 id='getVI'>getVI-forestry</h2><span id='topic+getVI'></span>

<h3>Description</h3>

<p>Calculate the percentage increase in OOB error of the forest
when each feature is shuffled.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getVI(object, noWarning, metric = "mse", seed = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getVI_+3A_object">object</code></td>
<td>
<p>A 'forestry' object.</p>
</td></tr>
<tr><td><code id="getVI_+3A_nowarning">noWarning</code></td>
<td>
<p>flag to not display warnings</p>
</td></tr>
<tr><td><code id="getVI_+3A_metric">metric</code></td>
<td>
<p>A parameter to determine how the predictions of the forest with
a permuted variable are compared to the predictions of the standard forest.
Must be one of c(&quot;mse&quot;,&quot;auc&quot;,&quot;tnr&quot;), &quot;mse&quot; gives the percentage increase in
mse when the feature is permuted, &quot;auc&quot; gives the percentage decrease in AUC
when the feature is permuted, and &quot;tnr&quot; gives the percentage decrease in
TNR when the TPR is 99% when the feature is permuted.</p>
</td></tr>
<tr><td><code id="getVI_+3A_seed">seed</code></td>
<td>
<p>A parameter to seed the random number generator for shuffling
the features of X.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The variable importance of the forest.
</p>


<h3>Note</h3>

<p>Pass a seed to this function so it is
possible to replicate the vector permutations used when measuring feature importance.
</p>

<hr>
<h2 id='honestRF'>Honest Random Forest</h2><span id='topic+honestRF'></span>

<h3>Description</h3>

<p>This function is deprecated and only exists for backwards
backwards compatibility. The function you want to use is 'forestry'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>honestRF(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="honestRF_+3A_...">...</code></td>
<td>
<p>parameters which are passed directly to 'forestry'</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 'forestry' object
</p>

<hr>
<h2 id='impute_features'>Feature imputation using random forests neighborhoods</h2><span id='topic+impute_features'></span>

<h3>Description</h3>

<p>This function uses the neighborhoods implied by a random forest
to impute missing features. The neighbors of a data point are all the
training points assigned to the same leaf in at least one tree in the
forest. The weight of each neighbor is the fraction of trees in the forest
for which it was assigned to the same leaf. We impute a missing feature
for a point by computing the weighted average feature value, using
neighborhood weights, using all of the point's neighbors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>impute_features(
  object,
  newdata,
  seed = round(runif(1) * 10000),
  use_mean_imputation_fallback = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="impute_features_+3A_object">object</code></td>
<td>
<p>an object of class 'forestry'</p>
</td></tr>
<tr><td><code id="impute_features_+3A_newdata">newdata</code></td>
<td>
<p>the feature data.frame we will impute missing features for.</p>
</td></tr>
<tr><td><code id="impute_features_+3A_seed">seed</code></td>
<td>
<p>a random seed passed to the predict method of forestry</p>
</td></tr>
<tr><td><code id="impute_features_+3A_use_mean_imputation_fallback">use_mean_imputation_fallback</code></td>
<td>
<p>if TRUE, mean imputation (for numeric
variables) and mode imputation (for factor variables) is used for missing
features for which all neighbors also had the corresponding feature
missing; if FALSE these missing features remain NAs in the data frame
returned by 'impute_features'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame that is newdata with imputed missing values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>iris_with_missing &lt;- iris
idx_miss_factor &lt;- sample(nrow(iris), 25, replace = TRUE)
iris_with_missing[idx_miss_factor, 5] &lt;- NA
idx_miss_numeric &lt;- sample(nrow(iris), 25, replace = TRUE)
iris_with_missing[idx_miss_numeric, 3] &lt;- NA

x &lt;- iris_with_missing[,-1]
y &lt;- iris_with_missing[, 1]

forest &lt;- forestry(x, y, ntree = 500, seed = 2,nthread = 2)
imputed_x &lt;- impute_features(forest, x, seed = 2)
</code></pre>

<hr>
<h2 id='loadForestry'>load RF</h2><span id='topic+loadForestry'></span>

<h3>Description</h3>

<p>This wrapper function checks the forestry object, makes it
saveable if needed, and then saves it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loadForestry(filename)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loadForestry_+3A_filename">filename</code></td>
<td>
<p>a filename in which to store the 'forestry' object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The loaded forest from filename.
</p>

<hr>
<h2 id='make_savable'>make_savable</h2><span id='topic+make_savable'></span><span id='topic+make_savable+2Cforestry-method'></span>

<h3>Description</h3>

<p>When a 'foresty' object is saved and then reloaded the Cpp
pointers for the data set and the Cpp forest have to be reconstructed
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_savable(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_savable_+3A_object">object</code></td>
<td>
<p>an object of class 'forestry'</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of lists. Each sublist contains the information to span a
tree.
</p>


<h3>Note</h3>

<p>'make_savable' does not translate all of the private member variables
of the C++ forestry object so when the forest is reconstructed with
'relinkCPP_prt' some attributes are lost. For example, 'nthreads' will be
reset to zero. This makes it impossible to disable threading when
predicting for forests loaded from disk.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(323652639)
x &lt;- iris[, -1]
y &lt;- iris[, 1]
forest &lt;- forestry(x, y, ntree = 3, nthread = 2)
y_pred_before &lt;- predict(forest, x)

forest &lt;- make_savable(forest)

wd &lt;- tempdir()
saveForestry(forest, filename = file.path(wd, "forest.Rda"))
rm(forest)

forest &lt;- loadForestry(file.path(wd, "forest.Rda"))

y_pred_after &lt;- predict(forest, x)

file.remove(file.path(wd, "forest.Rda"))
</code></pre>

<hr>
<h2 id='plot-forestry'>visualize a tree</h2><span id='topic+plot-forestry'></span><span id='topic+plot.forestry'></span>

<h3>Description</h3>

<p>plots a tree in the forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'forestry'
plot(x, tree.id = 1, print.meta_dta = FALSE, beta.char.len = 30, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot-forestry_+3A_x">x</code></td>
<td>
<p>A forestry x.</p>
</td></tr>
<tr><td><code id="plot-forestry_+3A_tree.id">tree.id</code></td>
<td>
<p>Specifies the tree number that should be visualized.</p>
</td></tr>
<tr><td><code id="plot-forestry_+3A_print.meta_dta">print.meta_dta</code></td>
<td>
<p>A flag indicating whether the data for the plot should be printed.</p>
</td></tr>
<tr><td><code id="plot-forestry_+3A_beta.char.len">beta.char.len</code></td>
<td>
<p>The length of the beta values in leaf node
representation. This is only required when plotting a forestry object with linear
aggregation functions (linear = TRUE).</p>
</td></tr>
<tr><td><code id="plot-forestry_+3A_...">...</code></td>
<td>
<p>additional arguments that are not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>plot
</p>


<h3>Value</h3>

<p>A plot of the specified tree in the forest.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(292315)
rf &lt;- forestry(x = iris[,-1],
               y = iris[, 1],
               nthread = 2)

plot(x = rf)
plot(x = rf, tree.id = 2)
plot(x = rf, tree.id = 500)


</code></pre>

<hr>
<h2 id='predict-forestry'>predict-forestry</h2><span id='topic+predict-forestry'></span><span id='topic+predict.forestry'></span>

<h3>Description</h3>

<p>Return the prediction from the forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'forestry'
predict(
  object,
  newdata = NULL,
  aggregation = "average",
  holdOutIdx = NULL,
  trainingIdx = NULL,
  seed = as.integer(runif(1) * 10000),
  nthread = 0,
  exact = NULL,
  trees = NULL,
  weightMatrix = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict-forestry_+3A_object">object</code></td>
<td>
<p>A 'forestry' object.</p>
</td></tr>
<tr><td><code id="predict-forestry_+3A_newdata">newdata</code></td>
<td>
<p>A data frame of testing predictors.</p>
</td></tr>
<tr><td><code id="predict-forestry_+3A_aggregation">aggregation</code></td>
<td>
<p>How the individual tree predictions are aggregated:
'average' returns the mean of all trees in the forest; 'terminalNodes' also returns
the weightMatrix, as well as &quot;terminalNodes&quot;, a matrix where
the ith entry of the jth column is the index of the leaf node to which the
ith observation is assigned in the jth tree; and &quot;sparse&quot;, a matrix
where the ith entry in the jth column is 1 if the ith observation in
newdata is assigned to the jth leaf and 0 otherwise. In each tree the
leaves are indexed using a depth first ordering, and, in the &quot;sparse&quot;
representation, the first leaf in the second tree has column index one more than
the number of leaves in the first tree and so on. So, for example, if the
first tree has 5 leaves, the sixth column of the &quot;sparse&quot; matrix corresponds
to the first leaf in the second tree.
'oob' returns the out-of-bag predictions for the forest. We assume
that the ordering of the observations in newdata have not changed from
training. If the ordering has changed, we will get the wrong OOB indices.
'doubleOOB' is an experimental flag, which can only be used when OOBhonest = TRUE
and doubleBootstrap = TRUE. When both of these settings are on, the
splitting set is selected as a bootstrap sample of observations and the
averaging set is selected as a bootstrap sample of the observations which
were left out of bag during the splitting set selection. This leaves a third
set which is the observations which were not selected in either bootstrap sample.
This predict flag gives the predictions using- for each observation- only the trees
in which the observation fell into this third set (so was neither a splitting
nor averaging example).
'coefs' is an aggregation option which works only when linear aggregation
functions have been used. This returns the linear coefficients for each
linear feature which were used in the leaf node regression of each predicted
point.</p>
</td></tr>
<tr><td><code id="predict-forestry_+3A_holdoutidx">holdOutIdx</code></td>
<td>
<p>This is an optional argument, containing a vector of indices
from the training data set that should be not be allowed to influence the
predictions of the forest. When a vector of indices of training observations are
given, the predictions will be made only with trees in the forest that
do not contain any of these indices in either the splitting or averaging sets.
This cannot be used at the same time as any other aggregation options.
If 'weightMatrix = TRUE', this will return the
weightMatrix corresponding to the predictions made with trees respecting
holdOutIdx. If there are no trees that have held out all of the indices
in holdOutIdx, then the predictions will return NaN.</p>
</td></tr>
<tr><td><code id="predict-forestry_+3A_trainingidx">trainingIdx</code></td>
<td>
<p>This is an optional parameter to give the indices of the observations
in 'newdata' from the training data set. This is used when we want to run predict on
only a subset of observations from the training data set and use 'aggregation = &quot;oob&quot;' or
'aggregation = &quot;doubleOOB&quot;'. For example, at the tree level, a tree make out of
bag ('aggregation = &quot;oob&quot;') predictions for the indices in the set
setdiff(trainingIdx,tree$averagingIndices) and will make double out-of-bag
predictions for the indices in the set
setdiff(trainingIdx,union(tree$averagingIndices,tree$splittingIndices).
Note, this parameter must be set when predict is called with an out-of-bag
aggregation option on a data set not matching the original training data size.
The order of indices in 'trainingIdx' also needs to match the order of observations
in newdata. So for an arbitrary index set 'trainingIdx' and dataframe 'newdata',
of the same size as the training set, the predictions from 'predict(rf, newdata[trainingIdx,],'
'aggregation = &quot;oob&quot;, trainingIdx = trainingIdx)' should match the
predictions of to 'predict(rf, newdata, aggregation = &quot;oob&quot;)[trainingIdx]'.
This option also works with the 'weightMatrix' option and will return the
(smaller) weightMatrix for the observations in the passed data frame.</p>
</td></tr>
<tr><td><code id="predict-forestry_+3A_seed">seed</code></td>
<td>
<p>random seed</p>
</td></tr>
<tr><td><code id="predict-forestry_+3A_nthread">nthread</code></td>
<td>
<p>The number of threads with which to run the predictions with.
This will default to the number of threads with which the forest was trained
with.</p>
</td></tr>
<tr><td><code id="predict-forestry_+3A_exact">exact</code></td>
<td>
<p>This specifies whether the forest predictions should be aggregated
in a reproducible ordering. Due to the non-associativity of floating point
addition, when we predict in parallel, predictions will be aggregated in
varied orders as different threads finish at different times.
By default, exact is TRUE unless N &gt; 100,000 or a custom aggregation
function is used.</p>
</td></tr>
<tr><td><code id="predict-forestry_+3A_trees">trees</code></td>
<td>
<p>A vector (1-indexed) of indices in the range 1:ntree which tells
predict which trees in the forest to use for the prediction. Predict will by
default take the average of all trees in the forest, although this flag
can be used to get single tree predictions, or averages of diffferent trees
with different weightings. Duplicate entries are allowed, so if trees = c(1,2,2)
this will predict the weighted average prediction of only trees 1 and 2 weighted by:
predict(..., trees = c(1,2,2)) = (predict(..., trees = c(1)) +
2*predict(..., trees = c(2))) / 3.
note we must have exact = TRUE, and aggregation = &quot;average&quot; to use tree indices.</p>
</td></tr>
<tr><td><code id="predict-forestry_+3A_weightmatrix">weightMatrix</code></td>
<td>
<p>An indicator of whether or not we should also return a
matrix of the weights given to each training observation when making each
prediction. When getting the weight matrix, aggregation must be one of
'average', 'oob', and 'doubleOOB'.</p>
</td></tr>
<tr><td><code id="predict-forestry_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predicted responses.
</p>

<hr>
<h2 id='predictInfo'>predictInfo-forestry</h2><span id='topic+predictInfo'></span>

<h3>Description</h3>

<p>Get the observations which are used to predict for a set of new
observations using either all trees (for out of sample observations), or
tree for which the observation is out of averaging set or out of sample entirely.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predictInfo(object, newdata, aggregation = "average")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predictInfo_+3A_object">object</code></td>
<td>
<p>A 'forestry' object.</p>
</td></tr>
<tr><td><code id="predictInfo_+3A_newdata">newdata</code></td>
<td>
<p>Data on which we want to do predictions. Must be the same length
as the training set if we are doing 'oob' or 'doubleOOB' aggregation.</p>
</td></tr>
<tr><td><code id="predictInfo_+3A_aggregation">aggregation</code></td>
<td>
<p>Specifies which aggregation version is used to predict for the
observation, must be one of 'average','oob', and 'doubleOOB'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with four entries. 'weightMatrix' is a matrix specifying the
weight given to training observatio i when prediction on observation j.
'avgIndices' gives the indices which are in the averaging set for each new
observation. 'avgWeights' gives the weights corresponding to each averaging
observation returned in 'avgIndices'. 'obsInfo' gives the full observation vectors
which were used to predict for an observation, as well as the weight given
each observation.
</p>

<hr>
<h2 id='preprocess_testing'>preprocess_testing</h2><span id='topic+preprocess_testing'></span>

<h3>Description</h3>

<p>Perform preprocessing for the testing data, including converting
data to dataframe, and testing if the columns are consistent with the
training data and encoding categorical data into numerical representation
in the same way as training data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>preprocess_testing(x, categoricalFeatureCols, categoricalFeatureMapping)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="preprocess_testing_+3A_x">x</code></td>
<td>
<p>A data frame of all training predictors.</p>
</td></tr>
<tr><td><code id="preprocess_testing_+3A_categoricalfeaturecols">categoricalFeatureCols</code></td>
<td>
<p>A list of index for all categorical data. Used
for trees to detect categorical columns.</p>
</td></tr>
<tr><td><code id="preprocess_testing_+3A_categoricalfeaturemapping">categoricalFeatureMapping</code></td>
<td>
<p>A list of encoding details for each
categorical column, including all unique factor values and their
corresponding numeric representation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A preprocessed training dataaset x
</p>

<hr>
<h2 id='preprocess_training'>preprocess_training</h2><span id='topic+preprocess_training'></span>

<h3>Description</h3>

<p>Perform preprocessing for the training data, including
converting data to dataframe, and encoding categorical data into numerical
representation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>preprocess_training(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="preprocess_training_+3A_x">x</code></td>
<td>
<p>A data frame of all training predictors.</p>
</td></tr>
<tr><td><code id="preprocess_training_+3A_y">y</code></td>
<td>
<p>A vector of all training responses.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of two datasets along with necessary information that encodes
the preprocessing.
</p>

<hr>
<h2 id='relinkCPP_prt'>relink CPP ptr</h2><span id='topic+relinkCPP_prt'></span>

<h3>Description</h3>

<p>When a 'foresty' object is saved and then reloaded the Cpp
pointers for the data set and the Cpp forest have to be reconstructed
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relinkCPP_prt(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relinkCPP_prt_+3A_object">object</code></td>
<td>
<p>an object of class 'forestry'</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Relinks the pointer to the correct C++ object.
</p>

<hr>
<h2 id='saveForestry'>save RF</h2><span id='topic+saveForestry'></span>

<h3>Description</h3>

<p>This wrapper function checks the forestry object, makes it
saveable if needed, and then saves it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>saveForestry(object, filename, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="saveForestry_+3A_object">object</code></td>
<td>
<p>an object of class 'forestry'</p>
</td></tr>
<tr><td><code id="saveForestry_+3A_filename">filename</code></td>
<td>
<p>a filename in which to store the 'forestry' object</p>
</td></tr>
<tr><td><code id="saveForestry_+3A_...">...</code></td>
<td>
<p>additional arguments useful for specifying compression type and level</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Saves the forest into filename.
</p>

<hr>
<h2 id='scale_center'>scale_center</h2><span id='topic+scale_center'></span>

<h3>Description</h3>

<p>Given a dataframe, scale and center the continous features
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scale_center(x, categoricalFeatureCols, colMeans, colSd)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scale_center_+3A_x">x</code></td>
<td>
<p>A dataframe in order to be processed.</p>
</td></tr>
<tr><td><code id="scale_center_+3A_categoricalfeaturecols">categoricalFeatureCols</code></td>
<td>
<p>A vector of the categorical features, we
don't want to scale/center these. Should be 1-indexed.</p>
</td></tr>
<tr><td><code id="scale_center_+3A_colmeans">colMeans</code></td>
<td>
<p>A vector of the means to center each column.</p>
</td></tr>
<tr><td><code id="scale_center_+3A_colsd">colSd</code></td>
<td>
<p>A vector of the standard deviations to scale each column with.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scaled and centered  dataset x
</p>

<hr>
<h2 id='testing_data_checker-forestry'>Test data check</h2><span id='topic+testing_data_checker-forestry'></span><span id='topic+testing_data_checker'></span>

<h3>Description</h3>

<p>Check the testing data to do prediction
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testing_data_checker(object, newdata, hasNas)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="testing_data_checker-forestry_+3A_object">object</code></td>
<td>
<p>A forestry object.</p>
</td></tr>
<tr><td><code id="testing_data_checker-forestry_+3A_newdata">newdata</code></td>
<td>
<p>A data frame of testing predictors.</p>
</td></tr>
<tr><td><code id="testing_data_checker-forestry_+3A_hasnas">hasNas</code></td>
<td>
<p>TRUE if the there were NAs in the training data FALSE otherwise.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A feature dataframe if it can be used for new predictions.
</p>

<hr>
<h2 id='training_data_checker'>Training data check</h2><span id='topic+training_data_checker'></span>

<h3>Description</h3>

<p>Check the input to forestry constructor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>training_data_checker(
  x,
  y,
  ntree,
  replace,
  sampsize,
  mtry,
  nodesizeSpl,
  nodesizeAvg,
  nodesizeStrictSpl,
  nodesizeStrictAvg,
  minSplitGain,
  maxDepth,
  interactionDepth,
  splitratio,
  OOBhonest,
  doubleBootstrap,
  nthread,
  middleSplit,
  doubleTree,
  linFeats,
  monotonicConstraints,
  groups,
  featureWeights,
  deepFeatureWeights,
  observationWeights,
  customSplitSample,
  customAvgSample,
  customExcludeSample,
  linear,
  scale,
  hasNas,
  naDirection
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="training_data_checker_+3A_x">x</code></td>
<td>
<p>A data frame of all training predictors.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_y">y</code></td>
<td>
<p>A vector of all training responses.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_ntree">ntree</code></td>
<td>
<p>The number of trees to grow in the forest. The default value is
500.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_replace">replace</code></td>
<td>
<p>An indicator of whether sampling of training data is with
replacement. The default value is TRUE.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_sampsize">sampsize</code></td>
<td>
<p>The size of total samples to draw for the training data. If
sampling with replacement, the default value is the length of the training
data. If sampling without replacement, the default value is two-thirds of
the length of the training data.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_mtry">mtry</code></td>
<td>
<p>The number of variables randomly selected at each split point.
The default value is set to be one-third of the total number of features of the training data.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_nodesizespl">nodesizeSpl</code></td>
<td>
<p>Minimum observations contained in terminal nodes.
The default value is 5.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_nodesizeavg">nodesizeAvg</code></td>
<td>
<p>Minimum size of terminal nodes for averaging dataset.
The default value is 5.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_nodesizestrictspl">nodesizeStrictSpl</code></td>
<td>
<p>Minimum observations to follow strictly in terminal nodes.
The default value is 1.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_nodesizestrictavg">nodesizeStrictAvg</code></td>
<td>
<p>The minimum size of terminal nodes for averaging data set to follow when predicting.
No splits are allowed that result in nodes with observations less than this parameter.
This parameter enforces overlap of the averaging data set with the splitting set when training.
When using honesty, splits that leave less than nodesizeStrictAvg averaging
observations in either child node will be rejected, ensuring every leaf node
also has at least nodesizeStrictAvg averaging observations. The default value is 1.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_minsplitgain">minSplitGain</code></td>
<td>
<p>Minimum loss reduction to split a node further in a tree.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_maxdepth">maxDepth</code></td>
<td>
<p>Maximum depth of a tree. The default value is 99.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_interactiondepth">interactionDepth</code></td>
<td>
<p>All splits at or above interaction depth must be on
variables that are not weighting variables (as provided by the interactionVariables argument).</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_splitratio">splitratio</code></td>
<td>
<p>Proportion of the training data used as the splitting dataset.
It is a ratio between 0 and 1. If the ratio is 1 (the default), then the splitting
set uses the entire data, as does the averaging set&mdash;i.e., the standard Breiman RF setup.
If the ratio is 0, then the splitting data set is empty, and the entire dataset is used
for the averaging set (This is not a good usage, however, since there will be no data available for splitting).</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_oobhonest">OOBhonest</code></td>
<td>
<p>In this version of honesty, the out-of-bag observations for each tree
are used as the honest (averaging) set. This setting also changes how predictions
are constructed. When predicting for observations that are out-of-sample
(using predict(..., aggregation = &quot;average&quot;)), all the trees in the forest
are used to construct predictions. When predicting for an observation that was in-sample (using
predict(..., aggregation = &quot;oob&quot;)), only the trees for which that observation
was not in the averaging set are used to construct the prediction for that observation.
aggregation=&quot;oob&quot; (out-of-bag) ensures that the outcome value for an observation
is never used to construct predictions for a given observation even when it is in sample.
This property does not hold in standard honesty, which relies on an asymptotic
subsampling argument. By default, when OOBhonest = TRUE, the out-of-bag observations
for each tree are resamples with replacement to be used for the honest (averaging)
set. This results in a third set of observations that are left out of both
the splitting and averaging set, we call these the double out-of-bag (doubleOOB)
observations. In order to get the predictions of only the trees in which each
observation fell into this doubleOOB set, one can run predict(... , aggregation = &quot;doubleOOB&quot;).
In order to not do this second bootstrap sample, the doubleBootstrap flag can
be set to FALSE.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_doublebootstrap">doubleBootstrap</code></td>
<td>
<p>The doubleBootstrap flag provides the option to resample
with replacement from the out-of-bag observations set for each tree to construct
the averaging set when using OOBhonest. If this is FALSE, the out-of-bag observations
are used as the averaging set. By default this option is TRUE when running OOBhonest = TRUE.
This option increases diversity across trees.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_nthread">nthread</code></td>
<td>
<p>Number of threads to train and predict the forest. The default
number is 0 which represents using all cores.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_middlesplit">middleSplit</code></td>
<td>
<p>Indicator of whether the split value is takes the average of two feature
values. If FALSE, it will take a point based on a uniform distribution
between two feature values. (Default = FALSE)</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_doubletree">doubleTree</code></td>
<td>
<p>if the number of tree is doubled as averaging and splitting
data can be exchanged to create decorrelated trees. (Default = FALSE)</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_linfeats">linFeats</code></td>
<td>
<p>A vector containing the indices of which features to split
linearly on when using linear penalized splits (defaults to use all numerical features).</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_monotonicconstraints">monotonicConstraints</code></td>
<td>
<p>Specifies monotonic relationships between the continuous
features and the outcome. Supplied as a vector of length p with entries in
1,0,-1 which 1 indicating an increasing monotonic relationship, -1 indicating
a decreasing monotonic relationship, and 0 indicating no constraint.
Constraints supplied for categorical variable will be ignored.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_groups">groups</code></td>
<td>
<p>A vector of factors specifying the group membership of each training observation.
these groups are used in the aggregation when doing out of bag predictions in
order to predict with only trees where the entire group was not used for aggregation.
This allows the user to specify custom subgroups which will be used to create
predictions which do not use any data from a common group to make predictions for
any observation in the group. This can be used to create general custom
resampling schemes, and provide predictions consistent with the Out-of-Group set.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_featureweights">featureWeights</code></td>
<td>
<p>weights used when subsampling features for nodes above or at interactionDepth.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_deepfeatureweights">deepFeatureWeights</code></td>
<td>
<p>weights used when subsampling features for nodes below interactionDepth.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_observationweights">observationWeights</code></td>
<td>
<p>Denotes the weights for each training observation
that determine how likely the observation is to be selected in each bootstrap sample.
This option is not allowed when sampling is done without replacement.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_customsplitsample">customSplitSample</code></td>
<td>
<p>List of vectors for user-defined splitting observations per tree. The vector at
index i contains the indices of the sampled splitting observations, with replacement allowed, for tree i.
This feature overrides other sampling parameters and must be set in conjunction with customAvgSample.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_customavgsample">customAvgSample</code></td>
<td>
<p>List of vectors for user-defined averaging observations per tree. The vector at
index i contains the indices of the sampled splitting observations, with replacement allowed, for tree i.
This feature overrides other sampling parameters and must be set in conjunction with customSplitSample.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_customexcludesample">customExcludeSample</code></td>
<td>
<p>An optional list of vectors for user-defined excluded observations per tree. The vector at
index i contains the indices of the excluded observations for tree i. An observation is considered excluded if it does
not appear in the splitting or averaging set and has been explicitly withheld from being sampled for a tree.
Excluded observations are not considered out-of-bag, so when we call predict with aggregation = &quot;oob&quot;,
when we predict for an observation, we will only use the predictions of trees in which the
observation was in the customSplitSample (and neither in the customAvgSample nor the customExcludeSample).
This parameter is optional even when customSplitSample and customAvgSample are set.
It is also optional at the tree level, so can have fewer than ntree entries. When given fewer than
ntree entries, for example K, the entries will be applied to the first K trees in the forest and
the remaining trees will have no excludedSamples.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_linear">linear</code></td>
<td>
<p>Indicator that enables Ridge penalized splits and linear aggregation
functions in the leaf nodes. This is recommended for data with linear outcomes.
For implementation details, see: https://arxiv.org/abs/1906.06463. Default is FALSE.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_scale">scale</code></td>
<td>
<p>A parameter which indicates whether or not we want to scale and center
the covariates and outcome before doing the regression. This can help with
stability, so by default is TRUE.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_hasnas">hasNas</code></td>
<td>
<p>indicates if there is any missingness in x.</p>
</td></tr>
<tr><td><code id="training_data_checker_+3A_nadirection">naDirection</code></td>
<td>
<p>Sets a default direction for missing values in each split
node during training. It test placing all missing values to the left and
right, then selects the direction that minimizes loss. If no missing values
exist, then a default direction is randomly selected in proportion to the
distribution of observations on the left and right. (Default = FALSE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of parameters after checking the selected parameters are valid.
</p>

<hr>
<h2 id='unscale_uncenter'>unscale_uncenter</h2><span id='topic+unscale_uncenter'></span>

<h3>Description</h3>

<p>Given a dataframe, un scale and un center the continous features
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unscale_uncenter(x, categoricalFeatureCols, colMeans, colSd)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unscale_uncenter_+3A_x">x</code></td>
<td>
<p>A dataframe in order to be processed.</p>
</td></tr>
<tr><td><code id="unscale_uncenter_+3A_categoricalfeaturecols">categoricalFeatureCols</code></td>
<td>
<p>A vector of the categorical features, we
don't want to scale/center these. Should be 1-indexed.</p>
</td></tr>
<tr><td><code id="unscale_uncenter_+3A_colmeans">colMeans</code></td>
<td>
<p>A vector of the means to add to each column.</p>
</td></tr>
<tr><td><code id="unscale_uncenter_+3A_colsd">colSd</code></td>
<td>
<p>A vector of the standard deviations to rescale each column with.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset x in it's original scaling
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
