<!DOCTYPE html><html><head><title>Help for package FSinR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {FSinR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#antColony'><p>Ant Colony Optimization (Advanced Binary Ant Colony Optimization)</p></a></li>
<li><a href='#binaryConsistency'><p>Binary consistency measure</p></a></li>
<li><a href='#breadthFirst'><p>Breadth First Search (exhaustive search)</p></a></li>
<li><a href='#chiSquared'><p>Chi squared measure</p></a></li>
<li><a href='#cramer'><p>Cramer V measure</p></a></li>
<li><a href='#deepFirst'><p>Deep First Search (exhaustive search)</p></a></li>
<li><a href='#determinationCoefficient'><p>R Squared, to continous features</p></a></li>
<li><a href='#directFeatureSelection'><p>Direct Feature Selection Proccess</p></a></li>
<li><a href='#directSearchAlgorithm'><p>Direct search algorithm generator</p></a></li>
<li><a href='#featureSelection'><p>Feature Selection Proccess</p></a></li>
<li><a href='#filterEvaluator'><p>Filter measure generator</p></a></li>
<li><a href='#fscore'><p>F-score measure</p></a></li>
<li><a href='#gainRatio'><p>The gain ratio measure</p></a></li>
<li><a href='#geneticAlgorithm'><p>Genetic Algorithm</p></a></li>
<li><a href='#giniIndex'><p>Gini index measure</p></a></li>
<li><a href='#hillClimbing'><p>Hill-Climbing</p></a></li>
<li><a href='#hybridFeatureSelection'><p>Hybrid Feature Selection Proccess</p></a></li>
<li><a href='#hybridSearchAlgorithm'><p>Hybrid search algorithm generator</p></a></li>
<li><a href='#IEConsistency'><p>Inconsistent Examples consistency measure</p></a></li>
<li><a href='#IEPConsistency'><p>Inconsistent Examples Pairs consistency measure</p></a></li>
<li><a href='#isDataframeContinuous'><p>isDataframeContinuous(dataframe)</p></a></li>
<li><a href='#isDataframeDiscrete'><p>isDataFrameDiscrete(dataframe)</p></a></li>
<li><a href='#Jd'><p>Jd evaluation measure</p></a></li>
<li><a href='#LasVegas'><p>Las Vegas</p></a></li>
<li><a href='#LCC'><p>Linear Consistency-Constrained algorithm</p></a></li>
<li><a href='#MDLC'><p>MDLC evaluation measure</p></a></li>
<li><a href='#mutualInformation'><p>The mutual information measure</p></a></li>
<li><a href='#normalizedRelief'><p>Normalized Relief</p></a></li>
<li><a href='#normalizedReliefFeatureSetMeasure'><p>Relief Feature Set Measure evaluation measure</p></a></li>
<li><a href='#relief'><p>Relief</p></a></li>
<li><a href='#ReliefFeatureSetMeasure'><p>Relief Feature Set Measure evaluation measure</p></a></li>
<li><a href='#roughsetConsistency'><p>Rough Set consistency measure</p></a></li>
<li><a href='#searchAlgorithm'><p>Search algorithm generator</p></a></li>
<li><a href='#selectDifference'><p>Select difference</p></a></li>
<li><a href='#selectKBest'><p>Select K best</p></a></li>
<li><a href='#selectPercentile'><p>Select Percentile</p></a></li>
<li><a href='#selectSlope'><p>Select slope</p></a></li>
<li><a href='#selectThreshold'><p>Select threshold</p></a></li>
<li><a href='#selectThresholdRange'><p>Select threshold range</p></a></li>
<li><a href='#sequentialBackwardSelection'><p>Sequential Backward Selection</p></a></li>
<li><a href='#sequentialFloatingBackwardSelection'><p>Sequential Floating Backward Selection</p></a></li>
<li><a href='#sequentialFloatingForwardSelection'><p>Sequential Floating Forward Selection</p></a></li>
<li><a href='#sequentialForwardSelection'><p>Sequential Forward Selection</p></a></li>
<li><a href='#simulatedAnnealing'><p>Simulated Annealing</p></a></li>
<li><a href='#symmetricalUncertain'><p>Symmetrical uncertain measure</p></a></li>
<li><a href='#tabu'><p>Tabu Search</p></a></li>
<li><a href='#whaleOptimization'><p>Whale Optimization Algorithm (Binary Whale Optimization Algorithm)</p></a></li>
<li><a href='#wrapperEvaluator'><p>Wrapper measure generator</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Maintainer:</td>
<td>Alfonso Jiménez-Vílchez &lt;i52jivia@uco.es&gt;</td>
</tr>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Feature Selection</td>
</tr>
<tr>
<td>Description:</td>
<td>Feature subset selection algorithms modularized in search algorithms and measure utilities. Full list and more information available at <a href="https://dicits.ugr.es/software/FSinR/">https://dicits.ugr.es/software/FSinR/</a>.</td>
</tr>
<tr>
<td>Version:</td>
<td>2.0.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-11-16</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>false</td>
</tr>
<tr>
<td>Imports:</td>
<td>rpart, neuralnet, class, digest, caret, mlbench, Rdpack, GA,
dplyr, tidyr, prodlim, rlang, purrr, e1071</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown, RSNNS</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-11-16 14:05:48 UTC; alfonso</td>
</tr>
<tr>
<td>Author:</td>
<td>Alfonso Jiménez-Vílchez [aut, cre],
  Francisco Aragón-Royón [aut],
  Adan M. Rodriguez [aut],
  Antonio Arauzo-Azofra [aut],
  José Manuel Benítez [aut]</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-11-23 10:50:26 UTC</td>
</tr>
</table>
<hr>
<h2 id='antColony'>Ant Colony Optimization (Advanced Binary Ant Colony Optimization)</h2><span id='topic+antColony'></span>

<h3>Description</h3>

<p>Generates a search function based on the ant colony optimization. This function is called internally within the <code><a href="#topic+searchAlgorithm">searchAlgorithm</a></code> function. The Ant Colony Optimization (Advanced Binary Ant Colony Optimization) (Kashef and Nezamabadi-pour 2015) algorithm consists of generating in each iteration a random population of individuals (ants) according to the values of a pheromone matrix (which is updated each iteration according to the paths most followed by the ants) and a heuristic (which determines how good is each path to follow by the ants). The evaluation measure is calculated for each individual. The algorithm ends once the established number of iterations has been reached
</p>


<h3>Usage</h3>

<pre><code class='language-R'>antColony(
  population = 10,
  iter = 10,
  a = 1,
  b = 1,
  p = 0.2,
  q = 1,
  t0 = 0.2,
  tmin = 0,
  tmax = 1,
  mode = 1,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="antColony_+3A_population">population</code></td>
<td>
<p>The number of ants population</p>
</td></tr>
<tr><td><code id="antColony_+3A_iter">iter</code></td>
<td>
<p>The number of iterations</p>
</td></tr>
<tr><td><code id="antColony_+3A_a">a</code></td>
<td>
<p>Parameter to control the influence of the pheromone (If a=0, no pheromone information is used)</p>
</td></tr>
<tr><td><code id="antColony_+3A_b">b</code></td>
<td>
<p>Parameter to control the influence of the heuristic (If b=0, the attractiveness of the movements is not taken into account)</p>
</td></tr>
<tr><td><code id="antColony_+3A_p">p</code></td>
<td>
<p>Rate of pheromone evaporation</p>
</td></tr>
<tr><td><code id="antColony_+3A_q">q</code></td>
<td>
<p>Constant to determine the amount of pheromone deposited by the best ant. This amount is determined by the Q/F equation (for minimization) where F is the cost of the solution (F/Q for maximization)</p>
</td></tr>
<tr><td><code id="antColony_+3A_t0">t0</code></td>
<td>
<p>Initial pheromone level</p>
</td></tr>
<tr><td><code id="antColony_+3A_tmin">tmin</code></td>
<td>
<p>Minimum pheromone value</p>
</td></tr>
<tr><td><code id="antColony_+3A_tmax">tmax</code></td>
<td>
<p>Maximum pheromone value</p>
</td></tr>
<tr><td><code id="antColony_+3A_mode">mode</code></td>
<td>
<p>Heuristic information measurement. 1 -&gt; min redundancy (by default). 2-&gt; max-relevance and min-redundancy. 3-&gt; feature-feature. 4-&gt; based on F-score</p>
</td></tr>
<tr><td><code id="antColony_+3A_verbose">verbose</code></td>
<td>
<p>Print the partial results in each iteration</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a search function that is used to guide the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>Kashef S, Nezamabadi-pour H (2015).
&ldquo;An advanced ACO algorithm for feature subset selection.&rdquo;
<em>Neurocomputing</em>, <b>147</b>, 271&ndash;279.
doi: <a href="https://doi.org/10.1016/j.neucom.2014.06.067">10.1016/j.neucom.2014.06.067</a>, Advances in Self-Organizing Maps Subtitle of the special issue: Selected Papers from the Workshop on Self-Organizing Maps 2012 (WSOM 2012), <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231214008601">https://www.sciencedirect.com/science/article/abs/pii/S0925231214008601</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a search process in a feature space
## Classification problem

# Generates the filter evaluation function with ACO
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the search function
aco_search &lt;- antColony()
# Performs the search process directly (parameters: dataset, target variable and evaluator)
aco_search(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='binaryConsistency'>Binary consistency measure</h2><span id='topic+binaryConsistency'></span>

<h3>Description</h3>

<p>Generates an evaluation function that calculates the binary consistency, also known as &quot;Sufficiency test&quot; from FOCUS (Almuallim and Dietterich 1991) (set measure). This function is called internally within the <code><a href="#topic+filterEvaluator">filterEvaluator</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>binaryConsistency()
</code></pre>


<h3>Value</h3>

<p>Returns a function that is used to generate an evaluation set measure using the binary consistency value for the selected features.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>


<h3>References</h3>

<p>Almuallim H, Dietterich TG (1991).
&ldquo;Learning With Many Irrelevant Features.&rdquo;
In <em>In Proceedings of the Ninth National Conference on Artificial Intelligence</em>, 547&ndash;552.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly to evaluate a set of features
## Classification problem

# A discrete dataset is used (in this case we use only several discrete columns)
adult &lt;- adult[,c(4,9,10,15)]

# Generate the evaluation function with Binary Consistency
bc_evaluator &lt;- binaryConsistency()
# Evaluate the features (parameters: dataset, target variable and features)
bc_evaluator(adult,'income',c('race','sex','education'))

## End(Not run)
</code></pre>

<hr>
<h2 id='breadthFirst'>Breadth First Search (exhaustive search)</h2><span id='topic+breadthFirst'></span>

<h3>Description</h3>

<p>Generates a search function based on the breadth first search. This function is called internally within the <code><a href="#topic+searchAlgorithm">searchAlgorithm</a></code> function. Breadth First Search searches the whole features subset in breadth first order (Kozen 1992).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>breadthFirst()
</code></pre>


<h3>Value</h3>

<p>Returns a search function that is used to guide the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>
<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>Kozen DC (1992).
<em>Depth-First and Breadth-First Search</em>.
Springer New York, New York, NY.
ISBN 978-1-4612-4400-4, doi: <a href="https://doi.org/10.1007/978-1-4612-4400-4_4">10.1007/978-1-4612-4400-4_4</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a search process in a feature space
## Classification problem

# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the search function with Breadth first
bfs_search &lt;- breadthFirst()
# Performs the search process directly (parameters: dataset, target variable and evaluator)
bfs_search(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='chiSquared'>Chi squared measure</h2><span id='topic+chiSquared'></span>

<h3>Description</h3>

<p>Generates an evaluation function that calculates the Chi squared value (F.R.S. 1900), evaluating the selected features individually (individual measure). This function is called internally within the <code><a href="#topic+filterEvaluator">filterEvaluator</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chiSquared()
</code></pre>


<h3>Value</h3>

<p>Returns a function that is used to generate an individual evaluation measure using chi squared.
</p>


<h3>References</h3>

<p>F.R.S. KP (1900).
&ldquo;X. On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling.&rdquo;
<em>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</em>, <b>50</b>, 157&ndash;175.
doi: <a href="https://doi.org/10.1080/14786440009463897">10.1080/14786440009463897</a>, <a href="https://doi.org/10.1080/14786440009463897">https://doi.org/10.1080/14786440009463897</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly to individually evaluate a set of features
## Classification problem

# Generate the evaluation function with Chi squared
chiSquared_evaluator &lt;- chiSquared()
# Evaluate the features (parameters: dataset, target variable and features)
chiSquared_evaluator(iris,'Species',c('Sepal.Length'))

## End(Not run)
</code></pre>

<hr>
<h2 id='cramer'>Cramer V measure</h2><span id='topic+cramer'></span>

<h3>Description</h3>

<p>Generates an evaluation function that calculates Cramer's V value (Cramer  1946 ), evaluating features individually (individual measure). This function is called internally within the <code><a href="#topic+filterEvaluator">filterEvaluator</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cramer()
</code></pre>


<h3>Value</h3>

<p>Returns a function that is used to generate an individual evaluation measure using Cramer V.
</p>


<h3>References</h3>

<p>Cramer H ( 1946 ).
<em> Mathematical methods of statistics / by Harald Cramer </em>.
 Princeton University Press Princeton .
ISBN ISBN 0-691-08004-6.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly to individually evaluate a set of features
## Classification problem

# Generate the evaluation function with Cramer
cramer_evaluator &lt;- cramer()
# Evaluate the features (parameters: dataset, target variable and features)
cramer_evaluator(iris,'Species',c('Sepal.Length'))

## End(Not run)
</code></pre>

<hr>
<h2 id='deepFirst'>Deep First Search (exhaustive search)</h2><span id='topic+deepFirst'></span>

<h3>Description</h3>

<p>Generates a search function based on the deep first search. This function is called internally within the <code><a href="#topic+searchAlgorithm">searchAlgorithm</a></code> function. Deep First Search searches the whole features subset in deep first order (Kozen 1992).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deepFirst()
</code></pre>


<h3>Value</h3>

<p>Returns a search function that is used to guide the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>Kozen DC (1992).
<em>Depth-First and Breadth-First Search</em>.
Springer New York, New York, NY.
ISBN 978-1-4612-4400-4, doi: <a href="https://doi.org/10.1007/978-1-4612-4400-4_4">10.1007/978-1-4612-4400-4_4</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a search process in a feature space
## Classification problem

# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the search function with Deep first
dfs_search &lt;- deepFirst()
# Performs the search process directly (parameters: dataset, target variable and evaluator)
dfs_search(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='determinationCoefficient'>R Squared, to continous features</h2><span id='topic+determinationCoefficient'></span>

<h3>Description</h3>

<p>Generates an evaluation function that calculates the determinantion coefficient (Dodge 2008) of continuous features (set measure). This function is called internally within the <code><a href="#topic+filterEvaluator">filterEvaluator</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>determinationCoefficient()
</code></pre>


<h3>Value</h3>

<p>Returns a function that is used to generate an evaluation set measure using the R squared value for the selected features.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>


<h3>References</h3>

<p>Dodge Y (2008).
<em>Coefficient of Determination</em>.
Springer New York, New York, NY.
ISBN 978-0-387-32833-1, doi: <a href="https://doi.org/10.1007/978-0-387-32833-1_62">10.1007/978-0-387-32833-1_62</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly to evaluate a set of features
## Classification problem

# Generate the evaluation function with Determination Coefficient
dc_evaluator &lt;- determinationCoefficient()
# Evaluate the features (parameters: dataset, target variable and features)
dc_evaluator(longley, 'Employed', c('GNP', 'Population','Year'))

## End(Not run)
</code></pre>

<hr>
<h2 id='directFeatureSelection'>Direct Feature Selection Proccess</h2><span id='topic+directFeatureSelection'></span>

<h3>Description</h3>

<p>Performs the direct feature selection process. Given a direct search algorithm and an evaluation method, it uses the direct search algorithm in combination with the evaluation results to guide the feature selection process to an optimal subset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>directFeatureSelection(data, class, directSearcher, evaluator)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="directFeatureSelection_+3A_data">data</code></td>
<td>
<p>A data.frame with the input dataset where the examples are in the rows and the features and the target variable are in the columns. The dataset should be discrete (feature columns are expected to be factors) if the following filter methods are used as evaluation methods: Rough Set Consistency, Binary Consistency, IE Consistency, IEP Consistency, Mutual Information, Gain Ratio, Symmetrical Uncertain, Gini Index or MDLC. The Jd and F-Score filter methods only work on classification problems with 2 classes in the target variable.</p>
</td></tr>
<tr><td><code id="directFeatureSelection_+3A_class">class</code></td>
<td>
<p>The name of the dependent variable</p>
</td></tr>
<tr><td><code id="directFeatureSelection_+3A_directsearcher">directSearcher</code></td>
<td>
<p>The algorithm to conduct the direct feature search. See <code><a href="#topic+directSearchAlgorithm">directSearchAlgorithm</a></code>.</p>
</td></tr>
<tr><td><code id="directFeatureSelection_+3A_evaluator">evaluator</code></td>
<td>
<p>The evaluation method to obtain a measure of the features. The evaluation method can be a filter (see <code><a href="#topic+filterEvaluator">filterEvaluator</a></code>) or a wrapper method (see <code><a href="#topic+wrapperEvaluator">wrapperEvaluator</a></code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list is returned with the results of the direct feature selection process:
</p>

<dl>
<dt>bestFeatures</dt><dd><p>A vector with all features. Selected features are marked with 1, unselected features are marked with 0.</p>
</dd>
<dt>featuresSelected</dt><dd><p>The names of the returned features sorted according to the result of the evaluation measure</p>
</dd>
<dt>valuePerFeature</dt><dd><p>The evaluation measures of the returned features</p>
</dd>
<dt>evaluationType</dt><dd><p>Type of evaluation based on how the features have been evaluated.</p>
</dd>
<dt>evaluationMethod</dt><dd><p>Evaluation method used.</p>
</dd>
<dt>searchMethod</dt><dd><p>Search method used during the feature selection process.</p>
</dd>
<dt>target</dt><dd><p>A character indicating if the objective of the process is to minimize or maximize the evaluation measure.</p>
</dd>
<dt>numFeatures</dt><dd><p>Number of features in the problem.</p>
</dd>
<dt>xNames</dt><dd><p>Name of the features.</p>
</dd>
<dt>yNames</dt><dd><p>Name of the dependent variable.</p>
</dd>
<dt>time</dt><dd><p>Value of class 'proc_time' containing the user time, system time, and total time of the feature selection process.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>There are no references for Rd macro <code style="white-space: pre;">&#8288;\insertAllCites&#8288;</code> on this help page.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## Examples of the direct feature selection process
## Classification problem with filter

# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('ReliefFeatureSetMeasure')
# Generates the direct search function
direct_search_method &lt;- directSearchAlgorithm('selectKBest')
# Runs the direct feature selection process
res &lt;- directFeatureSelection(iris, 'Species', direct_search_method, filter_evaluator)


## Classification problem with wrapper

# Generates the wraper evaluation function
wrapper_evaluator &lt;- wrapperEvaluator('knn')
# Generates the direct search function
direct_search_method &lt;- directSearchAlgorithm('selectKBest')
# Runs the direct feature selection process
res &lt;- directFeatureSelection(iris, 'Species', direct_search_method, wrapper_evaluator)


## Examples of the direct feature selection process (with parameters)
## Regression problem with filter

# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('relief', list(neighbours.count = 4))
# Generates the direct search function
direct_search_method &lt;- directSearchAlgorithm('selectKBest', list(k=2))
# Runs the direct feature selection process
res &lt;- directFeatureSelection(mtcars, 'mpg', direct_search_method, filter_evaluator)


## Regression problem with wrapper

# Values for the caret trainControl function (resampling parameters)
resamplingParams &lt;- list(method = "cv", repeats = 5)
# Values for the caret train function (fitting parameters)
fittingParams &lt;- list(preProc = c("center", "scale"), metric="RMSE",
                      tuneGrid = expand.grid(k = c(1:12)))
# Generates the wraper evaluation function
wrapper_evaluator &lt;- wrapperEvaluator('knn', resamplingParams, fittingParams)
# Generates the direct search function
direct_search_method &lt;- directSearchAlgorithm('selectKBest',list(k=2))
# Runs the direct feature selection process
res &lt;- directFeatureSelection(mtcars, 'mpg', direct_search_method, wrapper_evaluator)

## End(Not run)

</code></pre>

<hr>
<h2 id='directSearchAlgorithm'>Direct search algorithm generator</h2><span id='topic+directSearchAlgorithm'></span>

<h3>Description</h3>

<p>Generates a direct search function. This function in combination with the evaluator composes the feature selection process. Specifically, the result of calling this function is another function that is passed on as a parameter to the <code><a href="#topic+directFeatureSelection">directFeatureSelection</a></code> function. However, you can run this function directly to perform a direct search process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>directSearchAlgorithm(directSearcher, params = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="directSearchAlgorithm_+3A_directsearcher">directSearcher</code></td>
<td>
<p>Name of the direct search algorithm. The available direct search algorithms are:
</p>

<dl>
<dt>selectKBest</dt><dd><p>See <code><a href="#topic+selectKBest">selectKBest</a></code> </p>
</dd>
<dt>selectPercentile</dt><dd><p>See <code><a href="#topic+selectPercentile">selectPercentile</a></code> </p>
</dd>
<dt>selectThreshold</dt><dd><p>See <code><a href="#topic+selectThreshold">selectThreshold</a></code> </p>
</dd>
<dt>selectThresholdRange</dt><dd><p>See <code><a href="#topic+selectThresholdRange">selectThresholdRange</a></code> </p>
</dd>
<dt>selectDifference</dt><dd><p>See <code><a href="#topic+selectDifference">selectDifference</a></code> </p>
</dd>
<dt>selectSlope</dt><dd><p>See <code><a href="#topic+selectSlope">selectSlope</a></code> </p>
</dd>
</dl>
</td></tr>
<tr><td><code id="directSearchAlgorithm_+3A_params">params</code></td>
<td>
<p>List with the parameters of each direct search method. For more details see each method. Default: empty list.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a direct search function that is used in the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>There are no references for Rd macro <code style="white-space: pre;">&#8288;\insertAllCites&#8288;</code> on this help page.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## Examples of a direct search algorithm generation

direct_search_method_1 &lt;- directSearchAlgorithm('selectKBest')
direct_search_method_2 &lt;- directSearchAlgorithm('selectPercentile')
direct_search_method_3 &lt;- directSearchAlgorithm('selectThreshold')


## Examples of a direct search algorithm generation (with parameters)

direct_search_method_1 &lt;- directSearchAlgorithm('selectKBest', list(k=2))
direct_search_method_2 &lt;- directSearchAlgorithm('selectPercentile', list(percentile=25))
direct_search_method_3 &lt;- directSearchAlgorithm('selectThreshold', list(threshold=0.55))


## The direct application of this function is an advanced use that consists of using this 
# function directly to perform a direct search process
## Classification problem


# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the direct search function
direct_search_method &lt;- directSearchAlgorithm('selectKBest')
# Performs the diret search process directly (parameters: dataset, target variable and evaluator)
direct_search_method(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='featureSelection'>Feature Selection Proccess</h2><span id='topic+featureSelection'></span>

<h3>Description</h3>

<p>Performs the feature selection process. Given a search algorithm and an evaluation method, it uses the search algorithm in combination with the evaluation results to guide the feature selection process to an optimal subset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>featureSelection(data, class, searcher, evaluator)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="featureSelection_+3A_data">data</code></td>
<td>
<p>A data.frame with the input dataset where the examples are in the rows and the features and the target variable are in the columns. The dataset should be discrete (feature columns are expected to be factors) if the following filter methods are used as evaluation methods: Rough Set Consistency, Binary Consistency, IE Consistency, IEP Consistency, Mutual Information, Gain Ratio, Symmetrical Uncertain, Gini Index or MDLC. If Ant Colony Optimization is used as a search strategy, the dataset must be numerical since heuristics only work with continuous values. The Jd and F-Score filter methods only work on classification problems with 2 classes in the target variable.</p>
</td></tr>
<tr><td><code id="featureSelection_+3A_class">class</code></td>
<td>
<p>The name of the dependent variable</p>
</td></tr>
<tr><td><code id="featureSelection_+3A_searcher">searcher</code></td>
<td>
<p>The algorithm to guide the search in the feature space. See <code><a href="#topic+searchAlgorithm">searchAlgorithm</a></code>.</p>
</td></tr>
<tr><td><code id="featureSelection_+3A_evaluator">evaluator</code></td>
<td>
<p>The evaluation method to obtain a measure of the features. The evaluation method can be a filter (see <code><a href="#topic+filterEvaluator">filterEvaluator</a></code>) or a wrapper method (see <code><a href="#topic+wrapperEvaluator">wrapperEvaluator</a></code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list is returned with the results of the feature selection process:
</p>

<dl>
<dt>bestFeatures</dt><dd><p>A vector with all features. Selected features are marked with 1, unselected features are marked with 0.</p>
</dd>
<dt>bestValue</dt><dd><p>Evaluation measure obtained with the feature selection.</p>
</dd>
<dt>evaluationType</dt><dd><p>Type of evaluation based on how the features have been evaluated.</p>
</dd>
<dt>evaluationMethod</dt><dd><p>Evaluation method used.</p>
</dd>
<dt>measureType</dt><dd><p>Type of evaluation measure.</p>
</dd>
<dt>searchMethod</dt><dd><p>Search method used during the feature selection process.</p>
</dd>
<dt>target</dt><dd><p>A character indicating if the objective of the process is to minimize or maximize the evaluation measure.</p>
</dd>
<dt>numFeatures</dt><dd><p>Number of features in the problem.</p>
</dd>
<dt>xNames</dt><dd><p>Name of the features.</p>
</dd>
<dt>yNames</dt><dd><p>Name of the dependent variable.</p>
</dd>
<dt>time</dt><dd><p>Value of class 'proc_time' containing the user time, system time, and total time of the feature selection process.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>There are no references for Rd macro <code style="white-space: pre;">&#8288;\insertAllCites&#8288;</code> on this help page.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## Examples of the feature selection process
## Classification problem with filter

# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('ReliefFeatureSetMeasure')
# Generates the search function
search_method &lt;- searchAlgorithm('hillClimbing')
# Runs the feature selection process
res &lt;- featureSelection(iris, 'Species', search_method, filter_evaluator)


## Classification problem with wrapper

# Generates the wraper evaluation function
wrapper_evaluator &lt;- wrapperEvaluator('knn')
# Generates the search function
search_method &lt;- searchAlgorithm('hillClimbing')
# Runs the feature selection process
res &lt;- featureSelection(iris, 'Species', search_method, wrapper_evaluator)


## Examples of the feature selection process (with parameters)
## Regression problem with filter

# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('ReliefFeatureSetMeasure', list(iterations = 10))
# Generates the search function
search_method &lt;- searchAlgorithm('hillClimbing', list(repeats=2))
# Runs the feature selection process
res &lt;- featureSelection(mtcars, 'mpg', search_method, filter_evaluator)


## Regression problem with wrapper

# Values for the caret trainControl function (resampling parameters)
resamplingParams &lt;- list(method = "cv", repeats = 5)
# Values for the caret train function (fitting parameters)
fittingParams &lt;- list(preProc = c("center", "scale"), metric="RMSE",
                      tuneGrid = expand.grid(k = c(1:12)))
# Generates the wraper evaluation function
wrapper_evaluator &lt;- wrapperEvaluator('knn', resamplingParams, fittingParams)
# Generates the search function
search_method &lt;- searchAlgorithm('geneticAlgorithm',list(popSize=10, maxiter=25, verbose=TRUE))
# Runs the feature selection process
res &lt;- featureSelection(mtcars, 'mpg', search_method, wrapper_evaluator)

## End(Not run)

</code></pre>

<hr>
<h2 id='filterEvaluator'>Filter measure generator</h2><span id='topic+filterEvaluator'></span>

<h3>Description</h3>

<p>Generates a filter function to be used as an evaluator in the feature selection proccess. More specifically, the result of calling this function is another function that is passed on as a parameter to the <code><a href="#topic+featureSelection">featureSelection</a></code> function. However, you can also run this function directly to generate an evaluation measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filterEvaluator(filter, params = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="filterEvaluator_+3A_filter">filter</code></td>
<td>
<p>Name of the filter method. The available filter methods are:
</p>

<dl>
<dt>binaryConsistency</dt><dd><p>Binary consistency measure. See <code><a href="#topic+binaryConsistency">binaryConsistency</a></code> </p>
</dd>
<dt>chiSquared</dt><dd><p>Chi squared measure. See <code><a href="#topic+chiSquared">chiSquared</a></code> </p>
</dd>
<dt>cramer</dt><dd><p>Cramer V measure. See <code><a href="#topic+cramer">cramer</a></code> </p>
</dd>
<dt>determinationCoefficient</dt><dd><p>R Squared, to continous features. See <code><a href="#topic+determinationCoefficient">determinationCoefficient</a></code> </p>
</dd>
<dt>fscore</dt><dd><p>F-score measure. See <code><a href="#topic+fscore">fscore</a></code> </p>
</dd>
<dt>gainRatio</dt><dd><p>The gain ratio measure. See <code><a href="#topic+gainRatio">gainRatio</a></code> </p>
</dd>
<dt>giniIndex</dt><dd><p>Gini index measure. See <code><a href="#topic+giniIndex">giniIndex</a></code> </p>
</dd>
<dt>IEConsistency</dt><dd><p>Inconsistent Examples consistency measure. See <code><a href="#topic+IEConsistency">IEConsistency</a></code> </p>
</dd>
<dt>IEPConsistency</dt><dd><p>Inconsistent Examples Pairs consistency measure. See <code><a href="#topic+chiSquared">chiSquared</a></code> </p>
</dd>
<dt>Jd</dt><dd><p>Jd evaluation measure. See <code><a href="#topic+Jd">Jd</a></code> </p>
</dd>
<dt>MDLC</dt><dd><p>MDLC evaluation measure. See <code><a href="#topic+MDLC">MDLC</a></code> </p>
</dd>
<dt>mutualInformation</dt><dd><p>The mutual information measure. See <code><a href="#topic+mutualInformation">mutualInformation</a></code> </p>
</dd>
<dt>roughsetConsistency</dt><dd><p>Rough Set consistency measure. See <code><a href="#topic+roughsetConsistency">roughsetConsistency</a></code> </p>
</dd>
<dt>relief</dt><dd><p>Relief. See <code><a href="#topic+relief">relief</a></code> </p>
</dd>
<dt>ReliefFeatureSetMeasure</dt><dd><p>Relief Feature Set Measure evaluation measure. See <code><a href="#topic+ReliefFeatureSetMeasure">ReliefFeatureSetMeasure</a></code> </p>
</dd>
<dt>symmetricalUncertain</dt><dd><p>Symmetrical uncertain measure. See <code><a href="#topic+symmetricalUncertain">symmetricalUncertain</a></code> </p>
</dd>
</dl>
</td></tr>
<tr><td><code id="filterEvaluator_+3A_params">params</code></td>
<td>
<p>List with the parameters of each filter method. For more details see each method. Default: empty list.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a filter method that is used to generate an evaluation measure.
</p>


<h3>Author(s)</h3>

<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>There are no references for Rd macro <code style="white-space: pre;">&#8288;\insertAllCites&#8288;</code> on this help page.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## Examples of a filter evaluator generation

filter_evaluator_1 &lt;- filterEvaluator('cramer')
filter_evaluator_2 &lt;- filterEvaluator('gainRatio')
filter_evaluator_3 &lt;- filterEvaluator('MDLC')


## Examples of a filter evaluator generation (with parameters)

filter_evaluator_1 &lt;- filterEvaluator('relief', list(neighbours.count=4, sample.size=15))
filter_evaluator_2 &lt;- filterEvaluator('ReliefFeatureSetMeasure', list(iterations = 10))


## The direct application of this function is an advanced use that consists of using this 
# function directly to evaluate a set of features
## Classification problem

# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('ReliefFeatureSetMeasure')
# Evaluates features directly (parameters: dataset, target variable and features)
filter_evaluator(iris,'Species',c('Sepal.Length','Sepal.Width','Petal.Length','Petal.Width'))

## End(Not run)
</code></pre>

<hr>
<h2 id='fscore'>F-score measure</h2><span id='topic+fscore'></span>

<h3>Description</h3>

<p>Generates an evaluation function that calculates the F-score approach defined in (Wang et al. 2018) (individual measure). This function is called internally within the <code><a href="#topic+filterEvaluator">filterEvaluator</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fscore()
</code></pre>


<h3>Value</h3>

<p>Returns a function that is used to generate an individual evaluation measure using the F-score.
</p>


<h3>References</h3>

<p>Wang D, Zhang Z, Bai R, Mao Y (2018).
&ldquo;A hybrid system with filter approach and multiple population genetic algorithm for feature selection in credit scoring.&rdquo;
<em>Journal of Computational and Applied Mathematics</em>, <b>329</b>, 307&ndash;321.
doi: <a href="https://doi.org/10.1016/j.cam.2017.04.036">10.1016/j.cam.2017.04.036</a>, The International Conference on Information and Computational Science, 2&ndash;6 August 2016, Dalian, China, <a href="https://www.sciencedirect.com/science/article/abs/pii/S0377042717302078">https://www.sciencedirect.com/science/article/abs/pii/S0377042717302078</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly to individually evaluate a set of features
## Classification problem

# Generate the evaluation function with F-Score
fscore_evaluator &lt;- fscore()
# Evaluate the features (parameters: dataset, target variable and features)
fscore_evaluator(ToothGrowth, 'supp', c('len'))

## End(Not run)
</code></pre>

<hr>
<h2 id='gainRatio'>The gain ratio measure</h2><span id='topic+gainRatio'></span>

<h3>Description</h3>

<p>Generates an evaluation function that calculates the gain ratio value (Quinlan 1986-Mar-01), using the information theory (set measure). This function is called internally within the <code><a href="#topic+filterEvaluator">filterEvaluator</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gainRatio()
</code></pre>


<h3>Value</h3>

<p>Returns a function that is used to generate an evaluation set measure using the gain ratio value for the selected features.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>


<h3>References</h3>

<p>Quinlan JR (1986-Mar-01).
&ldquo;Induction of decision trees.&rdquo;
<em>Machine Learning</em>, <b>1</b>, 81&ndash;106.
doi: <a href="https://doi.org/10.1007/BF00116251">10.1007/BF00116251</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly to evaluate a set of features
## Classification problem

# A discrete dataset is used (in this case we use only several discrete columns)
adult &lt;- adult[,c(4,9,10,15)]

# Generate the evaluation function with Cramer
gr_evaluator &lt;- gainRatio()
# Evaluate the features (parameters: dataset, target variable and features)
gr_evaluator(adult,'income',c('race','sex','education'))

## End(Not run)
</code></pre>

<hr>
<h2 id='geneticAlgorithm'>Genetic Algorithm</h2><span id='topic+geneticAlgorithm'></span>

<h3>Description</h3>

<p>Generates a search function based on a genetic algorithm. This function is called internally within the <code><a href="#topic+searchAlgorithm">searchAlgorithm</a></code> function. The geneticAlgorithm method (Yang and Honavar 1998) starts with an initial population of solutions and at each step applies a series of operators to the individuals in order to obtain new and better population of individuals. These operators are selection, crossing and mutation methods. This method uses the GA package implementation (Scrucca 2013) (Scrucca 2017).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>geneticAlgorithm(
  popSize = 20,
  pcrossover = 0.8,
  pmutation = 0.1,
  maxiter = 100,
  run = 100,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="geneticAlgorithm_+3A_popsize">popSize</code></td>
<td>
<p>The popuplation size</p>
</td></tr>
<tr><td><code id="geneticAlgorithm_+3A_pcrossover">pcrossover</code></td>
<td>
<p>The probability of crossover between individuals</p>
</td></tr>
<tr><td><code id="geneticAlgorithm_+3A_pmutation">pmutation</code></td>
<td>
<p>The probability of mutation between individuals</p>
</td></tr>
<tr><td><code id="geneticAlgorithm_+3A_maxiter">maxiter</code></td>
<td>
<p>The number of iterations</p>
</td></tr>
<tr><td><code id="geneticAlgorithm_+3A_run">run</code></td>
<td>
<p>Number of consecutive iterations without fitness improvement to stop the algorithm</p>
</td></tr>
<tr><td><code id="geneticAlgorithm_+3A_verbose">verbose</code></td>
<td>
<p>Print the partial results in each iteration. This functionality is not available if the objective of the evaluation method is to minimize the target value (e.g. regression methods)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a search function that is used to guide the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>Scrucca L (2013).
&ldquo;GA: A Package for Genetic Algorithms in R.&rdquo;
<em>Journal of Statistical Software</em>, <b>53</b>, 1&ndash;37.
<a href="https://www.jstatsoft.org/article/view/v053i04">https://www.jstatsoft.org/article/view/v053i04</a>.<br /><br /> Scrucca L (2017).
&ldquo;On some extensions to GA package: hybrid optimisation, parallelisation and islands evolution.&rdquo;
<em>The R Journal</em>, <b>9</b>, 187&ndash;206.
<a href="https://journal.r-project.org/archive/2017/RJ-2017-008/">https://journal.r-project.org/archive/2017/RJ-2017-008/</a>.<br /><br /> Yang J, Honavar V (1998).
&ldquo;Feature subset selection using a genetic algorithm.&rdquo;
In <em>Feature extraction, construction and selection</em>, 117&ndash;136.
Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a search process in a feature space
## Classification problem

# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the search function with Genetic algorithm
ga_search &lt;- geneticAlgorithm()
# Performs the search process directly (parameters: dataset, target variable and evaluator)
ga_search(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='giniIndex'>Gini index measure</h2><span id='topic+giniIndex'></span>

<h3>Description</h3>

<p>Generates an evaluation function that calculates the gini index (Ceriani and Verme 2012-Sep-01) of discrete features (set measure). This function is called internally within the <code><a href="#topic+filterEvaluator">filterEvaluator</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>giniIndex()
</code></pre>


<h3>Value</h3>

<p>Returns a function that is used to generate an evaluation set measure using the Gini index value for the selected features.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>


<h3>References</h3>

<p>Ceriani L, Verme P (2012-Sep-01).
&ldquo;The origins of the Gini index: extracts from Variabilità e Mutabilità (1912) by Corrado Gini.&rdquo;
<em>The Journal of Economic Inequality</em>, <b>10</b>, 421&ndash;443.
doi: <a href="https://doi.org/10.1007/s10888-011-9188-x">10.1007/s10888-011-9188-x</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly to evaluate a set of features
## Classification problem

# A discrete dataset is used (in this case we use only several discrete columns)
adult &lt;- adult[,c(4,9,10,15)]

# Generate the evaluation function with Gini index
giniIndex_evaluator &lt;- giniIndex()
# Evaluate the features (parameters: dataset, target variable and features)
giniIndex_evaluator(adult,'income',c('race','sex','education'))

## End(Not run)
</code></pre>

<hr>
<h2 id='hillClimbing'>Hill-Climbing</h2><span id='topic+hillClimbing'></span>

<h3>Description</h3>

<p>Generates a search function based on the hill climbing method. This function is called internally within the <code><a href="#topic+searchAlgorithm">searchAlgorithm</a></code> function. The Hill-Climbing (Russell and Norvig 2009) method starts with a certain set of features and in each iteration it searches among its neighbors to advance towards a better solution. The method ends as soon as no better solutions are found.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hillClimbing(start = NULL, nneigh = NULL, repeats = 1, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hillClimbing_+3A_start">start</code></td>
<td>
<p>Binary vector with the set of initial features</p>
</td></tr>
<tr><td><code id="hillClimbing_+3A_nneigh">nneigh</code></td>
<td>
<p>Number of neighbors to evaluate in each iteration of the algorithm. By default: all posibles. It is important to note that a high value of this parameter considerably increases the computation time.</p>
</td></tr>
<tr><td><code id="hillClimbing_+3A_repeats">repeats</code></td>
<td>
<p>Number of repetitions of the algorithm</p>
</td></tr>
<tr><td><code id="hillClimbing_+3A_verbose">verbose</code></td>
<td>
<p>Print the partial results in each iteration</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a search function that is used to guide the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>Russell S, Norvig P (2009).
<em>Artificial Intelligence: A Modern Approach</em>, 3rd edition.
Prentice Hall Press, Upper Saddle River, NJ, USA.
ISBN 0136042597, 9780136042594.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a search process in a feature space
## Classification problem

# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the search function with Hill-Climbing
hc_search &lt;- hillClimbing()
# Performs the search process directly (parameters: dataset, target variable and evaluator)
hc_search(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='hybridFeatureSelection'>Hybrid Feature Selection Proccess</h2><span id='topic+hybridFeatureSelection'></span>

<h3>Description</h3>

<p>Performs the hybrid feature selection process. Given a hybrid search algorithm and an two evaluation methods, it uses the hybrid search algorithm in combination with the evaluation results to guide the feature selection process to an optimal subset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hybridFeatureSelection(data, class, hybridSearcher, evaluator_1, evaluator_2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hybridFeatureSelection_+3A_data">data</code></td>
<td>
<p>A data.frame with the input dataset where the examples are in the rows and the features and the target variable are in the columns. The dataset should be discrete (feature columns are expected to be factors) if the following filter methods are used as evaluation methods: Rough Set Consistency, Binary Consistency, IE Consistency, IEP Consistency, Mutual Information, Gain Ratio, Symmetrical Uncertain, Gini Index or MDLC. The Jd and F-Score filter methods only work on classification problems with 2 classes in the target variable.</p>
</td></tr>
<tr><td><code id="hybridFeatureSelection_+3A_class">class</code></td>
<td>
<p>The name of the dependent variable</p>
</td></tr>
<tr><td><code id="hybridFeatureSelection_+3A_hybridsearcher">hybridSearcher</code></td>
<td>
<p>The algorithm to guide the hybrid search in the feature space. See <code><a href="#topic+hybridSearchAlgorithm">hybridSearchAlgorithm</a></code>.</p>
</td></tr>
<tr><td><code id="hybridFeatureSelection_+3A_evaluator_1">evaluator_1</code></td>
<td>
<p>The first evaluation method. This method can be a filter (see <code><a href="#topic+filterEvaluator">filterEvaluator</a></code>) or a wrapper method (see <code><a href="#topic+wrapperEvaluator">wrapperEvaluator</a></code>).</p>
</td></tr>
<tr><td><code id="hybridFeatureSelection_+3A_evaluator_2">evaluator_2</code></td>
<td>
<p>The second evaluation method. This method can be a filter (see <code><a href="#topic+filterEvaluator">filterEvaluator</a></code>) or a wrapper method (see <code><a href="#topic+wrapperEvaluator">wrapperEvaluator</a></code>). If the LCC algorithm is used, the measure must evaluate feature sets.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list is returned with the results of the hybrid feature selection process:
</p>

<dl>
<dt>bestFeatures</dt><dd><p>A vector with all features. Selected features are marked with 1, unselected features are marked with 0.</p>
</dd>
<dt>bestValue</dt><dd><p>Evaluation measure obtained with the feature selection.</p>
</dd>
<dt>evaluationType_1</dt><dd><p>Type of evaluation based on how the features have been evaluated.</p>
</dd>
<dt>evaluationMethod_1</dt><dd><p>Evaluation method used for the first evaluator.</p>
</dd>
<dt>measureType_1</dt><dd><p>Type of evaluation measure for the first evaluator.</p>
</dd>
<dt>evaluationType_2</dt><dd><p>Type of evaluation based on how the features have been evaluated for the first evaluator.</p>
</dd>
<dt>evaluationMethod_2</dt><dd><p>Evaluation method used for the second evaluator.</p>
</dd>
<dt>measureType_2</dt><dd><p>Type of evaluation measure for the second evaluator.</p>
</dd>
<dt>searchMethod</dt><dd><p>Search method used during the feature selection process for the second evaluator.</p>
</dd>
<dt>target</dt><dd><p>A character indicating if the objective of the process is to minimize or maximize the evaluation measure.</p>
</dd>
<dt>numFeatures</dt><dd><p>Number of features in the problem.</p>
</dd>
<dt>xNames</dt><dd><p>Name of the features.</p>
</dd>
<dt>yNames</dt><dd><p>Name of the dependent variable.</p>
</dd>
<dt>time</dt><dd><p>Value of class 'proc_time' containing the user time, system time, and total time of the feature selection process.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>There are no references for Rd macro <code style="white-space: pre;">&#8288;\insertAllCites&#8288;</code> on this help page.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## Examples of the hybrid feature selection process
## Classification problem with filter

# Generates the first filter evaluation function (individual or set measure)
f_evaluator_1 &lt;- filterEvaluator('determinationCoefficient')
# Generates the second filter evaluation function (mandatory set measure)
f_evaluator_2 &lt;- filterEvaluator('ReliefFeatureSetMeasure')
# Generates the hybrid search function
hybrid_search_method &lt;- hybridSearchAlgorithm('LCC')
# Runs the hybrid feature selection process
res &lt;- hybridFeatureSelection(iris,'Species',hybrid_search_method,f_evaluator_1,f_evaluator_2)


## Classification problem with wrapper

# Generates the first wrapper evaluation function (individual or set measure)
w_evaluator_1 &lt;- wrapperEvaluator('rf')
# Generates the second wrapper evaluation function (mandatory set measure)
w_evaluator_2 &lt;- wrapperEvaluator('knn')
# Generates the hybrid search function
hybrid_search_method &lt;- hybridSearchAlgorithm('LCC')
# Runs the hybrid feature selection process
res &lt;- hybridFeatureSelection(iris,'Species',hybrid_search_method,w_evaluator_1,w_evaluator_2)


## Classification problem mixed (with filter &amp; wrapper)

# Generates the first filter evaluation function (individual or set measure)
f_evaluator &lt;- filterEvaluator('determinationCoefficient')
# Generates the second wrapper evaluation function (mandatory set measure)
w_evaluator &lt;- wrapperEvaluator('knn')
# Generates the hybrid search function
hybrid_search_method &lt;- hybridSearchAlgorithm('LCC')
# Runs the hybrid feature selection process
res &lt;- hybridFeatureSelection(iris, 'Species', hybrid_search_method, f_evaluator, w_evaluator)

## End(Not run)

</code></pre>

<hr>
<h2 id='hybridSearchAlgorithm'>Hybrid search algorithm generator</h2><span id='topic+hybridSearchAlgorithm'></span>

<h3>Description</h3>

<p>Generates a hybrid search function. This function in combination with the evaluator guides the feature selection process. Specifically, the result of calling this function is another function that is passed on as a parameter to the <code><a href="#topic+featureSelection">featureSelection</a></code> function. However, you can run this function directly to perform a search process in the features space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hybridSearchAlgorithm(hybridSearcher, params = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hybridSearchAlgorithm_+3A_hybridsearcher">hybridSearcher</code></td>
<td>
<p>Name of the hybrid search algorithm. The available hybrid search algorithms are:
</p>

<dl>
<dt>LCC</dt><dd><p>Linear Consistency-Constrained algorithm (LCC). See <code><a href="#topic+LCC">LCC</a></code> </p>
</dd>
</dl>
</td></tr>
<tr><td><code id="hybridSearchAlgorithm_+3A_params">params</code></td>
<td>
<p>List with the parameters of each hybrid search method. For more details see each method. Default: empty list.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a hybrid search function that is used to guide the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>There are no references for Rd macro <code style="white-space: pre;">&#8288;\insertAllCites&#8288;</code> on this help page.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## Examples of a hybrid search algorithm generation

hybrid_search_method &lt;- hybridSearchAlgorithm('LCC')


## Examples of a hybrid search algorithm generation (with parameters)

hybrid_search_method &lt;- hybridSearchAlgorithm('LCC', list(threshold = 0.8))



## The direct application of this function is an advanced use that consists of using this 
# function directly to perform a hybrid search process on a feature space
## Classification problem

# Generates the first filter evaluation function (individual or set measure)
filter_evaluator_1 &lt;- filterEvaluator('determinationCoefficient')
# Generates the second filter evaluation function (mandatory set measure)
filter_evaluator_2 &lt;- filterEvaluator('ReliefFeatureSetMeasure')

# Generates the hybrid search function
hybrid_search_method &lt;- hybridSearchAlgorithm('LCC')
# Run the search process directly (params: dataset, target variable, evaluator1 &amp; evaluator2)
hybrid_search_method(iris, 'Species', filter_evaluator_1, filter_evaluator_2)

## End(Not run)
</code></pre>

<hr>
<h2 id='IEConsistency'>Inconsistent Examples consistency measure</h2><span id='topic+IEConsistency'></span>

<h3>Description</h3>

<p>Generates an evaluation function that calculates the inconsistent examples consistency value (Dash and Liu 2003-dec), using hash tables (set measure). This function is called internally within the <code><a href="#topic+filterEvaluator">filterEvaluator</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IEConsistency()
</code></pre>


<h3>Value</h3>

<p>Returns a function that is used to generate an evaluation set measure using the inconsistent examples consistency value for the selected features.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>


<h3>References</h3>

<p>Dash M, Liu H (2003-dec).
&ldquo;Consistency-based Search in Feature Selection.&rdquo;
<em>Artif. Intell.</em>, <b>151</b>, 155&ndash;176.
doi: <a href="https://doi.org/10.1016/S0004-3702(03)00079-1">10.1016/S0004-3702(03)00079-1</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly to evaluate a set of features
## Classification problem

# A discrete dataset is used (in this case we use only several discrete columns)
adult &lt;- adult[,c(4,9,10,15)]

# Generate the evaluation function with IE Consistency
IEC_evaluator &lt;- IEConsistency()
# Evaluate the features (parameters: dataset, target variable and features)
IEC_evaluator(adult,'income',c('race','sex','education'))

## End(Not run)
</code></pre>

<hr>
<h2 id='IEPConsistency'>Inconsistent Examples Pairs consistency measure</h2><span id='topic+IEPConsistency'></span>

<h3>Description</h3>

<p>Generates an evaluation function that calculates the inconsistent examples pairs consistency value, using hash tables (Arauzo-Azofra et al. 2007-feb) (set measure). This function is called internally within the <code><a href="#topic+filterEvaluator">filterEvaluator</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IEPConsistency()
</code></pre>


<h3>Value</h3>

<p>Returns a function that is used to generate an evaluation set measure using the inconsistent examples pairs consistency value for the selected features.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>


<h3>References</h3>

<p>Arauzo-Azofra A, Benitez JM, Castro JL (2007-feb).
&ldquo;Consistency measures for feature selection.&rdquo;
<em>Journal of Intelligent Information Systems</em>, <b>30</b>, 273&ndash;292.
doi: <a href="https://doi.org/10.1007/s10844-007-0037-0">10.1007/s10844-007-0037-0</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly to evaluate a set of features
## Classification problem

# A discrete dataset is used (in this case we use only several discrete columns)
adult &lt;- adult[,c(4,9,10,15)]

# Generate the evaluation function with IEP Consistency
IEPC_evaluator &lt;- IEPConsistency()
# Evaluate the features (parameters: dataset, target variable and features)
IEPC_evaluator(adult,'income',c('race','sex','education'))

## End(Not run)
</code></pre>

<hr>
<h2 id='isDataframeContinuous'>isDataframeContinuous(dataframe)</h2><span id='topic+isDataframeContinuous'></span>

<h3>Description</h3>

<p>Estimate if all variables in a data frame are continuous
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isDataframeContinuous(dataframe)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="isDataframeContinuous_+3A_dataframe">dataframe</code></td>
<td>

<ul>
<li><p> A data frame
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p> True if all variables are continuous, False otherwise
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Alfonso Jiménez Vílchez
</p>


<h3>Examples</h3>

<pre><code class='language-R'>isDataframeContinuous(mtcars)
isDataframeContinuous(iris)
</code></pre>

<hr>
<h2 id='isDataframeDiscrete'>isDataFrameDiscrete(dataframe)</h2><span id='topic+isDataframeDiscrete'></span>

<h3>Description</h3>

<p>Estimate if all variables in a data frame are discrete
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isDataframeDiscrete(dataframe)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="isDataframeDiscrete_+3A_dataframe">dataframe</code></td>
<td>

<ul>
<li><p> A data frame
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p> True if all variables are discrete, False otherwise
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Alfonso Jiménez Vílchez
</p>


<h3>Examples</h3>

<pre><code class='language-R'>isDataframeDiscrete(mtcars)
isDataframeDiscrete(iris)
</code></pre>

<hr>
<h2 id='Jd'>Jd evaluation measure</h2><span id='topic+Jd'></span>

<h3>Description</h3>

<p>Generates an evaluation function that applies the discriminant function designed by Narendra and Fukunaga (Narendra and Fukunaga 1977-sep) to generate an evaluation measure for a set of features (set measure). This function is called internally within the <code><a href="#topic+filterEvaluator">filterEvaluator</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Jd()
</code></pre>


<h3>Value</h3>

<p>Returns a function that is used to generate an evaluation set measure using the Jd.
</p>


<h3>Author(s)</h3>

<p>Alfonso Jiménez-Vílchez
</p>


<h3>References</h3>

<p>Narendra P, Fukunaga K (1977-sep).
&ldquo;A Branch and Bound Algorithm for Feature Subset Selection.&rdquo;
<em>IEEE Transactions on Computers</em>, <b>26</b>, 917&ndash;922.
doi: <a href="https://doi.org/10.1109/TC.1977.1674939">10.1109/TC.1977.1674939</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly to evaluate a set of features
## Classification problem

# Generate the evaluation function with JD
Jd_evaluator &lt;- Jd()
# Evaluate the features (parametes: dataset, target variable and features)
Jd_evaluator(ToothGrowth,'supp',c('len','dose'))

## End(Not run)
</code></pre>

<hr>
<h2 id='LasVegas'>Las Vegas</h2><span id='topic+LasVegas'></span>

<h3>Description</h3>

<p>Generates a search function based on Las Vegas algorithm. This function is called internally within the <code><a href="#topic+searchAlgorithm">searchAlgorithm</a></code> function. The LasVegas method  (Liu and Setiono 1996) starts with a certain set of features and in each step a new set is randomly generated, if the new set is better it is saved as the best solution. The algorithm ends when there are no improvements in a certain number of iterations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LasVegas(start = NULL, K = 50, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LasVegas_+3A_start">start</code></td>
<td>
<p>Binary vector with the set of initial features (1: selected and 0: unselected) for the algorithm</p>
</td></tr>
<tr><td><code id="LasVegas_+3A_k">K</code></td>
<td>
<p>The maximum number of iterations without improvement to finalize the algorithm</p>
</td></tr>
<tr><td><code id="LasVegas_+3A_verbose">verbose</code></td>
<td>
<p>Print the partial results in each iteration</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a search function that is used to guide the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>Liu H, Setiono R (1996).
&ldquo;Feature Selection And Classification - A Probabilistic Wrapper Approach.&rdquo;
In <em>in Proceedings of the 9th International Conference on Industrial and Engineering Applications of AI and ES</em>, 419&ndash;424.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a search process in a feature space
## Classification problem

# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the search function with Las Vegas
LV_search &lt;- LasVegas()
# Performs the search process directly (parameters: dataset, target variable and evaluator)
LV_search(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='LCC'>Linear Consistency-Constrained algorithm</h2><span id='topic+LCC'></span>

<h3>Description</h3>

<p>Generates a hybrid search function based on Linear Consistency-Constrained algorithm described in (Shin and Xu 2009). The algorithm combines two evaluation measures, the first evaluates each feature individually, and the second measure evaluate feature sets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LCC(threshold = 0.9)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LCC_+3A_threshold">threshold</code></td>
<td>
<p>Threshold</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a hybrid search function that is used to guide the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Alfonso Jiménez-Vílchez
</p>


<h3>References</h3>

<p>Shin K, Xu XM (2009).
&ldquo;Consistency-Based Feature Selection.&rdquo;
In <em>Knowledge-Based and Intelligent Information and Engineering Systems</em>, 342&ndash;350.
ISBN 978-3-642-04595-0.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a hybrid search process in a feature space
## Classification problem

# Generates the first filter evaluation function (individual or set measure)
filter_evaluator_1 &lt;- filterEvaluator('determinationCoefficient')
# Generates the second filter evaluation function (mandatory set measure)
filter_evaluator_2 &lt;- filterEvaluator('ReliefFeatureSetMeasure')

  
# Generates the hybrid search function with LCC
LCC_hybrid_search &lt;- LCC()
# Run the search process directly (params: dataset, target variable, evaluator1 &amp; evaluator2)
LCC_hybrid_search(iris, 'Species', filter_evaluator_1, filter_evaluator_2)

## End(Not run)
</code></pre>

<hr>
<h2 id='MDLC'>MDLC evaluation measure</h2><span id='topic+MDLC'></span>

<h3>Description</h3>

<p>Generates an evaluation function that applies the Minimum-Description_Length-Criterion (MDLC) (Sheinvald et al. 1990-jun) to generate an evaluation measure for a set of features (set measure). This function is called internally within the <code><a href="#topic+filterEvaluator">filterEvaluator</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MDLC()
</code></pre>


<h3>Value</h3>

<p>Returns a function that is used to generate an evaluation set measure using MDLC value for the selected features
</p>


<h3>Author(s)</h3>

<p>Alfonso Jiménez-Vílchez
</p>


<h3>References</h3>

<p>Sheinvald J, Dom B, Niblack W (1990-jun).
&ldquo;A modeling approach to feature selection.&rdquo;
In <em>[1990] Proceedings. 10th International Conference on Pattern Recognition</em>, volume i, 535&ndash;539.
doi: <a href="https://doi.org/10.1109/ICPR.1990.118160">10.1109/ICPR.1990.118160</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly to evaluate a set of features
## Classification problem

# A discrete dataset is used (in this case we use only several discrete columns)
adult &lt;- adult[,c(4,9,10,15)]

# Generate the evaluation function with MDLC
MDLC_evaluator &lt;- MDLC()
# Evaluate the features (parameters: dataset, target variable and features)
MDLC_evaluator(adult,'income',c('race','sex','education'))

## End(Not run)
</code></pre>

<hr>
<h2 id='mutualInformation'>The mutual information measure</h2><span id='topic+mutualInformation'></span>

<h3>Description</h3>

<p>Generates an evaluation function that calculates the mutual information value, using the information theory (Qian and Shu 2015) (set measure). This function is called internally within the <code><a href="#topic+filterEvaluator">filterEvaluator</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mutualInformation()
</code></pre>


<h3>Value</h3>

<p>Returns a function that is used to generate an evaluation set measure using the mutual information value for the selected features.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>


<h3>References</h3>

<p>Qian W, Shu W (2015).
&ldquo;Mutual information criterion for feature selection from incomplete data.&rdquo;
<em>Neurocomputing</em>, <b>168</b>, 210&ndash;220.
doi: <a href="https://doi.org/10.1016/j.neucom.2015.05.105">10.1016/j.neucom.2015.05.105</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly to evaluate a set of features
## Classification problem

# A discrete dataset is used (in this case we use only several discrete columns)
adult &lt;- adult[,c(4,9,10,15)]

# Generate the evaluation function with Cramer
mi_evaluator &lt;- mutualInformation()
# Evaluate the features (parameters: dataset, target variable and features)
mi_evaluator(adult,'income',c('race','sex','education'))

## End(Not run)
</code></pre>

<hr>
<h2 id='normalizedRelief'>Normalized Relief</h2><span id='topic+normalizedRelief'></span>

<h3>Description</h3>

<p>Generates an evaluation function that calculates a measure of the set of features between 0 and 1 with relief (individual measure). The relief algorithm (Kira and Rendell 1992) finds weights of continous and discrete attributes basing on a distance between instances. Adapted from Piotr Romanski's Fselector package (Romanski and Kotthoff 2018). This function is called internally within the <code><a href="#topic+filterEvaluator">filterEvaluator</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalizedRelief(neighbours.count = 5, sample.size = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalizedRelief_+3A_neighbours.count">neighbours.count</code></td>
<td>

<ul>
<li><p> number of neighbours to find for every sampled instance
</p>
</li></ul>
</td></tr>
<tr><td><code id="normalizedRelief_+3A_sample.size">sample.size</code></td>
<td>

<ul>
<li><p> number of instances to sample
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>relief
classification and regression
continous and discrete data
</p>


<h3>Value</h3>

<p>Returns a function that is used to generate an individual evaluation measure using relief
</p>


<h3>Author(s)</h3>

<p>Alfonso Jiménez-Vílchez
</p>


<h3>References</h3>

<p>Kira K, Rendell LA (1992).
&ldquo;A practical approach to feature selection.&rdquo;
In <em>Machine Learning Proceedings 1992</em>, 249&ndash;256.
Elsevier.<br /><br /> Romanski P, Kotthoff L (2018).
<em>FSelector: Selecting Attributes</em>.
R package version 0.31, <a href="https://CRAN.R-project.org/package=FSelector">https://CRAN.R-project.org/package=FSelector</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly to individually evaluate a set of features
## Classification problem

# Generate the evaluation function with Cramer
relief_evaluator &lt;- normalizedRelief()
# Evaluate the features (parameters: dataset, target variable and features)
relief_evaluator(iris,'Species',c('Sepal.Length'))

## End(Not run)
</code></pre>

<hr>
<h2 id='normalizedReliefFeatureSetMeasure'>Relief Feature Set Measure evaluation measure</h2><span id='topic+normalizedReliefFeatureSetMeasure'></span>

<h3>Description</h3>

<p>Generates an evaluation function that applies Feature set measure based on Relief (set measure). Described in (Arauzo-Azofra et al. 2004-1). This function is called internally within the <code><a href="#topic+filterEvaluator">filterEvaluator</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalizedReliefFeatureSetMeasure(iterations = 5, kNeightbours = 4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalizedReliefFeatureSetMeasure_+3A_iterations">iterations</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code id="normalizedReliefFeatureSetMeasure_+3A_kneightbours">kNeightbours</code></td>
<td>
<p>Number of neighbours</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a function that is used to generate an evaluation set measure (between -1 and 1) using RFSM value for the selected features.
</p>


<h3>Author(s)</h3>

<p>Alfonso Jiménez-Vílchez
</p>


<h3>References</h3>

<p>Arauzo-Azofra A, Benítez J, Castro J (2004-1).
&ldquo;A feature set measure based on Relief.&rdquo;
<em>Proceedings of the 5th International Conference on Recent Advances in Soft Computing</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly to evaluate a set of features
## Classification problem

# Generate the evaluation function with Cramer
RFSM_evaluator &lt;- ReliefFeatureSetMeasure()
# Evaluate the features (parameters: dataset, target variable and features)
RFSM_evaluator(iris,'Species',c('Sepal.Length','Sepal.Width','Petal.Length','Petal.Width'))

## End(Not run)
</code></pre>

<hr>
<h2 id='relief'>Relief</h2><span id='topic+relief'></span>

<h3>Description</h3>

<p>Generates an evaluation function that calculates a measure of the set of features with relief (individual measure). The relief algorithm (Kira and Rendell 1992) finds weights of continous and discrete attributes basing on a distance between instances. Adapted from Piotr Romanski's Fselector package (Romanski and Kotthoff 2018). This function is called internally within the <code><a href="#topic+filterEvaluator">filterEvaluator</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relief(neighbours.count = 5, sample.size = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relief_+3A_neighbours.count">neighbours.count</code></td>
<td>

<ul>
<li><p> number of neighbours to find for every sampled instance
</p>
</li></ul>
</td></tr>
<tr><td><code id="relief_+3A_sample.size">sample.size</code></td>
<td>

<ul>
<li><p> number of instances to sample
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>relief
classification and regression
continous and discrete data
</p>


<h3>Value</h3>

<p>Returns a function that is used to generate an individual evaluation measure using relief
</p>


<h3>Author(s)</h3>

<p>Alfonso Jiménez-Vílchez
</p>


<h3>References</h3>

<p>Kira K, Rendell LA (1992).
&ldquo;A practical approach to feature selection.&rdquo;
In <em>Machine Learning Proceedings 1992</em>, 249&ndash;256.
Elsevier.<br /><br /> Romanski P, Kotthoff L (2018).
<em>FSelector: Selecting Attributes</em>.
R package version 0.31, <a href="https://CRAN.R-project.org/package=FSelector">https://CRAN.R-project.org/package=FSelector</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly to individually evaluate a set of features
## Classification problem

# Generate the evaluation function with Cramer
relief_evaluator &lt;- relief()
# Evaluate the features (parameters: dataset, target variable and features)
relief_evaluator(iris,'Species',c('Sepal.Length'))

## End(Not run)
</code></pre>

<hr>
<h2 id='ReliefFeatureSetMeasure'>Relief Feature Set Measure evaluation measure</h2><span id='topic+ReliefFeatureSetMeasure'></span>

<h3>Description</h3>

<p>Generates an evaluation function that applies Feature set measure based on Relief (set measure). Described in (Arauzo-Azofra et al. 2004-1). This function is called internally within the <code><a href="#topic+filterEvaluator">filterEvaluator</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ReliefFeatureSetMeasure(iterations = 5, kNeightbours = 4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ReliefFeatureSetMeasure_+3A_iterations">iterations</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code id="ReliefFeatureSetMeasure_+3A_kneightbours">kNeightbours</code></td>
<td>
<p>Number of neighbours</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a function that is used to generate an evaluation set measure (between -1 and 1) using RFSM value for the selected features.
</p>


<h3>Author(s)</h3>

<p>Alfonso Jiménez-Vílchez
</p>


<h3>References</h3>

<p>Arauzo-Azofra A, Benítez J, Castro J (2004-1).
&ldquo;A feature set measure based on Relief.&rdquo;
<em>Proceedings of the 5th International Conference on Recent Advances in Soft Computing</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly to evaluate a set of features
## Classification problem

# Generate the evaluation function with Cramer
RFSM_evaluator &lt;- ReliefFeatureSetMeasure()
# Evaluate the features (parameters: dataset, target variable and features)
RFSM_evaluator(iris,'Species',c('Sepal.Length','Sepal.Width','Petal.Length','Petal.Width'))

## End(Not run)
</code></pre>

<hr>
<h2 id='roughsetConsistency'>Rough Set consistency measure</h2><span id='topic+roughsetConsistency'></span>

<h3>Description</h3>

<p>Generates an evaluation function that calculates the rough sets consistency value (Pawlak 1982-october) (Pawlak 1991), using hash tables (set measure). This function is called internally within the <code><a href="#topic+filterEvaluator">filterEvaluator</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>roughsetConsistency()
</code></pre>


<h3>Value</h3>

<p>Returns a function that is used to generate an evaluation set measure using the rough sets consistency value for the selected features.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>


<h3>References</h3>

<p>Pawlak Z (1982-october).
&ldquo;Rough sets.&rdquo;
<em>International Journal of Computer \&amp; Information Sciences</em>, <b>11</b>, 341&ndash;356.
doi: <a href="https://doi.org/10.1007/BF01001956">10.1007/BF01001956</a>, <a href="https://www.sciencedirect.com/science/article/abs/pii/S0377042717302078">https://www.sciencedirect.com/science/article/abs/pii/S0377042717302078</a>.<br /><br /> Pawlak Z (1991).
<em>Rough sets: Theoretical aspects of reasoning about data</em>, volume 9(1).
Springer, Dordrecht.
doi: <a href="https://doi.org/10.1007/978-94-011-3534-4">10.1007/978-94-011-3534-4</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly to evaluate a set of features
## Classification problem

# A discrete dataset is used (in this case we use only several discrete columns)
adult &lt;- adult[,c(4,9,10,15)]

# Generate the evaluation function with Rough Set Consistency
rsc_evaluator &lt;- roughsetConsistency()
# Evaluate the features (parameters: dataset, target variable and features)
rsc_evaluator(adult,'income',c('race','sex','education'))

## End(Not run)
</code></pre>

<hr>
<h2 id='searchAlgorithm'>Search algorithm generator</h2><span id='topic+searchAlgorithm'></span>

<h3>Description</h3>

<p>Generates a search function. This function in combination with the evaluator guides the feature selection process. Specifically, the result of calling this function is another function that is passed on as a parameter to the <code><a href="#topic+featureSelection">featureSelection</a></code> function. However, you can run this function directly to perform a search process in the features space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>searchAlgorithm(searcher, params = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="searchAlgorithm_+3A_searcher">searcher</code></td>
<td>
<p>Name of the search algorithm. The available search algorithms are:
</p>

<dl>
<dt>antColony</dt><dd><p>Ant colony optimization (ACO). See <code><a href="#topic+antColony">antColony</a></code> </p>
</dd>
<dt>breadthFirst</dt><dd><p>Breadth first search. See <code><a href="#topic+breadthFirst">breadthFirst</a></code> </p>
</dd>
<dt>deepFirst</dt><dd><p>Deep first search. See <code><a href="#topic+deepFirst">deepFirst</a></code> </p>
</dd>
<dt>geneticAlgorithm</dt><dd><p>Genetic algorithm (GA). See <code><a href="#topic+geneticAlgorithm">geneticAlgorithm</a></code> </p>
</dd>
<dt>hillClimbing</dt><dd><p>Hill-Climbing (HC). See <code><a href="#topic+hillClimbing">hillClimbing</a></code> </p>
</dd>
<dt>LasVegas</dt><dd><p>Las Vegas (LV). See <code><a href="#topic+LasVegas">LasVegas</a></code> </p>
</dd>
<dt>sequentialBackwardSelection</dt><dd><p>Sequential backward selection (sbs). See <code><a href="#topic+sequentialBackwardSelection">sequentialBackwardSelection</a></code> </p>
</dd>
<dt>sequentialFloatingForwardSelection</dt><dd><p>Sequential floating forward selection (sffs). See <code><a href="#topic+sequentialFloatingForwardSelection">sequentialFloatingForwardSelection</a></code> </p>
</dd>
<dt>sequentialFloatingBackwardSelection</dt><dd><p>Sequential floating backward selection (sfbs). See <code><a href="#topic+sequentialFloatingBackwardSelection">sequentialFloatingBackwardSelection</a></code> </p>
</dd>
<dt>sequentialForwardSelection</dt><dd><p>Sequential forward selection (sfs). See <code><a href="#topic+sequentialForwardSelection">sequentialForwardSelection</a></code> </p>
</dd>
<dt>simulatedAnnealing</dt><dd><p>Simulated annealing (SA). See <code><a href="#topic+simulatedAnnealing">simulatedAnnealing</a></code> </p>
</dd>
<dt>tabu</dt><dd><p>Tabu search (TS). See <code><a href="#topic+tabu">tabu</a></code> </p>
</dd>
<dt>whaleOptimization</dt><dd><p>Whale optimization algorithm (WOA). See <code><a href="#topic+whaleOptimization">whaleOptimization</a></code> </p>
</dd>
</dl>
</td></tr>
<tr><td><code id="searchAlgorithm_+3A_params">params</code></td>
<td>
<p>List with the parameters of each search method. For more details see each method. Default: empty list.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a search function that is used to guide the feature selection process
</p>


<h3>Author(s)</h3>

<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>There are no references for Rd macro <code style="white-space: pre;">&#8288;\insertAllCites&#8288;</code> on this help page.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## Examples of a search algorithm generation

search_method_1 &lt;- searchAlgorithm('antColony')
search_method_2 &lt;- searchAlgorithm('sequentialBackwardSelection')
search_method_3 &lt;- searchAlgorithm('tabu')


## Examples of a search algorithm generation (with parameters)

search_method_1 &lt;- searchAlgorithm('antColony', list(population=25, iter=50, verbose=TRUE))
search_method_2 &lt;- searchAlgorithm('sequentialBackwardSelection', list(stop=TRUE))
search_method_3 &lt;- searchAlgorithm('tabu', list(intensification=1, iterIntensification=25))


## The direct application of this function is an advanced use that consists of using this 
# function directly to perform a search process on a feature space
## Classification problem

# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the search function
search_method &lt;- searchAlgorithm('hillClimbing')
# Performs the search process directly (parameters: dataset, target variable and evaluator)
search_method(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='selectDifference'>Select difference</h2><span id='topic+selectDifference'></span>

<h3>Description</h3>

<p>Generates a direct search function that selects features (in descending order from the best evaluation measure to the lowest) until evaluation difference is over a threshold (The features evaluation is individual). This function is called internally within the <code><a href="#topic+directSearchAlgorithm">directSearchAlgorithm</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selectDifference(d.threshold = 0.2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selectDifference_+3A_d.threshold">d.threshold</code></td>
<td>

<ul>
<li><p> Number between 0 and 1, to calculate the slope
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a direct search function that is used in the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>
<p>Francisco Aragón Royón
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a direct search process
## Classification problem


# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the direct search function with difference
sd_direct_search &lt;- selectDifference()
# Performs the direct search process directly (parameters: dataset, target variable and evaluator)
sd_direct_search(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='selectKBest'>Select K best</h2><span id='topic+selectKBest'></span>

<h3>Description</h3>

<p>Generates a direct search function that takes the 'k' features with the greatest evaluations (The features evaluation is individual). This function is called internally within the <code><a href="#topic+directSearchAlgorithm">directSearchAlgorithm</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selectKBest(k = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selectKBest_+3A_k">k</code></td>
<td>
<p>Number (positive integer) of returned features</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a direct search function that is used in the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>
<p>Francisco Aragón Royón
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a direct search process
## Classification problem


# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the direct search function with k-best
skb_direct_search &lt;- selectKBest()
# Performs the direct search process directly (parameters: dataset, target variable and evaluator)
skb_direct_search(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='selectPercentile'>Select Percentile</h2><span id='topic+selectPercentile'></span>

<h3>Description</h3>

<p>Generates a direct search function that selects a fraction, given as a percentage, of the total number of available features (The features evaluation is individual). This function is called internally within the <code><a href="#topic+directSearchAlgorithm">directSearchAlgorithm</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selectPercentile(percentile = 80)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selectPercentile_+3A_percentile">percentile</code></td>
<td>
<p>Number (positive integer) between 0 and 100</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a direct search function that is used in the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>
<p>Francisco Aragón Royón
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a direct search process
## Classification problem


# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the direct search function with percentile
sp_direct_search &lt;- selectPercentile()
# Performs the direct search process directly (parameters: dataset, target variable and evaluator)
sp_direct_search(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='selectSlope'>Select slope</h2><span id='topic+selectSlope'></span>

<h3>Description</h3>

<p>Generates a direct search function that selects features (in descending order from the best evaluation measure to the lowest) until the slope to the next feature is over a threshold (The features evaluation is individual). The slope is calculated as: (s.threshold) / (number of features). This function is called internally within the <code><a href="#topic+directSearchAlgorithm">directSearchAlgorithm</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selectSlope(s.threshold = 1.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selectSlope_+3A_s.threshold">s.threshold</code></td>
<td>

<ul>
<li><p> Number between 0 and 1
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a direct search function that is used in the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a direct search process
## Classification problem


# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the direct search function with slope
ss_direct_search &lt;- selectSlope()
# Performs the direct search process directly (parameters: dataset, target variable and evaluator)
ss_direct_search(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='selectThreshold'>Select threshold</h2><span id='topic+selectThreshold'></span>

<h3>Description</h3>

<p>Generates a direct search function that selects the features whose evaluation is over/under a user given threshold (It depends on the method that generates the evaluation measure. For example: under for regression methods, over for classification methods, etc.)(The features evaluation is individual). Features that do not satisfy the threshold, will be removed. This function is called internally within the <code><a href="#topic+directSearchAlgorithm">directSearchAlgorithm</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selectThreshold(threshold = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selectThreshold_+3A_threshold">threshold</code></td>
<td>

<ul>
<li><p> Number between 0 and 1
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a direct search function that is used in the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>
<p>Francisco Aragón Royón
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a direct search process
## Classification problem


# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the direct search function with threshold
st_direct_search &lt;- selectThreshold()
# Performs the direct search process directly (parameters: dataset, target variable and evaluator)
st_direct_search(iris, 'Species', filter_evaluator) 

## End(Not run)
</code></pre>

<hr>
<h2 id='selectThresholdRange'>Select threshold range</h2><span id='topic+selectThresholdRange'></span>

<h3>Description</h3>

<p>Generates a direct search function that selects the features whose evaluation is over a threshold, where this threshold is given as: (((min - max) * p.threshold) + max)(The features evaluation is individual). This function is called internally within the <code><a href="#topic+directSearchAlgorithm">directSearchAlgorithm</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selectThresholdRange(p.threshold = 0.8)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selectThresholdRange_+3A_p.threshold">p.threshold</code></td>
<td>

<ul>
<li><p> Number between 0 and 1
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a direct search function that is used in the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>
<p>Francisco Aragón Royón
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a direct search process
## Classification problem


# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the direct search function with threshold range
str_direct_search &lt;- selectThresholdRange()
# Performs the direct search process directly (parameters: dataset, target variable and evaluator)
str_direct_search(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='sequentialBackwardSelection'>Sequential Backward Selection</h2><span id='topic+sequentialBackwardSelection'></span>

<h3>Description</h3>

<p>Generates a search function based on sequential backward selection. This function is called internally within the <code><a href="#topic+searchAlgorithm">searchAlgorithm</a></code> function. The SBS method (Marill and Green 1963-02) starts with all the features and removes a single feature at each step with a view to improving the evaluation of the set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sequentialBackwardSelection(stopCriterion = -1, stop = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sequentialBackwardSelection_+3A_stopcriterion">stopCriterion</code></td>
<td>
<p>Define a maximum number of iterations. Disabled if the value is -1 (default: -1 )</p>
</td></tr>
<tr><td><code id="sequentialBackwardSelection_+3A_stop">stop</code></td>
<td>
<p>If true, the function stops if next iteration does not improve current results (default: FALSE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a search function that is used to guide the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>
<p>Alfonso Jiménez-Vílchez
</p>
<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>Marill T, Green D (1963-02).
&ldquo;On the effectiveness of receptors in recognition systems.&rdquo;
<em>Information Theory, IEEE Transactions on</em>, <b>9</b>, 11&ndash;17.
doi: <a href="https://doi.org/10.1109/TIT.1963.1057810">10.1109/TIT.1963.1057810</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a search process in a feature space
## Classification problem

# Generates the filter evaluation function with sbs
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the search function
sbs_search &lt;- sequentialBackwardSelection()
# Performs the search process directly (parameters: dataset, target variable and evaluator)
sbs_search(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='sequentialFloatingBackwardSelection'>Sequential Floating Backward Selection</h2><span id='topic+sequentialFloatingBackwardSelection'></span>

<h3>Description</h3>

<p>Generates a search function based on sequential floating backward selection. This function is called internally within the <code><a href="#topic+searchAlgorithm">searchAlgorithm</a></code> function. The sfbs method (Pudil et al. 1994) starts with all the features and removes a single feature at each step with a view to improving the evaluation of the set. In addition, it checks whether adding any of the removed features, improve the value of the set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sequentialFloatingBackwardSelection()
</code></pre>


<h3>Value</h3>

<p>Returns a search function that is used to guide the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>
<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>Pudil P, Novovičová J, Kittler J (1994).
&ldquo;Floating search methods in feature selection.&rdquo;
<em>Pattern recognition letters</em>, <b>15</b>, 1119&ndash;1125.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a search process in a feature space
## Classification problem

# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the search function with sfbs
sfbs_search &lt;- sequentialFloatingBackwardSelection()
# Performs the search process directly (parameters: dataset, target variable and evaluator)
sfbs_search(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='sequentialFloatingForwardSelection'>Sequential Floating Forward Selection</h2><span id='topic+sequentialFloatingForwardSelection'></span>

<h3>Description</h3>

<p>Generates a search function based on sequential floating forward selection. This function is called internally within the <code><a href="#topic+searchAlgorithm">searchAlgorithm</a></code> function. The sffs method (Pudil et al. 1994) starts with an empty set of features and add a single feature at each step with a view to improving the evaluation of the set. In addition, it checks whether removing any of the included features, improve the value of the set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sequentialFloatingForwardSelection()
</code></pre>


<h3>Value</h3>

<p>Returns a search function that is used to guide the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>
<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>Pudil P, Novovičová J, Kittler J (1994).
&ldquo;Floating search methods in feature selection.&rdquo;
<em>Pattern recognition letters</em>, <b>15</b>, 1119&ndash;1125.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a search process in a feature space
## Classification problem

# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the search function with sffs
sffs_search &lt;- sequentialFloatingForwardSelection()
# Performs the search process directly (parameters: dataset, target variable and evaluator)
sffs_search(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='sequentialForwardSelection'>Sequential Forward Selection</h2><span id='topic+sequentialForwardSelection'></span>

<h3>Description</h3>

<p>Generates a search function based on sequential forward selection. This function is called internally within the <code><a href="#topic+searchAlgorithm">searchAlgorithm</a></code> function. The SFS method (Whitney 1971-sep) starts with an empty set of features and add a single feature at each step with a view to improving the evaluation of the set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sequentialForwardSelection(stopCriterion = -1, stop = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sequentialForwardSelection_+3A_stopcriterion">stopCriterion</code></td>
<td>
<p>Define a maximum number of iterations. Disabled if the value is -1 (default: -1 )</p>
</td></tr>
<tr><td><code id="sequentialForwardSelection_+3A_stop">stop</code></td>
<td>
<p>If true, the function stops if next iteration does not improve current results (default: FALSE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a search function that is used to guide the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>
<p>Alfonso Jiménez-Vílchez
</p>
<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>Whitney AW (1971-sep).
&ldquo;A Direct Method of Nonparametric Measurement Selection.&rdquo;
<em>IEEE Trans. Comput.</em>, <b>20</b>, 1100&ndash;1103.
doi: <a href="https://doi.org/10.1109/T-C.1971.223410">10.1109/T-C.1971.223410</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a search process in a feature space
## Classification problem

# Generates the filter evaluation function with sfs
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the search function
sfs_search &lt;- sequentialForwardSelection()
# Performs the search process directly (parameters: dataset, target variable and evaluator)
sfs_search(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='simulatedAnnealing'>Simulated Annealing</h2><span id='topic+simulatedAnnealing'></span>

<h3>Description</h3>

<p>Generates a search function based on simulated annealing. This function is called internally within the <code><a href="#topic+searchAlgorithm">searchAlgorithm</a></code> function. The simulatedAnnealing method (Kirkpatrick et al. 1983) starts with a certain set of features and in each iteration modifies an element of the previous feature vector and decreases the temperature. If the energy of the new feature vector is better than that of the old vector, it is accepted and moved towards it, otherwise it is moved towards the new vector according to an acceptance probability.  The algorithm ends when the minimum temperature has been reached. Additionally, a number of internal iterations can be performed within each iteration of the algorithm. In this case, the same temperature value of the outer iteration is used for the inner iterations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulatedAnnealing(
  start = NULL,
  temperature = 1,
  temperature_min = 0.01,
  reduction = 0.6,
  innerIter = 1,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulatedAnnealing_+3A_start">start</code></td>
<td>
<p>Binary vector with the set of initial features</p>
</td></tr>
<tr><td><code id="simulatedAnnealing_+3A_temperature">temperature</code></td>
<td>
<p>Temperature initial</p>
</td></tr>
<tr><td><code id="simulatedAnnealing_+3A_temperature_min">temperature_min</code></td>
<td>
<p>Temperature to stops in the outer loop</p>
</td></tr>
<tr><td><code id="simulatedAnnealing_+3A_reduction">reduction</code></td>
<td>
<p>Temperature reduction in the outer loop</p>
</td></tr>
<tr><td><code id="simulatedAnnealing_+3A_inneriter">innerIter</code></td>
<td>
<p>Number of iterations of inner loop. By default no inner iterations are established</p>
</td></tr>
<tr><td><code id="simulatedAnnealing_+3A_verbose">verbose</code></td>
<td>
<p>Print the partial results in each iteration</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a search function that is used to guide the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>Kirkpatrick S, Gelatt CD, Vecchi MP (1983).
&ldquo;Optimization by simulated annealing.&rdquo;
<em>SCIENCE</em>, <b>220</b>, 671&ndash;680.
doi: <a href="https://doi.org/10.1126/science.220.4598.671">10.1126/science.220.4598.671</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a search process in a feature space
## Classification problem

# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the search function with Simulated annealing
sa_search &lt;- simulatedAnnealing()
# Performs the search process directly (parameters: dataset, target variable and evaluator)
sa_search(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='symmetricalUncertain'>Symmetrical uncertain measure</h2><span id='topic+symmetricalUncertain'></span>

<h3>Description</h3>

<p>Generates an evaluation function that calculates the symmetrical uncertain value (Witten and Frank 2005), using the information theory (set measure). This function is called internally within the <code><a href="#topic+filterEvaluator">filterEvaluator</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>symmetricalUncertain()
</code></pre>


<h3>Value</h3>

<p>Returns a function that is used to generate an evaluation set measure using the symmetrical uncertain value for the selected features.
</p>


<h3>Author(s)</h3>

<p>Adan M. Rodriguez
</p>


<h3>References</h3>

<p>Witten IH, Frank E (2005).
<em>Data Mining: Practical Machine Learning Tools and Techniques</em>, 2nd edition.
Morgan Kaufmann, San Francisco.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly to evaluate a set of features
## Classification problem

# A discrete dataset is used (in this case we use only several discrete columns)
adult &lt;- adult[,c(4,9,10,15)]

# Generate the evaluation function with Symmetrical Uncertain
su_evaluator &lt;- symmetricalUncertain()
# Evaluate the features (parameters: dataset, target variable and features)
su_evaluator(adult,'income',c('race','sex','education'))

## End(Not run)
</code></pre>

<hr>
<h2 id='tabu'>Tabu Search</h2><span id='topic+tabu'></span>

<h3>Description</h3>

<p>Generates a search function based on the tabu search. This function is called internally within the <code><a href="#topic+searchAlgorithm">searchAlgorithm</a></code> function. The Tabu Search(Glover 1986-may) (Glover 1989) method starts with a certain set of features and in each iteration it searches among its neighbors to advance towards a better solution. The method has a memory (tabu list) that prevents returning to recently visited neighbors. The method ends when a certain number of iterations are performed, or when a certain number of iterations are performed without improvement, or when there are no possible neighbors. Once the method is finished, an intensification phase can be carried out that begins in the space of the best solutions found, or a diversification phase can be carried out in which solutions not previously visited are explored.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tabu(
  start = NULL,
  numNeigh = NULL,
  tamTabuList = 5,
  iter = 100,
  iterNoImprovement = NULL,
  intensification = NULL,
  iterIntensification = 50,
  interPercentaje = 75,
  tamIntermediateMemory = 5,
  diversification = NULL,
  iterDiversification = 50,
  forgetTabuList = TRUE,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tabu_+3A_start">start</code></td>
<td>
<p>Binary vector with the set of initial features</p>
</td></tr>
<tr><td><code id="tabu_+3A_numneigh">numNeigh</code></td>
<td>
<p>The number of neighbor to consider in each iteration. By default: all posibles. It is important to note that a high value of this parameter considerably increases the computation time.</p>
</td></tr>
<tr><td><code id="tabu_+3A_tamtabulist">tamTabuList</code></td>
<td>
<p>The size of the tabu list. By default: 5</p>
</td></tr>
<tr><td><code id="tabu_+3A_iter">iter</code></td>
<td>
<p>The number of iterations of the algorithm. By default: 100</p>
</td></tr>
<tr><td><code id="tabu_+3A_iternoimprovement">iterNoImprovement</code></td>
<td>
<p>Number of iterations without improvement to start/reset the intensification/diversification phase. By default, it is not taken into account (all iterations are performed)</p>
</td></tr>
<tr><td><code id="tabu_+3A_intensification">intensification</code></td>
<td>
<p>Number of times the intensification phase is applied. None by default</p>
</td></tr>
<tr><td><code id="tabu_+3A_iterintensification">iterIntensification</code></td>
<td>
<p>Number of iterations of the intensification phase</p>
</td></tr>
<tr><td><code id="tabu_+3A_interpercentaje">interPercentaje</code></td>
<td>
<p>Percentage of the most significant features to be taken into account in the intensification phase</p>
</td></tr>
<tr><td><code id="tabu_+3A_tamintermediatememory">tamIntermediateMemory</code></td>
<td>
<p>Number of best solutions saved in the intermediate memory</p>
</td></tr>
<tr><td><code id="tabu_+3A_diversification">diversification</code></td>
<td>
<p>Number of times the diversification phase is applied. None by default</p>
</td></tr>
<tr><td><code id="tabu_+3A_iterdiversification">iterDiversification</code></td>
<td>
<p>Number of iterations of the diversification phase</p>
</td></tr>
<tr><td><code id="tabu_+3A_forgettabulist">forgetTabuList</code></td>
<td>
<p>Forget tabu list for intensification/diversification phases. By default: TRUE</p>
</td></tr>
<tr><td><code id="tabu_+3A_verbose">verbose</code></td>
<td>
<p>Print the partial results in each iteration</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a search function that is used to guide the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>Glover F (1986-may).
&ldquo;Future Paths for Integer Programming and Links to Artificial Intelligence.&rdquo;
<em>Comput. Oper. Res.</em>, <b>13</b>, 533&ndash;549.
doi: <a href="https://doi.org/10.1016/0305-0548(86)90048-1">10.1016/0305-0548(86)90048-1</a>.<br /><br /> Glover F (1989).
&ldquo;Tabu Search—Part I.&rdquo;
<em>ORSA Journal on Computing</em>, <b>1</b>, 190&ndash;206.
doi: <a href="https://doi.org/10.1287/ijoc.1.3.190">10.1287/ijoc.1.3.190</a>, <a href="https://doi.org/10.1287/ijoc.1.3.190">https://doi.org/10.1287/ijoc.1.3.190</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a search process in a feature space
## Classification problem

# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the search function wit Tabu search
ts_search &lt;- tabu()
# Performs the search process directly (parameters: dataset, target variable and evaluator)
ts_search(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='whaleOptimization'>Whale Optimization Algorithm (Binary Whale Optimization Algorithm)</h2><span id='topic+whaleOptimization'></span>

<h3>Description</h3>

<p>Generates a search function based on the whale optimization algorithm. This function is called internally within the <code><a href="#topic+searchAlgorithm">searchAlgorithm</a></code> function. Binary Whale Optimization Algorithm (Kumar and Kumar 2018-Oct-16) is an algorithm that simulates the social behavior of humpback whales. This algorithm employs a binary version of the bubble-net hunting strategy. The algorithm starts with an initial population of individuals, and in each iteration updates the individuals according to several possible actions: Encircling prey, Bubble-net attacking or Search for prey
</p>


<h3>Usage</h3>

<pre><code class='language-R'>whaleOptimization(population = 10, iter = 10, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="whaleOptimization_+3A_population">population</code></td>
<td>
<p>The number of whales population</p>
</td></tr>
<tr><td><code id="whaleOptimization_+3A_iter">iter</code></td>
<td>
<p>The number of iterations of the algorithm</p>
</td></tr>
<tr><td><code id="whaleOptimization_+3A_verbose">verbose</code></td>
<td>
<p>Print the partial results in each iteration</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a search function that is used to guide the feature selection process.
</p>


<h3>Author(s)</h3>

<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>Kumar V, Kumar D (2018-Oct-16).
&ldquo;Binary whale optimization algorithm and its application to unit commitment problem.&rdquo;
<em>Neural Computing and Applications</em>.
doi: <a href="https://doi.org/10.1007/s00521-018-3796-3">10.1007/s00521-018-3796-3</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## The direct application of this function is an advanced use that consists of using this 
# function directly and performing a search process in a feature space
## Classification problem

# Generates the filter evaluation function
filter_evaluator &lt;- filterEvaluator('determinationCoefficient')

# Generates the search function with WOA
woa_search &lt;- whaleOptimization()
# Performs the search process directly (parameters: dataset, target variable and evaluator)
woa_search(iris, 'Species', filter_evaluator)

## End(Not run)
</code></pre>

<hr>
<h2 id='wrapperEvaluator'>Wrapper measure generator</h2><span id='topic+wrapperEvaluator'></span>

<h3>Description</h3>

<p>Generates a wrapper function to be used as an evaluator (Kohavi and John 1997) in the feature selection proccess, given a learner algorithm and related customizable parameters (from Jed Wing et al. 2018). More specifically, the result of calling this function is another function that is passed on as a parameter to the <code><a href="#topic+featureSelection">featureSelection</a></code> function. However, you can also run this function directly to generate an evaluation measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wrapperEvaluator(learner, resamplingParams = list(), fittingParams = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wrapperEvaluator_+3A_learner">learner</code></td>
<td>
<p>Learner to be used. The models available are the models available in caret: http://topepo.github.io/caret/available-models.html</p>
</td></tr>
<tr><td><code id="wrapperEvaluator_+3A_resamplingparams">resamplingParams</code></td>
<td>
<p>Control parameters for evaluating the impact of model tuning parameters. The arguments are the same as those of the caret trainControl function. By default an empty list. In this case the default caret values are used for resampling and fitting.</p>
</td></tr>
<tr><td><code id="wrapperEvaluator_+3A_fittingparams">fittingParams</code></td>
<td>
<p>Control parameters for choose the best model across the parameters. The arguments are the same as those of the caret train function (minus the parameters: x, y, form, data, method and trainControl). By default an empty list. In this case the default caret values are used for resampling and fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>generaWrapper
</p>


<h3>Value</h3>

<p>Returns a wrapper function that is used to generate an evaluation measure
</p>


<h3>Author(s)</h3>

<p>Alfonso Jiménez-Vílchez
</p>
<p>Francisco Aragón Royón
</p>


<h3>References</h3>

<p>Kohavi R, John GH (1997).
&ldquo;Wrappers for feature subset selection.&rdquo;
<em>Artificial intelligence</em>, <b>97</b>, 273&ndash;324.<br /><br /> from Jed Wing MKC, Weston S, Williams A, Keefer C, Engelhardt A, Cooper T, Mayer Z, Kenkel B, the R Core Team, Benesty M, Lescarbeau R, Ziem A, Scrucca L, Tang Y, Candan C, Hunt. T (2018).
<em>caret: Classification and Regression Training</em>.
R package version 6.0-80, <a href="https://CRAN.R-project.org/package=caret">https://CRAN.R-project.org/package=caret</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  

## Examples of a wrapper evaluator generation

wrapper_evaluator_1 &lt;- wrapperEvaluator('knn')
wrapper_evaluator_2 &lt;- wrapperEvaluator('mlp')
wrapper_evaluator_3 &lt;- wrapperEvaluator('randomForest')


## Examples of a wrapper evaluator generation (with parameters)

# Values for the caret trainControl function (resampling parameters)
resamplingParams &lt;- list(method = "repeatedcv", repeats = 3)
# Values for the caret train function (fitting parameters)
fittingParams &lt;- list(preProc = c("center", "scale"), metric="Accuracy",
                      tuneGrid = expand.grid(k = c(1:12)))
                      
wrapper_evaluator &lt;- wrapperEvaluator('knn', resamplingParams, fittingParams)


## The direct application of this function is an advanced use that consists of using this 
# function directly to evaluate a set of features
## Classification problem

# Generates the wrapper evaluation function
wrapper_evaluator &lt;- wrapperEvaluator('knn')
# Evaluates features directly (parameters: dataset, target variable and features)
wrapper_evaluator(iris,'Species',c('Sepal.Length','Sepal.Width','Petal.Length','Petal.Width'))

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
