<!DOCTYPE html><html><head><title>Help for package rare</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rare}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#rare-package'><p>Model path for tree-based lasso framework for selecting rare features</p></a></li>
<li><a href='#data.dtm'><p>Document-term matrix for adjectives in TripAdvisor hotel reviews</p></a></li>
<li><a href='#data.hc'><p>Hierarchical clustering tree for adjectives in TripAdvisor data set</p></a></li>
<li><a href='#data.rating'><p>TripAdvisor hotel review ratings</p></a></li>
<li><a href='#find.leaves'><p>Find all descendant leaves of a node in an hclust tree</p></a></li>
<li><a href='#group.plot'><p>Visualize groups by coloring branches and leaves of an hclust tree</p></a></li>
<li><a href='#group.recover'><p>Recover aggregated groups of leaf indices</p></a></li>
<li><a href='#rarefit'><p>Fit the rare feature selection model</p></a></li>
<li><a href='#rarefit.cv'><p>Perform K-fold cross validation</p></a></li>
<li><a href='#rarefit.predict'><p>Make predictions from a rarefit object and a rarefit.cv object</p></a></li>
<li><a href='#tree.matrix'><p>Generate matrix A encoding ancestor-descendant relationships</p>
in an hclust tree</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Linear Model with Tree-Based Lasso Regularization for Rare
Features</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Author:</td>
<td>Xiaohan Yan [aut, cre], Jacob Bien [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Xiaohan Yan &lt;xy257@cornell.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implementation of an alternating direction method of multipliers
    algorithm for fitting a linear model with tree-based lasso regularization, 
    which is proposed in Algorithm 1 of Yan and Bien (2018) &lt;<a href="https://doi.org/10.48550/arXiv.1803.06675">doi:10.48550/arXiv.1803.06675</a>&gt;. 
    The package allows efficient model fitting on the entire 2-dimensional 
    regularization path for large datasets. The complete set of functions 
    also makes the entire process of tuning regularization parameters and 
    visualizing results hassle-free.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.1)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Matrix, glmnet, Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, dendextend, rmarkdown</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.0</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/yanxht/rare">https://github.com/yanxht/rare</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/yanxht/rare/issues">https://github.com/yanxht/rare/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-08-03 14:45:47 UTC; xy257</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-08-03 16:50:09 UTC</td>
</tr>
</table>
<hr>
<h2 id='rare-package'>Model path for tree-based lasso framework for selecting rare features</h2><span id='topic+rare-package'></span>

<h3>Description</h3>

<p>The package fits the linear model with tree-based lasso regularization proposed in
Yan and Bien (2018) using alternating direction method of multipliers
(ADMM). The ADMM algorithm is proposed in Algorithm 1 of the same paper.
The package also provides tools for tuning regularization
parameters, making predictions from the fitted model and visualizing recovered
groups of the covariates in a dendrogram.
</p>


<h3>Details</h3>

<p>Its main functions are <code><a href="#topic+rarefit">rarefit</a></code>, <code><a href="#topic+rarefit.cv">rarefit.cv</a></code>,
<code><a href="#topic+rarefit.predict">rarefit.predict</a></code>, <code><a href="#topic+group.recover">group.recover</a></code> and
<code><a href="#topic+group.plot">group.plot</a></code>.
</p>


<h3>Author(s)</h3>

<p>Xiaohan Yan <a href="mailto:xy257@cornell.edu">xy257@cornell.edu</a>, Jacob Bien
</p>


<h3>References</h3>

<p>Yan, X. and Bien, J. (2018) <em>Rare Feature Selection in High Dimensions</em>, <a href="https://arxiv.org/abs/1803.06675">https://arxiv.org/abs/1803.06675</a>.
</p>

<hr>
<h2 id='data.dtm'>Document-term matrix for adjectives in TripAdvisor hotel reviews</h2><span id='topic+data.dtm'></span>

<h3>Description</h3>

<p>A 500-by-200 document-term matrix for 200 adjectives appearing in 500 TripAdvisor reviews.
The document-term matrix is in sparse format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data.dtm
</code></pre>


<h3>Format</h3>

<p>An object of class <code>dgCMatrix</code> with 500 rows and 200 columns.</p>


<h3>See Also</h3>

<p><code><a href="#topic+data.rating">data.rating</a></code>, <code><a href="#topic+data.hc">data.hc</a></code>.
</p>

<hr>
<h2 id='data.hc'>Hierarchical clustering tree for adjectives in TripAdvisor data set</h2><span id='topic+data.hc'></span>

<h3>Description</h3>

<p>An <code>hclust</code> tree for the 200 adjectives appearing in the TripAdvisor reviews.
The tree was generated with 100-dimensional word embeddings pre-trained by GloVe
(Pennington et al., 2014) on Gigaword5 and Wikipedia2014 corpora for the adjectives.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data.hc
</code></pre>


<h3>Format</h3>

<p>An object of class <code>hclust</code> of length 7.</p>


<h3>Source</h3>

<p>Embeddings available at <a href="http://nlp.stanford.edu/data/glove.6B.zip">http://nlp.stanford.edu/data/glove.6B.zip</a>
</p>


<h3>References</h3>

<p>Pennington, J., Socher, R., and Manning, C. D. (2014).
Glove: Global vectors for word representation.
<em>In Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 1532â€“1543.
</p>

<hr>
<h2 id='data.rating'>TripAdvisor hotel review ratings</h2><span id='topic+data.rating'></span>

<h3>Description</h3>

<p>A length-500 TripAdvisor review ratings on the scale 1 to 5.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data.rating
</code></pre>


<h3>Format</h3>

<p>An object of class <code>integer</code> of length 500.</p>


<h3>Source</h3>

<p>TripAdvisor Data Set used in <a href="https://www.cs.virginia.edu/~hw5x/paper/rp166f-wang.pdf">https://www.cs.virginia.edu/~hw5x/paper/rp166f-wang.pdf</a>
</p>

<hr>
<h2 id='find.leaves'>Find all descendant leaves of a node in an hclust tree</h2><span id='topic+find.leaves'></span>

<h3>Description</h3>

<p>The function recursively finds all leaves that are descendants of a
node in an <code>hclust</code> tree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find.leaves(ind, merge)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find.leaves_+3A_ind">ind</code></td>
<td>
<p>Index of the tree node. For an <code>hclust</code> tree
of <code>p</code> leaves, -<code>j</code> denotes the <code>j</code>th leaf and <code>k</code>
denotes the interior node formed at the <code>k</code>th merging in constructing
the tree. The range of <code>ind</code> is
{-1, ..., -p, 1,..., p-1} where <code>p-1</code> is the number of interior nodes.</p>
</td></tr>
<tr><td><code id="find.leaves_+3A_merge">merge</code></td>
<td>
<p>A (<code>p-1</code>)-by-2 matrix that encodes the order of
mergings in constructing the tree. <code>merge</code> uses the same notation for
nodes and mergings in an <code>hclust</code> object.
See <code><a href="stats.html#topic+hclust">hclust</a></code> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a sequence of indices for descendant leaves
in the leaf set {1, ..., p}. Unlike the notation used in
<code>ind</code>, we use positive integers to denote leaves here.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
hc &lt;- hclust(dist(USArrests), "ave")
# Descendant leaves of the 10th leaf (should be iteself)
find.leaves(-10, hc$merge)

# Descendant leaves of the 10th interior node
find.leaves(10, hc$merge)

# Descendant leaves of the root (should be all leaves)
ind_root &lt;- nrow(hc$merge)
all.equal(find.leaves(ind_root, hc$merge), hc$order)

## End(Not run)

</code></pre>

<hr>
<h2 id='group.plot'>Visualize groups by coloring branches and leaves of an hclust tree</h2><span id='topic+group.plot'></span>

<h3>Description</h3>

<p>The function plots an <code>hclust</code> tree with branches and leaves colored
based on group membership. The groups span the covariate indices {1, ..., <code>nvars</code>}.
Covariates from the same group share equal coefficient (<code>beta</code>), and sibling
groups have different coefficients. The function determines groups based on
the sparsity in <code>gamma</code>. In an <code>hclust</code> tree with <code>beta[i]</code> on the
<code>i</code>th leaf, the branch and leaf are colored in blue, red or gray according to <code>beta[i]</code>
being positive, negative or zero, respectively. The larger the magnitude of <code>beta[i]</code> is,
the darker the color will be. So branches and leaves from the same group will have the
same color.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>group.plot(beta, gamma, A, hc, nbreaks = 20)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="group.plot_+3A_beta">beta</code></td>
<td>
<p>Length-<code>nvars</code> vector of covariate coefficient.</p>
</td></tr>
<tr><td><code id="group.plot_+3A_gamma">gamma</code></td>
<td>
<p>Length-<code>nnodes</code> vector of latent variable coefficient. Note that <code><a href="#topic+rarefit">rarefit</a></code>
returns <code>NA</code> as <code>gamma</code> value when <code>alpha</code> is zero,
in which case our problem becomes the lasso on <code>beta</code>.</p>
</td></tr>
<tr><td><code id="group.plot_+3A_a">A</code></td>
<td>
<p><code>nvars</code>-by-<code>nnodes</code> binary matrix encoding ancestor-descendant relationships
between leaves and nodes in the tree.</p>
</td></tr>
<tr><td><code id="group.plot_+3A_hc">hc</code></td>
<td>
<p>An <code>hclust</code> tree of <code>nvars</code> leaves where each leaf corresponds to a covariate.</p>
</td></tr>
<tr><td><code id="group.plot_+3A_nbreaks">nbreaks</code></td>
<td>
<p>Number of breaks in binning <code>beta</code> elements (positive part and negative part
are done separately). Each bin is associated with a color based on the magnitude and
positivity/negativity of <code>beta</code> elements in the bin.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# See vignette for more details.
set.seed(100)
ts &lt;- sample(1:length(data.rating), 400) # Train set indices
# Fit the model on train set
ourfit &lt;- rarefit(y = data.rating[ts], X = data.dtm[ts, ], hc = data.hc, lam.min.ratio = 1e-6,
                  nlam = 20, nalpha = 10, rho = 0.01, eps1 = 1e-5, eps2 = 1e-5, maxite = 1e4)
# Cross validation
ourfit.cv &lt;- rarefit.cv(ourfit, y = data.rating[ts], X = data.dtm[ts, ],
                        rho = 0.01, eps1 = 1e-5, eps2 = 1e-5, maxite = 1e4)
# Visualize the groups at optimal beta and gamma
ibest.lambda &lt;- ourfit.cv$ibest[1]
ibest.alpha &lt;- ourfit.cv$ibest[2]
beta.opt &lt;- ourfit$beta[[ibest.alpha]][, ibest.lambda]
gamma.opt &lt;- ourfit$gamma[[ibest.alpha]][, ibest.lambda] # works if ibest.alpha &gt; 1
# Visualize the groups at optimal beta and gamma
group.plot(beta.opt, gamma.opt, ourfit$A, data.hc)

## End(Not run)

</code></pre>

<hr>
<h2 id='group.recover'>Recover aggregated groups of leaf indices</h2><span id='topic+group.recover'></span>

<h3>Description</h3>

<p>The function finds aggregated groups of leaf indices by traversing non-zero
<code>gamma</code> elements and finding descendant leaves at each <code>gamma</code> element. In our problem,
<code>gamma</code> are latent variables corresponding to tree nodes. The order
of the traversal is post-order, i.e., a node is visited after its descendants.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>group.recover(gamma, A, postorder = seq(ncol(A)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="group.recover_+3A_gamma">gamma</code></td>
<td>
<p>Length-<code>nnodes</code> latent variable coefficients. Note that <code><a href="#topic+rarefit">rarefit</a></code>
returns <code>NA</code> as <code>gamma</code> value when <code>alpha</code> is zero,
in which case our problem becomes the lasso on <code>beta</code>.</p>
</td></tr>
<tr><td><code id="group.recover_+3A_a">A</code></td>
<td>
<p><code>nvars</code>-by-<code>nnodes</code> binary matrix encoding ancestor-descendant relationships
between leaves and nodes in the tree.</p>
</td></tr>
<tr><td><code id="group.recover_+3A_postorder">postorder</code></td>
<td>
<p>Length-<code>nnodes</code> integer vector encoding post-order traversal of the tree
nodes such that <code>seq(nnodes)[postorder]</code> ensures a node appear after its descendants.
Default is <code>seq(nnodes)</code>, which gives post-order when <code>A</code> is generated using <code><a href="#topic+tree.matrix">tree.matrix</a></code>
for an <code>hclust</code> tree.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list of recovered groups of leaf indices.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# See vignette for more details.
set.seed(100)
ts &lt;- sample(1:length(data.rating), 400) # Train set indices
# Fit the model on train set
ourfit &lt;- rarefit(y = data.rating[ts], X = data.dtm[ts, ], hc = data.hc, lam.min.ratio = 1e-6,
                  nlam = 20, nalpha = 10, rho = 0.01, eps1 = 1e-5, eps2 = 1e-5, maxite = 1e4)
# Cross validation
ourfit.cv &lt;- rarefit.cv(ourfit, y = data.rating[ts], X = data.dtm[ts, ],
                        rho = 0.01, eps1 = 1e-5, eps2 = 1e-5, maxite = 1e4)
# Group recovered at optimal beta and gamma
ibest.lambda &lt;- ourfit.cv$ibest[1]
ibest.alpha &lt;- ourfit.cv$ibest[2]
gamma.opt &lt;- ourfit$gamma[[ibest.alpha]][, ibest.lambda] # works if ibest.alpha &gt; 1
groups.opt &lt;- group.recover(gamma.opt, ourfit$A)

## End(Not run)

</code></pre>

<hr>
<h2 id='rarefit'>Fit the rare feature selection model</h2><span id='topic+rarefit'></span>

<h3>Description</h3>

<p>Fit the rare feature selection model proposed in Yan and Bien (2018):
</p>
<p style="text-align: center;"><code class="reqn">min_{\beta, \gamma} 0.5 * ||y - X\beta - \beta_01_n||_2^2 +
\lambda * (\alpha * ||\gamma_{-root}||_1 + (1-\alpha) * ||\beta||_1)</code>
</p>

<p>using an alternating direction method of multipliers (ADMM) algorithm
described in Algorithm 1 of the same paper.
The regularization path is computed over a two-dimensional grid of
regularization parameters: <code>lambda</code> and <code>alpha</code>. Of the two,
<code>lambda</code> controls the overall amount of regularization, and <code>alpha</code>
controls the tradeoff between sparsity and fusion of <code class="reqn">\beta</code> (larger <code>alpha</code>
induces more fusion in <code class="reqn">\beta</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rarefit(y, X, A = NULL, Q = NULL, hc, intercept = T, lambda = NULL,
  alpha = NULL, nlam = 50, lam.min.ratio = 1e-04, nalpha = 10,
  rho = 0.01, eps1 = 1e-06, eps2 = 1e-05, maxite = 1e+06)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rarefit_+3A_y">y</code></td>
<td>
<p>Length-<code>nobs</code> response variable.</p>
</td></tr>
<tr><td><code id="rarefit_+3A_x">X</code></td>
<td>
<p><code>nobs</code>-by-<code>nvars</code> input matrix:
each row is an observation vector and each column stores a count covariate.</p>
</td></tr>
<tr><td><code id="rarefit_+3A_a">A</code></td>
<td>
<p><code>nvars</code>-by-<code>nnodes</code> binary matrix encoding ancestor-descendant relationships
between leaves and tree nodes, where <code>nnodes</code> is the total number of tree nodes.
<code>A[i,j]</code> is 1 if the <code>i</code>th leaf is a descendant of the <code>j</code>th
node in the tree, and 0 otherwise. <code>A</code> should be in sparse matrix format
(inherit from class <code><a href="Matrix.html#topic+sparseMatrix">sparseMatrix</a></code> as in package <code>Matrix</code>).
When <code>A</code> is <code>NULL</code>, the function will learn <code>A</code> from <code>hc</code>.</p>
</td></tr>
<tr><td><code id="rarefit_+3A_q">Q</code></td>
<td>
<p><code>(nvars+nnodes)</code>-by-<code>nnodes</code> matrix with columns forming an orthonormal
basis for the null space of <code class="reqn">[I_nvars:-A]</code>. When <code>Q</code> is <code>NULL</code>, the function will learn
<code>Q</code> using the singular value decomposition.</p>
</td></tr>
<tr><td><code id="rarefit_+3A_hc">hc</code></td>
<td>
<p>An <code>hclust</code> tree of <code>nvars</code> leaves where each leaf corresponds
to a covariate. If the tree is not an <code>hclust</code> object, user needs to provide the matrix <code>A</code> instead.</p>
</td></tr>
<tr><td><code id="rarefit_+3A_intercept">intercept</code></td>
<td>
<p>Whether intercept be fitted (default = TRUE) or set to zero (FALSE).</p>
</td></tr>
<tr><td><code id="rarefit_+3A_lambda">lambda</code></td>
<td>
<p>A user-supplied <code>lambda</code> sequence. Typical usage is to
have the program compute its own <code>lambda</code> sequence based on
<code>nlam</code> and <code>lam.min.ratio</code>.</p>
</td></tr>
<tr><td><code id="rarefit_+3A_alpha">alpha</code></td>
<td>
<p>A user-supplied <code>alpha</code> sequence. If letting the program
compute its own <code>alpha</code> sequence, a length-<code>nalpha</code> sequence of
equally-spaced <code>alpha</code> values between 0 and 1 will be used. In practice,
user may want to provide a more fine <code>alpha</code> sequence to tune
the model to its best performance (e.g., <code>alpha = c(1-exp(seq(0, log(1e-2), len = nalpha - 1)), 1)</code>).</p>
</td></tr>
<tr><td><code id="rarefit_+3A_nlam">nlam</code></td>
<td>
<p>Number of <code>lambda</code> values (default = 50).</p>
</td></tr>
<tr><td><code id="rarefit_+3A_lam.min.ratio">lam.min.ratio</code></td>
<td>
<p>Smallest value for <code>lambda</code>, as a fraction of
<code>lambda.max</code> (i.e., the smallest value for which all coefficients are
zero). The default value is <code>1e-4</code>.</p>
</td></tr>
<tr><td><code id="rarefit_+3A_nalpha">nalpha</code></td>
<td>
<p>Number of <code>alpha</code> values (default = 10).</p>
</td></tr>
<tr><td><code id="rarefit_+3A_rho">rho</code></td>
<td>
<p>Penalty parameter for the quadratic penalty in the ADMM algorithm.
The default value is <code>1e-2</code>.</p>
</td></tr>
<tr><td><code id="rarefit_+3A_eps1">eps1</code></td>
<td>
<p>Convergence threshold in terms of the absolute tolerance level
for the ADMMM algorithm. The default value is <code>1e-6</code>.</p>
</td></tr>
<tr><td><code id="rarefit_+3A_eps2">eps2</code></td>
<td>
<p>Convergence threshold in terms of the relative tolerance level
for the ADMM algorithm. The default value is <code>1e-5</code>.</p>
</td></tr>
<tr><td><code id="rarefit_+3A_maxite">maxite</code></td>
<td>
<p>Maximum number of passes over the data for every pair of
(<code>lambda</code>, <code>alpha</code>). The default value is <code>1e6</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function splits model fitting path by <code>alpha</code>. At each <code>alpha</code> value,
the model is fit on the entire sequence of <code>lambda</code> with warm start. We recommend
including an intercept (by setting <code>intercept=T</code>) unless the input data have been
centered.
</p>


<h3>Value</h3>

<p>Returns regression coefficients for <code>beta</code> and <code>gamma</code> and
intercept <code>beta0</code>. We use a <em>matrix-nested-within-list</em> structure to store the coefficients: each list
item corresponds to an <code>alpha</code> value; matrix (or vector) in that list item stores
coefficients at various <code>lambda</code> values by columns (or entries).
</p>
<table>
<tr><td><code>beta0</code></td>
<td>
<p>Length-<code>nalpha</code> list with each item storing
intercept across various <code>lambda</code> in a vector: <code>beta0[[j]][i]</code>
is intercept fitted at (<code>lambda[i]</code>, <code>alpha[j]</code>).
If <code>intercept = FALSE</code>, <code>beta0</code> is <code>NULL</code>.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>Length-<code>nalpha</code> list with each item storing
<code>beta</code> coefficient at various <code>lambda</code> in columns of a <code>nvars</code>-by-<code>nlam</code> matrix:
<code>beta[[j]][, i]</code> is <code>beta</code> coeffcient fitted at (<code>lambda[i]</code>, <code>alpha[j]</code>).</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p>Length-<code>nalpha</code> list with each item storing
<code>gamma</code> coefficient at various <code>lambda</code> in columns of a <code>nnodes</code>-by-<code>nlam</code> matrix:
<code>gamma[[j]][, i]</code> is <code>gamma</code> coeffcient vector fitted at (<code>lambda[i]</code>, <code>alpha[j]</code>).
If <code>alpha[j] = 0</code>, the problem becomes the lasso on <code>beta</code> and is solved
with <code><a href="glmnet.html#topic+glmnet">glmnet</a></code> on <code>beta</code>, in which case <code>gamma[[j]] = NA</code>.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>Sequence of <code>lambda</code> values used in model fit.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Sequence of <code>alpha</code> values used in model fit.</p>
</td></tr>
<tr><td><code>A</code></td>
<td>
<p>Binary matrix encoding ancestor-descendant relationship between leaves and nodes in the tree.</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>Matrix with columns forming an orthonormal basis for the null space of <code class="reqn">[I_nvars:-A]</code>.</p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>Whether an intercept is included in model fit.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Yan, X. and Bien, J. (2018) <em>Rare Feature Selection in High Dimensions</em>, <a href="https://arxiv.org/abs/1803.06675">https://arxiv.org/abs/1803.06675</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rarefit.cv">rarefit.cv</a></code>, <code><a href="#topic+rarefit.predict">rarefit.predict</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# See vignette for more details.
set.seed(100)
ts &lt;- sample(1:length(data.rating), 400) # Train set indices
# Fit the model on train set
ourfit &lt;- rarefit(y = data.rating[ts], X = data.dtm[ts, ], hc = data.hc, lam.min.ratio = 1e-6,
                  nlam = 20, nalpha = 10, rho = 0.01, eps1 = 1e-5, eps2 = 1e-5, maxite = 1e4)

## End(Not run)

</code></pre>

<hr>
<h2 id='rarefit.cv'>Perform K-fold cross validation</h2><span id='topic+rarefit.cv'></span>

<h3>Description</h3>

<p>The function does K-fold cross validaton (CV) to choose an optimal pair of (<code>lambda</code>, <code>alpha</code>)
on which the model performs best according to the chosen error metric: mean squared error
or mean absolute error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rarefit.cv(fitObj, y, X, errtype = "mean-squared-error", nfolds = 5,
  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rarefit.cv_+3A_fitobj">fitObj</code></td>
<td>
<p>Output of <code>rarefit</code></p>
</td></tr>
<tr><td><code id="rarefit.cv_+3A_y">y</code></td>
<td>
<p>Response variable.</p>
</td></tr>
<tr><td><code id="rarefit.cv_+3A_x">X</code></td>
<td>
<p><code>nobs</code>-by-<code>nvars</code> input matrix:
each row is an observation vector and each column stores
a count covariate.</p>
</td></tr>
<tr><td><code id="rarefit.cv_+3A_errtype">errtype</code></td>
<td>
<p>Type of error metric used in cross validation.
Available choices are <em>mean-squared-error</em> (default)
and <em>mean-absolute-error</em>.</p>
</td></tr>
<tr><td><code id="rarefit.cv_+3A_nfolds">nfolds</code></td>
<td>
<p>Number of folds (default is 5)</p>
</td></tr>
<tr><td><code id="rarefit.cv_+3A_...">...</code></td>
<td>
<p>Other arguments that can be passed to <code>rarefit</code></p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>folds</code></td>
<td>
<p>A length-<code>nfolds</code> list with the kth element being elements in the <code>k</code>th fold.</p>
</td></tr>
<tr><td><code>errs</code></td>
<td>
<p>A <code>nlam</code>-by-<code>nalpha</code>-by-<code>nfolds</code> 3-dimensional array of errors.
<code>errs[i,j,k]</code> is error incurred in using <code>lambda[i]</code> and <code>alpha[j]</code> on the <code>k</code>th fold.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>A <code>nlam</code>-by-<code>nalpha</code> matrix for storing CV error (i.e., mean error across folds).
<code>m[i,j]</code> is CV error incurred in using <code>lambda[i]</code> and <code>alpha[j]</code>.</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>A <code>nlam</code>-by-<code>nalpha</code> matrix for storing standard error across folds.
<code>se[i,j]</code> is standard error incurred in using <code>lambda[i]</code> and <code>alpha[j]</code>.</p>
</td></tr>
<tr><td><code>ibest</code></td>
<td>
<p>Indices of pair of (<code>lambda</code>, <code>alpha</code>) minimizing CV error.</p>
</td></tr>
<tr><td><code>lambda.best</code></td>
<td>
<p>Value of <code>lambda</code> minimizing CV error.</p>
</td></tr>
<tr><td><code>alpha.best</code></td>
<td>
<p>Value of <code>alpha</code> minimizing CV error.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+rarefit">rarefit</a></code>, <code><a href="#topic+rarefit.predict">rarefit.predict</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# See vignette for more details.
set.seed(100)
ts &lt;- sample(1:length(data.rating), 400) # Train set indices
# Fit the model on train set
ourfit &lt;- rarefit(y = data.rating[ts], X = data.dtm[ts, ], hc = data.hc, lam.min.ratio = 1e-6,
                  nlam = 20, nalpha = 10, rho = 0.01, eps1 = 1e-5, eps2 = 1e-5, maxite = 1e4)
# Cross validation
ourfit.cv &lt;- rarefit.cv(ourfit, y = data.rating[ts], X = data.dtm[ts, ],
                        rho = 0.01, eps1 = 1e-5, eps2 = 1e-5, maxite = 1e4)

## End(Not run)

</code></pre>

<hr>
<h2 id='rarefit.predict'>Make predictions from a rarefit object and a rarefit.cv object</h2><span id='topic+rarefit.predict'></span>

<h3>Description</h3>

<p>The function makes predictions using a <code>rarefit</code> object at optimal
(<code>lambda</code>, <code>alpha</code>) chosen by <code>rarefit.cv</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rarefit.predict(fitObj, cvObj, newx)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rarefit.predict_+3A_fitobj">fitObj</code></td>
<td>
<p>Output of <code>rarefit</code>.</p>
</td></tr>
<tr><td><code id="rarefit.predict_+3A_cvobj">cvObj</code></td>
<td>
<p>Output of <code>rarefit.cv</code>.</p>
</td></tr>
<tr><td><code id="rarefit.predict_+3A_newx">newx</code></td>
<td>
<p>Matrix of new values for x at which predictions are made.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a sequence of predictions.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rarefit">rarefit</a></code>, <code><a href="#topic+rarefit.cv">rarefit.cv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# See vignette for more details.
set.seed(100)
ts &lt;- sample(1:length(data.rating), 400) # Train set indices
# Fit the model on train set
ourfit &lt;- rarefit(y = data.rating[ts], X = data.dtm[ts, ], hc = data.hc, lam.min.ratio = 1e-6,
                  nlam = 20, nalpha = 10, rho = 0.01, eps1 = 1e-5, eps2 = 1e-5, maxite = 1e4)
# Cross validation
ourfit.cv &lt;- rarefit.cv(ourfit, y = data.rating[ts], X = data.dtm[ts, ],
                        rho = 0.01, eps1 = 1e-5, eps2 = 1e-5, maxite = 1e4)
# Prediction on test set
pred &lt;- rarefit.predict(ourfit, ourfit.cv, data.dtm[-ts, ])
pred.error &lt;- mean((pred - data.rating[-ts])^2)

## End(Not run)

</code></pre>

<hr>
<h2 id='tree.matrix'>Generate matrix A encoding ancestor-descendant relationships
in an hclust tree</h2><span id='topic+tree.matrix'></span>

<h3>Description</h3>

<p>The function generates the binary matrix <code>A</code> defined in Yan
and Bien (2018). The matrix encodes ancestor-descendant relationships between leaves
and tree nodes in an <code>hclust</code> tree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tree.matrix(hc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tree.matrix_+3A_hc">hc</code></td>
<td>
<p>An <code>hclust</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code>nvars</code>-by-<code>nnodes</code> binary matrix <code>A</code>
where <code>nvars</code> is the number of leaves (we associate covariate with leaf),
and <code>nnodes</code> is the number of tree nodes (including both leaves and interior nodes).
For an <code>hclust</code> tree, <code>nnodes</code> = <code>2*nvars-1</code>. <code>A[i,j]</code> is 1 if the <code>i</code>th leaf
is a descendant of the <code>j</code>th node in the tree, and 0 otherwise. <em>By default, we
let the first <code>nvars</code> columns correspond to leaves and the remaining
<code>nvars-1</code> columns correspond to interior nodes.</em>
<code>A</code> is in sparse matrix format (inherit from class
<code><a href="Matrix.html#topic+sparseMatrix">sparseMatrix</a></code> as in package <code>Matrix</code>).
</p>


<h3>References</h3>

<p>Yan, X. and Bien, J. (2018) <em>Rare Feature Selection in High Dimensions</em>, <a href="https://arxiv.org/abs/1803.06675">https://arxiv.org/abs/1803.06675</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+find.leaves">find.leaves</a></code> for finding descendant leaves of a node.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# For a perfect binary tree of depth 2 below
#
#      3
#      /\
#    1    2
#   /\    /\
# -1 -2 -3 -4
#
# A can expressed as the following:
A_true &lt;- cbind(diag(4),
                as.matrix(c(1, 1, 0, 0)),
                as.matrix(c(0, 0, 1, 1)),
                as.matrix(c(1, 1, 1, 1)))
# Now use tree.matrix to generate A
tree0 &lt;- list()
tree0$merge &lt;- matrix(c(-1, -2, -3, -4, 1, 2),
                      ncol = 2, byrow = TRUE)
tree0$labels &lt;- c("leaf1", "leaf2", "leaf3", "leaf4")
A &lt;- tree.matrix(tree0)
all(A_true == as.matrix(A))

# Another example
hc &lt;- hclust(dist(USArrests), "ave")
A &lt;- tree.matrix(hc)

## End(Not run)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
