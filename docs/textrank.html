<!DOCTYPE html><html><head><title>Help for package textrank</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {textrank}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#joboffer'><p>The text of a job offer, annotated with the package udpipe</p></a></li>
<li><a href='#summary.textrank_sentences'><p>Extract the most important sentences which were identified with textrank_sentences</p></a></li>
<li><a href='#textrank_candidates_all'><p>Get all combinations of sentences</p></a></li>
<li><a href='#textrank_candidates_lsh'><p>Use locality-sensitive hashing to get combinations of sentences which contain words which are in the same minhash bucket</p></a></li>
<li><a href='#textrank_jaccard'><p>Calculate the distance between 2 vectors based on the Jaccard distance</p></a></li>
<li><a href='#textrank_keywords'><p>Textrank - extract relevant keywords</p></a></li>
<li><a href='#textrank_sentences'><p>Textrank - extract relevant sentences</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Summarize Text by Ranking Sentences and Finding Keywords</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.1</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jan Wijffels &lt;jwijffels@bnosac.be&gt;</td>
</tr>
<tr>
<td>Author:</td>
<td>Jan Wijffels [aut, cre, cph],
  BNOSAC [cph]</td>
</tr>
<tr>
<td>Description:</td>
<td>The 'textrank' algorithm is an extension of the 'Pagerank' algorithm for text. The algorithm allows to summarize text by calculating how sentences are related to one another. This is done by looking at overlapping terminology used in sentences in order to set up links between sentences. The resulting sentence network is next plugged into the 'Pagerank' algorithm which identifies the most important sentences in your text and ranks them. 
    In a similar way 'textrank' can also be used to extract keywords. A word network is constructed by looking if words are following one another. On top of that network the 'Pagerank' algorithm is applied to extract relevant words after which relevant words which are following one another are combined to get keywords.  
    More information can be found in the paper from Mihalcea, Rada &amp; Tarau, Paul (2004) <a href="https://www.aclweb.org/anthology/W04-3252/">https://www.aclweb.org/anthology/W04-3252/</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.mozilla.org/en-US/MPL/2.0/">MPL-2.0</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/bnosac/textrank">https://github.com/bnosac/textrank</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>utils, data.table (&ge; 1.9.6), igraph, digest</td>
</tr>
<tr>
<td>Suggests:</td>
<td>textreuse, knitr, rmarkdown, udpipe (&ge; 0.2)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-10-12 11:32:11 UTC; Jan</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-10-12 11:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='joboffer'>The text of a job offer, annotated with the package udpipe</h2><span id='topic+joboffer'></span>

<h3>Description</h3>

<p>The text of a job offer, annotated with the package udpipe
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(joboffer)
str(joboffer)
unique(joboffer$sentence)
</code></pre>

<hr>
<h2 id='summary.textrank_sentences'>Extract the most important sentences which were identified with textrank_sentences</h2><span id='topic+summary.textrank_sentences'></span>

<h3>Description</h3>

<p>Extract the most important sentences which were identified by <code><a href="#topic+textrank_sentences">textrank_sentences</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'textrank_sentences'
summary(object, n = 3, keep.sentence.order = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.textrank_sentences_+3A_object">object</code></td>
<td>
<p>an object of class textrank_sentences</p>
</td></tr>
<tr><td><code id="summary.textrank_sentences_+3A_n">n</code></td>
<td>
<p>integer indicating to extract only the top n sentences</p>
</td></tr>
<tr><td><code id="summary.textrank_sentences_+3A_keep.sentence.order">keep.sentence.order</code></td>
<td>
<p>logical indicating to keep the sentence order as provided
in the original <code>data</code> argument of the <code><a href="#topic+textrank_sentences">textrank_sentences</a></code> function
or to order it by the pagerank score. Defaults to FALSE indicating to order by pagerank score.</p>
</td></tr>
<tr><td><code id="summary.textrank_sentences_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector with the top <code>n</code> most important sentences
which were identified by <code><a href="#topic+textrank_sentences">textrank_sentences</a></code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+textrank_sentences">textrank_sentences</a></code>
</p>

<hr>
<h2 id='textrank_candidates_all'>Get all combinations of sentences</h2><span id='topic+textrank_candidates_all'></span>

<h3>Description</h3>

<p>Get all combinations of sentences
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textrank_candidates_all(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textrank_candidates_all_+3A_x">x</code></td>
<td>
<p>a character vector of sentence identifiers</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with 2 columns textrank_id_1 and textrank_id_2 listing up all possible combinations of <code>x</code>.
The columns textrank_id_1 and textrank_id_2 contain identifiers of sentences given in <code>sentence_id</code>.
This data.frame can be used as input in the <code><a href="#topic+textrank_sentences">textrank_sentences</a></code> algorithm.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+textrank_sentences">textrank_sentences</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(udpipe)
data(joboffer)
joboffer$textrank_id &lt;- unique_identifier(joboffer, c("doc_id", "paragraph_id", "sentence_id"))
candidates &lt;- textrank_candidates_all(unique(joboffer$textrank_id))
head(candidates, 50)

</code></pre>

<hr>
<h2 id='textrank_candidates_lsh'>Use locality-sensitive hashing to get combinations of sentences which contain words which are in the same minhash bucket</h2><span id='topic+textrank_candidates_lsh'></span>

<h3>Description</h3>

<p>This functionality is usefull if there are a lot of sentences and most of the sentences have no overlapping
words in there. In order not to compute the jaccard distance among all possible combinations of sentences as is
done by using <code><a href="#topic+textrank_candidates_all">textrank_candidates_all</a></code>, we can reduce the combinations of sentences by using the Minhash algorithm.
This function sets up the combinations of sentences which are in the same Minhash bucket.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textrank_candidates_lsh(x, sentence_id, minhashFUN, bands)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textrank_candidates_lsh_+3A_x">x</code></td>
<td>
<p>a character vector of words or terms</p>
</td></tr>
<tr><td><code id="textrank_candidates_lsh_+3A_sentence_id">sentence_id</code></td>
<td>
<p>a character vector of identifiers of sentences where the words/terms provided in <code>x</code> are part of the sentence.
The length of <code>sentence_id</code> should be the same length of <code>x</code></p>
</td></tr>
<tr><td><code id="textrank_candidates_lsh_+3A_minhashfun">minhashFUN</code></td>
<td>
<p>a function which returns a minhash of a character vector. See the examples or look at <code><a href="textreuse.html#topic+minhash_generator">minhash_generator</a></code></p>
</td></tr>
<tr><td><code id="textrank_candidates_lsh_+3A_bands">bands</code></td>
<td>
<p>integer indicating to break down the minhashes in <code>bands</code> number of bands. Mark that
the number of minhash signatures should always be a multiple of the number of local sensitive hashing bands. See the example</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with 2 columns textrank_id_1 and textrank_id_2 containing identifiers of sentences <code>sentence_id</code>
which contained terms in the same minhash bucket.
This data.frame can be used as input in the <code><a href="#topic+textrank_sentences">textrank_sentences</a></code> algorithm.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+textrank_sentences">textrank_sentences</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textreuse)
library(udpipe)
lsh_probability(h = 1000, b = 500, s = 0.1) # A 10 percent Jaccard overlap will be detected well

minhash &lt;- minhash_generator(n = 1000, seed = 123456789)

data(joboffer)
joboffer$textrank_id &lt;- unique_identifier(joboffer, c("doc_id", "paragraph_id", "sentence_id"))
sentences &lt;- unique(joboffer[, c("textrank_id", "sentence")])
terminology &lt;- subset(joboffer, upos %in% c("NOUN", "ADJ"), select = c("textrank_id", "lemma"))
candidates &lt;- textrank_candidates_lsh(x = terminology$lemma, sentence_id = terminology$textrank_id,
                                      minhashFUN = minhash, bands = 500)
head(candidates)
tr &lt;- textrank_sentences(data = sentences, terminology = terminology,
                         textrank_candidates = candidates)
summary(tr, n = 2)

</code></pre>

<hr>
<h2 id='textrank_jaccard'>Calculate the distance between 2 vectors based on the Jaccard distance</h2><span id='topic+textrank_jaccard'></span>

<h3>Description</h3>

<p>The jaccard distance computes the percentage of terms in the 2 vectors which are overlapping.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textrank_jaccard(termsa, termsb)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textrank_jaccard_+3A_termsa">termsa</code></td>
<td>
<p>a character vector of words</p>
</td></tr>
<tr><td><code id="textrank_jaccard_+3A_termsb">termsb</code></td>
<td>
<p>a character vector of words</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The Jaccard distance distance between the 2 vectors
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sentencea &lt;- c("I", "like", "champaign")
sentenceb &lt;- c("I", "prefer", "choco")
textrank_jaccard(termsa = sentencea, termsb = sentenceb)
</code></pre>

<hr>
<h2 id='textrank_keywords'>Textrank - extract relevant keywords</h2><span id='topic+textrank_keywords'></span>

<h3>Description</h3>

<p>The textrank algorithm allows to find relevant keywords in text.
Where keywords are a combination of words following each other. <br />
</p>
<p>In order to find relevant keywords, the textrank algorithm constructs a word network. This
network is constructed by looking which words follow one another.
A link is set up between two words if they follow one another, the link gets a higher weight if these 2 words occur
more frequenctly next to each other in the text.<br />
On top of the resulting network the 'Pagerank' algorithm is applied to get the importance of each word.
The top 1/3 of all these words are kept and are considered relevant. After this, a keywords table is constructed
by combining the relevant words together if they appear following one another in the text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textrank_keywords(
  x,
  relevant = rep(TRUE, length(x)),
  p = 1/3,
  ngram_max = 5,
  sep = "-"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textrank_keywords_+3A_x">x</code></td>
<td>
<p>a character vector of words.</p>
</td></tr>
<tr><td><code id="textrank_keywords_+3A_relevant">relevant</code></td>
<td>
<p>a logical vector indicating if the word is relevant or not. In the standard textrank
algorithm, this is normally done by doing a Parts of Speech tagging and selecting which of the words are
nouns and adjectives.</p>
</td></tr>
<tr><td><code id="textrank_keywords_+3A_p">p</code></td>
<td>
<p>percentage (between 0 and 1) of relevant words to keep. Defaults to 1/3.
Can also be an integer which than indicates how many words to keep. Specify +Inf if you want to keep all words.</p>
</td></tr>
<tr><td><code id="textrank_keywords_+3A_ngram_max">ngram_max</code></td>
<td>
<p>integer indicating to limit keywords which combine <code>ngram_max</code> combinations of words which follow one another</p>
</td></tr>
<tr><td><code id="textrank_keywords_+3A_sep">sep</code></td>
<td>
<p>character string with the separator to <code>paste</code> the subsequent relevant words together</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class textrank_keywords
which is a list with elements:
</p>

<ul>
<li><p> terms: a character vector of words from the word network with the highest pagerank
</p>
</li>
<li><p> pagerank: the result of a call to <code><a href="igraph.html#topic+page_rank">page_rank</a></code> on the word network
</p>
</li>
<li><p> keywords: the data.frame with keywords containing columns keyword, ngram, freq indicating the keywords found and the frequency of occurrence
</p>
</li>
<li><p> keywords_by_ngram: data.frame with columns keyword, ngram, freq indicating the keywords found and the frequency of occurrence
at each level of ngram. The difference with keywords being that if you have a sequence of words e.g. data science consultant, then in the keywords_by_ngram
you would still have the keywords data analysis and science consultant, while in the keywords list element you would only have data science consultant
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="igraph.html#topic+page_rank">page_rank</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(joboffer)
keywords &lt;- textrank_keywords(joboffer$lemma,
                              relevant = joboffer$upos %in% c("NOUN", "VERB", "ADJ"))
subset(keywords$keywords, ngram &gt; 1 &amp; freq &gt; 1)
keywords &lt;- textrank_keywords(joboffer$lemma,
                              relevant = joboffer$upos %in% c("NOUN"),
                              p = 1/2, sep = " ")
subset(keywords$keywords, ngram &gt; 1)

## plotting pagerank to see the relevance of each word
barplot(sort(keywords$pagerank$vector), horiz = TRUE,
        las = 2, cex.names = 0.5, col = "lightblue", xlab = "Pagerank")
</code></pre>

<hr>
<h2 id='textrank_sentences'>Textrank - extract relevant sentences</h2><span id='topic+textrank_sentences'></span>

<h3>Description</h3>

<p>The textrank algorithm is a technique to rank sentences in order of importance.<br />
</p>
<p>In order to find relevant sentences, the textrank algorithm needs 2 inputs:
a data.frame (<code>data</code>) with sentences and a data.frame (<code>terminology</code>)
containing tokens which are part of each sentence.<br />
Based on these 2 datasets, it calculates the pairwise distance between each sentence by computing
how many terms are overlapping (Jaccard distance, implemented in <code><a href="#topic+textrank_jaccard">textrank_jaccard</a></code>).
These pairwise distances among the sentences are next passed on to Google's pagerank algorithm
to identify the most relevant sentences.<br />
</p>
<p>If <code>data</code> contains many sentences, it makes sense not to compute all pairwise sentence distances but instead limiting
the calculation of the Jaccard distance to only sentence combinations which are limited by the Minhash algorithm.
This is implemented in <code><a href="#topic+textrank_candidates_lsh">textrank_candidates_lsh</a></code> and an example is show below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textrank_sentences(
  data,
  terminology,
  textrank_dist = textrank_jaccard,
  textrank_candidates = textrank_candidates_all(data$textrank_id),
  max = 1000,
  options_pagerank = list(directed = FALSE),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textrank_sentences_+3A_data">data</code></td>
<td>
<p>a data.frame with 1 row per sentence where the first column
is an identifier of a sentence (e.g. textrank_id) and the second column is the raw sentence. See the example.</p>
</td></tr>
<tr><td><code id="textrank_sentences_+3A_terminology">terminology</code></td>
<td>
<p>a data.frame with with one row per token indicating which token is part of each sentence.
The first column in this data.frame is the identifier which corresponds to the first column of <code>data</code>
and the second column indicates the token which is part of the sentence which will be passed on to <code>textrank_dist</code>.
See the example.</p>
</td></tr>
<tr><td><code id="textrank_sentences_+3A_textrank_dist">textrank_dist</code></td>
<td>
<p>a function which calculates the distance between 2 sentences which are represented by a vectors of tokens.
The first 2 arguments of the function are the tokens in sentence1 and sentence2.
The function should return a numeric value of length one. The larger the value,
the larger the connection between the 2 vectors indicating more strength. Defaults to the jaccard distance (<code><a href="#topic+textrank_jaccard">textrank_jaccard</a></code>),
indicating the percent of common tokens.</p>
</td></tr>
<tr><td><code id="textrank_sentences_+3A_textrank_candidates">textrank_candidates</code></td>
<td>
<p>a data.frame of candidate sentence to sentence comparisons with columns textrank_id_1 and textrank_id_2
indicating for which combination of sentences we want to compute the Jaccard distance or the distance function as provided in <code>textrank_dist</code>.
See for example <code><a href="#topic+textrank_candidates_all">textrank_candidates_all</a></code> or <code><a href="#topic+textrank_candidates_lsh">textrank_candidates_lsh</a></code>.</p>
</td></tr>
<tr><td><code id="textrank_sentences_+3A_max">max</code></td>
<td>
<p>integer indicating to reduce the number of sentence to sentence combinations to compute.
In case provided, we take only this max amount of rows from <code>textrank_candidates</code></p>
</td></tr>
<tr><td><code id="textrank_sentences_+3A_options_pagerank">options_pagerank</code></td>
<td>
<p>a list of arguments passed on to <code><a href="igraph.html#topic+page_rank">page_rank</a></code></p>
</td></tr>
<tr><td><code id="textrank_sentences_+3A_...">...</code></td>
<td>
<p>arguments passed on to <code>textrank_dist</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class textrank_sentences
which is a list with elements:
</p>

<ul>
<li><p> sentences: a data.frame with columns textrank_id, sentence and textrank where the textrank is the Google Pagerank importance metric of the sentence
</p>
</li>
<li><p> sentences_dist: a data.frame with columns textrank_id_1, textrank_id_2 (the sentence id) and weight which
is the result of the computed distance between the 2 sentences
</p>
</li>
<li><p> pagerank: the result of a call to <code><a href="igraph.html#topic+page_rank">page_rank</a></code>
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="igraph.html#topic+page_rank">page_rank</a></code>, <code><a href="#topic+textrank_candidates_all">textrank_candidates_all</a></code>, <code><a href="#topic+textrank_candidates_lsh">textrank_candidates_lsh</a></code>, <code><a href="#topic+textrank_jaccard">textrank_jaccard</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(udpipe)
data(joboffer)
head(joboffer)
joboffer$textrank_id &lt;- unique_identifier(joboffer, c("doc_id", "paragraph_id", "sentence_id"))
sentences &lt;- unique(joboffer[, c("textrank_id", "sentence")])
cat(sentences$sentence)
terminology &lt;- subset(joboffer, upos %in% c("NOUN", "ADJ"), select = c("textrank_id", "lemma"))
head(terminology)

## Textrank for finding the most relevant sentences
tr &lt;- textrank_sentences(data = sentences, terminology = terminology)
summary(tr, n = 2)
summary(tr, n = 5, keep.sentence.order = TRUE)

## Not run: 
## Using minhash to reduce sentence combinations - relevant if you have a lot of sentences
library(textreuse)
minhash &lt;- minhash_generator(n = 1000, seed = 123456789)
candidates &lt;- textrank_candidates_lsh(x = terminology$lemma, sentence_id = terminology$textrank_id,
                                      minhashFUN = minhash, bands = 500)
tr &lt;- textrank_sentences(data = sentences, terminology = terminology,
                         textrank_candidates = candidates)
summary(tr, n = 2)

## End(Not run)
## You can also reduce the number of sentence combinations by sampling
tr &lt;- textrank_sentences(data = sentences, terminology = terminology, max = 100)
tr
summary(tr, n = 2)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
