<!DOCTYPE html><html><head><title>Help for package RGF</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {RGF}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#FastRGF_Classifier'><p>A Fast Regularized Greedy Forest classifier</p></a></li>
<li><a href='#FastRGF_Regressor'><p>A Fast Regularized Greedy Forest regressor</p></a></li>
<li><a href='#Internal_class'><p>Internal R6 class for all secondary functions used in RGF and FastRGF</p></a></li>
<li><a href='#mat_2scipy_sparse'><p>conversion of an R matrix to a scipy sparse matrix</p></a></li>
<li><a href='#RGF_Classifier'><p>Regularized Greedy Forest classifier</p></a></li>
<li><a href='#RGF_cleanup_temp_files'><p>Delete all temporary files of the created RGF estimators</p></a></li>
<li><a href='#RGF_Regressor'><p>Regularized Greedy Forest regressor</p></a></li>
<li><a href='#TO_scipy_sparse'><p>conversion of an R sparse matrix to a scipy sparse matrix</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Regularized Greedy Forest</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-09-10</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/RGF-team/rgf/issues">https://github.com/RGF-team/rgf/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/RGF-team/rgf/tree/master/R-package">https://github.com/RGF-team/rgf/tree/master/R-package</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Regularized Greedy Forest wrapper of the 'Regularized Greedy Forest' <a href="https://github.com/RGF-team/rgf/tree/master/python-package">https://github.com/RGF-team/rgf/tree/master/python-package</a> 'python' package, which also includes a Multi-core implementation (FastRGF) <a href="https://github.com/RGF-team/rgf/tree/master/FastRGF">https://github.com/RGF-team/rgf/tree/master/FastRGF</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>Python (&gt;= 3.7), rgf_python, scikit-learn (&gt;=
0.18.0), scipy, numpy. Detailed installation instructions for
each operating system can be found in the README file.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R(&ge; 3.2.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>reticulate, R6, Matrix</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, covr, knitr, rmarkdown</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-09-10 12:38:08 UTC; lampros</td>
</tr>
<tr>
<td>Author:</td>
<td>Lampros Mouselimis
    <a href="https://orcid.org/0000-0002-8024-1546"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre],
  Ryosuke Fukatani [cph] (Author of the python wrapper of the
    'Regularized Greedy Forest' machine learning algorithm),
  Nikita Titov [cph] (Author of the python wrapper of the 'Regularized
    Greedy Forest' machine learning algorithm),
  Tong Zhang [cph] (Author of the 'Regularized Greedy Forest' and of the
    Multi-core implementation of Regularized Greedy Forest machine
    learning algorithm),
  Rie Johnson [cph] (Author of the 'Regularized Greedy Forest' machine
    learning algorithm)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Lampros Mouselimis &lt;mouselimislampros@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-09-12 06:42:59 UTC</td>
</tr>
</table>
<hr>
<h2 id='FastRGF_Classifier'>A Fast Regularized Greedy Forest classifier</h2><span id='topic+FastRGF_Classifier'></span>

<h3>Description</h3>

<p>A Fast Regularized Greedy Forest classifier
</p>
<p>A Fast Regularized Greedy Forest classifier
</p>


<h3>Usage</h3>

<pre><code class='language-R'># init &lt;- FastRGF_Classifier$new(n_estimators = 500, max_depth = 6,
#                                      max_leaf = 50, tree_gain_ratio = 1.0,
#                                      min_samples_leaf = 5, loss = "LS", l1 = 1.0,
#                                      l2 = 1000.0, opt_algorithm = "rgf",
#                                      learning_rate = 0.001, max_bin = NULL,
#                                      min_child_weight = 5.0, data_l2 = 2.0,
#                                      sparse_max_features = 80000,
#                                      sparse_min_occurences = 5,
#                                      calc_prob = "sigmoid", n_jobs = 1,
#                                      verbose = 0)
</code></pre>


<h3>Details</h3>

<p>the <em>fit</em> function builds a classifier from the training set (x, y).
</p>
<p>the <em>predict</em> function predicts the class for x.
</p>
<p>the <em>predict_proba</em> function predicts class probabilities for x.
</p>
<p>the <em>cleanup</em> function removes tempfiles used by this model. See the issue <em>https://github.com/RGF-team/rgf/issues/75</em>, which explains in which cases the <em>cleanup</em> function applies.
</p>
<p>the <em>get_params</em> function returns the parameters of the model.
</p>
<p>the <em>score</em> function returns the mean accuracy on the given test data and labels.
</p>


<h3>Methods</h3>


<dl>
<dt><code>FastRGF_Classifier$new(n_estimators = 500, max_depth = 6,
                                    max_leaf = 50, tree_gain_ratio = 1.0,
                                    min_samples_leaf = 5, loss = "LS", l1 = 1.0,
                                    l2 = 1000.0, opt_algorithm = "rgf",
                                    learning_rate = 0.001, max_bin = NULL,
                                    min_child_weight = 5.0, data_l2 = 2.0,
                                    sparse_max_features = 80000,
                                    sparse_min_occurences = 5,
                                    calc_prob = "sigmoid", n_jobs = 1,
                                    verbose = 0)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>fit(x, y, sample_weight = NULL)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>predict(x)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>predict_proba(x)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>cleanup()</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>get_params(deep = TRUE)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>score(x, y, sample_weight = NULL)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
</dl>



<h3>Super class</h3>

<p><code>RGF::Internal_class</code> -&gt; <code>FastRGF_Classifier</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-FastRGF_Classifier-new"><code>FastRGF_Classifier$new()</code></a>
</p>
</li>
<li> <p><a href="#method-FastRGF_Classifier-clone"><code>FastRGF_Classifier$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="cleanup"><a href='../../RGF/html/Internal_class.html#method-Internal_class-cleanup'><code>RGF::Internal_class$cleanup()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="dump_model"><a href='../../RGF/html/Internal_class.html#method-Internal_class-dump_model'><code>RGF::Internal_class$dump_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="feature_importances"><a href='../../RGF/html/Internal_class.html#method-Internal_class-feature_importances'><code>RGF::Internal_class$feature_importances()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="fit"><a href='../../RGF/html/Internal_class.html#method-Internal_class-fit'><code>RGF::Internal_class$fit()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="get_params"><a href='../../RGF/html/Internal_class.html#method-Internal_class-get_params'><code>RGF::Internal_class$get_params()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="predict"><a href='../../RGF/html/Internal_class.html#method-Internal_class-predict'><code>RGF::Internal_class$predict()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="predict_proba"><a href='../../RGF/html/Internal_class.html#method-Internal_class-predict_proba'><code>RGF::Internal_class$predict_proba()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="save_model"><a href='../../RGF/html/Internal_class.html#method-Internal_class-save_model'><code>RGF::Internal_class$save_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="score"><a href='../../RGF/html/Internal_class.html#method-Internal_class-score'><code>RGF::Internal_class$score()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-FastRGF_Classifier-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>FastRGF_Classifier$new(
  n_estimators = 500,
  max_depth = 6,
  max_leaf = 50,
  tree_gain_ratio = 1,
  min_samples_leaf = 5,
  loss = "LS",
  l1 = 1,
  l2 = 1000,
  opt_algorithm = "rgf",
  learning_rate = 0.001,
  max_bin = NULL,
  min_child_weight = 5,
  data_l2 = 2,
  sparse_max_features = 80000,
  sparse_min_occurences = 5,
  calc_prob = "sigmoid",
  n_jobs = 1,
  verbose = 0
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>n_estimators</code></dt><dd><p>an integer. The number of trees in the forest (Original name: forest.ntrees.)</p>
</dd>
<dt><code>max_depth</code></dt><dd><p>an integer. Maximum tree depth (Original name: dtree.max_level.)</p>
</dd>
<dt><code>max_leaf</code></dt><dd><p>an integer. Maximum number of leaf nodes in best-first search (Original name: dtree.max_nodes.)</p>
</dd>
<dt><code>tree_gain_ratio</code></dt><dd><p>a float. New tree is created when leaf-nodes gain &lt; this value * estimated gain of creating new tree (Original name: dtree.new_tree_gain_ratio.)</p>
</dd>
<dt><code>min_samples_leaf</code></dt><dd><p>an integer or float. Minimum number of training data points in each leaf node. If an integer, then consider min_samples_leaf as the minimum number. If a float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node (Original name: dtree.min_sample.)</p>
</dd>
<dt><code>loss</code></dt><dd><p>a character string. One of <em>&quot;LS&quot;</em> (Least squares loss), <em>&quot;MODLS&quot;</em> (Modified least squares loss) or <em>&quot;LOGISTIC&quot;</em> (Logistic loss) (Original name: dtree.loss.)</p>
</dd>
<dt><code>l1</code></dt><dd><p>a float. Used to control the degree of L1 regularization (Original name: dtree.lamL1.)</p>
</dd>
<dt><code>l2</code></dt><dd><p>a float. Used to control the degree of L2 regularization (Original name: dtree.lamL2.)</p>
</dd>
<dt><code>opt_algorithm</code></dt><dd><p>a character string. Either <em>&quot;rgf&quot;</em> or <em>&quot;epsilon-greedy&quot;</em>. Optimization method for training forest (Original name: forest.opt.)</p>
</dd>
<dt><code>learning_rate</code></dt><dd><p>a float. Step size of epsilon-greedy boosting. Meant for being used with opt_algorithm = &quot;epsilon-greedy&quot; (Original name: forest.stepsize.)</p>
</dd>
<dt><code>max_bin</code></dt><dd><p>an integer or NULL. Maximum number of discretized values (bins). If NULL, 65000 is used for dense data and 200 for sparse data (Original name: discretize.(sparse/dense).max_buckets.)</p>
</dd>
<dt><code>min_child_weight</code></dt><dd><p>a float. Minimum sum of data weights for each discretized value (bin) (Original name: discretize.(sparse/dense).min_bucket_weights.)</p>
</dd>
<dt><code>data_l2</code></dt><dd><p>a float. Used to control the degree of L2 regularization for discretization (Original name: discretize.(sparse/dense).lamL2.)</p>
</dd>
<dt><code>sparse_max_features</code></dt><dd><p>an integer. Maximum number of selected features. Meant for being used with sparse data (Original name: discretize.sparse.max_features.)</p>
</dd>
<dt><code>sparse_min_occurences</code></dt><dd><p>an integer. Minimum number of occurrences for a feature to be selected. Meant for being used with sparse data (Original name: discretize.sparse.min_occrrences.)</p>
</dd>
<dt><code>calc_prob</code></dt><dd><p>a character string. Either <em>&quot;sigmoid&quot;</em> or <em>&quot;softmax&quot;</em>. Method of probability calculation</p>
</dd>
<dt><code>n_jobs</code></dt><dd><p>an integer. The number of jobs to run in parallel for both fit and predict. If -1, all CPUs are used. If -2, all CPUs but one are used. If &lt; -1, (n_cpus + 1 + n_jobs) are used (Original name: set.nthreads.)</p>
</dd>
<dt><code>verbose</code></dt><dd><p>an integer. Controls the verbosity of the tree building process (Original name: set.verbose.)</p>
</dd>
</dl>

</div>


<hr>
<a id="method-FastRGF_Classifier-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>FastRGF_Classifier$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>

<p><em>https://github.com/RGF-team/rgf/tree/master/python-package</em>, <em>Tong Zhang, FastRGF: Multi-core Implementation of Regularized Greedy Forest (https://github.com/RGF-team/rgf/tree/master/FastRGF)</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
try({
    if (reticulate::py_available(initialize = FALSE)) {
        if (reticulate::py_module_available("rgf.sklearn")) {

            library(RGF)

            set.seed(1)
            x = matrix(runif(100000), nrow = 100, ncol = 1000)

            y = sample(1:2, 100, replace = TRUE)

            fast_RGF_class = FastRGF_Classifier$new(max_leaf = 50)

            fast_RGF_class$fit(x, y)

            preds = fast_RGF_class$predict_proba(x)
        }
    }
}, silent = TRUE)
</code></pre>

<hr>
<h2 id='FastRGF_Regressor'>A Fast Regularized Greedy Forest regressor</h2><span id='topic+FastRGF_Regressor'></span>

<h3>Description</h3>

<p>A Fast Regularized Greedy Forest regressor
</p>
<p>A Fast Regularized Greedy Forest regressor
</p>


<h3>Usage</h3>

<pre><code class='language-R'># init &lt;- FastRGF_Regressor$new(n_estimators = 500, max_depth = 6,
#                                      max_leaf = 50, tree_gain_ratio = 1.0,
#                                      min_samples_leaf = 5, l1 = 1.0,
#                                      l2 = 1000.0, opt_algorithm = "rgf",
#                                      learning_rate = 0.001, max_bin = NULL,
#                                      min_child_weight = 5.0, data_l2 = 2.0,
#                                      sparse_max_features = 80000,
#                                      sparse_min_occurences = 5,
#                                      n_jobs = 1, verbose = 0)
</code></pre>


<h3>Details</h3>

<p>the <em>fit</em> function builds a regressor from the training set (x, y).
</p>
<p>the <em>predict</em> function predicts the regression target for x.
</p>
<p>the <em>cleanup</em> function removes tempfiles used by this model. See the issue <em>https://github.com/RGF-team/rgf/issues/75</em>, which explains in which cases the <em>cleanup</em> function applies.
</p>
<p>the <em>get_params</em> function returns the parameters of the model.
</p>
<p>the <em>score</em> function returns the coefficient of determination ( R^2 ) for the predictions.
</p>


<h3>Methods</h3>


<dl>
<dt><code>FastRGF_Regressor$new(n_estimators = 500, max_depth = 6,
                                   max_leaf = 50, tree_gain_ratio = 1.0,
                                   min_samples_leaf = 5, l1 = 1.0,
                                   l2 = 1000.0, opt_algorithm = "rgf",
                                   learning_rate = 0.001, max_bin = NULL,
                                   min_child_weight = 5.0, data_l2 = 2.0,
                                   sparse_max_features = 80000,
                                   sparse_min_occurences = 5,
                                   n_jobs = 1, verbose = 0)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>fit(x, y, sample_weight = NULL)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>predict(x)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>cleanup()</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>get_params(deep = TRUE)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>score(x, y, sample_weight = NULL)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
</dl>



<h3>Super class</h3>

<p><code>RGF::Internal_class</code> -&gt; <code>FastRGF_Regressor</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-FastRGF_Regressor-new"><code>FastRGF_Regressor$new()</code></a>
</p>
</li>
<li> <p><a href="#method-FastRGF_Regressor-clone"><code>FastRGF_Regressor$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="cleanup"><a href='../../RGF/html/Internal_class.html#method-Internal_class-cleanup'><code>RGF::Internal_class$cleanup()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="dump_model"><a href='../../RGF/html/Internal_class.html#method-Internal_class-dump_model'><code>RGF::Internal_class$dump_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="feature_importances"><a href='../../RGF/html/Internal_class.html#method-Internal_class-feature_importances'><code>RGF::Internal_class$feature_importances()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="fit"><a href='../../RGF/html/Internal_class.html#method-Internal_class-fit'><code>RGF::Internal_class$fit()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="get_params"><a href='../../RGF/html/Internal_class.html#method-Internal_class-get_params'><code>RGF::Internal_class$get_params()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="predict"><a href='../../RGF/html/Internal_class.html#method-Internal_class-predict'><code>RGF::Internal_class$predict()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="predict_proba"><a href='../../RGF/html/Internal_class.html#method-Internal_class-predict_proba'><code>RGF::Internal_class$predict_proba()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="save_model"><a href='../../RGF/html/Internal_class.html#method-Internal_class-save_model'><code>RGF::Internal_class$save_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="score"><a href='../../RGF/html/Internal_class.html#method-Internal_class-score'><code>RGF::Internal_class$score()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-FastRGF_Regressor-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>FastRGF_Regressor$new(
  n_estimators = 500,
  max_depth = 6,
  max_leaf = 50,
  tree_gain_ratio = 1,
  min_samples_leaf = 5,
  l1 = 1,
  l2 = 1000,
  opt_algorithm = "rgf",
  learning_rate = 0.001,
  max_bin = NULL,
  min_child_weight = 5,
  data_l2 = 2,
  sparse_max_features = 80000,
  sparse_min_occurences = 5,
  n_jobs = 1,
  verbose = 0
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>n_estimators</code></dt><dd><p>an integer. The number of trees in the forest (Original name: forest.ntrees.)</p>
</dd>
<dt><code>max_depth</code></dt><dd><p>an integer. Maximum tree depth (Original name: dtree.max_level.)</p>
</dd>
<dt><code>max_leaf</code></dt><dd><p>an integer. Maximum number of leaf nodes in best-first search (Original name: dtree.max_nodes.)</p>
</dd>
<dt><code>tree_gain_ratio</code></dt><dd><p>a float. New tree is created when leaf-nodes gain &lt; this value * estimated gain of creating new tree (Original name: dtree.new_tree_gain_ratio.)</p>
</dd>
<dt><code>min_samples_leaf</code></dt><dd><p>an integer or float. Minimum number of training data points in each leaf node. If an integer, then consider min_samples_leaf as the minimum number. If a float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node (Original name: dtree.min_sample.)</p>
</dd>
<dt><code>l1</code></dt><dd><p>a float. Used to control the degree of L1 regularization (Original name: dtree.lamL1.)</p>
</dd>
<dt><code>l2</code></dt><dd><p>a float. Used to control the degree of L2 regularization (Original name: dtree.lamL2.)</p>
</dd>
<dt><code>opt_algorithm</code></dt><dd><p>a character string. Either <em>&quot;rgf&quot;</em> or <em>&quot;epsilon-greedy&quot;</em>. Optimization method for training forest (Original name: forest.opt.)</p>
</dd>
<dt><code>learning_rate</code></dt><dd><p>a float. Step size of epsilon-greedy boosting. Meant for being used with opt_algorithm = &quot;epsilon-greedy&quot; (Original name: forest.stepsize.)</p>
</dd>
<dt><code>max_bin</code></dt><dd><p>an integer or NULL. Maximum number of discretized values (bins). If NULL, 65000 is used for dense data and 200 for sparse data (Original name: discretize.(sparse/dense).max_buckets.)</p>
</dd>
<dt><code>min_child_weight</code></dt><dd><p>a float. Minimum sum of data weights for each discretized value (bin) (Original name: discretize.(sparse/dense).min_bucket_weights.)</p>
</dd>
<dt><code>data_l2</code></dt><dd><p>a float. Used to control the degree of L2 regularization for discretization (Original name: discretize.(sparse/dense).lamL2.)</p>
</dd>
<dt><code>sparse_max_features</code></dt><dd><p>an integer. Maximum number of selected features. Meant for being used with sparse data (Original name: discretize.sparse.max_features.)</p>
</dd>
<dt><code>sparse_min_occurences</code></dt><dd><p>an integer. Minimum number of occurrences for a feature to be selected. Meant for being used with sparse data (Original name: discretize.sparse.min_occrrences.)</p>
</dd>
<dt><code>n_jobs</code></dt><dd><p>an integer. The number of jobs to run in parallel for both fit and predict. If -1, all CPUs are used. If -2, all CPUs but one are used. If &lt; -1, (n_cpus + 1 + n_jobs) are used (Original name: set.nthreads.)</p>
</dd>
<dt><code>verbose</code></dt><dd><p>an integer. Controls the verbosity of the tree building process (Original name: set.verbose.)</p>
</dd>
</dl>

</div>


<hr>
<a id="method-FastRGF_Regressor-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>FastRGF_Regressor$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>

<p><em>https://github.com/RGF-team/rgf/tree/master/python-package</em>, <em>Tong Zhang, FastRGF: Multi-core Implementation of Regularized Greedy Forest (https://github.com/RGF-team/rgf/tree/master/FastRGF)</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
try({
    if (reticulate::py_available(initialize = FALSE)) {
        if (reticulate::py_module_available("rgf.sklearn")) {

            library(RGF)

            set.seed(1)
            x = matrix(runif(100000), nrow = 100, ncol = 1000)

            y = runif(100)

            fast_RGF_regr = FastRGF_Regressor$new(max_leaf = 50)

            fast_RGF_regr$fit(x, y)

            preds = fast_RGF_regr$predict(x)
        }
    }
}, silent = TRUE)
</code></pre>

<hr>
<h2 id='Internal_class'>Internal R6 class for all secondary functions used in RGF and FastRGF</h2><span id='topic+Internal_class'></span>

<h3>Description</h3>

<p>Internal R6 class for all secondary functions used in RGF and FastRGF
</p>
<p>Internal R6 class for all secondary functions used in RGF and FastRGF
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-Internal_class-fit"><code>Internal_class$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-Internal_class-predict"><code>Internal_class$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-Internal_class-predict_proba"><code>Internal_class$predict_proba()</code></a>
</p>
</li>
<li> <p><a href="#method-Internal_class-cleanup"><code>Internal_class$cleanup()</code></a>
</p>
</li>
<li> <p><a href="#method-Internal_class-get_params"><code>Internal_class$get_params()</code></a>
</p>
</li>
<li> <p><a href="#method-Internal_class-score"><code>Internal_class$score()</code></a>
</p>
</li>
<li> <p><a href="#method-Internal_class-feature_importances"><code>Internal_class$feature_importances()</code></a>
</p>
</li>
<li> <p><a href="#method-Internal_class-dump_model"><code>Internal_class$dump_model()</code></a>
</p>
</li>
<li> <p><a href="#method-Internal_class-save_model"><code>Internal_class$save_model()</code></a>
</p>
</li>
<li> <p><a href="#method-Internal_class-clone"><code>Internal_class$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-Internal_class-fit"></a>



<h4>Method <code>fit()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>Internal_class$fit(x, y, sample_weight = NULL)</pre></div>


<hr>
<a id="method-Internal_class-predict"></a>



<h4>Method <code>predict()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>Internal_class$predict(x)</pre></div>


<hr>
<a id="method-Internal_class-predict_proba"></a>



<h4>Method <code>predict_proba()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>Internal_class$predict_proba(x)</pre></div>


<hr>
<a id="method-Internal_class-cleanup"></a>



<h4>Method <code>cleanup()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>Internal_class$cleanup()</pre></div>


<hr>
<a id="method-Internal_class-get_params"></a>



<h4>Method <code>get_params()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>Internal_class$get_params(deep = TRUE)</pre></div>


<hr>
<a id="method-Internal_class-score"></a>



<h4>Method <code>score()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>Internal_class$score(x, y, sample_weight = NULL)</pre></div>


<hr>
<a id="method-Internal_class-feature_importances"></a>



<h4>Method <code>feature_importances()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>Internal_class$feature_importances()</pre></div>


<hr>
<a id="method-Internal_class-dump_model"></a>



<h4>Method <code>dump_model()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>Internal_class$dump_model()</pre></div>


<hr>
<a id="method-Internal_class-save_model"></a>



<h4>Method <code>save_model()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>Internal_class$save_model(filename)</pre></div>


<hr>
<a id="method-Internal_class-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>Internal_class$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='mat_2scipy_sparse'>conversion of an R matrix to a scipy sparse matrix</h2><span id='topic+mat_2scipy_sparse'></span>

<h3>Description</h3>

<p>conversion of an R matrix to a scipy sparse matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mat_2scipy_sparse(x, format = "sparse_row_matrix")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mat_2scipy_sparse_+3A_x">x</code></td>
<td>
<p>a data matrix</p>
</td></tr>
<tr><td><code id="mat_2scipy_sparse_+3A_format">format</code></td>
<td>
<p>a character string. Either <em>&quot;sparse_row_matrix&quot;</em> or <em>&quot;sparse_column_matrix&quot;</em></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function allows the user to convert an R matrix to a scipy sparse matrix. This is useful because the Regularized Greedy Forest algorithm accepts only python sparse matrices as input.
</p>


<h3>References</h3>

<p>https://docs.scipy.org/doc/scipy/reference/sparse.html
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
try({
    if (reticulate::py_available(initialize = FALSE)) {
        if (reticulate::py_module_available("scipy")) {

            library(RGF)

            set.seed(1)

            x = matrix(runif(1000), nrow = 100, ncol = 10)

            res = mat_2scipy_sparse(x)

            print(dim(x))

            print(res$shape)
        }
    }
}, silent = TRUE)
</code></pre>

<hr>
<h2 id='RGF_Classifier'>Regularized Greedy Forest classifier</h2><span id='topic+RGF_Classifier'></span>

<h3>Description</h3>

<p>Regularized Greedy Forest classifier
</p>
<p>Regularized Greedy Forest classifier
</p>


<h3>Usage</h3>

<pre><code class='language-R'># init &lt;- RGF_Classifier$new(max_leaf = 1000, test_interval = 100,
#                                  algorithm = "RGF", loss = "Log", reg_depth = 1.0,
#                                  l2 = 0.1, sl2 = NULL, normalize = FALSE,
#                                  min_samples_leaf = 10, n_iter = NULL,
#                                  n_tree_search = 1, opt_interval = 100,
#                                  learning_rate = 0.5, calc_prob = "sigmoid",
#                                  n_jobs = 1, memory_policy = "generous",
#                                  verbose = 0, init_model = NULL)
</code></pre>


<h3>Details</h3>

<p>the <em>fit</em> function builds a classifier from the training set (x, y).
</p>
<p>the <em>predict</em> function predicts the class for x.
</p>
<p>the <em>predict_proba</em> function predicts class probabilities for x.
</p>
<p>the <em>cleanup</em> function removes tempfiles used by this model. See the issue <em>https://github.com/RGF-team/rgf/issues/75</em>, which explains in which cases the <em>cleanup</em> function applies.
</p>
<p>the <em>get_params</em> function returns the parameters of the model.
</p>
<p>the <em>score</em> function returns the mean accuracy on the given test data and labels.
</p>
<p>the <em>feature_importances</em> function returns the feature importances for the data.
</p>
<p>the <em>dump_model</em> function currently prints information about the fitted model in the console
</p>
<p>the <em>save_model</em> function saves a model to a file from which training can do warm-start in the future.
</p>


<h3>Methods</h3>


<dl>
<dt><code>RGF_Classifier$new(max_leaf = 1000, test_interval = 100,
                               algorithm = "RGF", loss = "Log", reg_depth = 1.0,
                               l2 = 0.1, sl2 = NULL, normalize = FALSE,
                               min_samples_leaf = 10, n_iter = NULL,
                               n_tree_search = 1, opt_interval = 100,
                               learning_rate = 0.5, calc_prob = "sigmoid",
                               n_jobs = 1, memory_policy = "generous",
                               verbose = 0, init_model = NULL)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>fit(x, y, sample_weight = NULL)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>predict(x)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>predict_proba(x)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>cleanup()</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>get_params(deep = TRUE)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>score(x, y, sample_weight = NULL)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>feature_importances()</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>dump_model()</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>save_model(filename)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
</dl>



<h3>Super class</h3>

<p><code>RGF::Internal_class</code> -&gt; <code>RGF_Classifier</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-RGF_Classifier-new"><code>RGF_Classifier$new()</code></a>
</p>
</li>
<li> <p><a href="#method-RGF_Classifier-clone"><code>RGF_Classifier$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="cleanup"><a href='../../RGF/html/Internal_class.html#method-Internal_class-cleanup'><code>RGF::Internal_class$cleanup()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="dump_model"><a href='../../RGF/html/Internal_class.html#method-Internal_class-dump_model'><code>RGF::Internal_class$dump_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="feature_importances"><a href='../../RGF/html/Internal_class.html#method-Internal_class-feature_importances'><code>RGF::Internal_class$feature_importances()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="fit"><a href='../../RGF/html/Internal_class.html#method-Internal_class-fit'><code>RGF::Internal_class$fit()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="get_params"><a href='../../RGF/html/Internal_class.html#method-Internal_class-get_params'><code>RGF::Internal_class$get_params()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="predict"><a href='../../RGF/html/Internal_class.html#method-Internal_class-predict'><code>RGF::Internal_class$predict()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="predict_proba"><a href='../../RGF/html/Internal_class.html#method-Internal_class-predict_proba'><code>RGF::Internal_class$predict_proba()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="save_model"><a href='../../RGF/html/Internal_class.html#method-Internal_class-save_model'><code>RGF::Internal_class$save_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="score"><a href='../../RGF/html/Internal_class.html#method-Internal_class-score'><code>RGF::Internal_class$score()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-RGF_Classifier-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>RGF_Classifier$new(
  max_leaf = 1000,
  test_interval = 100,
  algorithm = "RGF",
  loss = "Log",
  reg_depth = 1,
  l2 = 0.1,
  sl2 = NULL,
  normalize = FALSE,
  min_samples_leaf = 10,
  n_iter = NULL,
  n_tree_search = 1,
  opt_interval = 100,
  learning_rate = 0.5,
  calc_prob = "sigmoid",
  n_jobs = 1,
  memory_policy = "generous",
  verbose = 0,
  init_model = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>max_leaf</code></dt><dd><p>an integer. Training will be terminated when the number of leaf nodes in the forest reaches this value.</p>
</dd>
<dt><code>test_interval</code></dt><dd><p>an integer. Test interval in terms of the number of leaf nodes.</p>
</dd>
<dt><code>algorithm</code></dt><dd><p>a character string specifying the <em>Regularization algorithm</em>. One of <em>&quot;RGF&quot;</em> (RGF with L2 regularization on leaf-only models), <em>&quot;RGF_Opt&quot;</em> (RGF with min-penalty regularization) or <em>&quot;RGF_Sib&quot;</em> (RGF with min-penalty regularization with the sum-to-zero sibling constraints).</p>
</dd>
<dt><code>loss</code></dt><dd><p>a character string specifying the <em>Loss function</em>. One of <em>&quot;LS&quot;</em> (Square loss), <em>&quot;Expo&quot;</em> (Exponential loss) or <em>&quot;Log&quot;</em> (Logistic loss).</p>
</dd>
<dt><code>reg_depth</code></dt><dd><p>a float. Must be no smaller than 1.0. Meant for being used with the algorithm <em>RGF Opt</em> or <em>RGF Sib</em>. A larger value penalizes deeper nodes more severely.</p>
</dd>
<dt><code>l2</code></dt><dd><p>a float. Used to control the degree of L2 regularization.</p>
</dd>
<dt><code>sl2</code></dt><dd><p>a float or NULL. Override L2 regularization parameter l2 for the process of growing the forest. That is, if specified, the weight correction process uses l2 and the forest growing process uses sl2. If NULL, no override takes place and l2 is used throughout training.</p>
</dd>
<dt><code>normalize</code></dt><dd><p>a boolean. If True, training targets are normalized so that the average becomes zero.</p>
</dd>
<dt><code>min_samples_leaf</code></dt><dd><p>an integer or a float. Minimum number of training data points in each leaf node. If an integer, then consider <em>min_samples_leaf</em> as the minimum number. If a float, then <em>min_samples_leaf</em> is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.</p>
</dd>
<dt><code>n_iter</code></dt><dd><p>an integer or NULL. The number of iterations of coordinate descent to optimize weights. If NULL, 10 is used for loss = &quot;LS&quot; and 5 for loss = &quot;Expo&quot; or &quot;Log&quot;.</p>
</dd>
<dt><code>n_tree_search</code></dt><dd><p>an integer. The number of trees to be searched for the nodes to split. The most recently grown trees are searched first.</p>
</dd>
<dt><code>opt_interval</code></dt><dd><p>an integer. Weight optimization interval in terms of the number of leaf nodes. For example, by default, weight optimization is performed every time approximately 100 leaf nodes are newly added to the forest.</p>
</dd>
<dt><code>learning_rate</code></dt><dd><p>a float. Step size of Newton updates used in coordinate descent to optimize weights.</p>
</dd>
<dt><code>calc_prob</code></dt><dd><p>a character string. One of <em>&quot;sigmoid&quot;</em> or <em>&quot;softmax&quot;</em>. Method of probability calculation.</p>
</dd>
<dt><code>n_jobs</code></dt><dd><p>an integer. The number of jobs (threads) to use for the computation. The substantial number of the jobs dependents on <em>classes_</em> (The number of classes when <em>fit</em> is performed). If classes_ = 2, the substantial max number of the jobs is one. If classes_ &gt; 2, the substantial max number of the jobs is the same as classes_. If n_jobs = 1, no parallel computing code is used at all regardless of classes_. If n_jobs = -1 and classes_ &gt;= number of CPU, all CPUs are used. For n_jobs = -2, all CPUs but one are used. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used.</p>
</dd>
<dt><code>memory_policy</code></dt><dd><p>a character string. One of <em>&quot;conservative&quot;</em> (it uses less memory at the expense of longer runtime. Try only when with default value it uses too much memory) or <em>&quot;generous&quot;</em> (it runs faster using more memory by keeping the sorted orders of the features on memory for reuse). Memory using policy.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>an integer. Controls the verbosity of the tree building process.</p>
</dd>
<dt><code>init_model</code></dt><dd><p>either NULL or a character string, optional (default=NULL). Filename of a previously saved model from which training should do warm-start. If model has been saved into multiple files, do not include numerical suffixes in the filename. <em>NOTE:</em> Make sure you haven't forgotten to increase the value of the max_leaf parameter regarding to the specified warm-start model because warm-start model trees are counted in the overall number of trees.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-RGF_Classifier-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>RGF_Classifier$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>

<p><em>https://github.com/RGF-team/rgf/tree/master/python-package</em>, <em>Rie Johnson and Tong Zhang, Learning Nonlinear Functions Using Regularized Greedy Forest</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
try({
    if (reticulate::py_available(initialize = FALSE)) {
        if (reticulate::py_module_available("rgf.sklearn")) {

            library(RGF)

            set.seed(1)
            x = matrix(runif(1000), nrow = 100, ncol = 10)

            y = sample(1:2, 100, replace = TRUE)

            RGF_class = RGF_Classifier$new(max_leaf = 50)

            RGF_class$fit(x, y)

            preds = RGF_class$predict_proba(x)
        }
    }
}, silent = TRUE)
</code></pre>

<hr>
<h2 id='RGF_cleanup_temp_files'>Delete all temporary files of the created RGF estimators</h2><span id='topic+RGF_cleanup_temp_files'></span>

<h3>Description</h3>

<p>Delete all temporary files of the created RGF estimators
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RGF_cleanup_temp_files()
</code></pre>


<h3>Details</h3>

<p>This function deletes all temporary files of the created RGF estimators. See the issue <em>https://github.com/RGF-team/rgf/issues/75</em> for more details.
</p>


<h3>References</h3>

<p><em>https://github.com/RGF-team/rgf/tree/master/python-package</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
library(RGF)

RGF_cleanup_temp_files()

## End(Not run)
</code></pre>

<hr>
<h2 id='RGF_Regressor'>Regularized Greedy Forest regressor</h2><span id='topic+RGF_Regressor'></span>

<h3>Description</h3>

<p>Regularized Greedy Forest regressor
</p>
<p>Regularized Greedy Forest regressor
</p>


<h3>Usage</h3>

<pre><code class='language-R'># init &lt;- RGF_Regressor$new(max_leaf = 500, test_interval = 100,
#                                  algorithm = "RGF", loss = "LS", reg_depth = 1.0,
#                                  l2 = 0.1, sl2 = NULL, normalize = TRUE,
#                                  min_samples_leaf = 10, n_iter = NULL,
#                                  n_tree_search = 1, opt_interval = 100,
#                                  learning_rate = 0.5, memory_policy = "generous",
#                                  verbose = 0, init_model = NULL)
</code></pre>


<h3>Details</h3>

<p>the <em>fit</em> function builds a regressor from the training set (x, y).
</p>
<p>the <em>predict</em> function predicts the regression target for x.
</p>
<p>the <em>cleanup</em> function removes tempfiles used by this model. See the issue <em>https://github.com/RGF-team/rgf/issues/75</em>, which explains in which cases the <em>cleanup</em> function applies.
</p>
<p>the <em>get_params</em> function returns the parameters of the model.
</p>
<p>the <em>score</em> function returns the coefficient of determination ( R^2 ) for the predictions.
</p>
<p>the <em>feature_importances</em> function returns the feature importances for the data.
</p>
<p>the <em>dump_model</em> function currently prints information about the fitted model in the console
</p>
<p>the <em>save_model</em> function saves a model to a file from which training can do warm-start in the future.
</p>


<h3>Methods</h3>


<dl>
<dt><code>RGF_Regressor$new(max_leaf = 500, test_interval = 100,
                               algorithm = "RGF", loss = "LS", reg_depth = 1.0,
                               l2 = 0.1, sl2 = NULL, normalize = TRUE,
                               min_samples_leaf = 10, n_iter = NULL,
                               n_tree_search = 1, opt_interval = 100,
                               learning_rate = 0.5, memory_policy = "generous",
                               verbose = 0, init_model = NULL)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>fit(x, y, sample_weight = NULL)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>predict(x)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>cleanup()</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>get_params(deep = TRUE)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>score(x, y, sample_weight = NULL)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>feature_importances()</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>dump_model()</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>save_model(filename)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
</dl>



<h3>Super class</h3>

<p><code>RGF::Internal_class</code> -&gt; <code>RGF_Regressor</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-RGF_Regressor-new"><code>RGF_Regressor$new()</code></a>
</p>
</li>
<li> <p><a href="#method-RGF_Regressor-clone"><code>RGF_Regressor$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="cleanup"><a href='../../RGF/html/Internal_class.html#method-Internal_class-cleanup'><code>RGF::Internal_class$cleanup()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="dump_model"><a href='../../RGF/html/Internal_class.html#method-Internal_class-dump_model'><code>RGF::Internal_class$dump_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="feature_importances"><a href='../../RGF/html/Internal_class.html#method-Internal_class-feature_importances'><code>RGF::Internal_class$feature_importances()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="fit"><a href='../../RGF/html/Internal_class.html#method-Internal_class-fit'><code>RGF::Internal_class$fit()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="get_params"><a href='../../RGF/html/Internal_class.html#method-Internal_class-get_params'><code>RGF::Internal_class$get_params()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="predict"><a href='../../RGF/html/Internal_class.html#method-Internal_class-predict'><code>RGF::Internal_class$predict()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="predict_proba"><a href='../../RGF/html/Internal_class.html#method-Internal_class-predict_proba'><code>RGF::Internal_class$predict_proba()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="save_model"><a href='../../RGF/html/Internal_class.html#method-Internal_class-save_model'><code>RGF::Internal_class$save_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="RGF" data-topic="Internal_class" data-id="score"><a href='../../RGF/html/Internal_class.html#method-Internal_class-score'><code>RGF::Internal_class$score()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-RGF_Regressor-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>RGF_Regressor$new(
  max_leaf = 500,
  test_interval = 100,
  algorithm = "RGF",
  loss = "LS",
  reg_depth = 1,
  l2 = 0.1,
  sl2 = NULL,
  normalize = TRUE,
  min_samples_leaf = 10,
  n_iter = NULL,
  n_tree_search = 1,
  opt_interval = 100,
  learning_rate = 0.5,
  memory_policy = "generous",
  verbose = 0,
  init_model = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>max_leaf</code></dt><dd><p>an integer. Training will be terminated when the number of leaf nodes in the forest reaches this value.</p>
</dd>
<dt><code>test_interval</code></dt><dd><p>an integer. Test interval in terms of the number of leaf nodes.</p>
</dd>
<dt><code>algorithm</code></dt><dd><p>a character string specifying the <em>Regularization algorithm</em>. One of <em>&quot;RGF&quot;</em> (RGF with L2 regularization on leaf-only models), <em>&quot;RGF_Opt&quot;</em> (RGF with min-penalty regularization) or <em>&quot;RGF_Sib&quot;</em> (RGF with min-penalty regularization with the sum-to-zero sibling constraints).</p>
</dd>
<dt><code>loss</code></dt><dd><p>a character string specifying the <em>Loss function</em>. One of <em>&quot;LS&quot;</em> (Square loss), <em>&quot;Expo&quot;</em> (Exponential loss) or <em>&quot;Log&quot;</em> (Logistic loss).</p>
</dd>
<dt><code>reg_depth</code></dt><dd><p>a float. Must be no smaller than 1.0. Meant for being used with the algorithm <em>RGF Opt</em> or <em>RGF Sib</em>. A larger value penalizes deeper nodes more severely.</p>
</dd>
<dt><code>l2</code></dt><dd><p>a float. Used to control the degree of L2 regularization.</p>
</dd>
<dt><code>sl2</code></dt><dd><p>a float or NULL. Override L2 regularization parameter l2 for the process of growing the forest. That is, if specified, the weight correction process uses l2 and the forest growing process uses sl2. If NULL, no override takes place and l2 is used throughout training.</p>
</dd>
<dt><code>normalize</code></dt><dd><p>a boolean. If True, training targets are normalized so that the average becomes zero.</p>
</dd>
<dt><code>min_samples_leaf</code></dt><dd><p>an integer or a float. Minimum number of training data points in each leaf node. If an integer, then consider <em>min_samples_leaf</em> as the minimum number. If a float, then <em>min_samples_leaf</em> is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.</p>
</dd>
<dt><code>n_iter</code></dt><dd><p>an integer or NULL. The number of iterations of coordinate descent to optimize weights. If NULL, 10 is used for loss = &quot;LS&quot; and 5 for loss = &quot;Expo&quot; or &quot;Log&quot;.</p>
</dd>
<dt><code>n_tree_search</code></dt><dd><p>an integer. The number of trees to be searched for the nodes to split. The most recently grown trees are searched first.</p>
</dd>
<dt><code>opt_interval</code></dt><dd><p>an integer. Weight optimization interval in terms of the number of leaf nodes. For example, by default, weight optimization is performed every time approximately 100 leaf nodes are newly added to the forest.</p>
</dd>
<dt><code>learning_rate</code></dt><dd><p>a float. Step size of Newton updates used in coordinate descent to optimize weights.</p>
</dd>
<dt><code>memory_policy</code></dt><dd><p>a character string. One of <em>&quot;conservative&quot;</em> (it uses less memory at the expense of longer runtime. Try only when with default value it uses too much memory) or <em>&quot;generous&quot;</em> (it runs faster using more memory by keeping the sorted orders of the features on memory for reuse). Memory using policy.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>an integer. Controls the verbosity of the tree building process.</p>
</dd>
<dt><code>init_model</code></dt><dd><p>either NULL or a character string, optional (default=NULL). Filename of a previously saved model from which training should do warm-start. If model has been saved into multiple files, do not include numerical suffixes in the filename. <em>NOTE:</em> Make sure you haven't forgotten to increase the value of the max_leaf parameter regarding to the specified warm-start model because warm-start model trees are counted in the overall number of trees.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-RGF_Regressor-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>RGF_Regressor$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>

<p><em>https://github.com/RGF-team/rgf/tree/master/python-package</em>, <em>Rie Johnson and Tong Zhang, Learning Nonlinear Functions Using Regularized Greedy Forest</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
try({
    if (reticulate::py_available(initialize = FALSE)) {
        if (reticulate::py_module_available("rgf.sklearn")) {

            library(RGF)

            set.seed(1)
            x = matrix(runif(1000), nrow = 100, ncol = 10)

            y = runif(100)

            RGF_regr = RGF_Regressor$new(max_leaf = 50)

            RGF_regr$fit(x, y)

            preds = RGF_regr$predict(x)
        }
    }
}, silent = TRUE)
</code></pre>

<hr>
<h2 id='TO_scipy_sparse'>conversion of an R sparse matrix to a scipy sparse matrix</h2><span id='topic+TO_scipy_sparse'></span>

<h3>Description</h3>

<p>conversion of an R sparse matrix to a scipy sparse matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TO_scipy_sparse(R_sparse_matrix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TO_scipy_sparse_+3A_r_sparse_matrix">R_sparse_matrix</code></td>
<td>
<p>an R sparse matrix. Acceptable input objects are either a <em>dgCMatrix</em> or a <em>dgRMatrix</em>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function allows the user to convert either an R <em>dgCMatrix</em> or a <em>dgRMatrix</em> to a scipy sparse matrix (<em>scipy.sparse.csc_matrix</em> or <em>scipy.sparse.csr_matrix</em>). This is useful because the <em>RGF</em> package accepts besides an R dense matrix also python sparse matrices as input.
</p>
<p>The <em>dgCMatrix</em> class is a class of sparse numeric matrices in the compressed, sparse, <em>column-oriented format</em>. The <em>dgRMatrix</em> class is a class of sparse numeric matrices in the compressed, sparse, <em>row-oriented format</em>.
</p>


<h3>References</h3>

<p>https://stat.ethz.ch/R-manual/R-devel/library/Matrix/html/dgCMatrix-class.html, https://stat.ethz.ch/R-manual/R-devel/library/Matrix/html/dgRMatrix-class.html, https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html#scipy.sparse.csc_matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
try({
    if (reticulate::py_available(initialize = FALSE)) {
        if (reticulate::py_module_available("scipy")) {

            if (Sys.info()["sysname"] != 'Darwin') {

                library(RGF)


                # 'dgCMatrix' sparse matrix
                #--------------------------

                data = c(1, 0, 2, 0, 0, 3, 4, 5, 6)

                dgcM = Matrix::Matrix(
                    data = data
                    , nrow = 3
                    , ncol = 3
                    , byrow = TRUE
                    , sparse = TRUE
                )

                print(dim(dgcM))

                res = TO_scipy_sparse(dgcM)

                print(res$shape)


                # 'dgRMatrix' sparse matrix
                #--------------------------

                dgrM = as(dgcM, "RsparseMatrix")

                print(dim(dgrM))

                res_dgr = TO_scipy_sparse(dgrM)

                print(res_dgr$shape)
            }
        }
    }
}, silent = TRUE)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
