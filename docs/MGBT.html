<!DOCTYPE html><html><head><title>Help for package MGBT</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {MGBT}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ASlo'><p> Regression of a Heuristic Method for Identification of Low Outliers in Texas Annual Peak Streamflow</p></a></li>
<li><a href='#BLlo'><p> Barnett and Lewis Test Adjusted for Low Outliers</p></a></li>
<li><a href='#CondMomsChi2'><p> Conditional Moments: N.B. Moments employ only observations above Xsi</p></a></li>
<li><a href='#CondMomsZ'><p> Conditional Moments: N.B. Moments employ only observations above Xsi</p></a></li>
<li><a href='#CritK'><p> Compute Critical Value of Grubbs&ndash;Beck statistic (eta) Given Probability</p></a></li>
<li><a href='#critK10'><p> Single Grubbs&ndash;Beck Critical Values for 10-percent Test as used in Bulletin 17B</p></a></li>
<li><a href='#EMS'><p> Expected values of M and S</p></a></li>
<li><a href='#GGBK'><p> Cohn Approximation for New Generalized Grubbs&ndash;Beck Critical Values for 10-Percent Test</p></a></li>
<li><a href='#gtmoms'><p> Moments of Observations Above the Threshold</p></a></li>
<li><a href='#makeWaterYear'><p> Make Water Year Column</p></a></li>
<li><a href='#MGBT'><p> Multiple Grubbs&ndash;Beck Test (MGBT) for Low Outliers</p></a></li>
<li><a href='#MGBT-package'><p>Multiple Grubbs&ndash;Beck Low-Outlier Test</p></a></li>
<li><a href='#peta'><p> Probability of Eta</p></a></li>
<li><a href='#plotFFQevol'><p> Plot Flood-Frequency in Time</p></a></li>
<li><a href='#plotPeaks'><p> Plot Peak Streamflows with Emphasis on Peak Discharge Qualification Codes</p></a></li>
<li><a href='#plotPeaks_batch'><p> Plot for More than One Streamgage that Peak Streamflows with Emphasis on Peak Discharge Qualification Codes</p></a></li>
<li><a href='#readNWISwatstore'><p>Read NWIS WATSTORE-Formatted Period of Record Peak Streamflows and Other Germane Operations</p></a></li>
<li><a href='#RSlo'><p> Rosner RST Test Adjusted for Low Outliers</p></a></li>
<li><a href='#RthOrderPValueOrthoT'><p> P-value for the Rth Order Statistic</p></a></li>
<li><a href='#splitPeakCodes'><p> Split the Peak Discharge Qualifications Codes into Separate Columns</p></a></li>
<li><a href='#V'><p>  Covariance matrix of M and S-squared</p></a></li>
<li><a href='#VMS'><p>  Covariance matrix of M and S</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Multiple Grubbs-Beck Low-Outlier Test</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.7</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>dataRetrieval, lmomco</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-07-20</td>
</tr>
<tr>
<td>Author:</td>
<td>William H. Asquith [aut, cre], John F. England [aut, ctb], George R. Herrmann [ctb]</td>
</tr>
<tr>
<td>Description:</td>
<td>Compute the multiple Grubbs-Beck low-outlier test on positively distributed
 data and utilities for noninterpretive U.S. Geological Survey annual peak-streamflow
 data processing discussed in Cohn et al. (2013) &lt;<a href="https://doi.org/10.1002%2Fwrcr.20392">doi:10.1002/wrcr.20392</a>&gt; and
 England et al. (2017) &lt;<a href="https://doi.org/10.3133%2Ftm4B5">doi:10.3133/tm4B5</a>&gt;.</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>William H. Asquith &lt;wasquith@usgs.gov&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://creativecommons.org/publicdomain/zero/1.0/legalcode">CC0</a></td>
</tr>
<tr>
<td>Copyright:</td>
<td>This software is in the public domain because it contains
materials that originally came from the United States
Geological Survey, an agency of the United States Department of
Interior. For more information, see the official USGS copyright
policy at
https://www.usgs.gov/information-policies-and-instructions/copyrights-and-credits</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://doi.org/10.5066/P9CW9EF0">https://doi.org/10.5066/P9CW9EF0</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-07-21 17:56:43 UTC; wasquith</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-07-21 18:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='ASlo'> Regression of a Heuristic Method for Identification of Low Outliers in Texas Annual Peak Streamflow </h2><span id='topic+ASlo'></span>

<h3>Description</h3>

<p>Asquith and others (1995) developed a regression equation based on the first three moments of non-low-outlier truncated annual peak streamflow data in an effort to semi-objectively compute low-outlier thresholds for log-Pearson type III (Bulletin 17B consistent; IACWD, 1982) analyses (Asquith and Slade, 1997). A career hydrologist in for USGS in Texas, Raymond M. Slade, Jr., was particularly emphatic that aggressive low-outlier identification is needed for Texas hydrology as protection from mixed population effects.
</p>
<p>WHA and RMS heuristically selected low-outlier thresholds for 262 streamgages in Texas with at least 20 years from unregulated and unurbanized watersheds. These thresholds were then regressed, along with help from Linda Judd, against the product moments of the logarithms (base-10) of the whole of the sample data (zeros not included). The regression equation is
</p>
<p style="text-align: center;"><code class="reqn">\log_{10}[AS_{\mathrm{Texas}}(\mu, \sigma, \gamma)]=1.09\mu-0.584\sigma+0.14\gamma - 0.799\mbox{,}</code>
</p>

<p>where <code class="reqn">AS_{\mathrm{Texas}}</code> is the low-outlier threshold, <code class="reqn">\mu</code> is the mean, <code class="reqn">\sigma</code> is the standard deviation, and <code class="reqn">\gamma</code> is skew. The R-squared is 0.75, and those authors unfortunately do not appear to list a residual standard error. The suggested limitations are <code class="reqn">1.9 &lt; \mu &lt; 4.842</code>, <code class="reqn">0.125 &lt; \sigma &lt; 1.814</code>, and <code class="reqn">-2.714 &lt; \gamma &lt; 0.698</code>.
</p>
<p>The <code class="reqn">AS_{\mathrm{Texas}}</code> equation was repeated in a footnote in Asquith and Roussel (2009, p. 19) because of difficulty in others acquiring copies of Asquith and others (1995). (File <code>AsquithLOT(1995).pdf</code> with this package is a copy.) Low-outlier thresholds using this regression were applied before the development of a generalized skew map in Texas (Judd and others, 1996) in turn used by Asquith and Slade (1997). A comparison of <code class="reqn">AS_{\mathrm{Texas}}</code> to the results of MGBT is shown in the <b>Examples</b>.
</p>
<p><b>The <code>ASlo</code> equation is no longer intended for any practical application with the advent of the MGBT approach.</b> It is provided here for historical context only and shows a heuristic line of thought independent from the mathematical rigor provided by TAC and others leading to MGBT. The <code class="reqn">AS_{\mathrm{Texas}}</code> incidentally was an extensive topic of long conversation between WHA and TAC at the National Surface Water Conference and Hydroacoustics Workshop (USGS Office of Surface Water), March 28&ndash;April 1, 2011, Tampa, Florida. The conversation was focused on the critical need for aggressive low-outlier identification in arid to semi-arid regions such as Texas. TAC was showcasing MGBT on a poster.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ASlo(mu, sigma, gamma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ASlo_+3A_mu">mu</code></td>
<td>
<p>The arthimetic mean of the logarithms of non-low-outlier truncated annual peak streamflow data (zeros removed);</p>
</td></tr>
<tr><td><code id="ASlo_+3A_sigma">sigma</code></td>
<td>
<p>The standard deviation of the logarithms of non-low-outlier truncated annual peak streamflow data (zeros removed); and</p>
</td></tr>
<tr><td><code id="ASlo_+3A_gamma">gamma</code></td>
<td>
<p>The skewness (product moment) of the logarithms of non-low-outlier truncated annual peak streamflow data (zeros removed).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The value for the regression equation <code class="reqn">AS_{\mathrm{Texas}}(\mu, \sigma, \gamma)</code> after re-transformation.
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith</p>


<h3>Source</h3>

<p>Original <span class="rlang"><b>R</b></span> by WHA for this package.
</p>


<h3>References</h3>

<p>Asquith, W.H., 2019, lmomco&mdash;L-moments, trimmed L-moments, L-comoments, censored <br /> L-moments, and many distributions: R package version 2.3.2 (September 20, 2018), accessed March 30, 2019, at <a href="https://cran.r-project.org/package=lmomco">https://cran.r-project.org/package=lmomco</a>.
</p>
<p>Asquith, W.H., Slade, R.M., and Judd, Linda, 1995, Analysis of low-outlier thresholds for log-Pearson type III peak-streamflow frequency analysis in Texas, in Texas Water *95, American Society of Civil Engineers First International Conference, San Antonio, Texas, 1995, Proceedings: San Antonio, Texas, American Society of Civil Engineers, pp. 379&ndash;384.
</p>
<p>Asquith, W.H., and Slade, R.M., 1997, Regional equations for estimation of peak-streamflow frequency for natural basins in Texas: U.S. Geological Survey Water-Resources Investigations Report 96&ndash;4307, 68 p., <a href="https://pubs.usgs.gov/wri/wri964307/">https://pubs.usgs.gov/wri/wri964307/</a>
</p>
<p>Asquith, W.H., and Roussel, M.C., 2009, Regression equations for estimation of annual peak-streamflow frequency for undeveloped watersheds in Texas using an L-moment-based, PRESS-minimized, residual-adjusted approach: U.S. Geological Survey Scientific Investigations Report 2009&ndash;5087, 48 p., <a href="https://pubs.usgs.gov/sir/2009/5087/">https://pubs.usgs.gov/sir/2009/5087/</a>.
</p>
<p>Interagency Advisory Committee on Water Data (IACWD), 1982, Guidelines for determining flood flow frequency: Bulletin 17B of the Hydrology Subcommittee, Office of Water Data Coordination, U.S. Geological Survey, Reston, Va., 183 p.
</p>
<p>Judd, Linda, Asquith, W.H., and Slade, R.M., 1996, Techniques to estimate generalized skew coefficients of annual peak streamflow for natural basins in Texas: U.S. Geological Survey Water Resources Investigations Report 96&ndash;4117, 28 p., <a href="https://pubs.usgs.gov/wri/wri97-4117/">https://pubs.usgs.gov/wri/wri97-4117/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># USGS 08066300 (1966--2016) # cubic feet per second (cfs)
#https://nwis.waterdata.usgs.gov/nwis/peak?site_no=08066300&amp;format=hn2
Peaks &lt;- c(3530, 284, 1810, 9660,  489,  292, 1000,  2640, 2910, 1900,  1120, 1020,
   632, 7160, 1750,  2730,  1630, 8210, 4270, 1730, 13200, 2550,  915, 11000, 2370,
  2230, 4650, 2750,  1860, 13700, 2290, 3390, 5160, 13200,  410, 1890,  4120, 3930,
  4290, 1890, 1480, 10300,  1190, 2320, 2480, 55.0,  7480,  351,  738,  2430, 6700)
#ASlo(3.3472,     0.4865,    -0.752)     # moments from USGS-PeakFQ (v7.1)
 ASlo(3.34715594, 0.4865250, -0.7517086) # values from lmomco::pmoms(log10(Peaks))
# computes 288 cubic feet per second, and now compare this to MGBT()
# MGBT(Peaks)$LOThres # computes 284 cubic feet per second
# ---***--------------***--- Remarkable similarity! ---***--------------***---
# ---***--------------***--- Not true in all cases. ---***--------------***---
</code></pre>

<hr>
<h2 id='BLlo'> Barnett and Lewis Test Adjusted for Low Outliers </h2><span id='topic+BLlo'></span>

<h3>Description</h3>

<p>The Barnett and Lewis (1995, p. 224; <code class="reqn">T_{\mathrm{N}3}</code>) so-labeled &ldquo;N3 method&rdquo; with TAC adjustment to look for low outliers. The essence of the method, given the order statistics <code class="reqn">x_{[1:n]} \le x_{[2:n]} \le \cdots \le x_{[(n-1):n]} \le x_{[n:n]}</code>, is the statistic
</p>
<p style="text-align: center;"><code class="reqn">BL_r = T_{\mathrm{N}3} =
\frac{ \sum_{i=1}^r x_{[i:n]} - r \times \mathrm{mean}\{x_{[1:n]}\} }
     {\sqrt{\mathrm{var}\{x_{[1:n]}\}}}\mbox{,}</code>
</p>

<p>for the mean and variance of the observations.  Barnett and Lewis (1995, p. 218) brand this statistic as a test of the &ldquo;<code class="reqn">k \ge 2</code> upper outliers&rdquo; but for the <span class="pkg">MGBT</span> package &ldquo;lower&rdquo; applies in TAC reformulation. Barnett and Lewis (1995, p. 218) show an example of a modification for two low outliers as <code class="reqn">(2\overline{x} - x_{[2:n]} - x_{[1:n]})/s</code> for the mean <code class="reqn">\mu</code> and standard deviation <code class="reqn">s</code>. TAC reformulation thus differs by a sign. The <code class="reqn">BL_r</code> is a sum of internally studentized deviations from the mean:
</p>
<p style="text-align: center;"><code class="reqn">SP(t) \le {n \choose k} P\biggl(\bm{t}(n-2) &gt; \biggr[\frac{n(n-2)t^2}{r(n-r)(n-1)-nt^2}\biggl]^{1/2}\biggr)\mbox{,}</code>
</p>

<p>where <code class="reqn">\bm{t}(df)</code> is the t-distribution for <code class="reqn">df</code> degrees of freedom, and this is an inequality when
</p>
<p style="text-align: center;"><code class="reqn">t \ge \sqrt{r^2(n-1)(n-r-1)/(nr+n)}\mbox{,}</code>
</p>

<p>where <code class="reqn">SP(t)</code> is the probability that <code class="reqn">T_{\mathrm{N}3} &gt; t</code> when the inequality holds. For reference, Barnett and Lewis (1995, p. 491) example tables of critical values for <code class="reqn">n=10</code> for <code class="reqn">k \in 2,3,4</code> at 5-percent significant level are <code class="reqn">3.18</code>, <code class="reqn">3.82</code>, and <code class="reqn">4.17</code>, respectively. One of these is evaluated in the <b>Examples</b>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BLlo(x, r, n=length(x))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BLlo_+3A_x">x</code></td>
<td>
<p>The data values and note that base-10 logarithms of these are not computed internally;</p>
</td></tr>
<tr><td><code id="BLlo_+3A_r">r</code></td>
<td>
<p>The number of truncated observations; and</p>
</td></tr>
<tr><td><code id="BLlo_+3A_n">n</code></td>
<td>
<p>The number of observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The value for <code class="reqn">BL_r</code>.
</p>


<h3>Note</h3>

<p>Regarding <code>n=length(x)</code>, it is not clear that TAC intended <code>n</code> to be not equal to the sample size. TAC chose to not determine the length of <code>x</code> internally to the function but to have it available as an argument. Also <code><a href="#topic+MGBTcohn2011">MGBTcohn2011</a></code> and <code><a href="#topic+RSlo">RSlo</a></code> were designed similarly.
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith consulting T.A. Cohn sources </p>


<h3>Source</h3>

<p><code>LowOutliers_jfe(R).txt</code> and <code>LowOutliers_wha(R).txt</code>&mdash;Named <code>BL_N3</code>
</p>


<h3>References</h3>

<p>Barnett, Vic, and Lewis, Toby, 1995, Outliers in statistical data: Chichester, John Wiley and Sons, ISBN~0&ndash;471&ndash;93094&ndash;6.
</p>
<p>Cohn, T.A., 2013&ndash;2016, Personal communication of original R source code: U.S. Geological Survey, Reston, Va.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MGBTcohn2011">MGBTcohn2011</a></code>, <code><a href="#topic+RSlo">RSlo</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See Examples under RSlo()

 # WHA experiments with BL_r()
n &lt;- 10; r &lt;- 3; nsim &lt;- 10000; alpha &lt;- 0.05; Tcrit &lt;- 3.82
BLs &lt;- Ho &lt;- RHS &lt;- SPt &lt;- rep(NA, nsim)
EQ &lt;- sqrt(r^2*(n-1)*(n-r-1)/(n*r+n))
for(i in 1:nsim) { # some simulation results shown below
   BLs[i] &lt;- abs(BLlo(rnorm(n), r)) # abs() correcting TAC sign convention
   t  &lt;- sqrt( (n*(n-2)*BLs[i]^2) / (r*(n-r)*(n-1)-n*BLs[i]^2) )
   RHS[i] &lt;- choose(n,r)*pt(t, n-2, lower.tail=FALSE)
   ifelse(t &gt;= EQ, SPt[i] &lt;- RHS[i], SPt[i] &lt;- 1) # set SP(t) to unity?
   Ho[i]  &lt;- BLs[i] &gt; Tcrit
}
results &lt;- c(quantile(BLs, prob=1-alpha), sum(Ho /nsim), sum(SPt &lt; alpha)/nsim)
names(results) &lt;- c("Critical_value", "Ho_rejected", "Coverage_SP(t)")
print(results) # minor differences are because of random number seeding
# Critical_value    Ho_rejected Coverage_SP(t)
#      3.817236       0.048200       0.050100 
</code></pre>

<hr>
<h2 id='CondMomsChi2'> Conditional Moments: N.B. Moments employ only observations above Xsi </h2><span id='topic+CondMomsChi2'></span>

<h3>Description</h3>

<p>Compute the <code class="reqn">\chi^2</code>-conditional moments (Chi-squared distributed moments) based on only those (<code class="reqn">n</code> <code class="reqn">-</code> <code class="reqn">r</code>) observations above a threshold <code class="reqn">X_{si}</code> for a sample size of <code class="reqn">n</code> and <code class="reqn">r</code> number of truncated observations. The first moment is <code>(gtmoms(xsi,2) -</code> <code>gtmoms(xsi,1)^2)</code> that is in the first returned column. The second moment (variance of S-squared) is <code>V(n,r,pnorm(xsi))[2,2]</code> that is in the second returned column. Further mathematical details are available under functions <code><a href="#topic+gtmoms">gtmoms</a></code> and <code><a href="#topic+V">V</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CondMomsChi2(n, r, xsi)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CondMomsChi2_+3A_n">n</code></td>
<td>
<p>The number of observations;</p>
</td></tr>
<tr><td><code id="CondMomsChi2_+3A_r">r</code></td>
<td>
<p>The number of truncated observations; and</p>
</td></tr>
<tr><td><code id="CondMomsChi2_+3A_xsi">xsi</code></td>
<td>
<p>The lower threshold (see <code><a href="#topic+gtmoms">gtmoms</a></code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The value a two-column, one-row <span class="rlang"><b>R</b></span> <code>matrix</code>.
</p>


<h3>Note</h3>

<p>TAC sources define a <code><a href="#topic+CondMomsZ">CondMomsZ</a></code> function along with this function. However, the <code><a href="#topic+CondMomsZ">CondMomsZ</a></code> function appears to not be used for any purpose. Only the <code>CondMomsChi2</code> is needed for the MGBT test.
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith consulting T.A. Cohn sources </p>


<h3>Source</h3>

<p><code>LowOutliers_jfe(R).txt</code>, <code>LowOutliers_wha(R).txt</code>, <code>P3_089(R).txt</code>&mdash;Named <code>CondMomsChi2</code>
</p>


<h3>References</h3>

<p>Cohn, T.A., 2013&ndash;2016, Personal communication of original R source code: U.S. Geological Survey, Reston, Va.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CondMomsZ">CondMomsZ</a></code>, <code><a href="#topic+gtmoms">gtmoms</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>CondMomsChi2(58, 2, -3.561143)
#          [,1]       [,2]
#[1,] 0.9974947 0.03574786

# Note that CondMomsChi2(58, 2, -3.561143)[2] == V(58, 2, pnorm(-3.561143))[2,2]
</code></pre>

<hr>
<h2 id='CondMomsZ'> Conditional Moments: N.B. Moments employ only observations above Xsi </h2><span id='topic+CondMomsZ'></span>

<h3>Description</h3>

<p>Compute the <code class="reqn">Z</code>-conditional moments (standard normal distributed moments) based on only those (<code class="reqn">n</code> <code class="reqn">-</code> <code class="reqn">r</code>) observations above a threshold <code class="reqn">X_{si}</code> for a sample size of <code class="reqn">n</code> and <code class="reqn">r</code> number of truncated observations. The first moment is <code>gtmoms(xsi,1)</code>, which is in the first returned column. The second moment is
</p>
<pre>
  (gtmoms(xsi,2) - gtmoms(xsi,1)^2)/(n-r)
</pre><p> that is in the second returned column. Further mathematical details are available under <code><a href="#topic+gtmoms">gtmoms</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CondMomsZ(n, r, xsi)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CondMomsZ_+3A_n">n</code></td>
<td>
<p>The number of observations;</p>
</td></tr>
<tr><td><code id="CondMomsZ_+3A_r">r</code></td>
<td>
<p>The number of truncated observations; and</p>
</td></tr>
<tr><td><code id="CondMomsZ_+3A_xsi">xsi</code></td>
<td>
<p>The lower threshold (see <code><a href="#topic+gtmoms">gtmoms</a></code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The value a two-column, one-row <span class="rlang"><b>R</b></span> <code>matrix</code>.
</p>


<h3>Note</h3>

<p>The <code>CondMomsZ</code> function appears to not be used for any purpose. Only the <code>CondMomsChi2</code> function is needed for MGBT. The author WHA hypothesizes that TAC has the simple logic of this function constructed in long hand as needed within other functions&mdash;Rigorous inquiry of TAC's design purposes is not possible.
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith consulting T.A. Cohn sources </p>


<h3>Source</h3>

<p><code>LowOutliers_jfe(R).txt</code>, <code>LowOutliers_wha(R).txt</code>, <code>P3_089(R).txt</code>&mdash;Named <code>CondMomsZ</code>
</p>


<h3>References</h3>

<p>Cohn, T.A., 2013&ndash;2016, Personal communication of original R source code: U.S. Geological Survey, Reston, Va.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CondMomsChi2">CondMomsChi2</a></code>, <code><a href="#topic+gtmoms">gtmoms</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>CondMomsZ(58, 2, -3.561143)
#             [,1]       [,2]
#[1,] 0.0007033727 0.01781241
</code></pre>

<hr>
<h2 id='CritK'> Compute Critical Value of Grubbs&ndash;Beck statistic (eta) Given Probability </h2><span id='topic+CritK'></span>

<h3>Description</h3>

<p>Compute critical value for the Grubbs&ndash;Beck statistic (<code>eta</code> = <code class="reqn">GB_r(p)</code>) given a probability (p-value), which is the &ldquo;pseudo-studentized&rdquo; magnitude of <code class="reqn">r</code>th smallest observation. The <code>CritK</code> function is the same as the <code class="reqn">GB_r(p)</code> quantile function.  In distribution notation, this is equivalent to saying <code class="reqn">GB_r(F)</code> for nonexceedance probability <code class="reqn">F \in (0,1)</code>, and cumulative distribution function <code class="reqn">F(GB_r)</code> is the value that comes from <code><a href="#topic+RthOrderPValueOrthoT">RthOrderPValueOrthoT</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CritK(n, r, p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CritK_+3A_n">n</code></td>
<td>
<p>The number of observations;</p>
</td></tr>
<tr><td><code id="CritK_+3A_r">r</code></td>
<td>
<p>The number of truncated observations; and</p>
</td></tr>
<tr><td><code id="CritK_+3A_p">p</code></td>
<td>
<p>The probability value (p-value).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The critical value of the Grubbs&ndash;Beck statistic (<code>eta</code> = <code class="reqn">GB_r(p)</code>).
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith  consulting T.A. Cohn sources </p>


<h3>Source</h3>

<p><code>LowOutliers_jfe(R).txt</code>, <code>LowOutliers_wha(R).txt</code>, not <code>P3_089(R).txt</code>&mdash;Named: <code>CritK</code>
</p>


<h3>References</h3>

<p>Cohn, T.A., 2013&ndash;2016, Personal communication of original R source code: U.S. Geological Survey, Reston, Va.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+critK10">critK10</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
CritK(58, 2, .001) # CPU heavy: -3.561143
</code></pre>

<hr>
<h2 id='critK10'> Single Grubbs&ndash;Beck Critical Values for 10-percent Test as used in Bulletin 17B </h2><span id='topic+critK10'></span>

<h3>Description</h3>

<p>Return the critical values at the 10-percent (<code class="reqn">\alpha_\mathrm{17B} = 0.10</code>) significance level for the single Grubbs&ndash;Beck test as in Bulletin 17B (IACWD, 1982).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>critK10(n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="critK10_+3A_n">n</code></td>
<td>
<p>The number of observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The critical value for sample size <code class="reqn">n</code> unless it is outside the range <code class="reqn">10 \le n \le 149</code> for which the critical value is <code>NA</code>.
</p>


<h3>Note</h3>

<p>In the context of <code>critK10</code>, TAC defines a <code>.kngb()</code> function, which is recast as <code>KJRS()</code> in the <b>Examples</b>. The function appears to be an approximation attributable to Jery R. Stedinger. The <b>Examples</b> show a &ldquo;test&rdquo; as working notes of TAC.
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith consulting T.A. Cohn sources </p>


<h3>Source</h3>

<p><code>LowOutliers_jfe(R).txt</code>, <code>LowOutliers_wha(R).txt</code>, not <code>P3_089(R).txt</code>&mdash;Named <code>critK10</code>
</p>


<h3>References</h3>

<p>Cohn, T.A., 2013&ndash;2016, Personal communication of original R source code: U.S. Geological Survey, Reston, Va.
</p>
<p>Interagency Advisory Committee on Water Data (IACWD), 1982, Guidelines for determining flood flow frequency: Bulletin 17B of the Hydrology Subcommittee, Office of Water Data Coordination, U.S. Geological Survey, Reston, Va., 183 p.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CritK">CritK</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>critK10(58)
#[1] 2.824

 # Modified slightly from TAC sources (Original has the # Not run:)
# KJRS() is the ".kngb()" function in TAC sources
n &lt;- 10:149; KJRS &lt;- function(n) -0.9043+3.345*sqrt(log10(n))-0.4046*log10(n)
result &lt;- data.frame(n=n, Ktrue=sapply(n, critK10), # 17B single Grubbs--Beck
                          KJRS= sapply(n, KJRS   )) # name mimic of TAC sources 

## Not run:  # Near verbatim from TAC sources, GGBK() does not work, issues a stop().
# KJRS() is the ".kngb()" function in TAC sources
n &lt;- 10:149; KJRS &lt;- function(n) -0.9043+3.345*sqrt(log10(n))-0.4046*log10(n)
result &lt;- data.frame(n=n, Ktrue=sapply(n, critK10), # 17B single Grubbs--Beck
                          KJRS= sapply(n, KJRS   ), # name mimic of TAC sources
                          KTAC= sapply(n, GGBK   )) # name mimic of TAC sources
## End(Not run)
</code></pre>

<hr>
<h2 id='EMS'> Expected values of M and S </h2><span id='topic+EMS'></span>

<h3>Description</h3>

<p>Compute expected values of <code class="reqn">M</code> and <code class="reqn">S</code> given <code class="reqn">q_\mathrm{min}</code> and define the quantity
</p>
<p style="text-align: center;"><code class="reqn">z_r = \Phi^{(1)}(q_\mathrm{min})\mbox{,}</code>
</p>

<p>where <code class="reqn">\Phi^{(1)}(\cdot)</code> is the inverse of the standard normal distribution. As result, <code class="reqn">q_\mathrm{min}</code> is itself a probability because it is an argument to the <code>qnorm()</code> function. The expected value <code class="reqn">M</code> is defined as
</p>
<p style="text-align: center;"><code class="reqn">M = \Psi(z_r, 1)\mbox{,}</code>
</p>

<p>where <code class="reqn">\Psi(a,b)</code> is the <code><a href="#topic+gtmoms">gtmoms</a></code> function. The <code class="reqn">S</code> requires the conditional moments of the Chi-square (<code><a href="#topic+CondMomsChi2">CondMomsChi2</a></code>) defined as the two value vector <code class="reqn">\mbox{}_2S</code> that provides the values <code class="reqn">\alpha = \mbox{}_2S_1^2 / \mbox{}_2S_2</code> and <code class="reqn">\beta = \mbox{}_2S_2 / \mbox{}_2S_1</code>. The <code class="reqn">S</code> is then defined by
</p>
<p style="text-align: center;"><code class="reqn">S = \sqrt{\beta}\biggl(\frac{\Gamma(\alpha+0.5)}{\Gamma(\alpha)}\biggr)\mbox{.}</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>EMS(n, r, qmin)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EMS_+3A_n">n</code></td>
<td>
<p>The number of observations;</p>
</td></tr>
<tr><td><code id="EMS_+3A_r">r</code></td>
<td>
<p>The number of truncated observations? (confirm); and</p>
</td></tr>
<tr><td><code id="EMS_+3A_qmin">qmin</code></td>
<td>
<p>A nonexceedance probability threshold for <code class="reqn">X &gt; q_\mathrm{min}</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The expected values of <code class="reqn">M</code> and <code class="reqn">S</code> in the form of an <span class="rlang"><b>R</b></span> <code>vector</code>.
</p>


<h3>Note</h3>

<p>TAC sources call on the explicit first index of <code class="reqn">M</code> as literally &ldquo;<code>Em[1]</code>&rdquo; for the returned vector, which seems unnecessary. This is a potential weak point in design because the <code>gtmoms</code> function is naturally vectorized and could potentially produce a vector of <code class="reqn">M</code> values. For the implementation here, only the first value in <code>qmin</code> is used and a warning otherwise issued. Such coding prevents the return value from <code>EMS</code> accidentally acquiring a length greater than two. For at least samples of size <code class="reqn">n=2</code>, overranging in a call to <code>lgamma(alpha)</code> happens for <code>alpha=0</code>. A <code>suppressWarnings()</code> is wrapped around the applicable line. The resulting <code>NaN</code> cascades up the chain, which will end up inside <code><a href="#topic+peta">peta</a></code>, but therein <code>SigmaMp</code> is not finite and a p-value of unity is returned.
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith consulting T.A. Cohn sources </p>


<h3>Source</h3>

<p><code>LowOutliers_jfe(R).txt</code>, <code>LowOutliers_wha(R).txt</code>, <code>P3_089(R).txt</code>&mdash;Named <code>EMS</code>
</p>


<h3>References</h3>

<p>Cohn, T.A., 2013&ndash;2016, Personal communication of original R source code: U.S. Geological Survey, Reston, Va.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CondMomsChi2">CondMomsChi2</a></code>, <code><a href="#topic+EMS">EMS</a></code>, <code><a href="#topic+VMS">VMS</a></code>, <code><a href="#topic+V">V</a></code>, <code><a href="#topic+gtmoms">gtmoms</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>EMS(58,2,.5)
#[1] 0.7978846 0.5989138

#  Monte Carlo experiment to test EMS and VMS functions
"test_EMS" &lt;- function(nrep=1000, n=100, r=0, qr=0.2, ss=1) { # TAC named function
   set.seed(ss)
   Moms &lt;- replicate(n=nrep, {
          x &lt;- qnorm(runif(n-r,min=qr,max=1));
          c(mean(x),var(x))}); xsi &lt;- qnorm(qr);
          list(
    MeanMS_obs = c(mean(Moms[1,]), mean(sqrt(Moms[2,])), mean(Moms[2,])),
    EMS        = c(EMS(n,r,qr), gtmoms(xsi,2) - gtmoms(xsi,1)^2),
    CovMS2_obs = cov(t(Moms)),
    VMS2       = V(n,r,qr),
    VMS_obs    = array(c(var(     Moms[1,]),
                         rep(cov( Moms[1,], sqrt(Moms[2,])),2),
                         var(sqrt(Moms[2,]))),    dim=c(2,2)),
    VMS        = VMS(n,r,qr)  )
}
test_EMS()
</code></pre>

<hr>
<h2 id='GGBK'> Cohn Approximation for New Generalized Grubbs&ndash;Beck Critical Values for 10-Percent Test </h2><span id='topic+GGBK'></span>

<h3>Description</h3>

<p>Compute Cohn's approximation of critical values at the 10-percent significance level for the new generalized Grubbs&ndash;Beck test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GGBK(n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GGBK_+3A_n">n</code></td>
<td>
<p>The number of observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The critical value of the test was to be returned, but a <code>stop()</code> is issued instead because there is a problem with the function's call of <code><a href="#topic+CritK">CritK</a></code> (see <b>Note</b>).
</p>


<h3>Note</h3>

<p>In TAC sources, <code>GGBK</code> is the consumer of two global scope functions <code>fw()</code> and <code>fw1()</code>. These should be defined within the function to keep the scope local as they are unneeded anywhere else in TAC sources, and these thus have local scope in the implementation for the <span class="pkg">MGBT</span> package.
</p>
<p><b>A BUG FIX NEEDED&mdash;</b>Note that TAC has a problem in sources in that this function is incomplete. The function <code><a href="#topic+CritK">CritK</a></code> is the issue, that function requires three arguments and appears to work (see <b>Examples</b> under <code><a href="#topic+CritK">CritK</a></code>), but TAC code passes four in the context of <code>GGBK</code>. At present (packaging of <span class="pkg">MGBT</span>), it is not known if the word &ldquo;generalized&rdquo; in this test has the same meaning as &ldquo;multiple&rdquo; in the Multiple Grubbs&ndash;Beck Test. Also in TAC sources, it is as yet unclear what the &ldquo;new&rdquo; in the title of this function means.
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith consulting T.A. Cohn sources </p>


<h3>Source</h3>

<p><code>LowOutliers_jfe(R).txt</code>, <code>LowOutliers_wha(R).txt</code>, not <code>P3_089(R).txt</code>&mdash;Named <code>GGBK</code>
</p>


<h3>References</h3>

<p>Cohn, T.A., 2013&ndash;2016, Personal communication of original R source code: U.S. Geological Survey, Reston, Va.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+critK10">critK10</a></code>, <code><a href="#topic+CritK">CritK</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
GGBK(34) # but presently the function does not work 
## End(Not run)
</code></pre>

<hr>
<h2 id='gtmoms'> Moments of Observations Above the Threshold </h2><span id='topic+gtmoms'></span>

<h3>Description</h3>

<p>Moments of observations above the threshold (<code>xsi</code>, <code class="reqn">x_{si}</code>), which has been standardized to a zero mean and unit standard deviation. Define the standard normal hazard function as
</p>
<p style="text-align: center;"><code class="reqn">H(x) = \phi(x) / (1 - \Phi(x))\mbox{,}</code>
</p>

<p>where <code class="reqn">\phi(x)</code> is the standard normal density function and <code class="reqn">\Phi(x)</code> is the standard normal distribution (cumulative) function. For a truncation index, <code class="reqn">r</code>, define the recursion formula, <code class="reqn">\Psi</code> for <code>gtmoms</code> as
</p>
<p style="text-align: center;"><code class="reqn">\Psi(x_{si}, r) = (r-1)\Psi(x_{si}, r-2) + x_{si}^{r-1}H(x_{si})\mbox{,}</code>
</p>

<p>for which <code class="reqn">\Psi(x_{si}, 0) = 1</code> and <code class="reqn">\Psi(x_{si}, 1) = H(x_{si})</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gtmoms(xsi, r)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gtmoms_+3A_xsi">xsi</code></td>
<td>
<p>The lower threshold; and</p>
</td></tr>
<tr><td><code id="gtmoms_+3A_r">r</code></td>
<td>
<p>The number of truncated observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The moments.
</p>


<h3>Note</h3>

<p><b>AUTHOR TODO&mdash;</b>Note that is it not clear in TAC documentation that <code class="reqn">X_{si}</code> is a scalar or vector quantity, and <code>gtmoms</code> is automatically vectoral in the <span class="rlang"><b>R</b></span> idioms if <code class="reqn">X_{si}</code> is. Also it is not immediately clear <code class="reqn">X_{si}</code> is or is not one of the order statistics. Based on MGBT operation in USGS-PeakFQ output (USGS, 2014), the threshold is &ldquo;known&rdquo; no better in accuracy than one of the sample order statistics, so <code class="reqn">X_{si}</code> might be written <code class="reqn">x_{[r:n]}</code>. But this answer could be only restricted to a implementation in software and perhaps not theory. Finally, although the computations involve the standard normal distribution, the standardization form of <code class="reqn">X_{si}</code> is not yet confirmed during the WHA porting process.
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith consulting T.A. Cohn sources </p>


<h3>Source</h3>

<p><code>LowOutliers_jfe(R).txt</code>, <code>LowOutliers_wha(R).txt</code>, <code>P3_089(R).txt</code>&mdash;Named <code>gtmoms</code>
</p>


<h3>References</h3>

<p>Cohn, T.A., 2013&ndash;2016, Personal communication of original R source code: U.S. Geological Survey, Reston, Va.
</p>
<p>U.S. Geological Survey (USGS), 2018, PeakFQ&mdash;Flood frequency analysis based on Bulletin 17B and recommendations of the Advisory Committee on Water Information (ACWI) Subcommittee on Hydrology (SOH) Hydrologic Frequency Analysis Work Group (HFAWG), version 7.2: Accessed November 29, 2018, at <a href="https://water.usgs.gov/software/PeakFQ/">https://water.usgs.gov/software/PeakFQ/</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CondMomsChi2">CondMomsChi2</a></code>, <code><a href="#topic+gtmoms">gtmoms</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>gtmoms(-3.561143, 2)  # Is this a meaningful example?
#[1] 0.9974952
</code></pre>

<hr>
<h2 id='makeWaterYear'> Make Water Year Column </h2><span id='topic+makeWaterYear'></span>

<h3>Description</h3>

<p>Make water year, year, month, and day columns from the date stamp of a U.S. Geological Survey peak-streamflow data retrieval from the National Water Information System (NWIS) (U.S. Geological Survey, 2019) in an <span class="rlang"><b>R</b></span> <code>data.frame</code> into separate columns of the input <code>data.frame</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeWaterYear(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makeWaterYear_+3A_x">x</code></td>
<td>
<p>A <code>data.frame</code> having a mandatory column titled <code>peak_dt</code> presented by <br /> <code>as.character</code>.  No other information in <code>x</code> is consulted or otherwise used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>x</code> is returned with the addition of these columns:
</p>
<table>
<tr><td><code>year_va</code></td>
<td>
<p>The calendar year extracted from <code>peak_dt</code>;</p>
</td></tr>
<tr><td><code>month_va</code></td>
<td>
<p>The optional month extracted from <code>peak_dt</code>;</p>
</td></tr>
<tr><td><code>day_va</code></td>
<td>
<p>The optional day extracted from <code>peak_dt</code>; and</p>
</td></tr>
<tr><td><code>water_yr</code></td>
<td>
<p>The water year, which is not equal to <code>year_va</code> if <code>month_va</code> is greater than or equal to 10 (October).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> W.H. Asquith</p>


<h3>References</h3>

<p>U.S. Geological Survey, 2019, USGS water data for the Nation: U.S. Geological Survey National Water Information System database, accessed October 11, 2019, at doi: <a href="https://doi.org/10.5066/F7P55KJN">10.5066/F7P55KJN</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+splitPeakCodes">splitPeakCodes</a></code>, <code><a href="#topic+plotPeaks">plotPeaks</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  # The dataRetrieval package is not required by MGBT algorithms.
  PK &lt;- dataRetrieval::readNWISpeak("08167000", convertType=FALSE)
  PK &lt;- makeWaterYear(PK) # Note: The convertType=FALSE is critical.
  names(PK) # See that the columns are there.
</code></pre>

<hr>
<h2 id='MGBT'> Multiple Grubbs&ndash;Beck Test (MGBT) for Low Outliers </h2><span id='topic+MGBT'></span><span id='topic+MGBTnb'></span><span id='topic+MGBTcohn2016'></span><span id='topic+MGBTcohn2013'></span><span id='topic+MGBTcohn2011'></span><span id='topic+MGBT17c'></span><span id='topic+MGBT17c.verb'></span>

<h3>Description</h3>

<p>Perform the Multiple Grubbs&ndash;Beck Test (MGBT; Cohn and others, 2013) for low outliers (LOTs, low-outlier threshold; potentially influential low floods, PILFs) that is implemented in the USGS-PeakFQ software (USGS, 2014; Veilleux and others, 2014) for implementation of Bulletin 17C (B17C) (England and others, 2018). The test internally transforms the data to logarithms (base-10) and thus is oriented for positively distributed data but accommodates zeros in the dataset.
</p>
<p>The essence of the MGBT, given the order statistics <code class="reqn">x_{[1:n]} \le x_{[2:n]} \le \cdots \le x_{[(n-1):n]} \le x_{[n:n]}</code>, is the statistic
</p>
<p style="text-align: center;"><code class="reqn">GB_r = \omega_r =
\frac{ x_{[r:n]} - \mathrm{mean}\{x_{[(r+1)\rightarrow n:n]}\} }
     {\sqrt{\mathrm{var}\{x_{[(r+1)\rightarrow n:n]}\}}}\mbox{,}
</code>
</p>

<p>which is can be computed by <code>MGBTcohn2011</code> that is a port a function of TAC's used in a testing script that is reproduced in the <b>Examples</b> of <code><a href="#topic+RSlo">RSlo</a></code>. Variations of this pseudo-standardization scheme are shown for <code><a href="#topic+BLlo">BLlo</a></code> and <code><a href="#topic+RSlo">RSlo</a></code>. Also, <code class="reqn">GB_r</code> is the canonical form of the variable <code>eta</code> in TAC sources and <code>peta</code>=<code><a href="#topic+peta">peta</a></code> will be its associated probability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MGBT(...) # A wrapper on MGBT17C()---This is THE function for end users.

     MGBT17c(x, alphaout=0.005, alphain=0, alphazeroin=0.10,
                n2=floor(length(x)/2), napv.zero=TRUE, offset=0, min.obs=0)
MGBT17c.verb(x, alphaout=0.005, alphain=0, alphazeroin=0.10,
                n2=floor(length(x)/2), napv.zero=TRUE, offset=0, min.obs=0)

MGBTcohn2016(x, alphaout=0.005, alphazeroin=0.10, n2=floor(length(x)/2),
                napv.zero=TRUE, offset=0)
MGBTcohn2013(x, alphaout=0.005, alphazeroin=0.10, n2=floor(length(x)/2),
                napv.zero=TRUE, offset=0)
      MGBTnb(x, alphaout=0.005, alphazeroin=0.10, n2=floor(length(x)/2),
                napv.zero=TRUE, offset=0)

MGBTcohn2011(x, r=NULL, n=length(x)) # only computes the GB_r, not a test
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MGBT_+3A_...">...</code></td>
<td>
<p>Arguments to pass to the MGBT family of functions;</p>
</td></tr>
<tr><td><code id="MGBT_+3A_x">x</code></td>
<td>
<p>The data values and note that base-10 logarithms of these are computed internally except for the operation of the <code>MGBTcohn2011</code> function, which does not (see <b>Examples</b> for <code><a href="#topic+RSlo">RSlo</a></code>). Also protection from zero or negative values is made by the <span class="rlang"><b>R</b></span> function <code>pmax</code>, and these values are replaced with a &ldquo;small&rdquo; value of <code>1e-8</code> and tacitly TAC has assumed that p-values for these will be significantly small and truncated away;</p>
</td></tr>
<tr><td><code id="MGBT_+3A_alphaout">alphaout</code></td>
<td>
<p>Literally the <code class="reqn">\alpha_\mathrm{out}</code> of Bulletin 17C. This is the significance level of the &ldquo;sweep out&rdquo; portion of MGBT;</p>
</td></tr>
<tr><td><code id="MGBT_+3A_alphain">alphain</code></td>
<td>
<p>This is the significance level of the &ldquo;sweep in&rdquo; portion of MGBT but starts at one plus the order statistic identified by <code>alphaout</code>;</p>
</td></tr>
<tr><td><code id="MGBT_+3A_alphazeroin">alphazeroin</code></td>
<td>
<p>Literally the <code class="reqn">\alpha_\mathrm{in}</code> of Bulletin 17C. This is the significance level of the &ldquo;sweep in&rdquo; portion of MGBT;</p>
</td></tr>
<tr><td><code id="MGBT_+3A_napv.zero">napv.zero</code></td>
<td>
<p>A logical switch to reset a returned <code>NA</code> from <code><a href="#topic+RthOrderPValueOrthoT">RthOrderPValueOrthoT</a></code> to zero. This is a unique extension by WHA based on large-scale batch testing of the USGS peak-values database (see <b>Note</b>). This being said, the fall-back to Monte Carlo integration if the numerical integration fails, seems to mostly make this argument superfluous;</p>
</td></tr>
<tr><td><code id="MGBT_+3A_offset">offset</code></td>
<td>
<p>The offset, if not <code>NA</code>, is added from the threshold unless the threshold itself is already zero. In practical application, this offset, if set, would likely be a negative quantity. This argument is a unique extension by WHA;</p>
</td></tr>
<tr><td><code id="MGBT_+3A_min.obs">min.obs</code></td>
<td>
<p>The minimum number of observations. This option is provided to streamline larger applications, but the underlying logic in <code>MGBT17C</code> is robust and on failures because of small sizes return a threshold of <code>0</code> anyway;</p>
</td></tr>
<tr><td><code id="MGBT_+3A_n2">n2</code></td>
<td>
<p>The number of <code>n2</code>-smallest values to be evaluated in the MGBT;</p>
</td></tr>
<tr><td><code id="MGBT_+3A_r">r</code></td>
<td>
<p>The number of truncated observations, which can be though of the rth order statistic and below; and</p>
</td></tr>
<tr><td><code id="MGBT_+3A_n">n</code></td>
<td>
<p>The number of observations. It is not clear that TAC intended <code>n</code> to be not equal to the sample size but TAC chose to not keep the length of <code>x</code> as determined internally to the function but to have it also available as an argument. Functions <code><a href="#topic+BLlo">BLlo</a></code> and <code><a href="#topic+RSlo">RSlo</a></code> also were designed similarly.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The MGBT results as an <span class="rlang"><b>R</b></span> <code>list</code>:
</p>
<table>
<tr><td><code>index</code></td>
<td>
<p>The sample size <code class="reqn">n</code>, the value for <code>n2</code>, and the three indices of the &ldquo;sweep out,&rdquo; &ldquo;sweep in,&rdquo; and &ldquo;sweep in from zero&rdquo; processing (only for <code>MGBT17c</code> as this is an extension from TAC);</p>
</td></tr>
<tr><td><code>omegas</code></td>
<td>
<p>The <code class="reqn">GB_r = \omega_r</code> statistics for which the p-values in <code>pvalues</code> are shown. These are mostly returned for aid in debugging and verification of the algorithms;</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>The <code>n2</code>-smallest values in increasing order (only for <code>MGBT17c</code> as this is an extension from TAC);</p>
</td></tr>
<tr><td><code>pvalues</code></td>
<td>
<p>The p-values of the <code>n2</code>-smallest values of the sample (not available for <code>MGBT17c</code> because of algorithm design for speed);</p>
</td></tr>
<tr><td><code>klow</code></td>
<td>
<p>The number of low outliers detected;</p>
</td></tr>
<tr><td><code>LOThresh</code></td>
<td>
<p>The low-outlier threshold for the <code>klow+1</code> index of the sample (and possibly adjusted by the <code>offset</code>) or simply zero; and</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>Possibly message in event of some internal difficulty.</p>
</td></tr>
</table>
<p>The inclusion of <code>x</code> in the returned value is to add symmetry because the p-values are present. The inclusion of <code class="reqn">n</code> and <code>n2</code> might make percentage computations of inward and outward sweep indices useful in exploratory analyses. Finally, the inclusion of the sweep indices is important as it was through inspection of these that the problems in TAC sources were discovered.
</p>


<h3>Note</h3>

<p><b>Porting from TAC sources</b>&mdash;TAC used MGBT for flood-frequency computations by a call
</p>
<pre>
  oMGBT &lt;- MGBT(Q=o_in@qu[o_in@typeSystematic])
</pre>
<p>in file <code>P3_089(R).txt</code>, and note the named argument <code>Q=</code> but consider in the definition <code>Q</code> is not defined as a named argument. For the <span class="pkg">MGBT</span> package, the <code>Q</code> has been converted to a more generic variable <code>x</code>. Development of TAC's B17C version through the <code>P3_089(R).txt</code> or similar sources will simply require <code>Q=</code> to be removed from the <code>MGBT</code> call.
</p>
<p>The original MGBT algorithms in <span class="rlang"><b>R</b></span> by TAC will throw some errors and warnings that required testing elsewhere for completeness (non-<code>NULL</code>) in algorithms &ldquo;up the chain.&rdquo; These errors appear to matter materially in pratical application in large-scale batch processing of USGS Texas peak-values data by WHA and GRH. For package <span class="pkg">MGBT</span>, the TAC computations have been modified to wrap a <code>try()</code> around the numerical integration within <code><a href="#topic+RthOrderPValueOrthoT">RthOrderPValueOrthoT</a></code> of <code><a href="#topic+peta">peta</a></code>, and insert a <code>NA</code> when the integration fails if and only if a second integration (fall-back) attempt using Monte Carlo integration fails as well.
</p>
<p>The following real-world data were discovered to trigger the error/warning messages. These example data crash TAC's MGBT and the data point of 25 cubic feet per second (cfs) is the culpret. If a failure is detected, Monte Carlo integration is attempted as a fall-back procedure using defaults of <code><a href="#topic+RthOrderPValueOrthoT">RthOrderPValueOrthoT</a></code>, and if that integration succeeds, <code>MGBT</code>, which is not aware, simply receives the p-value. If Monte Carlo integration also fails, then for the implementation in package <span class="pkg">MGBT</span>, the p-value is either a <code>NA</code> (<code>napv.zero=FALSE</code>) or set to zero if <code>napv.zero=TRUE</code>. Evidence suggests that numerical difficulties are encountered when small p-values are involved.
</p>
<pre>
  # Peak streamflows for 08385600 (1952--2015, systematic record only)
  #https://nwis.waterdata.usgs.gov/nwis/peak?site_no=08385600&amp;format=hn2
  Data  &lt;- c(  8100, 3300,  680, 14800,  25.0, 7310, 2150, 1110, 5200, 900, 1150,
   1050,  880, 2100, 2280, 2620,   830,  4900,  970,  560,  790, 1900, 830,  255,
   2900, 2100,    0,  550, 1200,  1300,   246,  700,  870, 4350,  870, 435, 3000,
    880, 2650,  185,  620, 1650,   680, 22900, 3290,  584, 7290, 1690, 2220, 217,
   4110,  853,  275, 1780, 1330,  3170,  7070, 2660) # cubic feet per second (cfs)
  MGBT17c(     Data, napv.zero=TRUE)$LOThres    # [1] 185
  MGBT17c.verb(Data, napv.zero=TRUE)$LOThres    # [1] 185
  MGBTcohn2016(Data, napv.zero=TRUE)$LOThres    # [1] 185
  MGBTcohn2013(Data, napv.zero=TRUE)$LOThres    # [1] 185
  MGBTnb(      Data, napv.zero=TRUE)$LOThres    # [1] 185
</pre>
<p>Without having the fall-back Monte Carlo integration in <code><a href="#topic+RthOrderPValueOrthoT">RthOrderPValueOrthoT</a></code>, if <code>napv.zero=</code> <code>FALSE</code>, then the low-outlier threshold is 25 cfs, but if <code>napv.zero=TRUE</code>, then the low-outlier threshold is 185 cfs, which is the value matching USGS-PeakFQ (v7.1) (FORTRAN code base). Hence, the recommendation that <code>napv.zero=TRUE</code> for the default, though such a setting will for this example will still result in 185 cfs because Monte Carlo integration can not be turned off for the implementation here.
</p>
<p>Noting that USGS-PeakFQ (7.1) reports the p-value for the 25 cfs as 0.0002, a test of the <span class="pkg">MGBT</span> implementation with the backup Monte Carlo integration in <code><a href="#topic+RthOrderPValueOrthoT">RthOrderPValueOrthoT</a></code> shows a p-value of 1.748946e-04 for 25 cfs, which is congruent with the 0.0002 of USGS-PeakFQ (v7.1). Another Monte Carlo integration produced a p-value of 1.990057e-04 for 25 cfs, and thus another result congruent with USGS-PeakFQ (v7.1) is evident.
</p>
<p>Using original TAC sources, here are the general errors that can be seen:
</p>
<pre>
   Error in integrate(peta, lower = 1e-07, upper = 1 - 1e-07, n = n, r = r, :
   the integral is probably divergent In addition:
   In pt(q, df = df, ncp = ncp, lower.tail = TRUE) :
     full precision may not have been achieved in 'pnt[final]'
</pre>
<p>For both the internal implementations of <code><a href="#topic+RthOrderPValueOrthoT">RthOrderPValueOrthoT</a></code> and <code><a href="#topic+peta">peta</a></code> error trapping is present to return a <code>NA</code>. It is not fully known whether the integral appears divergent when <code>pt()</code> (probability of the t-distribution) reaches an end point in apparent accuracy or not&mdash;although, this is suspected. For this package, a <code>suppressWarnings()</code> has been wrapped around the call to <code>pt()</code> in <code><a href="#topic+peta">peta</a></code> as well as in one other computation that at least in small samples can result in a square root of a negative number (see <b>Note</b> under <code><a href="#topic+peta">peta</a></code>).
</p>
<p>There is another error to trap for this package. If all the data values are identical, a low-outlier threshold set at that value leaks back. This is the motivation for a test added by WHA using <code>length(unique(x)) == 1</code> in the internals of <code>MGBTcohn2016</code>, <code>MGBTcohn2013</code>, and <code>MGBTnb</code>.
</p>
<p>A known warning message might be seen at least in microsamples:
</p>
<pre>
   MGBT(c( 1, 26300))       # throws    warnings, zero is the threshold
     # In EMS(n, r, qmin) : value out of range in 'lgamma'
   MGBT(c( 1, 26300, 2600)) # throws no warnings, zero is the threshold
</pre>
<p>The author wraps a <code>suppressWarnings()</code> on the line in <code><a href="#topic+EMS">EMS</a></code> requiring <code>lgamma()</code>. This warning appears restricted to nearly a degenerate situation anyway and failure will result in <code><a href="#topic+peta">peta</a></code> and therein the situation results in a p-value of unity and hence no risk of identifying a threshold.
</p>
<p>Regarding <code>n=length(x)</code> for <code>MGBTcohn2011</code>, it is not clear whether TAC intended <code>n</code> to be not equal to the sample size. TAC chose to not determine the length of <code>x</code> internally to the function but to have it available as an argument. Also <code><a href="#topic+BLlo">BLlo</a></code> and <code><a href="#topic+RSlo">RSlo</a></code> were designed similarly.
</p>
<p><b>(1) Lingering Issue of Inquiry</b>&mdash;TAC used a <code>j1</code> index in <code>MGBTcohn2016</code>, <code>MGBTcohn2013</code>, and <code>MGBTnb</code>, and this index is used as part of <code>alphaout</code>. The <code>j1</code> and is not involved in the returned content of the MGBT approach. This seems to imply a problem with the &ldquo;sweep out&rdquo; approach but TAC inadvertantly seems to make &ldquo;sweep out&rdquo; work in <code>MGBTnb</code> but therein creates a &ldquo;sweep in&rdquo; problem. The &ldquo;sweep in&rdquo; appears fully operational in <code>MGBTcohn2016</code> and <code>MGBTcohn2013</code>. Within the source for <code>MGBTcohn2016</code> and <code>MGBTcohn2013</code>, a fix can be made with just one line after the <code>n2</code> values have been processed. Here is the WHA commentary within the sources, and it is important to note that the fix is not turned on because <b>use of <code>MGBT17c</code> via the wrapper <code>MGBT</code> is the recommended interface</b>:
</p>
<pre>
   # ---***--------------***--- TAC CRITICAL BUG ---***--------------***---
   # j2 &lt;- min(c(j1,j2)) # WHA tentative completion of the 17C alogrithm!?!
   # HOWEVER MAJOR WARNING. WHA is using a minimum and not a maximum!!!!!!!
   # See MGBT17C() below. In that if the line a few lines above that reads
   # if((pvalueW[i] &lt; alpha1 )) { j1 &lt;- i; j2 &lt;- i }
   # is replaced with if((pvalueW[i] &lt; alpha1 )) j1 &lt;- i
   # then maximum and not the minimum becomes applicable.
   # ---***--------------***--- TAC CRITICAL BUG ---***--------------***---
</pre>
<p><b>(2) Lingering Issue of Inquiry</b>&mdash;TAC used recursion in <code>MGBTcohn2013</code> and <code>MGBTnb</code> for a condition <code>j2 == n2</code>. This recursion does not exist in <code>MGBTcohn2016</code>. TAC seems to have explored the idea of a modification to the <code>n2</code> to be related to an idea of setting a limit of at least five retained observations below half the sample (default <code>n2</code>) when up to half the sample is truncated away. Also in the recursion, TAC resets the two alpha's with <code>alphaout=0.01</code> from the default of <code>alpha1=0.005</code> and <code>alphazeroin=0.10</code>. This reset means that had TAC hardwired these inside <code>MGBTcohn2013</code>, which partially defeats the purpose of having them as arguments in the first place.
</p>
<p><b>(3) Lingering Issue of Inquiry</b>&mdash;TAC used recursion in <code>MGBTcohn2013</code> and <code>MGBTnb</code> for a condition <code>j2 == n2</code> but the recursion calls <code>MGBTcohn2013</code>. Other comments about the recursion in Inquiry (2) about the alpha's are applicable here as well. More curious about <code>MGBTnb</code> is that the <code>alphazeroin</code> is restricted to a test on only the p-value for the smallest observation and is made outside the loop through the data. The test is <code>if(pvalueW[1] &lt;</code> <code>alphazeroin &amp;</code> <code>j2 == 0)</code> <code>j2 &lt;- 1</code>, which is a logic flow that differs from that in <code>MGBTcohn2013</code> and <code>MGBTcohn2016</code>.
</p>
<p><b>On the Offset Argument</b>&mdash;The MGBT approach identifies the threshold as the first order statistic (<code class="reqn">x_{[r+1:n]}</code>) above that largest &ldquo;outlying&rdquo; order statistic <code class="reqn">x_{[r:n]}</code> with the requisite small p-value (see the example in the <b>Examples</b> section herein). There is a practical application in which a nudge on the MGBT-returned low-outlier threshold could be useful. Consider that the optional <code>offset</code> argument is added to the threshold unless the threshold itself is already zero. The offset should be small, and as an example <code class="reqn">{-}0.001</code> would be a magnitude below the resolution of the USGS peak-values database.
</p>
<p>Why? If algorithms other than USGS-PeakFQ are involved in frequency analyses, the concept of &ldquo;in&rdquo; or &ldquo;out&rdquo; of analyses, respectively, could be of the form <code>xin &lt;-</code> <code>x[x &gt; threshold]</code> and <code>xlo &lt;-</code> <code>x[x &lt;= threshold]</code>. Note the &ldquo;less than or equal to&rdquo; and its assocation with those data to be truncated away, which might be more consistent with the idea of truncation level of left-tail censored data in other datasets for which the MGBT approach might be used. For example, the <code>x2xlo()</code> function of the <span class="pkg">lmomco</span> package by Asquith (2019) uses such logic in its implementation of conditional probability adjustment for the presence of low outliers. This logic naturally supports a default truncation for zeros.
</p>
<p>Perhaps one reason for the difference in how a threshold is implemented is based on two primary considerations. First, if restricted to choosing a threshold to the sample order statistics, then the <code>MGBT</code> approach, by returning the first (earliest) order statistics that is not statistically significant, requires <code>x[x &lt; threshold]</code> for the values to leave out. TAC clearly intended this form. Second, TAC seems to approach the zero problem with <code>MGBT</code> by replacing all zeros with <code>1e-8</code> as in
</p>
<pre>
  log10(pmax(1e-8,x))
</pre>
<p>immediately before the logarithmic transformation. This might be a potential weak link in <code>MGBT</code>. It assumes that <code>1e-8</code> is small, which it certainly is for the problem of flood-frequency analysis using peaks in cubic feet per second. TAC has hardwired this value. Reasonable enough.
</p>
<p>But such logic thus requires the <code>MGBT</code> to identify these pseudo-zeros (now <code>1e-8</code>) in all circumstances as low outliers. Do the algorithms otherwise do this? This approach is attractive for B17C because one does not have to track a subsample of those values greater than zero because the low-outlier test will capture them. An implementation such as Asquith (2019) automatically removes zero without the need to use a low-outlier identification method; hence, Asquith's choice of <code>x[x &lt;= threshold]</code> with <code>threshold=0</code> by default for the values to leave out. The inclusion of <code>offset</code> permits cross compatibility for <span class="pkg">MGBT</span> package purposes trancending Bulletin 17C.
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith</p>


<h3>Source</h3>

<p><code>LowOutliers_jfe(R).txt</code>, <code>LowOutliers_wha(R).txt</code>, <code>P3_089(R).txt</code>&mdash;Named <code>MGBT</code> + <code>MGBTnb</code>
</p>


<h3>References</h3>

<p>Asquith, W.H., 2019, lmomco&mdash;L-moments, trimmed L-moments, L-comoments, censored <br /> L-moments, and many distributions: R package version 2.3.2 (September 20, 2018), accessed March 30, 2019, at <a href="https://cran.r-project.org/package=lmomco">https://cran.r-project.org/package=lmomco</a>.
</p>
<p>Cohn, T.A., 2013&ndash;2016, Personal communication of original R source code: U.S. Geological Survey, Reston, Va.
</p>
<p>Cohn, T.A., England, J.F., Berenbrock, C.E., Mason, R.R., Stedinger, J.R., and Lamontagne, J.R., 2013, A generalized Grubbs-Beck test statistic for detecting multiple potentially influential low outliers in flood series: Water Resources Research, v. 49, no. 8, pp. 5047&ndash;5058.
</p>
<p>England, J.F., Cohn, T.A., Faber, B.A., Stedinger, J.R., Thomas Jr., W.O., Veilleux, A.G., Kiang, J.E., and Mason, R.R., 2018, Guidelines for determining flood flow frequency Bulletin 17C: U.S. Geological Survey Techniques and Methods, book 4, chap. 5.B, 148 p., doi: <a href="https://doi.org/10.3133/tm4B5">10.3133/tm4B5</a>
</p>
<p>U.S. Geological Survey (USGS), 2018, PeakFQ&mdash;Flood frequency analysis based on Bulletin 17B and recommendations of the Advisory Committee on Water Information (ACWI) Subcommittee on Hydrology (SOH) Hydrologic Frequency Analysis Work Group (HFAWG), version 7.2: Accessed November 29, 2018, at <a href="https://water.usgs.gov/software/PeakFQ/">https://water.usgs.gov/software/PeakFQ/</a>.
</p>
<p>Veilleux, A.G., Cohn, T.A., Flynn, K.M., Mason, R.R., Jr., and Hummel, P.R., 2014, Estimating magnitude and frequency of floods using the PeakFQ 7.0 program: U.S. Geological Survey Fact Sheet 2013&ndash;3108, 2 p., doi: <a href="https://doi.org/10.3133/fs20133108">10.3133/fs20133108</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+RthOrderPValueOrthoT">RthOrderPValueOrthoT</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# USGS 08066300 (1966--2016) # cubic feet per second (cfs)
#https://nwis.waterdata.usgs.gov/nwis/peak?site_no=08066300&amp;format=hn2
Values &lt;- c(3530, 284, 1810, 9660,  489,  292, 1000,  2640, 2910, 1900,  1120, 1020,
   632, 7160, 1750,  2730,  1630, 8210, 4270, 1730, 13200, 2550,  915, 11000, 2370,
  2230, 4650, 2750,  1860, 13700, 2290, 3390, 5160, 13200,  410, 1890,  4120, 3930,
  4290, 1890, 1480, 10300,  1190, 2320, 2480, 55.0,  7480,  351,  738,  2430, 6700)
MGBT(Values) # Results LOT=284 cfs leaving 55.0 cfs (p-value=0.0119) censored.
#$index
#      n      n2    ix_alphaout     ix_alphain   ix_alphazeroin
#     51      25              0              0                1
#$omegas
# [1] -3.781980 -2.268554 -2.393569 -2.341027 -2.309990 -2.237571
# [7] -2.028614 -1.928391 -1.720404 -1.673523 -1.727138 -1.671534
#[13] -1.661346 -1.391819 -1.293324 -1.246974 -1.276485 -1.272878
#[19] -1.280917 -1.310286 -1.372402 -1.434898 -1.226588 -1.237743
#[25] -1.276794
#$x
# [1]   55  284  292  351  410  489  632  738  915 1000 1020 1120 1190 1480 1630 1730
#[17] 1750 1810 1860 1890 1890 1900 2230 2290 2320
#$pvalues
# [1] 0.01192184 0.30337879 0.08198836 0.04903091 0.02949836 0.02700114 0.07802324
# [8] 0.11185553 0.31531749 0.34257170 0.21560086 0.25950150 0.24113157 0.72747052
#[15] 0.86190920 0.89914152 0.84072131 0.82381908 0.78750571 0.70840262 0.55379730
#[22] 0.40255392 0.79430336 0.75515103 0.66031442
#$LOThresh
#[1] 284

# The USGS-PeakFQ (v7.1) software reports:
#   EMA003I-PILFS (LOS) WERE DETECTED USING MULTIPLE GRUBBS-BECK TEST   1     284.0
#      THE FOLLOWING PEAKS (WITH CORRESPONDING P-VALUES) WERE CENSORED:
#            55.0    (0.0123)
# As a curiosity, see Examples under ASlo().#


# MGBTnb() has a sweep in problem.
SweepIn &lt;- c(1, 1, 3200, 5270, 26300, 38400, 8710, 23200, 39300, 27800, 21000,
  21000, 21500, 57000, 53700, 5720, 10700, 4050, 4890, 10500, 26300, 16600, 20900,
  21400, 10800, 8910, 6360)  # sweep in and out both identify index 2.
MGBT17c(SweepIn, alphaout=0)$LOThres # LOT = 3200 # force no sweep outs
MGBTnb(SweepIn)$LOThres              # LOT = 3200 # because sweep out is the same!
MGBTnb(SweepIn, alphaout=0) # LOT = 1  # force no sweep outs, it fails.
</code></pre>

<hr>
<h2 id='MGBT-package'>Multiple Grubbs&ndash;Beck Low-Outlier Test</h2><span id='topic+MGBT-package'></span>

<h3>Description</h3>

<p>The <span class="pkg">MGBT</span> package provides the Multiple Grubbs&ndash;Beck low-outlier test (MGBT) (Cohn and others, 2013), and almost all users are only interested in the function <code><a href="#topic+MGBT">MGBT</a></code>. This function explicitly wraps the recommended implementation of the test, which is <code><a href="#topic+MGBT17c">MGBT17c</a></code>. Some other studies of low-outlier detection and study of the MGBT and related topic can be found in Cohn and others (2019), Lamontagne and Stedinger (2015), and Lamontagne and others (2016).
</p>
<p>The package also provides some handy utility functions for non-interpretive processing of U.S. Geological Survey National Water Information System (NWIS) annual-peak streamflow data. These utilities include <code><a href="#topic+makeWaterYear">makeWaterYear</a></code> that adds the water year and parses the date-time stamp into useful subcomponents, <code><a href="#topic+splitPeakCodes">splitPeakCodes</a></code> that splits the peak discharge qualification codes, and that <code><a href="#topic+plotPeaks">plotPeaks</a></code> plots the peak time series with emphasis on visualization of select codes, zero flow values, and missing records (annual gaps).
</p>
<p>The context of this package is useful to discuss. When logarithmic transformations of data prior to parameter estimation of probability models are used and interest in the the right-tail of the distribution exists, the MGBT is effective in adding robustness to flood-frequency analyses. Other similar distributed earth-system data analyses could also benefit from the test. The test can be used to objectively identify &ldquo;low outliers&rdquo; (generic) or specific to floods, &ldquo;potentially influential low floods&rdquo; (PILFs)&mdash;in general, these terms are synonymous.
</p>
<p>Motivations of the <span class="pkg">MGBT</span> package are related to the so-called &ldquo;Bulletin 17C&rdquo; guidelines (England and others, 2018) for flood-frequency analyses. These are updated guidelines to those in Bulletin 17B (IACWD, 1982).  Bulletin 17C (B17C) are Federal guidelines for performing flood-frequency analyses in the United States. The MGBT is implemented in the U.S. Geological Survey (USGS)-PeakFQ software (USGS, 2014; Veilleux and others, 2014), which implements much of B17C (England and others, 2018).
</p>
<p>The MGBT test is especially useful in practical applications in which small (possibly uninteresting) events (low-magnitude tail, left-tail side) can occur from divergent populations or processes than those forming the high-magnitude tail (right-tail side) of the probability distribution. One such large region of the earth is much of the arid to semi-arid hydrologic setting for much of Texas for which a heuristic method predating and unrelated to MGBT was used for low-outlier threshold identification (see <code><a href="#topic+ASlo">ASlo</a></code>). Arid and semi-arid regions are particularly sensitive to the greater topic motivating the MGBT (Timothy A. Cohn, personal commun., 2007).
</p>
<p><b>Note on Sources and Historical Preservation</b>&mdash;Various files (<code>.txt</code>) of <span class="rlang"><b>R</b></span> code are within this package and given and are located within the directory <code>/inst/sources</code>. The late Timothy A. Cohn (TAC) communicated <span class="rlang"><b>R</b></span> code to WHA (author William H. Asquith) in August 2013 for computation of MGBT within a flood-frequency project of WHA's. The August 2013 code is preserved verbatim in file <code>LowOutliers_wha(R).txt</code>, which also contains code by TAC to related concepts. Separately, TAC communicated <span class="rlang"><b>R</b></span> code to JFE (contributor John F. England) in 2016 (actually over many years they had extensive and independent communication from those with WHA) for computation of MGBT and other low-outlier related concepts. This 2016 code is preserved verbatim in file <code>LowOutliers_jfe(R).txt</code>. TAC also communicated <span class="rlang"><b>R</b></span> code to JFE for computation of MGBT and other low-outlier related concepts for production of figures for the MGBT paper (Cohn and others, 2013). (Disclosure, here it is unclear whether the <span class="rlang"><b>R</b></span> code given date as early as this paper or before when accounting for the publication process.)
</p>
<p>The code communications are preserved verbatim in file <code>FigureMacros_jfe(R)</code><code>.txt</code> for which that file is dependent on <code>P3_075_jfe(R).txt</code>. The <code>_jfe</code> has been added to denote stewardship at some point by JFE. The <code>P3_075_jfe(R).txt</code> though is superseded by <code>P3_089(R)</code><code>.txt</code> in which TAC was editing as late as the summer of 2016. The <code>P3_089(R).txt</code> comes to WHA through Julie E. Kiang (USGS, May 2016). This file should be considered TAC's canonical and last version for MGBT as it appears in the last set of algorithms TAC while he was working on development of a USGS-PeakFQ-like Bulletin 17C implementation in <span class="rlang"><b>R</b></span>. As another historical note, file <code>P3_085_wha(R).txt</code> is preserved verbatim and was given to WHA at the end of November 2015 less than two weeks before TAC dismissed himself for health reasons from their collaboration on a cooperative research project in cooperation with the U.S. Nuclear Regulatory Commission (Asquith and others, 2017).
</p>
<p>Because of a need for historical preservation at this juncture, there is considerable excess and directly-unrelated code to MGBT and low-outlier identification in the aforementioned files though MGBT obviously is contained therein. In greater measure, much of the other code is related to the expected moments algorithm (EMA) for fitting the log-Pearson type III distribution to annual flood data. The <span class="pkg">MGBT</span> package is purely organized around MGBT and related concepts in a framework suitable for more general application than the purposes of B17C and thus the contents of the <code>P3_###(R).txt</code> series of files. <b>It is, however, the prime objective of the <span class="pkg">MGBT</span> package to be nearly plug-in replacement for code presently (2019) bundled into <code>P3_###(R).txt</code> or derivative products. Therefore, any code related to B17C herein must not be considered canonical in any way.</b>
</p>
<p><b>Notes on Bugs in Sources by TAC</b>&mdash;During the core development phase of this package made in order to stabilized history left by TAC and other parts intimately known by JFE (co-author and contributor), several inconsistencies to even bugs manifested. These need very clear discussion.
</p>
<p>First, there is the risk that the author (WHA) ported TAC-based <span class="rlang"><b>R</b></span> to code in this package and introduced new problems. Second, there is a chance that TAC had errors and (or) WHA has misunderstood some idiom. Third, as examples herein show, there is (discovery circa June 2017) a great deal of evidence that TAC incompletely ported from presumably earlier(?) FORTRAN, which forms the basis of the USGS-PeakFQ software, that seems correct, into <span class="rlang"><b>R</b></span>&mdash;very subtle and technical issues are involved. Fourth, WHA and GRH (contributor George &ldquo;Rudy&rdquo; Herrmann) in several very large batch processing tests (1,400+ time series of real-world peak streamflows) on Texas data pushed limits of <span class="rlang"><b>R</b></span> numerics, and these are discussed in detail as are WHA's compensating mechanisms. Several questions of apparent bugs or encounters with the edges of <span class="rlang"><b>R</b></span> performance would be just minutes long conversations with TAC, but this is unfortunately not possible.
</p>
<p>In short, it appears that several versions of MGBT by TAC in <span class="rlang"><b>R</b></span> incorrectly performed a computation known as &ldquo;swept out&rdquo; from the median (<code><a href="#topic+MGBTcohn2016">MGBTcohn2016</a></code> and <code><a href="#topic+MGBTcohn2013">MGBTcohn2013</a></code>). Curiously a specific branch (<code><a href="#topic+MGBTnb">MGBTnb</a></code>) seems to fix that but caused a problem in a computation known as &ldquo;sweep in&rdquo; from the first order statistic.
</p>
<p>Further, numerical cases can be found triggering divergent integral warnings from <code>integrate()</code>&mdash;WHA compensated by adding a Monte Carlo integration routine as backup. It is beyond the scope here to speculate on FORTRAN code performance. In regards to <span class="rlang"><b>R</b></span> and numerically, cases can be found trigging numerical precision warnings from the cumulative distribution function of the t-distribution (<code>pt()</code>)&mdash;WHA compensated by setting p-value to limiting small value (zero). Also numerically, cases can be found triggering a square-root of a negative number in <code><a href="#topic+peta">peta</a></code>&mdash;WHA compensates by effectively using a vanishingly small probability of the t-distribution. TAC's MGBT approach fails if all data values are equal&mdash;WHA compensates by returning a default result of a zero-value MGBT threshold. This complexity leads to a lengthy <b>Examples</b> section in this immediate documentation as well as in the <code><a href="#topic+MGBT">MGBT</a></code> function. All of these issues finally led WHA to preserve within the <span class="pkg">MGBT</span> package several MGBT-focused implementations as distinct functions.
</p>
<p><b>Note on Mathematic Nomenclature</b>&mdash;On first development of this package, the mathematics largely represent the port from the sources into a minimal structure to complete description herein. TAC and others have published authoritative mathematics elsewhere. The primary author (WHA) deliberately decided to build the <span class="pkg">MGBT</span> package up from the TAC sources first. Little reference to TAC's publications otherwise is made.
</p>


<h3>Author(s)</h3>

<p>William H. Asquith (WHA) <a href="mailto:wasquith@usgs.gov">wasquith@usgs.gov</a> </p>


<h3>References</h3>

<p>Asquith, W.H., Kiang, J.E., and Cohn, T.A., 2017, Application of at-site peak-streamflow frequency analyses for very low annual exceedance probabilities: U.S. Geological Survey Scientific Investigation Report 2017&ndash;5038, 93 p., doi: <a href="https://doi.org/10.3133/sir20175038">10.3133/sir20175038</a>.
</p>
<p>Cohn, T.A., 2013&ndash;2016, Personal communication of original R source code: U.S. Geological Survey, Reston, Va.
</p>
<p>Cohn, T.A., Barth, N.A., England, J.F., Jr., Faber, B.A., Mason, R.R., Jr., and Stedinger, J.R., 2019, Evaluation of recommended revisions to Bulletin 17B: U.S. Geological Survey Open-File Report 2017&ndash;1064, 141 p., doi: <a href="https://doi.org/10.3133/ofr20171064">10.3133/ofr20171064</a>.
</p>
<p>Cohn, T.A., England, J.F., Berenbrock, C.E., Mason, R.R., Stedinger, J.R., and Lamontagne, J.R., 2013, A generalized Grubbs&ndash;Beck test statistic for detecting multiple potentially influential low outliers in flood series: Water Resources Research, v. 49, no. 8, pp. 5047&ndash;5058.
</p>
<p>England, J.F., Cohn, T.A., Faber, B.A., Stedinger, J.R., Thomas Jr., W.O., Veilleux, A.G., Kiang, J.E., and Mason, R.R., 2018, Guidelines for determining flood flow frequency Bulletin 17C: U.S. Geological Survey Techniques and Methods, book 4, chap. 5.B, 148 p., doi: <a href="https://doi.org/10.3133/tm4B5">10.3133/tm4B5</a>
</p>
<p>Interagency Advisory Committee on Water Data (IACWD), 1982, Guidelines for determining flood flow frequency: Bulletin 17B of the Hydrology Subcommittee, Office of Water Data Coordination, U.S. Geological Survey, Reston, Va., 183 p.
</p>
<p>Lamontagne, J.R., and Stedinger, J.R., 2015, Examination of the Spencer&ndash;McCuen outlier-detection test for log-Pearson type 3 distributed data: Journal of Hydrologic Engineering, v. 21, no. 3, pp. 04015069:1&ndash;7.
</p>
<p>Lamontagne, J.R., Stedinger, J.R., Yu, Xin, Whealton, C.A., and Xu, Ziyao, 2016, Robust flood frequency analysis&mdash;Performance of EMA with multiple Grubbs&ndash;Beck outlier tests: Water Resources Research, v. 52, pp. 3068&ndash;3084.
</p>
<p>U.S. Geological Survey (USGS), 2018, PeakFQ&mdash;Flood frequency analysis based on Bulletin 17B and recommendations of the Advisory Committee on Water Information (ACWI) Subcommittee on Hydrology (SOH) Hydrologic Frequency Analysis Work Group (HFAWG), version 7.2: Accessed November 29, 2018, at <a href="https://water.usgs.gov/software/PeakFQ/">https://water.usgs.gov/software/PeakFQ/</a>.
</p>
<p>Veilleux, A.G., Cohn, T.A., Flynn, K.M., Mason, R.R., Jr., and Hummel, P.R., 2014, Estimating magnitude and frequency of floods using the PeakFQ 7.0 program: U.S. Geological Survey Fact Sheet 2013&ndash;3108, 2 p., doi: <a href="https://doi.org/10.3133/fs20133108">10.3133/fs20133108</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MGBT">MGBT</a></code>, <code><a href="#topic+splitPeakCodes">splitPeakCodes</a></code>, <code><a href="#topic+plotPeaks">plotPeaks</a></code>, <code><a href="#topic+readNWISwatstore">readNWISwatstore</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Peaks for 08165300 (1968--2016, systematic record only)
#https://nwis.waterdata.usgs.gov/nwis/peak?site_no=08385600&amp;format=hn2
Peaks &lt;- c(3200, 44, 5270, 26300, 1230, 55, 38400, 8710, 143, 23200, 39300, 1890,
  27800, 21000, 21000, 124, 21, 21500, 57000, 53700, 5720, 50, 10700, 4050, 4890, 1110,
  10500, 475, 1590, 26300, 16600, 2370, 53, 20900, 21400, 313, 10800, 51, 35, 8910,
  57.4, 617, 6360, 59, 2640, 164, 297, 3150, 2690)

MGBTcohn2016(Peaks)
#$klow
#[1] 24
#$pvalues
# [1] 0.8245714657 0.7685258183 0.6359392507 0.4473443285 0.2151390091 0.0795065159
# [7] 0.0206034851 0.0036001474 0.0003376923 0.0028133490 0.0007396869 0.0001427225
#[13] 0.0011045550 0.0001456356 0.0004178758 0.0004138897 0.0123954279 0.0067934260
#[19] 0.0161448464 0.0207025800 0.0483890616 0.0429628125 0.0152045539 0.0190853626
#$LOThresh
#[1] 3200

# ---***--------------***--- Note the mismatch ---***--------------***---
#The USGS-PeakFQ (v7.1) software reports:
#EMA003I-PILFS (LOS) WERE DETECTED USING MULTIPLE GRUBBS-BECK TEST  16    1110.0
#  THE FOLLOWING PEAKS (WITH CORRESPONDING P-VALUES) WERE CENSORED:
#            21.0    (0.8243)
#            35.0    (0.7680)
#            44.0    (0.6349)
#            50.0    (0.4461)      # Authors' note:
#            51.0    (0.2150)      # Note that the p-values from USGS-PeakFQv7.1 are
#            53.0    (0.0806)      # slightly different from those emanating from R.
#            55.0    (0.0218)      # These are thought to be from numerical issues.
#            57.4    (0.0042)
#            59.0    (0.0005)
#           124.0    (0.0034)
#           143.0    (0.0010)
#           164.0    (0.0003)
#           297.0    (0.0015)
#           313.0    (0.0003)
#           475.0    (0.0007)
#           617.0    (0.0007)
# ---***--------------***--- Note the mismatch ---***--------------***---

# There is a problem somewhere. Let us test each of the TAC versions available.
# Note that MGBTnb() works because the example peaks are ultimately a "sweep out"
# problem. MGBT17c() is a WHA fix to TAC algorithm, whereas, MGBT17c.verb() is
# a verbatim, though slower, WHA port of the written language in Bulletin 17C.
MGBTcohn2016(Peaks)$LOThres # LOT=3200  (WHA sees TAC problem with "sweep out".)
MGBTcohn2013(Peaks)$LOThres # LOT=16600 (WHA sees TAC problem with "sweep out".)
MGBTnb(Peaks)$LOThres       # LOT=1110  (WHA sees TAC problem with "sweep in". )
MGBT17c(Peaks)$index        # LOT=1110  (sweep indices:
                            #   ix_alphaout=16, ix_alphain=16, ix_alphazeroin=0)
MGBT17c.verb(Peaks)$index   # LOT=1110  (sweep indices:
                            #   ix_alphaout=16, ix_alphain=NA, ix_alphazeroin=0)

# Let us now make a problem, which will have both "sweep in" and "sweep out"
# characteristics, and note the zero and unity outliers for the "sweep in" to grab.
Peaks &lt;- c(0,1,Peaks)
MGBTcohn2016(Peaks)$LOThres # LOT=3150         ..ditto..
MGBTcohn2013(Peaks)$LOThres # LOT=16600        ..ditto..
MGBTnb(Peaks)$LOThres       # LOT=1110         ..ditto..
MGBT17c(Peaks)$index        # LOT=1110  (sweep indices:
                            #   ix_alphaout=18, ix_alphain=18, ix_alphazeroin=2
MGBT17c.verb(Peaks)$index   # LOT=1110  (sweep indices:
                            #   ix_alphaout=18, ix_alphain=NA, ix_alphazeroin=2

#The USGS-PeakFQ (v7.1) software reports:
#    EMA003I-PILFS (LOS) WERE DETECTED USING MULTIPLE GRUBBS-BECK TEST  17    1110.0
#      THE FOLLOWING PEAKS (WITH CORRESPONDING P-VALUES) WERE CENSORED:
#               1 ZERO VALUES
#             1.0    (0.0074)
#            21.0    (0.4305)
#            35.0    (0.4881)
#            44.0    (0.3987)
#            50.0    (0.2619)
#            51.0    (0.1107)
#            53.0    (0.0377)
#            55.0    (0.0095)
#            57.4    (0.0018)
#            59.0    (0.0002)
#           124.0    (0.0018)
#           143.0    (0.0006)
#           164.0    (0.0002)
#           297.0    (0.0010)
#           313.0    (0.0002)
#           475.0    (0.0005)
#           617.0    (0.0005) #
</code></pre>

<hr>
<h2 id='peta'> Probability of Eta </h2><span id='topic+peta'></span>

<h3>Description</h3>

<p>Compute <code>peta</code>, which is the survival probability of the t-distribution for <code>eta =</code> <code class="reqn">\eta</code>.
</p>
<p>Define <code class="reqn">b_r</code> as the inverse (quantile) of the Beta distribution for nonexceedance probability <code class="reqn">F \in (0,1)</code> having two shape parameters (<code class="reqn">\alpha</code> and <code class="reqn">\beta</code>) as
</p>
<p style="text-align: center;"><code class="reqn">b_r = \mathrm{Beta}^{(-1)}(F; \alpha, \beta) = \mathrm{Beta}^{(-1)}(F; r, n+1-r)\mbox{,}</code>
</p>

<p>for sample size <code class="reqn">n</code> and number of truncated observations <code class="reqn">r</code> and note that <code class="reqn">b_r \in (0,1)</code>. Next, define <code class="reqn">z_r</code> as the <code class="reqn">Z</code>-score for <code class="reqn">b_r</code>
</p>
<p style="text-align: center;"><code class="reqn">z_r = \Phi^{(-1)}(b_r)\mbox{,}</code>
</p>

<p>where <code class="reqn">\Phi^{(-1)}(\cdots)</code> is the inverse of the standard normal distribution.
</p>
<p>Compute the covariance matrix <code class="reqn">COV</code> of <code class="reqn">M</code> and <code class="reqn">S</code> from <code><a href="#topic+VMS">VMS</a></code> as in <code>COV = VMS(n, r, qmin=br)</code>, and from which define
</p>
<p style="text-align: center;"><code class="reqn">\lambda = COV_{1,2} / COV_{2,2}\mbox{,}</code>
</p>

<p>which is a covariance divided by a variance, and then define
</p>
<p style="text-align: center;"><code class="reqn">\eta_p = \lambda + \eta\mbox{.}</code>
</p>

<p>Compute the expected values of <code class="reqn">M</code> and <code class="reqn">S</code> from <code><a href="#topic+EMS">EMS</a></code> as in <code class="reqn">EMp = </code> <code>EMp = EMS(n, r, qmin=br)</code>, and from which define
</p>
<p style="text-align: center;"><code class="reqn">\mu_{Mp} = EMp_1 - \lambda\times EMp_2\mbox{,}</code>
</p>

<p style="text-align: center;"><code class="reqn">\sigma_{Mp} = \sqrt{COV_{1,1} - COV_{1,2}^2/COV_{2,2}}\mbox{.}</code>
</p>

<p>Compute the conditional moments from <code><a href="#topic+CondMomsChi2">CondMomsChi2</a></code> as in <code class="reqn">momS2 =</code> <code>CondMomsChi2(n,r,zr)</code>, and from which define
</p>
<p style="text-align: center;"><code class="reqn">df = 2 momS2_1^2 / momS2_2\mbox{,}</code>
</p>

<p style="text-align: center;"><code class="reqn">\alpha = momS2_2 / momS2_1\mbox{,}</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>peta(pzr, n, r, eta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="peta_+3A_pzr">pzr</code></td>
<td>
<p>The probability level of a Beta distribution having shape1 <code class="reqn">\alpha = r</code> and shape2 <code class="reqn">\beta = n+1-r</code>;</p>
</td></tr>
<tr><td><code id="peta_+3A_n">n</code></td>
<td>
<p>The number of observations;</p>
</td></tr>
<tr><td><code id="peta_+3A_r">r</code></td>
<td>
<p>The number of truncated observations; and</p>
</td></tr>
<tr><td><code id="peta_+3A_eta">eta</code></td>
<td>
<p>The Grubbs&ndash;Beck statistic (<code class="reqn">GB_r</code>, see <code><a href="#topic+MGBT">MGBT</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Currently (2019), context is lost on the preformatted note of code note below. It seems possible that the intent by WHA was to leave a trail for future revisitation of the Beta distribution and its access, which exists in native <span class="rlang"><b>R</b></span> code.
</p>
<pre>
      zr       &lt;- qnorm(qbeta(the.pzr, shape1=r, shape2=n+1-r))
      CV       &lt;- VMS(n, r, qmin=pnorm(zr))
</pre>


<h3>Value</h3>

<p>The probability of the <code>eta</code> value.
</p>


<h3>Note</h3>

<p>Testing a very large streamgage dataset in Texas with GRH, shows at least one failure of the following computation was encountered for a short record streamgage numbered 08102900.
</p>
<pre>
  # USGS 08102900 (data sorted, 1967--1974)
  #https://nwis.waterdata.usgs.gov/nwis/peak?site_no=08102900&amp;format=hn2
  Peaks &lt;- c(40, 45, 53, 55, 88) # in cubic feet per second (cfs)
  MGBT(Peaks)
  # Here is the line in peta(): SigmaMp &lt;- sqrt(CV[1,1] - CV[1,2]^2/CV[2,2])
  # *** In sqrt(CV[1, 1] - CV[1, 2]^2/CV[2, 2]) : NaNs produced
</pre>
<p>In implementation, a <code>suppressWarnings()</code> is wrapped on the <code>SigmaMp</code>. If the authors make no action in response to <code>NaN</code>, then the low-outlier threshold is 53 cubic feet per second (cfs) with a p-value for 40 cfs as 0.81 and 45 cfs as 0.0. This does not seem logical. The <code>is.finite</code> catch in the next line (see sources) is provisional under a nave assumption that the negative in the square root has barely undershot. The function is immediately exited with the returned p-value set to unity. Testing indicates that this is a favorable innate trap here within the <span class="pkg">MGBT</span> package and will avoid higher up error trapping in larger application development.
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith consulting T.A. Cohn source</p>


<h3>Source</h3>

<p><code>LowOutliers_jfe(R).txt</code>, <code>LowOutliers_wha(R).txt</code>, <code>P3_089(R).txt</code>&mdash;Named <code>peta</code>
</p>


<h3>References</h3>

<p>Cohn, T.A., 2013&ndash;2016, Personal communication of original R source code: U.S. Geological Survey, Reston, Va.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+EMS">EMS</a></code>, <code><a href="#topic+VMS">VMS</a></code>, <code><a href="#topic+CondMomsChi2">CondMomsChi2</a></code>, <code><a href="#topic+gtmoms">gtmoms</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>peta(0.4, 58, 2, -2.3006)
#[1] 0.298834
</code></pre>

<hr>
<h2 id='plotFFQevol'> Plot Flood-Frequency in Time </h2><span id='topic+plotFFQevol'></span>

<h3>Description</h3>

<p>Plot the temporal evolution of peak-streamflow frequency using the method of L-moments on the systematic record, assuming that <code>appearsSystematic</code> fully identifies the systematic record (see <code><a href="#topic+splitPeakCodes">splitPeakCodes</a></code>), but have the results align on the far right side to other results. These other results are intended to be an official best estimate of the peak-streamflow frequency using all available information and are generally anticipated to be from Bulletin 17C (B17C) (England and others, 2018). The motivation for this function is that some have contacted one of the authors with a desire to show a temporal evolution of the estimates of the 2-, 10-, 100-, and 500-year peak-streamflow values but simultaneously have &ldquo;current&rdquo; (presumably a modern year [not necessarily the last year of record]) results. A target year (called the <code>final_water_yr</code>) is declared. Base10 logarithmic offsets from the so-called final quantile values at that year are computed. These then are used additively to correct the temporal evolution of the L-moment based peak-streamflow frequency values.
</p>
<p>The code herein makes use of the <code>f2flo</code>, <code>lmoms</code>, <code>parpe3</code>, <code>quape3</code>, <code>T2prob</code>, <code>x2xlo</code> functions from the <span class="pkg">lmomco</span> package, and because this is the only instance of this dependency, the <span class="pkg">lmomco</span> package is treated as a suggested package and not as a dependency for the <span class="pkg">MGBT</span> package. The <b>Examples</b> use the <span class="pkg">dataRetrieval</span> package for downloading peak streamflow data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotFFQevol(pkenv, lot=NULL, finalquas=NULL, log10offsets=NULL,
          minyrs=10, byr=1940, edgeyrs=c(NA, NA), logyaxs=TRUE,
          lego=NULL, maxs=NULL, mins=NULL, names=NULL, auxeyr=NA,
          xlab="Water Year", ylab="Peak streamflow, in cubic feet per second",
          title="Time Evolution of Peak-Frequency Streamflow Estimates\n",
          data_ext="_data.txt", ffq_ext="_ffq.txt", path=tempdir(),
          showfinalyr=TRUE, usewyall=FALSE, silent=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotFFQevol_+3A_pkenv">pkenv</code></td>
<td>
<p>A <code>environment</code> having a mandatory column titled <code>peak_va</code> and <code>peak_cd</code>. It is required to have run <code><a href="#topic+makeWaterYear">makeWaterYear</a></code> and <code><a href="#topic+splitPeakCodes">splitPeakCodes</a></code> first;</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_lot">lot</code></td>
<td>
<p>A vector of low-outlier thresholds for the stations stored in <code>pkenv</code>;</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_finalquas">finalquas</code></td>
<td>
<p>A <code>data.frame</code> (see <b>Examples</b>) of the final peak-streamflow frequency values to be soon (matched!) at the right side of the plot. These values could stem from B17C-like analyses using all the features therein. These values force, through an offsetting method in log10-space, the method of L-moment results in time to &ldquo;land&rdquo; to the results at the end (right side).</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_log10offsets">log10offsets</code></td>
<td>
<p>An optional offset <code>data.frame</code> (see <b>Examples</b>) to add to <code>finalquas</code> so that the final right side method of L-moment results match the results given in <code>finalquas</code>;</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_minyrs">minyrs</code></td>
<td>
<p>The minimum number of years (sample size) before trying to fit a log-Pearson type III distribution by method of L-moments&mdash;the <code>lot</code> will be honored through a conditional distribution trunction;</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_byr">byr</code></td>
<td>
<p>The beginning year for which to even start consultion for peaks in time frequency analysis;</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_edgeyrs">edgeyrs</code></td>
<td>
<p>The optional bounding years for the horizontal axis but potentially enlarged by the data as well as the setting of <code>usewyall</code>;</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_logyaxs">logyaxs</code></td>
<td>
<p>A logical to trigger a logarithmic plot. The logarithmic plot is based on the function <code><a href="#topic+plotPeaks">plotPeaks</a></code> and is more sophisticated. The linear option is provided herein as some users have deemed logarthimic axes too complicated for engineers to understand (seriously);</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_lego">lego</code></td>
<td>
<p>The year at which to start the legend;</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_maxs">maxs</code></td>
<td>
<p>The upper limit of the vertical axis;</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_mins">mins</code></td>
<td>
<p>The lower limit of the vertical axis;</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_names">names</code></td>
<td>
<p>An optional character string vector to be used as a plot title;</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_auxeyr">auxeyr</code></td>
<td>
<p>An optional auxillary ending year to superceed the <code>final_water_yr</code> from the <code>finalquas</code>. The <code>auxeyr</code> could lead to interesting conflict in the graphic. For example, a 500-year value being less than the 100-year. If a distribution swings between sign of the skew parameter and because the offsets are computed at a the discrete point in time (<code>final_water_yr</code>), then the offset could be too large at the 500-yr level and cause overlap (see <b>Examples</b>);</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_xlab">xlab</code></td>
<td>
<p>An optional x-label of the plot;</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_ylab">ylab</code></td>
<td>
<p>An optional y-label of the plot;</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_title">title</code></td>
<td>
<p>An optional super title for the plot to be shown above <code>names</code>;</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_data_ext">data_ext</code></td>
<td>
<p>An optional file name extension to the data (only water year, peak streamflow value, and <code>appearsSystematic</code>) retained in this file relative to the greater results inside the <code>pkenv</code>. If an output file is not desired, set to <code>NA</code>;</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_ffq_ext">ffq_ext</code></td>
<td>
<p>An optional file name extension to the flood-flow frequency (water year and 2, 10, 100, and 500 year) output. If an output file is not desired, set to <code>NA</code>;</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_path">path</code></td>
<td>
<p>The path argument for the output files with a default to a temporary directory for general protection of the user;</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_showfinalyr">showfinalyr</code></td>
<td>
<p>A logical to control whether a line is drawn showing the location of the final water year (the year of the quantile estimates, <code>final_water_yr</code>) that  spans top to bottom on the graphic;</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_usewyall">usewyall</code></td>
<td>
<p>A logical to consult the <code>isCode7</code> for determination of the horizontal axis limits;</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_silent">silent</code></td>
<td>
<p>A logical for some status update messaging; and</p>
</td></tr>
<tr><td><code id="plotFFQevol_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass to <code>plot</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The log10-offset values are returned if not given otherwise this function is used for its graphical side effects.
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith</p>


<h3>Source</h3>

<p>Earlier code developed by W.H. Asquith in about 2016.
</p>


<h3>References</h3>

<p>England, J.F., Cohn, T.A., Faber, B.A., Stedinger, J.R., Thomas Jr., W.O., Veilleux, A.G., Kiang, J.E., and Mason, R.R., 2018, Guidelines for determining flood flow frequency Bulletin 17C: U.S. Geological Survey Techniques and Methods, book 4, chap. 5.B, 148 p., <a href="https://pubs.er.usgs.gov/publication/tm4B5">https://pubs.er.usgs.gov/publication/tm4B5</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plotPeaks">plotPeaks</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# The dataRetrieval package is not required by MGBT algorithms.
opts &lt;- options(scipen=7)
opar &lt;- par(no.readonly=TRUE)
par(mgp=c(3,0.5,0), las=1) # going to tick inside, change some parameters

names &lt;- c("08167000 Guadalupe River at Comfort, Tex.")
stations &lt;- c("08167000"); LOT &lt;- c(3110)
maxs &lt;- c(525000); lego &lt;- c(1940)

PKS &lt;- new.env()
for(station in "08167000") {
  message("Pulling peaks for ",station)
  data &lt;- dataRetrieval::readNWISpeak(station, convertType=FALSE)
  data &lt;- splitPeakCodes(MGBT::makeWaterYear(data))
  assign(station, data, PKS)
}

# This example should run fine though the resulting curves will end in 2015 because
# this is the declared ending year of 2015. Data points after 2015 will be shown
# but the FFQ values will be the values plotted. Yes, other return periods are shown
# here and dealt with internally, but only the 2, 10, 100, and 500 are drawn.
FFQ &lt;- data.frame(site_no="08167000", final_water_yr=2015,
                  Q002= 3692, Q005= 21000, Q010= 40610, Q025= 69740,
                  Q050=91480, Q100=111600, Q200=129400, Q500=149200)
# Now compute the offsets associated with those given above.
OFFSET &lt;- plotFFQevol(PKS, lot=LOT, finalquas=FFQ)
# Notice the plotFFQevol() is called twice. One call is to compute the
# offsets, and the next is to use them and make a pretty plot.
plotFFQevol(PKS, lot=LOT, finalquas=FFQ, log10offsets=OFFSET,
            maxs=maxs, mins=rep(0,length(maxs)), names=names,
            lego=lego, logyaxs=FALSE, edgeyrs=c(1940,2020), usewyall=TRUE)

# Now a change up, lets say these values are good through the year 1980, and yes,
# these are the same values shown above.
FFQ$final_water_yr &lt;- 1980
OFFSET &lt;- plotFFQevol(PKS, lot=LOT, finalquas=FFQ) # compute offsets
# Now using auxeyr=2020, will trigger the evolution through time and the results in
# 1980 will match these given in the FFQ. One will see (presumably for many years
# after 2017) the crossing of the 500 year to the 100 year in about 1993.
plotFFQevol(PKS, lot=LOT, finalquas=FFQ, log10offsets=OFFSET,
            maxs=maxs, mins=rep(0,length(maxs)), names=names, auxeyr=2020,
            lego=lego, logyaxs=FALSE, edgeyrs=c(1940,2020), usewyall=FALSE)

# Now back to the original FFQ but in log10 space and plotPeaks().
FFQ$final_water_yr &lt;- 2017
OFFSET &lt;- plotFFQevol(PKS, lot=LOT, finalquas=FFQ) # compute offsets
# Now using logyaxs=TRUE and some other argument changes
plotFFQevol(PKS, lot=LOT, finalquas=FFQ, log10offsets=OFFSET, title="",
            maxs=maxs, mins=rep(0,length(maxs)), names=names, auxeyr=2020,
            lego=NULL, logyaxs=TRUE, edgeyrs=c(1890,2020), usewyall=TRUE,
            showfinalyr=FALSE)
options(opts) # restore the defaults
par(opar)     # restore the defaults
</code></pre>

<hr>
<h2 id='plotPeaks'> Plot Peak Streamflows with Emphasis on Peak Discharge Qualification Codes </h2><span id='topic+plotPeaks'></span>

<h3>Description</h3>

<p>Plot U.S. Geological Survey peak streamflows in <code>peak_va</code> and discharge qualifications codes in the <code>peak_cd</code> columns of a peak-streamflow data retrieval from the National Water Information System (NWIS) (U.S. Geological Survey, 2019). This code makes use of the <code>add.log.axis</code> function from the <span class="pkg">lmomco</span> package, and because this is the only instance of this dependency, the <span class="pkg">lmomco</span> package is treated as a suggested package and not as a dependency for the <span class="pkg">MGBT</span> package. The <code>add.log.axis</code> function also is used for the basis of the logarithmic plot within the <code><a href="#topic+plotFFQevol">plotFFQevol</a></code> function.
</p>
<p>This function accommodates the plotting of various nuances of the peak including less than (code 4), greater than (code 8), zero peaks (plotted by a green tick on the horizontal axis), and peaks that are missing but the gage height was available (plotted by a light-blue tick on the horizontal axis if the <code>showGHyrs</code> argument is set). So-called code 5, 6, and 7 peaks are plotted by the numeric code as the plotting symbol, and so-called code C peaks are plotted by the letter &ldquo;C.&rdquo; The very unusual circumstances of codes 3 and O are plotted by the letters &ldquo;D&rdquo; (dam failure) and &ldquo;O&rdquo; (opportunistic), respectively. These codes are are summarized within <code><a href="#topic+splitPeakCodes">splitPeakCodes</a></code>. The greater symbology set is described in the directory <code>MGBT/inst/legend</code> of the package sources.
</p>
<p>The logic herein also makes allowances for &ldquo;plotting&rdquo; gage-height only streamgages but this requires that the <code><a href="#topic+splitPeakCodes">splitPeakCodes</a></code> function was called with the <code>all_peak_na_okay</code> set to <code>TRUE</code>. The gap analysis for gage-height only streamgages or streamgages for which the site changes from gage-height only to discharge and then back again or other permutations will likely result in the gap lines not being authoritative. The sub-bottom axis ticks for the gage-height only water years should always be plotting correctly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotPeaks(x, codes=TRUE, lot=NULL, site="",
             xlab="", ylab="", xlim=NULL, ylim=NULL,
             xlim.inflate=TRUE, ylim.inflate=TRUE, aux.y=NULL,
             log.ticks=c(1, 2, 3, 5, 8),
             show48=FALSE, showDubNA=FALSE, showGHyrs=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotPeaks_+3A_x">x</code></td>
<td>
<p>A <code>data.frame</code> having a mandatory column titled <code>peak_va</code> and <code>peak_cd</code>. It is advised to have run <code><a href="#topic+makeWaterYear">makeWaterYear</a></code> and <code><a href="#topic+splitPeakCodes">splitPeakCodes</a></code> first, but if columns resulting from those two functions are are not detected, then the water year column and code split are made internally but not returned;</p>
</td></tr>
<tr><td><code id="plotPeaks_+3A_codes">codes</code></td>
<td>
<p>A logical to trigger use of character plotting characters and not symbols;</p>
</td></tr>
<tr><td><code id="plotPeaks_+3A_lot">lot</code></td>
<td>
<p>The low-outlier threshold, but if omitted, then <code><a href="#topic+MGBT">MGBT</a></code> is triggered internally. Use of <code>lot=0</code> is a mechansim to effectively bypass the plotting of a low-outlier threshold because a logarithmic vertical axis is used, and this setting would bypass a call to <code><a href="#topic+MGBT">MGBT</a></code>;</p>
</td></tr>
<tr><td><code id="plotPeaks_+3A_site">site</code></td>
<td>
<p>An optional character string for a plot title, and in practice, this is expected to be a streamgage location;</p>
</td></tr>
<tr><td><code id="plotPeaks_+3A_xlab">xlab</code></td>
<td>
<p>An optional x-label of the plot;</p>
</td></tr>
<tr><td><code id="plotPeaks_+3A_ylab">ylab</code></td>
<td>
<p>An optional y-label of the plot;</p>
</td></tr>
<tr><td><code id="plotPeaks_+3A_xlim">xlim</code></td>
<td>
<p>An optional x-limit of the plot;</p>
</td></tr>
<tr><td><code id="plotPeaks_+3A_ylim">ylim</code></td>
<td>
<p>An optional y-limit of the plot;</p>
</td></tr>
<tr><td><code id="plotPeaks_+3A_xlim.inflate">xlim.inflate</code></td>
<td>
<p>A logical to trigger nudging the horizontal axis left/right to the previous/future decade;</p>
</td></tr>
<tr><td><code id="plotPeaks_+3A_ylim.inflate">ylim.inflate</code></td>
<td>
<p>A logical to trigger nudging the vertical axis down/up to the nearest sane increment of log10-cycles. This inflation also includes a <code class="reqn">\pm</code>0.01 log10-cycle adjustment to the lower and upper values of the limit. This ensures (say) that a 50,000 cubic feet per second maximum, which is on a sane increment, is nudged up enough to make the upper limit 60,000 instead. The point itself plots at 50,000 cfs;</p>
</td></tr>
<tr><td><code id="plotPeaks_+3A_aux.y">aux.y</code></td>
<td>
<p>An optional set of values to pack into the data just ahead of the the vertical axis limits computation;</p>
</td></tr>
<tr><td><code id="plotPeaks_+3A_log.ticks">log.ticks</code></td>
<td>
<p>The argument <code>logs</code> of the <code>add.log.axis</code> function;</p>
</td></tr>
<tr><td><code id="plotPeaks_+3A_show48">show48</code></td>
<td>
<p>A logical, if <code>codes</code> is set, will draw a &ldquo;4&rdquo; and &ldquo;8&rdquo; for those respective codes instead of a reddish dot;</p>
</td></tr>
<tr><td><code id="plotPeaks_+3A_showdubna">showDubNA</code></td>
<td>
<p>A logical, if set, will draw a <code class="reqn">\pm</code>half-year polygon for the water years having both <code>NA</code> discharge (<code>peak_va</code>) and <code>NA</code> gage height (<code>gage_ht</code>). To some analysts, it might be useful to study the degree of these double empty entries in contrast to just the records of such years not even being present in NWIS. Double empty entries could represent incomplete data handling on part of the USGS or inadvertently missing zero discharges for <code>peak_va</code>;</p>
</td></tr>
<tr><td><code id="plotPeaks_+3A_showghyrs">showGHyrs</code></td>
<td>
<p>A logical to trigger the light-blue special ticks for the water years having only gage height. The horizontal limits will be inflated accordingly if gage heights are outside the general discharge record; and</p>
</td></tr>
<tr><td><code id="plotPeaks_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass to <code>plot</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No values are returned; this function is used for its graphical side effects.
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith</p>


<h3>References</h3>

<p>U.S. Geological Survey, 2019, USGS water data for the Nation: U.S. Geological Survey National Water Information System database, accessed October 11, 2019, at doi: <a href="https://doi.org/10.5066/F7P55KJN">10.5066/F7P55KJN</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+makeWaterYear">makeWaterYear</a></code>, <code><a href="#topic+splitPeakCodes">splitPeakCodes</a></code>, <code><a href="#topic+plotFFQevol">plotFFQevol</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  # The dataRetrieval package is not required by MGBT algorithms.
  # Note that makeWaterYear() is not needed because splitPeakCodes() requires
  # the water_yr for gap analyses, and will call makeWaterYear() if it needs to.
  PK &lt;- dataRetrieval::readNWISpeak("08167000", convertType=FALSE)
  PK &lt;- splitPeakCodes(makeWaterYear(PK))
  plotPeaks(PK, codes=TRUE, showGHyrs=FALSE,
                xlab="Water year", ylab="Discharge, cfs") # 


  # The dataRetrieval package is not required by MGBT algorithms.
  # An example with zero flows
  PK &lt;- dataRetrieval::readNWISpeak("07148400", convertType=FALSE)
  PK &lt;- splitPeakCodes(PK)
  plotPeaks(PK, codes=TRUE,  showDubNA=TRUE,
                xlab="Water year", ylab="Discharge, cfs") # 


  # The dataRetrieval package is not required by MGBT algorithms.
  PK &lt;- dataRetrieval::readNWISpeak("08329935", convertType=FALSE)
  PK &lt;- splitPeakCodes(PK)
  plotPeaks(PK, codes=TRUE, showDubNA=TRUE,
                xlab="Water year", ylab="Discharge, cfs") # 
</code></pre>

<hr>
<h2 id='plotPeaks_batch'> Plot for More than One Streamgage that Peak Streamflows with Emphasis on Peak Discharge Qualification Codes </h2><span id='topic+plotPeaks_batch'></span>

<h3>Description</h3>

<p>Plot U.S. Geological Survey peak streamflows for multiple streamgages. This function is a wrapper on calls to <code><a href="#topic+plotPeaks">plotPeaks</a></code> with data retrieval occurring just ahead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotPeaks_batch(sites, file=NA, do_plot=TRUE,
                       silent=FALSE, envir=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotPeaks_batch_+3A_sites">sites</code></td>
<td>
<p>A list of USGS site identification numbers for which one-by-one, the peaks will be pulled from <code>dataRetrieval::</code><code>readNWISpeak</code>;</p>
</td></tr>
<tr><td><code id="plotPeaks_batch_+3A_file">file</code></td>
<td>
<p>A portable document format output path and file name, setting to <code>NA</code> will plot into the running application unless <code>do_plot=FALSE</code>;</p>
</td></tr>
<tr><td><code id="plotPeaks_batch_+3A_do_plot">do_plot</code></td>
<td>
<p>A logical triggering the plotting operations. This is a useful feature for test or for a iterative use as seen in the second <b>Example</b>. A setting of false will set <code>file=NA</code> internally regardless of argument given;</p>
</td></tr>
<tr><td><code id="plotPeaks_batch_+3A_silent">silent</code></td>
<td>
<p>A logical to control status messaging;</p>
</td></tr>
<tr><td><code id="plotPeaks_batch_+3A_envir">envir</code></td>
<td>
<p>An optional and previously populated enviroment by site number storing a table of peaks from the <code><a href="#topic+splitPeakCodes">splitPeakCodes</a></code> function (see <b>Examples</b>); and</p>
</td></tr>
<tr><td><code id="plotPeaks_batch_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass to <code><a href="#topic+plotPeaks">plotPeaks</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list is returned by streamgage of the retrieved data with an attribute <code>empty_sites</code> storing those site numbers given for which no peaks were retrieved/processed. The data structure herein recognizes a <code>NA</code> for a site.
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith</p>


<h3>References</h3>

<p>U.S. Geological Survey, 2019, USGS water data for the Nation: U.S. Geological Survey National Water Information System database, accessed October 11, 2019, at doi: <a href="https://doi.org/10.5066/F7P55KJN">10.5066/F7P55KJN</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plotPeaks">plotPeaks</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  # The dataRetrieval package is not required by MGBT algorithms, but needed
  # for the the plotPeaks_batch() function.
  sites &lt;- c("07358570", "07358280", "07058980", "07258000")
  PK &lt;- plotPeaks_batch(sites, xlab="WATER YEAR",         lot=0,
                               ylab="STREAMFLOW, IN CFS", file=NA) # 


  # In this example, two calls to plotPeaks_batch() are made. The first is to use
  # the function as a means to cache the retrieval of the peaks without the
  # plotting overhead etc. The second is to actually perform the plotting.
  pdffile &lt;- tempfile(pattern = "peaks", tmpdir = tempdir(), fileext = ".pdf")
  sites &lt;- c("08106300", "08106310", "08106350", "08106500") # 08106350 no peaks

  PK &lt;- plotPeaks_batch(sites, do_plot=FALSE)    # a hack to use its wrapper on
  # dataRetrieval::readNWISpeak() to get the peaks retrieved and code parsed by
  # splitPeakCodes() and then we can save the PK for later purposes as needed.

  empty_sites &lt;- attr(PK, "empty_sites") # we harvest 08106350 as empty
  message("EMPTY SITES: ", paste(empty_sites, collapse=", "))

  PK &lt;- as.environment(PK) # now flipping to environment for the actually plotting
                           # plotting pass to follow next and retrieval is not made
  # save(empty_sites, PK, file="PEAKS.RData") # a way to save the work for later
  PK &lt;- plotPeaks_batch(sites, xlab="WATER YEAR",        lot=0, envir=PK,
                               ylab="STREAMFLOW, IN CFS", file=pdffile)
  message("Peaks plotted in file=", pdffile) # 
</code></pre>

<hr>
<h2 id='readNWISwatstore'>Read NWIS WATSTORE-Formatted Period of Record Peak Streamflows and Other Germane Operations</h2><span id='topic+readNWISwatstore'></span>

<h3>Description</h3>

<p>Read U.S. Geological Survey (USGS) National Water Information System (NWIS) (U.S. Geological Survey, 2019) peak streamflows. But with a major high-level twist what is retained or produced by the operation. This function retrieves the WATSTORE formatted version of the peak streamflow data on a streamgage by streamgage basis and this is the format especially suitable for the USGS PeakFQ software (U.S. Geological Survey, 2020). That format will reflect the period of record. This function uses direct URL pathing to NWIS to retrieve this format and write the results to the user's file system. The function does not use the <span class="pkg">dataRetrieval</span> package for the WATSTORE format. Then optionally, this function uses the <span class="pkg">dataRetrieval</span> package to retrieve a tab-delimited version of the peak streamflows and write those to the user's file system. Peak streamflow visualization with attention to the peak discharge qualification codes and other features is optionally made here by the <code><a href="#topic+plotPeaks">plotPeaks</a></code> function and optionally a portable document formatted graphics file can be written as well. This function is explicitly design around a single directory for each streamgage to store the retrieved and plotted data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readNWISwatstore(siteNumbers, path=".", dirend="d",
                              tabpk=TRUE, vispk=TRUE, vispdf=TRUE,
                              unlinkpath=FALSE, citeNWISdoi=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readNWISwatstore_+3A_sitenumbers">siteNumbers</code></td>
<td>
<p>USGS site number(or multiple sites) as string(s). This is usually an 8 digit number but not exclusively so and is the <code>site_no</code> slot in the NWIS database. The WATSTORE formatted data will be written into  file name being set to <code>site_no".pkf"</code> (the extension <code>".pkf"</code> is to be read as &ldquo;peak flows&rdquo;);</p>
</td></tr>
<tr><td><code id="readNWISwatstore_+3A_path">path</code></td>
<td>
<p>A directory path with default to current working directory;</p>
</td></tr>
<tr><td><code id="readNWISwatstore_+3A_dirend">dirend</code></td>
<td>
<p>Optional character content appended to the site number as part of directory creation. For example, if the <code>site_no</code> is <code>08167000</code> and the default <code>dirend="d"</code>, then if the <code>path</code> is the current working directory, then the full path to the directory staged to contain results of this function is <code>./08167000d</code>;</p>
</td></tr>
<tr><td><code id="readNWISwatstore_+3A_tabpk">tabpk</code></td>
<td>
<p>A logical to trigger writing of the period of record of the peak streamflows in a tab-delimited format by the <code>write.table()</code> function with the file name being set to <code>site_no".txt"</code>;</p>
</td></tr>
<tr><td><code id="readNWISwatstore_+3A_vispk">vispk</code></td>
<td>
<p>A logical to trigger <code><a href="#topic+plotPeaks">plotPeaks</a></code> for the peak streamflow visualization;</p>
</td></tr>
<tr><td><code id="readNWISwatstore_+3A_vispdf">vispdf</code></td>
<td>
<p>A logical to trigger <code>pdf()</code> and <code>dev.off()</code> to create a portable document format of the peak streamflow visualization by <code><a href="#topic+plotPeaks">plotPeaks</a></code> file name being set to <code>site_no".pdf"</code>;</p>
</td></tr>
<tr><td><code id="readNWISwatstore_+3A_unlinkpath">unlinkpath</code></td>
<td>
<p>A logical to trigger unlinking of the full path ahead of its recreation and then operation of the side effects of this function. The unlinking is recursive, so attention to settings of <code>path</code> and <code>dirend</code> are self evident;</p>
</td></tr>
<tr><td><code id="readNWISwatstore_+3A_citenwisdoi">citeNWISdoi</code></td>
<td>
<p>A logical to trigger the writing of a <code>CITATION.md</code> file to the NWIS DOI on the date of the retrieval. It is USGS policy to use the citation to NWIS by its digital object identifier (DOI) and provide an accessed date. The<code>CITATION.md</code> has been written in a simple markdown format so that software repository infrastructure (such as <a href="https://code.usgs.gov">https://code.usgs.gov</a>) would markup this file to a simple web page when selected by the user in an internet browser; and</p>
</td></tr>
<tr><td><code id="readNWISwatstore_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass to <code><a href="#topic+plotPeaks">plotPeaks</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No values are returned; this function is used for its file system side effects. Those effects will include <code>".pkf"</code> (WATSTORE formatted peak streamflow data) and potentially include the following additional file by extension: <code>".pdf"</code>, <code>".txt"</code> (tab-delimited peak streamflow data), and <code>CITATION.md</code> file recording a correct policy-compliant citation to the USGS NWIS DOI number with access date as per additional arguments to this function. This function does not recognize the starting and ending period options that the <span class="pkg">dataRetrieval</span> package provides because the WATSTORE format is ensured to be period of record. Therefore, the disabling of optional period retrievals ensures that the <code>".pkf"</code> and <code>".[txt|pdf]"</code> files have the same data.
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith</p>


<h3>References</h3>

<p>U.S. Geological Survey, 2019, USGS water data for the Nation: U.S. Geological Survey National Water Information System database, accessed October 11, 2019, at doi: <a href="https://doi.org/10.5066/F7P55KJN">10.5066/F7P55KJN</a>.
</p>
<p>U.S. Geological Survey, 2020, PeakFQ&mdash;Flood frequency analysis based on Bulletin 17C and recommendations of the Advisory Committee on Water Information (ACWI) Subcommittee on Hydrology (SOH) Hydrologic Frequency Analysis Work Group (HFAWG), version 7.3, accessed February 1, 2020, at <a href="https://water.usgs.gov/software/PeakFQ/">https://water.usgs.gov/software/PeakFQ/</a>.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+plotPeaks">plotPeaks</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  # The dataRetrieval package provides a readNWISpeak() function but that works on
  # the tab-delimited like format structure and not the WATSTORE structure. The
  # WATSTORE structure is used by the USGS PeakFQ software and direct URL is needed
  # to acquire such data
  path &lt;- tempdir()
  readNWISwatstore("08167000", path=path)
  # path/08167000d/08167000.[pdf|pkf|txt]
  #                CITATION.md
  # files will be created in the temporary directory 
</code></pre>

<hr>
<h2 id='RSlo'> Rosner RST Test Adjusted for Low Outliers </h2><span id='topic+RSlo'></span>

<h3>Description</h3>

<p>The Rosner (1975) method or the essence of the method, given the order statistics <code class="reqn">x_{[1:n]} \le x_{[2:n]} \le \cdots \le x_{[(n-1):n]} \le x_{[n:n]}</code>, is the statistic:
</p>
<p style="text-align: center;"><code class="reqn">RS_r =
\frac{ x_{[r:n]} - \mathrm{mean}\{x_{[(r+1)\rightarrow(n-r):n]}\} }
     {\sqrt{\mathrm{var}\{x_{[(r+1)\rightarrow(n-r):n]}\}}}\mbox{,}
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>RSlo(x, r, n=length(x))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RSlo_+3A_x">x</code></td>
<td>
<p>The data values and note that base-10 logarithms of these are not computed internally;</p>
</td></tr>
<tr><td><code id="RSlo_+3A_r">r</code></td>
<td>
<p>The number of truncated observations; and</p>
</td></tr>
<tr><td><code id="RSlo_+3A_n">n</code></td>
<td>
<p>The number of observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The value for <code class="reqn">RS_r</code>.
</p>


<h3>Note</h3>

<p>Regarding <code>n=length(x)</code>, it is not clear that TAC intended <code>n</code> to be not equal to the sample size. TAC chose to not determine the length of <code>x</code> internally to the function but to have it available as an argument. Also <code><a href="#topic+MGBTcohn2011">MGBTcohn2011</a></code> and <code><a href="#topic+BLlo">BLlo</a></code> were similarly designed.
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith consulting T.A. Cohn sources </p>


<h3>Source</h3>

<p><code>LowOutliers_jfe(R).txt</code> and <code>LowOutliers_wha(R).txt</code>&mdash;Named <code>RST</code>
</p>


<h3>References</h3>

<p>Cohn, T.A., 2013&ndash;2016, Personal communication of original R source code: U.S. Geological Survey, Reston, Va.
</p>
<p>Rosner, Bernard, 1975, On the detection of many outliers: Technometrics, v. 17, no. 2, pp. 221&ndash;227.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MGBTcohn2011">MGBTcohn2011</a></code>, <code><a href="#topic+BLlo">BLlo</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Long CPU time, arguments slightly modified to run faster and TAC had.
# TAC has maxr=n/2 (the lower tail) but WHA has changed to maxr=3 for
# speed to get the function usable during automated tests.
testMGBvsN3 &lt;- function(n=100, maxr=3, nrep=10000) { # TAC named function
   for(r in 1:maxr) {
      set.seed(123457)
      res1 &lt;- replicate(nrep, { x &lt;- sort(rnorm(n))
                 c(MGBTcohn2011(x,r),BLlo(x,r),RSlo(x,r)) })
      r1 &lt;- rank(res1[1,]); r2 &lt;- rank(res1[2,]); r3 &lt;- rank(res1[3,])
      v &lt;- quantile(r1,.1); h &lt;- quantile(r2,.1)
      plot(r1,r2)
      abline(v=v, col=2); abline(h=h, col=2)
      message(' BLlo ',r, " ", cor(r1,r2), " ", mean((r1 &lt;= v) &amp; (r2 &lt;= h)))
      v &lt;- quantile(r1,.1); h &lt;- quantile(r2,.1)
      plot(r1,r3)
      abline(v=v, col=2); abline(h=h, col=2)
      mtext(paste0("Order statistic iteration =",r," of ",maxr))
      message('RSTlo ',r, " ", cor(r1,r3), " ", mean((r1 &lt;= v) &amp; (r3 &lt;= h)))
   }
}
testMGBvsN3() #
</code></pre>

<hr>
<h2 id='RthOrderPValueOrthoT'> P-value for the Rth Order Statistic </h2><span id='topic+RthOrderPValueOrthoT'></span>

<h3>Description</h3>

<p>Compute the p-value for the <code class="reqn">r</code>th order statistic
</p>
<p style="text-align: center;"><code class="reqn">\eta(r,n) = \frac{x_{[r:n]} - \mathrm{mean}\{x_{[(r+1)\rightarrow n:n]}\}}
                 {\sqrt{\mathrm{var}\{x_{[(r+1)\rightarrow n:n]}\}}}\mbox{.}</code>
</p>

<p>This function is the cumulative distribution function of the Grubbs&ndash;Beck statistic (<code>eta</code> = <code class="reqn">GB_r(p)</code>). In distribution notation, this is equivalent to saying <code class="reqn">F(GB_r)</code> for nonexceedance probability <br />
<code class="reqn">F \in (0,1)</code>. The inverse or quantile function <code class="reqn">GB_r(F)</code> is <code><a href="#topic+CritK">CritK</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RthOrderPValueOrthoT(n, r, eta, n.sim=10000, silent=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RthOrderPValueOrthoT_+3A_n">n</code></td>
<td>
<p>The number of observations;</p>
</td></tr>
<tr><td><code id="RthOrderPValueOrthoT_+3A_r">r</code></td>
<td>
<p>The number of truncated observations; and</p>
</td></tr>
<tr><td><code id="RthOrderPValueOrthoT_+3A_eta">eta</code></td>
<td>
<p>The pseudo-studentized magnitude of <code class="reqn">r</code>th smallest observation;</p>
</td></tr>
<tr><td><code id="RthOrderPValueOrthoT_+3A_n.sim">n.sim</code></td>
<td>
<p>The sample size to attempt a Monte Carlo integration in case the numerical integration via <code>integrate()</code> encounters a divergent integral; and</p>
</td></tr>
<tr><td><code id="RthOrderPValueOrthoT_+3A_silent">silent</code></td>
<td>
<p>A logical controlling the silence of <code>try</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The value a two-column <span class="rlang"><b>R</b></span> <code>matrix</code>.
</p>


<h3>Note</h3>

<p>The extension to Monte Carlo integration in event of failure of the numerical integration an extension is by WHA. The <b>Note</b> for <code><a href="#topic+MGBT">MGBT</a></code> provides extensive details in the context of a practical application.
</p>
<p>Note that in conjunction with <code>RthOrderPValueOrthoT</code>, TAC provided an enhanced numerical integration interface (<code>integrateV()</code>) to <code>integrate()</code> built-in to <span class="rlang"><b>R</b></span>. In fact, all that TAC did was wrap a vectorization scheme using <code>sapply()</code> on top of <code><a href="#topic+peta">peta</a></code>. The issue is that <code><a href="#topic+peta">peta</a></code> was not designed to be vectorized. WHA has simply inserted the <code>sapply</code> <span class="rlang"><b>R</b></span> idiom inside <code><a href="#topic+peta">peta</a></code> and hence vectorizing it and removed the need in the <span class="pkg">MGBT</span> package for the <code>integrateV()</code> function in the TAC sources.
</p>
<p>TAC named this function with the <code>Kth</code> order. In code, however, TAC uses the variable <code>r</code>. WHA has migrated all references to <code>Kth</code> to <code>Rth</code> for systematic consistency. Hence, this function has been renamed to <code>RthOrderPValueOrthoT</code>.
</p>
<p>TAC also provides a <code>KthOrderPValueOrthoTb</code> function and notes that it employs simple Gaussian quadrature to compute the integral much more quickly. However, it is slightly less accurate for tail probabilities. The Gaussian quadrature is from a function <code>gauss.quad.prob()</code>, which seems to not be found in the TAC sources given to WHA.
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith consulting T.A. Cohn sources </p>


<h3>Source</h3>

<p><code>LowOutliers_jfe(R).txt</code>, <code>LowOutliers_wha(R).txt</code>, <code>P3_089(R).txt</code>&mdash;<br />
Named <code>KthOrderPValueOrthoT</code> + <code>KthOrderPValueOrthoTb</code>
</p>


<h3>References</h3>

<p>Cohn, T.A., 2013&ndash;2016, Personal communication of original R source code: U.S. Geological Survey, Reston, Va.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MGBT">MGBT</a></code>, <code><a href="#topic+CritK">CritK</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Running next line without the $value will show:
#0.001000002 with absolute error &lt; 1.7e-05 # This is output from the integrate()
# function, which means that the numerical integration worked.
RthOrderPValueOrthoT(58, 2, -3.561143)$value


# Long CPU time
CritK(58, 2, RthOrderPValueOrthoT(58, 2, -3.561143)$value)
#[1] -3.561143  # Therefore CritK() is the inverse of this function.


# Long CPU time
# Monte Carlo distribution of rth pseudo-studentized order statistic (TAC note)
testRthOrderPValueOrthoT &lt;- function(nrep=1E4, r=2, n=100,
               test_quants = c(0.05,0.1,0.5,0.9,0.95),  ndigits=3, seed=1) {
   set.seed(seed)
   z &lt;- replicate(nrep, { x &lt;- sort(rnorm(n)); xr &lt;- x[r]; x2 &lt;- x[(r+1):n]
                         (xr - mean(x2))/sqrt(var(x2)) })
     res &lt;- sapply(quantile(z, test_quants), function(q) {
                 c(q, RthOrderPValueOrthoT(n,r,q)$value) })
   round(res,ndigits)
}

nsim &lt;- 1E4
for(n in 50) {   # original TAC sources had c(10,15,25,50,100,500)
   for(r in 5) { # original TAC sources had 1:min(10,floor(n/2))
      message("n=",n, " and r=",r)
      print(testRthOrderPValueOrthoT(nrep=nsim, n=n, r=r))
   }
}
# Output like this will be seen
# n=50 and r=5
#         5%    10%    50%    90%    95%
#[1,] -2.244 -2.127 -1.788 -1.523 -1.460
#[2,]  0.046  0.096  0.499  0.897  0.946
# that shows simulated percentages near the theoretical

# To get the MSE of the results (TAC note). See WHA note on a change below and
# it is suspected that TAC's "tests" might have been fluid in the sense that
# he would modify as needed and did not fully design as Examples for end users.
rr &lt;- rep(0,10)
for(n in 50) {   # original TAC sources had c(10,15,25,50,100,500)
   for(r in 5) { # original TAC sources had 1:min(10,floor(n/2))
      message("n=",n, " and r=",r)
      for(i in 1:10) { # The [1,1] is WHA addition to get function to run.
         # extract the score for the 5% level
         rr[i] &lt;- testRthOrderPValueOrthoT(nrep=nsim, n=n, r=r, seed=i)[1,1]
      }
      message("var (MSE):", sqrt(var(rr/100)))
   }
}
# Output like this will be seen
# n=50 and r=5
# var (MSE):6.915361322608e-05 


#  Long CPU time
#  Monte Carlo computation of critical values for special cases (TAC note)
CritValuesMC &lt;-
function(nrep=50, kvs=c(1,3,0.25,0.5), n=100, ndigits=3, seed=1,
         test_quants=c(0.01,0.10,0.50)) {
   set.seed(seed)
   k_values &lt;- ifelse(kvs &gt;= 1, kvs, ceiling(n*kvs))
   z  &lt;- replicate(nrep, {
      x &lt;- sort(rnorm(n))
      sapply(k_values, function(r) {
          xr &lt;- x[r]; x2 &lt;- x[(r+1):n]
         (xr-mean(x2)) / sqrt(var(x2)) })  })
   res &lt;- round(apply(z, MARGIN=1, quantile, test_quants), ndigits)
   colnames(res) &lt;- k_values; return(res)
}

# TAC example. Note that z acquires its square dimension from test_quants
# but Vr is used in the sapply(). WHA has reset Vr to
n=100; nrep=10000; test_quants=c(.05,.5,1); Vr=1:10 # This Vr by TAC
z &lt;- CritValuesMC(n=n, nrep=nrep, test_quants=test_quants)
Vr &lt;- 1:length(z[,1]) # WHA reset of Vr to use TAC code below. TAC Vr bug?
HH &lt;- sapply(Vr, function(r) RthOrderPValueOrthoT(n, r, z[1,r])$value)
TT &lt;- sapply(Vr, function(r) RthOrderPValueOrthoT(n, r, z[2,r])$value) #
</code></pre>

<hr>
<h2 id='splitPeakCodes'> Split the Peak Discharge Qualifications Codes into Separate Columns </h2><span id='topic+splitPeakCodes'></span>

<h3>Description</h3>

<p>Split the U.S. Geological Survey (USGS) peak discharge qualifications codes (Asquith and others, 2017) in the <code>peak_cd</code> column of a peak-streamflow data retrieval from the USGS National Water Information System (NWIS) (U.S. Geological Survey, 2019) in a <code>data.frame</code> into separate columns of the input <code>data.frame</code>. The NWIS system stores all the codes within a single database field. It can be useful for graphical (<code><a href="#topic+plotPeaks">plotPeaks</a></code>) or other statistical study to have single logical variable for each of the codes, and such is the purpose of this function. Note because of the <code>appearsSystematic</code> field is based computations involving the <code>water_yr</code> (water year), this function needs the <code><a href="#topic+makeWaterYear">makeWaterYear</a></code> to have been run first; however, the function will autodetect and call that function internally if needed and those effects are provided on the returned <code>data.frame</code>. (See also the <code>inst/legend/</code> subdirectory of this package for a script to produce a legend as well as <code>inst/legend/legend_camera.pdf</code>, which has been dressed up in a vector graphics editing program.)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>splitPeakCodes(x, all_peaks_na_okay=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="splitPeakCodes_+3A_x">x</code></td>
<td>
<p>A <code>data.frame</code> having a mandatory column titled <code>peak_cd</code> with discharge qualification codes. Except for a check on only one station being present in <code>site_no</code> column, no other information in <code>x</code> is consulted or otherwise used; and</p>
</td></tr>
<tr><td><code id="splitPeakCodes_+3A_all_peaks_na_okay">all_peaks_na_okay</code></td>
<td>
<p>A logical controlling whether a test on all the peak values (<code>peak_va</code>) being <code>NA</code> is made and if all the peak values are missing, then <code>NULL</code> is returned. Because much of this package is built around working with real peak discharges, the default is to be rejectionary to gage-height only streamgages. However, the <code><a href="#topic+plotPeaks">plotPeaks</a></code> function does have logic to work out ways to make plots of gage-height only streamgages.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>x</code> as originally inputted is returned with the addition of these columns:
</p>
<table>
<tr><td><code>appearsSystematic</code></td>
<td>
<p>The <code>appearsSystematic</code> column produced by the <code>splitPeakCodes</code> function is intended to provide a type of <em>canonical</em> flag on which to subset the record, which can be important for many statistical procedures;</p>
</td></tr>
<tr><td><code>anyCodes</code></td>
<td>
<p>Are <em>any</em> of the codes that follow present for a given record (row, water year) in the input data;</p>
</td></tr>
<tr><td><code>isCode1</code></td>
<td>
<p>Is a discharge qualification code of 1 present for a given record&mdash;Streamflow is a maximum daily average;</p>
</td></tr>
<tr><td><code>isCode2</code></td>
<td>
<p>Is a discharge qualification code of 2 present for a given record&mdash;Streamflow is an estimate;</p>
</td></tr>
<tr><td><code>isCode3</code></td>
<td>
<p>Is a discharge qualification code of 3 present for a given record&mdash;Streamflow affected by dam failure;</p>
</td></tr>
<tr><td><code>isCode4</code></td>
<td>
<p>Is a discharge qualification code of 4 present for a given record&mdash;Streamflow is less than indicated value, which is the minimum recordable value at this site;</p>
</td></tr>
<tr><td><code>isCode5</code></td>
<td>
<p>Is a discharge qualification code of 5 present for a given record&mdash;Streamflow affected to an unknown degree by regulation or diversion;</p>
</td></tr>
<tr><td><code>isCode6</code></td>
<td>
<p>Is a discharge qualification code of 6 present for a given record&mdash;Streamflow is affected by regulation or diversion;</p>
</td></tr>
<tr><td><code>isCode7</code></td>
<td>
<p>Is a discharge qualification code of 7 present for a given record&mdash;Streamflow is a historical peak;</p>
</td></tr>
<tr><td><code>isCode8</code></td>
<td>
<p>Is a discharge qualification code of 8 present for a given record&mdash;Streamflow is actually greater than the indicated value;</p>
</td></tr>
<tr><td><code>isCode9</code></td>
<td>
<p>Is a discharge qualification code of 9 present&mdash;Streamflow is affected by snow melt, hurricane, ice-jam, or debris-dam breakup;</p>
</td></tr>
<tr><td><code>isCodeA</code></td>
<td>
<p>Is a discharge qualification code of A present for a given record&mdash;Year of occurrence is unknown or not exact;</p>
</td></tr>
<tr><td><code>isCodeB</code></td>
<td>
<p>Is a discharge qualification code of B present for a given record&mdash;Month or day of occurrence is unknown or not exact;</p>
</td></tr>
<tr><td><code>isCodeC</code></td>
<td>
<p>Is a discharge qualification code of C present for a given record&mdash;All or part of the record is affected by urbanization, mining, agricultural changes, channelization, or other anthropogenic activity;</p>
</td></tr>
<tr><td><code>isCodeD</code></td>
<td>
<p>Is a discharge qualification code of D present for a given record&mdash;Base streamflow changed during this year;</p>
</td></tr>
<tr><td><code>isCodeE</code></td>
<td>
<p>Is a discharge qualification code of E present for a given record&mdash;Only annual peak streamflow available for this year; and</p>
</td></tr>
<tr><td><code>isCodeO</code></td>
<td>
<p>Is a discharge qualification code of E present for a given record&ndash;Opportunistic value not from systematic data collection. By extension, the presence of this code will trigger the <code>appearsSystematic</code> to false even if the peak itself otherwise appears part of systematic record from the gap analysis.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Concerning <code>appearsSystematic</code>: All records but missing discharges are assumed as systematic records unless the peak streamflows are missing or peaks have a code 7 but there are existing years on either side of individual peaks coded as 7. The logic also handles a so-called &ldquo;roll-on&rdquo; and &ldquo;roll-off&rdquo; of the data by only testing the leading or trailing year&mdash;except this becomes a problem if multiple stations are involved, so the code will return early with a warning. Importantly, it is possible that some code 7s can be flagged as systematic and these are not necessarily in error. Testing indicates that some USGS Water Science Centers (maintainers of databases) have historically slightly different tendencies in application of the code 7. The USGS NWIS database does not actually contain a field indicating that a peak was collected as part of systematic operation and hence that peak is part of an assumed random sample. Peaks with gage height only are flagged as nonsystematic by fiat&mdash;this might not be the best solution over all, but because virtually all statistics use the discharge column this seems okay (feature is subject to future changes).
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith</p>


<h3>References</h3>

<p>Asquith, W.H., Kiang, J.E., and Cohn, T.A., 2017, Application of at-site peak-streamflow frequency analyses for very low annual exceedance probabilities: U.S. Geological Survey Scientific Investigation Report 2017&ndash;5038, 93 p., doi: <a href="https://doi.org/10.3133/sir20175038">10.3133/sir20175038</a>.
</p>
<p>U.S. Geological Survey, 2019, USGS water data for the Nation: U.S. Geological Survey National Water Information System database, accessed October 11, 2019, at doi: <a href="https://doi.org/10.5066/F7P55KJN">10.5066/F7P55KJN</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+makeWaterYear">makeWaterYear</a></code>, <code><a href="#topic+plotPeaks">plotPeaks</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  # The dataRetrieval package is not required by MGBT algorithms.
  PK &lt;- dataRetrieval::readNWISpeak("08167000", convertType=FALSE)
  PK &lt;- splitPeakCodes(PK)
  names(PK) # See that the columns are there.
</code></pre>

<hr>
<h2 id='V'>  Covariance matrix of M and S-squared </h2><span id='topic+V'></span>

<h3>Description</h3>

<p>Compute the covariance matrix of <code class="reqn">M</code> and <code class="reqn">S^2</code> (S-squared) given <code class="reqn">q_\mathrm{min}</code>. Define the vector of four moment expectations
</p>
<p style="text-align: center;"><code class="reqn">E_{i\in 1,2,3,4} = \Psi\bigl(\Phi^{(-1)}(q_\mathrm{min}), i\bigr)\mbox{,}</code>
</p>

<p>where <code class="reqn">\Psi(a,b)</code> is the <code><a href="#topic+gtmoms">gtmoms</a></code> function and <code class="reqn">\Phi^{(-1)}</code> is the inverse of the standard normal distribution. Using these <code class="reqn">E</code>, define a vector <code class="reqn">C_{i\in 1,2,3,4}</code> as a system of nonlinear combinations:
</p>
<p style="text-align: center;"><code class="reqn">C_1 = E_1\mbox{,}</code>
</p>

<p style="text-align: center;"><code class="reqn">C_2 = E_2 -     E_1^2\mbox{,}</code>
</p>

<p style="text-align: center;"><code class="reqn">C_3 = E_3 - 3E_2E_1 + 2E_1^3\mbox{, and}</code>
</p>

<p style="text-align: center;"><code class="reqn">C_4 = E_4 - 4E_3E_1 + 6E_2E_1^2 - 3E_1^4\mbox{.}</code>
</p>

<p>Given <code class="reqn">k = n - r</code> from the arguments of this function, compute the symmetrical covariance matrix <code class="reqn">COV</code> with variance of <code class="reqn">M</code> as
</p>
<p style="text-align: center;"><code class="reqn">COV_{1,1} = C_2/k\mbox{,}</code>
</p>

<p>the covariance between <code class="reqn">M</code> and <code class="reqn">S^2</code> as
</p>
<p style="text-align: center;"><code class="reqn">COV_{1,2} = COV_{2,1} = \frac{C_3}{\sqrt{k(k-1)}}\mbox{, and}</code>
</p>

<p>the variance of <code class="reqn">S^2</code> as
</p>
<p style="text-align: center;"><code class="reqn">COV_{2,2} = \frac{C_4 -  C_2^2}{k} + \frac{2C_2^2}{k(k-1)}\mbox{.}</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>V(n, r, qmin)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="V_+3A_n">n</code></td>
<td>
<p>The number of observations;</p>
</td></tr>
<tr><td><code id="V_+3A_r">r</code></td>
<td>
<p>The number of truncated observations; and</p>
</td></tr>
<tr><td><code id="V_+3A_qmin">qmin</code></td>
<td>
<p>A nonexceedance probability threshold for <code class="reqn">X &gt; q_\mathrm{min}</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 2-by-2 covariance <code>matrix</code>.
</p>


<h3>Note</h3>

<p>Because the <code><a href="#topic+gtmoms">gtmoms</a></code> function is naturally vectorized and TAC sources provide no protection if <code>qmin</code> is a vector (see <b>Note</b> under <code><a href="#topic+EMS">EMS</a></code>). For the implementation here, only the first value in <code>qmin</code> is used and a warning issued if it is a vector.
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith consulting T.A. Cohn sources </p>


<h3>Source</h3>

<p><code>LowOutliers_jfe(R).txt</code>, <code>LowOutliers_wha(R).txt</code>, <code>P3_089(R).txt</code>&mdash;Named <code>V</code>
</p>


<h3>References</h3>

<p>Cohn, T.A., 2013&ndash;2016, Personal communication of original R source code: U.S. Geological Survey, Reston, Va.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+EMS">EMS</a></code>, <code><a href="#topic+VMS">VMS</a></code>, <code><a href="#topic+gtmoms">gtmoms</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>V(58,2,.5)
#            [,1]        [,2]
#[1,] 0.006488933 0.003928333
#[2,] 0.003928333 0.006851120
</code></pre>

<hr>
<h2 id='VMS'>  Covariance matrix of M and S </h2><span id='topic+VMS'></span>

<h3>Description</h3>

<p>Compute the covariance matrix of <code class="reqn">M</code> and <code class="reqn">S</code> given <code class="reqn">q_\mathrm{min}</code>.  Define the vector of four moment expectations
</p>
<p style="text-align: center;"><code class="reqn">E_{i\in 1,2} = \Psi\bigl(\Phi^{(-1)}(q_\mathrm{min}), i\bigr)\mbox{,}</code>
</p>

<p>where <code class="reqn">\Psi(a,b)</code> is the <code><a href="#topic+gtmoms">gtmoms</a></code> function and <code class="reqn">\Phi^{(-1)}</code> is the inverse of the standard normal distribution. Define the scalar quantity <code class="reqn">Es = </code> <code>EMS(n,r,qmin)[2]</code> as the expectation of <code class="reqn">S</code> using the <code><a href="#topic+EMS">EMS</a></code> function, and define the scalar quantity <code class="reqn">E_s^2 = E_2 - E_1^2</code> as the expectation of <code class="reqn">S^2</code>. Finally, compute the covariance matrix <code class="reqn">COV</code> of <code class="reqn">M</code> and <code class="reqn">S</code> using the <code><a href="#topic+V">V</a></code> function:
</p>
<p style="text-align: center;"><code class="reqn">COV_{1,1} = V_{1,1}\mbox{,}</code>
</p>

<p style="text-align: center;"><code class="reqn">COV_{1,2} = COV_{2,1} = V_{1,2}/2Es\mbox{,}</code>
</p>

<p style="text-align: center;"><code class="reqn">COV_{2,2} = E_s^2 - (E_s)^2\mbox{.}</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>VMS(n, r, qmin)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="VMS_+3A_n">n</code></td>
<td>
<p>The number of observations;</p>
</td></tr>
<tr><td><code id="VMS_+3A_r">r</code></td>
<td>
<p>The number of truncated observations; and</p>
</td></tr>
<tr><td><code id="VMS_+3A_qmin">qmin</code></td>
<td>
<p>A nonexceedance probability threshold for <code class="reqn">X &gt; q_\mathrm{min}</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 2-by-2 covariance <code>matrix</code>.
</p>


<h3>Note</h3>

<p>Because the <code>gtmoms</code> function is naturally vectorized and TAC sources provide no protection if <code>qmin</code> is a vector (see <b>Note</b> under <code><a href="#topic+EMS">EMS</a></code>). For the implementation here, only the first value in <code>qmin</code> is used and a warning issued if it is a vector.
</p>


<h3>Author(s)</h3>

<p> W.H. Asquith consulting T.A. Cohn sources </p>


<h3>Source</h3>

<p><code>LowOutliers_jfe(R).txt</code>, <code>LowOutliers_wha(R).txt</code>, <code>P3_089(R).txt</code>&mdash;Named <code>VMS</code>
</p>


<h3>References</h3>

<p>Cohn, T.A., 2013&ndash;2016, Personal communication of original R source code: U.S. Geological Survey, Reston, Va.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+EMS">EMS</a></code>, <code><a href="#topic+V">V</a></code>, <code><a href="#topic+gtmoms">gtmoms</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>VMS(58,2,.5) # Note that [1,1] is the same as [1,1] for Examples under V().
#            [,1]        [,2]
#[1,] 0.006488933 0.003279548
#[2,] 0.003279548 0.004682506
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
