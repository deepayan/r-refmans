<!DOCTYPE html><html lang="en"><head><title>Help for package TextForecast</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {TextForecast}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#get_collocations'><p>get_collocations function</p></a></li>
<li><a href='#get_terms'><p>get_terms function</p></a></li>
<li><a href='#get_words'><p>get_words function</p></a></li>
<li><a href='#hard_thresholding'><p>hard thresholding</p></a></li>
<li><a href='#news_data'><p>News Data</p></a></li>
<li><a href='#optimal_alphas'><p>Title optimal alphas function</p></a></li>
<li><a href='#optimal_factors'><p>Optimal Factors</p></a></li>
<li><a href='#optimal_number_factors'><p>optimal number of factors function</p></a></li>
<li><a href='#optimal_x'><p>Optimal x</p></a></li>
<li><a href='#stock_data'><p>Stock Data</p></a></li>
<li><a href='#text_forecast'><p>Text Forecast function</p></a></li>
<li><a href='#text_nowcast'><p>text nowcast</p></a></li>
<li><a href='#tf_idf'><p>tf-idf function</p></a></li>
<li><a href='#top_terms'><p>Top Terms Function</p></a></li>
<li><a href='#tv_dictionary'><p>tv dictionary function</p></a></li>
<li><a href='#tv_sentiment_index'><p>tv sentiment index function</p></a></li>
<li><a href='#tv_sentiment_index_all_coefs'><p>TV sentiment index using all positive and negative coefficients.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Regression Analysis and Forecasting Using Textual Data from a
Time-Varying Dictionary</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.3</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides functionalities based on the paper "Time Varying Dictionary and the Predictive Power of FED Minutes" (Lima, 2018) &lt;<a href="https://doi.org/10.2139%2Fssrn.3312483">doi:10.2139/ssrn.3312483</a>&gt;. It selects the most predictive terms, that we call time-varying dictionary using supervised machine learning techniques as lasso and elastic net.     </td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>Imports:</td>
<td>forecast, stats, tidyr, tidytext, tm, wordcloud, dplyr, plyr,
udpipe, RColorBrewer, ggplot2, glmnet, pdftools, parallel,
doParallel, pracma, forcats, Matrix</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/lucasgodeiro/TextForecast">https://github.com/lucasgodeiro/TextForecast</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/lucasgodeiro/TextForecast/issues">https://github.com/lucasgodeiro/TextForecast/issues</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, covr</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-04-22 11:49:52 UTC; Lucas</td>
</tr>
<tr>
<td>Author:</td>
<td>Luiz Renato Lima [aut],
  Lucas Godeiro [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Lucas Godeiro &lt;lucas.godeiro@hotmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-04-25 08:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='get_collocations'>get_collocations function</h2><span id='topic+get_collocations'></span>

<h3>Description</h3>

<p>get_collocations function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_collocations(
  corpus_dates,
  path_name,
  ntrms,
  ngrams_number,
  min_freq,
  language
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_collocations_+3A_corpus_dates">corpus_dates</code></td>
<td>
<p>a character vector indicating the subfolders where are located the texts.</p>
</td></tr>
<tr><td><code id="get_collocations_+3A_path_name">path_name</code></td>
<td>
<p>the folders path where the subfolders with the dates are located.</p>
</td></tr>
<tr><td><code id="get_collocations_+3A_ntrms">ntrms</code></td>
<td>
<p>maximum numbers of collocations  that will be filtered by tf-idf. We rank the collocations by tf-idf in a decreasing order. Then, after we select the words with the ntrms highest tf-idf.</p>
</td></tr>
<tr><td><code id="get_collocations_+3A_ngrams_number">ngrams_number</code></td>
<td>
<p>integer indicating the size of the collocations. Defaults to 2, indicating to compute bigrams. If set to 3, will find collocations of bigrams and trigrams.</p>
</td></tr>
<tr><td><code id="get_collocations_+3A_min_freq">min_freq</code></td>
<td>
<p>integer indicating the frequency of how many times a collocation should at least occur in the data in order to be returned.</p>
</td></tr>
<tr><td><code id="get_collocations_+3A_language">language</code></td>
<td>
<p>the texts language. Default is english.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing  a sparse matrix with the all collocations couting and another with a tf-idf filtered collocations counting according to the ntrms.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
st_year=2017
end_year=2018
path_name=system.file("news",package="TextForecast")
#qt=paste0(sort(rep(seq(from=st_year,to=end_year,by=1),12)),
#c("m1","m2","m3","m4","m5","m6","m7","m8","m9","m10","m11","m12"))
#z_coll=get_collocations(corpus_dates=qt[1:23],path_name=path_name,
#ntrms=500,ngrams_number=3,min_freq=10)
#
path_name=system.file("news",package="TextForecast")
days=c("2019-30-01","2019-31-01")
z_coll=get_collocations(corpus_dates=days[1],path_name=path_name,
ntrms=500,ngrams_number=3,min_freq=1)

</code></pre>

<hr>
<h2 id='get_terms'>get_terms function</h2><span id='topic+get_terms'></span>

<h3>Description</h3>

<p>get_terms function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_terms(
  corpus_dates,
  ntrms_words,
  st,
  path.name,
  ntrms_collocation,
  ngrams_number,
  min_freq,
  language
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_terms_+3A_corpus_dates">corpus_dates</code></td>
<td>
<p>a character vector indicating the subfolders where the texts are located.</p>
</td></tr>
<tr><td><code id="get_terms_+3A_ntrms_words">ntrms_words</code></td>
<td>
<p>maximum numbers of words  that will be filtered by tf-idf. We rank the word by tf-idf in a decreasing order. Then, we select the words with the ntrms highest tf-idf.</p>
</td></tr>
<tr><td><code id="get_terms_+3A_st">st</code></td>
<td>
<p>set 0 to stem the words and 1 otherwise.</p>
</td></tr>
<tr><td><code id="get_terms_+3A_path.name">path.name</code></td>
<td>
<p>the folders path where the subfolders with the dates are located.</p>
</td></tr>
<tr><td><code id="get_terms_+3A_ntrms_collocation">ntrms_collocation</code></td>
<td>
<p>maximum numbers of collocations  that will be filtered by tf-idf. We rank the collocations by tf-idf in a decreasing order. Then, after we select the words with the ntrms highest tf-idf.</p>
</td></tr>
<tr><td><code id="get_terms_+3A_ngrams_number">ngrams_number</code></td>
<td>
<p>integer indicating the size of the collocations. Defaults to 2, indicating to compute bigrams. If set to 3, will find collocations of bigrams and trigrams.</p>
</td></tr>
<tr><td><code id="get_terms_+3A_min_freq">min_freq</code></td>
<td>
<p>integer indicating the frequency of how many times a collocation should at least occur in the data in order to be returned.</p>
</td></tr>
<tr><td><code id="get_terms_+3A_language">language</code></td>
<td>
<p>the texts language. Default is english.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing  a sparse matrix with the all collocations and words couting and another with a tf-idf filtered collocations and words counting according to the ntrms.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
st_year=2017
end_year=2018
path_name=system.file("news",package="TextForecast")
#qt=paste0(sort(rep(seq(from=st_year,to=end_year,by=1),12)),
#c("m1","m2","m3","m4","m5","m6","m7","m8","m9","m10","m11","m12"))
#z_terms=get_terms(corpus_dates=qt[1:23],path.name=path_name,
#ntrms_words=500,ngrams_number=3,st=0,ntrms_collocation=500,min_freq=10)
#
path_name=system.file("news",package="TextForecast")
days=c("2019-30-01","2019-31-01")
z_terms=get_terms(corpus_dates=days[1],path.name=path_name,
ntrms_words=500,ngrams_number=3,st=0,ntrms_collocation=500,min_freq=1)

</code></pre>

<hr>
<h2 id='get_words'>get_words function</h2><span id='topic+get_words'></span>

<h3>Description</h3>

<p>get_words function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_words(corpus_dates, ntrms, st, path_name, language)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_words_+3A_corpus_dates">corpus_dates</code></td>
<td>
<p>A vector of characters indicating the subfolders where are located the texts.</p>
</td></tr>
<tr><td><code id="get_words_+3A_ntrms">ntrms</code></td>
<td>
<p>maximum numbers of words  that will be filtered by tf-idf. We rank the word by tf-idf in a decreasing order. Then, we select the words with the ntrms highest tf-idf.</p>
</td></tr>
<tr><td><code id="get_words_+3A_st">st</code></td>
<td>
<p>set 0 to stem the words and 1 otherwise.</p>
</td></tr>
<tr><td><code id="get_words_+3A_path_name">path_name</code></td>
<td>
<p>the folders path where the subfolders with the dates are located.</p>
</td></tr>
<tr><td><code id="get_words_+3A_language">language</code></td>
<td>
<p>The texts language.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing  a sparse matrix with the all words couting and another with a td-idf filtered words counting according to the ntrms.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
st_year=2017
end_year=2018
path_name=system.file("news",package="TextForecast")
#qt=paste0(sort(rep(seq(from=st_year,to=end_year,by=1),12)),
#c("m1","m2","m3","m4","m5","m6","m7","m8","m9","m10","m11","m12"))
#z_wrd=get_words(corpus_dates=qt[1:23],path_name=path_name,ntrms=500,st=0)
#
path_name=system.file("news",package="TextForecast")
days=c("2019-31-01","2019-31-01")
z_wrd=get_words(corpus_dates=days,path_name=path_name,ntrms=500,st=0)

</code></pre>

<hr>
<h2 id='hard_thresholding'>hard thresholding</h2><span id='topic+hard_thresholding'></span>

<h3>Description</h3>

<p>hard thresholding
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hard_thresholding(x, w, y, p_value, newx)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hard_thresholding_+3A_x">x</code></td>
<td>
<p>the input matrix x.</p>
</td></tr>
<tr><td><code id="hard_thresholding_+3A_w">w</code></td>
<td>
<p>the optional input matrix w, that cannot be selected.</p>
</td></tr>
<tr><td><code id="hard_thresholding_+3A_y">y</code></td>
<td>
<p>the response variable.</p>
</td></tr>
<tr><td><code id="hard_thresholding_+3A_p_value">p_value</code></td>
<td>
<p>the threshold p-value.</p>
</td></tr>
<tr><td><code id="hard_thresholding_+3A_newx">newx</code></td>
<td>
<p>matrix  that selection will applied. Useful for time series, when we need the observation at time t.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the variables less than p-value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("stock_data")
data("optimal_factors")
y=as.matrix(stock_data[,2])
y=as.vector(y)
w=as.matrix(stock_data[,3])
pc=as.matrix(optimal_factors)
t=length(y)
news_factor &lt;- hard_thresholding(w=w[1:(t-1),],x=pc[1:(t-1),],y=y[2:t],p_value = 0.01,newx = pc)

</code></pre>

<hr>
<h2 id='news_data'>News Data</h2><span id='topic+news_data'></span>

<h3>Description</h3>

<p>A simple <code>tibble</code> containing the term counting of the financial news from the wall street journal and the news york times from 1992:01 through 2018:11.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>news_data
</code></pre>


<h3>Format</h3>

<p>A tibble with 1631 components.
</p>

<dl>
<dt>dates</dt><dd><p>The vector of dates.</p>
</dd>
<dt>X</dt><dd><p>The terms counting.</p>
</dd>
</dl>


<hr>
<h2 id='optimal_alphas'>Title optimal alphas function</h2><span id='topic+optimal_alphas'></span>

<h3>Description</h3>

<p>Title optimal alphas function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimal_alphas(x, w, y, grid_alphas, cont_folds, family)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="optimal_alphas_+3A_x">x</code></td>
<td>
<p>A matrix of variables to be selected by shrinkrage methods.</p>
</td></tr>
<tr><td><code id="optimal_alphas_+3A_w">w</code></td>
<td>
<p>A matrix or vector of variables that cannot be selected(no shrinkrage).</p>
</td></tr>
<tr><td><code id="optimal_alphas_+3A_y">y</code></td>
<td>
<p>response variable.</p>
</td></tr>
<tr><td><code id="optimal_alphas_+3A_grid_alphas">grid_alphas</code></td>
<td>
<p>a grid of alphas between 0 and 1.</p>
</td></tr>
<tr><td><code id="optimal_alphas_+3A_cont_folds">cont_folds</code></td>
<td>
<p>Set TRUE for contiguous folds used in time depedent data.</p>
</td></tr>
<tr><td><code id="optimal_alphas_+3A_family">family</code></td>
<td>
<p>The glmnet family.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>lambdas_opt a vector with the optimal alpha and lambda.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
data("stock_data")
data("news_data")
y=as.matrix(stock_data[1:200,2])
w=as.matrix(stock_data[1:200,3])
data("news_data")
X=news_data[1:200,2:ncol(news_data)]
x=as.matrix(X)
grid_alphas=seq(by=0.25,to=1,from=0.5)
cont_folds=TRUE
t=length(y)
optimal_alphas=optimal_alphas(x[1:(t-1),],
w[1:(t-1),],y[2:t],grid_alphas,TRUE,"gaussian")

</code></pre>

<hr>
<h2 id='optimal_factors'>Optimal Factors</h2><span id='topic+optimal_factors'></span>

<h3>Description</h3>

<p>A simple vector containing the Optimal factors  select by optimal_number_factors function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimal_factors
</code></pre>


<h3>Format</h3>

<p>A vector with 1 component.
</p>

<dl>
<dt>optimal fators x</dt><dd><p>The vector of factor.</p>
</dd>
</dl>


<hr>
<h2 id='optimal_number_factors'>optimal number of factors function</h2><span id='topic+optimal_number_factors'></span>

<h3>Description</h3>

<p>optimal number of factors function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimal_number_factors(x, kmax)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="optimal_number_factors_+3A_x">x</code></td>
<td>
<p>a matrix x.</p>
</td></tr>
<tr><td><code id="optimal_number_factors_+3A_kmax">kmax</code></td>
<td>
<p>the maximum number of factors</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with the optimal factors.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("optimal_x")
optimal_factor &lt;- optimal_number_factors(x=optimal_x,kmax=8)
</code></pre>

<hr>
<h2 id='optimal_x'>Optimal x</h2><span id='topic+optimal_x'></span>

<h3>Description</h3>

<p>A simple <code>matrix</code> containing the optimal words selected by Elastic Net from 1992:01 through 2018:11.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimal_x
</code></pre>


<h3>Format</h3>

<p>A matrix with the most predictive terms.
</p>

<dl>
<dt>x</dt><dd><p>The matrix with 4 components.</p>
</dd>
</dl>


<hr>
<h2 id='stock_data'>Stock Data</h2><span id='topic+stock_data'></span>

<h3>Description</h3>

<p>A simple <code>tibble</code> containing the S&amp;P 500 return and the VIX volatility index from 1992:01 through 2018:11.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stock_data
</code></pre>


<h3>Format</h3>

<p>A tibble with 3 components.
</p>

<dl>
<dt>dates</dt><dd><p>The vector of dates.</p>
</dd>
<dt>sp_return</dt><dd><p>The S&amp;P 500 returns.</p>
</dd>
<dt>vix</dt><dd><p>The volatility index.</p>
</dd>
</dl>


<hr>
<h2 id='text_forecast'>Text Forecast function</h2><span id='topic+text_forecast'></span>

<h3>Description</h3>

<p>Text Forecast function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>text_forecast(x, y, h, intercept)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="text_forecast_+3A_x">x</code></td>
<td>
<p>the input matrix x.</p>
</td></tr>
<tr><td><code id="text_forecast_+3A_y">y</code></td>
<td>
<p>the response variable</p>
</td></tr>
<tr><td><code id="text_forecast_+3A_h">h</code></td>
<td>
<p>the forecast horizon</p>
</td></tr>
<tr><td><code id="text_forecast_+3A_intercept">intercept</code></td>
<td>
<p>TRUE for include intercept in the forecast equation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The h step ahead forecast
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
data("stock_data")
data("news_data")
y=as.matrix(stock_data[,2])
w=as.matrix(stock_data[,3])
data("news_data")
data("optimal_factors")
pc=optimal_factors
z=cbind(w,pc)
fcsts=text_forecast(z,y,1,TRUE)
</code></pre>

<hr>
<h2 id='text_nowcast'>text nowcast</h2><span id='topic+text_nowcast'></span>

<h3>Description</h3>

<p>text nowcast
</p>


<h3>Usage</h3>

<pre><code class='language-R'>text_nowcast(x, y, intercept)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="text_nowcast_+3A_x">x</code></td>
<td>
<p>the input matrix x. It should have 1 observation more that y.</p>
</td></tr>
<tr><td><code id="text_nowcast_+3A_y">y</code></td>
<td>
<p>the response variable</p>
</td></tr>
<tr><td><code id="text_nowcast_+3A_intercept">intercept</code></td>
<td>
<p>TRUE for include intercept in the forecast equation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the nowcast h=0 for the variable y.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
data("stock_data")
data("news_data")
y=as.matrix(stock_data[,2])
w=as.matrix(stock_data[,3])
data("news_data")
data("optimal_factors")
pc=optimal_factors
z=cbind(w,pc)
t=length(y)
ncsts=text_nowcast(z,y[1:(t-1)],TRUE)
</code></pre>

<hr>
<h2 id='tf_idf'>tf-idf function</h2><span id='topic+tf_idf'></span>

<h3>Description</h3>

<p>tf-idf function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tf_idf(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tf_idf_+3A_x">x</code></td>
<td>
<p>a input matrix x of terms counting.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with the terms tf-idf and the terms tf-idf in descending order.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("news_data")
X=as.matrix(news_data[,2:ncol(news_data)])
tf_idf_terms = tf_idf(X)
</code></pre>

<hr>
<h2 id='top_terms'>Top Terms Function</h2><span id='topic+top_terms'></span>

<h3>Description</h3>

<p>Top Terms Function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>top_terms(
  x,
  w,
  y,
  alpha,
  lambda,
  k,
  wordcloud,
  max.words,
  scale,
  rot.per,
  family
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="top_terms_+3A_x">x</code></td>
<td>
<p>the input matrix of terms to be selected.</p>
</td></tr>
<tr><td><code id="top_terms_+3A_w">w</code></td>
<td>
<p>optional argument. the input matrix of structured data to not be selected.</p>
</td></tr>
<tr><td><code id="top_terms_+3A_y">y</code></td>
<td>
<p>the response variable</p>
</td></tr>
<tr><td><code id="top_terms_+3A_alpha">alpha</code></td>
<td>
<p>the glmnet alpha</p>
</td></tr>
<tr><td><code id="top_terms_+3A_lambda">lambda</code></td>
<td>
<p>the glmnet lambda</p>
</td></tr>
<tr><td><code id="top_terms_+3A_k">k</code></td>
<td>
<p>the k top terms</p>
</td></tr>
<tr><td><code id="top_terms_+3A_wordcloud">wordcloud</code></td>
<td>
<p>set TRUE to plot the wordcloud</p>
</td></tr>
<tr><td><code id="top_terms_+3A_max.words">max.words</code></td>
<td>
<p>the maximum number of words in the wordcloud</p>
</td></tr>
<tr><td><code id="top_terms_+3A_scale">scale</code></td>
<td>
<p>the wordcloud size.</p>
</td></tr>
<tr><td><code id="top_terms_+3A_rot.per">rot.per</code></td>
<td>
<p>wordcloud proportion 90 degree terms</p>
</td></tr>
<tr><td><code id="top_terms_+3A_family">family</code></td>
<td>
<p>glmnet family</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the top k terms and the corresponding wordcloud.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
data("stock_data")
data("news_data")
y=as.matrix(stock_data[,2])
w=as.matrix(stock_data[,3])
data("news_data")
X=news_data[,2:ncol(news_data)]
x=as.matrix(X)
grid_alphas=seq(by=0.05,to=0.95,from=0.05)
cont_folds=TRUE
t=length(y)
optimal_alphas=optimal_alphas(x[1:(t-1),],w[1:(t-1),],
y[2:t],grid_alphas,TRUE,"gaussian")
top_trms&lt;- top_terms(x[1:(t-1),],w[1:(t-1),],y[2:t],
optimal_alphas[[1]], optimal_alphas[[2]],10,TRUE,
10,c(2,0.3),.15,"gaussian")


</code></pre>

<hr>
<h2 id='tv_dictionary'>tv dictionary function</h2><span id='topic+tv_dictionary'></span>

<h3>Description</h3>

<p>tv dictionary function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tv_dictionary(x, w, y, alpha, lambda, newx, family)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tv_dictionary_+3A_x">x</code></td>
<td>
<p>A matrix of variables to be selected by shrinkrage methods.</p>
</td></tr>
<tr><td><code id="tv_dictionary_+3A_w">w</code></td>
<td>
<p>Optional Argument. A matrix of variables to be selected by shrinkrage methods.</p>
</td></tr>
<tr><td><code id="tv_dictionary_+3A_y">y</code></td>
<td>
<p>the response variable.</p>
</td></tr>
<tr><td><code id="tv_dictionary_+3A_alpha">alpha</code></td>
<td>
<p>the alpha required in glmnet.</p>
</td></tr>
<tr><td><code id="tv_dictionary_+3A_lambda">lambda</code></td>
<td>
<p>the lambda required in glmnet.</p>
</td></tr>
<tr><td><code id="tv_dictionary_+3A_newx">newx</code></td>
<td>
<p>Matrix  that selection will applied. Useful for time series, when we need the observation at time t.</p>
</td></tr>
<tr><td><code id="tv_dictionary_+3A_family">family</code></td>
<td>
<p>the glmnet family.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>X_star: a list with the coefficients and a sparse matrix with the most predictive terms.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
data("stock_data")
data("news_data")
y=as.matrix(stock_data[1:200,2])
w=as.matrix(stock_data[1:200,3])
data("news_data")
X=news_data[1:200,2:ncol(news_data)]
x=as.matrix(X)
grid_alphas=seq(by=0.5,to=1,from=0.5)
cont_folds=TRUE
t=length(y)
optimal_alphas=optimal_alphas(x[1:(t-1),],w[1:(t-1),],
y[2:t],grid_alphas,TRUE,"gaussian")
x_star=tv_dictionary(x=x[1:(t-1),],w=w[1:(t-1),],y=y[2:t],
alpha=optimal_alphas[1],lambda=optimal_alphas[2],newx=x,family="gaussian")

</code></pre>

<hr>
<h2 id='tv_sentiment_index'>tv sentiment index function</h2><span id='topic+tv_sentiment_index'></span>

<h3>Description</h3>

<p>tv sentiment index function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tv_sentiment_index(x, w, y, alpha, lambda, newx, family, k)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tv_sentiment_index_+3A_x">x</code></td>
<td>
<p>A matrix of variables to be selected by shrinkrage methods.</p>
</td></tr>
<tr><td><code id="tv_sentiment_index_+3A_w">w</code></td>
<td>
<p>Optional Argument. A matrix of variables to be selected by shrinkrage methods.</p>
</td></tr>
<tr><td><code id="tv_sentiment_index_+3A_y">y</code></td>
<td>
<p>the response variable.</p>
</td></tr>
<tr><td><code id="tv_sentiment_index_+3A_alpha">alpha</code></td>
<td>
<p>the alpha required in glmnet.</p>
</td></tr>
<tr><td><code id="tv_sentiment_index_+3A_lambda">lambda</code></td>
<td>
<p>the lambda required in glmnet.</p>
</td></tr>
<tr><td><code id="tv_sentiment_index_+3A_newx">newx</code></td>
<td>
<p>Matrix  that selection will be applied. Useful for time series, when we need the observation at time t.</p>
</td></tr>
<tr><td><code id="tv_sentiment_index_+3A_family">family</code></td>
<td>
<p>the glmnet family.</p>
</td></tr>
<tr><td><code id="tv_sentiment_index_+3A_k">k</code></td>
<td>
<p>the highest positive and negative coefficients to be used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The time-varying sentiment index. The index is based on the word/term counting and is computed using: tv_index=(pos-neg)/(pos+neg).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>suppressWarnings(RNGversion("3.5.0"))
set.seed(1)
data("stock_data")
data("news_data")
y=as.matrix(stock_data[,2])
w=as.matrix(stock_data[,3])
data("news_data")
X=news_data[,2:ncol(news_data)]
x=as.matrix(X)
grid_alphas=0.05
cont_folds=TRUE
t=length(y)
optimal_alphas=optimal_alphas(x[1:(t-1),],w[1:(t-1),],
y[2:t],grid_alphas,TRUE,"gaussian")
tv_index &lt;- tv_sentiment_index(x[1:(t-1),],w[1:(t-1),],y[2:t],
optimal_alphas[[1]],optimal_alphas[[2]],x,"gaussian",2)

</code></pre>

<hr>
<h2 id='tv_sentiment_index_all_coefs'>TV sentiment index using all positive and negative coefficients.</h2><span id='topic+tv_sentiment_index_all_coefs'></span>

<h3>Description</h3>

<p>TV sentiment index using all positive and negative coefficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tv_sentiment_index_all_coefs(
  x,
  w,
  y,
  alpha,
  lambda,
  newx,
  family,
  scaled,
  k_mov_avg,
  type_mov_avg
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tv_sentiment_index_all_coefs_+3A_x">x</code></td>
<td>
<p>A matrix of variables to be selected by shrinkrage methods.</p>
</td></tr>
<tr><td><code id="tv_sentiment_index_all_coefs_+3A_w">w</code></td>
<td>
<p>Optional Argument. A matrix of variables to be selected by shrinkrage methods.</p>
</td></tr>
<tr><td><code id="tv_sentiment_index_all_coefs_+3A_y">y</code></td>
<td>
<p>the response variable.</p>
</td></tr>
<tr><td><code id="tv_sentiment_index_all_coefs_+3A_alpha">alpha</code></td>
<td>
<p>the alpha required in glmnet.</p>
</td></tr>
<tr><td><code id="tv_sentiment_index_all_coefs_+3A_lambda">lambda</code></td>
<td>
<p>the lambda required in glmnet.</p>
</td></tr>
<tr><td><code id="tv_sentiment_index_all_coefs_+3A_newx">newx</code></td>
<td>
<p>Matrix  that selection will be applied. Useful for time series, when we need the observation at time t.</p>
</td></tr>
<tr><td><code id="tv_sentiment_index_all_coefs_+3A_family">family</code></td>
<td>
<p>the glmnet family.</p>
</td></tr>
<tr><td><code id="tv_sentiment_index_all_coefs_+3A_scaled">scaled</code></td>
<td>
<p>Set TRUE for scale and FALSE for no scale.</p>
</td></tr>
<tr><td><code id="tv_sentiment_index_all_coefs_+3A_k_mov_avg">k_mov_avg</code></td>
<td>
<p>The moving average order.</p>
</td></tr>
<tr><td><code id="tv_sentiment_index_all_coefs_+3A_type_mov_avg">type_mov_avg</code></td>
<td>
<p>The type of moving average. See <a href="pracma.html#topic+movavg">movavg</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the net, postive and negative sentiment index. The net time-varying sentiment index. The index is based on the word/term counting and is computed using: tv_index=(pos-neg)/(pos+neg). The postive sentiment index is computed using: tv_index_pos=pos/(pos+neg) and the negative tv_index_neg=neg/(pos+neg).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>suppressWarnings(RNGversion("3.5.0"))
set.seed(1)
data("stock_data")
data("news_data")
y=as.matrix(stock_data[,2])
w=as.matrix(stock_data[,3])
data("news_data")
X=news_data[,2:ncol(news_data)]
x=as.matrix(X)
grid_alphas=0.05
cont_folds=TRUE
t=length(y)
optimal_alphas=optimal_alphas(x=x[1:(t-1),],
                              y=y[2:t],grid_alphas=grid_alphas,cont_folds=TRUE,family="gaussian")
tv_idx=tv_sentiment_index_all_coefs(x=x[1:(t-1),],y=y[2:t],alpha = optimal_alphas[1],
                                 lambda = optimal_alphas[2],newx=x,
                                 scaled = TRUE,k_mov_avg = 4,type_mov_avg = "s")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
