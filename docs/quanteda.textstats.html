<!DOCTYPE html><html><head><title>Help for package quanteda.textstats</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {quanteda.textstats}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#quanteda.textstats-package'><p>quanteda.textstats: Textual Statistics for the Quantitative Analysis of Textual Data</p></a></li>
<li><a href='#as.list.textstat_proxy'><p>textstat_simil/dist coercion methods</p></a></li>
<li><a href='#as.matrix+2Ctextstat_simil_sparse-method'><p>as.matrix method for textstat_simil_sparse</p></a></li>
<li><a href='#check_dots'><p>Check arguments passed to other functions via ...</p></a></li>
<li><a href='#compute_lexdiv_stats'><p>Compute lexical diversity from a dfm or tokens</p></a></li>
<li><a href='#compute_mattr'><p>Compute the Moving-Average Type-Token Ratio (MATTR)</p></a></li>
<li><a href='#compute_msttr'><p>Compute the Mean Segmental Type-Token Ratio (MSTTR)</p></a></li>
<li><a href='#data_char_wordlists'><p>Word lists for readability statistics</p></a></li>
<li><a href='#dfm_split_hyphenated_features'><p>Split a dfm's hyphenated features into constituent parts</p></a></li>
<li><a href='#diag2na'><p>convert same-value pairs to NA in a textstat_proxy object</p></a></li>
<li><a href='#head.textstat_proxy'><p>Return the first or last part of a textstat_proxy object</p></a></li>
<li><a href='#nscrabble'><p>Count the Scrabble letter values of text</p></a></li>
<li><a href='#nsyllable.tokens'><p>nsyllable methods for tokens</p></a></li>
<li><a href='#textstat_collocations'><p>Identify and score multi-word expressions</p></a></li>
<li><a href='#textstat_entropy'><p>Compute entropies of documents or features</p></a></li>
<li><a href='#textstat_frequency'><p>Tabulate feature frequencies</p></a></li>
<li><a href='#textstat_keyness'><p>Calculate keyness statistics</p></a></li>
<li><a href='#textstat_lexdiv'><p>Calculate lexical diversity</p></a></li>
<li><a href='#textstat_proxy'><p>[Experimental] Compute document/feature proximity</p></a></li>
<li><a href='#textstat_proxy-class'><p>textstat_simil/dist classes</p></a></li>
<li><a href='#textstat_readability'><p>Calculate readability</p></a></li>
<li><a href='#textstat_select'><p>Select rows of textstat objects by glob, regex or fixed patterns</p></a></li>
<li><a href='#textstat_simil'><p>Similarity and distance computation between documents or features</p></a></li>
<li><a href='#textstat_summary'><p>Summarize documents as syntactic and lexical feature counts</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.97</td>
</tr>
<tr>
<td>Title:</td>
<td>Textual Statistics for the Quantitative Analysis of Textual Data</td>
</tr>
<tr>
<td>Description:</td>
<td>Textual statistics functions formerly in the 'quanteda' package.
    Textual statistics for characterizing and comparing textual data. Includes 
    functions for measuring term and document frequency, the co-occurrence of 
    words, similarity and distance between features and documents, feature entropy, 
    keyword occurrence, readability, and lexical diversity.  These functions 
    extend the 'quanteda' package and are specially designed for sparse textual data.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>quanteda (&ge; 4.0.0), Matrix (&ge; 1.5-0), methods, nsyllable,
proxyC (&ge; 0.1.4), Rcpp (&ge; 0.12.12), stringi</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo (&ge; 0.7.600.1.0), quanteda</td>
</tr>
<tr>
<td>Suggests:</td>
<td>entropy, ExPosition, proxy, rmarkdown, spelling, svs,
testthat, knitr, covr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://quanteda.io">https://quanteda.io</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/quanteda/quanteda.textstats/issues">https://github.com/quanteda/quanteda.textstats/issues</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Language:</td>
<td>en-GB</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-04-08 10:13:24 UTC; kbenoit</td>
</tr>
<tr>
<td>Author:</td>
<td>Kenneth Benoit <a href="https://orcid.org/0000-0002-0797-564X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [cre, aut, cph],
  Kohei Watanabe <a href="https://orcid.org/0000-0001-6519-5265"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Haiyan Wang <a href="https://orcid.org/0000-0003-4992-4311"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Jiong Wei Lua [aut],
  Jouni Kuha <a href="https://orcid.org/0000-0002-1156-8465"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  European Research Council [fnd] (ERC-2011-StG 283794-QUANTESS)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kenneth Benoit &lt;kbenoit@lse.ac.uk&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-04-08 11:50:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='quanteda.textstats-package'>quanteda.textstats: Textual Statistics for the Quantitative Analysis of Textual Data</h2><span id='topic+quanteda.textstats'></span><span id='topic+quanteda.textstats-package'></span>

<h3>Description</h3>

<p>Textual statistics functions formerly in the 'quanteda' package. Textual statistics for characterizing and comparing textual data. Includes functions for measuring term and document frequency, the co-occurrence of words, similarity and distance between features and documents, feature entropy, keyword occurrence, readability, and lexical diversity. These functions extend the 'quanteda' package and are specially designed for sparse textual data.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Kenneth Benoit <a href="mailto:kbenoit@lse.ac.uk">kbenoit@lse.ac.uk</a> (<a href="https://orcid.org/0000-0002-0797-564X">ORCID</a>) [copyright holder]
</p>
<p>Authors:
</p>

<ul>
<li><p> Kohei Watanabe <a href="mailto:watanabe.kohei@gmail.com">watanabe.kohei@gmail.com</a> (<a href="https://orcid.org/0000-0001-6519-5265">ORCID</a>)
</p>
</li>
<li><p> Haiyan Wang <a href="mailto:whyinsa@yahoo.com">whyinsa@yahoo.com</a> (<a href="https://orcid.org/0000-0003-4992-4311">ORCID</a>)
</p>
</li>
<li><p> Jiong Wei Lua <a href="mailto:J.W.Lua@lse.ac.uk">J.W.Lua@lse.ac.uk</a>
</p>
</li>
<li><p> Jouni Kuha <a href="mailto:j.kuha@lse.ac.uk">j.kuha@lse.ac.uk</a> (<a href="https://orcid.org/0000-0002-1156-8465">ORCID</a>)
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> European Research Council (ERC-2011-StG 283794-QUANTESS) [funder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://quanteda.io">https://quanteda.io</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/quanteda/quanteda.textstats/issues">https://github.com/quanteda/quanteda.textstats/issues</a>
</p>
</li></ul>


<hr>
<h2 id='as.list.textstat_proxy'>textstat_simil/dist coercion methods</h2><span id='topic+as.list.textstat_proxy'></span><span id='topic+as.data.frame.textstat_proxy'></span>

<h3>Description</h3>

<p>Coercion methods for objects created by <code><a href="#topic+textstat_simil">textstat_simil()</a></code> and
<code><a href="#topic+textstat_dist">textstat_dist()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'textstat_proxy'
as.list(x, sorted = TRUE, n = NULL, diag = FALSE, ...)

## S3 method for class 'textstat_proxy'
as.data.frame(
  x,
  row.names = NULL,
  optional = FALSE,
  diag = FALSE,
  upper = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.list.textstat_proxy_+3A_x">x</code></td>
<td>
<p>any <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
<tr><td><code id="as.list.textstat_proxy_+3A_sorted">sorted</code></td>
<td>
<p>sort results in descending order if <code>TRUE</code></p>
</td></tr>
<tr><td><code id="as.list.textstat_proxy_+3A_n">n</code></td>
<td>
<p>the top <code>n</code> highest-ranking items will be returned.  If n is
<code>NULL</code>, return all items.</p>
</td></tr>
<tr><td><code id="as.list.textstat_proxy_+3A_diag">diag</code></td>
<td>
<p>logical; if <code>FALSE</code>, exclude the item's comparison with itself</p>
</td></tr>
<tr><td><code id="as.list.textstat_proxy_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed to or from methods.</p>
</td></tr>
<tr><td><code id="as.list.textstat_proxy_+3A_row.names">row.names</code></td>
<td>
<p><code>NULL</code> or a character vector giving the row
names for the data frame.  Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="as.list.textstat_proxy_+3A_optional">optional</code></td>
<td>
<p>logical. If <code>TRUE</code>, setting row names and
converting column names (to syntactic names: see
<code><a href="base.html#topic+make.names">make.names</a></code>) is optional.  Note that all of <span class="rlang"><b>R</b></span>'s
<span class="pkg">base</span> package <code>as.data.frame()</code> methods use
<code>optional</code> only for column names treatment, basically with the
meaning of <code><a href="base.html#topic+data.frame">data.frame</a>(*, check.names = !optional)</code>.
See also the <code>make.names</code> argument of the <code>matrix</code> method.</p>
</td></tr>
<tr><td><code id="as.list.textstat_proxy_+3A_upper">upper</code></td>
<td>
<p>logical; if <code>TRUE</code>, return pairs as both (A, B) and (B, A)</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>as.data.list</code> for a <code>textstat_simil</code> or
<code>textstat_dist</code> object returns a list equal in length to the columns of the
simil or dist object, with the rows and their values as named  elements.  By default,
this list excludes same-time pairs (when <code>diag = FALSE</code>) and sorts the values
in descending order (when <code>sorted = TRUE</code>).
</p>
<p><code>as.data.frame</code> for a <code>textstat_simil</code> or
<code>textstat_dist</code> object returns a data.frame of pairwise combinations
and the and their similarity or distance value.
</p>

<hr>
<h2 id='as.matrix+2Ctextstat_simil_sparse-method'>as.matrix method for textstat_simil_sparse</h2><span id='topic+as.matrix+2Ctextstat_simil_sparse-method'></span><span id='topic+as.matrix+2Ctextstat_simil_symm_sparse-method'></span>

<h3>Description</h3>

<p>as.matrix method for textstat_simil_sparse
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'textstat_simil_sparse'
as.matrix(x, omitted = NA, ...)

## S4 method for signature 'textstat_simil_symm_sparse'
as.matrix(x, omitted = NA, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.matrix+2B2Ctextstat_simil_sparse-method_+3A_x">x</code></td>
<td>
<p>an object returned by <a href="#topic+textstat_simil">textstat_simil</a> when <code>min_simil &gt; 0</code></p>
</td></tr>
<tr><td><code id="as.matrix+2B2Ctextstat_simil_sparse-method_+3A_omitted">omitted</code></td>
<td>
<p>value that will replace the omitted cells</p>
</td></tr>
<tr><td><code id="as.matrix+2B2Ctextstat_simil_sparse-method_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <a href="base.html#topic+matrix">matrix</a> object
</p>

<hr>
<h2 id='check_dots'>Check arguments passed to other functions via ...</h2><span id='topic+check_dots'></span>

<h3>Description</h3>

<p>Check arguments passed to other functions via ...
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_dots(..., method = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_dots_+3A_...">...</code></td>
<td>
<p>dots to check</p>
</td></tr>
<tr><td><code id="check_dots_+3A_method">method</code></td>
<td>
<p>the names of functions <code>...</code> is passed to</p>
</td></tr>
</table>

<hr>
<h2 id='compute_lexdiv_stats'>Compute lexical diversity from a dfm or tokens</h2><span id='topic+compute_lexdiv_stats'></span><span id='topic+compute_lexdiv_dfm_stats'></span><span id='topic+compute_lexdiv_tokens_stats'></span>

<h3>Description</h3>

<p>Internal functions used in <code><a href="#topic+textstat_lexdiv">textstat_lexdiv()</a></code>, for computing
lexical diversity measures on dfms or tokens objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_lexdiv_dfm_stats(x, measure = NULL, log.base = 10)

compute_lexdiv_tokens_stats(
  x,
  measure = c("MATTR", "MSTTR"),
  MATTR_window,
  MSTTR_segment
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_lexdiv_stats_+3A_x">x</code></td>
<td>
<p>a <a href="quanteda.html#topic+dfm">dfm</a> object</p>
</td></tr>
<tr><td><code id="compute_lexdiv_stats_+3A_measure">measure</code></td>
<td>
<p>a list of lexical diversity measures.</p>
</td></tr>
<tr><td><code id="compute_lexdiv_stats_+3A_log.base">log.base</code></td>
<td>
<p>a numeric value defining the base of the logarithm (for
measures using logs)</p>
</td></tr>
<tr><td><code id="compute_lexdiv_stats_+3A_mattr_window">MATTR_window</code></td>
<td>
<p>a numeric value defining the size of the moving window
for computation of the Moving-Average Type-Token Ratio (Covington &amp; McFall, 2010)</p>
</td></tr>
<tr><td><code id="compute_lexdiv_stats_+3A_msttr_segment">MSTTR_segment</code></td>
<td>
<p>a numeric value defining the size of the each segment
for the computation of the the Mean Segmental Type-Token Ratio (Johnson, 1944)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>compute_lexdiv_dfm_stats</code> in an internal function that
computes the lexical diversity measures from a <a href="quanteda.html#topic+dfm">dfm</a> input.
</p>
<p><code>compute_lexdiv_tokens_stats</code> in an internal function that
computes the lexical diversity measures from a <a href="quanteda.html#topic+dfm">dfm</a> input.
</p>


<h3>Value</h3>

<p>a <code>data.frame</code> with a <code>document</code> column containing the
input document name, followed by columns with the lexical diversity
statistic, in the order in which they were supplied as the <code>measure</code>
argument.
</p>

<hr>
<h2 id='compute_mattr'>Compute the Moving-Average Type-Token Ratio (MATTR)</h2><span id='topic+compute_mattr'></span>

<h3>Description</h3>

<p>From a tokens object, computes the Moving-Average Type-Token Ratio (MATTR)
from Covington &amp; McFall (2010), averaging all of the sequential moving
windows of tokens of size <code>MATTR_window</code> across the text, returning the
average as the MATTR.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_mattr(x, MATTR_window = 100L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_mattr_+3A_x">x</code></td>
<td>
<p>a <a href="quanteda.html#topic+tokens">tokens</a> object</p>
</td></tr>
<tr><td><code id="compute_mattr_+3A_mattr_window">MATTR_window</code></td>
<td>
<p>integer; the size of the moving window for computation of
TTR, between 1 and the number of tokens of the document</p>
</td></tr>
</table>

<hr>
<h2 id='compute_msttr'>Compute the Mean Segmental Type-Token Ratio (MSTTR)</h2><span id='topic+compute_msttr'></span>

<h3>Description</h3>

<p>Compute the Mean Segmental Type-Token Ratio (Johnson 1944) for a tokens input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_msttr(x, MSTTR_segment)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_msttr_+3A_x">x</code></td>
<td>
<p>input <a href="quanteda.html#topic+tokens">tokens</a></p>
</td></tr>
<tr><td><code id="compute_msttr_+3A_msttr_segment">MSTTR_segment</code></td>
<td>
<p>a numeric value defining the size of the each segment
for the computation of the the Mean Segmental Type-Token Ratio (Johnson, 1944)</p>
</td></tr>
</table>

<hr>
<h2 id='data_char_wordlists'>Word lists for readability statistics</h2><span id='topic+data_char_wordlists'></span>

<h3>Description</h3>

<p><code>data_char_wordlists</code> provides word lists used in some readability indexes;
it is a named list of character vectors where each list element
corresponds to a different readability index.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_char_wordlists
</code></pre>


<h3>Format</h3>

<p>A list of length two:
</p>

<dl>
<dt><code>DaleChall</code></dt><dd><p>The long Dale-Chall list of 3,000 familiar (English)
words needed to compute the Dale-Chall Readability Formula.</p>
</dd>
<dt><code>Spache</code></dt><dd><p>The revised Spache word list (see Klare 1975, 73; Spache
1974) needed to compute the Spache Revised Formula of readability (Spache
1953).</p>
</dd>
</dl>



<h3>References</h3>

<p>Chall, J.S., &amp; Dale, E. (1995). <em>Readability Revisited: The New
Dale-Chall Readability Formula</em>. Brookline Books.
</p>
<p>Dale, E. &amp; Chall, J.S. (1948). A Formula for Predicting
Readability. <em>Educational Research Bulletin</em>, 27(1): 11&ndash;20.
</p>
<p>Dale, E. &amp; Chall, J.S. (1948). A Formula for Predicting Readability:
Instructions. <em>Educational Research Bulletin</em>, 27(2): 37&ndash;54.
</p>
<p>Klare, G.R. (1975). Assessing Readability. <em>Reading Research Quarterly</em>
10(1), 62&ndash;102.
</p>
<p>Spache, G. (1953). A New Readability Formula for Primary-Grade Reading
Materials. <em>The Elementary School Journal</em>, 53, 410&ndash;413.
</p>
<p>Spache, G. (1974).  <em>Good reading for poor readers</em>. (Rvd. 9th Ed.)
Champaign, Illinois: Garrard, 1974.
</p>

<hr>
<h2 id='dfm_split_hyphenated_features'>Split a dfm's hyphenated features into constituent parts</h2><span id='topic+dfm_split_hyphenated_features'></span>

<h3>Description</h3>

<p>Takes a dfm that contains features with hyphenated words, such as
&quot;split-second&quot; and turns them into features that split the elements
in the same was as <code>tokens(x, remove_hyphens = TRUE)</code> would have done.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfm_split_hyphenated_features(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfm_split_hyphenated_features_+3A_x">x</code></td>
<td>
<p>input <a href="quanteda.html#topic+dfm">dfm</a></p>
</td></tr>
</table>

<hr>
<h2 id='diag2na'>convert same-value pairs to NA in a textstat_proxy object</h2><span id='topic+diag2na'></span>

<h3>Description</h3>

<p>Converts the diagonal, or the same-pair equivalent in an object
where the columns have been selected, to NA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diag2na(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="diag2na_+3A_x">x</code></td>
<td>
<p>the return from <code><a href="#topic+textstat_simil">textstat_simil()</a></code> or <code><a href="#topic+textstat_dist">textstat_dist()</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>sparse Matrix format with same-pair values replaced with <code>NA</code>
</p>

<hr>
<h2 id='head.textstat_proxy'>Return the first or last part of a textstat_proxy object</h2><span id='topic+head.textstat_proxy'></span><span id='topic+tail.textstat_proxy'></span>

<h3>Description</h3>

<p>For a similarity or distance object computed via <a href="#topic+textstat_simil">textstat_simil</a> or
<a href="#topic+textstat_dist">textstat_dist</a>, returns the first or last <code>n</code> rows.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'textstat_proxy'
head(x, n = 6L, ...)

## S3 method for class 'textstat_proxy'
tail(x, n = 6L, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="head.textstat_proxy_+3A_x">x</code></td>
<td>
<p>a textstat_simil/textstat_dist object</p>
</td></tr>
<tr><td><code id="head.textstat_proxy_+3A_n">n</code></td>
<td>
<p>a single, positive integer.  If positive, size for the resulting
object: number of first/last documents for the dfm. If negative, all but
the n last/first number of documents of x.</p>
</td></tr>
<tr><td><code id="head.textstat_proxy_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="base.html#topic+matrix">matrix</a> corresponding to the subset defined
by <code>n</code>.
</p>

<hr>
<h2 id='nscrabble'>Count the Scrabble letter values of text</h2><span id='topic+nscrabble'></span>

<h3>Description</h3>

<p>Tally the Scrabble letter values of text given a user-supplied function, such
as the sum (default) or mean of the character values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nscrabble(x, FUN = sum)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nscrabble_+3A_x">x</code></td>
<td>
<p>a character vector</p>
</td></tr>
<tr><td><code id="nscrabble_+3A_fun">FUN</code></td>
<td>
<p>function to be applied to the character values in the text;
default is <code>sum</code>, but could also be <code>mean</code> or a user-supplied
function.  Missing values are automatically removed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a (named) integer vector of Scrabble letter values, computed using
<code>FUN</code>, corresponding to the input text(s)
</p>


<h3>Note</h3>

<p>Character values are only defined for non-accented Latin a-z, A-Z
letters.  Lower-casing is unnecessary.
</p>
<p>We would be happy to add more languages to this <em>extremely useful
function</em> if you send us the values for your language!
</p>


<h3>Author(s)</h3>

<p>Kenneth Benoit
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nscrabble(c("muzjiks", "excellency"))
nscrabble(quanteda::data_corpus_inaugural[1:5], mean)
</code></pre>

<hr>
<h2 id='nsyllable.tokens'>nsyllable methods for tokens</h2><span id='topic+nsyllable.tokens'></span>

<h3>Description</h3>

<p>Extends <code>nsyllable()</code> methods for <a href="quanteda.html#topic+tokens">tokens</a> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tokens'
nsyllable(
  x,
  language = "en",
  syllable_dictionary = nsyllable::data_syllables_en,
  use.names = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nsyllable.tokens_+3A_x">x</code></td>
<td>
<p>character vector whose
syllables will be counted.  This will count all syllables in a character
vector without regard to separating tokens, so it is recommended that x be
individual terms.</p>
</td></tr>
<tr><td><code id="nsyllable.tokens_+3A_language">language</code></td>
<td>
<p>specify the language for syllable counts by <a href="https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes">ISO 639-1</a> code. The
default is English, using the data object <code><a href="nsyllable.html#topic+data_syllables_en">data_syllables_en</a></code>, an English
pronunciation dictionary from CMU.</p>
</td></tr>
<tr><td><code id="nsyllable.tokens_+3A_syllable_dictionary">syllable_dictionary</code></td>
<td>
<p>optional named integer vector of syllable counts
where the names are lower case tokens.  This can be used to override the
language setting, when set to <code>NULL</code> (the default).  If a syllable
dictionary is supplied, this will override the <code>language</code> argument.</p>
</td></tr>
<tr><td><code id="nsyllable.tokens_+3A_use.names">use.names</code></td>
<td>
<p>logical; if <code>TRUE</code>, assign the tokens as the names of the
syllable count vector</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
</code></pre>

<hr>
<h2 id='textstat_collocations'>Identify and score multi-word expressions</h2><span id='topic+textstat_collocations'></span>

<h3>Description</h3>

<p>Identify and score multi-word expressions, or adjacent fixed-length
collocations, from text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textstat_collocations(
  x,
  method = "lambda",
  size = 2,
  min_count = 2,
  smoothing = 0.5,
  tolower = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textstat_collocations_+3A_x">x</code></td>
<td>
<p>a character, <a href="quanteda.html#topic+corpus">corpus</a>, or <a href="quanteda.html#topic+tokens">tokens</a> object whose collocations will be
scored.  The tokens object should include punctuation, and if any words
have been removed, these should have been removed with <code>padding = TRUE</code>.
While identifying collocations for tokens objects is supported, you will
get better results with character or corpus objects due to relatively
imperfect detection of sentence boundaries from texts already tokenized.</p>
</td></tr>
<tr><td><code id="textstat_collocations_+3A_method">method</code></td>
<td>
<p>association measure for detecting collocations. Currently this
is limited to <code>"lambda"</code>.  See Details.</p>
</td></tr>
<tr><td><code id="textstat_collocations_+3A_size">size</code></td>
<td>
<p>integer; the length of the collocations
to be scored</p>
</td></tr>
<tr><td><code id="textstat_collocations_+3A_min_count">min_count</code></td>
<td>
<p>numeric; minimum frequency of collocations that will be
scored</p>
</td></tr>
<tr><td><code id="textstat_collocations_+3A_smoothing">smoothing</code></td>
<td>
<p>numeric; a smoothing parameter added to the observed counts
(default is 0.5)</p>
</td></tr>
<tr><td><code id="textstat_collocations_+3A_tolower">tolower</code></td>
<td>
<p>logical; if <code>TRUE</code>, form collocations as lower-cased
combinations</p>
</td></tr>
<tr><td><code id="textstat_collocations_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="quanteda.html#topic+tokens">tokens()</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Documents are grouped for the purposes of scoring, but collocations will not
span sentences. If <code>x</code> is a <a href="quanteda.html#topic+tokens">tokens</a> object and some tokens have been
removed, this should be done using <code style="white-space: pre;">&#8288;[tokens_remove](x, pattern, padding = TRUE)&#8288;</code> so that counts will still be accurate, but the pads will prevent those
collocations from being scored.
</p>
<p>The <code>lambda</code> computed for a size = <code class="reqn">K</code>-word target multi-word expression
the coefficient for the  <code class="reqn">K</code>-way interaction parameter in the saturated
log-linear model fitted to the counts of the terms forming the set of
eligible multi-word expressions. This is the same as the &quot;lambda&quot; computed in
Blaheta and Johnson's (2001), where all multi-word expressions are considered
(rather than just verbs, as in that paper). The <code>z</code> is the Wald
<code class="reqn">z</code>-statistic computed as the quotient of <code>lambda</code> and the Wald statistic
for <code>lambda</code> as described below.
</p>
<p>In detail:
</p>
<p>Consider a <code class="reqn">K</code>-word target expression <code class="reqn">x</code>, and let <code class="reqn">z</code> be any
<code class="reqn">K</code>-word expression. Define a comparison function <code class="reqn">c(x,z)=(j_{1},
\dots, j_{K})=c</code> such that the <code class="reqn">k</code>th element of <code class="reqn">c</code> is 1 if the
<code class="reqn">k</code>th word in <code class="reqn">z</code> is equal to the <code class="reqn">k</code>th word in <code class="reqn">x</code>, and 0
otherwise. Let <code class="reqn">c_{i}=(j_{i1}, \dots, j_{iK})</code>, <code class="reqn">i=1, \dots,
2^{K}=M</code>, be the possible values of <code class="reqn">c(x,z)</code>, with <code class="reqn">c_{M}=(1,1,
\dots, 1)</code>. Consider the set of <code class="reqn">c(x,z_{r})</code> across all expressions
<code class="reqn">z_{r}</code> in a corpus of text, and let <code class="reqn">n_{i}</code>, for <code class="reqn">i=1,\dots,M</code>,
denote the number of the <code class="reqn">c(x,z_{r})</code> which equal <code class="reqn">c_{i}</code>, plus the
smoothing constant <code>smoothing</code>. The <code class="reqn">n_{i}</code> are the counts in a
<code class="reqn">2^{K}</code> contingency table whose dimensions are defined by the
<code class="reqn">c_{i}</code>.
</p>
<p><code class="reqn">\lambda</code>: The <code class="reqn">K</code>-way interaction parameter in the saturated
loglinear model fitted to the <code class="reqn">n_{i}</code>. It can be calculated as
</p>
<p style="text-align: center;"><code class="reqn">\lambda  = \sum_{i=1}^{M} (-1)^{K-b_{i}} * log n_{i}</code>
</p>

<p>where <code class="reqn">b_{i}</code> is the number of the elements of <code class="reqn">c_{i}</code> which are
equal to 1.
</p>
<p>Wald test <code class="reqn">z</code>-statistic <code class="reqn">z</code> is calculated as:
</p>
<p style="text-align: center;"><code class="reqn">z = \frac{\lambda}{[\sum_{i=1}^{M} n_{i}^{-1}]^{(1/2)}}</code>
</p>



<h3>Value</h3>

<p><code>textstat_collocations</code> returns a data.frame of collocations and
their scores and statistics. This consists of the collocations, their
counts, length, and <code class="reqn">\lambda</code> and <code class="reqn">z</code> statistics.  When <code>size</code> is a
vector, then <code>count_nested</code> counts the lower-order collocations that occur
within a higher-order collocation (but this does not affect the
statistics).
</p>


<h3>Author(s)</h3>

<p>Kenneth Benoit, Jouni Kuha, Haiyan Wang, and Kohei Watanabe
</p>


<h3>References</h3>

<p>Blaheta, D. &amp; Johnson, M. (2001). <a href="http://web.science.mq.edu.au/~mjohnson/papers/2001/dpb-colloc01.pdf">Unsupervised learning of multi-word verbs</a>.
Presented at the ACLEACL Workshop on the Computational Extraction, Analysis
and Exploitation of Collocations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("quanteda")
corp &lt;- data_corpus_inaugural[1:2]
head(cols &lt;- textstat_collocations(corp, size = 2, min_count = 2), 10)
head(cols &lt;- textstat_collocations(corp, size = 3, min_count = 2), 10)

# extracting multi-part proper nouns (capitalized terms)
toks1 &lt;- tokens(data_corpus_inaugural)
toks2 &lt;- tokens_remove(toks1, pattern = stopwords("english"), padding = TRUE)
toks3 &lt;- tokens_select(toks2, pattern = "^([A-Z][a-z\\-]{2,})", valuetype = "regex",
                       case_insensitive = FALSE, padding = TRUE)
tstat &lt;- textstat_collocations(toks3, size = 3, tolower = FALSE)
head(tstat, 10)

# vectorized size
txt &lt;- c(". . . . a b c . . a b c . . . c d e",
         "a b . . a b . . a b . . a b . a b",
         "b c d . . b c . b c . . . b c")
textstat_collocations(txt, size = 2:3)

# compounding tokens from collocations
toks &lt;- tokens("This is the European Union.")
colls &lt;- tokens("The new European Union is not the old European Union.") %&gt;%
    textstat_collocations(size = 2, min_count = 1, tolower = FALSE)
colls
tokens_compound(toks, colls, case_insensitive = FALSE)

#' # from a collocations object
(coll &lt;- textstat_collocations(tokens("a b c a b d e b d a b")))
phrase(coll)
</code></pre>

<hr>
<h2 id='textstat_entropy'>Compute entropies of documents or features</h2><span id='topic+textstat_entropy'></span>

<h3>Description</h3>

<p>Compute entropies of documents or features
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textstat_entropy(x, margin = c("documents", "features"), base = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textstat_entropy_+3A_x">x</code></td>
<td>
<p>a <code>dfm</code></p>
</td></tr>
<tr><td><code id="textstat_entropy_+3A_margin">margin</code></td>
<td>
<p>character indicating for which margin to compute entropy</p>
</td></tr>
<tr><td><code id="textstat_entropy_+3A_base">base</code></td>
<td>
<p>base for logarithm function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame of entropies for the given document or feature
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("quanteda")
textstat_entropy(data_dfm_lbgexample)
textstat_entropy(data_dfm_lbgexample, "features")
</code></pre>

<hr>
<h2 id='textstat_frequency'>Tabulate feature frequencies</h2><span id='topic+textstat_frequency'></span>

<h3>Description</h3>

<p>Produces counts and document frequencies summaries of the features in a
<a href="quanteda.html#topic+dfm">dfm</a>, optionally grouped by a <a href="quanteda.html#topic+docvars">docvars</a> variable or other supplied
grouping variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textstat_frequency(
  x,
  n = NULL,
  groups = NULL,
  ties_method = c("min", "average", "first", "random", "max", "dense"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textstat_frequency_+3A_x">x</code></td>
<td>
<p>a <a href="quanteda.html#topic+dfm">dfm</a> object</p>
</td></tr>
<tr><td><code id="textstat_frequency_+3A_n">n</code></td>
<td>
<p>(optional) integer specifying the top <code>n</code> features to be returned,
within group if <code>groups</code> is specified</p>
</td></tr>
<tr><td><code id="textstat_frequency_+3A_groups">groups</code></td>
<td>
<p>grouping variable for sampling, equal in length to the number
of documents. This will be evaluated in the docvars data.frame, so that
docvars may be referred to by name without quoting. This also changes
previous behaviours for <code>groups</code>. See <code>news(Version &gt;= "3.0", package = "quanteda")</code> for details.</p>
</td></tr>
<tr><td><code id="textstat_frequency_+3A_ties_method">ties_method</code></td>
<td>
<p>character string specifying how ties are treated.  See
<code><a href="base.html#topic+rank">base::rank()</a></code> for details.  Unlike that function, however, the default is
<code>"min"</code>, so that frequencies of 10, 10, 11 would be ranked 1, 1, 3.</p>
</td></tr>
<tr><td><code id="textstat_frequency_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="quanteda.html#topic+dfm_group">dfm_group()</a></code>.  This can
be useful in passing <code>force = TRUE</code>, for instance, if you are grouping a
dfm that has been weighted.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame containing the following variables:
</p>

<dl>
<dt><code>feature</code></dt><dd><p>(character) the feature</p>
</dd>
<dt><code>frequency</code></dt><dd><p>count of the feature</p>
</dd>
<dt><code>rank</code></dt><dd><p>rank of the feature, where 1 indicates the greatest
frequency</p>
</dd>
<dt><code>docfreq</code></dt><dd><p>document frequency of the feature, as a count (the
number of documents in which this feature occurred at least once)</p>
</dd>
<dt><code>docfreq</code></dt><dd><p>document frequency of the feature, as a count</p>
</dd>
<dt><code>group</code></dt><dd><p>(only if <code>groups</code> is specified) the label of the group.
If the features have been grouped, then all counts, ranks, and document
frequencies are within group.  If groups is not specified, the <code>group</code>
column is omitted from the returned data.frame.</p>
</dd>
</dl>

<p><code>textstat_frequency</code> returns a data.frame of features and
their term and document frequencies within groups.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("quanteda")
set.seed(20)
dfmat1 &lt;- dfm(tokens(c("a a b b c d", "a d d d", "a a a")))

textstat_frequency(dfmat1)
textstat_frequency(dfmat1, groups = c("one", "two", "one"), ties_method = "first")
textstat_frequency(dfmat1, groups = c("one", "two", "one"), ties_method = "average")

dfmat2 &lt;- corpus_subset(data_corpus_inaugural, President == "Obama") %&gt;%
   tokens(remove_punct = TRUE) %&gt;%
   tokens_remove(stopwords("en")) %&gt;%
   dfm()
tstat1 &lt;- textstat_frequency(dfmat2)
head(tstat1, 10)

dfmat3 &lt;- head(data_corpus_inaugural) %&gt;%
   tokens(remove_punct = TRUE) %&gt;%
   tokens_remove(stopwords("en")) %&gt;%
   dfm()
textstat_frequency(dfmat3, n = 2, groups = President)


## Not run: 
# plot 20 most frequent words
library("ggplot2")
ggplot(tstat1[1:20, ], aes(x = reorder(feature, frequency), y = frequency)) +
    geom_point() +
    coord_flip() +
    labs(x = NULL, y = "Frequency")

# plot relative frequencies by group
dfmat3 &lt;- data_corpus_inaugural %&gt;%
    corpus_subset(Year &gt; 2000) %&gt;%
    tokens(remove_punct = TRUE) %&gt;%
    tokens_remove(stopwords("en")) %&gt;%
    dfm() %&gt;%
    dfm_group(groups = President) %&gt;%
    dfm_weight(scheme = "prop")

# calculate relative frequency by president
tstat2 &lt;- textstat_frequency(dfmat3, n = 10, groups = President)

# plot frequencies
ggplot(data = tstat2, aes(x = factor(nrow(tstat2):1), y = frequency)) +
    geom_point() +
    facet_wrap(~ group, scales = "free") +
    coord_flip() +
    scale_x_discrete(breaks = nrow(tstat2):1,
                       labels = tstat2$feature) +
    labs(x = NULL, y = "Relative frequency")

## End(Not run)
</code></pre>

<hr>
<h2 id='textstat_keyness'>Calculate keyness statistics</h2><span id='topic+textstat_keyness'></span>

<h3>Description</h3>

<p>Calculate &quot;keyness&quot;, a score for features that occur differentially across
different categories.  Here, the categories are defined by reference to a
&quot;target&quot; document index in the <a href="quanteda.html#topic+dfm">dfm</a>, with the reference group
consisting of all other documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textstat_keyness(
  x,
  target = 1L,
  measure = c("chi2", "exact", "lr", "pmi"),
  sort = TRUE,
  correction = c("default", "yates", "williams", "none"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textstat_keyness_+3A_x">x</code></td>
<td>
<p>a <a href="quanteda.html#topic+dfm">dfm</a> containing the features to be examined for keyness</p>
</td></tr>
<tr><td><code id="textstat_keyness_+3A_target">target</code></td>
<td>
<p>the document index (numeric, character or logical) identifying
the document forming the &quot;target&quot; for computing keyness; all other
documents' feature frequencies will be combined for use as a reference</p>
</td></tr>
<tr><td><code id="textstat_keyness_+3A_measure">measure</code></td>
<td>
<p>(signed) association measure to be used for computing keyness.
Currently available: <code>"chi2"</code>; <code>"exact"</code> (Fisher's exact test); <code>"lr"</code> for
the likelihood ratio; <code>"pmi"</code> for pointwise mutual information.  Note that
the &quot;exact&quot; test is very computationally intensive and therefore much
slower than the other methods.</p>
</td></tr>
<tr><td><code id="textstat_keyness_+3A_sort">sort</code></td>
<td>
<p>logical; if <code>TRUE</code> sort features scored in descending order
of the measure, otherwise leave in original feature order</p>
</td></tr>
<tr><td><code id="textstat_keyness_+3A_correction">correction</code></td>
<td>
<p>if <code>"default"</code>, Yates correction is applied to
<code>"chi2"</code>; William's correction is applied to <code>"lr"</code>; and no
correction is applied for the <code>"exact"</code> and <code>"pmi"</code> measures.
Specifying a value other than the default can be used to override the
defaults, for instance to apply the Williams correction to the chi2
measure.  Specifying a correction for the <code>"exact"</code> and <code>"pmi"</code>
measures has no effect and produces a warning.</p>
</td></tr>
<tr><td><code id="textstat_keyness_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame of computed statistics and associated p-values, where
the features scored name each row, and the number of occurrences for both
the target and reference groups. For <code>measure = "chi2"</code> this is the
chi-squared value, signed positively if the observed value in the target
exceeds its expected value; for <code>measure = "exact"</code> this is the
estimate of the odds ratio; for <code>measure = "lr"</code> this is the
likelihood ratio <code class="reqn">G2</code> statistic; for <code>"pmi"</code> this is the pointwise
mutual information statistics.
</p>
<p><code>textstat_keyness</code> returns a data.frame of features and
their keyness scores and frequency counts.
</p>


<h3>References</h3>

<p>Bondi, M. &amp; Scott, M. (eds) (2010). <em>Keyness in
Texts</em>. Amsterdam, Philadelphia: John Benjamins.
</p>
<p>Stubbs, M. (2010). Three Concepts of Keywords. In <em>Keyness in
Texts</em>, Bondi, M. &amp; Scott, M. (eds): 1&ndash;42. Amsterdam, Philadelphia:
John Benjamins.
</p>
<p>Scott, M. &amp; Tribble, C. (2006). <em>Textual Patterns: Keyword and Corpus
Analysis in Language Education</em>. Amsterdam: Benjamins: 55.
</p>
<p>Dunning, T. (1993). <a href="https://dl.acm.org/doi/10.5555/972450.972454">Accurate Methods for the Statistics of Surprise and Coincidence</a>. <em>Computational
Linguistics</em>, 19(1): 61&ndash;74.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("quanteda")

# compare pre- v. post-war terms using grouping
period &lt;- ifelse(docvars(data_corpus_inaugural, "Year") &lt; 1945, "pre-war", "post-war")
dfmat1 &lt;- tokens(data_corpus_inaugural) %&gt;%
    dfm() %&gt;%
    dfm_group(groups = period)
head(dfmat1) # make sure 'post-war' is in the first row
head(tstat1 &lt;- textstat_keyness(dfmat1), 10)
tail(tstat1, 10)

# compare pre- v. post-war terms using logical vector
dfmat2 &lt;- dfm(tokens(data_corpus_inaugural))
head(textstat_keyness(dfmat2, docvars(data_corpus_inaugural, "Year") &gt;= 1945), 10)

# compare Trump 2017 to other post-war preseidents
dfmat3 &lt;- dfm(tokens(corpus_subset(data_corpus_inaugural, period == "post-war")))
head(textstat_keyness(dfmat3, target = "2017-Trump"), 10)

# using the likelihood ratio method
head(textstat_keyness(dfm_smooth(dfmat3), measure = "lr", target = "2017-Trump"), 10)
</code></pre>

<hr>
<h2 id='textstat_lexdiv'>Calculate lexical diversity</h2><span id='topic+textstat_lexdiv'></span>

<h3>Description</h3>

<p>Calculate the lexical diversity of text(s).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textstat_lexdiv(
  x,
  measure = c("TTR", "C", "R", "CTTR", "U", "S", "K", "I", "D", "Vm", "Maas", "MATTR",
    "MSTTR", "all"),
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_hyphens = FALSE,
  log.base = 10,
  MATTR_window = 100L,
  MSTTR_segment = 100L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textstat_lexdiv_+3A_x">x</code></td>
<td>
<p>an <a href="quanteda.html#topic+dfm">dfm</a> or <a href="quanteda.html#topic+tokens">tokens</a> input object for whose documents
lexical diversity will be computed</p>
</td></tr>
<tr><td><code id="textstat_lexdiv_+3A_measure">measure</code></td>
<td>
<p>a character vector defining the measure to compute</p>
</td></tr>
<tr><td><code id="textstat_lexdiv_+3A_remove_numbers">remove_numbers</code></td>
<td>
<p>logical; if <code>TRUE</code> remove features or tokens that
consist only of numerals (the Unicode &quot;Number&quot; <code style="white-space: pre;">&#8288;[N]&#8288;</code> class)</p>
</td></tr>
<tr><td><code id="textstat_lexdiv_+3A_remove_punct">remove_punct</code></td>
<td>
<p>logical; if <code>TRUE</code> remove all features or tokens
that consist only of the Unicode &quot;Punctuation&quot; <code style="white-space: pre;">&#8288;[P]&#8288;</code> class)</p>
</td></tr>
<tr><td><code id="textstat_lexdiv_+3A_remove_symbols">remove_symbols</code></td>
<td>
<p>logical; if <code>TRUE</code> remove all features or tokens
that consist only of the Unicode &quot;Punctuation&quot; <code style="white-space: pre;">&#8288;[S]&#8288;</code> class)</p>
</td></tr>
<tr><td><code id="textstat_lexdiv_+3A_remove_hyphens">remove_hyphens</code></td>
<td>
<p>logical; if <code>TRUE</code> split words that are connected
by hyphenation and hyphenation-like characters in between words, e.g.
&quot;self-storage&quot; becomes two features or tokens &quot;self&quot; and &quot;storage&quot;. Default
is FALSE to preserve such words as is, with the hyphens.</p>
</td></tr>
<tr><td><code id="textstat_lexdiv_+3A_log.base">log.base</code></td>
<td>
<p>a numeric value defining the base of the logarithm (for
measures using logarithms)</p>
</td></tr>
<tr><td><code id="textstat_lexdiv_+3A_mattr_window">MATTR_window</code></td>
<td>
<p>a numeric value defining the size of the moving window
for computation of the Moving-Average Type-Token Ratio (Covington &amp; McFall, 2010)</p>
</td></tr>
<tr><td><code id="textstat_lexdiv_+3A_msttr_segment">MSTTR_segment</code></td>
<td>
<p>a numeric value defining the size of the each segment
for the computation of the the Mean Segmental Type-Token Ratio (Johnson, 1944)</p>
</td></tr>
<tr><td><code id="textstat_lexdiv_+3A_...">...</code></td>
<td>
<p>not used directly</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>textstat_lexdiv</code> calculates the lexical diversity of documents
using a variety of indices.
</p>
<p>In the following formulas, <code class="reqn">N</code> refers to the total number of
tokens, <code class="reqn">V</code> to the number of types, and <code class="reqn">f_v(i, N)</code> to the numbers
of types occurring <code class="reqn">i</code> times in a sample of length <code class="reqn">N</code>.
</p>

<dl>
<dt><code>"TTR"</code>:</dt><dd><p>The ordinary <em>Type-Token Ratio</em>: </p>
<p style="text-align: center;"><code class="reqn">TTR =
  \frac{V}{N}</code>
</p>
</dd>
<dt><code>"C"</code>:</dt><dd><p>Herdan's <em>C</em> (Herdan, 1960, as cited in Tweedie &amp;
Baayen, 1998; sometimes referred to as <em>LogTTR</em>): </p>
<p style="text-align: center;"><code class="reqn">C =
  \frac{\log{V}}{\log{N}}</code>
</p>
</dd>
<dt><code>"R"</code>:</dt><dd><p>Guiraud's <em>Root TTR</em> (Guiraud, 1954, as cited in
Tweedie &amp; Baayen, 1998): </p>
<p style="text-align: center;"><code class="reqn">R = \frac{V}{\sqrt{N}}</code>
</p>
</dd>
<dt><code>"CTTR"</code>:</dt><dd><p>Carroll's <em>Corrected TTR</em>: </p>
<p style="text-align: center;"><code class="reqn">CTTR =
  \frac{V}{\sqrt{2N}}</code>
</p>
</dd>
<dt><code>"U"</code>:</dt><dd><p>Dugast's <em>Uber Index</em>  (Dugast, 1978, as cited in
Tweedie &amp; Baayen, 1998): </p>
<p style="text-align: center;"><code class="reqn">U = \frac{(\log{N})^2}{\log{N} - \log{V}}</code>
</p>
</dd>
<dt><code>"S"</code>:</dt><dd><p>Summer's index: </p>
<p style="text-align: center;"><code class="reqn">S =
  \frac{\log{\log{V}}}{\log{\log{N}}}</code>
</p>
</dd>
<dt><code>"K"</code>:</dt><dd><p>Yule's <em>K</em>  (Yule, 1944, as presented in Tweedie &amp;
Baayen, 1998, Eq. 16) is calculated by: </p>
<p style="text-align: center;"><code class="reqn">K = 10^4 \times
  \left[ -\frac{1}{N} + \sum_{i=1}^{V} f_v(i, N) \left( \frac{i}{N} \right)^2 \right] </code>
</p>
</dd>
<dt><code>"I"</code>:</dt><dd><p>Yule's <em>I</em>  (Yule, 1944) is calculated by: </p>
<p style="text-align: center;"><code class="reqn">I = \frac{V^2}{M_2 - V}</code>
</p>

<p style="text-align: center;"><code class="reqn">M_2 = \sum_{i=1}^{V} i^2 * f_v(i, N)</code>
</p>
</dd>
<dt><code>"D"</code>:</dt><dd><p>Simpson's <em>D</em>  (Simpson 1949, as presented in
Tweedie &amp; Baayen, 1998, Eq. 17) is calculated by:
</p>
<p style="text-align: center;"><code class="reqn">D = \sum_{i=1}^{V} f_v(i, N) \frac{i}{N} \frac{i-1}{N-1}</code>
</p>
</dd>
<dt><code>"Vm"</code>:</dt><dd><p>Herdan's <code class="reqn">V_m</code>  (Herdan 1955, as presented in
Tweedie &amp; Baayen, 1998, Eq. 18) is calculated by:
</p>
<p style="text-align: center;"><code class="reqn">V_m = \sqrt{ \sum_{i=1}^{V} f_v(i, N) (i/N)^2 - \frac{i}{V} }</code>
</p>
</dd>
<dt><code>"Maas"</code>:</dt><dd><p>Maas' indices (<code class="reqn">a</code>, <code class="reqn">\log{V_0}</code> &amp;
<code class="reqn">\log{}_{e}{V_0}</code>): </p>
<p style="text-align: center;"><code class="reqn">a^2 = \frac{\log{N} -
  \log{V}}{\log{N}^2}</code>
</p>
 <p style="text-align: center;"><code class="reqn">\log{V_0} =
  \frac{\log{V}}{\sqrt{1 - \frac{\log{V}}{\log{N}}^2}}</code>
</p>
<p> The measure was derived from a formula by
Mueller (1969, as cited in Maas, 1972). <code class="reqn">\log{}_{e}{V_0}</code> is equivalent
to <code class="reqn">\log{V_0}</code>, only with <code class="reqn">e</code> as the base for the logarithms. Also
calculated are <code class="reqn">a</code>, <code class="reqn">\log{V_0}</code> (both not the same as before) and
<code class="reqn">V'</code> as measures of relative vocabulary growth while the text
progresses. To calculate these measures, the first half of the text and the
full text will be examined (see Maas, 1972, p. 67 ff. for details).  Note:
for the current method (for a dfm) there is no computation on separate
halves of the text.</p>
</dd>
<dt><code>"MATTR"</code>:</dt><dd><p>The Moving-Average Type-Token Ratio (Covington &amp;
McFall, 2010) calculates TTRs for a moving window of tokens from the first
to the last token, computing a TTR for each window. The MATTR is the mean
of the TTRs of each window.</p>
</dd>
<dt><code>"MSTTR"</code>:</dt><dd><p>Mean Segmental Type-Token Ratio (sometimes referred
to as <em>Split TTR</em>) splits the tokens into segments of the given size,
TTR for each segment is calculated and the mean of these values returned.
When this value is &lt; 1.0, it splits the tokens into equal, non-overlapping
sections of that size.  When this value is &gt; 1, it defines the segments as
windows of that size. Tokens at the end which do not make a full segment
are ignored.</p>
</dd>
</dl>



<h3>Value</h3>

<p>A data.frame of documents and their lexical diversity scores.
</p>


<h3>Author(s)</h3>

<p>Kenneth Benoit and Jiong Wei Lua. Many of the formulas have been
reimplemented from functions written by Meik Michalke in the <span class="pkg">koRpus</span>
package.
</p>


<h3>References</h3>

<p>Covington, M.A. &amp; McFall, J.D. (2010). Cutting the Gordian Knot: The
Moving-Average Type-Token Ratio (MATTR) <em>Journal of Quantitative
Linguistics</em>, 17(2), 94&ndash;100.
<a href="https://doi.org/10.1080/09296171003643098">doi:10.1080/09296171003643098</a>
</p>
<p>Herdan, G. (1955). <a href="https://link.springer.com/article/10.1007/BF01587632">A New Derivation and Interpretation of Yule's 'Characteristic' <em>K</em></a>. <em>Zeitschrift
fr angewandte Mathematik und Physik</em>, 6(4): 332&ndash;334.
</p>
<p>Maas, H.D. (1972). ber den Zusammenhang zwischen Wortschatzumfang und
Lnge eines Textes. <em>Zeitschrift fr Literaturwissenschaft und Linguistik</em>,
2(8), 73&ndash;96.
</p>
<p>McCarthy, P.M. &amp;  Jarvis, S. (2007). vocd: A Theoretical and Empirical
Evaluation. <em>Language Testing</em>, 24(4), 459&ndash;488.
<a href="https://doi.org/10.1177/0265532207080767">doi:10.1177/0265532207080767</a>
</p>
<p>McCarthy, P.M. &amp; Jarvis, S. (2010). <a href="https://link.springer.com/article/10.3758/BRM.42.2.381">MTLD, vocd-D, and HD-D: A Validation Study of Sophisticated Approaches to Lexical Diversity Assessment</a>.
<em>Behaviour Research Methods</em>, 42(2), 381&ndash;392.
</p>
<p>Michalke, M. (2014). <em>koRpus: An R Package for Text Analysis (Version
0.05-4)</em>. Available from <a href="https://reaktanz.de/?c=hacking&amp;s=koRpus">https://reaktanz.de/?c=hacking&amp;s=koRpus</a>.
</p>
<p>Simpson, E.H. (1949). Measurement of Diversity. <em>Nature</em>, 163: 688.
<a href="https://doi.org/10.1038/163688a0">doi:10.1038/163688a0</a>
</p>
<p>Tweedie. F.J. and Baayen, R.H. (1998). How Variable May a Constant Be?
Measures of Lexical Richness in Perspective. <em>Computers and the
Humanities</em>, 32(5), 323&ndash;352.  <a href="https://doi.org/10.1023/A%3A1001749303137">doi:10.1023/A:1001749303137</a>
</p>
<p>Yule, G. U. (1944)  <em>The Statistical Study of Literary Vocabulary.</em>
Cambridge: Cambridge University Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("quanteda")

txt &lt;- c("Anyway, like I was sayin', shrimp is the fruit of the sea. You can
          barbecue it, boil it, broil it, bake it, saute it.",
         "There's shrimp-kabobs,
          shrimp creole, shrimp gumbo. Pan fried, deep fried, stir-fried. There's
          pineapple shrimp, lemon shrimp, coconut shrimp, pepper shrimp, shrimp soup,
          shrimp stew, shrimp salad, shrimp and potatoes, shrimp burger, shrimp
          sandwich.")
tokens(txt) %&gt;%
    textstat_lexdiv(measure = c("TTR", "CTTR", "K"))
dfm(tokens(txt)) %&gt;%
    textstat_lexdiv(measure = c("TTR", "CTTR", "K"))

toks &lt;- tokens(corpus_subset(data_corpus_inaugural, Year &gt; 2000))
textstat_lexdiv(toks, c("CTTR", "TTR", "MATTR"), MATTR_window = 100)
</code></pre>

<hr>
<h2 id='textstat_proxy'>[Experimental] Compute document/feature proximity</h2><span id='topic+textstat_proxy'></span>

<h3>Description</h3>

<p>This is an underlying function for <code>textstat_dist</code> and
<code>textstat_simil</code> but returns <code>TsparseMatrix</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textstat_proxy(
  x,
  y = NULL,
  margin = c("documents", "features"),
  method = c("cosine", "correlation", "jaccard", "ejaccard", "dice", "edice", "hamann",
    "simple matching", "euclidean", "chisquared", "hamming", "kullback", "manhattan",
    "maximum", "canberra", "minkowski"),
  p = 2,
  min_proxy = NULL,
  rank = NULL,
  use_na = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textstat_proxy_+3A_y">y</code></td>
<td>
<p>if a <a href="quanteda.html#topic+dfm">dfm</a> object is provided, proximity between documents or
features in <code>x</code> and <code>y</code> is computed.</p>
</td></tr>
<tr><td><code id="textstat_proxy_+3A_margin">margin</code></td>
<td>
<p>identifies the margin of the dfm on which similarity or
difference will be computed:  <code>"documents"</code> for documents or
<code>"features"</code> for word/term features.</p>
</td></tr>
<tr><td><code id="textstat_proxy_+3A_method">method</code></td>
<td>
<p>character; the method identifying the similarity or distance
measure to be used; see Details.</p>
</td></tr>
<tr><td><code id="textstat_proxy_+3A_p">p</code></td>
<td>
<p>The power of the Minkowski distance.</p>
</td></tr>
<tr><td><code id="textstat_proxy_+3A_min_proxy">min_proxy</code></td>
<td>
<p>the minimum proximity value to be recoded.</p>
</td></tr>
<tr><td><code id="textstat_proxy_+3A_rank">rank</code></td>
<td>
<p>an integer value specifying top-n most proximity values to be
recorded.</p>
</td></tr>
<tr><td><code id="textstat_proxy_+3A_use_na">use_na</code></td>
<td>
<p>if <code>TRUE</code>, return <code>NA</code> for proximity to empty
vectors. Note that use of <code>NA</code> makes the proximity matrices denser.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+textstat_dist">textstat_dist()</a></code>, <code><a href="#topic+textstat_simil">textstat_simil()</a></code>
</p>

<hr>
<h2 id='textstat_proxy-class'>textstat_simil/dist classes</h2><span id='topic+textstat_proxy-class'></span><span id='topic+textstat_dist-class'></span><span id='topic+textstat_dist_symm-class'></span><span id='topic+textstat_simil-class'></span><span id='topic+textstat_simil_symm-class'></span><span id='topic+textstat_simil_sparse-class'></span><span id='topic+textstat_simil_symm_sparse-class'></span><span id='topic+validate_min_simil'></span><span id='topic+show+2Ctextstat_proxy-method'></span>

<h3>Description</h3>

<p>Sparse classes for similarity and distance matrices created by
<code><a href="#topic+textstat_simil">textstat_simil()</a></code> and <code><a href="#topic+textstat_dist">textstat_dist()</a></code>.
</p>
<p>Sparse classes for similarity and distance matrices created by
<code><a href="#topic+textstat_simil">textstat_simil()</a></code> and
<code><a href="#topic+textstat_dist">textstat_dist()</a></code>.
</p>
<p>Print/show method for objects created by <code>textstat_simil</code> and
<code>textstat_dist</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate_min_simil(object)

## S4 method for signature 'textstat_proxy'
show(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textstat_proxy-class_+3A_object">object</code></td>
<td>
<p>the textstat_proxy object to be printed</p>
</td></tr>
</table>


<h3>Slots</h3>


<dl>
<dt><code>.Data</code></dt><dd><p>a sparse <span class="pkg">Matrix</span> object, symmetric if selection is
<code>NULL</code></p>
</dd>
<dt><code>method</code></dt><dd><p>the method used for computing similarity or distance</p>
</dd>
<dt><code>min_simil</code></dt><dd><p>numeric; a threshold for the similarity values below which similarity
values are not computed</p>
</dd>
<dt><code>margin</code></dt><dd><p>identifies the margin of the dfm on which similarity or
difference was computed:  <code>"documents"</code> for documents or
<code>"features"</code> for word/term features.</p>
</dd>
<dt><code>type</code></dt><dd><p>either <code>"textstat_simil"</code> or <code>"textstat_dist"</code></p>
</dd>
<dt><code>selection</code></dt><dd><p>target units, if any</p>
</dd>
</dl>


<h3>See Also</h3>

<p><code><a href="#topic+textstat_simil">textstat_simil()</a></code>
</p>
<p><code><a href="#topic+textstat_simil">textstat_simil()</a></code>
</p>

<hr>
<h2 id='textstat_readability'>Calculate readability</h2><span id='topic+textstat_readability'></span>

<h3>Description</h3>

<p>Calculate the readability of text(s) using one of a variety of computed
indexes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textstat_readability(
  x,
  measure = "Flesch",
  remove_hyphens = TRUE,
  min_sentence_length = 1,
  max_sentence_length = 10000,
  intermediate = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textstat_readability_+3A_x">x</code></td>
<td>
<p>a character or <a href="quanteda.html#topic+corpus">corpus</a> object containing the texts</p>
</td></tr>
<tr><td><code id="textstat_readability_+3A_measure">measure</code></td>
<td>
<p>character vector defining the readability measure to calculate.
Matches are case-insensitive.  See other valid measures under Details.</p>
</td></tr>
<tr><td><code id="textstat_readability_+3A_remove_hyphens">remove_hyphens</code></td>
<td>
<p>if <code>TRUE</code>, treat constituent words in hyphenated as
separate terms, for purposes of computing word lengths, e.g.
&quot;decision-making&quot; as two terms of lengths 8 and 6 characters respectively,
rather than as a single word of 15 characters</p>
</td></tr>
<tr><td><code id="textstat_readability_+3A_min_sentence_length">min_sentence_length</code>, <code id="textstat_readability_+3A_max_sentence_length">max_sentence_length</code></td>
<td>
<p>set the minimum and maximum
sentence lengths (in tokens, excluding punctuation) to include in the
computation of readability.  This makes it easy to exclude &quot;sentences&quot; that
may not really be sentences, such as section titles, table elements, and
other cruft that might be in the texts following conversion.
</p>
<p>For finer-grained control, consider filtering sentences prior first,
including through pattern-matching, using <code><a href="quanteda.html#topic+corpus_trim">corpus_trim()</a></code>.</p>
</td></tr>
<tr><td><code id="textstat_readability_+3A_intermediate">intermediate</code></td>
<td>
<p>if <code>TRUE</code>, include intermediate quantities in the output</p>
</td></tr>
<tr><td><code id="textstat_readability_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The following readability formulas have been implemented, where
</p>

<ul>
<li><p> Nw = <code class="reqn">n_{w}</code> = number of words
</p>
</li>
<li><p> Nc = <code class="reqn">n_{c}</code> = number of characters
</p>
</li>
<li><p> Nst = <code class="reqn">n_{st}</code> = number of sentences
</p>
</li>
<li><p> Nsy = <code class="reqn">n_{sy}</code> = number of syllables
</p>
</li>
<li><p> Nwf = <code class="reqn">n_{wf}</code> = number of words matching the Dale-Chall List
of 3000 &quot;familiar words&quot;
</p>
</li>
<li><p> ASL = Average Sentence Length: number of words / number of sentences
</p>
</li>
<li><p> AWL = Average Word Length: number of characters / number of words
</p>
</li>
<li><p> AFW = Average Familiar Words: count of words matching the Dale-Chall
list of 3000 &quot;familiar words&quot; / number of all words
</p>
</li>
<li><p> Nwd = <code class="reqn">n_{wd}</code> = number of &quot;difficult&quot; words not matching the
Dale-Chall list of &quot;familiar&quot; words
</p>
</li></ul>


<dl>
<dt><code>"ARI"</code>:</dt><dd><p>Automated Readability Index (Senter and Smith 1967)
</p>
<p style="text-align: center;"><code class="reqn">0.5 ASL  + 4.71 AWL - 21.34</code>
</p>
</dd>
<dt><code>"ARI.Simple"</code>:</dt><dd><p>A simplified version of Senter and Smith's (1967) Automated Readability Index.
</p>
<p style="text-align: center;"><code class="reqn">ASL + 9 AWL</code>
</p>
</dd>
<dt><code>"Bormuth.MC"</code>:</dt><dd><p>Bormuth's (1969) Mean Cloze Formula.
</p>
<p style="text-align: center;"><code class="reqn">0.886593 - 0.03640 \times AWL + 0.161911 \times AFW  - 0.21401 \times
  ASL - 0.000577 \times ASL^2 - 0.000005 \times ASL^3</code>
</p>
</dd>
<dt><code>"Bormuth.GP"</code>:</dt><dd><p>Bormuth's (1969) Grade Placement score.
</p>
<p style="text-align: center;"><code class="reqn">4.275 + 12.881M - 34.934M^2 + 20.388 M^3 + 26.194 CCS -
  2.046 CCS^2 - 11.767 CCS^3 - 42.285(M \times CCS) + 97.620(M \times CCS)^2 -
  59.538(M \times CCS)^2</code>
</p>

<p>where <code class="reqn">M</code> is the Bormuth Mean Cloze Formula as in
<code>"Bormuth"</code> above, and <code class="reqn">CCS</code> is the Cloze Criterion Score (Bormuth,
1968).</p>
</dd>
<dt><code>"Coleman"</code>:</dt><dd><p>Coleman's (1971) Readability Formula 1.
</p>
<p style="text-align: center;"><code class="reqn">1.29 \times \frac{100 \times n_{wsy=1}}{n_{w}} - 38.45</code>
</p>

<p>where <code class="reqn">n_{wsy=1}</code> = Nwsy1 = the number of one-syllable words.  The
scaling by 100 in this and the other Coleman-derived measures arises
because the Coleman measures are calculated on a per 100 words basis.</p>
</dd>
<dt><code>"Coleman.C2"</code>:</dt><dd><p>Coleman's (1971) Readability Formula 2.
</p>
<p style="text-align: center;"><code class="reqn">1.16 \times \frac{100 \times n_{wsy=1}}{
  Nw + 1.48 \times \frac{100 \times n_{st}}{n_{w}} - 37.95}</code>
</p>
</dd>
<dt><code>"Coleman.Liau.ECP"</code>:</dt><dd><p>Coleman-Liau Estimated Cloze Percent
(ECP) (Coleman and Liau 1975).
</p>
<p style="text-align: center;"><code class="reqn">141.8401 - 0.214590 \times 100
  \times AWL + 1.079812 \times \frac{n_{st} \times 100}{n_{w}}</code>
</p>
</dd>
<dt><code>"Coleman.Liau.grade"</code>:</dt><dd><p>Coleman-Liau Grade Level (Coleman
and Liau 1975).
</p>
<p style="text-align: center;"><code class="reqn">-27.4004 \times \mathtt{Coleman.Liau.ECP} \times 100 +
  23.06395</code>
</p>
</dd>
<dt><code>"Coleman.Liau.short"</code>:</dt><dd><p>Coleman-Liau Index (Coleman and Liau 1975).
</p>
<p style="text-align: center;"><code class="reqn">5.88 \times AWL + 29.6 \times \frac{n_{st}}{n_{w}} - 15.8</code>
</p>
</dd>
<dt><code>"Dale.Chall"</code>:</dt><dd><p>The New Dale-Chall Readability formula (Chall
and Dale 1995).
</p>
<p style="text-align: center;"><code class="reqn">64 - (0.95 \times 100 \times \frac{n_{wd}}{n_{w}}) - (0.69 \times ASL)</code>
</p>
</dd>
<dt><code>"Dale.Chall.Old"</code>:</dt><dd><p>The original Dale-Chall Readability formula
(Dale and Chall (1948).
</p>
<p style="text-align: center;"><code class="reqn">0.1579 \times 100 \times \frac{n_{wd}}{n_{w}} + 0.0496 \times ASL [+ 3.6365]</code>
</p>

<p>The additional constant 3.6365 is only added if (Nwd / Nw) &gt; 0.05.</p>
</dd>
<dt><code>"Dale.Chall.PSK"</code>:</dt><dd><p>The Powers-Sumner-Kearl Variation of the
Dale and Chall Readability formula (Powers, Sumner and Kearl, 1958).
</p>
<p style="text-align: center;"><code class="reqn">0.1155 \times
  100 \frac{n_{wd}}{n_{w}}) + (0.0596 \times ASL) + 3.2672 </code>
</p>
</dd>
<dt><code>"Danielson.Bryan"</code>:</dt><dd><p>Danielson-Bryan's (1963) Readability Measure 1. </p>
<p style="text-align: center;"><code class="reqn">
  (1.0364 \times \frac{n_{c}}{n_{blank}}) +
  (0.0194 \times \frac{n_{c}}{n_{st}}) -
  0.6059</code>
</p>

<p>where <code class="reqn">n_{blank}</code> = Nblank = the number of blanks.</p>
</dd>
<dt><code>"Danielson.Bryan2"</code>:</dt><dd><p>Danielson-Bryan's (1963) Readability Measure 2. </p>
<p style="text-align: center;"><code class="reqn">
  131.059- (10.364 \times \frac{n_{c}}{n_{blank}}) + (0.0194
   \times \frac{n_{c}}{n_{st}})</code>
</p>

<p>where <code class="reqn">n_{blank}</code> = Nblank = the number of blanks.</p>
</dd>
<dt><code>"Dickes.Steiwer"</code>:</dt><dd><p>Dickes-Steiwer Index (Dicks and Steiwer 1977). </p>
<p style="text-align: center;"><code class="reqn">
  235.95993 - (7.3021 \times AWL)  - (12.56438 \times ASL) -
  (50.03293 \times TTR)</code>
</p>

<p>where TTR is the Type-Token Ratio (see <code><a href="#topic+textstat_lexdiv">textstat_lexdiv()</a></code>)</p>
</dd>
<dt><code>"DRP"</code>:</dt><dd><p>Degrees of Reading Power. </p>
<p style="text-align: center;"><code class="reqn">(1 - Bormuth.MC) *
  100</code>
</p>

<p>where Bormuth.MC refers to Bormuth's (1969)  Mean Cloze Formula (documented above)</p>
</dd>
<dt><code>"ELF"</code>:</dt><dd><p>Easy Listening Formula (Fang 1966): </p>
<p style="text-align: center;"><code class="reqn">\frac{n_{wsy&gt;=2}}{n_{st}}</code>
</p>

<p>where <code class="reqn">n_{wsy&gt;=2}</code> = Nwmin2sy = the number of words with 2 syllables or more.</p>
</dd>
<dt><code>"Farr.Jenkins.Paterson"</code>:</dt><dd><p>Farr-Jenkins-Paterson's
Simplification of Flesch's Reading Ease Score (Farr, Jenkins and Paterson 1951). </p>
<p style="text-align: center;"><code class="reqn">
   -31.517 - (1.015 \times ASL) + (1.599 \times
  \frac{n_{wsy=1}}{n_{w}})</code>
</p>

<p>where <code class="reqn">n_{wsy=1}</code> = Nwsy1 = the number of one-syllable words.</p>
</dd>
<dt><code>"Flesch"</code>:</dt><dd><p>Flesch's Reading Ease Score (Flesch 1948).
</p>
<p style="text-align: center;"><code class="reqn">206.835 - (1.015 \times ASL) - (84.6 \times \frac{n_{sy}}{n_{w}})</code>
</p>
</dd>
<dt><code>"Flesch.PSK"</code>:</dt><dd><p>The Powers-Sumner-Kearl's Variation of Flesch Reading Ease Score
(Powers, Sumner and Kearl, 1958). </p>
<p style="text-align: center;"><code class="reqn"> (0.0778 \times
  ASL) + (4.55 \times \frac{n_{sy}}{n_{w}}) -
  2.2029</code>
</p>
</dd>
<dt><code>"Flesch.Kincaid"</code>:</dt><dd><p>Flesch-Kincaid Readability Score (Flesch and Kincaid 1975). </p>
<p style="text-align: center;"><code class="reqn">
  0.39 \times ASL + 11.8  \times \frac{n_{sy}}{n_{w}} -
  15.59</code>
</p>
</dd>
<dt><code>"FOG"</code>:</dt><dd><p>Gunning's Fog Index (Gunning 1952). </p>
<p style="text-align: center;"><code class="reqn">0.4
  \times (ASL + 100 \times \frac{n_{wsy&gt;=3}}{n_{w}})</code>
</p>

<p>where <code class="reqn">n_{wsy&gt;=3}</code> = Nwmin3sy = the number of words with 3-syllables or more.
The scaling by 100 arises because the original FOG index is based on
just a sample of 100 words)</p>
</dd>
<dt><code>"FOG.PSK"</code>:</dt><dd><p>The Powers-Sumner-Kearl Variation of Gunning's
Fog Index (Powers, Sumner and Kearl, 1958). </p>
<p style="text-align: center;"><code class="reqn">3.0680 \times
  (0.0877 \times ASL) +(0.0984 \times 100 \times \frac{n_{wsy&gt;=3}}{n_{w}})</code>
</p>

<p>where <code class="reqn">n_{wsy&gt;=3}</code> = Nwmin3sy = the number of words with 3-syllables or more.
The scaling by 100 arises because the original FOG index is based on
just a sample of 100 words)</p>
</dd>
<dt><code>"FOG.NRI"</code>:</dt><dd><p>The Navy's Adaptation of Gunning's Fog Index (Kincaid, Fishburne, Rogers and Chissom 1975).
</p>
<p style="text-align: center;"><code class="reqn">(\frac{(n_{wsy&lt;3} + 3 \times n_{wsy=3})}{(100 \times \frac{N_{st}}{N_{w}})}  -
  3) / 2 </code>
</p>

<p>where <code class="reqn">n_{wsy&lt;3}</code> = Nwless3sy = the number of words with <em>less than</em> 3 syllables, and
<code class="reqn">n_{wsy=3}</code> = Nw3sy = the number of 3-syllable words. The scaling by 100
arises because the original FOG index is based on just a sample of 100 words)</p>
</dd>
<dt><code>"FORCAST"</code>:</dt><dd><p>FORCAST (Simplified Version of FORCAST.RGL) (Caylor and
Sticht 1973). </p>
<p style="text-align: center;"><code class="reqn"> 20 - \frac{n_{wsy=1} \times
  150)}{(n_{w} \times 10)}</code>
</p>

<p>where <code class="reqn">n_{wsy=1}</code> = Nwsy1 = the number of one-syllable words. The scaling by 150
arises because the original FORCAST index is based on just a sample of 150 words.</p>
</dd>
<dt><code>"FORCAST.RGL"</code>:</dt><dd><p>FORCAST.RGL (Caylor and Sticht 1973).
</p>
<p style="text-align: center;"><code class="reqn">20.43 - 0.11 \times \frac{n_{wsy=1} \times
  150)}{(n_{w} \times 10)}</code>
</p>

<p>where <code class="reqn">n_{wsy=1}</code> = Nwsy1 = the number of one-syllable words. The scaling by 150 arises
because the original FORCAST index is based on just a sample of 150 words.</p>
</dd>
<dt><code>"Fucks"</code>:</dt><dd><p>Fucks' (1955) Stilcharakteristik (Style
Characteristic). </p>
<p style="text-align: center;"><code class="reqn">AWL * ASL</code>
</p>
</dd>
<dt><code>"Linsear.Write"</code>:</dt><dd><p>Linsear Write (Klare 1975).
</p>
<p style="text-align: center;"><code class="reqn">\frac{[(100 - (\frac{100 \times n_{wsy&lt;3}}{n_{w}})) +
  (3 \times \frac{100 \times n_{wsy&gt;=3}}{n_{w}})]}{(100 \times
  \frac{n_{st}}{n_{w}})}</code>
</p>

<p>where <code class="reqn">n_{wsy&lt;3}</code> = Nwless3sy = the number of words with <em>less than</em> 3 syllables, and
<code class="reqn">n_{wsy&gt;=3}</code> = Nwmin3sy = the number of words with 3-syllables or more. The scaling
by 100 arises because the original Linsear.Write measure is based on just a sample of 100 words)</p>
</dd>
<dt><code>"LIW"</code>:</dt><dd><p>Bjrnsson's (1968) Lsbarhetsindex (For Swedish
Texts). </p>
<p style="text-align: center;"><code class="reqn">ASL + \frac{100 \times n_{wsy&gt;=7}}{n_{w}}</code>
</p>

<p>where <code class="reqn">n_{wsy&gt;=7}</code> = Nwmin7sy = the number of words with 7-syllables or more. The scaling
by 100 arises because the Lsbarhetsindex index is based on just a sample of 100 words)</p>
</dd>
<dt><code>"nWS"</code>:</dt><dd><p>Neue Wiener Sachtextformeln 1 (Bamberger and
Vanecek 1984). </p>
<p style="text-align: center;"><code class="reqn">19.35 \times \frac{n_{wsy&gt;=3}}{n_{w}} +
  0.1672 \times ASL + 12.97 \times \frac{b_{wchar&gt;=6}}{n_{w}} - 3.27 \times
   \frac{n_{wsy=1}}{n_{w}} - 0.875</code>
</p>

<p>where <code class="reqn">n_{wsy&gt;=3}</code> = Nwmin3sy = the number of words with 3 syllables or more,
<code class="reqn">n_{wchar&gt;=6}</code> = Nwmin6char = the number of words with 6 characters or more, and
<code class="reqn">n_{wsy=1}</code> = Nwsy1 = the number of one-syllable words.</p>
</dd>
<dt><code>"nWS.2"</code>:</dt><dd><p>Neue Wiener Sachtextformeln 2 (Bamberger and
Vanecek 1984). </p>
<p style="text-align: center;"><code class="reqn">20.07 \times \frac{n_{wsy&gt;=3}}{n_{w}} + 0.1682 \times ASL +
  13.73 \times \frac{n_{wchar&gt;=6}}{n_{w}} - 2.779</code>
</p>

<p>where <code class="reqn">n_{wsy&gt;=3}</code> = Nwmin3sy = the number of words with 3 syllables or more, and
<code class="reqn">n_{wchar&gt;=6}</code> = Nwmin6char = the number of words with 6 characters or more.</p>
</dd>
<dt><code>"nWS.3"</code>:</dt><dd><p>Neue Wiener Sachtextformeln 3 (Bamberger and
Vanecek 1984). </p>
<p style="text-align: center;"><code class="reqn">29.63 \times \frac{n_{wsy&gt;=3}}{n_{w}} + 0.1905 \times
  ASL - 1.1144</code>
</p>

<p>where <code class="reqn">n_{wsy&gt;=3}</code> = Nwmin3sy = the number of words with 3 syllables or more.</p>
</dd>
<dt><code>"nWS.4"</code>:</dt><dd><p>Neue Wiener Sachtextformeln 4 (Bamberger and
Vanecek 1984). </p>
<p style="text-align: center;"><code class="reqn">27.44 \times \frac{n_{wsy&gt;=3}}{n_{w}} + 0.2656 \times
  ASL - 1.693</code>
</p>

<p>where <code class="reqn">n_{wsy&gt;=3}</code> = Nwmin3sy = the number of words with 3 syllables or more.</p>
</dd>
<dt><code>"RIX"</code>:</dt><dd><p>Anderson's (1983) Readability Index. </p>
<p style="text-align: center;"><code class="reqn">
  \frac{n_{wsy&gt;=7}}{n_{st}}</code>
</p>

<p>where <code class="reqn">n_{wsy&gt;=7}</code> = Nwmin7sy = the number of words with 7-syllables or more.</p>
</dd>
<dt><code>"Scrabble"</code>:</dt><dd><p>Scrabble Measure. </p>
<p style="text-align: center;"><code class="reqn">Mean
  Scrabble Letter Values of All Words</code>
</p>
<p>.
Scrabble values are for English.  There is no reference for this, as we
created it experimentally.  It's not part of any accepted readability
index!</p>
</dd>
<dt><code>"SMOG"</code>:</dt><dd><p>Simple Measure of Gobbledygook (SMOG) (McLaughlin 1969). </p>
<p style="text-align: center;"><code class="reqn"> 1.043
   \times \sqrt{n_{wsy&gt;=3}} \times \frac{30}{n_{st}} + 3.1291</code>
</p>

<p>where <code class="reqn">n_{wsy&gt;=3}</code> = Nwmin3sy = the number of words with 3 syllables or more.
This measure is regression equation D in McLaughlin's original paper.</p>
</dd>
<dt><code>"SMOG.C"</code>:</dt><dd><p>SMOG (Regression Equation C) (McLaughlin's 1969) </p>
<p style="text-align: center;"><code class="reqn">0.9986 \times
  \sqrt{Nwmin3sy \times \frac{30}{n_{st}} +
  5} +  2.8795</code>
</p>

<p>where <code class="reqn">n_{wsy&gt;=3}</code> = Nwmin3sy = the number of words with 3 syllables or more.
This measure is regression equation C in McLaughlin's original paper.</p>
</dd>
<dt><code>"SMOG.simple"</code>:</dt><dd><p>Simplified Version of McLaughlin's (1969) SMOG Measure. </p>
<p style="text-align: center;"><code class="reqn">
  \sqrt{Nwmin3sy \times \frac{30}{n_{st}}} +
  3</code>
</p>
</dd>
<dt><code>"SMOG.de"</code>:</dt><dd><p>Adaptation of McLaughlin's (1969) SMOG Measure for German Texts.
</p>
<p style="text-align: center;"><code class="reqn"> \sqrt{Nwmin3sy \times \frac{30}{n_{st}}-2}</code>
</p>
</dd>
<dt><code>"Spache"</code>:</dt><dd><p>Spache's (1952) Readability Measure. </p>
<p style="text-align: center;"><code class="reqn"> 0.121 \times
  ASL + 0.082 \times \frac{n_{wnotinspache}}{n_{w}}  +
  0.659</code>
</p>

<p>where <code class="reqn">n_{wnotinspache}</code> = Nwnotinspache = number of unique words not in the Spache word list.</p>
</dd>
<dt><code>"Spache.old"</code>:</dt><dd><p>Spache's (1952) Readability Measure (Old). </p>
<p style="text-align: center;"><code class="reqn">0.141
  \times ASL + 0.086 \times \frac{n_{wnotinspache}}{n_{w}}  +
  0.839</code>
</p>

<p>where <code class="reqn">n_{wnotinspache}</code> = Nwnotinspache = number of unique words not in the Spache word list.</p>
</dd>
<dt><code>"Strain"</code>:</dt><dd><p>Strain Index (Solomon 2006). </p>
<p style="text-align: center;"><code class="reqn">n_{sy} /
  \frac{n_{st}}{3} /10</code>
</p>

<p>The scaling by 3 arises because the original Strain index is based on just the first 3 sentences.</p>
</dd>
<dt><code>"Traenkle.Bailer"</code>:</dt><dd><p>Trnkle &amp; Bailer's (1984) Readability Measure 1.
</p>
<p style="text-align: center;"><code class="reqn">224.6814 - (79.8304 \times AWL) - (12.24032 \times
  ASL) - (1.292857 \times 100 \times \frac{n_{prep}}{n_{w}}</code>
</p>

<p>where <code class="reqn">n_{prep}</code> = Nprep = the number of prepositions. The scaling by 100 arises because the original
Trnkle &amp; Bailer index is based on just a sample of 100 words.</p>
</dd>
<dt><code>"Traenkle.Bailer2"</code>:</dt><dd><p>Trnkle &amp; Bailer's (1984) Readability Measure 2.
</p>
<p style="text-align: center;"><code class="reqn">Trnkle.Bailer2 =  234.1063 - (96.11069 \times AWL
  ) - (2.05444 \times 100 \times \frac{n_{prep}}{n_{w}}) -
  (1.02805 \times 100 \times \frac{n_{conj}}{n_{w}}</code>
</p>

<p>where <code class="reqn">n_{prep}</code> = Nprep = the number of prepositions,
<code class="reqn">n_{conj}</code> = Nconj = the number of conjunctions,
The scaling by 100 arises because the original Trnkle &amp; Bailer index is based on
just a sample of 100 words)</p>
</dd>
<dt><code>"Wheeler.Smith"</code>:</dt><dd><p>Wheeler &amp; Smith's (1954) Readability Measure.
</p>
<p style="text-align: center;"><code class="reqn"> ASL \times 10 \times \frac{n_{wsy&gt;=2}}{n_{words}}</code>
</p>

<p>where <code class="reqn">n_{wsy&gt;=2}</code> = Nwmin2sy = the number of words with 2 syllables or more.</p>
</dd>
<dt><code>"meanSentenceLength"</code>:</dt><dd><p>Average Sentence Length (ASL).
</p>
<p style="text-align: center;"><code class="reqn">\frac{n_{w}}{n_{st}}</code>
</p>
</dd>
<dt><code>"meanWordSyllables"</code>:</dt><dd><p>Average Word Syllables (AWL).
</p>
<p style="text-align: center;"><code class="reqn">\frac{n_{sy}}{n_{w}}</code>
</p>
</dd>
</dl>



<h3>Value</h3>

<p><code>textstat_readability</code> returns a data.frame of documents and
their readability scores.
</p>


<h3>Author(s)</h3>

<p>Kenneth Benoit, re-engineered from Meik Michalke's <span class="pkg">koRpus</span>
package.
</p>


<h3>References</h3>

<p>Anderson, J. (1983). Lix and rix: Variations on a little-known readability
index. <em>Journal of Reading</em>, 26(6),
490&ndash;496.  <code style="white-space: pre;">&#8288;https://www.jstor.org/stable/40031755&#8288;</code>
</p>
<p>Bamberger, R. &amp; Vanecek, E. (1984). <em>Lesen-Verstehen-Lernen-Schreiben</em>.
Wien: Jugend und Volk.
</p>
<p>Bjrnsson, C. H. (1968). <em>Lsbarhet</em>. Stockholm: Liber.
</p>
<p>Bormuth, J.R. (1969). <a href="https://files.eric.ed.gov/fulltext/ED029166.pdf">Development of Readability Analysis</a>.
</p>
<p>Bormuth, J.R. (1968). Cloze test readability: Criterion reference
scores. <em>Journal of educational
measurement</em>, 5(3), 189&ndash;196. <code style="white-space: pre;">&#8288;https://www.jstor.org/stable/1433978&#8288;</code>
</p>
<p>Caylor, J.S. (1973). Methodologies for Determining Reading Requirements of
Military Occupational Specialities.  <code style="white-space: pre;">&#8288;https://eric.ed.gov/?id=ED074343&#8288;</code>
</p>
<p>Caylor, J.S. &amp; Sticht, T.G. (1973). <em>Development of a Simple Readability
Index for Job Reading Material</em>
<code style="white-space: pre;">&#8288;https://archive.org/details/ERIC_ED076707&#8288;</code>
</p>
<p>Coleman, E.B. (1971). Developing a technology of written instruction: Some
determiners of the complexity of prose. <em>Verbal learning research and the
technology of written instruction</em>, 155&ndash;204.
</p>
<p>Coleman, M. &amp; Liau, T.L. (1975). A Computer Readability Formula Designed
for Machine Scoring. <em>Journal of Applied Psychology</em>, 60(2), 283.
<a href="https://doi.org/10.1037/h0076540">doi:10.1037/h0076540</a>
</p>
<p>Dale, E. and Chall, J.S. (1948). A Formula for Predicting Readability:
Instructions.  <em>Educational Research
Bulletin</em>, 37-54.  <code style="white-space: pre;">&#8288;https://www.jstor.org/stable/1473169&#8288;</code>
</p>
<p>Chall, J.S. and Dale, E. (1995). <em>Readability Revisited: The New Dale-Chall
Readability Formula</em>. Brookline Books.
</p>
<p>Dickes, P. &amp; Steiwer, L. (1977). Ausarbeitung von Lesbarkeitsformeln fr
die Deutsche Sprache. <em>Zeitschrift fr Entwicklungspsychologie und
Pdagogische Psychologie</em> 9(1), 20&ndash;28.
</p>
<p>Danielson, W.A., &amp; Bryan, S.D. (1963). Computer Automation of Two
Readability
Formulas.
<em>Journalism Quarterly</em>, 40(2), 201&ndash;206. <a href="https://doi.org/10.1177/107769906304000207">doi:10.1177/107769906304000207</a>
</p>
<p>DuBay, W.H. (2004). <a href="https://files.eric.ed.gov/fulltext/ED490073.pdf"><em>The Principles of Readability</em></a>.
</p>
<p>Fang, I. E. (1966). The &quot;Easy listening formula&quot;. <em>Journal of Broadcasting
&amp; Electronic Media</em>, 11(1), 63&ndash;68.  <a href="https://doi.org/10.1080/08838156609363529">doi:10.1080/08838156609363529</a>
</p>
<p>Farr, J. N., Jenkins, J.J., &amp; Paterson, D.G. (1951). Simplification of
Flesch Reading Ease Formula. <em>Journal of Applied Psychology</em>, 35(5): 333.
<a href="https://doi.org/10.1037/h0057532">doi:10.1037/h0057532</a>
</p>
<p>Flesch, R. (1948). A New Readability Yardstick. <em>Journal of Applied
Psychology</em>, 32(3), 221.  <a href="https://doi.org/10.1037/h0057532">doi:10.1037/h0057532</a>
</p>
<p>Fucks, W. (1955). Der Unterschied des Prosastils von Dichtern und anderen
Schriftstellern. <em>Sprachforum</em>, 1, 233-244.
</p>
<p>Gunning, R. (1952). <em>The Technique of Clear Writing</em>.  New York:
McGraw-Hill.
</p>
<p>Klare, G.R. (1975). Assessing Readability.  <em>Reading Research Quarterly</em>,
10(1), 62-102.  <a href="https://doi.org/10.2307/747086">doi:10.2307/747086</a>
</p>
<p>Kincaid, J. P., Fishburne Jr, R.P., Rogers, R.L., &amp; Chissom, B.S. (1975).
<a href="https://stars.library.ucf.edu/istlibrary/56/">Derivation of New Readability Formulas (Automated Readability Index, FOG count and Flesch Reading Ease Formula) for Navy Enlisted Personnel</a>.
</p>
<p>McLaughlin, G.H. (1969). <a href="https://ogg.osu.edu/media/documents/health_lit/WRRSMOG_Readability_Formula_G._Harry_McLaughlin__1969_.pdf">SMOG Grading: A New Readability Formula.</a>
<em>Journal of Reading</em>, 12(8), 639-646.
</p>
<p>Michalke, M. (2014). <em>koRpus: An R Package for Text Analysis (Version 0.05-4)</em>.
Available from <a href="https://reaktanz.de/?c=hacking&amp;s=koRpus">https://reaktanz.de/?c=hacking&amp;s=koRpus</a>.
</p>
<p>Powers, R.D., Sumner, W.A., and Kearl, B.E. (1958). A Recalculation of
Four Adult Readability Formulas. <em>Journal of Educational Psychology</em>,
49(2), 99.  <a href="https://doi.org/10.1037/h0043254">doi:10.1037/h0043254</a>
</p>
<p>Senter, R. J., &amp; Smith, E. A. (1967). <a href="https://apps.dtic.mil/sti/pdfs/AD0667273.pdf">Automated readability index.</a>
Wright-Patterson Air Force Base. Report No. AMRL-TR-6620.
</p>
<p>*Solomon, N. W. (2006). <em>Qualitative Analysis of Media Language</em>. India.
</p>
<p>Spache, G. (1953). &quot;A new readability formula for primary-grade reading
materials.&quot; <em>The Elementary School Journal</em>, 53, 410&ndash;413.
<code style="white-space: pre;">&#8288;https://www.jstor.org/stable/998915&#8288;</code>
</p>
<p>Trnkle, U. &amp; Bailer, H. (1984). Kreuzvalidierung und Neuberechnung von
Lesbarkeitsformeln fr die deutsche Sprache. <em>Zeitschrift fr
Entwicklungspsychologie und Pdagogische Psychologie</em>, 16(3), 231&ndash;244.
</p>
<p>Wheeler, L.R. &amp; Smith, E.H. (1954). A Practical Readability Formula for the
Classroom Teacher in the Primary Grades. <em>Elementary English</em>, 31,
397&ndash;399.  <code style="white-space: pre;">&#8288;https://www.jstor.org/stable/41384251&#8288;</code>
</p>
<p>*Nimaldasan is the pen name of N. Watson Solomon, Assistant Professor of
Journalism, School of Media Studies, SRM University, India.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>txt &lt;- c(doc1 = "Readability zero one. Ten, Eleven.",
         doc2 = "The cat in a dilapidated tophat.")
textstat_readability(txt, measure = "Flesch")
textstat_readability(txt, measure = c("FOG", "FOG.PSK", "FOG.NRI"))

textstat_readability(quanteda::data_corpus_inaugural[48:58],
                     measure = c("Flesch.Kincaid", "Dale.Chall.old"))
</code></pre>

<hr>
<h2 id='textstat_select'>Select rows of textstat objects by glob, regex or fixed patterns</h2><span id='topic+textstat_select'></span>

<h3>Description</h3>

<p>Users can subset output object of <code>textstat_collocations</code>,
<code>textstat_keyness</code> or <code>textstat_frequency</code> based on
<code>"glob"</code>, <code>"regex"</code> or <code>"fixed"</code> patterns using this method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textstat_select(
  x,
  pattern = NULL,
  selection = c("keep", "remove"),
  valuetype = c("glob", "regex", "fixed"),
  case_insensitive = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textstat_select_+3A_x">x</code></td>
<td>
<p>a <code>textstat</code> object</p>
</td></tr>
<tr><td><code id="textstat_select_+3A_pattern">pattern</code></td>
<td>
<p>see <a href="quanteda.html#topic+pattern">quanteda::pattern</a></p>
</td></tr>
<tr><td><code id="textstat_select_+3A_selection">selection</code></td>
<td>
<p>whether to <code>"keep"</code> or <code>"remove"</code> the rows that
match the pattern</p>
</td></tr>
<tr><td><code id="textstat_select_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <a href="quanteda.html#topic+valuetype">valuetype</a> for details.</p>
</td></tr>
<tr><td><code id="textstat_select_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>logical; if <code>TRUE</code>, ignore case when matching a
<code>pattern</code> or <a href="quanteda.html#topic+dictionary">dictionary</a> values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library("quanteda")

period &lt;- ifelse(docvars(data_corpus_inaugural, "Year") &lt; 1945, "pre-war", "post-war")
dfmat &lt;- tokens(data_corpus_inaugural) %&gt;%
    dfm() %&gt;%
    dfm_group(groups = period)
tstat &lt;- textstat_keyness(dfmat)
textstat_select(tstat, 'america*')

</code></pre>

<hr>
<h2 id='textstat_simil'>Similarity and distance computation between documents or features</h2><span id='topic+textstat_simil'></span><span id='topic+textstat_dist'></span>

<h3>Description</h3>

<p>These functions compute matrixes of distances and similarities between
documents or features from a <code><a href="quanteda.html#topic+dfm">dfm()</a></code> and return a matrix of
similarities or distances in a sparse format.  These methods are fast
and robust because they operate directly on the sparse <a href="quanteda.html#topic+dfm">dfm</a> objects.
The output can easily be coerced to an ordinary matrix, a data.frame of
pairwise comparisons, or a <a href="stats.html#topic+dist">dist</a> format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textstat_simil(
  x,
  y = NULL,
  selection = NULL,
  margin = c("documents", "features"),
  method = c("correlation", "cosine", "jaccard", "ejaccard", "dice", "edice", "hamann",
    "simple matching"),
  min_simil = NULL,
  ...
)

textstat_dist(
  x,
  y = NULL,
  selection = NULL,
  margin = c("documents", "features"),
  method = c("euclidean", "manhattan", "maximum", "canberra", "minkowski"),
  p = 2,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textstat_simil_+3A_x">x</code>, <code id="textstat_simil_+3A_y">y</code></td>
<td>
<p>a <a href="quanteda.html#topic+dfm">dfm</a> objects; <code>y</code> is an optional target matrix matching
<code>x</code> in the margin on which the similarity or distance will be computed.</p>
</td></tr>
<tr><td><code id="textstat_simil_+3A_selection">selection</code></td>
<td>
<p>(deprecated - use <code>y</code> instead).</p>
</td></tr>
<tr><td><code id="textstat_simil_+3A_margin">margin</code></td>
<td>
<p>identifies the margin of the dfm on which similarity or
difference will be computed:  <code>"documents"</code> for documents or
<code>"features"</code> for word/term features.</p>
</td></tr>
<tr><td><code id="textstat_simil_+3A_method">method</code></td>
<td>
<p>character; the method identifying the similarity or distance
measure to be used; see Details.</p>
</td></tr>
<tr><td><code id="textstat_simil_+3A_min_simil">min_simil</code></td>
<td>
<p>numeric; a threshold for the similarity values below which similarity
values will not be returned</p>
</td></tr>
<tr><td><code id="textstat_simil_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
<tr><td><code id="textstat_simil_+3A_p">p</code></td>
<td>
<p>The power of the Minkowski distance.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>textstat_simil</code> options are: <code>"correlation"</code> (default),
<code>"cosine"</code>, <code>"jaccard"</code>, <code>"ejaccard"</code>, <code>"dice"</code>,
<code>"edice"</code>, <code>"simple matching"</code>, and <code>"hamann"</code>.
</p>
<p><code>textstat_dist</code> options are: <code>"euclidean"</code> (default),
<code>"manhattan"</code>, <code>"maximum"</code>, <code>"canberra"</code>,
and <code>"minkowski"</code>.
</p>


<h3>Value</h3>

<p>A sparse matrix from the <span class="pkg">Matrix</span> package that will be symmetric
unless <code>y</code> is specified.
</p>


<h3>Conversion to other data types</h3>

<p>The output objects from <code>textstat_simil()</code> and <code>textstat_dist()</code> can be
transformed easily into a list format using
<code><a href="#topic+as.list.textstat_proxy">as.list()</a></code>, which returns a list for each unique
element of the second of the pairs, a data.frame using
<code><a href="#topic+as.data.frame.textstat_proxy">as.data.frame()</a></code>, which returns pairwise
scores, <code>as.dist()</code>for a <a href="stats.html#topic+dist">dist</a> object,
or <code>as.matrix()</code> to convert it into an ordinary matrix.
</p>


<h3>Note</h3>

<p>If you want to compute similarity on a &quot;normalized&quot; dfm object
(controlling for variable document lengths, for methods such as correlation
for which different document lengths matter), then wrap the input dfm in
<code style="white-space: pre;">&#8288;[dfm_weight](x, "prop")&#8288;</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+as.list.textstat_proxy">as.list.textstat_proxy()</a></code>, <code><a href="#topic+as.data.frame.textstat_proxy">as.data.frame.textstat_proxy()</a></code>,
<code><a href="stats.html#topic+dist">stats::as.dist()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># similarities for documents
library("quanteda")
dfmat &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 2000) %&gt;%
    tokens(remove_punct = TRUE) %&gt;%
    tokens_remove(stopwords("english")) %&gt;%
    dfm()
(tstat1 &lt;- textstat_simil(dfmat, method = "cosine", margin = "documents"))
as.matrix(tstat1)
as.list(tstat1)
as.list(tstat1, diag = TRUE)

# min_simil
(tstat2 &lt;- textstat_simil(dfmat, method = "cosine", margin = "documents", min_simil = 0.6))
as.matrix(tstat2)

# similarities for for specific documents
textstat_simil(dfmat, dfmat["2017-Trump", ], margin = "documents")
textstat_simil(dfmat, dfmat["2017-Trump", ], method = "cosine", margin = "documents")
textstat_simil(dfmat, dfmat[c("2009-Obama", "2013-Obama"), ], margin = "documents")

# compute some term similarities
tstat3 &lt;- textstat_simil(dfmat, dfmat[, c("fair", "health", "terror")], method = "cosine",
                         margin = "features")
head(as.matrix(tstat3), 10)
as.list(tstat3, n = 6)


# distances for documents
(tstat4 &lt;- textstat_dist(dfmat, margin = "documents"))
as.matrix(tstat4)
as.list(tstat4)
as.dist(tstat4)

# distances for specific documents
textstat_dist(dfmat, dfmat["2017-Trump", ], margin = "documents")
(tstat5 &lt;- textstat_dist(dfmat, dfmat[c("2009-Obama" , "2013-Obama"), ], margin = "documents"))
as.matrix(tstat5)
as.list(tstat5)

## Not run: 
# plot a dendrogram after converting the object into distances
plot(hclust(as.dist(tstat4)))

## End(Not run)
</code></pre>

<hr>
<h2 id='textstat_summary'>Summarize documents as syntactic and lexical feature counts</h2><span id='topic+textstat_summary'></span>

<h3>Description</h3>

<p>Count syntactic and lexical features of documents such as tokens, types,
sentences, and character categories.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textstat_summary(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textstat_summary_+3A_x">x</code></td>
<td>
<p>corpus to be summarized</p>
</td></tr>
<tr><td><code id="textstat_summary_+3A_...">...</code></td>
<td>
<p>additional arguments passed through to <code><a href="quanteda.html#topic+dfm">dfm()</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Count the total number of characters, tokens and sentences as well as special
tokens such as numbers, punctuation marks, symbols, tags and emojis.
</p>

<ul>
<li><p> chars = number of characters; equal to <code><a href="base.html#topic+nchar">nchar()</a></code>
</p>
</li>
<li><p> sents
= number of sentences; equal <code>ntoken(tokens(x), what = "sentence")</code>
</p>
</li>
<li>
<p>tokens = number of tokens; equal to <code><a href="quanteda.html#topic+ntoken">ntoken()</a></code>
</p>
</li>
<li><p> types = number of unique tokens; equal to <code><a href="quanteda.html#topic+ntype">ntype()</a></code>
</p>
</li>
<li><p> puncts = number of punctuation marks (<code style="white-space: pre;">&#8288;^\p{P}+$&#8288;</code>)
</p>
</li>
<li><p> numbers = number of numeric tokens
(<code style="white-space: pre;">&#8288;^\p{Sc}{0,1}\p{N}+([.,]*\p{N})*\p{Sc}{0,1}$&#8288;</code>)
</p>
</li>
<li><p> symbols = number of symbols (<code style="white-space: pre;">&#8288;^\p{S}$&#8288;</code>)
</p>
</li>
<li><p> tags = number of tags; sum of <code>pattern_username</code> and <code>pattern_hashtag</code>
in <code><a href="quanteda.html#topic+quanteda_options">quanteda::quanteda_options()</a></code>
</p>
</li>
<li><p> emojis = number of emojis (<code style="white-space: pre;">&#8288;^\p{Emoji_Presentation}+$&#8288;</code>)
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (Sys.info()["sysname"] != "SunOS") {
library("quanteda")
corp &lt;- data_corpus_inaugural[1:5]
textstat_summary(corp)
toks &lt;- tokens(corp)
textstat_summary(toks)
dfmat &lt;- dfm(toks)
textstat_summary(dfmat)
}
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
