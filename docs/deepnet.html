<!DOCTYPE html><html><head><title>Help for package deepnet</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {deepnet}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#dbn.dnn.train'><p>Training a Deep neural network with weights initialized by DBN</p></a></li>
<li><a href='#load.mnist'><p>Load MNIST DataSet</p></a></li>
<li><a href='#nn.predict'><p>Predict new samples by Trainded NN</p></a></li>
<li><a href='#nn.test'><p>Test new samples by Trainded NN</p></a></li>
<li><a href='#nn.train'><p>Training Neural Network</p></a></li>
<li><a href='#rbm.down'><p>Generate visible vector by hidden units states</p></a></li>
<li><a href='#rbm.train'><p>Training a RBM(restricted Boltzmann Machine)</p></a></li>
<li><a href='#rbm.up'><p>Infer hidden units state by visible units</p></a></li>
<li><a href='#sae.dnn.train'><p>Training a Deep neural network with weights initialized by Stacked AutoEncoder</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Deep Learning Toolkit in R</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2014-03-20</td>
</tr>
<tr>
<td>Author:</td>
<td>Xiao Rong</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Xiao Rong &lt;runxiao@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implement some deep learning architectures and neural network
    algorithms, including BP,RBM,DBN,Deep autoencoder and so on.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL]</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-06-24 12:10:21 UTC; hornik</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-06-24 12:29:27 UTC</td>
</tr>
</table>
<hr>
<h2 id='dbn.dnn.train'>Training a Deep neural network with weights initialized by DBN</h2><span id='topic+dbn.dnn.train'></span>

<h3>Description</h3>

<p>Training a Deep neural network with weights initialized by
DBN
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dbn.dnn.train(x, y, hidden = c(1), activationfun = "sigm", learningrate = 0.8, 
    momentum = 0.5, learningrate_scale = 1, output = "sigm", numepochs = 3, 
    batchsize = 100, hidden_dropout = 0, visible_dropout = 0, cd = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dbn.dnn.train_+3A_x">x</code></td>
<td>
<p>matrix of x values for examples</p>
</td></tr>
<tr><td><code id="dbn.dnn.train_+3A_y">y</code></td>
<td>
<p>vector or matrix of target values for examples</p>
</td></tr>
<tr><td><code id="dbn.dnn.train_+3A_hidden">hidden</code></td>
<td>
<p>vector for number of units of hidden
layers.Default is c(10).</p>
</td></tr>
<tr><td><code id="dbn.dnn.train_+3A_activationfun">activationfun</code></td>
<td>
<p>activation function of hidden
unit.Can be &quot;sigm&quot;,&quot;linear&quot; or &quot;tanh&quot;.Default is &quot;sigm&quot;
for logistic function</p>
</td></tr>
<tr><td><code id="dbn.dnn.train_+3A_learningrate">learningrate</code></td>
<td>
<p>learning rate for gradient descent.
Default is 0.8.</p>
</td></tr>
<tr><td><code id="dbn.dnn.train_+3A_momentum">momentum</code></td>
<td>
<p>momentum for gradient descent. Default is
0.5 .</p>
</td></tr>
<tr><td><code id="dbn.dnn.train_+3A_learningrate_scale">learningrate_scale</code></td>
<td>
<p>learning rate will be mutiplied
by this scale after every iteration. Default is 1 .</p>
</td></tr>
<tr><td><code id="dbn.dnn.train_+3A_numepochs">numepochs</code></td>
<td>
<p>number of iteration for samples Default
is 3.</p>
</td></tr>
<tr><td><code id="dbn.dnn.train_+3A_batchsize">batchsize</code></td>
<td>
<p>size of mini-batch. Default is 100.</p>
</td></tr>
<tr><td><code id="dbn.dnn.train_+3A_output">output</code></td>
<td>
<p>function of output unit, can be
&quot;sigm&quot;,&quot;linear&quot; or &quot;softmax&quot;. Default is &quot;sigm&quot;.</p>
</td></tr>
<tr><td><code id="dbn.dnn.train_+3A_hidden_dropout">hidden_dropout</code></td>
<td>
<p>drop out fraction for hidden layer.
Default is 0.</p>
</td></tr>
<tr><td><code id="dbn.dnn.train_+3A_visible_dropout">visible_dropout</code></td>
<td>
<p>drop out fraction for input layer
Default is 0.</p>
</td></tr>
<tr><td><code id="dbn.dnn.train_+3A_cd">cd</code></td>
<td>
<p>number of iteration for Gibbs sample of CD
algorithm.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Xiao Rong
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Var1 &lt;- c(rnorm(50, 1, 0.5), rnorm(50, -0.6, 0.2))
Var2 &lt;- c(rnorm(50, -0.8, 0.2), rnorm(50, 2, 1))
x &lt;- matrix(c(Var1, Var2), nrow = 100, ncol = 2)
y &lt;- c(rep(1, 50), rep(0, 50))
dnn &lt;- dbn.dnn.train(x, y, hidden = c(5, 5))
## predict by dnn
test_Var1 &lt;- c(rnorm(50, 1, 0.5), rnorm(50, -0.6, 0.2))
test_Var2 &lt;- c(rnorm(50, -0.8, 0.2), rnorm(50, 2, 1))
test_x &lt;- matrix(c(test_Var1, test_Var2), nrow = 100, ncol = 2)
nn.test(dnn, test_x, y)
</code></pre>

<hr>
<h2 id='load.mnist'>Load MNIST DataSet</h2><span id='topic+load.mnist'></span>

<h3>Description</h3>

<p>Load MNIST DataSet
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load.mnist(dir)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="load.mnist_+3A_dir">dir</code></td>
<td>
<p>dir of minst dataset</p>
</td></tr>
</table>


<h3>Value</h3>

<p>mnist dataset train$n number of train samples train$x pix
of every train sample image train$y label of every train
sample image train$yy one-of-c vector of label of train
sample image test$n number of test samples test$x pix of
every test sample image test$y label of every test sample
image test$yy one-of-c vector of label of test sample image
</p>


<h3>Author(s)</h3>

<p>Xiao Rong
</p>

<hr>
<h2 id='nn.predict'>Predict new samples by Trainded NN</h2><span id='topic+nn.predict'></span>

<h3>Description</h3>

<p>Predict new samples by Trainded NN
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn.predict(nn, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn.predict_+3A_nn">nn</code></td>
<td>
<p>nerual network trained by function nn.train</p>
</td></tr>
<tr><td><code id="nn.predict_+3A_x">x</code></td>
<td>
<p>new samples to predict</p>
</td></tr>
</table>


<h3>Value</h3>

<p>return raw output value of neural network.For
classification task,return the probability of a class
</p>


<h3>Author(s)</h3>

<p>Xiao Rong
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Var1 &lt;- c(rnorm(50, 1, 0.5), rnorm(50, -0.6, 0.2))
Var2 &lt;- c(rnorm(50, -0.8, 0.2), rnorm(50, 2, 1))
x &lt;- matrix(c(Var1, Var2), nrow = 100, ncol = 2)
y &lt;- c(rep(1, 50), rep(0, 50))
nn &lt;- nn.train(x, y, hidden = c(5))
## predict by nn
test_Var1 &lt;- c(rnorm(50, 1, 0.5), rnorm(50, -0.6, 0.2))
test_Var2 &lt;- c(rnorm(50, -0.8, 0.2), rnorm(50, 2, 1))
test_x &lt;- matrix(c(test_Var1, test_Var2), nrow = 100, ncol = 2)
yy &lt;- nn.predict(nn, test_x)
</code></pre>

<hr>
<h2 id='nn.test'>Test new samples by Trainded NN</h2><span id='topic+nn.test'></span>

<h3>Description</h3>

<p>Test new samples by Trainded NN,return error rate for
classification
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn.test(nn, x, y, t = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn.test_+3A_nn">nn</code></td>
<td>
<p>nerual network trained by function nn.train</p>
</td></tr>
<tr><td><code id="nn.test_+3A_x">x</code></td>
<td>
<p>new samples to predict</p>
</td></tr>
<tr><td><code id="nn.test_+3A_y">y</code></td>
<td>
<p>new samples' label</p>
</td></tr>
<tr><td><code id="nn.test_+3A_t">t</code></td>
<td>
<p>threshold for classification. If nn.predict
value &gt;= t then label 1,else label 0</p>
</td></tr>
</table>


<h3>Value</h3>

<p>error rate
</p>


<h3>Author(s)</h3>

<p>Xiao Rong
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Var1 &lt;- c(rnorm(50, 1, 0.5), rnorm(50, -0.6, 0.2))
Var2 &lt;- c(rnorm(50, -0.8, 0.2), rnorm(50, 2, 1))
x &lt;- matrix(c(Var1, Var2), nrow = 100, ncol = 2)
y &lt;- c(rep(1, 50), rep(0, 50))
nn &lt;- nn.train(x, y, hidden = c(5))
test_Var1 &lt;- c(rnorm(50, 1, 0.5), rnorm(50, -0.6, 0.2))
test_Var2 &lt;- c(rnorm(50, -0.8, 0.2), rnorm(50, 2, 1))
test_x &lt;- matrix(c(test_Var1, test_Var2), nrow = 100, ncol = 2)
err &lt;- nn.test(nn, test_x, y)
</code></pre>

<hr>
<h2 id='nn.train'>Training Neural Network</h2><span id='topic+nn.train'></span>

<h3>Description</h3>

<p>Training single or mutiple hidden layers neural network by
BP
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn.train(x, y, initW = NULL, initB = NULL, hidden = c(10), activationfun = "sigm", 
    learningrate = 0.8, momentum = 0.5, learningrate_scale = 1, output = "sigm", 
    numepochs = 3, batchsize = 100, hidden_dropout = 0, visible_dropout = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nn.train_+3A_x">x</code></td>
<td>
<p>matrix of x values for examples</p>
</td></tr>
<tr><td><code id="nn.train_+3A_y">y</code></td>
<td>
<p>vector or matrix of target values for examples</p>
</td></tr>
<tr><td><code id="nn.train_+3A_initw">initW</code></td>
<td>
<p>initial weights. If missing chosen at
random</p>
</td></tr>
<tr><td><code id="nn.train_+3A_initb">initB</code></td>
<td>
<p>initial bias. If missing chosen at random</p>
</td></tr>
<tr><td><code id="nn.train_+3A_hidden">hidden</code></td>
<td>
<p>vector for number of units of hidden
layers.Default is c(10).</p>
</td></tr>
<tr><td><code id="nn.train_+3A_activationfun">activationfun</code></td>
<td>
<p>activation function of hidden
unit.Can be &quot;sigm&quot;,&quot;linear&quot; or &quot;tanh&quot;.Default is &quot;sigm&quot;
for logistic function</p>
</td></tr>
<tr><td><code id="nn.train_+3A_learningrate">learningrate</code></td>
<td>
<p>learning rate for gradient descent.
Default is 0.8.</p>
</td></tr>
<tr><td><code id="nn.train_+3A_momentum">momentum</code></td>
<td>
<p>momentum for gradient descent. Default is
0.5 .</p>
</td></tr>
<tr><td><code id="nn.train_+3A_learningrate_scale">learningrate_scale</code></td>
<td>
<p>learning rate will be mutiplied
by this scale after every iteration. Default is 1 .</p>
</td></tr>
<tr><td><code id="nn.train_+3A_numepochs">numepochs</code></td>
<td>
<p>number of iteration for samples Default
is 3.</p>
</td></tr>
<tr><td><code id="nn.train_+3A_batchsize">batchsize</code></td>
<td>
<p>size of mini-batch. Default is 100.</p>
</td></tr>
<tr><td><code id="nn.train_+3A_output">output</code></td>
<td>
<p>function of output unit, can be
&quot;sigm&quot;,&quot;linear&quot; or &quot;softmax&quot;. Default is &quot;sigm&quot;.</p>
</td></tr>
<tr><td><code id="nn.train_+3A_hidden_dropout">hidden_dropout</code></td>
<td>
<p>drop out fraction for hidden layer.
Default is 0.</p>
</td></tr>
<tr><td><code id="nn.train_+3A_visible_dropout">visible_dropout</code></td>
<td>
<p>drop out fraction for input layer
Default is 0.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Xiao Rong
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Var1 &lt;- c(rnorm(50, 1, 0.5), rnorm(50, -0.6, 0.2))
Var2 &lt;- c(rnorm(50, -0.8, 0.2), rnorm(50, 2, 1))
x &lt;- matrix(c(Var1, Var2), nrow = 100, ncol = 2)
y &lt;- c(rep(1, 50), rep(0, 50))
nn &lt;- nn.train(x, y, hidden = c(5))
</code></pre>

<hr>
<h2 id='rbm.down'>Generate visible vector by hidden units states</h2><span id='topic+rbm.down'></span>

<h3>Description</h3>

<p>Generate visible vector by hidden units states
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbm.down(rbm, h)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rbm.down_+3A_rbm">rbm</code></td>
<td>
<p>an rbm object trained by function train.rbm</p>
</td></tr>
<tr><td><code id="rbm.down_+3A_h">h</code></td>
<td>
<p>hidden units states</p>
</td></tr>
</table>


<h3>Value</h3>

<p>generated visible vector
</p>


<h3>Author(s)</h3>

<p>Xiao Rong
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Var1 &lt;- c(rep(1, 50), rep(0, 50))
Var2 &lt;- c(rep(0, 50), rep(1, 50))
x3 &lt;- matrix(c(Var1, Var2), nrow = 100, ncol = 2)
r1 &lt;- rbm.train(x3, 3, numepochs = 20, cd = 10)
h &lt;- c(0.2, 0.8, 0.1)
v &lt;- rbm.down(r1, h)
</code></pre>

<hr>
<h2 id='rbm.train'>Training a RBM(restricted Boltzmann Machine)</h2><span id='topic+rbm.train'></span>

<h3>Description</h3>

<p>Training a RBM(restricted Boltzmann Machine)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbm.train(x, hidden, numepochs = 3, batchsize = 100, learningrate = 0.8, 
    learningrate_scale = 1, momentum = 0.5, visible_type = "bin", hidden_type = "bin", 
    cd = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rbm.train_+3A_x">x</code></td>
<td>
<p>matrix of x values for examples</p>
</td></tr>
<tr><td><code id="rbm.train_+3A_hidden">hidden</code></td>
<td>
<p>number of hidden units</p>
</td></tr>
<tr><td><code id="rbm.train_+3A_visible_type">visible_type</code></td>
<td>
<p>activation function of input
unit.Only support &quot;sigm&quot; now</p>
</td></tr>
<tr><td><code id="rbm.train_+3A_hidden_type">hidden_type</code></td>
<td>
<p>activation function of hidden
unit.Only support &quot;sigm&quot; now</p>
</td></tr>
<tr><td><code id="rbm.train_+3A_learningrate">learningrate</code></td>
<td>
<p>learning rate for gradient descent.
Default is 0.8.</p>
</td></tr>
<tr><td><code id="rbm.train_+3A_momentum">momentum</code></td>
<td>
<p>momentum for gradient descent. Default is
0.5 .</p>
</td></tr>
<tr><td><code id="rbm.train_+3A_learningrate_scale">learningrate_scale</code></td>
<td>
<p>learning rate will be mutiplied
by this scale after every iteration. Default is 1 .</p>
</td></tr>
<tr><td><code id="rbm.train_+3A_numepochs">numepochs</code></td>
<td>
<p>number of iteration for samples Default
is 3.</p>
</td></tr>
<tr><td><code id="rbm.train_+3A_batchsize">batchsize</code></td>
<td>
<p>size of mini-batch. Default is 100.</p>
</td></tr>
<tr><td><code id="rbm.train_+3A_cd">cd</code></td>
<td>
<p>number of iteration for Gibbs sample of CD
algorithm.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Xiao Rong
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Var1 &lt;- c(rep(1, 50), rep(0, 50))
Var2 &lt;- c(rep(0, 50), rep(1, 50))
x3 &lt;- matrix(c(Var1, Var2), nrow = 100, ncol = 2)
r1 &lt;- rbm.train(x3, 10, numepochs = 20, cd = 10)
</code></pre>

<hr>
<h2 id='rbm.up'>Infer hidden units state by visible units</h2><span id='topic+rbm.up'></span>

<h3>Description</h3>

<p>Infer hidden units states by visible units
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbm.up(rbm, v)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rbm.up_+3A_rbm">rbm</code></td>
<td>
<p>an rbm object trained by function train.rbm</p>
</td></tr>
<tr><td><code id="rbm.up_+3A_v">v</code></td>
<td>
<p>visible units states</p>
</td></tr>
</table>


<h3>Value</h3>

<p>hidden units states
</p>


<h3>Author(s)</h3>

<p>Xiao Rong
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Var1 &lt;- c(rep(1, 50), rep(0, 50))
Var2 &lt;- c(rep(0, 50), rep(1, 50))
x3 &lt;- matrix(c(Var1, Var2), nrow = 100, ncol = 2)
r1 &lt;- rbm.train(x3, 3, numepochs = 20, cd = 10)
v &lt;- c(0.2, 0.8)
h &lt;- rbm.up(r1, v)
</code></pre>

<hr>
<h2 id='sae.dnn.train'>Training a Deep neural network with weights initialized by Stacked AutoEncoder</h2><span id='topic+sae.dnn.train'></span>

<h3>Description</h3>

<p>Training a Deep neural network with weights initialized by
Stacked AutoEncoder
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sae.dnn.train(x, y, hidden = c(1), activationfun = "sigm", learningrate = 0.8, 
    momentum = 0.5, learningrate_scale = 1, output = "sigm", sae_output = "linear", 
    numepochs = 3, batchsize = 100, hidden_dropout = 0, visible_dropout = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sae.dnn.train_+3A_x">x</code></td>
<td>
<p>matrix of x values for examples</p>
</td></tr>
<tr><td><code id="sae.dnn.train_+3A_y">y</code></td>
<td>
<p>vector or matrix of target values for examples</p>
</td></tr>
<tr><td><code id="sae.dnn.train_+3A_hidden">hidden</code></td>
<td>
<p>vector for number of units of hidden
layers.Default is c(10).</p>
</td></tr>
<tr><td><code id="sae.dnn.train_+3A_activationfun">activationfun</code></td>
<td>
<p>activation function of hidden
unit.Can be &quot;sigm&quot;,&quot;linear&quot; or &quot;tanh&quot;.Default is &quot;sigm&quot;
for logistic function</p>
</td></tr>
<tr><td><code id="sae.dnn.train_+3A_learningrate">learningrate</code></td>
<td>
<p>learning rate for gradient descent.
Default is 0.8.</p>
</td></tr>
<tr><td><code id="sae.dnn.train_+3A_momentum">momentum</code></td>
<td>
<p>momentum for gradient descent. Default is
0.5 .</p>
</td></tr>
<tr><td><code id="sae.dnn.train_+3A_learningrate_scale">learningrate_scale</code></td>
<td>
<p>learning rate will be mutiplied
by this scale after every iteration. Default is 1 .</p>
</td></tr>
<tr><td><code id="sae.dnn.train_+3A_numepochs">numepochs</code></td>
<td>
<p>number of iteration for samples Default
is 3.</p>
</td></tr>
<tr><td><code id="sae.dnn.train_+3A_batchsize">batchsize</code></td>
<td>
<p>size of mini-batch. Default is 100.</p>
</td></tr>
<tr><td><code id="sae.dnn.train_+3A_output">output</code></td>
<td>
<p>function of output unit, can be
&quot;sigm&quot;,&quot;linear&quot; or &quot;softmax&quot;. Default is &quot;sigm&quot;.</p>
</td></tr>
<tr><td><code id="sae.dnn.train_+3A_sae_output">sae_output</code></td>
<td>
<p>function of autoencoder output unit,
can be &quot;sigm&quot;,&quot;linear&quot; or &quot;softmax&quot;. Default is
&quot;linear&quot;.</p>
</td></tr>
<tr><td><code id="sae.dnn.train_+3A_hidden_dropout">hidden_dropout</code></td>
<td>
<p>drop out fraction for hidden layer.
Default is 0.</p>
</td></tr>
<tr><td><code id="sae.dnn.train_+3A_visible_dropout">visible_dropout</code></td>
<td>
<p>drop out fraction for input layer
Default is 0.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Xiao Rong
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Var1 &lt;- c(rnorm(50, 1, 0.5), rnorm(50, -0.6, 0.2))
Var2 &lt;- c(rnorm(50, -0.8, 0.2), rnorm(50, 2, 1))
x &lt;- matrix(c(Var1, Var2), nrow = 100, ncol = 2)
y &lt;- c(rep(1, 50), rep(0, 50))
dnn &lt;- sae.dnn.train(x, y, hidden = c(5, 5))
## predict by dnn
test_Var1 &lt;- c(rnorm(50, 1, 0.5), rnorm(50, -0.6, 0.2))
test_Var2 &lt;- c(rnorm(50, -0.8, 0.2), rnorm(50, 2, 1))
test_x &lt;- matrix(c(test_Var1, test_Var2), nrow = 100, ncol = 2)
nn.test(dnn, test_x, y)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
