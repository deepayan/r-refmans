<!DOCTYPE html><html><head><title>Help for package LSAfun</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {LSAfun}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#analogy'><p>Analogy</p></a></li>
<li><a href='#asym'><p>Asymmetric Similarity functions</p></a></li>
<li><a href='#choose.target'><p>Random Target Selection</p></a></li>
<li><a href='#coherence'><p>Coherence of a text</p></a></li>
<li><a href='#compose'><p>Two-Word Composition</p></a></li>
<li><a href='#conSIM'><p>Similarity in Context</p></a></li>
<li><a href='#Cosine'><p>Compute cosine similarity</p></a></li>
<li><a href='#costring'><p>Sentence Comparison</p></a></li>
<li><a href='#distance'><p>Compute distance</p></a></li>
<li><a href='#genericSummary'><p>Summarize a text</p></a></li>
<li><a href='#LSAfun-package'><p>Computations based on Latent Semantic Analysis</p></a></li>
<li><a href='#multicos'><p>Vector x Vector Comparison</p></a></li>
<li><a href='#multicostring'><p>Sentence x Vector Comparison</p></a></li>
<li><a href='#multidocs'><p>Comparison of sentence sets</p></a></li>
<li><a href='#MultipleChoice'><p>Answers Multiple Choice Questions</p></a></li>
<li><a href='#neighbors'><p>Find nearest neighbors</p></a></li>
<li><a href='#normalize'><p>Normalize a vector</p></a></li>
<li><a href='#oldbooks'><p>A collection of five classic books</p></a></li>
<li><a href='#pairwise'><p>Pairwise cosine computation</p></a></li>
<li><a href='#plausibility'><p>Compute word (or compound) plausibility</p></a></li>
<li><a href='#plot_doclist'><p>2D- or 3D-Plot of a list of sentences/documents</p></a></li>
<li><a href='#plot_neighbors'><p>2D- or 3D-Plot of neighbors</p></a></li>
<li><a href='#plot_wordlist'><p>2D- or 3D-Plot of a list of words</p></a></li>
<li><a href='#Predication'><p>Compute Vector for Predicate-Argument-Expressions</p></a></li>
<li><a href='#priming'><p>Simulated data for a Semantic Priming Experiment</p></a></li>
<li><a href='#SND'><p>Semantic neighborhood density</p></a></li>
<li><a href='#syntest'><p>A multiple choice test for synonyms and antonyms</p></a></li>
<li><a href='#wonderland'><p>LSA Space: Alice's Adventures in Wonderland</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Applied Latent Semantic Analysis (LSA) Functions</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides functions that allow for convenient working with vector space models of semantics/distributional semantic models/word embeddings. 
	Originally built for LSA models (hence the name), but can be used for all such vector-based models. 
	For actually building a vector semantic space, use the package 'lsa' or other specialized software. 
	Downloadable semantic spaces can be found at <a href="https://sites.google.com/site/fritzgntr/software-resources">https://sites.google.com/site/fritzgntr/software-resources</a>.</td>
</tr>
<tr>
<td>Version:</td>
<td>0.7.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-11-08</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.0), lsa, rgl</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-17 16:08:07 UTC; fritz</td>
</tr>
<tr>
<td>Author:</td>
<td>Fritz Guenther [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Fritz Guenther &lt;fritz.guenther@uni-tuebingen.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-17 17:40:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='analogy'>Analogy</h2><span id='topic+analogy'></span>

<h3>Description</h3>

<p>Implements the <em>king - man + woman = queen</em> analogy solving algorithm</p>


<h3>Usage</h3>

<pre><code class='language-R'>analogy(x1,x2,y1=NA,n,tvectors=tvectors)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="analogy_+3A_x1">x1</code></td>
<td>
<p>a character vector specifying the first word of the first pair (<em>man</em> in <em>man : king = woman : ?</em>)</p>
</td></tr>
<tr><td><code id="analogy_+3A_x2">x2</code></td>
<td>
<p>a character vector specifying the second word of the first pair (<em>king</em> in <em>man : king = woman : ?</em>)</p>
</td></tr>
<tr><td><code id="analogy_+3A_y1">y1</code></td>
<td>
<p>a character vector specifying the first word of the second pair (<em>woman</em> in <em>man : king = woman : ?</em>)</p>
</td></tr>
<tr><td><code id="analogy_+3A_n">n</code></td>
<td>
<p>the number of neighbors to be computed</p>
</td></tr>
<tr><td><code id="analogy_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The analogy task is a popular benchmark for vector space models of meaning/word embeddings.
It is based on the rationale that proportinal analogies <em>x1 is to x2 as y1 is to y2</em>, like <em>man : king = woman : ?</em>  (correct answer: <em>queen</em>), can be solved via the following operation on the respective word vectors (all normalized to unit norm) <code>king - man + woman = queen</code> (that is, the nearest vector to <code>king - man + woman</code> should be <code>queen</code>) (Mikolov et al., 2013).
</p>
<p>The <code>analogy()</code> function comes in two variants, taking as input either three words (<code>x1</code>, <code>x2</code>, and <code>y1</code>) or two words (<code>x1</code> and <code>x2</code>)
</p>

<ul>
<li><p> The variant with three input words (<code>x1</code>, <code>x2</code>, and <code>y1</code>) implements the standard analogy solving algorithm for analogies of the type <code>x1 : x2 = y1 : ?</code>, searching the <code>n</code> nearest neighbors for <code>x2 - x1 + y1</code> (all normalized to unit norm) as the best-fitting candidates for <code>y2</code>
</p>
</li>
<li><p> The variant with two input words (<code>x1</code> and <code>x2</code>) only computes the difference between the two vectors (both normalized to unit norm) and the <code>n</code> nearest neighbors to the resulting difference vector 
</p>
</li></ul>



<h3>Value</h3>

<p>Returns a list containing a numeric vector and the nearest neighbors to that vector:
</p>

<ul>
<li><p> In the variant with three input words (<code>x1</code>, <code>x2</code>, and <code>y1</code>), returns:
</p>

<ul>
<li><p><code>y2_vec</code> The result of <code>x2 - x1 + y1</code> (all normalized to unit norm) as a numeric vector 
</p>
</li>
<li><p><code>y2_neighbors</code> A named numeric vector of the <code>n</code> nearest neighbors to <code>y2_vec</code>. The neighbors are given as names of the vector, and their respective cosines to <code>y2_vec</code> as vector entries.
</p>
</li></ul>

</li>
<li><p> In the variant with two input words (<code>x1</code> and <code>x2</code>), returns:
</p>

<ul>
<li><p><code>x_diff_vec</code> The result of <code>x2 - x1</code> (both normalized to unit norm) as a numeric vector 
</p>
</li>
<li><p><code>x_diff_neighbors</code> A named numeric vector of the <code>n</code> nearest neighbors to <code>x_diff_vec</code>. The neighbors are given as names of the vector, and their respective cosines to <code>x_diff_vec</code> as vector entries.
</p>
</li></ul>

</li></ul>



<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>References</h3>

<p>Mikolov, T., Yih, W. T., &amp; Zweig, G. (2013). Linguistic regularities in continuous space word representations. In  <em>Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2013)</em>. Association for Computational Linguistics.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neighbors">neighbors</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)

analogy(x1="hatter",x2="mad",y1="cat",n=10,tvectors=wonderland)

analogy(x1="hatter",x2="mad",n=10,tvectors=wonderland)
</code></pre>

<hr>
<h2 id='asym'>Asymmetric Similarity functions</h2><span id='topic+asym'></span>

<h3>Description</h3>

<p>Compute various asymmetric similarities between words</p>


<h3>Usage</h3>

<pre><code class='language-R'>asym(x,y,method,t=0,tvectors)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="asym_+3A_x">x</code></td>
<td>
<p>A single word, given as a character of <code>length(x) = 1</code></p>
</td></tr>
<tr><td><code id="asym_+3A_y">y</code></td>
<td>
<p>A single word, given as a character of <code>length(y) = 1</code></p>
</td></tr>
<tr><td><code id="asym_+3A_method">method</code></td>
<td>
<p>Specifying the formula to use for asymmetric similarity computation</p>
</td></tr>
<tr><td><code id="asym_+3A_t">t</code></td>
<td>
<p>A numeric threshold a dimension value of the vectors has to exceed so that the dimension is considered <em>active</em>; not needed for the <code>kintsch</code> method</p>
</td></tr>
<tr><td><code id="asym_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Asymmetric (or directional) similarities can be useful e.g. for examining <em>hypernymy</em> (category inclusion), for example the relation between <em>dog</em> and <em>animal</em> should be asymmetrical. The general idea is that, if one word is a hypernym of another (i.e. it is semantically narrower), then a significant number of dimensions that are salient in this word should also be salient in the semantically broader term (Lenci &amp; Benotto, 2012).
</p>
<p>In the formulas below, <code class="reqn">w_x(f)</code> denotes the value of vector <code class="reqn">x</code> on dimension <code class="reqn">f</code>. Furthermore, <code class="reqn">F_x</code> is the set of <em>active</em> dimensions of vector <code class="reqn">x</code>. A dimension <code class="reqn">f</code> is considered active if
<code class="reqn">w_x(f) &gt; t</code>, with <code class="reqn">t</code> being a pre-defined, free parameter.
</p>
<p>The options for <code>method</code> are defined as follows (see Kotlerman et al., 2010) (1)):
</p>

<ul>
<li><p><code>method = "weedsprec"</code> </p>
<p style="text-align: center;"><code class="reqn">weedsprec(u,v) = \frac{\sum\nolimits_{f \in F_u \cap F_v}w_u(f)}{\sum\nolimits_{f \in F_u}w_u(f)}</code>
</p>
      
</li>
<li><p><code>method = "cosweeds"</code> </p>
<p style="text-align: center;"><code class="reqn">cosweeds(u,v) = \sqrt{weedsprec(u,v) \times cosine(u,v)}</code>
</p>
    
</li>
<li><p><code>method = "clarkede"</code> </p>
<p style="text-align: center;"><code class="reqn">clarkede(u,v) = \frac{\sum\nolimits_{f \in F_u \cap F_v}min(w_u(f),w_v(f))}{\sum\nolimits_{f \in F_u}w_u(f)}</code>
</p>
     
</li>
<li><p><code>method = "invcl"</code> </p>
<p style="text-align: center;"><code class="reqn">invcl(u,v) = \sqrt{clarkede(u,v)\times(1-clarkede(u,v)})</code>
</p>
      
</li>
<li><p><code>method = "kintsch"</code>
</p>
<p>Unlike the other  methods, this one is not derived from the logic of hypernymy, but rather from asymmetrical similarities between words due to different amounts of knowledge about them. Here, asymmteric similarities between two words are computed by taking into account the vector length (i.e. the amount of information about those words). This is done by projecting one vector onto the other, and normalizing this resulting vector by dividing its length by the length of the longer of the two vectors (Details in Kintsch, 2014, see References). 
</p>
</li></ul>



<h3>Value</h3>

<p>A numeric giving the asymmetric similarity between <code>x</code> and <code>y</code></p>


<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>References</h3>

<p>Kintsch, W. (2015). Similarity as a Function of Semantic Distance and Amount of Knowledge. <em>Psychological Review, 121,</em> 559-561.
</p>
<p>Kotlerman, L., Dagan, I., Szpektor, I., &amp; Zhitomirsky-Geffet, M (2010). Directional distributional
similarity for lexical inference. <em>Natural Language Engineering, 16,</em> 359-389.
</p>
<p>Lenci, A., &amp; Benotto, G. (2012). Identifying hypernyms in distributional semantic spaces. In <em>Proceedings of *SEM</em> (pp. 75-79), Montreal, Canada.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Cosine">Cosine</a></code>
<code><a href="#topic+conSIM">conSIM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)

asym("alice","girl",method="cosweeds",t=0,tvectors=wonderland)
asym("alice","rabbit",method="cosweeds",tvectors=wonderland)</code></pre>

<hr>
<h2 id='choose.target'>Random Target Selection</h2><span id='topic+choose.target'></span>

<h3>Description</h3>

<p>Randomly samples words within a given similarity range to the input</p>


<h3>Usage</h3>

<pre><code class='language-R'>choose.target(x,lower,upper,n,tvectors=tvectors)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="choose.target_+3A_x">x</code></td>
<td>
<p>a character vector of <code>length(x) = 1</code> specifying a word or a sentence/document</p>
</td></tr>
<tr><td><code id="choose.target_+3A_lower">lower</code></td>
<td>
<p>the lower bound of the similarity range; a numeric</p>
</td></tr>
<tr><td><code id="choose.target_+3A_upper">upper</code></td>
<td>
<p>the upper bound of the similarity range; a numeric</p>
</td></tr>
<tr><td><code id="choose.target_+3A_n">n</code></td>
<td>
<p>an integer giving the number of target words to be sampled</p>
</td></tr>
<tr><td><code id="choose.target_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes cosine values between the input <code>x</code> and all the word vectors in <code>tvectors</code>. Then only selects words with a cosine similarity between <code>lower</code> and <code>upper</code> to the input, and randomly samples <code>n</code> of these words.
</p>
<p>This function is designed for randomly selecting target words with a predefined similarity towards a given prime word (or sentence/document).
</p>


<h3>Value</h3>

<p>A named numeric vector. The names of the vector give the target words, the entries their respective cosine similarity to the input.</p>


<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>References</h3>

<p>Landauer, T.K., &amp; Dumais, S.T. (1997). A solution to Plato's problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge. <em>Psychological Review, 104,</em> 211-240.
</p>


<h3>See Also</h3>

<p><code><a href="lsa.html#topic+cosine">cosine</a></code>,
<code><a href="#topic+Cosine">Cosine</a></code>,
<code><a href="#topic+neighbors">neighbors</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)

choose.target("mad hatter",lower=.2,upper=.3,
                n=20, tvectors=wonderland)</code></pre>

<hr>
<h2 id='coherence'>Coherence of a text</h2><span id='topic+coherence'></span>

<h3>Description</h3>

<p>Computes coherence of a given paragraph/document</p>


<h3>Usage</h3>

<pre><code class='language-R'>coherence(x,split=c(".","!","?"),tvectors=tvectors, remove.punctuation=TRUE, 
stopwords = NULL, method ="Add")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coherence_+3A_x">x</code></td>
<td>
<p>a character vector of <code>length(x) = 1</code> containing the document</p>
</td></tr>
<tr><td><code id="coherence_+3A_split">split</code></td>
<td>
<p>a vector of expressions that determine where to split sentences</p>
</td></tr>
<tr><td><code id="coherence_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
<tr><td><code id="coherence_+3A_remove.punctuation">remove.punctuation</code></td>
<td>
<p>removes punctuation from <code>x</code> <em>after</em> splitting the sentences; <code>TRUE</code> by default</p>
</td></tr>
<tr><td><code id="coherence_+3A_stopwords">stopwords</code></td>
<td>
<p>a character vector defining a list of words that are <em>not</em> used to compute the  sentence vectors for <code>x</code></p>
</td></tr>
<tr><td><code id="coherence_+3A_method">method</code></td>
<td>
<p>the compositional model to compute the document vector from its word vectors. The default option <code>method = "Add"</code> computes the document vector as the vector sum. With <code>method = "Multiply"</code>, the document vector is computed via element-wise multiplication (see <code><a href="#topic+compose">compose</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function applies the method described in Landauer &amp; Dumais (1997): The <em>local coherence</em> is the cosine
between two adjacent sentences. The <em>global coherence</em> is then computed as the mean value of these local
coherences.
</p>
<p>The format of <code>x</code> should be of the kind <code>x &lt;- "sentence1. sentence2. sentence3"</code> Every sentence can also just consist of one single word.
</p>
<p>To import a document Document.txt to from a directory for coherence computation, set your working
directory to this directory using <code>setwd()</code>. Then use the following command lines:
</p>
<p><code>fileName1 &lt;- "Alice_in_Wonderland.txt"</code>
</p>
<p><code>x &lt;- readChar(fileName1, file.info(fileName1)$size)</code>
</p>
<p>In the traditional LSA approach, the vector <em>D</em> for a document (or a sentence) consisting of the words <em>(t1, . , tn)</em> is computed as
</p>
<p style="text-align: center;"><code class="reqn">D = \sum\limits_{i=1}^n t_n</code>
</p>

<p>This is the default method (<code>method="Add"</code>) for this function. Alternatively, this function provided the possibility of computing the document vector from its word vectors using element-wise multiplication (see Mitchell &amp; Lapata, 2010 and <code><a href="#topic+compose">compose</a></code>).
</p>
<p>A note will be displayed whenever not all words of one input string are found in the semantic space. <em><b>Caution:</b></em> In that case, the function will still produce a result, by omitting the words not found in the semantic space. Depending on the specific requirements of a task, this may compromise the results. Please check your input when you receive this message. 
</p>
<p>A warning message will be displayed whenever no word of one  input string is found in the semantic space.
</p>


<h3>Value</h3>

<p>A list of two elements; the first element (<code>$local</code>) contains the local coherences as a numeric vector, the second element (<code>$global</code>) contains the global coherence as a numeric.</p>


<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>References</h3>

<p>Landauer, T.K., &amp; Dumais, S.T. (1997). A solution to Plato's problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge. <em>Psychological Review, 104,</em> 211-240.
</p>
<p>Mitchell, J., &amp; Lapata, M. (2010). Composition in Distributional Models of Semantics. 
<em>Cognitive Science, 34,</em> 1388-1429.
</p>


<h3>See Also</h3>

<p><code><a href="lsa.html#topic+cosine">cosine</a></code>,
<code><a href="#topic+Cosine">Cosine</a></code>,
<code><a href="#topic+costring">costring</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)

coherence ("there was certainly too much of it in the air. even the duchess
sneezed occasionally; and as for the baby, it was sneezing and howling
alternately without a moment's pause. the only things in the kitchen
that did not sneeze, were the cook, and a large cat which was sitting on
the hearth and grinning from ear to ear.",
tvectors=wonderland)</code></pre>

<hr>
<h2 id='compose'>Two-Word Composition</h2><span id='topic+compose'></span>

<h3>Description</h3>

<p>Computes the vector of a complex expression p consisting of two single words u and v,
following the methods examined in Mitchell &amp; Lapata (2008) (see <em>Details</em>).</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default 
compose(x,y,method="Add", a=1,b=1,c=1,m,k,lambda=2,
      tvectors=tvectors, norm="none")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compose_+3A_x">x</code></td>
<td>
<p>a single word (character vector with <code>length(x) = 1)</code></p>
</td></tr>
<tr><td><code id="compose_+3A_y">y</code></td>
<td>
<p>a single word (character vector with <code>length(y) = 1)</code></p>
</td></tr>
<tr><td><code id="compose_+3A_a">a</code>, <code id="compose_+3A_b">b</code>, <code id="compose_+3A_c">c</code></td>
<td>
<p>weighting parameters, see <em>Details</em></p>
</td></tr>
<tr><td><code id="compose_+3A_m">m</code></td>
<td>
<p>number of nearest words to the Predicate that are initially activated (see <code><a href="#topic+Predication">Predication</a></code>)</p>
</td></tr>
<tr><td><code id="compose_+3A_k">k</code></td>
<td>
<p>size of the <code>k</code>-neighborhood; <code>k</code> <code class="reqn">\le</code> <code>m</code> (see <code><a href="#topic+Predication">Predication</a></code>)</p>
</td></tr>
<tr><td><code id="compose_+3A_lambda">lambda</code></td>
<td>
<p>dilation parameter for <code>method = "Dilation"</code> </p>
</td></tr>
<tr><td><code id="compose_+3A_method">method</code></td>
<td>
<p>the composition method to be used (see <em>Details</em>)</p>
</td></tr>
<tr><td><code id="compose_+3A_norm">norm</code></td>
<td>
<p>whether to <code><a href="#topic+normalize">normalize</a></code> the single word vectors before applying a composition function. Setting <code>norm = "none"</code> will not perform any normalizations, setting <code>norm = "all"</code> will normalize every involved word vector. Setting <code>norm = "block"</code> is only valid for the <code><a href="#topic+Predication">Predication</a></code> method</p>
</td></tr>
<tr><td><code id="compose_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">p</code> be the vector with entries <code class="reqn">p_i</code> for the two-word phrase consisiting of <code class="reqn">u</code> with entries <code class="reqn">u_i</code> and <code class="reqn">v</code> with entries <code class="reqn">v_i</code>.
The different composition methods as described by Mitchell &amp; Lapata (2008, 2010) are as follows:
</p>

<ul>
<li><p> Additive Model (<code>method = "Add"</code>)    </p>
<p style="text-align: center;"><code class="reqn">p_i = u_i + v_i</code>
</p>

</li>
<li><p> Weighted Additive Model (<code>method = "WeightAdd"</code>) </p>
<p style="text-align: center;"><code class="reqn">p_i = a*u_i + b*v_i</code>
</p>

</li>
<li><p> Multiplicative Model (<code>method = "Multiply"</code>) </p>
<p style="text-align: center;"><code class="reqn">p_i = u_i * v_i</code>
</p>

</li>
<li><p> Combined Model (<code>method = "Combined"</code>) </p>
<p style="text-align: center;"><code class="reqn">p_i = a*u_i + b*v_i + c*u_i*v_i </code>
</p>

</li>
<li><p> Predication (<code>method = "Predication"</code>) (see <code><a href="#topic+Predication">Predication</a></code>)
</p>
<p>If <code>method="Predication"</code> is used, <code>x</code> will be taken as Predicate and <code>y</code> will be taken as Argument of the phrase (see <em>Examples</em>)
</p>
</li>
<li><p> Circular Convolution (<code>method = "CConv"</code>) </p>
<p style="text-align: center;"><code class="reqn">p_i = \sum\limits_{j} u_j * v_{i-j}</code>
</p>
<p>,
</p>
<p>where the subscripts of <code class="reqn">v</code> are interpreted modulo <code class="reqn">n</code> with <code class="reqn">n =</code> <code>length(x)</code>(= <code>length(y)</code>)
</p>
</li>
<li><p> Dilation (<code>method = "Dilation"</code>) </p>
<p style="text-align: center;"><code class="reqn">p = (u*u)*v + (\lambda - 1)*(u*v)*u</code>
</p>
<p>,
</p>
<p>with <code class="reqn">(u*u)</code> being the dot product of <code class="reqn">u</code> and <code class="reqn">u</code> (and <code class="reqn">(u*v)</code> being the dot product of <code class="reqn">u</code> and <code class="reqn">v</code>).  
</p>
</li></ul>

<p>The <code>Add, Multiply,</code> and <code>CConv</code> methods are <em>symmetrical</em> composition methods,
</p>
<p>i.e. <code>compose(x="word1",y="word2")</code> will give the same results as <code>compose(x="word2",y="word1")</code>
</p>
<p>On the other hand, <code>WeightAdd, Combined, Predication</code> and <code>Dilation</code> are <em>asymmetrical</em>, i.e. <code>compose(x="word1",y="word2")</code> will give different results than <code>compose(x="word2",y="word1")</code>
</p>


<h3>Value</h3>

<p>The phrase vector as a numeric vector
</p>


<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>References</h3>

<p>Kintsch, W. (2001). Predication. <em>Cognitive science, 25,</em> 173-202.
</p>
<p>Mitchell, J., &amp; Lapata, M. (2008). Vector-based Models of Semantic
Composition. In <em>Proceedings of ACL-08: HLT</em> (pp. 236-244).
Columbus, Ohio.
</p>
<p>Mitchell, J., &amp; Lapata, M. (2010). Composition in Distributional Models of Semantics. 
<em>Cognitive Science, 34,</em> 1388-1429.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Predication">Predication</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)

compose(x="mad",y="hatter",method="Add",tvectors=wonderland)

compose(x="mad",y="hatter",method="Combined",a=1,b=2,c=3,
tvectors=wonderland)

compose(x="mad",y="hatter",method="Predication",m=20,k=3,
tvectors=wonderland)

compose(x="mad",y="hatter",method="Dilation",lambda=3,
tvectors=wonderland)</code></pre>

<hr>
<h2 id='conSIM'>Similarity in Context</h2><span id='topic+conSIM'></span>

<h3>Description</h3>

<p>Compute Similarity of a word with a set of two other test words, given a third context word</p>


<h3>Usage</h3>

<pre><code class='language-R'>conSIM(x,y,z,c,tvectors=tvectors)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conSIM_+3A_x">x</code></td>
<td>
<p>The relevant word, given as a character of <code>length(x) = 1</code></p>
</td></tr>
<tr><td><code id="conSIM_+3A_y">y</code>, <code id="conSIM_+3A_z">z</code></td>
<td>
<p>The two test words, given each as a character of <code>length(y) = 1</code></p>
</td></tr>
<tr><td><code id="conSIM_+3A_c">c</code></td>
<td>
<p>The context word in respect to which the similarity of <code>x</code> to <code>y</code> and <code>z</code> is to be computed (a character of <code>length(y) = 1</code>) </p>
</td></tr>
<tr><td><code id="conSIM_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Following the example from Kintsch (2014): If one has to judge the similarity between <em>France</em> one the one hand and the test words <em>Germany</em> and <em>Spain</em> on the other hand, this similarity judgement varies as a function of a fourth context word. If <em>Portugal</em> is given as a context word, <em>France</em> is considered to be more similar to <em>Germany</em> than to <em>Spain</em>, and vice versa for the context word <em>Poland</em>. Kintsch (2014) proposed a context sensitive, asymmetrical similarity measure for cases like this, which is implemented here
</p>


<h3>Value</h3>

<p>A list of two similarity values
</p>
<p><code>SIM_XY_zc</code>: Similarity of <code>x</code> and <code>y</code>, given the alternative <code>z</code> and the context <code>c</code>
</p>
<p><code>SIM_XZ_yc</code>: Similarity of <code>x</code> and <code>z</code>, given the alternative <code>y</code> and the context <code>c</code></p>


<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>References</h3>

<p>Kintsch, W. (2015). Similarity as a Function of Semantic Distance and Amount of Knowledge. <em>Psychological Review, 121,</em> 559-561.
</p>
<p>Tversky, A. (1977). Features of similarity. <em>Psychological Review, 84,</em> 327-352.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Cosine">Cosine</a></code>
<code><a href="#topic+asym">asym</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)

conSIM(x="rabbit",y="alice",z="hatter",c="dormouse",tvectors=wonderland)</code></pre>

<hr>
<h2 id='Cosine'>Compute cosine similarity</h2><span id='topic+Cosine'></span>

<h3>Description</h3>

<p>Computes the cosine similarity for two single words</p>


<h3>Usage</h3>

<pre><code class='language-R'>Cosine(x,y,tvectors=tvectors)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cosine_+3A_x">x</code></td>
<td>
<p>A single word, given as a character of <code>length(x) = 1</code></p>
</td></tr>
<tr><td><code id="Cosine_+3A_y">y</code></td>
<td>
<p>A single word, given as a character of <code>length(y) = 1</code></p>
</td></tr>
<tr><td><code id="Cosine_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Instead of using numeric vectors, as the <code>cosine()</code> function from the lsa package does, this function allows for the direct computation of the cosine between two single words (i.e. Characters). which are automatically searched for in the LSA space given in as <code>tvectors</code>.</p>


<h3>Value</h3>

<p>The cosine similarity as a numeric</p>


<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>References</h3>

<p>Landauer, T.K., &amp; Dumais, S.T. (1997). A solution to Plato's problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge. <em>Psychological Review, 104,</em> 211-240.
</p>
<p>Dennis, S. (2007). How to use the LSA Web Site. In T. K. Landauer, D. S. McNamara, S. Dennis, &amp; W. Kintsch (Eds.), <em>Handbook of Latent Semantic Analysis</em> (pp. 35-56). Mahwah, NJ: Erlbaum.
</p>
<p><a href="http://wordvec.colorado.edu/">http://wordvec.colorado.edu/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+distance">distance</a></code>
<code><a href="#topic+asym">asym</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)

Cosine("alice","rabbit",tvectors=wonderland)</code></pre>

<hr>
<h2 id='costring'>Sentence Comparison</h2><span id='topic+costring'></span>

<h3>Description</h3>

<p>Computes cosine values between sentences and/or documents</p>


<h3>Usage</h3>

<pre><code class='language-R'>costring(x,y,tvectors=tvectors,split=" ",remove.punctuation=TRUE, 
stopwords = NULL, method ="Add")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="costring_+3A_x">x</code></td>
<td>
<p>a character vector</p>
</td></tr>
<tr><td><code id="costring_+3A_y">y</code></td>
<td>
<p>a character vector</p>
</td></tr>
<tr><td><code id="costring_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
<tr><td><code id="costring_+3A_split">split</code></td>
<td>
<p>a character vector defining the character used to split the documents into words (white space by default)</p>
</td></tr>
<tr><td><code id="costring_+3A_remove.punctuation">remove.punctuation</code></td>
<td>
<p>removes punctuation from <code>x</code> and <code>y</code>; <code>TRUE</code> by default</p>
</td></tr>
<tr><td><code id="costring_+3A_stopwords">stopwords</code></td>
<td>
<p>a character vector defining a list of words that are <em>not</em> used to compute the document/sentence vector for <code>x</code> and <code>y</code></p>
</td></tr>
<tr><td><code id="costring_+3A_method">method</code></td>
<td>
<p>the compositional model to compute the document vector from its word vectors. The default option <code>method = "Add"</code> computes the document vector as the vector sum. With <code>method = "Multiply"</code>, the document vector is computed via element-wise multiplication (see <code><a href="#topic+compose">compose</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the cosine between two documents (or sentences) or the cosine between a single word and a document (or sentence).
</p>
<p>In the traditional LSA approach, the vector <em>D</em> for a document (or a sentence) consisting of the words <em>(t1, . , tn)</em> is computed as
</p>
<p style="text-align: center;"><code class="reqn">D = \sum\limits_{i=1}^n t_n</code>
</p>

<p>This is the default method (<code>method="Add"</code>) for this function. Alternatively, this function provided the possibility of computing the document vector from its word vectors using element-wise multiplication (see Mitchell &amp; Lapata, 2010 and <code><a href="#topic+compose">compose</a></code>).
</p>
<p>The format of <code>x</code> (or <code>y</code>) can be of the kind <code>x &lt;- "word1 word2 word3"</code> , but also of the kind <code>x &lt;- c("word1", "word2", "word3")</code>. This allows for simple copy&amp;paste-inserting of text,
but also for using character vectors, e.g. the output of <code>neighbors()</code>.
</p>
<p>To import a document <em>Document.txt</em> to from a directory for comparisons, set your working
directory to this directory using <code>setwd()</code>. Then use the following command lines:
</p>
<p><code>fileName1 &lt;- "Alice_in_Wonderland.txt"</code>
</p>
<p><code>x &lt;- readChar(fileName1, file.info(fileName1)$size)</code>
</p>
<p>A note will be displayed whenever not all words of one input string are found in the semantic space. <em><b>Caution:</b></em> In that case, the function will still produce a result, by omitting the words not found in the semantic space. Depending on the specific requirements of a task, this may compromise the results. Please check your input when you receive this message. 
</p>
<p>A warning message will be displayed whenever no word of one  input string is found in the semantic space.
</p>


<h3>Value</h3>

<p>A numeric giving the cosine between the input documents/sentences</p>


<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>References</h3>

<p>Landauer, T.K., &amp; Dumais, S.T. (1997). A solution to Plato's problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge. <em>Psychological Review, 104,</em> 211-240.
</p>
<p>Dennis, S. (2007). How to use the LSA Web Site. In T. K. Landauer, D. S. McNamara, S. Dennis, &amp; W. Kintsch (Eds.), <em>Handbook of Latent
Semantic Analysis</em> (pp. 35-56). Mahwah, NJ: Erlbaum.
</p>
<p>Mitchell, J., &amp; Lapata, M. (2010). Composition in Distributional Models of Semantics. 
<em>Cognitive Science, 34,</em> 1388-1429.
</p>
<p><a href="http://wordvec.colorado.edu/">http://wordvec.colorado.edu/</a>
</p>


<h3>See Also</h3>

<p><code><a href="lsa.html#topic+cosine">cosine</a></code>,
<code><a href="#topic+Cosine">Cosine</a></code>,
<code><a href="#topic+multicos">multicos</a></code>,
<code><a href="#topic+multidocs">multidocs</a></code>,
<code><a href="#topic+multicostring">multicostring</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)
costring("alice was beginning to get very tired.",
      "a white rabbit with a clock ran close to her.",
      tvectors=wonderland)</code></pre>

<hr>
<h2 id='distance'>Compute distance</h2><span id='topic+distance'></span>

<h3>Description</h3>

<p>Computes distance metrics for two single words</p>


<h3>Usage</h3>

<pre><code class='language-R'>distance(x,y,method="euclidean",tvectors=tvectors)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distance_+3A_x">x</code></td>
<td>
<p>A single word, given as a character of <code>length(x) = 1</code></p>
</td></tr>
<tr><td><code id="distance_+3A_y">y</code></td>
<td>
<p>A single word, given as a character of <code>length(y) = 1</code></p>
</td></tr>
<tr><td><code id="distance_+3A_method">method</code></td>
<td>
<p>Specifies whether to compute <code>euclidean</code> or <code>cityblock</code> metric</p>
</td></tr>
<tr><td><code id="distance_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes Minkowski metrics, i.e. geometric distances between the vectors for two given words. Possible options are <code>euclidean</code> for the Euclidean Distance, <code class="reqn"> d(x,y) = \sqrt{\sum{(x-y)^2}}</code>, and <code>cityblock</code> for the City Block metric, <code class="reqn"> d(x,y) = \sum{|x-y|}</code>
</p>


<h3>Value</h3>

<p>The distance value as a numeric</p>


<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Cosine">Cosine</a></code>
<code><a href="#topic+asym">asym</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)

distance("alice","rabbit",method="euclidean",tvectors=wonderland)</code></pre>

<hr>
<h2 id='genericSummary'>Summarize a text</h2><span id='topic+genericSummary'></span>

<h3>Description</h3>

<p>Selects sentences from a text that best describe its topic</p>


<h3>Usage</h3>

<pre><code class='language-R'>genericSummary(text,k,split=c(".","!","?"),min=5,...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="genericSummary_+3A_text">text</code></td>
<td>
<p>A character vector of <code>length(text) = 1</code> specifiying the text to be summarized</p>
</td></tr>
<tr><td><code id="genericSummary_+3A_k">k</code></td>
<td>
<p>The number of sentences to be used in the summary</p>
</td></tr>
<tr><td><code id="genericSummary_+3A_split">split</code></td>
<td>
<p>A character vector specifying which symbols determine the end of a sentence in the document</p>
</td></tr>
<tr><td><code id="genericSummary_+3A_min">min</code></td>
<td>
<p>The minimum amount of words a sentence must have to be included in the computations</p>
</td></tr>
<tr><td><code id="genericSummary_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed on to <code><a href="lsa.html#topic+textmatrix">textmatrix</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Applies the method of Gong &amp; Liu (2001) for generic text summarization of text document <em>D</em> via Latent Semantic Analysis:
</p>

<ol>
<li><p>Decompose the document <em>D</em> into individual sentences,
and use these sentences to form the candidate sentence
set <em>S</em>, and set <em>k</em> = 1.
</p>
</li>
<li><p>Construct the terms by sentences matrix <em>A</em> for the
document <em>D</em>.
</p>
</li>
<li><p>Perform the SVD on A to obtain the singular value
matrix <code class="reqn">\Sigma</code>, and the right singular vector matrix <em><code class="reqn">V^t</code></em>. In
the singular vector space, each sentence i is represented
by the column vector <code class="reqn">\psi _i = [v_i1, v_i2, ... , v_ir]^t</code> of <em><code class="reqn">V^t</code></em>.
</p>
</li>
<li><p>Select the <em>k</em>'th right singular vector from matrix <em><code class="reqn">V^t</code></em>.
</p>
</li>
<li><p>Select the sentence which has the largest index value
with the <em>k</em>'th right singular vector, and include it in
the summary.
</p>
</li>
<li><p>If <em>k</em> reaches the predefined number, terminate the op-
eration; otherwise, increment <em>k</em> by one, and go to Step
4.
</p>
</li></ol>

<p>(Cited directly from Gong &amp; Liu, 2001, <em>p.</em> 21)
</p>


<h3>Value</h3>

<p>A character vector of the length <em>k</em></p>


<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>See Also</h3>

<p><code><a href="lsa.html#topic+textmatrix">textmatrix</a></code>,
<code><a href="lsa.html#topic+lsa">lsa</a></code>,
<code><a href="base.html#topic+svd">svd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>D &lt;- "This is just a test document. It is set up just to throw some random 
sentences in this example. So do not expect it to make much sense. Probably, even 
the summary won't be very meaningful. But this is mainly due to the document not being
meaningful at all. For test purposes, I will also include a sentence in this 
example that is not at all related to the rest of the document. Lions are larger than cats."

genericSummary(D,k=1)</code></pre>

<hr>
<h2 id='LSAfun-package'>Computations based on Latent Semantic Analysis</h2><span id='topic+LSAfun-package'></span>

<h3>Description</h3>

<p>Offers methods and functions for working with Vector Space Models of semantics/distributional semantic models/word embeddings. The package was originally written for Latent Semantic Analysis (LSA), but can be used with all vector space models.
Such models are created by algorithms working on a corpus of text documents. Those algorithms achieve a high-dimensional vector representation for word (and document) meanings. The exact LSA algorithm is described in Martin &amp; Berry (2007).
</p>
<p>Such a representation allows for the computation of word (and document) similarities, for example by computing cosine values of angles between two vectors.
</p>


<h3>The focus of this package</h3>

<p>This package is not designed to create LSA semantic spaces. In R, this functionality is provided by the package <code><a href="lsa.html#topic+lsa">lsa</a></code>. The focus of the package <em>LSAfun</em> is to provide functions to be applied on existing LSA (or other) semantic spaces, such as
</p>

<ol>
<li><p>Similarity Computations
</p>
</li>
<li><p>Neighborhood Computations
</p>
</li>
<li><p>Applied Functions
</p>
</li>
<li><p>Composition Methods
</p>
</li></ol>



<h3>Video Tutorials</h3>

<p>A video tutorial for this package can be found here:
<a href="https://youtu.be/IlwIZvM2kg8">https://youtu.be/IlwIZvM2kg8</a>
</p>
<p>A video tutorial for using this package with vision-based representations from deep convolutional neural networks can be found here:
<a href="https://youtu.be/0PNrXraWfzI">https://youtu.be/0PNrXraWfzI</a>
</p>


<h3>How to obtain a semantic space</h3>

<p><em>LSAfun</em> comes with one example LSA space, the <a href="#topic+wonderland">wonderland</a> space.
</p>
<p>This package can also directly use LSA semantic spaces created with the <code><a href="lsa.html#topic+lsa">lsa</a></code>-package. Thus, it allows the user to use own LSA spaces. 
(Note that the function <code><a href="lsa.html#topic+lsa">lsa</a></code> gives a list of three matrices. Of those, the term matrix <code>U</code> should be used.)
</p>
<p>The <code><a href="lsa.html#topic+lsa">lsa</a></code> package works with (very) small corpora, but gets difficulties in scaling up to larger corpora. In this case, it is recommended to use specialized software for creating semantic spaces, such as
</p>

<ul>
<li><p>S-Space (Jurgens &amp; Stevens, 2010), available <a href="https://github.com/fozziethebeat/S-Space">here</a>
</p>
</li>
<li><p>SemanticVectors (Widdows &amp; Ferraro, 2008), available <a href="https://github.com/semanticvectors/semanticvectors">here</a>
</p>
</li>
<li><p>gensim (Rehurek &amp; Sojka, 2010), available <a href="https://radimrehurek.com/gensim/">here</a>
</p>
</li>
<li><p>DISSECT (Dinu, Pham, &amp; Baroni, 2013), available <a href="https://github.com/composes-toolkit/dissect">here</a>
</p>
</li></ul>



<h4>Downloading semantic spaces</h4>

<p>Another possibility is to use one of the semantic spaces provided at <a href="https://sites.google.com/site/fritzgntr/software-resources">https://sites.google.com/site/fritzgntr/software-resources</a>. These are stored in the <code>.rda</code> format. To load one of these spaces into the <code>R</code> workspace, save them into a directory, set the working directory to that directory, and load the space using <code>load()</code>.
</p>



<h3>Author(s)</h3>

<p>Fritz Guenther
</p>

<hr>
<h2 id='multicos'>Vector x Vector Comparison</h2><span id='topic+multicos'></span>

<h3>Description</h3>

<p>Computes a cosine matrix from given word vectors</p>


<h3>Usage</h3>

<pre><code class='language-R'>multicos(x,y=x,tvectors=tvectors)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multicos_+3A_x">x</code></td>
<td>
<p>a character vector or numeric of <code>length=ncol(tvectors)</code>
(vector with same dimensionality as LSA space)</p>
</td></tr>
<tr><td><code id="multicos_+3A_y">y</code></td>
<td>
<p>a character vector; y = x by default</p>
</td></tr>
<tr><td><code id="multicos_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Submit a character vector consisting of <em>n</em> words to get a <em>n x n</em> cosine matrix of all their pairwise
cosines.
</p>
<p>Alternatively, submit two different character vectors to get their pairwise cosines.
Single words are also possible arguments.
</p>
<p>Also allows for computation of cosines between a given numeric vector with the same
dimensionality as the LSA space and a vector consisting of n words.
</p>


<h3>Value</h3>

<p>A matrix containing the pairwise cosines of <code>x</code> and <code>y</code></p>


<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>References</h3>

<p>Landauer, T.K., &amp; Dumais, S.T. (1997). A solution to Plato's problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge. <em>Psychological Review, 104,</em> 211-240.
</p>
<p>Dennis, S. (2007). How to use the LSA Web Site. In T. K. Landauer, D. S. McNamara, S. Dennis, &amp; W. Kintsch (Eds.), <em>Handbook of Latent Semantic Analysis</em> (pp. 35-56). Mahwah, NJ: Erlbaum.
</p>
<p><a href="http://wordvec.colorado.edu/">http://wordvec.colorado.edu/</a>
</p>


<h3>See Also</h3>

<p><code><a href="lsa.html#topic+cosine">cosine</a></code>,
<code><a href="#topic+Cosine">Cosine</a></code>,
<code><a href="#topic+costring">costring</a></code>,
<code><a href="#topic+multicostring">multicostring</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)
multicos("mouse rabbit cat","king queen",
          tvectors=wonderland)</code></pre>

<hr>
<h2 id='multicostring'>Sentence x Vector Comparison</h2><span id='topic+multicostring'></span>

<h3>Description</h3>

<p>Computes cosines between a sentence/ document and multiple words</p>


<h3>Usage</h3>

<pre><code class='language-R'>multicostring(x,y,tvectors=tvectors,split=" ",remove.punctuation=TRUE, 
stopwords = NULL, method ="Add")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multicostring_+3A_x">x</code></td>
<td>
<p>a character vector specifying a sentence/ document (or also a single word)</p>
</td></tr>
<tr><td><code id="multicostring_+3A_y">y</code></td>
<td>
<p>a character vector specifying multiple single words</p>
</td></tr>
<tr><td><code id="multicostring_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
<tr><td><code id="multicostring_+3A_split">split</code></td>
<td>
<p>a character vector defining the character used to split the documents into words (white space by default)</p>
</td></tr>
<tr><td><code id="multicostring_+3A_remove.punctuation">remove.punctuation</code></td>
<td>
<p>removes punctuation from <code>x</code> and <code>y</code>; <code>TRUE</code> by default</p>
</td></tr>
<tr><td><code id="multicostring_+3A_stopwords">stopwords</code></td>
<td>
<p>a character vector defining a list of words that are <em>not</em> used to compute the document/sentence vector for <code>x</code></p>
</td></tr>
<tr><td><code id="multicostring_+3A_method">method</code></td>
<td>
<p>the compositional model to compute the document vector from its word vectors. The default option <code>method = "Add"</code> computes the document vector as the vector sum. With <code>method = "Multiply"</code>, the document vector is computed via element-wise multiplication (see <code><a href="#topic+compose">compose</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The format of <code>x</code> (or <code>y</code>) can be of the kind <code>x &lt;- "word1 word2 word3"</code> , but also of the kind <code>x &lt;- c("word1", "word2", "word3")</code>. This allows for simple copy&amp;paste-inserting of text, but also for using character vectors, e.g. the output of <code><a href="#topic+neighbors">neighbors</a></code>.
</p>
<p>Both x and y can also just consist of one single word.
In the traditional LSA approach, the vector <em>D</em> for the document (or sentence) <code>x</code> consisting of the words <em>(t1, . , tn)</em> is computed as
</p>
<p style="text-align: center;"><code class="reqn">D = \sum\limits_{i=1}^n t_n</code>
</p>

<p>This is the default method (<code>method="Add"</code>) for this function. Alternatively, this function provided the possibility of computing the document vector from its word vectors using element-wise multiplication (see Mitchell &amp; Lapata, 2010 and <code><a href="#topic+compose">compose</a></code>). See also <code><a href="#topic+costring">costring</a></code>).
</p>
<p>A note will be displayed whenever not all words of one input string are found in the semantic space. <em><b>Caution:</b></em> In that case, the function will still produce a result, by omitting the words not found in the semantic space. Depending on the specific requirements of a task, this may compromise the results. Please check your input when you receive this message.
</p>
<p>A warning message will be displayed whenever no word of one  input string is found in the semantic space.
</p>


<h3>Value</h3>

<p>A numeric giving the cosine between the input sentences/documents</p>


<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>References</h3>

<p>Landauer, T.K., &amp; Dumais, S.T. (1997). A solution to Plato's problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge. <em>Psychological Review, 104,</em> 211-240.
</p>
<p>Dennis, S. (2007). How to use the LSA Web Site. In T. K. Landauer, D. S. McNamara, S. Dennis, &amp; W. Kintsch (Eds.), <em>Handbook of Latent Semantic Analysis</em> (pp. 35-56). Mahwah, NJ: Erlbaum.
</p>
<p>Mitchell, J., &amp; Lapata, M. (2010). Composition in Distributional Models of Semantics. 
<em>Cognitive Science, 34,</em> 1388-1429.
</p>
<p><a href="http://wordvec.colorado.edu/">http://wordvec.colorado.edu/</a>
</p>


<h3>See Also</h3>

<p><code><a href="lsa.html#topic+cosine">cosine</a></code>,
<code><a href="#topic+Cosine">Cosine</a></code>,
<code><a href="#topic+multicos">multicos</a></code>,
<code><a href="#topic+costring">costring</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)

multicostring("alice was beginning to get very tired.",
        "a white rabbit with a clock ran close to her.",
        tvectors=wonderland)

multicostring("suddenly, a cat appeared in the woods",
names(neighbors("cheshire",n=20,tvectors=wonderland)), 
tvectors=wonderland)</code></pre>

<hr>
<h2 id='multidocs'>Comparison of sentence sets</h2><span id='topic+multidocs'></span>

<h3>Description</h3>

<p>Computes cosine values between sets of sentences and/or documents</p>


<h3>Usage</h3>

<pre><code class='language-R'>multidocs(x,y=x,chars=10,tvectors=tvectors,remove.punctuation=TRUE,
stopwords = NULL,method ="Add")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multidocs_+3A_x">x</code></td>
<td>
<p>a character vector containing different sentences/documents</p>
</td></tr>
<tr><td><code id="multidocs_+3A_y">y</code></td>
<td>
<p>a character vector containing different sentences/documents (<code>y = x</code> by default)</p>
</td></tr>
<tr><td><code id="multidocs_+3A_chars">chars</code></td>
<td>
<p>an integer specifying how many letters (starting from the first) of each sentence/document are to be printed in the row.names and col.names of the output matrix</p>
</td></tr>
<tr><td><code id="multidocs_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
<tr><td><code id="multidocs_+3A_remove.punctuation">remove.punctuation</code></td>
<td>
<p>removes punctuation from <code>x</code> and <code>y</code>; <code>TRUE</code> by default</p>
</td></tr>
<tr><td><code id="multidocs_+3A_stopwords">stopwords</code></td>
<td>
<p>a character vector defining a list of words that are <em>not</em> used to compute the document/sentence vector for <code>x</code> and <code>y</code></p>
</td></tr>
<tr><td><code id="multidocs_+3A_method">method</code></td>
<td>
<p>the compositional model to compute the document vector from its word vectors. The default option <code>method = "Add"</code> computes the document vector as the vector sum. With <code>method = "Multiply"</code>, the document vector is computed via element-wise multiplication (see <code><a href="#topic+compose">compose</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the traditional LSA approach, the vector <em>D</em> for a document (or a sentence) consisting of the words <em>(t1, . , tn)</em> is computed as
</p>
<p style="text-align: center;"><code class="reqn">D = \sum\limits_{i=1}^n t_n</code>
</p>

<p>This is the default method (<code>method="Add"</code>) for this function. Alternatively, this function provided the possibility of computing the document vector from its word vectors using element-wise multiplication (see Mitchell &amp; Lapata, 2010 and <code><a href="#topic+compose">compose</a></code>).
</p>
<p>This function computes the cosines between two sets of documents (or sentences).<br /><br />
The format of <code>x</code> (or <code>y</code>) should be of the kind <code>x &lt;- c("this is the first text","here is another text")</code> (or <code>y &lt;- c("this is a third text","and here is yet another text"))</code>
<br /><br />
A note will be displayed whenever not all words of one input string are found in the semantic space. <em><b>Caution:</b></em> In that case, the function will still produce a result, by omitting the words not found in the semantic space. Depending on the specific requirements of a task, this may compromise the results. Please check your input when you receive this message. <br /><br />
A warning message will be displayed whenever no word of one  input string is found in the semantic space.
</p>


<h3>Value</h3>

<p>A list of three elements:
</p>
<table>
<tr><td><code>cosmat</code></td>
<td>
<p>A numeric matrix giving the cosines between the input sentences/documents</p>
</td></tr>
<tr><td><code>xdocs</code></td>
<td>
<p>A legend for the row.names of <code>cosmat</code></p>
</td></tr>
<tr><td><code>ydocs</code></td>
<td>
<p>A legend for the col.names of <code>cosmat</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>References</h3>

<p>Landauer, T.K., &amp; Dumais, S.T. (1997). A solution to Plato's problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge. <em>Psychological Review, 104,</em> 211-240.
</p>
<p>Dennis, S. (2007). How to use the LSA Web Site. In T. K. Landauer, D. S. McNamara, S. Dennis, &amp; W. Kintsch (Eds.), <em>Handbook of Latent Semantic Analysis</em> (pp. 35-56). Mahwah, NJ: Erlbaum.
</p>
<p>Mitchell, J., &amp; Lapata, M. (2010). Composition in Distributional Models of Semantics. 
<em>Cognitive Science, 34,</em> 1388-1429.
</p>
<p><a href="http://wordvec.colorado.edu/">http://wordvec.colorado.edu/</a>
</p>


<h3>See Also</h3>

<p><code><a href="lsa.html#topic+cosine">cosine</a></code>,
<code><a href="#topic+Cosine">Cosine</a></code>,
<code><a href="#topic+multicos">multicos</a></code>,
<code><a href="#topic+costring">costring</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)
multidocs(x = c("alice was beginning to get very tired.",
                "the red queen greeted alice."),
          y = c("the mad hatter and the mare hare are having a party.",
                "the hatter sliced the cup of tea in half."), 
      tvectors=wonderland)</code></pre>

<hr>
<h2 id='MultipleChoice'>Answers Multiple Choice Questions</h2><span id='topic+MultipleChoice'></span>

<h3>Description</h3>

<p>Selects the nearest word to an input out of a set of options</p>


<h3>Usage</h3>

<pre><code class='language-R'>MultipleChoice(x,y,tvectors=tvectors,remove.punctuation=TRUE, stopwords = NULL,
   method ="Add", all.results=FALSE)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MultipleChoice_+3A_x">x</code></td>
<td>
<p>a character vector of <code>length(x) = 1</code> specifying a sentence/ document (or also a single word)</p>
</td></tr>
<tr><td><code id="MultipleChoice_+3A_y">y</code></td>
<td>
<p>a character vector specifying multiple answer options (with each element of the vector being one answer option)</p>
</td></tr>
<tr><td><code id="MultipleChoice_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
<tr><td><code id="MultipleChoice_+3A_remove.punctuation">remove.punctuation</code></td>
<td>
<p>removes punctuation from <code>x</code> and <code>y</code>; <code>TRUE</code> by default</p>
</td></tr>
<tr><td><code id="MultipleChoice_+3A_stopwords">stopwords</code></td>
<td>
<p>a character vector defining a list of words that are <em>not</em> used to compute the document/sentence vector for <code>x</code> and <code>y</code></p>
</td></tr>
<tr><td><code id="MultipleChoice_+3A_method">method</code></td>
<td>
<p>the compositional model to compute the document vector from its word vectors. The default option <code>method = "Add"</code> computes the document vector as the vector sum. With <code>method = "Multiply"</code>, the document vector is computed via element-wise multiplication (see <code><a href="#topic+compose">compose</a></code> and <code><a href="#topic+costring">costring</a></code>). With <code>method = "Analogy"</code>, the document vector is computed via vector subtraction; see <em>Description</em> for more information.</p>
</td></tr>
<tr><td><code id="MultipleChoice_+3A_all.results">all.results</code></td>
<td>
<p>If <code>all.results=FALSE</code> (default), the function will only return the best answer as a character string. If <code>all.results=TRUE</code>, it will return a named numeric vector, where the names are the different answer options in <code>y</code> and the numeric values their respective cosine similarity to <code>x</code>, sorted by decreasing similarity.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes all the cosines between a given sentence/document or word and multiple answer options. Then
selects the nearest option to the input (the option with the highest cosine). This function relies entirely on the <code><a href="#topic+costring">costring</a></code> function.
</p>
<p>A note will be displayed whenever not all words of one answer alternative are found in the semantic space. <em><b>Caution:</b></em> In that case, the function will still produce a result, by omitting the words not found in the semantic space. Depending on the specific requirements of a task, this may compromise the results. Please check your input when you receive this message.
</p>
<p>A warning message will be displayed whenever no word of one answer alternative is found in the semantic space.
</p>
<p>Using <code>method="Analogy"</code> requires the input in <em>both</em> <code>x</code> and <code>y</code> to only consist of word pairs (for example <code>x = c("helmet head")</code> and  <code>y = c("kneecap knee", "atmosphere earth", "grass field")</code>). In that case, the function will try to identify the best-fitting answer in <code>y</code> by applying the <code>king - man + woman = queen</code> rationale to solve <em>man : king = woman : ?</em> (Mikolov et al., 2013): In that case, one should also have <code>king - man = queen - woman</code>. With <code>method="Analogy"</code>, the function will compute the  difference between the normalized vectors <code>head - helmet</code>, and search the nearest of the vector differences <code>knee - kneecap</code>, <code>earth - atmosphere</code>, and <code>field - grass</code>.
</p>


<h3>Value</h3>

<p>If <code>all.results=FALSE</code> (default), the function will only return the best answer as a character string. If <code>all.results=TRUE</code>, it will return a named numeric vector, where the names are the different answer options in <code>y</code> and the numeric values their respective cosine similarity to <code>x</code>, sorted by decreasing similarity.</p>


<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>References</h3>

<p>Landauer, T.K., &amp; Dumais, S.T. (1997). A solution to Plato's problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge. <em>Psychological Review, 104,</em> 211-240.
</p>
<p>Mikolov, T., Yih, W. T., &amp; Zweig, G. (2013). Linguistic regularities in continuous space word representations. In  <em>Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2013)</em>. Association for Computational Linguistics.
</p>


<h3>See Also</h3>

<p><code><a href="lsa.html#topic+cosine">cosine</a></code>,
<code><a href="#topic+Cosine">Cosine</a></code>,
<code><a href="#topic+costring">costring</a></code>,
<code><a href="#topic+multicostring">multicostring</a></code>,
<code><a href="#topic+analogy">analogy</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)

LSAfun:::MultipleChoice("who does the march hare celebrate his unbirthday with?",
                 c("mad hatter","red queen","caterpillar","cheshire Cat"),
                 tvectors=wonderland)</code></pre>

<hr>
<h2 id='neighbors'>Find nearest neighbors</h2><span id='topic+neighbors'></span>

<h3>Description</h3>

<p>Returns the n nearest words to a given word or sentence/document</p>


<h3>Usage</h3>

<pre><code class='language-R'>neighbors(x,n,tvectors=tvectors)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neighbors_+3A_x">x</code></td>
<td>
<p>a character vector of <code>length(x) = 1</code> or a numeric of <code>length=ncol(tvectors)</code> vector with same dimensionality as the semantic space</p>
</td></tr>
<tr><td><code id="neighbors_+3A_n">n</code></td>
<td>
<p>the number of neighbors to be computed</p>
</td></tr>
<tr><td><code id="neighbors_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The format of <code>x</code> should be of the kind <code>x &lt;- "word1 word2 word3"</code> instead of 
</p>
<p><code>x &lt;- c("word1", "word2", "word3")</code> if sentences/documents are used as input. This allows for simple copy&amp;paste-inserting of text.
</p>
<p>To import a document <em>Document.txt</em> to from a directory for comparisons, set your working
directory to this directory using <code>setwd()</code>. Then use the following command lines:
</p>
<p><code>fileName1 &lt;- "Alice_in_Wonderland.txt"</code>
</p>
<p><code>x &lt;- readChar(fileName1, file.info(fileName1)$size)</code>.
</p>
<p>Since <code>x</code> can also be chosen to be any vector of the active LSA Space, this function can be
combined with <code>compose()</code> to compute neighbors of complex expressions (see examples)
</p>


<h3>Value</h3>

<p>A named numeric vector. The neighbors are given as names of the vector, and their respective cosines to the input as vector entries.</p>


<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>References</h3>

<p>Landauer, T.K., &amp; Dumais, S.T. (1997). A solution to Plato's problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge. <em>Psychological Review, 104,</em> 211-240.
</p>
<p>Dennis, S. (2007). How to use the LSA Web Site. In T. K. Landauer, D. S. McNamara, S. Dennis, &amp; W. Kintsch (Eds.), <em>Handbook of Latent Semantic Analysis</em> (pp. 35-56). Mahwah, NJ: Erlbaum.
</p>
<p><a href="http://wordvec.colorado.edu/">http://wordvec.colorado.edu/</a>
</p>


<h3>See Also</h3>

<p><code><a href="lsa.html#topic+cosine">cosine</a></code>,
<code><a href="#topic+plot_neighbors">plot_neighbors</a></code>,
<code><a href="#topic+compose">compose</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)

neighbors("cheshire",n=20,tvectors=wonderland) 

neighbors(compose("mad","hatter",method="Add",tvectors=wonderland),
n=20,tvectors=wonderland)</code></pre>

<hr>
<h2 id='normalize'>Normalize a vector</h2><span id='topic+normalize'></span>

<h3>Description</h3>

<p>Normalizes a character vector to a unit vector</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalize(x)</code></pre>


<h3>Arguments</h3>

 
<table>
<tr><td><code id="normalize_+3A_x">x</code></td>
<td>
<p>a numeric or integer vector</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The (euclidean) norm of a vector <code class="reqn">x</code> is defined as 
</p>
<p style="text-align: center;"><code class="reqn">||x|| =  \sqrt{\Sigma(x^2)}</code>
</p>

<p>To normalize a vector to a unit vector <code class="reqn">u</code> with <code class="reqn">||u|| = 1</code>, the following equation is applied:
</p>
<p style="text-align: center;"><code class="reqn">x' = x/ ||x||</code>
</p>



<h3>Value</h3>

<p>The normalized vector as a numeric</p>


<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>Examples</h3>

<pre><code class='language-R'>normalize(1:2)


## check vector norms:

x &lt;- 1:2

sqrt(sum(x^2))              ## vector norm
sqrt(sum(normalize(x)^2))   ## norm = 1

</code></pre>

<hr>
<h2 id='oldbooks'>A collection of five classic books</h2><span id='topic+oldbooks'></span>

<h3>Description</h3>

<p>This object is a list containing five classical books:
</p>

<ul>
<li> <p><em>Around the World in Eighty Days</em> by Jules Verne
</p>
</li>
<li> <p><em>The Three Musketeers</em> by Alexandre Dumas
</p>
</li>
<li> <p><em>Frankenstein</em> by Mary Shelley
</p>
</li>
<li> <p><em>Dracula</em> by Bram Stoker
</p>
</li>
<li> <p><em>The Strange Case of Dr Jekyll and Mr Hyde</em> by Robert Stevenson</p>
</li></ul>

<p>as single-element character vectors. All five books were taken from the <a href="http://www.gutenberg.org">Project Gutenberg homepage</a> and contain formatting symbols, such as \n for breaks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(oldbooks)</code></pre>


<h3>Format</h3>

<p>A named list containing five character vectors as elements</p>


<h3>Source</h3>

<p><a href="http://www.gutenberg.org">Project Gutenberg</a></p>


<h3>References</h3>

 
<p>Dumas, A. (1844). <em>The Three Musketeers</em>. Retrieved from
http://www.gutenberg.org/ebooks/1257
</p>
<p>Shelley, M. W. (1818). <em>Frankenstein; Or, The Modern Prometheus</em>. Retrieved from
http://www.gutenberg.org/ebooks/84
</p>
<p>Stevenson, R. L. (1886). <em>The Strange Case of Dr. Jekyll and Mr. Hyde</em>. Retrieved from
http://www.gutenberg.org/ebooks/42
</p>
<p>Stoker, B. (1897). <em>Dracula</em>. Retrieved from
http://www.gutenberg.org/ebooks/345
</p>
<p>Verne, J.(1873). <em>Around the World in Eighty Days</em>. Retrieved from
http://www.gutenberg.org/ebooks/103
</p>

<hr>
<h2 id='pairwise'>Pairwise cosine computation</h2><span id='topic+pairwise'></span>

<h3>Description</h3>

<p>Computes pairwise cosine similarities</p>


<h3>Usage</h3>

<pre><code class='language-R'>pairwise(x,y,tvectors=tvectors)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pairwise_+3A_x">x</code></td>
<td>
<p>a character vector</p>
</td></tr>
<tr><td><code id="pairwise_+3A_y">y</code></td>
<td>
<p>a character vector</p>
</td></tr>
<tr><td><code id="pairwise_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes pairwise cosine similarities for two vectors of words. These vectors need to have the same length.</p>


<h3>Value</h3>

<p>A vector of the same length as <code>x</code> and <code>y</code> containing the pairwise cosine similarities. Returns <code>NA</code> if at least one word in a pair is not found in the semantic space.</p>


<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>References</h3>

<p>Landauer, T.K., &amp; Dumais, S.T. (1997). A solution to Plato's problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge. <em>Psychological Review, 104,</em> 211-240.
</p>
<p>Dennis, S. (2007). How to use the LSA Web Site. In T. K. Landauer, D. S. McNamara, S. Dennis, &amp; W. Kintsch (Eds.), <em>Handbook of Latent Semantic Analysis</em> (pp. 35-56). Mahwah, NJ: Erlbaum.
</p>
<p><a href="http://wordvec.colorado.edu/">http://wordvec.colorado.edu/</a>
</p>


<h3>See Also</h3>

<p><code><a href="lsa.html#topic+cosine">cosine</a></code>,
<code><a href="#topic+Cosine">Cosine</a></code>,
<code><a href="#topic+multicos">multicos</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)
pairwise("mouse rabbit cat","king queen hearts",
          tvectors=wonderland)</code></pre>

<hr>
<h2 id='plausibility'>Compute word (or compound) plausibility</h2><span id='topic+plausibility'></span>

<h3>Description</h3>

<p>Gives measures of semantic transparency (plausibility) for words or compounds</p>


<h3>Usage</h3>

<pre><code class='language-R'>plausibility(x,method, n=10,stem,tvectors=tvectors)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plausibility_+3A_x">x</code></td>
<td>
<p>a character vector of <code>length(x) = 1</code> or a numeric of <code>length=ncol(tvectors)</code> vector with same dimensionality as LSA space</p>
</td></tr>
<tr><td><code id="plausibility_+3A_method">method</code></td>
<td>
<p>the measure of semantic transparency, can be one of <code>n_density</code>,<code>length</code>, <code>proximity</code>, or <code>entropy</code> (see <em>Details</em>)</p>
</td></tr>
<tr><td><code id="plausibility_+3A_n">n</code></td>
<td>
<p>the number of neighbors for the <code>n_density</code> method</p>
</td></tr>
<tr><td><code id="plausibility_+3A_stem">stem</code></td>
<td>
<p>the stem (or word) of comparison for the <code>proximity</code> method</p>
</td></tr>
<tr><td><code id="plausibility_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The format of <code>x</code> should be of the kind <code>x &lt;- "word1 word2 word3"</code> instead of <code>x &lt;- c("word1", "word2", "word3")</code> if phrases of more than one word are used as input. Simple vector addition of the constituent vectors is then used to compute the phrase vector.
</p>
<p>Since <code>x</code> can also be chosen to be any vector of the active LSA Space, this function can be combined with <code>compose()</code> to compute semantic transparency measures of complex expressions (see examples). Since semantic transparency methods were developed as measures for composed vectors, applying them makes most sense for those.
</p>
<p>The methods are defined as follows:
</p>

<ul>
<li><p><code>method = "n_density"</code> The average cosine between a (word or phrase) vector and its <em>n</em> nearest neighbors, excluding the word itself when a single word is submitted (see also <code><a href="#topic+SND">SND</a></code> for a more detailed version)
</p>
</li>
<li><p><code>method = "length"</code> The length of a vector (as computed by the standard Euclidean norm)
</p>
</li>
<li><p><code>method = "proximity"</code> The cosine similarity between a compound vector and its stem word (for example between <em>mad hatter</em> and <em>hatter</em> or between <em>objectify</em> and <em>object</em>)
</p>
</li>
<li><p><code>method = "entropy"</code> The entropy of the <em>K</em>-dimensional vector with the vector components <code class="reqn">t_1,...,t_K </code>, as computed by 
</p>
<p style="text-align: center;"><code class="reqn">entropy = \log{K} - \sum{t_i * \log{t_i}}    </code>
</p>

</li></ul>



<h3>Value</h3>

<p>The semantic transparency as a numeric</p>


<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>References</h3>

<p>Lazaridou, A., Vecchi, E., &amp; Baroni, M. (2013). Fish transporters and miracle homes:
How compositional distributional semantics can help NP parsing. In <em>Proceedings
of EMNLP 2013</em> (pp. 1908 - 1913). Seattle, WA.
</p>
<p>Marelli, M., &amp; Baroni, M. (2015). Affixation in semantic space: Modeling morpheme meanings with compositional distributional semantics. <em>Psychological Review, 122,.</em> 485-515.
</p>
<p>Vecchi, E. M., Baroni, M., &amp; Zamparelli, R. (2011). (Linear) maps of the impossible:
Capturing semantic anomalies in distributional space. In <em>Proceedings of the
ACL Workshop on Distributional Semantics and Compositionality</em> (pp. 1-9).
Portland, OR.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Cosine">Cosine</a></code>,
<code><a href="#topic+neighbors">neighbors</a></code>,
<code><a href="#topic+compose">compose</a></code>,
<code><a href="#topic+SND">SND</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)

plausibility("cheshire cat",method="n_density",n=10,tvectors=wonderland) 

plausibility(compose("mad","hatter",method="Multiply",tvectors=wonderland),
method="proximity",stem="hatter",tvectors=wonderland)</code></pre>

<hr>
<h2 id='plot_doclist'>2D- or 3D-Plot of a list of sentences/documents</h2><span id='topic+plot_doclist'></span>

<h3>Description</h3>

<p>2D or 3D-Plot of mutual word similarities to a given list of sentences/documents</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_doclist(x,connect.lines="all",method="PCA",dims=3,
   axes=F,box=F,cex=1,chars=10,legend=T, size = c(800,800),
   alpha="graded",alpha.grade=1,col="rainbow",
   tvectors=tvectors,remove.punctuation=TRUE,...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_doclist_+3A_x">x</code></td>
<td>
<p>a character vector of <code>length(x) &gt; 1</code> that contains multiple sentences/documents</p>
</td></tr>
<tr><td><code id="plot_doclist_+3A_dims">dims</code></td>
<td>
<p>the dimensionality of the plot; set either <code>dims = 2</code> or <code>dims = 3</code></p>
</td></tr>
<tr><td><code id="plot_doclist_+3A_method">method</code></td>
<td>
<p>the method to be applied; either a Principal Component Analysis (<code>method="PCA"</code>) or a Multidimensional Scaling (<code>method="MDS"</code>) </p>
</td></tr>
<tr><td><code id="plot_doclist_+3A_connect.lines">connect.lines</code></td>
<td>
<p>(3d plot only) the number of closest associate words each word is connected with via line. Setting <code>connect.lines="all"</code> (default) will draw all connecting lines and will automatically apply <code>alpha="graded"</code></p>
</td></tr>
<tr><td><code id="plot_doclist_+3A_axes">axes</code></td>
<td>
<p>(3d plot only) whether axes shall be included in the plot</p>
</td></tr>
<tr><td><code id="plot_doclist_+3A_box">box</code></td>
<td>
<p>(3d plot only) whether a box shall be drawn around the plot</p>
</td></tr>
<tr><td><code id="plot_doclist_+3A_cex">cex</code></td>
<td>
<p>(2d Plot only) A numerical value giving the amount by which plotting text should be magnified relative to the default.</p>
</td></tr>
<tr><td><code id="plot_doclist_+3A_chars">chars</code></td>
<td>
<p>an integer specifying how many letters (starting from the first) of each sentence/document are to be printed in the plot</p>
</td></tr>
<tr><td><code id="plot_doclist_+3A_legend">legend</code></td>
<td>
<p>(3d plot only) whether a legend shall be drawn illustrating the color scheme of the <code>connect.lines</code>. The legend is inserted as a background bitmap to the plot using <code><a href="rgl.html#topic+bgplot3d">bgplot3d</a></code>. Therefore, they do not resize very gracefully (see the <code><a href="rgl.html#topic+bgplot3d">bgplot3d</a></code> documentation for more information).</p>
</td></tr>
<tr><td><code id="plot_doclist_+3A_size">size</code></td>
<td>
<p>(3d plot only) A numeric vector with two elements, the first specifying the width and the second specifying the height of the plot device.</p>
</td></tr>
<tr><td><code id="plot_doclist_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
<tr><td><code id="plot_doclist_+3A_remove.punctuation">remove.punctuation</code></td>
<td>
<p>removes punctuation from <code>x</code> and <code>y</code>; <code>TRUE</code> by default</p>
</td></tr>
<tr><td><code id="plot_doclist_+3A_alpha">alpha</code></td>
<td>
<p>(3d plot only) A numeric vector specifying the luminance of the <code>connect.lines</code>. By setting <code>alpha="graded"</code>, the luminance of every line will be adjusted to the cosine between the two words it connects.</p>
</td></tr>
<tr><td><code id="plot_doclist_+3A_alpha.grade">alpha.grade</code></td>
<td>
<p>(3d plot only) Only relevant if <code>alpha="graded"</code>. Specify a numeric value for <code>alpha.grade</code> to scale the luminance of all <code>connect.lines</code> up (<code>alpha.grade</code> &gt; 1) or down (<code>alpha.grade</code> &lt; 1) by that factor.</p>
</td></tr>
<tr><td><code id="plot_doclist_+3A_col">col</code></td>
<td>
<p>(3d plot only) A vector specifying the color of the <code>connect.lines</code>.  With setting <code>col ="rainbow"</code> (default), the color of every line will be adjusted to the cosine between the two words it connects, according to the rainbow palette. Other available color palettes for this purpose are <code>heat.colors</code>, <code>terrain.colors</code>, <code>topo.colors</code>, and <code>cm.colors</code> (see <code><a href="grDevices.html#topic+rainbow">rainbow</a></code>). Additionally, you can customize any color scale of your choice by providing an input specifying more than one color (for example <code>col = c("black","blue","red")</code>).</p>
</td></tr>
<tr><td><code id="plot_doclist_+3A_...">...</code></td>
<td>
<p>additional arguments which will be passed to <code><a href="rgl.html#topic+plot3d">plot3d</a></code> (in a three-dimensional plot only)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes all pairwise similarities within a given list of sentences/documents. On this similarity matrix, a Principal Component Analysis (PCA) or a Multidimensional Sclaing (MDS) is applied to get a two- or three-dimensional solution that best captures the similarity structure. This solution is then plotted.
</p>
<p>In the traditional LSA approach, the vector <em>D</em> for a document (or a sentence) consisting of the words <em>(t1, . , tn)</em> is computed as
</p>
<p style="text-align: center;"><code class="reqn">D = \sum\limits_{i=1}^n t_n</code>
</p>

<p>This function then computes the the cosines between two sets of documents (or sentences).
</p>
<p>The format of <code>x</code> should be of the kind <code>x &lt;- c("this is the first text","here is another text")</code>
</p>
<p>For creating pretty plots showing the similarity structure within this list of words best, set <code>connect.lines="all"</code> and <code>col="rainbow"</code>
</p>


<h3>Value</h3>

<p>see <code><a href="rgl.html#topic+plot3d">plot3d</a></code>: this function is called for the side effect of drawing the plot; a vector of object IDs is returned.
</p>
<p><code>plot_doclist</code> further prints a list with two elements:
</p>
<table>
<tr><td><code>coordinates</code></td>
<td>
<p>the coordinate vectors of the sentences/documents in the plot as a data frame</p>
</td></tr>
<tr><td><code>xdocs</code></td>
<td>
<p>A legend for the sentence/document labels in the plot and in the <code>coordinates</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Fritz Guenther, Taylor Fedechko
</p>


<h3>References</h3>

<p>Landauer, T.K., &amp; Dumais, S.T. (1997). A solution to Plato's problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge. <em>Psychological Review, 104,</em> 211-240.
</p>
<p>Mardia, K.V., Kent, J.T., &amp; Bibby, J.M. (1979). <em>Multivariate Analysis</em>, London: Academic Press.
</p>


<h3>See Also</h3>

<p><code><a href="lsa.html#topic+cosine">cosine</a></code>,
<code><a href="#topic+multidocs">multidocs</a></code>,
<code><a href="#topic+plot_neighbors">plot_neighbors</a></code>,
<code><a href="#topic+plot_wordlist">plot_wordlist</a></code>,
<code><a href="rgl.html#topic+plot3d">plot3d</a></code>,
<code><a href="stats.html#topic+princomp">princomp</a></code>,
<code><a href="grDevices.html#topic+rainbow">rainbow</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)

## Standard Plot

docs &lt;- c("alice was beginning to get very tired.",
          "the red queen greeted alice.",
          "the mad hatter and the mare hare are having a party.",
          "the hatter sliced the cup of tea in half.")
          
plot_doclist(docs,tvectors=wonderland,method="MDS",dims=2)

</code></pre>

<hr>
<h2 id='plot_neighbors'>2D- or 3D-Plot of neighbors</h2><span id='topic+plot_neighbors'></span>

<h3>Description</h3>

<p>2D- or 3D-Approximation of the neighborhood of a given word/sentence</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_neighbors(x,n,connect.lines="all",start.lines=T,
   method="PCA",dims=3,axes=F,box=F,cex=1,legend=T, size = c(800,800),
   alpha="graded",alpha.grade = 1, col="rainbow",tvectors=tvectors,...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_neighbors_+3A_x">x</code></td>
<td>
<p>a character vector of <code>length(x) = 1</code> or a numeric of <code>length=ncol(tvectors)</code> vector with same dimensionality as LSA space</p>
</td></tr>
<tr><td><code id="plot_neighbors_+3A_n">n</code></td>
<td>
<p>the number of neighbors to be computed</p>
</td></tr>
<tr><td><code id="plot_neighbors_+3A_dims">dims</code></td>
<td>
<p>the dimensionality of the plot; set either <code>dims = 2</code> or <code>dims = 3</code></p>
</td></tr>
<tr><td><code id="plot_neighbors_+3A_method">method</code></td>
<td>
<p>the method to be applied; either a Principal Component Analysis (<code>method="PCA"</code>) or a Multidimensional Scaling (<code>method="MDS"</code>) </p>
</td></tr>
<tr><td><code id="plot_neighbors_+3A_connect.lines">connect.lines</code></td>
<td>
<p>(3d plot only) the number of closest associate words each word is connected with via line. Setting <code>connect.lines="all"</code> (default) will draw all connecting lines and will automatically apply <code>alpha="graded"</code>; it will furthermore override the <code>start.lines</code> argument</p>
</td></tr>
<tr><td><code id="plot_neighbors_+3A_start.lines">start.lines</code></td>
<td>
<p>(3d plot only) whether lines shall be drawn between <code>x</code> and all the neighbors</p>
</td></tr>
<tr><td><code id="plot_neighbors_+3A_axes">axes</code></td>
<td>
<p>(3d plot only) whether axes shall be included in the plot</p>
</td></tr>
<tr><td><code id="plot_neighbors_+3A_box">box</code></td>
<td>
<p>(3d plot only) whether a box shall be drawn around the plot</p>
</td></tr>
<tr><td><code id="plot_neighbors_+3A_cex">cex</code></td>
<td>
<p>(2d Plot only) A numerical value giving the amount by which plotting text should be magnified relative to the default.</p>
</td></tr>
<tr><td><code id="plot_neighbors_+3A_legend">legend</code></td>
<td>
<p>(3d plot only) whether a legend shall be drawn illustrating the color scheme of the <code>connect.lines</code>. The legend is inserted as a background bitmap to the plot using <code><a href="rgl.html#topic+bgplot3d">bgplot3d</a></code>. Therefore, they do not resize very gracefully (see the <code><a href="rgl.html#topic+bgplot3d">bgplot3d</a></code> documentation for more information).</p>
</td></tr>
<tr><td><code id="plot_neighbors_+3A_size">size</code></td>
<td>
<p>(3d plot only) A numeric vector with two elements, the first specifying the width and the second specifying the height of the plot device.</p>
</td></tr>
<tr><td><code id="plot_neighbors_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
<tr><td><code id="plot_neighbors_+3A_alpha">alpha</code></td>
<td>
<p>(3d plot only) a vector of one or two numerics between 0 and 1 specifying the luminance of <code>start.lines</code> (first entry) and <code>connect.lines</code> (second entry). Specifying only one numeric will pass this value to both kinds of lines. With setting <code>alpha="graded"</code>, the luminance of every line will be adjusted to the cosine between the two words it connects.</p>
</td></tr>
<tr><td><code id="plot_neighbors_+3A_alpha.grade">alpha.grade</code></td>
<td>
<p>(3d plot only) Only relevant if <code>alpha="graded"</code>. Specify a numeric value for <code>alpha.grade</code> to scale the luminance of all <code>start.lines</code> and <code>connect.lines</code> up (<code>alpha.grade</code> &gt; 1) or down (<code>alpha.grade</code> &lt; 1) by that factor.</p>
</td></tr>
<tr><td><code id="plot_neighbors_+3A_col">col</code></td>
<td>
<p>(3d plot only) a vector of one or two characters specifying the color of <code>start.lines</code> (first entry) and <code>connect.lines</code> (second entry). Specifying only one colour will pass this colour to both kinds of lines. With setting <code>col ="rainbow"</code> (default), the colour of every line will be adjusted to the cosine between the two words it connects, according to the rainbow palette. Other available color palettes for this purpose are <code>heat.colors</code>, <code>terrain.colors</code>, <code>topo.colors</code>, and <code>cm.colors</code> (see <code><a href="grDevices.html#topic+rainbow">rainbow</a></code>). Additionally, you can customize any color scale of your choice by providing an input specifying more than two colors  
</p>
<p>(for example <code>col = c("black","blue","red")</code>).</p>
</td></tr>
<tr><td><code id="plot_neighbors_+3A_...">...</code></td>
<td>
<p>additional arguments which will be passed to <code><a href="rgl.html#topic+plot3d">plot3d</a></code> (in a three-dimensional plot only)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Attempts to create an image of the semantic neighborhood (based on cosine similarity) to a
given word, sentence/ document, or vector. An attempt is made to depict this subpart of the LSA
space in a two- or three-dimensional plot.
</p>
<p>To achieve this, either a Principal Component
Analysis (PCA) or a Multidimensional Scaling (MDS) is computed to preserve the interconnections between all the words in this
neighborhod as good as possible. Therefore, it is important to note that the image created from
this function is only the best two- or three-dimensional approximation to the true LSA space subpart.
</p>
<p>For creating pretty plots showing the similarity structure within this neighborhood best, set <code>connect.lines="all"</code> and <code>col="rainbow"</code>
</p>


<h3>Value</h3>

<p>For three-dimensional plots:see <code><a href="rgl.html#topic+plot3d">plot3d</a></code>: this function is called for the side effect of drawing the plot; a vector of object IDs is returned
</p>
<p><code>plot_neighbors</code> also gives the coordinate vectors of the words in the plot as a data frame</p>


<h3>Author(s)</h3>

<p>Fritz Guenther, Taylor Fedechko
</p>


<h3>References</h3>

<p>Landauer, T.K., &amp; Dumais, S.T. (1997). A solution to Plato's problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge. <em>Psychological Review, 104,</em> 211-240.
</p>
<p>Mardia, K.V., Kent, J.T., &amp; Bibby, J.M. (1979). <em>Multivariate Analysis</em>, London: Academic Press.
</p>


<h3>See Also</h3>

<p><code><a href="lsa.html#topic+cosine">cosine</a></code>,
<code><a href="#topic+neighbors">neighbors</a></code>,
<code><a href="#topic+multicos">multicos</a></code>,
<code><a href="#topic+plot_wordlist">plot_wordlist</a></code>,
<code><a href="rgl.html#topic+plot3d">plot3d</a></code>,
<code><a href="stats.html#topic+princomp">princomp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)

## Standard Plot
plot_neighbors("cheshire",n=20,tvectors=wonderland)  

## Pretty Plot
plot_neighbors("cheshire",n=20,tvectors=wonderland,
                connect.lines="all",col="rainbow")  



plot_neighbors(compose("mad","hatter",tvectors=wonderland),
                n=20, connect.lines=2,tvectors=wonderland)</code></pre>

<hr>
<h2 id='plot_wordlist'>2D- or 3D-Plot of a list of words</h2><span id='topic+plot_wordlist'></span>

<h3>Description</h3>

<p>2D or 3D-Plot of mutual word similarities to a given list of words</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_wordlist(x,connect.lines="all",method="PCA",dims=3,
   axes=F,box=F,cex=1,legend=T, size = c(800,800),
   alpha="graded",alpha.grade=1,col="rainbow",
   tvectors=tvectors,...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_wordlist_+3A_x">x</code></td>
<td>
<p>a character vector of <code>length(x) &gt; 1</code> that contains multiple sentences/documents</p>
</td></tr>
<tr><td><code id="plot_wordlist_+3A_dims">dims</code></td>
<td>
<p>the dimensionality of the plot; set either <code>dims = 2</code> or <code>dims = 3</code></p>
</td></tr>
<tr><td><code id="plot_wordlist_+3A_method">method</code></td>
<td>
<p>the method to be applied; either a Principal Component Analysis (<code>method="PCA"</code>) or a Multidimensional Scaling (<code>method="MDS"</code>) </p>
</td></tr>
<tr><td><code id="plot_wordlist_+3A_connect.lines">connect.lines</code></td>
<td>
<p>(3d plot only) the number of closest associate words each word is connected with via line. Setting <code>connect.lines="all"</code> (default) will draw all connecting lines and will automatically apply <code>alpha="graded"</code>.</p>
</td></tr>
<tr><td><code id="plot_wordlist_+3A_axes">axes</code></td>
<td>
<p>(3d plot only) whether axes shall be included in the plot</p>
</td></tr>
<tr><td><code id="plot_wordlist_+3A_box">box</code></td>
<td>
<p>(3d plot only) whether a box shall be drawn around the plot</p>
</td></tr>
<tr><td><code id="plot_wordlist_+3A_cex">cex</code></td>
<td>
<p>(2d Plot only) A numerical value giving the amount by which plotting text should be magnified relative to the default.</p>
</td></tr>
<tr><td><code id="plot_wordlist_+3A_legend">legend</code></td>
<td>
<p>(3d plot only) whether a legend shall be drawn illustrating the color scheme of the <code>connect.lines</code>. The legend is inserted as a background bitmap to the plot using <code><a href="rgl.html#topic+bgplot3d">bgplot3d</a></code>. Therefore, they do not resize very gracefully (see the <code><a href="rgl.html#topic+bgplot3d">bgplot3d</a></code> documentation for more information).</p>
</td></tr>
<tr><td><code id="plot_wordlist_+3A_size">size</code></td>
<td>
<p>(3d plot only) A numeric vector with two elements, the first specifying the width and the second specifying the height of the plot device.</p>
</td></tr>
<tr><td><code id="plot_wordlist_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
<tr><td><code id="plot_wordlist_+3A_alpha">alpha</code></td>
<td>
<p>(3d plot only) A numeric vector specifying the luminance of the <code>connect.lines</code>. By setting <code>alpha="graded"</code>, the luminance of every line will be adjusted to the cosine between the two words it connects.</p>
</td></tr>
<tr><td><code id="plot_wordlist_+3A_alpha.grade">alpha.grade</code></td>
<td>
<p>(3d plot only) Only relevant if <code>alpha="graded"</code>. Specify a numeric value for <code>alpha.grade</code> to scale the luminance of all <code>connect.lines</code> up (<code>alpha.grade</code> &gt; 1) or down (<code>alpha.grade</code> &lt; 1) by that factor.</p>
</td></tr>
<tr><td><code id="plot_wordlist_+3A_col">col</code></td>
<td>
<p>(3d plot only) A vector specifying the color of the <code>connect.lines</code>.  With setting <code>col ="rainbow"</code> (default), the color of every line will be adjusted to the cosine between the two words it connects, according to the rainbow palette. Other available color palettes for this purpose are <code>heat.colors</code>, <code>terrain.colors</code>, <code>topo.colors</code>, and <code>cm.colors</code> (see <code><a href="grDevices.html#topic+rainbow">rainbow</a></code>). Additionally, you can customize any color scale of your choice by providing an input specifying more than one color  (for example <code>col = c("black","blue","red")</code>).</p>
</td></tr>
<tr><td><code id="plot_wordlist_+3A_...">...</code></td>
<td>
<p>additional arguments which will be passed to <code><a href="rgl.html#topic+plot3d">plot3d</a></code> (in a three-dimensional plot only)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes all pairwise similarities within a given list of words. On this similarity matrix, a Principal Component Analysis (PCA) or a Multidimensional Sclaing (MDS) is applied to get a two- or three-dimensional solution that best captures the similarity structure. This solution is then plotted.
</p>
<p>For creating pretty plots showing the similarity structure within this list of words best, set <code>connect.lines="all"</code> and <code>col="rainbow"</code>
</p>


<h3>Value</h3>

<p>see <code><a href="rgl.html#topic+plot3d">plot3d</a></code>: this function is called for the side effect of drawing the plot; a vector of object IDs is returned.
</p>
<p><code>plot_wordlist</code> also gives the coordinate vectors of the words in the plot as a data frame</p>


<h3>Author(s)</h3>

<p>Fritz Guenther, Taylor Fedechko
</p>


<h3>References</h3>

<p>Landauer, T.K., &amp; Dumais, S.T. (1997). A solution to Plato's problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge. <em>Psychological Review, 104,</em> 211-240.
</p>
<p>Mardia, K.V., Kent, J.T., &amp; Bibby, J.M. (1979). <em>Multivariate Analysis</em>, London: Academic Press.
</p>


<h3>See Also</h3>

<p><code><a href="lsa.html#topic+cosine">cosine</a></code>,
<code><a href="#topic+neighbors">neighbors</a></code>,
<code><a href="#topic+multicos">multicos</a></code>,
<code><a href="#topic+plot_neighbors">plot_neighbors</a></code>,
<code><a href="rgl.html#topic+plot3d">plot3d</a></code>,
<code><a href="stats.html#topic+princomp">princomp</a></code>,
<code><a href="grDevices.html#topic+rainbow">rainbow</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)

## Standard Plot

words &lt;- c("alice","hatter","queen","knight","hare","cheshire") 
            
plot_wordlist(words,tvectors=wonderland,method="MDS",dims=2)

</code></pre>

<hr>
<h2 id='Predication'>Compute Vector for Predicate-Argument-Expressions</h2><span id='topic+Predication'></span>

<h3>Description</h3>

<p>Computes vectors for complex expressions of type PREDICATE[ARGUMENT] by applying the method of Kintsch (2001) (see <em>Details</em>).</p>


<h3>Usage</h3>

<pre><code class='language-R'>Predication(P,A,m,k,tvectors=tvectors,norm="none")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Predication_+3A_p">P</code></td>
<td>
<p>Predicate of the expression, a single word (character vector)</p>
</td></tr>
<tr><td><code id="Predication_+3A_a">A</code></td>
<td>
<p>Argument of the expression, a single word (character vector)</p>
</td></tr>
<tr><td><code id="Predication_+3A_m">m</code></td>
<td>
<p>number of nearest words to the Predicate that are initially activated</p>
</td></tr>
<tr><td><code id="Predication_+3A_k">k</code></td>
<td>
<p>size of the <code>k</code>-neighborhood; <code>k</code> <code class="reqn">\le</code> <code>m</code></p>
</td></tr>
<tr><td><code id="Predication_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
<tr><td><code id="Predication_+3A_norm">norm</code></td>
<td>
<p>whether to <code><a href="#topic+normalize">normalize</a></code> the single word vectors before applying a composition function. Setting <code>norm = "none"</code> will not perform any normalizations, setting <code>norm = "all"</code> will normalize every involved word vector (Predicate, Argument, and every single activated neighbor). Setting <code>norm = "block"</code> will normalize the Argument vector and will normalize the [Predicate + neighbors] vector, to weight the Argument and the &quot;Predicate in context&quot; equally.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The vector for the expression is computed following the Predication Process by Kintsch (2001):
</p>
<p>The <code>m</code> nearest neighbors to the Predicate are computed. Of those, the <code>k</code> nearest neighbors to
the Argument are selected. The vector for the expression is then computed as the sum of
Predicate vector, Argument vector, and the vectors of those <code>k</code> neighbors (the <code>k</code>-neighborhood).
</p>


<h3>Value</h3>

<p>An object of class <code>Pred</code>: This object is a list consisting of:
</p>
<table>
<tr><td><code>$PA</code></td>
<td>
<p>The vector for the complex expression as described above</p>
</td></tr>
<tr><td><code>$P.Pred</code></td>
<td>
<p>The vector for Predicate plus the <em>k</em>-neighborhoodvectors without the Argument vector</p>
</td></tr>
<tr><td><code>$neighbors</code></td>
<td>
<p>The words in the <em>k</em>-neighborhood.</p>
</td></tr> 
<tr><td><code>$P</code></td>
<td>
<p>The Predicate given as input</p>
</td></tr>
<tr><td><code>$A</code></td>
<td>
<p>The Argument given as input</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>References</h3>

<p>Kintsch, W. (2001). Predication. <em>Cognitive Science, 25,</em> 173-202.
</p>


<h3>See Also</h3>

<p><code><a href="lsa.html#topic+cosine">cosine</a></code>,
<code><a href="#topic+neighbors">neighbors</a></code>,
<code><a href="#topic+multicos">multicos</a></code>,
<code><a href="#topic+compose">compose</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)

Predication(P="mad",A="hatter",m=20,k=3,tvectors=wonderland)</code></pre>

<hr>
<h2 id='priming'>Simulated data for a Semantic Priming Experiment</h2><span id='topic+priming'></span>

<h3>Description</h3>

<p>A data frame containing simulated data for a Semantic Priming Experiment. This data contains 514 prime-target pairs, which are taken from the Hutchison, Balota, Cortese and Watson (2008) study. These pairs are generated by pairing each of 257 target words with one semantically related and one semantically unrelated prime.
</p>
<p>The data frame contains four columns:
</p>

<ul>
<li><p> First column: Prime Words
</p>
</li>
<li><p> Second column: Target Words
</p>
</li>
<li><p> Third column: <strong>Simulated</strong> Reaction Times
</p>
</li>
<li><p> Fourth column: Specifies whether a prime-target pair is considered semantically related or unrelated
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>data(priming)</code></pre>


<h3>Format</h3>

<p>A data frame with 514 rows and 4 columns</p>


<h3>References</h3>

<p>Hutchison, K. A., Balota, D. A., Cortese, M. &amp; Watson, J. M. (2008). Predicting semantic priming at the item level. <em>Quarterly Journal of Experimental Psychology, 61,</em> 1036-1066.
</p>

<hr>
<h2 id='SND'>Semantic neighborhood density</h2><span id='topic+SND'></span>

<h3>Description</h3>

<p>Returns semantic neighborhood with semantic neighborhood size and density</p>


<h3>Usage</h3>

<pre><code class='language-R'>SND(x,n=NA,threshold=3.5,tvectors=tvectors)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SND_+3A_x">x</code></td>
<td>
<p>a character vector of <code>length(x) = 1</code> or a numeric of <code>length=ncol(tvectors)</code> vector with same dimensionality as the semantic space</p>
</td></tr>
<tr><td><code id="SND_+3A_n">n</code></td>
<td>
<p>if specified as a numeric, determines the size of the neighborhood as the <code>n</code> nearest words to <code>x</code>. If <code>n=NA</code> (default), the semantic neighborhood will be determined according to a similarity threshold (see <code>threshold</code>)</p>
</td></tr>
<tr><td><code id="SND_+3A_threshold">threshold</code></td>
<td>
<p>specifies the similarity threshold that determines if a word is counted as a neighbor for <code>x</code>, following the method by Buchanan et al. (2011) (see <code>Description</code> below)</p>
</td></tr>
<tr><td><code id="SND_+3A_tvectors">tvectors</code></td>
<td>
<p>the semantic space in which the computation is to be done (a numeric matrix where every row is a word vector)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are two principle approaches to determine the semantic neighborhood of a target word:
</p>

<ul>
<li><p> Set an a priori size of the semantic neighborhood to a fixed value <code>n</code> (e.g., Marelli &amp; Baroni, 2015). The <code>n</code> closest words to the target word are counted as its semantic neighbors. The semantic neighborhood size is then necessarily <code>n</code>; the semantic neighborhood density is the mean similarity between these neighbors and the target word (see also <code><a href="#topic+plausibility">plausibility</a></code>)
</p>
</li>
<li><p> Determine the semantic neighborhood based on a similarity threshold; all words whose similarity to the target word exceeds this threshold are counted as its semantic neighbors (e.g., Buchanan, Westbury, &amp; Burgess, 2001). First, the similarity between the target word and all words in the semantic space is computed. These similarities are then transformed into <em>z</em>-scores. Traditionally, the threshold is set to <em>z = 3.5</em> (e.g., Buchanan, Westbury, &amp; Burgess, 2001).
</p>
</li></ul>

<p>If a single target word is used as <code>x</code>, this  target word itself (which always has a similarity of 1 to itself) is excluded from these computations so that it cannot be counted as its own neighbor
</p>


<h3>Value</h3>

<p>A list of three elements:
</p>

<ul>
<li><p>neighbors: A names numeric vector of all identified neighbors, with the names being these neighbors and the values their similarity to <code>x</code>
</p>
</li>
<li><p>n_size: The number of neighbors as a numeric
</p>
</li>
<li><p>SND: The semantic neighborhood density (SND) as a numeric
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Fritz Guenther
</p>


<h3>References</h3>

<p>Buchanan, L., Westbury, C., &amp; Burgess, C. (2001). Characterizing semantic space: Neighborhood effects in word recognition. <em>Psychonomic Bulletin &amp; Review, 8,</em> 531-544.
</p>
<p>Marelli, M., &amp; Baroni, M. (2015). Affixation in semantic space: Modeling morpheme meanings with compositional distributional semantics. <em>Psychological Review, 122,</em> 485-515.
</p>


<h3>See Also</h3>

<p><code><a href="lsa.html#topic+cosine">cosine</a></code>,
<code><a href="#topic+plot_neighbors">plot_neighbors</a></code>,
<code><a href="#topic+compose">compose</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wonderland)

SND("cheshire",n=20,tvectors=wonderland)

SND("alice",threshold=2,tvectors=wonderland)
</code></pre>

<hr>
<h2 id='syntest'>A multiple choice test for synonyms and antonyms</h2><span id='topic+syntest'></span>

<h3>Description</h3>

<p>This object multiple choice test for synonyms and antonyms, consisting of seven columns. 
</p>

<ol>
<li><p> The first column defines the question, i.e. the word a synonym or an antonym has to be found for. 
</p>
</li>
<li><p> The second up to the fifth column show the possible answer alternatives.
</p>
</li>
<li><p> The sixth column defines the correct answer.
</p>
</li>
<li><p> The seventh column indicates whether a synonym or an antonym has to be found for the word in question.
</p>
</li></ol>

<p>The test consists of twenty questions, which are given in the twenty rows of the data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(syntest)</code></pre>


<h3>Format</h3>

<p>A data frame with 20 rows and 7 columns</p>

<hr>
<h2 id='wonderland'>LSA Space: Alice's Adventures in Wonderland</h2><span id='topic+wonderland'></span>

<h3>Description</h3>

<p>This data set is a 50-dimensional LSA space derived from Lewis Carrol's book &quot;Alice's Adventures in Wonderland&quot;. The book was split into 791 paragraphs which served as documents for the LSA algorithm (Landauer, Foltz &amp; Laham, 1998). Only words that appeared in at least two documents were used for building the LSA space.
</p>
<p>This LSA space contains 1123 different terms, all in lower case letters, and was created using the <code><a href="lsa.html#topic+lsa">lsa</a></code>-package. It can be used as <code>tvectors</code> for all the functions in the <code>LSAfun</code>-package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(wonderland)</code></pre>


<h3>Format</h3>

<p>A 1123x50 matrix with terms as rownames.</p>


<h3>Source</h3>

<p><a href="http://www.gutenberg.org/cache/epub/11/pg11.txt">Alice in Wonderland from Project Gutenberg</a></p>


<h3>References</h3>

<p>Landauer, T., Foltz, P., and Laham, D. (1998) <em>Introduction to Latent Semantic Analysis</em>. In: Discourse Processes 25, pp. 259-284.
</p>
<p>Carroll, L. (1865). <em>Alice's Adventures in Wonderland</em>. New York: MacMillan.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
