<!DOCTYPE html><html><head><title>Help for package mmb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mmb}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bayesCaret'><p>Provides a caret-compatible wrapper around functionality for classification</p>
and regression, as implemented by mmb.</a></li>
<li><a href='#bayesComputeMarginalFactor'><p>Compute a marginal factor (continuous or discrete random variable).</p></a></li>
<li><a href='#bayesComputeProductFactor'><p>Computes one single factor that is needed for full Bayesian inferencing.</p></a></li>
<li><a href='#bayesConvertData'><p>Convert data for usage within Bayesian models.</p></a></li>
<li><a href='#bayesFeaturesToSample'><p>Transform a collection of Bayesian features back to a sample.</p></a></li>
<li><a href='#bayesInferSimple'><p>Perform simple (network) Bayesian inferencing and regression.</p></a></li>
<li><a href='#bayesProbability'><p>Full Bayesian inferencing for determining the probability or</p>
relative likelihood of a given value.</a></li>
<li><a href='#bayesProbabilityAssign'><p>Assign probabilities to one or more samples, given some training data.</p></a></li>
<li><a href='#bayesProbabilityNaive'><p>Naive Bayesian inferencing for determining the probability or</p>
relative likelihood of a given value.</a></li>
<li><a href='#bayesProbabilitySimple'><p>Assign a probability using a simple (network) Bayesian classifier.</p></a></li>
<li><a href='#bayesRegress'><p>Perform full-dependency Bayesian regression for a sample.</p></a></li>
<li><a href='#bayesRegressAssign'><p>Regression for one or more samples, given some training data.</p></a></li>
<li><a href='#bayesRegressSimple'><p>Perform simple (network) Bayesian regression.</p></a></li>
<li><a href='#bayesToLatex'><p>Create a string that can be used in Latex in an e.g. equation-environment.</p></a></li>
<li><a href='#centralities'><p>Given a neighborhood of data, computes the similarity of each sample</p>
in the neighborhood to the neighborhood.</a></li>
<li><a href='#checkBayesFeature'><p>Validate a Bayesian feature using some sanity checks.</p></a></li>
<li><a href='#conditionalDataMin'><p>Segment data according to one or more random variables.</p></a></li>
<li><a href='#createFeatureForBayes'><p>Create a Bayesian feature by name and value.</p></a></li>
<li><a href='#discretizeVariableToRanges'><p>Discretize a continuous random variable to ranges/buckets.</p></a></li>
<li><a href='#distance'><p>Given a neighborhood of data and two samples from that neighborhood,</p>
calculates the distance between the samples.</a></li>
<li><a href='#estimatePdf'><p>Safe PDF estimation that works also for sparse random variables.</p></a></li>
<li><a href='#getDefaultRegressor'><p>Get the system-wide default regressor.</p></a></li>
<li><a href='#getMessages'><p>Get a boolean indicating whether messages are enabled system-wide.</p></a></li>
<li><a href='#getProbForDiscrete'><p>Get a probability of a discrete value.</p></a></li>
<li><a href='#getRangeForDiscretizedValue'><p>Get the range-/bucket-ID of a given value.</p></a></li>
<li><a href='#getValueKeyOfBayesFeatures'><p>Obtain the type of the value of a Bayesian feature.</p></a></li>
<li><a href='#getValueOfBayesFeatures'><p>Obtain the value of a Bayesian feature.</p></a></li>
<li><a href='#getWarnings'><p>Get a boolean indicating whether warnings are enabled system-wide.</p></a></li>
<li><a href='#make.varClosure'><p>Creates a closure over a variable and returns its getter and setter.</p></a></li>
<li><a href='#neighborhood'><p>Given Bayesian features, returns those samples from a dataset that</p>
exhibit a similarity (i.e., the neighborhood).</a></li>
<li><a href='#sampleToBayesFeatures'><p>Transform an entire sample into a collection of Bayesian features.</p></a></li>
<li><a href='#setDefaultRegressor'><p>Set a system-wide default regressor.</p></a></li>
<li><a href='#setMessages'><p>Enable or disable messages system-wide.</p></a></li>
<li><a href='#setWarnings'><p>Enable or disable warnings system-wide.</p></a></li>
<li><a href='#vicinities'><p>Segment a dataset by each row once, then compute vicinities of</p>
samples in the neighborhood.</a></li>
<li><a href='#vicinitiesForSample'><p>Segment a dataset by a single sample and compute vicinities for it and</p>
the remaining samples in the neighborhood.</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Arbitrary Dependency Mixed Multivariate Bayesian Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.13.3</td>
</tr>
<tr>
<td>Author:</td>
<td>Sebastian Hönel</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Sebastian Hönel &lt;sebastian.honel@lnu.se&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Supports Bayesian models with full and partial (hence
    arbitrary) dependencies between random variables. Discrete and continuous
    variables are supported, and conditional joint probabilities and probability
    densities are estimated using Kernel Density Estimation (KDE). The full
    general form, which implements an extension to Bayes' theorem, as well as
    the simple form, which is just a Bayesian network, both support regression
    through segmentation and KDE and estimation of probability or relative
    likelihood of discrete or continuous target random variables. This package
    also provides true statistical distance measures based on Bayesian models.
    Furthermore, these measures can be facilitated on neighborhood searches,
    and to estimate the similarity and distance between data points.
    Related work is by Bayes (1763) &lt;<a href="https://doi.org/10.1098%2Frstl.1763.0053">doi:10.1098/rstl.1763.0053</a>&gt;
    and by Scutari (2010) &lt;<a href="https://doi.org/10.18637%2Fjss.v035.i03">doi:10.18637/jss.v035.i03</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/MrShoenel/R-mmb">https://github.com/MrShoenel/R-mmb</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/MrShoenel/R-mmb/issues">https://github.com/MrShoenel/R-mmb/issues</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>devtools, testthat, covr, e1071, caret, knitr, rmarkdown,
ggplot2, ggpubr, cowplot, philentropy, Rtsne</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rdpack, datasets, stats, foreach, parallel, doParallel</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-09-15 19:37:43 UTC; Admin</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-09-23 08:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bayesCaret'>Provides a caret-compatible wrapper around functionality for classification
and regression, as implemented by mmb.</h2><span id='topic+bayesCaret'></span>

<h3>Description</h3>

<p>A wrapper to be used with the package/function <code>caret::train()</code>.
Supports regression and classification and an extensive default grid.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bayesCaret
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 7.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
trainIndex &lt;- caret::createDataPartition(
  iris$Species, p = .8, list = FALSE, times = 1)
train &lt;- iris[ trainIndex, ]
test  &lt;- iris[-trainIndex, ]

fitControl &lt;- caret::trainControl(
  method = "repeatedcv", number = 2, repeats = 2)

fit &lt;- caret::train(
  Species ~ ., data = train, method = mmb::bayesCaret,
  trControl = fitControl)

</code></pre>

<hr>
<h2 id='bayesComputeMarginalFactor'>Compute a marginal factor (continuous or discrete random variable).</h2><span id='topic+bayesComputeMarginalFactor'></span>

<h3>Description</h3>

<p>Computes the probability (discrete feature) or relative likelihood
(continuous feature) of one given feature and a concrete value for it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bayesComputeMarginalFactor(df, feature, doEcdf = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bayesComputeMarginalFactor_+3A_df">df</code></td>
<td>
<p>data.frame that contains all the feature's data</p>
</td></tr>
<tr><td><code id="bayesComputeMarginalFactor_+3A_feature">feature</code></td>
<td>
<p>data.frame containing the designated feature as created
by @seealso <code>mmb::createFeatureForBayes()</code>.</p>
</td></tr>
<tr><td><code id="bayesComputeMarginalFactor_+3A_doecdf">doEcdf</code></td>
<td>
<p>default FALSE a boolean to indicate whether to use the
empirical CDF to return a probability when inferencing a continuous
feature. If false, uses the empirical PDF to return the rel. likelihood.
This parameter does not have any effect when inferring discrete values.
Using the ECDF, a probability to find a value less than or equal to the
given value is returned.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric the probability or likelihood of the given feature
assuming its given value.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>feat &lt;- mmb::createFeatureForBayes(
  name = "Petal.Length", value = mean(iris$Petal.Length))
mmb::bayesComputeMarginalFactor(df = iris, feature = feat)
mmb::bayesComputeMarginalFactor(df = iris, feature = feat, doEcdf = TRUE)
</code></pre>

<hr>
<h2 id='bayesComputeProductFactor'>Computes one single factor that is needed for full Bayesian inferencing.</h2><span id='topic+bayesComputeProductFactor'></span>

<h3>Description</h3>

<p>In an equation such as P(A|B) = P(B|A) * P(A) / P(B), the target
feature is A, while the conditional feature is B. There are three factors in that
equation (two in the numerator and one in the denominator). This function
calculates exactly one factor and expects all features to be given in the
right order. If computing the denominator, no target-feature is required.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bayesComputeProductFactor(
  df,
  conditionalFeatures,
  targetFeature,
  computeNumerator,
  retainMinValues = 1,
  doEcdf = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bayesComputeProductFactor_+3A_df">df</code></td>
<td>
<p>data.frame with data that is used to segment</p>
</td></tr>
<tr><td><code id="bayesComputeProductFactor_+3A_conditionalfeatures">conditionalFeatures</code></td>
<td>
<p>data.frame with Bayesian features, as produced
by @seealso <code>mmb::createFeatureForBayes()</code>. This data.frame must not
be empty, as we need to depend on at least one feature.</p>
</td></tr>
<tr><td><code id="bayesComputeProductFactor_+3A_targetfeature">targetFeature</code></td>
<td>
<p>data.frame with exactly one Bayesian feature. Any
excessive features are discarded and a warning is produced. If computing a
factor for the denominator, this data.frame may be empty.</p>
</td></tr>
<tr><td><code id="bayesComputeProductFactor_+3A_computenumerator">computeNumerator</code></td>
<td>
<p>boolean to indicate whether a factor for the
numerator is build. In that case, the target feature is required.</p>
</td></tr>
<tr><td><code id="bayesComputeProductFactor_+3A_retainminvalues">retainMinValues</code></td>
<td>
<p>integer the amount of rows to minimally retain
during segmenting using the conditional features.</p>
</td></tr>
<tr><td><code id="bayesComputeProductFactor_+3A_doecdf">doEcdf</code></td>
<td>
<p>default FALSE a boolean to indicate whether to use the
empirical CDF to return a probability when inferencing a continuous
feature. If false, uses the empirical PDF to return the rel. likelihood.
This parameter does not have any effect when inferring discrete values.
Using the ECDF, a probability to find a value less than or equal to the
given value is returned. Setting this parameter to true in conjunction
with a non-zero shiftAmount must be done with caution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric the factor as probability or relative likelihood. If the
target feature is discrete, a probability is returned; a relative
likelihood, otherwise.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>References</h3>

<p>Bayes T (1763).
&ldquo;LII. An essay towards solving a problem in the doctrine of chances. By the late Rev. Mr. Bayes, FRS communicated by Mr. Price, in a letter to John Canton, AMFR S.&rdquo;
<em>Philosophical transactions of the Royal Society of London</em>, 370&ndash;418.
</p>

<hr>
<h2 id='bayesConvertData'>Convert data for usage within Bayesian models.</h2><span id='topic+bayesConvertData'></span>

<h3>Description</h3>

<p>Converts all columns in a data.frame that are factors to
character, except for the target column.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bayesConvertData(df)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bayesConvertData_+3A_df">df</code></td>
<td>
<p>data.frame to be used for bayesian inferencing.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the same data.frame with all factors converted to character.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>df &lt;- mmb::bayesConvertData(df = iris)
</code></pre>

<hr>
<h2 id='bayesFeaturesToSample'>Transform a collection of Bayesian features back to a sample.</h2><span id='topic+bayesFeaturesToSample'></span>

<h3>Description</h3>

<p>Counter operation to @seealso <code>mmb::sampleToBayesFeatures()</code>.
Takes a Bayes-feature data.frame and transforms it back to a row.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bayesFeaturesToSample(dfOrg, features)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bayesFeaturesToSample_+3A_dforg">dfOrg</code></td>
<td>
<p>data.frame containing at least one row of the original
format, so that we can rebuild the sample matching exactly the
original column names.</p>
</td></tr>
<tr><td><code id="bayesFeaturesToSample_+3A_features">features</code></td>
<td>
<p>data.frame of Bayes-features, as for example
previously created using <code>mmb::sampleToBayesFeatures()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame the sample as 1-row data.frame.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>samp &lt;- mmb::sampleToBayesFeatures(dfRow = iris[15,], targetCol = "Species")

# Convert the sample (as features) back to a sample that can be, e.g.,
# appended to the data again:
row &lt;- mmb::bayesFeaturesToSample(dfOrg = iris, features = samp)
</code></pre>

<hr>
<h2 id='bayesInferSimple'>Perform simple (network) Bayesian inferencing and regression.</h2><span id='topic+bayesInferSimple'></span>

<h3>Description</h3>

<p>Uses simple Bayesian inference to determine the probability or relative
likelihood of a given value. This function can also regress to the most
likely value instead. Simple means that segmented data is used in a way
that is equal to how a Bayesian network works. For a finite set of labels,
this function needs to be called for each, to obtain the probability of
each label (or, for n-1 labels or until a label with &gt;.5 probability is
found). For obtaining the probability of a continuous value, this function
is useful for deciding between picking among a finite set of values. The
empirical CDF may be used to obtain an actual probability for a given
continuous value, otherwise, the empirical PDF is estimated and a relative
likelihood is returned. For regression, set <code>doRegress = TRUE</code> to
obtain the most likely value of the target feature, instead of obtaining
its relative likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bayesInferSimple(
  df,
  features,
  targetCol,
  selectedFeatureNames = c(),
  retainMinValues = 1,
  doRegress = FALSE,
  doEcdf = FALSE,
  regressor = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bayesInferSimple_+3A_df">df</code></td>
<td>
<p>data.frame</p>
</td></tr>
<tr><td><code id="bayesInferSimple_+3A_features">features</code></td>
<td>
<p>data.frame with bayes-features. One of the features needs
to be the label-column.</p>
</td></tr>
<tr><td><code id="bayesInferSimple_+3A_targetcol">targetCol</code></td>
<td>
<p>string with the name of the feature that represents the
label.</p>
</td></tr>
<tr><td><code id="bayesInferSimple_+3A_selectedfeaturenames">selectedFeatureNames</code></td>
<td>
<p>vector default <code>c()</code>. Vector of strings
that are the names of the features the to-predict label depends on. If an
empty vector is given, then all of the features are used (except for the
label). The order then depends on the features' order.</p>
</td></tr>
<tr><td><code id="bayesInferSimple_+3A_retainminvalues">retainMinValues</code></td>
<td>
<p>integer to require a minimum amount of data points
when segmenting the data feature by feature.</p>
</td></tr>
<tr><td><code id="bayesInferSimple_+3A_doregress">doRegress</code></td>
<td>
<p>default FALSE a boolean to indicate whether to do a
regression instead of returning the relative likelihood of a continuous
feature. If the target feature is discrete and regression is requested,
will issue a warning.</p>
</td></tr>
<tr><td><code id="bayesInferSimple_+3A_doecdf">doEcdf</code></td>
<td>
<p>default FALSE a boolean to indicate whether to use the
empirical CDF to return a probability when inferencing a continuous
feature. If false, uses the empirical PDF to return the rel. likelihood.
This parameter does not have any effect when inferring discrete values
or when doing a regression.</p>
</td></tr>
<tr><td><code id="bayesInferSimple_+3A_regressor">regressor</code></td>
<td>
<p>Function that is given the collected values for
regression and thus finally used to select a most likely value. Defaults
to the built-in estimator for the empirical PDF and returns its argmax.
However, any other function can be used, too, such as min, max, median,
average etc. You may also use this function to obtain the raw values
for further processing. This function is ignored if not doing regression.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric probability (inferring discrete labels) or relative
likelihood (regression, inferring likelihood of continuous value) or most
likely value given the conditional features.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>References</h3>

<p>Scutari M (2010).
&ldquo;Learning Bayesian Networks with the <span class="pkg">bnlearn</span> R
      Package.&rdquo;
<em>Journal of Statistical Software</em>, <b>35</b>(3), 1&ndash;22.
doi: <a href="https://doi.org/10.18637/jss.v035.i03">10.18637/jss.v035.i03</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>feat1 &lt;- mmb::createFeatureForBayes(
  name = "Petal.Length", value = mean(iris$Petal.Length))
feat2 &lt;- mmb::createFeatureForBayes(
  name = "Petal.Width", value = mean(iris$Petal.Width))
featT &lt;- mmb::createFeatureForBayes(
  name = "Species", iris[1,]$Species, isLabel = TRUE)

# Infer likelihood of featT's label:
feats &lt;- rbind(feat1, feat2, featT)
mmb::bayesInferSimple(df = iris, features = feats, targetCol = featT$name)

# Infer likelihood of feat1's value:
featT$isLabel = FALSE
feat1$isLabel = TRUE
# We do not bind featT this time:
feats &lt;- rbind(feat1, feat2)
mmb::bayesInferSimple(df = iris, features = feats, targetCol = feat1$name)
</code></pre>

<hr>
<h2 id='bayesProbability'>Full Bayesian inferencing for determining the probability or
relative likelihood of a given value.</h2><span id='topic+bayesProbability'></span>

<h3>Description</h3>

<p>Uses the full extended theorem of Bayes, taking all selected features
into account. Expands Bayes' theorem to accomodate all dependent
features, then calculates each conditional probability (or relative
likelihood) and returns a single result reflecting the probability or
relative likelihood of the target feature assuming its given value,
given that all the other dependent features assume their given value.
The target feature (designated by 'labelCol') may be discrete or continuous.
If at least one of the depending features or the the target feature
is continuous and the PDF ('doEcdf' = FALSE) is built, the result of this
function is a relative likelihood of the target feature's value. If
all of the features are discrete or the empirical CDF is used instead
of the PDF, the result of this function is a probability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bayesProbability(
  df,
  features,
  targetCol,
  selectedFeatureNames = c(),
  shiftAmount = 0.1,
  retainMinValues = 1,
  doEcdf = FALSE,
  useParallel = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bayesProbability_+3A_df">df</code></td>
<td>
<p>data.frame that contains all the feature's data</p>
</td></tr>
<tr><td><code id="bayesProbability_+3A_features">features</code></td>
<td>
<p>data.frame with bayes-features. One of the features needs
to be the label-column.</p>
</td></tr>
<tr><td><code id="bayesProbability_+3A_targetcol">targetCol</code></td>
<td>
<p>string with the name of the feature that represents the
label.</p>
</td></tr>
<tr><td><code id="bayesProbability_+3A_selectedfeaturenames">selectedFeatureNames</code></td>
<td>
<p>vector default <code>c()</code>. Vector of strings
that are the names of the features the to-predict label depends on. If an
empty vector is given, then all of the features are used (except for the
label). The order then depends on the features' order.</p>
</td></tr>
<tr><td><code id="bayesProbability_+3A_shiftamount">shiftAmount</code></td>
<td>
<p>numeric an offset value used to increase any one
probability (factor) in the full built equation. In scenarios with many
dependencies, it is more likely that a single conditional probability
becomes zero, which would result in the entire probability being zero.
Since this is often useless, the 'shiftAmount' can be added to each
factor, resulting in a non-zero probability that can at least be used
to order samples by likelihood. Note that, with a positive 'shiftAmount',
the result of this function cannot be said to be a probability any
longer, but rather results in a comparable likelihood (a 'probability
score').</p>
</td></tr>
<tr><td><code id="bayesProbability_+3A_retainminvalues">retainMinValues</code></td>
<td>
<p>integer to require a minimum amount of data points
when segmenting the data feature by feature.</p>
</td></tr>
<tr><td><code id="bayesProbability_+3A_doecdf">doEcdf</code></td>
<td>
<p>default FALSE a boolean to indicate whether to use the
empirical CDF to return a probability when inferencing a continuous
feature. If false, uses the empirical PDF to return the rel. likelihood.
This parameter does not have any effect if all of the variables are
discrete or when doing a regression. Otherwise, for each continuous
variable, the probability to find a value less then or equal - given
the conditions - is returned. Note that the interpretation of probability
using the ECDF much deviates and must be used with care, especially
since it affects each factor in Bayes equation that is continuous. This
is especially true for the case where <code>shiftAmount &gt; 0</code>.</p>
</td></tr>
<tr><td><code id="bayesProbability_+3A_useparallel">useParallel</code></td>
<td>
<p>default NULL a boolean to indicate whether to use a
previously registered parallel backend. If no explicit value was given,
calls <code>foreach::getDoParRegistered()</code> to check for a parallel
backend. When using parallelism, this function calculates each factor
in the numerator and denominator of the final equation in parallel.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric probability (inferring discrete labels) or relative
likelihood (regression, inferring likelihood of continuous value) or most
likely value given the conditional features. If using a positive
<code>shiftAmount</code>, the result is a 'probability score'.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>References</h3>

<p>Bayes T (1763).
&ldquo;LII. An essay towards solving a problem in the doctrine of chances. By the late Rev. Mr. Bayes, FRS communicated by Mr. Price, in a letter to John Canton, AMFR S.&rdquo;
<em>Philosophical transactions of the Royal Society of London</em>, 370&ndash;418.
</p>


<h3>See Also</h3>

<p>test-case &quot;a zero denominator can happen&quot;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>feat1 &lt;- mmb::createFeatureForBayes(
  name = "Petal.Length", value = mean(iris$Petal.Length))
feat2 &lt;- mmb::createFeatureForBayes(
  name = "Petal.Width", value = mean(iris$Petal.Width))
featT &lt;- mmb::createFeatureForBayes(
  name = "Species", iris[1,]$Species, isLabel = TRUE)

# Check the probability of Species=setosa, given the other 2 features:
mmb::bayesProbability(
  df = iris, features = rbind(feat1, feat2, featT), targetCol = "Species")

# Now check the probability of Species=versicolor:
featT$valueChar &lt;- "versicolor"
mmb::bayesProbability(
  df = iris, features = rbind(feat1, feat2, featT), targetCol = "Species")
</code></pre>

<hr>
<h2 id='bayesProbabilityAssign'>Assign probabilities to one or more samples, given some training data.</h2><span id='topic+bayesProbabilityAssign'></span>

<h3>Description</h3>

<p>This method uses full-dependency (<code>simple=F</code>) Bayesian
inferencing to assign a probability to the target feature in all of the
samples given in <code>dfValid</code>. Tests each sample using @seealso
<code>mmb::bayesProbability()</code> or @seealso <code>mmb::bayesProbabilitySimple()</code>.
It mostly forwards the given arguments to these functions, and you will find
good documentation there.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bayesProbabilityAssign(
  dfTrain,
  dfValid,
  targetCol,
  selectedFeatureNames = c(),
  shiftAmount = 0.1,
  retainMinValues = 1,
  doEcdf = FALSE,
  online = 0,
  simple = FALSE,
  naive = FALSE,
  useParallel = NULL,
  returnProbabilityTable = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bayesProbabilityAssign_+3A_dftrain">dfTrain</code></td>
<td>
<p>data.frame that holds the training data.</p>
</td></tr>
<tr><td><code id="bayesProbabilityAssign_+3A_dfvalid">dfValid</code></td>
<td>
<p>data.frame that holds the validation samples, for each of which
a probability is sought. The convention is, that if you attempt to assign a
probability to a numeric value, it ought to be found in the target column of
this data frame (otherwise, the target column is not required in it).</p>
</td></tr>
<tr><td><code id="bayesProbabilityAssign_+3A_targetcol">targetCol</code></td>
<td>
<p>character the name of targeted feature, i.e., the feature to
assign a probability to.</p>
</td></tr>
<tr><td><code id="bayesProbabilityAssign_+3A_selectedfeaturenames">selectedFeatureNames</code></td>
<td>
<p>character defaults to empty vector which defaults
to using all available features. Use this to select subsets of features and to
order features.</p>
</td></tr>
<tr><td><code id="bayesProbabilityAssign_+3A_shiftamount">shiftAmount</code></td>
<td>
<p>numeric an offset value used to increase any one
probability (factor) in the full built equation.</p>
</td></tr>
<tr><td><code id="bayesProbabilityAssign_+3A_retainminvalues">retainMinValues</code></td>
<td>
<p>integer to require a minimum amount of data points
when segmenting the data feature by feature.</p>
</td></tr>
<tr><td><code id="bayesProbabilityAssign_+3A_doecdf">doEcdf</code></td>
<td>
<p>default FALSE a boolean to indicate whether to use the
empirical CDF to return a probability when inferencing a continuous
feature.</p>
</td></tr>
<tr><td><code id="bayesProbabilityAssign_+3A_online">online</code></td>
<td>
<p>default 0 integer to indicate how many rows should be used to
do inferencing. If zero, then only the initially given data.frame dfTrain is
used. If &gt; 0, then each inferenced sample will be attached to it and the
resulting data.frame is truncated to this number. Use an integer large enough
(i.e., sum of training and validation rows) to keep all samples during
inferencing. A smaller amount as, e.g., in dfTrain, will keep the amount of data
restricted, discarding older rows. A larger amount than, e.g., in dfTrain is
also fine; dfTrain will grow to it and then discard rows.</p>
</td></tr>
<tr><td><code id="bayesProbabilityAssign_+3A_simple">simple</code></td>
<td>
<p>default FALSE boolean to indicate whether or not to use simple
Bayesian inferencing instead of full. This is faster but the results are less
good. If true, uses <code>mmb::bayesProbabilitySimple()</code>. Otherwise, uses
<code>mmb::bayesProbability()</code>.</p>
</td></tr>
<tr><td><code id="bayesProbabilityAssign_+3A_naive">naive</code></td>
<td>
<p>default FALSE boolean to indicate whether or not to use naive
Bayesian inferencing instead of full or simple.</p>
</td></tr>
<tr><td><code id="bayesProbabilityAssign_+3A_useparallel">useParallel</code></td>
<td>
<p>boolean DEFAULT NULL this is forwarded to the underlying
function <code>mmb::bayesProbability()</code> (only in simple=FALSE mode).</p>
</td></tr>
<tr><td><code id="bayesProbabilityAssign_+3A_returnprobabilitytable">returnProbabilityTable</code></td>
<td>
<p>default FALSE boolean to indicate whether to
return only the probabilities for each validation sample or whether a table
with a probability for each tested label should be returned. This has no
effect when inferencing probabilities for numeric values, as the table then
only has one column &quot;probability&quot;. The first column of this table is always
called &quot;rowname&quot; and corresponds to the rownames of dfValid.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>References</h3>

<p>Bayes T (1763).
&ldquo;LII. An essay towards solving a problem in the doctrine of chances. By the late Rev. Mr. Bayes, FRS communicated by Mr. Price, in a letter to John Canton, AMFR S.&rdquo;
<em>Philosophical transactions of the Royal Society of London</em>, 370&ndash;418.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>w &lt;- mmb::getWarnings()
mmb::setWarnings(FALSE)

set.seed(84735)
rn &lt;- base::sample(rownames(iris), 150)
dfTrain &lt;- iris[rn[1:120], ]
dfValid &lt;- iris[rn[121:150], !(colnames(iris) %in% "Species") ]
mmb::bayesProbabilityAssign(dfTrain, dfValid, "Species")

mmb::setWarnings(w)
</code></pre>

<hr>
<h2 id='bayesProbabilityNaive'>Naive Bayesian inferencing for determining the probability or
relative likelihood of a given value.</h2><span id='topic+bayesProbabilityNaive'></span>

<h3>Description</h3>

<p>A complementary implementation using methods common in mmb,
such as computing factors or segmenting data. Supports Laplacian smoothing
and early-stopping segmenting, as well as PDF and CDF and selecting any
subset of features for dependency.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bayesProbabilityNaive(
  df,
  features,
  targetCol,
  selectedFeatureNames = c(),
  shiftAmount = 0.1,
  retainMinValues = 1,
  doEcdf = FALSE,
  useParallel = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bayesProbabilityNaive_+3A_df">df</code></td>
<td>
<p>data.frame that contains all the feature's data</p>
</td></tr>
<tr><td><code id="bayesProbabilityNaive_+3A_features">features</code></td>
<td>
<p>data.frame with bayes-features. One of the features needs
to be the label-column.</p>
</td></tr>
<tr><td><code id="bayesProbabilityNaive_+3A_targetcol">targetCol</code></td>
<td>
<p>string with the name of the feature that represents the
label.</p>
</td></tr>
<tr><td><code id="bayesProbabilityNaive_+3A_selectedfeaturenames">selectedFeatureNames</code></td>
<td>
<p>vector default <code>c()</code>. Vector of strings
that are the names of the features the to-predict label depends on. If an
empty vector is given, then all of the features are used (except for the
label). The order then depends on the features' order.</p>
</td></tr>
<tr><td><code id="bayesProbabilityNaive_+3A_shiftamount">shiftAmount</code></td>
<td>
<p>numeric an offset value used to increase any one
probability (factor) in the full built equation. In scenarios with many
dependencies, it is more likely that a single conditional probability
becomes zero, which would result in the entire probability being zero.
Since this is often useless, the 'shiftAmount' can be added to each
factor, resulting in a non-zero probability that can at least be used
to order samples by likelihood. Note that, with a positive 'shiftAmount',
the result of this function cannot be said to be a probability any
longer, but rather results in a comparable likelihood (a 'probability
score').</p>
</td></tr>
<tr><td><code id="bayesProbabilityNaive_+3A_retainminvalues">retainMinValues</code></td>
<td>
<p>integer to require a minimum amount of data points
when segmenting the data feature by feature.</p>
</td></tr>
<tr><td><code id="bayesProbabilityNaive_+3A_doecdf">doEcdf</code></td>
<td>
<p>default FALSE a boolean to indicate whether to use the
empirical CDF to return a probability when inferencing a continuous
feature. If false, uses the empirical PDF to return the rel. likelihood.
This parameter does not have any effect if all of the variables are
discrete or when doing a regression. Otherwise, for each continuous
variable, the probability to find a value less then or equal - given
the conditions - is returned. Note that the interpretation of probability
using the ECDF much deviates and must be used with care, especially
since it affects each factor in Bayes equation that is continuous. This
is especially true for the case where <code>shiftAmount &gt; 0</code>.</p>
</td></tr>
<tr><td><code id="bayesProbabilityNaive_+3A_useparallel">useParallel</code></td>
<td>
<p>default NULL a boolean to indicate whether to use a
previously registered parallel backend. If no explicit value was given,
calls <code>foreach::getDoParRegistered()</code> to check for a parallel
backend. When using parallelism, this function calculates each factor
in the numerator and denominator of the final equation in parallel.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric probability (inferring discrete labels) or relative
likelihood (regression, inferring likelihood of continuous value) or most
likely value given the conditional features. If using a positive
<code>shiftAmount</code>, the result is a 'probability score'.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>feat1 &lt;- mmb::createFeatureForBayes(
  name = "Petal.Length", value = mean(iris$Petal.Length))
feat2 &lt;- mmb::createFeatureForBayes(
  name = "Petal.Width", value = mean(iris$Petal.Width))
featT &lt;- mmb::createFeatureForBayes(
  name = "Species", iris[1,]$Species, isLabel = TRUE)

# Check the probability of Species=setosa, given the other 2 features:
mmb::bayesProbabilityNaive(
  df = iris, features = rbind(feat1, feat2, featT), targetCol = "Species")

# Now check the probability of Species=versicolor:
featT$valueChar &lt;- "versicolor"
mmb::bayesProbabilityNaive(
  df = iris, features = rbind(feat1, feat2, featT), targetCol = "Species")
</code></pre>

<hr>
<h2 id='bayesProbabilitySimple'>Assign a probability using a simple (network) Bayesian classifier.</h2><span id='topic+bayesProbabilitySimple'></span>

<h3>Description</h3>

<p>Uses simple Bayesian inference to return the probability or
relative likelihood or a discrete label or continuous value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bayesProbabilitySimple(
  df,
  features,
  targetCol,
  selectedFeatureNames = c(),
  retainMinValues = 1,
  doEcdf = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bayesProbabilitySimple_+3A_df">df</code></td>
<td>
<p>data.frame</p>
</td></tr>
<tr><td><code id="bayesProbabilitySimple_+3A_features">features</code></td>
<td>
<p>data.frame with bayes-features. One of the features needs to
be the label-column.</p>
</td></tr>
<tr><td><code id="bayesProbabilitySimple_+3A_targetcol">targetCol</code></td>
<td>
<p>string with the name of the feature that represents the label.</p>
</td></tr>
<tr><td><code id="bayesProbabilitySimple_+3A_selectedfeaturenames">selectedFeatureNames</code></td>
<td>
<p>vector default <code>c()</code>. Vector of strings that
are the names of the features the to-predict label depends on. If an empty
vector is given, then all of the features are used (except for the label). The
order then depends on the features' order.</p>
</td></tr>
<tr><td><code id="bayesProbabilitySimple_+3A_retainminvalues">retainMinValues</code></td>
<td>
<p>integer to require a minimum amount of data points
when segmenting the data feature by feature.</p>
</td></tr>
<tr><td><code id="bayesProbabilitySimple_+3A_doecdf">doEcdf</code></td>
<td>
<p>default FALSE a boolean to indicate whether to use the
empirical CDF to return a probability when inferencing a continuous
feature. If false, uses the empirical PDF to return the rel. likelihood.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>double the probability of the target-label, using the maximum a
posteriori estimate.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>References</h3>

<p>Scutari M (2010).
&ldquo;Learning Bayesian Networks with the <span class="pkg">bnlearn</span> R
      Package.&rdquo;
<em>Journal of Statistical Software</em>, <b>35</b>(3), 1&ndash;22.
doi: <a href="https://doi.org/10.18637/jss.v035.i03">10.18637/jss.v035.i03</a>.
</p>


<h3>See Also</h3>

<p><code>mmb::bayesInferSimple()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>feat1 &lt;- mmb::createFeatureForBayes(
  name = "Sepal.Length", value = mean(iris$Sepal.Length))
feat2 &lt;- mmb::createFeatureForBayes(
  name = "Sepal.Width", value = mean(iris$Sepal.Width), isLabel = TRUE)

# Assign a probability to a continuous variable (also works with nominal):
mmb::bayesProbabilitySimple(df = iris, features = rbind(feat1, feat2),
  targetCol = feat2$name, retainMinValues = 5, doEcdf = TRUE)
</code></pre>

<hr>
<h2 id='bayesRegress'>Perform full-dependency Bayesian regression for a sample.</h2><span id='topic+bayesRegress'></span>

<h3>Description</h3>

<p>This method performs full-dependency regression by
discretizing the continuous target variable into ranges (buckets),
then finding the most probable ranges. It can either regress on
the values in the most likely range or sample from all ranges,
according to their likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bayesRegress(
  df,
  features,
  targetCol,
  selectedFeatureNames = c(),
  shiftAmount = 0.1,
  retainMinValues = 2,
  doEcdf = FALSE,
  useParallel = NULL,
  numBuckets = ceiling(log2(nrow(df))),
  sampleFromAllBuckets = TRUE,
  regressor = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bayesRegress_+3A_df">df</code></td>
<td>
<p>data.frame that contains all the feature's data</p>
</td></tr>
<tr><td><code id="bayesRegress_+3A_features">features</code></td>
<td>
<p>data.frame with bayes-features. One of the features needs
to be the label-column.</p>
</td></tr>
<tr><td><code id="bayesRegress_+3A_targetcol">targetCol</code></td>
<td>
<p>string with the name of the feature that represents the
label.</p>
</td></tr>
<tr><td><code id="bayesRegress_+3A_selectedfeaturenames">selectedFeatureNames</code></td>
<td>
<p>vector default <code>c()</code>. Vector of strings
that are the names of the features the to-predict label depends on. If an
empty vector is given, then all of the features are used (except for the
label). The order then depends on the features' order.</p>
</td></tr>
<tr><td><code id="bayesRegress_+3A_shiftamount">shiftAmount</code></td>
<td>
<p>numeric an offset value used to increase any one
probability (factor) in the full built equation. In scenarios with many
dependencies, it is more likely that a single conditional probability
becomes zero, which would result in the entire probability being zero.
Since this is often useless, the 'shiftAmount' can be added to each
factor, resulting in a non-zero probability that can at least be used
to order samples by likelihood. Note that, with a positive 'shiftAmount',
the result of this function cannot be said to be a probability any
longer, but rather results in a comparable likelihood (a 'probability
score').</p>
</td></tr>
<tr><td><code id="bayesRegress_+3A_retainminvalues">retainMinValues</code></td>
<td>
<p>integer to require a minimum amount of data points
when segmenting the data feature by feature.</p>
</td></tr>
<tr><td><code id="bayesRegress_+3A_doecdf">doEcdf</code></td>
<td>
<p>default FALSE a boolean to indicate whether to use the
empirical CDF to return a probability when inferencing a continuous
feature. If false, uses the empirical PDF to return the rel. likelihood.
This parameter does not have any effect if all of the variables are
discrete or when doing a regression. Otherwise, for each continuous
variable, the probability to find a value less then or equal - given
the conditions - is returned. Note that the interpretation of probability
using the ECDF much deviates and must be used with care, especially
since it affects each factor in Bayes equation that is continuous. This
is especially true for the case where <code>shiftAmount &gt; 0</code>.</p>
</td></tr>
<tr><td><code id="bayesRegress_+3A_useparallel">useParallel</code></td>
<td>
<p>default NULL a boolean to indicate whether to use a
previously registered parallel backend. If no explicit value was given,
calls <code>foreach::getDoParRegistered()</code> to check for a parallel
backend. When using parallelism, this function calculates each factor
in the numerator and denominator of the final equation in parallel.</p>
</td></tr>
<tr><td><code id="bayesRegress_+3A_numbuckets">numBuckets</code></td>
<td>
<p>integer the amount of buckets to for discretization.
Buckets are built in an equidistant manner, not as quantiles (i.e., one
bucket has likely a different amount of values than another).</p>
</td></tr>
<tr><td><code id="bayesRegress_+3A_samplefromallbuckets">sampleFromAllBuckets</code></td>
<td>
<p>default TRUE boolean to indicate how to
obtain values for regression from the buckets. If true, than takes
values from those buckets with a non-zero probability, and according
to their probability. If false, selects all values from the bucket
with the highest probability.</p>
</td></tr>
<tr><td><code id="bayesRegress_+3A_regressor">regressor</code></td>
<td>
<p>Function that is given the collected values for
regression and thus finally used to select a most likely value. Defaults
to the built-in estimator for the empirical PDF and returns its argmax.
However, any other function can be used, too, such as min, max, median,
average etc. You may also use this function to obtain the raw values
for further processing.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>w &lt;- mmb::getWarnings()
mmb::setWarnings(FALSE)

df &lt;- iris[, ]
set.seed(84735)
rn &lt;- base::sample(rownames(df), 150)
dfTrain &lt;- df[1:120, ]
dfValid &lt;- df[121:150, ]
tf &lt;- mmb::sampleToBayesFeatures(dfValid[1,], "Sepal.Length")
mmb::bayesRegress(dfTrain, tf, "Sepal.Length")

mmb::setWarnings(w)
</code></pre>

<hr>
<h2 id='bayesRegressAssign'>Regression for one or more samples, given some training data.</h2><span id='topic+bayesRegressAssign'></span>

<h3>Description</h3>

<p>This method uses full-dependency (<code>simple=F</code>) Bayesian
inferencing to to a regression for the target features for all of the
samples given in <code>dfValid</code>. Assigns a regression value using either
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bayesRegressAssign(
  dfTrain,
  dfValid,
  targetCol,
  selectedFeatureNames = c(),
  shiftAmount = 0.1,
  retainMinValues = 2,
  doEcdf = FALSE,
  online = 0,
  simple = FALSE,
  useParallel = NULL,
  numBuckets = ceiling(log2(nrow(df))),
  sampleFromAllBuckets = TRUE,
  regressor = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bayesRegressAssign_+3A_dftrain">dfTrain</code></td>
<td>
<p>data.frame that holds the training data.</p>
</td></tr>
<tr><td><code id="bayesRegressAssign_+3A_dfvalid">dfValid</code></td>
<td>
<p>data.frame that holds the validation samples, for each of which
a probability is sought. The convention is, that if you attempt to assign a
probability to a numeric value, it ought to be found in the target column of
this data frame (otherwise, the target column is not required in it).</p>
</td></tr>
<tr><td><code id="bayesRegressAssign_+3A_targetcol">targetCol</code></td>
<td>
<p>character the name of targeted feature, i.e., the feature to
assign a probability to.</p>
</td></tr>
<tr><td><code id="bayesRegressAssign_+3A_selectedfeaturenames">selectedFeatureNames</code></td>
<td>
<p>character defaults to empty vector which defaults
to using all available features. Use this to select subsets of features and to
order features.</p>
</td></tr>
<tr><td><code id="bayesRegressAssign_+3A_shiftamount">shiftAmount</code></td>
<td>
<p>numeric an offset value used to increase any one
probability (factor) in the full built equation.</p>
</td></tr>
<tr><td><code id="bayesRegressAssign_+3A_retainminvalues">retainMinValues</code></td>
<td>
<p>integer to require a minimum amount of data points
when segmenting the data feature by feature.</p>
</td></tr>
<tr><td><code id="bayesRegressAssign_+3A_doecdf">doEcdf</code></td>
<td>
<p>default FALSE a boolean to indicate whether to use the
empirical CDF to return a probability when inferencing a continuous
feature.</p>
</td></tr>
<tr><td><code id="bayesRegressAssign_+3A_online">online</code></td>
<td>
<p>default 0 integer to indicate how many rows should be used to
do inferencing. If zero, then only the initially given data.frame dfTrain is
used. If &gt; 0, then each inferenced sample will be attached to it and the
resulting data.frame is truncated to this number. Use an integer large enough
(i.e., sum of training and validation rows) to keep all samples during
inferencing. A smaller amount as, e.g., in dfTrain, will keep the amount of data
restricted, discarding older rows. A larger amount than, e.g., in dfTrain is
also fine; dfTrain will grow to it and then discard rows.</p>
</td></tr>
<tr><td><code id="bayesRegressAssign_+3A_simple">simple</code></td>
<td>
<p>default FALSE boolean to indicate whether or not to use simple
Bayesian inferencing instead of full. This is faster but the results are less
good. If true, uses <code>mmb::bayesRegressSimple()</code>. Otherwise, uses
<code>mmb::bayesRegress()</code>.</p>
</td></tr>
<tr><td><code id="bayesRegressAssign_+3A_useparallel">useParallel</code></td>
<td>
<p>boolean DEFAULT NULL this is forwarded to the underlying
function <code>mmb::bayesRegress()</code> (only in simple=FALSE mode).</p>
</td></tr>
<tr><td><code id="bayesRegressAssign_+3A_numbuckets">numBuckets</code></td>
<td>
<p>integer the amount of buckets to for discretization.
Buckets are built in an equidistant manner, not as quantiles (i.e., one
bucket has likely a different amount of values than another).</p>
</td></tr>
<tr><td><code id="bayesRegressAssign_+3A_samplefromallbuckets">sampleFromAllBuckets</code></td>
<td>
<p>default TRUE boolean to indicate how to
obtain values for regression from the buckets. If true, than takes
values from those buckets with a non-zero probability, and according
to their probability. If false, selects all values from the bucket
with the highest probability.</p>
</td></tr>
<tr><td><code id="bayesRegressAssign_+3A_regressor">regressor</code></td>
<td>
<p>Function that is given the collected values for
regression and thus finally used to select a most likely value. Defaults
to the built-in estimator for the empirical PDF and returns its argmax.
However, any other function can be used, too, such as min, max, median,
average etc. You may also use this function to obtain the raw values
for further processing.#'</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>See Also</h3>

<p><code>mmb::bayesRegress()</code> (full) or @seealso <code>mmb::bayesRegressSimple()</code>
if <code>simple=T</code>. It mostly forwards the given arguments to these functions,
and you will find good documentation there.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
df &lt;- iris[, ]
set.seed(84735)
rn &lt;- base::sample(rownames(df), 150)
dfTrain &lt;- df[1:120, ]
dfValid &lt;- df[121:150, ]
res &lt;- mmb::bayesRegressAssign(
  dfTrain, dfValid[, !(colnames(dfValid) %in% "Sepal.Length")],
  "Sepal.Length", sampleFromAllBuckets = TRUE, doEcdf = TRUE)
cov(res, iris[121:150,]$Sepal.Length)^2

</code></pre>

<hr>
<h2 id='bayesRegressSimple'>Perform simple (network) Bayesian regression.</h2><span id='topic+bayesRegressSimple'></span>

<h3>Description</h3>

<p>Uses simple Bayesian inferencing to segment the data given the
conditional features. Then estimates a density over the remaining values of
the target feature and returns the most likely value using a maximum a posteriori
estimate of the kernel (returning its mode).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bayesRegressSimple(
  df,
  features,
  targetCol,
  selectedFeatureNames = c(),
  retainMinValues = 2,
  regressor = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bayesRegressSimple_+3A_df">df</code></td>
<td>
<p>data.frame</p>
</td></tr>
<tr><td><code id="bayesRegressSimple_+3A_features">features</code></td>
<td>
<p>data.frame with bayes-features. One of the features needs to
be the label-column (not required or no value required).</p>
</td></tr>
<tr><td><code id="bayesRegressSimple_+3A_targetcol">targetCol</code></td>
<td>
<p>string with the name of the feature that represents the label
(here the target variable for regression).</p>
</td></tr>
<tr><td><code id="bayesRegressSimple_+3A_selectedfeaturenames">selectedFeatureNames</code></td>
<td>
<p>vector default <code>c()</code>. Vector of strings that
are the names of the features the to-predict label depends on. If an empty
vector is given, then all of the features are used (except for the label). The
order then depends on the features' order.</p>
</td></tr>
<tr><td><code id="bayesRegressSimple_+3A_retainminvalues">retainMinValues</code></td>
<td>
<p>integer to require a minimum amount of data points
when segmenting the data feature by feature.</p>
</td></tr>
<tr><td><code id="bayesRegressSimple_+3A_regressor">regressor</code></td>
<td>
<p>Function that is given the collected values for
regression and thus finally used to select a most likely value. Defaults
to the built-in estimator for the empirical PDF and returns its argmax.
However, any other function can be used, too, such as min, max, median,
average etc. You may also use this function to obtain the raw values
for further processing.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>References</h3>

<p>Scutari M (2010).
&ldquo;Learning Bayesian Networks with the <span class="pkg">bnlearn</span> R
      Package.&rdquo;
<em>Journal of Statistical Software</em>, <b>35</b>(3), 1&ndash;22.
doi: <a href="https://doi.org/10.18637/jss.v035.i03">10.18637/jss.v035.i03</a>.
</p>


<h3>See Also</h3>

<p><code>mmb::bayesInferSimple()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>feat1 &lt;- mmb::createFeatureForBayes(
  name = "Sepal.Length", value = mean(iris$Sepal.Length))
feat2 &lt;- mmb::createFeatureForBayes(
  name = "Sepal.Width", value = mean(iris$Sepal.Width))

# Note how we do not require "Petal.Length" among the features when regressing:
mmb::bayesRegressSimple(df = iris, features = rbind(feat1, feat2),
  targetCol = "Petal.Length")
</code></pre>

<hr>
<h2 id='bayesToLatex'>Create a string that can be used in Latex in an e.g. equation-environment.</h2><span id='topic+bayesToLatex'></span>

<h3>Description</h3>

<p>This function can be used to generate Latex-markup that models the
full dependency between covariates and a target variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bayesToLatex(conditionalFeatures, targetFeature, includeValues = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bayesToLatex_+3A_conditionalfeatures">conditionalFeatures</code></td>
<td>
<p>data.frame of Bayesian features, the target
feature depends on.</p>
</td></tr>
<tr><td><code id="bayesToLatex_+3A_targetfeature">targetFeature</code></td>
<td>
<p>data.frame that holds exactly one Bayesian feature,
that is supposed to be the target-feture for Bayesian inferencing.</p>
</td></tr>
<tr><td><code id="bayesToLatex_+3A_includevalues">includeValues</code></td>
<td>
<p>default FALSE boolean to indicate whether to include
the features' values or not, i.e. &quot;A&quot; vs. &quot;A = setosa&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a string that can be used in Latex documents.
</p>


<h3>Note</h3>

<p>Use <code>cat()</code> to print a string that can be copy-pasted.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>feat1 &lt;- mmb::createFeatureForBayes(
  name = "Petal.Length", value = mean(iris$Petal.Length))
feat2 &lt;- mmb::createFeatureForBayes(
  name = "Petal.Width", value = mean(iris$Petal.Width))
featT &lt;- mmb::createFeatureForBayes(
  name = "Species", iris[1,]$Species, isLabel = TRUE)

cat(mmb::bayesToLatex(conditionalFeatures = rbind(feat1, feat2),
  targetFeature = featT, includeValues = TRUE))
</code></pre>

<hr>
<h2 id='centralities'>Given a neighborhood of data, computes the similarity of each sample
in the neighborhood to the neighborhood.</h2><span id='topic+centralities'></span>

<h3>Description</h3>

<p>Takes a data.frame of samples, then builds a PDF/PMF or ECDF
for each of the selected features. Then, for each sample, computes the product
of probabilities. The result is a vector that holds a probability for each
sample. That probability (or relative likelihood) then represents the
vicinity (or similarity) of the sample to the given neighborhood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>centralities(
  dfNeighborhood,
  selectedFeatureNames = c(),
  shiftAmount = 0.1,
  doEcdf = FALSE,
  ecdfMinusOne = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="centralities_+3A_dfneighborhood">dfNeighborhood</code></td>
<td>
<p>data.frame that holds all rows that make up the neighborhood.</p>
</td></tr>
<tr><td><code id="centralities_+3A_selectedfeaturenames">selectedFeatureNames</code></td>
<td>
<p>vector of names of features to use. The centrality
of each row in the neighborhood is calculated based on the selected features.</p>
</td></tr>
<tr><td><code id="centralities_+3A_shiftamount">shiftAmount</code></td>
<td>
<p>numeric DEFAULT 0.1 optional amount to shift each features
probability by. This is useful for when the centrality not necessarily must be
an actual probability and too many features are selected. To obtain actual
probabilities, this needs to be 0, and you must use the ECDF.</p>
</td></tr>
<tr><td><code id="centralities_+3A_doecdf">doEcdf</code></td>
<td>
<p>boolean DEFAULT FALSE whether to use the ECDF instead of the EPDF
to find the likelihood of continuous values.</p>
</td></tr>
<tr><td><code id="centralities_+3A_ecdfminusone">ecdfMinusOne</code></td>
<td>
<p>boolean DEFAULT FALSE only has an effect if the ECDF is
used. If true, uses 1 minus the ECDF to find the probability of a continuous
value. Depending on the interpretation of what you try to do, this may be of use.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named vector, where the names correspond to the rownames of the rows
in the given neighborhood, and the value is the centrality of that row.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create a neighborhood:
nbh &lt;- mmb::neighborhood(df = iris, features = mmb::createFeatureForBayes(
  name = "Sepal.Width", value = mean(iris$Sepal.Width)))

cent &lt;- mmb::centralities(dfNeighborhood = nbh, shiftAmount = 0.1,
  doEcdf = TRUE, ecdfMinusOne = TRUE)

# Plot the ordered samples to get an idea of the centralities in the neighborhood:
plot(x = names(cent), y=cent)
</code></pre>

<hr>
<h2 id='checkBayesFeature'>Validate a Bayesian feature using some sanity checks.</h2><span id='topic+checkBayesFeature'></span>

<h3>Description</h3>

<p>Internal function to check common arguments for function
that use samples transformed to bayes-features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>checkBayesFeature(dfFeature, featName)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="checkBayesFeature_+3A_dffeature">dfFeature</code></td>
<td>
<p>a data.frame for a single feature or variable
as constructed by @seealso <code>createFeatureForBayes</code>.</p>
</td></tr>
<tr><td><code id="checkBayesFeature_+3A_featname">featName</code></td>
<td>
<p>the name of the feature or variable of which to
obtain the value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame the row corresponding to the given feature name
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>

<hr>
<h2 id='conditionalDataMin'>Segment data according to one or more random variables.</h2><span id='topic+conditionalDataMin'></span>

<h3>Description</h3>

<p>Takes a data.frame and segments it, according to the selected
variables. Only rows satisfying all conditions are kept. Supports discrete
and continuous variables. Supports NA, NaN and NULL by using is.na, is.nan
and is.null as comparator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conditionalDataMin(
  df,
  features,
  selectedFeatureNames = c(),
  retainMinValues = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conditionalDataMin_+3A_df">df</code></td>
<td>
<p>data.frame with data to segment. If it contains less than or
equally many rows as specified by <code>retainMinValues</code>, then the same
data.frame is returned.</p>
</td></tr>
<tr><td><code id="conditionalDataMin_+3A_features">features</code></td>
<td>
<p>data.frame of bayes-features that are used to segment.
Each feature's value is used to segment the data, and the features are
used in the order as given by <code>selectedFeatureNames</code>. If those are
not given, then the order of this data.frame is used.</p>
</td></tr>
<tr><td><code id="conditionalDataMin_+3A_selectedfeaturenames">selectedFeatureNames</code></td>
<td>
<p>default <code>c()</code>. Character vector with the
names of the variables that shall be used for segmenting. Segmenting is
done variable by variable, and the order depends on this vector. If this
vector is empty, then the originally given data.frame is returned.</p>
</td></tr>
<tr><td><code id="conditionalDataMin_+3A_retainminvalues">retainMinValues</code></td>
<td>
<p>default 1. The minimum amount of rows to retain.
Filtering the data by the selected features may reduce the amount of
remaining rows quickly, and this can be used as an early stopping criteria.
Note that filtering is done variable by variable, and the amount of
remaining rows is evaluated after each segmenting-step. If the threshold
is undercut, then the result from the previous round is returned.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame that is segmented according to the selected variables
and the minimum amount of rows to retain.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>See Also</h3>

<p><code>getValueKeyOfBayesFeatures()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>feat1 &lt;- mmb::createFeatureForBayes(
  name = "Petal.Length", value = mean(iris$Petal.Length))
feat2 &lt;- mmb::createFeatureForBayes(
  name = "Petal.Width", value = mean(iris$Petal.Width))
feats &lt;- rbind(feat1, feat2)

data &lt;- mmb::conditionalDataMin(df = iris, features = feats,
  selectedFeatureNames = feats$name, retainMinValues = 1)
</code></pre>

<hr>
<h2 id='createFeatureForBayes'>Create a Bayesian feature by name and value.</h2><span id='topic+createFeatureForBayes'></span>

<h3>Description</h3>

<p>Transforms a sample's feature's value into a dataframe, that holds
its name, type and value. Currently supports numeric, factor, character
and boolean values. Note that factor is internally converted to
character.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createFeatureForBayes(name, value, isLabel = FALSE, isDiscrete = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="createFeatureForBayes_+3A_name">name</code></td>
<td>
<p>the name of the feature or variable.</p>
</td></tr>
<tr><td><code id="createFeatureForBayes_+3A_value">value</code></td>
<td>
<p>the value of the feature or variable.</p>
</td></tr>
<tr><td><code id="createFeatureForBayes_+3A_islabel">isLabel</code></td>
<td>
<p>default FALSE. Indicates whether this feature or variable
is the target variable (the label or value to predict).</p>
</td></tr>
<tr><td><code id="createFeatureForBayes_+3A_isdiscrete">isDiscrete</code></td>
<td>
<p>default FALSE. Used to indicate whether the feature or
variable given is discrete. This will also be set to true if the value
given is a charater, factor or a logical.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame with one row holding all the feature's value's
properties.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>See Also</h3>

<p><code>sampleToBayesFeatures</code> that uses this function
</p>


<h3>Examples</h3>

<pre><code class='language-R'>feat &lt;- mmb::createFeatureForBayes(
  name = "Petal.Width", value = mean(iris$Petal.Width))
featTarget &lt;- mmb::createFeatureForBayes(
  name = "Species", iris[1,]$Species, isLabel = TRUE)
</code></pre>

<hr>
<h2 id='discretizeVariableToRanges'>Discretize a continuous random variable to ranges/buckets.</h2><span id='topic+discretizeVariableToRanges'></span>

<h3>Description</h3>

<p>Discretizes a continuous random variable into buckets (ranges).
Each range is delimited by an exclusive minimum value and an inclusive maximum value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>discretizeVariableToRanges(
  data,
  openEndRanges = TRUE,
  numRanges = NA,
  exclMinVal = NULL,
  inclMaxVal = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="discretizeVariableToRanges_+3A_data">data</code></td>
<td>
<p>a vector with numeric data</p>
</td></tr>
<tr><td><code id="discretizeVariableToRanges_+3A_openendranges">openEndRanges</code></td>
<td>
<p>boolean default True. If true, then the minimum value
of the first range will be set to @seealso <code>.Machine$double.xmin</code> and
the maximum value of the last range will be set to @seealso <code>.Machine$double.xmax</code>,
so that all values get covered.</p>
</td></tr>
<tr><td><code id="discretizeVariableToRanges_+3A_numranges">numRanges</code></td>
<td>
<p>integer default NA. If NULL, then the amount of ranges
(buckets) depends on the amount of data given. A minimum of two buckets is
used then, and a maximum of ceiling(log2(length(data))).</p>
</td></tr>
<tr><td><code id="discretizeVariableToRanges_+3A_exclminval">exclMinVal</code></td>
<td>
<p>numeric default NULL. Used to delimit the lower bound of
the given data. If not given, then no value is excluded, as the exclusive
lower bound becomes the minimum of the given data minus an epsilon of 1e-15.</p>
</td></tr>
<tr><td><code id="discretizeVariableToRanges_+3A_inclmaxval">inclMaxVal</code></td>
<td>
<p>numeric default NULL. Used to delimit the upper bound of
the given data. If not given, then the upper inclusive bound is the max of
the given data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List a List of vectors, where each vector has two values, the first
being the exclusive minimum value of the range, and the second being the
inclusive maximum value of the range. The list will be as long as the number
of buckets requested.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>buckets &lt;- mmb::discretizeVariableToRanges(
  data = iris$Sepal.Length, openEndRanges = TRUE)

length(buckets)
buckets[[5]]
</code></pre>

<hr>
<h2 id='distance'>Given a neighborhood of data and two samples from that neighborhood,
calculates the distance between the samples.</h2><span id='topic+distance'></span>

<h3>Description</h3>

<p>The distance of two samples x,y from each other within a given
neighborhood is defined as the absolute value of the subtraction of each
sample's centrality to the neighborhood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distance(
  dfNeighborhood,
  rowNrOfSample1,
  rowNrOfSample2,
  selectedFeatureNames = c(),
  shiftAmount = 0.1,
  doEcdf = FALSE,
  ecdfMinusOne = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distance_+3A_dfneighborhood">dfNeighborhood</code></td>
<td>
<p>data.frame that holds all rows that make up the neighborhood.</p>
</td></tr>
<tr><td><code id="distance_+3A_rownrofsample1">rowNrOfSample1</code></td>
<td>
<p>character the name of the row that constitutes the first
sample from the given neighborhood.</p>
</td></tr>
<tr><td><code id="distance_+3A_rownrofsample2">rowNrOfSample2</code></td>
<td>
<p>character the name of the row that constitutes the second
sample from the given neighborhood.</p>
</td></tr>
<tr><td><code id="distance_+3A_selectedfeaturenames">selectedFeatureNames</code></td>
<td>
<p>vector of names of features to use. The centrality
of each row in the neighborhood is calculated based on the selected features.</p>
</td></tr>
<tr><td><code id="distance_+3A_shiftamount">shiftAmount</code></td>
<td>
<p>numeric DEFAULT 0.1 optional amount to shift each features
probability by. This is useful for when the centrality not necessarily must be
an actual probability and too many features are selected. To obtain actual
probabilities, this needs to be 0, and you must use the ECDF.</p>
</td></tr>
<tr><td><code id="distance_+3A_doecdf">doEcdf</code></td>
<td>
<p>boolean DEFAULT FALSE whether to use the ECDF instead of the EPDF
to find the likelihood of continuous values.</p>
</td></tr>
<tr><td><code id="distance_+3A_ecdfminusone">ecdfMinusOne</code></td>
<td>
<p>boolean DEFAULT FALSE only has an effect if the ECDF is
used. If true, uses 1 minus the ECDF to find the probability of a continuous
value. Depending on the interpretation of what you try to do, this may be of use.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric the distance as a positive number.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Show the distance between two samples using all their features:
mmb::distance(dfNeighborhood = iris, rowNrOfSample1 = 10, rowNrOfSample2 = 99)

# Let's use an actual neighborhood:
nbh &lt;- mmb::neighborhood(df = iris, features = mmb::createFeatureForBayes(
  name = "Sepal.Length", value = mean(iris$Sepal.Length)))
mmb::distance(dfNeighborhood = nbh, rowNrOfSample1 = 1, rowNrOfSample2 = 30,
  selectedFeatureNames = colnames(iris)[1:3])

# Let's compare this to the distances as they are in iris (should be smaller):
mmb::distance(dfNeighborhood = iris, rowNrOfSample1 = 1, rowNrOfSample2 = 30,
  selectedFeatureNames = colnames(iris)[1:3])
</code></pre>

<hr>
<h2 id='estimatePdf'>Safe PDF estimation that works also for sparse random variables.</h2><span id='topic+estimatePdf'></span>

<h3>Description</h3>

<p>Given a few observations of a random variable, this function
returns an approximation of the PDF as a function. Returns also the PDF's
support and argmax and works when only zero or one value was given. Depending
on the used density function, two values are often enough to estimate a PDF.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimatePdf(
  data = c(),
  densFun = function(vec) {     stats::density(vec, bw = "SJ") }
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimatePdf_+3A_data">data</code></td>
<td>
<p>vector of numeric data. Used to compute the empirical density
of the data.</p>
</td></tr>
<tr><td><code id="estimatePdf_+3A_densfun">densFun</code></td>
<td>
<p>function default <code>stats::density</code> with bandwith 'SJ'.
Function to compute the empirical density of a non-empty vector of numerical
data. Note that this function needs to return the properties 'x' and 'y' as
<code>stats::density</code> does, so that we can return the argmax.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with a function that is the empirical PDF using KDE. The list
also has two properties 'min' and 'max' which represent the integratable
range of that function. 'min' and 'max' are both zero if not data (an
empty vector) was given. If one data point was given, then they correspond
to its value -/+ <code>.Machine$double.eps</code>. The list further contains two
numeric vectors 'x' and 'y', and a property 'argmax'. If no data was given,
'x' and 'y' are zero, and 'argmax' is NA. If one data points was given,
then 'x' and 'argmax' equal it, and 'y' is set to 1. If two or more data
points given, then the empirical density is estimated and 'x' and y' are
filled from its estimate. 'argmax' is then set to that 'x', where 'y'
becomes max.
</p>


<h3>Note</h3>

<p>If the given vector is empty, warns and returns a constant function
that always returns zero for all values.
</p>
<p>If the given vector contains only one observation, then a function
is returned that returns 1 iff the value supplied is the same as the
observation. Otherwise, that function will return zero.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>epdf &lt;- mmb::estimatePdf(data = iris$Petal.Width)
print(epdf$argmax)
plot(epdf)

# Get relative likelihood of some values:
epdf$fun(0.5)
epdf$fun(1.7)
</code></pre>

<hr>
<h2 id='getDefaultRegressor'>Get the system-wide default regressor.</h2><span id='topic+getDefaultRegressor'></span>

<h3>Description</h3>

<p>Getting and setting the default regressor affects all functions
that have an overridable regressor. If this is not given, the default has
defined here will be obtained.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getDefaultRegressor()
</code></pre>


<h3>Value</h3>

<p>Function the function used as the regressor. Defaults to
<code>function(data) mmb::estimatePdf(data)$argmax</code>.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>

<hr>
<h2 id='getMessages'>Get a boolean indicating whether messages are enabled system-wide.</h2><span id='topic+getMessages'></span>

<h3>Description</h3>

<p>Getter for the state of messages. Returns true if enabled.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getMessages()
</code></pre>


<h3>Value</h3>

<p>Boolean to indicate whether messages are enabled or not.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>

<hr>
<h2 id='getProbForDiscrete'>Get a probability of a discrete value.</h2><span id='topic+getProbForDiscrete'></span>

<h3>Description</h3>

<p>Similar to @seealso <code>estimatePdf</code>, this function returns
the probability for a discrete value, given some observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getProbForDiscrete(data, value)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getProbForDiscrete_+3A_data">data</code></td>
<td>
<p>vector of observations that have the same type as the given value.</p>
</td></tr>
<tr><td><code id="getProbForDiscrete_+3A_value">value</code></td>
<td>
<p>a single observation of the same type as the data vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the probability of value given data.
</p>


<h3>Note</h3>

<p>If no observations are given, then this function will warn and return
a probability of zero for the value given. While we could technically return
positive infinity, 0 is more suitable in the context of Bayesian inferencing.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mmb::getProbForDiscrete(data = c(), value = iris[1,]$Species)
mmb::getProbForDiscrete(data = iris$Species, value = iris[1,]$Species)
</code></pre>

<hr>
<h2 id='getRangeForDiscretizedValue'>Get the range-/bucket-ID of a given value.</h2><span id='topic+getRangeForDiscretizedValue'></span>

<h3>Description</h3>

<p>Given a list of previously computed ranges for a random
variable, this function returns the index of the range the given value
belongs to (i.e., in which bucket it belongs). The indexes start R-typically
at 1. Per definition, a value is within a range, if it is larger than the
range's minimum and less than or equal to its maximum.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getRangeForDiscretizedValue(ranges, value)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getRangeForDiscretizedValue_+3A_ranges">ranges</code></td>
<td>
<p>list of ranges, as obtained by @seealso <code>discretizeVariableToRanges</code></p>
</td></tr>
<tr><td><code id="getRangeForDiscretizedValue_+3A_value">value</code></td>
<td>
<p>numeric a value drawn from the previously discretized
random variable.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>integer the index of the range the given value falls into.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>buckets &lt;- mmb::discretizeVariableToRanges(
  data = iris$Sepal.Length, openEndRanges = TRUE)

mmb::getRangeForDiscretizedValue(
  ranges = buckets, value = mean(iris$Sepal.Length))
</code></pre>

<hr>
<h2 id='getValueKeyOfBayesFeatures'>Obtain the type of the value of a Bayesian feature.</h2><span id='topic+getValueKeyOfBayesFeatures'></span>

<h3>Description</h3>

<p>Given a data.frame with one or multiple features as
constructed by @seealso <code>createFeatureForBayes</code> and a name,
extracts the type of the feature specified by name. Note that this
is only used internally.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getValueKeyOfBayesFeatures(dfFeature, featName)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getValueKeyOfBayesFeatures_+3A_dffeature">dfFeature</code></td>
<td>
<p>a data.frame for a single feature or variable
as constructed by @seealso <code>createFeatureForBayes</code>.</p>
</td></tr>
<tr><td><code id="getValueKeyOfBayesFeatures_+3A_featname">featName</code></td>
<td>
<p>the name of the feature or variable of which to
obtain the type.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the (internal) type of the feature.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>feats &lt;- rbind(
  mmb::createFeatureForBayes(
    "Petal.Width", value = mean(iris$Petal.Width)),
  mmb::createFeatureForBayes(
    name = "Species", iris[1,]$Species, isLabel = TRUE)
)

print(mmb::getValueKeyOfBayesFeatures(feats, "Species"))
print(mmb::getValueKeyOfBayesFeatures(feats, "Petal.Width"))
</code></pre>

<hr>
<h2 id='getValueOfBayesFeatures'>Obtain the value of a Bayesian feature.</h2><span id='topic+getValueOfBayesFeatures'></span>

<h3>Description</h3>

<p>Given a data.frame with one or multiple features as
constructed by @seealso <code>createFeatureForBayes</code> and a name,
extracts the value of the feature specified by name.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getValueOfBayesFeatures(dfFeature, featName)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getValueOfBayesFeatures_+3A_dffeature">dfFeature</code></td>
<td>
<p>a data.frame for a single feature or variable
as constructed by @seealso <code>createFeatureForBayes</code>.</p>
</td></tr>
<tr><td><code id="getValueOfBayesFeatures_+3A_featname">featName</code></td>
<td>
<p>the name of the feature or variable of which to
obtain the value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the value of the feature.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>feats &lt;- rbind(
  mmb::createFeatureForBayes(
    "Petal.Width", value = mean(iris$Petal.Width)),
  mmb::createFeatureForBayes(
    name = "Species", iris[1,]$Species, isLabel = TRUE)
)

print(mmb::getValueOfBayesFeatures(feats, "Species"))
print(mmb::getValueOfBayesFeatures(feats, "Petal.Width"))
</code></pre>

<hr>
<h2 id='getWarnings'>Get a boolean indicating whether warnings are enabled system-wide.</h2><span id='topic+getWarnings'></span>

<h3>Description</h3>

<p>Getter for the state of warnings. Returns true if enabled.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getWarnings()
</code></pre>


<h3>Value</h3>

<p>Boolean to indicate whether warnings are enabled or not.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>

<hr>
<h2 id='make.varClosure'>Creates a closure over a variable and returns its getter and setter.</h2><span id='topic+make.varClosure'></span>

<h3>Description</h3>

<p>Creates a closure over a variable and returns its getter and setter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.varClosure(initVarVal = NULL, valValidator = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.varClosure_+3A_initvarval">initVarVal</code></td>
<td>
<p>the initial value of the closed variable.</p>
</td></tr>
<tr><td><code id="make.varClosure_+3A_valvalidator">valValidator</code></td>
<td>
<p>an optional function to validate each value. Function
must return boolean. If this function returns FALSE for any value, an
error is thrown.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with entries 'get' and 'set' which are getter/setter for the
variable that a closure was made over.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>

<hr>
<h2 id='neighborhood'>Given Bayesian features, returns those samples from a dataset that
exhibit a similarity (i.e., the neighborhood).</h2><span id='topic+neighborhood'></span>

<h3>Description</h3>

<p>The neighborhood <code class="reqn">N_i</code> is defined as the set of samples that
have a similarity greater than zero to the given sample <code class="reqn">s_i</code>. Segmentation
is done using equality (<code>==</code>) for discrete features and less than or equal
(<code>&lt;=</code>) for continuous features. Note that feature values <code>NA</code> and <code>NaN</code>
are also supported using <code>is.na()</code> and <code>is.nan()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neighborhood(df, features, selectedFeatureNames = c(), retainMinValues = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neighborhood_+3A_df">df</code></td>
<td>
<p>data.frame to select the neighborhood from</p>
</td></tr>
<tr><td><code id="neighborhood_+3A_features">features</code></td>
<td>
<p>data.frame of Bayes-features, used to segment/select the
rows that should make up the neighborhood.</p>
</td></tr>
<tr><td><code id="neighborhood_+3A_selectedfeaturenames">selectedFeatureNames</code></td>
<td>
<p>vector of names of features to use to demarcate
the neighborhood. If empty, uses all features' names.</p>
</td></tr>
<tr><td><code id="neighborhood_+3A_retainminvalues">retainMinValues</code></td>
<td>
<p>DEFAULT 0 the amount of samples to retain during
segmentation. For separating a neighborhood, this value typically should
be 0, so that no samples are included that are not within it. However,
for very sparse data or a great amount of variables, it might still make
sense to retain samples.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame with rows that were selected as neighborhood. It is
guaranteed that the rownames are maintained.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nbh &lt;- mmb::neighborhood(df = iris, features = mmb::createFeatureForBayes(
  name = "Sepal.Width", value = mean(iris$Sepal.Width)))

print(nrow(nbh))
</code></pre>

<hr>
<h2 id='sampleToBayesFeatures'>Transform an entire sample into a collection of Bayesian features.</h2><span id='topic+sampleToBayesFeatures'></span>

<h3>Description</h3>

<p>Helper function that takes one sample (e.g., a row of a dataframe
with validation data) and transforms it into a data.frame where
each row corresponds to one feature (and its value) of the sample.
This is done using @seealso <code>createFeatureForBayes</code>. This
operation can be thought of transposing a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleToBayesFeatures(dfRow, targetCol)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sampleToBayesFeatures_+3A_dfrow">dfRow</code></td>
<td>
<p>a row of a data.frame with a value for each feature.</p>
</td></tr>
<tr><td><code id="sampleToBayesFeatures_+3A_targetcol">targetCol</code></td>
<td>
<p>the name of the feature (column in the data.frame)
that is the target variable for classification or regression.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame where the first row is the feature that
represents the label.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Converts all features of iris; the result is a data.frame of length
# equal to the amount of features in iris (5). The first feature is
# targetCol (has isLabel=TRUE).
samp &lt;- mmb::sampleToBayesFeatures(dfRow = iris[15,], targetCol = "Species")
</code></pre>

<hr>
<h2 id='setDefaultRegressor'>Set a system-wide default regressor.</h2><span id='topic+setDefaultRegressor'></span>

<h3>Description</h3>

<p>Getting and setting the default regressor affects all functions
that have an overridable regressor. If this is not given, the default has
defined here will be obtained.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setDefaultRegressor(func)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setDefaultRegressor_+3A_func">func</code></td>
<td>
<p>a Function to use a regressor, should accept one argument,
which is a vector of numeric, and return one value, the regression.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>void
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>

<hr>
<h2 id='setMessages'>Enable or disable messages system-wide.</h2><span id='topic+setMessages'></span>

<h3>Description</h3>

<p>Setter for enabling or disabling messages. Messages are
disabled by default. Use these to enable high verbosity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setMessages(enable = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setMessages_+3A_enable">enable</code></td>
<td>
<p>a boolean to indicate whether to enable messages or not.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Boolean the state of enabled
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>

<hr>
<h2 id='setWarnings'>Enable or disable warnings system-wide.</h2><span id='topic+setWarnings'></span>

<h3>Description</h3>

<p>Setter for enabling or disabling warnings. Warnings are
enabled by default.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setWarnings(enable = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setWarnings_+3A_enable">enable</code></td>
<td>
<p>a boolean to indicate whether to enable warnings or not.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Boolean the state of enabled
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>

<hr>
<h2 id='vicinities'>Segment a dataset by each row once, then compute vicinities of
samples in the neighborhood.</h2><span id='topic+vicinities'></span>

<h3>Description</h3>

<p>Given an entire dataset, uses each instance in it to demarcate
a neighborhood using the selected features. Then, for each neighborhood,
the vicinity of all samples to it is computed. The result of this is an
N x N matrix, where the entry <code class="reqn">m_{i,j}</code> corresponds to the vicinity of
sample <code class="reqn">s_j</code> in neighborhood <code class="reqn">N_i</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vicinities(
  df,
  selectedFeatureNames = c(),
  shiftAmount = 0.1,
  doEcdf = FALSE,
  ecdfMinusOne = FALSE,
  retainMinValues = 0,
  useParallel = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vicinities_+3A_df">df</code></td>
<td>
<p>data.frame to compute the matrix of vicinites for.</p>
</td></tr>
<tr><td><code id="vicinities_+3A_selectedfeaturenames">selectedFeatureNames</code></td>
<td>
<p>vector of names of features to use for computing
the vicinity/centrality of each sample to each neighborhood.</p>
</td></tr>
<tr><td><code id="vicinities_+3A_shiftamount">shiftAmount</code></td>
<td>
<p>numeric DEFAULT 0.1 optional amount to shift each features
probability by. This is useful for when the centrality not necessarily must be
an actual probability and too many features are selected. To obtain actual
probabilities, this needs to be 0, and you must use the ECDF.</p>
</td></tr>
<tr><td><code id="vicinities_+3A_doecdf">doEcdf</code></td>
<td>
<p>boolean DEFAULT FALSE whether to use the ECDF instead of the EPDF
to find the likelihood of continuous values.</p>
</td></tr>
<tr><td><code id="vicinities_+3A_ecdfminusone">ecdfMinusOne</code></td>
<td>
<p>boolean DEFAULT FALSE only has an effect if the ECDF is
used. If true, uses 1 minus the ECDF to find the probability of a continuous
value. Depending on the interpretation of what you try to do, this may be of use.</p>
</td></tr>
<tr><td><code id="vicinities_+3A_retainminvalues">retainMinValues</code></td>
<td>
<p>DEFAULT 0 the amount of samples to retain during
segmentation. For separating a neighborhood, this value typically should
be 0, so that no samples are included that are not within it. However,
for very sparse data or a great amount of variables, it might still make
sense to retain samples.</p>
</td></tr>
<tr><td><code id="vicinities_+3A_useparallel">useParallel</code></td>
<td>
<p>boolean DEFAULT NULL whether to use parallelism or not. Setting this to
true requires also having previously registered a parallel backend. If parallel
computing is enabled, then each neighborhood is computed separately.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix of length <code class="reqn">N^2</code> (N being the length of the data.frame). Each
row i demarcates the neighborhood as selected by sample i, and each column j then
is the vicinity of sample <code class="reqn">s_j</code> to that neighborhood. No value of the diagonal
is zero, because each neighborhood always contains the sample it was demarcated
by, and that sample has a similarity greater than zero to it.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>See Also</h3>

<p><code>vicinitiesForSample()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>w &lt;- mmb::getWarnings()
mmb::setWarnings(FALSE)
mmb::vicinities(df = iris[1:10,])

# Run the same, but use the ECDF and retain more values:
mmb::vicinities(df = iris[1:10,], doEcdf = TRUE, retainMinValues = 10)
mmb::setWarnings(w)
</code></pre>

<hr>
<h2 id='vicinitiesForSample'>Segment a dataset by a single sample and compute vicinities for it and
the remaining samples in the neighborhood.</h2><span id='topic+vicinitiesForSample'></span>

<h3>Description</h3>

<p>Given some data and one sample <code class="reqn">s_i</code> from it, constructs the
neighborhood <code class="reqn">N_i</code> of that sample and assigns centralities to all other
samples in that neighborhood to it. Samples that lie outside the neighborhood
are assigned a vicinity of zero. Uses <code>mmb::neighborhood()</code> and
<code>mmb::centralities()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vicinitiesForSample(
  df,
  sampleFromDf,
  selectedFeatureNames = c(),
  shiftAmount = 0.1,
  doEcdf = FALSE,
  ecdfMinusOne = FALSE,
  retainMinValues = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vicinitiesForSample_+3A_df">df</code></td>
<td>
<p>data.frame that holds the data (and also the sample to use to define
the neighborhood). Each sample in this data.frame is assigned a vicinity.</p>
</td></tr>
<tr><td><code id="vicinitiesForSample_+3A_samplefromdf">sampleFromDf</code></td>
<td>
<p>data.frame a single row from the given data.frame. This is
used to select a neighborhood from the given data.</p>
</td></tr>
<tr><td><code id="vicinitiesForSample_+3A_selectedfeaturenames">selectedFeatureNames</code></td>
<td>
<p>vector of names of features to use to compute the
vicinity/centrality. This is passed to <code>mmb::neighborhood()</code>.</p>
</td></tr>
<tr><td><code id="vicinitiesForSample_+3A_shiftamount">shiftAmount</code></td>
<td>
<p>numeric DEFAULT 0.1 optional amount to shift each features
probability by. This is useful for when the centrality not necessarily must be
an actual probability and too many features are selected. To obtain actual
probabilities, this needs to be 0, and you must use the ECDF.</p>
</td></tr>
<tr><td><code id="vicinitiesForSample_+3A_doecdf">doEcdf</code></td>
<td>
<p>boolean DEFAULT FALSE whether to use the ECDF instead of the EPDF
to find the likelihood of continuous values.</p>
</td></tr>
<tr><td><code id="vicinitiesForSample_+3A_ecdfminusone">ecdfMinusOne</code></td>
<td>
<p>boolean DEFAULT FALSE only has an effect if the ECDF is
used. If true, uses 1 minus the ECDF to find the probability of a continuous
value. Depending on the interpretation of what you try to do, this may be of use.</p>
</td></tr>
<tr><td><code id="vicinitiesForSample_+3A_retainminvalues">retainMinValues</code></td>
<td>
<p>DEFAULT 0 the amount of samples to retain during
segmentation. For separating a neighborhood, this value typically should
be 0, so that no samples are included that are not within it. However,
for very sparse data or a great amount of variables, it might still make
sense to retain samples.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame with a single column 'vicinity' and the same rownames as the
given data.frame. Each row then holds the vicinity for the corresponding row.
</p>


<h3>Author(s)</h3>

<p>Sebastian Hönel <a href="mailto:sebastian.honel@lnu.se">sebastian.honel@lnu.se</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>vic &lt;- mmb::vicinitiesForSample(
  df = iris, sampleFromDf = iris[1,], shiftAmount = 0.1)
vic$vicinity

# Plot the ordered samples to get an idea which ones have a vicinity &gt; 0
plot(x=rownames(vic), y=vic$vicinity)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
