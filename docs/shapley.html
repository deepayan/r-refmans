<!DOCTYPE html><html><head><title>Help for package shapley</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {shapley}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#h2o.get_ids'><p>h2o.get_ids</p></a></li>
<li><a href='#normalize'><p>Normalize a vector based on specified minimum and maximum values</p></a></li>
<li><a href='#shapley'><p>Weighted average of SHAP values and weighted SHAP confidence intervals</p>
for a grid of fine-tuned models or base-learners of a stacked ensemble
model</a></li>
<li><a href='#shapley.plot'><p>Plot weighted SHAP contributions</p></a></li>
<li><a href='#shapley.test'><p>Normalize a vector based on specified minimum and maximum values</p></a></li>
<li><a href='#shapley.top'><p>Select top features in a model</p></a></li>
<li><a href='#test'><p>Weighted Permutation Test for Difference of Means</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Weighted Mean SHAP for Feature Selection in ML Grid and Ensemble</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0),</td>
</tr>
<tr>
<td>Description:</td>
<td>This R package introduces an innovative method for calculating SHapley Additive exPlanations (SHAP) values 
             for a grid of fine-tuned base-learner machine learning models as well as stacked ensembles, a method not 
             previously available due to the common reliance on single best-performing models. By integrating the weighted 
             mean SHAP values from individual base-learners comprising the ensemble or individual base-learners in a tuning grid search, 
             the package weights SHAP contributions according to each model's performance, assessed by the Area Under the 
             Precision-Recall Curve (AUCPR) for binary classifiers (currently implemented). It further extends this framework to 
             implement weighted confidence intervals for weighted mean SHAP values, offering a more comprehensive and robust 
             feature importance evaluation over a grid of machine learning models, instead of solely computing SHAP values for 
             the best-performing model. This methodology is particularly beneficial for addressing the severe class imbalance 
             (class rarity) problem by providing a transparent, generalized measure of feature importance that mitigates the 
             risk of reporting SHAP values for an overfitted or biased model and maintains robustness under severe class imbalance,
             where there is no universal criteria of identifying the absolute best model. Furthermore, the package implements
             hypothesis testing to ascertain the statistical significance of SHAP values for individual features, as well as 
             comparative significance testing of SHAP contributions between features. Additionally, it tackles a critical 
             gap in feature selection literature by presenting criteria for the automatic feature selection of the most important 
             features across a grid of models or stacked ensembles, eliminating the need for arbitrary determination of the 
             number of top features to be extracted. This utility is invaluable for researchers analyzing feature significance, 
             particularly within severely imbalanced outcomes where conventional methods fall short. In addition, it is also 
             expected to report democratic feature importance across a grid of models, resulting in a more comprehensive and 
             generalizable feature selection. The package further implements a novel method for visualizing SHAP values both 
             at subject level and feature level as well as a plot for feature selection based on the weighted mean SHAP ratios.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>ggplot2 (&ge; 3.4.2), h2o (&ge; 3.34.0.0), curl (&ge; 4.3.0), waffle
(&ge; 1.0.2)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/haghish/shapley">https://github.com/haghish/shapley</a>,
<a href="https://www.sv.uio.no/psi/english/people/academic/haghish/">https://www.sv.uio.no/psi/english/people/academic/haghish/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/haghish/shapley/issues">https://github.com/haghish/shapley/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-07 09:43:38 UTC; U-Shaped-Valley</td>
</tr>
<tr>
<td>Author:</td>
<td>E. F. Haghish [aut, cre, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>E. F. Haghish &lt;haghish@uio.no&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-07 19:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='h2o.get_ids'>h2o.get_ids</h2><span id='topic+h2o.get_ids'></span>

<h3>Description</h3>

<p>extracts the model IDs from H2O AutoML object or H2O grid
</p>


<h3>Usage</h3>

<pre><code class='language-R'>h2o.get_ids(automl)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="h2o.get_ids_+3A_automl">automl</code></td>
<td>
<p>a h2o <code>"AutoML"</code> grid object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector of trained models' names (IDs)
</p>


<h3>Author(s)</h3>

<p>E. F. Haghish
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
library(h2o)
h2o.init(ignore_config = TRUE, nthreads = 2, bind_to_localhost = FALSE, insecure = TRUE)
prostate_path &lt;- system.file("extdata", "prostate.csv", package = "h2o")
prostate &lt;- h2o.importFile(path = prostate_path, header = TRUE)
y &lt;- "CAPSULE"
prostate[,y] &lt;- as.factor(prostate[,y])  #convert to factor for classification
aml &lt;- h2o.automl(y = y, training_frame = prostate, max_runtime_secs = 30)

# get the model IDs
ids &lt;- h2o.ids(aml)

## End(Not run)
</code></pre>

<hr>
<h2 id='normalize'>Normalize a vector based on specified minimum and maximum values</h2><span id='topic+normalize'></span>

<h3>Description</h3>

<p>This function normalizes a vector based on specified minimum
and maximum values. If the minimum and maximum values are not
specified, the function will use the minimum and maximum values
of the vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalize(x, min = NULL, max = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalize_+3A_x">x</code></td>
<td>
<p>numeric vector</p>
</td></tr>
<tr><td><code id="normalize_+3A_min">min</code></td>
<td>
<p>minimum value</p>
</td></tr>
<tr><td><code id="normalize_+3A_max">max</code></td>
<td>
<p>maximum value</p>
</td></tr>
</table>


<h3>Value</h3>

<p>normalized numeric vector
</p>


<h3>Author(s)</h3>

<p>E. F. Haghish
</p>

<hr>
<h2 id='shapley'>Weighted average of SHAP values and weighted SHAP confidence intervals
for a grid of fine-tuned models or base-learners of a stacked ensemble
model</h2><span id='topic+shapley'></span>

<h3>Description</h3>

<p>Weighted average of SHAP values and weighted SHAP confidence intervals
provide a measure of feature importance for a grid of fine-tuned models
or base-learners of a stacked ensemble model. Instead of reporting
relative SHAP contributions for a single model, this function
takes the variability in feature importance of multiple models
into account and computes weighted mean and confidence intervals
for each feature, taking the performance metric of each model as
the weight. The function also provides a plot of the weighted
SHAP values and confidence intervals. Currently only models
trained by h2o machine learning software or autoEnsemble
package are supported.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shapley(
  models,
  newdata,
  plot = TRUE,
  family = "binary",
  performance_metric = c("aucpr"),
  method = c("lowerCI"),
  cutoff = 0.01,
  top_n_features = NULL,
  normalize_to = "upperCI"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shapley_+3A_models">models</code></td>
<td>
<p>H2O search grid, AutoML grid, or a character vector of H2O model IDs.
the <code>"h2o.get_ids"</code> function from <code>"h2otools"</code> can retrieve
the IDs from grids.</p>
</td></tr>
<tr><td><code id="shapley_+3A_newdata">newdata</code></td>
<td>
<p>h2o frame (data.frame). the data.frame must be already uploaded
on h2o server (cloud). when specified, this dataset will be used
for evaluating the models. if not specified, model performance
on the training dataset will be reported.</p>
</td></tr>
<tr><td><code id="shapley_+3A_plot">plot</code></td>
<td>
<p>logical. if TRUE, the weighted mean and confidence intervals of
the SHAP values are plotted. The default is TRUE.</p>
</td></tr>
<tr><td><code id="shapley_+3A_family">family</code></td>
<td>
<p>character. currently only &quot;binary&quot; classification models trained
by h2o machine learning are supported.</p>
</td></tr>
<tr><td><code id="shapley_+3A_performance_metric">performance_metric</code></td>
<td>
<p>character, specifying the performance metric to be
used for weighting the SHAP values (mean and 95
&quot;aucpr&quot; (area under the precision-recall curve). Other options include
&quot;auc&quot; (area under the ROC curve), &quot;mcc&quot; (Matthews correlation coefficient),
and &quot;f2&quot; (F2 score).</p>
</td></tr>
<tr><td><code id="shapley_+3A_method">method</code></td>
<td>
<p>character, specifying the method used for identifying the most
important features according to their weighted SHAP values.
The default selection method is &quot;lowerCI&quot;, which includes
features whose lower weighted confidence interval exceeds the
predefined 'cutoff' value (default is relative SHAP of 1
Alternatively, the &quot;mean&quot; option can be specified, indicating
any feature with normalized weighted mean SHAP contribution above
the specified 'cutoff' should be selected. Another
alternative options is &quot;shapratio&quot;, a method that filters
for features where the proportion of their relative weighted SHAP
value exceeds the 'cutoff'. This approach calculates the relative
contribution of each feature's weighted SHAP value against the
aggregate of all features, with those surpassing the 'cutoff'
being selected as top feature.</p>
</td></tr>
<tr><td><code id="shapley_+3A_cutoff">cutoff</code></td>
<td>
<p>numeric, specifying the cutoff for the method used for selecting
the top features.</p>
</td></tr>
<tr><td><code id="shapley_+3A_top_n_features">top_n_features</code></td>
<td>
<p>integer. if specified, the top n features with the
highest weighted SHAP values will be selected, overrullung
the 'cutoff' and 'method' arguments.</p>
</td></tr>
<tr><td><code id="shapley_+3A_normalize_to">normalize_to</code></td>
<td>
<p>character. The default value is &quot;upperCI&quot;, which sets the feature with
the maximum SHAP value to one, allowing the higher CI to
go beyond one. Setting this value is mainly for aesthetic
reason to adjust the Plot, but also, it can influence the
feature selection process, depending on the method in use,
because it changes how the SHAP values should be normalized.
the alternative is 'feature', specifying that
in the normalization of the SHAP values, the maximum confidence
interval of the weighted SHAP values should be equal to
&quot;1&quot;, in order to limit the plot values to maximum of one.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list including the GGPLOT2 object, the data frame of SHAP values,
and performance metric of all models, as well as the model IDs.
</p>


<h3>Author(s)</h3>

<p>E. F. Haghish
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
# load the required libraries for building the base-learners and the ensemble models
library(h2o)            #shapley supports h2o models
library(shapley)

# initiate the h2o server
h2o.init(ignore_config = TRUE, nthreads = 2, bind_to_localhost = FALSE, insecure = TRUE)

# upload data to h2o cloud
prostate_path &lt;- system.file("extdata", "prostate.csv", package = "h2o")
prostate &lt;- h2o.importFile(path = prostate_path, header = TRUE)

set.seed(10)

### H2O provides 2 types of grid search for tuning the models, which are
### AutoML and Grid. Below, I demonstrate how weighted mean shapley values
### can be computed for both types.

#######################################################
### PREPARE AutoML Grid (takes a couple of minutes)
#######################################################
# run AutoML to tune various models (GBM) for 60 seconds
y &lt;- "CAPSULE"
prostate[,y] &lt;- as.factor(prostate[,y])  #convert to factor for classification
aml &lt;- h2o.automl(y = y, training_frame = prostate, max_runtime_secs = 120,
                 include_algos=c("GBM"),

                 # this setting ensures the models are comparable for building a meta learner
                 seed = 2023, nfolds = 10,
                 keep_cross_validation_predictions = TRUE)

### call 'shapley' function to compute the weighted mean and weighted confidence intervals
### of SHAP values across all trained models.
### Note that the 'newdata' should be the testing dataset!
result &lt;- shapley(models = aml, newdata = prostate, plot = TRUE)

#######################################################
### PREPARE H2O Grid (takes a couple of minutes)
#######################################################
# make sure equal number of "nfolds" is specified for different grids
grid &lt;- h2o.grid(algorithm = "gbm", y = y, training_frame = prostate,
                 hyper_params = list(ntrees = seq(1,50,1)),
                 grid_id = "ensemble_grid",

                 # this setting ensures the models are comparable for building a meta learner
                 seed = 2023, fold_assignment = "Modulo", nfolds = 10,
                 keep_cross_validation_predictions = TRUE)

result2 &lt;- shapley(models = grid, newdata = prostate, plot = TRUE)

#######################################################
### PREPARE autoEnsemble STACKED ENSEMBLE MODEL
#######################################################

### get the models' IDs from the AutoML and grid searches.
### this is all that is needed before building the ensemble,
### i.e., to specify the model IDs that should be evaluated.
library(autoEnsemble)
ids    &lt;- c(h2o.get_ids(aml), h2o.get_ids(grid))
autoSearch &lt;- ensemble(models = ids, training_frame = prostate, strategy = "search")
result3 &lt;- shapley(models = autoSearch, newdata = prostate, plot = TRUE)



## End(Not run)
</code></pre>

<hr>
<h2 id='shapley.plot'>Plot weighted SHAP contributions</h2><span id='topic+shapley.plot'></span>

<h3>Description</h3>

<p>This function applies different criteria to visualize SHAP contributions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shapley.plot(
  shapley,
  plot = "bar",
  legendstyle = "continuous",
  scale_colour_gradient = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shapley.plot_+3A_shapley">shapley</code></td>
<td>
<p>object of class 'shapley', as returned by the 'shapley' function</p>
</td></tr>
<tr><td><code id="shapley.plot_+3A_plot">plot</code></td>
<td>
<p>character, specifying the type of the plot, which can be either
'bar', 'waffle', or 'shap'. The default is 'bar'.</p>
</td></tr>
<tr><td><code id="shapley.plot_+3A_legendstyle">legendstyle</code></td>
<td>
<p>character, specifying the style of the plot legend, which
can be either 'continuous' (default) or 'discrete'. the
continuous legend is only applicable to 'shap' plots and
other plots only use 'discrete' legend.</p>
</td></tr>
<tr><td><code id="shapley.plot_+3A_scale_colour_gradient">scale_colour_gradient</code></td>
<td>
<p>character vector for specifying the color gradients
for the plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot object
</p>


<h3>Author(s)</h3>

<p>E. F. Haghish
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
# load the required libraries for building the base-learners and the ensemble models
library(h2o)            #shapley supports h2o models
library(shapley)

# initiate the h2o server
h2o.init(ignore_config = TRUE, nthreads = 2, bind_to_localhost = FALSE, insecure = TRUE)

# upload data to h2o cloud
prostate_path &lt;- system.file("extdata", "prostate.csv", package = "h2o")
prostate &lt;- h2o.importFile(path = prostate_path, header = TRUE)

### H2O provides 2 types of grid search for tuning the models, which are
### AutoML and Grid. Below, I demonstrate how weighted mean shapley values
### can be computed for both types.

set.seed(10)

#######################################################
### PREPARE AutoML Grid (takes a couple of minutes)
#######################################################
# run AutoML to tune various models (GBM) for 60 seconds
y &lt;- "CAPSULE"
prostate[,y] &lt;- as.factor(prostate[,y])  #convert to factor for classification
aml &lt;- h2o.automl(y = y, training_frame = prostate, max_runtime_secs = 120,
                 include_algos=c("GBM"),

                 # this setting ensures the models are comparable for building a meta learner
                 seed = 2023, nfolds = 10,
                 keep_cross_validation_predictions = TRUE)

### call 'shapley' function to compute the weighted mean and weighted confidence intervals
### of SHAP values across all trained models.
### Note that the 'newdata' should be the testing dataset!
result &lt;- shapley(models = aml, newdata = prostate, plot = TRUE)

#######################################################
### PLOT THE WEIGHTED MEAN SHAP VALUES
#######################################################

shapley.plot(result, plot = "bar")
shapley.plot(result, plot = "waffle")

## End(Not run)
</code></pre>

<hr>
<h2 id='shapley.test'>Normalize a vector based on specified minimum and maximum values</h2><span id='topic+shapley.test'></span>

<h3>Description</h3>

<p>This function normalizes a vector based on specified minimum
and maximum values. If the minimum and maximum values are not
specified, the function will use the minimum and maximum values
of the vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shapley.test(shapley, features, n = 5000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shapley.test_+3A_shapley">shapley</code></td>
<td>
<p>object of class 'shapley', as returned by the 'shapley' function</p>
</td></tr>
<tr><td><code id="shapley.test_+3A_features">features</code></td>
<td>
<p>character, name of two features to be compared with permutation test</p>
</td></tr>
<tr><td><code id="shapley.test_+3A_n">n</code></td>
<td>
<p>integer, number of permutations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>normalized numeric vector
</p>


<h3>Author(s)</h3>

<p>E. F. Haghish
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
# load the required libraries for building the base-learners and the ensemble models
library(h2o)            #shapley supports h2o models
library(autoEnsemble)   #autoEnsemble models, particularly useful under severe class imbalance
library(shapley)

# initiate the h2o server
h2o.init(ignore_config = TRUE, nthreads = 2, bind_to_localhost = FALSE, insecure = TRUE)

# upload data to h2o cloud
prostate_path &lt;- system.file("extdata", "prostate.csv", package = "h2o")
prostate &lt;- h2o.importFile(path = prostate_path, header = TRUE)

### H2O provides 2 types of grid search for tuning the models, which are
### AutoML and Grid. Below, I demonstrate how weighted mean shapley values
### can be computed for both types.

set.seed(10)

#######################################################
### PREPARE AutoML Grid (takes a couple of minutes)
#######################################################
# run AutoML to tune various models (GBM) for 60 seconds
y &lt;- "CAPSULE"
prostate[,y] &lt;- as.factor(prostate[,y])  #convert to factor for classification
aml &lt;- h2o.automl(y = y, training_frame = prostate, max_runtime_secs = 120,
                 include_algos=c("GBM"),

                 # this setting ensures the models are comparable for building a meta learner
                 seed = 2023, nfolds = 10,
                 keep_cross_validation_predictions = TRUE)

### call 'shapley' function to compute the weighted mean and weighted confidence intervals
### of SHAP values across all trained models.
### Note that the 'newdata' should be the testing dataset!
result &lt;- shapley(models = aml, newdata = prostate, plot = TRUE)

#######################################################
### Significance testing of contributions of two features
#######################################################

shapley.test(result, features = c("GLEASON", "PSA"), n=5000)

## End(Not run)
</code></pre>

<hr>
<h2 id='shapley.top'>Select top features in a model</h2><span id='topic+shapley.top'></span>

<h3>Description</h3>

<p>This function applies different criteria simultaniously to identify
the most important features in a model. The criteria include:
1) minimum limit of lower weighted confidence intervals of SHAP values
relative to the feature with highest SHAP value.
2) minimum limit of percentage of weighted mean SHAP values relative to
over all SHAP values of all features. These are specified with two
different cutoff values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shapley.top(shapley, lowerci = 0.01, shapratio = 0.005)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shapley.top_+3A_shapley">shapley</code></td>
<td>
<p>object of class 'shapley', as returned by the 'shapley' function</p>
</td></tr>
<tr><td><code id="shapley.top_+3A_lowerci">lowerci</code></td>
<td>
<p>numeric, specifying the lower limit of weighted confidence intervals
of SHAP values relative to the feature with highest SHAP value.
the default is 0.01</p>
</td></tr>
<tr><td><code id="shapley.top_+3A_shapratio">shapratio</code></td>
<td>
<p>numeric, specifying the lower limit of percentage of weighted mean
SHAP values relative to over all SHAP values of all features.
the default is 0.005</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame of selected features
</p>


<h3>Author(s)</h3>

<p>E. F. Haghish
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
# load the required libraries for building the base-learners and the ensemble models
library(h2o)            #shapley supports h2o models
library(shapley)

# initiate the h2o server
h2o.init(ignore_config = TRUE, nthreads = 2, bind_to_localhost = FALSE, insecure = TRUE)

# upload data to h2o cloud
prostate_path &lt;- system.file("extdata", "prostate.csv", package = "h2o")
prostate &lt;- h2o.importFile(path = prostate_path, header = TRUE)

### H2O provides 2 types of grid search for tuning the models, which are
### AutoML and Grid. Below, I demonstrate how weighted mean shapley values
### can be computed for both types.

set.seed(10)

#######################################################
### PREPARE AutoML Grid (takes a couple of minutes)
#######################################################
# run AutoML to tune various models (GBM) for 60 seconds
y &lt;- "CAPSULE"
prostate[,y] &lt;- as.factor(prostate[,y])  #convert to factor for classification
aml &lt;- h2o.automl(y = y, training_frame = prostate, max_runtime_secs = 120,
                 include_algos=c("GBM"),

                 # this setting ensures the models are comparable for building a meta learner
                 seed = 2023, nfolds = 10,
                 keep_cross_validation_predictions = TRUE)

### call 'shapley' function to compute the weighted mean and weighted confidence intervals
### of SHAP values across all trained models.
### Note that the 'newdata' should be the testing dataset!
result &lt;- shapley(models = aml, newdata = prostate, plot = TRUE)

#######################################################
### Significance testing of contributions of two features
#######################################################

shapley.top(result, lowerci = 0.01, shapratio = 0.005)

## End(Not run)
</code></pre>

<hr>
<h2 id='test'>Weighted Permutation Test for Difference of Means</h2><span id='topic+test'></span>

<h3>Description</h3>

<p>This function performs a weighted permutation test to determine if there is a significant
difference between the means of two weighted numeric vectors. It tests the null hypothesis
that the difference in means is zero against the alternative that it is not zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test(var1, var2, weights, n = 2000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test_+3A_var1">var1</code></td>
<td>
<p>A numeric vector.</p>
</td></tr>
<tr><td><code id="test_+3A_var2">var2</code></td>
<td>
<p>A numeric vector of the same length as <code>var1</code>.</p>
</td></tr>
<tr><td><code id="test_+3A_weights">weights</code></td>
<td>
<p>A numeric vector of weights, assumed to be the same for both <code>var1</code> and <code>var2</code>.</p>
</td></tr>
<tr><td><code id="test_+3A_n">n</code></td>
<td>
<p>The number of permutations to perform (default is 1000).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the observed difference in means and the p-value of the test.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
