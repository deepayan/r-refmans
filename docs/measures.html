<!DOCTYPE html><html><head><title>Help for package measures</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {measures}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ACC'><p>Accuracy</p></a></li>
<li><a href='#ARSQ'><p>Adjusted coefficient of determination</p></a></li>
<li><a href='#AUC'><p>Area under the curve</p></a></li>
<li><a href='#BAC'><p>Balanced accuracy</p></a></li>
<li><a href='#BER'><p>Balanced error rate</p></a></li>
<li><a href='#Brier'><p>Brier score</p></a></li>
<li><a href='#BrierScaled'><p>Brier scaled</p></a></li>
<li><a href='#EXPVAR'><p>Explained variance</p></a></li>
<li><a href='#F1'><p>F1 measure</p></a></li>
<li><a href='#FDR'><p>False discovery rate</p></a></li>
<li><a href='#FN'><p>False negatives</p></a></li>
<li><a href='#FNR'><p>False negative rate</p></a></li>
<li><a href='#FP'><p>False positives</p></a></li>
<li><a href='#FPR'><p>False positive rate</p></a></li>
<li><a href='#GMEAN'><p>G-mean</p></a></li>
<li><a href='#GPR'><p>Geometric mean of precision and recall.</p></a></li>
<li><a href='#KAPPA'><p>Cohen's kappa</p></a></li>
<li><a href='#KendallTau'><p>Kendall's tau</p></a></li>
<li><a href='#listAllMeasures'><p>List all measures</p></a></li>
<li><a href='#Logloss'><p>Logarithmic loss</p></a></li>
<li><a href='#LSR'><p>Logarithmic Scoring Rule</p></a></li>
<li><a href='#MAE'><p>Mean of absolute errors</p></a></li>
<li><a href='#MAPE'><p>Mean absolute percentage error</p></a></li>
<li><a href='#MCC'><p>Matthews correlation coefficient</p></a></li>
<li><a href='#MEDAE'><p>Median of absolute errors</p></a></li>
<li><a href='#MEDSE'><p>Median of squared errors</p></a></li>
<li><a href='#MMCE'><p>Mean misclassification error</p></a></li>
<li><a href='#MSE'><p>Mean of squared errors</p></a></li>
<li><a href='#MSLE'><p>Mean squared logarithmic error</p></a></li>
<li><a href='#multiclass.AU1P'><p>Weighted average 1 vs. 1 multiclass AUC</p></a></li>
<li><a href='#multiclass.AU1U'><p>Average 1 vs. 1 multiclass AUC</p></a></li>
<li><a href='#multiclass.AUNP'><p>Weighted average 1 vs. rest multiclass AUC</p></a></li>
<li><a href='#multiclass.AUNU'><p>Average 1 vs. rest multiclass AUC</p></a></li>
<li><a href='#multiclass.Brier'><p>Multiclass Brier score</p></a></li>
<li><a href='#MultilabelACC'><p>Accuracy (multilabel)</p></a></li>
<li><a href='#MultilabelF1'><p>F1 measure (multilabel)</p></a></li>
<li><a href='#MultilabelHamloss'><p>Hamming loss</p></a></li>
<li><a href='#MultilabelPPV'><p>Positive predictive value (multilabel)</p></a></li>
<li><a href='#MultilabelSubset01'><p>Subset-0-1 loss</p></a></li>
<li><a href='#MultilabelTPR'><p>TPR (multilabel)</p></a></li>
<li><a href='#NPV'><p>Negative predictive value</p></a></li>
<li><a href='#PPV'><p>Positive predictive value</p></a></li>
<li><a href='#QSR'><p>Quadratic Scoring Rule</p></a></li>
<li><a href='#RAE'><p>Relative absolute error</p></a></li>
<li><a href='#RMSE'><p>Root mean squared error</p></a></li>
<li><a href='#RMSLE'><p>Root mean squared logarithmic error</p></a></li>
<li><a href='#RRSE'><p>Root relative squared error</p></a></li>
<li><a href='#RSQ'><p>Coefficient of determination</p></a></li>
<li><a href='#SAE'><p>Sum of absolute errors</p></a></li>
<li><a href='#SpearmanRho'><p>Spearman's rho</p></a></li>
<li><a href='#SSE'><p>Sum of squared errors</p></a></li>
<li><a href='#SSR'><p>Spherical Scoring Rule</p></a></li>
<li><a href='#TN'><p>True negatives</p></a></li>
<li><a href='#TNR'><p>True negative rate</p></a></li>
<li><a href='#TP'><p>True positives</p></a></li>
<li><a href='#TPR'><p>True positive rate</p></a></li>
<li><a href='#WKAPPA'><p>Mean quadratic weighted kappa</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Performance Measures for Statistical Learning</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides the biggest amount of statistical measures in the whole R world. Includes measures of regression, (multiclass) classification and multilabel classification. The measures come mainly from the 'mlr' package and were programed by several 'mlr' developers. </td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0), stats</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.0.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-01-19 14:31:34 UTC; philipp</td>
</tr>
<tr>
<td>Author:</td>
<td>Philipp Probst [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Philipp Probst &lt;philipp_probst@gmx.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-01-19 15:10:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='ACC'>Accuracy</h2><span id='topic+ACC'></span>

<h3>Description</h3>

<p>Defined as: mean(response == truth)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ACC(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ACC_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="ACC_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(122)
truth = as.factor(sample(c(1,2,3), n, replace = TRUE))
response = as.factor(sample(c(1,2,3), n, replace = TRUE))
ACC(truth, response)
</code></pre>

<hr>
<h2 id='ARSQ'>Adjusted coefficient of determination</h2><span id='topic+ARSQ'></span>

<h3>Description</h3>

<p>Defined as: 1 - (1 - rsq) * (p / (n - p - 1L)). Adjusted R-squared is only defined for normal linear regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ARSQ(truth, response, n, p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ARSQ_+3A_truth">truth</code></td>
<td>
<p>[numeric] vector of true values</p>
</td></tr>
<tr><td><code id="ARSQ_+3A_response">response</code></td>
<td>
<p>[numeric] vector of predicted values</p>
</td></tr>
<tr><td><code id="ARSQ_+3A_n">n</code></td>
<td>
<p>[numeric] number of observations</p>
</td></tr>
<tr><td><code id="ARSQ_+3A_p">p</code></td>
<td>
<p>[numeric] number of predictors</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
p = 5
set.seed(123)
truth = rnorm(n)
response = rnorm(n)
ARSQ(truth, response, n, p)
</code></pre>

<hr>
<h2 id='AUC'>Area under the curve</h2><span id='topic+AUC'></span>

<h3>Description</h3>

<p>Integral over the graph that results from computing fpr and tpr for many different thresholds.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AUC(probabilities, truth, negative, positive)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AUC_+3A_probabilities">probabilities</code></td>
<td>
<p>[numeric] vector of predicted probabilities</p>
</td></tr>
<tr><td><code id="AUC_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="AUC_+3A_negative">negative</code></td>
<td>
<p>negative class</p>
</td></tr>
<tr><td><code id="AUC_+3A_positive">positive</code></td>
<td>
<p>positive class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
positive = 1
negative = 0
AUC(probabilities, truth, negative, positive)
</code></pre>

<hr>
<h2 id='BAC'>Balanced accuracy</h2><span id='topic+BAC'></span>

<h3>Description</h3>

<p>Mean of true positive rate and true negative rate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BAC(truth, response, negative, positive)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BAC_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="BAC_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
<tr><td><code id="BAC_+3A_negative">negative</code></td>
<td>
<p>negative class</p>
</td></tr>
<tr><td><code id="BAC_+3A_positive">positive</code></td>
<td>
<p>positive class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
positive = 1
negative = 0
BAC(truth, response, negative, positive)
</code></pre>

<hr>
<h2 id='BER'>Balanced error rate</h2><span id='topic+BER'></span>

<h3>Description</h3>

<p>Mean of misclassification error rates on all individual classes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BER(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BER_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="BER_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(122)
truth = as.factor(sample(c(1,2,3), n, replace = TRUE))
response = as.factor(sample(c(1,2,3), n, replace = TRUE))
BER(truth, response)
</code></pre>

<hr>
<h2 id='Brier'>Brier score</h2><span id='topic+Brier'></span>

<h3>Description</h3>

<p>The Brier score is defined as the quadratic difference between the probability and the value (1,0) for the class.
That means we use the numeric representation 1 and 0 for our target classes. It is similiar to the mean squared error in regression.
multiclass.brier is the sum over all one vs. all comparisons and for a binary classifcation 2 * brier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Brier(probabilities, truth, negative, positive)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Brier_+3A_probabilities">probabilities</code></td>
<td>
<p>[numeric] vector of predicted probabilities</p>
</td></tr>
<tr><td><code id="Brier_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="Brier_+3A_negative">negative</code></td>
<td>
<p>negative class</p>
</td></tr>
<tr><td><code id="Brier_+3A_positive">positive</code></td>
<td>
<p>positive class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
positive = 1
negative = 0
Brier(probabilities, truth, negative, positive)
</code></pre>

<hr>
<h2 id='BrierScaled'>Brier scaled</h2><span id='topic+BrierScaled'></span>

<h3>Description</h3>

<p>Brier score scaled to [0,1], see http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3575184/.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BrierScaled(probabilities, truth, negative, positive)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BrierScaled_+3A_probabilities">probabilities</code></td>
<td>
<p>[numeric] vector of predicted probabilities</p>
</td></tr>
<tr><td><code id="BrierScaled_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="BrierScaled_+3A_negative">negative</code></td>
<td>
<p>negative class</p>
</td></tr>
<tr><td><code id="BrierScaled_+3A_positive">positive</code></td>
<td>
<p>positive class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
positive = 1
negative = 0
BrierScaled(probabilities, truth, negative, positive)
</code></pre>

<hr>
<h2 id='EXPVAR'>Explained variance</h2><span id='topic+EXPVAR'></span>

<h3>Description</h3>

<p>Similar to RSQ (R-squared). Defined as explained_sum_of_squares / total_sum_of_squares.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EXPVAR(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EXPVAR_+3A_truth">truth</code></td>
<td>
<p>[numeric] vector of true values</p>
</td></tr>
<tr><td><code id="EXPVAR_+3A_response">response</code></td>
<td>
<p>[numeric] vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(123)
truth = rnorm(n)
response = rnorm(n)
EXPVAR(truth, response)
</code></pre>

<hr>
<h2 id='F1'>F1 measure</h2><span id='topic+F1'></span>

<h3>Description</h3>

<p>Defined as: 2 * tp/ (sum(truth == positive) + sum(response == positive))
</p>


<h3>Usage</h3>

<pre><code class='language-R'>F1(truth, response, positive)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="F1_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="F1_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
<tr><td><code id="F1_+3A_positive">positive</code></td>
<td>
<p>positive class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
positive = 1
F1(truth, response, positive)
</code></pre>

<hr>
<h2 id='FDR'>False discovery rate</h2><span id='topic+FDR'></span>

<h3>Description</h3>

<p>Defined as: fp / (tp + fp)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FDR(truth, response, positive)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FDR_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="FDR_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
<tr><td><code id="FDR_+3A_positive">positive</code></td>
<td>
<p>positive class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
positive = 1
FDR(truth, response, positive)
</code></pre>

<hr>
<h2 id='FN'>False negatives</h2><span id='topic+FN'></span>

<h3>Description</h3>

<p>Sum of misclassified observations in the negative class. Also called misses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FN(truth, response, negative)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FN_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="FN_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
<tr><td><code id="FN_+3A_negative">negative</code></td>
<td>
<p>negative class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
negative = 0
FN(truth, response, negative)
</code></pre>

<hr>
<h2 id='FNR'>False negative rate</h2><span id='topic+FNR'></span>

<h3>Description</h3>

<p>Percentage of misclassified observations in the negative class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FNR(truth, response, negative, positive)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FNR_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="FNR_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
<tr><td><code id="FNR_+3A_negative">negative</code></td>
<td>
<p>negative class</p>
</td></tr>
<tr><td><code id="FNR_+3A_positive">positive</code></td>
<td>
<p>positive class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
positive = 1
negative = 0
FNR(truth, response, negative, positive)
</code></pre>

<hr>
<h2 id='FP'>False positives</h2><span id='topic+FP'></span>

<h3>Description</h3>

<p>Sum of misclassified observations in the positive class. Also called false alarms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FP(truth, response, positive)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FP_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="FP_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
<tr><td><code id="FP_+3A_positive">positive</code></td>
<td>
<p>positive class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
positive = 1
FP(truth, response, positive)
</code></pre>

<hr>
<h2 id='FPR'>False positive rate</h2><span id='topic+FPR'></span>

<h3>Description</h3>

<p>Percentage of misclassified observations in the positive class. Also called false alarm rate or fall-out.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FPR(truth, response, negative, positive)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FPR_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="FPR_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
<tr><td><code id="FPR_+3A_negative">negative</code></td>
<td>
<p>negative class</p>
</td></tr>
<tr><td><code id="FPR_+3A_positive">positive</code></td>
<td>
<p>positive class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
positive = 1
negative = 0
FPR(truth, response, negative, positive)
</code></pre>

<hr>
<h2 id='GMEAN'>G-mean</h2><span id='topic+GMEAN'></span>

<h3>Description</h3>

<p>Geometric mean of recall and specificity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GMEAN(truth, response, negative, positive)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GMEAN_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="GMEAN_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
<tr><td><code id="GMEAN_+3A_negative">negative</code></td>
<td>
<p>negative class</p>
</td></tr>
<tr><td><code id="GMEAN_+3A_positive">positive</code></td>
<td>
<p>positive class</p>
</td></tr>
</table>


<h3>References</h3>

<p>He, H. &amp; Garcia, E. A. (2009)
*Learning from Imbalanced Data.*
IEEE Transactions on Knowledge and Data Engineering, vol. 21, no. 9. pp. 1263-1284.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
positive = 1
negative = 0
GMEAN(truth, response, negative, positive)
</code></pre>

<hr>
<h2 id='GPR'>Geometric mean of precision and recall.</h2><span id='topic+GPR'></span>

<h3>Description</h3>

<p>Defined as: sqrt(ppv * tpr)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GPR(truth, response, positive)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GPR_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="GPR_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
<tr><td><code id="GPR_+3A_positive">positive</code></td>
<td>
<p>positive class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
positive = 1
GPR(truth, response, positive)
</code></pre>

<hr>
<h2 id='KAPPA'>Cohen's kappa</h2><span id='topic+KAPPA'></span>

<h3>Description</h3>

<p>Defined as: 1 - (1 - p0) / (1 - pe). With: p0 = 'observed frequency of
agreement' and pe = 'expected agremeent frequency under independence
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KAPPA(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KAPPA_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="KAPPA_+3A_response">response</code></td>
<td>
<p>vector of predicted values
n = 20
set.seed(122)
truth = as.factor(sample(c(1,2,3), n, replace = TRUE))
response = as.factor(sample(c(1,2,3), n, repla
KAPPA(truth, response)</p>
</td></tr>
</table>

<hr>
<h2 id='KendallTau'>Kendall's tau</h2><span id='topic+KendallTau'></span>

<h3>Description</h3>

<p>Defined as: Kendall's tau correlation between truth and response. Only looks at the order.
See Rosset et al.: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.95.1398&amp;rep=rep1&amp;type=pdf.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KendallTau(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KendallTau_+3A_truth">truth</code></td>
<td>
<p>[numeric] vector of true values</p>
</td></tr>
<tr><td><code id="KendallTau_+3A_response">response</code></td>
<td>
<p>[numeric] vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(123)
truth = rnorm(n)
response = rnorm(n)
KendallTau(truth, response)
</code></pre>

<hr>
<h2 id='listAllMeasures'>List all measures</h2><span id='topic+listAllMeasures'></span>

<h3>Description</h3>

<p>Lists all measures that are available in the package with their corresponding task.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>listAllMeasures()
</code></pre>


<h3>Value</h3>

<p>Dataframe with all available measures and the correspoding task
</p>


<h3>Examples</h3>

<pre><code class='language-R'>listAllMeasures()
</code></pre>

<hr>
<h2 id='Logloss'>Logarithmic loss</h2><span id='topic+Logloss'></span>

<h3>Description</h3>

<p>Defined as: -mean(log(p_i)), where p_i is the predicted probability of the true 
class of observation i. Inspired by https://www.kaggle.com/wiki/MultiClassLogLoss.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Logloss(probabilities, truth)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Logloss_+3A_probabilities">probabilities</code></td>
<td>
<p>[numeric] vector (or matrix with column names of the classes) of predicted probabilities</p>
</td></tr>
<tr><td><code id="Logloss_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(122)
truth = as.factor(sample(c(1,2,3), n, replace = TRUE))
probabilities = matrix(runif(60), 20, 3)
probabilities = probabilities/rowSums(probabilities)
colnames(probabilities) = c(1,2,3)
Logloss(probabilities, truth)
</code></pre>

<hr>
<h2 id='LSR'>Logarithmic Scoring Rule</h2><span id='topic+LSR'></span>

<h3>Description</h3>

<p>Defined as: mean(log(p_i)), where p_i is the predicted probability of the true class of observation i.
This scoring rule is the same as the negative logloss, self-information or surprisal.
See: Bickel, J. E. (2007). Some comparisons among quadratic, spherical, and logarithmic scoring rules. Decision Analysis, 4(2), 49-65.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LSR(probabilities, truth)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LSR_+3A_probabilities">probabilities</code></td>
<td>
<p>[numeric] vector (or matrix with column names of the classes) of predicted probabilities</p>
</td></tr>
<tr><td><code id="LSR_+3A_truth">truth</code></td>
<td>
<p>vector of true values 
n = 20
set.seed(122)
truth = as.factor(sample(c(1,2,3), n, replace = TRUE))
probabilities = matrix(runif(60), 20, 3)
probabilities = probabilities/rowSums(probabilities)
colnames(probabilities) = c(1,2,3)
LSR(probabilities, truth)</p>
</td></tr>
</table>

<hr>
<h2 id='MAE'>Mean of absolute errors</h2><span id='topic+MAE'></span>

<h3>Description</h3>

<p>Defined as: mean(abs(response - truth))
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MAE(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MAE_+3A_truth">truth</code></td>
<td>
<p>[numeric] vector of true values</p>
</td></tr>
<tr><td><code id="MAE_+3A_response">response</code></td>
<td>
<p>[numeric] vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(123)
truth = rnorm(n)
response = rnorm(n)
MAE(truth, response)
</code></pre>

<hr>
<h2 id='MAPE'>Mean absolute percentage error</h2><span id='topic+MAPE'></span>

<h3>Description</h3>

<p>Defined as the abs(truth_i - response_i) / truth_i. Won't work if any truth value is equal to zero. In this case the output will be NA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MAPE(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MAPE_+3A_truth">truth</code></td>
<td>
<p>[numeric] vector of true values</p>
</td></tr>
<tr><td><code id="MAPE_+3A_response">response</code></td>
<td>
<p>[numeric] vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(123)
truth = rnorm(n)
response = rnorm(n)
MAPE(truth, response)
</code></pre>

<hr>
<h2 id='MCC'>Matthews correlation coefficient</h2><span id='topic+MCC'></span>

<h3>Description</h3>

<p>Defined as (tp * tn - fp * fn) / sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)), denominator set to 1 if 0.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MCC(truth, response, negative, positive)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MCC_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="MCC_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
<tr><td><code id="MCC_+3A_negative">negative</code></td>
<td>
<p>negative class</p>
</td></tr>
<tr><td><code id="MCC_+3A_positive">positive</code></td>
<td>
<p>positive class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
positive = 1
negative = 0
MCC(truth, response, negative, positive)
</code></pre>

<hr>
<h2 id='MEDAE'>Median of absolute errors</h2><span id='topic+MEDAE'></span>

<h3>Description</h3>

<p>Defined as: median(abs(response - truth)).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MEDAE(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MEDAE_+3A_truth">truth</code></td>
<td>
<p>[numeric] vector of true values</p>
</td></tr>
<tr><td><code id="MEDAE_+3A_response">response</code></td>
<td>
<p>[numeric] vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(123)
truth = rnorm(n)
response = rnorm(n)
MEDAE(truth, response)
</code></pre>

<hr>
<h2 id='MEDSE'>Median of squared errors</h2><span id='topic+MEDSE'></span>

<h3>Description</h3>

<p>Defined as: median((response - truth)^2).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MEDSE(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MEDSE_+3A_truth">truth</code></td>
<td>
<p>[numeric] vector of true values</p>
</td></tr>
<tr><td><code id="MEDSE_+3A_response">response</code></td>
<td>
<p>[numeric] vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(123)
truth = rnorm(n)
response = rnorm(n)
MEDSE(truth, response)
</code></pre>

<hr>
<h2 id='MMCE'>Mean misclassification error</h2><span id='topic+MMCE'></span>

<h3>Description</h3>

<p>Defined as: mean(response != truth)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MMCE(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MMCE_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="MMCE_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(122)
truth = as.factor(sample(c(1,2,3), n, replace = TRUE))
response = as.factor(sample(c(1,2,3), n, replace = TRUE))
MMCE(truth, response)
</code></pre>

<hr>
<h2 id='MSE'>Mean of squared errors</h2><span id='topic+MSE'></span>

<h3>Description</h3>

<p>Defined as: mean((response - truth)^2)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MSE(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MSE_+3A_truth">truth</code></td>
<td>
<p>[numeric] vector of true values</p>
</td></tr>
<tr><td><code id="MSE_+3A_response">response</code></td>
<td>
<p>[numeric] vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(123)
truth = rnorm(n)
response = rnorm(n)
MSE(truth, response)
</code></pre>

<hr>
<h2 id='MSLE'>Mean squared logarithmic error</h2><span id='topic+MSLE'></span>

<h3>Description</h3>

<p>Defined as: mean((log(response + 1, exp(1)) - log(truth + 1, exp(1)))^2).
This is mostly used for count data, note that all predicted and actual target values must be greater or equal '-1'
to compute the mean squared logarithmic error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MSLE(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MSLE_+3A_truth">truth</code></td>
<td>
<p>[numeric] vector of true values</p>
</td></tr>
<tr><td><code id="MSLE_+3A_response">response</code></td>
<td>
<p>[numeric] vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(123)
truth = abs(rnorm(n))
response = abs(rnorm(n))
MSLE(truth, response)
</code></pre>

<hr>
<h2 id='multiclass.AU1P'>Weighted average 1 vs. 1 multiclass AUC</h2><span id='topic+multiclass.AU1P'></span>

<h3>Description</h3>

<p>Computes AUC of c(c - 1) binary classifiers while considering the a priori distribution of the classes. 
See Ferri et al.: https://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multiclass.AU1P(probabilities, truth)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multiclass.AU1P_+3A_probabilities">probabilities</code></td>
<td>
<p>[numeric] matrix of predicted probabilities with columnnames of the classes</p>
</td></tr>
<tr><td><code id="multiclass.AU1P_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(122)
truth = as.factor(sample(c(1,2,3), n, replace = TRUE))
probabilities = matrix(runif(60), 20, 3)
probabilities = probabilities/rowSums(probabilities)
colnames(probabilities) = c(1,2,3)
multiclass.AU1P(probabilities, truth)
</code></pre>

<hr>
<h2 id='multiclass.AU1U'>Average 1 vs. 1 multiclass AUC</h2><span id='topic+multiclass.AU1U'></span>

<h3>Description</h3>

<p>Computes AUC of c(c - 1) binary classifiers (all possible pairwise combinations) 
while considering uniform distribution of the classes. 
See Ferri et al.: https://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multiclass.AU1U(probabilities, truth)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multiclass.AU1U_+3A_probabilities">probabilities</code></td>
<td>
<p>[numeric] matrix of predicted probabilities with columnnames of the classes</p>
</td></tr>
<tr><td><code id="multiclass.AU1U_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(122)
truth = as.factor(sample(c(1,2,3), n, replace = TRUE))
probabilities = matrix(runif(60), 20, 3)
probabilities = probabilities/rowSums(probabilities)
colnames(probabilities) = c(1,2,3)
multiclass.AU1U(probabilities, truth)
</code></pre>

<hr>
<h2 id='multiclass.AUNP'>Weighted average 1 vs. rest multiclass AUC</h2><span id='topic+multiclass.AUNP'></span>

<h3>Description</h3>

<p>Computes the AUC treating a c-dimensional classifier as c two-dimensional classifiers, 
taking into account the prior probability of each class. 
See Ferri et al.: https://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multiclass.AUNP(probabilities, truth)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multiclass.AUNP_+3A_probabilities">probabilities</code></td>
<td>
<p>[numeric] matrix of predicted probabilities with columnnames of the classes</p>
</td></tr>
<tr><td><code id="multiclass.AUNP_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(122)
truth = as.factor(sample(c(1,2,3), n, replace = TRUE))
probabilities = matrix(runif(60), 20, 3)
probabilities = probabilities/rowSums(probabilities)
colnames(probabilities) = c(1,2,3)
multiclass.AUNP(probabilities, truth)
</code></pre>

<hr>
<h2 id='multiclass.AUNU'>Average 1 vs. rest multiclass AUC</h2><span id='topic+multiclass.AUNU'></span>

<h3>Description</h3>

<p>Computes the AUC treating a c-dimensional classifier as c two-dimensional classifiers, 
where classes are assumed to have uniform distribution, in order to have a measure which is 
independent of class distribution change. 
See Ferri et al.: https://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multiclass.AUNU(probabilities, truth)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multiclass.AUNU_+3A_probabilities">probabilities</code></td>
<td>
<p>[numeric] matrix of predicted probabilities with columnnames of the classes</p>
</td></tr>
<tr><td><code id="multiclass.AUNU_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(122)
truth = as.factor(sample(c(1,2,3), n, replace = TRUE))
probabilities = matrix(runif(60), 20, 3)
probabilities = probabilities/rowSums(probabilities)
colnames(probabilities) = c(1,2,3)
multiclass.AUNU(probabilities, truth)
</code></pre>

<hr>
<h2 id='multiclass.Brier'>Multiclass Brier score</h2><span id='topic+multiclass.Brier'></span>

<h3>Description</h3>

<p>Defined as: (1/n) sum_i sum_j (y_ij - p_ij)^2, where y_ij = 1 if observation i has class j (else 0), 
and p_ij is the predicted probability of observation i for class j. 
From http://docs.lib.noaa.gov/rescue/mwr/078/mwr-078-01-0001.pdf.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multiclass.Brier(probabilities, truth)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multiclass.Brier_+3A_probabilities">probabilities</code></td>
<td>
<p>[numeric] matrix of predicted probabilities with columnnames of the classes</p>
</td></tr>
<tr><td><code id="multiclass.Brier_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(122)
truth = as.factor(sample(c(1,2,3), n, replace = TRUE))
probabilities = matrix(runif(60), 20, 3)
probabilities = probabilities/rowSums(probabilities)
colnames(probabilities) = c(1,2,3)
multiclass.Brier(probabilities, truth)
</code></pre>

<hr>
<h2 id='MultilabelACC'>Accuracy (multilabel)</h2><span id='topic+MultilabelACC'></span>

<h3>Description</h3>

<p>Averaged proportion of correctly predicted labels with respect to the total number of labels for each instance,
following the definition by Charte and Charte: https: / /journal.r-project.org / archive / 2015 - 2 / charte-charte.pdf.
Fractions where the denominator becomes 0 are replaced with 1 before computing the average across all instances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MultilabelACC(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MultilabelACC_+3A_truth">truth</code></td>
<td>
<p>matrix of true values</p>
</td></tr>
<tr><td><code id="MultilabelACC_+3A_response">response</code></td>
<td>
<p>matrix of predicted values
n = 20
set.seed(122)
truth = matrix(sample(c(0,1), 60, replace = TRUE), 20, 3)
response = matrix(sample(c(0,1), 60, replace = TRUE), 20, 3)
MultilabelACC(truth, response)</p>
</td></tr>
</table>

<hr>
<h2 id='MultilabelF1'>F1 measure (multilabel)</h2><span id='topic+MultilabelF1'></span>

<h3>Description</h3>

<p>Harmonic mean of precision and recall on a per instance basis (Micro-F1), following the
definition by Montanes et al.: http: / /www.sciencedirect.com / science / article / pii / S0031320313004019.
Fractions where the denominator becomes 0 are replaced with 1 before computing the average across all instances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MultilabelF1(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MultilabelF1_+3A_truth">truth</code></td>
<td>
<p>matrix of true values</p>
</td></tr>
<tr><td><code id="MultilabelF1_+3A_response">response</code></td>
<td>
<p>matrix of predicted values
n = 20
set.seed(122)
truth = matrix(sample(c(0,1), 60, replace = TRUE), 20, 3)
response = matrix(sample(c(0,1), 60, replace = TRUE), 20, 3)
MultilabelF1(truth, response)</p>
</td></tr>
</table>

<hr>
<h2 id='MultilabelHamloss'>Hamming loss</h2><span id='topic+MultilabelHamloss'></span>

<h3>Description</h3>

<p>Proportion of labels that are predicted incorrectly, following the definition
by Charte and Charte: https://journal.r-project.org/archive/2015-2/charte-charte.pdf.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MultilabelHamloss(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MultilabelHamloss_+3A_truth">truth</code></td>
<td>
<p>matrix of true values</p>
</td></tr>
<tr><td><code id="MultilabelHamloss_+3A_response">response</code></td>
<td>
<p>matrix of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(122)
truth = matrix(sample(c(0,1), 60, replace = TRUE), 20, 3)
response = matrix(sample(c(0,1), 60, replace = TRUE), 20, 3)
MultilabelHamloss(truth, response)
</code></pre>

<hr>
<h2 id='MultilabelPPV'>Positive predictive value (multilabel)</h2><span id='topic+MultilabelPPV'></span>

<h3>Description</h3>

<p>Also called precision. Averaged ratio of correctly predicted labels for each instance,
following the definition by Charte and Charte: https: / /journal.r-project.org / archive / 2015 - 2 / charte-charte.pdf.
Fractions where the denominator becomes 0 are ignored in the average calculation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MultilabelPPV(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MultilabelPPV_+3A_truth">truth</code></td>
<td>
<p>matrix of true values</p>
</td></tr>
<tr><td><code id="MultilabelPPV_+3A_response">response</code></td>
<td>
<p>matrix of predicted values
n = 20
set.seed(122)
truth = matrix(sample(c(0,1), 60, replace = TRUE), 20, 3)
response = matrix(sample(c(0,1), 60, replace = TRUE), 20, 3)
MultilabelPPV(truth, response)</p>
</td></tr>
</table>

<hr>
<h2 id='MultilabelSubset01'>Subset-0-1 loss</h2><span id='topic+MultilabelSubset01'></span>

<h3>Description</h3>

<p>Proportion of observations where the complete multilabel set (all 0-1-labels) is predicted incorrectly,
following the definition by Charte and Charte: https://journal.r-project.org/archive/2015-2/charte-charte.pdf.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MultilabelSubset01(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MultilabelSubset01_+3A_truth">truth</code></td>
<td>
<p>matrix of true values</p>
</td></tr>
<tr><td><code id="MultilabelSubset01_+3A_response">response</code></td>
<td>
<p>matrix of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(122)
truth = matrix(sample(c(0,1), 60, replace = TRUE), 20, 3)
response = matrix(sample(c(0,1), 60, replace = TRUE), 20, 3)
MultilabelSubset01(truth, response)
</code></pre>

<hr>
<h2 id='MultilabelTPR'>TPR (multilabel)</h2><span id='topic+MultilabelTPR'></span>

<h3>Description</h3>

<p>Also called recall. Averaged proportion of predicted labels which are relevant for each instance,
following the definition by Charte and Charte: https: / /journal.r-project.org / archive / 2015 - 2 / charte-charte.pdf.
Fractions where the denominator becomes 0 are ignored in the average calculation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MultilabelTPR(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MultilabelTPR_+3A_truth">truth</code></td>
<td>
<p>matrix of true values</p>
</td></tr>
<tr><td><code id="MultilabelTPR_+3A_response">response</code></td>
<td>
<p>matrix of predicted values
n = 20
set.seed(122)
truth = matrix(sample(c(0,1), 60, replace = TRUE), 20, 3)
response = matrix(sample(c(0,1), 60, replace = TRUE), 20, 3)
MultilabelTPR(truth, response)</p>
</td></tr>
</table>

<hr>
<h2 id='NPV'>Negative predictive value</h2><span id='topic+NPV'></span>

<h3>Description</h3>

<p>Defined as: tn / (tn + fn).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NPV(truth, response, negative)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NPV_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="NPV_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
<tr><td><code id="NPV_+3A_negative">negative</code></td>
<td>
<p>negative class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
negative = 0
NPV(truth, response, negative)
</code></pre>

<hr>
<h2 id='PPV'>Positive predictive value</h2><span id='topic+PPV'></span>

<h3>Description</h3>

<p>Defined as: tp / (tp + fp). Also called precision. If the denominator is 0, PPV is set to be either 1 or 0 
depending on whether the highest probability prediction is positive (1) or negative (0).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PPV(truth, response, positive, probabilities = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PPV_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="PPV_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
<tr><td><code id="PPV_+3A_positive">positive</code></td>
<td>
<p>positive class</p>
</td></tr>
<tr><td><code id="PPV_+3A_probabilities">probabilities</code></td>
<td>
<p>[numeric] vector of predicted probabilities</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
positive = 1
PPV(truth, response, positive, probabilities = NULL)
</code></pre>

<hr>
<h2 id='QSR'>Quadratic Scoring Rule</h2><span id='topic+QSR'></span>

<h3>Description</h3>

<p>Defined as: 1 - (1/n) sum_i sum_j (y_ij - p_ij)^2, where y_ij = 1 if observation i has class j (else 0), 
and p_ij is the predicted probablity of observation i for class j.
This scoring rule is the same as 1 - multiclass.brier.
See: Bickel, J. E. (2007). Some comparisons among quadratic, spherical, and logarithmic scoring rules. Decision Analysis, 4(2), 49-65.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>QSR(probabilities, truth)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="QSR_+3A_probabilities">probabilities</code></td>
<td>
<p>[numeric] vector (or matrix with column names of the classes) of predicted probabilities</p>
</td></tr>
<tr><td><code id="QSR_+3A_truth">truth</code></td>
<td>
<p>vector of true values 
n = 20
set.seed(122)
truth = as.factor(sample(c(1,2,3), n, replace = TRUE))
probabilities = matrix(runif(60), 20, 3)
probabilities = probabilities/rowSums(probabilities)
colnames(probabilities) = c(1,2,3)
QSR(probabilities, truth)</p>
</td></tr>
</table>

<hr>
<h2 id='RAE'>Relative absolute error</h2><span id='topic+RAE'></span>

<h3>Description</h3>

<p>Defined as sum_of_absolute_errors / mean_absolute_deviation. Undefined for single instances and when every truth value is identical. In this case the output will be NA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RAE(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RAE_+3A_truth">truth</code></td>
<td>
<p>[numeric] vector of true values</p>
</td></tr>
<tr><td><code id="RAE_+3A_response">response</code></td>
<td>
<p>[numeric] vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(123)
truth = rnorm(n)
response = rnorm(n)
RAE(truth, response)
</code></pre>

<hr>
<h2 id='RMSE'>Root mean squared error</h2><span id='topic+RMSE'></span>

<h3>Description</h3>

<p>The RMSE is aggregated as sqrt(mean(rmse.vals.on.test.sets^2))
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RMSE(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RMSE_+3A_truth">truth</code></td>
<td>
<p>[numeric] vector of true values</p>
</td></tr>
<tr><td><code id="RMSE_+3A_response">response</code></td>
<td>
<p>[numeric] vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(123)
truth = rnorm(n)
response = rnorm(n)
RMSE(truth, response)
</code></pre>

<hr>
<h2 id='RMSLE'>Root mean squared logarithmic error</h2><span id='topic+RMSLE'></span>

<h3>Description</h3>

<p>Definition taken from: https: / /www.kaggle.com / wiki / RootMeanSquaredLogarithmicError.
This  is mostly used for count data, note that all predicted and actual target values
must be greater or equal '-1' to compute the root mean squared logarithmic error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RMSLE(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RMSLE_+3A_truth">truth</code></td>
<td>
<p>[numeric] vector of true values</p>
</td></tr>
<tr><td><code id="RMSLE_+3A_response">response</code></td>
<td>
<p>[numeric] vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(123)
truth = abs(rnorm(n))
response = abs(rnorm(n))
RMSLE(truth, response)
</code></pre>

<hr>
<h2 id='RRSE'>Root relative squared error</h2><span id='topic+RRSE'></span>

<h3>Description</h3>

<p>Defined as sqrt (sum_of_squared_errors / total_sum_of_squares). Undefined for single instances and when every truth value is identical. In this case the output will be NA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RRSE(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RRSE_+3A_truth">truth</code></td>
<td>
<p>[numeric] vector of true values</p>
</td></tr>
<tr><td><code id="RRSE_+3A_response">response</code></td>
<td>
<p>[numeric] vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(123)
truth = rnorm(n)
response = rnorm(n)
RRSE(truth, response)
</code></pre>

<hr>
<h2 id='RSQ'>Coefficient of determination</h2><span id='topic+RSQ'></span>

<h3>Description</h3>

<p>Also called R-squared, which is 1 - residual_sum_of_squares / total_sum_of_squares.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RSQ(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RSQ_+3A_truth">truth</code></td>
<td>
<p>[numeric] vector of true values</p>
</td></tr>
<tr><td><code id="RSQ_+3A_response">response</code></td>
<td>
<p>[numeric] vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(123)
truth = rnorm(n)
response = rnorm(n)
RSQ(truth, response)
</code></pre>

<hr>
<h2 id='SAE'>Sum of absolute errors</h2><span id='topic+SAE'></span>

<h3>Description</h3>

<p>Defined as: sum(abs(response - truth))&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SAE(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SAE_+3A_truth">truth</code></td>
<td>
<p>[numeric] vector of true values</p>
</td></tr>
<tr><td><code id="SAE_+3A_response">response</code></td>
<td>
<p>[numeric] vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(123)
truth = rnorm(n)
response = rnorm(n)
SAE(truth, response)
</code></pre>

<hr>
<h2 id='SpearmanRho'>Spearman's rho</h2><span id='topic+SpearmanRho'></span>

<h3>Description</h3>

<p>Defined as: Spearman's rho correlation between truth and response. Only looks at the order.
See Rosset et al.: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.95.1398&amp;rep=rep1&amp;type=pdf.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SpearmanRho(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SpearmanRho_+3A_truth">truth</code></td>
<td>
<p>[numeric] vector of true values</p>
</td></tr>
<tr><td><code id="SpearmanRho_+3A_response">response</code></td>
<td>
<p>[numeric] vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(123)
truth = rnorm(n)
response = rnorm(n)
SpearmanRho(truth, response)
</code></pre>

<hr>
<h2 id='SSE'>Sum of squared errors</h2><span id='topic+SSE'></span>

<h3>Description</h3>

<p>Defined as: sum((response - truth)^2)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSE(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSE_+3A_truth">truth</code></td>
<td>
<p>[numeric] vector of true values</p>
</td></tr>
<tr><td><code id="SSE_+3A_response">response</code></td>
<td>
<p>[numeric] vector of predicted values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(123)
truth = rnorm(n)
response = rnorm(n)
SSE(truth, response)
</code></pre>

<hr>
<h2 id='SSR'>Spherical Scoring Rule</h2><span id='topic+SSR'></span>

<h3>Description</h3>

<p>Defined as: mean(p_i(sum_j(p_ij))), where p_i is the predicted probability of the true 
class of observation i and p_ij is the predicted probablity of observation i for class j.
See: Bickel, J. E. (2007). Some comparisons among quadratic, spherical, and logarithmic 
scoring rules. Decision Analysis, 4(2), 49-65.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSR(probabilities, truth)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSR_+3A_probabilities">probabilities</code></td>
<td>
<p>[numeric] vector (or matrix with column names of the classes) of predicted probabilities</p>
</td></tr>
<tr><td><code id="SSR_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(122)
truth = as.factor(sample(c(1,2,3), n, replace = TRUE))
probabilities = matrix(runif(60), 20, 3)
probabilities = probabilities/rowSums(probabilities)
colnames(probabilities) = c(1,2,3)
SSR(probabilities, truth)
</code></pre>

<hr>
<h2 id='TN'>True negatives</h2><span id='topic+TN'></span>

<h3>Description</h3>

<p>Sum of correctly classified observations in the negative class. Also called correct rejections.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TN(truth, response, negative)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TN_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="TN_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
<tr><td><code id="TN_+3A_negative">negative</code></td>
<td>
<p>negative class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
negative = 0
TN(truth, response, negative)
</code></pre>

<hr>
<h2 id='TNR'>True negative rate</h2><span id='topic+TNR'></span>

<h3>Description</h3>

<p>Percentage of correctly classified observations in the negative class. Also called specificity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TNR(truth, response, negative)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TNR_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="TNR_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
<tr><td><code id="TNR_+3A_negative">negative</code></td>
<td>
<p>negative class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
negative = 0
TNR(truth, response, negative)
</code></pre>

<hr>
<h2 id='TP'>True positives</h2><span id='topic+TP'></span>

<h3>Description</h3>

<p>Sum of all correctly classified observations in the positive class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TP(truth, response, positive)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TP_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="TP_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
<tr><td><code id="TP_+3A_positive">positive</code></td>
<td>
<p>positive class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
positive = 1
TP(truth, response, positive)
</code></pre>

<hr>
<h2 id='TPR'>True positive rate</h2><span id='topic+TPR'></span>

<h3>Description</h3>

<p>Percentage of correctly classified observations in the positive class. Also called hit rate or recall or sensitivity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TPR(truth, response, positive)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TPR_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="TPR_+3A_response">response</code></td>
<td>
<p>vector of predicted values</p>
</td></tr>
<tr><td><code id="TPR_+3A_positive">positive</code></td>
<td>
<p>positive class</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>n = 20
set.seed(125)
truth = as.factor(sample(c(1,0), n, replace = TRUE))
probabilities = runif(n)
response = as.factor(as.numeric(probabilities &gt; 0.5))
positive = 1
TPR(truth, response, positive)
</code></pre>

<hr>
<h2 id='WKAPPA'>Mean quadratic weighted kappa</h2><span id='topic+WKAPPA'></span>

<h3>Description</h3>

<p>Defined as: 1 - sum(weights * conf.mat) / sum(weights * expected.mat),
the weight matrix measures seriousness of disagreement with the squared euclidean metric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>WKAPPA(truth, response)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="WKAPPA_+3A_truth">truth</code></td>
<td>
<p>vector of true values</p>
</td></tr>
<tr><td><code id="WKAPPA_+3A_response">response</code></td>
<td>
<p>vector of predicted values
n = 20
set.seed(122)
truth = as.factor(sample(c(1,2,3), n, replace = TRUE))
response = as.factor(sample(c(1,2,3), n, repla
WKAPPA(truth, response)</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
