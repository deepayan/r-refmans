<!DOCTYPE html><html lang="en"><head><title>Help for package randomMachines</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {randomMachines}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bolsafam'><p>Bolsa Família Dataset</p></a></li>
<li><a href='#brier_score'><p>Brier Score function</p></a></li>
<li><a href='#ionosphere'><p>Ionosphere Dataset</p></a></li>
<li><a href='#predict.rm_class'><p>Prediction function for the rm_class_model</p></a></li>
<li><a href='#predict.rm_reg'><p>Prediction function for the rm_reg_model</p></a></li>
<li><a href='#randomMachines'><p>Random Machines</p></a></li>
<li><a href='#rm_class-class'><p>S4 class for RM classification</p></a></li>
<li><a href='#rm_reg-class'><p>S4 class for RM regression</p></a></li>
<li><a href='#RMSE'><p>Root Mean Squared Error (RMSE) Function</p></a></li>
<li><a href='#sim_class'><p>Generate a binary classification data set from normal distribution</p></a></li>
<li><a href='#sim_reg1'><p>Simulation for a regression toy examples from Random Machines Regression 1</p></a></li>
<li><a href='#sim_reg2'><p>Simulation for a regression toy examples from Random Machines Regression 2</p></a></li>
<li><a href='#sim_reg3'><p>Simulation for a regression toy examples from Random Machines Regression 3</p></a></li>
<li><a href='#sim_reg4'><p>Simulation for a regression toy examples from Random Machines Regression 3</p></a></li>
<li><a href='#sim_reg5'><p>Simulation for a regression toy examples from Random Machines Regression 3</p></a></li>
<li><a href='#whosale'><p>Wholesale Dataset</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>An Ensemble Modeling using Random Machines</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>A novel ensemble method employing Support Vector Machines (SVMs) as base learners. This powerful ensemble model is designed for both classification (Ara A., et. al, 2021) &lt;<a href="https://doi.org/10.6339%2F21-JDS1014">doi:10.6339/21-JDS1014</a>&gt;, and regression (Ara A., et. al, 2021) &lt;<a href="https://doi.org/10.1016%2Fj.eswa.2022.117107">doi:10.1016/j.eswa.2022.117107</a>&gt; problems, offering versatility and robust performance across different datasets and compared with other consolidated methods as Random Forests (Maia M, et. al, 2021) &lt;<a href="https://doi.org/10.6339%2F21-JDS1025">doi:10.6339/21-JDS1025</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Imports:</td>
<td>kernlab, methods, stats</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-14 14:52:32 UTC; mateusmaia</td>
</tr>
<tr>
<td>Author:</td>
<td>Mateus Maia <a href="https://orcid.org/0000-0001-7056-386X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Anderson Ara <a href="https://orcid.org/0000-0002-1041-2768"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [cte],
  Gabriel Ribeiro [cte]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Mateus Maia &lt;mateus.maiamarques.2021@mumail.ie&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-14 16:40:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='bolsafam'>Bolsa Família Dataset</h2><span id='topic+bolsafam'></span>

<h3>Description</h3>

<p>The 'bolsafam' dataset contains information about the utilization rate of the Bolsa Família program in Brazilian municipalities. The utilization rate <code class="reqn">y_{i}</code> is defined as the number of people benefiting from the assistance divided by the total population of the city.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  data(bolsafam)
</code></pre>


<h3>Format</h3>

<p>A data frame with 5564 rows and 11 columns.
</p>


<h3>Details</h3>

<p>This dataset includes the following columns:
</p>

<dl>
<dt>y</dt><dd><p>Rate of use of the social assistance program by municipality.</p>
</dd>
<dt>COD_UF</dt><dd><p>Code to identify the Brazilian state to which the city belongs.</p>
</dd>
<dt>T_DENS</dt><dd><p>Percentage of the population living in households with a density greater than 2 people per bedroom.</p>
</dd>
<dt>TRABSC</dt><dd><p>Percentage of employed persons aged 18 or over who are employed without a formal contract.</p>
</dd>
<dt>PPOB</dt><dd><p>Proportion of people vulnerable to poverty.</p>
</dd>
<dt>T_NESTUDA_NTRAB_MMEIO</dt><dd><p>Percentage of people aged 15 to 24 who do not study or work and are vulnerable to poverty.</p>
</dd>
<dt>T_FUND15A17</dt><dd><p>Percentage of the population aged 15 to 17 with complete primary education.</p>
</dd>
<dt>RAZDEP</dt><dd><p>Dependency ratio.</p>
</dd>
<dt>T_ATRASO_0_BASICO</dt><dd><p>Percentage of the population aged 6 to 17 years attending basic education that does not have an age-grade delay.</p>
</dd>
<dt>T_AGUA</dt><dd><p>Percentage of the population living in households with running water.</p>
</dd>
<dt>REGIAO</dt><dd><p>Aggregation of states according to the regions defined by IBGE.</p>
</dd>
</dl>



<h3>Source</h3>

<p>The 'bolsafam' dataset is sourced from the Brazilian organizational site called <em>Transparency Portal</em>.
</p>


<h3>References</h3>

<p>Mateus Maia &amp; Anderson Ara (2023). rmachines: Random Machines: a package for a support vector ensemble based on random kernel space. R package version 0.1.0.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    data(bolsafam)
    head(bolsafam)
</code></pre>

<hr>
<h2 id='brier_score'>Brier Score function</h2><span id='topic+brier_score'></span>

<h3>Description</h3>

<p>Calculate the Brier Score for a set of predicted probabilities and observed outcomes.
The Brier Score is a measure of the accuracy of probabilistic predictions. It is commonly used in the evaluation of predictive models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>brier_score(prob, observed, levels)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="brier_score_+3A_prob">prob</code></td>
<td>
<p>predicted probabilities</p>
</td></tr>
<tr><td><code id="brier_score_+3A_observed">observed</code></td>
<td>
<p><code class="reqn">y</code> observed values (it assumed that the positive class is coded is equal to one and the negative 0)</p>
</td></tr>
<tr><td><code id="brier_score_+3A_levels">levels</code></td>
<td>
<p>A string vector with the original levels from the target variable</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the Brier Score, a numeric value indicating the accuracy of the predictions.
</p>

<hr>
<h2 id='ionosphere'>Ionosphere Dataset</h2><span id='topic+ionosphere'></span>

<h3>Description</h3>

<p>The 'ionosphere' dataset contains radar data for the classification of radar returns as either 'good' or 'bad'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  data(ionosphere)
</code></pre>


<h3>Format</h3>

<p>A data frame with 351 rows and 35 columns.
</p>


<h3>Details</h3>

<p>This dataset includes the following columns:
</p>

<dl>
<dt>X1-X34</dt><dd><p>Features extracted from radar signals.</p>
</dd>
<dt>y</dt><dd><p>Class label indicating whether the radar return is 'g' (good) or 'b' (bad).</p>
</dd>
</dl>



<h3>Source</h3>

<p>The 'ionosphere' dataset is sourced from the UCI Machine Learning Repository:
<a href="https://archive.ics.uci.edu/ml/datasets/ionosphere">https://archive.ics.uci.edu/ml/datasets/ionosphere</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    data(ionosphere)
    head(ionosphere)
</code></pre>

<hr>
<h2 id='predict.rm_class'>Prediction function for the rm_class_model</h2><span id='topic+predict.rm_class'></span><span id='topic+predict+2Crm_class-method'></span>

<h3>Description</h3>

<p>This function predicts the outcome for a RM object model using new data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'rm_class'
predict(object,newdata)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.rm_class_+3A_object">object</code></td>
<td>
<p>A fitted RM model object of class <code>rm_class</code>.</p>
</td></tr>
<tr><td><code id="predict.rm_class_+3A_newdata">newdata</code></td>
<td>
<p>A data frame or matrix containing the new data to be predicted.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predicted outcomes: probabilities in case of 'prob_model = TRUE' and classes in case of 'prob_model = FALSE'.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generating a sample for the simulation
library(randomMachines)
sim_data &lt;- sim_class(n = 75)
sim_new &lt;- sim_class(n = 25)
rm_mod &lt;- randomMachines(y~., train = sim_data)
y_hat &lt;- predict(rm_mod, newdata = sim_new)
</code></pre>

<hr>
<h2 id='predict.rm_reg'>Prediction function for the rm_reg_model</h2><span id='topic+predict.rm_reg'></span><span id='topic+predict+2Crm_reg-method'></span>

<h3>Description</h3>

<p>This function predicts the outcome for a RM object model using new data for continuous <code class="reqn">y</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'rm_reg'
predict(object,newdata)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.rm_reg_+3A_object">object</code></td>
<td>
<p>A fitted RM model object of class <code>rm_reg</code>.</p>
</td></tr>
<tr><td><code id="predict.rm_reg_+3A_newdata">newdata</code></td>
<td>
<p>A data frame or matrix containing the new data to be predicted.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Predicted values <code>newdata</code> object from the Random Machines model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generating a sample for the simulation
library(randomMachines)
sim_data &lt;- sim_reg1(n = 75)
sim_new &lt;- sim_reg1(n = 25)
rm_mod_reg &lt;- randomMachines(y~., train = sim_data)
y_hat &lt;- predict(rm_mod_reg, newdata = sim_new)
</code></pre>

<hr>
<h2 id='randomMachines'>Random Machines</h2><span id='topic+randomMachines'></span>

<h3>Description</h3>

<p>Random Machines is an ensemble model which uses the combination of different kernel functions to improve the diversity in the bagging approach, improving the predictions in general. Random Machines was developed for classification and regression problems by bagging multiple kernel functions in support vector models.
</p>
<p>Random Machines uses SVMs (Cortes and Vapnik, 1995) as base learners in the bagging procedure with a random sample of kernel functions to build them.
</p>
<p>Let a training sample given by <code class="reqn">(\boldsymbol{x_{i}},y_i)</code> with <code class="reqn">i=1,\dots, n</code> observations, where <code class="reqn">\boldsymbol{x_{i}}</code> is the vector of independent variables and <code class="reqn">y_{i}</code> the dependent one. The kernel bagging method initializes by training of the <code class="reqn">r</code> single learner, where <code class="reqn">r=1,\dots,R</code> and <code class="reqn">R</code> is the total number of different kernel functions that could be used in support vector models. In this implementation the default value is <code class="reqn">R=4</code> (gaussian, polynomial, laplacian and linear). See more details below.
</p>
<p>Each single learner is internally validated and the weights <code class="reqn">\lambda_{r}</code> are calculated proportionally to the strength from the single predictive performance.
</p>
<p>Afterwards, <code class="reqn">B</code> bootstrap samples are sampled from the training set. A support vector machine model <code class="reqn">g_{b}</code> is trained for each bootstrap sample, <code class="reqn">b=i,\dots,B</code> and the kernel function that will be used for <code class="reqn">g_{b}</code> will be determined by a random choice with probability <code class="reqn">\lambda_{r}</code>. The final weight <code class="reqn">w_b</code> in the bagging procedure is calculated by out-of-bag samples.
</p>
<p>The final model <code class="reqn">G(\boldsymbol{x}_i)</code> for a new <code class="reqn">\boldsymbol{x}_i</code> is given by,
</p>
<p>The weights <code class="reqn">\lambda_{r}</code> and <code class="reqn">w_b</code> are different calculated for each task (classification, probabilistic classification and regression). See more details in the references.
</p>

<ul>
<li><p> For a binary classification problem <code class="reqn">\mathbin{{ G(\boldsymbol{x_{i}})= \text{sgn} \left( \sum_{b=1}^{B}w_{b}g_{b}(\boldsymbol{x_{i}})\right)}}</code>, where <code class="reqn">g_b</code> are single binary classification outputs;
</p>
</li>
<li><p> For a probabilistic binary classification problem <code class="reqn">\mathbin{{ G(\boldsymbol{x_{i}})= \sum_{b=1}^{B}w_{b}g_{b}(\boldsymbol{x_{i}})}}</code>, where <code class="reqn">g_b</code> are single probabilistic classification outputs;
</p>
</li>
<li><p> For a regression problem <code class="reqn">G(\boldsymbol{x_{i}})= \sum_{b=1}^{B}w_{b}g_{b}(\boldsymbol{x_{i}})</code>, , where <code class="reqn">g_b</code> are single regression outputs.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>randomMachines(
     formula,
     train,validation,
     B = 25, cost = 1,
     automatic_tuning = FALSE,
     gamma_rbf = 1,
     gamma_lap = 1,
     degree = 2,
     poly_scale = 1,
     offset = 0,
     gamma_cau = 1,
     d_t = 2,
     kernels = c("rbfdot", "polydot", "laplacedot", "vanilladot"),
     prob_model = TRUE,
     loss_function = RMSE,
     epsilon = 0.1,
     beta = 2
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="randomMachines_+3A_formula">formula</code></td>
<td>

<p>an object of class <code><a href="stats.html#topic+formula">formula</a></code>: it should contain a symbolic description of the model to be fitted, indicating the dependent variable and all predictors that should be included.
</p>
</td></tr>
<tr><td><code id="randomMachines_+3A_train">train</code></td>
<td>

<p>the training data <code class="reqn">\left\{\left( \mathbf{x}_{i},y_{i} \right)\right\}_{i=1}^{n}</code> used to train the model.
</p>
</td></tr>
<tr><td><code id="randomMachines_+3A_validation">validation</code></td>
<td>

<p>the validation data <code class="reqn">\left\{\left( \mathbf{x}_{i},y_{i}\right)  \right\}_{i=1}^{V}</code> used to calculate probabilities <code class="reqn">\lambda_{r}</code>. If <code>validation = NULL</code>,the validation set is going be selected as 0.25 partition from the training data, and the remaining partition is selected as the new training sample.
</p>
</td></tr>
<tr><td><code id="randomMachines_+3A_b">B</code></td>
<td>

<p>number of bootstrap samples. The default value is <code>B=25</code>.
</p>
</td></tr>
<tr><td><code id="randomMachines_+3A_cost">cost</code></td>
<td>

<p>the <code class="reqn">C</code>-constant term of the regularization on soft margins at support vector models. The default value is <code>cost=1</code>.
</p>
</td></tr>
<tr><td><code id="randomMachines_+3A_automatic_tuning">automatic_tuning</code></td>
<td>

<p>boolean to define if the kernel hyperparameters will be selected using the <code>sigest</code> from the <code>ksvm</code> function. The default value is <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="randomMachines_+3A_gamma_rbf">gamma_rbf</code></td>
<td>

<p>the hyperparameter <code class="reqn">\gamma_{g}</code> used in the RBF kernel. The default value is <code>gamma_rbf=1</code>.
</p>
</td></tr>
<tr><td><code id="randomMachines_+3A_gamma_lap">gamma_lap</code></td>
<td>

<p>the hyperparameter <code class="reqn">\gamma_{l}</code> used in the Laplacian kernel. The default value is <code>gamma_lap=1</code>.
</p>
</td></tr>
<tr><td><code id="randomMachines_+3A_degree">degree</code></td>
<td>

<p>the degree used in the Polynomial kernel. The default value is <code>degree=2</code>.
</p>
</td></tr>
<tr><td><code id="randomMachines_+3A_poly_scale">poly_scale</code></td>
<td>

<p>the scale parameter from the Polynomial kernel. The default value is <code>poly_scale=1</code>.
</p>
</td></tr>
<tr><td><code id="randomMachines_+3A_offset">offset</code></td>
<td>

<p>the offset parameter from the Polynomial kernel. The default value is <code>offset=0</code>.
</p>
</td></tr>
<tr><td><code id="randomMachines_+3A_gamma_cau">gamma_cau</code></td>
<td>

<p>the hyperparameter <code class="reqn">\gamma_{c}</code> used in the Cauchy kernel. The default value is <code>gamma_cau=1</code>.
</p>
</td></tr>
<tr><td><code id="randomMachines_+3A_d_t">d_t</code></td>
<td>

<p>the <code class="reqn">d_{t}</code>-norm from the t-Student kernel. The default value is <code>d_t=2</code>.
</p>
</td></tr>
<tr><td><code id="randomMachines_+3A_kernels">kernels</code></td>
<td>

<p>a vector with the name of kernel functions that will be used in the Random Machines model. The default include the kernel functions: <code>c("rbfdot", "polydot", "laplacedot", "vanilladot").</code> The other kernel functions as <code>"cauchydot"</code> and <code>"tdot"</code> are exclusive to the binary classification setting.
</p>
</td></tr>
<tr><td><code id="randomMachines_+3A_prob_model">prob_model</code></td>
<td>

<p>a boolean to define if the algorithm will be using a probabilistic approach to the define the predictions (default = <code>TRUE</code>).
</p>
</td></tr>
<tr><td><code id="randomMachines_+3A_loss_function">loss_function</code></td>
<td>

<p>Define which loss function is going to be used in the regression approach. The default is the <code>RMSE</code> function but others can be added.
</p>
</td></tr>
<tr><td><code id="randomMachines_+3A_epsilon">epsilon</code></td>
<td>

<p>The epsilon in the loss function used from the SVR implementation. The default value is <code>epsilon=0.1</code>.
</p>
</td></tr>
<tr><td><code id="randomMachines_+3A_beta">beta</code></td>
<td>

<p>The correlation parameter <code class="reqn">\beta</code> which calibrates the penalisation of each kernel performance in regression tasks.  The default value is <code>beta=2</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Random Machines is an ensemble method which combines the bagging procedure proposed by Breiman (1996), using Support Vector Machine models as base learners jointly with a random selection of kernel functions that add diversity to the ensemble without harming its predictive performance. The kernel functions <code class="reqn">k(x,y)</code> are described by the functions below,
</p>

<ul>
<li> <p>Linear Kernel: <code class="reqn">k(x,y) = (x\cdot y)</code>
</p>
</li>
<li> <p>Polynomial Kernel: <code class="reqn">k(x,y) = \left(scale( x\cdot y) + offset\right)^{degree}</code>
</p>
</li>
<li> <p>Gaussian Kernel: <code class="reqn">k(x,y) = e^{-\gamma_{g}||x-y||^2}</code>
</p>
</li>
<li> <p>Laplacian Kernel:  <code class="reqn">k(x,y) = e^{-\gamma_{\ell}||x-y||}</code>
</p>
</li>
<li> <p>Cauchy Kernel: <code class="reqn">k(x,y) = \frac{1}{1 + \frac{||x-y||^{2}}{\gamma_{c}}}</code>
</p>
</li>
<li> <p>Student's t Kernel: <code class="reqn">k(x,y) = \frac{1}{1 + ||x-y||^{d_{t}}}</code>
</p>
</li></ul>



<h3>Value</h3>

 <p><code>randomMachines()</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;rm_class&quot; for classification tasks or &quot;rm_reg&quot; for if the target variable is a continuous numerical response. See <code><a href="#topic+predict.rm_class">predict.rm_class</a></code> or <code><a href="#topic+predict.rm_reg">predict.rm_reg</a></code> for more details of how to obtain predictions from each model respectively.</p>


<h3>Author(s)</h3>

<p>Mateus Maia: <a href="mailto:mateusmaia11@gmail.com">mateusmaia11@gmail.com</a>,
Gabriel Felipe Ribeiro: <a href="mailto:brielribeiro08@gmail.com">brielribeiro08@gmail.com</a>,
Anderson Ara: <a href="mailto:ara@ufpr.br">ara@ufpr.br</a>
</p>


<h3>References</h3>

<p>Ara, Anderson, et al. &quot;Regression random machines: An ensemble support vector regression model with free kernel choice.&quot; Expert Systems with Applications 202 (2022): 117107.
</p>
<p>Ara, Anderson, et al. &quot;Random machines: A bagged-weighted support vector model with free kernel choice.&quot; Journal of Data Science 19.3 (2021): 409-428.
</p>
<p>Breiman, L. (1996). Bagging predictors. Machine learning, 24, 123-140.
</p>
<p>Cortes, C., and Vapnik, V. (1995). Support-vector networks. Machine learning, 20, 273-297.
</p>
<p>Maia, Mateus, Arthur R. Azevedo, and Anderson Ara. &quot;Predictive comparison between random machines and random forests.&quot; Journal of Data Science 19.4 (2021): 593-614.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(randomMachines)

# Simulation from a binary output context
sim_data &lt;- sim_class(n = 75)

## Setting the training and validation set
sim_new &lt;- sim_class(n = 75)

# Modelling Random Machines (probabilistic output)
rm_mod_prob &lt;- randomMachines(y~., train = sim_data)

## Modelling Random Machines (binary class output)
rm_mod_label &lt;- randomMachines(y~., train = sim_data,prob_model = FALSE)

## Predicting for new data
y_hat &lt;- predict(rm_mod_label,sim_new)
</code></pre>

<hr>
<h2 id='rm_class-class'>S4 class for RM classification</h2><span id='topic+rm_class-class'></span><span id='topic+rm_class'></span>

<h3>Description</h3>

<p>S4 class for RM classification
</p>


<h3>Details</h3>

<p>For more details see Ara, Anderson, et al. &quot;Random machines: A bagged-weighted support vector model with free kernel choice.&quot; Journal of Data Science 19.3 (2021): 409-428.
</p>


<h3>Slots</h3>


<dl>
<dt><code>train</code></dt><dd><p>a <code>data.frame</code> corresponding to the training data used into the model</p>
</dd>
<dt><code>class_name</code></dt><dd><p>a string with target variable used in the model</p>
</dd>
<dt><code>kernel_weight</code></dt><dd><p>a numeric vector corresponding to the weights for each bootstrap model contribution</p>
</dd>
<dt><code>lambda_values</code></dt><dd><p>a named list with value of the vector of <code class="reqn">\boldsymbol{\lambda}</code> sampling probabilities associated with each each kernel function</p>
</dd>
<dt><code>model_params</code></dt><dd><p>a list with all used model specifications</p>
</dd>
<dt><code>bootstrap_models</code></dt><dd><p>a list with all <code>ksvm</code> objects for each bootstrap sample</p>
</dd>
<dt><code>bootstrap_samples</code></dt><dd><p>a list with all bootstrap samples used to train each base model of the ensemble</p>
</dd>
<dt><code>prob</code></dt><dd><p>a boolean indicating if a probabilitistic approch was used in the classification Random Machines</p>
</dd>
</dl>

<hr>
<h2 id='rm_reg-class'>S4 class for RM regression</h2><span id='topic+rm_reg-class'></span><span id='topic+rm_reg'></span>

<h3>Description</h3>

<p>S4 class for RM regression
</p>


<h3>Details</h3>

<p>For more details see Ara, Anderson, et al. &quot;Regression random machines: An ensemble support vector regression model with free kernel choice.&quot; Expert Systems with Applications 202 (2022): 117107.
</p>


<h3>Slots</h3>


<dl>
<dt><code>y_train_hat</code></dt><dd><p>a numeric  corresponding to the predictions <code class="reqn">\hat{y}_{i}</code> for the training set</p>
</dd>
<dt><code>lambda_values</code></dt><dd><p>a named list with value of the vector of <code class="reqn">\boldsymbol{\lambda}</code> sampling probabilities associated with each each kernel function</p>
</dd>
<dt><code>model_params</code></dt><dd><p>a list with all used model specifications</p>
</dd>
<dt><code>bootstrap_models</code></dt><dd><p>a list with all <code>ksvm</code> objects for each bootstrap sample</p>
</dd>
<dt><code>bootstrap_samples</code></dt><dd><p>a list with all bootstrap samples used to train each base model of the ensemble</p>
</dd>
<dt><code>kernel_weight_norm</code></dt><dd><p>a numeric vector corresponding to the normalised weights for each bootstrap model contribution</p>
</dd>
</dl>

<hr>
<h2 id='RMSE'>Root Mean Squared Error (RMSE) Function</h2><span id='topic+RMSE'></span>

<h3>Description</h3>

<p>Computes the Root Mean Squared Error (RMSE), a widely used metric for evaluating the accuracy of predictions in regression tasks. The formula is given by
</p>
<p style="text-align: center;"><code class="reqn">RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}}</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>RMSE(predicted, observed)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RMSE_+3A_predicted">predicted</code></td>
<td>
<p>A vector of predicted values <code class="reqn">\hat{\mathbf{y}}</code>.</p>
</td></tr>
<tr><td><code id="RMSE_+3A_observed">observed</code></td>
<td>
<p>A vector of observed values <code class="reqn">\mathbf{y}</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a the Root Mean Squared error calculated by the formula in the description.
</p>

<hr>
<h2 id='sim_class'>Generate a binary classification data set from normal distribution</h2><span id='topic+sim_class'></span>

<h3>Description</h3>

<p>Simulation used as example of a classification task based on a separation of two
normal multivariate distributions with different vector of means and differerent covariate matrices.
For the label <code class="reqn">A</code> the <code class="reqn">\mathbf{X}_{A}</code> are sampled from a normal distribution <code class="reqn">{MVN}\left(\mu_{A}\mathbf{1}_{p},\sigma_{A}^{2}\mathbf{I}_{p}\right)</code>
while for label <code class="reqn">B</code> the samples <code class="reqn">\mathbf{X}_{B}</code> are from a normal distribution <code class="reqn">{MVN} \left(\mu_{B}\mathbf{1}_{p},\sigma_{B}^{2}\mathbf{I}_{p}\right)</code>. For more details see Ara <em>et. al</em> (2021), and Breiman L (1998).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim_class(
  n,
  p = 2,
  ratio = 0.5,
  mu_a = 0,
  sigma_a = 1,
  mu_b = 1,
  sigma_b = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sim_class_+3A_n">n</code></td>
<td>
<p>Sample size</p>
</td></tr>
<tr><td><code id="sim_class_+3A_p">p</code></td>
<td>
<p>Number of predictors</p>
</td></tr>
<tr><td><code id="sim_class_+3A_ratio">ratio</code></td>
<td>
<p>Ratio between class A and class B</p>
</td></tr>
<tr><td><code id="sim_class_+3A_mu_a">mu_a</code></td>
<td>
<p>Mean of <code class="reqn">X_{1}</code>.</p>
</td></tr>
<tr><td><code id="sim_class_+3A_sigma_a">sigma_a</code></td>
<td>
<p>Standard deviation of <code class="reqn">X_{1}</code>.</p>
</td></tr>
<tr><td><code id="sim_class_+3A_mu_b">mu_b</code></td>
<td>
<p>Mean of <code class="reqn">X_{2}</code></p>
</td></tr>
<tr><td><code id="sim_class_+3A_sigma_b">sigma_b</code></td>
<td>
<p>Standard devation of <code class="reqn">X_{2}</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A simulated data.frame with two predictors for a binary classification problem
</p>


<h3>Author(s)</h3>

<p>Mateus Maia: <a href="mailto:mateusmaia11@gmail.com">mateusmaia11@gmail.com</a>,
Anderson Ara: <a href="mailto:ara@ufpr.br">ara@ufpr.br</a>
</p>


<h3>References</h3>

<p>Ara, Anderson, et al. &quot;Random machines: A bagged-weighted support vector model with free kernel choice.&quot; Journal of Data Science 19.3 (2021): 409-428.
</p>
<p>Breiman, L. (1998). Arcing classifier (with discussion and a rejoinder by the author). The annals of statistics, 26(3), 801-849.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(randomMachines)
sim_data &lt;- sim_class(n = 100)
</code></pre>

<hr>
<h2 id='sim_reg1'>Simulation for a regression toy examples from Random Machines Regression 1</h2><span id='topic+sim_reg1'></span>

<h3>Description</h3>

<p>Simulation toy example initially found in Scornet (2016), and used and escribed by Ara <em>et. al</em> (2022).
Inputs are 2 independent variables uniformly distributed on the interval <code class="reqn">[-1,1]</code>. Outputs are generated following the equation
</p>
<p style="text-align: center;"><code class="reqn">Y={X^{2}_{1}}+e^{{-{X^{2}_{2}}}} +  \varepsilon, \varepsilon \sim \mathcal{N}(0,\sigma^{2})</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>sim_reg1(n, sigma)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sim_reg1_+3A_n">n</code></td>
<td>
<p>Sample size</p>
</td></tr>
<tr><td><code id="sim_reg1_+3A_sigma">sigma</code></td>
<td>
<p>Standard deviation of residual noise</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A simulated data.frame with two predictors and the target variable.
</p>


<h3>Author(s)</h3>

<p>Mateus Maia: <a href="mailto:mateusmaia11@gmail.com">mateusmaia11@gmail.com</a>,
Anderson Ara: <a href="mailto:ara@ufpr.br">ara@ufpr.br</a>
</p>


<h3>References</h3>

<p>Ara, Anderson, et al. &quot;Regression random machines: An ensemble support vector regression model with free kernel choice.&quot; Expert Systems with Applications 202 (2022): 117107.
</p>
<p>Scornet, E. (2016). Random forests and kernel methods. IEEE Transactions on Information Theory, 62(3), 1485-1500.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(randomMachines)
sim_data &lt;- sim_reg1(n=100)
</code></pre>

<hr>
<h2 id='sim_reg2'>Simulation for a regression toy examples from Random Machines Regression 2</h2><span id='topic+sim_reg2'></span>

<h3>Description</h3>

<p>Simulation toy example initially found in Scornet (2016), and used and escribed by Ara <em>et. al</em> (2022).
Inputs are 8 independent variables uniformly distributed on the interval <code class="reqn">[-1,1]</code>. Outputs are generated following the equation
</p>
<p style="text-align: center;"><code class="reqn">Y={X_{1}}{X_{2}}+{X^{2}_{3}}-{X_{4}}{X_{7}}+{X_{5}}{X_{8}}-{X^{2}_{6}}+ \varepsilon, \varepsilon \sim \mathcal{N}(0,\sigma^{2})</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>sim_reg2(n, sigma)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sim_reg2_+3A_n">n</code></td>
<td>
<p>Sample size</p>
</td></tr>
<tr><td><code id="sim_reg2_+3A_sigma">sigma</code></td>
<td>
<p>Standard deviation of residual noise</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A simulated data.frame with two predictors and the target variable.
</p>


<h3>Author(s)</h3>

<p>Mateus Maia: <a href="mailto:mateusmaia11@gmail.com">mateusmaia11@gmail.com</a>,
Anderson Ara: <a href="mailto:ara@ufpr.br">ara@ufpr.br</a>
</p>


<h3>References</h3>

<p>Ara, Anderson, et al. &quot;Regression random machines: An ensemble support vector regression model with free kernel choice.&quot; Expert Systems with Applications 202 (2022): 117107.
</p>
<p>Scornet, E. (2016). Random forests and kernel methods. IEEE Transactions on Information Theory, 62(3), 1485-1500.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(randomMachines)
sim_data &lt;- sim_reg2(n=100)
</code></pre>

<hr>
<h2 id='sim_reg3'>Simulation for a regression toy examples from Random Machines Regression 3</h2><span id='topic+sim_reg3'></span>

<h3>Description</h3>

<p>Simulation toy example initially found in Scornet (2016), and used and escribed by Ara <em>et. al</em> (2022).
Inputs are 4 independent variables uniformly distributed on the interval <code class="reqn">[-1,1]</code>. Outputs are generated following the equation
</p>
<p style="text-align: center;"><code class="reqn">Y= -\sin({X}_{1})+{X}^{2}_{2}+{X}_{3}-e^{{-X^{2}_{4}}} + \varepsilon, \varepsilon \sim \mathcal{N}(0,0.5)</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>sim_reg3(n, sigma)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sim_reg3_+3A_n">n</code></td>
<td>
<p>Sample size</p>
</td></tr>
<tr><td><code id="sim_reg3_+3A_sigma">sigma</code></td>
<td>
<p>Standard deviation of residual noise</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A simulated data.frame with two predictors and the target variable.
</p>


<h3>Author(s)</h3>

<p>Mateus Maia: <a href="mailto:mateusmaia11@gmail.com">mateusmaia11@gmail.com</a>,
Anderson Ara: <a href="mailto:ara@ufpr.br">ara@ufpr.br</a>
</p>


<h3>References</h3>

<p>Ara, Anderson, et al. &quot;Regression random machines: An ensemble support vector regression model with free kernel choice.&quot; Expert Systems with Applications 202 (2022): 117107.
</p>
<p>Scornet, E. (2016). Random forests and kernel methods. IEEE Transactions on Information Theory, 62(3), 1485-1500.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(randomMachines)
sim_data &lt;- sim_reg3(n=100)
</code></pre>

<hr>
<h2 id='sim_reg4'>Simulation for a regression toy examples from Random Machines Regression 3</h2><span id='topic+sim_reg4'></span>

<h3>Description</h3>

<p>Simulation toy example initially found in Van der Laan, <em>et.al</em> (2016), and used and escribed by Ara <em>et. al</em> (2022).
Inputs are 6 independent variables uniformly distributed on the interval <code class="reqn">[-1,1]</code>. Outputs are generated following the equation
</p>
<p style="text-align: center;"><code class="reqn">Y={X^{2}_{1}}+{X}^{2}_{2}{X_{3}}e^{-|{X_{4}}|}+{X_{6}}-{X_{5}}+ \varepsilon, \varepsilon \sim \mathcal{N}(0,\sigma^{2})</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>sim_reg4(n, sigma)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sim_reg4_+3A_n">n</code></td>
<td>
<p>Sample size</p>
</td></tr>
<tr><td><code id="sim_reg4_+3A_sigma">sigma</code></td>
<td>
<p>Standard deviation of residual noise</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A simulated data.frame with two predictors and the target variable.
</p>


<h3>Author(s)</h3>

<p>Mateus Maia: <a href="mailto:mateusmaia11@gmail.com">mateusmaia11@gmail.com</a>,
Anderson Ara: <a href="mailto:ara@ufpr.br">ara@ufpr.br</a>
</p>


<h3>References</h3>

<p>Ara, Anderson, et al. &quot;Regression random machines: An ensemble support vector regression model with free kernel choice.&quot; Expert Systems with Applications 202 (2022): 117107.
</p>
<p>Van der Laan, M. J., Polley, E. C., &amp; Hubbard, A. E. (2007). Super learner. Statistical applications in genetics and molecular biology, 6(1).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(randomMachines)
sim_data &lt;- sim_reg4(n=100)
</code></pre>

<hr>
<h2 id='sim_reg5'>Simulation for a regression toy examples from Random Machines Regression 3</h2><span id='topic+sim_reg5'></span>

<h3>Description</h3>

<p>Simulation toy example initially found in Van der Laan, <em>et.al</em> (2016), and used and escribed by Ara <em>et. al</em> (2022).
Inputs are 6 independent variables sampled from <code class="reqn">N(0,1)</code>. Outputs are generated following the equation
</p>
<p style="text-align: center;"><code class="reqn">Y=X_{1}+0.707 X^{2}_{2} + 2\mathcal{1}_{(X_{3}&gt;0)}+0.873 \log (X_{1})|X_{3}|+0.894 X_{2} X_{4}+2\mathcal{1}_{(X_{5}&gt;0)}+0.464e^{X_{6}}+ \varepsilon, \varepsilon \sim \mathcal{N}(0,\sigma^{2})</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>sim_reg5(n, sigma)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sim_reg5_+3A_n">n</code></td>
<td>
<p>Sample size</p>
</td></tr>
<tr><td><code id="sim_reg5_+3A_sigma">sigma</code></td>
<td>
<p>Standard deviation of residual noise</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A simulated data.frame with two predictors and the target variable.
</p>


<h3>Author(s)</h3>

<p>Mateus Maia: <a href="mailto:mateusmaia11@gmail.com">mateusmaia11@gmail.com</a>,
Anderson Ara: <a href="mailto:ara@ufpr.br">ara@ufpr.br</a>
</p>


<h3>References</h3>

<p>Ara, Anderson, et al. &quot;Regression random machines: An ensemble support vector regression model with free kernel choice.&quot; Expert Systems with Applications 202 (2022): 117107.
</p>
<p>Roy, M. H., &amp; Larocque, D. (2012). Robustness of random forests for regression. Journal of Nonparametric Statistics, 24(4), 993-1006.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(randomMachines)
sim_data &lt;- sim_reg5(n=100)
</code></pre>

<hr>
<h2 id='whosale'>Wholesale Dataset</h2><span id='topic+whosale'></span>

<h3>Description</h3>

<p>The 'whosale' dataset contains information about wholesale customers' annual spending on various product categories.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  data(whosale)
</code></pre>


<h3>Format</h3>

<p>A data frame with 440 rows and 8 columns.
</p>


<h3>Details</h3>

<p>This dataset includes the following columns:
</p>

<dl>
<dt>y</dt><dd><p>Type of customer, either 'Horeca' (Hotel/Restaurant/Cafe), coded as <code>1</code> or 'Retail' coded as <code>2</code>.</p>
</dd>
<dt>Region</dt><dd><p>Geographic region of the customer, either 'Lisbon', 'Oporto', or 'Other'. Coded as <code>{1,2,3}</code>, respectively.</p>
</dd>
<dt>Fresh</dt><dd><p>Annual spending (in monetary units) on fresh products.</p>
</dd>
<dt>Milk</dt><dd><p>Annual spending on milk products.</p>
</dd>
<dt>Grocery</dt><dd><p>Annual spending on grocery products.</p>
</dd>
<dt>Frozen</dt><dd><p>Annual spending on frozen products.</p>
</dd>
<dt>Detergents Paper</dt><dd><p>Annual spending on detergents and paper products.</p>
</dd>
<dt>Delicassen</dt><dd><p>Annual spending on delicatessen products.</p>
</dd>
</dl>



<h3>Source</h3>

<p>The 'whosale' dataset is sourced from the UCI Machine Learning Repository:
<a href="https://archive.ics.uci.edu/ml/datasets/wholesale+customers">https://archive.ics.uci.edu/ml/datasets/wholesale+customers</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    data(whosale)
    head(whosale)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
