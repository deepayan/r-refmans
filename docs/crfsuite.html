<!DOCTYPE html><html><head><title>Help for package crfsuite</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {crfsuite}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#airbnb'><p>Dutch reviews of AirBnB customers on Brussels address locations available at www.insideairbnb.com</p></a></li>
<li><a href='#airbnb_chunks'><p>Dutch reviews of AirBnB customers on Brussels address locations manually tagged with entities</p></a></li>
<li><a href='#as.crf'><p>Convert a model built with CRFsuite to an object of class crf</p></a></li>
<li><a href='#crf'><p>Linear-chain Conditional Random Field</p></a></li>
<li><a href='#crf_caretmethod'><p>Functionality allowing to tune a crfsuite model using caret</p></a></li>
<li><a href='#crf_cbind_attributes'><p>Enrich a data.frame by adding frequently used CRF attributes</p></a></li>
<li><a href='#crf_evaluation'><p>Basic classification evaluation metrics for multi-class labelling</p></a></li>
<li><a href='#crf_options'><p>Conditional Random Fields parameters</p></a></li>
<li><a href='#merge.chunkrange'><p>CRF Training data construction: add chunk entity category to a tokenised dataset</p></a></li>
<li><a href='#ner_download_modeldata'><p>CRF Training data: download training data for doing Named Entity Recognition (NER)</p></a></li>
<li><a href='#predict.crf'><p>Predict the label sequence based on the Conditional Random Field</p></a></li>
<li><a href='#txt_feature'><p>Extract basic text features which are useful for entity recognition</p></a></li>
<li><a href='#txt_sprintf'><p><code>NA</code> friendly version of sprintf</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Conditional Random Fields for Labelling Sequential Data in
Natural Language Processing</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.2</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jan Wijffels &lt;jwijffels@bnosac.be&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Wraps the 'CRFsuite' library <a href="https://github.com/chokkan/crfsuite">https://github.com/chokkan/crfsuite</a> allowing users 
    to fit a Conditional Random Field model and to apply it on existing data.
    The focus of the implementation is in the area of Natural Language Processing where this R package allows you to easily build and apply models 
    for named entity recognition, text chunking, part of speech tagging, intent recognition or classification of any category you have in mind. Next to training, a small web application
    is included in the package to allow you to easily construct training data.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/BSD-3-Clause">BSD_3_clause</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/bnosac/crfsuite">https://github.com/bnosac/crfsuite</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, data.table (&ge; 1.9.6), utils, tools, stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>udpipe, knitr, rmarkdown</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-09-15 17:17:12 UTC; jwijf</td>
</tr>
<tr>
<td>Author:</td>
<td>Jan Wijffels [aut, cre, cph] (R wrapper),
  BNOSAC [cph] (R wrapper),
  Naoaki Okazaki [aut, ctb, cph] (CRFsuite library (BSD licensed),
    libLBFGS library (MIT licensed), Constant Quark Database software
    (BSD licensed)),
  Bob Jenkins [aut, ctb] (File src/cqdb/src/lookup3.c (Public Domain)),
  Jorge Nocedal [aut, ctb, cph] (libLBFGS library (MIT licensed)),
  Jesse Long [aut, ctb, cph] (RumAVL library (MIT licensed))</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-09-17 00:00:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='airbnb'>Dutch reviews of AirBnB customers on Brussels address locations available at www.insideairbnb.com</h2><span id='topic+airbnb'></span>

<h3>Description</h3>

<p>The data contains 500 reviews in Dutch of people who visited an AirBnB appartment in Brussels. <br />
The data frame contains the fields 
</p>

<ul>
<li><p>doc_id: a unique identifier of the review
</p>
</li>
<li><p>listing_id: the airbnb address identifier
</p>
</li>
<li><p>text: text with the feedback of a customer on his visit in the AirBnB appartment
</p>
</li></ul>



<h3>Source</h3>

<p><a href="http://insideairbnb.com/brussels">http://insideairbnb.com/brussels</a>: information of 2015-10-03
</p>


<h3>See Also</h3>

<p><code><a href="#topic+airbnb_chunks">airbnb_chunks</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(airbnb)
str(airbnb)
head(airbnb)
</code></pre>

<hr>
<h2 id='airbnb_chunks'>Dutch reviews of AirBnB customers on Brussels address locations manually tagged with entities</h2><span id='topic+airbnb_chunks'></span>

<h3>Description</h3>

<p>The <code><a href="#topic+airbnb">airbnb</a></code> dataset was manually annotated with the shiny app inside this R package.
The annotation shows chunks of data which have been flagged with the following categories: PERSON, LOCATION, DISTANCE.
The dataset is an object of class <code>chunkrange</code> and of type data.frame which contains the following fields:
</p>

<ul>
<li><p>doc_id: a unique identifier of the review, which is also available in <code><a href="#topic+airbnb">airbnb</a></code>
</p>
</li>
<li><p>listing_id: the airbnb address identifier
</p>
</li>
<li><p>text: text with the feedback of a customer on his visit in the AirBnB appartment
</p>
</li>
<li><p>chunk_id: a chunk identifier
</p>
</li>
<li><p>chunk_entity: a chunk entity label
</p>
</li>
<li><p>chunk: the text of the chunk which is a substring of <code>text</code>
</p>
</li>
<li><p>start: the starting position in <code>text</code> where the <code>chunk</code> is found
</p>
</li>
<li><p>end: the end position in <code>text</code> where the <code>chunk</code> is found
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+airbnb_chunks">airbnb_chunks</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(airbnb_chunks)
str(airbnb_chunks)
head(airbnb_chunks)
</code></pre>

<hr>
<h2 id='as.crf'>Convert a model built with CRFsuite to an object of class crf</h2><span id='topic+as.crf'></span>

<h3>Description</h3>

<p>If you have a model built with CRFsuite either by this R package
or by another software library which wraps CRFsuite (e.g. Python/Java), you can 
convert it to an object of class <code>crf</code> which this package can use 
to inspect the model and to use it for prediction (if you can mimic the way the attributes are created).<br />
This is for expert use only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.crf(file, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.crf_+3A_file">file</code></td>
<td>
<p>the path to a file on disk containing the CRFsuite model</p>
</td></tr>
<tr><td><code id="as.crf_+3A_...">...</code></td>
<td>
<p>other arguments which can be set except the path to the file, namely method, type, options, attribute_names, log (expert use only)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class <code>crf</code>
</p>

<hr>
<h2 id='crf'>Linear-chain Conditional Random Field</h2><span id='topic+crf'></span>

<h3>Description</h3>

<p>Fits a Linear-chain (first-order Markov) CRF on the provided label sequence and saves it on disk in order to do sequence labelling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crf(
  x,
  y,
  group,
  method = c("lbfgs", "l2sgd", "averaged-perceptron", "passive-aggressive", "arow"),
  options = crf_options(method)$default,
  embeddings,
  file = "annotator.crfsuite",
  trace = FALSE,
  FUN = identity,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crf_+3A_x">x</code></td>
<td>
<p>a character matrix of data containing attributes about the label sequence <code>y</code> or an object which can be coerced to a character matrix.
It is important to note that an attribute which has the same value in a different column is considered the same.</p>
</td></tr>
<tr><td><code id="crf_+3A_y">y</code></td>
<td>
<p>a character vector with the sequence of labels to model</p>
</td></tr>
<tr><td><code id="crf_+3A_group">group</code></td>
<td>
<p>an integer or character vector of the same length as <code>y</code> indicating the group the sequence <code>y</code> belongs to (e.g. a document or sentence identifier)</p>
</td></tr>
<tr><td><code id="crf_+3A_method">method</code></td>
<td>
<p>character string with the type of training method. Either one of:
</p>

<ul>
<li><p>lbfgs: L-BFGS with L1/L2 regularization
</p>
</li>
<li><p>l2sgd: SGD with L2-regularization
</p>
</li>
<li><p>averaged-perceptron: Averaged Perceptron
</p>
</li>
<li><p>passive-aggressive: Passive Aggressive
</p>
</li>
<li><p>arow: Adaptive Regularization of Weights (AROW)
</p>
</li></ul>
</td></tr>
<tr><td><code id="crf_+3A_options">options</code></td>
<td>
<p>a list of options to provide to the training algorithm. See <code><a href="#topic+crf_options">crf_options</a></code> for possible options and the example below on how to provide them.</p>
</td></tr>
<tr><td><code id="crf_+3A_embeddings">embeddings</code></td>
<td>
<p>a matrix with the same number of rows as <code>x</code> and in the same order with numeric information used in model building (experimental)</p>
</td></tr>
<tr><td><code id="crf_+3A_file">file</code></td>
<td>
<p>a character string with the path to the file on disk where the CRF model will be stored.</p>
</td></tr>
<tr><td><code id="crf_+3A_trace">trace</code></td>
<td>
<p>a logical indicating to show the trace of the training output. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="crf_+3A_fun">FUN</code></td>
<td>
<p>a function which can be applied on raw text in order to obtain the attribute matrix used in <code>predict.crf</code>. Currently not used yet.</p>
</td></tr>
<tr><td><code id="crf_+3A_...">...</code></td>
<td>
<p>arguments to FUN. Currently not used yet.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class crf which is a list with elements
</p>

<ul>
<li><p>method: The training method
</p>
</li>
<li><p>type: The type of graphical model which is always set crf1d: Linear-chain (first-order Markov) CRF
</p>
</li>
<li><p>labels: The training labels
</p>
</li>
<li><p>options: A data.frame with the training options provided to the algorithm
</p>
</li>
<li><p>file_model: The path where the CRF model is stored
</p>
</li>
<li><p>attribute_names: The column names of <code>x</code>
</p>
</li>
<li><p>log: The training log of the algorithm
</p>
</li>
<li><p>FUN: The argument passed on to FUN
</p>
</li>
<li><p>ldots: A list with the arguments passed on to ...
</p>
</li></ul>



<h3>References</h3>

<p>More details about this model is available at <a href="http://www.chokkan.org/software/crfsuite/">http://www.chokkan.org/software/crfsuite/</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.crf">predict.crf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Download modeldata (conll 2002 shared task in Dutch)

x         &lt;- ner_download_modeldata("conll2002-nl")

# for CRAN only - word on a subset of the data
x &lt;- ner_download_modeldata("conll2002-nl", docs = 10)
if(is.data.frame(x)){
  ##
  ## Build Named Entity Recognition model on conll2002-nl
  ##
  x$pos     &lt;- txt_sprintf("Parts of Speech: %s", x$pos)
  x$token   &lt;- txt_sprintf("Token: %s", x$token)
  crf_train &lt;- subset(x, data == "ned.train")
  crf_test  &lt;- subset(x, data == "testa")

  model &lt;- crf(y = crf_train$label, 
               x = crf_train[, c("token", "pos")], 
               group = crf_train$doc_id, 
               method = "lbfgs", 
               options = list(max_iterations = 3, feature.minfreq = 5, 
                              c1 = 0, c2 = 1)) 
  model
  weights &lt;- coefficients(model)
  head(weights$states, n = 20)
  head(weights$transitions, n = 20)
  stats   &lt;- summary(model, "modeldetails.txt")
  stats
  plot(stats$iterations$loss)

  ## Use the CRF model to label a sequence
  scores &lt;- predict(model, 
                    newdata = crf_test[, c("token", "pos")], 
                    group = crf_test$doc_id)
  head(scores)
  crf_test$label &lt;- scores$label
  
  ## cleanup for CRAN
  if(file.exists(model$file_model)) file.remove(model$file_model)
  if(file.exists("modeldetails.txt")) file.remove("modeldetails.txt")
}

##
## More detailed example where text data was annotated with the webapp in the package
## This data is joined with a tokenised dataset to construct the training data which
## is further enriched with attributes of upos/lemma in the neighbourhood
##

library(udpipe)
data(airbnb_chunks, package = "crfsuite")
udmodel       &lt;- udpipe_download_model("dutch-lassysmall")
if(!udmodel$download_failed){
udmodel       &lt;- udpipe_load_model(udmodel$file_model)
airbnb_tokens &lt;- udpipe(x = unique(airbnb_chunks[, c("doc_id", "text")]), 
                        object = udmodel)
x &lt;- merge(airbnb_chunks, airbnb_tokens)
x &lt;- crf_cbind_attributes(x, terms = c("upos", "lemma"), by = "doc_id")
model &lt;- crf(y = x$chunk_entity, 
             x = x[, grep("upos|lemma", colnames(x), value = TRUE)], 
             group = x$doc_id, 
             method = "lbfgs", options = list(max_iterations = 5)) 
stats &lt;- summary(model)
stats
plot(stats$iterations$loss, type = "b", xlab = "Iteration", ylab = "Loss")
scores &lt;- predict(model, 
                  newdata = x[, grep("upos|lemma", colnames(x))], 
                  group = x$doc_id)
head(scores)
}


</code></pre>

<hr>
<h2 id='crf_caretmethod'>Functionality allowing to tune a crfsuite model using caret</h2><span id='topic+crf_caretmethod'></span>

<h3>Description</h3>

<p>The object <code>crf_caretmethod</code> contains functionality to tune a crf model using caret.
Each list elment of <code>crf_caretmethod</code> is a list of functions 
which can be passed on to the <code>method</code> argument of <code>caret::train</code> to tune the hyperparameters of the crfsuite model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crf_caretmethod
</code></pre>


<h3>Format</h3>

<p>see details
</p>


<h3>Details</h3>

<p>If you want to tune the hyperparameters of a crfsuite model 
(see <code><a href="#topic+crf_options">crf_options</a></code> and the <code>options</code> argument of <code><a href="#topic+crf">crf</a></code>), you can use the <code>caret</code> package. <br />
In order to facilitate this tuning, an object called <code>crf_caretmethod</code> has been made available. 
The object <code>crf_caretmethod</code> is a list with 6 elements, where each of these 6 elements can be used in 
tuning the CRF hyperparemeters by passing it on to the <code>method</code> argument of the <code>train</code> function of the <code>caret</code> package.<br />
The list has elements 'default', 'lbfgs', 'l2sgd', 'averaged_perceptron', 'passive_aggressive' and 'arow'.
Each list element corresponds to arguments that you need to tune for each <code>method</code> as used in <code><a href="#topic+crf">crf</a></code>. <br />
For <code>crf_caretmethod</code>
</p>

<ol>
<li><p> lbfgs: Tuning across all hyperparameters for method lbfgs: L-BFGS with L1/L2 regularization
</p>
</li>
<li><p> l2sgd: Tuning across all hyperparameters for method l2sgd: SGD with L2-regularization
</p>
</li>
<li><p> averaged_perceptron: Tuning across all hyperparameters for method averaged-perceptron: Averaged Perceptron
</p>
</li>
<li><p> passive_aggressive: Tuning across all hyperparameters for method passive-aggressive: Passive Aggressive
</p>
</li>
<li><p> arow: Tuning across all hyperparameters for method arow: Adaptive Regularization of Weights (AROW)
</p>
</li>
<li><p> default: Tune over the hyperparameters feature.minfreq, feature.possible_states, feature.possible_transitions, max_iterations. While tuning these, it uses the default hyperparameters for each method. This tuning allows you to compare the 5 methods.
</p>
</li></ol>

<p>For details on the hyperparameter definitions: see <code><a href="#topic+crf_options">crf_options</a></code>
</p>

<hr>
<h2 id='crf_cbind_attributes'>Enrich a data.frame by adding frequently used CRF attributes</h2><span id='topic+crf_cbind_attributes'></span>

<h3>Description</h3>

<p>The CRF attributes which are implemented in this function 
are merely the neighbouring information of a certain field.
For example the previous word, the next word, the combination of the previous 2 words.
This function <code>cbind</code>s these neighbouring attributes as columns to the provided data.frame.<br />
</p>
<p>By default it adds the following columns to the data.frame 
</p>

<ul>
<li><p>the term itself <code>(term[t])</code>
</p>
</li>
<li><p>the next term <code>(term[t+1])</code>
</p>
</li>
<li><p>the term after that <code>(term[t+2])</code>
</p>
</li>
<li><p>the previous term <code>(term[t-1])</code>
</p>
</li>
<li><p>the term before the previous term <code>(term[t-2])</code>
</p>
</li>
<li><p>as well as all combinations of these terms (bigrams/trigrams/...) where up to <code>ngram_max</code>
number of terms are combined.
</p>
</li></ul>

<p>See the examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crf_cbind_attributes(
  data,
  terms,
  by,
  from = -2,
  to = 2,
  ngram_max = 3,
  sep = "-"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crf_cbind_attributes_+3A_data">data</code></td>
<td>
<p>a data.frame which will be coerced to a data.table (cbinding will be done by reference on the existing data.frame)</p>
</td></tr>
<tr><td><code id="crf_cbind_attributes_+3A_terms">terms</code></td>
<td>
<p>a character vector of column names which are part of <code>data</code> 
for which the function will look to the preceding and following rows in order to cbind this information to the <code>data</code></p>
</td></tr>
<tr><td><code id="crf_cbind_attributes_+3A_by">by</code></td>
<td>
<p>a character vector of column names which are part of <code>data</code> indicating the fields which define the sequence. 
Preceding/following terms will be looked for within data of <code>by</code>. 
Typically this will be a document identifier or sentence identifier in an NLP context.</p>
</td></tr>
<tr><td><code id="crf_cbind_attributes_+3A_from">from</code></td>
<td>
<p>integer, by default set to -2, indicating to look up to 2 terms before the current term</p>
</td></tr>
<tr><td><code id="crf_cbind_attributes_+3A_to">to</code></td>
<td>
<p>integer, by default  set to 2, indicating to look up to 2 terms after the current term</p>
</td></tr>
<tr><td><code id="crf_cbind_attributes_+3A_ngram_max">ngram_max</code></td>
<td>
<p>integer indicating the maximum number of terms to combine (2 means bigrams, 3 trigrams, ...)</p>
</td></tr>
<tr><td><code id="crf_cbind_attributes_+3A_sep">sep</code></td>
<td>
<p>character indicating how to combine the previous/next/current terms. Defaults to '-'.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- data.frame(doc_id = sort(sample.int(n = 10, size = 1000, replace = TRUE)))
x$pos &lt;- sample(c("Art", "N", "Prep", "V", "Adv", "Adj", "Conj", 
                  "Punc", "Num", "Pron", "Int", "Misc"), 
                  size = nrow(x), replace = TRUE)
x &lt;- crf_cbind_attributes(x, terms = "pos", by = "doc_id", 
                          from = -1, to = 1, ngram_max = 3)
head(x)


## Example on some real data
x &lt;- ner_download_modeldata("conll2002-nl")
x &lt;- crf_cbind_attributes(x, terms = c("token", "pos"), 
                          by = c("doc_id", "sentence_id"),
                          ngram_max = 3, sep = "|")

</code></pre>

<hr>
<h2 id='crf_evaluation'>Basic classification evaluation metrics for multi-class labelling</h2><span id='topic+crf_evaluation'></span>

<h3>Description</h3>

<p>The accuracy, precision, recall, specificity, F1 measure and support metrics are provided for each label in a one-versus the rest setting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crf_evaluation(
  pred,
  obs,
  labels = na.exclude(unique(c(as.character(pred), as.character(obs)))),
  labels_overall = setdiff(labels, "O")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crf_evaluation_+3A_pred">pred</code></td>
<td>
<p>a factor with predictions</p>
</td></tr>
<tr><td><code id="crf_evaluation_+3A_obs">obs</code></td>
<td>
<p>a factor with gold labels</p>
</td></tr>
<tr><td><code id="crf_evaluation_+3A_labels">labels</code></td>
<td>
<p>a character vector of possible values that <code>pred</code> and <code>obs</code> can take. Defaults to the values in the data</p>
</td></tr>
<tr><td><code id="crf_evaluation_+3A_labels_overall">labels_overall</code></td>
<td>
<p>a character vector of either labels which is either the same as <code>labels</code> or a subset of <code>labels</code> in order to compute a weighted average of the by-label statistics</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with 2 elements:
</p>

<ul>
<li><p>bylabel: data.frame with the accuracy, precision, recall, specificity, F1 score and support (number of occurrences) for each label
</p>
</li>
<li><p>overall: a vector containing 
</p>

<ul>
<li><p>the overall accuracy
</p>
</li>
<li><p>the metrics precision, recall, specificity and F1 score which are weighted averages of these metrics from list element <code>bylabel</code>, where the weight is the support
</p>
</li>
<li><p>the metrics precision, recall, specificity and F1 score which are averages of these metrics from list element <code>bylabel</code> giving equal weight to each label
</p>
</li></ul>


</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>pred &lt;- sample(LETTERS, 1000, replace = TRUE)
gold &lt;- sample(LETTERS, 1000, replace = TRUE)
crf_evaluation(pred = pred, obs = gold, labels = LETTERS) 


x &lt;- ner_download_modeldata("conll2002-nl")
x &lt;- crf_cbind_attributes(x, terms = c("token", "pos"), 
                          by = c("doc_id", "sentence_id"))
crf_train &lt;- subset(x, data == "ned.train")
crf_test &lt;- subset(x, data == "testa")
attributes &lt;- grep("token|pos", colnames(x), value=TRUE)
model &lt;- crf(y = crf_train$label, 
             x = crf_train[, attributes], 
             group = crf_train$doc_id, 
             method = "lbfgs") 
             
## Use the model to score on existing tokenised data
scores &lt;- predict(model, 
                  newdata = crf_test[, attributes], 
                  group = crf_test$doc_id)
crf_evaluation(pred = scores$label, obs = crf_test$label)
crf_evaluation(pred = scores$label, obs = crf_test$label, 
  labels = c("O", 
             "B-ORG", "I-ORG", "B-PER", "I-PER", 
             "B-LOC", "I-LOC", "B-MISC", "I-MISC"))
             
         
library(udpipe)
pred &lt;- txt_recode(scores$label, 
                   from = c("B-ORG", "I-ORG", "B-PER", "I-PER", 
                            "B-LOC", "I-LOC", "B-MISC", "I-MISC"),
                   to = c("ORG", "ORG", "PER", "PER", 
                          "LOC", "LOC", "MISC", "MISC"))
obs &lt;- txt_recode(crf_test$label, 
                  from = c("B-ORG", "I-ORG", "B-PER", "I-PER", 
                           "B-LOC", "I-LOC", "B-MISC", "I-MISC"),
                  to = c("ORG", "ORG", "PER", "PER", 
                         "LOC", "LOC", "MISC", "MISC"))
crf_evaluation(pred = pred, obs = obs, 
               labels = c("ORG", "LOC", "PER", "MISC", "O"))


</code></pre>

<hr>
<h2 id='crf_options'>Conditional Random Fields parameters</h2><span id='topic+crf_options'></span>

<h3>Description</h3>

<p>Conditional Random Fields parameters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crf_options(
  method = c("lbfgs", "l2sgd", "averaged-perceptron", "passive-aggressive", "arow")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crf_options_+3A_method">method</code></td>
<td>
<p>character string with the type of training method. Either one of:
</p>

<ul>
<li><p>lbfgs: L-BFGS with L1/L2 regularization
</p>
</li>
<li><p>l2sgd: SGD with L2-regularization
</p>
</li>
<li><p>averaged-perceptron: Averaged Perceptron
</p>
</li>
<li><p>passive-aggressive: Passive Aggressive
</p>
</li>
<li><p>arow: Adaptive Regularization of Weights (AROW)
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with elements 
</p>

<ul>
<li><p>method: The training method
</p>
</li>
<li><p>type: The type of graphical model which is always set crf1d: Linear-chain (first-order Markov) CRF
</p>
</li>
<li><p>params: A data.frame with fields arg, arg_default and description indicating the possible hyperparameters of the algorithm, the default values and the description
</p>
</li>
<li><p>default: A list of default values which can be used to pass on to the <code>options</code> argument of <code><a href="#topic+crf">crf</a></code>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># L-BFGS with L1/L2 regularization
opts &lt;- crf_options("lbfgs")
str(opts)

# SGD with L2-regularization
crf_options("l2sgd")

# Averaged Perceptron
crf_options("averaged-perceptron")

# Passive Aggressive
crf_options("passive-aggressive")

# Adaptive Regularization of Weights (AROW)
crf_options("arow")
</code></pre>

<hr>
<h2 id='merge.chunkrange'>CRF Training data construction: add chunk entity category to a tokenised dataset</h2><span id='topic+merge.chunkrange'></span>

<h3>Description</h3>

<p>Chunks annotated with the shiny app in this R package indicate for a chunk of text of a document
the entity that it belongs to. As text chunks can contains several words, we need to have a way in
order to add this chunk category to each word of a tokenised dataset. That's what this function is doing.<br />
If you have a tokenised data.frame with one row per token/document which indicates the start and end position
where the token is found in the text of the document, this function allows to assign the chunk label to each token 
of the document.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'chunkrange'
merge(x, y, by.x = "doc_id", by.y = "doc_id", default_entity = "O", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="merge.chunkrange_+3A_x">x</code></td>
<td>
<p>an object of class <code>chunkrange</code>. A <code>chunkrange</code> is just a data.frame which contains 
one row per chunk/doc_id. It should have the columns doc_id, text, chunk_id, chunk_entity, start and end.<br />
The fields <code>start</code> and <code>end</code> indicate in the original <code>text</code> where the chunks of words starts and where it ends. 
The <code>chunk_entity</code> is a label you have assigned to the chunk (e.g. ORGANISATION / LOCATION / MONEY / LABELXYZ / ...).</p>
</td></tr>
<tr><td><code id="merge.chunkrange_+3A_y">y</code></td>
<td>
<p>a tokenised data.frame containing one row per doc_id/token It should have the columns <code>doc_id</code>, <code>start</code> and <code>end</code> where
the fields <code>start</code> and <code>end</code> indicate the positions in the original text of the <code>doc_id</code> where the token starts and where it ends. 
See the examples.</p>
</td></tr>
<tr><td><code id="merge.chunkrange_+3A_by.x">by.x</code></td>
<td>
<p>a character string of a column of <code>x</code> which is an identifier which defines the sequence. Defaults to 'doc_id'.</p>
</td></tr>
<tr><td><code id="merge.chunkrange_+3A_by.y">by.y</code></td>
<td>
<p>a character string of a column of <code>y</code> which is an identifier which defines the sequence. Defaults to 'doc_id'.</p>
</td></tr>
<tr><td><code id="merge.chunkrange_+3A_default_entity">default_entity</code></td>
<td>
<p>character string with the default <code>chunk_entity</code> to be assigned to the token if the token is not part of any chunk range.
Defaults to 'O'.</p>
</td></tr>
<tr><td><code id="merge.chunkrange_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the data.frame <code>y</code> where 2 columns are added, namely:
</p>

<ul>
<li><p>chunk_entity: The chunk entity of the token if the token is inside the chunk defined in <code>x</code>. If the token is not part of any chunk, the chunk category will be set to the <code>default</code> value.
</p>
</li>
<li><p>chunk_id: The chunk identifier of the chunk for which the token is inside the chunk.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>

library(udpipe)
udmodel &lt;- udpipe_download_model("dutch-lassysmall")
if(packageVersion("udpipe") &gt;= "0.7"){
  data(airbnb_chunks, package = "crfsuite")
  airbnb_chunks &lt;- head(airbnb_chunks, 20)
  airbnb_tokens &lt;- unique(airbnb_chunks[, c("doc_id", "text")])

  airbnb_tokens &lt;- udpipe(airbnb_tokens, object = udmodel)
  head(airbnb_tokens)
  head(airbnb_chunks)

  ## Add the entity of the chunk to the tokenised dataset
  x &lt;- merge(airbnb_chunks, airbnb_tokens)
  x[, c("doc_id", "token", "chunk_entity")]
  table(x$chunk_entity)
}

## cleanup for CRAN
file.remove(udmodel$file_model)


</code></pre>

<hr>
<h2 id='ner_download_modeldata'>CRF Training data: download training data for doing Named Entity Recognition (NER)</h2><span id='topic+ner_download_modeldata'></span>

<h3>Description</h3>

<p>Download training data for doing Named Entity Recognition (NER)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ner_download_modeldata(
  type = c("conll2002-nl", "conll2002-es", "GermanNER", "wikiner-de-wp2",
    "wikiner-de-wp3", "wikiner-en-wp2", "wikiner-en-wp3", "wikiner-es-wp2",
    "wikiner-es-wp3", "wikiner-fr-wp2", "wikiner-fr-wp3", "wikiner-it-wp2",
    "wikiner-it-wp3", "wikiner-nl-wp2", "wikiner-nl-wp3", "wikiner-pl-wp3",
    "wikiner-pt-wp3", "wikiner-ru-wp2", "wikiner-ru-wp3"),
  docs = -Inf
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ner_download_modeldata_+3A_type">type</code></td>
<td>
<p>a character string with the type of data to download. See the function usage for all possible values.
These data will be downloaded from either:
</p>

<ul>
<li><p>NLTK-data forked repository: <a href="https://github.com/bnosac-dev/nltk_data/blob/gh-pages/packages/corpora/conll2002.zip">https://github.com/bnosac-dev/nltk_data/blob/gh-pages/packages/corpora/conll2002.zip</a>
</p>
</li>
<li><p>FOX forked repository of GermanNER: <a href="https://github.com/bnosac-dev/FOX/tree/master/input/GermanNER">https://github.com/bnosac-dev/FOX/tree/master/input/GermanNER</a>
</p>
</li>
<li><p>FOX forked repository of WikiNER: <a href="https://github.com/bnosac-dev/FOX/tree/master/input/Wikiner">https://github.com/bnosac-dev/FOX/tree/master/input/Wikiner</a>
</p>
</li></ul>

<p>Please visit the information on these repositories first before you use these data in any commercial product.</p>
</td></tr>
<tr><td><code id="ner_download_modeldata_+3A_docs">docs</code></td>
<td>
<p>integer indicating how many documents to sample from the data (only used for data from the NLTK repository). 
This is only used to reduce CRAN R CMD check training time in the examples of this R package.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with training data for a Named Entity Recognition task or an object of try-error in case of failure of downloading the data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
x &lt;- ner_download_modeldata("conll2002-nl")
x &lt;- ner_download_modeldata("conll2002-es")
x &lt;- ner_download_modeldata("GermanNER")
x &lt;- ner_download_modeldata("wikiner-en-wp2")
x &lt;- ner_download_modeldata("wikiner-nl-wp3")
x &lt;- ner_download_modeldata("wikiner-fr-wp3")

## End(Not run)
## reduce number of docs
x &lt;- ner_download_modeldata("conll2002-es", docs = 10)
</code></pre>

<hr>
<h2 id='predict.crf'>Predict the label sequence based on the Conditional Random Field</h2><span id='topic+predict.crf'></span>

<h3>Description</h3>

<p>Predict the label sequence based on the Conditional Random Field
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'crf'
predict(
  object,
  newdata,
  embeddings,
  group,
  type = c("marginal", "sequence"),
  trace = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.crf_+3A_object">object</code></td>
<td>
<p>an object of class crf as returned by <code><a href="#topic+crf">crf</a></code></p>
</td></tr>
<tr><td><code id="predict.crf_+3A_newdata">newdata</code></td>
<td>
<p>a character matrix of data containing attributes about the label sequence <code>y</code> or an object which can be coerced to a character matrix. 
This data should be provided in the same format as was used for training the model</p>
</td></tr>
<tr><td><code id="predict.crf_+3A_embeddings">embeddings</code></td>
<td>
<p>a matrix with the same number of rows as <code>x</code> and in the same order with numeric information used to predict</p>
</td></tr>
<tr><td><code id="predict.crf_+3A_group">group</code></td>
<td>
<p>an integer or character vector of the same length as nrow <code>newdata</code> indicating the group the sequence <code>y</code> belongs to (e.g. a document or sentence identifier)</p>
</td></tr>
<tr><td><code id="predict.crf_+3A_type">type</code></td>
<td>
<p>either 'marginal' or 'sequence' to get predictions at the level of <code>newdata</code> or a the level of the sequence <code>group</code>. Defaults to <code>'marginal'</code></p>
</td></tr>
<tr><td><code id="predict.crf_+3A_trace">trace</code></td>
<td>
<p>a logical indicating to show the trace of the labelling output. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="predict.crf_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>type</code> is 'marginal': a data.frame with columns label and marginal containing the viterbi decoded predicted label and marginal probability. <br />
If <code>type</code> is 'sequence': a data.frame with columns group and probability containing for each sequence group the probability of the sequence.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+crf">crf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

library(udpipe)
data(airbnb_chunks, package = "crfsuite")
udmodel &lt;- udpipe_download_model("dutch-lassysmall")
udmodel &lt;- udpipe_load_model(udmodel$file_model)
airbnb_tokens &lt;- unique(airbnb_chunks[, c("doc_id", "text")])
airbnb_tokens &lt;- udpipe_annotate(udmodel, 
                                 x = airbnb_tokens$text, 
                                 doc_id = airbnb_tokens$doc_id)
airbnb_tokens &lt;- as.data.frame(airbnb_tokens)
x &lt;- merge(airbnb_chunks, airbnb_tokens)
x &lt;- crf_cbind_attributes(x, terms = c("upos", "lemma"), by = "doc_id")
model &lt;- crf(y = x$chunk_entity, 
             x = x[, grep("upos|lemma", colnames(x))], 
             group = x$doc_id, 
             method = "lbfgs", options = list(max_iterations = 5)) 
scores &lt;- predict(model, 
                  newdata = x[, grep("upos|lemma", colnames(x))], 
                  group = x$doc_id, type = "marginal")
head(scores)
scores &lt;- predict(model, 
                  newdata = x[, grep("upos|lemma", colnames(x))], 
                  group = x$doc_id, type = "sequence")
head(scores)


## cleanup for CRAN
file.remove(model$file_model)
file.remove("modeldetails.txt")
file.remove(udmodel$file)


</code></pre>

<hr>
<h2 id='txt_feature'>Extract basic text features which are useful for entity recognition</h2><span id='topic+txt_feature'></span>

<h3>Description</h3>

<p>Extract basic text features which are useful for entity recognition
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_feature(
  x,
  type = c("is_capitalised", "is_url", "is_email", "is_number", "prefix", "suffix",
    "shape"),
  n = 4
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="txt_feature_+3A_x">x</code></td>
<td>
<p>a character vector</p>
</td></tr>
<tr><td><code id="txt_feature_+3A_type">type</code></td>
<td>
<p>a character string, which can be one of 'is_capitalised', 'is_url', 'is_email', 'is_number', 'prefix', 'suffix', 'shape'</p>
</td></tr>
<tr><td><code id="txt_feature_+3A_n">n</code></td>
<td>
<p>for type 'prefix' or 'suffix', the number of characters of the prefix/suffix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For type 'is_capitalised', 'is_url', 'is_email', 'is_number': a logical vector of the same length as <code>x</code>, indicating if <code>x</code> is capitalised, a url, an email or a number<br />
For type 'prefix', 'suffix': a character vector of the same length as <code>x</code>, containing the prefix or suffix <code>n</code> number of characters of <code>x</code><br />
For type 'shape': a character vector of the same length as <code>x</code>, where lowercased elements are replaced with x and uppercased elements with X
</p>


<h3>Examples</h3>

<pre><code class='language-R'>txt_feature("Red Devils", type = "is_capitalised")
txt_feature("red devils", type = "is_capitalised")
txt_feature("http://www.bnosac.be", type = "is_url")
txt_feature("info@google.com", type = "is_email")
txt_feature("hi there", type = "is_email")
txt_feature("1230000", type = "is_number")
txt_feature("123.15", type = "is_number")
txt_feature("123,15", type = "is_number")
txt_feature("123abc", type = "is_number")
txt_feature("abcdefghijklmnopqrstuvwxyz", type = "prefix", n = 3)
txt_feature("abcdefghijklmnopqrstuvwxyz", type = "suffix", n = 3)
txt_feature("Red Devils", type = "shape")
txt_feature("red devils", type = "shape")
</code></pre>

<hr>
<h2 id='txt_sprintf'><code>NA</code> friendly version of sprintf</h2><span id='topic+txt_sprintf'></span>

<h3>Description</h3>

<p>Does the same as the function <code><a href="base.html#topic+sprintf">sprintf</a></code> except that if 
in ... <code>NA</code> values are passed, also <code>NA</code> values are returned instead of being replaced by the character string <code>'NA'</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_sprintf(fmt, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="txt_sprintf_+3A_fmt">fmt</code></td>
<td>
<p>a character vector of format strings, which will be fed on to <code><a href="base.html#topic+sprintf">sprintf</a></code></p>
</td></tr>
<tr><td><code id="txt_sprintf_+3A_...">...</code></td>
<td>
<p>values to be passed into <code>fmt</code>, the <code>...</code> will be passed on to <code><a href="base.html#topic+sprintf">sprintf</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>The same as what <code><a href="base.html#topic+sprintf">sprintf</a></code> returns:
a character vector of length that of the longest input in <code>...</code>. <br />
Except, in case any of the values passed on to <code>...</code> are <code>NA</code>, 
the corresponding returned value will be set to <code>NA</code> for that element of the vector. <br />
See the examples to see the difference with <code><a href="base.html#topic+sprintf">sprintf</a></code>
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+sprintf">sprintf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sprintf("(w-1):%s", c("xyz", NA, "abc"))
txt_sprintf("(w-1):%s", c("xyz", NA, "abc"))
sprintf("(w-1):%s_%s", c("xyz", NA, "abc"), c(NA, "xyz", "abc"))
txt_sprintf("(w-1):%s_%s", c("xyz", NA, "abc"), c(NA, "xyz", "abc"))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
