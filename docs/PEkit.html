<!DOCTYPE html><html><head><title>Help for package PEkit</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {PEkit}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#abundance'><p>Vector of frequencies of frequencies</p></a></li>
<li><a href='#classifier.fit'><p>Fit the supervised classifier under partition exchangeability</p></a></li>
<li><a href='#dPD'><p>The Poisson-Dirichlet distribution</p></a></li>
<li><a href='#is.PD'><p>Test for the shape of the distribution</p></a></li>
<li><a href='#MLEp'><p>Maximum Likelihood Estimate of <code class="reqn">\psi</code></p></a></li>
<li><a href='#MLEp.bsci'><p>Bootstrap confidence interval for the MLE of <code class="reqn">\psi</code></p></a></li>
<li><a href='#mult.sample.test'><p>Test for <code class="reqn">\psi</code> of multiple samples</p></a></li>
<li><a href='#rPD'><p>Random sampling from the Poisson-Dirichlet Distribution</p></a></li>
<li><a href='#sample.test'><p>Lagrange Multiplier Test for <code class="reqn">\psi</code></p></a></li>
<li><a href='#tMarLab'><p>Marginally predicted labels of the test data given training data classification.</p></a></li>
<li><a href='#tSimLab'><p>Simultaneously predicted labels of the test data given the training data classification.</p></a></li>
<li><a href='#two.sample.test'><p>Two sample test for <code class="reqn">\psi</code></p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Partition Exchangeability Toolkit</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0.1000</td>
</tr>
<tr>
<td>Description:</td>
<td>Bayesian supervised predictive classifiers, hypothesis testing, and parametric estimation under Partition Exchangeability are implemented. The two classifiers presented are the marginal classifier (that assumes test data is i.i.d.) next to a more computationally costly but accurate simultaneous classifier (that finds a labelling for the entire test dataset at once based on simultanous use of all the test data to predict each label). We also provide the Maximum Likelihood Estimation (MLE) of the only underlying parameter of the partition exchangeability generative model as well as hypothesis testing statistics for equality of this parameter with a single value, alternative, or multiple samples. We present functions to simulate the sequences from Ewens Sampling Formula as the realisation of the Poisson-Dirichlet distribution and their respective probabilities.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1.9001</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats (&ge; 4.1.0)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-11-19 13:16:51 UTC; villekin</td>
</tr>
<tr>
<td>Author:</td>
<td>Ville Kinnula [aut],
  Jing Tang <a href="https://orcid.org/0000-0001-7480-7710"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb],
  Ali Amiryousefi <a href="https://orcid.org/0000-0002-6317-3860"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ali Amiryousefi &lt;ali.amiryousefi@helsinki.fi&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-11-22 08:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='abundance'>Vector of frequencies of frequencies</h2><span id='topic+abundance'></span>

<h3>Description</h3>

<p>A function to calculate the abundance vector, or frequencies of frequencies of discrete or partly discrete
data vector <code>x</code>. The abundance vector is used as input in the functions <code>dPD()</code>, <code>MLEp()</code>, and <code>LMTp()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>abundance(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="abundance_+3A_x">x</code></td>
<td>
<p>Data vector <code>x</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is equivalent to <code>table(table(x))</code>.
</p>


<h3>Value</h3>

<p>This function returns a named vector with the frequencies of the frequencies in the data vector x.
The function <code>base::table(x)</code> returns a contingency table with the frequencies in the input data vector <code>x</code> as
values. The <code>names(table(x))</code> are the unique values in data vector <code>x</code>. In <code>abundance(x)</code>,
the unique values in <code>table(x)</code> become the names of the values, while the values
themselves are the frequencies of the frequencies of data vector <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(111)
x&lt;-rpois(10,10)
## The frequency table of x:
print(table(x))
## The frequency table of the frequency table of x:
abundance(x)
</code></pre>

<hr>
<h2 id='classifier.fit'>Fit the supervised classifier under partition exchangeability</h2><span id='topic+classifier.fit'></span>

<h3>Description</h3>

<p>Fits the model according to training data x, where x is assumed to follow
the Poisson-Dirichlet distribution, and discrete labels y.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classifier.fit(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="classifier.fit_+3A_x">x</code></td>
<td>
<p>data vector, or matrix with rows as data points and columns as features.</p>
</td></tr>
<tr><td><code id="classifier.fit_+3A_y">y</code></td>
<td>
<p>training data label vector of length equal to the amount of rows in <code>x</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used to learn the model parameters from the
training data, and gather them into an object that is used by the
classification algorithms <code>tMarLab()</code> and <code>tSimLab()</code>. The parameters it learns
are the Maximum Likelihood Estimate of the <code class="reqn">\psi</code> of each feature within
each class in the training data. It also records the frequencies of the data
for each feature within each class as well. These are used in calculating the
predictive probability of each test data being in each of the classes.
</p>


<h3>Value</h3>

<p>Returns an object used as training data objects for the classification
algorithms <code>tMarLab()</code> and <code>tSimLab()</code>.
</p>
<p>If <code>x</code> is multidimensional, each list described below is returned for each dimension.
</p>
<p>Returns a list of classwise lists, each with components:
</p>
<p><code>frequencies</code>: the frequencies of values in the class.
</p>
<p><code>psi</code>: the Maximum Likelihood estimate of <code class="reqn">\psi</code> for the class.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Create training data x and its class labels y from Poisson-Dirichlet distributions
## with different psis:
set.seed(111)
x1&lt;-rPD(5000,10)
x2&lt;-rPD(5000,100)
x&lt;-c(x1,x2)
y1&lt;-rep("1", 5000)
y2&lt;-rep("2", 5000)
y&lt;-c(y1,y2)
fit&lt;-classifier.fit(x,y)

## With multidimensional x:
set.seed(111)
x1&lt;-cbind(rPD(5000,10),rPD(5000,50))
x2&lt;-cbind(rPD(5000,100),rPD(5000,500))
x&lt;-rbind(x1,x2)
y1&lt;-rep("1", 5000)
y2&lt;-rep("2", 5000)
y&lt;-c(y1,y2)
fit&lt;-classifier.fit(x,y)
</code></pre>

<hr>
<h2 id='dPD'>The Poisson-Dirichlet distribution</h2><span id='topic+dPD'></span>

<h3>Description</h3>

<p>Distribution function for the Poisson-Dirichlet distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dPD(abund, psi)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dPD_+3A_abund">abund</code></td>
<td>
<p>An abundance vector.</p>
</td></tr>
<tr><td><code id="dPD_+3A_psi">psi</code></td>
<td>
<p>Dispersal parameter <code class="reqn">\psi</code>. Accepted input values are positive real
numbers, &quot;a&quot; for absolute value <code class="reqn">\psi</code>=1 by default, or &quot;r&quot; for relative
value <code class="reqn">\psi=n</code>, where <code class="reqn">n</code> is the size of the input sample.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given an abundance vector <code>abunds</code>, calculates the probability
of a data vector <code>x</code> given by the Poisson-Dirichlet distribution. The higher the
dispersal parameter <code class="reqn">\psi</code>, the higher the amount of distinct observed
species. In terms of the paintbox process, a high <code class="reqn">\psi</code> increases the
size of the continuous part <code class="reqn">p_0</code> of the process, while a low <code class="reqn">\psi</code> will increase
the size of the discrete parts <code class="reqn">p_{\neq 0}</code>.
</p>


<h3>Value</h3>

<p>The probability of the Poisson-Dirichlet distribution for the input
abundance vector, e.g. an exchangeable random partition,  and a dispersal parameter <code class="reqn">\psi</code>.
</p>


<h3>References</h3>

<p>W.J. Ewens, The sampling theory of selectively neutral alleles, Theoretical Population Biology, Volume 3, Issue 1,
1972, Pages 87-112, ISSN 0040-5809, &lt;doi: <a href="https://doi.org/10.1016/0040-5809(72)90035-4">10.1016/0040-5809(72)90035-4</a>&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Get a random sample from the Poisson Dirichlet distribution, and
## find the probability of such a sample with psi=5:
set.seed(111)
s &lt;- rPD(n=100,psi=5)
a=abundance(s)
dPD(a, psi=5)

</code></pre>

<hr>
<h2 id='is.PD'>Test for the shape of the distribution</h2><span id='topic+is.PD'></span>

<h3>Description</h3>

<p>This function performs a statistical test on the null hypothesis that a given
sample's underlying distribution is the Poisson-Dirichlet distribution. It
calculates a test statistic that is then used to gain a p-value from an
empirical distribution of the statistic from simulated samples from a
PD distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.PD(x, rounds)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.PD_+3A_x">x</code></td>
<td>
<p>A discrete data vector.</p>
</td></tr>
<tr><td><code id="is.PD_+3A_rounds">rounds</code></td>
<td>
<p>How many samples are simulated to obtain the empirical distribution.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The calculated test statistic is </p>
<p style="text-align: center;"><code class="reqn">W=\sum_{i=1}^n n_i^2 / n ,</code>
</p>

<p>which is calculated from the sample. Here <code class="reqn">n_i</code> are the frequencies of each unique value in the sample.
The MLE of <code class="reqn">\psi</code> is then estimated from the sample with the function <code>MLEp()</code>, and an amount of samples
equal to the input parameter <code>rounds</code> are generated with that estimate of <code class="reqn">\psi</code>
and sample size <code class="reqn">n</code>. The test statistic <code class="reqn">W</code> is then calculated for each of the simulated samples.
The original <code class="reqn">W</code> is then given a p-value based on what percentage of the simulated <code class="reqn">W</code> it exceeds.
</p>


<h3>Value</h3>

<p>A p-value.
</p>


<h3>References</h3>

<p>Watterson, G.A., (1978), The homozygosity test of neutrality. Genetics. 88(2):405-417.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##Test whether a typical sample follows PD:
x&lt;-rPD(100,10)
is.PD(x, 100)

##Test whether a very atypical sample where frequencies of different values
## are similar:

x&lt;-c(rep(1, 200), rep(2, 200), rep(3, 200), rep(4, 200), rep(5,200))
is.PD(x,50)
</code></pre>

<hr>
<h2 id='MLEp'>Maximum Likelihood Estimate of <code class="reqn">\psi</code></h2><span id='topic+MLEp'></span>

<h3>Description</h3>

<p>Numerically searches for the MLE of <code class="reqn">\psi</code> given an abundance vector with a binary search algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MLEp(abund)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MLEp_+3A_abund">abund</code></td>
<td>
<p>An abundance vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Numerically searches for the MLE of <code class="reqn">\psi</code> as the root of equation
</p>
<p style="text-align: center;"><code class="reqn">K=\sum_{i=1}^n\psi/(\psi+i-1),</code>
</p>
<p> where <code class="reqn">K</code> is the observed number of
different species in the sample. The right side of the equation is monotonically
increasing when <code class="reqn">\psi&gt;0</code>, so a binary search is used to find the root.
An accepted <code class="reqn">\psi</code> sets value of the right side
of the equation within R's smallest possible value of the actual value of <code class="reqn">K</code>.
</p>


<h3>Value</h3>

<p>The MLE of <code class="reqn">\psi</code>.
</p>


<h3>References</h3>

<p>W.J. Ewens, The sampling theory of selectively neutral alleles, Theoretical Population Biology, Volume 3, Issue 1,
1972, Pages 87-112, ISSN 0040-5809, &lt;doi: <a href="https://doi.org/10.1016/0040-5809(72)90035-4">10.1016/0040-5809(72)90035-4</a>&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##Find the MLE of psi of the vector (1,2,2).
##The frequencies of the frequencies of the data vector are given as input:
MLEp(abundance(c(1,2,2)))

##Find the MLE of psi of a sample from the Poisson-Dirichlet distribution:
set.seed(1000)
x&lt;-rPD(n=10000, psi=100)
MLEp(abundance(x))
</code></pre>

<hr>
<h2 id='MLEp.bsci'>Bootstrap confidence interval for the MLE of <code class="reqn">\psi</code></h2><span id='topic+MLEp.bsci'></span>

<h3>Description</h3>

<p>A bootstrapped confidence interval for the Maximum Likelihood Estimate for
<code class="reqn">\psi</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MLEp.bsci(x, level = 0.95, rounds = 1000, frac = 0.8)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MLEp.bsci_+3A_x">x</code></td>
<td>
<p>A data vector.</p>
</td></tr>
<tr><td><code id="MLEp.bsci_+3A_level">level</code></td>
<td>
<p>Level of confidence interval as number between 0 and 1.</p>
</td></tr>
<tr><td><code id="MLEp.bsci_+3A_rounds">rounds</code></td>
<td>
<p>Number of bootstrap rounds. Default is 1000.</p>
</td></tr>
<tr><td><code id="MLEp.bsci_+3A_frac">frac</code></td>
<td>
<p>Percentage of data <code>x</code> used for each bootstrap round. 0.8 by default with accepted values between 0 and 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The MLE of <code class="reqn">\psi</code> as well as lower and upper bounds of the bootstrap
confidence interval.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Find a 95% -confidence interval for the MLE of psi given a sample from the
## Poisson-Dirichlet distribution:
x&lt;-rPD(n=10000, psi=100)
MLEp.bsci(x, 0.95, 100, 0.8)

</code></pre>

<hr>
<h2 id='mult.sample.test'>Test for <code class="reqn">\psi</code> of multiple samples</h2><span id='topic+mult.sample.test'></span>

<h3>Description</h3>

<p>Likelihood ratio test for the hypotheses <code class="reqn">H_0: \: \psi_1=\psi_2=...=\psi_d</code> and
<code class="reqn">H_1: \: \psi_1 \neq \psi_2 \neq ... \neq \psi_d</code>, where <code class="reqn">\psi_1,\psi_2,</code>...<code class="reqn">,\psi_d</code> are the
dispersal parameters of the <code class="reqn">d</code> samples in the columns of the input data array <code>x</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mult.sample.test(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mult.sample.test_+3A_x">x</code></td>
<td>
<p>The data array to be tested. Each column of <code>x</code> is an independent sample.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the Likelihood Ratio Test statistic
</p>
<p style="text-align: center;"><code class="reqn">-2log(L(\hat{\psi})/L(\hat{\psi}_1, \hat{\psi}_2, ..., \hat{\psi}_d)),</code>
</p>

<p>where L is the likelihood function of observing the <code class="reqn">d</code> input samples given
a single <code class="reqn">\psi</code> in the numerator and <code class="reqn">d</code> different parameters <code class="reqn">\psi_1,\psi_2,</code>...<code class="reqn">,\psi_d</code>
for each sample respectively in the denominator. According
to the theory of Likelihood Ratio Tests, this statistic converges in
distribution to a <code class="reqn">\chi_{d-1}^2</code>-distribution when the null-hypothesis is true, where <code class="reqn">d-1</code> is the
difference in the amount of parameters between the considered models. To
calculate the statistic, the Maximum Likelihood Estimate for
<code class="reqn">\psi_1,\: \psi_2,\: ..., \: \psi_d</code> of <code class="reqn">H_1</code> and the shared <code class="reqn">\psi</code> of <code class="reqn">H_0</code>
are calculated.
</p>


<h3>Value</h3>

<p>Gives a vector with the Likelihood Ratio Test -statistic <code>Lambda</code>, as well as the
p-value of the test <code>p</code>.
</p>


<h3>References</h3>

<p>Neyman, J., &amp; Pearson, E. S. (1933). On the problem of the most
efficient tests of statistical hypotheses. Philosophical Transactions of the
Royal Society of London. Series A, Containing Papers of a Mathematical Or
Physical Character, 231(694-706), 289-337. &lt;doi: <a href="https://doi.org/10.1098/rsta.1933.0009">10.1098/rsta.1933.0009</a>&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##Create samples with different n and psi:
set.seed(111)
x&lt;-rPD(1200, 15)
y&lt;-c( rPD(1000, 20), rep(NA, 200) )
z&lt;-c( rPD(800, 30), rep(NA, 400) )
samples&lt;-cbind(cbind(x, y), z)
##Run test
mult.sample.test(samples)
</code></pre>

<hr>
<h2 id='rPD'>Random sampling from the Poisson-Dirichlet Distribution</h2><span id='topic+rPD'></span>

<h3>Description</h3>

<p>rPD samples randomly from the PD distribution with a given <code class="reqn">\psi</code> by simulating the Hoppe urn model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rPD(n, psi)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rPD_+3A_n">n</code></td>
<td>
<p>number of observations.</p>
</td></tr>
<tr><td><code id="rPD_+3A_psi">psi</code></td>
<td>
<p>dispersal parameter.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Samples random values with a given <code class="reqn">\psi</code> from the Poisson-Dirichlet distribution by simulating the Hoppe urn model.
</p>


<h3>Value</h3>

<p>Returns a vector with a sample of size <code class="reqn">n</code> from the Hoppe urn model with parameter <code class="reqn">\psi</code>.
</p>


<h3>References</h3>

<p>Hoppe, F.M. The sampling theory of neutral alleles and an urn model in population genetics.
J. Math. Biology 25, 123–159 (1987). &lt;doi: <a href="https://doi.org/10.1007/BF00276386">10.1007/BF00276386</a>&gt;.
</p>
<p>W.J. Ewens, The sampling theory of selectively neutral alleles, Theoretical Population Biology, Volume 3, Issue 1,
1972, Pages 87-112, ISSN 0040-5809, &lt;doi: <a href="https://doi.org/10.1016/0040-5809(72)90035-4">10.1016/0040-5809(72)90035-4</a>&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Get random sample from the PD distribution with different psi,
## and estimate the psi of the samples:
s1&lt;-rPD(1000, 10)
s2&lt;- rPD(1000, 50)
print(c(MLEp(abundance(s1)), MLEp(abundance(s2))))

</code></pre>

<hr>
<h2 id='sample.test'>Lagrange Multiplier Test for <code class="reqn">\psi</code></h2><span id='topic+sample.test'></span>

<h3>Description</h3>

<p>Performs the Lagrange Multiplier test for the equality of the dispersion parameter <code class="reqn">\psi</code> of a sample.
The null hypothesis of the test is <code class="reqn">H_0: \psi = \psi_0</code>, where <code class="reqn">\psi_0</code> is given as input here.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample.test(abund, psi = "a")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample.test_+3A_abund">abund</code></td>
<td>
<p>An abundance vector of a sample.</p>
</td></tr>
<tr><td><code id="sample.test_+3A_psi">psi</code></td>
<td>
<p>Target positive number <code class="reqn">\psi_0</code> to be tested. Accepted values are &quot;a&quot; for absolute value 1,
&quot;r&quot; for relative value <code class="reqn">n</code> (sample size), or any positive number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the Lagrange Multiplier test statistic </p>
<p style="text-align: center;"><code class="reqn">S\, = \,U(\psi_0)^2 / I(\psi_0),</code>
</p>

<p>where <code class="reqn">U</code> is the log-likelihood function of <code class="reqn">\psi</code> and <code class="reqn">I</code> is its Fisher information.
The statistic <code class="reqn">S</code> follows <code class="reqn">\chi^2</code>-distribution with 1 degree of freedom
when the null hypothesis <code class="reqn">H_0:\psi=\psi_0</code> is true.
</p>


<h3>Value</h3>

<p>The statistic <code class="reqn">S</code> and a p-value of the two-sided test of the hypothesis.
</p>


<h3>References</h3>

<p>Radhakrishna Rao, C, (1948), Large sample tests of statistical
hypotheses concerning several parameters with applications to problems of
estimation. Mathematical Proceedings of the Cambridge Philosophical Society,
44(1), 50-57. &lt;doi: <a href="https://doi.org/10.1017/S0305004100023987">10.1017/S0305004100023987</a>&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Test the psi of a sample from the Poisson-Dirichlet distribution:
set.seed(10000)
x&lt;-rPD(1000, 10)
## Find the abundance of the data vector:
abund=abundance(x)
## Test for the psi that was used, as well as a higher and a lower one:
sample.test(abund, 10)
sample.test(abund, 15)
sample.test(abund, 5)
sample.test(abund)       #test for psi=1
sample.test(abund, "r")  #test for psi=n
</code></pre>

<hr>
<h2 id='tMarLab'>Marginally predicted labels of the test data given training data classification.</h2><span id='topic+tMarLab'></span>

<h3>Description</h3>

<p>Classifies the test data <code>x</code> based on the training data object.
The test data is considered i.i.d., so each
data point is classified one by one.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tMarLab(training, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tMarLab_+3A_training">training</code></td>
<td>
<p>A training data object from the function <code>classifier.fit()</code>.</p>
</td></tr>
<tr><td><code id="tMarLab_+3A_x">x</code></td>
<td>
<p>Test data vector or matrix with rows as data points and columns as features.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Independently assigns a class label for each test data point according to a
<code class="reqn">maximum \, a \, posteriori</code> rule. The predictive probability of data point
<code class="reqn">x_i</code> arising from class <code class="reqn">c</code> assuming the training data of size <code class="reqn">m_c</code> in the class
arises from a Poisson-Dirichlet(<code class="reqn">\hat{\psi}_c</code>) distribution is:
</p>
<p style="text-align: center;"><code class="reqn">\hat{\psi}_c / (m_c + \hat{\psi}_c),</code>
</p>

<p>if no value equal to <code class="reqn">x_i</code> exists in the training data of class <code class="reqn">c</code>, and
</p>
<p style="text-align: center;"><code class="reqn">m_{ci} / (m_c + \hat{\psi}_c),</code>
</p>

<p>if there does, where <code class="reqn">m_{ci}</code> is the frequency of the value of <code class="reqn">x_i</code>
in the training data.
</p>


<h3>Value</h3>

<p>A vector of predicted labels for test data x.
</p>


<h3>References</h3>

<p>Amiryousefi A. Asymptotic supervised predictive classifiers under
partition exchangeability. . 2021. <a href="https://arxiv.org/abs/2101.10950">https://arxiv.org/abs/2101.10950</a>.
</p>
<p>Corander, J., Cui, Y., Koski, T., and Siren, J.: Have I seen you before?
Principles of Bayesian predictive classification revisited. Springer, Stat.
Comput. 23, (2011), 59–73, (&lt;doi: <a href="https://doi.org/10.1007/s11222-011-9291-7">10.1007/s11222-011-9291-7</a>&gt;).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Create random samples x from Poisson-Dirichlet distributions with different
## psis, treating each sample as coming from a class of its own:
set.seed(111)
x1&lt;-rPD(10500,10)
x2&lt;-rPD(10500,1000)
test.ind1&lt;-sample.int(10500,500) # Sample test datasets from the
test.ind2&lt;-sample.int(10500,500) # original samples
x&lt;-c(x1[-test.ind1],x2[-test.ind2])
## create training data labels:
y1&lt;-rep("1", 10000)
y2&lt;-rep("2", 10000)
y&lt;-c(y1,y2)

## Test data t, with first half belonging to class "1", second have in "2":
t1&lt;-x1[test.ind1]
t2&lt;-x2[test.ind2]
t&lt;-c(t1,t2)

fit&lt;-classifier.fit(x,y)

## Run the classifier, which returns
tM&lt;-tMarLab(fit, t)

##With multidimensional x:
set.seed(111)
x1&lt;-cbind(rPD(5500,10),rPD(5500,50))
x2&lt;-cbind(rPD(5500,100),rPD(5500,500))
test.ind1&lt;-sample.int(5500,500)
test.ind2&lt;-sample.int(5500,500)
x&lt;-rbind(x1[-test.ind1,],x2[-test.ind2,])
y1&lt;-rep("1", 5000)
y2&lt;-rep("2", 5000)
y&lt;-c(y1,y2)
fit&lt;-classifier.fit(x,y)
t1&lt;-x1[test.ind1,]
t2&lt;-x2[test.ind2,]
t&lt;-rbind(t1,t2)

tM&lt;-tMarLab(fit, t)
</code></pre>

<hr>
<h2 id='tSimLab'>Simultaneously predicted labels of the test data given the training data classification.</h2><span id='topic+tSimLab'></span>

<h3>Description</h3>

<p>Classifies the test data <code>x</code> based on the training data object.
All of the test data is used simultaneously to make the classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tSimLab(training, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tSimLab_+3A_training">training</code></td>
<td>
<p>A training data object from the function <code>classifier.fit()</code>.</p>
</td></tr>
<tr><td><code id="tSimLab_+3A_x">x</code></td>
<td>
<p>Test data vector or matrix with rows as data points and columns as features.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The test data are first labeled with the marginal classifier. The simultaneous
classifier then iterates over all test data, assigning each a label by finding
the maximum predictive probability given the current classification structure of
the test data as a whole. This is repeated until the classification structure
converges after iterating over all data.
</p>


<h3>Value</h3>

<p>A vector of predicted labels for test data x.
</p>


<h3>References</h3>

<p>Amiryousefi A. Asymptotic supervised predictive classifiers under
partition exchangeability. . 2021. <a href="https://arxiv.org/abs/2101.10950">https://arxiv.org/abs/2101.10950</a>.
</p>
<p>Corander, J., Cui, Y., Koski, T., and Siren, J.: Have I seen you before?
Principles of Bayesian predictive classification revisited. Springer, Stat.
Comput. 23, (2011), 59–73, (&lt;doi: <a href="https://doi.org/10.1007/s11222-011-9291-7">10.1007/s11222-011-9291-7</a>&gt;).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Create random samples x from Poisson-Dirichlet distributions with different
## psis, treating each sample as coming from a class of its own:
set.seed(111)
x1&lt;-rPD(1050,10)
x2&lt;-rPD(1050,1000)
test.ind1&lt;-sample.int(1050,50) # Sample test datasets from the
test.ind2&lt;-sample.int(1050,50) # original samples
x&lt;-c(x1[-test.ind1],x2[-test.ind2])
## create training data labels:
y1&lt;-rep("1", 1000)
y2&lt;-rep("2", 1000)
y&lt;-c(y1,y2)

## Test data t, with first half belonging to class "1", second have in "2":
t1&lt;-x1[test.ind1]
t2&lt;-x2[test.ind2]
t&lt;-c(t1,t2)

fit&lt;-classifier.fit(x,y)

## Run the classifier, which returns
tS&lt;-tSimLab(fit, t)

##With multidimensional x:
set.seed(111)
x1&lt;-cbind(rPD(500,1),rPD(500,5))
x2&lt;-cbind(rPD(500,10),rPD(500,50))
test.ind1&lt;-sample.int(500,50)
test.ind2&lt;-sample.int(500,50)
x&lt;-rbind(x1[-test.ind1,],x2[-test.ind2,])
y1&lt;-rep("1", 450)
y2&lt;-rep("2", 450)
y&lt;-c(y1,y2)
fit&lt;-classifier.fit(x,y)
t1&lt;-x1[test.ind1,]
t2&lt;-x2[test.ind2,]
t&lt;-rbind(t1,t2)

tS&lt;-tSimLab(fit, t)
</code></pre>

<hr>
<h2 id='two.sample.test'>Two sample test for <code class="reqn">\psi</code></h2><span id='topic+two.sample.test'></span>

<h3>Description</h3>

<p>Likelihood ratio test for the hypotheses <code class="reqn">H_0: \: \psi_1=\psi_2</code> and
<code class="reqn">H_1: \: \psi_1 \neq \psi_2</code>, where <code class="reqn">\psi_1</code> and <code class="reqn">\psi_2</code> are the
dispersal parameters of two input samples <code>s1</code> and <code>s2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>two.sample.test(s1, s2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="two.sample.test_+3A_s1">s1</code>, <code id="two.sample.test_+3A_s2">s2</code></td>
<td>
<p>The two data vectors to be tested.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the Likelihood Ratio Test statistic
</p>
<p style="text-align: center;"><code class="reqn">-2log(L(\hat{\psi})/L(\hat{\psi}_1, \hat{\psi}_2)),</code>
</p>

<p>where L is the likelihood function of observing the two input samples given
a single <code class="reqn">\psi</code> in the numerator and two different parameters <code class="reqn">\psi_1</code>
and <code class="reqn">\psi_2</code> for each sample respectively in the denominator. According
to the theory of Likelihood Ratio Tests, this statistic converges in
distribution to a <code class="reqn">\chi_d^2</code>-distribution under the null-hypothesis, where <code class="reqn">d</code> is the
difference in the amount of parameters between the considered models, which
is 1 here. To calculate the statistic, the Maximum Likelihood Estimate for
<code class="reqn">\psi_1,\: \psi_2</code> of <code class="reqn">H_1</code> and the shared <code class="reqn">\psi</code> of <code class="reqn">H_0</code>
are calculated.
</p>


<h3>Value</h3>

<p>Gives a vector with the Likelihood Ratio Test -statistic <code>Lambda</code>, as well as the
p-value of the test <code>p</code>.
</p>


<h3>References</h3>

<p>Neyman, J., &amp; Pearson, E. S. (1933). On the problem of the most
efficient tests of statistical hypotheses. Philosophical Transactions of the
Royal Society of London. Series A, Containing Papers of a Mathematical Or
Physical Character, 231(694-706), 289-337. &lt;doi: <a href="https://doi.org/10.1098/rsta.1933.0009">10.1098/rsta.1933.0009</a>&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##Create samples with different n and psi:
set.seed(111)
x&lt;-rPD(500, 15)
y&lt;-rPD(1000, 20)
z&lt;-rPD(800, 30)
##Run tests
two.sample.test(x,y)
two.sample.test(x,z)
two.sample.test(y,z)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
