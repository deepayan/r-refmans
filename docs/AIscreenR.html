<!DOCTYPE html><html lang="en"><head><title>Help for package AIscreenR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {AIscreenR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#AIscreenR-package'><p>AIscreenR: AI Screening Tools in R for Systematic Reviewing</p></a></li>
<li><a href='#approximate_price_gpt'><p>Approximate price estimation for title and abstract screening using OpenAI's GPT API models</p></a></li>
<li><a href='#filges2015_dat'><p>RIS file data from Functional Family Therapy (FFT) systematic review</p></a></li>
<li><a href='#get_api_key'><p>Get API key from R environment variable.</p></a></li>
<li><a href='#is_chatgpt'><p>Test if the object is a <code>'chatgpt'</code> object</p></a></li>
<li><a href='#is_chatgpt_tbl'><p>Test if the object is a <code>'chatgpt_tbl'</code> object</p></a></li>
<li><a href='#is_gpt'><p>Test if the object is a <code>'gpt'</code> object</p></a></li>
<li><a href='#is_gpt_agg_tbl'><p>Test if the object is a <code>'gpt_agg_tbl'</code> object</p></a></li>
<li><a href='#is_gpt_tbl'><p>Test if the object is a <code>'gpt_tbl'</code> object</p></a></li>
<li><a href='#model_prizes'><p>Model prize data (last updated November 5, 2024)</p></a></li>
<li><a href='#print.chatgpt'><p>Print methods for <code>'chatgpt'</code> objects</p></a></li>
<li><a href='#print.gpt'><p>Print methods for <code>'gpt'</code> objects</p></a></li>
<li><a href='#print.gpt_price'><p>Print methods for <code>'gpt_price'</code> objects</p></a></li>
<li><a href='#rate_limits_per_minute'><p>Find updated rate limits for API models</p></a></li>
<li><a href='#sample_references'><p>Random sample references</p></a></li>
<li><a href='#screen_analyzer'><p>Analyze performance between the human and AI screening.</p></a></li>
<li><a href='#screen_errors'><p>Generic function to re-screen failed title and abstract requests.</p></a></li>
<li><a href='#screen_errors.chatgpt'><p>Re-screen failed requests.</p></a></li>
<li><a href='#screen_errors.gpt'><p>Re-screen failed requests.</p></a></li>
<li><a href='#set_api_key'><p>Creating a temporary R environment API key variable</p></a></li>
<li><a href='#tabscreen_gpt.original'><p>Title and abstract screening with GPT API models using function calls via the original function call arguments</p></a></li>
<li><a href='#tabscreen_gpt.tools'><p>Title and abstract screening with GPT API models using function calls via the tools argument</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>AI Screening Tools in R for Systematic Reviewing</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides functions to conduct title and abstract screening in systematic reviews using large language models, such as the Generative Pre-trained Transformer (GPT) models from 'OpenAI' <a href="https://platform.openai.com/">https://platform.openai.com/</a>. These functions can enhance the quality of title and abstract screenings while reducing the total screening time significantly. In addition, the package includes tools for quality assessment of title and abstract screenings, as described in Vembye, Christensen, Mølgaard, and Schytt (2024) &lt;<a href="https://doi.org/10.31219%2Fosf.io%2Fyrhzm">doi:10.31219/osf.io/yrhzm</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>dplyr, tibble, httr2, stringr, furrr, tidyr, tictoc, askpass,
curl, purrr, lifecycle, jsonlite</td>
</tr>
<tr>
<td>Suggests:</td>
<td>future, knitr, rmarkdown, usethis, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Config/testthat/parallel:</td>
<td>true</td>
</tr>
<tr>
<td>Config/testthat/start-first:</td>
<td>tabscreen_gpt, approximate_price_gpt,
rate_limits, api_key_functions</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://mikkelvembye.github.io/AIscreenR/">https://mikkelvembye.github.io/AIscreenR/</a>,
<a href="https://github.com/MikkelVembye/AIscreenR">https://github.com/MikkelVembye/AIscreenR</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/MikkelVembye/AIscreenR/issues">https://github.com/MikkelVembye/AIscreenR/issues</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-11-26 11:22:33 UTC; B199526</td>
</tr>
<tr>
<td>Author:</td>
<td>Mikkel H. Vembye <a href="https://orcid.org/0000-0001-9071-0724"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Mikkel H. Vembye &lt;mikkel.vembye@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-11-26 14:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='AIscreenR-package'>AIscreenR: AI Screening Tools in R for Systematic Reviewing</h2><span id='topic+AIscreenR'></span><span id='topic+AIscreenR-package'></span>

<h3>Description</h3>

<p>Provides functions to conduct title and abstract screening in systematic reviews using large language models, such as the Generative Pre-trained Transformer (GPT) models from 'OpenAI' <a href="https://platform.openai.com/">https://platform.openai.com/</a>. These functions can enhance the quality of title and abstract screenings while reducing the total screening time significantly. In addition, the package includes tools for quality assessment of title and abstract screenings, as described in Vembye, Christensen, Mølgaard, and Schytt (2024) <a href="https://doi.org/10.31219/osf.io/yrhzm">doi:10.31219/osf.io/yrhzm</a>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Mikkel H. Vembye <a href="mailto:mikkel.vembye@gmail.com">mikkel.vembye@gmail.com</a> (<a href="https://orcid.org/0000-0001-9071-0724">ORCID</a>) [copyright holder]
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://mikkelvembye.github.io/AIscreenR/">https://mikkelvembye.github.io/AIscreenR/</a>
</p>
</li>
<li> <p><a href="https://github.com/MikkelVembye/AIscreenR">https://github.com/MikkelVembye/AIscreenR</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/MikkelVembye/AIscreenR/issues">https://github.com/MikkelVembye/AIscreenR/issues</a>
</p>
</li></ul>


<hr>
<h2 id='approximate_price_gpt'>Approximate price estimation for title and abstract screening using OpenAI's GPT API models</h2><span id='topic+approximate_price_gpt'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a><br>
<br>
This function supports the approximation of the price of title and abstract
screenings when using OpenAI's GPT API models. The function only provide approximately accurate price
estimates. When detailed descriptions are used,
this will increase the completion tokens with an unknown amount.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>approximate_price_gpt(
  data,
  prompt,
  studyid,
  title,
  abstract,
  model = "gpt-4o-mini",
  reps = 1,
  top_p = 1,
  token_word_ratio = 1.6
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="approximate_price_gpt_+3A_data">data</code></td>
<td>
<p>Dataset containing the titles and abstracts.</p>
</td></tr>
<tr><td><code id="approximate_price_gpt_+3A_prompt">prompt</code></td>
<td>
<p>Prompt(s) to be added before the title and abstract.</p>
</td></tr>
<tr><td><code id="approximate_price_gpt_+3A_studyid">studyid</code></td>
<td>
<p>Unique Study ID. If missing, this is generated
automatically.</p>
</td></tr>
<tr><td><code id="approximate_price_gpt_+3A_title">title</code></td>
<td>
<p>Name of the variable containing the title information.</p>
</td></tr>
<tr><td><code id="approximate_price_gpt_+3A_abstract">abstract</code></td>
<td>
<p>Name of variable containing the abstract information.</p>
</td></tr>
<tr><td><code id="approximate_price_gpt_+3A_model">model</code></td>
<td>
<p>Character string with the name of the completion model. Can take
multiple models, including gpt-4 models. Default = <code>"gpt-4o-mini"</code>.
Find available model at
<a href="https://platform.openai.com/docs/models/model-endpoint-compatibility">https://platform.openai.com/docs/models/model-endpoint-compatibility</a>.</p>
</td></tr>
<tr><td><code id="approximate_price_gpt_+3A_reps">reps</code></td>
<td>
<p>Numerical value indicating the number of times the same
question should be sent to the GPT server. This can be useful to test consistency
between answers. Default is <code>1</code> but when using gpt-3.5-turbo or gpt-4o-mini models, we recommend setting this
value to <code>10</code>.</p>
</td></tr>
<tr><td><code id="approximate_price_gpt_+3A_top_p">top_p</code></td>
<td>
<p>'An alternative to sampling with temperature, called nucleus sampling,
where the model considers the results of the tokens with top_p probability mass.
So 0.1 means only the tokens comprising the top 10% probability mass are considered.
We generally recommend altering this or temperature but not both.' (OPEN-AI). Default is 1.
Find documentation at
<a href="https://platform.openai.com/docs/api-reference/chat/create#chat/create-top_p">https://platform.openai.com/docs/api-reference/chat/create#chat/create-top_p</a>.</p>
</td></tr>
<tr><td><code id="approximate_price_gpt_+3A_token_word_ratio">token_word_ratio</code></td>
<td>
<p>The multiplier used to approximate the number of tokens per word.
Default is <code>1.6</code> which we empirically have found to be the average number of tokens per word.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"gpt_price"</code>. The object is a list containing the following
components:
</p>
<table role = "presentation">
<tr><td><code>price</code></td>
<td>
<p>numerical value indicating the total approximate price (in USD) of the screening across all gpt-models expected to be used for the screening.</p>
</td></tr>
<tr><td><code>price_data</code></td>
<td>
<p>dataset with prices across all gpt models expected to be used for screening.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>prompt &lt;- "This is a prompt"

app_price &lt;- approximate_price_gpt(
  data = filges2015_dat[1:2,],
  prompt = prompt,
  studyid = studyid,
  title = title,
  abstract = abstract,
  model = c("gpt-4o-mini", "gpt-4"),
  reps = c(10, 1)
)

app_price
app_price$price_dollar
app_price$price_data
</code></pre>

<hr>
<h2 id='filges2015_dat'>RIS file data from Functional Family Therapy (FFT) systematic review</h2><span id='topic+filges2015_dat'></span>

<h3>Description</h3>

<p>Bibliometric toy data from a systematic review regarding
Functional Family Therapy (FFT) for Young People in Treatment for
Non-opioid Drug Use (Filges et al., 2015). The data includes all 90 included and 180 excluded
randomly sampled references from the literature search of the
systematic review.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filges2015_dat
</code></pre>


<h3>Format</h3>

<p>A <code>tibble</code> with 270 rows/studies and 6 variables/columns
</p>

<table>
<tr>
 <td style="text-align: left;">
<b>author</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the authors of the reference </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>eppi_id</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating a unique eppi-ID for each study </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>studyid</b> </td><td style="text-align: left;"> <code>numeric</code> </td><td style="text-align: left;"> indicating a unique study-ID for each study </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>title</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> with the title of the study </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>abstract</b>  </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> with the study abstract </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>human_code</b> </td><td style="text-align: left;"> <code>numeric</code> </td><td style="text-align: left;"> indicating the human screening decision.
1 = included, 0 = excluded. </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>References</h3>

<p>Filges, T., Andersen, D, &amp; Jørgensen, A-M. K (2015).
Functional Family Therapy (FFT) for Young People in Treatment for Non-opioid Drug Use: A Systematic Review
<em>Campbell Systematic Reviews</em>, <a href="https://doi.org/10.4073/csr.2015.14">doi:10.4073/csr.2015.14</a>
</p>

<hr>
<h2 id='get_api_key'>Get API key from R environment variable.</h2><span id='topic+get_api_key'></span>

<h3>Description</h3>

<p>Get API key from R environment variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_api_key(env_var = "CHATGPT_KEY")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_api_key_+3A_env_var">env_var</code></td>
<td>
<p>Character string indicating the name of the temporary R environment variable with
the API key and the used AI model. Currently, the argument only takes <code>env_var = "CHATGPT_KEY"</code>.
See <code><a href="#topic+set_api_key">set_api_key()</a></code> to set/create this variable.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>get_api_key()</code> can be used after executing <code><a href="#topic+set_api_key">set_api_key()</a></code> or by adding the
api key permanently to your R environment by using <code><a href="usethis.html#topic+edit">usethis::edit_r_environ()</a></code>.
Then write <code style="white-space: pre;">&#8288;CHATGPT_KEY=[insert your api key here]&#8288;</code> and close the <code>.Renviron</code> window and restart R.
</p>


<h3>Value</h3>

<p>The specified API key (NOTE: Avoid exposing this in the console).
</p>


<h3>Note</h3>

<p>Find your personal API key at <a href="https://platform.openai.com/account/api-keys">https://platform.openai.com/account/api-keys</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+set_api_key">set_api_key</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
get_api_key()

## End(Not run)
</code></pre>

<hr>
<h2 id='is_chatgpt'>Test if the object is a <code>'chatgpt'</code> object</h2><span id='topic+is_chatgpt'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a><br>
<br>
This function returns <code>TRUE</code> for <code>chatgpt</code> objects,
and <code>FALSE</code> for all other objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_chatgpt(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is_chatgpt_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>chatgpt</code> class.
</p>

<hr>
<h2 id='is_chatgpt_tbl'>Test if the object is a <code>'chatgpt_tbl'</code> object</h2><span id='topic+is_chatgpt_tbl'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a><br>
<br>
This function returns <code>TRUE</code> for <code>chatgpt_tbl</code> objects,
and <code>FALSE</code> for all other objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_chatgpt_tbl(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is_chatgpt_tbl_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>chatgpt_tbl</code> class.
</p>

<hr>
<h2 id='is_gpt'>Test if the object is a <code>'gpt'</code> object</h2><span id='topic+is_gpt'></span>

<h3>Description</h3>

<p>This function returns <code>TRUE</code> for <code>gpt</code> objects,
and <code>FALSE</code> for all other objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_gpt(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is_gpt_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>gpt</code> class.
</p>

<hr>
<h2 id='is_gpt_agg_tbl'>Test if the object is a <code>'gpt_agg_tbl'</code> object</h2><span id='topic+is_gpt_agg_tbl'></span>

<h3>Description</h3>

<p>This function returns <code>TRUE</code> for <code>gpt_agg_tbl</code> objects,
and <code>FALSE</code> for all other objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_gpt_agg_tbl(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is_gpt_agg_tbl_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>gpt_agg_tbl</code> class.
</p>

<hr>
<h2 id='is_gpt_tbl'>Test if the object is a <code>'gpt_tbl'</code> object</h2><span id='topic+is_gpt_tbl'></span>

<h3>Description</h3>

<p>This function returns <code>TRUE</code> for <code>gpt_tbl</code> objects,
and <code>FALSE</code> for all other objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_gpt_tbl(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is_gpt_tbl_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the object inherits from the <code>gpt_tbl</code> class.
</p>

<hr>
<h2 id='model_prizes'>Model prize data (last updated November 5, 2024)</h2><span id='topic+model_prizes'></span>

<h3>Description</h3>

<p>Data set containing input and output prizes for all OpenAI's GPT API models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>model_prizes
</code></pre>


<h3>Format</h3>

<p>A <code>data.frame</code> containing 15 rows/models and 3 variables/columns
</p>

<table>
<tr>
 <td style="text-align: left;">
<b>model</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the specific GPT model </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>price_in_per_token</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the input prize per token </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>price_out_per_token</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the output prize per token </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>References</h3>

<p>OpenAI. <em>Pricing</em>. <a href="https://openai.com/api/pricing/">https://openai.com/api/pricing/</a>
</p>

<hr>
<h2 id='print.chatgpt'>Print methods for <code>'chatgpt'</code> objects</h2><span id='topic+print.chatgpt'></span>

<h3>Description</h3>

<p>Print methods for <code>'chatgpt'</code> objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'chatgpt'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.chatgpt_+3A_x">x</code></td>
<td>
<p>an object of class <code>'chatgpt'</code>.</p>
</td></tr>
<tr><td><code id="print.chatgpt_+3A_...">...</code></td>
<td>
<p>other print arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Information about how to find answer data sets and pricing information.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
print(x)

## End(Not run)
</code></pre>

<hr>
<h2 id='print.gpt'>Print methods for <code>'gpt'</code> objects</h2><span id='topic+print.gpt'></span>

<h3>Description</h3>

<p>Print methods for <code>'gpt'</code> objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gpt'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.gpt_+3A_x">x</code></td>
<td>
<p>an object of class <code>'gpt'</code>.</p>
</td></tr>
<tr><td><code id="print.gpt_+3A_...">...</code></td>
<td>
<p>other print arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Information about how to find answer data sets and pricing information.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
print(x)

## End(Not run)
</code></pre>

<hr>
<h2 id='print.gpt_price'>Print methods for <code>'gpt_price'</code> objects</h2><span id='topic+print.gpt_price'></span>

<h3>Description</h3>

<p>Print methods for <code>'gpt_price'</code> objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gpt_price'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.gpt_price_+3A_x">x</code></td>
<td>
<p>an object of class <code>"gpt_price"</code>.</p>
</td></tr>
<tr><td><code id="print.gpt_price_+3A_...">...</code></td>
<td>
<p>other print arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The total price of the screening across all gpt-models expected to be used for the screening.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
print(x)

## End(Not run)
</code></pre>

<hr>
<h2 id='rate_limits_per_minute'>Find updated rate limits for API models</h2><span id='topic+rate_limits_per_minute'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#stable"><img src="../help/figures/lifecycle-stable.svg" alt='[Stable]' /></a><br>
<br>
<code>rate_limits_per_minute</code> reports the rate limits for a given API model.
The function returns the available requests per minute (RPM) as well as tokens per minute (TPM).
Find general information at
<a href="https://platform.openai.com/docs/guides/rate-limits/overview">https://platform.openai.com/docs/guides/rate-limits/overview</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rate_limits_per_minute(
  model = "gpt-4o-mini",
  AI_tool = "gpt",
  api_key = get_api_key()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rate_limits_per_minute_+3A_model">model</code></td>
<td>
<p>Character string with the name of the completion model.
Default is <code>"gpt-4o-mini"</code>. Can take multiple values.
Find available model at
<a href="https://platform.openai.com/docs/models/model-endpoint-compatibility">https://platform.openai.com/docs/models/model-endpoint-compatibility</a>.</p>
</td></tr>
<tr><td><code id="rate_limits_per_minute_+3A_ai_tool">AI_tool</code></td>
<td>
<p>Character string specifying the AI tool from which the API is
issued. Default is <code>"gpt"</code>.</p>
</td></tr>
<tr><td><code id="rate_limits_per_minute_+3A_api_key">api_key</code></td>
<td>
<p>Numerical value with your personal API key. Default setting draws
on the <code><a href="#topic+get_api_key">get_api_key()</a></code> to retrieve the API key from the R environment, so that the key is not
compromised. The API key can be added to the R environment via <code><a href="#topic+set_api_key">set_api_key()</a></code>
or by using <code><a href="usethis.html#topic+edit">usethis::edit_r_environ()</a></code>. In the <code>.Renviron</code> file, write <code>CHATGPT_KEY=INSERT_YOUR_KEY_HERE</code>.
After entering the API key, close and save the <code>.Renviron</code> file and restart <code>RStudio</code> (ctrl + shift + F10).
Alternatively, one can use <code><a href="httr2.html#topic+secrets">httr2::secret_make_key()</a></code>, <code><a href="httr2.html#topic+secrets">httr2::secret_encrypt()</a></code>, and
<code><a href="httr2.html#topic+secrets">httr2::secret_decrypt()</a></code> to scramble and decrypt the API key.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> including variables with information about the model used,
the number of requests and tokens per minute.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set_api_key()

rate_limits_per_minute()

## End(Not run)
</code></pre>

<hr>
<h2 id='sample_references'>Random sample references</h2><span id='topic+sample_references'></span>

<h3>Description</h3>

<p><code>sample_references</code>samples n rows from the dataset with titles and abstracts either with or without replacement.
This function is supposed to support the construct of a test dataset,
as suggested by <a href="https://osf.io/preprints/osf/yrhzm">Vembye et al. (2024)</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_references(
  data,
  n,
  with_replacement = FALSE,
  prob_vec = rep(1/n, nrow(data))
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sample_references_+3A_data">data</code></td>
<td>
<p>Dataset containing the titles and abstracts wanted to be screened.</p>
</td></tr>
<tr><td><code id="sample_references_+3A_n">n</code></td>
<td>
<p>A non-negative integer giving the number of rows to choose.</p>
</td></tr>
<tr><td><code id="sample_references_+3A_with_replacement">with_replacement</code></td>
<td>
<p>Logical indicating if sampling should be done with of without replacement.
Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="sample_references_+3A_prob_vec">prob_vec</code></td>
<td>
<p>'A vector of probability weights for obtaining the elements of the vector being sampled.'
Default is a vector of 1/n.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset with n rows.
</p>


<h3>References</h3>

<p>Vembye, M. H., Christensen, J., Mølgaard, A. B., &amp; Schytt, F. L. W. (2024)
<em>GPT API Models Can Function as Highly Reliable Second Screeners of Titles and Abstracts in Systematic Reviews:
A Proof of Concept and Common Guidelines</em> <a href="https://osf.io/preprints/osf/yrhzm">https://osf.io/preprints/osf/yrhzm</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
excl_test_dat &lt;- filges2015_dat[1:200,] |&gt; sample_references(100)

</code></pre>

<hr>
<h2 id='screen_analyzer'>Analyze performance between the human and AI screening.</h2><span id='topic+screen_analyzer'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a><br>
<br>
When both the human and AI title and abstract screening has been done, this function
allows you to calculate performance measures of the screening, including the overall
accuracy, specificity and sensitivity of the screening, as well as
inter-rater reliability kappa statistics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>screen_analyzer(x, human_decision = human_code, key_result = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="screen_analyzer_+3A_x">x</code></td>
<td>
<p>An object of either class<code>'gpt'</code> or <code>'chatgpt'</code>
or a dataset of either class <code>'gpt_tbl'</code>, <code>'chatgpt_tbl'</code>, or <code>'gpt_agg_tbl'</code></p>
</td></tr>
<tr><td><code id="screen_analyzer_+3A_human_decision">human_decision</code></td>
<td>
<p>Indicate the variable in the data that contains the human_decision.
This variable must be numeric, containing 1 (for included references) and 0 (for excluded references) only.</p>
</td></tr>
<tr><td><code id="screen_analyzer_+3A_key_result">key_result</code></td>
<td>
<p>Logical indicating if only the raw agreement, recall, and specificity measures should be returned.
Default is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> with screening performance measures. The <code>tibble</code> includes the following variables:
</p>

<table>
<tr>
 <td style="text-align: left;">
<b>promptid</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the prompt ID. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>model</b> </td><td style="text-align: left;"> <code>character</code>   </td><td style="text-align: left;"> indicating the specific gpt-model used. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>reps</b>  </td><td style="text-align: left;"> <code>integer</code>  </td><td style="text-align: left;"> indicating the number of times the same question was sent to GPT server. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>top_p</b> </td><td style="text-align: left;"> <code>numeric</code>  </td><td style="text-align: left;"> indicating the applied top_p. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>n_screened</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the number of screened references. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>n_missing</b> </td><td style="text-align: left;"> <code>numeric</code> </td><td style="text-align: left;"> indicating the number of missing responses. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>n_refs</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the total number of references expected to be screened for the given condition. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>human_in_gpt_ex</b> </td><td style="text-align: left;"> <code>numeric</code>  </td><td style="text-align: left;"> indicating the number of references included by humans and excluded by gpt. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>human_ex_gpt_in</b> </td><td style="text-align: left;"> <code>numeric</code>  </td><td style="text-align: left;"> indicating the number of references excluded by humans and included by gpt. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>human_in_gpt_in</b> </td><td style="text-align: left;"> <code>numeric</code>  </td><td style="text-align: left;"> indicating the number of references included by humans and included by gpt. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>human_ex_gpt_ex</b> </td><td style="text-align: left;"> <code>numeric</code>  </td><td style="text-align: left;"> indicating the number of references excluded by humans and excluded by gpt. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>accuracy</b> </td><td style="text-align: left;"> <code>numeric</code>  </td><td style="text-align: left;"> indicating the overall percent disagreement between human and gpt (Gartlehner et al., 2019). </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>p_agreement</b> </td><td style="text-align: left;"> <code>numeric</code> </td><td style="text-align: left;"> indicating the overall percent agreement between human and gpt. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>precision</b>  </td><td style="text-align: left;"> <code>numeric</code>  </td><td style="text-align: left;"> "measures the ability to include only articles that should be included" (Syriani et al., 2023). </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>recall</b>  </td><td style="text-align: left;"> <code>numeric</code> </td><td style="text-align: left;"> "measures the ability to include all articles that should be included" (Syriani et al., 2023). </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>npv</b>  </td><td style="text-align: left;"> <code>numeric</code>  </td><td style="text-align: left;"> Negative predictive value (NPV) "measures the ability to exclude only articles that should be excluded" (Syriani et al., 2023). </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>specificity</b>  </td><td style="text-align: left;"> <code>numeric</code> </td><td style="text-align: left;"> "measures the ability to exclude all articles that should be excluded" (Syriani et al., 2023). </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>bacc</b>  </td><td style="text-align: left;"> <code>numeric</code>  </td><td style="text-align: left;"> "capture the accuracy of deciding both inclusion and exclusion classes" (Syriani et al., 2023). </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>F2</b>  </td><td style="text-align: left;"> <code>numeric</code> </td><td style="text-align: left;"> F-measure that "consider the cost of getting false negatives twice as costly as getting false positives" (Syriani et al., 2023). </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>mcc</b>  </td><td style="text-align: left;"> <code>numeric</code> </td><td style="text-align: left;"> indicating percent agreement for excluded references (Gartlehner et al., 2019). </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>irr</b>  </td><td style="text-align: left;"> <code>numeric</code>  </td><td style="text-align: left;"> indicating the inter-rater reliability as described in McHugh (2012). </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>se_irr</b> </td><td style="text-align: left;"> <code>numeric</code> </td><td style="text-align: left;"> indicating standard error for the inter-rater reliability. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>cl_irr</b> </td><td style="text-align: left;"> <code>numeric</code> </td><td style="text-align: left;"> indicating lower confidence interval for the inter-rater reliability. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>cu_irr</b> </td><td style="text-align: left;"> <code>numeric</code> </td><td style="text-align: left;"> indicating upper confidence interval for the inter-rater reliability. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>level_of_agreement</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> interpretation of the inter-rater reliability as suggested by McHugh (2012). </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>References</h3>

<p>Gartlehner, G., Wagner, G., Lux, L., Affengruber, L., Dobrescu, A., Kaminski-Hartenthaler, A., &amp; Viswanathan, M. (2019).
Assessing the accuracy of machine-assisted abstract screening with DistillerAI: a user study.
<em>Systematic Reviews</em>, 8(1), 277. <a href="https://doi.org/10.1186/s13643-019-1221-3">doi:10.1186/s13643-019-1221-3</a>
</p>
<p>McHugh, M. L. (2012).
Interrater reliability: The kappa statistic.
<em>Biochemia Medica</em>, 22(3), 276-282. <a href="https://pubmed.ncbi.nlm.nih.gov/23092060/">https://pubmed.ncbi.nlm.nih.gov/23092060/</a>
</p>
<p>Syriani, E., David, I., &amp; Kumar, G. (2023).
Assessing the Ability of ChatGPT to Screen Articles for Systematic Reviews.
<em>ArXiv Preprint ArXiv:2307.06464</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(future)

set_api_key()

prompt &lt;- "Is this study about a Functional Family Therapy (FFT) intervention?"

plan(multisession)

res &lt;- tabscreen_gpt(
  data = filges2015_dat[1:2,],
  prompt = prompt,
  studyid = studyid,
  title = title,
  abstract = abstract
  )

plan(sequential)

res |&gt; screen_analyzer()


## End(Not run)
</code></pre>

<hr>
<h2 id='screen_errors'>Generic function to re-screen failed title and abstract requests.</h2><span id='topic+screen_errors'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a><br>
<br>
</p>
<p>This is a generic function to re-screen of failed title and abstract requests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>screen_errors(
  object,
  api_key = get_api_key(),
  max_tries = 4,
  max_seconds,
  is_transient,
  backoff,
  after,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="screen_errors_+3A_object">object</code></td>
<td>
<p>An object of either class <code>'gpt'</code> or <code>'chatgpt'</code>.</p>
</td></tr>
<tr><td><code id="screen_errors_+3A_api_key">api_key</code></td>
<td>
<p>Numerical value with your personal API key. Default setting draws
on the <code><a href="#topic+get_api_key">get_api_key()</a></code> to retrieve the API key from the R environment, so that the key is not
compromised. The API key can be added to the R environment via <code><a href="#topic+set_api_key">set_api_key()</a></code>
or by using <code><a href="usethis.html#topic+edit">usethis::edit_r_environ()</a></code>. In the <code>.Renviron</code> file, write <code>CHATGPT_KEY=INSERT_YOUR_KEY_HERE</code>.
After entering the API key, close and save the <code>.Renviron</code> file and restart <code>RStudio</code> (ctrl + shift + F10).
Alternatively, one can use <code><a href="httr2.html#topic+secrets">httr2::secret_make_key()</a></code>, <code><a href="httr2.html#topic+secrets">httr2::secret_encrypt()</a></code>, and
<code><a href="httr2.html#topic+secrets">httr2::secret_decrypt()</a></code> to scramble and decrypt the API key.</p>
</td></tr>
<tr><td><code id="screen_errors_+3A_max_tries">max_tries</code>, <code id="screen_errors_+3A_max_seconds">max_seconds</code></td>
<td>
<p>'Cap the maximum number of attempts with
<code>max_tries</code> or the total elapsed time from the first request with
<code>max_seconds</code>. If neither option is supplied (the default), <code><a href="httr2.html#topic+req_perform">httr2::req_perform()</a></code>
will not retry' (Wickham, 2023). Default <code>max_tries</code> is 16. If missing, the value of <code>max_seconds</code>
from the original screening conducted with <code><a href="#topic+tabscreen_gpt">tabscreen_gpt()</a></code> will be used.</p>
</td></tr>
<tr><td><code id="screen_errors_+3A_is_transient">is_transient</code></td>
<td>
<p>'A predicate function that takes a single argument
(the response) and returns <code>TRUE</code> or <code>FALSE</code> specifying whether or not
the response represents a transient error' (Wickham, 2023). If missing, the <code>is_transient</code>
function from the original screening conducted with <code><a href="#topic+tabscreen_gpt">tabscreen_gpt()</a></code> will be used.</p>
</td></tr>
<tr><td><code id="screen_errors_+3A_backoff">backoff</code></td>
<td>
<p>'A function that takes a single argument (the number of failed
attempts so far) and returns the number of seconds to wait' (Wickham, 2023).
If missing, the <code>backoff</code>value from the original screening conducted with <code><a href="#topic+tabscreen_gpt">tabscreen_gpt()</a></code> will be used.</p>
</td></tr>
<tr><td><code id="screen_errors_+3A_after">after</code></td>
<td>
<p>'A function that takes a single argument (the response) and
returns either a number of seconds to wait or <code>NULL</code>, which indicates
that a precise wait time is not available that the <code>backoff</code> strategy
should be used instead' (Wickham, 2023). If missing, the <code>after</code> value
from the original screening conducted with <code><a href="#topic+tabscreen_gpt">tabscreen_gpt()</a></code> will be used.</p>
</td></tr>
<tr><td><code id="screen_errors_+3A_...">...</code></td>
<td>
<p>Further argument to pass to the request body. See <a href="https://platform.openai.com/docs/api-reference/chat/create">https://platform.openai.com/docs/api-reference/chat/create</a>.
If used in the original screening in <code><a href="#topic+tabscreen_gpt">tabscreen_gpt()</a></code>, the argument(s)
must be specified here again.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>'gpt'</code> or <code>'chatgpt'</code> similar to the object returned by <code><a href="#topic+tabscreen_gpt">tabscreen_gpt()</a></code>.
See documentation for <code><a href="#topic+tabscreen_gpt">tabscreen_gpt()</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+screen_errors.gpt">screen_errors.gpt()</a></code>, <code><a href="#topic+screen_errors.chatgpt">screen_errors.chatgpt()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

set_api_key()
prompt &lt;- "Is this study about a Functional Family Therapy (FFT) intervention?"

obj_with_error &lt;-
  tabscreen_gpt(
    data = filges2015_dat[1:2,],
    prompt = prompt,
    studyid = studyid,
    title = title,
    abstract = abstract,
    model = "gpt-4o-mini"
    )

obj_rescreened &lt;-
  obj_with_error |&gt;
  screen_error()


## End(Not run)

</code></pre>

<hr>
<h2 id='screen_errors.chatgpt'>Re-screen failed requests.</h2><span id='topic+screen_errors.chatgpt'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a><br>
<br>
</p>
<p>This function supports re-screening of all failed title and abstract requests
screened with <code><a href="#topic+tabscreen_gpt.original">tabscreen_gpt.original()</a></code>. This function has been deprecated because
OpenAI has deprecated the function_call and and functions argument that was used
in <code><a href="#topic+tabscreen_gpt.original">tabscreen_gpt.original()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'chatgpt'
screen_errors(
  object,
  ...,
  api_key = get_api_key(),
  max_tries = 4,
  max_seconds,
  is_transient,
  backoff,
  after
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="screen_errors.chatgpt_+3A_object">object</code></td>
<td>
<p>An object of class <code>'chatgpt'</code>.</p>
</td></tr>
<tr><td><code id="screen_errors.chatgpt_+3A_...">...</code></td>
<td>
<p>Further argument to pass to the request body.
See <a href="https://platform.openai.com/docs/api-reference/chat/create">https://platform.openai.com/docs/api-reference/chat/create</a>.
If used in the original screening (e.g., with <code><a href="#topic+tabscreen_gpt.original">tabscreen_gpt.original()</a></code>), the argument(s)
must be specified again here.</p>
</td></tr>
<tr><td><code id="screen_errors.chatgpt_+3A_api_key">api_key</code></td>
<td>
<p>Numerical value with your personal API key.</p>
</td></tr>
<tr><td><code id="screen_errors.chatgpt_+3A_max_tries">max_tries</code>, <code id="screen_errors.chatgpt_+3A_max_seconds">max_seconds</code></td>
<td>
<p>'Cap the maximum number of attempts with
<code>max_tries</code> or the total elapsed time from the first request with
<code>max_seconds</code>. If neither option is supplied (the default), <code><a href="httr2.html#topic+req_perform">httr2::req_perform()</a></code>
will not retry' (Wickham, 2023). Default <code>max_tries</code> is 4. If missing, the value of <code>max_seconds</code>
from the original screening (e.g., conducted with <code><a href="#topic+tabscreen_gpt.original">tabscreen_gpt.original()</a></code>) will be used.</p>
</td></tr>
<tr><td><code id="screen_errors.chatgpt_+3A_is_transient">is_transient</code></td>
<td>
<p>'A predicate function that takes a single argument
(the response) and returns <code>TRUE</code> or <code>FALSE</code> specifying whether or not
the response represents a transient error' (Wickham, 2023). If missing, the <code>is_transient</code>
function from the original screening (e.g., conducted with <code><a href="#topic+tabscreen_gpt.original">tabscreen_gpt.original()</a></code>) will be used.</p>
</td></tr>
<tr><td><code id="screen_errors.chatgpt_+3A_backoff">backoff</code></td>
<td>
<p>'A function that takes a single argument (the number of failed
attempts so far) and returns the number of seconds to wait' (Wickham, 2023).
If missing, the <code>backoff</code>value from the original screening (e.g., conducted
with <code><a href="#topic+tabscreen_gpt.original">tabscreen_gpt.original()</a></code>) will be used.</p>
</td></tr>
<tr><td><code id="screen_errors.chatgpt_+3A_after">after</code></td>
<td>
<p>'A function that takes a single argument (the response) and
returns either a number of seconds to wait or <code>NULL</code>, which indicates
that a precise wait time is not available that the <code>backoff</code> strategy
should be used instead' (Wickham, 2023). If missing, the <code>after</code> value
from the original screening (e.g., conducted with <code><a href="#topic+tabscreen_gpt.original">tabscreen_gpt.original()</a></code>) will be used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>'chatgpt'</code> similar to the object returned by <code><a href="#topic+tabscreen_gpt.original">tabscreen_gpt.original()</a></code>.
See documentation value for <code><a href="#topic+tabscreen_gpt.original">tabscreen_gpt.original()</a></code>.
</p>


<h3>References</h3>

<p>Wickham H (2023).
<em>httr2: Perform HTTP Requests and Process the Responses</em>.
https://httr2.r-lib.org, https://github.com/r-lib/httr2.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tabscreen_gpt.original">tabscreen_gpt.original()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

set_api_key()

prompt &lt;- "Is this study about a Functional Family Therapy (FFT) intervention?"

obj_with_error &lt;-
  tabscreen_gpt(
    data = filges2015_dat[1:2,],
    prompt = prompt,
    studyid = studyid,
    title = title,
    abstract = abstract,
    model = c("gpt-3.5-turbo-0613", "gpt-3.5-turbo-16k-0613"),
    max_tries = 1,
    reps = 10
    )

obj_rescreened &lt;-
  obj_with_error |&gt;
  screen_error()

# Alternatively re-set max_tries if errors still appear
obj_rescreened &lt;-
  obj_with_error |&gt;
  screen_error(max_tries = 16)

## End(Not run)

</code></pre>

<hr>
<h2 id='screen_errors.gpt'>Re-screen failed requests.</h2><span id='topic+screen_errors.gpt'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a><br>
<br>
</p>
<p>This function supports re-screening of all failed title and abstract requests
screened with <code><a href="#topic+tabscreen_gpt">tabscreen_gpt()</a></code>/<code><a href="#topic+tabscreen_gpt.tools">tabscreen_gpt.tools()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gpt'
screen_errors(
  object,
  api_key = get_api_key(),
  max_tries = 16,
  max_seconds,
  is_transient,
  backoff,
  after,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="screen_errors.gpt_+3A_object">object</code></td>
<td>
<p>An object of class <code>'gpt'</code>.</p>
</td></tr>
<tr><td><code id="screen_errors.gpt_+3A_api_key">api_key</code></td>
<td>
<p>Numerical value with your personal API key. Default setting draws
on the <code><a href="#topic+get_api_key">get_api_key()</a></code> to retrieve the API key from the R environment, so that the key is not
compromised. The API key can be added to the R environment via <code><a href="#topic+set_api_key">set_api_key()</a></code>
or by using <code><a href="usethis.html#topic+edit">usethis::edit_r_environ()</a></code>. In the <code>.Renviron</code> file, write <code>CHATGPT_KEY=INSERT_YOUR_KEY_HERE</code>.
After entering the API key, close and save the <code>.Renviron</code> file and restart <code>RStudio</code> (ctrl + shift + F10).
Alternatively, one can use <code><a href="httr2.html#topic+secrets">httr2::secret_make_key()</a></code>, <code><a href="httr2.html#topic+secrets">httr2::secret_encrypt()</a></code>, and
<code><a href="httr2.html#topic+secrets">httr2::secret_decrypt()</a></code> to scramble and decrypt the API key.</p>
</td></tr>
<tr><td><code id="screen_errors.gpt_+3A_max_tries">max_tries</code>, <code id="screen_errors.gpt_+3A_max_seconds">max_seconds</code></td>
<td>
<p>'Cap the maximum number of attempts with
<code>max_tries</code> or the total elapsed time from the first request with
<code>max_seconds</code>. If neither option is supplied (the default), <code><a href="httr2.html#topic+req_perform">httr2::req_perform()</a></code>
will not retry' (Wickham, 2023). Default <code>max_tries</code> is 16. If missing, the value of <code>max_seconds</code>
from the original screening conducted with <code><a href="#topic+tabscreen_gpt">tabscreen_gpt()</a></code> will be used.</p>
</td></tr>
<tr><td><code id="screen_errors.gpt_+3A_is_transient">is_transient</code></td>
<td>
<p>'A predicate function that takes a single argument
(the response) and returns <code>TRUE</code> or <code>FALSE</code> specifying whether or not
the response represents a transient error' (Wickham, 2023). If missing, the <code>is_transient</code>
function from the original screening conducted with <code><a href="#topic+tabscreen_gpt">tabscreen_gpt()</a></code> will be used.</p>
</td></tr>
<tr><td><code id="screen_errors.gpt_+3A_backoff">backoff</code></td>
<td>
<p>'A function that takes a single argument (the number of failed
attempts so far) and returns the number of seconds to wait' (Wickham, 2023).
If missing, the <code>backoff</code>value from the original screening conducted with <code><a href="#topic+tabscreen_gpt">tabscreen_gpt()</a></code> will be used.</p>
</td></tr>
<tr><td><code id="screen_errors.gpt_+3A_after">after</code></td>
<td>
<p>'A function that takes a single argument (the response) and
returns either a number of seconds to wait or <code>NULL</code>, which indicates
that a precise wait time is not available that the <code>backoff</code> strategy
should be used instead' (Wickham, 2023). If missing, the <code>after</code> value
from the original screening conducted with <code><a href="#topic+tabscreen_gpt">tabscreen_gpt()</a></code> will be used.</p>
</td></tr>
<tr><td><code id="screen_errors.gpt_+3A_...">...</code></td>
<td>
<p>Further argument to pass to the request body. See <a href="https://platform.openai.com/docs/api-reference/chat/create">https://platform.openai.com/docs/api-reference/chat/create</a>.
If used in the original screening in <code><a href="#topic+tabscreen_gpt">tabscreen_gpt()</a></code>, the argument(s)
must be specified again here.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>'gpt'</code> similar to the object returned by <code><a href="#topic+tabscreen_gpt">tabscreen_gpt()</a></code>.
See documentation for <code><a href="#topic+tabscreen_gpt">tabscreen_gpt()</a></code>.
</p>


<h3>References</h3>

<p>Wickham H (2023).
<em>httr2: Perform HTTP Requests and Process the Responses</em>.
https://httr2.r-lib.org, https://github.com/r-lib/httr2.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tabscreen_gpt">tabscreen_gpt()</a></code>, <code><a href="#topic+tabscreen_gpt.tools">tabscreen_gpt.tools()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
prompt &lt;- "Is this study about a Functional Family Therapy (FFT) intervention?"

obj_with_error &lt;-
  tabscreen_gpt(
    data = filges2015_dat[1:10,],
    prompt = prompt,
    studyid = studyid,
    title = title,
    abstract = abstract,
    model = "gpt-4o"
    )

obj_rescreened &lt;-
  obj_with_error |&gt;
  screen_error()


## End(Not run)

</code></pre>

<hr>
<h2 id='set_api_key'>Creating a temporary R environment API key variable</h2><span id='topic+set_api_key'></span>

<h3>Description</h3>

<p>This function automatically sets/creates an interim R environment variable with
the API key to call a given AI model (e.g. ChatGPT). Thereby users avoid exposing their API keys.
If the API key is set in the console, it will/can be revealed via the .Rhistory.
Find more information about this issue at <a href="https://httr2.r-lib.org/articles/wrapping-apis.html">https://httr2.r-lib.org/articles/wrapping-apis.html</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_api_key(key, env_var = "CHATGPT_KEY")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="set_api_key_+3A_key">key</code></td>
<td>
<p>Character string with an (ideally encrypt) API key. See how to encrypt key here:
<a href="https://httr2.r-lib.org/articles/wrapping-apis.html#basics">https://httr2.r-lib.org/articles/wrapping-apis.html#basics</a>. If not provided, it
returns a password box in which the true API key can be secretly entered.</p>
</td></tr>
<tr><td><code id="set_api_key_+3A_env_var">env_var</code></td>
<td>
<p>Character string indicating the name of the temporary R environment variable with
the API key and the used AI model. Currently, the argument only takes <code>env_var = "CHATGPT_KEY"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When set_api_key() has successfully been executed, <code>get_api_key()</code> automatically
retrieves the API key from the R environment and the users do not need to specify the API when running
functions from the package that call the API. The API key can be permanently set by
using <code><a href="usethis.html#topic+edit">usethis::edit_r_environ()</a></code>. Then write <code style="white-space: pre;">&#8288;CHATGPT_KEY=[insert your api key here]&#8288;</code> and close
the <code>.Renviron</code> window and restart R.
</p>


<h3>Value</h3>

<p>A temporary environment variable with the name from <code>env_var</code>.
If <code>key</code> is missing, it returns a password box in which the true API key can be entered.
</p>


<h3>Note</h3>

<p>Find your personal API key at <a href="https://platform.openai.com/account/api-keys">https://platform.openai.com/account/api-keys</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get_api_key">get_api_key</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set_api_key()

## End(Not run)
</code></pre>

<hr>
<h2 id='tabscreen_gpt.original'>Title and abstract screening with GPT API models using function calls via the original function call arguments</h2><span id='topic+tabscreen_gpt.original'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a><br>
<br>
</p>
<p>This function has been deprecated (but can still be used) because
OpenAI has deprecated the function_call and and functions argument which is
used in this function. Instead use the <code><a href="#topic+tabscreen_gpt.tools">tabscreen_gpt.tools()</a></code> that handles
the function calling via the tools and tool_choice arguments.
<br>

</p>
<p>This function supports the conduct of title and abstract screening with GPT API models in R.
This function only works with GPT-4, more specifically gpt-4-0613. To draw on other models,
use <code><a href="#topic+tabscreen_gpt.tools">tabscreen_gpt.tools()</a></code>.
The function allows to run title and abstract screening across multiple prompts and with
repeated questions to check for consistency across answers. This function draws
on the newly developed function calling to better steer the output of the responses.
This function was used in <a href="https://osf.io/preprints/osf/yrhzm">Vembye et al. (2024)</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tabscreen_gpt.original(
  data,
  prompt,
  studyid,
  title,
  abstract,
  ...,
  model = "gpt-4",
  role = "user",
  functions = incl_function_simple,
  function_call_name = list(name = "inclusion_decision_simple"),
  top_p = 1,
  time_info = TRUE,
  token_info = TRUE,
  api_key = get_api_key(),
  max_tries = 16,
  max_seconds = NULL,
  is_transient = gpt_is_transient,
  backoff = NULL,
  after = NULL,
  rpm = 10000,
  reps = 1,
  seed_par = NULL,
  progress = TRUE,
  messages = TRUE,
  incl_cutoff_upper = 0.5,
  incl_cutoff_lower = incl_cutoff_upper - 0.1,
  force = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tabscreen_gpt.original_+3A_data">data</code></td>
<td>
<p>Dataset containing the titles and abstracts.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_prompt">prompt</code></td>
<td>
<p>Prompt(s) to be added before the title and abstract.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_studyid">studyid</code></td>
<td>
<p>Unique Study ID. If missing, this is generated
automatically.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_title">title</code></td>
<td>
<p>Name of the variable containing the title information.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_abstract">abstract</code></td>
<td>
<p>Name of variable containing the abstract information.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_...">...</code></td>
<td>
<p>Further argument to pass to the request body.
See <a href="https://platform.openai.com/docs/api-reference/chat/create">https://platform.openai.com/docs/api-reference/chat/create</a>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_model">model</code></td>
<td>
<p>Character string with the name of the completion model. Can take
multiple models, including gpt-4 models. Default = <code>"gpt-4"</code> (i.e., gpt-4-0613). This model has
been shown to outperform the gpt-3.5-turbo models in terms of its ability to detect
relevant studies (Vembye et al., Under preparation).
Find available model at
<a href="https://platform.openai.com/docs/models/model-endpoint-compatibility">https://platform.openai.com/docs/models/model-endpoint-compatibility</a>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_role">role</code></td>
<td>
<p>Character string indicate the role of the user. Default is <code>"user"</code>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_functions">functions</code></td>
<td>
<p>Function to steer output. Default is <code>incl_function_simple</code>.
To get detailed responses use the hidden function call <code>incl_function</code> from the package. Also see 'Examples below.
Find further documentation for function calling at
<a href="https://openai.com/blog/function-calling-and-other-api-updates">https://openai.com/blog/function-calling-and-other-api-updates</a>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_function_call_name">function_call_name</code></td>
<td>
<p>Functions to call.
Default is <code>list(name = "inclusion_decision_simple")</code>. To get detailed responses
use <code>list(name = "inclusion_decision")</code>. Also see 'Examples below.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_top_p">top_p</code></td>
<td>
<p>'An alternative to sampling with temperature, called nucleus sampling,
where the model considers the results of the tokens with top_p probability mass.
So 0.1 means only the tokens comprising the top 10% probability mass are considered.
We generally recommend altering this or temperature but not both.' (OPEN-AI). Default is 1.
Find documentation at
<a href="https://platform.openai.com/docs/api-reference/chat/create#chat/create-top_p">https://platform.openai.com/docs/api-reference/chat/create#chat/create-top_p</a>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_time_info">time_info</code></td>
<td>
<p>Logical indicating whether the run time of each
request/question should be included in the data. Default = <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_token_info">token_info</code></td>
<td>
<p>Logical indicating whether the number of prompt and completion tokens
per request should be included in the output data. Default = <code>TRUE</code>. When <code>TRUE</code>,
the output object will include price information of the conducted screening.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_api_key">api_key</code></td>
<td>
<p>Numerical value with your personal API key. Find at
<a href="https://platform.openai.com/account/api-keys">https://platform.openai.com/account/api-keys</a>. Use
<code><a href="httr2.html#topic+secrets">httr2::secret_make_key()</a></code>, <code><a href="httr2.html#topic+secrets">httr2::secret_encrypt()</a></code>, and
<code><a href="httr2.html#topic+secrets">httr2::secret_decrypt()</a></code> to scramble and decrypt the api key and
use <code><a href="#topic+set_api_key">set_api_key()</a></code> to securely automate the use of the
api key by setting the api key as a locale environment variable.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_max_tries">max_tries</code>, <code id="tabscreen_gpt.original_+3A_max_seconds">max_seconds</code></td>
<td>
<p>'Cap the maximum number of attempts with
<code>max_tries</code> or the total elapsed time from the first request with
<code>max_seconds</code>. If neither option is supplied (the default), <code><a href="httr2.html#topic+req_perform">httr2::req_perform()</a></code>
will not retry' (Wickham, 2023).</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_is_transient">is_transient</code></td>
<td>
<p>'A predicate function that takes a single argument
(the response) and returns <code>TRUE</code> or <code>FALSE</code> specifying whether or not
the response represents a transient error' (Wickham, 2023).</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_backoff">backoff</code></td>
<td>
<p>'A function that takes a single argument (the number of failed
attempts so far) and returns the number of seconds to wait' (Wickham, 2023).</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_after">after</code></td>
<td>
<p>'A function that takes a single argument (the response) and
returns either a number of seconds to wait or <code>NULL</code>, which indicates
that a precise wait time is not available that the <code>backoff</code> strategy
should be used instead' (Wickham, 2023).</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_rpm">rpm</code></td>
<td>
<p>Numerical value indicating the number of requests per minute (rpm)
available for the specified api key. Find more information at
<a href="https://platform.openai.com/docs/guides/rate-limits/what-are-the-rate-limits-for-our-api">https://platform.openai.com/docs/guides/rate-limits/what-are-the-rate-limits-for-our-api</a>.
Alternatively, use <code><a href="#topic+rate_limits_per_minute">rate_limits_per_minute()</a></code>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_reps">reps</code></td>
<td>
<p>Numerical value indicating the number of times the same
question should be sent to OpenAI's GPT API models. This can be useful to test consistency
between answers. Default is <code>1</code> but when using 3.5 models, we recommend setting this
value to <code>10</code>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_seed_par">seed_par</code></td>
<td>
<p>Numerical value for a seed to ensure that proper,
parallel-safe random numbers are produced.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_progress">progress</code></td>
<td>
<p>Logical indicating whether a progress line should be shown when running
the title and abstract screening in parallel. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_messages">messages</code></td>
<td>
<p>Logical indicating whether to print messages embedded in the function.
Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_incl_cutoff_upper">incl_cutoff_upper</code></td>
<td>
<p>Numerical value indicating the probability threshold
for which a studies should be included. Default is 0.5, which indicates that
titles and abstracts that OpenAI's GPT API model has included more than 50 percent of the times
should be included.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_incl_cutoff_lower">incl_cutoff_lower</code></td>
<td>
<p>Numerical value indicating the probability threshold
above which studies should be check by a human. Default is 0.4, which means
that if you ask OpenAI's GPT API model the same questions 10 times and it includes the
title and abstract 4 times, we suggest that the study should be check by a human.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.original_+3A_force">force</code></td>
<td>
<p>Logical argument indicating whether to force the function to use more than
10 iterations for gpt-3.5 models and more than 1 iteration for gpt-4 models.
This argument is developed to avoid the conduct of wrong and extreme sized screening.
Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"chatgpt"</code>. The object is a list containing the following
components:
</p>
<table role = "presentation">
<tr><td><code>answer_data_sum</code></td>
<td>
<p>dataset with the summarized, probabilistic inclusion decision
for each title and abstract across multiple repeated questions.</p>
</td></tr>
<tr><td><code>answer_data_all</code></td>
<td>
<p>dataset with all individual answers.</p>
</td></tr>
<tr><td><code>price</code></td>
<td>
<p>numerical value indicating the total price (in USD) of the screening.</p>
</td></tr>
<tr><td><code>price_data</code></td>
<td>
<p>dataset with prices across all gpt models used for screening.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The <code>answer_data_sum</code> data contains the following mandatory variables:
</p>

<table>
<tr>
 <td style="text-align: left;">
<b>studyid</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the study ID of the reference. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>title</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the title of the reference. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>abstract</b> </td><td style="text-align: left;"> <code>character</code>   </td><td style="text-align: left;"> indicating the abstract of the reference. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>promptid</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the prompt ID. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>prompt</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the prompt. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>model</b> </td><td style="text-align: left;"> <code>character</code>   </td><td style="text-align: left;"> indicating the specific gpt-model used. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>question</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the final question sent to OpenAI's GPT API models. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>top_p</b> </td><td style="text-align: left;"> <code>numeric</code>  </td><td style="text-align: left;"> indicating the applied top_p. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>incl_p</b> </td><td style="text-align: left;"> <code>numeric</code>  </td><td style="text-align: left;"> indicating the probability of inclusion calculated across multiple repeated responses on the same title and abstract. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>final_decision_gpt</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the final decision reached by gpt - either 'Include', 'Exclude', or 'Check'. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>final_decision_gpt_num</b>  </td><td style="text-align: left;"> <code>integer</code>  </td><td style="text-align: left;"> indicating the final numeric decision reached by gpt - either 1 or 0. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>longest_answer</b>  </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the longest gpt response obtained
across multiple repeated responses on the same title and abstract. Only included if the detailed function calling
function is used. See 'Examples' below for how to use this function. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>reps</b>  </td><td style="text-align: left;"> <code>integer</code>  </td><td style="text-align: left;"> indicating the number of times the same question has been sent to OpenAI's GPT API models. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>n_mis_answers</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the number of missing responses. </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<br>
<p>The <code>answer_data_all</code> data contains the following mandatory variables:
</p>

<table>
<tr>
 <td style="text-align: left;">
<b>studyid</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the study ID of the reference. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>title</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the title of the reference. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>abstract</b> </td><td style="text-align: left;"> <code>character</code>   </td><td style="text-align: left;"> indicating the abstract of the reference. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>promptid</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the prompt ID. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>prompt</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the prompt. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>model</b> </td><td style="text-align: left;"> <code>character</code>   </td><td style="text-align: left;"> indicating the specific gpt-model used. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>iterations</b> </td><td style="text-align: left;"> <code>numeric</code> </td><td style="text-align: left;"> indicating the number of times the same question has been sent to OpenAI's GPT API models. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>question</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the final question sent to OpenAI's GPT API models. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>top_p</b>  </td><td style="text-align: left;"> <code>numeric</code> </td><td style="text-align: left;"> indicating the applied top_p. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>decision_gpt</b>  </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the raw gpt decision - either <code>"1", "0", "1.1"</code> for inclusion, exclusion, or uncertainty, respectively. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>detailed_description</b>  </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating detailed description of the given decision made by OpenAI's GPT API models.
Only included if the detailed function calling function is used. See 'Examples' below for how to use this function. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>decision_binary</b>  </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the binary gpt decision,
that is 1 for inclusion and 0 for exclusion. 1.1 decision are coded equal to 1 in this case. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>prompt_tokens</b>  </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the number of prompt tokens sent to the server for the given request. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>completion_tokens</b>  </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the number of completion tokens sent to the server for the given request. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>run_time</b>  </td><td style="text-align: left;"> <code>numeric</code> </td><td style="text-align: left;"> indicating the time it took to obtain a response from the server for the given request. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>n</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating request ID.  </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<br>
<p>If any requests failed to reach the server, the <code>chatgpt</code> object contains an
error data set (<code>error_data</code>) having the same variables as <code>answer_data_all</code>
but with failed request references only.
<br>

</p>
<p>The <code>price_data</code> data contains the following variables:
</p>

<table>
<tr>
 <td style="text-align: left;">
<b>model</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> gpt model. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>input_price_dollar</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> price for all prompt/input tokens for the correspondent gpt-model. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>output_price_dollar</b>  </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> price for all completion/output tokens for the correspondent gpt-model. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>price_total_dollar</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> total price for all tokens for the correspondent gpt-model. </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Find current token pricing at <a href="https://openai.com/pricing">https://openai.com/pricing</a>.
</p>


<h3>References</h3>

<p>Vembye, M. H., Christensen, J., Mølgaard, A. B., &amp; Schytt, F. L. W. (2024)
<em>GPT API Models Can Function as Highly Reliable Second Screeners of Titles and Abstracts in Systematic Reviews:
A Proof of Concept and Common Guidelines</em> <a href="https://osf.io/preprints/osf/yrhzm">https://osf.io/preprints/osf/yrhzm</a>
</p>
<p>Wickham H (2023).
<em>httr2: Perform HTTP Requests and Process the Responses</em>.
https://httr2.r-lib.org, https://github.com/r-lib/httr2.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

set_api_key()

prompt &lt;- "Is this study about a Functional Family Therapy (FFT) intervention?"

tabscreen_gpt.original(
  data = filges2015_dat[1:2,],
  prompt = prompt,
  studyid = studyid,
  title = title,
  abstract = abstract,
  max_tries = 2
  )

 # Get detailed descriptions of the gpt decisions by using the
 # embedded function calling functions from the package. See example below.
 tabscreen_gpt.original(
   data = filges2015_dat[1:2,],
   prompt = prompt,
   studyid = studyid,
   title = title,
   abstract = abstract,
   functions = incl_function,
   function_call_name = list(name = "inclusion_decision"),
   max_tries = 2
 )

## End(Not run)
</code></pre>

<hr>
<h2 id='tabscreen_gpt.tools'>Title and abstract screening with GPT API models using function calls via the tools argument</h2><span id='topic+tabscreen_gpt.tools'></span><span id='topic+tabscreen_gpt'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#stable"><img src="../help/figures/lifecycle-stable.svg" alt='[Stable]' /></a><br>
<br>
This function supports the conduct of title and abstract screening with GPT API models in R.
Specifically, it allows the user to draw on GPT-3.5, GPT-4, GPT-4o, GPT-4o-mini, and fine-tuned models.
The function allows to run title and abstract screening across multiple prompts and with
repeated questions to check for consistency across answers. All of which can be done in parallel.
The function draws on the newly developed function calling which is called via the
tools argument in the request body. This is the main different between <code><a href="#topic+tabscreen_gpt.tools">tabscreen_gpt.tools()</a></code>
and <code><a href="#topic+tabscreen_gpt.original">tabscreen_gpt.original()</a></code>. Function calls ensure more reliable and consistent responses to ones
requests. See <a href="https://osf.io/preprints/osf/yrhzm">Vembye et al. (2024)</a>
for guidance on how adequately to conduct title and abstract screening with GPT models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tabscreen_gpt.tools(data, prompt, studyid, title, abstract,
   model = "gpt-4o-mini", role = "user", tools = NULL, tool_choice = NULL, top_p = 1,
   time_info = TRUE, token_info = TRUE, api_key = get_api_key(), max_tries = 16,
   max_seconds = NULL, is_transient = gpt_is_transient, backoff = NULL,
   after = NULL, rpm = 10000, reps = 1, seed_par = NULL, progress = TRUE,
   decision_description = FALSE, messages = TRUE, incl_cutoff_upper = NULL,
   incl_cutoff_lower = NULL, force = FALSE, fine_tuned = FALSE, ...)

tabscreen_gpt(data, prompt, studyid, title, abstract,
   model = "gpt-4o-mini", role = "user", tools = NULL, tool_choice = NULL, top_p = 1,
   time_info = TRUE, token_info = TRUE, api_key = get_api_key(), max_tries = 16,
   max_seconds = NULL, is_transient = gpt_is_transient, backoff = NULL,
   after = NULL, rpm = 10000, reps = 1, seed_par = NULL, progress = TRUE,
   decision_description = FALSE, messages = TRUE, incl_cutoff_upper = NULL,
   incl_cutoff_lower = NULL, force = FALSE, fine_tuned = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tabscreen_gpt.tools_+3A_data">data</code></td>
<td>
<p>Dataset containing the titles and abstracts.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_prompt">prompt</code></td>
<td>
<p>Prompt(s) to be added before the title and abstract.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_studyid">studyid</code></td>
<td>
<p>Unique Study ID. If missing, this is generated
automatically.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_title">title</code></td>
<td>
<p>Name of the variable containing the title information.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_abstract">abstract</code></td>
<td>
<p>Name of variable containing the abstract information.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_model">model</code></td>
<td>
<p>Character string with the name of the completion model. Can take
multiple models. Default is the latest <code>"gpt-4o-mini"</code>.
Find available model at
<a href="https://platform.openai.com/docs/models/model-endpoint-compatibility">https://platform.openai.com/docs/models/model-endpoint-compatibility</a>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_role">role</code></td>
<td>
<p>Character string indicating the role of the user. Default is <code>"user"</code>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_tools">tools</code></td>
<td>
<p>This argument allows this user to apply customized functions.
See <a href="https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools">https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools</a>.
Default is <code>NULL</code>. If not specified the default function calls from <code>AIscreenR</code> are used.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_tool_choice">tool_choice</code></td>
<td>
<p>If a customized function is provided this argument
'controls which (if any) tool is called by the model' (OpenAI). Default is <code>NULL</code>.
If set to <code>NULL</code> when using a customized function, the default is <code>"auto"</code>.
See <a href="https://platform.openai.com/docs/api-reference/chat/create#chat-create-tool_choice">https://platform.openai.com/docs/api-reference/chat/create#chat-create-tool_choice</a>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_top_p">top_p</code></td>
<td>
<p>'An alternative to sampling with temperature, called nucleus sampling,
where the model considers the results of the tokens with top_p probability mass.
So 0.1 means only the tokens comprising the top 10% probability mass are considered.
We generally recommend altering this or temperature but not both.' (OpenAI). Default is 1.
Find documentation at
<a href="https://platform.openai.com/docs/api-reference/chat/create#chat/create-top_p">https://platform.openai.com/docs/api-reference/chat/create#chat/create-top_p</a>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_time_info">time_info</code></td>
<td>
<p>Logical indicating whether the run time of each
request/question should be included in the data. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_token_info">token_info</code></td>
<td>
<p>Logical indicating whether token information should be included
in the output data. Default is <code>TRUE</code>. When <code>TRUE</code>, the output object will
include price information of the conducted screening.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_api_key">api_key</code></td>
<td>
<p>Numerical value with your personal API key. Default setting draws
on the <code><a href="#topic+get_api_key">get_api_key()</a></code> to retrieve the API key from the R environment, so that the key is not
compromised. The API key can be added to the R environment via <code><a href="#topic+set_api_key">set_api_key()</a></code>
or by using <code><a href="usethis.html#topic+edit">usethis::edit_r_environ()</a></code>. In the <code>.Renviron</code> file, write <code>CHATGPT_KEY=INSERT_YOUR_KEY_HERE</code>.
After entering the API key, close and save the <code>.Renviron</code> file and restart <code>RStudio</code> (ctrl + shift + F10).
Alternatively, one can use <code><a href="httr2.html#topic+secrets">httr2::secret_make_key()</a></code>, <code><a href="httr2.html#topic+secrets">httr2::secret_encrypt()</a></code>, and
<code><a href="httr2.html#topic+secrets">httr2::secret_decrypt()</a></code> to scramble and decrypt the API key.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_max_tries">max_tries</code>, <code id="tabscreen_gpt.tools_+3A_max_seconds">max_seconds</code></td>
<td>
<p>'Cap the maximum number of attempts with
<code>max_tries</code> or the total elapsed time from the first request with
<code>max_seconds</code>. If neither option is supplied (the default), <code><a href="httr2.html#topic+req_perform">httr2::req_perform()</a></code>
will not retry' (Wickham, 2023). The default of <code>max_tries</code> is 16.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_is_transient">is_transient</code></td>
<td>
<p>'A predicate function that takes a single argument
(the response) and returns <code>TRUE</code> or <code>FALSE</code> specifying whether or not
the response represents a transient error' (Wickham, 2023). This function runs
automatically in the AIscreenR but can be customized by the user if necessary.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_backoff">backoff</code></td>
<td>
<p>'A function that takes a single argument (the number of failed
attempts so far) and returns the number of seconds to wait' (Wickham, 2023).</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_after">after</code></td>
<td>
<p>'A function that takes a single argument (the response) and
returns either a number of seconds to wait or <code>NULL</code>, which indicates
that a precise wait time is not available that the <code>backoff</code> strategy
should be used instead' (Wickham, 2023).</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_rpm">rpm</code></td>
<td>
<p>Numerical value indicating the number of requests per minute (rpm)
available for the specified model. Find more information at
<a href="https://platform.openai.com/docs/guides/rate-limits/what-are-the-rate-limits-for-our-api">https://platform.openai.com/docs/guides/rate-limits/what-are-the-rate-limits-for-our-api</a>.
Alternatively, use <code><a href="#topic+rate_limits_per_minute">rate_limits_per_minute()</a></code>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_reps">reps</code></td>
<td>
<p>Numerical value indicating the number of times the same
question should be send to the server. This can be useful to test consistency
between answers, and/or can be used to make inclusion judgments based on how many times
a study has been included across a the given number of screenings.
Default is <code>1</code> but when using gpt-3.5-turbo models or gpt-4o-mini,
we recommend setting this value to <code>10</code> to catch model uncertainty.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_seed_par">seed_par</code></td>
<td>
<p>Numerical value for a seed to ensure that proper,
parallel-safe random numbers are produced.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_progress">progress</code></td>
<td>
<p>Logical indicating whether a progress line should be shown when running
the title and abstract screening in parallel. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_decision_description">decision_description</code></td>
<td>
<p>Logical indicating whether a detailed description should follow
the decision made by GPT. Default is <code>FALSE</code>. When conducting large-scale screening,
we generally recommend not using this feature as it will substantially increase the cost of the
screening. We generally recommend using it when encountering disagreements between GPT and
human decisions.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_messages">messages</code></td>
<td>
<p>Logical indicating whether to print messages embedded in the function.
Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_incl_cutoff_upper">incl_cutoff_upper</code></td>
<td>
<p>Numerical value indicating the probability threshold
for which a studies should be included. ONLY relevant when the same questions is requested
multiple times (i.e., when any reps &gt; 1). Default is 0.5, indicating that
titles and abstracts should only be included if GPT has included the study more than 50 percent of the times.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_incl_cutoff_lower">incl_cutoff_lower</code></td>
<td>
<p>Numerical value indicating the probability threshold
above which studies should be check by a human. ONLY relevant when the same questions is requested
multiple times (i.e., when any reps &gt; 1). Default is 0.4, meaning
that if you ask GPT the same questions 10 times and it includes the
title and abstract 4 times, we suggest that the study should be check by a human.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_force">force</code></td>
<td>
<p>Logical argument indicating whether to force the function to use more than
10 iterations for gpt-3.5 models and more than 1 iteration for gpt-4 models other than gpt-4o-mini.
This argument is developed to avoid the conduct of wrong and extreme sized screening.
Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_fine_tuned">fine_tuned</code></td>
<td>
<p>Logical indicating whether a fine-tuned model is used. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="tabscreen_gpt.tools_+3A_...">...</code></td>
<td>
<p>Further argument to pass to the request body.
See <a href="https://platform.openai.com/docs/api-reference/chat/create">https://platform.openai.com/docs/api-reference/chat/create</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>'gpt'</code>. The object is a list containing the following
datasets and components:
</p>
<table role = "presentation">
<tr><td><code>answer_data</code></td>
<td>
<p>dataset of class <code>'gpt_tbl'</code> with all individual answers.</p>
</td></tr>
<tr><td><code>price_dollar</code></td>
<td>
<p>numerical value indicating the total price (in USD) of the screening.</p>
</td></tr>
<tr><td><code>price_data</code></td>
<td>
<p>dataset with prices across all gpt models used for screening.</p>
</td></tr>
<tr><td><code>run_date</code></td>
<td>
<p>string indicating the date when the screening was ran. In some frameworks,
time details are considered important to report (see e.g., Thomas et al., 2024).</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>some additional attributed values/components, including an attributed list with the arguments used in the function.
These are used in  <code><a href="#topic+screen_errors">screen_errors()</a></code> to re-screen transient errors.</p>
</td></tr>
</table>
<p>If the same question is requested multiple times, the object will also contain the
following dataset with results aggregated across the iterated requests/questions.
</p>
<table role = "presentation">
<tr><td><code>answer_data_aggregated</code></td>
<td>
<p>dataset of class <code>'gpt_agg_tbl'</code> with the summarized, probabilistic inclusion decision
for each title and abstract across multiple repeated questions.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The <code>answer_data</code> data contains the following <em>mandatory</em> variables:
</p>

<table>
<tr>
 <td style="text-align: left;">
<b>studyid</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the study ID of the reference. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>title</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the title of the reference. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>abstract</b> </td><td style="text-align: left;"> <code>character</code>   </td><td style="text-align: left;"> indicating the abstract of the reference. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>promptid</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the prompt ID. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>prompt</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the prompt. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>model</b> </td><td style="text-align: left;"> <code>character</code>   </td><td style="text-align: left;"> indicating the specific gpt-model used. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>iterations</b> </td><td style="text-align: left;"> <code>numeric</code> </td><td style="text-align: left;"> indicating the number of times the same question has been sent to OpenAI's GPT API models. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>question</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the final question sent to OpenAI's GPT API models. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>top_p</b>  </td><td style="text-align: left;"> <code>numeric</code> </td><td style="text-align: left;"> indicating the applied top_p. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>decision_gpt</b>  </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the raw gpt decision - either <code>"1", "0", "1.1"</code> for inclusion, exclusion, or uncertainty, respectively. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>detailed_description</b>  </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating detailed description of the given decision made by OpenAI's GPT API models.
ONLY included if the detailed function calling function is used. See 'Examples' below for how to use this function. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>decision_binary</b>  </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the binary gpt decision,
that is 1 for inclusion and 0 for exclusion. 1.1 decision are coded equal to 1 in this case. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>prompt_tokens</b>  </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the number of prompt tokens sent to the server for the given request. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>completion_tokens</b>  </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the number of completion tokens sent to the server for the given request. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>submodel</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the exact (sub)model used for screening. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>run_time</b>  </td><td style="text-align: left;"> <code>numeric</code> </td><td style="text-align: left;"> indicating the time it took to obtain a response from the server for the given request. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>run_date</b>  </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the date the given response was received. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>n</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating iteration ID. Is only different from 1, when <code>reps &gt; 1</code>.  </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<br>
<p>If any requests failed, the <code>gpt</code> object contains an
error dataset (<code>error_data</code>) containing the same variables as <code>answer_data</code>
but with failed request references only.
<br>

</p>
<p>When the same question is requested multiple times, the <code>answer_data_aggregated</code> data contains the following <em>mandatory</em> variables:
</p>

<table>
<tr>
 <td style="text-align: left;">
<b>studyid</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the study ID of the reference. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>title</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the title of the reference. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>abstract</b> </td><td style="text-align: left;"> <code>character</code>   </td><td style="text-align: left;"> indicating the abstract of the reference. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>promptid</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the prompt ID. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>prompt</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the prompt. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>model</b> </td><td style="text-align: left;"> <code>character</code>   </td><td style="text-align: left;"> indicating the specific gpt-model used. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>question</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the final question sent to OpenAI's GPT API models. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>top_p</b> </td><td style="text-align: left;"> <code>numeric</code>  </td><td style="text-align: left;"> indicating the applied top_p. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>incl_p</b> </td><td style="text-align: left;"> <code>numeric</code>  </td><td style="text-align: left;"> indicating the probability of inclusion calculated across multiple repeated responses on the same title and abstract. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>final_decision_gpt</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the final decision reached by gpt - either 'Include', 'Exclude', or 'Check'. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>final_decision_gpt_num</b>  </td><td style="text-align: left;"> <code>integer</code>  </td><td style="text-align: left;"> indicating the final numeric decision reached by gpt - either 1 or 0. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>longest_answer</b>  </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the longest gpt response obtained
across multiple repeated responses on the same title and abstract. Only included when <code>decision_description = TRUE</code>.
See 'Examples' below for how to use this function. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>reps</b>  </td><td style="text-align: left;"> <code>integer</code>  </td><td style="text-align: left;"> indicating the number of times the same question has been sent to OpenAI's GPT API models. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>n_mis_answers</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the number of missing responses. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>submodel</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> indicating the exact (sub)model used for screening. </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<br>
<p>The <code>price_data</code> data contains the following variables:
</p>

<table>
<tr>
 <td style="text-align: left;">
<b>prompt</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> if multiple prompts are used this variable indicates the given prompt-id. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>model</b> </td><td style="text-align: left;"> <code>character</code> </td><td style="text-align: left;"> the specific gpt model used. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>iterations</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> indicating the number of times the same question was requested.  </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>input_price_dollar</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> price for all prompt/input tokens for the correspondent gpt-model. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>output_price_dollar</b>  </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> price for all completion/output tokens for the correspondent gpt-model. </td>
</tr>
<tr>
 <td style="text-align: left;">
<b>total_price_dollar</b> </td><td style="text-align: left;"> <code>integer</code> </td><td style="text-align: left;"> total price for all tokens for the correspondent gpt-model. </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Find current token pricing at <a href="https://openai.com/pricing">https://openai.com/pricing</a> or <a href="#topic+model_prizes">model_prizes</a>.
</p>


<h3>References</h3>

<p>Vembye, M. H., Christensen, J., Mølgaard, A. B., &amp; Schytt, F. L. W. (2024)
<em>GPT API Models Can Function as Highly Reliable Second Screeners of Titles and Abstracts in Systematic Reviews:
A Proof of Concept and Common Guidelines</em> <a href="https://osf.io/preprints/osf/yrhzm">https://osf.io/preprints/osf/yrhzm</a>
</p>
<p>Thomas, J. et al. (2024).
Responsible AI in Evidence SynthEsis (RAISE): guidance and recommendations.
<a href="https://osf.io/cn7x4">https://osf.io/cn7x4</a>
</p>
<p>Wickham H (2023).
<em>httr2: Perform HTTP Requests and Process the Responses</em>.
<a href="https://httr2.r-lib.org">https://httr2.r-lib.org</a>, <a href="https://github.com/r-lib/httr2">https://github.com/r-lib/httr2</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(future)

set_api_key()

prompt &lt;- "Is this study about a Functional Family Therapy (FFT) intervention?"

plan(multisession)

tabscreen_gpt(
  data = filges2015_dat[1:2,],
  prompt = prompt,
  studyid = studyid,
  title = title,
  abstract = abstract
  )

plan(sequential)

 # Get detailed descriptions of the gpt decisions.

 plan(multisession)

 tabscreen_gpt(
   data = filges2015_dat[1:2,],
   prompt = prompt,
   studyid = studyid,
   title = title,
   abstract = abstract,
   decision_description = TRUE
 )

plan(sequential)


## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
