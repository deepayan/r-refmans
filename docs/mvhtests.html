<!DOCTYPE html><html><head><title>Help for package mvhtests</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mvhtests}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#mvhtests-package'>
<p>Multivariate Hypothesis Tests</p></a></li>
<li><a href='#Box's M test for equality of two or more covariance matrices'>
<p>Box's M test for equality of two or more covariance matrices</p></a></li>
<li><a href='#Empirical likelihood for a one sample mean vector hypothesis testing'>
<p>Empirical likelihood for a one sample mean vector hypothesis testing</p></a></li>
<li><a href='#Empirical likelihood hypothesis testing for two mean vectors'>
<p>Empirical likelihood hypothesis testing for two mean vectors</p></a></li>
<li><a href='#Exponential empirical likelihood for a one sample mean vector hypothesis testing'>
<p>Exponential empirical likelihood for a one sample mean vector hypothesis testing</p></a></li>
<li><a href='#Exponential empirical likelihood hypothesis testing for two mean vectors'>
<p>Exponential empirical likelihood hypothesis testing for two mean vectors</p></a></li>
<li><a href='#Hotelling's multivariate version of the 1 sample t-test for Euclidean data'>
<p>Hotelling's multivariate version of the 1 sample t-test for Euclidean data</p></a></li>
<li><a href='#Hotelling's multivariate version of the 2 sample t-test for Euclidean data'>
<p>Hotelling's multivariate version of the 2 sample t-test for Euclidean data</p></a></li>
<li><a href='#Hypothesis test for two high-dimensional mean vectors'>
<p>Hypothesis test for two high-dimensional mean vectors</p></a></li>
<li><a href='#James multivariate version of the t-test'>
<p>James multivariate version of the t-test</p></a></li>
<li><a href='#Log-likelihood ratio test for equality of one covariance matrix'>
<p>Log-likelihood ratio test for equality of one covariance matrix</p></a></li>
<li><a href='#Log-likelihood ratio test for equality of two or more covariance matrices'>
<p>Log-likelihood ratio test for equality of two or more covariance matrices</p></a></li>
<li><a href='#Multivariate analysis of variance (James test)'>
<p>Multivariate analysis of variance (James test)</p></a></li>
<li><a href='#Multivariate analysis of variance assuming equality of the covariance matrices'>
<p>Multivariate analysis of variance assuming equality of the covariance matrices</p></a></li>
<li><a href='#Relationship between Hotelling's T2 test and James' MANOVA'>
<p>Relationship between Hotelling's <code class="reqn">T^2</code> test and James' MANOVA</p></a></li>
<li><a href='#Relationship between the Hotelling's and James' tests'>
<p>Relationship between the Hotelling's <code class="reqn">T^2</code> and James' test</p></a></li>
<li><a href='#Repeated measures ANOVA (univariate data) using Hotelling's T2 test'>
<p>Repeated measures ANOVA (univariate data) using Hotelling's <code class="reqn">T^2</code> test</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Multivariate Hypothesis Tests</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-10-19</td>
</tr>
<tr>
<td>Author:</td>
<td>Michail Tsagris [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michail Tsagris &lt;mtsagris@uoc.gr&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>doParallel, emplik, foreach, parallel, Rfast, Rfast2, stats</td>
</tr>
<tr>
<td>Description:</td>
<td>Hypothesis tests for multivariate data. Tests for one and two mean vectors, multivariate analysis of variance, tests for one, two or more covariance matrices. References include: Mardia K.V., Kent J.T. and Bibby J.M. (1979). Multivariate Analysis. ISBN: 978-0124712522. London: Academic Press.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-19 18:21:55 UTC; mtsag</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-20 11:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='mvhtests-package'>
Multivariate Hypothesis Tests
</h2><span id='topic+mvhtests-package'></span>

<h3>Description</h3>

<p>Multivariate Hypothesis Tests.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> mvhtests</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2023-10-19</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Maintainers</h3>

<p>Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Aitchison J. (1986). The statistical analysis of compositional data. Chapman &amp; Hall.
</p>
<p>Amaral G.J.A., Dryden I.L. and Wood A.T.A. (2007). Pivotal bootstrap methods for k-sample
problems in directional statistics and shape analysis.
Journal of the American Statistical Association, 102(478): 695&ndash;707.
</p>
<p>Efron B. (1981) Nonparametric standard errors and confidence intervals. Canadian Journal of
Statistics, 9(2): 139&ndash;158.
</p>
<p>Emerson S. (2009). Small sample performance and calibration of the Empirical Likelihood method.
PhD thesis, Stanford university.
</p>
<p>Everitt B. (2005). An R and S-Plus Companion to Multivariate Analysis. Springer.
</p>
<p>James G.S. (1954). Tests of Linear Hypotheses in Univariate and Multivariate Analysis
when the Ratios of the Population Variances are Unknown. Biometrika, 41(1/2): 19&ndash;43.
</p>
<p>Jing B.Y. and Wood A.T.A. (1996). Exponential empirical likelihood is not
Bartlett correctable. Annals of Statistics, 24(1): 365&ndash;369.
</p>
<p>Jing B.Y. and Robinson J. (1997). Two-sample nonparametric tilting method. Australian Journal of
Statistics, 39(1): 25&ndash;34.
</p>
<p>Johnson R.A. and Wichern D.W. (2007, 6th Edition). Applied Multivariate Statistical Analysis.
</p>
<p>Krishnamoorthy K. and Yu J. (2004). Modified Nel and Van der Merwe test for the multivariate
Behrens-Fisher problem. Statistics &amp; Probability Letters, 66(2): 161&ndash;169.
</p>
<p>Krishnamoorthy K. and Yanping X. (2006). On Selecting Tests for Equality of Two Normal
Mean Vectors. Multivariate Behavioral Research, 41(4): 533&ndash;548.
</p>
<p>Mardia K.V., Kent J.T. and Bibby J.M. (1979). Multivariate Analysis.
London: Academic Press.
</p>
<p>Owen A.B. (1988). Empirical likelihood ratio confidence intervals for a single functional.
Biometrika, 75(2): 237&ndash;249.
</p>
<p>Owen A. (1990). Empirical likelihood ratio confidence regions.
Annals of Statistics, 18(1): 90&ndash;120.
</p>
<p>Owen A. B. (2001). Empirical likelihood. Chapman and Hall/CRC Press.
</p>
<p>Preston S.P. and Wood A.T.A. (2010). Two-Sample Bootstrap Hypothesis Tests for
Three-Dimensional Labelled Landmark Data. Scandinavian Journal of Statistics 37(4): 568&ndash;587.
</p>
<p>Todorov V. and Filzmoser P. (2010). Robust Statistic for the One-way MANOVA.
Computational Statistics &amp; Data Analysis 54(1): 37&ndash;48.
</p>

<hr>
<h2 id='Box+27s+20M+20test+20for+20equality+20of+20two+20or+20more+20covariance+20matrices'>
Box's M test for equality of two or more covariance matrices
</h2><span id='topic+Mtest.cov'></span>

<h3>Description</h3>

<p>Box's M test for equality of two or more covariance matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Mtest.cov(x, ina, a = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Box+2B27s+2B20M+2B20test+2B20for+2B20equality+2B20of+2B20two+2B20or+2B20more+2B20covariance+2B20matrices_+3A_x">x</code></td>
<td>

<p>A matrix containing Euclidean data.
</p>
</td></tr>
<tr><td><code id="Box+2B27s+2B20M+2B20test+2B20for+2B20equality+2B20of+2B20two+2B20or+2B20more+2B20covariance+2B20matrices_+3A_ina">ina</code></td>
<td>

<p>A vector denoting the groups of the data.
</p>
</td></tr>
<tr><td><code id="Box+2B27s+2B20M+2B20test+2B20for+2B20equality+2B20of+2B20two+2B20or+2B20more+2B20covariance+2B20matrices_+3A_a">a</code></td>
<td>

<p>The significance level, set to 0.05 by default.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>According to Mardia, Kent and Bibby (1979, pg. 140), it may be argued that if <code class="reqn">n_i</code> is small, then the log-likelihood ratio test (function <code><a href="#topic+likel.cov">likel.cov</a></code>) gives too much weight to the contribution of <code class="reqn">{\bf S}</code>. This consideration led Box (1949) to propose another test statistic in place of that seen in <code><a href="#topic+likel.cov">likel.cov</a></code> . Box's <code class="reqn">M</code> is given by
</p>
<p style="text-align: center;"><code class="reqn">
M=\gamma\sum_{i=1}^k\left(n_i-1\right)\log{\left|{\bf S}_{i}^{-1}{\bf S}_p \right|},
</code>
</p>

<p>where <code class="reqn">\gamma=1-\frac{2p^2+3p-1}{6\left(p+1\right)\left(k-1\right)}\left(\sum_{i=1}^k\frac{1}{n_i-1}-\frac{1}{n-k}\right)</code> and <code class="reqn">{\bf S}_{i}</code> and <code class="reqn">{\bf S}_{p}</code> are the <code class="reqn">i</code>-th unbiased covariance estimator and the pooled covariance matrix, respectively with <code class="reqn">{\bf S}_p=\frac{\sum_{i=1}^k\left(n_i-1\right){\bf S}_i}{n-k}</code>. Box's <code class="reqn">M</code> also has an asymptotic <code class="reqn">\chi^2</code> distribution with <code class="reqn">\frac{1}{2}\left(p+1\right)\left(k-1\right)</code> degrees of freedom. Box's approximation seems to be good if each <code class="reqn">n_i</code> exceeds 20 and if <code class="reqn">k</code> and <code class="reqn">p</code> do not exceed 5 (Bibby and Kent (1979) pg. 140).
</p>


<h3>Value</h3>

<p>A vector with the the test statistic, the p-value, the degrees of freedom and the critical value of the test.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Mardia K.V., Kent J.T. and Bibby J.M. (1979). Multivariate Analysis. London: Academic Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+equal.cov">equal.cov</a>, <a href="#topic+likel.cov">likel.cov</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix( iris[, 1:4] )
ina &lt;- iris[, 5]
Mtest.cov(x, ina)
</code></pre>

<hr>
<h2 id='Empirical+20likelihood+20for+20a+20one+20sample+20mean+20vector+20hypothesis+20testing'>
Empirical likelihood for a one sample mean vector hypothesis testing
</h2><span id='topic+el.test1'></span>

<h3>Description</h3>

<p>Empirical likelihood for a one sample mean vector hypothesis testing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>el.test1(x, mu, R = 1, ncores = 1, graph = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Empirical+2B20likelihood+2B20for+2B20a+2B20one+2B20sample+2B20mean+2B20vector+2B20hypothesis+2B20testing_+3A_x">x</code></td>
<td>

<p>A matrix containing Euclidean data.
</p>
</td></tr>
<tr><td><code id="Empirical+2B20likelihood+2B20for+2B20a+2B20one+2B20sample+2B20mean+2B20vector+2B20hypothesis+2B20testing_+3A_mu">mu</code></td>
<td>

<p>The hypothesized mean vector.
</p>
</td></tr>
<tr><td><code id="Empirical+2B20likelihood+2B20for+2B20a+2B20one+2B20sample+2B20mean+2B20vector+2B20hypothesis+2B20testing_+3A_r">R</code></td>
<td>

<p>If R is 1 no bootstrap calibration is performed and the classical p-value via the <code class="reqn">\chi^2</code> distribution is returned. If R is greater than 1, the bootstrap p-value is returned.
</p>
</td></tr>
<tr><td><code id="Empirical+2B20likelihood+2B20for+2B20a+2B20one+2B20sample+2B20mean+2B20vector+2B20hypothesis+2B20testing_+3A_ncores">ncores</code></td>
<td>

<p>The number of cores to use, set to 1 by default.
</p>
</td></tr>
<tr><td><code id="Empirical+2B20likelihood+2B20for+2B20a+2B20one+2B20sample+2B20mean+2B20vector+2B20hypothesis+2B20testing_+3A_graph">graph</code></td>
<td>

<p>A boolean variable which is taken into consideration only when bootstrap calibration is performed. IF TRUE the histogram of the bootstrap test statistic values is plotted.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code class="reqn">H_0</code> is that <code class="reqn">\pmb{\mu} = \pmb{\mu}_0</code> and the constraint imposed by EL is
</p>
<p style="text-align: center;"><code class="reqn">
\frac{1}{n}\sum_{i=1}^{n}\left\lbrace\left[1+\pmb{\lambda}^T\left({\bf x}_i-\pmb{\mu}_0 \right)\right]^{-1}\left({\bf x}_i-\pmb{\mu}_0\right)\right\rbrace={\bf 0},
</code>
</p>

<p>where the <code class="reqn">\pmb{\lambda}</code> is the Lagrangian parameter introduced to maximize the above expression. Note that the maximization of is with respect to the <code class="reqn">\pmb{\lambda}</code>. The probabilities have the following form
</p>
<p style="text-align: center;"><code class="reqn">
p_i=\frac{1}{n}\left[1+\pmb{\lambda}^T \left({\bf x}_i-\pmb{\mu}_0 \right)\right]^{-1}.
</code>
</p>

<p>The log-likelihood ratio test statistic can be written as
</p>
<p style="text-align: center;"><code class="reqn">
\Lambda=\sum_{i=1}^{n}\log{np_i}.
</code>
</p>

<p>where <code class="reqn">d</code> denotes the number of variables. Under <code class="reqn">H_0</code> <code class="reqn">\Lambda \sim \chi^2_d</code>, asymptotically. Alternatively the bootstrap p-value may be computed.
</p>


<h3>Value</h3>

<p>A list with the outcome of the function <code><a href="emplik.html#topic+el.test">el.test</a></code> which includes
the -2 log-likelihood ratio, the observed P-value by chi-square approximation, the final value of Lagrange multiplier <code class="reqn">\lambda</code>, the gradient at the maximum, the Hessian matrix, the weights on the observations (probabilities multiplied by the sample size) and the number of iteration performed.
In addition the runtime of the procedure is reported. In the case of bootstrap, the bootstrap p-value is also returned.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Jing B.Y. and Wood A.T.A. (1996). Exponential empirical likelihood is
not Bartlett correctable. Annals of Statistics, 24(1): 365&ndash;369.
</p>
<p>Owen A. (1990). Empirical likelihood ratio confidence regions.
Annals of Statistics, 18(1): 90&ndash;120.
</p>
<p>Owen A.B. (2001). Empirical likelihood. Chapman and Hall/CRC Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+eel.test1">eel.test1</a>, <a href="#topic+hotel1T2">hotel1T2</a>, <a href="#topic+james">james</a>, <a href="#topic+hotel2T2">hotel2T2</a>, <a href="#topic+maov">maov</a>, <a href="#topic+el.test2">el.test2</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4])
el.test1(x, mu = numeric(4) )
eel.test1(x, mu = numeric(4) )
</code></pre>

<hr>
<h2 id='Empirical+20likelihood+20hypothesis+20testing+20for+20two+20mean+20vectors'>
Empirical likelihood hypothesis testing for two mean vectors
</h2><span id='topic+el.test2'></span>

<h3>Description</h3>

<p>Empirical likelihood hypothesis testing for two mean vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>el.test2(y1, y2, R = 0, ncores = 1, graph = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Empirical+2B20likelihood+2B20hypothesis+2B20testing+2B20for+2B20two+2B20mean+2B20vectors_+3A_y1">y1</code></td>
<td>

<p>A matrix containing the Euclidean data of the first group.
</p>
</td></tr>
<tr><td><code id="Empirical+2B20likelihood+2B20hypothesis+2B20testing+2B20for+2B20two+2B20mean+2B20vectors_+3A_y2">y2</code></td>
<td>

<p>A matrix containing the Euclidean data of the second group.
</p>
</td></tr>
<tr><td><code id="Empirical+2B20likelihood+2B20hypothesis+2B20testing+2B20for+2B20two+2B20mean+2B20vectors_+3A_r">R</code></td>
<td>

<p>If R is 0, the classical chi-square distribution is used, if R = 1, the corrected chi-square distribution (James, 1954) is used and
if R = 2, the modified F distribution (Krishnamoorthy and Yanping, 2006) is used. If R is greater than 3 bootstrap calibration is performed.
</p>
</td></tr>
<tr><td><code id="Empirical+2B20likelihood+2B20hypothesis+2B20testing+2B20for+2B20two+2B20mean+2B20vectors_+3A_ncores">ncores</code></td>
<td>

<p>How many to cores to use.
</p>
</td></tr>
<tr><td><code id="Empirical+2B20likelihood+2B20hypothesis+2B20testing+2B20for+2B20two+2B20mean+2B20vectors_+3A_graph">graph</code></td>
<td>

<p>A boolean variable which is taken into consideration only when bootstrap calibration is performed. IF TRUE the histogram of the bootstrap test
statistic values is plotted.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code class="reqn">H_0</code> is that <code class="reqn">\pmb{\mu}_1 = \pmb{\mu}_2</code> and the two constraints imposed by EL are
</p>
<p style="text-align: center;"><code class="reqn">
\frac{1}{n_j}\sum_{i=1}^{n_j}\left\lbrace\left[1+\pmb{\lambda}_j^T\left({\bf x}_{ji}-\pmb{\mu} \right)\right]^{-1}\left({\bf x}_{ij}-\pmb{\mu}\right)\right\rbrace={\bf 0},
</code>
</p>

<p>where <code class="reqn">j=1,2</code> and the <code class="reqn">\pmb{\lambda}_js</code> are Lagrangian parameters introduced to maximize the above expression. Note that the maximization of is with respect to the <code class="reqn">\pmb{\lambda}_js</code>. The probabilities of the <code class="reqn">j</code>-th sample have the following form
</p>
<p style="text-align: center;"><code class="reqn">
p_{ji}=\frac{1}{n_j} \left[1+\pmb{\lambda}_j^T \left({\bf x}_{ji}-\pmb{\mu} \right)\right]^{-1}</code>
</p>
<p>. The log-likelihood ratio test statistic can be written as
</p>
<p style="text-align: center;"><code class="reqn">
\Lambda=\sum_{j=1}^2\sum_{i=1}^{n_j}\log{n_jp_{ij}}.
</code>
</p>

<p>The test is implemented by searching for the mean vector that minimizes the sum of the two one sample EL test statistics. See <code><a href="#topic+el.test1">el.test1</a></code> for the test statistic in the one-sample case.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>test</code></td>
<td>

<p>The empirical likelihood test statistic value.
</p>
</td></tr>
<tr><td><code>modif.test</code></td>
<td>

<p>The modified test statistic, either via the chi-square or the F distribution.
</p>
</td></tr>
<tr><td><code>dof</code></td>
<td>

<p>Thre degrees of freedom of the chi-square or the F distribution.
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>

<p>The asymptotic or the bootstrap p-value.
</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>

<p>The estimated common mean vector.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime of the bootstrap calibration.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Amaral G.J.A., Dryden I.L. and Wood A.T.A. (2007). Pivotal bootstrap methods for k-sample
problems in directional statistics and shape analysis.
Journal of the American Statistical Association, 102(478): 695&ndash;707.
</p>
<p>Owen A. B. (2001). Empirical likelihood. Chapman and Hall/CRC Press.
</p>
<p>Owen A.B. (1988). Empirical likelihood ratio confidence intervals for a single functional.
Biometrika, 75(2): 237&ndash;249.
</p>
<p>Preston S.P. and Wood A.T.A. (2010). Two-Sample Bootstrap Hypothesis Tests for Three-Dimensional
Labelled Landmark Data. Scandinavian Journal of Statistics, 37(4): 568&ndash;587.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+eel.test2">eel.test2</a>, <a href="#topic+maovjames">maovjames</a>, <a href="#topic+maov">maov</a>, <a href="#topic+hotel2T2">hotel2T2</a>, <a href="#topic+james">james</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>el.test2( y1 = as.matrix(iris[1:25, 1:4]), y2 = as.matrix(iris[26:50, 1:4]), R = 0 )
el.test2( y1 = as.matrix(iris[1:25, 1:4]), y2 = as.matrix(iris[26:50, 1:4]), R = 1 )
el.test2( y1 =as.matrix(iris[1:25, 1:4]), y2 = as.matrix(iris[26:50, 1:4]), R = 2 )
</code></pre>

<hr>
<h2 id='Exponential+20empirical+20likelihood+20for+20a+20one+20sample+20mean+20vector+20hypothesis+20testing'>
Exponential empirical likelihood for a one sample mean vector hypothesis testing
</h2><span id='topic+eel.test1'></span>

<h3>Description</h3>

<p>Exponential empirical likelihood for a one sample mean vector hypothesis testing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eel.test1(x, mu, tol = 1e-06, R = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Exponential+2B20empirical+2B20likelihood+2B20for+2B20a+2B20one+2B20sample+2B20mean+2B20vector+2B20hypothesis+2B20testing_+3A_x">x</code></td>
<td>

<p>A matrix containing Euclidean data.
</p>
</td></tr>
<tr><td><code id="Exponential+2B20empirical+2B20likelihood+2B20for+2B20a+2B20one+2B20sample+2B20mean+2B20vector+2B20hypothesis+2B20testing_+3A_mu">mu</code></td>
<td>

<p>The hypothesized mean vector.
</p>
</td></tr>
<tr><td><code id="Exponential+2B20empirical+2B20likelihood+2B20for+2B20a+2B20one+2B20sample+2B20mean+2B20vector+2B20hypothesis+2B20testing_+3A_tol">tol</code></td>
<td>

<p>The tolerance value used to stop the Newton-Raphson algorithm.
</p>
</td></tr>
<tr><td><code id="Exponential+2B20empirical+2B20likelihood+2B20for+2B20a+2B20one+2B20sample+2B20mean+2B20vector+2B20hypothesis+2B20testing_+3A_r">R</code></td>
<td>

<p>The number of bootstrap samples used to calculate the p-value. If R = 1 (default value), no bootstrap calibration is performed
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Exponential empirical likelihood or exponential tilting was first introduced by Efron (1981) as a way to perform a &quot;tilted&quot; version of the bootstrap for the one sample mean hypothesis testing. Similarly to the empirical likelihood, positive weights <code class="reqn">p_i</code>, which sum to one, are allocated to the observations, such that the weighted sample mean <code class="reqn">{\bf \bar{x}}</code> is equal to some population mean <code class="reqn">\pmb{\mu}_0</code>, under the <code class="reqn">H_0</code>. Under <code class="reqn">H_1</code> the weights are equal to <code class="reqn">\frac{1}{n}</code>, where <code class="reqn">n</code> is the sample size. Following Efron (1981), the choice of <code class="reqn">p_is</code> will minimize the Kullback-Leibler distance from <code class="reqn">H_0</code> to <code class="reqn">H_1</code>
</p>
<p style="text-align: center;"><code class="reqn">
D\left(L_0,L_1\right)=\sum_{i=1}^np_i\log\left(np_i\right),
</code>
</p>

<p>subject to the constraint <code class="reqn">\sum_{i=1}^np_i{\bf x}_i=\pmb{\mu}_0</code>. The probabilities take the form
</p>
<p style="text-align: center;"><code class="reqn">
p_i=\frac{e^{\pmb{\lambda}^T{\bf x}_i}}{\sum_{j=1}^ne^{\pmb{\lambda}^T{\bf x}_j}}
</code>
</p>

<p>and the constraint becomes
</p>
<p style="text-align: center;"><code class="reqn">
\frac{\sum_{i=1}^ne^{\pmb{\lambda}^T{\bf x}_i}\left({\bf x}_i-\pmb{\mu}_0\right)}{\sum_{j=1}^ne^{\pmb{\lambda}^T{\bf x}_j}}=0 \Rightarrow \frac{\sum_{i=1}^n{\bf x}_ie^{\pmb{\lambda}^T{\bf x}_i}}{\sum_{j=1}^ne^{\pmb{\lambda}^T{\bf x}_j}}-\pmb{\mu}_0={\bf 0}.
</code>
</p>

<p>A numerical search over <code class="reqn">\pmb{\lambda}</code> is required. Under <code class="reqn">H_0</code> <code class="reqn">\Lambda \sim \chi^2_d</code>, where <code class="reqn">d</code> denotes the number of variables. Alternatively the bootstrap p-value may be computed.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>p</code></td>
<td>

<p>The estimated probabilities.
</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>

<p>The value of the Lagrangian parameter <code class="reqn">\lambda</code>.
</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>

<p>The number of iterations required by the newton-Raphson algorithm.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>The value of the log-likelihood ratio test statistic along with its corresponding p-value.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime of the process.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Efron B. (1981) Nonparametric standard errors and confidence intervals. Canadian Journal of
Statistics, 9(2): 139&ndash;158.
</p>
<p>Jing B.Y. and Wood A.T.A. (1996). Exponential empirical likelihood is not
Bartlett correctable. Annals of Statistics, 24(1): 365&ndash;369.
</p>
<p>Owen A. B. (2001). Empirical likelihood. Chapman and Hall/CRC Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+el.test1">el.test1</a>, <a href="#topic+hotel1T2">hotel1T2</a>, <a href="#topic+james">james</a>, <a href="#topic+hotel2T2">hotel2T2</a>, <a href="#topic+maov">maov</a>, <a href="#topic+el.test2">el.test2</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix( iris[, 1:4] )
eel.test1(x, numeric(4) )
el.test1(x, numeric(4) )
</code></pre>

<hr>
<h2 id='Exponential+20empirical+20likelihood+20hypothesis+20testing+20for+20two+20mean+20vectors'>
Exponential empirical likelihood hypothesis testing for two mean vectors
</h2><span id='topic+eel.test2'></span>

<h3>Description</h3>

<p>Exponential empirical likelihood hypothesis testing for two mean vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eel.test2(y1, y2, tol = 1e-07, R = 0, graph = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Exponential+2B20empirical+2B20likelihood+2B20hypothesis+2B20testing+2B20for+2B20two+2B20mean+2B20vectors_+3A_y1">y1</code></td>
<td>

<p>A matrix containing the Euclidean data of the first group.
</p>
</td></tr>
<tr><td><code id="Exponential+2B20empirical+2B20likelihood+2B20hypothesis+2B20testing+2B20for+2B20two+2B20mean+2B20vectors_+3A_y2">y2</code></td>
<td>

<p>A matrix containing the Euclidean data of the second group.
</p>
</td></tr>
<tr><td><code id="Exponential+2B20empirical+2B20likelihood+2B20hypothesis+2B20testing+2B20for+2B20two+2B20mean+2B20vectors_+3A_tol">tol</code></td>
<td>

<p>The tolerance level used to terminate the Newton-Raphson algorithm.
</p>
</td></tr>
<tr><td><code id="Exponential+2B20empirical+2B20likelihood+2B20hypothesis+2B20testing+2B20for+2B20two+2B20mean+2B20vectors_+3A_r">R</code></td>
<td>

<p>If R is 0, the classical chi-square distribution is used, if R = 1,
the corrected chi-square distribution (James, 1954) is used and if R = 2,
the modified F distribution (Krishnamoorthy and Yanping, 2006) is used.
If R is greater than 3 bootstrap calibration is performed.
</p>
</td></tr>
<tr><td><code id="Exponential+2B20empirical+2B20likelihood+2B20hypothesis+2B20testing+2B20for+2B20two+2B20mean+2B20vectors_+3A_graph">graph</code></td>
<td>

<p>A boolean variable which is taken into consideration only when bootstrap
calibration is performed. IF TRUE the histogram of the bootstrap test
statistic values is plotted.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Exponential empirical likelihood or exponential tilting was first introduced by Efron (1981) as a way to perform a &quot;tilted&quot; version of the bootstrap for the one sample mean hypothesis testing. Similarly to the empirical likelihood, positive weights <code class="reqn">p_i</code>, which sum to one, are allocated to the observations, such that the weighted sample mean <code class="reqn">{\bf \bar{x}}</code> is equal to some population mean <code class="reqn">\pmb{\mu}</code>, under the <code class="reqn">H_0</code>. Under <code class="reqn">H_1</code> the weights are equal to <code class="reqn">\frac{1}{n}</code>, where <code class="reqn">n</code> is the sample size. Following Efron (1981), the choice of <code class="reqn">p_is</code> will minimize the Kullback-Leibler distance from <code class="reqn">H_0</code> to <code class="reqn">H_1</code>
</p>
<p style="text-align: center;"><code class="reqn">
D\left(L_0,L_1\right)=\sum_{i=1}^np_i\log\left(np_i\right),
</code>
</p>

<p>subject to the constraint <code class="reqn">\sum_{i=1}^np_i{\bf x}_i=\pmb{\mu}</code>. The probabilities take the form
</p>
<p style="text-align: center;"><code class="reqn">
p_i=\frac{e^{\pmb{\lambda}^T{\bf x}_i}}{\sum_{j=1}^ne^{\pmb{\lambda}^T{\bf x}_j}}
</code>
</p>

<p>and the constraint becomes
</p>
<p style="text-align: center;"><code class="reqn">
\frac{\sum_{i=1}^ne^{\pmb{\lambda}^T{\bf x}_i}\left({\bf x}_i-\pmb{\mu}\right)}{\sum_{j=1}^ne^{\pmb{\lambda}^T{\bf x}_j}}=0 \Rightarrow \frac{\sum_{i=1}^n{\bf x}_ie^{\pmb{\lambda}^T{\bf x}_i}}{\sum_{j=1}^ne^{\pmb{\lambda}^T{\bf x}_j}}-\pmb{\mu}=0.
</code>
</p>

<p>Similarly to empirical likelihood a numerical search over <code class="reqn">\pmb{\lambda}</code> is required.
</p>
<p>We can derive the asymptotic form of the test statistic in the two sample means case but in a simpler form, generalizing the approach of Jing and Robinson (1997) to the multivariate case as follows. The three constraints are
</p>
<p style="text-align: center;"><code class="reqn">
{\begin{array}{ccc}
\left(\sum_{j=1}^{n_1}e^{\pmb {\lambda}_1^T{\bf x}_j}\right)^{-1}\left(\sum_{i=1}^{n_1}{\bf x}_ie^{\pmb{\lambda}_1^T
{\bf x}_i}\right) -\pmb{\mu} &amp; = &amp; {\bf 0} \\
\left(\sum_{j=1}^{n_2}e^{\pmb {\lambda}_2^T{\bf y}_j}\right)^{-1}\left(\sum_{i=1}^{n_2}{\bf y}_ie^{\pmb{\lambda}_2^T
{\bf y}_i}\right) -\pmb{\mu} &amp; = &amp; {\bf 0} \\
n_1\pmb{\lambda}_1+n_2\pmb{\lambda}_2 &amp; = &amp; {\bf 0}.
\end{array}}
</code>
</p>

<p>Similarly to EL the sum of a linear combination of the <code class="reqn">\pmb{\lambda}s</code> is set to zero. We can equate the first two constraints of
</p>
<p style="text-align: center;"><code class="reqn">
\left(\sum_{j=1}^{n_1}e^{\pmb {\lambda}_1^T{\bf x}_j}\right)^{-1}\left(\sum_{i=1}^{n_1}{\bf x}_ie^{\pmb{\lambda}_1^T
{\bf x}_i}\right)=
\left(\sum_{j=1}^{n_2}e^{\pmb {\lambda}_2^T{\bf y}_j}\right)^{-1}\left(\sum_{i=1}^{n_2}{\bf y}_ie^{\pmb{\lambda}_2^T
{\bf y}_i}\right).
</code>
</p>

<p>Also, we can write the third constraint of as <code class="reqn">\pmb{\lambda}_2=-\frac{n_1}{n_2}\pmb{\lambda}_1</code> and thus rewrite the first two constraints as
</p>
<p style="text-align: center;"><code class="reqn">
\left(\sum_{j=1}^{n_1}e^{\pmb{\lambda}^T{\bf x}_j}\right)^{-1}\left(\sum_{i=1}^{n_1}{\bf x}_ie^{\pmb{\lambda}^T
{\bf x}_i}\right) =
\left(\sum_{j=1}^{n_2}e^{-\frac{n_1}{n_2}\pmb{\lambda}^T{\bf y}_j}\right)^{-1}\left(\sum_{i=1}^{n_2}{\bf y}_ie^{-\frac{n_1}{n_2}\pmb{\lambda}^T
{\bf y}_i}\right).
</code>
</p>

<p>This trick allows us to avoid the estimation of the common mean. It is not possible though to do this in the empirical likelihood method. Instead of minimisation of the sum of the one-sample test statistics from the common mean, we can define the probabilities by searching for the <code class="reqn">\pmb{\lambda}</code> which makes the last equation hold true. The third constraint of is a convenient constraint, but Jing and Robinson (1997) mention that even though as a constraint is simple it does not lead to second-order accurate confidence intervals unless the two sample sizes are equal. Asymptotically, the test statistic follows a <code class="reqn">\chi^2_d</code> under the null hypothesis.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>test</code></td>
<td>

<p>The empirical likelihood test statistic value.
</p>
</td></tr>
<tr><td><code>modif.test</code></td>
<td>

<p>The modified test statistic, either via the chi-square or the F distribution.
</p>
</td></tr>
<tr><td><code>dof</code></td>
<td>

<p>The degrees of freedom of the chi-square or the F distribution.
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>

<p>The asymptotic or the bootstrap p-value.
</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>

<p>The estimated common mean vector.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime of the bootstrap calibration.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Efron B. (1981) Nonparametric standard errors and confidence intervals. Canadian Journal of
Statistics, 9(2): 139&ndash;158.
</p>
<p>Jing B.Y. and Wood A.T.A. (1996). Exponential empirical likelihood is
not Bartlett correctable. Annals of Statistics, 24(1): 365&ndash;369.
</p>
<p>Jing B.Y. and Robinson J. (1997). Two-sample nonparametric tilting method. Australian Journal of
Statistics, 39(1): 25&ndash;34.
</p>
<p>Owen A.B. (2001). Empirical likelihood. Chapman and Hall/CRC Press.
</p>
<p>Preston S.P. and Wood A.T.A. (2010). Two-Sample Bootstrap Hypothesis Tests
for Three-Dimensional Labelled Landmark Data. Scandinavian Journal of
Statistics 37(4): 568&ndash;587.
</p>
<p>Tsagris M., Preston S. and Wood A.T.A. (2017). Nonparametric hypothesis
testing for equality of means on the simplex.
Journal of Statistical Computation and Simulation, 87(2): 406&ndash;422.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+el.test2">el.test2</a>, <a href="#topic+maovjames">maovjames</a>, <a href="#topic+maov">maov</a>, <a href="#topic+hotel2T2">hotel2T2</a>,
<a href="#topic+james">james</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y1 = as.matrix(iris[1:25, 1:4])
y2 = as.matrix(iris[26:50, 1:4])
eel.test2(y1, y2)
eel.test2(y1, y2 )
eel.test2( y1, y2 )
</code></pre>

<hr>
<h2 id='Hotelling+27s+20multivariate+20version+20of+20the+201+20sample+20t-test+20for+20Euclidean+20data'>
Hotelling's multivariate version of the 1 sample t-test for Euclidean data
</h2><span id='topic+hotel1T2'></span>

<h3>Description</h3>

<p>Hotelling's test for testing one Euclidean population mean vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hotel1T2(x, M, a = 0.05, R = 999, graph = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Hotelling+2B27s+2B20multivariate+2B20version+2B20of+2B20the+2B201+2B20sample+2B20t-test+2B20for+2B20Euclidean+2B20data_+3A_x">x</code></td>
<td>

<p>A matrix containing Euclidean data.
</p>
</td></tr>
<tr><td><code id="Hotelling+2B27s+2B20multivariate+2B20version+2B20of+2B20the+2B201+2B20sample+2B20t-test+2B20for+2B20Euclidean+2B20data_+3A_a">a</code></td>
<td>

<p>The significance level, set to 0.05 by default.
</p>
</td></tr>
<tr><td><code id="Hotelling+2B27s+2B20multivariate+2B20version+2B20of+2B20the+2B201+2B20sample+2B20t-test+2B20for+2B20Euclidean+2B20data_+3A_m">M</code></td>
<td>

<p>The hypothesized mean vector.
</p>
</td></tr>
<tr><td><code id="Hotelling+2B27s+2B20multivariate+2B20version+2B20of+2B20the+2B201+2B20sample+2B20t-test+2B20for+2B20Euclidean+2B20data_+3A_r">R</code></td>
<td>

<p>If R is 1 no bootstrap calibration is performed and the classical p-value via the F distribution is returned. If R is greater than 1, the bootstrap p-value is returned.
</p>
</td></tr>
<tr><td><code id="Hotelling+2B27s+2B20multivariate+2B20version+2B20of+2B20the+2B201+2B20sample+2B20t-test+2B20for+2B20Euclidean+2B20data_+3A_graph">graph</code></td>
<td>

<p>A boolean variable which is taken into consideration only when bootstrap calibration is performed. IF TRUE the histogram of the bootstrap test statistic values is plotted.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The hypothesis test is that a mean vector is equal to some specified vector <code class="reqn">H_0:\pmb{\mu}=\pmb{\mu}_0</code>. We assume that <code class="reqn">\pmb{\Sigma}</code> is unknown. The first approach to this hypothesis test is parametrically, using the Hotelling's <code class="reqn">T^2</code> test Mardia, Bibby and Kent (1979, pg. 125-126). The test statistic is given by
</p>
<p style="text-align: center;"><code class="reqn">
T^2=\frac{\left(n-p\right)n}{\left(n-1\right)p}\left(\bar{{\bf X}}-\pmb{\mu}\right)^T{\bf S}^{-1}\left(\bar{{\bf X}}-\pmb{\mu} \right).
</code>
</p>

<p>Under the null hypothesis, the above test statistic follows the <code class="reqn">F_{p,n-p}</code> distribution. The bootstrap version of the one-sample multivariate generalization of the simple t-test is also included in the function. An extra argument (R) indicates whether bootstrap calibration should be used or not. If R=1, then the asymptotic theory applies, if R&gt;1, then the bootstrap p-value will be applied and the number of re-samples is equal to R.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>m</code></td>
<td>

<p>The sample mean vector.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>The test statistic, the p-value, the critical value and the degrees of freedom of the F distribution (numerator and denominator).
This is given if no bootstrap calibration is employed.
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>

<p>The bootstrap p-value is bootstrap is employed.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime of the bootstrap calibration.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Mardia K.V., Kent J.T. and Bibby J.M. (1979). Multivariate analysis. London: Academic Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+eel.test1">eel.test1</a>, <a href="#topic+el.test1">el.test1</a>, <a href="#topic+james">james</a>, <a href="#topic+hotel2T2">hotel2T2</a>, <a href="#topic+maov">maov</a>, <a href="#topic+el.test2">el.test2</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( rnorm( 100 * 4), ncol = 4)
hotel1T2(x, numeric(4), R = 1)
hotel1T2(x, numeric(4), R = 999, graph = TRUE)
</code></pre>

<hr>
<h2 id='Hotelling+27s+20multivariate+20version+20of+20the+202+20sample+20t-test+20for+20Euclidean+20data'>
Hotelling's multivariate version of the 2 sample t-test for Euclidean data
</h2><span id='topic+hotel2T2'></span>

<h3>Description</h3>

<p>Hotelling's test for testing the equality of two Euclidean population mean vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hotel2T2(x1, x2, a = 0.05, R = 999, graph = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Hotelling+2B27s+2B20multivariate+2B20version+2B20of+2B20the+2B202+2B20sample+2B20t-test+2B20for+2B20Euclidean+2B20data_+3A_x1">x1</code></td>
<td>

<p>A matrix containing the Euclidean data of the first group.
</p>
</td></tr>
<tr><td><code id="Hotelling+2B27s+2B20multivariate+2B20version+2B20of+2B20the+2B202+2B20sample+2B20t-test+2B20for+2B20Euclidean+2B20data_+3A_x2">x2</code></td>
<td>

<p>A matrix containing the Euclidean data of the second group.
</p>
</td></tr>
<tr><td><code id="Hotelling+2B27s+2B20multivariate+2B20version+2B20of+2B20the+2B202+2B20sample+2B20t-test+2B20for+2B20Euclidean+2B20data_+3A_a">a</code></td>
<td>

<p>The significance level, set to 0.05 by default.
</p>
</td></tr>
<tr><td><code id="Hotelling+2B27s+2B20multivariate+2B20version+2B20of+2B20the+2B202+2B20sample+2B20t-test+2B20for+2B20Euclidean+2B20data_+3A_r">R</code></td>
<td>

<p>If R is 1 no bootstrap calibration is performed and the classical p-value via the F distribution is returned. If R is greater than 1, the bootstrap p-value is returned.
</p>
</td></tr>
<tr><td><code id="Hotelling+2B27s+2B20multivariate+2B20version+2B20of+2B20the+2B202+2B20sample+2B20t-test+2B20for+2B20Euclidean+2B20data_+3A_graph">graph</code></td>
<td>

<p>A boolean variable which is taken into consideration only when bootstrap calibration is performed. IF TRUE the histogram of the bootstrap test statistic values is plotted.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The fist case scenario is when we assume equality of the two covariance matrices. This is called the two-sample Hotelling's <code class="reqn">T^2</code> test (Mardia, Kent and Bibby, 1979, pg. 131-140) and Everitt (2005, pg. 139). The test statistic is defined as
</p>
<p style="text-align: center;"><code class="reqn">
T^2=\frac{n_1n_2}{n_1+n_2}\left(\bar{{\bf X}}_1- \bar{{\bf X}}_2\right)^T{\bf S}^{-1}\left(\bar{{\bf X}}_1- \bar{{\bf X}}_2\right),
</code>
</p>

<p>where <code class="reqn">\bf S</code> is the pooled covariance matrix calculated under the assumption of equal covariance matrices
<code class="reqn">{\bf S}=\frac{\left(n_1-1\right){\bf S}_1+\left(n_2-1\right){\bf S}_2}{n_1+n_2-2}.</code>
Under <code class="reqn">H_0</code> the statistic <code class="reqn">F</code> given by
</p>
<p style="text-align: center;"><code class="reqn">
F=\frac{\left( n_1+n_2-p-1 \right)T^2}{\left(n_1+n_2-2 \right)p}
</code>
</p>

<p>follows the <code class="reqn">F</code> distribution with <code class="reqn">p</code> and <code class="reqn">n_1+n_2-p-1</code> degrees of freedom. Similar to the one-sample test, an extra argument (R) indicates whether bootstrap calibration should be used or not. If R=1, then the asymptotic theory applies, if R&gt;1, then the bootstrap p-value will be applied and the number of re-samples is equal to R. The estimate of the common mean used in the bootstrap to transform the data under the null hypothesis the mean vector of the combined sample, of all the observations.
</p>
<p>The built-in command <code><a href="stats.html#topic+manova">manova</a></code> does the same thing exactly. Try it, the asymptotic <code class="reqn">F</code> test is what you have to see. In addition, this command allows for more mean vector hypothesis testing for more than two groups. I noticed this command after I had written my function and nevertheless as I mention in the introduction this document has an educational character as well.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>mesoi</code></td>
<td>

<p>The two mean vectors.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>The test statistic, the p-value, the critical value and the degrees of freedom of the F distribution (numerator and denominator).
This is given if no bootstrap calibration is employed.
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>

<p>The bootstrap p-value is bootstrap is employed.
</p>
</td></tr>
<tr><td><code>note</code></td>
<td>

<p>A message informing the user that bootstrap calibration has been employed.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime of the bootstrap calibration.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Everitt B. (2005). An R and S-Plus Companion to Multivariate Analysis. Springer.
</p>
<p>Mardia K.V., Kent J.T. and Bibby J.M. (1979). Multivariate Analysis. London: Academic
Press.
</p>
<p>Tsagris M., Preston S. and Wood A.T.A. (2017). Nonparametric hypothesis
testing for equality of means on the simplex.
Journal of Statistical Computation and Simulation, 87(2): 406&ndash;422.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+james">james</a>, <a href="#topic+maov">maov</a>, <a href="#topic+el.test2">el.test2</a>, <a href="#topic+eel.test2">eel.test2</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>hotel2T2( as.matrix(iris[1:25, 1:4]), as.matrix(iris[26:50, 1:4]) )
hotel2T2( as.matrix(iris[1:25, 1:4]), as.matrix(iris[26:50, 1:4]), R = 1 )
</code></pre>

<hr>
<h2 id='Hypothesis+20test+20for+20two+20high-dimensional+20mean+20vectors'>
Hypothesis test for two high-dimensional mean vectors
</h2><span id='topic+sarabai'></span>

<h3>Description</h3>

<p>Hypothesis test for two high-dimensional mean vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sarabai(x1, x2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Hypothesis+2B20test+2B20for+2B20two+2B20high-dimensional+2B20mean+2B20vectors_+3A_x1">x1</code></td>
<td>

<p>A matrix containing the Euclidean data of the first group.
</p>
</td></tr>
<tr><td><code id="Hypothesis+2B20test+2B20for+2B20two+2B20high-dimensional+2B20mean+2B20vectors_+3A_x2">x2</code></td>
<td>

<p>A matrix containing the Euclidean data of the second group.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>High dimensional data are the multivariate data which have many variables (<code class="reqn">p</code>) and usually a small number of observations (<code class="reqn">n</code>). It also happens that <code class="reqn">p&gt;n</code> and this is the case here in this Section. We will see a simple test for the case of <code class="reqn">p&gt;n</code>. In this case, the covariance matrix is not invertible and in addition it can have a lot of zero eigenvalues.
</p>
<p>The test we will see was proposed by Bai and Saranadasa (1996). Ever since, there have been some more suggestions but I chose this one for its simplicity. There are two datasets, <code class="reqn">{\bf X}_1</code> and <code class="reqn">{\bf X}_2</code> of sample sizes <code class="reqn">n_1</code> and <code class="reqn">n_2</code>, respectively. Their corresponding sample mean vectors and covariance matrices are <code class="reqn">\bar{{\bf X}}_1</code>, <code class="reqn">\bar{{\bf X}}_2</code> and <code class="reqn">{\bf S}_1</code>, <code class="reqn">{\bf S}_2</code> respectively. The assumption here is the same as that of the Hotelling's test we saw before.
</p>
<p>Let us define the pooled covariance matrix at first, calculated under the assumption of equal covariance matrices
<code class="reqn">
{\bf S}_n=\frac{\left(n_1-1\right){\bf S}_1+\left(n_2-1\right){\bf S}_2}{n}</code>,
where <code class="reqn">n=n_1+n_2</code>. Then define <code class="reqn">B_n=\sqrt{ \frac{n^2}{\left(n+2\right)\left(n-1\right)}\left\lbrace\text{tr}\left({\bf S}_n^2\right)-
\frac{1}{n}\left[\text{tr}\left({\bf S}_n\right)\right]^2 \right\rbrace }</code>.
The test statistic is
</p>
<p style="text-align: center;"><code class="reqn">
Z=\frac{\frac{n_1n_2}{n_1+n_2}\left(\bar{{\bf X}}_1-\bar{{\bf X}}_2\right)^T\left(\bar{{\bf X}}_1-\bar{{\bf X}}_2\right)
-\text{tr}\left({\bf S}_n\right)}{\sqrt{\frac{2\left(n+1\right)}{n}}B_n}.
</code>
</p>

<p>Under the null hypothesis (equality of the two mean vectors) the test statistic follows the standard normal distribution. Bai and Saranadasa (1996) established the asymptotic normality of the test statistics and showed that it has attractive power property when <code class="reqn">p/n \rightarrow c &lt; \infty</code> and under some restriction on the maximum eigenvalue of the common population covariance matrix. However, the requirement of <code class="reqn">p</code> and <code class="reqn">n</code> being of the same order is too restrictive to be used in the &quot;large <code class="reqn">p</code> small <code class="reqn">n</code>&quot; situation.
</p>


<h3>Value</h3>

<p>A vector with the test statistic and the p-value.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Bai Z. D. and Saranadasa H. (1996). Effect of high dimension: by an example of a two
sample problem. Statistica Sinica, 6(2): 311&ndash;329.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hotel2T2">hotel2T2</a>, <a href="#topic+maov">maov</a>, <a href="#topic+el.test2">el.test2</a>, <a href="#topic+eel.test2">eel.test2</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x1 &lt;- matrix( rnorm(40 * 100), ncol = 100 )
x2 &lt;- matrix( rnorm(50 * 100), ncol = 100 )
sarabai(x1, x2)
</code></pre>

<hr>
<h2 id='James+20multivariate+20version+20of+20the+20t-test'>
James multivariate version of the t-test
</h2><span id='topic+james'></span>

<h3>Description</h3>

<p>James test for testing the equality of two population mean vectors without assuming equality of the covariance matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>james(y1, y2, a = 0.05, R = 999, graph = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="James+2B20multivariate+2B20version+2B20of+2B20the+2B20t-test_+3A_y1">y1</code></td>
<td>

<p>A matrix containing the Euclidean data of the first group.
</p>
</td></tr>
<tr><td><code id="James+2B20multivariate+2B20version+2B20of+2B20the+2B20t-test_+3A_y2">y2</code></td>
<td>

<p>A matrix containing the Euclidean data of the second group.
</p>
</td></tr>
<tr><td><code id="James+2B20multivariate+2B20version+2B20of+2B20the+2B20t-test_+3A_a">a</code></td>
<td>

<p>The significance level, set to 0.05 by default.
</p>
</td></tr>
<tr><td><code id="James+2B20multivariate+2B20version+2B20of+2B20the+2B20t-test_+3A_r">R</code></td>
<td>

<p>If R is 1 no bootstrap calibration is performed and the classical p-value via the
F distribution is returned. If R is greater than 1, the bootstrap p-value is returned.
</p>
</td></tr>
<tr><td><code id="James+2B20multivariate+2B20version+2B20of+2B20the+2B20t-test_+3A_graph">graph</code></td>
<td>

<p>A boolean variable which is taken into consideration only when bootstrap calibration is performed.
If TRUE the histogram of the bootstrap test statistic values is plotted.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Here we show the modified version of the two-sample <code class="reqn">T^2</code> test (function <code><a href="#topic+hotel2T2">hotel2T2</a></code>) in the case where the two covariances matrices cannot be assumed to be equal.
</p>
<p>James (1954) proposed a test for linear hypotheses of the population means when the variances (or the covariance matrices) are not known. Its form for two <code class="reqn">p</code>-dimensional samples is:
</p>
<p style="text-align: center;"><code class="reqn">
T^2_u=\left(\bar{{\bf X}}_1-\bar{{\bf X}}_2\right)^T\tilde{{\bf S}}^{-1}\left(\bar{{\bf X}}_1-\bar{{\bf X}}_2\right),
</code>
</p>

<p>where
<code class="reqn">\tilde{{\bf S}}=\tilde{{\bf S}_1}+\tilde{{\bf S}_2}=\frac{{\bf S}_1}{n_1}+\frac{{\bf S}_2}{n_2}
</code>.
</p>
<p>James (1954) suggested that the test statistic is compared with <code class="reqn">2h\left(\alpha\right)</code>, a corrected <code class="reqn">\chi^2</code> distribution whose form is
</p>
<p style="text-align: center;"><code class="reqn">
2h\left(\alpha\right)=\chi^2\left(A+B\chi^2\right),
</code>
</p>

<p>where
<code class="reqn">A=1+\frac{1}{2p}\sum_{i=1}^2\frac{\left(tr \tilde{{\bf S}}^{-1}\tilde{{\bf S}_i}\right)^2}{n_i-1}</code> and
<code class="reqn">B=\frac{1}{p\left(p+2\right)}\left[\sum_{i=1}^2\frac{tr\left(\tilde{{\bf S}}^{-1}\tilde{{\bf S}_i}\right)^2}{n_i-1}+\frac{1}{2}\sum_{i=1}^2\frac{\left(\text{tr} \tilde{{\bf S}}^{-1}\tilde{{\bf S}_i}\right)^2}{n_i-1} \right]</code>.
</p>
<p>If you want to do bootstrap to get the p-value, then you must transform the data under the null hypothesis. The estimate of the common mean is given by Aitchison (1986)
</p>
<p style="text-align: center;"><code class="reqn">
\hat{\pmb{\mu}}_c =
\left(n_1{\bf S}_1^{-1}+n_2{\bf S}_2^{-1}\right)^{-1}\left(n_1{\bf S}_1^{-1}\bar{{\bf X}}_1+n_2{\bf S}_2^{-1}\bar{{\bf X}}_2\right)=
\left(\tilde{{\bf S}}_1^{-1}+\tilde{{\bf S}}_2^{-1}\right)^{-1}\left(\tilde{{\bf S}}_1^{-1}\bar{{\bf X}}_1+\tilde{{\bf S}}_2^{-1}\bar{{\bf X}}_2\right).
</code>
</p>

<p>The modified Nel and van der Merwe (1986) test is based on the same quadratic form as that of James (1954) but the distribution used to compare the value of the test statistic is different.
It is shown in Krishnamoorthy and Yanping (2006) that <code class="reqn">T^2_u \sim \frac{\nu p}{\nu-p+1}F_{p,\nu-p+1}</code> approximately, where
<code class="reqn">
\nu=\frac{p+p^2}{\frac{1}{n_1}\left\lbrace \text{tr}\left[ \left( {\bf S}_1\tilde{{\bf S}} \right)^2\right]+
\text{tr}\left[ \left( {\bf S}_1\tilde{{\bf S}} \right)\right]^2 \right\rbrace +
\frac{1}{n_2}\left\lbrace \text{tr}\left[ \left( {\bf S}_2\tilde{{\bf S}}\right)^2\right]+
\text{tr}\left[ \left( {\bf S}_2\tilde{{\bf S}} \right)\right]^2 \right\rbrace }.
</code>
</p>
<p>The algorithm is taken by Krishnamoorthy and Yu (2004).
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>note</code></td>
<td>

<p>A message informing the user about the test used.
</p>
</td></tr>
<tr><td><code>mesoi</code></td>
<td>

<p>The two mean vectors.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>The test statistic, the p-value, the correction factor and the corrected critical value
of the chi-square distribution if the James test has been used or, the test statistic,
the p-value, the critical value and the degrees of freedom (numerator and denominator)
of the F distribution if the modified James test has been used.
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>

<p>The bootstrap p-value if bootstrap is employed.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime of the bootstrap calibration.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Aitchison J. (1986). The statistical analysis of compositional data. Chapman &amp; Hall.
</p>
<p>James G.S. (1954). Tests of Linear Hypothese in Univariate and Multivariate Analysis
when the Ratios of the Population Variances are Unknown. Biometrika, 41(1/2): 19&ndash;43.
</p>
<p>Krishnamoorthy K. and Yu J. (2004). Modified Nel and Van der Merwe test for the multivariate
Behrens-Fisher problem. Statistics &amp; Probability Letters, 66(2): 161&ndash;169.
</p>
<p>Krishnamoorthy K. and Yanping Xia (2006). On Selecting Tests for Equality of Two Normal Mean Vectors.
Multivariate Behavioral Research, 41(4): 533&ndash;548.
</p>
<p>Tsagris M., Preston S. and Wood A.T.A. (2017). Nonparametric hypothesis testing for
equality of means on the simplex. Journal of Statistical Computation and Simulation, 87(2): 406&ndash;422.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hotel2T2">hotel2T2</a>, <a href="#topic+maovjames">maovjames</a>, <a href="#topic+el.test2">el.test2</a>, <a href="#topic+eel.test2">eel.test2</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>james( as.matrix(iris[1:25, 1:4]), as.matrix(iris[26:50, 1:4]), R = 1 )
james( as.matrix(iris[1:25, 1:4]), as.matrix(iris[26:50, 1:4]), R = 2 )
james( as.matrix(iris[1:25, 1:4]), as.matrix(iris[26:50, 1:4]) )
</code></pre>

<hr>
<h2 id='Log-likelihood+20ratio+20test+20for+20equality+20of+20one+20covariance+20matrix'>
Log-likelihood ratio test for equality of one covariance matrix
</h2><span id='topic+equal.cov'></span>

<h3>Description</h3>

<p>Log-likelihood ratio test for equality of one covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>equal.cov(x, Sigma, a = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Log-likelihood+2B20ratio+2B20test+2B20for+2B20equality+2B20of+2B20one+2B20covariance+2B20matrix_+3A_x">x</code></td>
<td>

<p>A matrix containing Euclidean data.
</p>
</td></tr>
<tr><td><code id="Log-likelihood+2B20ratio+2B20test+2B20for+2B20equality+2B20of+2B20one+2B20covariance+2B20matrix_+3A_sigma">Sigma</code></td>
<td>

<p>The hypothesis covariance matrix.
</p>
</td></tr>
<tr><td><code id="Log-likelihood+2B20ratio+2B20test+2B20for+2B20equality+2B20of+2B20one+2B20covariance+2B20matrix_+3A_a">a</code></td>
<td>

<p>The significance level, set to 0.05 by default.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The hypothesis test is that the the sample covariance is equal to some specified covariance matrix: <code class="reqn">H_0:\pmb{\Sigma}=\pmb{\Sigma}_0</code>, with <code class="reqn">\pmb{\mu}</code> unknown. The algorithm for this test is taken from Mardia, Bibby and Kent (1979, pg. 126-127).
The test is based upon the log-likelihood ratio test. The form of the test is
</p>
<p style="text-align: center;"><code class="reqn">
-2\log{\lambda}=n \text{tr}\left\lbrace \pmb{\Sigma}_0^{-1}{\bf S}\right\rbrace-n\log{\left|\pmb{\Sigma}_0^{-1}{\bf S} \right|}-np,
</code>
</p>

<p>where <code class="reqn">n</code> is the sample size, <code class="reqn">\pmb{\Sigma}_0</code> is the specified covariance matrix under the null hypothesis, <code class="reqn">{\bf S}</code> is the sample covariance matrix and <code class="reqn">p</code> is the dimensionality of the data (or the number of variables). Let <code class="reqn">\alpha</code> and <code class="reqn">g</code> denote the arithmetic mean and the geometric mean respectively of the eigenvalues of <code class="reqn">\pmb{\Sigma}_0^{-1}{\bf S}</code>, so that <code class="reqn">tr\left\lbrace \pmb{\Sigma}_0^{-1}{\bf S}\right\rbrace=p\alpha</code> and
<code class="reqn">\left|\pmb{\Sigma}_0^{-1}{\bf S} \right|=g^p</code>, then the test statistic becomes
</p>
<p style="text-align: center;"><code class="reqn">
-2\log{\lambda}=np\left(\alpha-log{(g)}-1 \right).
</code>
</p>

<p>The degrees of freedom of the <code class="reqn">\chi^2</code> distribution are <code class="reqn">\frac{1}{2}p\left(p+1\right)</code>.
</p>


<h3>Value</h3>

<p>A vector with the the test statistic, the p-value, the degrees of freedom and the critical value of the test.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Mardia K.V., Kent J.T. and Bibby J.M. (1979). Multivariate Analysis. London: Academic Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+likel.cov">likel.cov</a>, <a href="#topic+Mtest.cov">Mtest.cov</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix( iris[, 1:4] )
s &lt;- cov(x) * 1.5
equal.cov(x, s)
</code></pre>

<hr>
<h2 id='Log-likelihood+20ratio+20test+20for+20equality+20of+20two+20or+20more+20covariance+20matrices'>
Log-likelihood ratio test for equality of two or more covariance matrices
</h2><span id='topic+likel.cov'></span>

<h3>Description</h3>

<p>Log-likelihood ratio test for equality of two or more covariance matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>likel.cov(x, ina, a = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Log-likelihood+2B20ratio+2B20test+2B20for+2B20equality+2B20of+2B20two+2B20or+2B20more+2B20covariance+2B20matrices_+3A_x">x</code></td>
<td>

<p>A matrix containing Euclidean data.
</p>
</td></tr>
<tr><td><code id="Log-likelihood+2B20ratio+2B20test+2B20for+2B20equality+2B20of+2B20two+2B20or+2B20more+2B20covariance+2B20matrices_+3A_ina">ina</code></td>
<td>

<p>A vector denoting the groups of the data.
</p>
</td></tr>
<tr><td><code id="Log-likelihood+2B20ratio+2B20test+2B20for+2B20equality+2B20of+2B20two+2B20or+2B20more+2B20covariance+2B20matrices_+3A_a">a</code></td>
<td>

<p>The significance level, set to 0.05 by default.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Tthe hypothesis test is that of the equality of at least two covariance matrices: <code class="reqn">H_0:\pmb{\Sigma}_1=\ldots=\pmb{\Sigma}_k</code>. The algorithm is taken from Mardia, Bibby and Kent (1979, pg. 140). The log-likelihood ratio test is the multivariate generalization of Bartlett's test of homogeneity of variances. The test statistic takes the following form
</p>
<p style="text-align: center;"><code class="reqn">
-2log{\lambda}=n\log{\left|{\bf S}\right|}-\sum_{i=1}^kn_i\log{\left|{\bf S_i}\right|}=\sum_{i=1}^kn_i\log{\left|{\bf S}_i^{-1}{\bf S}\right|},
</code>
</p>

<p>where <code class="reqn">{\bf S}_i</code> is the <code class="reqn">i</code>-th sample biased covariance matrix and <code class="reqn">{\bf S}=n^{-1}\sum_{i=1}^kn_i{\bf S}_i</code> is the maximum likelihood estimate of the common covariance matrix (under the null hypothesis) with <code class="reqn">n=\sum_{i=1}^kn_i</code>. The degrees of freedom of the asymptotic chi-square distribution are <code class="reqn">\frac{1}{2}\left(p+1\right)\left(k-1\right)</code>.
</p>


<h3>Value</h3>

<p>A vector with the the test statistic, the p-value, the degrees of freedom and the critical value of the test.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Mardia K.V., Kent J.T. and Bibby J.M. (1979). Multivariate Analysis. London: Academic Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+equal.cov">equal.cov</a>, <a href="#topic+Mtest.cov">Mtest.cov</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix( iris[, 1:4] )
ina &lt;- iris[, 5]
likel.cov(x, ina)
</code></pre>

<hr>
<h2 id='Multivariate+20analysis+20of+20variance+20+28James+20test+29'>
Multivariate analysis of variance (James test)
</h2><span id='topic+maovjames'></span>

<h3>Description</h3>

<p>Multivariate analysis of variance without assuming equality of the covariance matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maovjames(x, ina, a = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Multivariate+2B20analysis+2B20of+2B20variance+2B20+2B28James+2B20test+2B29_+3A_x">x</code></td>
<td>

<p>A matrix containing Euclidean data.
</p>
</td></tr>
<tr><td><code id="Multivariate+2B20analysis+2B20of+2B20variance+2B20+2B28James+2B20test+2B29_+3A_ina">ina</code></td>
<td>

<p>A numerical or factor variable indicating the groups of the data.
</p>
</td></tr>
<tr><td><code id="Multivariate+2B20analysis+2B20of+2B20variance+2B20+2B28James+2B20test+2B29_+3A_a">a</code></td>
<td>

<p>The significance level, set to 0.005 by default.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>James (1954) also proposed an alternative to MANOVA  when the covariance matrices are not assumed equal. The test statistic for <code class="reqn">k</code> samples is
</p>
<p style="text-align: center;"><code class="reqn">
J=\sum_{i=1}^k\left(\bar{{\bf x}}_i-\bar{{\bf X}}\right)^T{\bf W}_i\left(\bar{{\bf x}}_i-\bar{{\bf X}}\right),
</code>
</p>

<p>where <code class="reqn">\bar{{\bf x}}_i</code> and <code class="reqn">n_i</code> are the sample mean vector and sample size of the <code class="reqn">i</code>-th sample respectively and <code class="reqn">{\bf W}_i=\left(\frac{{\bf S}_i}{n_i}\right)^{-1}</code>, where <code class="reqn">{\bf S}_i</code> is the covariance matrix of the <code class="reqn">i</code>-sample mean vector and <code class="reqn">\bar{{\bf X}}</code> is the estimate of the common mean <code class="reqn">\bar{{\bf X}}=\left(\sum_{i=1}^k{\bf W}_i\right)^{-1}\sum_{i=1}^k{\bf W}_i\bar{{\bf x}}_i</code>.
</p>
<p>Normally one would compare the test statistic with a <code class="reqn">\chi^2_{r,1-\alpha}</code>, where <code class="reqn">r=p\left(k-1\right)</code> are the degrees of freedom with <code class="reqn">k</code> denoting the number of groups and <code class="reqn">p</code> the dimensionality of the data. There are <code class="reqn">r</code> constraints (how many univariate means must be equal, so that the null hypothesis, that all the mean vectors are equal, holds true), that is where these degrees of freedom come from. James (1954) compared the test statistic with a corrected <code class="reqn">\chi^2</code> distribution instead. Let
<code class="reqn">A</code> and <code class="reqn">B</code> be
<code class="reqn">A= 1+\frac{1}{2r}\sum_{i=1}^k\frac{\left[\text{tr}\left({\bf I}_p-{\bf W}^{-1}{\bf W}_i\right)\right]^2}{n_i-1}</code> and <code class="reqn">B= \frac{1}{r\left(r+2\right)}\sum_{i=1}^k\left\lbrace\frac{\text{tr}\left[\left({\bf I}_p-{\bf W}^{-1}{\bf W}_i\right)^2\right]}{n_i-1}+\frac{\left[\text{tr}\left({\bf I}_p-{\bf W}^{-1}{\bf W}_i\right)\right]^2}{2\left(n_i-1\right)}\right\rbrace</code>.
</p>
<p>The corrected quantile of the <code class="reqn">\chi^2</code> distribution is given as before by
<code class="reqn">2h\left(\alpha\right)=\chi^2\left(A+B\chi^2\right)</code>.
</p>


<h3>Value</h3>

<p>A vector with the next 4 elements:
</p>
<table>
<tr><td><code>test</code></td>
<td>

<p>The test statistic.
</p>
</td></tr>
<tr><td><code>correction</code></td>
<td>

<p>The value of the correction factor.
</p>
</td></tr>
<tr><td><code>corr.critical</code></td>
<td>

<p>The corrected critical value of the chi-square distribution.
</p>
</td></tr>
<tr><td><code>p-value</code></td>
<td>

<p>The p-value of the corrected test statistic.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>James G.S. (1954). Tests of Linear Hypotheses in Univariate and Multivariate Analysis
when the Ratios of the Population Variances are Unknown. Biometrika, 41(1/2): 19&ndash;43.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+maov">maov</a>, <a href="#topic+hotel2T2">hotel2T2</a>, <a href="#topic+james">james</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>maov( as.matrix(iris[,1:4]), iris[,5] )
maovjames( as.matrix(iris[,1:4]), iris[,5] )
</code></pre>

<hr>
<h2 id='Multivariate+20analysis+20of+20variance+20assuming+20equality+20of+20the+20covariance+20matrices'>
Multivariate analysis of variance assuming equality of the covariance matrices
</h2><span id='topic+maov'></span>

<h3>Description</h3>

<p>Multivariate analysis of variance assuming equality of the covariance matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maov(x, ina)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Multivariate+2B20analysis+2B20of+2B20variance+2B20assuming+2B20equality+2B20of+2B20the+2B20covariance+2B20matrices_+3A_x">x</code></td>
<td>

<p>A matrix containing Euclidean data.
</p>
</td></tr>
<tr><td><code id="Multivariate+2B20analysis+2B20of+2B20variance+2B20assuming+2B20equality+2B20of+2B20the+2B20covariance+2B20matrices_+3A_ina">ina</code></td>
<td>

<p>A numerical or factor variable indicating the groups of the data.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Multivariate analysis of variance assuming equality of the covariance matrices.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>note</code></td>
<td>

<p>A message stating whether the <code class="reqn">F</code> or the <code class="reqn">chi^2</code> approximation has been used.
</p>
</td></tr>
<tr><td><code>result</code></td>
<td>

<p>The test statistic and the p-value.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Johnson R.A. and Wichern D.W. (2007, 6th Edition). Applied Multivariate Statistical Analysis, pg. 302&ndash;303.
</p>
<p>Todorov V. and Filzmoser P. (2010). Robust Statistic for the One-way MANOVA.
Computational Statistics &amp; Data Analysis, 54(1): 37&ndash;48.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+maovjames">maovjames</a>, <a href="#topic+hotel2T2">hotel2T2</a>, <a href="#topic+james">james</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>maov( as.matrix(iris[,1:4]), iris[,5] )
maovjames( as.matrix(iris[,1:4]), iris[,5] )
</code></pre>

<hr>
<h2 id='Relationship+20between+20Hotelling+27s+20T2+20test+20and+20James+27+20MANOVA'>
Relationship between Hotelling's <code class="reqn">T^2</code> test and James' MANOVA
</h2><span id='topic+maovjames.hotel'></span>

<h3>Description</h3>

<p>Relationship between Hotelling's <code class="reqn">T^2</code> test and James' MANOVA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maovjames.hotel(x, ina)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Relationship+2B20between+2B20Hotelling+2B27s+2B20T2+2B20test+2B20and+2B20James+2B27+2B20MANOVA_+3A_x">x</code></td>
<td>

<p>A matrix containing the Euclidean data of the first group.
</p>
</td></tr>
<tr><td><code id="Relationship+2B20between+2B20Hotelling+2B27s+2B20T2+2B20test+2B20and+2B20James+2B27+2B20MANOVA_+3A_ina">ina</code></td>
<td>

<p>A numerical or factor variable indicating the groups of the data.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The relationship for the James two sample test (see the function <code><a href="#topic+james.hotel">james.hotel</a></code>) is true for the case of the MANOVA. The estimate of the common mean, <code class="reqn">\pmb{mu}_c</code> (see the function <code><a href="#topic+james">james</a></code> for the expression of <code class="reqn">\pmb{\mu}_c</code>), is in general, for <code class="reqn">g</code> groups, each of sample size <code class="reqn">n_i</code>, written as
</p>
<p style="text-align: center;"><code class="reqn">
\hat{\pmb{\mu}}_c = \left(\sum_{i=1}^gn_i{\bf S}_i^{-1}\right)^{-1}\sum_{i=1}^gn_i{\bf S}_i^{-1}\bar{{\bf X}}_i.
</code>
</p>

<p>The function is just a proof of the mathematics you will find in Emerson (2009, pg. 76&ndash;81) and is again intended for educational purposes.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>test</code></td>
<td>

<p>The value of the test statistic, the sum of the two Hotelling's test statistic using
the common mean.
</p>
</td></tr>
<tr><td><code>mc</code></td>
<td>

<p>The common mean.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Emerson S. (2009). Small sample performance and calibration of the Empirical Likelihood method.
PhD thesis, Stanford university.
</p>
<p>James G.S. (1954). Tests of Linear Hypothese in Univariate and Multivariate Analysis
when the Ratios of the Population Variances are Unknown. Biometrika, 41(1/2): 19&ndash;43.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hotel2T2">hotel2T2</a>, <a href="#topic+maovjames">maovjames</a>, <a href="#topic+el.test2">el.test2</a>, <a href="#topic+eel.test2">eel.test2</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>maovjames.hotel( as.matrix(iris[, 1:4]), iris[, 5] )
maovjames( as.matrix(iris[, 1:4]), iris[, 5] )
</code></pre>

<hr>
<h2 id='Relationship+20between+20the+20Hotelling+27s+20and+20James+27+20tests'>
Relationship between the Hotelling's <code class="reqn">T^2</code> and James' test
</h2><span id='topic+james.hotel'></span>

<h3>Description</h3>

<p>Relationship between the Hotelling's <code class="reqn">T^2</code> and James' test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>james.hotel(x1, x2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Relationship+2B20between+2B20the+2B20Hotelling+2B27s+2B20and+2B20James+2B27+2B20tests_+3A_x1">x1</code></td>
<td>

<p>A matrix containing the Euclidean data of the first group.
</p>
</td></tr>
<tr><td><code id="Relationship+2B20between+2B20the+2B20Hotelling+2B27s+2B20and+2B20James+2B27+2B20tests_+3A_x2">x2</code></td>
<td>

<p>A matrix containing the Euclidean data of the second group.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Emerson (2009, pg. 76&ndash;81) mentioned a very nice result between the Hotelling's one sample <code class="reqn">T^2</code> and James test for two mean vectors
</p>
<p style="text-align: center;"><code class="reqn">
J\left(\pmb{\mu}\right) = T_1^2\left(\pmb{\mu}\right) + T_2^2\left(\pmb{\mu}\right),
</code>
</p>

<p>where <code class="reqn">J\left(\pmb{\mu}\right)</code> is the James test statistic (James, 1954) and <code class="reqn">T_1^2\left(\pmb{\mu}\right)</code> and <code class="reqn">T_1^2\left(\pmb{\mu}\right)</code> are the two one sample Hotelling's <code class="reqn">T^2</code> test statistic values (see function <code><a href="#topic+hotel1T2">hotel1T2</a></code>) for each sample from their common mean vector <code class="reqn">\pmb{\mu}_c</code> (see the help file of <code><a href="#topic+james">james</a></code>). In fact, James test statistic is found from minimizing the right hand side of the above expression with respect to <code class="reqn">\pmb{\mu}</code>. The sum is mimized when <code class="reqn">\pmb{\mu}</code> takes the form of the common mean vector <code class="reqn">\pmb{\mu}_c</code>. The same is true for the t-test in the univariate case.
</p>
<p>I have created this function illustrating this result, so this one is for educational purposes. It calculates the James test statistic, the sum of the two <code class="reqn">T^2</code> test statistics, the common mean vector and the one found via numerical optimization. In the univariate case, the common mean vector is a weighted linear combination of the two sample means. So, if we take a segment connecting the two means, the common mean is somewhere on that segment.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>tests</code></td>
<td>

<p>A vector with two values, the James test statistic value and the sum of the two Hotelling's test statistic using
the common mean.
</p>
</td></tr>
<tr><td><code>mathematics.mean</code></td>
<td>

<p>The common mean computed the closed form expression seen in the help file of <code><a href="#topic+james">james</a></code>.
</p>
</td></tr>
<tr><td><code>optimised.mean</code></td>
<td>

<p>The common mean vector obtained from the minimisation process.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Emerson S. (2009). Small sample performance and calibration of the Empirical Likelihood method.
PhD thesis, Stanford university.
</p>
<p>James G.S. (1954). Tests of Linear Hypothese in Univariate and Multivariate Analysis
when the Ratios of the Population Variances are Unknown. Biometrika, 41(1/2): 19&ndash;43.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hotel2T2">hotel2T2</a>, <a href="#topic+maovjames">maovjames</a>, <a href="#topic+el.test2">el.test2</a>, <a href="#topic+eel.test2">eel.test2</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>james.hotel( as.matrix(iris[1:50, 1:4]), as.matrix(iris[51:100, 1:4]) )
james( as.matrix(iris[1:50, 1:4]), as.matrix(iris[51:100, 1:4]), R = 1 )
</code></pre>

<hr>
<h2 id='Repeated+20measures+20ANOVA+20+28univariate+20data+29+20using+20Hotelling+27s+20T2+20test'>
Repeated measures ANOVA (univariate data) using Hotelling's <code class="reqn">T^2</code> test
</h2><span id='topic+rm.hotel'></span>

<h3>Description</h3>

<p>Repeated measures ANOVA (univariate data) using Hotelling's <code class="reqn">T^2</code> test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rm.hotel(x, a = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Repeated+2B20measures+2B20ANOVA+2B20+2B28univariate+2B20data+2B29+2B20using+2B20Hotelling+2B27s+2B20T2+2B20test_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the repeated measurements. Each column contains the
values of the repeated measurements.
</p>
</td></tr>
<tr><td><code id="Repeated+2B20measures+2B20ANOVA+2B20+2B28univariate+2B20data+2B29+2B20using+2B20Hotelling+2B27s+2B20T2+2B20test_+3A_a">a</code></td>
<td>

<p>The level of significance, default value is equal to 0.05.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We now show how can one use Hotelling's <code class="reqn">T^2</code> test to analyse univariate repeated measures. Univariate analysis of variance for repeated measures is the classical way, but we can use this multivariate test as well. In the repeated measures ANOVA case, we have many repeated observations from the same <code class="reqn">n</code> subjects, usually at different time points and the interest is to see whether the means of the samples are equal or not <code class="reqn">\mu_1=\mu_2=\ldots=\mu_k</code> assuming <code class="reqn">k</code> repeated measurements. We can of course change this null hypothesis and test many combinations of means. The idea in any case is to construct a matrix of contrasts. I will focus here in the first case only and in particular the null hypothesis and the matrix of contrasts <code class="reqn">\bf C</code> are
</p>
<p style="text-align: center;"><code class="reqn">
\left( {\begin{array}{c}
\mu_1=\mu_2 \\
\mu_2=\mu_3 \\
\vdots  \\
\mu_{k-1}=\mu_k \end{array}} \right)=
\left( {\begin{array}{ccccc}
1 &amp; -1 &amp; 0 &amp; \ldots &amp; 0 \\
1 &amp;  0 &amp; -1 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; 0 &amp; 0 &amp; \ldots &amp; -1 \\
\end{array}} \right)\pmb{\mu}={\bf C}\pmb{\mu}.
</code>
</p>

<p>The contrast matrix <code class="reqn">\bf C</code> has <code class="reqn">k-1</code> independent rows and if there is no treatment effect, <code class="reqn">{\bf C}\pmb{\mu}={\bf 0}</code>.
</p>
<p>The test statistic is
</p>
<p style="text-align: center;"><code class="reqn">
T_r^2=\frac{\left(n-k+1\right)}{\left(n-1\right)\left(k-1\right)}n\left({\bf C}\bar{\bf x}\right)^T
\left({\bf CSC}^T\right)^{-1}\left({\bf C}\bar{\bf x}\right) \sim F_{k-1,n-k+1}.
</code>
</p>



<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>m</code></td>
<td>

<p>The mean vector.
</p>
</td></tr>
<tr><td><code>result</code></td>
<td>

<p>A vector with the test statistic value, it's associated p-value, the numerator and denominator
degrees of freedom and the critical value.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+maov">maov</a>, <a href="#topic+hotel2T2">hotel2T2</a>, <a href="#topic+james">james</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4]) ## assume they are repeated measurements
rm.hotel(x)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
