<!DOCTYPE html><html><head><title>Help for package simest</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {simest}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cpen'>
<p>C code for convex penalized least squares regression.</p></a></li>
<li><a href='#cvx.lip.reg'>
<p>Convex Least Squares Regression with Lipschitz Constraint</p></a></li>
<li><a href='#cvx.lse.con.reg'>
<p>Convex Least Squares Regression.</p></a></li>
<li><a href='#cvx.lse.reg'>
<p>Convex Least Squares Regression.</p></a></li>
<li><a href='#cvx.pen.reg'>
<p>Penalized Smooth Convex Regression.</p></a></li>
<li><a href='#derivcvxpec'>
<p>C code for prediction using cvx.lse.reg, cvx.lip.reg and cvx.lse.con.reg.</p></a></li>
<li><a href='#fastmerge'>
<p>Pre-binning of Data Points.</p></a></li>
<li><a href='#penta'>
<p>C code for solving pentadiagonal linear equations.</p></a></li>
<li><a href='#predcvxpen'>
<p>C code for prediction using cvx.lse.reg, cvx.lip.reg and cvx.lse.con.reg for function and its derivatives.</p></a></li>
<li><a href='#sim.est'>
<p>Single Index Model Estimation: Objective Function Approach.</p></a></li>
<li><a href='#simestgcv'>
<p>Single Index Model Estimation: Objective Function Approach.</p></a></li>
<li><a href='#smooth.pen.reg'>
<p>Penalized Smooth/Smoothing Spline Regression.</p></a></li>
<li><a href='#solve.pentadiag'>
<p>Pentadiagonal Linear Solver.</p></a></li>
<li><a href='#spen_egcv'>
<p>C code for smoothing splines with randomized GCV computation.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Constrained Single Index Model Estimation</td>
</tr>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4</td>
</tr>
<tr>
<td>Author:</td>
<td>Arun Kumar Kuchibhotla &lt;arunku@wharton.upenn.edu&gt;,
		Rohit Kumar Patra &lt;rohit@stat.columbia.edu&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Arun Kumar Kuchibhotla &lt;arunku@wharton.upenn.edu&gt;</td>
</tr>
<tr>
<td>Date:</td>
<td>2017-04-08.</td>
</tr>
<tr>
<td>Depends:</td>
<td>nnls, cobs</td>
</tr>
<tr>
<td>Description:</td>
<td>Estimation of function and index vector in single index model with and without shape constraints including different smoothness conditions.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2017-04-24 22:50:33 UTC; arunku</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2017-04-25 14:35:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='cpen'>
C code for convex penalized least squares regression.
</h2><span id='topic+cpen'></span>

<h3>Description</h3>

<p>This function is only intended for an internal use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cpen(dim, t_input, z_input, w_input, a0_input,
	lambda_input, Ky_input, L_input, U_input,
	fun_input, res_input, flag, tol_input, 
	zhat_input, iter, Deriv_input)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cpen_+3A_dim">dim</code></td>
<td>
<p>vector of sample size and maximum iteration.</p>
</td></tr>
<tr><td><code id="cpen_+3A_t_input">t_input</code></td>
<td>
<p>x-vector in cvx.pen.reg.</p>
</td></tr>
<tr><td><code id="cpen_+3A_z_input">z_input</code></td>
<td>
<p>y-vector in cvx.pen.reg.</p>
</td></tr>
<tr><td><code id="cpen_+3A_w_input">w_input</code></td>
<td>
<p>w-vector in cvx.pen.reg.</p>
</td></tr>
<tr><td><code id="cpen_+3A_a0_input">a0_input</code></td>
<td>
<p>initial vector for iterative algorithm.</p>
</td></tr>
<tr><td><code id="cpen_+3A_lambda_input">lambda_input</code></td>
<td>
<p>lambda-value in cvx.pen.reg.</p>
</td></tr>
<tr><td><code id="cpen_+3A_ky_input">Ky_input</code></td>
<td>
<p>Internal vector used for algorithm.</p>
</td></tr>
<tr><td><code id="cpen_+3A_l_input">L_input</code></td>
<td>
<p>Internal vector. Set to 0.</p>
</td></tr>
<tr><td><code id="cpen_+3A_u_input">U_input</code></td>
<td>
<p>Internal vector. Set to 0.</p>
</td></tr>
<tr><td><code id="cpen_+3A_fun_input">fun_input</code></td>
<td>
<p>Internal vector. Set to 0.</p>
</td></tr>
<tr><td><code id="cpen_+3A_res_input">res_input</code></td>
<td>
<p>Internal vector. Set to 0.</p>
</td></tr>
<tr><td><code id="cpen_+3A_flag">flag</code></td>
<td>
<p>Logical for stop criterion.</p>
</td></tr>
<tr><td><code id="cpen_+3A_tol_input">tol_input</code></td>
<td>
<p>tolerance level used in cvx.pen.reg.</p>
</td></tr>
<tr><td><code id="cpen_+3A_zhat_input">zhat_input</code></td>
<td>
<p>Internal vector. Set to zero. Stores the final output.</p>
</td></tr>
<tr><td><code id="cpen_+3A_iter">iter</code></td>
<td>
<p>Iteration number inside the algorithm.</p>
</td></tr>
<tr><td><code id="cpen_+3A_deriv_input">Deriv_input</code></td>
<td>
<p>Internal vector. Set to zero. Stores the derivative vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the source for more details about the algorithm.
</p>


<h3>Value</h3>

<p>Does not return anything. Changes the inputs according to the iterations.
</p>


<h3>Author(s)</h3>

<p>Arun Kumar Kuchibhotla, arunku@wharton.upenn.edu.</p>


<h3>Source</h3>

<p>Dontchev, A. L., Qi, H. and Qi, L. (2003). Quadratic Convergence of Newton's Method for Convex Interpolation and Smoothing. Constructive Approximation, 19(1):123-143.
</p>

<hr>
<h2 id='cvx.lip.reg'>
Convex Least Squares Regression with Lipschitz Constraint
</h2><span id='topic+cvx.lip.reg'></span><span id='topic+cvx.lip.reg.default'></span><span id='topic+plot.cvx.lip.reg'></span><span id='topic+predict.cvx.lip.reg'></span><span id='topic+print.cvx.lip.reg'></span>

<h3>Description</h3>

<p>This function provides an estimate of the non-parametric regression function with a shape constraint of convexity and a smoothness constraint via a Lipschitz bound.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvx.lip.reg(t, z, w = NULL, L,...)
## Default S3 method:
cvx.lip.reg(t, z, w = NULL, L, ...)
## S3 method for class 'cvx.lip.reg'
plot(x,...)
## S3 method for class 'cvx.lip.reg'
print(x,...)
## S3 method for class 'cvx.lip.reg'
predict(object, newdata = NULL, deriv = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvx.lip.reg_+3A_t">t</code></td>
<td>
<p>a numeric vector giving the values of the predictor variable.</p>
</td></tr>
<tr><td><code id="cvx.lip.reg_+3A_z">z</code></td>
<td>
<p>a numeric vector giving the values of the response variable.</p>
</td></tr>
<tr><td><code id="cvx.lip.reg_+3A_w">w</code></td>
<td>
<p>an optional numeric vector of the same length as x; Defaults to all elements <code class="reqn">1/n</code>.</p>
</td></tr>
<tr><td><code id="cvx.lip.reg_+3A_l">L</code></td>
<td>
<p>a numeric value providing the Lipschitz bound on the function.</p>
</td></tr>
<tr><td><code id="cvx.lip.reg_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
<tr><td><code id="cvx.lip.reg_+3A_x">x</code></td>
<td>
<p>an object of class &lsquo;cvx.lip.reg&rsquo;.</p>
</td></tr>
<tr><td><code id="cvx.lip.reg_+3A_object">object</code></td>
<td>
<p>An object of class &lsquo;cvx.lip.reg&rsquo;.</p>
</td></tr>
<tr><td><code id="cvx.lip.reg_+3A_newdata">newdata</code></td>
<td>
<p>a matrix of new data points in the predict function.</p>
</td></tr>
<tr><td><code id="cvx.lip.reg_+3A_deriv">deriv</code></td>
<td>
<p>a numeric either 0 or 1 representing which derivative to evaluate.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function minimizes 
</p>
<p style="text-align: center;"><code class="reqn">\sum_{i=1}^n w_i(z_i - \theta_i)^2</code>
</p>
 
<p>subject to
</p>
<p style="text-align: center;"><code class="reqn">-L\le\frac{\theta_2 - \theta_1}{t_2 - t_1}\le\cdots\le\frac{\theta_n - \theta_{n-1}}{t_n - t_{n-1}}\le L</code>
</p>

<p>for sorted <code class="reqn">t</code> values and <code class="reqn">z</code> reorganized such that <code class="reqn">z_i</code> corresponds to the new sorted <code class="reqn">t_i</code>. This function uses the <code>nnls</code> function from the <code>nnls</code> package to perform the constrained minimization of least squares. <code>plot</code> function provides the scatterplot along with fitted curve; it also includes some diagnostic plots for residuals. Predict function now allows calculating the first derivative also.
</p>


<h3>Value</h3>

<p>An object of class &lsquo;cvx.lip.reg&rsquo;, basically a list including the elements
</p>
<table>
<tr><td><code>x.values</code></td>
<td>
<p>sorted &lsquo;t&rsquo; values provided as input.</p>
</td></tr>
<tr><td><code>y.values</code></td>
<td>
<p>corresponding &lsquo;z&rsquo; values in input.</p>
</td></tr>
<tr><td><code>fit.values</code></td>
<td>
<p>corresponding fit values of same length as that of &lsquo;x.values&rsquo;.</p>
</td></tr>
<tr><td><code>deriv</code></td>
<td>
<p>corresponding values of the derivative of same length as that of &lsquo;x.values&rsquo;.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>residuals obtained from the fit.</p>
</td></tr>
<tr><td><code>minvalue</code></td>
<td>
<p>minimum value of the objective function attained.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Always set to 1.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>a numeric indicating the convergence of the code.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arun Kumar Kuchibhotla, arunku@wharton.upenn.edu.</p>


<h3>Source</h3>

<p>Lawson, C. L and Hanson, R. J. (1995). Solving Least Squares Problems. SIAM.
</p>


<h3>References</h3>

<p>Chen, D. and Plemmons, R. J. (2009). Non-negativity Constraints in Numerical Analysis. Symposium on the Birth of Numerical Analysis. 
</p>


<h3>See Also</h3>

<p>See also the function <code><a href="nnls.html#topic+nnls">nnls</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>args(cvx.lip.reg)
x &lt;- runif(50,-1,1)
y &lt;- x^2 + rnorm(50,0,0.3)
tmp &lt;- cvx.lip.reg(x, y, L = 10)
print(tmp)
plot(tmp)
predict(tmp, newdata = rnorm(10,0,0.1))
</code></pre>

<hr>
<h2 id='cvx.lse.con.reg'>
Convex Least Squares Regression.
</h2><span id='topic+cvx.lse.con.reg'></span><span id='topic+cvx.lse.con.reg.default'></span>

<h3>Description</h3>

<p>This function provides an estimate of the non-parametric regression function with a shape constraint of convexity and no smoothness constraint. Note that convexity by itself provides some implicit smoothness.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvx.lse.con.reg(t, z, w = NULL,...)
## Default S3 method:
cvx.lse.con.reg(t, z, w = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvx.lse.con.reg_+3A_t">t</code></td>
<td>
<p>a numeric vector giving the values of the predictor variable.</p>
</td></tr>
<tr><td><code id="cvx.lse.con.reg_+3A_z">z</code></td>
<td>
<p>a numeric vector giving the values of the response variable.</p>
</td></tr>
<tr><td><code id="cvx.lse.con.reg_+3A_w">w</code></td>
<td>
<p>an optional numeric vector of the same length as t; Defaults to all elements <code class="reqn">1/n</code>.</p>
</td></tr>
<tr><td><code id="cvx.lse.con.reg_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>	
</table>


<h3>Details</h3>

<p>This function does the same thing as <code>cvx.lse.reg</code> except that here we use conreg function from <code>cobs</code> package which is faster than <code>cvx.lse.reg</code>. The plot, predict, print functions of cvx.lse.reg also apply for cvx.lse.con.reg.
</p>


<h3>Value</h3>

<p>An object of class &lsquo;cvx.lse.reg&rsquo;, basically a list including the elements
</p>
<table>
<tr><td><code>x.values</code></td>
<td>
<p>sorted &lsquo;t&rsquo; values provided as input.</p>
</td></tr>
<tr><td><code>y.values</code></td>
<td>
<p>corresponding &lsquo;z&rsquo; values in input.</p>
</td></tr>
<tr><td><code>fit.values</code></td>
<td>
<p>corresponding fit values of same length as that of &lsquo;x.values&rsquo;.</p>
</td></tr>
<tr><td><code>deriv</code></td>
<td>
<p>corresponding values of the derivative of same length as that of &lsquo;x.values&rsquo;.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of steps taken to complete the iterations.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>residuals obtained from the fit.</p>
</td></tr>
<tr><td><code>minvalue</code></td>
<td>
<p>minimum value of the objective function attained.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>a numeric indicating the convergence of the code. Always set to 1.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arun Kumar Kuchibhotla, arunku@wharton.upenn.edu</p>


<h3>Source</h3>

<p>Lawson, C. L and Hanson, R. J. (1995). Solving Least Squares Problems. SIAM.
</p>


<h3>References</h3>

<p>Chen, D. and Plemmons, R. J. (2009). Non-negativity Constraints in Numerical Analysis. Symposium on the Birth of Numerical Analysis. 
</p>
<p>Liao, X. and Meyer, M. C. (2014). coneproj: An R package for the primal or dual cone projections with routines for constrained regression. Journal of Statistical Software 61(12), 1 &ndash; 22.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>args(cvx.lse.con.reg)
x &lt;- runif(50,-1,1)
y &lt;- x^2 + rnorm(50,0,0.3)
tmp &lt;- cvx.lse.con.reg(x, y)
print(tmp)
plot(tmp)
predict(tmp, newdata = rnorm(10,0,0.1))
</code></pre>

<hr>
<h2 id='cvx.lse.reg'>
Convex Least Squares Regression.
</h2><span id='topic+cvx.lse.reg'></span><span id='topic+cvx.lse.reg.default'></span><span id='topic+plot.cvx.lse.reg'></span><span id='topic+predict.cvx.lse.reg'></span><span id='topic+print.cvx.lse.reg'></span>

<h3>Description</h3>

<p>This function provides an estimate of the non-parametric regression function with a shape constraint of convexity and no smoothness constraint. Note that convexity by itself provides some implicit smoothness.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvx.lse.reg(t, z, w = NULL,...)
## Default S3 method:
cvx.lse.reg(t, z, w = NULL, ...)
## S3 method for class 'cvx.lse.reg'
plot(x,...)
## S3 method for class 'cvx.lse.reg'
print(x,...)
## S3 method for class 'cvx.lse.reg'
predict(object, newdata = NULL, deriv = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvx.lse.reg_+3A_t">t</code></td>
<td>
<p>a numeric vector giving the values of the predictor variable.</p>
</td></tr>
<tr><td><code id="cvx.lse.reg_+3A_z">z</code></td>
<td>
<p>a numeric vector giving the values of the response variable.</p>
</td></tr>
<tr><td><code id="cvx.lse.reg_+3A_w">w</code></td>
<td>
<p>an optional numeric vector of the same length as t; Defaults to all elements <code class="reqn">1/n</code>.</p>
</td></tr>
<tr><td><code id="cvx.lse.reg_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>	
<tr><td><code id="cvx.lse.reg_+3A_x">x</code></td>
<td>
<p>An object of class &lsquo;cvx.lse.reg&rsquo;. This is for plot and print function.</p>
</td></tr>
<tr><td><code id="cvx.lse.reg_+3A_object">object</code></td>
<td>
<p>An object of class &lsquo;cvx.lse.reg&rsquo;.</p>
</td></tr>
<tr><td><code id="cvx.lse.reg_+3A_newdata">newdata</code></td>
<td>
<p>a matrix of new data points in the predict function.</p>
</td></tr>
<tr><td><code id="cvx.lse.reg_+3A_deriv">deriv</code></td>
<td>
<p>a numeric either 0 or 1 representing which derivative to evaluate.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function minimizes 
</p>
<p style="text-align: center;"><code class="reqn">\sum_{i=1}^n w_i(z_i - \theta_i)^2</code>
</p>
 
<p>subject to
</p>
<p style="text-align: center;"><code class="reqn">\frac{\theta_2 - \theta_1}{t_2 - t_1}\le\cdots\le\frac{\theta_n - \theta_{n-1}}{t_n - t_{n-1}}</code>
</p>

<p>for sorted <code class="reqn">t</code> values and <code class="reqn">z</code> reorganized such that <code class="reqn">z_i</code> corresponds to the new sorted <code class="reqn">t_i</code>. This function previously used the <code>coneA</code> function from the <code>coneproj</code> package to perform the constrained minimization of least squares. Currently, the code makes use of the <code>nnls</code> function from <code>nnls</code> package for the same purpose. <code>plot</code> function provides the scatterplot along with fitted curve; it also includes some diagnostic plots for residuals. Predict function now allows computation of the first derivative.
</p>


<h3>Value</h3>

<p>An object of class &lsquo;cvx.lse.reg&rsquo;, basically a list including the elements
</p>
<table>
<tr><td><code>x.values</code></td>
<td>
<p>sorted &lsquo;t&rsquo; values provided as input.</p>
</td></tr>
<tr><td><code>y.values</code></td>
<td>
<p>corresponding &lsquo;z&rsquo; values in input.</p>
</td></tr>
<tr><td><code>fit.values</code></td>
<td>
<p>corresponding fit values of same length as that of &lsquo;x.values&rsquo;.</p>
</td></tr>
<tr><td><code>deriv</code></td>
<td>
<p>corresponding values of the derivative of same length as that of &lsquo;x.values&rsquo;.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of steps taken to complete the iterations.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>residuals obtained from the fit.</p>
</td></tr>
<tr><td><code>minvalue</code></td>
<td>
<p>minimum value of the objective function attained.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>a numeric indicating the convergence of the code.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arun Kumar Kuchibhotla, arunku@wharton.upenn.edu</p>


<h3>Source</h3>

<p>Lawson, C. L and Hanson, R. J. (1995). Solving Least Squares Problems. SIAM.
</p>


<h3>References</h3>

<p>Chen, D. and Plemmons, R. J. (2009). Non-negativity Constraints in Numerical Analysis. Symposium on the Birth of Numerical Analysis. 
</p>
<p>Liao, X. and Meyer, M. C. (2014). coneproj: An R package for the primal or dual cone projections with routines for constrained regression. Journal of Statistical Software 61(12), 1 &ndash; 22.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>args(cvx.lse.reg)
x &lt;- runif(50,-1,1)
y &lt;- x^2 + rnorm(50,0,0.3)
tmp &lt;- cvx.lse.reg(x, y)
print(tmp)
plot(tmp)
predict(tmp, newdata = rnorm(10,0,0.1))
</code></pre>

<hr>
<h2 id='cvx.pen.reg'>
Penalized Smooth Convex Regression.
</h2><span id='topic+cvx.pen.reg'></span><span id='topic+cvx.pen.reg.default'></span><span id='topic+plot.cvx.pen.reg'></span><span id='topic+predict.cvx.pen.reg'></span><span id='topic+print.cvx.pen.reg'></span>

<h3>Description</h3>

<p>This function provides an estimate of the non-parametric regression function with a shape constraint of convexity and smoothness constraint provided through square integral of second derivative.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvx.pen.reg(x, y, lambda, w = NULL, tol = 1e-05, maxit = 1000,...)
## Default S3 method:
cvx.pen.reg(x, y, lambda, w = NULL, tol = 1e-05, maxit = 1000,...)
## S3 method for class 'cvx.pen.reg'
plot(x,...)
## S3 method for class 'cvx.pen.reg'
print(x,...)
## S3 method for class 'cvx.pen.reg'
predict(object, newdata = NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvx.pen.reg_+3A_x">x</code></td>
<td>
<p>a numeric vector giving the values of the predictor variable. For <code>plot</code> and <code>print</code> functions, <code>x</code> represents an object of class <code>cvx.pen.reg</code>.</p>
</td></tr>
<tr><td><code id="cvx.pen.reg_+3A_y">y</code></td>
<td>
<p>a numeric vector giving the values of the response variable.</p>
</td></tr>
<tr><td><code id="cvx.pen.reg_+3A_lambda">lambda</code></td>
<td>
<p>a numeric value giving the penalty value.</p>
</td></tr>
<tr><td><code id="cvx.pen.reg_+3A_w">w</code></td>
<td>
<p>an optional numeric vector of the same length as x; Defaults to all 1.</p>
</td></tr>
<tr><td><code id="cvx.pen.reg_+3A_maxit">maxit</code></td>
<td>
<p>an integer giving the maxmimum number of steps taken by the algorithm; defaults to 1000.</p>
</td></tr>
<tr><td><code id="cvx.pen.reg_+3A_tol">tol</code></td>
<td>
<p>a numeric providing the tolerance level for convergence.</p>
</td></tr>
<tr><td><code id="cvx.pen.reg_+3A_...">...</code></td>
<td>
<p>any additional arguments.</p>
</td></tr>
<tr><td><code id="cvx.pen.reg_+3A_object">object</code></td>
<td>
<p>An object of class &lsquo;cvx.pen.reg&rsquo;. This is for predict function.</p>
</td></tr>
<tr><td><code id="cvx.pen.reg_+3A_newdata">newdata</code></td>
<td>
<p>a vector of new data points to be used in the predict function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function minimizes 
</p>
<p style="text-align: center;"><code class="reqn">\sum_{i=1}^n w_i(y_i - f(x_i))^2 + \lambda\int\{f''(x)\}^2dx</code>
</p>
 
<p>subject to convexity constraint on <code class="reqn">f</code>. <code>plot</code> function provides the scatterplot along with fitted curve; it also includes some diagnostic plots for residuals. Predict function returns a matrix containing the inputted newdata along with the function values, derivatives and second derivatives.
</p>


<h3>Value</h3>

<p>An object of class &lsquo;cvx.pen.reg&rsquo;, basically a list including the elements
</p>
<table>
<tr><td><code>x.values</code></td>
<td>
<p>sorted &lsquo;x&rsquo; values provided as input.</p>
</td></tr>
<tr><td><code>y.values</code></td>
<td>
<p>corresponding &lsquo;y&rsquo; values in input.</p>
</td></tr>
<tr><td><code>fit.values</code></td>
<td>
<p>corresponding fit values of same length as that of &lsquo;x.values&rsquo;.</p>
</td></tr>
<tr><td><code>deriv</code></td>
<td>
<p>corresponding values of the derivative of same length as that of &lsquo;x.values&rsquo;.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of steps taken to complete the iterations.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>residuals obtained from the fit.</p>
</td></tr>
<tr><td><code>minvalue</code></td>
<td>
<p>minimum value of the objective function attained.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>a numeric indicating the convergence of the code.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>a numeric vector of length 2 less than &lsquo;x&rsquo;. This represents the coefficients of the B-splines in the second derivative of the estimator.</p>
</td></tr>
<tr><td><code>AlphaMVal</code></td>
<td>
<p>a numeric vector needed for predict function.</p>
</td></tr>
<tr><td><code>lower</code></td>
<td>
<p>a numeric vector needed for predict function.</p>
</td></tr>
<tr><td><code>upper</code></td>
<td>
<p>a numeric vector needed for predict function.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arun Kumar Kuchibhotla, arunku@wharton.upenn.edu, 
Rohit Kumar Patra, rohit@stat.columbia.edu.</p>


<h3>Source</h3>

<p>Elfving, T. and Andersson, L. (1988). An Algorithm for Computing Constrained Smoothing Spline Functions. Numer. Math., 52(5):583&ndash;595.
</p>
<p>Dontchev, A. L., Qi, H. and Qi, L. (2003). Quadratic Convergence of Newton's Method for Convex Interpolation and Smoothing. Constructive Approximation, 19(1):123-143.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>args(cvx.pen.reg)
x &lt;- runif(50,-1,1)
y &lt;- x^2 + rnorm(50,0,0.3)
tmp &lt;- cvx.pen.reg(x, y, lambda = 0.01)
print(tmp)
plot(tmp)
predict(tmp, newdata = rnorm(10,0,0.1))
</code></pre>

<hr>
<h2 id='derivcvxpec'>
C code for prediction using cvx.lse.reg, cvx.lip.reg and cvx.lse.con.reg.
</h2><span id='topic+derivcvxpec'></span>

<h3>Description</h3>

<p>This function is only intended for an internal use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>derivcvxpec(dim, t, zhat, D, kk)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="derivcvxpec_+3A_dim">dim</code></td>
<td>
<p>vector of sample size, size of newdata and which derivative to compute.</p>
</td></tr>
<tr><td><code id="derivcvxpec_+3A_t">t</code></td>
<td>
<p>x-vector in cvx.lse.reg and others.</p>
</td></tr>
<tr><td><code id="derivcvxpec_+3A_zhat">zhat</code></td>
<td>
<p>prediction obtained from cvx.lse.reg and others.</p>
</td></tr>
<tr><td><code id="derivcvxpec_+3A_d">D</code></td>
<td>
<p>derivative vector obtained from cvx.lse.reg and others.</p>
</td></tr>
<tr><td><code id="derivcvxpec_+3A_kk">kk</code></td>
<td>
<p>vector storing the final prediction.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimate is a linear interpolator and the algorithm implements this.
</p>


<h3>Value</h3>

<p>Does not return anything. Changes the inputs according to the algorithm.
</p>


<h3>Author(s)</h3>

<p>Arun Kumar Kuchibhotla, arunku@wharton.upenn.edu.</p>

<hr>
<h2 id='fastmerge'>
Pre-binning of Data Points. 
</h2><span id='topic+fastmerge'></span>

<h3>Description</h3>

<p>Numerical tolerance problems in non-parametric regression makes it necessary for pre-binning of data points. This procedure is implicitly performed by most of the regression function in R. This function implements this procedure with a given tolerance level.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fastmerge(DataMat, w = NULL, tol = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fastmerge_+3A_datamat">DataMat</code></td>
<td>
<p>a numeric matrix/vector with rows as data points.</p>
</td></tr>
<tr><td><code id="fastmerge_+3A_w">w</code></td>
<td>
<p>an optional numeric vector of the same length as <code class="reqn">x</code>; Defaults to all elements 1.</p>
</td></tr>
<tr><td><code id="fastmerge_+3A_tol">tol</code></td>
<td>
<p>a numeric value providing the tolerance for identifying duplicates with respect to the first column.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If two values in the first column of DataMat are separated by a value less than tol then the corresponding rows are merged.
</p>


<h3>Value</h3>

<p>A list including the elements
</p>
<table>
<tr><td><code>DataMat</code></td>
<td>
<p>a numeric matrix/vector with rows sorted with respect to the first column.</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>obtained weights corresponding to the merged points.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arun Kumar Kuchibhotla, arunku@wharton.upenn.edu.</p>


<h3>See Also</h3>

<p>See also the function <code><a href="stats.html#topic+smooth.spline">smooth.spline</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>args(fastmerge)
x &lt;- runif(100,-1,1)
y &lt;- runif(100,-1,1)
DataMat &lt;- cbind(x, y)
tmp &lt;- fastmerge(DataMat)
</code></pre>

<hr>
<h2 id='penta'>
C code for solving pentadiagonal linear equations.
</h2><span id='topic+penta'></span>

<h3>Description</h3>

<p>This function is only intended for an internal use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>penta(dim, E, A, D, C, F, B, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="penta_+3A_dim">dim</code></td>
<td>
<p>vector containing dimension of linear system.</p>
</td></tr>
<tr><td><code id="penta_+3A_e">E</code></td>
<td>
<p>Internal vector storing for one of the sub-diagonals.</p>
</td></tr>
<tr><td><code id="penta_+3A_a">A</code></td>
<td>
<p>Internal vector storing for one of the sub-diagonals.</p>
</td></tr>
<tr><td><code id="penta_+3A_d">D</code></td>
<td>
<p>Internal vector storing for one of the sub-diagonals.</p>
</td></tr>
<tr><td><code id="penta_+3A_c">C</code></td>
<td>
<p>Internal vector storing for one of the sub-diagonals.</p>
</td></tr>
<tr><td><code id="penta_+3A_f">F</code></td>
<td>
<p>Internal vector storing for one of the sub-diagonals.</p>
</td></tr>
<tr><td><code id="penta_+3A_b">B</code></td>
<td>
<p>Internal vector storing for the right hand side of linear equation.</p>
</td></tr>
<tr><td><code id="penta_+3A_x">X</code></td>
<td>
<p>Vector to store the solution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Does not return anything. Changes the inputs according to the algorithm.
</p>


<h3>Author(s)</h3>

<p>Arun Kumar Kuchibhotla, arunku@wharton.upenn.edu.</p>

<hr>
<h2 id='predcvxpen'>
C code for prediction using cvx.lse.reg, cvx.lip.reg and cvx.lse.con.reg for function and its derivatives.
</h2><span id='topic+predcvxpen'></span>

<h3>Description</h3>

<p>This function is only intended for an internal use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predcvxpen(dim, x, t, zhat, deriv, L, U, fun, P, Q, R)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predcvxpen_+3A_dim">dim</code></td>
<td>
<p>vector of sample size, size of newdata.</p>
</td></tr>
<tr><td><code id="predcvxpen_+3A_x">x</code></td>
<td>
<p>Newdata.</p>
</td></tr>
<tr><td><code id="predcvxpen_+3A_t">t</code></td>
<td>
<p>x-vector in cvx.pen.reg</p>
</td></tr>
<tr><td><code id="predcvxpen_+3A_zhat">zhat</code></td>
<td>
<p>prediction obtained from cvx.pen.reg</p>
</td></tr>
<tr><td><code id="predcvxpen_+3A_deriv">deriv</code></td>
<td>
<p>derivative vector obtained from cvx.pen.reg</p>
</td></tr>
<tr><td><code id="predcvxpen_+3A_l">L</code></td>
<td>
<p>Internal vector obtained from cpen function.</p>
</td></tr>
<tr><td><code id="predcvxpen_+3A_u">U</code></td>
<td>
<p>Internal vector obtained from cpen function.</p>
</td></tr>
<tr><td><code id="predcvxpen_+3A_fun">fun</code></td>
<td>
<p>vector containing the function estimate.</p>
</td></tr>
<tr><td><code id="predcvxpen_+3A_p">P</code></td>
<td>
<p>Internal vector set to zero.</p>
</td></tr>
<tr><td><code id="predcvxpen_+3A_q">Q</code></td>
<td>
<p>Internal vector set to zero.</p>
</td></tr>
<tr><td><code id="predcvxpen_+3A_r">R</code></td>
<td>
<p>Internal vector set to zero.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimate is characterized by a fixed point equation which gives the algorithm for prediction.
</p>


<h3>Value</h3>

<p>Does not return anything. Changes the inputs according to the algorithm.
</p>


<h3>Author(s)</h3>

<p>Arun Kumar Kuchibhotla, arunku@wharton.upenn.edu.</p>

<hr>
<h2 id='sim.est'>
Single Index Model Estimation: Objective Function Approach.
</h2><span id='topic+sim.est'></span><span id='topic+sim.est.default'></span><span id='topic+print.sim.est'></span><span id='topic+plot.sim.est'></span><span id='topic+predict.sim.est'></span>

<h3>Description</h3>

<p>This function provides an estimate of the non-parametric function and the index vector by minimizing an objective function specified by the method argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.est(x, y, w = NULL, beta.init = NULL, nmulti = NULL, L = NULL,
                    lambda = NULL, maxit = 100, bin.tol = 1e-05, beta.tol = 1e-05,
                    method = c("cvx.pen","cvx.lip","cvx.lse","smooth.pen"), 
                    progress = TRUE, force = FALSE)
## Default S3 method:
sim.est(x, y, w = NULL, beta.init = NULL, nmulti = NULL, L = NULL,
                    lambda = NULL, maxit = 100, bin.tol = 1e-05, beta.tol = 1e-05,
                    method = c("cvx.pen","cvx.lip","cvx.lse","smooth.pen"), 
                    progress = TRUE, force = FALSE)
## S3 method for class 'sim.est'
plot(x,...)
## S3 method for class 'sim.est'
print(x,...)
## S3 method for class 'sim.est'
predict(object, newdata = NULL, deriv = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim.est_+3A_x">x</code></td>
<td>
<p>a numeric matrix giving the values of the predictor variables or covariates. For functions plot and print, &lsquo;x&rsquo; is an object of class &lsquo;sim.est&rsquo;.</p>
</td></tr>
<tr><td><code id="sim.est_+3A_y">y</code></td>
<td>
<p>a numeric vector giving the values of the response variable.</p>
</td></tr>
<tr><td><code id="sim.est_+3A_method">method</code></td>
<td>
<p>a string indicating which method to use for regression.</p>
</td></tr>
<tr><td><code id="sim.est_+3A_lambda">lambda</code></td>
<td>
<p>a numeric value giving the penalty value for <code>cvx.pen</code> and <code>cvx.lip</code>.</p>
</td></tr>
<tr><td><code id="sim.est_+3A_l">L</code></td>
<td>
<p>a numeric value giving the Lipschitz bound for <code>cvx.lip</code>.</p>
</td></tr>
<tr><td><code id="sim.est_+3A_w">w</code></td>
<td>
<p>an optional numeric vector of the same length as <code class="reqn">x</code>; Defaults to all 1.</p>
</td></tr>
<tr><td><code id="sim.est_+3A_beta.init">beta.init</code></td>
<td>
<p>An numeric vector giving the initial value for the index vector.</p>
</td></tr>
<tr><td><code id="sim.est_+3A_nmulti">nmulti</code></td>
<td>
<p>An integer giving the number of multiple starts to be used for iterative algorithm. If beta.init is provided then the nmulti is set to 1.</p>
</td></tr>
<tr><td><code id="sim.est_+3A_bin.tol">bin.tol</code></td>
<td>
<p>A tolerance level upto which the x values used in regression are recognized as distinct values.</p>
</td></tr>
<tr><td><code id="sim.est_+3A_beta.tol">beta.tol</code></td>
<td>
<p>A tolerance level for stopping iterative algorithm for the index vector.</p>
</td></tr>
<tr><td><code id="sim.est_+3A_maxit">maxit</code></td>
<td>
<p>An integer specifying the maximum number of iterations for each initial <code class="reqn">\beta</code> vector.</p>
</td></tr>
<tr><td><code id="sim.est_+3A_progress">progress</code></td>
<td>
<p>A logical denoting if progress of the algorithm is to be printed. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="sim.est_+3A_force">force</code></td>
<td>
<p>A logical indicating the use of <code>cvx.lse.reg</code> or <code>cvx.lse.con.reg</code>. Defaults to FALSE and uses <code>cvx.lse.con.reg</code></p>
</td></tr>
<tr><td><code id="sim.est_+3A_object">object</code></td>
<td>
<p>An object of class &lsquo;sim.est&rsquo;.</p>
</td></tr>
<tr><td><code id="sim.est_+3A_...">...</code></td>
<td>
<p>Any additional arguments to be passed.</p>
</td></tr>
<tr><td><code id="sim.est_+3A_newdata">newdata</code></td>
<td>
<p>a matrix of new data points in the predict function.</p>
</td></tr>
<tr><td><code id="sim.est_+3A_deriv">deriv</code></td>
<td>
<p>a numeric either 0 or 1 representing which derivative to evaluate.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function minimizes
</p>
<p style="text-align: center;"><code class="reqn">\sum_{i=1}^n w_i(y_i - f(x_i^{\top}\beta))^2 + \lambda\int\{f''(x)\}^2dx</code>
</p>

<p>with constraints on <code class="reqn">f</code> dictated by method = &lsquo;cvx.pen&rsquo; or &lsquo;smooth.pen&rsquo;. For method = &lsquo;cvx.lip&rsquo; or &lsquo;cvx.lse&rsquo;, the function minimizes
</p>
<p style="text-align: center;"><code class="reqn">\sum_{i=1}^n w_i(y_i - f(x_i^{\top}\beta))^2</code>
</p>

<p>with constraints on <code class="reqn">f</code> disctated by method = &lsquo;cvx.lip&rsquo; or &lsquo;cvx.lse&rsquo;. The penalty parameter <code class="reqn">\lambda</code> is not choosen by any criteria. It has to be specified for using method <code class="reqn">=</code> &lsquo;cvx.pen&rsquo;, &lsquo;cvx.lip&rsquo; or &lsquo;smooth.pen&rsquo; and <code class="reqn">\lambda</code> denotes the Lipschitz constant for using the method <code class="reqn">=</code> &lsquo;cvx.lip.reg&rsquo;. <code>plot</code> function provides the scatterplot along with fitted curve; it also includes some diagnostic plots for residuals and progression of the algorithm. Predict function now allows calculation of the first derivative. In applications, it might be advantageous to scale of the covariate matrix <code>x</code> before passing into the function which brings more stability to the algorithm.
</p>


<h3>Value</h3>

<p>An object of class &lsquo;sim.est&rsquo;, basically a list including the elements
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>A numeric vector storing the estimate of the index vector.</p>
</td></tr>
<tr><td><code>nmulti</code></td>
<td>
<p>Number of multistarts used.</p>
</td></tr>
<tr><td><code>x.mat</code></td>
<td>
<p>the input &lsquo;x&rsquo; matrix with possibly aggregated rows.</p>
</td></tr>
<tr><td><code>BetaInit</code></td>
<td>
<p>a matrix storing the initial vectors taken or given for the index parameter.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>Given input <code>lambda</code>.</p>
</td></tr>
<tr><td><code>L</code></td>
<td>
<p>Given input <code>L</code>.</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>an integer storing the row index of <code>BetaInit</code> which lead to the estimator <code>beta</code>.</p>
</td></tr>
<tr><td><code>BetaPath</code></td>
<td>
<p>a list containing the paths taken by each initial index vector for nmulti times.</p>
</td></tr>
<tr><td><code>ObjValPath</code></td>
<td>
<p>a matrix with nmulti rows storing the path of objective function value for multiple starts.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>a numeric storing convergence status for the index parameter.</p>
</td></tr>
<tr><td><code>itervec</code></td>
<td>
<p>a vector of length nmulti storing the number of iterations taken by each of the multiple starts.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>a numeric giving the total number of iterations taken.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>method given as input.</p>
</td></tr>
<tr><td><code>regress</code></td>
<td>
<p>An output of the regression function used needed for predict.</p>
</td></tr>
<tr><td><code>x.values</code></td>
<td>
<p>sorted &lsquo;x.betahat&rsquo; values obtained by the algorithm.</p>
</td></tr>
<tr><td><code>y.values</code></td>
<td>
<p>corresponding &lsquo;y&rsquo; values in input.</p>
</td></tr>
<tr><td><code>fit.values</code></td>
<td>
<p>corresponding fit values of same length as that of <code class="reqn">x\beta</code>.</p>
</td></tr>
<tr><td><code>deriv</code></td>
<td>
<p>corresponding values of the derivative of same length as that of <code class="reqn">x\beta</code>.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>residuals obtained from the fit.</p>
</td></tr>
<tr><td><code>minvalue</code></td>
<td>
<p>minimum value of the objective function attained.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arun Kumar Kuchibhotla, arunku@wharton.upenn.edu</p>


<h3>Source</h3>

<p>Kuchibhotla, A. K., Patra, R. K. and Sen, B. (2015+). On Single Index Models with Convex Link.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>args(sim.est)
x &lt;- matrix(runif(50*3,-1,1),ncol = 3)
b0 &lt;- rep_len(1,3)/sqrt(3)
y &lt;- (x%*%b0)^2 + rnorm(50,0,0.3)
tmp1 &lt;- sim.est(x, y, lambda = 0.01, method = "cvx.pen", nmulti = 5)

tmp3 &lt;- sim.est(x, y, lambda = 0.01, method = "smooth.pen", nmulti = 5)

print(tmp1)

print(tmp3)

plot(tmp1)

plot(tmp3)

predict(tmp1, newdata = c(0,0,0))

predict(tmp3, newdata = c(0,0,0))

</code></pre>

<hr>
<h2 id='simestgcv'>
Single Index Model Estimation: Objective Function Approach.
</h2><span id='topic+simestgcv'></span><span id='topic+simestgcv.default'></span>

<h3>Description</h3>

<p>This function provides an estimate of the non-parametric function and the index vector by minimizing an objective function specified by the method argument and also by choosing tuning parameter using GCV.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simestgcv(x, y, w = NULL, beta.init = NULL, nmulti = NULL,
                lambda = NULL, maxit = 100, bin.tol = 1e-06, 
                beta.tol = 1e-05, agcv.iter = 100, progress = TRUE)

## Default S3 method:
simestgcv(x, y, w = NULL, beta.init = NULL, nmulti = NULL,
            lambda = NULL, maxit = 100, bin.tol = 1e-06, 
            beta.tol = 1e-05, agcv.iter = 100, progress = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simestgcv_+3A_x">x</code></td>
<td>
<p>a numeric matrix giving the values of the predictor variables or covariates. For functions plot and print, &lsquo;x&rsquo; is an object of class &lsquo;sim.est&rsquo;.</p>
</td></tr>
<tr><td><code id="simestgcv_+3A_y">y</code></td>
<td>
<p>a numeric vector giving the values of the response variable.</p>
</td></tr>
<tr><td><code id="simestgcv_+3A_lambda">lambda</code></td>
<td>
<p>a numeric vector giving lower and upper bounds for penalty used in <code>cvx.pen</code> and <code>cvx.lip</code>.</p>
</td></tr>
<tr><td><code id="simestgcv_+3A_w">w</code></td>
<td>
<p>an optional numeric vector of the same length as <code class="reqn">x</code>; Defaults to all 1.</p>
</td></tr>
<tr><td><code id="simestgcv_+3A_beta.init">beta.init</code></td>
<td>
<p>An numeric vector giving the initial value for the index vector.</p>
</td></tr>
<tr><td><code id="simestgcv_+3A_nmulti">nmulti</code></td>
<td>
<p>An integer giving the number of multiple starts to be used for iterative algorithm. If beta.init is provided then the nmulti is set to 1.</p>
</td></tr>
<tr><td><code id="simestgcv_+3A_agcv.iter">agcv.iter</code></td>
<td>
<p>An integer providing the number of random numbers to be used in estimating GCV. See <code>smooth.pen.reg</code> for more details.</p>
</td></tr>
<tr><td><code id="simestgcv_+3A_progress">progress</code></td>
<td>
<p>A logical denoting if progress of the algorithm to be printed. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="simestgcv_+3A_bin.tol">bin.tol</code></td>
<td>
<p>A tolerance level upto which the x values used in regression are recognized as distinct values.</p>
</td></tr>
<tr><td><code id="simestgcv_+3A_beta.tol">beta.tol</code></td>
<td>
<p>A tolerance level for stopping iterative algorithm for the index vector.</p>
</td></tr>
<tr><td><code id="simestgcv_+3A_maxit">maxit</code></td>
<td>
<p>An integer specifying the maximum number of iterations for each initial <code class="reqn">\beta</code> vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function minimizes 
</p>
<p style="text-align: center;"><code class="reqn">\sum_{i=1}^n w_i(y_i - f(x_i^{\top}\beta))^2 + \lambda\int\{f''(x)\}^2dx</code>
</p>

<p>with no constraints on f. The penalty parameter <code class="reqn">\lambda</code> is choosen by the GCV criterion between the bounds given by <code>lambda</code>. Plot and predict function work as in the case of <code>sim.est</code> function.
</p>


<h3>Value</h3>

<p>An object of class &lsquo;sim.est&rsquo;, basically a list including the elements
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>A numeric vector storing the estimate of the index vector.</p>
</td></tr>
<tr><td><code>nmulti</code></td>
<td>
<p>Number of multistarts used.</p>
</td></tr>
<tr><td><code>x.mat</code></td>
<td>
<p>the input &lsquo;x&rsquo; matrix with possibly aggregated rows.</p>
</td></tr>
<tr><td><code>BetaInit</code></td>
<td>
<p>a matrix storing the initial vectors taken or given for the index parameter.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>Given input <code>lambda</code>.</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>an integer storing the row index of <code>BetaInit</code> which lead to the estimator <code>beta</code>.</p>
</td></tr>
<tr><td><code>BetaPath</code></td>
<td>
<p>a list containing the paths taken by each initial index vector for nmulti times.</p>
</td></tr>
<tr><td><code>ObjValPath</code></td>
<td>
<p>a matrix with nmulti rows storing the path of objective function value for multiple starts.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>a numeric storing convergence status for the index parameter.</p>
</td></tr>
<tr><td><code>itervec</code></td>
<td>
<p>a vector of length nmulti storing the number of iterations taken by each of the multiple starts.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>a numeric giving the total number of iterations taken.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>method is always set to &quot;smooth.pen.reg&quot;.</p>
</td></tr>
<tr><td><code>regress</code></td>
<td>
<p>An output of the regression function used needed for predict.</p>
</td></tr>
<tr><td><code>x.values</code></td>
<td>
<p>sorted &lsquo;x.betahat&rsquo; values obtained by the algorithm.</p>
</td></tr>
<tr><td><code>y.values</code></td>
<td>
<p>corresponding &lsquo;y&rsquo; values in input.</p>
</td></tr>
<tr><td><code>fit.values</code></td>
<td>
<p>corresponding fit values of same length as that of <code class="reqn">x\beta</code>.</p>
</td></tr>
<tr><td><code>deriv</code></td>
<td>
<p>corresponding values of the derivative of same length as that of <code class="reqn">x\beta</code>.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>residuals obtained from the fit.</p>
</td></tr>
<tr><td><code>minvalue</code></td>
<td>
<p>minimum value of the objective function attained.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arun Kumar Kuchibhotla, arunku@wharton.upenn.edu</p>


<h3>Source</h3>

<p>Kuchibhotla, A. K., Patra, R. K. and Sen, B. (2015+). On Single Index Models with Convex Link.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>args(sim.est)
x &lt;- matrix(runif(20*2,-1,1),ncol = 2)
b0 &lt;- rep_len(1,2)/sqrt(2)
y &lt;- (x%*%b0)^2 + rnorm(20,0,0.3)
tmp2 &lt;- simestgcv(x, y, lambda = c(20^{1/6}, 20^{1/4}), nmulti = 1, 
					agcv.iter = 10, maxit = 10, beta.tol = 1e-03)
print(tmp2)
plot(tmp2)
predict(tmp2, newdata = c(0,0))
</code></pre>

<hr>
<h2 id='smooth.pen.reg'>
Penalized Smooth/Smoothing Spline Regression.
</h2><span id='topic+smooth.pen.reg'></span><span id='topic+smooth.pen.reg.default'></span><span id='topic+plot.smooth.pen.reg'></span><span id='topic+predict.smooth.pen.reg'></span><span id='topic+print.smooth.pen.reg'></span>

<h3>Description</h3>

<p>This function provides an estimate of the non-parameteric regression function using smoothing splines.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smooth.pen.reg(x, y, lambda, w = NULL, agcv = FALSE, agcv.iter = 100, ...)
## Default S3 method:
smooth.pen.reg(x, y, lambda, w = NULL, agcv = FALSE, agcv.iter = 100, ...)
## S3 method for class 'smooth.pen.reg'
plot(x,...)
## S3 method for class 'smooth.pen.reg'
print(x,...)
## S3 method for class 'smooth.pen.reg'
predict(object, newdata = NULL, deriv = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smooth.pen.reg_+3A_x">x</code></td>
<td>
<p>a numeric vector giving the values of the predictor variable. For functions plot and print, &lsquo;x&rsquo; is an object of class &lsquo;smooth.pen.reg&rsquo;.</p>
</td></tr>
<tr><td><code id="smooth.pen.reg_+3A_y">y</code></td>
<td>
<p>a numeric vector giving the values of the response variable.</p>
</td></tr>
<tr><td><code id="smooth.pen.reg_+3A_lambda">lambda</code></td>
<td>
<p>a numeric value giving the penalty value.</p>
</td></tr>
<tr><td><code id="smooth.pen.reg_+3A_w">w</code></td>
<td>
<p>an optional numeric vector of the same length as x; Defaults to all 1.</p>
</td></tr>
<tr><td><code id="smooth.pen.reg_+3A_agcv">agcv</code></td>
<td>
<p>a logical denoting if an estimate of generalized cross-validation is needed.</p>
</td></tr>
<tr><td><code id="smooth.pen.reg_+3A_agcv.iter">agcv.iter</code></td>
<td>
<p>a numeric denoting the number of random vectors used to estimate the GCV. See details.</p>
</td></tr>
<tr><td><code id="smooth.pen.reg_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
<tr><td><code id="smooth.pen.reg_+3A_object">object</code></td>
<td>
<p>An object of class &lsquo;smooth.pen.reg&rsquo;.</p>
</td></tr>
<tr><td><code id="smooth.pen.reg_+3A_newdata">newdata</code></td>
<td>
<p>a matrix of new data points in the predict function.</p>
</td></tr>
<tr><td><code id="smooth.pen.reg_+3A_deriv">deriv</code></td>
<td>
<p>a numeric either 0 or 1 representing which derivative to evaluate.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function minimizes 
</p>
<p style="text-align: center;"><code class="reqn">\sum_{i=1}^n w_i(y_i - f(x_i))^2 + \lambda\int\{f''(x)\}^2dx</code>
</p>
 
<p>without any constraint on <code class="reqn">f</code>. This function implements in R the algorithm noted in Green and Silverman (1994). The function smooth.spline in R is not suitable for single index model estimation as it chooses <code class="reqn">\lambda</code> using GCV by default. <code>plot</code> function provides the scatterplot along with fitted curve; it also includes some diagnostic plots for residuals. Predict function now allows computation of the first derivative. Calculation of generalized cross-validation requires the computation of diagonal elements of the hat matrix involved which is cumbersone and is computationally expensive (and also is unstable). <code>smooth.Pspline</code> of <code>pspline</code> package provides the GCV criterion value which matches the usual GCV when all the weights are equal to 1 but is not clear what it is for weights unequal. We use an estimate of GCV (formula of which is given in Green and Silverman (1994)) proposed by Girard which is very stable and computationally cheap. For more details about this randomized GCV, see Girard (1989). 
</p>


<h3>Value</h3>

<p>An object of class &lsquo;smooth.pen.reg&rsquo;, basically a list including the elements
</p>
<table>
<tr><td><code>x.values</code></td>
<td>
<p>sorted &lsquo;x&rsquo; values provided as input.</p>
</td></tr>
<tr><td><code>y.values</code></td>
<td>
<p>corresponding &lsquo;y&rsquo; values in input.</p>
</td></tr>
<tr><td><code>fit.values</code></td>
<td>
<p>corresponding fit values of same length as that of &lsquo;x.values&rsquo;.</p>
</td></tr>
<tr><td><code>deriv</code></td>
<td>
<p>corresponding values of the derivative of same length as that of &lsquo;x.values&rsquo;.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Always set to 1.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>residuals obtained from the fit.</p>
</td></tr>
<tr><td><code>minvalue</code></td>
<td>
<p>minimum value of the objective function attained.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>Always set to 0.</p>
</td></tr>
<tr><td><code>agcv.score</code></td>
<td>
<p>Asymptotic GCV approximation. Proposed in Silverman (1982) as a computationally fast approximation to GCV.</p>
</td></tr>
<tr><td><code>splinefun</code></td>
<td>
<p>An object of class &lsquo;smooth.spline&rsquo; needed for predict.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arun Kumar Kuchibhotla, arunku@wharton.upenn.edu.</p>


<h3>Source</h3>

<p>Green, P. J. and Silverman, B. W. (1994) Non-parametric Regression and Generalized Linear Models: A Roughness Penalty Approach. Chapman and Hall.
</p>
<p>Girard, D. A. (1989) A Fast ' Monte-Carlo Cross-Validation' Procedure for Large Least Squares Problems with Noisy Data. Numerische
Mathematik, 56, 1-23.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>args(smooth.pen.reg)
x &lt;- runif(50,-1,1)
y &lt;- x^2 + rnorm(50,0,0.3)
tmp &lt;- smooth.pen.reg(x, y, lambda = 0.01, agcv = TRUE)
print(tmp)
plot(tmp)
predict(tmp, newdata = rnorm(10,0,0.1))
</code></pre>

<hr>
<h2 id='solve.pentadiag'>
Pentadiagonal Linear Solver.
</h2><span id='topic+solve.pentadiag'></span>

<h3>Description</h3>

<p>A function to solve pentadiagonal system of linear equations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pentadiag'
solve(a, b, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="solve.pentadiag_+3A_a">a</code></td>
<td>
<p>a numeric square matrix with pentadiagonal rows. The function does NOT check for pentadiagonal matrix.</p>
</td></tr>
<tr><td><code id="solve.pentadiag_+3A_b">b</code></td>
<td>
<p>a numeric vector of the same length as nrows(a). This argument cannot be a matrix.</p>
</td></tr>
<tr><td><code id="solve.pentadiag_+3A_...">...</code></td>
<td>
<p>any additional arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is written mainly for use in this package. It may not be the most efficient code.
</p>


<h3>Value</h3>

<p>A vector containing the solution.
</p>


<h3>Author(s)</h3>

<p>Arun Kumar Kuchibhotla, arunku@wharton.upenn.edu</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- matrix(c(2,1,1,0,0,
			  1,2,1,1,0,
			  1,1,2,1,1,
			  0,1,1,2,1,
			  0,0,1,1,2),nrow = 5)
b &lt;- rnorm(5)
tmp &lt;- solve.pentadiag(A, b)
</code></pre>

<hr>
<h2 id='spen_egcv'>
C code for smoothing splines with randomized GCV computation.
</h2><span id='topic+spen_egcv'></span>

<h3>Description</h3>

<p>This function is only intended for an internal use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spen_egcv(dim, x, y, w, h, QtyPerm, lambda, m, nforApp,
		 EGCVflag, agcv)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spen_egcv_+3A_dim">dim</code></td>
<td>
<p>vector of sample size.</p>
</td></tr>
<tr><td><code id="spen_egcv_+3A_x">x</code></td>
<td>
<p>x-vector in smooth.pen.reg.</p>
</td></tr>
<tr><td><code id="spen_egcv_+3A_y">y</code></td>
<td>
<p>y-vector in smooth.pen.reg.</p>
</td></tr>
<tr><td><code id="spen_egcv_+3A_w">w</code></td>
<td>
<p>w-vector in smooth.pen.reg.</p>
</td></tr>
<tr><td><code id="spen_egcv_+3A_h">h</code></td>
<td>
<p>difference vector for x for internal use.</p>
</td></tr>
<tr><td><code id="spen_egcv_+3A_qtyperm">QtyPerm</code></td>
<td>
<p>Second order difference for x for internal use.</p>
</td></tr>
<tr><td><code id="spen_egcv_+3A_lambda">lambda</code></td>
<td>
<p>smoothing parameter input for smooth.pen.reg.</p>
</td></tr>
<tr><td><code id="spen_egcv_+3A_m">m</code></td>
<td>
<p>vector to store the prediction vector.</p>
</td></tr>
<tr><td><code id="spen_egcv_+3A_nforapp">nforApp</code></td>
<td>
<p>Number of iterations for approximate GCV.</p>
</td></tr>
<tr><td><code id="spen_egcv_+3A_egcvflag">EGCVflag</code></td>
<td>
<p>Logical when GCV is needed.</p>
</td></tr>
<tr><td><code id="spen_egcv_+3A_agcv">agcv</code></td>
<td>
<p>Internal scalar. Set to 0. Stores the approximate GCV.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is same as smooth.spline except for small changes.
</p>


<h3>Value</h3>

<p>Does not return anything. Changes the inputs according to the iterations.
</p>


<h3>Author(s)</h3>

<p>Arun Kumar Kuchibhotla, arunku@wharton.upenn.edu.</p>


<h3>See Also</h3>

<p>smooth.spline
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
