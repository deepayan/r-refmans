<!DOCTYPE html><html lang="en"><head><title>Help for package lfda</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {lfda}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#+25+5E+25'><p>Negative One Half Matrix Power Operator</p></a></li>
<li><a href='#Cols'><p>Assigning Colors to A Vector</p></a></li>
<li><a href='#getAffinityMatrix'><p>Get Affinity Matrix</p></a></li>
<li><a href='#getMetricOfType'><p>Get Requested Type of Transforming Metric</p></a></li>
<li><a href='#klfda'><p>Kernel Local Fisher Discriminant Analysis for</p>
Supervised Dimensionality Reduction</a></li>
<li><a href='#kmatrixGauss'><p>Gaussian Kernel Computation</p>
(Particularly used in Kernel Local Fisher Discriminant Analysis)</a></li>
<li><a href='#lfda'><p>Local Fisher Discriminant Analysis for</p>
Supervised Dimensionality Reduction</a></li>
<li><a href='#plot.lfda'><p>3D Visualization for LFDA/KLFDA Result</p></a></li>
<li><a href='#predict.lfda'><p>LFDA Transformation/Prediction on New Data</p></a></li>
<li><a href='#print.lfda'><p>Print an lfda object</p></a></li>
<li><a href='#repmat'><p>Matlab-Syntaxed Repmat</p></a></li>
<li><a href='#self'><p>Semi-Supervised Local Fisher Discriminant Analysis(SELF) for</p>
Semi-Supervised Dimensionality Reduction</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Local Fisher Discriminant Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2019-07-31</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/terrytangyuan/lfda">https://github.com/terrytangyuan/lfda</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/terrytangyuan/lfda/issues">https://github.com/terrytangyuan/lfda/issues</a></td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Yuan Tang &lt;terrytangyuan@gmail.com&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Description:</td>
<td>Functions for performing and visualizing Local Fisher Discriminant
    Analysis(LFDA), Kernel Fisher Discriminant Analysis(KLFDA), and Semi-supervised
    Local Fisher Discriminant Analysis(SELF).</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>plyr, grDevices, rARPACK</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, rgl</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.0</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-07-31 16:46:28 UTC; yuan.tang</td>
</tr>
<tr>
<td>Author:</td>
<td>Yuan Tang <a href="https://orcid.org/0000-0001-5243-233X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Wenxuan Li [ctb],
  Nan Xiao [ctb, cph],
  Zachary Deane-Mayer [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-07-31 17:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25+5E+25'>Negative One Half Matrix Power Operator</h2><span id='topic++25+5E+25'></span>

<h3>Description</h3>

<p>This function defines operation for negative one half matrix
power operator
</p>


<h3>Usage</h3>

<pre><code class='language-R'>x %^% n
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="+2B25+2B5E+2B25_+3A_x">x</code></td>
<td>
<p>the matrix we want to operate on</p>
</td></tr>
<tr><td><code id="+2B25+2B5E+2B25_+3A_n">n</code></td>
<td>
<p>the exponent</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the matrix after negative one half power
</p>

<hr>
<h2 id='Cols'>Assigning Colors to A Vector</h2><span id='topic+Cols'></span>

<h3>Description</h3>

<p>This function assigns a color to each distinct value in the given vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Cols(vec)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Cols_+3A_vec">vec</code></td>
<td>
<p>The vector where each distinct value will be assigned a color.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The colors for each element in the given vector
</p>

<hr>
<h2 id='getAffinityMatrix'>Get Affinity Matrix</h2><span id='topic+getAffinityMatrix'></span>

<h3>Description</h3>

<p>This function returns an affinity matrix within knn-nearest neighbors from the distance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getAffinityMatrix(distance2, knn, nc)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getAffinityMatrix_+3A_distance2">distance2</code></td>
<td>
<p>The distance matrix for each observation</p>
</td></tr>
<tr><td><code id="getAffinityMatrix_+3A_knn">knn</code></td>
<td>
<p>The number of nearest neighbors</p>
</td></tr>
<tr><td><code id="getAffinityMatrix_+3A_nc">nc</code></td>
<td>
<p>The number of observations for data in this class</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an affinity matrix - the larger the element in the matrix, the closer two data points are
</p>

<hr>
<h2 id='getMetricOfType'>Get Requested Type of Transforming Metric</h2><span id='topic+getMetricOfType'></span>

<h3>Description</h3>

<p>This function returns the requested type of transforming metric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getMetricOfType(metric, eigVec, eigVal, total)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getMetricOfType_+3A_metric">metric</code></td>
<td>
<p>The type of metric to be requested</p>
</td></tr>
<tr><td><code id="getMetricOfType_+3A_eigvec">eigVec</code></td>
<td>
<p>The eigenvectors of the problem</p>
</td></tr>
<tr><td><code id="getMetricOfType_+3A_eigval">eigVal</code></td>
<td>
<p>The eigenvalues of the problem</p>
</td></tr>
<tr><td><code id="getMetricOfType_+3A_total">total</code></td>
<td>
<p>The number of total rows to be used for weighting denominator</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The transformation metric in requested type
</p>

<hr>
<h2 id='klfda'>Kernel Local Fisher Discriminant Analysis for
Supervised Dimensionality Reduction</h2><span id='topic+klfda'></span>

<h3>Description</h3>

<p>Performs kernel local fisher discriminant analysis on the given data,
which is the non-linear version of LFDA (see details <code><a href="#topic+lfda">lfda</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>klfda(k, y, r, metric = c("weighted", "orthonormalized", "plain"),
  knn = 6, reg = 0.001)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="klfda_+3A_k">k</code></td>
<td>
<p>n x n kernel matrix. Result of the <code><a href="#topic+kmatrixGauss">kmatrixGauss</a></code> function.
n is the number of samples</p>
</td></tr>
<tr><td><code id="klfda_+3A_y">y</code></td>
<td>
<p>n dimensional vector of class labels</p>
</td></tr>
<tr><td><code id="klfda_+3A_r">r</code></td>
<td>
<p>dimensionality of reduced space (default: d)</p>
</td></tr>
<tr><td><code id="klfda_+3A_metric">metric</code></td>
<td>
<p>type of metric in the embedding space (default: 'weighted')
'weighted'        &mdash; weighted eigenvectors
'orthonormalized' &mdash; orthonormalized
'plain'           &mdash; raw eigenvectors</p>
</td></tr>
<tr><td><code id="klfda_+3A_knn">knn</code></td>
<td>
<p>parameter used in local scaling method (default: 6)</p>
</td></tr>
<tr><td><code id="klfda_+3A_reg">reg</code></td>
<td>
<p>regularization parameter (default: 0.001)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of the LFDA results:
</p>
<table role = "presentation">
<tr><td><code>T</code></td>
<td>
<p>d x r transformation matrix (Z = t(T) * X)</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>r x n matrix of dimensionality reduced samples</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yuan Tang
</p>


<h3>References</h3>

<p>Sugiyama, M (2007). - contain implementation
Dimensionality reduction of multimodal labeled data by
local Fisher discriminant analysis.
<em>Journal of Machine Learning Research</em>, vol.<b>8</b>, 1027&ndash;1061.
</p>
<p>Sugiyama, M (2006).
Local Fisher discriminant analysis for supervised dimensionality reduction.
In W. W. Cohen and A. Moore (Eds.), <em>Proceedings of 23rd International
Conference on Machine Learning (ICML2006)</em>, 905&ndash;912.
</p>
<p>Original Matlab Implementation: http://www.ms.k.u-tokyo.ac.jp/software.html#LFDA
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+lfda">lfda</a></code> for the linear version.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
k &lt;- kmatrixGauss(iris[, -5])
y &lt;- iris[, 5]
r &lt;- 3
klfda(k, y, r, metric = "plain")
</code></pre>

<hr>
<h2 id='kmatrixGauss'>Gaussian Kernel Computation
(Particularly used in Kernel Local Fisher Discriminant Analysis)</h2><span id='topic+kmatrixGauss'></span>

<h3>Description</h3>

<p>Gaussian kernel computation for klfda, which maps the original
data space to non-linear and higher dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmatrixGauss(x, sigma = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kmatrixGauss_+3A_x">x</code></td>
<td>
<p>n x d matrix of original samples.
n is the number of samples.</p>
</td></tr>
<tr><td><code id="kmatrixGauss_+3A_sigma">sigma</code></td>
<td>
<p>dimensionality of reduced space. (default: 1)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>K n x n kernel matrix.
n is the number of samples.
</p>


<h3>Author(s)</h3>

<p>Yuan Tang
</p>


<h3>References</h3>

<p>Sugiyama, M (2007).
Dimensionality reduction of multimodal labeled data by
local Fisher discriminant analysis.
<em>Journal of Machine Learning Research</em>, vol.<b>8</b>, 1027&ndash;1061.
</p>
<p>Sugiyama, M (2006).
Local Fisher discriminant analysis for supervised dimensionality reduction.
In W. W. Cohen and A. Moore (Eds.), <em>Proceedings of 23rd International
Conference on Machine Learning (ICML2006)</em>, 905&ndash;912.
</p>
<p>https://shapeofdata.wordpress.com/2013/07/23/gaussian-kernels/
</p>


<h3>See Also</h3>

<p>See <code>klfda</code> for the computation of
kernel local fisher discriminant analysis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
kmatrixGauss(iris[, -5])
</code></pre>

<hr>
<h2 id='lfda'>Local Fisher Discriminant Analysis for
Supervised Dimensionality Reduction</h2><span id='topic+lfda'></span>

<h3>Description</h3>

<p>Performs local fisher discriminant analysis (LFDA) on the given data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lfda(x, y, r, metric = c("orthonormalized", "plain", "weighted"),
  knn = 5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lfda_+3A_x">x</code></td>
<td>
<p>n x d matrix of original samples.
n is the number of samples.</p>
</td></tr>
<tr><td><code id="lfda_+3A_y">y</code></td>
<td>
<p>length n vector of class labels</p>
</td></tr>
<tr><td><code id="lfda_+3A_r">r</code></td>
<td>
<p>dimensionality of reduced space (default: d)</p>
</td></tr>
<tr><td><code id="lfda_+3A_metric">metric</code></td>
<td>
<p>type of metric in the embedding space (no default)
'weighted'        &mdash; weighted eigenvectors
'orthonormalized' &mdash; orthonormalized
'plain'           &mdash; raw eigenvectors</p>
</td></tr>
<tr><td><code id="lfda_+3A_knn">knn</code></td>
<td>
<p>parameter used in local scaling method (default: 5)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>LFDA is a method for linear dimensionality reduction that maximizes
between-class scatter and minimizes within-class scatter while at the
same time maintain the local structure of the data so that multimodal
data can be embedded appropriately. Its limitation is that it only
looks for linear boundaries between clusters. In this case, a non-linear
version called kernel LFDA will be used instead. Three metric types can
be used if needed.
</p>


<h3>Value</h3>

<p>list of the LFDA results:
</p>
<table role = "presentation">
<tr><td><code>T</code></td>
<td>
<p>d x r transformation matrix (Z = x * T)</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>n x r matrix of dimensionality reduced samples</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yuan Tang
</p>


<h3>References</h3>

<p>Sugiyama, M (2007).
Dimensionality reduction of multimodal labeled data by
local Fisher discriminant analysis.
<em>Journal of Machine Learning Research</em>, vol.<b>8</b>, 1027&ndash;1061.
</p>
<p>Sugiyama, M (2006).
Local Fisher discriminant analysis for supervised dimensionality reduction.
In W. W. Cohen and A. Moore (Eds.), <em>Proceedings of 23rd International
Conference on Machine Learning (ICML2006)</em>, 905&ndash;912.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+klfda">klfda</a></code> for the kernelized variant of
LFDA (Kernel LFDA).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
k &lt;- iris[, -5]
y &lt;- iris[, 5]
r &lt;- 3
lfda(k, y, r, metric = "plain")
</code></pre>

<hr>
<h2 id='plot.lfda'>3D Visualization for LFDA/KLFDA Result</h2><span id='topic+plot.lfda'></span>

<h3>Description</h3>

<p>This function plot 3 dimensions of the lfda/klfda result.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lfda'
plot(x, labels, cleanText = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.lfda_+3A_x">x</code></td>
<td>
<p>The lfda/klfda result.</p>
</td></tr>
<tr><td><code id="plot.lfda_+3A_labels">labels</code></td>
<td>
<p>A list of class labels used for lfda/klfda training.</p>
</td></tr>
<tr><td><code id="plot.lfda_+3A_cleantext">cleanText</code></td>
<td>
<p>A boolean value to specify whether to make the labels in the plot cleaner (default: FALSE)</p>
</td></tr>
<tr><td><code id="plot.lfda_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>See <code><a href="#topic+lfda">lfda</a></code> and <code><a href="#topic+klfda">klfda</a></code> for the metric learning method used for this visualization.
</p>

<hr>
<h2 id='predict.lfda'>LFDA Transformation/Prediction on New Data</h2><span id='topic+predict.lfda'></span>

<h3>Description</h3>

<p>This function transforms a data set, usually a testing set, using the trained LFDA metric
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lfda'
predict(object, newdata = NULL, type = "raw", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.lfda_+3A_object">object</code></td>
<td>
<p>The result from lfda function, which contains a transformed data and a transforming
matrix that can be used for transforming testing set</p>
</td></tr>
<tr><td><code id="predict.lfda_+3A_newdata">newdata</code></td>
<td>
<p>The data to be transformed</p>
</td></tr>
<tr><td><code id="predict.lfda_+3A_type">type</code></td>
<td>
<p>The output type, in this case it defaults to &quot;raw&quot; since the output is a matrix</p>
</td></tr>
<tr><td><code id="predict.lfda_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the transformed matrix
</p>


<h3>Author(s)</h3>

<p>Yuan Tang
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
k &lt;- iris[, -5]
y &lt;- iris[, 5]
r &lt;- 3
model &lt;- lfda(k, y, r = 4, metric = "plain")
predict(model, iris[, -5])
</code></pre>

<hr>
<h2 id='print.lfda'>Print an lfda object</h2><span id='topic+print.lfda'></span>

<h3>Description</h3>

<p>Print an lfda object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lfda'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.lfda_+3A_x">x</code></td>
<td>
<p>The result from lfda function, which contains a transformed data and a transforming</p>
</td></tr>
<tr><td><code id="print.lfda_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>

<hr>
<h2 id='repmat'>Matlab-Syntaxed Repmat</h2><span id='topic+repmat'></span>

<h3>Description</h3>

<p>This function mimics the behavior and syntax of repmat() in Matlab
it generates a large matrix consisting of an N-by-M tiling copies of A
</p>


<h3>Usage</h3>

<pre><code class='language-R'>repmat(A, N, M)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="repmat_+3A_a">A</code></td>
<td>
<p>original matrix to be used as copies</p>
</td></tr>
<tr><td><code id="repmat_+3A_n">N</code></td>
<td>
<p>the number of rows of tiling copies of A</p>
</td></tr>
<tr><td><code id="repmat_+3A_m">M</code></td>
<td>
<p>the number of columns of tiling copies of A</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix consisting of an N-by-M tiling copies of A
</p>

<hr>
<h2 id='self'>Semi-Supervised Local Fisher Discriminant Analysis(SELF) for
Semi-Supervised Dimensionality Reduction</h2><span id='topic+self'></span>

<h3>Description</h3>

<p>Performs semi-supervised local fisher discriminant analysis (SELF) on the given data.
SELF is a linear semi-supervised dimensionality reduction method smoothly bridges supervised
LFDA and unsupervised principal component analysis, by which a natural regularization effect
can be obtained when only a small number of labeled samples are available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>self(X, Y, beta = 0.5, r, metric = c("orthonormalized", "plain",
  "weighted"), kNN = 5, minObsPerLabel = 5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="self_+3A_x">X</code></td>
<td>
<p>n x d matrix of original samples.
n is the number of samples.</p>
</td></tr>
<tr><td><code id="self_+3A_y">Y</code></td>
<td>
<p>length n vector of class labels</p>
</td></tr>
<tr><td><code id="self_+3A_beta">beta</code></td>
<td>
<p>degree of semi-supervisedness (0 &lt;= beta &lt;= 1; default is 0.5 )
0: totally supervised (discard all unlabeled samples)
1: totally unsupervised (discard all label information)</p>
</td></tr>
<tr><td><code id="self_+3A_r">r</code></td>
<td>
<p>dimensionality of reduced space (default: d)</p>
</td></tr>
<tr><td><code id="self_+3A_metric">metric</code></td>
<td>
<p>type of metric in the embedding space (no default)
'weighted'        &mdash; weighted eigenvectors
'orthonormalized' &mdash; orthonormalized
'plain'           &mdash; raw eigenvectors</p>
</td></tr>
<tr><td><code id="self_+3A_knn">kNN</code></td>
<td>
<p>parameter used in local scaling method (default: 5)</p>
</td></tr>
<tr><td><code id="self_+3A_minobsperlabel">minObsPerLabel</code></td>
<td>
<p>the minimum number observations required for each different label(default: 5)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of the SELF results:
</p>
<table role = "presentation">
<tr><td><code>T</code></td>
<td>
<p>d x r transformation matrix (Z = x * T)</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>n x r matrix of dimensionality reduced samples</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yuan Tang
</p>


<h3>References</h3>

<p>Sugiyama, Masashi, et al (2010).
Semi-supervised local Fisher discriminant analysis for dimensionality reduction.
<em>Machine learning</em> 78.1-2: 35-61.
</p>
<p>Sugiyama, M (2007).
Dimensionality reduction of multimodal labeled data by
local Fisher discriminant analysis.
<em>Journal of Machine Learning Research</em>, vol.<b>8</b>, 1027&ndash;1061.
</p>
<p>Sugiyama, M (2006).
Local Fisher discriminant analysis for supervised dimensionality reduction.
In W. W. Cohen and A. Moore (Eds.), <em>Proceedings of 23rd International
Conference on Machine Learning (ICML2006)</em>, 905&ndash;912.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+lfda">lfda</a></code> for LFDA and  <code><a href="#topic+klfda">klfda</a></code> for the kernelized variant of
LFDA (Kernel LFDA).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- iris[, -5]
y &lt;- iris[, 5]
self(x, y, beta = 0.1, r = 3, metric = "plain")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
