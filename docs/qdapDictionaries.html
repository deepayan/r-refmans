<!DOCTYPE html><html lang="en"><head><title>Help for package qdapDictionaries</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {qdapDictionaries}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#qdapDictionaries'><p>qdapDictionaries</p></a></li>
<li><a href='#abbreviations'><p>Small Abbreviations Data Set</p></a></li>
<li><a href='#action.verbs'><p>Action Word List</p></a></li>
<li><a href='#adverb'><p>Adverb Word List</p></a></li>
<li><a href='#amplification.words'><p>Amplifying Words</p></a></li>
<li><a href='#BuckleySaltonSWL'><p>Buckley &amp; Salton Stopword List</p></a></li>
<li><a href='#contractions'><p>Contraction Conversions</p></a></li>
<li><a href='#deamplification.words'><p>De-amplifying Words</p></a></li>
<li><a href='#DICTIONARY'><p>Nettalk Corpus Syllable Data Set</p></a></li>
<li><a href='#discourse.markers.alemany'><p>Alemany's Discourse Markers</p></a></li>
<li><a href='#Dolch'><p>Dolch List of 220 Common Words</p></a></li>
<li><a href='#emoticon'><p>Emoticons Data Set</p></a></li>
<li><a href='#Fry_1000'><p>Fry's 1000 Most Commonly Used English Words</p></a></li>
<li><a href='#function.words'><p>Function Words</p></a></li>
<li><a href='#GradyAugmented'><p>Augmented List of Grady Ward's English Words and Mark Kantrowitz's Names List</p></a></li>
<li><a href='#interjections'><p>Interjections</p></a></li>
<li><a href='#key.pol'><p>Polarity Lookup Key</p></a></li>
<li><a href='#key.power'><p>Power Lookup Key</p></a></li>
<li><a href='#key.strength'><p>Strength Lookup Key</p></a></li>
<li><a href='#key.syl'><p>Syllable Lookup Key</p></a></li>
<li><a href='#key.syn'><p>Synonym Lookup Key</p></a></li>
<li><a href='#labMT'><p>Language Assessment by Mechanical Turk (labMT) Sentiment Words</p></a></li>
<li><a href='#Leveled_Dolch'><p>Leveled Dolch List of 220 Common Words</p></a></li>
<li><a href='#NAMES'><p>First Names and Gender (U.S.)</p></a></li>
<li><a href='#NAMES_LIST'><p>First Names and Predictive Gender (U.S.) List</p></a></li>
<li><a href='#NAMES_SEX'><p>First Names and Predictive Gender (U.S.)</p></a></li>
<li><a href='#negation.words'><p>Negating Words</p></a></li>
<li><a href='#negative.words'><p>Negative Words</p></a></li>
<li><a href='#OnixTxtRetToolkitSWL1'><p>Onix Text Retrieval Toolkit Stopword List 1</p></a></li>
<li><a href='#positive.words'><p>Positive Words</p></a></li>
<li><a href='#power.words'><p>Words that Indicate Power</p></a></li>
<li><a href='#preposition'><p>Preposition Words</p></a></li>
<li><a href='#print.view_data'><p>Prints a view_data Object</p></a></li>
<li><a href='#strong.words'><p>Words that Indicate Strength</p></a></li>
<li><a href='#submit.words'><p>Words that Indicate Submission</p></a></li>
<li><a href='#Top100Words'><p>Fry's  100 Most Commonly Used English Words</p></a></li>
<li><a href='#Top200Words'><p>Fry's 200 Most Commonly Used English Words</p></a></li>
<li><a href='#Top25Words'><p>Fry's 25 Most Commonly Used English Words</p></a></li>
<li><a href='#view_data'><p>List all data sets available in a <span class="pkg">qdapDictionaries</span></p></a></li>
<li><a href='#weak.words'><p>Words that Indicate Weakness</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Dictionaries and Word Lists for the 'qdap' Package</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.7</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-03-04</td>
</tr>
<tr>
<td>Author:</td>
<td>Tyler Rinker</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Tyler Rinker &lt;tyler.rinker@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, utils</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Description:</td>
<td>A collection of text analysis dictionaries and word lists for use with
        the 'qdap' package.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://trinker.github.com/qdapDictionaries/">http://trinker.github.com/qdapDictionaries/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="http://github.com/trinker/qdapDictionaries/issues">http://github.com/trinker/qdapDictionaries/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-03-04 18:47:03 UTC; Tyler</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-03-05 11:29:08 UTC</td>
</tr>
</table>
<hr>
<h2 id='qdapDictionaries'>qdapDictionaries</h2><span id='topic+qdapDictionaries'></span><span id='topic+package-qdapDictionaries'></span><span id='topic+qdapDictionaries-package'></span>

<h3>Description</h3>

<p>A collection of dictionaries and Word Lists to Accompany the qdap Package
</p>

<hr>
<h2 id='abbreviations'>Small Abbreviations Data Set</h2><span id='topic+abbreviations'></span>

<h3>Description</h3>

<p>A dataset containing abbreviations and their qdap friendly form.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(abbreviations)
</code></pre>


<h3>Format</h3>

<p>A data frame with 14 rows and 2 variables</p>


<h3>Details</h3>

 
<ul>
<li><p> abv. Common transcript abbreviations
</p>
</li>
<li><p> rep. qdap representation of those abbreviations
</p>
</li></ul>


<hr>
<h2 id='action.verbs'>Action Word List</h2><span id='topic+action.verbs'></span>

<h3>Description</h3>

<p>A dataset containing a vector of action words.  This is a subset of the 
Moby project: Moby Part-of-Speech.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(action.verbs)
</code></pre>


<h3>Format</h3>

<p>A vector with 1569 elements</p>


<h3>Details</h3>

<p>From Grady Ward's Moby project:
&quot;This second edition is a particularly thorough revision of the original Moby
Part-of-Speech. Beyond the fifteen thousand new entries, many thousand more
entries have been scrutinized for correctness and modernity. This is
unquestionably the largest P-O-S list in the world. Note that the many included
phrases means that parsing algorithms can now tokenize in units larger than a
single word, increasing both speed and accuracy.&quot;
</p>

<hr>
<h2 id='adverb'>Adverb Word List</h2><span id='topic+adverb'></span>

<h3>Description</h3>

<p>A dataset containing a vector of adverbs words.  This is a subset of the 
Moby project: Moby Part-of-Speech.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(adverb)
</code></pre>


<h3>Format</h3>

<p>A vector with 13398 elements</p>


<h3>Details</h3>

<p>From Grady Ward's Moby project:
&quot;This second edition is a particularly thorough revision of the original Moby
Part-of-Speech. Beyond the fifteen thousand new entries, many thousand more
entries have been scrutinized for correctness and modernity. This is
unquestionably the largest P-O-S list in the world. Note that the many included
phrases means that parsing algorithms can now tokenize in units larger than a
single word, increasing both speed and accuracy.&quot;
</p>

<hr>
<h2 id='amplification.words'>Amplifying Words</h2><span id='topic+amplification.words'></span>

<h3>Description</h3>

<p>A dataset containing a vector of words that amplify word meaning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(amplification.words)
</code></pre>


<h3>Format</h3>

<p>A vector with 49 elements</p>


<h3>Details</h3>

<p>Valence shifters are words that alter or intensify the meaning of the polarized
words and include negators and amplifiers. Negators are, generally, adverbs
that negate sentence meaning; for example the word like in the sentence, &quot;I do
like pie.&quot;, is given the opposite meaning in the sentence, &quot;I do not like
pie.&quot;, now containing the negator not. Amplifiers are, generally, adverbs or
adjectives that intensify sentence meaning. Using our previous example, the
sentiment of the negator altered sentence, &quot;I seriously do not like pie.&quot;, is
heightened with addition of the amplifier seriously.  Whereas de-amplifiers 
decrease the intensity of a polarized word as in the sentence &quot;I barely like
pie&quot;; the word &quot;barely&quot; deamplifies the word like.
</p>

<hr>
<h2 id='BuckleySaltonSWL'>Buckley &amp; Salton Stopword List</h2><span id='topic+BuckleySaltonSWL'></span>

<h3>Description</h3>

<p>A stopword list containing a character vector of stopwords.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(BuckleySaltonSWL)
</code></pre>


<h3>Format</h3>

<p>A character vector with 546 elements</p>


<h3>Details</h3>

<p><a href="http://www.lextek.com/manuals/onix/stopwords2.html">From Onix Text Retrieval Toolkit API Reference</a>:
&quot;This stopword list was built by Gerard Salton and Chris Buckley for the
experimental SMART information retrieval system at Cornell University.
This stopword list is generally considered to be on the larger side and so
when it is used, some implementations edit it so that it is better suited
for a given domain and audience while others use this stopword list as it
stands.&quot;
</p>


<h3>Note</h3>

<p>Reduced from the original 571 words to 546.
</p>


<h3>References</h3>

<p><a href="http://www.lextek.com/manuals/onix/stopwords2.html">http://www.lextek.com/manuals/onix/stopwords2.html</a>
</p>

<hr>
<h2 id='contractions'>Contraction Conversions</h2><span id='topic+contractions'></span>

<h3>Description</h3>

<p>A dataset containing common contractions and their expanded form.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(contractions)
</code></pre>


<h3>Format</h3>

<p>A data frame with 70 rows and 2 variables</p>


<h3>Details</h3>

 
<ul>
<li><p> contraction. The contraction word.
</p>
</li>
<li><p> expanded. The expanded form of the contraction.
</p>
</li></ul>


<hr>
<h2 id='deamplification.words'>De-amplifying Words</h2><span id='topic+deamplification.words'></span>

<h3>Description</h3>

<p>A dataset containing a vector of words that de-amplify word meaning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(deamplification.words)
</code></pre>


<h3>Format</h3>

<p>A vector with 13 elements</p>


<h3>Details</h3>

<p>Valence shifters are words that alter or intensify the meaning of the polarized
words and include negators and amplifiers. Negators are, generally, adverbs
that negate sentence meaning; for example the word like in the sentence, &quot;I do
like pie.&quot;, is given the opposite meaning in the sentence, &quot;I do not like
pie.&quot;, now containing the negator not. Amplifiers are, generally, adverbs or
adjectives that intensify sentence meaning. Using our previous example, the
sentiment of the negator altered sentence, &quot;I seriously do not like pie.&quot;, is
heightened with addition of the amplifier seriously.  Whereas de-amplifiers 
decrease the intensity of a polarized word as in the sentence &quot;I barely like
pie&quot;; the word &quot;barely&quot; deamplifies the word like.
</p>

<hr>
<h2 id='DICTIONARY'>Nettalk Corpus Syllable Data Set</h2><span id='topic+DICTIONARY'></span>

<h3>Description</h3>

<p>A dataset containing syllable counts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(DICTIONARY)
</code></pre>


<h3>Format</h3>

<p>A data frame with 20137 rows and 2 variables</p>


<h3>Details</h3>

 
<ul>
<li><p> word. The word
</p>
</li>
<li><p> syllables. Number of syllables
</p>
</li></ul>



<h3>Note</h3>

<p>This data set is based on the Nettalk Corpus but has some researcher 
word deletions and additions based on the needs of the 
<code><a href="qdap.html#topic+syllable_sum">syllable_sum</a></code> algorithm.
</p>


<h3>References</h3>

<p>Sejnowski, T.J., and Rosenberg, C.R. (1987). &quot;Parallel networks 
that learn to pronounce English text&quot; in Complex Systems, 1, 145-168. 
Retrieved from: <a href="http://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Nettalk+Corpus)">http://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Nettalk+Corpus)</a>
</p>
<p><a href="http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/nettalk/">UCI Machine Learning Repository website</a>
</p>

<hr>
<h2 id='discourse.markers.alemany'>Alemany's Discourse Markers</h2><span id='topic+discourse.markers.alemany'></span>

<h3>Description</h3>

<p>A dataset containing discourse markers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(discourse.markers.alemany)
</code></pre>


<h3>Format</h3>

<p>A data frame with 97 rows and 5 variables</p>


<h3>Details</h3>

<p>A dictionary of <em>discourse markers</em> from 
<a href="http://www.cs.famaf.unc.edu.ar/~laura/shallowdisc4summ/tesi_electronica.pdf">Alemany (2005)</a>. 
&quot;In this lexicon, discourse markers are characterized by their structural 
(continuation or elaboration) and semantic (revision, cause, equality, 
context) meanings, and they are also associated to a morphosyntactic class 
(part of speech, PoS), one of adverbial (A), phrasal (P) or conjunctive (C)...
Sometimes a discourse marker is <b>underspecified</b> with respect to a 
meaning. We encode this with a hash. This tends to happen with structural 
meanings, because these meanings can well be established by discursive 
mechanisms other than discourse markers, and the presence of the discourse 
marker just reinforces the relation, whichever it may be.&quot; (p. 191).
</p>
 
<ul>
<li><p> marker. The discourse marker
</p>
</li>
<li><p> type. The semantic type (typically overlaps with <code>semantic</code> except in the special types
</p>
</li>
<li><p> structural. How the marker is used structurally
</p>
</li>
<li><p> semantic. How the marker is used semantically
</p>
</li>
<li><p> pos. Part of speech: adverbial (A), phrasal (P) or conjunctive (C)
</p>
</li></ul>



<h3>References</h3>

<p>Alemany, L. A. (2005). Representing discourse for automatic text summarization via 
shallow NLP techniques (Unpublished doctoral dissertation). Universitat de Barcelona, Barcelona.<br /> 
<br />
<a href="http://www.cs.famaf.unc.edu.ar/~laura/shallowdisc4summ/tesi_electronica.pdf">http://www.cs.famaf.unc.edu.ar/~laura/shallowdisc4summ/tesi_electronica.pdf</a> <br />
<a href="http://russell.famaf.unc.edu.ar/~laura/shallowdisc4summ/discmar/#description">http://russell.famaf.unc.edu.ar/~laura/shallowdisc4summ/discmar/#description</a>
</p>

<hr>
<h2 id='Dolch'>Dolch List of 220 Common Words</h2><span id='topic+Dolch'></span>

<h3>Description</h3>

<p>Edward William Dolch's list of 220 Most Commonly Used Words.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Dolch)
</code></pre>


<h3>Format</h3>

<p>A vector with 220 elements</p>


<h3>Details</h3>

<p>Dolch's Word List made up 50-75% of all printed text in 1936.
</p>


<h3>References</h3>

<p>Dolch, E. W. (1936). A basic sight vocabulary. Elementary School
Journal, 36, 456-460.
</p>

<hr>
<h2 id='emoticon'>Emoticons Data Set</h2><span id='topic+emoticon'></span>

<h3>Description</h3>

<p>A dataset containing common emoticons (adapted from 
<a href="http://www.lingo2word.com/lists/emoticon_listH.html">Popular Emoticon List</a>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(emoticon)
</code></pre>


<h3>Format</h3>

<p>A data frame with 81 rows and 2 variables</p>


<h3>Details</h3>

 
<ul>
<li><p> meaning. The meaning of the emoticon
</p>
</li>
<li><p> emoticon. The graphic representation of the emoticon
</p>
</li></ul>



<h3>References</h3>

<p><a href="http://www.lingo2word.com/lists/emoticon_listH.html">http://www.lingo2word.com/lists/emoticon_listH.html</a>
</p>

<hr>
<h2 id='Fry_1000'>Fry's 1000 Most Commonly Used English Words</h2><span id='topic+Fry_1000'></span>

<h3>Description</h3>

<p>A stopword list containing a character vector of stopwords.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Fry_1000)
</code></pre>


<h3>Format</h3>

<p>A vector with 1000 elements</p>


<h3>Details</h3>

<p>Fry's 1000 Word List makes up 90% of all printed text.
</p>


<h3>References</h3>

<p>Fry, E. B. (1997). Fry 1000 instant words. Lincolnwood, IL: 
Contemporary Books.
</p>

<hr>
<h2 id='function.words'>Function Words</h2><span id='topic+function.words'></span>

<h3>Description</h3>

<p>A vector of function words from 
<a href="http://myweb.tiscali.co.uk/wordscape/museum/funcword.html">John and Muriel Higgins's list</a> 
used for the text game ECLIPSE.  The lest is augmented with additional 
contractions from <code><a href="#topic+contractions">contractions</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(function.words)
</code></pre>


<h3>Format</h3>

<p>A vector with 350 elements</p>


<h3>References</h3>

<p><a href="http://myweb.tiscali.co.uk/wordscape/museum/funcword.html">http://myweb.tiscali.co.uk/wordscape/museum/funcword.html</a>
</p>

<hr>
<h2 id='GradyAugmented'>Augmented List of Grady Ward's English Words and Mark Kantrowitz's Names List</h2><span id='topic+GradyAugmented'></span>

<h3>Description</h3>

<p>A dataset containing a vector of Grady Ward's English words augmented with 
<code><a href="#topic+DICTIONARY">DICTIONARY</a></code>, 
<a href="http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names">Mark Kantrowitz's names list</a>, 
other proper nouns, and contractions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(GradyAugmented)
</code></pre>


<h3>Format</h3>

<p>A vector with 122806 elements</p>


<h3>Details</h3>

<p>A dataset containing a vector of Grady Ward's English words 
augmented with proper nouns (U.S. States, Countries, Mark Kantrowitz's Names List, 
and months) and contractions. That dataset is augmented for spell checking purposes.
</p>


<h3>References</h3>

<p>Moby Thesaurus List by Grady Ward <a href="http://www.gutenberg.org">http://www.gutenberg.org</a> <br /> <br />
List of names from Mark Kantrowitz <a href="http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/">http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/</a>.  
A copy of the <a href="http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/readme.txt">README</a> 
is available <a href="http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/readme.txt">here</a> 
per the author's request.
</p>

<hr>
<h2 id='interjections'>Interjections</h2><span id='topic+interjections'></span>

<h3>Description</h3>

<p>A dataset containing a character vector of common interjections.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(interjections)
</code></pre>


<h3>Format</h3>

<p>A character vector with 139 elements</p>


<h3>References</h3>

<p><a href="http://www.vidarholen.net/contents/interjections/">http://www.vidarholen.net/contents/interjections/</a>
</p>

<hr>
<h2 id='key.pol'>Polarity Lookup Key</h2><span id='topic+key.pol'></span>

<h3>Description</h3>

<p>A dataset containing a polarity lookup key (see <code><a href="qdap.html#topic+polarity">polarity</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(key.pol)
</code></pre>


<h3>Format</h3>

<p>A hash key with words and corresponding values.</p>


<h3>References</h3>

<p>Hu, M., &amp; Liu, B. (2004). Mining opinion features in customer 
reviews. National Conference on Artificial Intelligence. 
</p>
<p><a href="http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html">http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html</a>
</p>

<hr>
<h2 id='key.power'>Power Lookup Key</h2><span id='topic+key.power'></span>

<h3>Description</h3>

<p>A dataset containing a power lookup key.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(key.power)
</code></pre>


<h3>Format</h3>

<p>A hash key with power words.</p>


<h3>References</h3>

<p><a href="http://www.wjh.harvard.edu/~inquirer/inqdict.txt">http://www.wjh.harvard.edu/~inquirer/inqdict.txt</a>
</p>

<hr>
<h2 id='key.strength'>Strength Lookup Key</h2><span id='topic+key.strength'></span>

<h3>Description</h3>

<p>A dataset containing a strength lookup key.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(key.strength)
</code></pre>


<h3>Format</h3>

<p>A hash key with strength words.</p>


<h3>References</h3>

<p><a href="http://www.wjh.harvard.edu/~inquirer/inqdict.txt">http://www.wjh.harvard.edu/~inquirer/inqdict.txt</a>
</p>

<hr>
<h2 id='key.syl'>Syllable Lookup Key</h2><span id='topic+key.syl'></span>

<h3>Description</h3>

<p>A dataset containing a syllable lookup key (see <code>DICTIONARY</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(key.syl)
</code></pre>


<h3>Format</h3>

<p>A hash key with a modified DICTIONARY data set.</p>


<h3>Details</h3>

<p>For internal use.
</p>


<h3>References</h3>

<p><a href="http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/nettalk/">UCI Machine Learning Repository website</a>
</p>

<hr>
<h2 id='key.syn'>Synonym Lookup Key</h2><span id='topic+key.syn'></span>

<h3>Description</h3>

<p>A dataset containing a synonym lookup key.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(key.syn)
</code></pre>


<h3>Format</h3>

<p>A hash key with 10976 rows and 2 variables (words and synonyms).</p>


<h3>References</h3>

<p>Scraped from:
<a href="http://dictionary.reverso.net/english-synonyms/">Reverso Online Dictionary</a>.
The word list fed to <a href="http://dictionary.reverso.net/english-synonyms/">Reverso</a> 
is the unique words from the combination of <code>DICTIONARY</code> and
<code>labMT</code>.
</p>

<hr>
<h2 id='labMT'>Language Assessment by Mechanical Turk (labMT) Sentiment Words</h2><span id='topic+labMT'></span>

<h3>Description</h3>

<p>A dataset containing words, average happiness score (polarity), standard 
deviations, and rankings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(labMT)
</code></pre>


<h3>Format</h3>

<p>A data frame with 10222 rows and 8 variables</p>


<h3>Details</h3>

 
<ul>
<li><p> word. The word.
</p>
</li>
<li><p> happiness_rank. Happiness ranking of words based on average happiness 
scores.
</p>
</li>
<li><p> happiness_average. Average happiness score.
</p>
</li>
<li><p> happiness_standard_deviation. Standard deviations of the happiness 
scores.
</p>
</li>
<li><p> twitter_rank. Twitter ranking of the word.
</p>
</li>
<li><p> google_rank. Google ranking of the word.
</p>
</li>
<li><p> nyt_rank. New York Times ranking of the word.
</p>
</li>
<li><p> lyrics_rank. lyrics ranking of the word.
</p>
</li></ul>



<h3>References</h3>

<p>Dodds, P.S., Harris, K.D., Kloumann, I.M., Bliss, C.A., &amp; Danforth, C.M. (2011) 
Temporal patterns of happiness and information in a global social network: 
Hedonometrics and twitter. PLoS ONE 6(12): e26752. 
doi:10.1371/journal.pone.0026752
</p>
<p><a href="http://www.plosone.org/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0026752.s001">http://www.plosone.org/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0026752.s001</a>
</p>

<hr>
<h2 id='Leveled_Dolch'>Leveled Dolch List of 220 Common Words</h2><span id='topic+Leveled_Dolch'></span>

<h3>Description</h3>

<p>Edward William Dolch's list of 220 Most Commonly Used Words by reading level.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Leveled_Dolch)
</code></pre>


<h3>Format</h3>

<p>A data frame with 220 rows and 2 variables</p>


<h3>Details</h3>

<p>Dolch's Word List made up 50-75% of all printed text in 1936.
</p>
 
<ul>
<li><p> Word. The word
</p>
</li>
<li><p> Level. The reading level of the word
</p>
</li></ul>



<h3>References</h3>

<p>Dolch, E. W. (1936). A basic sight vocabulary. Elementary School
Journal, 36, 456-460.
</p>

<hr>
<h2 id='NAMES'>First Names and Gender (U.S.)</h2><span id='topic+NAMES'></span>

<h3>Description</h3>

<p>A dataset containing 1990 U.S. census data on first names.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(NAMES)
</code></pre>


<h3>Format</h3>

<p>A data frame with 5493 rows and 7 variables</p>


<h3>Details</h3>

 
<ul>
<li><p> name. A first name.
</p>
</li>
<li><p> per.freq. Frequency in percent of the name by gender.
</p>
</li>
<li><p> cum.freq. Cumulative frequency in percent of the name by gender.
</p>
</li>
<li><p> rank. Rank of the name by gender.
</p>
</li>
<li><p> gender. Gender of the combined male/female list (M/F).
</p>
</li>
<li><p> gender2. Gender of the combined male/female list with &quot;B&quot; in place of 
overlapping (M/F) names.
</p>
</li>
<li><p> pred.sex. Predicted gender of the names with B's in <code>gender2</code> 
replaced with the gender that had a higher <code>per.freq</code>.
</p>
</li></ul>



<h3>References</h3>

<p><a href="http://www.census.gov">http://www.census.gov</a>
</p>

<hr>
<h2 id='NAMES_LIST'>First Names and Predictive Gender (U.S.) List</h2><span id='topic+NAMES_LIST'></span>

<h3>Description</h3>

<p>A list version of the <code>NAMES_SEX</code> dataset broken down by 
first letter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(NAMES_LIST)
</code></pre>


<h3>Format</h3>

<p>A list with 26 elements</p>


<h3>Details</h3>

<p>Alphabetical list of dataframes with the following variables:
</p>
 
<ul>
<li><p> name. A first name. 
</p>
</li>
<li><p> gender2. Gender of the combined male/female list with &quot;B&quot; in place of 
overlapping (M/F) names.
</p>
</li>
<li><p> pred.sex. Predicted gender of the names with B's in <code>gender2</code> 
replaced with the gender that had a higher <code>per.freq</code>.
</p>
</li></ul>



<h3>References</h3>

<p><a href="http://www.census.gov">http://www.census.gov</a>
</p>

<hr>
<h2 id='NAMES_SEX'>First Names and Predictive Gender (U.S.)</h2><span id='topic+NAMES_SEX'></span>

<h3>Description</h3>

<p>A truncated version of the <code>NAMES</code> dataset used for predicting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(NAMES_SEX)
</code></pre>


<h3>Format</h3>

<p>A data frame with 5162 rows and 3 variables</p>


<h3>Details</h3>

 
<ul>
<li><p> name. A first name. 
</p>
</li>
<li><p> gender2. Gender of the combined male/female list with &quot;B&quot; in place of 
overlapping (M/F) names.
</p>
</li>
<li><p> pred.sex. Predicted gender of the names with B's in <code>gender2</code> 
replaced with the gender that had a higher <code>per.freq</code>.
</p>
</li></ul>



<h3>References</h3>

<p><a href="http://www.census.gov">http://www.census.gov</a>
</p>

<hr>
<h2 id='negation.words'>Negating Words</h2><span id='topic+negation.words'></span>

<h3>Description</h3>

<p>A dataset containing a vector of words that negate word meaning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(negation.words)
</code></pre>


<h3>Format</h3>

<p>A vector with 23 elements</p>


<h3>Details</h3>

<p>Valence shifters are words that alter or intensify the meaning of the polarized
words and include negators and amplifiers. Negators are, generally, adverbs
that negate sentence meaning; for example the word like in the sentence, &quot;I do
like pie.&quot;, is given the opposite meaning in the sentence, &quot;I do not like
pie.&quot;, now containing the negator not. Amplifiers are, generally, adverbs or
adjectives that intensify sentence meaning. Using our previous example, the
sentiment of the negator altered sentence, &quot;I seriously do not like pie.&quot;, is
heightened with addition of the amplifier seriously.  Whereas de-amplifiers 
decrease the intensity of a polarized word as in the sentence &quot;I barely like
pie&quot;; the word &quot;barely&quot; deamplifies the word like.
</p>

<hr>
<h2 id='negative.words'>Negative Words</h2><span id='topic+negative.words'></span>

<h3>Description</h3>

<p>A dataset containing a vector of negative words.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(negative.words)
</code></pre>


<h3>Format</h3>

<p>A vector with 4776 elements</p>


<h3>Details</h3>

<p>A sentence containing more negative words would be deemed a negative sentence,
whereas a sentence containing more positive words would be considered positive.
</p>


<h3>References</h3>

<p>Hu, M., &amp; Liu, B. (2004). Mining opinion features in customer 
reviews. National Conference on Artificial Intelligence. 
</p>
<p><a href="http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html">http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html</a>
</p>

<hr>
<h2 id='OnixTxtRetToolkitSWL1'>Onix Text Retrieval Toolkit Stopword List 1</h2><span id='topic+OnixTxtRetToolkitSWL1'></span>

<h3>Description</h3>

<p>A stopword list containing a character vector of stopwords.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(OnixTxtRetToolkitSWL1)
</code></pre>


<h3>Format</h3>

<p>A character vector with 404 elements</p>


<h3>Details</h3>

<p><a href="http://www.lextek.com/manuals/onix/stopwords1.html">From Onix Text Retrieval Toolkit API Reference</a>:
&quot;This stopword list is probably the most widely used stopword list. It
covers a wide number of stopwords without getting too aggressive and
including too many words which a user might search upon.&quot;
</p>


<h3>Note</h3>

<p>Reduced from the original 429 words to 404.
</p>


<h3>References</h3>

<p><a href="http://www.lextek.com/manuals/onix/stopwords1.html">http://www.lextek.com/manuals/onix/stopwords1.html</a>
</p>

<hr>
<h2 id='positive.words'>Positive Words</h2><span id='topic+positive.words'></span>

<h3>Description</h3>

<p>A dataset containing a vector of positive words.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(positive.words)
</code></pre>


<h3>Format</h3>

<p>A vector with 2003 elements</p>


<h3>Details</h3>

<p>A sentence containing more negative words would be deemed a negative sentence,
whereas a sentence containing more positive words would be considered positive.
</p>


<h3>References</h3>

<p>Hu, M., &amp; Liu, B. (2004). Mining opinion features in customer 
reviews. National Conference on Artificial Intelligence. 
</p>
<p><a href="http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html">http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html</a>
</p>

<hr>
<h2 id='power.words'>Words that Indicate Power</h2><span id='topic+power.words'></span>

<h3>Description</h3>

<p>A subset of the 
<a href="http://www.wjh.harvard.edu/~inquirer/inqdict.txt">Harvard IV Dictionary</a> 
containing a vector of words indicating power.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(power.words)
</code></pre>


<h3>Format</h3>

<p>A vector with 624 elements</p>


<h3>References</h3>

<p><a href="http://www.wjh.harvard.edu/~inquirer/inqdict.txt">http://www.wjh.harvard.edu/~inquirer/inqdict.txt</a>
</p>

<hr>
<h2 id='preposition'>Preposition Words</h2><span id='topic+preposition'></span>

<h3>Description</h3>

<p>A dataset containing a vector of common prepositions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(preposition)
</code></pre>


<h3>Format</h3>

<p>A vector with 162 elements</p>

<hr>
<h2 id='print.view_data'>Prints a view_data Object</h2><span id='topic+print.view_data'></span>

<h3>Description</h3>

<p>Prints a view_data object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'view_data'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.view_data_+3A_x">x</code></td>
<td>
<p>The view_data object.</p>
</td></tr>
<tr><td><code id="print.view_data_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>

<hr>
<h2 id='strong.words'>Words that Indicate Strength</h2><span id='topic+strong.words'></span>

<h3>Description</h3>

<p>A subset of the 
<a href="http://www.wjh.harvard.edu/~inquirer/inqdict.txt">Harvard IV Dictionary</a> 
containing a vector of words indicating strength.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(strong.words)
</code></pre>


<h3>Format</h3>

<p>A vector with 1474 elements</p>


<h3>References</h3>

<p><a href="http://www.wjh.harvard.edu/~inquirer/inqdict.txt">http://www.wjh.harvard.edu/~inquirer/inqdict.txt</a>
</p>

<hr>
<h2 id='submit.words'>Words that Indicate Submission</h2><span id='topic+submit.words'></span>

<h3>Description</h3>

<p>A subset of the 
<a href="http://www.wjh.harvard.edu/~inquirer/inqdict.txt">Harvard IV Dictionary</a> 
containing a vector of words indicating submission.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(submit.words)
</code></pre>


<h3>Format</h3>

<p>A vector with 262 elements</p>


<h3>References</h3>

<p><a href="http://www.wjh.harvard.edu/~inquirer/inqdict.txt">http://www.wjh.harvard.edu/~inquirer/inqdict.txt</a>
</p>

<hr>
<h2 id='Top100Words'>Fry's  100 Most Commonly Used English Words</h2><span id='topic+Top100Words'></span>

<h3>Description</h3>

<p>A stopword list containing a character vector of stopwords.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Top100Words)
</code></pre>


<h3>Format</h3>

<p>A character vector with 100 elements</p>


<h3>Details</h3>

<p>Fry's Word List: The first 25 make up about one-third of all printed 
material in English. The first 100 make up about one-half of all printed 
material in English. The first 300 make up about 65% of all printed 
material in English.&quot;
</p>


<h3>References</h3>

<p>Fry, E. B. (1997). Fry 1000 instant words. Lincolnwood, IL: 
Contemporary Books.
</p>

<hr>
<h2 id='Top200Words'>Fry's 200 Most Commonly Used English Words</h2><span id='topic+Top200Words'></span>

<h3>Description</h3>

<p>A stopword list containing a character vector of stopwords.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Top200Words)
</code></pre>


<h3>Format</h3>

<p>A character vector with 200 elements</p>


<h3>Details</h3>

<p>Fry's Word List: The first 25 make up about one-third of all printed 
material in English. The first 100 make up about one-half of all printed 
material in English. The first 300 make up about 65% of all printed 
material in English.&quot;
</p>


<h3>References</h3>

<p>Fry, E. B. (1997). Fry 1000 instant words. Lincolnwood, IL: 
Contemporary Books.
</p>

<hr>
<h2 id='Top25Words'>Fry's 25 Most Commonly Used English Words</h2><span id='topic+Top25Words'></span>

<h3>Description</h3>

<p>A stopword list containing a character vector of stopwords.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Top25Words)
</code></pre>


<h3>Format</h3>

<p>A character vector with 25 elements</p>


<h3>Details</h3>

<p>Fry's Word List: The first 25 make up about one-third of all printed 
material in English. The first 100 make up about one-half of all printed 
material in English. The first 300 make up about 65% of all printed 
material in English.&quot;
</p>


<h3>References</h3>

<p>Fry, E. B. (1997). Fry 1000 instant words. Lincolnwood, IL: 
Contemporary Books.
</p>

<hr>
<h2 id='view_data'>List all data sets available in a <span class="pkg">qdapDictionaries</span></h2><span id='topic+view_data'></span>

<h3>Description</h3>

<p>Lists and describes all the data sets available in <span class="pkg">qdapDictionaries</span>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>view_data(package = "qdapDictionaries")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="view_data_+3A_package">package</code></td>
<td>
<p>The name of the package.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the data sets of <span class="pkg">qdapDictionaries</span> as a dataframe.
</p>


<h3>See Also</h3>

<p><code><a href="utils.html#topic+data">data</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>view_data()
</code></pre>

<hr>
<h2 id='weak.words'>Words that Indicate Weakness</h2><span id='topic+weak.words'></span>

<h3>Description</h3>

<p>A subset of the 
<a href="http://www.wjh.harvard.edu/~inquirer/inqdict.txt">Harvard IV Dictionary</a> 
containing a vector of words indicating weakness.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(weak.words)
</code></pre>


<h3>Format</h3>

<p>A vector with 647 elements</p>


<h3>References</h3>

<p><a href="http://www.wjh.harvard.edu/~inquirer/inqdict.txt">http://www.wjh.harvard.edu/~inquirer/inqdict.txt</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
