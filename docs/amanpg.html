<!DOCTYPE html><html><head><title>Help for package amanpg</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {amanpg}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#normalize'>
<p>Matrix Normalization</p></a></li>
<li><a href='#prox.l1'>
<p>Proximal L1 Mapping</p></a></li>
<li><a href='#spca.amanpg'>
<p>Alternating Manifold Proximal Gradient algorithm for Sparse PCA</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.3.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-10-02</td>
</tr>
<tr>
<td>Title:</td>
<td>Alternating Manifold Proximal Gradient Method for Sparse PCA</td>
</tr>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Description:</td>
<td>Alternating Manifold Proximal Gradient Method for Sparse PCA uses the Alternating Manifold Proximal 
    Gradient (AManPG) method to find sparse principal components from a data or covariance matrix. Provides
    a novel algorithm for solving the sparse principal component analysis problem which provides
    advantages over existing methods in terms of efficiency and convergence guarantees.
    Chen, S., Ma, S., Xue, L., &amp; Zou, H. (2020) &lt;<a href="https://doi.org/10.1287%2Fijoo.2019.0032">doi:10.1287/ijoo.2019.0032</a>&gt;.
    Zou, H., Hastie, T., &amp; Tibshirani, R. (2006) &lt;<a href="https://doi.org/10.1198%2F106186006X113430">doi:10.1198/106186006X113430</a>&gt;.
    Zou, H., &amp; Xue, L. (2018) &lt;<a href="https://doi.org/10.1109%2FJPROC.2018.2846588">doi:10.1109/JPROC.2018.2846588</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Author:</td>
<td>Shixiang Chen [aut],
  Justin Huang [aut],
  Benjamin Jochem [aut],
  Shiqian Ma [aut],
  Haichuan Xu [aut],
  Lingzhou Xue [aut],
  Zhong Zheng [cre, aut],
  Hui Zou [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Zhong Zheng &lt;zvz5337@psu.edu&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-10-02 15:38:42 UTC; 15000</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-10-02 16:10:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='normalize'>
Matrix Normalization
</h2><span id='topic+normalize'></span>

<h3>Description</h3>

<p>Center the input matrix to mean 0 and scale to Euclidean length 1
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalize(x, center=TRUE, scale=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalize_+3A_x">x</code></td>
<td>
<p>matrix to be normalized</p>
</td></tr>
<tr><td><code id="normalize_+3A_center">center</code></td>
<td>
<p>centers the input matrix to mean 0 if TRUE, default if TRUE</p>
</td></tr>
<tr><td><code id="normalize_+3A_scale">scale</code></td>
<td>
<p>scales the input matrix to Euclidean length 1 if TRUE, default is TRUE</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>x</code></td>
<td>
<p>normalized matrix</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Shixiang Chen, Justin Huang, Benjamin Jochem, Shiqian Ma, Lingzhou Xue and Hui Zou
</p>

<hr>
<h2 id='prox.l1'>
Proximal L1 Mapping
</h2><span id='topic+prox.l1'></span>

<h3>Description</h3>

<p>Calculates the proximal L1 mapping for the given input matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prox.l1(z, lambda, r)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prox.l1_+3A_z">z</code></td>
<td>
<p>input matrix</p>
</td></tr>
<tr><td><code id="prox.l1_+3A_lambda">lambda</code></td>
<td>
<p>parameters for calculating proximal L1 mapping</p>
</td></tr>
<tr><td><code id="prox.l1_+3A_r">r</code></td>
<td>
<p>number of columns used in matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>x_prox</code></td>
<td>
<p>proximal L1 Mapping</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Shixiang Chen, Justin Huang, Benjamin Jochem, Shiqian Ma, Lingzhou Xue and Hui Zou
</p>


<h3>References</h3>

<p>Chen, S., Ma, S., Xue, L., and Zou, H. (2020) &quot;An Alternating Manifold Proximal Gradient Method for Sparse Principal Component Analysis and Sparse Canonical Correlation Analysis&quot; *INFORMS Journal on Optimization* 2:3, 192-208
</p>

<hr>
<h2 id='spca.amanpg'>
Alternating Manifold Proximal Gradient algorithm for Sparse PCA
</h2><span id='topic+spca.amanpg'></span>

<h3>Description</h3>

<p>Performs sparse principal component analysis on the input matrix using an alternating manifold
proximal gradient (AManPG) method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spca.amanpg(z, lambda1, lambda2, f_palm = 1e5, x0 = NULL, y0 = NULL, k = 0, type = 0,
       gamma = 0.5, maxiter = 1e4, tol = 1e-5, normalize = TRUE, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spca.amanpg_+3A_z">z</code></td>
<td>
<p>Either the data matrix or sample covariance matrix</p>
</td></tr>
<tr><td><code id="spca.amanpg_+3A_lambda1">lambda1</code></td>
<td>
<p>List of parameters of length n for L1-norm penalty</p>
</td></tr>
<tr><td><code id="spca.amanpg_+3A_lambda2">lambda2</code></td>
<td>
<p>L2-norm penalty term</p>
</td></tr>
<tr><td><code id="spca.amanpg_+3A_f_palm">f_palm</code></td>
<td>
<p>Upper bound for the gradient value to reach convergence, default value is 1e5</p>
</td></tr>
<tr><td><code id="spca.amanpg_+3A_x0">x0</code></td>
<td>
<p>Initial x-values for the gradient method, default value is the first n right singular vectors</p>
</td></tr>
<tr><td><code id="spca.amanpg_+3A_y0">y0</code></td>
<td>
<p>Initial y-values for the gradient method, default value is the first n right singular vectors</p>
</td></tr>
<tr><td><code id="spca.amanpg_+3A_k">k</code></td>
<td>
<p>Number of principal components desired, default is 0 (returns min(n-1, p) principal components)</p>
</td></tr>
<tr><td><code id="spca.amanpg_+3A_type">type</code></td>
<td>
<p>If 0, b is expected to be a data matrix, and otherwise b is expected to be a covariance matrix; default is 0</p>
</td></tr>
<tr><td><code id="spca.amanpg_+3A_gamma">gamma</code></td>
<td>
<p>Parameter to control how quickly the step size changes in each iteration, default is 0.5</p>
</td></tr>
<tr><td><code id="spca.amanpg_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations allowed in the gradient method, default is 1e4</p>
</td></tr>
<tr><td><code id="spca.amanpg_+3A_tol">tol</code></td>
<td>
<p>Tolerance value required to indicate convergence (calculated as difference between iteration f-values), default is 1e-5</p>
</td></tr>
<tr><td><code id="spca.amanpg_+3A_normalize">normalize</code></td>
<td>
<p>Center and normalize rows to Euclidean length 1 if True, default is True</p>
</td></tr>
<tr><td><code id="spca.amanpg_+3A_verbose">verbose</code></td>
<td>
<p>Function prints progress between iterations if True, default is False</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>iter</code></td>
<td>
<p>total number of iterations executed in the algorithm</p>
</td></tr>
<tr><td><code>f_amanpg</code></td>
<td>
<p>final gradient value</p>
</td></tr>
<tr><td><code>sparsity</code></td>
<td>
<p>Number of sparse loadings (loadings == 0) divided by number of all loadings</p>
</td></tr>
<tr><td><code>time</code></td>
<td>
<p>execution time in seconds</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>corresponding matrix in subproblem to the loadings</p>
</td></tr>
<tr><td><code>loadings</code></td>
<td>
<p>loadings of the sparse principal components</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Shixiang Chen, Justin Huang, Benjamin Jochem, Shiqian Ma, Lingzhou Xue and Hui Zou
</p>


<h3>References</h3>

<p>Chen, S., Ma, S., Xue, L., and Zou, H. (2020) &quot;An Alternating Manifold Proximal Gradient Method for Sparse Principal Component Analysis and Sparse Canonical Correlation Analysis&quot; *INFORMS Journal on Optimization* 2:3, 192-208
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#see SPCA.R for a more in-depth example
      d &lt;- 500  # dimension
      m &lt;- 1000 # sample size
      a &lt;- normalize(matrix(rnorm(m * d), m, d))
      lambda1 &lt;- 0.1 * matrix(data=1, nrow=4, ncol=1)
      x0 &lt;- svd(a, nv=4)$v
      sprout &lt;- spca.amanpg(a, lambda1, lambda2=Inf, f_palm=1e5, x0=x0, y0=x0, k=4, type=0, 
                            gamma=0.5, maxiter=1e4, tol=1e-5, normalize = FALSE, verbose=FALSE)
      print(paste(sprout$iter, "iterations,", sprout$sparsity, "sparsity,", sprout$time))

      #extract loadings
      #print(sprout$loadings)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
