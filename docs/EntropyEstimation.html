<!DOCTYPE html><html lang="en"><head><title>Help for package EntropyEstimation</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {EntropyEstimation}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#EntropyEstimation-package'><p>Estimation of Entropy and Related Quantities</p></a></li>
<li><a href='#Entropy.sd'><p>Entropy.sd</p></a></li>
<li><a href='#Entropy.z'><p>Entropy.z</p></a></li>
<li><a href='#GenSimp.sd'><p>GenSimp.sd</p></a></li>
<li><a href='#GenSimp.z'><p>GenSimp.z</p></a></li>
<li><a href='#Hill.sd'><p>Hill.sd</p></a></li>
<li><a href='#Hill.z'><p>Hill.z</p></a></li>
<li><a href='#KL.Plugin'><p>KL.Plugin</p></a></li>
<li><a href='#KL.sd'><p>KL.sd</p></a></li>
<li><a href='#KL.z'><p>KL.z</p></a></li>
<li><a href='#MI.sd'><p>MI.sd</p></a></li>
<li><a href='#MI.z'><p>MI.z</p></a></li>
<li><a href='#Renyi.sd'><p>Renyi.sd</p></a></li>
<li><a href='#Renyi.z'><p>Renyi.z</p></a></li>
<li><a href='#RenyiEq.sd'><p>RenyiEq.sd</p></a></li>
<li><a href='#RenyiEq.z'><p>RenyiEq.z</p></a></li>
<li><a href='#SymKL.Plugin'><p>SymKL.Plugin</p></a></li>
<li><a href='#SymKL.sd'><p>SymKL.sd</p></a></li>
<li><a href='#SymKL.z'><p>SymKL.z</p></a></li>
<li><a href='#Tsallis.sd'><p>Tsallis.sd</p></a></li>
<li><a href='#Tsallis.z'><p>Tsallis.z</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Estimation of Entropy and Related Quantities</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-09-14</td>
</tr>
<tr>
<td>Description:</td>
<td>Contains methods for the estimation of Shannon's entropy, variants of Renyi's entropy, mutual information, Kullback-Leibler divergence, and generalized Simpson's indices. The estimators used have a bias that decays exponentially fast. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-09-14 16:59:05 UTC; lcao2</td>
</tr>
<tr>
<td>Author:</td>
<td>Lijuan Cao [aut],
  Michael Grabchak [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michael Grabchak &lt;mgrabcha@charlotte.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-09-15 00:10:01 UTC</td>
</tr>
</table>
<hr>
<h2 id='EntropyEstimation-package'>Estimation of Entropy and Related Quantities</h2><span id='topic+EntropyEstimation-package'></span><span id='topic+EntropyEstimation'></span>

<h3>Description</h3>

<p>Contains methods for the estimation of Shannon's entropy, variants of Renyi's entropy, mutual Information, Kullback-Leibler divergence, and generalized Simpson's indices. These estimators have a bias that decays exponentially fast. For more information see Z. Zhang and J. Zhou (2010), Zhang (2012), Zhang (2013), Zhang and Grabchak (2013), Zhang and Grabchak (2014a), Zhang and Grabchak (2014b), and Zhang and Zheng (2014). 
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> EntropyEstimation</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.2.1</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2024-09-14</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL3</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Lijuan Cao &lt;lcao2@charlotte.edu&gt; and Michael Grabchak &lt;mgrabcha@charlotte.edu&gt; 
</p>


<h3>References</h3>

<p>Z. Zhang (2012). Entropy estimation in Turing's' perspective. Neural Computation 24(5), 1368&ndash;1389.
</p>
<p>Z. Zhang (2013). Asymptotic normality of an entropy estimator with asymptotically decaying bias. IEEE Transactions on Information Theory 59(1), 504&ndash;508.
</p>
<p>Z. Zhang and M. Grabchak (2013). Bias Adjustment for a Nonparametric Entropy Estimator. Entropy, 15(6), 1999-2011.
</p>
<p>Z. Zhang and M. Grabchak (2014a). Entropic representation and estimation of diversity indices. http://arxiv.org/abs/1403.3031.
</p>
<p>Z. Zhang and M. Grabchak (2014b). Nonparametric Estimation of Kullback-Leibler Divergence. Neural Computation, 26(11): 2570-2593.
</p>
<p>Z. Zhang and L. Zheng (2014). A Mutual Information Estimator with Exponentially Decaying Bias.
</p>
<p>Z. Zhang and J. Zhou (2010). Re-parameterization of multinomial distributions and diversity indices. Journal of Statistical Planning and Inference 140(7), 1731-1738.
</p>

<hr>
<h2 id='Entropy.sd'>Entropy.sd</h2><span id='topic+Entropy.sd'></span>

<h3>Description</h3>

<p>Returns the estimated asymptotic standard deviation for the Z estimator of Shannon's Entropy. Note that this is also the asymptotic standard deviation of the plug-in estimator. See Zhang and Grabchak (2014a) for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Entropy.sd(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Entropy.sd_+3A_x">x</code></td>
<td>
<p>Vector of counts. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014a). Entropic representation and estimation of diversity indices. http://arxiv.org/abs/1403.3031.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = c(1,3,7,4,8) # vector of counts
 Entropy.sd(x)  # Estimated standard deviation
 
 data = rbinom(10,20,.5)
 counts = tabulate(as.factor(data))
 Entropy.sd(counts)
</code></pre>

<hr>
<h2 id='Entropy.z'>Entropy.z</h2><span id='topic+Entropy.z'></span>

<h3>Description</h3>

<p>Returns the Z estimator of Shannon's Entropy. This estimator has exponentially decaying bias. See Zhang (2012), Zhang (2013), and Zhang and Grabchak (2014a) for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Entropy.z(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Entropy.z_+3A_x">x</code></td>
<td>
<p>Vector of counts. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang (2012). Entropy estimation in Turing's' perspective. Neural Computation 24(5), 1368&ndash;1389.
</p>
<p>Z. Zhang (2013). Asymptotic normality of an entropy estimator with asymptotically decaying bias. IEEE Transactions on Information Theory 59(1), 504&ndash;508.
</p>
<p>Z. Zhang and M. Grabchak (2014a). Entropic representation and estimation of diversity indices. http://arxiv.org/abs/1403.3031.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = c(1,3,7,4,8) 
 Entropy.z(x)  
 
 data = rbinom(10,20,.5)
 counts = tabulate(as.factor(data))
 Entropy.z(counts)
</code></pre>

<hr>
<h2 id='GenSimp.sd'>GenSimp.sd</h2><span id='topic+GenSimp.sd'></span>

<h3>Description</h3>

<p>Returns the estimated asymptotic standard deviation of the Z estimator of the generalized Simpson's index of order r, i.e. of the index sum_k p_k(1-p_k)^r. This estimate of the standard deviation is based on the formula in Zhang and Grabchak (2014a) and not the one in Zhang and Zhou (2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GenSimp.sd(x, r)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="GenSimp.sd_+3A_x">x</code></td>
<td>

<p>Vector of counts. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
<tr><td><code id="GenSimp.sd_+3A_r">r</code></td>
<td>
<p>Positive integer representing the order of the generalized Simpson's index. If a noninteger value is given then the integer part is taken. Must be strictly less than sum(x).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014a). Entropic representation and estimation of diversity indices. http://arxiv.org/abs/1403.3031.
</p>
<p>Z. Zhang and J. Zhou (2010). Re-parameterization of multinomial distributions and diversity indices. Journal of Statistical Planning and Inference 140(7), 1731-1738.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = c(1,3,7,4,8)
 GenSimp.sd(x,2)  
 
 data = rbinom(10,20,.5)
 counts = tabulate(as.factor(data))
 GenSimp.sd(counts,2)
</code></pre>

<hr>
<h2 id='GenSimp.z'>GenSimp.z</h2><span id='topic+GenSimp.z'></span>

<h3>Description</h3>

<p>Returns the Z estimator of the generalized Simpson's index of order r, i.e. of the index sum_k p_k(1-p_k)^r. See Zhang and Zhou (2010) and Zhang and Grabchak (2014a) for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GenSimp.z(x,r)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="GenSimp.z_+3A_x">x</code></td>
<td>

<p>Vector of counts. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
<tr><td><code id="GenSimp.z_+3A_r">r</code></td>
<td>
<p>Positive integer representing the order of the generalized Simpson's index. If a noninteger value is given then the integer part is taken. Must be strictly less than sum(x).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014a). Entropic representation and estimation of diversity indices. http://arxiv.org/abs/1403.3031.
</p>
<p>Z. Zhang and J. Zhou (2010). Re-parameterization of multinomial distributions and diversity indices. Journal of Statistical Planning and Inference 140(7), 1731-1738.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = c(1,3,7,4,8) 
 GenSimp.z(x,2)  
 
 data = rbinom(10,20,.5)
 counts = tabulate(as.factor(data))
 GenSimp.z(counts,2)
</code></pre>

<hr>
<h2 id='Hill.sd'>Hill.sd</h2><span id='topic+Hill.sd'></span>

<h3>Description</h3>

<p>Returns the estimated asymptotic standard deviation for the Z estimator of Hill's diversity numbe. Note that this is also the asymptotic standard deviation of the plug-in estimator. See Zhang and Grabchak (2014a) for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Hill.sd(x, r)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Hill.sd_+3A_x">x</code></td>
<td>

<p>Vector of counts. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
<tr><td><code id="Hill.sd_+3A_r">r</code></td>
<td>
<p> Order of Hill's deversity numbe. Must be a strictly positive real number.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014a). Entropic representation and estimation of diversity indices. http://arxiv.org/abs/1403.3031.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = c(1,3,7,4,8)
 Hill.sd(x,2)  
 
 data = rbinom(10,20,.5)
 counts = tabulate(as.factor(data))
 Hill.sd(counts,2)
</code></pre>

<hr>
<h2 id='Hill.z'>Hill.z</h2><span id='topic+Hill.z'></span>

<h3>Description</h3>

<p>Returns the Z estimator of Hill's diversity number. This is based on raising the Z estimator of Renyi's equivalent entropy to the 1/(r-1) power.  When r=1 returns exp(H), where H is the Z estimator of Shannon's entropy. See Zhang and Grabchak (2014a) for details.</p>


<h3>Usage</h3>

<pre><code class='language-R'>Hill.z(x, r)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Hill.z_+3A_x">x</code></td>
<td>

<p>Vector of counts. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
<tr><td><code id="Hill.z_+3A_r">r</code></td>
<td>
<p>Order of Renyi's equivalent entropy this index is based on. Must be a strictly positive real number.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014a). Entropic representation and estimation of diversity indices. http://arxiv.org/abs/1403.3031.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = c(1,3,7,4,8)
 Hill.z(x,2)  
 
 data = rbinom(10,20,.5)
 counts = tabulate(as.factor(data))
 Hill.z(counts,2)
</code></pre>

<hr>
<h2 id='KL.Plugin'>KL.Plugin</h2><span id='topic+KL.Plugin'></span>

<h3>Description</h3>

<p>Returns the augmented plugin estimator of Kullback-Leibler Divergence.  See Zhang and Grabchak (2014b) for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KL.Plugin(x, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="KL.Plugin_+3A_x">x</code></td>
<td>

<p>Vector of counts from first distribution. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
<tr><td><code id="KL.Plugin_+3A_y">y</code></td>
<td>

<p>Vector of counts from second distribution. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014b). Nonparametric Estimation of Kullback-Leibler Divergence. Neural Computation, 26(11): 2570-2593.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = c(1,3,7,4,8) 
 y = c(2,5,1,3,6) 
 KL.Plugin(x,y)  
 KL.Plugin(y,x)  
</code></pre>

<hr>
<h2 id='KL.sd'>KL.sd</h2><span id='topic+KL.sd'></span>

<h3>Description</h3>

<p>Returns the estimated asymptotic standard deviation for the Z estimator of Kullback-Leibler's divergence. Note that this is also the asymptotic standard deviation of the plug-in estimator. See Zhang and Grabchak (2014b) for details.</p>


<h3>Usage</h3>

<pre><code class='language-R'>KL.sd(x, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="KL.sd_+3A_x">x</code></td>
<td>

<p>Vector of counts from the first distribution. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
<tr><td><code id="KL.sd_+3A_y">y</code></td>
<td>

<p>Vector of counts from the second distribution. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014b). Nonparametric Estimation of Kullback-Leibler Divergence. Neural Computation, 26(11): 2570-2593.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = c(1,3,7,4,8) # first vector of counts
 y = c(2,5,1,3,6) # second vector of counts
 KL.sd(x,y)  # Estimated standard deviation
 KL.sd(y,x)  # Estimated standard deviation
</code></pre>

<hr>
<h2 id='KL.z'>KL.z</h2><span id='topic+KL.z'></span>

<h3>Description</h3>

<p>Returns the Z estimator of Kullback-Leibler Divergence, which has exponentially decaying bias.  See Zhang and Grabchak (2014b) for details.</p>


<h3>Usage</h3>

<pre><code class='language-R'>KL.z(x, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="KL.z_+3A_x">x</code></td>
<td>

<p>Vector of counts from the first distribution. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
<tr><td><code id="KL.z_+3A_y">y</code></td>
<td>

<p>Vector of counts from the second distribution. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014b). Nonparametric Estimation of Kullback-Leibler Divergence. Neural Computation, 26(11): 2570-2593.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = c(1,3,7,4,8) 
 y = c(2,5,1,3,6) 
 KL.z(x,y)  
 KL.z(y,x)  
</code></pre>

<hr>
<h2 id='MI.sd'>MI.sd</h2><span id='topic+MI.sd'></span>

<h3>Description</h3>

<p>Returns the estimated asymptotic standard deviation for the Z estimator of mutual information. Note that this is also the asymptotic standard deviation of the plug-in estimator. See Zhang and Zheng (2014) for details.</p>


<h3>Usage</h3>

<pre><code class='language-R'>MI.sd(y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="MI.sd_+3A_y">y</code></td>
<td>

<p>Matrix of counts. Must be integer valued. Each entry represents the number of observations of a distinct combination of letters from the two alphabets.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and L. Zheng (2014). A Mutual Information Estimator with Exponentially Decaying Bias. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = matrix(c(0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
       0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
       0, 0, 0, 2, 1, 0, 1, 0, 0, 1,
       0, 0, 0, 1, 1, 2, 0, 0, 0, 0,
       0, 0, 0, 3, 6, 2, 2, 0, 0, 0,
       2, 0, 2, 5, 6, 5, 1, 0, 0, 0,
       0, 0, 4, 6, 11, 5, 1, 1, 0, 1,
       0, 0, 5, 10, 21, 7, 5, 1, 0, 1,
       0, 0, 7, 11, 9, 6, 3, 0, 0, 1,
       0, 0, 4, 10, 6, 5, 1, 0, 0, 0),10,10,byrow=TRUE)
MI.sd(x)  

x = rbinom(100,20,.5)
y = rbinom(100,20,.5)
MI.sd(table(x,y))
</code></pre>

<hr>
<h2 id='MI.z'>MI.z</h2><span id='topic+MI.z'></span>

<h3>Description</h3>

<p>Returns the Z estimator of Mutual Information. This estimator has exponentially decaying bias. See Zhang and Zheng (2014) for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MI.z(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="MI.z_+3A_x">x</code></td>
<td>

<p>Matrix of counts. Must be integer valued. Each entry represents the number of observations of a distinct combination of letters from the two alphabets.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and L. Zheng (2014). A Mutual Information Estimator with Exponentially Decaying Bias.</p>


<h3>Examples</h3>

<pre><code class='language-R'>x = matrix(c(0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
       0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
       0, 0, 0, 2, 1, 0, 1, 0, 0, 1,
       0, 0, 0, 1, 1, 2, 0, 0, 0, 0,
       0, 0, 0, 3, 6, 2, 2, 0, 0, 0,
       2, 0, 2, 5, 6, 5, 1, 0, 0, 0,
       0, 0, 4, 6, 11, 5, 1, 1, 0, 1,
       0, 0, 5, 10, 21, 7, 5, 1, 0, 1,
       0, 0, 7, 11, 9, 6, 3, 0, 0, 1,
       0, 0, 4, 10, 6, 5, 1, 0, 0, 0),10,10,byrow=TRUE)
MI.z(x)       


x = rbinom(100,20,.5)
y = rbinom(100,20,.5)
MI.z(table(x,y))
</code></pre>

<hr>
<h2 id='Renyi.sd'>Renyi.sd</h2><span id='topic+Renyi.sd'></span>

<h3>Description</h3>

<p>Returns the estimated asymptotic standard deviation for the Z estimator of Renyi's Entropy. Note that this is also the asymptotic standard deviation of the plug-in estimator. See Zhang and Grabchak (2014a) for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Renyi.sd(x, r)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Renyi.sd_+3A_x">x</code></td>
<td>

<p>Vector of counts. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
<tr><td><code id="Renyi.sd_+3A_r">r</code></td>
<td>
<p> Order of Renyi's entropy. Must be a strictly positive real number. Not allowed to be 1, in that case use Entropy.sd instead.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014a). Entropic representation and estimation of diversity indices. http://arxiv.org/abs/1403.3031.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = c(1,3,7,4,8)
 Renyi.sd(x,2)  
 
 data = rbinom(10,20,.5)
 counts = tabulate(as.factor(data))
 Renyi.sd(counts,2)
</code></pre>

<hr>
<h2 id='Renyi.z'>Renyi.z</h2><span id='topic+Renyi.z'></span>

<h3>Description</h3>

<p>Returns the Z estimator of Renyi's entropy. This is based on taking the log of the Z estimator of Renyi's equivalent entropy and dividing by (1-r). When r=1 returns the Z estimator of Shannon's entropy. See Zhang and Grabchak (2014a) for details.</p>


<h3>Usage</h3>

<pre><code class='language-R'>Renyi.z(x, r)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Renyi.z_+3A_x">x</code></td>
<td>

<p>Vector of counts. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
<tr><td><code id="Renyi.z_+3A_r">r</code></td>
<td>
<p> Order of Renyi's equivalent entropy this index is based on. Must be a strictly positive real number.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014a). Entropic representation and estimation of diversity indices. http://arxiv.org/abs/1403.3031.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = c(1,3,7,4,8)
 Renyi.z(x,2)  
 
 data = rbinom(10,20,.5)
 counts = tabulate(as.factor(data))
 Renyi.z(counts,2)
</code></pre>

<hr>
<h2 id='RenyiEq.sd'>RenyiEq.sd</h2><span id='topic+RenyiEq.sd'></span>

<h3>Description</h3>

<p>Returns the estimated asymptotic standard deviation for the Z estimator of Renyi Equivalent Entropy. Note that this is also the asymptotic standard deviation of the plug-in estimator.  When r=1, returns 0. See Zhang and Grabchak (2014a) for details.</p>


<h3>Usage</h3>

<pre><code class='language-R'>RenyiEq.sd(x, r)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RenyiEq.sd_+3A_x">x</code></td>
<td>

<p>Vector of counts. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
<tr><td><code id="RenyiEq.sd_+3A_r">r</code></td>
<td>

<p>Order of Renyi's equivalent entropy. Must be a strictly positive real number.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014a). Entropic representation and estimation of diversity indices. http://arxiv.org/abs/1403.3031.</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = c(1,3,7,4,8) 
 RenyiEq.sd(x,2)  
 
 data = rbinom(10,20,.5)
 counts = tabulate(as.factor(data))
 RenyiEq.sd(counts,2)
</code></pre>

<hr>
<h2 id='RenyiEq.z'>RenyiEq.z</h2><span id='topic+RenyiEq.z'></span>

<h3>Description</h3>

<p>Returns the Z estimator of Renyi's equivalent entropy. This estimator has exponentially decaying bias. When r=1 returns 1. See Zhang and Grabchak (2014a) for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RenyiEq.z(x, r)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RenyiEq.z_+3A_x">x</code></td>
<td>

<p>Vector of counts. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
<tr><td><code id="RenyiEq.z_+3A_r">r</code></td>
<td>

<p>Order of Renyi's equivalent entropy. Must be a strictly positive real number.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014a). Entropic representation and estimation of diversity indices. http://arxiv.org/abs/1403.3031.</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = c(1,3,7,4,8) 
 RenyiEq.z(x,2)  
 
 data = rbinom(10,20,.5)
 counts = tabulate(as.factor(data))
 RenyiEq.z(counts,2)
</code></pre>

<hr>
<h2 id='SymKL.Plugin'>SymKL.Plugin</h2><span id='topic+SymKL.Plugin'></span>

<h3>Description</h3>

<p>Returns the augmented plugin estimator of Symetrized Kullback-Leibler Divergence.  See Zhang and Grabchak (2014b) for details.</p>


<h3>Usage</h3>

<pre><code class='language-R'>SymKL.Plugin(x, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SymKL.Plugin_+3A_x">x</code></td>
<td>

<p>Vector of counts from first distribution. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
<tr><td><code id="SymKL.Plugin_+3A_y">y</code></td>
<td>

<p>Vector of counts from second distribution. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014b). Nonparametric Estimation of Kullback-Leibler Divergence. Neural Computation, DOI 10.1162/NECO_a_00646.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = c(1,3,7,4,8) # first vector of counts
 y = c(2,5,1,3,6) # second vector of counts
 SymKL.Plugin(x,y)  # Estimated standard deviation
</code></pre>

<hr>
<h2 id='SymKL.sd'>SymKL.sd</h2><span id='topic+SymKL.sd'></span>

<h3>Description</h3>

<p>Returns the estimated asymptotic standard deviation for the Z estimator of Symmetrized Kullback-Leibler's divergence. Note that this is also the asymptotic standard deviation of the plug-in estimator. See Zhang and Grabchak (2014b) for details.</p>


<h3>Usage</h3>

<pre><code class='language-R'>SymKL.sd(x, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SymKL.sd_+3A_x">x</code></td>
<td>

<p>Vector of counts from first distribution. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
<tr><td><code id="SymKL.sd_+3A_y">y</code></td>
<td>

<p>Vector of counts from second distribution. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014b). Nonparametric Estimation of Kullback-Leibler Divergence. Neural Computation, DOI 10.1162/NECO_a_00646.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = c(1,3,7,4,8) # first vector of counts
 y = c(2,5,1,3,6) # second vector of counts
 SymKL.sd(x,y)  # Estimated standard deviation
</code></pre>

<hr>
<h2 id='SymKL.z'>SymKL.z</h2><span id='topic+SymKL.z'></span>

<h3>Description</h3>

<p>Returns the Z estimator of Symetrized Kullback-Leibler Divergence, which has exponentialy decaying bias.  See Zhang and Grabchak (2014b) for details.</p>


<h3>Usage</h3>

<pre><code class='language-R'>SymKL.z(x, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SymKL.z_+3A_x">x</code></td>
<td>

<p>Vector of counts from first distribution. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
<tr><td><code id="SymKL.z_+3A_y">y</code></td>
<td>

<p>Vector of counts from second distribution. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014b). Nonparametric Estimation of Kullback-Leibler Divergence. Neural Computation, DOI 10.1162/NECO_a_00646.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = c(1,3,7,4,8) 
 y = c(2,5,1,3,6) 
 SymKL.z(x,y) 
</code></pre>

<hr>
<h2 id='Tsallis.sd'>Tsallis.sd</h2><span id='topic+Tsallis.sd'></span>

<h3>Description</h3>

<p>Returns the estimated asymptotic standard deviation for the Z estimator of Tsallis Entropy. Note that this is also the asymptotic standard deviation of the plug-in estimator. See Zhang and Grabchak (2014a) for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Tsallis.sd(x, r)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Tsallis.sd_+3A_x">x</code></td>
<td>

<p>Vector of counts. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
<tr><td><code id="Tsallis.sd_+3A_r">r</code></td>
<td>
<p> Order of Tsallis entropy. Must be a strictly positive real number. Not allowed to be 1, in that case use Entropy.sd instead.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014a). Entropic representation and estimation of diversity indices. http://arxiv.org/abs/1403.3031.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = c(1,3,7,4,8)
 Tsallis.sd(x,2)  
 
 data = rbinom(10,20,.5)
 counts = tabulate(as.factor(data))
 Tsallis.sd(counts,2)
</code></pre>

<hr>
<h2 id='Tsallis.z'>Tsallis.z</h2><span id='topic+Tsallis.z'></span>

<h3>Description</h3>

<p>Returns the Z estimator of Tsallis entropy. This is based on scaling and shifting the Z estimator of Renyi's equivalent entropy. When r=1 returns the Z estimator of Shannon's entropy. See Zhang and Grabchak (2014a) for details.</p>


<h3>Usage</h3>

<pre><code class='language-R'>Tsallis.z(x, r)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Tsallis.z_+3A_x">x</code></td>
<td>

<p>Vector of counts. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td></tr>
<tr><td><code id="Tsallis.z_+3A_r">r</code></td>
<td>
<p>Order or Renyi's equivalent entropy this index is based on. Must be a strictly positive real number.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014a). Entropic representation and estimation of diversity indices. http://arxiv.org/abs/1403.3031.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> x = c(1,3,7,4,8)
 Tsallis.z(x,2)  
 
 data = rbinom(10,20,.5)
 counts = tabulate(as.factor(data))
 Tsallis.z(counts,2)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
