<!DOCTYPE html><html><head><title>Help for package bst</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {bst}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bfunc'><p> Compute upper bound of second derivative of loss</p></a></li>
<li><a href='#bst'><p> Boosting for Classification and Regression</p></a></li>
<li><a href='#bst_control'><p> Control Parameters for Boosting</p></a></li>
<li><a href='#bst.sel'><p>Function to select number of predictors</p></a></li>
<li><a href='#cv.bst'><p> Cross-Validation for Boosting</p></a></li>
<li><a href='#cv.mada'><p> Cross-Validation for one-vs-all AdaBoost with multi-class problem</p></a></li>
<li><a href='#cv.mbst'><p> Cross-Validation for Multi-class Boosting</p></a></li>
<li><a href='#cv.mhingebst'><p> Cross-Validation for Multi-class Hinge Boosting</p></a></li>
<li><a href='#cv.mhingeova'><p> Cross-Validation for one-vs-all HingeBoost with multi-class problem</p></a></li>
<li><a href='#cv.rbst'><p> Cross-Validation for Nonconvex Loss Boosting</p></a></li>
<li><a href='#cv.rmbst'><p> Cross-Validation for Nonconvex Multi-class Loss Boosting</p></a></li>
<li><a href='#evalerr'><p> Compute prediction errors</p></a></li>
<li><a href='#ex1data'><p> Generating Three-class Data with 50 Predictors</p></a></li>
<li><a href='#loss'><p>Internal Function</p></a></li>
<li><a href='#mada'><p> Multi-class AdaBoost</p></a></li>
<li><a href='#mbst'><p> Boosting for Multi-Classification</p></a></li>
<li><a href='#mhingebst'><p> Boosting for Multi-class Classification</p></a></li>
<li><a href='#mhingeova'><p> Multi-class HingeBoost</p></a></li>
<li><a href='#nsel'><p> Find Number of Variables In Multi-class Boosting Iterations</p></a></li>
<li><a href='#rbst'><p> Robust Boosting for Robust Loss Functions</p></a></li>
<li><a href='#rbstpath'><p> Robust Boosting Path for Nonconvex Loss Functions</p></a></li>
<li><a href='#rmbst'><p> Robust Boosting for Multi-class Robust Loss Functions</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Gradient Boosting</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3-24</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-12-20</td>
</tr>
<tr>
<td>Author:</td>
<td>Zhu Wang <a href="https://orcid.org/0000-0002-0773-0052"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Torsten Hothorn [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Zhu Wang &lt;zwang145@uthsc.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Functional gradient descent algorithm for a variety of convex and non-convex loss functions, for both classical and robust regression and classification problems. See Wang (2011) &lt;<a href="https://doi.org/10.2202%2F1557-4679.1304">doi:10.2202/1557-4679.1304</a>&gt;, Wang (2012) &lt;<a href="https://doi.org/10.3414%2FME11-02-0020">doi:10.3414/ME11-02-0020</a>&gt;, Wang (2018) &lt;<a href="https://doi.org/10.1080%2F10618600.2018.1424635">doi:10.1080/10618600.2018.1424635</a>&gt;, Wang (2018) &lt;<a href="https://doi.org/10.1214%2F18-EJS1404">doi:10.1214/18-EJS1404</a>&gt;.</td>
</tr>
<tr>
<td>Imports:</td>
<td>rpart, methods, foreach, doParallel, gbm</td>
</tr>
<tr>
<td>Suggests:</td>
<td>hdi, pROC, R.rsp, knitr, gdata</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>R.rsp, knitr</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-12-21 19:48:28 UTC; zhu</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-01-06 18:50:56 UTC</td>
</tr>
</table>
<hr>
<h2 id='bfunc'> Compute upper bound of second derivative of loss</h2><span id='topic+bfunc'></span>

<h3>Description</h3>

<p>Compute upper bound of second derivative of loss.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bfunc(family, s)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bfunc_+3A_family">family</code></td>
<td>
<p> a family from &quot;closs&quot;, &quot;gloss&quot;, &quot;qloss&quot; for classification and &quot;clossR&quot; for robust regression. </p>
</td></tr>
<tr><td><code id="bfunc_+3A_s">s</code></td>
<td>
<p> a parameter related to robustness. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>A finite upper bound is required in quadratic majorization.
</p>


<h3>Value</h3>

<p>A positive number.
</p>


<h3>Author(s)</h3>

<p> Zhu Wang </p>

<hr>
<h2 id='bst'> Boosting for Classification and Regression</h2><span id='topic+bst'></span><span id='topic+print.bst'></span><span id='topic+predict.bst'></span><span id='topic+plot.bst'></span><span id='topic+coef.bst'></span><span id='topic+fpartial.bst'></span>

<h3>Description</h3>

<p>Gradient boosting for optimizing loss functions with componentwise
linear, smoothing splines, tree models as base learners.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bst(x, y, cost = 0.5, family = c("gaussian", "hinge", "hinge2", "binom", "expo", 
"poisson", "tgaussianDC", "thingeDC", "tbinomDC", "binomdDC", "texpoDC", "tpoissonDC",
 "huber", "thuberDC", "clossR", "clossRMM", "closs", "gloss", "qloss", "clossMM",
"glossMM", "qlossMM", "lar"), ctrl = bst_control(), control.tree = list(maxdepth = 1), 
learner = c("ls", "sm", "tree"))
## S3 method for class 'bst'
print(x, ...)
## S3 method for class 'bst'
predict(object, newdata=NULL, newy=NULL, mstop=NULL, 
type=c("response", "all.res", "class", "loss", "error"), ...)
## S3 method for class 'bst'
plot(x, type = c("step", "norm"),...)
## S3 method for class 'bst'
coef(object, which=object$ctrl$mstop, ...)
## S3 method for class 'bst'
fpartial(object, mstop=NULL, newdata=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bst_+3A_x">x</code></td>
<td>
<p> a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="bst_+3A_y">y</code></td>
<td>
<p> vector of responses. <code>y</code> must be in {1, -1} for <code>family</code> = &quot;hinge&quot;.</p>
</td></tr>
<tr><td><code id="bst_+3A_cost">cost</code></td>
<td>
<p> price to pay for false positive, 0 &lt; <code>cost</code> &lt; 1; price of false negative is 1-<code>cost</code>.</p>
</td></tr>
<tr><td><code id="bst_+3A_family">family</code></td>
<td>
<p> A variety of loss functions.
<code>family</code> = &quot;hinge&quot; for hinge loss and <code>family</code>=&quot;gaussian&quot; for squared error loss. 
Implementing the negative gradient corresponding
to the loss function to be minimized. For hinge loss, +1/-1 binary responses is used.</p>
</td></tr>
<tr><td><code id="bst_+3A_ctrl">ctrl</code></td>
<td>
<p> an object of class <code><a href="#topic+bst_control">bst_control</a></code>.</p>
</td></tr>
<tr><td><code id="bst_+3A_type">type</code></td>
<td>
<p> type of prediction or plot, see <code><a href="stats.html#topic+predict">predict</a></code>, <code><a href="graphics.html#topic+plot">plot</a></code></p>
</td></tr>
<tr><td><code id="bst_+3A_control.tree">control.tree</code></td>
<td>
<p> control parameters of rpart. </p>
</td></tr>
<tr><td><code id="bst_+3A_learner">learner</code></td>
<td>
<p> a character specifying the component-wise base learner to be used:
<code>ls</code> linear models, 
<code>sm</code> smoothing splines,
<code>tree</code> regression trees.
</p>
</td></tr>
<tr><td><code id="bst_+3A_object">object</code></td>
<td>
<p> class of <code><a href="#topic+bst">bst</a></code>. </p>
</td></tr>
<tr><td><code id="bst_+3A_newdata">newdata</code></td>
<td>
<p> new data for prediction with the same number of columns as <code>x</code>. </p>
</td></tr>
<tr><td><code id="bst_+3A_newy">newy</code></td>
<td>
<p> new response. </p>
</td></tr>
<tr><td><code id="bst_+3A_mstop">mstop</code></td>
<td>
<p> boosting iteration for prediction. </p>
</td></tr>
<tr><td><code id="bst_+3A_which">which</code></td>
<td>
<p> at which boosting <code>mstop</code> to extract coefficients. </p>
</td></tr>
<tr><td><code id="bst_+3A_...">...</code></td>
<td>
<p> additional arguments. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Boosting algorithms for classification and regression problems. In a classification problem, suppose <code class="reqn">f</code> is a classifier for a response <code class="reqn">y</code>. A cost-sensitive or weighted loss function is </p>
<p style="text-align: center;"><code class="reqn">L(y,f,cost)=l(y,f,cost)\max(0, (1-yf))</code>
</p>
<p> For <code>family="hinge"</code>, 
</p>
<p style="text-align: center;"><code class="reqn">l(y,f,cost)= 
1-cost, if \, y= +1;
\quad cost, if \, y= -1</code>
</p>
<p> For <code>family="hinge2"</code>, 
l(y,f,cost)= 1, if y = +1 and f &gt; 0 ; = 1-cost, if y = +1 and f &lt; 0; = cost, if y = -1 and f &gt; 0; = 1, if y = -1 and f &lt; 0. 
</p>
<p>For twin boosting if <code>twinboost=TRUE</code>, there are two types of adaptive boosting if <code>learner="ls"</code>: for <code>twintype=1</code>, weights are based on coefficients in the first round of boosting; for <code>twintype=2</code>, weights are based on predictions in the first round of boosting. See Buehlmann and Hothorn (2010).
</p>


<h3>Value</h3>

<p>An object of class <code>bst</code> with <code><a href="base.html#topic+print">print</a></code>, <code><a href="stats.html#topic+coef">coef</a></code>,
<code><a href="graphics.html#topic+plot">plot</a></code> and <code><a href="stats.html#topic+predict">predict</a></code> methods are available for linear models.
For nonlinear models, methods <code><a href="base.html#topic+print">print</a></code> and <code><a href="stats.html#topic+predict">predict</a></code> are available.
</p>
<table>
<tr><td><code>x</code>, <code>y</code>, <code>cost</code>, <code>family</code>, <code>learner</code>, <code>control.tree</code>, <code>maxdepth</code></td>
<td>
<p>These are input variables and parameters</p>
</td></tr>
<tr><td><code>ctrl</code></td>
<td>
<p>the input <code>ctrl</code> with possible updated <code>fk</code> if <code>family="thingeDC", "tbinomDC", "binomdDC"</code></p>
</td></tr>
<tr><td><code>yhat</code></td>
<td>
<p>predicted function estimates</p>
</td></tr>
<tr><td><code>ens</code></td>
<td>
<p>a list of length <code>mstop</code>. Each element is a fitted model to the pseudo residuals, defined as negative gradient of loss function at the current estimated function</p>
</td></tr>
<tr><td><code>ml.fit</code></td>
<td>
<p>the last element of <code>ens</code></p>
</td></tr>
<tr><td><code>ensemble</code></td>
<td>
<p>a vector of length <code>mstop</code>. Each element is the variable selected in each boosting step when applicable</p>
</td></tr>
<tr><td><code>xselect</code></td>
<td>
<p>selected variables in <code>mstop</code></p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>estimated coefficients in each iteration. Used internally only</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Zhu Wang </p>


<h3>References</h3>

<p>Zhu Wang (2011),
HingeBoost: ROC-Based Boost for Classification and Variable Selection.
<em>The International Journal of Biostatistics</em>, <b>7</b>(1), Article 13.
</p>
<p>Peter Buehlmann and Torsten Hothorn (2010),
Twin Boosting: improved feature selection and prediction,
<em>Statistics and Computing</em>, <b>20</b>, 119-138.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.bst">cv.bst</a></code> for cross-validated stopping iteration. Furthermore see
<code><a href="#topic+bst_control">bst_control</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(rnorm(100*5),ncol=5)
c &lt;- 2*x[,1]
p &lt;- exp(c)/(exp(c)+exp(-c))
y &lt;- rbinom(100,1,p)
y[y != 1] &lt;- -1
x &lt;- as.data.frame(x)
dat.m &lt;- bst(x, y, ctrl = bst_control(mstop=50), family = "hinge", learner = "ls")
predict(dat.m)
dat.m1 &lt;- bst(x, y, ctrl = bst_control(twinboost=TRUE, 
coefir=coef(dat.m), xselect.init = dat.m$xselect, mstop=50))
dat.m2 &lt;- rbst(x, y, ctrl = bst_control(mstop=50, s=0, trace=TRUE), 
rfamily = "thinge", learner = "ls")
predict(dat.m2)
</code></pre>

<hr>
<h2 id='bst_control'> Control Parameters for Boosting </h2><span id='topic+bst_control'></span>

<h3>Description</h3>

<p>Specification of the number of boosting iterations, step size
and other parameters for boosting algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bst_control(mstop = 50, nu = 0.1, twinboost = FALSE, twintype=1, threshold=c("standard", 
"adaptive"), f.init = NULL, coefir = NULL, xselect.init = NULL, center = FALSE, 
trace = FALSE, numsample = 50, df = 4, s = NULL, sh = NULL, q = NULL, qh = NULL, 
fk = NULL, start=FALSE, iter = 10, intercept = FALSE, trun=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bst_control_+3A_mstop">mstop</code></td>
<td>
<p> an integer giving the number of boosting iterations. </p>
</td></tr>
<tr><td><code id="bst_control_+3A_nu">nu</code></td>
<td>
<p> a small number (between 0 and 1) defining the step size or shrinkage parameter. </p>
</td></tr>
<tr><td><code id="bst_control_+3A_twinboost">twinboost</code></td>
<td>
<p> a logical value: <code>TRUE</code> for twin boosting. </p>
</td></tr>
<tr><td><code id="bst_control_+3A_twintype">twintype</code></td>
<td>
<p> for <code>twinboost=TRUE</code> only. For <code>learner="ls"</code>, if <code>twintype=1</code>, twin boosting with weights from magnitude of coefficients in the first round of boosting. If <code>twintype=2</code>, weights are correlations between predicted values in the first round of boosting and current predicted values. For learners not componentwise least squares, <code>twintype=2</code>. </p>
</td></tr>
<tr><td><code id="bst_control_+3A_threshold">threshold</code></td>
<td>
<p> if <code>threshold="adaptive"</code>, the estimated function <code>ctrl$fk</code> is updated in every boosting step. Otherwise, no update for <code>ctrl$fk</code> in boosting steps. Only used in robust nonconvex loss function. </p>
</td></tr>
<tr><td><code id="bst_control_+3A_f.init">f.init</code></td>
<td>
<p> the estimate from the first round of twin boosting. Only useful when <code>twinboost=TRUE</code> and <code>learner="sm" or "tree"</code>. </p>
</td></tr>
<tr><td><code id="bst_control_+3A_coefir">coefir</code></td>
<td>
<p> the estimated coefficients from the first round of twin boosting. Only useful when <code>twinboost=TRUE</code> and <code>learner="ls"</code>. </p>
</td></tr>
<tr><td><code id="bst_control_+3A_xselect.init">xselect.init</code></td>
<td>
<p> the variable selected from the first round of twin boosting. Only useful when <code>twinboost=TRUE</code>. </p>
</td></tr>
<tr><td><code id="bst_control_+3A_center">center</code></td>
<td>
<p> a logical value: <code>TRUE</code> to center covariates with mean. </p>
</td></tr>
<tr><td><code id="bst_control_+3A_trace">trace</code></td>
<td>
<p> a logical value for printout of more details of information during
the fitting process. </p>
</td></tr>
<tr><td><code id="bst_control_+3A_numsample">numsample</code></td>
<td>
<p> number of random sample variable selected in the first round of twin boosting. This is potentially useful in the future implementation. </p>
</td></tr>
<tr><td><code id="bst_control_+3A_df">df</code></td>
<td>
<p> degree of freedom used in smoothing splines. </p>
</td></tr>
<tr><td><code id="bst_control_+3A_s">s</code>, <code id="bst_control_+3A_q">q</code></td>
<td>
<p> nonconvex loss tuning parameter <code>s</code> or frequency <code>q</code> of outliers for robust regression and classification. If <code>s</code> is missing but <code>q</code> is available, <code>s</code> may be computed as the <code>1-q</code> quantile of robust loss values using conventional software.</p>
</td></tr>
<tr><td><code id="bst_control_+3A_sh">sh</code>, <code id="bst_control_+3A_qh">qh</code></td>
<td>
<p> threshold value or frequency <code>qh</code> of outliers for Huber regression <code>family="huber"</code> or <code>family="rhuberDC"</code>.
For <code>family="huber"</code>, if <code>sh</code> is not provided, <code>sh</code> is then updated adaptively with the median of <code>y-yhat</code> where <code>yhat</code> is the estimated <code>y</code> in the last boosting iteration. For <code>family="rhuberDC"</code>, if <code>sh</code> is missing but <code>qh</code> is available, <code>sh</code> may be computed as the <code>1-qh</code> quantile of robust loss values using conventional software.</p>
</td></tr>
<tr><td><code id="bst_control_+3A_fk">fk</code></td>
<td>
<p> predicted values at an iteration in the MM algorithm </p>
</td></tr>
<tr><td><code id="bst_control_+3A_start">start</code></td>
<td>
<p> a logical value, if <code>start=TRUE</code> and <code>fk</code> is a vector of values, then <code>bst</code> iterations begin with <code>fk</code>. Otherwise, <code>bst</code> iterations begin with the default values. This can be useful, for instance, in <code>rbst</code> for the MM boosting algorithm. </p>
</td></tr>
<tr><td><code id="bst_control_+3A_iter">iter</code></td>
<td>
<p>number of iteration in the MM algorithm</p>
</td></tr>
<tr><td><code id="bst_control_+3A_intercept">intercept</code></td>
<td>
<p> logical value, if TRUE, estimation of intercept with linear predictor model</p>
</td></tr>
<tr><td><code id="bst_control_+3A_trun">trun</code></td>
<td>
<p>logical value, if TRUE, predicted value in each boosting iteration is truncated at -1, 1, for <code>family="closs"</code> in <code>bst</code> and <code>rfamily="closs"</code> in <code>rbst</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Objects to specify parameters of the boosting algorithms implemented in <code><a href="#topic+bst">bst</a></code>, via the <code>ctrl</code> argument.  
The <code>s</code> value is for robust nonconvex loss where smaller <code>s</code> value is more robust to outliers with <code>family="closs", "tbinom", "thinge", "tbinomd"</code>, and larger <code>s</code> value more robust with <code>family="clossR", "gloss", "qloss"</code>.
</p>
<p>For <code>family="closs"</code>, if <code>s=2</code>, the loss is similar to the square loss; if <code>s=1</code>, the loss function is an approximation of the hinge loss; for smaller values, the loss function approaches the 0-1 loss function if <code>s&lt;1</code>, the loss function is a nonconvex function of the margin.
</p>
<p>The default value of <code>s</code> is -1 if <code>family="thinge"</code>, -log(3) if <code>family="tbinom"</code>, and 4 if <code>family="binomd"</code>. If <code>trun=TRUE</code>, boosting classifiers can produce real values in [-1, 1] indicating their confidence in [-1, 1]-valued classification. cf. R. E. Schapire and Y. Singer. Improved boosting algorithms using confidence-rated predictions. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, pages 80-91, 1998.
</p>


<h3>Value</h3>

<p>An object of class <code>bst_control</code>, a list. Note <code>fk</code> may be updated for robust boosting.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+bst">bst</a></code></p>

<hr>
<h2 id='bst.sel'>Function to select number of predictors</h2><span id='topic+bst.sel'></span>

<h3>Description</h3>

<p>Function to determine the first q predictors in the boosting path, or perform (10-fold) cross-validation and determine the optimal set of parameters</p>


<h3>Usage</h3>

<pre><code class='language-R'>bst.sel(x, y, q, type=c("firstq", "cv"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bst.sel_+3A_x">x</code></td>
<td>
<p>Design matrix (without intercept).</p>
</td></tr>
<tr><td><code id="bst.sel_+3A_y">y</code></td>
<td>
<p>Continuous response vector for linear regression</p>
</td></tr>
<tr><td><code id="bst.sel_+3A_q">q</code></td>
<td>
<p>Maximum number of predictors that should be selected if <code>type="firstq"</code>.</p>
</td></tr>
<tr><td><code id="bst.sel_+3A_type">type</code></td>
<td>
<p>if <code>type="firstq"</code>, return the first <code>q</code> predictors in the boosting path. if <code>type="cv"</code>, perform (10-fold) cross-validation and determine the optimal set of parameters</p>
</td></tr>
<tr><td><code id="bst.sel_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code><a href="#topic+bst">bst</a></code>, <code><a href="#topic+cv.bst">cv.bst</a>.</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function to determine the first q predictors in the boosting path, or perform (10-fold) cross-validation and determine the optimal set of parameters. This may be used for p-value calculation. See below.</p>


<h3>Value</h3>

<p>Vector of selected predictors.</p>


<h3>Author(s)</h3>

<p>Zhu Wang</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
x &lt;- matrix(rnorm(100*100), nrow = 100, ncol = 100)
y &lt;- x[,1] * 2 + x[,2] * 2.5 + rnorm(100)
sel &lt;- bst.sel(x, y, q=10)
library("hdi")
fit.multi &lt;- hdi(x, y, method = "multi.split",
model.selector =bst.sel,
args.model.selector=list(type="firstq", q=10))
fit.multi
fit.multi$pval[1:10] ## the first 10 p-values
fit.multi &lt;- hdi(x, y, method = "multi.split",
model.selector =bst.sel,
args.model.selector=list(type="cv"))
fit.multi
fit.multi$pval[1:10] ## the first 10 p-values

## End(Not run)
</code></pre>

<hr>
<h2 id='cv.bst'> Cross-Validation for Boosting</h2><span id='topic+cv.bst'></span>

<h3>Description</h3>

<p>Cross-validated estimation of the empirical risk/error
for boosting parameter selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.bst(x,y,K=10,cost=0.5,family=c("gaussian", "hinge", "hinge2", "binom", "expo", 
"poisson", "tgaussianDC", "thingeDC", "tbinomDC", "binomdDC", "texpoDC", "tpoissonDC", 
"clossR", "closs", "gloss", "qloss", "lar"), learner = c("ls", "sm", "tree"), 
ctrl = bst_control(), type = c("loss", "error"), 
plot.it = TRUE, main = NULL, se = TRUE, n.cores=2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.bst_+3A_x">x</code></td>
<td>
<p> a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="cv.bst_+3A_y">y</code></td>
<td>
<p> vector of responses. <code>y</code> must be in {1, -1} for binary classifications. </p>
</td></tr>
<tr><td><code id="cv.bst_+3A_k">K</code></td>
<td>
<p> K-fold cross-validation </p>
</td></tr>
<tr><td><code id="cv.bst_+3A_cost">cost</code></td>
<td>
<p> price to pay for false positive, 0 &lt; <code>cost</code> &lt; 1; price of false negative is 1-<code>cost</code>.</p>
</td></tr>
<tr><td><code id="cv.bst_+3A_family">family</code></td>
<td>
 <p><code>family</code> = &quot;hinge&quot; for hinge loss and <code>family</code>=&quot;gaussian&quot; for squared error loss. </p>
</td></tr>
<tr><td><code id="cv.bst_+3A_learner">learner</code></td>
<td>
<p> a character specifying the component-wise base learner to be used:
<code>ls</code> linear models,
<code>sm</code> smoothing splines,
<code>tree</code> regression trees.
</p>
</td></tr>
<tr><td><code id="cv.bst_+3A_ctrl">ctrl</code></td>
<td>
<p> an object of class <code><a href="#topic+bst_control">bst_control</a></code>.</p>
</td></tr>
<tr><td><code id="cv.bst_+3A_type">type</code></td>
<td>
<p> cross-validation criteria. For <code>type="loss"</code>, loss function values and <code>type="error"</code> is misclassification error. </p>
</td></tr>
<tr><td><code id="cv.bst_+3A_plot.it">plot.it</code></td>
<td>
<p> a logical value, to plot the estimated loss or error with cross validation if <code>TRUE</code>. </p>
</td></tr>
<tr><td><code id="cv.bst_+3A_main">main</code></td>
<td>
<p> title of plot </p>
</td></tr>
<tr><td><code id="cv.bst_+3A_se">se</code></td>
<td>
<p> a logical value, to plot with standard errors. </p>
</td></tr>
<tr><td><code id="cv.bst_+3A_n.cores">n.cores</code></td>
<td>
<p>The number of CPU cores to use. The cross-validation loop
will attempt to send different CV folds off to different cores.</p>
</td></tr>
<tr><td><code id="cv.bst_+3A_...">...</code></td>
<td>
<p> additional arguments. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>object with
</p>
<table>
<tr><td><code>residmat</code></td>
<td>
<p> empirical risks in each cross-validation at boosting iterations </p>
</td></tr>
<tr><td><code>mstop</code></td>
<td>
<p> boosting iteration steps at which CV curve should be computed. </p>
</td></tr>
<tr><td><code>cv</code></td>
<td>
<p>The CV curve at each value of mstop</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>The standard error of the CV curve</p>
</td></tr>
<tr><td><code>family</code></td>
<td>
<p> loss function types</p>
</td></tr>
</table>
<p>...
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+bst">bst</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
x &lt;- matrix(rnorm(100*5),ncol=5)
c &lt;- 2*x[,1]
p &lt;- exp(c)/(exp(c)+exp(-c))
y &lt;- rbinom(100,1,p)
y[y != 1] &lt;- -1
x &lt;- as.data.frame(x)
cv.bst(x, y, ctrl = bst_control(mstop=50), family = "hinge", learner = "ls", type="loss")
cv.bst(x, y, ctrl = bst_control(mstop=50), family = "hinge", learner = "ls", type="error")
dat.m &lt;- bst(x, y, ctrl = bst_control(mstop=50), family = "hinge", learner = "ls")
dat.m1 &lt;- cv.bst(x, y, ctrl = bst_control(twinboost=TRUE, coefir=coef(dat.m), 
xselect.init = dat.m$xselect, mstop=50), family = "hinge", learner="ls")

## End(Not run)
</code></pre>

<hr>
<h2 id='cv.mada'> Cross-Validation for one-vs-all AdaBoost with multi-class problem </h2><span id='topic+cv.mada'></span>

<h3>Description</h3>

<p>Cross-validated estimation of the empirical misclassification error for boosting parameter selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.mada(x, y, balance=FALSE, K=10, nu=0.1, mstop=200, interaction.depth=1, 
trace=FALSE, plot.it = TRUE, se = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.mada_+3A_x">x</code></td>
<td>
<p> a data matrix containing the variables in the model.</p>
</td></tr>
<tr><td><code id="cv.mada_+3A_y">y</code></td>
<td>
<p> vector of multi class responses. <code>y</code> must be an integer vector from 1 to C for C class problem.</p>
</td></tr>
<tr><td><code id="cv.mada_+3A_balance">balance</code></td>
<td>
<p> logical value. If TRUE, The K parts were roughly balanced, ensuring that the classes were distributed proportionally among each of the K parts.</p>
</td></tr>
<tr><td><code id="cv.mada_+3A_k">K</code></td>
<td>
<p> K-fold cross-validation </p>
</td></tr>
<tr><td><code id="cv.mada_+3A_nu">nu</code></td>
<td>
<p> a small number (between 0 and 1) defining the step size or shrinkage parameter. </p>
</td></tr>
<tr><td><code id="cv.mada_+3A_mstop">mstop</code></td>
<td>
<p> number of boosting iteration. </p>
</td></tr>
<tr><td><code id="cv.mada_+3A_interaction.depth">interaction.depth</code></td>
<td>
<p> used in gbm to specify the depth of trees. </p>
</td></tr>
<tr><td><code id="cv.mada_+3A_trace">trace</code></td>
<td>
<p> if TRUE, iteration results printed out. </p>
</td></tr>
<tr><td><code id="cv.mada_+3A_plot.it">plot.it</code></td>
<td>
<p> a logical value, to plot the cross-validation error if <code>TRUE</code>. </p>
</td></tr>
<tr><td><code id="cv.mada_+3A_se">se</code></td>
<td>
<p> a logical value, to plot with 1 standard deviation curves. </p>
</td></tr>
<tr><td><code id="cv.mada_+3A_...">...</code></td>
<td>
<p> additional arguments. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>object with
</p>
<table>
<tr><td><code>residmat</code></td>
<td>
<p> empirical risks in each cross-validation at boosting iterations </p>
</td></tr>
<tr><td><code>fraction</code></td>
<td>
<p> abscissa values at which CV curve should be computed. </p>
</td></tr>
<tr><td><code>cv</code></td>
<td>
<p>The CV curve at each value of fraction</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>The standard error of the CV curve</p>
</td></tr>
</table>
<p>...
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+mada">mada</a></code> </p>

<hr>
<h2 id='cv.mbst'> Cross-Validation for Multi-class Boosting</h2><span id='topic+cv.mbst'></span>

<h3>Description</h3>

<p>Cross-validated estimation of the empirical multi-class loss
for boosting parameter selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.mbst(x, y, balance=FALSE, K = 10, cost = NULL, 
family = c("hinge","hinge2","thingeDC", "closs", "clossMM"), 
learner = c("tree", "ls", "sm"), ctrl = bst_control(), 
type = c("loss","error"), plot.it = TRUE, se = TRUE, n.cores=2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.mbst_+3A_x">x</code></td>
<td>
<p> a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="cv.mbst_+3A_y">y</code></td>
<td>
<p> vector of responses. <code>y</code> must be integers from 1 to C for C class problem. </p>
</td></tr>
<tr><td><code id="cv.mbst_+3A_balance">balance</code></td>
<td>
<p> logical value. If TRUE, The K
parts were roughly balanced, ensuring that the classes were distributed
proportionally among each of the K parts.</p>
</td></tr>
<tr><td><code id="cv.mbst_+3A_k">K</code></td>
<td>
<p> K-fold cross-validation </p>
</td></tr>
<tr><td><code id="cv.mbst_+3A_cost">cost</code></td>
<td>
<p> price to pay for false positive, 0 &lt; <code>cost</code> &lt; 1; price of false negative is 1-<code>cost</code>.</p>
</td></tr>
<tr><td><code id="cv.mbst_+3A_family">family</code></td>
<td>
 <p><code>family</code> = &quot;hinge&quot; for hinge loss. &quot;hinge2&quot; is a different hinge loss</p>
</td></tr>
<tr><td><code id="cv.mbst_+3A_learner">learner</code></td>
<td>
<p> a character specifying the component-wise base learner to be used:
<code>ls</code> linear models,
<code>sm</code> smoothing splines,
<code>tree</code> regression trees.
</p>
</td></tr>
<tr><td><code id="cv.mbst_+3A_ctrl">ctrl</code></td>
<td>
<p> an object of class <code><a href="#topic+bst_control">bst_control</a></code>.</p>
</td></tr>
<tr><td><code id="cv.mbst_+3A_type">type</code></td>
<td>
<p> for <code>family="hinge"</code>, <code>type="loss"</code> is hinge risk. For <code>family="thingeDC"</code>, <code>type="loss"</code></p>
</td></tr>
<tr><td><code id="cv.mbst_+3A_plot.it">plot.it</code></td>
<td>
<p> a logical value, to plot the estimated risks if <code>TRUE</code>. </p>
</td></tr>
<tr><td><code id="cv.mbst_+3A_se">se</code></td>
<td>
<p> a logical value, to plot with standard errors. </p>
</td></tr>
<tr><td><code id="cv.mbst_+3A_n.cores">n.cores</code></td>
<td>
<p>The number of CPU cores to use. The cross-validation loop
will attempt to send different CV folds off to different cores.</p>
</td></tr>
<tr><td><code id="cv.mbst_+3A_...">...</code></td>
<td>
<p> additional arguments. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>object with
</p>
<table>
<tr><td><code>residmat</code></td>
<td>
<p> empirical risks in each cross-validation at boosting iterations </p>
</td></tr>
<tr><td><code>fraction</code></td>
<td>
<p> abscissa values at which CV curve should be computed. </p>
</td></tr>
<tr><td><code>cv</code></td>
<td>
<p>The CV curve at each value of fraction</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>The standard error of the CV curve</p>
</td></tr>
</table>
<p>...
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+mbst">mbst</a></code> </p>

<hr>
<h2 id='cv.mhingebst'> Cross-Validation for Multi-class Hinge Boosting</h2><span id='topic+cv.mhingebst'></span>

<h3>Description</h3>

<p>Cross-validated estimation of the empirical multi-class hinge loss
for boosting parameter selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.mhingebst(x, y, balance=FALSE, K = 10, cost = NULL, family = "hinge", 
learner = c("tree", "ls", "sm"), ctrl = bst_control(), 
type = c("loss","error"), plot.it = TRUE, main = NULL, se = TRUE, n.cores=2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.mhingebst_+3A_x">x</code></td>
<td>
<p> a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="cv.mhingebst_+3A_y">y</code></td>
<td>
<p> vector of responses. <code>y</code> must be integers from 1 to C for C class problem. </p>
</td></tr>
<tr><td><code id="cv.mhingebst_+3A_balance">balance</code></td>
<td>
<p> logical value. If TRUE, The K
parts were roughly balanced, ensuring that the classes were distributed
proportionally among each of the K parts.</p>
</td></tr>
<tr><td><code id="cv.mhingebst_+3A_k">K</code></td>
<td>
<p> K-fold cross-validation </p>
</td></tr>
<tr><td><code id="cv.mhingebst_+3A_cost">cost</code></td>
<td>
<p> price to pay for false positive, 0 &lt; <code>cost</code> &lt; 1; price of false negative is 1-<code>cost</code>.</p>
</td></tr>
<tr><td><code id="cv.mhingebst_+3A_family">family</code></td>
<td>
 <p><code>family</code> = &quot;hinge&quot; for hinge loss.</p>
</td></tr>
</table>
<p>Implementing the negative gradient corresponding
to the loss function to be minimized. 
</p>
<table>
<tr><td><code id="cv.mhingebst_+3A_learner">learner</code></td>
<td>
<p> a character specifying the component-wise base learner to be used:
<code>ls</code> linear models,
<code>sm</code> smoothing splines,
<code>tree</code> regression trees.
</p>
</td></tr>
<tr><td><code id="cv.mhingebst_+3A_ctrl">ctrl</code></td>
<td>
<p> an object of class <code><a href="#topic+bst_control">bst_control</a></code>.</p>
</td></tr>
<tr><td><code id="cv.mhingebst_+3A_type">type</code></td>
<td>
<p> for <code>family="hinge"</code>, <code>type="loss"</code> is hinge risk.</p>
</td></tr>
<tr><td><code id="cv.mhingebst_+3A_plot.it">plot.it</code></td>
<td>
<p> a logical value, to plot the estimated loss or error with cross validation if <code>TRUE</code>. </p>
</td></tr>
<tr><td><code id="cv.mhingebst_+3A_main">main</code></td>
<td>
<p> title of plot</p>
</td></tr>
<tr><td><code id="cv.mhingebst_+3A_se">se</code></td>
<td>
<p> a logical value, to plot with standard errors. </p>
</td></tr>
<tr><td><code id="cv.mhingebst_+3A_n.cores">n.cores</code></td>
<td>
<p>The number of CPU cores to use. The cross-validation loop
will attempt to send different CV folds off to different cores.</p>
</td></tr>
<tr><td><code id="cv.mhingebst_+3A_...">...</code></td>
<td>
<p> additional arguments. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>object with
</p>
<table>
<tr><td><code>residmat</code></td>
<td>
<p> empirical risks in each cross-validation at boosting iterations </p>
</td></tr>
<tr><td><code>fraction</code></td>
<td>
<p> abscissa values at which CV curve should be computed. </p>
</td></tr>
<tr><td><code>cv</code></td>
<td>
<p>The CV curve at each value of fraction</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>The standard error of the CV curve</p>
</td></tr>
</table>
<p>...
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+mhingebst">mhingebst</a></code> </p>

<hr>
<h2 id='cv.mhingeova'> Cross-Validation for one-vs-all HingeBoost with multi-class problem </h2><span id='topic+cv.mhingeova'></span>

<h3>Description</h3>

<p>Cross-validated estimation of the empirical misclassification error for boosting parameter selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.mhingeova(x, y, balance=FALSE, K=10, cost = NULL, nu=0.1, 
learner=c("tree", "ls", "sm"), maxdepth=1, m1=200, twinboost = FALSE, 
m2=200, trace=FALSE, plot.it = TRUE, se = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.mhingeova_+3A_x">x</code></td>
<td>
<p> a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="cv.mhingeova_+3A_y">y</code></td>
<td>
<p> vector of multi class responses. <code>y</code> must be an integer vector from 1 to C for C class problem. </p>
</td></tr>
<tr><td><code id="cv.mhingeova_+3A_balance">balance</code></td>
<td>
<p> logical value. If TRUE, The K
parts were roughly balanced, ensuring that the classes were distributed
proportionally among each of the K parts.</p>
</td></tr>
<tr><td><code id="cv.mhingeova_+3A_k">K</code></td>
<td>
<p> K-fold cross-validation </p>
</td></tr>
<tr><td><code id="cv.mhingeova_+3A_cost">cost</code></td>
<td>
<p> price to pay for false positive, 0 &lt; <code>cost</code> &lt; 1; price of false negative is 1-<code>cost</code>.</p>
</td></tr>
<tr><td><code id="cv.mhingeova_+3A_nu">nu</code></td>
<td>
<p> a small number (between 0 and 1) defining the step size or shrinkage parameter. </p>
</td></tr>
<tr><td><code id="cv.mhingeova_+3A_learner">learner</code></td>
<td>
<p> a character specifying the component-wise base learner to be used:
<code>ls</code> linear models,
<code>sm</code> smoothing splines,
<code>tree</code> regression trees.
</p>
</td></tr>
<tr><td><code id="cv.mhingeova_+3A_maxdepth">maxdepth</code></td>
<td>
<p> tree depth used in <code>learner=tree</code></p>
</td></tr>
<tr><td><code id="cv.mhingeova_+3A_m1">m1</code></td>
<td>
<p> number of boosting iteration </p>
</td></tr>
<tr><td><code id="cv.mhingeova_+3A_twinboost">twinboost</code></td>
<td>
<p> logical: twin boosting? </p>
</td></tr>
<tr><td><code id="cv.mhingeova_+3A_m2">m2</code></td>
<td>
<p> number of twin boosting iteration </p>
</td></tr>
<tr><td><code id="cv.mhingeova_+3A_trace">trace</code></td>
<td>
<p> if TRUE, iteration results printed out </p>
</td></tr>
<tr><td><code id="cv.mhingeova_+3A_plot.it">plot.it</code></td>
<td>
<p> a logical value, to plot the estimated risks if <code>TRUE</code>. </p>
</td></tr>
<tr><td><code id="cv.mhingeova_+3A_se">se</code></td>
<td>
<p> a logical value, to plot with standard errors. </p>
</td></tr>
<tr><td><code id="cv.mhingeova_+3A_...">...</code></td>
<td>
<p> additional arguments. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>object with
</p>
<table>
<tr><td><code>residmat</code></td>
<td>
<p> empirical risks in each cross-validation at boosting iterations </p>
</td></tr>
<tr><td><code>fraction</code></td>
<td>
<p> abscissa values at which CV curve should be computed. </p>
</td></tr>
<tr><td><code>cv</code></td>
<td>
<p>The CV curve at each value of fraction</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>The standard error of the CV curve</p>
</td></tr>
</table>
<p>...
</p>


<h3>Note</h3>

<p> The functions for balanced cross validation were from R package pmar. </p>


<h3>See Also</h3>

 <p><code><a href="#topic+mhingeova">mhingeova</a></code> </p>

<hr>
<h2 id='cv.rbst'> Cross-Validation for Nonconvex Loss Boosting</h2><span id='topic+cv.rbst'></span>

<h3>Description</h3>

<p>Cross-validated estimation of the empirical risk/error, can be used for tuning parameter selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.rbst(x, y, K = 10, cost = 0.5, rfamily = c("tgaussian", "thuber", "thinge", 
"tbinom", "binomd", "texpo", "tpoisson", "clossR", "closs", "gloss", "qloss"), 
learner = c("ls", "sm", "tree"), ctrl = bst_control(), type = c("loss", "error"), 
plot.it = TRUE, main = NULL, se = TRUE, n.cores=2,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.rbst_+3A_x">x</code></td>
<td>
<p> a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="cv.rbst_+3A_y">y</code></td>
<td>
<p> vector of responses. <code>y</code> must be in {1, -1} for binary classification</p>
</td></tr>
<tr><td><code id="cv.rbst_+3A_k">K</code></td>
<td>
<p> K-fold cross-validation </p>
</td></tr>
<tr><td><code id="cv.rbst_+3A_cost">cost</code></td>
<td>
<p> price to pay for false positive, 0 &lt; <code>cost</code> &lt; 1; price of false negative is 1-<code>cost</code>.</p>
</td></tr>
<tr><td><code id="cv.rbst_+3A_rfamily">rfamily</code></td>
<td>
<p> nonconvex loss function types. </p>
</td></tr>
<tr><td><code id="cv.rbst_+3A_learner">learner</code></td>
<td>
<p> a character specifying the component-wise base learner to be used:
<code>ls</code> linear models,
<code>sm</code> smoothing splines,
<code>tree</code> regression trees.
</p>
</td></tr>
<tr><td><code id="cv.rbst_+3A_ctrl">ctrl</code></td>
<td>
<p> an object of class <code><a href="#topic+bst_control">bst_control</a></code>.</p>
</td></tr>
<tr><td><code id="cv.rbst_+3A_type">type</code></td>
<td>
<p> cross-validation criteria. For <code>type="loss"</code>, loss function values and <code>type="error"</code> is misclassification error. </p>
</td></tr>
<tr><td><code id="cv.rbst_+3A_plot.it">plot.it</code></td>
<td>
<p> a logical value, to plot the estimated loss or error with cross validation if <code>TRUE</code>. </p>
</td></tr>
<tr><td><code id="cv.rbst_+3A_main">main</code></td>
<td>
<p> title of plot</p>
</td></tr>
<tr><td><code id="cv.rbst_+3A_se">se</code></td>
<td>
<p> a logical value, to plot with standard errors. </p>
</td></tr>
<tr><td><code id="cv.rbst_+3A_n.cores">n.cores</code></td>
<td>
<p>The number of CPU cores to use. The cross-validation loop
will attempt to send different CV folds off to different cores.</p>
</td></tr>
<tr><td><code id="cv.rbst_+3A_...">...</code></td>
<td>
<p> additional arguments. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>object with
</p>
<table>
<tr><td><code>residmat</code></td>
<td>
<p> empirical risks in each cross-validation at boosting iterations </p>
</td></tr>
<tr><td><code>mstop</code></td>
<td>
<p> boosting iteration steps at which CV curve should be computed. </p>
</td></tr>
<tr><td><code>cv</code></td>
<td>
<p>The CV curve at each value of mstop</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>The standard error of the CV curve</p>
</td></tr>
<tr><td><code>rfamily</code></td>
<td>
<p> nonconvex loss function types. </p>
</td></tr>
</table>
<p>...
</p>


<h3>Author(s)</h3>

<p> Zhu Wang </p>


<h3>See Also</h3>

 <p><code><a href="#topic+rbst">rbst</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
x &lt;- matrix(rnorm(100*5),ncol=5)
c &lt;- 2*x[,1]
p &lt;- exp(c)/(exp(c)+exp(-c))
y &lt;- rbinom(100,1,p)
y[y != 1] &lt;- -1
x &lt;- as.data.frame(x)
cv.rbst(x, y, ctrl = bst_control(mstop=50), rfamily = "thinge", learner = "ls", type="lose")
cv.rbst(x, y, ctrl = bst_control(mstop=50), rfamily = "thinge", learner = "ls", type="error")
dat.m &lt;- rbst(x, y, ctrl = bst_control(mstop=50), rfamily = "thinge", learner = "ls")
dat.m1 &lt;- cv.rbst(x, y, ctrl = bst_control(twinboost=TRUE, coefir=coef(dat.m), 
xselect.init = dat.m$xselect, mstop=50), family = "thinge", learner="ls")

## End(Not run)
</code></pre>

<hr>
<h2 id='cv.rmbst'> Cross-Validation for Nonconvex Multi-class Loss Boosting</h2><span id='topic+cv.rmbst'></span>

<h3>Description</h3>

<p>Cross-validated estimation of the empirical multi-class loss, can be used for tuning parameter selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.rmbst(x, y, balance=FALSE, K = 10, cost = NULL, rfamily = c("thinge", "closs"), 
learner = c("tree", "ls", "sm"), ctrl = bst_control(), type = c("loss","error"), 
plot.it = TRUE, main = NULL, se = TRUE, n.cores=2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.rmbst_+3A_x">x</code></td>
<td>
<p> a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="cv.rmbst_+3A_y">y</code></td>
<td>
<p> vector of responses. <code>y</code> must be integers from 1 to C for C class problem. </p>
</td></tr>
<tr><td><code id="cv.rmbst_+3A_balance">balance</code></td>
<td>
<p> logical value. If TRUE, The K
parts were roughly balanced, ensuring that the classes were distributed
proportionally among each of the K parts.</p>
</td></tr>
<tr><td><code id="cv.rmbst_+3A_k">K</code></td>
<td>
<p> K-fold cross-validation </p>
</td></tr>
<tr><td><code id="cv.rmbst_+3A_cost">cost</code></td>
<td>
<p> price to pay for false positive, 0 &lt; <code>cost</code> &lt; 1; price of false negative is 1-<code>cost</code>.</p>
</td></tr>
<tr><td><code id="cv.rmbst_+3A_rfamily">rfamily</code></td>
<td>
 <p><code>rfamily</code> = &quot;thinge&quot; for truncated multi-class hinge loss.</p>
</td></tr>
</table>
<p>Implementing the negative gradient corresponding
to the loss function to be minimized. 
</p>
<table>
<tr><td><code id="cv.rmbst_+3A_learner">learner</code></td>
<td>
<p> a character specifying the component-wise base learner to be used:
<code>ls</code> linear models,
<code>sm</code> smoothing splines,
<code>tree</code> regression trees.
</p>
</td></tr>
<tr><td><code id="cv.rmbst_+3A_ctrl">ctrl</code></td>
<td>
<p> an object of class <code><a href="#topic+bst_control">bst_control</a></code>.</p>
</td></tr>
<tr><td><code id="cv.rmbst_+3A_type">type</code></td>
<td>
<p> loss value or misclassification error. </p>
</td></tr>
<tr><td><code id="cv.rmbst_+3A_plot.it">plot.it</code></td>
<td>
<p> a logical value, to plot the estimated loss or error with cross validation if <code>TRUE</code>. </p>
</td></tr>
<tr><td><code id="cv.rmbst_+3A_main">main</code></td>
<td>
<p> title of plot</p>
</td></tr>
<tr><td><code id="cv.rmbst_+3A_se">se</code></td>
<td>
<p> a logical value, to plot with standard errors. </p>
</td></tr>
<tr><td><code id="cv.rmbst_+3A_n.cores">n.cores</code></td>
<td>
<p>The number of CPU cores to use. The cross-validation loop
will attempt to send different CV folds off to different cores.</p>
</td></tr>
<tr><td><code id="cv.rmbst_+3A_...">...</code></td>
<td>
<p> additional arguments. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>object with
</p>
<table>
<tr><td><code>residmat</code></td>
<td>
<p> empirical risks in each cross-validation at boosting iterations </p>
</td></tr>
<tr><td><code>fraction</code></td>
<td>
<p> abscissa values at which CV curve should be computed. </p>
</td></tr>
<tr><td><code>cv</code></td>
<td>
<p>The CV curve at each value of fraction</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>The standard error of the CV curve</p>
</td></tr>
</table>
<p>...
</p>


<h3>Author(s)</h3>

<p> Zhu Wang </p>


<h3>See Also</h3>

 <p><code><a href="#topic+rmbst">rmbst</a></code></p>

<hr>
<h2 id='evalerr'> Compute prediction errors </h2><span id='topic+evalerr'></span>

<h3>Description</h3>

<p>Compute prediction errors for classification and regression problems.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evalerr(family, y, yhat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="evalerr_+3A_family">family</code></td>
<td>
<p> a family used in <code>bst</code>. Classification or regression family. </p>
</td></tr>
<tr><td><code id="evalerr_+3A_y">y</code></td>
<td>
<p> response variable. For classification problems, y must be 1/-1. </p>
</td></tr>
<tr><td><code id="evalerr_+3A_yhat">yhat</code></td>
<td>
<p> predicted values. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>For classification, returns misclassification error. For regression, returns mean squared error. 
</p>


<h3>Value</h3>

<p>For classification, returns misclassification error. For regression, returns mean squared error. 
</p>


<h3>Author(s)</h3>

<p> Zhu Wang </p>

<hr>
<h2 id='ex1data'> Generating Three-class Data with 50 Predictors</h2><span id='topic+ex1data'></span>

<h3>Description</h3>

<p>Randomly generate data for a three-class model. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ex1data(n.data, p=50)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ex1data_+3A_n.data">n.data</code></td>
<td>
<p> number of data samples.</p>
</td></tr>
<tr><td><code id="ex1data_+3A_p">p</code></td>
<td>
<p> number of predictors. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data is generated based on Example 1 described in Wang (2012).
</p>


<h3>Value</h3>

<p>A list with n.data by p predictor matrix <code>x</code>, three-class response <code>y</code> and conditional probabilities.
</p>


<h3>Author(s)</h3>

<p> Zhu Wang </p>


<h3>References</h3>

<p>Zhu Wang (2012), Multi-class HingeBoost: Method and Application to the Classification of Cancer Types Using Gene Expression Data. <em>Methods of Information in Medicine</em>, <b>51</b>(2), 162&ndash;7.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dat &lt;- ex1data(100, p=5)
mhingebst(x=dat$x, y=dat$y)

## End(Not run)
</code></pre>

<hr>
<h2 id='loss'>Internal Function</h2><span id='topic+gaussloss'></span><span id='topic+gaussngra'></span><span id='topic+hingeloss'></span><span id='topic+hingengra'></span><span id='topic+loss'></span><span id='topic+gradient'></span><span id='topic+ngradient'></span><span id='topic+plotCVbst'></span><span id='topic+cvfolds'></span><span id='topic+error.bars'></span><span id='topic+loss.mhingebst'></span><span id='topic+loss.mbst'></span><span id='topic+mhingebst_fit'></span><span id='topic+mbst_fit'></span><span id='topic+balanced.folds'></span><span id='topic+permute.rows'></span>

<h3>Description</h3>

<p>Internal Function</p>

<hr>
<h2 id='mada'> Multi-class AdaBoost </h2><span id='topic+mada'></span>

<h3>Description</h3>

<p> One-vs-all multi-class AdaBoost </p>


<h3>Usage</h3>

<pre><code class='language-R'>mada(xtr, ytr, xte=NULL, yte=NULL, mstop=50, nu=0.1, interaction.depth=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mada_+3A_xtr">xtr</code></td>
<td>
<p> training data matrix containing the predictor variables in the model.</p>
</td></tr>
<tr><td><code id="mada_+3A_ytr">ytr</code></td>
<td>
<p> training vector of responses. <code>ytr</code> must be integers from 1 to C, for C class problem. </p>
</td></tr>
<tr><td><code id="mada_+3A_xte">xte</code></td>
<td>
<p> test data matrix containing the predictor variables in the model.</p>
</td></tr>
<tr><td><code id="mada_+3A_yte">yte</code></td>
<td>
<p> test vector of responses. <code>yte</code> must be integers from 1 to C, for C class problem. </p>
</td></tr>
<tr><td><code id="mada_+3A_mstop">mstop</code></td>
<td>
<p> number of boosting iteration. </p>
</td></tr>
<tr><td><code id="mada_+3A_nu">nu</code></td>
<td>
<p> a small number (between 0 and 1) defining the step size or shrinkage parameter. </p>
</td></tr>
<tr><td><code id="mada_+3A_interaction.depth">interaction.depth</code></td>
<td>
<p> used in gbm to specify the depth of trees. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a C-class problem (C &gt; 2), each class is separately compared against all other classes with AdaBoost, and C functions are estimated to represent confidence for each class. The classification rule is to assign the class with the largest estimate.
</p>


<h3>Value</h3>

<p>A list contains variable selected <code>xselect</code> and training and testing error <code>err.tr, err.te</code>. </p>


<h3>Author(s)</h3>

<p> Zhu Wang </p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.mada">cv.mada</a></code> for cross-validated stopping iteration. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
mada(xtr=iris[,-5], ytr=iris[,5])
</code></pre>

<hr>
<h2 id='mbst'> Boosting for Multi-Classification</h2><span id='topic+mbst'></span><span id='topic+print.mbst'></span><span id='topic+predict.mbst'></span><span id='topic+fpartial.mbst'></span>

<h3>Description</h3>

<p>Gradient boosting for optimizing multi-class loss functions with componentwise
linear, smoothing splines, tree models as base learners.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mbst(x, y, cost = NULL, family = c("hinge", "hinge2", "thingeDC", "closs", "clossMM"), 
ctrl = bst_control(), control.tree=list(fixed.depth=TRUE, 
n.term.node=6, maxdepth = 1), learner = c("ls", "sm", "tree"))
## S3 method for class 'mbst'
print(x, ...)
## S3 method for class 'mbst'
predict(object, newdata=NULL, newy=NULL, mstop=NULL, 
type=c("response", "class", "loss", "error"), ...)
## S3 method for class 'mbst'
fpartial(object, mstop=NULL, newdata=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mbst_+3A_x">x</code></td>
<td>
<p> a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="mbst_+3A_y">y</code></td>
<td>
<p> vector of responses. <code>y</code> must be 1, 2, ..., k for a k classification problem</p>
</td></tr>
<tr><td><code id="mbst_+3A_cost">cost</code></td>
<td>
<p> price to pay for false positive, 0 &lt; <code>cost</code> &lt; 1; price of false negative is 1-<code>cost</code>.</p>
</td></tr>
<tr><td><code id="mbst_+3A_family">family</code></td>
<td>
 <p><code>family</code> = &quot;hinge&quot; for hinge loss, <code>family</code>=&quot;hinge2&quot; for hinge loss but the response is not recoded (see details). <code>family="thingeDC"</code> for DCB loss function, see <code>rmbst</code>.</p>
</td></tr> 
<tr><td><code id="mbst_+3A_ctrl">ctrl</code></td>
<td>
<p> an object of class <code><a href="#topic+bst_control">bst_control</a></code>.</p>
</td></tr>
<tr><td><code id="mbst_+3A_control.tree">control.tree</code></td>
<td>
<p> control parameters of rpart. </p>
</td></tr>
<tr><td><code id="mbst_+3A_learner">learner</code></td>
<td>
<p> a character specifying the component-wise base learner to be used:
<code>ls</code> linear models, 
<code>sm</code> smoothing splines,
<code>tree</code> regression trees.
</p>
</td></tr>
<tr><td><code id="mbst_+3A_type">type</code></td>
<td>
<p> in <code>predict</code> a character indicating whether the response, all responses across the boosting iterations, classes, loss or classification errors should be predicted in case of <code>hinge</code> 
problems. in <code>plot</code>, plot of boosting iteration or $L_1$ norm. </p>
</td></tr>
<tr><td><code id="mbst_+3A_object">object</code></td>
<td>
<p> class of <code><a href="#topic+mbst">mbst</a></code>. </p>
</td></tr>
<tr><td><code id="mbst_+3A_newdata">newdata</code></td>
<td>
<p> new data for prediction with the same number of columns as <code>x</code>. </p>
</td></tr>
<tr><td><code id="mbst_+3A_newy">newy</code></td>
<td>
<p> new response. </p>
</td></tr>
<tr><td><code id="mbst_+3A_mstop">mstop</code></td>
<td>
<p> boosting iteration for prediction. </p>
</td></tr>
<tr><td><code id="mbst_+3A_...">...</code></td>
<td>
<p> additional arguments. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>A linear or nonlinear classifier is fitted using a boosting algorithm for multi-class responses. This function is different from <code>mhingebst</code> on how to deal with zero-to-sum constraint and loss functions. If <code>family="hinge"</code>, the loss function is the same as in <code>mhingebst</code> but the boosting algorithm is different. If <code>family="hinge2"</code>, the loss function is different from <code>family="hinge"</code>: the response is not recoded as in Wang (2012). In this case, the loss function is  
</p>
<p style="text-align: center;"><code class="reqn">\sum{I(y_i \neq j)(f_j+1)_+}.</code>
</p>
 <p><code>family="thingeDC"</code> for robust loss function used in the DCB algorithm.
</p>


<h3>Value</h3>

<p>An object of class <code>mbst</code> with <code><a href="base.html#topic+print">print</a></code>, <code><a href="stats.html#topic+coef">coef</a></code>,
<code><a href="graphics.html#topic+plot">plot</a></code> and <code><a href="stats.html#topic+predict">predict</a></code> methods are available for linear models.
For nonlinear models, methods <code><a href="base.html#topic+print">print</a></code> and <code><a href="stats.html#topic+predict">predict</a></code> are available.
</p>
<table>
<tr><td><code>x</code>, <code>y</code>, <code>cost</code>, <code>family</code>, <code>learner</code>, <code>control.tree</code>, <code>maxdepth</code></td>
<td>
<p>These are input variables and parameters</p>
</td></tr>
<tr><td><code>ctrl</code></td>
<td>
<p>the input <code>ctrl</code> with possible updated <code>fk</code> if <code>family="thingeDC"</code></p>
</td></tr>
<tr><td><code>yhat</code></td>
<td>
<p>predicted function estimates</p>
</td></tr>
<tr><td><code>ens</code></td>
<td>
<p>a list of length <code>mstop</code>. Each element is a fitted model to the pseudo residuals, defined as negative gradient of loss function at the current estimated function</p>
</td></tr>
<tr><td><code>ml.fit</code></td>
<td>
<p>the last element of <code>ens</code></p>
</td></tr>
<tr><td><code>ensemble</code></td>
<td>
<p>a vector of length <code>mstop</code>. Each element is the variable selected in each boosting step when applicable</p>
</td></tr>
<tr><td><code>xselect</code></td>
<td>
<p>selected variables in <code>mstop</code></p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>estimated coefficients in each iteration. Used internally only</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Zhu Wang </p>


<h3>References</h3>

<p>Zhu Wang (2011),
HingeBoost: ROC-Based Boost for Classification and Variable Selection.
<em>The International Journal of Biostatistics</em>, <b>7</b>(1), Article 13.
</p>
<p>Zhu Wang (2012), Multi-class HingeBoost: Method and Application to the Classification of Cancer Types Using Gene Expression Data. <em>Methods of Information in Medicine</em>, <b>51</b>(2), 162&ndash;7.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.mbst">cv.mbst</a></code> for cross-validated stopping iteration. Furthermore see
<code><a href="#topic+bst_control">bst_control</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(rnorm(100*5),ncol=5)
c &lt;- quantile(x[,1], prob=c(0.33, 0.67))
y &lt;- rep(1, 100)
y[x[,1] &gt; c[1] &amp; x[,1] &lt; c[2] ] &lt;- 2
y[x[,1] &gt; c[2]] &lt;- 3
x &lt;- as.data.frame(x)
dat.m &lt;- mbst(x, y, ctrl = bst_control(mstop=50), family = "hinge", learner = "ls")
predict(dat.m)
dat.m1 &lt;- mbst(x, y, ctrl = bst_control(twinboost=TRUE, 
f.init=predict(dat.m), xselect.init = dat.m$xselect, mstop=50))
dat.m2 &lt;- rmbst(x, y, ctrl = bst_control(mstop=50, s=1, trace=TRUE), 
rfamily = "thinge", learner = "ls")
predict(dat.m2)
</code></pre>

<hr>
<h2 id='mhingebst'> Boosting for Multi-class Classification</h2><span id='topic+mhingebst'></span><span id='topic+print.mhingebst'></span><span id='topic+predict.mhingebst'></span><span id='topic+fpartial.mhingebst'></span>

<h3>Description</h3>

<p>Gradient boosting for optimizing multi-class hinge loss functions with componentwise linear least squares, smoothing splines and trees as base learners.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mhingebst(x, y, cost = NULL, family = c("hinge"), ctrl = bst_control(), 
control.tree = list(fixed.depth=TRUE, n.term.node=6, maxdepth = 1), 
learner = c("ls", "sm", "tree"))
## S3 method for class 'mhingebst'
print(x, ...)
## S3 method for class 'mhingebst'
predict(object, newdata=NULL, newy=NULL, mstop=NULL, 
type=c("response", "class", "loss", "error"), ...)
## S3 method for class 'mhingebst'
fpartial(object, mstop=NULL, newdata=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mhingebst_+3A_x">x</code></td>
<td>
<p> a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="mhingebst_+3A_y">y</code></td>
<td>
<p> vector of responses. <code>y</code> must be in {1, -1} for <code>family</code> = &quot;hinge&quot;.</p>
</td></tr>
<tr><td><code id="mhingebst_+3A_cost">cost</code></td>
<td>
<p> equal costs for now and unequal costs will be implemented in the future.</p>
</td></tr>
<tr><td><code id="mhingebst_+3A_family">family</code></td>
<td>
 <p><code>family</code> = &quot;hinge&quot; for multi-class hinge loss. </p>
</td></tr>
<tr><td><code id="mhingebst_+3A_ctrl">ctrl</code></td>
<td>
<p> an object of class <code><a href="#topic+bst_control">bst_control</a></code>.</p>
</td></tr>
<tr><td><code id="mhingebst_+3A_control.tree">control.tree</code></td>
<td>
<p> control parameters of rpart. </p>
</td></tr>
<tr><td><code id="mhingebst_+3A_learner">learner</code></td>
<td>
<p> a character specifying the component-wise base learner to be used:
<code>ls</code> linear models, 
<code>sm</code> smoothing splines,
<code>tree</code> regression trees.
</p>
</td></tr>
<tr><td><code id="mhingebst_+3A_type">type</code></td>
<td>
<p> in <code>predict</code> a character indicating whether the response, classes, loss or classification errors should be predicted in case of <code>hinge</code></p>
</td></tr> 
<tr><td><code id="mhingebst_+3A_object">object</code></td>
<td>
<p> class of <code><a href="#topic+mhingebst">mhingebst</a></code>. </p>
</td></tr>
<tr><td><code id="mhingebst_+3A_newdata">newdata</code></td>
<td>
<p> new data for prediction with the same number of columns as <code>x</code>. </p>
</td></tr>
<tr><td><code id="mhingebst_+3A_newy">newy</code></td>
<td>
<p> new response. </p>
</td></tr>
<tr><td><code id="mhingebst_+3A_mstop">mstop</code></td>
<td>
<p> boosting iteration for prediction. </p>
</td></tr>
<tr><td><code id="mhingebst_+3A_...">...</code></td>
<td>
<p> additional arguments. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>A linear or nonlinear classifier is fitted using a boosting algorithm based on component-wise
base learners for multi-class responses. 
</p>


<h3>Value</h3>

<p>An object of class <code>mhingebst</code> with <code><a href="base.html#topic+print">print</a></code> and <code><a href="stats.html#topic+predict">predict</a></code> methods being available for fitted models.
</p>


<h3>Author(s)</h3>

<p> Zhu Wang </p>


<h3>References</h3>

<p>Zhu Wang (2011),
HingeBoost: ROC-Based Boost for Classification and Variable Selection.
<em>The International Journal of Biostatistics</em>, <b>7</b>(1), Article 13.
</p>
<p>Zhu Wang (2012), Multi-class HingeBoost: Method and Application to the Classification of Cancer Types Using Gene Expression Data. <em>Methods of Information in Medicine</em>, <b>51</b>(2), 162&ndash;7.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.mhingebst">cv.mhingebst</a></code> for cross-validated stopping iteration. Furthermore see
<code><a href="#topic+bst_control">bst_control</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dat &lt;- ex1data(100, p=5)
res &lt;- mhingebst(x=dat$x, y=dat$y)

## End(Not run)
</code></pre>

<hr>
<h2 id='mhingeova'> Multi-class HingeBoost</h2><span id='topic+mhingeova'></span><span id='topic+print.mhingeova'></span>

<h3>Description</h3>

<p>Multi-class algorithm with one-vs-all binary HingeBoost which optimizes the hinge loss functions with componentwise
linear, smoothing splines, tree models as base learners.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mhingeova(xtr, ytr, xte=NULL, yte=NULL, cost = NULL, nu=0.1, 
learner=c("tree", "ls", "sm"), maxdepth=1, m1=200, twinboost = FALSE, m2=200)
## S3 method for class 'mhingeova'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mhingeova_+3A_xtr">xtr</code></td>
<td>
<p> training data containing the predictor variables.</p>
</td></tr>
<tr><td><code id="mhingeova_+3A_ytr">ytr</code></td>
<td>
<p> vector of training data responses. <code>ytr</code> must be in {1,2,...,k}.</p>
</td></tr>
<tr><td><code id="mhingeova_+3A_xte">xte</code></td>
<td>
<p> test data containing the predictor variables.</p>
</td></tr>
<tr><td><code id="mhingeova_+3A_yte">yte</code></td>
<td>
<p> vector of test data responses. <code>yte</code> must be in {1,2,...,k}.</p>
</td></tr>
<tr><td><code id="mhingeova_+3A_cost">cost</code></td>
<td>
<p> default is NULL for equal cost; otherwise a numeric vector indicating price to pay for false positive, 0 &lt; <code>cost</code> &lt; 1; price of false negative is 1-<code>cost</code>.</p>
</td></tr>
<tr><td><code id="mhingeova_+3A_nu">nu</code></td>
<td>
<p> a small number (between 0 and 1) defining the step size or shrinkage parameter. </p>
</td></tr>
<tr><td><code id="mhingeova_+3A_learner">learner</code></td>
<td>
<p> a character specifying the component-wise base learner to be used:
<code>ls</code> linear models, 
<code>sm</code> smoothing splines,
<code>tree</code> regression trees.
</p>
</td></tr>
<tr><td><code id="mhingeova_+3A_maxdepth">maxdepth</code></td>
<td>
<p> tree depth used in <code>learner=tree</code></p>
</td></tr>
<tr><td><code id="mhingeova_+3A_m1">m1</code></td>
<td>
<p> number of boosting iteration </p>
</td></tr>
<tr><td><code id="mhingeova_+3A_twinboost">twinboost</code></td>
<td>
<p> logical: twin boosting? </p>
</td></tr>
<tr><td><code id="mhingeova_+3A_m2">m2</code></td>
<td>
<p> number of twin boosting iteration </p>
</td></tr>
<tr><td><code id="mhingeova_+3A_x">x</code></td>
<td>
<p> class of <code><a href="#topic+mhingeova">mhingeova</a></code>. </p>
</td></tr>
<tr><td><code id="mhingeova_+3A_...">...</code></td>
<td>
<p> additional arguments. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a C-class problem (C &gt; 2), each class is separately compared against all other classes with HingeBoost, and C functions are estimated to represent confidence for each class. The classification rule is to assign the class with the largest estimate.
A linear or nonlinear multi-class HingeBoost classifier is fitted using a boosting algorithm based on one-against component-wise
base learners for +1/-1 responses, with possible cost-sensitive hinge loss function. 
</p>


<h3>Value</h3>

<p>An object of class <code>mhingeova</code> with <code><a href="base.html#topic+print">print</a></code> method being available.
</p>


<h3>Author(s)</h3>

<p> Zhu Wang </p>


<h3>References</h3>

<p>Zhu Wang (2011),
HingeBoost: ROC-Based Boost for Classification and Variable Selection.
<em>The International Journal of Biostatistics</em>, <b>7</b>(1), Article 13.
</p>
<p>Zhu Wang (2012), Multi-class HingeBoost: Method and Application to the Classification of Cancer Types Using Gene Expression Data. <em>Methods of Information in Medicine</em>, <b>51</b>(2), 162&ndash;7.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bst">bst</a></code> for HingeBoost binary classification. Furthermore see <code><a href="#topic+cv.bst">cv.bst</a></code> for stopping iteration selection by cross-validation, and <code><a href="#topic+bst_control">bst_control</a></code> for control parameters.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dat1 &lt;- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/
thyroid-disease/ann-train.data")
dat2 &lt;- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/
thyroid-disease/ann-test.data")
res &lt;- mhingeova(xtr=dat1[,-22], ytr=dat1[,22], xte=dat2[,-22], yte=dat2[,22], 
cost=c(2/3, 0.5, 0.5), nu=0.5, learner="ls", m1=100, K=5, cv1=FALSE, 
twinboost=TRUE, m2= 200, cv2=FALSE)
res &lt;- mhingeova(xtr=dat1[,-22], ytr=dat1[,22], xte=dat2[,-22], yte=dat2[,22], 
cost=c(2/3, 0.5, 0.5), nu=0.5, learner="ls", m1=100, K=5, cv1=FALSE, 
twinboost=TRUE, m2= 200, cv2=TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='nsel'> Find Number of Variables In Multi-class Boosting Iterations</h2><span id='topic+nsel'></span>

<h3>Description</h3>

<p> Find Number of Variables In Multi-class Boosting Iterations</p>


<h3>Usage</h3>

<pre><code class='language-R'>nsel(object, mstop)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nsel_+3A_object">object</code></td>
<td>
<p>an object of <code><a href="#topic+mhingebst">mhingebst</a></code>, <code><a href="#topic+mbst">mbst</a></code>, or <code><a href="#topic+rmbst">rmbst</a></code></p>
</td></tr>
<tr><td><code id="nsel_+3A_mstop">mstop</code></td>
<td>
<p>boosting iteration number</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of length <code>mstop</code> indicating number of variables selected in each boosting iteration</p>


<h3>Author(s)</h3>

<p> Zhu Wang </p>

<hr>
<h2 id='rbst'> Robust Boosting for Robust Loss Functions</h2><span id='topic+rbst'></span>

<h3>Description</h3>

<p>MM (majorization/minimization) algorithm based gradient boosting for optimizing nonconvex robust loss functions with componentwise
linear, smoothing splines, tree models as base learners.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbst(x, y, cost = 0.5, rfamily = c("tgaussian", "thuber","thinge", "tbinom", "binomd", 
"texpo", "tpoisson", "clossR", "closs", "gloss", "qloss"), ctrl=bst_control(), 
control.tree=list(maxdepth = 1), learner=c("ls","sm","tree"),del=1e-10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rbst_+3A_x">x</code></td>
<td>
<p> a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="rbst_+3A_y">y</code></td>
<td>
<p> vector of responses. <code>y</code> must be in {1, -1} for classification. </p>
</td></tr>
<tr><td><code id="rbst_+3A_cost">cost</code></td>
<td>
<p> price to pay for false positive, 0 &lt; <code>cost</code> &lt; 1; price of false negative is 1-<code>cost</code>.</p>
</td></tr>
<tr><td><code id="rbst_+3A_rfamily">rfamily</code></td>
<td>
<p> robust loss function, see details. </p>
</td></tr>
<tr><td><code id="rbst_+3A_ctrl">ctrl</code></td>
<td>
<p> an object of class <code><a href="#topic+bst_control">bst_control</a></code>.</p>
</td></tr>
<tr><td><code id="rbst_+3A_control.tree">control.tree</code></td>
<td>
<p> control parameters of rpart. </p>
</td></tr>
<tr><td><code id="rbst_+3A_learner">learner</code></td>
<td>
<p> a character specifying the component-wise base learner to be used:
<code>ls</code> linear models, 
<code>sm</code> smoothing splines,
<code>tree</code> regression trees.
</p>
</td></tr>
<tr><td><code id="rbst_+3A_del">del</code></td>
<td>
<p>convergency criteria</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An MM algorithm operates by creating a convex surrogate function that majorizes the nonconvex objective function. When the surrogate function is minimized with gradient boosting algorithm, the desired objective function is decreased. The MM algorithm contains difference of convex (DC) algorithm for <code>rfamily=c("tgaussian", "thuber","thinge", "tbinom", "binomd", "texpo", "tpoisson")</code> and quadratic majorization boosting algorithm (QMBA) for <code>rfamily=c("clossR", "closs", "gloss", "qloss")</code>. 
</p>
<p><code>rfamily</code> = &quot;tgaussian&quot; for truncated square error loss, &quot;thuber&quot; for truncated Huber loss, &quot;thinge&quot; for truncated hinge loss, &quot;tbinom&quot; for truncated logistic loss, &quot;binomd&quot; for logistic difference loss, &quot;texpo&quot; for truncated exponential loss, &quot;tpoisson&quot; for truncated Poisson loss, &quot;clossR&quot; for C-loss in regression, &quot;closs&quot; for C-loss in classification, &quot;gloss&quot; for G-loss, &quot;qloss&quot; for Q-loss.
</p>
<p><code>s</code> must be a numeric value to be specified in <code>bst_control</code>. For <code>rfamily="thinge", "tbinom", "texpo"</code>  <code>s &lt; 0</code>. For <code>rfamily="binomd", "tpoisson", "closs", "qloss", "clossR"</code> , <code>s &gt; 0</code> and for <code>rfamily="gloss"</code>, <code>s &gt; 1</code>. Some suggested <code>s</code> values: &quot;thinge&quot;= -1, &quot;tbinom&quot;= -log(3), &quot;binomd&quot;= log(4), &quot;texpo&quot;= log(0.5), &quot;closs&quot;=1, &quot;gloss&quot;=1.5, &quot;qloss&quot;=2, &quot;clossR&quot;=1.
</p>


<h3>Value</h3>

<p>An object of class <code>bst</code> with <code><a href="base.html#topic+print">print</a></code>, <code><a href="stats.html#topic+coef">coef</a></code>,
<code><a href="graphics.html#topic+plot">plot</a></code> and <code><a href="stats.html#topic+predict">predict</a></code> methods are available for linear models.
For nonlinear models, methods <code><a href="base.html#topic+print">print</a></code> and <code><a href="stats.html#topic+predict">predict</a></code> are available.
</p>
<table>
<tr><td><code>x</code>, <code>y</code>, <code>cost</code>, <code>rfamily</code>, <code>learner</code>, <code>control.tree</code>, <code>maxdepth</code></td>
<td>
<p>These are input variables and parameters</p>
</td></tr>
<tr><td><code>ctrl</code></td>
<td>
<p>the input <code>ctrl</code> with possible updated <code>fk</code> if <code>family="tgaussian"</code>, <code>"thingeDC"</code>, <code>"tbinomDC"</code>, <code>"binomdDC"</code> or <code>"tpoisson"</code>.</p>
</td></tr>
<tr><td><code>yhat</code></td>
<td>
<p>predicted function estimates</p>
</td></tr>
<tr><td><code>ens</code></td>
<td>
<p>a list of length <code>mstop</code>. Each element is a fitted model to the pseudo residuals, defined as negative gradient of loss function at the current estimated function</p>
</td></tr>
<tr><td><code>ml.fit</code></td>
<td>
<p>the last element of <code>ens</code></p>
</td></tr>
<tr><td><code>ensemble</code></td>
<td>
<p>a vector of length <code>mstop</code>. Each element is the variable selected in each boosting step when applicable</p>
</td></tr>
<tr><td><code>xselect</code></td>
<td>
<p>selected variables in <code>mstop</code></p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>estimated coefficients in <code>mstop</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Zhu Wang </p>


<h3>References</h3>

<p>Zhu Wang (2018),
Quadratic Majorization for Nonconvex Loss with
Applications to the Boosting Algorithm, <em>Journal of Computational and Graphical Statistics</em>, <b>27</b>(3), 491-502, doi: <a href="https://doi.org/10.1080/10618600.2018.1424635">10.1080/10618600.2018.1424635</a>
</p>
<p>Zhu Wang (2018), Robust boosting with truncated loss functions, <em>Electronic Journal of Statistics</em>, <b>12</b>(1), 599-650, doi: <a href="https://doi.org/10.1214/18-EJS1404">10.1214/18-EJS1404</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.rbst">cv.rbst</a></code> for cross-validated stopping iteration. Furthermore see
<code><a href="#topic+bst_control">bst_control</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(rnorm(100*5),ncol=5)
c &lt;- 2*x[,1]
p &lt;- exp(c)/(exp(c)+exp(-c))
y &lt;- rbinom(100,1,p)
y[y != 1] &lt;- -1
y[1:10] &lt;- -y[1:10]
x &lt;- as.data.frame(x)
dat.m &lt;- bst(x, y, ctrl = bst_control(mstop=50), family = "hinge", learner = "ls")
predict(dat.m)
dat.m1 &lt;- bst(x, y, ctrl = bst_control(twinboost=TRUE, 
coefir=coef(dat.m), xselect.init = dat.m$xselect, mstop=50))
dat.m2 &lt;- rbst(x, y, ctrl = bst_control(mstop=50, s=0, trace=TRUE), 
rfamily = "thinge", learner = "ls")
predict(dat.m2)
</code></pre>

<hr>
<h2 id='rbstpath'> Robust Boosting Path for Nonconvex Loss Functions</h2><span id='topic+rbstpath'></span>

<h3>Description</h3>

<p>Gradient boosting path for optimizing robust loss functions with componentwise
linear, smoothing splines, tree models as base learners. See details below before use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbstpath(x, y, rmstop=seq(40, 400, by=20), ctrl=bst_control(), del=1e-16, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rbstpath_+3A_x">x</code></td>
<td>
<p> a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="rbstpath_+3A_y">y</code></td>
<td>
<p> vector of responses. <code>y</code> must be in {1, -1}. </p>
</td></tr>
<tr><td><code id="rbstpath_+3A_rmstop">rmstop</code></td>
<td>
<p> vector of boosting iterations</p>
</td></tr>
<tr><td><code id="rbstpath_+3A_ctrl">ctrl</code></td>
<td>
<p> an object of class <code><a href="#topic+bst_control">bst_control</a></code>.</p>
</td></tr>
<tr><td><code id="rbstpath_+3A_del">del</code></td>
<td>
<p>convergency criteria</p>
</td></tr>
<tr><td><code id="rbstpath_+3A_...">...</code></td>
<td>
<p>arguments passed to <code>rbst</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function invokes <code>rbst</code> with <code>mstop</code> being each element of vector <code>rmstop</code>. It can provide different paths. Thus <code>rmstop</code> serves as another hyper-parameter. However, the most important hyper-parameter is the loss truncation point or the point determines the level of nonconvexity. This is an experimental function and may not be needed in practice. 
</p>


<h3>Value</h3>

<p>A length <code>rmstop</code> vector of lists with each element being an object of class <code>rbst</code>.
</p>


<h3>Author(s)</h3>

<p> Zhu Wang </p>


<h3>See Also</h3>

<p><code><a href="#topic+rbst">rbst</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(rnorm(100*5),ncol=5)
c &lt;- 2*x[,1]
p &lt;- exp(c)/(exp(c)+exp(-c))
y &lt;- rbinom(100,1,p)
y[y != 1] &lt;- -1
y[1:10] &lt;- -y[1:10]
x &lt;- as.data.frame(x)
dat.m &lt;- bst(x, y, ctrl = bst_control(mstop=50), family = "hinge", learner = "ls")
predict(dat.m)
dat.m1 &lt;- bst(x, y, ctrl = bst_control(twinboost=TRUE, 
coefir=coef(dat.m), xselect.init = dat.m$xselect, mstop=50))
dat.m2 &lt;- rbst(x, y, ctrl = bst_control(mstop=50, s=0, trace=TRUE), 
rfamily = "thinge", learner = "ls")
predict(dat.m2)
rmstop &lt;- seq(10, 40, by=10)
dat.m3 &lt;- rbstpath(x, y, rmstop, ctrl=bst_control(s=0), rfamily = "thinge", learner = "ls")
</code></pre>

<hr>
<h2 id='rmbst'> Robust Boosting for Multi-class Robust Loss Functions</h2><span id='topic+rmbst'></span>

<h3>Description</h3>

<p>MM (majorization/minimization) based gradient boosting for optimizing nonconvex robust loss functions with componentwise
linear, smoothing splines, tree models as base learners.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmbst(x, y, cost = 0.5, rfamily = c("thinge", "closs"), ctrl=bst_control(),
control.tree=list(maxdepth = 1),learner=c("ls","sm","tree"),del=1e-10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmbst_+3A_x">x</code></td>
<td>
<p> a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="rmbst_+3A_y">y</code></td>
<td>
<p> vector of responses. <code>y</code> must be in {1, 2, ..., k}. </p>
</td></tr>
<tr><td><code id="rmbst_+3A_cost">cost</code></td>
<td>
<p> price to pay for false positive, 0 &lt; <code>cost</code> &lt; 1; price of false negative is 1-<code>cost</code>.</p>
</td></tr>
<tr><td><code id="rmbst_+3A_rfamily">rfamily</code></td>
<td>
 <p><code>family</code> = &quot;thinge&quot; is currently implemented.</p>
</td></tr>
<tr><td><code id="rmbst_+3A_ctrl">ctrl</code></td>
<td>
<p> an object of class <code><a href="#topic+bst_control">bst_control</a></code>.</p>
</td></tr>
<tr><td><code id="rmbst_+3A_control.tree">control.tree</code></td>
<td>
<p> control parameters of rpart. </p>
</td></tr>
<tr><td><code id="rmbst_+3A_learner">learner</code></td>
<td>
<p> a character specifying the component-wise base learner to be used:
<code>ls</code> linear models, 
<code>sm</code> smoothing splines,
<code>tree</code> regression trees.
</p>
</td></tr>
<tr><td><code id="rmbst_+3A_del">del</code></td>
<td>
<p>convergency criteria</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An MM algorithm operates by creating a convex surrogate function that majorizes the nonconvex objective function. When the surrogate function is minimized with gradient boosting algorithm, the desired objective function is decreased. The MM algorithm contains difference of convex (DC) for <code>rfamily="thinge"</code>, and quadratic majorization boosting algorithm (QMBA) for <code>rfamily="closs"</code>. 
</p>


<h3>Value</h3>

<p>An object of class <code>bst</code> with <code><a href="base.html#topic+print">print</a></code>, <code><a href="stats.html#topic+coef">coef</a></code>,
<code><a href="graphics.html#topic+plot">plot</a></code> and <code><a href="stats.html#topic+predict">predict</a></code> methods are available for linear models.
For nonlinear models, methods <code><a href="base.html#topic+print">print</a></code> and <code><a href="stats.html#topic+predict">predict</a></code> are available.
</p>
<table>
<tr><td><code>x</code>, <code>y</code>, <code>cost</code>, <code>rfamily</code>, <code>learner</code>, <code>control.tree</code>, <code>maxdepth</code></td>
<td>
<p>These are input variables and parameters</p>
</td></tr>
<tr><td><code>ctrl</code></td>
<td>
<p>the input <code>ctrl</code> with possible updated <code>fk</code> if <code>type="adaptive"</code></p>
</td></tr>
<tr><td><code>yhat</code></td>
<td>
<p>predicted function estimates</p>
</td></tr>
<tr><td><code>ens</code></td>
<td>
<p>a list of length <code>mstop</code>. Each element is a fitted model to the pseudo residuals, defined as negative gradient of loss function at the current estimated function</p>
</td></tr>
<tr><td><code>ml.fit</code></td>
<td>
<p>the last element of <code>ens</code></p>
</td></tr>
<tr><td><code>ensemble</code></td>
<td>
<p>a vector of length <code>mstop</code>. Each element is the variable selected in each boosting step when applicable</p>
</td></tr>
<tr><td><code>xselect</code></td>
<td>
<p>selected variables in <code>mstop</code></p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>estimated coefficients in <code>mstop</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Zhu Wang </p>


<h3>References</h3>

<p>Zhu Wang (2018),
Quadratic Majorization for Nonconvex Loss with
Applications to the Boosting Algorithm, <em>Journal of Computational and Graphical Statistics</em>, <b>27</b>(3), 491-502, doi: <a href="https://doi.org/10.1080/10618600.2018.1424635">10.1080/10618600.2018.1424635</a>
</p>
<p>Zhu Wang (2018), Robust boosting with truncated loss functions, <em>Electronic Journal of Statistics</em>, <b>12</b>(1), 599-650, doi: <a href="https://doi.org/10.1214/18-EJS1404">10.1214/18-EJS1404</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.rmbst">cv.rmbst</a></code> for cross-validated stopping iteration. Furthermore see
<code><a href="#topic+bst_control">bst_control</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(rnorm(100*5),ncol=5)
c &lt;- quantile(x[,1], prob=c(0.33, 0.67))
y &lt;- rep(1, 100)
y[x[,1] &gt; c[1] &amp; x[,1] &lt; c[2] ] &lt;- 2
y[x[,1] &gt; c[2]] &lt;- 3
x &lt;- as.data.frame(x)
x &lt;- as.data.frame(x)
dat.m &lt;- mbst(x, y, ctrl = bst_control(mstop=50), family = "hinge", learner = "ls")
predict(dat.m)
dat.m1 &lt;- mbst(x, y, ctrl = bst_control(twinboost=TRUE, 
f.init=predict(dat.m), xselect.init = dat.m$xselect, mstop=50))
dat.m2 &lt;- rmbst(x, y, ctrl = bst_control(mstop=50, s=1, trace=TRUE), 
rfamily = "thinge", learner = "ls")
predict(dat.m2)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
