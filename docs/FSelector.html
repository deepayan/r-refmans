<!DOCTYPE html><html><head><title>Help for package FSelector</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {FSelector}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#as.simple.formula'><p> Converting to formulas</p></a></li>
<li><a href='#best.first.search'><p> Best-first search</p></a></li>
<li><a href='#cfs'><p> CFS filter</p></a></li>
<li><a href='#chi.squared'><p> Chi-squared filter</p></a></li>
<li><a href='#consistency'><p> Consistency-based filter</p></a></li>
<li><a href='#correlation'><p> Correlation filter</p></a></li>
<li><a href='#cutoff'><p> Cutoffs</p></a></li>
<li><a href='#entropy.based'><p> Entropy-based filters</p></a></li>
<li><a href='#exhaustive.search'><p> Exhaustive search</p></a></li>
<li><a href='#FSelector-package'>
<p>Package for selecting attributes</p></a></li>
<li><a href='#greedy.search'><p> Greedy search</p></a></li>
<li><a href='#hill.climbing.search'><p> Hill climbing search</p></a></li>
<li><a href='#oneR'><p> OneR algorithm</p></a></li>
<li><a href='#random.forest.importance'><p> RandomForest filter</p></a></li>
<li><a href='#relief'><p> RReliefF filter</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Selecting Attributes</td>
</tr>
<tr>
<td>Version:</td>
<td>0.34</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-08-22</td>
</tr>
<tr>
<td>Author:</td>
<td>Piotr Romanski, Lars Kotthoff, Patrick Schratz</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Lars Kotthoff &lt;larsko@uwyo.edu&gt;</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/larskotthoff/fselector/issues">https://github.com/larskotthoff/fselector/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/larskotthoff/fselector">https://github.com/larskotthoff/fselector</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Functions for selecting attributes from a given
    dataset. Attribute subset selection is the process of identifying and
    removing as much of the irrelevant and redundant information as
    possible.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>digest, entropy, randomForest, RWeka</td>
</tr>
<tr>
<td>Suggests:</td>
<td>mlbench, rpart</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-22 14:17:42 UTC; larsko</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-22 17:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='as.simple.formula'> Converting to formulas </h2><span id='topic+as.simple.formula'></span>

<h3>Description</h3>

<p>Converts character vector of atrributes' names and destination attribute's name to a simple formula.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.simple.formula(attributes, class)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.simple.formula_+3A_attributes">attributes</code></td>
<td>
<p> character vector of attributes' names </p>
</td></tr>
<tr><td><code id="as.simple.formula_+3A_class">class</code></td>
<td>
<p> name of destination attribute </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A simple formula like &quot;class ~ attr1 + attr2&quot;
</p>


<h3>Author(s)</h3>

<p> Piotr Romanski </p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  result &lt;- cfs(Species ~ ., iris)
  f &lt;- as.simple.formula(result, "Species")
</code></pre>

<hr>
<h2 id='best.first.search'> Best-first search </h2><span id='topic+best.first.search'></span>

<h3>Description</h3>

<p>The algorithm for searching atrribute subset space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>best.first.search(attributes, eval.fun, max.backtracks = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="best.first.search_+3A_attributes">attributes</code></td>
<td>
<p> a character vector of all attributes to search in </p>
</td></tr>
<tr><td><code id="best.first.search_+3A_eval.fun">eval.fun</code></td>
<td>
<p> a function taking as first parameter a character vector of all attributes and returning a numeric indicating how important a given subset is </p>
</td></tr>
<tr><td><code id="best.first.search_+3A_max.backtracks">max.backtracks</code></td>
<td>
<p> an integer indicating a maximum allowed number of backtracks, default is 5 </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm is similar to <code><a href="#topic+forward.search">forward.search</a></code> besides the fact that is chooses the best node from all already evaluated ones and evaluates it. The selection of the best node is repeated approximately <code>max.backtracks</code> times in case no better node found.
</p>


<h3>Value</h3>

<p>A character vector of selected attributes.
</p>


<h3>Author(s)</h3>

<p> Piotr Romanski </p>


<h3>See Also</h3>

 <p><code><a href="#topic+forward.search">forward.search</a></code>, <code><a href="#topic+backward.search">backward.search</a></code>, <code><a href="#topic+hill.climbing.search">hill.climbing.search</a></code>, <code><a href="#topic+exhaustive.search">exhaustive.search</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(rpart)
  data(iris)
  
  evaluator &lt;- function(subset) {
    #k-fold cross validation
    k &lt;- 5
    splits &lt;- runif(nrow(iris))
    results = sapply(1:k, function(i) {
      test.idx &lt;- (splits &gt;= (i - 1) / k) &amp; (splits &lt; i / k)
      train.idx &lt;- !test.idx
      test &lt;- iris[test.idx, , drop=FALSE]
      train &lt;- iris[train.idx, , drop=FALSE]
      tree &lt;- rpart(as.simple.formula(subset, "Species"), train)
      error.rate = sum(test$Species != predict(tree, test, type="c")) / nrow(test)
      return(1 - error.rate)
    })
    print(subset)
    print(mean(results))
    return(mean(results))
  }
  
  subset &lt;- best.first.search(names(iris)[-5], evaluator)
  f &lt;- as.simple.formula(subset, "Species")
  print(f)

  
</code></pre>

<hr>
<h2 id='cfs'> CFS filter </h2><span id='topic+cfs'></span>

<h3>Description</h3>

<p>The algorithm finds attribute subset using correlation and entropy measures for continous and discrete data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cfs(formula, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cfs_+3A_formula">formula</code></td>
<td>
<p> a symbolic description of a model </p>
</td></tr>
<tr><td><code id="cfs_+3A_data">data</code></td>
<td>
<p> data to process </p>
</td></tr></table>


<h3>Details</h3>

<p>The alorithm makes use of <code><a href="#topic+best.first.search">best.first.search</a></code> for searching the attribute subset space.
</p>


<h3>Value</h3>

<p>a character vector containing chosen attributes
</p>


<h3>Author(s)</h3>

<p> Piotr Romanski </p>


<h3>See Also</h3>

 <p><code><a href="#topic+best.first.search">best.first.search</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  
  subset &lt;- cfs(Species~., iris)
  f &lt;- as.simple.formula(subset, "Species")
  print(f)
</code></pre>

<hr>
<h2 id='chi.squared'> Chi-squared filter </h2><span id='topic+chi.squared'></span>

<h3>Description</h3>

<p>The algorithm finds weights of discrete attributes basing on a chi-squared test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chi.squared(formula, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chi.squared_+3A_formula">formula</code></td>
<td>
<p> a symbolic description of a model </p>
</td></tr>
<tr><td><code id="chi.squared_+3A_data">data</code></td>
<td>
<p> a symbolic description of a model </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The result is equal to Cramer's V coefficient between source attributes and destination attribute.
</p>


<h3>Value</h3>

<p>a data.frame containing the worth of attributes in the first column and their names as row names
</p>


<h3>Author(s)</h3>

<p> Piotr Romanski </p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(mlbench)
  data(HouseVotes84)

  weights &lt;- chi.squared(Class~., HouseVotes84)
  print(weights)
  subset &lt;- cutoff.k(weights, 5)
  f &lt;- as.simple.formula(subset, "Class")
  print(f)
</code></pre>

<hr>
<h2 id='consistency'> Consistency-based filter </h2><span id='topic+consistency'></span>

<h3>Description</h3>

<p>The algorithm finds attribute subset using consistency measure for continous and discrete data.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>consistency(formula, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="consistency_+3A_formula">formula</code></td>
<td>
<p> a symbolic description of a model </p>
</td></tr>
<tr><td><code id="consistency_+3A_data">data</code></td>
<td>
<p> data to process </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The alorithm makes use of <code><a href="#topic+best.first.search">best.first.search</a></code> for searching the attribute subset space.
</p>


<h3>Value</h3>

<p>a character vector containing chosen attributes
</p>


<h3>Author(s)</h3>

<p> Piotr Romanski </p>


<h3>See Also</h3>

 <p><code><a href="#topic+best.first.search">best.first.search</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  library(mlbench)
  data(HouseVotes84)
  
  subset &lt;- consistency(Class~., HouseVotes84)
  f &lt;- as.simple.formula(subset, "Class")
  print(f)

## End(Not run)
</code></pre>

<hr>
<h2 id='correlation'> Correlation filter</h2><span id='topic+linear.correlation'></span><span id='topic+rank.correlation'></span>

<h3>Description</h3>

<p>The algorithm finds weights of continous attributes basing on their correlation with continous class attribute.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linear.correlation(formula, data)
rank.correlation(formula, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="correlation_+3A_formula">formula</code></td>
<td>
<p> a symbolic description of a model </p>
</td></tr>
<tr><td><code id="correlation_+3A_data">data</code></td>
<td>
<p> data to process </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>linear.correlation</code> uses Pearson's correlation
</p>
<p><code>rank.correlation</code> uses Spearman's correlation
</p>
<p>Rows with <code>NA</code> values are not taken into consideration.
</p>


<h3>Value</h3>

<p>a data.frame containing the worth of attributes in the first column and their names as row names
</p>


<h3>Author(s)</h3>

<p> Piotr Romanski </p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(mlbench)
  data(BostonHousing)
  d=BostonHousing[-4] # only numeric variables
  
  weights &lt;- linear.correlation(medv~., d)
  print(weights)
  subset &lt;- cutoff.k(weights, 3)
  f &lt;- as.simple.formula(subset, "medv")
  print(f)

  weights &lt;- rank.correlation(medv~., d)
  print(weights)
  subset &lt;- cutoff.k(weights, 3)
  f &lt;- as.simple.formula(subset, "medv")
  print(f)
</code></pre>

<hr>
<h2 id='cutoff'> Cutoffs </h2><span id='topic+cutoff.k'></span><span id='topic+cutoff.k.percent'></span><span id='topic+cutoff.biggest.diff'></span>

<h3>Description</h3>

<p>The algorithms select a subset from a ranked attributes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cutoff.k(attrs, k)
cutoff.k.percent(attrs, k)
cutoff.biggest.diff(attrs)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cutoff_+3A_attrs">attrs</code></td>
<td>
<p> a data.frame containing ranks for attributes in the first column and their names as row names </p>
</td></tr>
<tr><td><code id="cutoff_+3A_k">k</code></td>
<td>
<p> a positive integer in case of <code>cutoff.k</code> and a numeric between 0 and 1 in case of <code>cutoff.k.percent</code> </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>cutoff.k</code> chooses k best attributes
</p>
<p><code>cutoff.k.percent</code> chooses best k * 100% of attributes
</p>
<p><code>cutoff.biggest.diff</code> chooses a subset of attributes which are significantly better than other.
</p>


<h3>Value</h3>

<p>A character vector containing selected attributes.
</p>


<h3>Author(s)</h3>

<p> Piotr Romanski </p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)

  weights &lt;- information.gain(Species~., iris)
  print(weights)

  subset &lt;- cutoff.k(weights, 1)
  f &lt;- as.simple.formula(subset, "Species")
  print(f)

  subset &lt;- cutoff.k.percent(weights, 0.75)
  f &lt;- as.simple.formula(subset, "Species")
  print(f)

  subset &lt;- cutoff.biggest.diff(weights)
  f &lt;- as.simple.formula(subset, "Species")
  print(f)
  
</code></pre>

<hr>
<h2 id='entropy.based'> Entropy-based filters </h2><span id='topic+information.gain'></span><span id='topic+gain.ratio'></span><span id='topic+symmetrical.uncertainty'></span>

<h3>Description</h3>

<p>The algorithms find weights of discrete attributes basing on their correlation with continous class attribute.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>information.gain(formula, data, unit)
gain.ratio(formula, data, unit)
symmetrical.uncertainty(formula, data, unit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="entropy.based_+3A_formula">formula</code></td>
<td>
<p> A symbolic description of a model. </p>
</td></tr>
<tr><td><code id="entropy.based_+3A_data">data</code></td>
<td>
<p> Data to process. </p>
</td></tr>
<tr><td><code id="entropy.based_+3A_unit">unit</code></td>
<td>
<p> Unit for computing entropy (passed to <code><a href="entropy.html#topic+entropy">entropy</a></code>. Default is &quot;log&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>information.gain</code> is </p>
<p style="text-align: center;"><code class="reqn">H(Class) + H(Attribute) - H(Class, Attribute)</code>
</p>
<p>.
</p>
<p><code>gain.ratio</code> is </p>
<p style="text-align: center;"><code class="reqn">\frac{H(Class) + H(Attribute) - H(Class, Attribute)}{H(Attribute)}</code>
</p>

<p><code>symmetrical.uncertainty</code> is </p>
<p style="text-align: center;"><code class="reqn">2\frac{H(Class) + H(Attribute) - H(Class, Attribute)}{H(Attribute) + H(Class)}</code>
</p>



<h3>Value</h3>

<p>a data.frame containing the worth of attributes in the first column and their names as row names
</p>


<h3>Author(s)</h3>

<p> Piotr Romanski, Lars Kotthoff </p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)

  weights &lt;- information.gain(Species~., iris)
  print(weights)
  subset &lt;- cutoff.k(weights, 2)
  f &lt;- as.simple.formula(subset, "Species")
  print(f)

  weights &lt;- information.gain(Species~., iris, unit = "log2")
  print(weights)

  weights &lt;- gain.ratio(Species~., iris)
  print(weights)
  subset &lt;- cutoff.k(weights, 2)
  f &lt;- as.simple.formula(subset, "Species")
  print(f)

  weights &lt;- symmetrical.uncertainty(Species~., iris)
  print(weights)
  subset &lt;- cutoff.biggest.diff(weights)
  f &lt;- as.simple.formula(subset, "Species")
  print(f)

</code></pre>

<hr>
<h2 id='exhaustive.search'> Exhaustive search </h2><span id='topic+exhaustive.search'></span>

<h3>Description</h3>

<p>The algorithm for searching atrribute subset space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>exhaustive.search(attributes, eval.fun)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="exhaustive.search_+3A_attributes">attributes</code></td>
<td>
<p> a character vector of all attributes to search in </p>
</td></tr>
<tr><td><code id="exhaustive.search_+3A_eval.fun">eval.fun</code></td>
<td>
<p> a function taking as first parameter a character vector of all attributes and returning a numeric indicating how important a given subset is </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm searches the whole attribute subset space in breadth-first order.
</p>


<h3>Value</h3>

<p>A character vector of selected attributes.
</p>


<h3>Author(s)</h3>

<p> Piotr Romanski </p>


<h3>See Also</h3>

 <p><code><a href="#topic+forward.search">forward.search</a></code>, <code><a href="#topic+backward.search">backward.search</a></code>, <code><a href="#topic+best.first.search">best.first.search</a></code>, <code><a href="#topic+hill.climbing.search">hill.climbing.search</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(rpart)
  data(iris)
  
  evaluator &lt;- function(subset) {
    #k-fold cross validation
    k &lt;- 5
    splits &lt;- runif(nrow(iris))
    results = sapply(1:k, function(i) {
      test.idx &lt;- (splits &gt;= (i - 1) / k) &amp; (splits &lt; i / k)
      train.idx &lt;- !test.idx
      test &lt;- iris[test.idx, , drop=FALSE]
      train &lt;- iris[train.idx, , drop=FALSE]
      tree &lt;- rpart(as.simple.formula(subset, "Species"), train)
      error.rate = sum(test$Species != predict(tree, test, type="c")) / nrow(test)
      return(1 - error.rate)
    })
    print(subset)
    print(mean(results))
    return(mean(results))
  }
  
  subset &lt;- exhaustive.search(names(iris)[-5], evaluator)
  f &lt;- as.simple.formula(subset, "Species")
  print(f)

  
</code></pre>

<hr>
<h2 id='FSelector-package'>
Package for selecting attributes
</h2><span id='topic+FSelector-package'></span><span id='topic+FSelector'></span>

<h3>Description</h3>

<p>Package containing functions for selecting attributes from a given dataset and a destination attribute.
</p>


<h3>Details</h3>

<p>This package contains:
</p>

<ul>
<li><p>-Algorithms for filtering attributes: cfs, chi.squared, information.gain, gain.ratio, symmetrical.uncertainty, linear.correlation, rank.correlation, oneR, relief, consistency, random.forest.importance
</p>
</li>
<li><p>-Algorithms for wrapping classifiers and search attribute subset space: best.first.search, backward.search, forward.search, hill.climbing.search
</p>
</li>
<li><p>-Algorithm for choosing a subset of attributes based on attributes' weights: cutoff.k, cutoff.k.percent, cutoff.biggest.diff
</p>
</li>
<li><p>-Algorithm for creating formulas: as.simple.formula
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Piotr Romanski<br />
Maintainer: Lars Kotthoff &lt;larsko@uwyo.edu&gt;
</p>

<hr>
<h2 id='greedy.search'> Greedy search </h2><span id='topic+backward.search'></span><span id='topic+forward.search'></span>

<h3>Description</h3>

<p>The algorithms for searching atrribute subset space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>backward.search(attributes, eval.fun)
forward.search(attributes, eval.fun)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="greedy.search_+3A_attributes">attributes</code></td>
<td>
<p> a character vector of all attributes to search in </p>
</td></tr>
<tr><td><code id="greedy.search_+3A_eval.fun">eval.fun</code></td>
<td>
<p> a function taking as first parameter a character vector of all attributes and returning a numeric indicating how important a given subset is </p>
</td></tr>
</table>


<h3>Details</h3>

<p>These algorithms implement greedy search. At first, the algorithms expand starting node, evaluate its children and choose the best one which becomes a new starting node. This process goes only in one direction. <code>forward.search</code> starts from an empty and <code>backward.search</code> from a full set of attributes.
</p>


<h3>Value</h3>

<p>A character vector of selected attributes.
</p>


<h3>Author(s)</h3>

<p> Piotr Romanski </p>


<h3>See Also</h3>

 <p><code><a href="#topic+best.first.search">best.first.search</a></code>, <code><a href="#topic+hill.climbing.search">hill.climbing.search</a></code>, <code><a href="#topic+exhaustive.search">exhaustive.search</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(rpart)
  data(iris)
  
  evaluator &lt;- function(subset) {
    #k-fold cross validation
    k &lt;- 5
    splits &lt;- runif(nrow(iris))
    results = sapply(1:k, function(i) {
      test.idx &lt;- (splits &gt;= (i - 1) / k) &amp; (splits &lt; i / k)
      train.idx &lt;- !test.idx
      test &lt;- iris[test.idx, , drop=FALSE]
      train &lt;- iris[train.idx, , drop=FALSE]
      tree &lt;- rpart(as.simple.formula(subset, "Species"), train)
      error.rate = sum(test$Species != predict(tree, test, type="c")) / nrow(test)
      return(1 - error.rate)
    })
    print(subset)
    print(mean(results))
    return(mean(results))
  }
  
  subset &lt;- forward.search(names(iris)[-5], evaluator)
  f &lt;- as.simple.formula(subset, "Species")
  print(f)

  
</code></pre>

<hr>
<h2 id='hill.climbing.search'> Hill climbing search </h2><span id='topic+hill.climbing.search'></span>

<h3>Description</h3>

<p>The algorithm for searching atrribute subset space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hill.climbing.search(attributes, eval.fun)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hill.climbing.search_+3A_attributes">attributes</code></td>
<td>
<p> a character vector of all attributes to search in </p>
</td></tr>
<tr><td><code id="hill.climbing.search_+3A_eval.fun">eval.fun</code></td>
<td>
<p> a function taking as first parameter a character vector of all attributes and returning a numeric indicating how important a given subset is </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm starts with a random attribute set. Then it evaluates all its neighbours and chooses the best one. It might be susceptible to local maximum.
</p>


<h3>Value</h3>

<p>A character vector of selected attributes.
</p>


<h3>Author(s)</h3>

<p> Piotr Romanski </p>


<h3>See Also</h3>

 <p><code><a href="#topic+forward.search">forward.search</a></code>, <code><a href="#topic+backward.search">backward.search</a></code>, <code><a href="#topic+best.first.search">best.first.search</a></code>, <code><a href="#topic+exhaustive.search">exhaustive.search</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(rpart)
  data(iris)
  
  evaluator &lt;- function(subset) {
    #k-fold cross validation
    k &lt;- 5
    splits &lt;- runif(nrow(iris))
    results = sapply(1:k, function(i) {
      test.idx &lt;- (splits &gt;= (i - 1) / k) &amp; (splits &lt; i / k)
      train.idx &lt;- !test.idx
      test &lt;- iris[test.idx, , drop=FALSE]
      train &lt;- iris[train.idx, , drop=FALSE]
      tree &lt;- rpart(as.simple.formula(subset, "Species"), train)
      error.rate = sum(test$Species != predict(tree, test, type="c")) / nrow(test)
      return(1 - error.rate)
    })
    print(subset)
    print(mean(results))
    return(mean(results))
  }
  
  subset &lt;- hill.climbing.search(names(iris)[-5], evaluator)
  f &lt;- as.simple.formula(subset, "Species")
  print(f)

  
</code></pre>

<hr>
<h2 id='oneR'> OneR algorithm </h2><span id='topic+oneR'></span>

<h3>Description</h3>

<p>The algorithms find weights of discrete attributes basing on very simple association rules involving only one attribute in condition part.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>oneR(formula, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="oneR_+3A_formula">formula</code></td>
<td>
<p> a symbolic description of a model </p>
</td></tr>
<tr><td><code id="oneR_+3A_data">data</code></td>
<td>
<p> data to process </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm uses OneR classifier to find out the attributes' weights. For each attribute it creates a simple rule based only on that attribute and then calculates its error rate.
</p>


<h3>Value</h3>

<p>a data.frame containing the worth of attributes in the first column and their names as row names
</p>


<h3>Author(s)</h3>

<p> Piotr Romanski </p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(mlbench)
  data(HouseVotes84)
  
  weights &lt;- oneR(Class~., HouseVotes84)
  print(weights)
  subset &lt;- cutoff.k(weights, 5)
  f &lt;- as.simple.formula(subset, "Class")
  print(f)
</code></pre>

<hr>
<h2 id='random.forest.importance'> RandomForest filter </h2><span id='topic+random.forest.importance'></span>

<h3>Description</h3>

<p>The algorithm finds weights of attributes using RandomForest algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>random.forest.importance(formula, data, importance.type = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="random.forest.importance_+3A_formula">formula</code></td>
<td>
<p> a symbolic description of a model </p>
</td></tr>
<tr><td><code id="random.forest.importance_+3A_data">data</code></td>
<td>
<p> data to process </p>
</td></tr>
<tr><td><code id="random.forest.importance_+3A_importance.type">importance.type</code></td>
<td>
<p> either 1 or 2, specifying the type of importance measure (1=mean decrease in accuracy, 2=mean decrease in node impurity) </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a wrapper for <code><a href="randomForest.html#topic+importance">importance</a>.</code>
</p>


<h3>Value</h3>

<p>a data.frame containing the worth of attributes in the first column and their names as row names
</p>


<h3>Author(s)</h3>

<p> Piotr Romanski </p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(mlbench)
  data(HouseVotes84)
  
  weights &lt;- random.forest.importance(Class~., HouseVotes84, importance.type = 1)
  print(weights)
  subset &lt;- cutoff.k(weights, 5)
  f &lt;- as.simple.formula(subset, "Class")
  print(f)
</code></pre>

<hr>
<h2 id='relief'> RReliefF filter </h2><span id='topic+relief'></span>

<h3>Description</h3>

<p>The algorithm finds weights of continous and discrete attributes basing on a distance between instances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relief(formula, data, neighbours.count = 5, sample.size = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relief_+3A_formula">formula</code></td>
<td>
<p> a symbolic description of a model </p>
</td></tr>
<tr><td><code id="relief_+3A_data">data</code></td>
<td>
<p> data to process </p>
</td></tr>
<tr><td><code id="relief_+3A_neighbours.count">neighbours.count</code></td>
<td>
<p> number of neighbours to find for every sampled instance </p>
</td></tr>
<tr><td><code id="relief_+3A_sample.size">sample.size</code></td>
<td>
<p> number of instances to sample </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm samples instances and finds their nearest hits and misses. Considering that result, it evaluates weights of attributes.
</p>


<h3>Value</h3>

<p>a data.frame containing the worth of attributes in the first column and their names as row names
</p>


<h3>Author(s)</h3>

<p> Piotr Romanski </p>


<h3>References</h3>


<ul>
<li><p>-Igor Kononenko: Estimating Attributes: Analysis and Extensions of RELIEF. In: European Conference on Machine Learning, 171-182, 1994.
</p>
</li>
<li><p>-Marko Robnik-Sikonja, Igor Kononenko: An adaptation of Relief for attribute estimation in regression. In: Fourteenth International Conference on Machine Learning, 296-304, 1997.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  
  weights &lt;- relief(Species~., iris, neighbours.count = 5, sample.size = 20)
  print(weights)
  subset &lt;- cutoff.k(weights, 2)
  f &lt;- as.simple.formula(subset, "Species")
  print(f)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
