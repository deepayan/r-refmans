<!DOCTYPE html><html lang="en"><head><title>Help for package randomForest</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {randomForest}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#classCenter'><p>Prototypes of groups.</p></a></li>
<li><a href='#combine'><p>Combine Ensembles of Trees</p></a></li>
<li><a href='#getTree'><p>Extract a single tree from a forest.</p></a></li>
<li><a href='#grow'><p>Add trees to an ensemble</p></a></li>
<li><a href='#importance'><p>Extract variable importance measure</p></a></li>
<li><a href='#imports85'><p>The Automobile Data</p></a></li>
<li><a href='#margin'><p>Margins of randomForest Classifier</p></a></li>
<li><a href='#MDSplot'><p>Multi-dimensional Scaling Plot of Proximity matrix from randomForest</p></a></li>
<li><a href='#na.roughfix'><p>Rough Imputation of Missing Values</p></a></li>
<li><a href='#outlier'><p>Compute outlying measures</p></a></li>
<li><a href='#partialPlot'><p>Partial dependence plot</p></a></li>
<li><a href='#plot.randomForest'><p>Plot method for randomForest objects</p></a></li>
<li><a href='#predict.randomForest'><p>predict method for random forest objects</p></a></li>
<li><a href='#randomForest'><p>Classification and Regression with Random Forest</p></a></li>
<li><a href='#rfcv'><p>Random Forest Cross-Valdidation for feature selection</p></a></li>
<li><a href='#rfImpute'><p>Missing Value Imputations by randomForest</p></a></li>
<li><a href='#rfNews'><p>Show the NEWS file</p></a></li>
<li><a href='#treesize'><p>Size of trees in an ensemble</p></a></li>
<li><a href='#tuneRF'><p>Tune randomForest for the optimal mtry parameter</p></a></li>
<li><a href='#varImpPlot'><p>Variable Importance Plot</p></a></li>
<li><a href='#varUsed'><p>Variables used in a random forest</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Breiman and Cutlers Random Forests for Classification and
Regression</td>
</tr>
<tr>
<td>Version:</td>
<td>4.7-1.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-01-24</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0), stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>RColorBrewer, MASS</td>
</tr>
<tr>
<td>Description:</td>
<td>Classification and regression based on a forest of trees using random inputs, based on Breiman (2001) &lt;<a href="https://doi.org/10.1023%2FA%3A1010933404324">doi:10.1023/A:1010933404324</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/">https://www.stat.berkeley.edu/~breiman/RandomForests/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-09-22 08:30:17 UTC; hornik</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-09-22 09:14:44 UTC</td>
</tr>
<tr>
<td>Author:</td>
<td>Leo Breiman [aut] (Fortran original),
  Adele Cutler [aut] (Fortran original),
  Andy Liaw [aut, cre] (R port),
  Matthew Wiener [aut] (R port)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Andy Liaw &lt;andy_liaw@merck.com&gt;</td>
</tr>
</table>
<hr>
<h2 id='classCenter'>Prototypes of groups.</h2><span id='topic+classCenter'></span>

<h3>Description</h3>

<p>Prototypes are &lsquo;representative&rsquo; cases of a group of data points, given
the similarity matrix among the points.  They are very similar to
medoids.  The function is named &lsquo;classCenter&rsquo; to avoid conflict with
the function <code>prototype</code> in the <code>methods</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classCenter(x, label, prox, nNbr = min(table(label))-1) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="classCenter_+3A_x">x</code></td>
<td>
<p>a matrix or data frame</p>
</td></tr>
<tr><td><code id="classCenter_+3A_label">label</code></td>
<td>
<p>group labels of the rows in <code>x</code></p>
</td></tr>
<tr><td><code id="classCenter_+3A_prox">prox</code></td>
<td>
<p>the proximity (or similarity) matrix, assumed to be
symmetric with 1 on the diagonal and in [0, 1] off the diagonal (the
order of row/column must match that of <code>x</code>)</p>
</td></tr>
<tr><td><code id="classCenter_+3A_nnbr">nNbr</code></td>
<td>
<p>number of nearest neighbors used to find the prototypes.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This version only computes one prototype per class.  For each case in
<code>x</code>, the <code>nNbr</code> nearest neighors are found.  Then, for each
class, the case that has most neighbors of that class is identified.
The prototype for that class is then the medoid of these neighbors
(coordinate-wise medians for numerical variables and modes for
categorical variables).
</p>
<p>This version only computes one prototype per class.  In the future
more prototypes may be computed (by removing the &lsquo;neighbors&rsquo; used,
then iterate).
</p>


<h3>Value</h3>

<p>A data frame containing one prototype in each row.
</p>


<h3>Author(s)</h3>

<p>Andy Liaw</p>


<h3>See Also</h3>

<p><code><a href="#topic+randomForest">randomForest</a></code>, <code><a href="#topic+MDSplot">MDSplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
iris.rf &lt;- randomForest(iris[,-5], iris[,5], prox=TRUE)
iris.p &lt;- classCenter(iris[,-5], iris[,5], iris.rf$prox)
plot(iris[,3], iris[,4], pch=21, xlab=names(iris)[3], ylab=names(iris)[4],
     bg=c("red", "blue", "green")[as.numeric(factor(iris$Species))],
     main="Iris Data with Prototypes")
points(iris.p[,3], iris.p[,4], pch=21, cex=2, bg=c("red", "blue", "green"))
</code></pre>

<hr>
<h2 id='combine'>Combine Ensembles of Trees</h2><span id='topic+combine'></span>

<h3>Description</h3>

<p>Combine two more more ensembles of trees into one.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>combine(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="combine_+3A_...">...</code></td>
<td>
<p>two or more objects of class <code>randomForest</code>, to be
combined into one.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>randomForest</code>.
</p>


<h3>Note</h3>

<p>The <code>confusion</code>, <code>err.rate</code>, <code>mse</code> and <code>rsq</code>
components (as well as the corresponding components in the <code>test</code>
compnent, if exist) of the combined object will be <code>NULL</code>.  
</p>


<h3>Author(s)</h3>

<p>Andy Liaw <a href="mailto:andy_liaw@merck.com">andy_liaw@merck.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+randomForest">randomForest</a></code>, <code><a href="#topic+grow">grow</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
rf1 &lt;- randomForest(Species ~ ., iris, ntree=50, norm.votes=FALSE)
rf2 &lt;- randomForest(Species ~ ., iris, ntree=50, norm.votes=FALSE)
rf3 &lt;- randomForest(Species ~ ., iris, ntree=50, norm.votes=FALSE)
rf.all &lt;- combine(rf1, rf2, rf3)
print(rf.all)
</code></pre>

<hr>
<h2 id='getTree'>Extract a single tree from a forest.</h2><span id='topic+getTree'></span>

<h3>Description</h3>

<p>This function extract the structure of a tree from a
<code>randomForest</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getTree(rfobj, k=1, labelVar=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getTree_+3A_rfobj">rfobj</code></td>
<td>
<p>a <code><a href="#topic+randomForest">randomForest</a></code> object.</p>
</td></tr>
<tr><td><code id="getTree_+3A_k">k</code></td>
<td>
<p>which tree to extract?</p>
</td></tr>
<tr><td><code id="getTree_+3A_labelvar">labelVar</code></td>
<td>
<p>Should better labels be used for splitting variables
and predicted class?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For numerical predictors, data with values of the variable less than
or equal to the splitting point go to the left daughter node.
</p>
<p>For categorical predictors, the splitting point is represented by an
integer, whose binary expansion gives the identities of the categories
that goes to left or right.  For example, if a predictor has four
categories, and the split point is 13.  The binary expansion of 13 is
(1, 0, 1, 1) (because <code class="reqn">13 = 1*2^0 + 0*2^1 + 1*2^2 + 1*2^3</code>), so cases with
categories 1, 3, or 4 in this predictor get sent to the left, and the rest
to the right.
</p>


<h3>Value</h3>

<p>A matrix (or data frame, if <code>labelVar=TRUE</code>) with six columns and
number of rows equal to total number of nodes in the tree.  The six
columns are:
</p>
<table role = "presentation">
<tr><td><code>left daughter</code></td>
<td>
<p>the row where the left daughter node is; 0 if the
node is terminal</p>
</td></tr>
<tr><td><code>right daughter</code></td>
<td>
<p>the row where the right daughter node is; 0 if
the node is terminal</p>
</td></tr>
<tr><td><code>split var</code></td>
<td>
<p>which variable was used to split the node; 0 if the
node is terminal</p>
</td></tr>
<tr><td><code>split point</code></td>
<td>
<p>where the best split is; see Details for
categorical predictor</p>
</td></tr>
<tr><td><code>status</code></td>
<td>
<p>is the node terminal (-1) or not (1)</p>
</td></tr>
<tr><td><code>prediction</code></td>
<td>
<p>the prediction for the node; 0 if the node is not
terminal</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andy Liaw <a href="mailto:andy_liaw@merck.com">andy_liaw@merck.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+randomForest">randomForest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
## Look at the third trees in the forest.
getTree(randomForest(iris[,-5], iris[,5], ntree=10), 3, labelVar=TRUE)
</code></pre>

<hr>
<h2 id='grow'>Add trees to an ensemble</h2><span id='topic+grow'></span><span id='topic+grow.default'></span><span id='topic+grow.randomForest'></span>

<h3>Description</h3>

<p>Add additional trees to an existing ensemble of trees.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'randomForest'
grow(x, how.many, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="grow_+3A_x">x</code></td>
<td>
<p>an object of class <code>randomForest</code>, which contains a
<code>forest</code> component.</p>
</td></tr>
<tr><td><code id="grow_+3A_how.many">how.many</code></td>
<td>
<p>number of trees to add to the <code>randomForest</code>
object.</p>
</td></tr>
<tr><td><code id="grow_+3A_...">...</code></td>
<td>
<p>currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>randomForest</code>, containing <code>how.many</code>
additional trees. 
</p>


<h3>Note</h3>

<p>The <code>confusion</code>, <code>err.rate</code>, <code>mse</code> and <code>rsq</code>
components (as well as the corresponding components in the <code>test</code>
compnent, if exist) of the combined object will be <code>NULL</code>.  
</p>


<h3>Author(s)</h3>

<p>Andy Liaw <a href="mailto:andy_liaw@merck.com">andy_liaw@merck.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+combine">combine</a></code>, <code><a href="#topic+randomForest">randomForest</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
iris.rf &lt;- randomForest(Species ~ ., iris, ntree=50, norm.votes=FALSE)
iris.rf &lt;- grow(iris.rf, 50)
print(iris.rf)
</code></pre>

<hr>
<h2 id='importance'>Extract variable importance measure</h2><span id='topic+importance'></span><span id='topic+importance.default'></span><span id='topic+importance.randomForest'></span>

<h3>Description</h3>

<p>This is the extractor function for variable importance measures as
produced by <code><a href="#topic+randomForest">randomForest</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'randomForest'
importance(x, type=NULL, class=NULL, scale=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="importance_+3A_x">x</code></td>
<td>
<p>an object of class <code><a href="#topic+randomForest">randomForest</a></code></p>
</td></tr></table>
<p>.
</p>
<table role = "presentation">
<tr><td><code id="importance_+3A_type">type</code></td>
<td>
<p>either 1 or 2, specifying the type of importance measure
(1=mean decrease in accuracy, 2=mean decrease in node impurity).</p>
</td></tr>
<tr><td><code id="importance_+3A_class">class</code></td>
<td>
<p>for classification problem, which class-specific measure
to return.</p>
</td></tr>
<tr><td><code id="importance_+3A_scale">scale</code></td>
<td>
<p>For permutation based measures, should the measures be
divided their &ldquo;standard errors&rdquo;?</p>
</td></tr>
<tr><td><code id="importance_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Here are the definitions of the variable importance measures.  The
first measure is computed from permuting OOB data:  For
each tree, the prediction error on the out-of-bag portion of the
data is recorded (error rate for classification, MSE for regression).
Then the same is done after permuting each predictor variable.  The
difference between the two are then averaged over all trees, and
normalized by the standard deviation of the differences.  If the
standard deviation of the differences is equal to 0 for a variable,
the division is not done (but the average is almost always equal to 0
in that case).
</p>
<p>The second measure is the total decrease in node impurities from
splitting on the variable, averaged over all trees.  For
classification, the node impurity is measured by the Gini index.
For regression, it is measured by residual sum of squares.
</p>


<h3>Value</h3>

<p>A matrix of importance measure, one row for each predictor variable.
The column(s) are different importance measures.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+randomForest">randomForest</a></code>, <code><a href="#topic+varImpPlot">varImpPlot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(4543)
data(mtcars)
mtcars.rf &lt;- randomForest(mpg ~ ., data=mtcars, ntree=1000,
                          keep.forest=FALSE, importance=TRUE)
importance(mtcars.rf)
importance(mtcars.rf, type=1)
</code></pre>

<hr>
<h2 id='imports85'>The Automobile Data</h2><span id='topic+imports85'></span>

<h3>Description</h3>

<p>This is the &lsquo;Automobile&rsquo; data from the UCI Machine Learning Repository.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(imports85)
</code></pre>


<h3>Format</h3>

<p><code>imports85</code> is a data frame with 205 cases (rows) and 26
variables (columns).  This data set consists of three types of
entities: (a) the specification of an auto in terms of various
characteristics, (b) its assigned insurance risk rating, (c) its
normalized losses in use as compared to other cars.  The second rating
corresponds to the degree to which the auto is more risky than its
price indicates.  Cars are initially assigned a risk factor symbol
associated with its price.   Then, if it is more risky (or less), this
symbol is adjusted by moving it up (or down) the scale.  Actuarians
call this process &lsquo;symboling&rsquo;.  A value of +3 indicates that the auto
is risky, -3 that it is probably pretty safe.
</p>
<p>The third factor is the relative average loss payment per insured
vehicle year.  This value is normalized for all autos within a
particular size classification (two-door small, station wagons,
sports/speciality, etc...), and represents the average loss per car
per year.
</p>


<h3>Author(s)</h3>

<p>Andy Liaw</p>


<h3>Source</h3>

<p>Originally created by Jeffrey C. Schlimmer, from 1985 Model Import Car
and Truck Specifications, 1985 Ward's Automotive Yearbook, Personal
Auto Manuals, Insurance Services Office, and Insurance Collision
Report, Insurance Institute for Highway Safety.
</p>
<p>The original data is at <a href="https://doi.org/10.24432/C5B01C">doi:10.24432/C5B01C</a>.
</p>


<h3>References</h3>

<p>1985 Model Import Car and Truck Specifications, 1985 Ward's Automotive
Yearbook.
</p>
<p>Personal Auto Manuals, Insurance Services Office,
160 Water Street, New York, NY 10038 
</p>
<p>Insurance Collision Report, Insurance Institute for Highway Safety,
Watergate 600, Washington, DC 20037
</p>


<h3>See Also</h3>

<p><code><a href="#topic+randomForest">randomForest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(imports85)
imp85 &lt;- imports85[,-2]  # Too many NAs in normalizedLosses.
imp85 &lt;- imp85[complete.cases(imp85), ]
## Drop empty levels for factors.
imp85[] &lt;- lapply(imp85, function(x) if (is.factor(x)) x[, drop=TRUE] else x)

stopifnot(require(randomForest))
price.rf &lt;- randomForest(price ~ ., imp85, do.trace=10, ntree=100)
print(price.rf)
numDoors.rf &lt;- randomForest(numOfDoors ~ ., imp85, do.trace=10, ntree=100)
print(numDoors.rf)
</code></pre>

<hr>
<h2 id='margin'>Margins of randomForest Classifier</h2><span id='topic+margin'></span><span id='topic+margin.default'></span><span id='topic+margin.randomForest'></span><span id='topic+plot.margin'></span>

<h3>Description</h3>

<p>Compute or plot the margin of predictions from a randomForest classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'randomForest'
margin(x, ...)
## Default S3 method:
margin(x, observed, ...)
## S3 method for class 'margin'
plot(x, sort=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="margin_+3A_x">x</code></td>
<td>
<p>an object of class <code><a href="#topic+randomForest">randomForest</a></code>, whose
<code>type</code> is not <code>regression</code>, or a matrix of predicted
probabilities, one column per class and one row per observation.
For the <code>plot</code> method, <code>x</code> should be an object returned by
<code>margin</code>.</p>
</td></tr>
<tr><td><code id="margin_+3A_observed">observed</code></td>
<td>
<p>the true response corresponding to the data in <code>x</code>.</p>
</td></tr>
<tr><td><code id="margin_+3A_sort">sort</code></td>
<td>
<p>Should the data be sorted by their class labels?</p>
</td></tr>
<tr><td><code id="margin_+3A_...">...</code></td>
<td>
<p>other graphical parameters to be passed to <code>plot.default</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>margin</code>, the <em>margin</em> of observations from the
<code><a href="#topic+randomForest">randomForest</a></code> classifier (or whatever classifier that
produced the predicted probability matrix given to <code>margin</code>).
The margin of a data point is defined as the proportion of votes for
the correct class minus maximum proportion of votes for the other
classes.  Thus under majority votes, positive margin means correct
classification, and vice versa.
</p>


<h3>Author(s)</h3>

<p>Robert Gentlemen, with slight modifications by Andy Liaw</p>


<h3>See Also</h3>

<p><code><a href="#topic+randomForest">randomForest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
data(iris)
iris.rf &lt;- randomForest(Species ~ ., iris, keep.forest=FALSE)
plot(margin(iris.rf))
</code></pre>

<hr>
<h2 id='MDSplot'>Multi-dimensional Scaling Plot of Proximity matrix from randomForest</h2><span id='topic+MDSplot'></span>

<h3>Description</h3>

<p>Plot the scaling coordinates of the proximity matrix from randomForest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MDSplot(rf, fac, k=2, palette=NULL, pch=20, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="MDSplot_+3A_rf">rf</code></td>
<td>
<p>an object of class <code><a href="#topic+randomForest">randomForest</a></code> that contains
the <code>proximity</code> component.</p>
</td></tr>
<tr><td><code id="MDSplot_+3A_fac">fac</code></td>
<td>
<p>a factor that was used as response to train <code>rf</code>.</p>
</td></tr>
<tr><td><code id="MDSplot_+3A_k">k</code></td>
<td>
<p>number of dimensions for the scaling coordinates.</p>
</td></tr>
<tr><td><code id="MDSplot_+3A_palette">palette</code></td>
<td>
<p>colors to use to distinguish the classes; length must
be the equal to the number of levels.</p>
</td></tr>
<tr><td><code id="MDSplot_+3A_pch">pch</code></td>
<td>
<p>plotting symbols to use.</p>
</td></tr>
<tr><td><code id="MDSplot_+3A_...">...</code></td>
<td>
<p>other graphical parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The output of <code><a href="stats.html#topic+cmdscale">cmdscale</a></code> on 1 - <code>rf$proximity</code> is
returned invisibly.
</p>


<h3>Note</h3>

<p>If <code>k &gt; 2</code>, <code><a href="graphics.html#topic+pairs">pairs</a></code> is used to produce the
scatterplot matrix of the coordinates.
</p>


<h3>Author(s)</h3>

<p>Robert Gentleman, with slight modifications by Andy Liaw</p>


<h3>See Also</h3>

<p><code><a href="#topic+randomForest">randomForest</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
data(iris)
iris.rf &lt;- randomForest(Species ~ ., iris, proximity=TRUE,
                        keep.forest=FALSE)
MDSplot(iris.rf, iris$Species)
## Using different symbols for the classes:
MDSplot(iris.rf, iris$Species, palette=rep(1, 3), pch=as.numeric(iris$Species))
</code></pre>

<hr>
<h2 id='na.roughfix'>Rough Imputation of Missing Values</h2><span id='topic+na.roughfix'></span><span id='topic+na.roughfix.default'></span><span id='topic+na.roughfix.data.frame'></span>

<h3>Description</h3>

<p>Impute Missing Values by median/mode.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>na.roughfix(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="na.roughfix_+3A_object">object</code></td>
<td>
<p>a data frame or numeric matrix.</p>
</td></tr>
<tr><td><code id="na.roughfix_+3A_...">...</code></td>
<td>
<p>further arguments special methods could require.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A completed data matrix or data frame.  For numeric variables,
<code>NA</code>s are replaced with column medians.  For factor variables,
<code>NA</code>s are replaced with the most frequent levels (breaking ties
at random).  If <code>object</code> contains no <code>NA</code>s, it is returned
unaltered. 
</p>


<h3>Note</h3>

<p>This is used as a starting point for imputing missing values by random
forest.
</p>


<h3>Author(s)</h3>

<p>Andy Liaw</p>


<h3>See Also</h3>

<p><code><a href="#topic+rfImpute">rfImpute</a></code>, <code><a href="#topic+randomForest">randomForest</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
iris.na &lt;- iris
set.seed(111)
## artificially drop some data values.
for (i in 1:4) iris.na[sample(150, sample(20, 1)), i] &lt;- NA
iris.roughfix &lt;- na.roughfix(iris.na)
iris.narf &lt;- randomForest(Species ~ ., iris.na, na.action=na.roughfix)
print(iris.narf)
</code></pre>

<hr>
<h2 id='outlier'>Compute outlying measures</h2><span id='topic+outlier'></span><span id='topic+outlier.randomForest'></span><span id='topic+outlier.default'></span>

<h3>Description</h3>

<p>Compute outlying measures based on a proximity matrix.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
outlier(x, cls=NULL, ...)
## S3 method for class 'randomForest'
outlier(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="outlier_+3A_x">x</code></td>
<td>
<p>a proximity matrix (a square matrix with 1 on the diagonal
and values between 0 and 1 in the off-diagonal positions); or an object of
class <code><a href="#topic+randomForest">randomForest</a></code>, whose <code>type</code> is not
<code>regression</code>.</p>
</td></tr>
<tr><td><code id="outlier_+3A_cls">cls</code></td>
<td>
<p>the classes the rows in the proximity matrix belong to.  If
not given, all data are assumed to come from the same class.</p>
</td></tr>
<tr><td><code id="outlier_+3A_...">...</code></td>
<td>
<p>arguments for other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector containing the outlying measures.  The outlying
measure of a case is computed as n / sum(squared proximity), normalized by
subtracting the median and divided by the MAD, within each class.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+randomForest">randomForest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
iris.rf &lt;- randomForest(iris[,-5], iris[,5], proximity=TRUE)
plot(outlier(iris.rf), type="h",
     col=c("red", "green", "blue")[as.numeric(iris$Species)])
</code></pre>

<hr>
<h2 id='partialPlot'>Partial dependence plot</h2><span id='topic+partialPlot'></span><span id='topic+partialPlot.default'></span><span id='topic+partialPlot.randomForest'></span>

<h3>Description</h3>

<p>Partial dependence plot gives a graphical depiction of the marginal
effect of a variable on the class probability (classification) or
response (regression).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'randomForest'
partialPlot(x, pred.data, x.var, which.class,
      w, plot = TRUE, add = FALSE,
      n.pt = min(length(unique(pred.data[, xname])), 51),
      rug = TRUE, xlab=deparse(substitute(x.var)), ylab="",
      main=paste("Partial Dependence on", deparse(substitute(x.var))),
      ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="partialPlot_+3A_x">x</code></td>
<td>
<p>an object of class <code>randomForest</code>, which contains a
<code>forest</code> component.</p>
</td></tr>
<tr><td><code id="partialPlot_+3A_pred.data">pred.data</code></td>
<td>
<p>a data frame used for contructing the plot, usually
the training data used to contruct the random forest.</p>
</td></tr>
<tr><td><code id="partialPlot_+3A_x.var">x.var</code></td>
<td>
<p>name of the variable for which partial
dependence is to be examined.</p>
</td></tr>
<tr><td><code id="partialPlot_+3A_which.class">which.class</code></td>
<td>
<p>For classification data, the class to focus on
(default the first class).</p>
</td></tr>
<tr><td><code id="partialPlot_+3A_w">w</code></td>
<td>
<p>weights to be used in averaging; if not supplied, mean is not
weighted</p>
</td></tr>
<tr><td><code id="partialPlot_+3A_plot">plot</code></td>
<td>
<p>whether the plot should be shown on the graphic device.</p>
</td></tr>
<tr><td><code id="partialPlot_+3A_add">add</code></td>
<td>
<p>whether to add to existing plot (<code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="partialPlot_+3A_n.pt">n.pt</code></td>
<td>
<p>if <code>x.var</code> is continuous, the number of points on the
grid for evaluating partial dependence.</p>
</td></tr>
<tr><td><code id="partialPlot_+3A_rug">rug</code></td>
<td>
<p>whether to draw hash marks at the bottom of the plot
indicating the deciles of <code>x.var</code>.</p>
</td></tr>
<tr><td><code id="partialPlot_+3A_xlab">xlab</code></td>
<td>
<p>label for the x-axis.</p>
</td></tr>
<tr><td><code id="partialPlot_+3A_ylab">ylab</code></td>
<td>
<p>label for the y-axis.</p>
</td></tr>
<tr><td><code id="partialPlot_+3A_main">main</code></td>
<td>
<p>main title for the plot.</p>
</td></tr>
<tr><td><code id="partialPlot_+3A_...">...</code></td>
<td>
<p>other graphical parameters to be passed on to <code>plot</code>
or <code>lines</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function being plotted is defined as:
</p>
<p style="text-align: center;"><code class="reqn">
    \tilde{f}(x) = \frac{1}{n} \sum_{i=1}^n f(x, x_{iC}),
  </code>
</p>

<p>where <code class="reqn">x</code> is the variable for which partial dependence is sought,
and <code class="reqn">x_{iC}</code> is the other variables in the data.  The summand is
the predicted regression function for regression, and logits
(i.e., log of fraction of votes) for <code>which.class</code> for
classification:
</p>
<p style="text-align: center;"><code class="reqn"> f(x) = \log p_k(x) - \frac{1}{K} \sum_{j=1}^K \log p_j(x),</code>
</p>

<p>where <code class="reqn">K</code> is the number of classes, <code class="reqn">k</code> is <code>which.class</code>,
and <code class="reqn">p_j</code> is the proportion of votes for class <code class="reqn">j</code>.
</p>


<h3>Value</h3>

<p>A list with two components: <code>x</code> and <code>y</code>, which are the values
used in the plot.
</p>


<h3>Note</h3>

<p>The <code>randomForest</code> object must contain the <code>forest</code>
component; i.e., created with <code>randomForest(...,
    keep.forest=TRUE)</code>.
</p>
<p>This function runs quite slow for large data sets.
</p>


<h3>Author(s)</h3>

<p>Andy Liaw <a href="mailto:andy_liaw@merck.com">andy_liaw@merck.com</a></p>


<h3>References</h3>

<p>Friedman, J. (2001). Greedy function approximation: the gradient
boosting machine, <em>Ann. of Stat.</em></p>


<h3>See Also</h3>

<p><code><a href="#topic+randomForest">randomForest</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
set.seed(543)
iris.rf &lt;- randomForest(Species~., iris)
partialPlot(iris.rf, iris, Petal.Width, "versicolor")

## Looping over variables ranked by importance:
data(airquality)
airquality &lt;- na.omit(airquality)
set.seed(131)
ozone.rf &lt;- randomForest(Ozone ~ ., airquality, importance=TRUE)
imp &lt;- importance(ozone.rf)
impvar &lt;- rownames(imp)[order(imp[, 1], decreasing=TRUE)]
op &lt;- par(mfrow=c(2, 3))
for (i in seq_along(impvar)) {
    partialPlot(ozone.rf, airquality, impvar[i], xlab=impvar[i],
                main=paste("Partial Dependence on", impvar[i]),
                ylim=c(30, 70))
}
par(op)
</code></pre>

<hr>
<h2 id='plot.randomForest'>Plot method for randomForest objects</h2><span id='topic+plot.randomForest'></span>

<h3>Description</h3>

<p>Plot the error rates or MSE of a randomForest object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'randomForest'
plot(x, type="l", main=deparse(substitute(x)), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.randomForest_+3A_x">x</code></td>
<td>
<p>an object of class <code>randomForest</code>.</p>
</td></tr>
<tr><td><code id="plot.randomForest_+3A_type">type</code></td>
<td>
<p>type of plot.</p>
</td></tr>
<tr><td><code id="plot.randomForest_+3A_main">main</code></td>
<td>
<p>main title of the plot.</p>
</td></tr>
<tr><td><code id="plot.randomForest_+3A_...">...</code></td>
<td>
<p>other graphical parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly, the error rates or MSE of the <code>randomForest</code> object.
If the object has a non-null <code>test</code> component, then the returned
object is a matrix where the first column is the out-of-bag estimate
of error, and the second column is for the test set.
</p>


<h3>Note</h3>

<p>This function does not work for <code>randomForest</code> objects that have
<code>type=unsupervised</code>.
</p>
<p>If the <code>x</code> has a non-null <code>test</code> component, then the test
set errors are also plotted.
</p>


<h3>Author(s)</h3>

<p>Andy Liaw</p>


<h3>See Also</h3>

<p><code><a href="#topic+randomForest">randomForest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mtcars)
plot(randomForest(mpg ~ ., mtcars, keep.forest=FALSE, ntree=100), log="y")
</code></pre>

<hr>
<h2 id='predict.randomForest'>predict method for random forest objects</h2><span id='topic+predict.randomForest'></span>

<h3>Description</h3>

<p>Prediction of test data using random forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'randomForest'
predict(object, newdata, type="response",
  norm.votes=TRUE, predict.all=FALSE, proximity=FALSE, nodes=FALSE,
  cutoff, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.randomForest_+3A_object">object</code></td>
<td>
<p>an object of class <code>randomForest</code>, as that
created by the function <code>randomForest</code>.</p>
</td></tr>
<tr><td><code id="predict.randomForest_+3A_newdata">newdata</code></td>
<td>
<p>a data frame or matrix containing new data.  (Note: If
not given, the out-of-bag prediction in <code>object</code> is returned.</p>
</td></tr>
<tr><td><code id="predict.randomForest_+3A_type">type</code></td>
<td>
<p>one of <code>response</code>, <code>prob</code>. or <code>votes</code>,
indicating the type of output: predicted values, matrix of class
probabilities, or matrix of vote counts.  <code>class</code> is allowed, but
automatically converted to &quot;response&quot;, for backward compatibility.</p>
</td></tr>
<tr><td><code id="predict.randomForest_+3A_norm.votes">norm.votes</code></td>
<td>
<p>Should the vote counts be normalized (i.e.,
expressed as fractions)?  Ignored if <code>object$type</code> is
<code>regression</code>.</p>
</td></tr>
<tr><td><code id="predict.randomForest_+3A_predict.all">predict.all</code></td>
<td>
<p>Should the predictions of all trees be kept?</p>
</td></tr>
<tr><td><code id="predict.randomForest_+3A_proximity">proximity</code></td>
<td>
<p>Should proximity measures be computed?  An error is
issued if <code>object$type</code> is <code>regression</code>.</p>
</td></tr>
<tr><td><code id="predict.randomForest_+3A_nodes">nodes</code></td>
<td>
<p>Should the terminal node indicators (an n by ntree
matrix) be return?  If so, it is in the &ldquo;nodes&rdquo; attribute of the
returned object.</p>
</td></tr>
<tr><td><code id="predict.randomForest_+3A_cutoff">cutoff</code></td>
<td>
<p>(Classification only)  A vector of length equal to
number of classes.  The &lsquo;winning&rsquo; class for an observation is the
one with the maximum ratio of proportion of votes to cutoff.
Default is taken from the <code>forest$cutoff</code> component of
<code>object</code> (i.e., the setting used when running
<code><a href="#topic+randomForest">randomForest</a></code>).</p>
</td></tr>
<tr><td><code id="predict.randomForest_+3A_...">...</code></td>
<td>
<p>not used currently.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>object$type</code> is <code>regression</code>, a vector of predicted
values is returned.  If <code>predict.all=TRUE</code>, then the returned
object is a list of two components: <code>aggregate</code>, which is the
vector of predicted values by the forest, and <code>individual</code>, which
is a matrix where each column contains prediction by a tree in the
forest.
</p>
<p>If <code>object$type</code> is <code>classification</code>, the object returned
depends on the argument <code>type</code>:
</p>
<table role = "presentation">
<tr><td><code>response</code></td>
<td>
<p>predicted classes (the classes with majority vote).</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>matrix of class probabilities (one column for each class
and one row for each input).</p>
</td></tr>
<tr><td><code>vote</code></td>
<td>
<p>matrix of vote counts (one column for each class
and one row for each new input); either in raw counts or in fractions
(if <code>norm.votes=TRUE</code>).</p>
</td></tr>
</table>
<p>If <code>predict.all=TRUE</code>, then the <code>individual</code> component of the
returned object is a character matrix where each column contains the
predicted class by a tree in the forest.
</p>
<p>If <code>proximity=TRUE</code>, the returned object is a list with two
components: <code>pred</code> is the prediction (as described above) and
<code>proximity</code> is the proximitry matrix.  An error is issued if
<code>object$type</code> is <code>regression</code>.
</p>
<p>If <code>nodes=TRUE</code>, the returned object has a &ldquo;nodes&rdquo; attribute,
which is an n by ntree matrix, each column containing the node number
that the cases fall in for that tree.
</p>
<p>NOTE: If the <code>object</code> inherits from <code>randomForest.formula</code>,
then any data with <code>NA</code> are silently omitted from the prediction.
The returned value will contain <code>NA</code> correspondingly in the
aggregated and individual tree predictions (if requested), but not in
the proximity or node matrices.
</p>
<p>NOTE2: Any ties are broken at random, so if this is undesirable, avoid it by
using odd number <code>ntree</code> in <code>randomForest()</code>.
</p>


<h3>Author(s)</h3>

<p> Andy Liaw <a href="mailto:andy_liaw@merck.com">andy_liaw@merck.com</a> and Matthew Wiener
<a href="mailto:matthew_wiener@merck.com">matthew_wiener@merck.com</a>, based on original Fortran code by
Leo Breiman and Adele Cutler.</p>


<h3>References</h3>

<p>Breiman, L. (2001), <em>Random Forests</em>, Machine Learning 45(1),
5-32.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+randomForest">randomForest</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
set.seed(111)
ind &lt;- sample(2, nrow(iris), replace = TRUE, prob=c(0.8, 0.2))
iris.rf &lt;- randomForest(Species ~ ., data=iris[ind == 1,])
iris.pred &lt;- predict(iris.rf, iris[ind == 2,])
table(observed = iris[ind==2, "Species"], predicted = iris.pred)
## Get prediction for all trees.
predict(iris.rf, iris[ind == 2,], predict.all=TRUE)
## Proximities.
predict(iris.rf, iris[ind == 2,], proximity=TRUE)
## Nodes matrix.
str(attr(predict(iris.rf, iris[ind == 2,], nodes=TRUE), "nodes"))
</code></pre>

<hr>
<h2 id='randomForest'>Classification and Regression with Random Forest</h2><span id='topic+randomForest'></span><span id='topic+randomForest.formula'></span><span id='topic+randomForest.default'></span><span id='topic+print.randomForest'></span>

<h3>Description</h3>

<p><code>randomForest</code> implements Breiman's random forest algorithm (based on
Breiman and Cutler's original Fortran code) for classification and
regression.  It can also be used in unsupervised mode for assessing
proximities among data points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'formula'
randomForest(formula, data=NULL, ..., subset, na.action=na.fail)
## Default S3 method:
randomForest(x, y=NULL,  xtest=NULL, ytest=NULL, ntree=500,
             mtry=if (!is.null(y) &amp;&amp; !is.factor(y))
             max(floor(ncol(x)/3), 1) else floor(sqrt(ncol(x))),
             weights=NULL,
             replace=TRUE, classwt=NULL, cutoff, strata,
             sampsize = if (replace) nrow(x) else ceiling(.632*nrow(x)),
             nodesize = if (!is.null(y) &amp;&amp; !is.factor(y)) 5 else 1,
             maxnodes = NULL,
             importance=FALSE, localImp=FALSE, nPerm=1,
             proximity, oob.prox=proximity,
             norm.votes=TRUE, do.trace=FALSE,
             keep.forest=!is.null(y) &amp;&amp; is.null(xtest), corr.bias=FALSE,
             keep.inbag=FALSE, ...)
## S3 method for class 'randomForest'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="randomForest_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the model.
By default the variables are taken from the environment which
<code>randomForest</code> is called from.</p>
</td></tr>
<tr><td><code id="randomForest_+3A_subset">subset</code></td>
<td>
<p>an index vector indicating which rows should be used.
(NOTE: If given, this argument must be named.)</p>
</td></tr>
<tr><td><code id="randomForest_+3A_na.action">na.action</code></td>
<td>
<p>A function to specify the action to be taken if NAs
are found.  (NOTE: If given, this argument must be named.)</p>
</td></tr>
<tr><td><code id="randomForest_+3A_x">x</code>, <code id="randomForest_+3A_formula">formula</code></td>
<td>
<p>a data frame or a matrix of predictors, or a formula
describing the model to be fitted (for the
<code>print</code> method, an <code>randomForest</code> object).</p>
</td></tr>
<tr><td><code id="randomForest_+3A_y">y</code></td>
<td>
<p>A response vector.  If a factor, classification is assumed,
otherwise regression is assumed.  If omitted, <code>randomForest</code>
will run in unsupervised mode.</p>
</td></tr>
<tr><td><code id="randomForest_+3A_xtest">xtest</code></td>
<td>
<p>a data frame or matrix (like <code>x</code>) containing
predictors for the test set.</p>
</td></tr>
<tr><td><code id="randomForest_+3A_ytest">ytest</code></td>
<td>
<p>response for the test set.</p>
</td></tr>
<tr><td><code id="randomForest_+3A_ntree">ntree</code></td>
<td>
<p>Number of trees to grow.  This should not be set to too
small a number, to ensure that every input row gets predicted at
least a few times. </p>
</td></tr>
<tr><td><code id="randomForest_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables randomly sampled as candidates at each
split.  Note that the default values are different for
classification (sqrt(p) where p is number of variables in <code>x</code>)
and regression (p/3)</p>
</td></tr>
<tr><td><code id="randomForest_+3A_weights">weights</code></td>
<td>
<p>A vector of length same as <code>y</code> that are positive 
weights used only in sampling data to grow each tree (not used in any
other calculation)</p>
</td></tr>
<tr><td><code id="randomForest_+3A_replace">replace</code></td>
<td>
<p>Should sampling of cases be done with or without
replacement?</p>
</td></tr>
<tr><td><code id="randomForest_+3A_classwt">classwt</code></td>
<td>
<p>Priors of the classes.  Need not add up to one.
Ignored for regression.</p>
</td></tr>
<tr><td><code id="randomForest_+3A_cutoff">cutoff</code></td>
<td>
<p>(Classification only)  A vector of length equal to
number of classes.  The &lsquo;winning&rsquo; class for an observation is the
one with the maximum ratio of proportion of votes to cutoff.
Default is 1/k where k is the number of classes (i.e., majority vote
wins).</p>
</td></tr>
<tr><td><code id="randomForest_+3A_strata">strata</code></td>
<td>
<p>A (factor) variable that is used for stratified sampling.</p>
</td></tr>
<tr><td><code id="randomForest_+3A_sampsize">sampsize</code></td>
<td>
<p>Size(s) of sample to draw.  For classification, if
sampsize is a vector of the length the number of strata, then
sampling is stratified by strata, and the elements of sampsize
indicate the numbers to be drawn from the strata.</p>
</td></tr>
<tr><td><code id="randomForest_+3A_nodesize">nodesize</code></td>
<td>
<p>Minimum size of terminal nodes.  Setting this number
larger causes smaller trees to be grown (and thus take less time).
Note that the default values are different for classification (1)
and regression (5).</p>
</td></tr>
<tr><td><code id="randomForest_+3A_maxnodes">maxnodes</code></td>
<td>
<p>Maximum number of terminal nodes trees in the forest
can have.  If not given, trees are grown to the maximum possible
(subject to limits by <code>nodesize</code>).  If set larger than maximum
possible, a warning is issued.</p>
</td></tr>
<tr><td><code id="randomForest_+3A_importance">importance</code></td>
<td>
<p>Should importance of predictors be assessed? </p>
</td></tr>
<tr><td><code id="randomForest_+3A_localimp">localImp</code></td>
<td>
<p>Should casewise importance measure be computed?
(Setting this to <code>TRUE</code> will override <code>importance</code>.) </p>
</td></tr>
<tr><td><code id="randomForest_+3A_nperm">nPerm</code></td>
<td>
<p>Number of times the OOB data are permuted per tree for
assessing variable importance.  Number larger than 1 gives slightly
more stable estimate, but not very effective.  Currently only
implemented for regression.</p>
</td></tr>
<tr><td><code id="randomForest_+3A_proximity">proximity</code></td>
<td>
<p>Should proximity measure among the rows be
calculated?</p>
</td></tr>
<tr><td><code id="randomForest_+3A_oob.prox">oob.prox</code></td>
<td>
<p>Should proximity be calculated only on &ldquo;out-of-bag&rdquo;
data?</p>
</td></tr>
<tr><td><code id="randomForest_+3A_norm.votes">norm.votes</code></td>
<td>
<p>If <code>TRUE</code> (default), the final result of votes
are expressed as fractions.  If <code>FALSE</code>, raw vote counts are
returned (useful for combining results from different runs).
Ignored for regression.</p>
</td></tr>
<tr><td><code id="randomForest_+3A_do.trace">do.trace</code></td>
<td>
<p>If set to <code>TRUE</code>, give a more verbose output as
<code>randomForest</code> is run.  If set to some integer, then running
output is printed for every <code>do.trace</code> trees.</p>
</td></tr>
<tr><td><code id="randomForest_+3A_keep.forest">keep.forest</code></td>
<td>
<p>If set to <code>FALSE</code>, the forest will not be
retained in the output object.  If <code>xtest</code> is given, defaults
to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="randomForest_+3A_corr.bias">corr.bias</code></td>
<td>
<p>perform bias correction for regression?  Note:
Experimental.  Use at your own risk.</p>
</td></tr>
<tr><td><code id="randomForest_+3A_keep.inbag">keep.inbag</code></td>
<td>
<p>Should an <code>n</code> by <code>ntree</code> matrix be
returned that keeps track of which samples are &ldquo;in-bag&rdquo; in which
trees (but not how many times, if sampling with replacement)</p>
</td></tr>
<tr><td><code id="randomForest_+3A_...">...</code></td>
<td>
<p>optional parameters to be passed to the low level function
<code>randomForest.default</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>randomForest</code>, which is a list with the
following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the original call to <code>randomForest</code></p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>one of <code>regression</code>, <code>classification</code>, or
<code>unsupervised</code>.</p>
</td></tr>
<tr><td><code>predicted</code></td>
<td>
<p>the predicted values of the input data based on
out-of-bag samples.</p>
</td></tr>
<tr><td><code>importance</code></td>
<td>
<p>a matrix with <code>nclass</code> + 2 (for classification)
or two (for regression) columns.  For classification, the first
<code>nclass</code> columns are the class-specific measures computed as
mean descrease in accuracy.  The <code>nclass</code> + 1st column is the
mean descrease in accuracy over all classes.  The last column is the
mean decrease in Gini index.  For Regression, the first column is
the mean decrease in accuracy and the second the mean decrease in MSE.
If <code>importance=FALSE</code>, the last measure is still returned as a
vector.</p>
</td></tr>
<tr><td><code>importanceSD</code></td>
<td>
<p>The &ldquo;standard errors&rdquo; of the permutation-based
importance measure.  For classification, a <code>p</code> by <code>nclass
      + 1</code> matrix corresponding to the first <code>nclass + 1</code> columns
of the importance matrix.  For regression, a length <code>p</code> vector.</p>
</td></tr>
<tr><td><code>localImp</code></td>
<td>
<p>a p by n matrix containing the casewise importance
measures, the [i,j] element of which is the importance of i-th
variable on the j-th case. <code>NULL</code> if <code>localImp=FALSE</code>.</p>
</td></tr>
<tr><td><code>ntree</code></td>
<td>
<p>number of trees grown.</p>
</td></tr>
<tr><td><code>mtry</code></td>
<td>
<p>number of predictors sampled for spliting at each node.</p>
</td></tr>
<tr><td><code>forest</code></td>
<td>
<p>(a list that contains the entire forest; <code>NULL</code> if
<code>randomForest</code> is run in unsupervised mode or if
<code>keep.forest=FALSE</code>.</p>
</td></tr>
<tr><td><code>err.rate</code></td>
<td>
<p>(classification only) vector error rates of the
prediction on the input data, the i-th element being the (OOB) error rate
for all trees up to the i-th.</p>
</td></tr>
<tr><td><code>confusion</code></td>
<td>
<p>(classification only) the confusion matrix of the
prediction (based on OOB data).</p>
</td></tr>
<tr><td><code>votes</code></td>
<td>
<p>(classification only) a matrix with one row for each
input data point and one column for each class, giving the fraction
or number of (OOB) &lsquo;votes&rsquo; from the random forest.</p>
</td></tr>
<tr><td><code>oob.times</code></td>
<td>
<p>number of times cases are &lsquo;out-of-bag&rsquo; (and thus used
in computing OOB error estimate)</p>
</td></tr>
<tr><td><code>proximity</code></td>
<td>
<p>if <code>proximity=TRUE</code> when
<code>randomForest</code> is called, a matrix of proximity measures among
the input (based on the frequency that pairs of data points are in
the same terminal nodes).</p>
</td></tr>
<tr><td><code>mse</code></td>
<td>
<p>(regression only) vector of mean square errors: sum of squared
residuals divided by <code>n</code>.</p>
</td></tr>
<tr><td><code>rsq</code></td>
<td>
<p>(regression only) &ldquo;pseudo R-squared&rdquo;: 1 - <code>mse</code> /
Var(y).</p>
</td></tr>
<tr><td><code>test</code></td>
<td>
<p>if test set is given (through the <code>xtest</code> or additionally
<code>ytest</code> arguments), this component is a list which contains the
corresponding <code>predicted</code>, <code>err.rate</code>, <code>confusion</code>,
<code>votes</code> (for classification) or <code>predicted</code>, <code>mse</code> and
<code>rsq</code> (for regression) for the test set.  If
<code>proximity=TRUE</code>, there is also a component, <code>proximity</code>,
which contains the proximity among the test set as well as proximity
between test and training data.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The <code>forest</code> structure is slightly different between
classification and regression.  For details on how the trees are
stored, see the help page for <code><a href="#topic+getTree">getTree</a></code>.
</p>
<p>If <code>xtest</code> is given, prediction of the test set is done &ldquo;in
place&rdquo; as the trees are grown.  If <code>ytest</code> is also given, and
<code>do.trace</code> is set to some positive integer, then for every
<code>do.trace</code> trees, the test set error is printed.  Results for the
test set is returned in the <code>test</code> component of the resulting
<code>randomForest</code> object.  For classification, the <code>votes</code>
component (for training or test set data) contain the votes the cases
received for the classes.  If <code>norm.votes=TRUE</code>, the fraction is
given, which can be taken as predicted probabilities for the classes.
</p>
<p>For large data sets, especially those with large number of variables,
calling <code>randomForest</code> via the formula interface is not advised:
There may be too much overhead in handling the formula.
</p>
<p>The &ldquo;local&rdquo; (or casewise) variable importance is computed as
follows:  For classification, it is the increase in percent of times a
case is OOB and misclassified when the variable is permuted.  For
regression, it is the average increase in squared OOB residuals when
the variable is permuted.
</p>


<h3>Author(s)</h3>

<p>Andy Liaw <a href="mailto:andy_liaw@merck.com">andy_liaw@merck.com</a> and Matthew Wiener
<a href="mailto:matthew_wiener@merck.com">matthew_wiener@merck.com</a>, based on original Fortran code by
Leo Breiman and Adele Cutler.</p>


<h3>References</h3>

<p>Breiman, L. (2001), <em>Random Forests</em>, Machine Learning 45(1),
5-32.
</p>
<p>Breiman, L (2002), &ldquo;Manual On Setting Up, Using, And Understanding
Random Forests V3.1&rdquo;, <a href="https://www.stat.berkeley.edu/~breiman/Using_random_forests_V3.1.pdf">https://www.stat.berkeley.edu/~breiman/Using_random_forests_V3.1.pdf</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.randomForest">predict.randomForest</a></code>, <code><a href="#topic+varImpPlot">varImpPlot</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Classification:
##data(iris)
set.seed(71)
iris.rf &lt;- randomForest(Species ~ ., data=iris, importance=TRUE,
                        proximity=TRUE)
print(iris.rf)
## Look at variable importance:
round(importance(iris.rf), 2)
## Do MDS on 1 - proximity:
iris.mds &lt;- cmdscale(1 - iris.rf$proximity, eig=TRUE)
op &lt;- par(pty="s")
pairs(cbind(iris[,1:4], iris.mds$points), cex=0.6, gap=0,
      col=c("red", "green", "blue")[as.numeric(iris$Species)],
      main="Iris Data: Predictors and MDS of Proximity Based on RandomForest")
par(op)
print(iris.mds$GOF)

## The `unsupervised' case:
set.seed(17)
iris.urf &lt;- randomForest(iris[, -5])
MDSplot(iris.urf, iris$Species)

## stratified sampling: draw 20, 30, and 20 of the species to grow each tree.
(iris.rf2 &lt;- randomForest(iris[1:4], iris$Species, 
                          sampsize=c(20, 30, 20)))

## Regression:
## data(airquality)
set.seed(131)
ozone.rf &lt;- randomForest(Ozone ~ ., data=airquality, mtry=3,
                         importance=TRUE, na.action=na.omit)
print(ozone.rf)
## Show "importance" of variables: higher value mean more important:
round(importance(ozone.rf), 2)

## "x" can be a matrix instead of a data frame:
set.seed(17)
x &lt;- matrix(runif(5e2), 100)
y &lt;- gl(2, 50)
(myrf &lt;- randomForest(x, y))
(predict(myrf, x))

## "complicated" formula:
(swiss.rf &lt;- randomForest(sqrt(Fertility) ~ . - Catholic + I(Catholic &lt; 50),
                          data=swiss))
(predict(swiss.rf, swiss))
## Test use of 32-level factor as a predictor:
set.seed(1)
x &lt;- data.frame(x1=gl(53, 10), x2=runif(530), y=rnorm(530))
(rf1 &lt;- randomForest(x[-3], x[[3]], ntree=10))

## Grow no more than 4 nodes per tree:
(treesize(randomForest(Species ~ ., data=iris, maxnodes=4, ntree=30)))

## test proximity in regression
iris.rrf &lt;- randomForest(iris[-1], iris[[1]], ntree=101, proximity=TRUE, oob.prox=FALSE)
str(iris.rrf$proximity)

## Using weights: make versicolors having 3 times larger weights
iris_wt &lt;- ifelse( iris$Species == "versicolor", 3, 1 )
set.seed(15)
iris.wcrf &lt;- randomForest(iris[-5], iris[[5]], weights=iris_wt, keep.inbag=TRUE)
print(rowSums(iris.wcrf$inbag))
set.seed(15)
iris.wrrf &lt;- randomForest(iris[-1], iris[[1]], weights=iris_wt, keep.inbag=TRUE)
print(rowSums(iris.wrrf$inbag))
</code></pre>

<hr>
<h2 id='rfcv'>Random Forest Cross-Valdidation for feature selection</h2><span id='topic+rfcv'></span>

<h3>Description</h3>

<p>This function shows the cross-validated prediction performance of
models with sequentially reduced number of predictors (ranked by
variable importance) via a nested cross-validation procedure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rfcv(trainx, trainy, cv.fold=5, scale="log", step=0.5,
     mtry=function(p) max(1, floor(sqrt(p))), recursive=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rfcv_+3A_trainx">trainx</code></td>
<td>
<p>matrix or data frame containing columns of predictor
variables</p>
</td></tr>
<tr><td><code id="rfcv_+3A_trainy">trainy</code></td>
<td>
<p>vector of response, must have length equal to the number
of rows in <code>trainx</code></p>
</td></tr>
<tr><td><code id="rfcv_+3A_cv.fold">cv.fold</code></td>
<td>
<p>number of folds in the cross-validation</p>
</td></tr>
<tr><td><code id="rfcv_+3A_scale">scale</code></td>
<td>
<p>if <code>"log"</code>, reduce a fixed proportion (<code>step</code>)
of variables at each step, otherwise reduce <code>step</code> variables at a
time</p>
</td></tr>
<tr><td><code id="rfcv_+3A_step">step</code></td>
<td>
<p>if <code>log=TRUE</code>, the fraction of variables to remove at
each step, else remove this many variables at a time</p>
</td></tr>
<tr><td><code id="rfcv_+3A_mtry">mtry</code></td>
<td>
<p>a function of number of remaining predictor variables to
use as the <code>mtry</code> parameter in the <code>randomForest</code> call</p>
</td></tr>
<tr><td><code id="rfcv_+3A_recursive">recursive</code></td>
<td>
<p>whether variable importance is (re-)assessed at each
step of variable reduction</p>
</td></tr>
<tr><td><code id="rfcv_+3A_...">...</code></td>
<td>
<p>other arguments passed on to <code>randomForest</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components:
</p>
<p>list(n.var=n.var, error.cv=error.cv, predicted=cv.pred)
</p>
<table role = "presentation">
<tr><td><code>n.var</code></td>
<td>
<p>vector of number of variables used at each step</p>
</td></tr>
<tr><td><code>error.cv</code></td>
<td>
<p>corresponding vector of error rates or MSEs at each
step</p>
</td></tr>
<tr><td><code>predicted</code></td>
<td>
<p>list of <code>n.var</code> components, each containing
the predicted values from the cross-validation</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andy Liaw</p>


<h3>References</h3>

<p>Svetnik, V., Liaw, A., Tong, C. and Wang, T., &ldquo;Application of Breiman's
Random Forest to Modeling Structure-Activity Relationships of
Pharmaceutical Molecules&rdquo;, MCS 2004, Roli, F. and Windeatt, T. (Eds.)
pp. 334-343.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+randomForest">randomForest</a></code>, <code><a href="#topic+importance">importance</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(647)
myiris &lt;- cbind(iris[1:4], matrix(runif(96 * nrow(iris)), nrow(iris), 96))
result &lt;- rfcv(myiris, iris$Species, cv.fold=3)
with(result, plot(n.var, error.cv, log="x", type="o", lwd=2))

## The following can take a while to run, so if you really want to try
## it, copy and paste the code into R.

## Not run: 
result &lt;- replicate(5, rfcv(myiris, iris$Species), simplify=FALSE)
error.cv &lt;- sapply(result, "[[", "error.cv")
matplot(result[[1]]$n.var, cbind(rowMeans(error.cv), error.cv), type="l",
        lwd=c(2, rep(1, ncol(error.cv))), col=1, lty=1, log="x",
        xlab="Number of variables", ylab="CV Error")

## End(Not run)
</code></pre>

<hr>
<h2 id='rfImpute'>Missing Value Imputations by randomForest</h2><span id='topic+rfImpute'></span><span id='topic+rfImpute.formula'></span><span id='topic+rfImpute.default'></span>

<h3>Description</h3>

<p>Impute missing values in predictor data using proximity from randomForest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
rfImpute(x, y, iter=5, ntree=300, ...)
## S3 method for class 'formula'
rfImpute(x, data, ..., subset)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rfImpute_+3A_x">x</code></td>
<td>
<p>A data frame or matrix of predictors, some containing
<code>NA</code>s, or a formula.</p>
</td></tr>
<tr><td><code id="rfImpute_+3A_y">y</code></td>
<td>
<p>Response vector (<code>NA</code>'s not allowed).</p>
</td></tr>
<tr><td><code id="rfImpute_+3A_data">data</code></td>
<td>
<p>A data frame containing the predictors and response.</p>
</td></tr>
<tr><td><code id="rfImpute_+3A_iter">iter</code></td>
<td>
<p>Number of iterations to run the imputation.</p>
</td></tr>
<tr><td><code id="rfImpute_+3A_ntree">ntree</code></td>
<td>
<p>Number of trees to grow in each iteration of
randomForest.</p>
</td></tr>
<tr><td><code id="rfImpute_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to
<code><a href="#topic+randomForest">randomForest</a></code>.</p>
</td></tr>
<tr><td><code id="rfImpute_+3A_subset">subset</code></td>
<td>
<p>A logical vector indicating which observations to use.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm starts by imputing <code>NA</code>s using
<code><a href="#topic+na.roughfix">na.roughfix</a></code>.  Then <code><a href="#topic+randomForest">randomForest</a></code> is called
with the completed data.  The proximity matrix from the randomForest
is used to update the imputation of the <code>NA</code>s.  For continuous
predictors, the imputed value is the weighted average of the
non-missing obervations, where the weights are the proximities.  For
categorical predictors, the imputed value is the category with the
largest average proximity.  This process is iterated <code>iter</code>
times.
</p>
<p>Note: Imputation has not (yet) been implemented for the unsupervised
case.  Also, Breiman (2003) notes that the OOB estimate of error from
randomForest tend to be optimistic when run on the data matrix with
imputed values.
</p>


<h3>Value</h3>

<p>A data frame or matrix containing the completed data matrix, where
<code>NA</code>s are imputed using proximity from randomForest.  The first
column contains the response.
</p>


<h3>Author(s)</h3>

<p>Andy Liaw</p>


<h3>References</h3>

<p>Leo Breiman (2003).  Manual for Setting Up, Using, and Understanding
Random Forest V4.0.
<a href="https://www.stat.berkeley.edu/~breiman/Using_random_forests_v4.0.pdf">https://www.stat.berkeley.edu/~breiman/Using_random_forests_v4.0.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+na.roughfix">na.roughfix</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
iris.na &lt;- iris
set.seed(111)
## artificially drop some data values.
for (i in 1:4) iris.na[sample(150, sample(20, 1)), i] &lt;- NA
set.seed(222)
iris.imputed &lt;- rfImpute(Species ~ ., iris.na)
set.seed(333)
iris.rf &lt;- randomForest(Species ~ ., iris.imputed)
print(iris.rf)
</code></pre>

<hr>
<h2 id='rfNews'>Show the NEWS file</h2><span id='topic+rfNews'></span>

<h3>Description</h3>

<p>Show the NEWS file of the randomForest package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rfNews()
</code></pre>


<h3>Value</h3>

<p>None.
</p>

<hr>
<h2 id='treesize'>Size of trees in an ensemble</h2><span id='topic+treesize'></span>

<h3>Description</h3>

<p>Size of trees (number of nodes) in and ensemble.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>treesize(x, terminal=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="treesize_+3A_x">x</code></td>
<td>
<p>an object of class <code>randomForest</code>, which contains a
<code>forest</code> component.</p>
</td></tr>
<tr><td><code id="treesize_+3A_terminal">terminal</code></td>
<td>
<p>count terminal nodes only (<code>TRUE</code>) or all nodes
(<code>FALSE</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector containing number of nodes for the trees in the
<code>randomForest</code> object.
</p>


<h3>Note</h3>

<p>The <code>randomForest</code> object must contain the <code>forest</code>
component; i.e., created with <code>randomForest(...,
    keep.forest=TRUE)</code>. 
</p>


<h3>Author(s)</h3>

<p>Andy Liaw <a href="mailto:andy_liaw@merck.com">andy_liaw@merck.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+randomForest">randomForest</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
iris.rf &lt;- randomForest(Species ~ ., iris)
hist(treesize(iris.rf))
</code></pre>

<hr>
<h2 id='tuneRF'>Tune randomForest for the optimal mtry parameter</h2><span id='topic+tuneRF'></span>

<h3>Description</h3>

<p>Starting with the default value of mtry, search for the optimal value
(with respect to Out-of-Bag error estimate) of mtry for randomForest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuneRF(x, y, mtryStart, ntreeTry=50, stepFactor=2, improve=0.05,
       trace=TRUE, plot=TRUE, doBest=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tuneRF_+3A_x">x</code></td>
<td>
<p>matrix or data frame of predictor variables</p>
</td></tr>
<tr><td><code id="tuneRF_+3A_y">y</code></td>
<td>
<p>response vector (factor for classification, numeric for
regression)</p>
</td></tr>
<tr><td><code id="tuneRF_+3A_mtrystart">mtryStart</code></td>
<td>
<p>starting value of mtry; default is the same as in
<code><a href="#topic+randomForest">randomForest</a></code></p>
</td></tr>
<tr><td><code id="tuneRF_+3A_ntreetry">ntreeTry</code></td>
<td>
<p>number of trees used at the tuning step</p>
</td></tr>
<tr><td><code id="tuneRF_+3A_stepfactor">stepFactor</code></td>
<td>
<p>at each iteration, mtry is inflated (or deflated) by
this value</p>
</td></tr>
<tr><td><code id="tuneRF_+3A_improve">improve</code></td>
<td>
<p>the (relative) improvement in OOB error must be by this
much for the search to continue</p>
</td></tr>
<tr><td><code id="tuneRF_+3A_trace">trace</code></td>
<td>
<p>whether to print the progress of the search</p>
</td></tr>
<tr><td><code id="tuneRF_+3A_plot">plot</code></td>
<td>
<p>whether to plot the OOB error as function of mtry</p>
</td></tr>
<tr><td><code id="tuneRF_+3A_dobest">doBest</code></td>
<td>
<p>whether to run a forest using the optimal mtry found</p>
</td></tr>
<tr><td><code id="tuneRF_+3A_...">...</code></td>
<td>
<p>options to be given to <code><a href="#topic+randomForest">randomForest</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>doBest=FALSE</code> (default), it returns a matrix whose first
column contains the mtry values searched, and the second column the
corresponding OOB error.
</p>
<p>If <code>doBest=TRUE</code>, it returns the <code><a href="#topic+randomForest">randomForest</a></code>
object produced with the optimal <code>mtry</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+randomForest">randomForest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(fgl, package="MASS")
fgl.res &lt;- tuneRF(fgl[,-10], fgl[,10], stepFactor=1.5)
</code></pre>

<hr>
<h2 id='varImpPlot'>Variable Importance Plot</h2><span id='topic+varImpPlot'></span>

<h3>Description</h3>

<p>Dotchart of variable importance as measured by a Random Forest
</p>


<h3>Usage</h3>

<pre><code class='language-R'>varImpPlot(x, sort=TRUE, n.var=min(30, nrow(x$importance)),
           type=NULL, class=NULL, scale=TRUE, 
           main=deparse(substitute(x)), ...) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="varImpPlot_+3A_x">x</code></td>
<td>
<p>An object of class <code>randomForest</code>.</p>
</td></tr>
<tr><td><code id="varImpPlot_+3A_sort">sort</code></td>
<td>
<p>Should the variables be sorted in decreasing order of
importance?</p>
</td></tr>
<tr><td><code id="varImpPlot_+3A_n.var">n.var</code></td>
<td>
<p>How many variables to show? (Ignored if
<code>sort=FALSE</code>.)</p>
</td></tr>
<tr><td><code id="varImpPlot_+3A_type">type</code>, <code id="varImpPlot_+3A_class">class</code>, <code id="varImpPlot_+3A_scale">scale</code></td>
<td>
<p>arguments to be passed on to
<code><a href="#topic+importance">importance</a></code></p>
</td></tr>
<tr><td><code id="varImpPlot_+3A_main">main</code></td>
<td>
<p>plot title.</p>
</td></tr>
<tr><td><code id="varImpPlot_+3A_...">...</code></td>
<td>
<p>Other graphical parameters to be passed on to
<code><a href="graphics.html#topic+dotchart">dotchart</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly, the importance of the variables that were plotted.
</p>


<h3>Author(s)</h3>

<p>Andy Liaw <a href="mailto:andy_liaw@merck.com">andy_liaw@merck.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+randomForest">randomForest</a></code>, <code><a href="#topic+importance">importance</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(4543)
data(mtcars)
mtcars.rf &lt;- randomForest(mpg ~ ., data=mtcars, ntree=1000, keep.forest=FALSE,
                          importance=TRUE)
varImpPlot(mtcars.rf)
</code></pre>

<hr>
<h2 id='varUsed'>Variables used in a random forest</h2><span id='topic+varUsed'></span>

<h3>Description</h3>

<p>Find out which predictor variables are actually used in the random forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>varUsed(x, by.tree=FALSE, count=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="varUsed_+3A_x">x</code></td>
<td>
<p>An object of class <code>randomForest</code>.</p>
</td></tr>
<tr><td><code id="varUsed_+3A_by.tree">by.tree</code></td>
<td>
<p>Should the list of variables used be broken down by
trees in the forest?</p>
</td></tr>
<tr><td><code id="varUsed_+3A_count">count</code></td>
<td>
<p>Should the frequencies that variables appear in trees be
returned?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>count=TRUE</code> and <code>by.tree=FALSE</code>, a integer vector containing
frequencies that variables are used in the forest.  If
<code>by.tree=TRUE</code>, a matrix is returned, breaking down the counts by
tree (each column corresponding to one tree and each row to a variable).
</p>
<p>If <code>count=FALSE</code> and <code>by.tree=TRUE</code>, a list of integer
indices is returned giving the variables used in the trees, else if
<code>by.tree=FALSE</code>, a vector of integer indices giving the
variables used in the entire forest.
</p>


<h3>Author(s)</h3>

<p>Andy Liaw</p>


<h3>See Also</h3>

<p><code><a href="#topic+randomForest">randomForest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
set.seed(17)
varUsed(randomForest(Species~., iris, ntree=100))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
