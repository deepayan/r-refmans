<!DOCTYPE html><html><head><title>Help for package planningML</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {planningML}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#featureselection'><p>Feature selection</p></a></li>
<li><a href='#fit_learningcurve'><p>Generate descriptive summary for objects returned by functions in EHRsampling</p></a></li>
<li><a href='#learningcurve_data'><p>Generate descriptive summary for objects returned by functions in EHRsampling</p></a></li>
<li><a href='#plot.planningML'><p>Plot sample size dependent AUC or MCC based on number of selected features</p></a></li>
<li><a href='#samplesize'><p>Sample size determination</p></a></li>
<li><a href='#summary.planningML'><p>Generate descriptive summary for objects returned by functions in EHRsampling</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>A Sample Size Calculator for Machine Learning Applications in
Healthcare</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Advances in automated document classification has led to identifying massive numbers of clinical concepts from handwritten clinical notes. These high dimensional clinical concepts can serve as highly informative predictors in building classification algorithms for identifying patients with different clinical conditions, commonly referred to as patient phenotyping. However, from a planning perspective, it is critical to ensure that enough data is available for the phenotyping algorithm to obtain a desired classification performance. This challenge in sample size planning is further exacerbated by the high dimension of the feature space and the inherent imbalance of the response class. Currently available sample size planning methods can be categorized into: (i) model-based approaches that predict the sample size required for achieving a desired accuracy using a linear machine learning classifier and (ii) learning curve-based approaches (Figueroa et al. (2012) &lt;<a href="https://doi.org/10.1186%2F1472-6947-12-8">doi:10.1186/1472-6947-12-8</a>&gt;) that fit an inverse power law curve to pilot data to extrapolate performance. We develop model-based approaches for imbalanced data with correlated features, deriving sample size formulas for performance metrics that are sensitive to class imbalance such as Area Under the receiver operating characteristic Curve (AUC) and Matthews Correlation Coefficient (MCC). This is done using a two-step approach where we first perform feature selection using the innovated High Criticism thresholding method (Hall and Jin (2010) &lt;<a href="https://doi.org/10.1214%2F09-AOS764">doi:10.1214/09-AOS764</a>&gt;), then determine the sample size by optimizing the two performance metrics. Further, we develop software in the form of an R package named 'planningML' and an 'R' 'Shiny' app to facilitate the convenient implementation of the developed model-based approaches and learning curve approaches for imbalanced data. We apply our methods to the problem of phenotyping rare outcomes using the MIMIC-III electronic health record database. We show that our developed methods which relate training data size and performance on AUC and MCC, can predict the true or observed performance from linear ML classifiers such as LASSO and SVM at different training data sizes. Therefore, in high-dimensional classification analysis with imbalanced data and correlated features, our approach can efficiently and accurately determine the sample size needed for machine-learning based classification.</td>
</tr>
<tr>
<td>Imports:</td>
<td>glmnet, caret, lubridate, Matrix, MESS, dplyr, pROC, stats</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr,rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-06-23 04:42:17 UTC; xfang</td>
</tr>
<tr>
<td>Author:</td>
<td>Xinying Fang [aut, cre],
  Satabdi Saha [aut],
  Jaejoon Song [aut],
  Sai Dharmarajan [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Xinying Fang &lt;fxy950225@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-06-23 05:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='featureselection'>Feature selection</h2><span id='topic+featureselection'></span>

<h3>Description</h3>

<p>This function selects important features from the dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>featureselection(x = NULL, y = NULL, method = "iHCT")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="featureselection_+3A_x">x</code></td>
<td>
<p>a matrix of predictor variables</p>
</td></tr>
<tr><td><code id="featureselection_+3A_y">y</code></td>
<td>
<p>a vector of binary outcome</p>
</td></tr>
<tr><td><code id="featureselection_+3A_method">method</code></td>
<td>
<p>feature selection method, default is iHCT</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>featureselection()</code> returns selected features and other outcomes needed for sample size determination.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load data
#pilot.data = readRDS(system.file("extdata", "pilotdata.rds", package = "planningML"))
#x = pilot.data[,-ncol(pilot.data)]
#y = pilot.data$DEPRESSION

## select important features
#features = featureselection(x = x, y = y)
#summary(features)

</code></pre>

<hr>
<h2 id='fit_learningcurve'>Generate descriptive summary for objects returned by functions in EHRsampling</h2><span id='topic+fit_learningcurve'></span>

<h3>Description</h3>

<p>Generate descriptive summary for objects returned by functions in EHRsampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_learningcurve(df, testX, target = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_learningcurve_+3A_df">df</code></td>
<td>
<p>data for learning curve fitting; first column is sample size, second column is AUC measurement.</p>
</td></tr>
<tr><td><code id="fit_learningcurve_+3A_testx">testX</code></td>
<td>
<p>test data for prediction</p>
</td></tr>
<tr><td><code id="fit_learningcurve_+3A_target">target</code></td>
<td>
<p>target MCC/AUC that you want to achieve</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>fit_learningcurve()</code> returns the estimated power law model for the learning curve.
</p>

<hr>
<h2 id='learningcurve_data'>Generate descriptive summary for objects returned by functions in EHRsampling</h2><span id='topic+learningcurve_data'></span>

<h3>Description</h3>

<p>Generate descriptive summary for objects returned by functions in EHRsampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>learningcurve_data(
  x,
  y,
  method = "log",
  metric = "MCC",
  batchsize = 60,
  class.prob,
  pct.train = 0.8,
  nfold = 5,
  nrepeat = 10
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="learningcurve_data_+3A_x">x</code></td>
<td>
<p>a matrix of predictor variables</p>
</td></tr>
<tr><td><code id="learningcurve_data_+3A_y">y</code></td>
<td>
<p>a vector of binary outcome, encoded as a factor and denoted by 1 for events and 0 for non-events</p>
</td></tr>
<tr><td><code id="learningcurve_data_+3A_method">method</code></td>
<td>
<p>training method to get performance measurements. Available options are &quot;log&quot; (logistic regression, default),
&quot;regul.log&quot; (regularized logistic regression), &quot;svm&quot; (support vector machine), &quot;rf&quot; (random forest) and &quot;lda&quot; (linear discriminant analysis)</p>
</td></tr>
<tr><td><code id="learningcurve_data_+3A_metric">metric</code></td>
<td>
<p>default = &quot;MCC&quot;. The target performance estimation metric that you want to optimize. Other choice
can be &quot;AUC&quot;.</p>
</td></tr>
<tr><td><code id="learningcurve_data_+3A_batchsize">batchsize</code></td>
<td>
<p>sample size for each training batch</p>
</td></tr>
<tr><td><code id="learningcurve_data_+3A_class.prob">class.prob</code></td>
<td>
<p>probability of the event</p>
</td></tr>
<tr><td><code id="learningcurve_data_+3A_pct.train">pct.train</code></td>
<td>
<p>the percentage of data that goes to training. Default is 0.8</p>
</td></tr>
<tr><td><code id="learningcurve_data_+3A_nfold">nfold</code></td>
<td>
<p>number of folds in cross validation</p>
</td></tr>
<tr><td><code id="learningcurve_data_+3A_nrepeat">nrepeat</code></td>
<td>
<p>number of repeats for cross validation</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>learningcurve_data()</code> returns a data frame of sample size and the corresponding performance measurements.
</p>

<hr>
<h2 id='plot.planningML'>Plot sample size dependent AUC or MCC based on number of selected features</h2><span id='topic+plot.planningML'></span>

<h3>Description</h3>

<p>Plot the output returned by samplesize function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'planningML'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.planningML_+3A_x">x</code></td>
<td>
<p>the output returned by the samplesize function</p>
</td></tr>
<tr><td><code id="plot.planningML_+3A_...">...</code></td>
<td>
<p>ignored arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>plot()</code> returns a scatterplot of sample size dependent performance measurement metrics (AUC or MCC) based on number of selected features
</p>

<hr>
<h2 id='samplesize'>Sample size determination</h2><span id='topic+samplesize'></span>

<h3>Description</h3>

<p>This function determine the optimal sample size based on the performance evaluation metric and number of selected features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>samplesize(
  features = NULL,
  sample.size = seq(10, 1000, 20),
  method = "HCT",
  m = NULL,
  effectsize = NULL,
  class.prob = NULL,
  totalnum_features = NULL,
  threshold = 0.1,
  metric = "MCC",
  target = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="samplesize_+3A_features">features</code></td>
<td>
<p>feature selection results from the featureselection function in the package.</p>
</td></tr>
<tr><td><code id="samplesize_+3A_sample.size">sample.size</code></td>
<td>
<p>sample size grid</p>
</td></tr>
<tr><td><code id="samplesize_+3A_method">method</code></td>
<td>
<p>default is HCT method, sample size dependent performance metric based on HCT method (HCT) or DS method (DS).</p>
</td></tr>
<tr><td><code id="samplesize_+3A_m">m</code></td>
<td>
<p>the number of features involved in the sample size determination. Default is NULL, which means
the number of features are determined by the featureselection results based on the iHCT method.
Otherwise, users can select the number based on their needs. The self-defined m should be smaller
than the optimal number of features determined by the featureselection function.</p>
</td></tr>
<tr><td><code id="samplesize_+3A_effectsize">effectsize</code></td>
<td>
<p>common effect size the the m features. NULL means the effect size is directly calculated from the
data. Users can also provide the effect sizes based on historical data.</p>
</td></tr>
<tr><td><code id="samplesize_+3A_class.prob">class.prob</code></td>
<td>
<p>probability of the event</p>
</td></tr>
<tr><td><code id="samplesize_+3A_totalnum_features">totalnum_features</code></td>
<td>
<p>total number of features</p>
</td></tr>
<tr><td><code id="samplesize_+3A_threshold">threshold</code></td>
<td>
<p>default = 0.1. Threshold needed to determine the sample size.</p>
</td></tr>
<tr><td><code id="samplesize_+3A_metric">metric</code></td>
<td>
<p>default = &quot;MCC&quot;. The target performance estimation metric that you want to optimize. Other choices
can be AUC.</p>
</td></tr>
<tr><td><code id="samplesize_+3A_target">target</code></td>
<td>
<p>target MCC/AUC that you want to achieve</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>samplesize()</code> returns sample size needed to achieve corresponding performance measurements.
</p>

<hr>
<h2 id='summary.planningML'>Generate descriptive summary for objects returned by functions in EHRsampling</h2><span id='topic+summary.planningML'></span>

<h3>Description</h3>

<p>Generate descriptive summary for objects returned by functions in EHRsampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'planningML'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.planningML_+3A_object">object</code></td>
<td>
<p>the object returned by other functions.</p>
</td></tr>
<tr><td><code id="summary.planningML_+3A_...">...</code></td>
<td>
<p>ignored arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>summary()</code> prints the objects returned by other functions.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
