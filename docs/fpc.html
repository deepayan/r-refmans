<!DOCTYPE html><html><head><title>Help for package fpc</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {fpc}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#adcoord'><p>Asymmetric discriminant coordinates</p></a></li>
<li><a href='#ancoord'><p>Asymmetric neighborhood based discriminant coordinates</p></a></li>
<li><a href='#awcoord'><p>Asymmetric weighted discriminant coordinates</p></a></li>
<li><a href='#batcoord'><p>Bhattacharyya discriminant projection</p></a></li>
<li><a href='#bhattacharyya.dist'><p>Bhattacharyya distance between Gaussian distributions</p></a></li>
<li><a href='#bhattacharyya.matrix'><p>Matrix of pairwise Bhattacharyya distances</p></a></li>
<li><a href='#calinhara'><p>Calinski-Harabasz index</p></a></li>
<li><a href='#can'><p>Generation of the tuning constant for regression fixed point clusters</p></a></li>
<li><a href='#cat2bin'><p>Recode nominal variables to binary variables</p></a></li>
<li><a href='#cdbw'><p>CDbw-index for cluster validation</p></a></li>
<li><a href='#cgrestandard'><p>Standardise cluster validation statistics by random clustering results</p></a></li>
<li><a href='#classifdist'><p>Classification of unclustered points</p></a></li>
<li><a href='#clucols'><p>Sets of colours and symbols for cluster plotting</p></a></li>
<li><a href='#clujaccard'><p>Jaccard similarity between logical vectors</p></a></li>
<li><a href='#clusexpect'><p>Expected value of the number of times a fixed point</p>
cluster is found</a></li>
<li><a href='#clustatsum'><p>Compute and format cluster validation statistics</p></a></li>
<li><a href='#cluster.magazine'><p>Run many clustering methods on many numbers of clusters</p></a></li>
<li><a href='#cluster.stats'><p>Cluster validation statistics</p></a></li>
<li><a href='#cluster.varstats'><p>Variablewise statistics for clusters</p></a></li>
<li><a href='#clusterbenchstats'><p>Run and validate many clusterings</p></a></li>
<li><a href='#clusterboot'><p>Clusterwise cluster stability assessment by resampling</p></a></li>
<li><a href='#cmahal'><p>Generation of tuning constant for Mahalanobis fixed point clusters.</p></a></li>
<li><a href='#con.comp'><p>Connectivity components of an undirected graph</p></a></li>
<li><a href='#confusion'><p>Misclassification probabilities in mixtures</p></a></li>
<li><a href='#cov.wml'><p>Weighted Covariance Matrices (Maximum Likelihood)</p></a></li>
<li><a href='#cqcluster.stats'><p>Cluster validation statistics (version for use with clusterbenchstats</p></a></li>
<li><a href='#cvnn'><p>Cluster validation based on nearest neighbours</p></a></li>
<li><a href='#cweight'><p>Weight function for AWC</p></a></li>
<li><a href='#dbscan'><p>DBSCAN density reachability and connectivity clustering</p></a></li>
<li><a href='#dipp.tantrum'><p>Simulates p-value for dip test</p></a></li>
<li><a href='#diptest.multi'><p>Diptest for discriminant coordinate projection</p></a></li>
<li><a href='#discrcoord'><p>Discriminant coordinates/canonical variates</p></a></li>
<li><a href='#discrete.recode'><p>Recodes mixed variables dataset</p></a></li>
<li><a href='#discrproj'><p>Linear dimension reduction for classification</p></a></li>
<li><a href='#distancefactor'><p>Factor for dissimilarity of mixed type data</p></a></li>
<li><a href='#distcritmulti'><p>Distance based validity criteria for large data sets</p></a></li>
<li><a href='#distrsimilarity'><p>Similarity of within-cluster distributions to normal and uniform</p></a></li>
<li><a href='#dridgeline'><p>Density along the ridgeline</p></a></li>
<li><a href='#dudahart2'><p>Duda-Hart test for splitting</p></a></li>
<li><a href='#extract.mixturepars'><p>Extract parameters for certain components from mclust</p></a></li>
<li><a href='#findrep'><p>Finding representatives for cluster border</p></a></li>
<li><a href='#fixmahal'><p>Mahalanobis Fixed Point Clusters</p></a></li>
<li><a href='#fixreg'><p>Linear Regression Fixed Point Clusters</p></a></li>
<li><a href='#flexmixedruns'><p>Fitting mixed Gaussian/multinomial mixtures with flexmix</p></a></li>
<li><a href='#fpc-package'><p>fpc package overview</p></a></li>
<li><a href='#fpclusters'><p>Extracting clusters from fixed point cluster objects</p></a></li>
<li><a href='#itnumber'><p>Number of regression fixed point cluster iterations</p></a></li>
<li><a href='#jittervar'><p>Jitter variables in a data matrix</p></a></li>
<li><a href='#kmeansCBI'><p>Interface functions for clustering methods</p></a></li>
<li><a href='#kmeansruns'><p>k-means with estimating k and initialisations</p></a></li>
<li><a href='#lcmixed'><p>flexmix method for mixed Gaussian/multinomial mixtures</p></a></li>
<li><a href='#localshape'><p>Local shape matrix</p></a></li>
<li><a href='#mahalanodisc'><p>Mahalanobis for AWC</p></a></li>
<li><a href='#mahalanofix'><p>Mahalanobis distances from center of indexed points</p></a></li>
<li><a href='#mahalconf'><p>Mahalanobis fixed point clusters initial configuration</p></a></li>
<li><a href='#mergenormals'><p>Clustering by merging Gaussian mixture components</p></a></li>
<li><a href='#mergeparameters'><p>New parameters from merging two Gaussian mixture components</p></a></li>
<li><a href='#minsize'><p>Minimum size of regression fixed point cluster</p></a></li>
<li><a href='#mixdens'><p>Density of multivariate Gaussian mixture, mclust parameterisation</p></a></li>
<li><a href='#mixpredictive'><p>Prediction strength of merged Gaussian mixture</p></a></li>
<li><a href='#mvdcoord'><p>Mean/variance differences discriminant coordinates</p></a></li>
<li><a href='#ncoord'><p>Neighborhood based discriminant coordinates</p></a></li>
<li><a href='#neginc'><p>Neg-entropy normality index for cluster validation</p></a></li>
<li><a href='#nselectboot'><p>Selection of the number of clusters via bootstrap</p></a></li>
<li><a href='#pamk'><p>Partitioning around medoids with estimation of number of clusters</p></a></li>
<li><a href='#piridge'><p>Ridgeline Pi-function</p></a></li>
<li><a href='#piridge.zeroes'><p>Extrema of two-component Gaussian mixture</p></a></li>
<li><a href='#plot.valstat'><p>Simulation-standardised plot and print of cluster validation statistics</p></a></li>
<li><a href='#plotcluster'><p>Discriminant projection plot.</p></a></li>
<li><a href='#prediction.strength'><p>Prediction strength for estimating number of clusters</p></a></li>
<li><a href='#randcmatrix'><p>Random partition matrix</p></a></li>
<li><a href='#randconf'><p>Generate a sample indicator vector</p></a></li>
<li><a href='#randomclustersim'><p>Simulation of validity indexes based on random clusterings</p></a></li>
<li><a href='#regmix'><p>Mixture Model ML for Clusterwise Linear Regression</p></a></li>
<li><a href='#rFace'><p>&quot;Face-shaped&quot; clustered benchmark datasets</p></a></li>
<li><a href='#ridgeline'><p>Ridgeline computation</p></a></li>
<li><a href='#ridgeline.diagnosis'><p>Ridgeline plots, ratios and unimodality</p></a></li>
<li><a href='#simmatrix'><p>Extracting intersections between clusters from fpc-object</p></a></li>
<li><a href='#solvecov'><p>Inversion of (possibly singular) symmetric matrices</p></a></li>
<li><a href='#sseg'><p>Position in a similarity vector</p></a></li>
<li><a href='#stupidkaven'><p>Stupid average dissimilarity random clustering</p></a></li>
<li><a href='#stupidkcentroids'><p>Stupid k-centroids random clustering</p></a></li>
<li><a href='#stupidkfn'><p>Stupid farthest neighbour random clustering</p></a></li>
<li><a href='#stupidknn'><p>Stupid nearest neighbour random clustering</p></a></li>
<li><a href='#tdecomp'><p>Root of singularity-corrected eigenvalue decomposition</p></a></li>
<li><a href='#tonedata'><p>Tone perception data</p></a></li>
<li><a href='#unimodal.ind'><p>Is a fitted denisity unimodal or not?</p></a></li>
<li><a href='#valstat.object'><p>Cluster validation statistics - object</p></a></li>
<li><a href='#weightplots'><p>Ordered posterior plots</p></a></li>
<li><a href='#wfu'><p>Weight function (for Mahalabobis distances)</p></a></li>
<li><a href='#xtable'><p>Partition crosstable with empty clusters</p></a></li>
<li><a href='#zmisclassification.matrix'><p>Matrix of misclassification probabilities between mixture components</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Flexible Procedures for Clustering</td>
</tr>
<tr>
<td>Version:</td>
<td>2.2-11</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-12-14</td>
</tr>
<tr>
<td>Author:</td>
<td>Christian Hennig &lt;christian.hennig@unibo.it&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS, cluster, mclust, flexmix, prabclus, class, diptest,
robustbase, kernlab, grDevices, graphics, methods, stats,
utils, parallel</td>
</tr>
<tr>
<td>Suggests:</td>
<td>tclust, pdfCluster, mvtnorm</td>
</tr>
<tr>
<td>Description:</td>
<td>Various methods for clustering and cluster validation.
  Fixed point clustering. Linear regression clustering. Clustering by 
  merging Gaussian mixture components. Symmetric 
  and asymmetric discriminant projections for visualisation of the 
  separation of groupings. Cluster validation statistics
  for distance based clustering including corrected Rand index. 
  Standardisation of cluster validation statistics by random clusterings and 
  comparison between many clustering methods and numbers of clusters based on
  this.  
  Cluster-wise cluster stability assessment. Methods for estimation of 
  the number of clusters: Calinski-Harabasz, Tibshirani and Walther's 
  prediction strength, Fang and Wang's bootstrap stability. 
  Gaussian/multinomial mixture fitting for mixed 
  continuous/categorical variables. Variable-wise statistics for cluster
  interpretation. DBSCAN clustering. Interface functions for many 
  clustering methods implemented in R, including estimating the number of
  clusters with kmeans, pam and clara. Modality diagnosis for Gaussian
  mixtures. For an overview see package?fpc.</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Christian Hennig &lt;christian.hennig@unibo.it&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-14 21:49:22 UTC; chrish</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-15 11:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='adcoord'>Asymmetric discriminant coordinates</h2><span id='topic+adcoord'></span>

<h3>Description</h3>

<p>Asymmetric discriminant coordinates as defined
in Hennig (2003). Asymmetric discriminant projection means that there
are two classes, one of which is treated as the homogeneous class
(i.e., it should appear homogeneous and separated in the resulting projection)
while the other may be heterogeneous. 
The principle is to maximize the ratio between the projection of a between
classes separation matrix and the projection of the covariance matrix
within the homogeneous class.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adcoord(xd, clvecd, clnum=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adcoord_+3A_xd">xd</code></td>
<td>
<p>the data matrix; a numerical object which can be coerced
to a matrix.</p>
</td></tr>
<tr><td><code id="adcoord_+3A_clvecd">clvecd</code></td>
<td>
<p>integer vector of class numbers; length must equal
<code>nrow(xd)</code>.</p>
</td></tr>
<tr><td><code id="adcoord_+3A_clnum">clnum</code></td>
<td>
<p>integer. Number of the homogeneous class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The square root of the homogeneous classes covariance matrix
is inverted by use of
<code><a href="#topic+tdecomp">tdecomp</a></code>, which can be expected to give
reasonable results for singular within-class covariance matrices.
</p>


<h3>Value</h3>

<p>List with the following components
</p>
<table>
<tr><td><code>ev</code></td>
<td>
<p>eigenvalues in descending order.</p>
</td></tr>
<tr><td><code>units</code></td>
<td>
<p>columns are coordinates of projection basis vectors.
New points <code>x</code> can be projected onto the projection basis vectors
by <code>x %*% units</code></p>
</td></tr>
<tr><td><code>proj</code></td>
<td>
<p>projections of <code>xd</code> onto <code>units</code>.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2004) Asymmetric linear dimension reduction for classification.
Journal of Computational and Graphical Statistics 13, 930-945 .
</p>
<p>Hennig, C. (2005)  A method for visual cluster validation.  In:
Weihs, C. and Gaul, W. (eds.): Classification - The Ubiquitous
Challenge. Springer, Heidelberg 2005, 153-160.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plotcluster">plotcluster</a></code> for straight forward discriminant plots.
<code><a href="#topic+discrproj">discrproj</a></code> for alternatives.
<code><a href="#topic+rFace">rFace</a></code> for generation of the example data used below.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(4634)
  face &lt;- rFace(600,dMoNo=2,dNoEy=0)
  grface &lt;- as.integer(attr(face,"grouping"))
  adcf &lt;- adcoord(face,grface==2)
  adcf2 &lt;- adcoord(face,grface==4)
  plot(adcf$proj,col=1+(grface==2))
  plot(adcf2$proj,col=1+(grface==4))
  # ...done in one step by function plotcluster.
</code></pre>

<hr>
<h2 id='ancoord'>Asymmetric neighborhood based discriminant coordinates</h2><span id='topic+ancoord'></span>

<h3>Description</h3>

<p>Asymmetric neighborhood based discriminant coordinates as defined
in Hennig (2003). Asymmetric discriminant projection means that there
are two classes, one of which is treated as the homogeneous class
(i.e., it should appear homogeneous and separated in the resulting projection)
while the other may be heterogeneous. 
The principle is to maximize the ratio between the projection of a between
classes covariance matrix, which is defined by averaging the
between classes covariance matrices in the neighborhoods of the points
of the homogeneous class and the projection of the covariance matrix
within the homogeneous class.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ancoord(xd, clvecd, clnum=1, nn=50, method="mcd", countmode=1000, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ancoord_+3A_xd">xd</code></td>
<td>
<p>the data matrix; a numerical object which can be coerced
to a matrix.</p>
</td></tr>
<tr><td><code id="ancoord_+3A_clvecd">clvecd</code></td>
<td>
<p>integer vector of class numbers; length must equal
<code>nrow(xd)</code>.</p>
</td></tr>
<tr><td><code id="ancoord_+3A_clnum">clnum</code></td>
<td>
<p>integer. Number of the homogeneous class.</p>
</td></tr>
<tr><td><code id="ancoord_+3A_nn">nn</code></td>
<td>
<p>integer. Number of points which belong to the neighborhood
of each point (including the point itself).</p>
</td></tr>
<tr><td><code id="ancoord_+3A_method">method</code></td>
<td>
<p>one of
&quot;mve&quot;, &quot;mcd&quot; or &quot;classical&quot;. Covariance matrix used within the
homogeneous class.
&quot;mcd&quot; and &quot;mve&quot; are robust covariance matrices as implemented
in <code><a href="MASS.html#topic+cov.rob">cov.rob</a></code>. &quot;classical&quot; refers to the classical
covariance matrix.</p>
</td></tr>
<tr><td><code id="ancoord_+3A_countmode">countmode</code></td>
<td>
<p>optional positive integer. Every <code>countmode</code>
algorithm runs <code>ancoord</code> shows a message.</p>
</td></tr>
<tr><td><code id="ancoord_+3A_...">...</code></td>
<td>
<p>no effect</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The square root of the homogeneous classes covariance matrix
is inverted by use of
<code><a href="#topic+tdecomp">tdecomp</a></code>, which can be expected to give
reasonable results for singular within-class covariance matrices.
</p>


<h3>Value</h3>

<p>List with the following components
</p>
<table>
<tr><td><code>ev</code></td>
<td>
<p>eigenvalues in descending order.</p>
</td></tr>
<tr><td><code>units</code></td>
<td>
<p>columns are coordinates of projection basis vectors.
New points <code>x</code> can be projected onto the projection basis vectors
by <code>x %*% units</code></p>
</td></tr>
<tr><td><code>proj</code></td>
<td>
<p>projections of <code>xd</code> onto <code>units</code>.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2004) Asymmetric linear dimension reduction for classification.
Journal of Computational and Graphical Statistics 13, 930-945 .
</p>
<p>Hennig, C. (2005)  A method for visual cluster validation.  In:
Weihs, C. and Gaul, W. (eds.): Classification - The Ubiquitous
Challenge. Springer, Heidelberg 2005, 153-160.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plotcluster">plotcluster</a></code> for straight forward discriminant plots.
<code><a href="#topic+discrproj">discrproj</a></code> for alternatives.
<code><a href="#topic+rFace">rFace</a></code> for generation of the example data used below.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(4634)
  face &lt;- rFace(600,dMoNo=2,dNoEy=0)
  grface &lt;- as.integer(attr(face,"grouping"))
  ancf2 &lt;- ancoord(face,grface==4)
  plot(ancf2$proj,col=1+(grface==4))
  # ...done in one step by function plotcluster.
</code></pre>

<hr>
<h2 id='awcoord'>Asymmetric weighted discriminant coordinates</h2><span id='topic+awcoord'></span>

<h3>Description</h3>

<p>Asymmetric weighted discriminant coordinates as defined
in Hennig (2003). Asymmetric discriminant projection means that there
are two classes, one of which is treated as the homogeneous class
(i.e., it should appear homogeneous and separated in the resulting projection)
while the other may be heterogeneous. 
The principle is to maximize the ratio between the projection of a between
classes separation matrix and the projection of the covariance matrix
within the homogeneous class. Points are weighted according to their
(robust) Mahalanobis distance to the homogeneous class. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>awcoord(xd, clvecd, clnum=1, mahal="square", method="classical",
                     clweight=switch(method,classical=FALSE,TRUE),
                     alpha=0.99, subsample=0, countmode=1000, ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="awcoord_+3A_xd">xd</code></td>
<td>
<p>the data matrix; a numerical object which can be coerced
to a matrix.</p>
</td></tr>
<tr><td><code id="awcoord_+3A_clvecd">clvecd</code></td>
<td>
<p>integer vector of class numbers; length must equal
<code>nrow(xd)</code>.</p>
</td></tr>
<tr><td><code id="awcoord_+3A_clnum">clnum</code></td>
<td>
<p>integer. Number of the homogeneous class.</p>
</td></tr>
<tr><td><code id="awcoord_+3A_mahal">mahal</code></td>
<td>
<p>&quot;md&quot; or &quot;square&quot;. If &quot;md&quot;, the points are weighted by the
square root of the <code>alpha</code>-quantile of the
corresponding chi squared distribution
over the roots of their Mahalanobis distance to the
homogeneous class, unless
this is smaller than 1. If &quot;square&quot; (which is recommended), the
(originally squared) Mahalanobis distance and the
unrooted quantile is used.</p>
</td></tr>
<tr><td><code id="awcoord_+3A_method">method</code></td>
<td>
<p>one of
&quot;mve&quot;, &quot;mcd&quot; or &quot;classical&quot;. Covariance matrix used within the
homogeneous class and for the computation of the Mahalanobis distances.
&quot;mcd&quot; and &quot;mve&quot; are robust covariance matrices as implemented
in <code><a href="MASS.html#topic+cov.rob">cov.rob</a></code>. &quot;classical&quot; refers to the classical
covariance matrix.</p>
</td></tr>
<tr><td><code id="awcoord_+3A_clweight">clweight</code></td>
<td>
<p>logical. If <code>FALSE</code>, only the points of the
heterogeneous class are weighted. This, together with
<code>method="classical"</code>, computes AWC as defined in Hennig (2003). If
<code>TRUE</code>, all points are weighted. This, together with
<code>method="mcd"</code>, computes ARC as defined in Hennig (2003).</p>
</td></tr>
<tr><td><code id="awcoord_+3A_alpha">alpha</code></td>
<td>
<p>numeric between 0 and 1. The corresponding quantile of
the chi squared distribution is used for the downweighting
of points. Points with a smaller Mahalanobis distance to the
homogeneous class get full weight.</p>
</td></tr>
<tr><td><code id="awcoord_+3A_subsample">subsample</code></td>
<td>
<p>integer. If 0, all points are used. Else, only a
subsample of <code>subsample</code> of the points is used.</p>
</td></tr>
<tr><td><code id="awcoord_+3A_countmode">countmode</code></td>
<td>
<p>optional positive integer. Every <code>countmode</code>
algorithm runs <code>awcoord</code> shows a message.</p>
</td></tr>
<tr><td><code id="awcoord_+3A_...">...</code></td>
<td>
<p>no effect</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The square root of the homogeneous classes covariance matrix
is inverted by use of
<code><a href="#topic+tdecomp">tdecomp</a></code>, which can be expected to give
reasonable results for singular within-class covariance matrices.
</p>


<h3>Value</h3>

<p>List with the following components
</p>
<table>
<tr><td><code>ev</code></td>
<td>
<p>eigenvalues in descending order.</p>
</td></tr>
<tr><td><code>units</code></td>
<td>
<p>columns are coordinates of projection basis vectors.
New points <code>x</code> can be projected onto the projection basis vectors
by <code>x %*% units</code></p>
</td></tr>
<tr><td><code>proj</code></td>
<td>
<p>projections of <code>xd</code> onto <code>units</code>.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2004) Asymmetric linear dimension reduction for classification.
Journal of Computational and Graphical Statistics 13, 930-945 .
</p>
<p>Hennig, C. (2005)  A method for visual cluster validation.  In:
Weihs, C. and Gaul, W. (eds.): Classification - The Ubiquitous
Challenge. Springer, Heidelberg 2005, 153-160.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plotcluster">plotcluster</a></code> for straight forward discriminant plots.
<code><a href="#topic+discrproj">discrproj</a></code> for alternatives.
<code><a href="#topic+rFace">rFace</a></code> for generation of the example data used below.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(4634)
  face &lt;- rFace(600,dMoNo=2,dNoEy=0)
  grface &lt;- as.integer(attr(face,"grouping"))
  awcf &lt;- awcoord(face,grface==1)
  # awcf2 &lt;- ancoord(face,grface==1, method="mcd")
  plot(awcf$proj,col=1+(grface==1))
  # plot(awcf2$proj,col=1+(grface==1))
  # ...done in one step by function plotcluster.
</code></pre>

<hr>
<h2 id='batcoord'>Bhattacharyya discriminant projection</h2><span id='topic+batcoord'></span><span id='topic+batvarcoord'></span>

<h3>Description</h3>

<p>Computes Bhattacharyya discriminant projection coordinates
as described in Fukunaga (1990), p. 455 ff.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>batcoord(xd, clvecd, clnum=1, dom="mean")
batvarcoord(xd, clvecd, clnum=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="batcoord_+3A_xd">xd</code></td>
<td>
<p>the data matrix; a numerical object which can be coerced
to a matrix.</p>
</td></tr>
<tr><td><code id="batcoord_+3A_clvecd">clvecd</code></td>
<td>
<p>integer or logical vector of class numbers; length must equal
<code>nrow(xd)</code>.</p>
</td></tr>
<tr><td><code id="batcoord_+3A_clnum">clnum</code></td>
<td>
<p>integer, one of the values of <code>clvecd</code>, if this is
an integer vector. Bhattacharyya projections can only be computed if
there are only two classes in the dataset. <code>clnum</code> is the number
of one of the two classes. All the points indicated by other values
of <code>clvecd</code> are interpreted as the second class.</p>
</td></tr>
<tr><td><code id="batcoord_+3A_dom">dom</code></td>
<td>
<p>string. <code>dom="mean"</code> means that the discriminant
coordinate for the group means is computed as the first projection
direction by
<code><a href="#topic+discrcoord">discrcoord</a></code> (option <code>pool="equal"</code>; both classes
have the same weight for computing the within-class covariance
matrix). Then the data is projected into a subspace orthogonal
(w.r.t. the within-class covariance) to the
discriminant coordinate, and the projection coordinates to maximize
the differences in variance are computed. <br />
<code>dom="variance"</code> means that the projection coordinates
maximizing the difference in variances are computed. Then they are
ordered with respect to the Bhattacharyya distance, which takes also
the mean differences into account. Both procedures are implemented
as described in Fukunaga (1990).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>batvarcoord</code> computes the optimal projection coordinates with
respect to the difference in variances. <code>batcoord</code> combines the
differences in mean and variance as explained for the argument <code>dom</code>.
</p>


<h3>Value</h3>

<p><code>batcoord</code> returns a list with the components <code>ev, rev,
    units, proj</code>. <code>batvarcoord</code> returns a list with the components
<code>ev, rev, units, proj, W, S1, S2</code>. 
</p>
<table>
<tr><td><code>ev</code></td>
<td>
<p>vector of eigenvalues. If <code>dom="mean"</code>, then first eigenvalue
from <code><a href="#topic+discrcoord">discrcoord</a></code>. Further eigenvalues are of
<code class="reqn">S_1^{-1}S_2</code>, where <code class="reqn">S_i</code> is the covariance matrix of class
i. For <code>batvarcoord</code> or
if <code>dom="variance"</code>, all eigenvalues come from
<code class="reqn">S_1^{-1}S_2</code> and are ordered by <code>rev</code>.</p>
</td></tr>
<tr><td><code>rev</code></td>
<td>
<p>for <code>batcoord</code>:
vector of projected Bhattacharyya distances (Fukunaga
(1990), p. 99). Determine quality of the projection coordinates.
For <code>batvarcoord</code>: vector of amount of projected difference in
variances.</p>
</td></tr>
<tr><td><code>units</code></td>
<td>
<p>columns are coordinates of projection basis vectors.
New points <code>x</code> can be projected onto the projection basis vectors
by <code>x %*% units</code>.</p>
</td></tr>
<tr><td><code>proj</code></td>
<td>
<p>projections of <code>xd</code> onto <code>units</code>.</p>
</td></tr> 
<tr><td><code>W</code></td>
<td>
<p>matrix <code class="reqn">S_1^{-1}S_2</code>.</p>
</td></tr>
<tr><td><code>S1</code></td>
<td>
<p>covariance matrix of the first class.</p>
</td></tr>    
<tr><td><code>S2</code></td>
<td>
<p>covariance matrix of the second class.</p>
</td></tr>    
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>References</h3>

<p>Fukunaga, K. (1990). <em>Introduction to Statistical Pattern
Recognition</em> (2nd ed.). Boston: Academic Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plotcluster">plotcluster</a></code> for straight forward discriminant plots.
</p>
<p><code><a href="#topic+discrcoord">discrcoord</a></code> for discriminant coordinates.
</p>
<p><code><a href="#topic+rFace">rFace</a></code> for generation of the example data used below.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(4634)
face &lt;- rFace(600,dMoNo=2,dNoEy=0)
grface &lt;- as.integer(attr(face,"grouping"))
bcf2 &lt;- batcoord(face,grface==2)
plot(bcf2$proj,col=1+(grface==2))
bcfv2 &lt;- batcoord(face,grface==2,dom="variance")
plot(bcfv2$proj,col=1+(grface==2))
bcfvv2 &lt;- batvarcoord(face,grface==2)
plot(bcfvv2$proj,col=1+(grface==2))
</code></pre>

<hr>
<h2 id='bhattacharyya.dist'>Bhattacharyya distance between Gaussian distributions</h2><span id='topic+bhattacharyya.dist'></span>

<h3>Description</h3>

<p>Computes Bhattacharyya distance between two multivariate
Gaussian distributions. See Fukunaga (1990).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bhattacharyya.dist(mu1, mu2, Sigma1, Sigma2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bhattacharyya.dist_+3A_mu1">mu1</code></td>
<td>
<p>mean vector of component 1.</p>
</td></tr>
<tr><td><code id="bhattacharyya.dist_+3A_mu2">mu2</code></td>
<td>
<p>mean vector of component 2.</p>
</td></tr>
<tr><td><code id="bhattacharyya.dist_+3A_sigma1">Sigma1</code></td>
<td>
<p>covariance matrix of component 1.</p>
</td></tr>
<tr><td><code id="bhattacharyya.dist_+3A_sigma2">Sigma2</code></td>
<td>
<p>covariance matrix of component 2.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The Bhattacharyya distance between the two Gaussian distributions.
</p>


<h3>Note</h3>

<p>Thanks to David Pinto for improving this function.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Fukunaga, K. (1990) <em>Introduction to Statistical Pattern
Recognition</em>, 2nd edition, Academic
Press, New York.
</p>
<p>Hennig, C. (2010) Methods for merging Gaussian mixture components,
<em>Advances in Data Analysis and Classification</em>, 4, 3-34.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  round(bhattacharyya.dist(c(1,1),c(2,5),diag(2),diag(2)),digits=2)
</code></pre>

<hr>
<h2 id='bhattacharyya.matrix'>Matrix of pairwise Bhattacharyya distances</h2><span id='topic+bhattacharyya.matrix'></span>

<h3>Description</h3>

<p>Computes Bhattachryya distances for pairs of components
given the parameters of a Gaussian mixture. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bhattacharyya.matrix(muarray,Sigmaarray,ipairs="all", 
                                 misclassification.bound=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bhattacharyya.matrix_+3A_muarray">muarray</code></td>
<td>
<p>matrix of component means (different components are in
different columns).</p>
</td></tr>
<tr><td><code id="bhattacharyya.matrix_+3A_sigmaarray">Sigmaarray</code></td>
<td>
<p>three dimensional array with component covariance
matrices (the third dimension refers to components).</p>
</td></tr>
<tr><td><code id="bhattacharyya.matrix_+3A_ipairs">ipairs</code></td>
<td>
<p><code>"all"</code> or list of vectors of two integers. If
<code>ipairs="all"</code>, computations are carried out for all pairs of
components. Otherwise, ipairs gives the pairs of components for
which computations are carried out.</p>
</td></tr>
<tr><td><code id="bhattacharyya.matrix_+3A_misclassification.bound">misclassification.bound</code></td>
<td>
<p>logical. If <code>TRUE</code>, upper bounds
for misclassification probabilities <code>exp(-b)</code>
are given out instead of the original Bhattacharyya distances <code>b</code>.</p>
</td></tr>  
</table>


<h3>Value</h3>

<p>A matrix with Bhattacharyya distances (or derived misclassification
bounds, see above) between pairs of Gaussian distributions with the
provided parameters. If <code>ipairs!="all"</code>, the Bhattacharyya
distance and the misclassification bound are given as <code>NA</code> for
pairs not included in <code>ipairs</code>.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Fukunaga, K. (1990) <em>Introduction to Statistical Pattern
Recognition</em>, 2nd edition, Academic
Press, New York.
</p>
<p>Hennig, C. (2010) Methods for merging Gaussian mixture components,
<em>Advances in Data Analysis and Classification</em>, 4, 3-34.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bhattacharyya.dist">bhattacharyya.dist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  muarray &lt;-cbind(c(0,0),c(0,0.1),c(10,10))
  sigmaarray &lt;- array(c(diag(2),diag(2),diag(2)),dim=c(2,2,3))
  bhattacharyya.matrix(muarray,sigmaarray,ipairs=list(c(1,2),c(2,3)))

</code></pre>

<hr>
<h2 id='calinhara'>Calinski-Harabasz index</h2><span id='topic+calinhara'></span>

<h3>Description</h3>

<p>Calinski-Harabasz index for estimating the number of clusters,
based on an observations/variables-matrix here. A distance based
version is available through <code>cluster.stats</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  calinhara(x,clustering,cn=max(clustering))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calinhara_+3A_x">x</code></td>
<td>
<p>data matrix or data frame.</p>
</td></tr>
<tr><td><code id="calinhara_+3A_clustering">clustering</code></td>
<td>
<p>vector of integers. Clustering.</p>
</td></tr>
<tr><td><code id="calinhara_+3A_cn">cn</code></td>
<td>
<p>integer. Number of clusters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Calinski-Harabasz statistic, which is 
<code>(n-cn)*sum(diag(B))/((cn-1)*sum(diag(W)))</code>. B being the
between-cluster means, 
and W being the within-clusters covariance matrix.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en">https://www.unibo.it/sitoweb/christian.hennig/en</a></p>


<h3>References</h3>

<p>Calinski, T., and Harabasz, J. (1974) A Dendrite Method for Cluster 
Analysis, <em>Communications in Statistics</em>, 3, 1-27.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cluster.stats">cluster.stats</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(98765)
  iriss &lt;- iris[sample(150,20),-5]
  km &lt;- kmeans(iriss,3)
  round(calinhara(iriss,km$cluster),digits=2)
</code></pre>

<hr>
<h2 id='can'>Generation of the tuning constant for regression fixed point clusters</h2><span id='topic+can'></span>

<h3>Description</h3>

<p>Generates tuning constants <code>ca</code>
for <code><a href="#topic+fixreg">fixreg</a></code> dependent on
the number of points and variables of the dataset.
</p>
<p>Only thought for use in <code><a href="#topic+fixreg">fixreg</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>can(n, p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="can_+3A_n">n</code></td>
<td>
<p>positive integer. Number of points.</p>
</td></tr>
<tr><td><code id="can_+3A_p">p</code></td>
<td>
<p>positive integer. Number of independent variables.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The formula is
<code class="reqn">3+33/(n*2^{-(p-1)/2})^{1/3}+2900000/(n*2^{-(p-1)/2})^3</code>. For
justification cf. Hennig (2002).
</p>


<h3>Value</h3>

<p>A number.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>References</h3>

<p>Hennig, C. (2002) Fixed point clusters for linear regression:
computation and comparison, <em>Journal of
Classification</em> 19, 249-276.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fixreg">fixreg</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  can(429,3)
</code></pre>

<hr>
<h2 id='cat2bin'>Recode nominal variables to binary variables</h2><span id='topic+cat2bin'></span>

<h3>Description</h3>

<p>Recodes a dataset with nominal variables so that the nominal
variables are replaced by binary variables for the categories.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  cat2bin(x,categorical=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cat2bin_+3A_x">x</code></td>
<td>
<p>data matrix or data frame. The data need to be organised
case-wise, i.e., if there are categorical variables only, and 15
cases with values c(1,1,2) on the 3 variables, the data matrix needs
15 rows with values 1 1 2. (Categorical variables could take numbers
or strings or anything that can be coerced to factor levels as values.)</p>
</td></tr>
<tr><td><code id="cat2bin_+3A_categorical">categorical</code></td>
<td>
<p>vector of numbers of variables to be recoded.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>data</code></td>
<td>
<p>data matrix with variables specified in <code>categorical</code>
replaced by 0-1 variables, one for each category.</p>
</td></tr>
<tr><td><code>variableinfo</code></td>
<td>
<p>list of lists. One list for every variable in the
original dataset, with four components each, namely <code>type</code>
(<code>"categorical"</code> or <code>"not recoded"</code>), <code>levels</code>
(levels of nominal recoded variables in order of binary variable in
output dataset), <code>ncat</code> (number of categories for recoded
variables), <code>varnum</code> (number of variables in output dataset
belonging to this original variable).</p>
</td></tr>  
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en">https://www.unibo.it/sitoweb/christian.hennig/en</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+discrete.recode">discrete.recode</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(776655)
  v1 &lt;- rnorm(20)
  v2 &lt;- rnorm(20)
  d1 &lt;- sample(1:5,20,replace=TRUE)
  d2 &lt;- sample(1:4,20,replace=TRUE)
  ldata &lt;-cbind(v1,v2,d1,d2)
  lc &lt;- cat2bin(ldata,categorical=3:4)
</code></pre>

<hr>
<h2 id='cdbw'>CDbw-index for cluster validation</h2><span id='topic+cdbw'></span>

<h3>Description</h3>

<p>CDbw-index for cluster validation, as defined in Halkidi and
Vazirgiannis (2008), Halkidi et al. (2015).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cdbw(x,clustering,r=10,s=seq(0.1,0.8,by=0.1),
                 clusterstdev=TRUE,trace=FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cdbw_+3A_x">x</code></td>
<td>
<p>something that can be coerced into a numerical
matrix. Euclidean dataset.</p>
</td></tr>
<tr><td><code id="cdbw_+3A_clustering">clustering</code></td>
<td>
<p>vector of integers with length <code>=nrow(x)</code>;
indicating the cluster for each observation.</p>
</td></tr>
<tr><td><code id="cdbw_+3A_r">r</code></td>
<td>
<p>integer. Number of cluster border representatives.</p>
</td></tr>
<tr><td><code id="cdbw_+3A_s">s</code></td>
<td>
<p>numerical vector of shrinking factors (between 0 and 1).</p>
</td></tr>
<tr><td><code id="cdbw_+3A_clusterstdev">clusterstdev</code></td>
<td>
<p>logical. If <code>TRUE</code>, the neighborhood radius
for intra-cluster density is the within-cluster estimated squared
distance from the mean of the cluster; otherwise it is the average of
these over all clusters.</p>
</td></tr>
<tr><td><code id="cdbw_+3A_trace">trace</code></td>
<td>
<p>logical. If <code>TRUE</code>, results are printed for the
steps to compute the index.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with components (see Halkidi and Vazirgiannis (2008), Halkidi et
al. (2015) for details)
</p>
<table>
<tr><td><code>cdbw</code></td>
<td>
<p>value of CDbw index (the higher the better).</p>
</td></tr>
<tr><td><code>cohesion</code></td>
<td>
<p>cohesion.</p>
</td></tr>
<tr><td><code>compactness</code></td>
<td>
<p>compactness.</p>
</td></tr>
<tr><td><code>sep</code></td>
<td>
<p>separation.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Halkidi, M. and Vazirgiannis, M. (2008) A density-based cluster
validity approach using multi-representatives. <em>Pattern
Recognition Letters</em> 29, 773-786.
</p>
<p>Halkidi, M., Vazirgiannis, M. and Hennig, C. (2015) Method-independent
indices for cluster validation. In C. Hennig, M. Meila, F. Murtagh,
R. Rocci (eds.) <em>Handbook of Cluster Analysis</em>, CRC
Press/Taylor <code>&amp;</code> Francis, Boca Raton.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  options(digits=3)
  iriss &lt;- as.matrix(iris[c(1:5,51:55,101:105),-5])
  irisc &lt;- as.numeric(iris[c(1:5,51:55,101:105),5])
  cdbw(iriss,irisc)
</code></pre>

<hr>
<h2 id='cgrestandard'>Standardise cluster validation statistics by random clustering results</h2><span id='topic+cgrestandard'></span>

<h3>Description</h3>

<p>Standardises cluster validity statistics as produced by
<code><a href="#topic+clustatsum">clustatsum</a></code> relative to results that were achieved by
random clusterings on the same data by
<code><a href="#topic+randomclustersim">randomclustersim</a></code>. The aim is to make differences between
values comparable between indexes, see Hennig (2019), Akhanli and
Hennig (2020).
</p>
<p>This is mainly for use within <code><a href="#topic+clusterbenchstats">clusterbenchstats</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cgrestandard(clusum,clusim,G,percentage=FALSE,
                               useallmethods=FALSE,
                             useallg=FALSE, othernc=list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cgrestandard_+3A_clusum">clusum</code></td>
<td>
<p>object of class &quot;valstat&quot;, see <code><a href="#topic+clusterbenchstats">clusterbenchstats</a></code>.</p>
</td></tr>
<tr><td><code id="cgrestandard_+3A_clusim">clusim</code></td>
<td>
<p>list; output object of <code><a href="#topic+randomclustersim">randomclustersim</a></code>,
see there.</p>
</td></tr> 
<tr><td><code id="cgrestandard_+3A_g">G</code></td>
<td>
<p>vector of integers. Numbers of clusters to consider.</p>
</td></tr>
<tr><td><code id="cgrestandard_+3A_percentage">percentage</code></td>
<td>
<p>logical. If <code>FALSE</code>, standardisation is done to
mean zero and standard deviation 1 using the random clusterings. If
<code>TRUE</code>, the output is the percentage of simulated values below
the result (more precisely, this number plus one divided by the
total plus one).</p>
</td></tr>
<tr><td><code id="cgrestandard_+3A_useallmethods">useallmethods</code></td>
<td>
<p>logical. If <code>FALSE</code>, only random clustering
results from <code>clusim</code> are used for standardisation. If
<code>TRUE</code>, also clustering results from other methods as given in
<code>clusum</code> are used.</p>
</td></tr>
<tr><td><code id="cgrestandard_+3A_useallg">useallg</code></td>
<td>
<p>logical. If <code>TRUE</code>, standardisation uses results
from all numbers of clusters in <code>G</code>. If <code>FALSE</code>,
standardisation of results for a specific number of cluster only
uses results from that number of clusters.</p>
</td></tr> 
<tr><td><code id="cgrestandard_+3A_othernc">othernc</code></td>
<td>
<p>list of integer vectors of length 2. This allows the
incorporation of methods that bring forth other numbers of clusters
than those in <code>G</code>, for example because a method may have
automatically estimated a number of clusters. The first number is
the number of the clustering method (the order is determined by
argument <code>clustermethod</code> in
<code><a href="#topic+clusterbenchstats">clusterbenchstats</a></code>), the second number is the
number of clusters. Results specified here are only standardised in
<code>useallg=TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>cgrestandard</code> will add a statistic named <code>dmode</code> to the
input set of validation statistics, which is defined as
<code>0.75*dindex+0.25*highdgap</code>, aggregating these two closely
related statistics, see <code><a href="#topic+clustatsum">clustatsum</a></code>.
</p>


<h3>Value</h3>

<p>List of class <code>"valstat"</code>, see
<code><a href="#topic+valstat.object">valstat.object</a></code>, with standardised results as
explained above.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2019) Cluster validation by measurement of clustering
characteristics relevant to the user. In C. H. Skiadas (ed.)
<em>Data Analysis and Applications 1: Clustering and Regression,
Modeling-estimating, Forecasting and Data Mining, Volume 2</em>, Wiley,
New York 1-24,
<a href="https://arxiv.org/abs/1703.09282">https://arxiv.org/abs/1703.09282</a>
</p>
<p>Akhanli, S. and Hennig, C. (2020) Calibrating and aggregating cluster
validity indexes for context-adapted comparison of clusterings.
<em>Statistics and Computing</em>, 30, 1523-1544,
<a href="https://link.springer.com/article/10.1007/s11222-020-09958-2">https://link.springer.com/article/10.1007/s11222-020-09958-2</a>, <a href="https://arxiv.org/abs/2002.01822">https://arxiv.org/abs/2002.01822</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+valstat.object">valstat.object</a></code>, <code><a href="#topic+clusterbenchstats">clusterbenchstats</a></code>, <code><a href="#topic+stupidkcentroids">stupidkcentroids</a></code>, <code><a href="#topic+stupidknn">stupidknn</a></code>, <code><a href="#topic+stupidkfn">stupidkfn</a></code>, <code><a href="#topic+stupidkaven">stupidkaven</a></code>, <code><a href="#topic+clustatsum">clustatsum</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
  set.seed(20000)
  options(digits=3)
  face &lt;- rFace(10,dMoNo=2,dNoEy=0,p=2)
  dif &lt;- dist(face)
  clusum &lt;- list()
  clusum[[2]] &lt;- list()
  cl12 &lt;- kmeansCBI(face,2)
  cl13 &lt;- kmeansCBI(face,3)
  cl22 &lt;- claraCBI(face,2)
  cl23 &lt;- claraCBI(face,2)
  ccl12 &lt;- clustatsum(dif,cl12$partition)
  ccl13 &lt;- clustatsum(dif,cl13$partition)
  ccl22 &lt;- clustatsum(dif,cl22$partition)
  ccl23 &lt;- clustatsum(dif,cl23$partition)
  clusum[[1]] &lt;- list()
  clusum[[1]][[2]] &lt;- ccl12
  clusum[[1]][[3]] &lt;- ccl13
  clusum[[2]][[2]] &lt;- ccl22
  clusum[[2]][[3]] &lt;- ccl23
  clusum$maxG &lt;- 3
  clusum$minG &lt;- 2
  clusum$method &lt;- c("kmeansCBI","claraCBI")
  clusum$name &lt;- c("kmeansCBI","claraCBI")
  clusim &lt;- randomclustersim(dist(face),G=2:3,nnruns=1,kmruns=1,
    fnruns=1,avenruns=1,monitor=FALSE)
  cgr &lt;- cgrestandard(clusum,clusim,2:3)
  cgr2 &lt;- cgrestandard(clusum,clusim,2:3,useallg=TRUE)
  cgr3 &lt;- cgrestandard(clusum,clusim,2:3,percentage=TRUE)
  print(str(cgr))
  print(str(cgr2))
  print(cgr3[[1]][[2]])
</code></pre>

<hr>
<h2 id='classifdist'>Classification of unclustered points</h2><span id='topic+classifdist'></span><span id='topic+classifnp'></span>

<h3>Description</h3>

<p>Various methods for classification of unclustered points from
clustered points for use within functions <code>nselectboot</code>
and <code>prediction.strength</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classifdist(cdist,clustering,
                      method="averagedist",
                      centroids=NULL,nnk=1)

classifnp(data,clustering,
                      method="centroid",cdist=NULL,
                      centroids=NULL,nnk=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="classifdist_+3A_cdist">cdist</code></td>
<td>
<p>dissimilarity matrix or <code>dist</code>-object. Necessary for
<code>classifdist</code> but optional for <code>classifnp</code> and there only
used if <code>method="averagedist"</code> (if not provided, <code>dist</code> is
applied to <code>data</code>).</p>
</td></tr>
<tr><td><code id="classifdist_+3A_data">data</code></td>
<td>
<p>something that can be coerced into a an
<code>n*p</code>-data matrix.</p>
</td></tr>
<tr><td><code id="classifdist_+3A_clustering">clustering</code></td>
<td>
<p>integer vector. Gives the cluster number (between 1
and k for k clusters) for clustered points and should be -1 for
points to be classified.</p>
</td></tr>
<tr><td><code id="classifdist_+3A_method">method</code></td>
<td>
<p>one of <code>"averagedist", "centroid", "qda",
      "knn"</code>. See details.</p>
</td></tr>
<tr><td><code id="classifdist_+3A_centroids">centroids</code></td>
<td>
<p>for <code>classifnp</code> a k times p matrix of cluster
centroids. For <code>classifdist</code> a vector of numbers of centroid
objects as provided by <code><a href="cluster.html#topic+pam">pam</a></code>. Only used if
<code>method="centroid"</code>; in that case mandatory for
<code>classifdist</code> but optional for <code>classifnp</code>, where cluster mean
vectors are computed if <code>centroids=NULL</code>.</p>
</td></tr>
<tr><td><code id="classifdist_+3A_nnk">nnk</code></td>
<td>
<p>number of nearest neighbours if <code>method="knn"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>classifdist</code> is for data given as dissimilarity matrix,
<code>classifnp</code> is for data given as n times p data matrix. 
The following methods are supported:
</p>

<dl>
<dt>&quot;centroid&quot;</dt><dd><p>assigns observations to the cluster with closest
cluster centroid as specified in argument <code>centroids</code> (this
is associated to k-means and pam/clara-clustering).</p>
</dd>
<dt>&quot;qda&quot;</dt><dd><p>only in <code>classifnp</code>. Classifies by quadratic
discriminant analysis (this is associated to Gaussian clusters
with flexible covariance matrices), calling
<code><a href="MASS.html#topic+qda">qda</a></code> with default settings. If
<code><a href="MASS.html#topic+qda">qda</a></code> gives an error (usually because a class
was too small), <code><a href="MASS.html#topic+lda">lda</a></code> is used.</p>
</dd>
<dt>&quot;lda&quot;</dt><dd><p>only in <code>classifnp</code>. Classifies by linear
discriminant analysis (this is associated to Gaussian clusters
with equal covariance matrices), calling
<code><a href="MASS.html#topic+lda">lda</a></code> with default settings.</p>
</dd>
<dt>&quot;averagedist&quot;</dt><dd><p>assigns to the cluster to which an observation
has the minimum average dissimilarity to all points in the cluster
(this is associated with average linkage clustering).</p>
</dd>
<dt>&quot;knn&quot;</dt><dd><p>classifies by <code>nnk</code> nearest neighbours (for
<code>nnk=1</code>, this is associated with single linkage clustering).
Calls <code><a href="class.html#topic+knn">knn</a></code> in <code>classifnp</code>.</p>
</dd>
<dt>&quot;fn&quot;</dt><dd><p>classifies by the minimum distance to the farthest
neighbour. This is associated with complete linkage clustering).</p>
</dd>
</dl>



<h3>Value</h3>

<p>An integer vector giving cluster numbers for all observations; those
for the observations already clustered in the input are the same as in
the input.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prediction.strength">prediction.strength</a></code>, <code><a href="#topic+nselectboot">nselectboot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
set.seed(20000)
x1 &lt;- rnorm(50)
y &lt;- rnorm(100)
x2 &lt;- rnorm(40,mean=20)
x3 &lt;- rnorm(10,mean=25,sd=100)
x &lt;-cbind(c(x1,x2,x3),y)
truec &lt;- c(rep(1,50),rep(2,40),rep(3,10))
topredict &lt;- c(1,2,51,52,91)
clumin &lt;- truec
clumin[topredict] &lt;- -1

classifnp(x,clumin, method="averagedist")
classifnp(x,clumin, method="qda")
classifdist(dist(x),clumin, centroids=c(3,53,93),method="centroid")
classifdist(dist(x),clumin,method="knn")

</code></pre>

<hr>
<h2 id='clucols'>Sets of colours and symbols for cluster plotting</h2><span id='topic+clucols'></span><span id='topic+clugrey'></span><span id='topic+clusym'></span>

<h3>Description</h3>

<p><code>clucols</code> gives out a vector of different random colours.
<code>clugrey</code> gives out a vector of equidistant grey scales.
<code>clusym</code> is a vector of different symbols starting from &quot;1&quot;,
&quot;2&quot;,...
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  clucols(i, seed=NULL)
  clugrey(i,max=0.9)
  clusym
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clucols_+3A_i">i</code></td>
<td>
<p>integer. Length of output vector (number of clusters).</p>
</td></tr>
<tr><td><code id="clucols_+3A_seed">seed</code></td>
<td>
<p>integer. Random seed.</p>
</td></tr>
<tr><td><code id="clucols_+3A_max">max</code></td>
<td>
<p>between 0 and 1. Maximum grey scale value, see
<code><a href="grDevices.html#topic+grey">grey</a></code> (close to 1 is bright).</p>
</td></tr>    
</table>


<h3>Value</h3>

<p><code>clucols</code> gives out a vector of different random colours.
<code>clugrey</code> gives out a vector of equidistant grey scales.
<code>clusym</code> is a vector of different characters starting from &quot;1&quot;,
&quot;2&quot;,...
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en">https://www.unibo.it/sitoweb/christian.hennig/en</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(112233)
  require(MASS)
  require(flexmix)
  data(Cars93)
  Cars934 &lt;- Cars93[,c(3,5,8,10)]
  cc &lt;-
    discrete.recode(Cars934,xvarsorted=FALSE,continuous=c(2,3),discrete=c(1,4))
  fcc &lt;- flexmix(cc$data~1,k=3,
  model=lcmixed(continuous=2,discrete=2,ppdim=c(6,3),diagonal=TRUE))
  plot(Cars934[,c(2,3)],col=clucols(3)[fcc@cluster],pch=clusym[fcc@cluster])
</code></pre>

<hr>
<h2 id='clujaccard'>Jaccard similarity between logical vectors</h2><span id='topic+clujaccard'></span>

<h3>Description</h3>

<p>Jaccard similarity between logical or 0-1 vectors:
<code>sum(c1 &amp; c2)/sum(c1 | c2)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clujaccard(c1,c2,zerobyzero=NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clujaccard_+3A_c1">c1</code></td>
<td>
<p>logical or 0-1-vector.</p>
</td></tr>
<tr><td><code id="clujaccard_+3A_c2">c2</code></td>
<td>
<p>logical or 0-1-vector (same length).</p>
</td></tr>
<tr><td><code id="clujaccard_+3A_zerobyzero">zerobyzero</code></td>
<td>
<p>result if <code>sum(c1 | c2)=0</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric between 0 and 1.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  c1 &lt;- rep(TRUE,10)
  c2 &lt;- c(FALSE,rep(TRUE,9))
  clujaccard(c1,c2)
</code></pre>

<hr>
<h2 id='clusexpect'>Expected value of the number of times a fixed point
cluster is found</h2><span id='topic+clusexpect'></span>

<h3>Description</h3>

<p>A rough approximation of the expectation of the number of times a well
separated fixed point
cluster (FPC) of size <code>n</code> is found in <code>ir</code> fixed point
iterations of <code><a href="#topic+fixreg">fixreg</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  clusexpect(n, p, cn, ir)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clusexpect_+3A_n">n</code></td>
<td>
<p>positive integer. Total number of points.</p>
</td></tr>
<tr><td><code id="clusexpect_+3A_p">p</code></td>
<td>
<p>positive integer. Number of independent variables.</p>
</td></tr>
<tr><td><code id="clusexpect_+3A_cn">cn</code></td>
<td>
<p>positive integer smaller or equal to <code>n</code>.
Size of the FPC.</p>
</td></tr>
<tr><td><code id="clusexpect_+3A_ir">ir</code></td>
<td>
<p>positive integer. Number of fixed point iterations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The approximation is based on the assumption that a well separated FPC
is found iff all <code>p+2</code> points of the initial coinfiguration come
from the FPC. The value is <code>ir</code> times the probability for
this. For a discussion of this assumption cf. Hennig (2002).
</p>


<h3>Value</h3>

<p>A number.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>References</h3>

<p>Hennig, C. (2002) Fixed point clusters for linear regression:
computation and comparison, <em>Journal of
Classification</em> 19, 249-276.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fixreg">fixreg</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  round(clusexpect(500,4,150,2000),digits=2)
</code></pre>

<hr>
<h2 id='clustatsum'>Compute and format cluster validation statistics</h2><span id='topic+clustatsum'></span>

<h3>Description</h3>

<p><code>clustatsum</code> computes cluster validation statistics by running
<code><a href="#topic+cqcluster.stats">cqcluster.stats</a></code>, 
and potentially <code><a href="#topic+distrsimilarity">distrsimilarity</a></code>, and collecting some key
statistics values with a somewhat different nomenclature.
</p>
<p>This was implemented as a helper function for use inside of
<code><a href="#topic+clusterbenchstats">clusterbenchstats</a></code> and <code><a href="#topic+cgrestandard">cgrestandard</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clustatsum(datadist=NULL,clustering,noisecluster=FALSE,
                       datanp=NULL,npstats=FALSE,useboot=FALSE,
                       bootclassif=NULL,
                       bootmethod="nselectboot",
                       bootruns=25, cbmethod=NULL,methodpars=NULL,
                       distmethod=NULL,dnnk=2,
                       pamcrit=TRUE,...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clustatsum_+3A_datadist">datadist</code></td>
<td>
<p>distances on which validation-measures are based, <code>dist</code>
object or distance matrix. If <code>NULL</code>, this is computed from
<code>datanp</code>; at least one of <code>datadist</code> and <code>datanp</code>
must be specified.</p>
</td></tr>
<tr><td><code id="clustatsum_+3A_clustering">clustering</code></td>
<td>
<p>an integer vector of length of the number of cases,
which indicates a clustering. The clusters have to be numbered
from 1 to the number of clusters.</p>
</td></tr>
<tr><td><code id="clustatsum_+3A_noisecluster">noisecluster</code></td>
<td>
<p>logical. If <code>TRUE</code>, it is assumed that the
largest cluster number in <code>clustering</code> denotes a 'noise
class', i.e. points that do not belong to any cluster. These points
are not taken into account for the computation of all functions of
within and between cluster distances including the validation
indexes.</p>
</td></tr> 
<tr><td><code id="clustatsum_+3A_datanp">datanp</code></td>
<td>
<p>optional observations times variables data matrix, see
<code>npstats</code>.</p>
</td></tr> 
<tr><td><code id="clustatsum_+3A_npstats">npstats</code></td>
<td>
<p>logical. If <code>TRUE</code>, <code><a href="#topic+distrsimilarity">distrsimilarity</a></code>
is called and the two statistics computed there are added to the
output. These are based on <code>datanp</code> and require <code>datanp</code>
to be specified.</p>
</td></tr>
<tr><td><code id="clustatsum_+3A_useboot">useboot</code></td>
<td>
<p>logical. If <code>TRUE</code>, a stability index (either
<code>nselectboot</code> or <code>prediction.strength</code>) will be involved.</p>
</td></tr>
<tr><td><code id="clustatsum_+3A_bootclassif">bootclassif</code></td>
<td>
<p>If <code>useboot=TRUE</code>, a string indicating the
classification method to be used with the stability index, see the
<code>classification</code> argument of <code>nselectboot</code> and
<code>prediction.strength</code>.</p>
</td></tr>
<tr><td><code id="clustatsum_+3A_bootmethod">bootmethod</code></td>
<td>
<p>either <code>"nselectboot"</code> or
<code>"prediction.strength"</code>; stability index to be used if
<code>useboot=TRUE</code>.</p>
</td></tr>
<tr><td><code id="clustatsum_+3A_bootruns">bootruns</code></td>
<td>
<p>integer. Number of resampling runs. If
<code>useboot=TRUE</code>, passed on as <code>B</code> to
<code><a href="#topic+nselectboot">nselectboot</a></code> or
<code>M</code> to <code><a href="#topic+prediction.strength">prediction.strength</a></code>.</p>
</td></tr>
<tr><td><code id="clustatsum_+3A_cbmethod">cbmethod</code></td>
<td>
<p>CBI-function (see <code><a href="#topic+kmeansCBI">kmeansCBI</a></code>); clustering
method to be used for
stability assessment if <code>useboot=TRUE</code>.</p>
</td></tr>
<tr><td><code id="clustatsum_+3A_methodpars">methodpars</code></td>
<td>
<p>parameters to be passed on to <code>cbmethod</code>.</p>
</td></tr>
<tr><td><code id="clustatsum_+3A_distmethod">distmethod</code></td>
<td>
<p>logical. In case of <code>useboot=TRUE</code> indicates
whether <code>cbmethod</code> will interpret data as distances.</p>
</td></tr> 
<tr><td><code id="clustatsum_+3A_dnnk">dnnk</code></td>
<td>
<p><code>nnk</code>-argument to be passed on to
<code><a href="#topic+distrsimilarity">distrsimilarity</a></code>.</p>
</td></tr>
<tr><td><code id="clustatsum_+3A_pamcrit">pamcrit</code></td>
<td>
<p><code>pamcrit</code>-argument to be passed on to
<code><a href="#topic+cqcluster.stats">cqcluster.stats</a></code>.</p>
</td></tr> 
<tr><td><code id="clustatsum_+3A_...">...</code></td>
<td>
<p>further arguments to be passed on to
<code><a href="#topic+cqcluster.stats">cqcluster.stats</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>clustatsum</code> returns a list. The components, as listed below, are
outputs of <code><a href="#topic+summary.cquality">summary.cquality</a></code> with default parameters,
which means that they are partly transformed versions of those given
out by <code><a href="#topic+cqcluster.stats">cqcluster.stats</a></code>, i.e., their range is between 0
and 1 and large values are good. Those from
<code><a href="#topic+distrsimilarity">distrsimilarity</a></code> are computed with
<code>largeisgood=TRUE</code>, correspondingly.
</p>
<table>
<tr><td><code>avewithin</code></td>
<td>
<p>average distance within clusters (reweighted so
that every observation, rather than every distance, has the same weight).</p>
</td></tr>
<tr><td><code>mnnd</code></td>
<td>
<p>average distance to <code>nnk</code>th nearest neighbour within
cluster.</p>
</td></tr>
<tr><td><code>cvnnd</code></td>
<td>
<p>coefficient of variation of dissimilarities to
<code>nnk</code>th nearest wthin-cluster neighbour, measuring uniformity of
within-cluster densities, weighted over all clusters, see Sec. 3.7 of
Hennig (2019).</p>
</td></tr>
<tr><td><code>maxdiameter</code></td>
<td>
<p>maximum cluster diameter.</p>
</td></tr>
<tr><td><code>widestgap</code></td>
<td>
<p>widest within-cluster gap or average of cluster-wise
widest within-cluster gap, depending on parameter <code>averagegap</code>.</p>
</td></tr>
<tr><td><code>sindex</code></td>
<td>
<p>separation index, see argument <code>sepindex</code>.</p>
</td></tr>
<tr><td><code>minsep</code></td>
<td>
<p>minimum cluster separation.</p>
</td></tr>
<tr><td><code>asw</code></td>
<td>
<p>average silhouette
width. See <code><a href="cluster.html#topic+silhouette">silhouette</a></code>.</p>
</td></tr>
<tr><td><code>dindex</code></td>
<td>
<p>this index measures to what extent the density decreases
from the cluster mode to the outskirts; I-densdec in Sec. 3.6 of
Hennig (2019).</p>
</td></tr>
<tr><td><code>denscut</code></td>
<td>
<p>this index measures whether cluster boundaries run
through density valleys; I-densbound in Sec. 3.6 of Hennig (2019).</p>
</td></tr>
<tr><td><code>highdgap</code></td>
<td>
<p>this measures whether there is a large within-cluster
gap with high density on both sides; I-highdgap in Sec. 3.6 of
Hennig (2019).</p>
</td></tr>
<tr><td><code>pearsongamma</code></td>
<td>
<p>correlation between distances and a
0-1-vector where 0 means same cluster, 1 means different clusters.
&quot;Normalized gamma&quot; in Halkidi et al. (2001).</p>
</td></tr>
<tr><td><code>withinss</code></td>
<td>
<p>a generalisation of the within clusters sum
of squares (k-means objective function), which is obtained if
<code>d</code> is a Euclidean distance matrix.  For general distance
measures, this is half
the sum of the within cluster squared dissimilarities divided by the
cluster size.</p>
</td></tr>
<tr><td><code>entropy</code></td>
<td>
<p>entropy of the distribution of cluster memberships,
see Meila(2007).</p>
</td></tr>
<tr><td><code>pamc</code></td>
<td>
<p>average distance to cluster centroid.</p>
</td></tr>
<tr><td><code>kdnorm</code></td>
<td>
<p>Kolmogorov distance between distribution of
within-cluster Mahalanobis
distances and appropriate chi-squared distribution, aggregated over
clusters (I am grateful to Agustin Mayo-Iscar for the idea).</p>
</td></tr>
<tr><td><code>kdunif</code></td>
<td>
<p>Kolmogorov distance between distribution of distances to
<code>nnk</code>th nearest within-cluster neighbor and appropriate
Gamma-distribution, see Byers and Raftery (1998), aggregated over
clusters.</p>
</td></tr>
<tr><td><code>boot</code></td>
<td>
<p>if <code>useboot=TRUE</code>, stability value; <code>stabk</code> for
method <code><a href="#topic+nselectboot">nselectboot</a></code>; <code>mean.pred</code> for method
<code><a href="#topic+prediction.strength">prediction.strength</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Akhanli, S. and Hennig, C. (2020) Calibrating and aggregating cluster
validity indexes for context-adapted comparison of clusterings.
<em>Statistics and Computing</em>, 30, 1523-1544,
<a href="https://link.springer.com/article/10.1007/s11222-020-09958-2">https://link.springer.com/article/10.1007/s11222-020-09958-2</a>, <a href="https://arxiv.org/abs/2002.01822">https://arxiv.org/abs/2002.01822</a>
</p>
<p>Halkidi, M., Batistakis, Y., Vazirgiannis, M. (2001) On Clustering
Validation Techniques, <em>Journal of Intelligent Information
Systems</em>, 17, 107-145.
</p>
<p>Hennig, C. (2019) Cluster validation by measurement of clustering
characteristics relevant to the user. In C. H. Skiadas (ed.)
<em>Data Analysis and Applications 1: Clustering and Regression,
Modeling-estimating, Forecasting and Data Mining, Volume 2</em>, Wiley,
New York 1-24,
<a href="https://arxiv.org/abs/1703.09282">https://arxiv.org/abs/1703.09282</a>
</p>
<p>Kaufman, L. and Rousseeuw, P.J. (1990). &quot;Finding Groups in Data:
An Introduction to Cluster Analysis&quot;. Wiley, New York.
</p>
<p>Meila, M. (2007) Comparing clusterings?an information based distance,
<em>Journal of Multivariate Analysis</em>, 98, 873-895.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cqcluster.stats">cqcluster.stats</a></code>, <code><a href="#topic+distrsimilarity">distrsimilarity</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
  set.seed(20000)
  options(digits=3)
  face &lt;- rFace(20,dMoNo=2,dNoEy=0,p=2)
  dface &lt;- dist(face)
  complete3 &lt;- cutree(hclust(dface),3)
  clustatsum(dface,complete3)
  
</code></pre>

<hr>
<h2 id='cluster.magazine'>Run many clustering methods on many numbers of clusters</h2><span id='topic+cluster.magazine'></span>

<h3>Description</h3>

<p>Runs a user-specified set of clustering methods (CBI-functions, see
<code><a href="#topic+kmeansCBI">kmeansCBI</a></code> with several numbers of clusters on a dataset
with unified output.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster.magazine(data,G,diss = inherits(data, "dist"),
                             scaling=TRUE, clustermethod,
                             distmethod=rep(TRUE,length(clustermethod)),
                             ncinput=rep(TRUE,length(clustermethod)),
                             clustermethodpars,
                             trace=TRUE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster.magazine_+3A_data">data</code></td>
<td>
<p>data matrix or <code>dist</code>-object.</p>
</td></tr>
<tr><td><code id="cluster.magazine_+3A_g">G</code></td>
<td>
<p>vector of integers. Numbers of clusters to consider.</p>
</td></tr>
<tr><td><code id="cluster.magazine_+3A_diss">diss</code></td>
<td>
<p>logical. If <code>TRUE</code>, the data matrix is assumed to be
a distance/dissimilariy matrix, otherwise it's observations times
variables.</p>
</td></tr>
<tr><td><code id="cluster.magazine_+3A_scaling">scaling</code></td>
<td>
<p>either a logical or a numeric vector of length equal to
the number of columns of <code>data</code>. If <code>FALSE</code>, data won't be
scaled, otherwise <code>scaling</code> is passed on to <code><a href="base.html#topic+scale">scale</a></code> as
argument<code>scale</code>.</p>
</td></tr> 
<tr><td><code id="cluster.magazine_+3A_clustermethod">clustermethod</code></td>
<td>
<p>vector of strings specifying names of
CBI-functions (see <code><a href="#topic+kmeansCBI">kmeansCBI</a></code>). These are the
clustering methods to be applied.</p>
</td></tr>
<tr><td><code id="cluster.magazine_+3A_distmethod">distmethod</code></td>
<td>
<p>vector of logicals, of the same length as
<code>clustermethod</code>. <code>TRUE</code> means that the clustering method
operates on distances. If <code>diss=TRUE</code>, all entries have to be
<code>TRUE</code>. Otherwise, if an entry is true, the corresponding
method will be applied on <code>dist(data)</code>.</p>
</td></tr>  
<tr><td><code id="cluster.magazine_+3A_ncinput">ncinput</code></td>
<td>
<p>vector of logicals, of the same length as
<code>clustermethod</code>. <code>TRUE</code> indicates that the corresponding
clustering method requires the number of clusters as input and will
not estimate the number of clusters itself.</p>
</td></tr>
<tr><td><code id="cluster.magazine_+3A_clustermethodpars">clustermethodpars</code></td>
<td>
<p>list of the same length as
<code>clustermethod</code>. Specifies parameters for all involved
clustering methods. Its jth entry is passed to clustermethod number
k. Can be an empty entry in case all defaults are used for a
clustering method. The number of clusters does not need to be
specified here.</p>
</td></tr>
<tr><td><code id="cluster.magazine_+3A_trace">trace</code></td>
<td>
<p>logical. If <code>TRUE</code>, some runtime information is
printed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of lists comprising
</p>
<table>
<tr><td><code>output</code></td>
<td>
<p>Two-dimensional list. The first list index i is the number
of the clustering method (ordering as specified in
<code>clustermethod</code>), the second list index j is the number of
clusters. This stores the full output of clustermethod i run on
number of clusters j.</p>
</td></tr>
<tr><td><code>clustering</code></td>
<td>
<p>Two-dimensional list. The first list index i is the number
of the clustering method (ordering as specified in
<code>clustermethod</code>), the second list index j is the number of
clusters. This stores the clustering integer vector (i.e., the
<code>partition</code>-component of the CBI-function, see
<code><a href="#topic+kmeansCBI">kmeansCBI</a></code>) of clustermethod i run on
number of clusters j.</p>
</td></tr>
<tr><td><code>noise</code></td>
<td>
<p>Two-dimensional list. The first list index i is the number
of the clustering method (ordering as specified in
<code>clustermethod</code>), the second list index j is the number of
clusters. List entries are single logicals. If <code>TRUE</code>, the
clustering method estimated some noise, i.e., points not belonging
to any cluster, which in the clustering vector are indicated by the
highest number (number of clusters plus one in case that the number
of clusters was fixed).</p>
</td></tr>
<tr><td><code>othernc</code></td>
<td>
<p>list of integer vectors of length 2. The first number is
the number of the clustering method (the order is determined by
argument <code>clustermethod</code>), the second number is the
number of clusters for those methods that estimate the number of
clusters themselves and estimate a number that is smaller than
<code>min(G)</code> or larger than <code>max(G)</code>.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2017) Cluster validation by measurement of clustering
characteristics relevant to the user. In C. H. Skiadas (ed.)
<em>Proceedings of ASMDA 2017</em>, 501-520,
<a href="https://arxiv.org/abs/1703.09282">https://arxiv.org/abs/1703.09282</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+clusterbenchstats">clusterbenchstats</a></code>, <code><a href="#topic+kmeansCBI">kmeansCBI</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
  set.seed(20000)
  options(digits=3)
  face &lt;- rFace(10,dMoNo=2,dNoEy=0,p=2)
  clustermethod=c("kmeansCBI","hclustCBI","hclustCBI")
# A clustering method can be used more than once, with different
# parameters
  clustermethodpars &lt;- list()
  clustermethodpars[[2]] &lt;- clustermethodpars[[3]] &lt;- list()
  clustermethodpars[[2]]$method &lt;- "complete"
  clustermethodpars[[3]]$method &lt;- "average"
  cmf &lt;-  cluster.magazine(face,G=2:3,clustermethod=clustermethod,
    distmethod=rep(FALSE,3),clustermethodpars=clustermethodpars)
  print(str(cmf))

</code></pre>

<hr>
<h2 id='cluster.stats'>Cluster validation statistics</h2><span id='topic+cluster.stats'></span>

<h3>Description</h3>

<p>Computes a number of distance based statistics, which can be used for cluster
validation, comparison between clusterings and decision about
the number of clusters: cluster sizes, cluster diameters,
average distances within and between clusters, cluster separation,
biggest within cluster gap, 
average silhouette widths, the Calinski and Harabasz index,
a Pearson version of
Hubert's gamma coefficient, the Dunn index and two indexes
to assess the similarity of two clusterings, namely the corrected Rand
index and Meila's VI.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster.stats(d = NULL, clustering, alt.clustering = NULL,
                           noisecluster=FALSE,
                              silhouette = TRUE, G2 = FALSE, G3 = FALSE,
                              wgap=TRUE, sepindex=TRUE, sepprob=0.1,
                              sepwithnoise=TRUE,
                              compareonly = FALSE,
                              aggregateonly = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster.stats_+3A_d">d</code></td>
<td>
<p>a distance object (as generated by <code>dist</code>) or a distance
matrix between cases.</p>
</td></tr>
<tr><td><code id="cluster.stats_+3A_clustering">clustering</code></td>
<td>
<p>an integer vector of length of the number of cases,
which indicates a clustering. The clusters have to be numbered
from 1 to the number of clusters.</p>
</td></tr>
<tr><td><code id="cluster.stats_+3A_alt.clustering">alt.clustering</code></td>
<td>
<p>an integer vector such as for
<code>clustering</code>, indicating an alternative clustering. If provided, the
corrected Rand index and Meila's VI for <code>clustering</code>
vs. <code>alt.clustering</code> are computed.</p>
</td></tr>
<tr><td><code id="cluster.stats_+3A_noisecluster">noisecluster</code></td>
<td>
<p>logical. If <code>TRUE</code>, it is assumed that the
largest cluster number in <code>clustering</code> denotes a 'noise
class', i.e. points that do not belong to any cluster. These points
are not taken into account for the computation of all functions of
within and between cluster distances including the validation
indexes.</p>
</td></tr> 
<tr><td><code id="cluster.stats_+3A_silhouette">silhouette</code></td>
<td>
<p>logical. If <code>TRUE</code>, the silhouette statistics
are computed, which requires package <code>cluster</code>.</p>
</td></tr>
<tr><td><code id="cluster.stats_+3A_g2">G2</code></td>
<td>
<p>logical. If <code>TRUE</code>, Goodman and Kruskal's index G2
(cf. Gordon (1999), p. 62) is computed. This executes lots of
sorting algorithms and can be very slow (it has been improved
by R. Francois - thanks!)</p>
</td></tr>
<tr><td><code id="cluster.stats_+3A_g3">G3</code></td>
<td>
<p>logical. If <code>TRUE</code>, the index G3
(cf. Gordon (1999), p. 62) is computed. This executes <code>sort</code>
on all distances and can be extremely slow.</p>
</td></tr>
<tr><td><code id="cluster.stats_+3A_wgap">wgap</code></td>
<td>
<p>logical. If <code>TRUE</code>, the widest within-cluster gaps
(largest link in within-cluster minimum spanning tree) are
computed. This is used for finding a good number of clusters in
Hennig (2013).</p>
</td></tr>
<tr><td><code id="cluster.stats_+3A_sepindex">sepindex</code></td>
<td>
<p>logical. If <code>TRUE</code>, a separation index is
computed, defined based on the distances for every point to the
closest point not in the same cluster. The separation index is then
the mean of the smallest proportion <code>sepprob</code> of these. This
allows to formalise separation less sensitive to a single or a few
ambiguous points. The output component corresponding to this is
<code>sindex</code>, not <code>separation</code>! This is used for finding a
good number of clusters in Hennig (2013).
</p>
</td></tr>
<tr><td><code id="cluster.stats_+3A_sepprob">sepprob</code></td>
<td>
<p>numerical between 0 and 1, see <code>sepindex</code>.</p>
</td></tr>
<tr><td><code id="cluster.stats_+3A_sepwithnoise">sepwithnoise</code></td>
<td>
<p>logical. If <code>TRUE</code> and <code>sepindex</code> and
<code>noisecluster</code> are both <code>TRUE</code>, the noise points are
incorporated as cluster in the separation index (<code>sepindex</code>)
computation. Also
they are taken into account for the computation for the minimum
cluster separation.</p>
</td></tr> 
<tr><td><code id="cluster.stats_+3A_compareonly">compareonly</code></td>
<td>
<p>logical. If <code>TRUE</code>, only the corrected Rand index
and Meila's VI are
computed and given out (this requires <code>alt.clustering</code> to be
specified).</p>
</td></tr>
<tr><td><code id="cluster.stats_+3A_aggregateonly">aggregateonly</code></td>
<td>
<p>logical. If <code>TRUE</code> (and not
<code>compareonly</code>), no clusterwise but only aggregated information
is given out (this cuts the size of the output down a bit).</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>cluster.stats</code> returns a list containing the components
<code>n, cluster.number, cluster.size,  min.cluster.size, noisen,
    diameter,
    average.distance, median.distance, separation, average.toother,
    separation.matrix, average.between, average.within,
    n.between, n.within, within.cluster.ss, clus.avg.silwidths, avg.silwidth,
    g2, g3, pearsongamma, dunn, entropy, wb.ratio, ch, cwidegap,
    widestgap, sindex, 
    corrected.rand, vi</code> except if <code>compareonly=TRUE</code>, in which case
only the last two components are computed.
</p>
<table>
<tr><td><code>n</code></td>
<td>
<p>number of cases.</p>
</td></tr>
<tr><td><code>cluster.number</code></td>
<td>
<p>number of clusters.</p>
</td></tr>
<tr><td><code>cluster.size</code></td>
<td>
<p>vector of cluster sizes (number of points).</p>
</td></tr>
<tr><td><code>min.cluster.size</code></td>
<td>
<p>size of smallest cluster.</p>
</td></tr>
<tr><td><code>noisen</code></td>
<td>
<p>number of noise points, see argument <code>noisecluster</code>
(<code>noisen=0</code> if <code>noisecluster=FALSE</code>).</p>
</td></tr>
<tr><td><code>diameter</code></td>
<td>
<p>vector of cluster diameters (maximum within cluster
distances).</p>
</td></tr>
<tr><td><code>average.distance</code></td>
<td>
<p>vector of clusterwise
within cluster average distances.</p>
</td></tr>
<tr><td><code>median.distance</code></td>
<td>
<p>vector of clusterwise
within cluster distance medians.</p>
</td></tr>
<tr><td><code>separation</code></td>
<td>
<p>vector of clusterwise minimum distances of a point
in the cluster to a point of another cluster.</p>
</td></tr>
<tr><td><code>average.toother</code></td>
<td>
<p>vector of clusterwise average distances of a point
in the cluster to the points of other clusters.</p>
</td></tr>
<tr><td><code>separation.matrix</code></td>
<td>
<p>matrix of separation values between all pairs
of clusters.</p>
</td></tr>
<tr><td><code>ave.between.matrix</code></td>
<td>
<p>matrix of mean dissimilarities between
points of every pair of clusters.</p>
</td></tr>
<tr><td><code>average.between</code></td>
<td>
<p>average distance between clusters.</p>
</td></tr>
<tr><td><code>average.within</code></td>
<td>
<p>average distance within clusters (reweighted so
that every observation, rather than every distance, has the same weight).</p>
</td></tr>
<tr><td><code>n.between</code></td>
<td>
<p>number of distances between clusters.</p>
</td></tr>
<tr><td><code>n.within</code></td>
<td>
<p>number of distances within clusters.</p>
</td></tr>
<tr><td><code>max.diameter</code></td>
<td>
<p>maximum cluster diameter.</p>
</td></tr>
<tr><td><code>min.separation</code></td>
<td>
<p>minimum cluster separation.</p>
</td></tr>
<tr><td><code>within.cluster.ss</code></td>
<td>
<p>a generalisation of the within clusters sum
of squares (k-means objective function), which is obtained if
<code>d</code> is a Euclidean distance matrix.  For general distance
measures, this is half
the sum of the within cluster squared dissimilarities divided by the
cluster size.</p>
</td></tr>
<tr><td><code>clus.avg.silwidths</code></td>
<td>
<p>vector of cluster average silhouette
widths. See
<code><a href="cluster.html#topic+silhouette">silhouette</a></code>.</p>
</td></tr>
<tr><td><code>avg.silwidth</code></td>
<td>
<p>average silhouette
width. See
<code><a href="cluster.html#topic+silhouette">silhouette</a></code>.</p>
</td></tr>
<tr><td><code>g2</code></td>
<td>
<p>Goodman and Kruskal's Gamma coefficient. See Milligan and
Cooper (1985), Gordon (1999, p. 62).</p>
</td></tr>
<tr><td><code>g3</code></td>
<td>
<p>G3 coefficient. See Gordon (1999, p. 62).</p>
</td></tr>
<tr><td><code>pearsongamma</code></td>
<td>
<p>correlation between distances and a
0-1-vector where 0 means same cluster, 1 means different clusters.
&quot;Normalized gamma&quot; in Halkidi et al. (2001).</p>
</td></tr>
<tr><td><code>dunn</code></td>
<td>
<p>minimum separation / maximum diameter. Dunn index, see
Halkidi et al. (2002).</p>
</td></tr>
<tr><td><code>dunn2</code></td>
<td>
<p>minimum average dissimilarity between two cluster /
maximum average within cluster dissimilarity, another version of
the family of Dunn indexes.</p>
</td></tr> 
<tr><td><code>entropy</code></td>
<td>
<p>entropy of the distribution of cluster memberships,
see Meila(2007).</p>
</td></tr>
<tr><td><code>wb.ratio</code></td>
<td>
<p><code>average.within/average.between</code>.</p>
</td></tr>
<tr><td><code>ch</code></td>
<td>
<p>Calinski and Harabasz index (Calinski and Harabasz 1974,
optimal in Milligan and Cooper 1985; generalised for dissimilarites
in Hennig and Liao 2013).</p>
</td></tr>
<tr><td><code>cwidegap</code></td>
<td>
<p>vector of widest within-cluster gaps.</p>
</td></tr>
<tr><td><code>widestgap</code></td>
<td>
<p>widest within-cluster gap.</p>
</td></tr>
<tr><td><code>sindex</code></td>
<td>
<p>separation index, see argument <code>sepindex</code>.</p>
</td></tr>
<tr><td><code>corrected.rand</code></td>
<td>
<p>corrected Rand index (if <code>alt.clustering</code>
has been specified), see Gordon (1999, p. 198).</p>
</td></tr>
<tr><td><code>vi</code></td>
<td>
<p>variation of information (VI) index (if <code>alt.clustering</code>
has been specified), see Meila (2007).</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Because <code>cluster.stats</code> processes a full dissimilarity matrix, it
isn't suitable for large data sets. You may consider
<code><a href="#topic+distcritmulti">distcritmulti</a></code> in that case.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Calinski, T., and Harabasz, J. (1974) A Dendrite Method for Cluster 
Analysis, <em>Communications in Statistics</em>, 3, 1-27.
</p>
<p>Gordon, A. D. (1999) <em>Classification</em>, 2nd ed. Chapman and Hall.
</p>
<p>Halkidi, M., Batistakis, Y., Vazirgiannis, M. (2001) On Clustering
Validation Techniques, <em>Journal of Intelligent Information
Systems</em>, 17, 107-145.
</p>
<p>Hennig, C. and Liao, T. (2013) How to find an appropriate clustering
for mixed-type variables with application to socio-economic
stratification, <em>Journal of the Royal Statistical Society, Series
C Applied Statistics</em>, 62, 309-369.
</p>
<p>Hennig, C. (2013) How many bee species? A case study in
determining the number of clusters. In: Spiliopoulou, L. Schmidt-Thieme, R. Janning (eds.): &quot;Data Analysis, Machine Learning and Knowledge Discovery&quot;, Springer, Berlin, 41-49. 
</p>
<p>Kaufman, L. and Rousseeuw, P.J. (1990). &quot;Finding Groups in Data:
An Introduction to Cluster Analysis&quot;. Wiley, New York.
</p>
<p>Meila, M. (2007) Comparing clusterings?an information based distance,
<em>Journal of Multivariate Analysis</em>, 98, 873-895.
</p>
<p>Milligan, G. W. and Cooper, M. C. (1985) An examination of procedures
for determining the number of clusters. <em>Psychometrika</em>, 50, 159-179.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cqcluster.stats">cqcluster.stats</a></code> is a more sophisticated version of
<code>cluster.stats</code> with more options.
<code><a href="cluster.html#topic+silhouette">silhouette</a></code>, <code><a href="stats.html#topic+dist">dist</a></code>, <code><a href="#topic+calinhara">calinhara</a></code>,
<code><a href="#topic+distcritmulti">distcritmulti</a></code>.
<code><a href="#topic+clusterboot">clusterboot</a></code> computes clusterwise stability statistics by
resampling. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
  set.seed(20000)
  options(digits=3)
  face &lt;- rFace(200,dMoNo=2,dNoEy=0,p=2)
  dface &lt;- dist(face)
  complete3 &lt;- cutree(hclust(dface),3)
  cluster.stats(dface,complete3,
                alt.clustering=as.integer(attr(face,"grouping")))
  
</code></pre>

<hr>
<h2 id='cluster.varstats'>Variablewise statistics for clusters</h2><span id='topic+cluster.varstats'></span><span id='topic+print.varwisetables'></span>

<h3>Description</h3>

<p>This function gives some helpful variable-wise information for cluster
interpretation, given a clustering and a data set. The output object
contains some tables. For categorical variables, tables compare
clusterwise distributions with overall distributions. Continuous
variables are categorised for this.
</p>
<p>If desired, tables, histograms, some standard statistics of
continuous variables and validation plots as available through
<code><a href="#topic+discrproj">discrproj</a></code> (Hennig 2004) are given out on the fly. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster.varstats(clustering,vardata,contdata=vardata,
                             clusterwise=TRUE,
                            tablevar=NULL,catvar=NULL,
                             quantvar=NULL, catvarcats=10,
                             proportions=FALSE,
                            projmethod="none",minsize=ncol(contdata)+2,
                          ask=TRUE,rangefactor=1)

## S3 method for class 'varwisetables'
print(x,digits=3,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster.varstats_+3A_clustering">clustering</code></td>
<td>
<p>vector of integers. Clustering (needs to be in
standard coding, 1,2,...).</p>
</td></tr>
<tr><td><code id="cluster.varstats_+3A_vardata">vardata</code></td>
<td>
<p>data matrix or data frame of which variables are
summarised.</p>
</td></tr>
<tr><td><code id="cluster.varstats_+3A_contdata">contdata</code></td>
<td>
<p>variable matrix or data frame, normally all or some
variables from <code>vardata</code>, on which cluster visualisation by
projection methods is performed unless <code>projmethod="none"</code>. It
should make sense to interpret these variables in a quantitative
(interval-scaled) way.</p>
</td></tr>
<tr><td><code id="cluster.varstats_+3A_clusterwise">clusterwise</code></td>
<td>
<p>logical. If <code>FALSE</code>, only the output tables
are computed but no more detail and graphs are given on the fly.</p>
</td></tr>
<tr><td><code id="cluster.varstats_+3A_tablevar">tablevar</code></td>
<td>
<p>vector of integers. Numbers of variables treated as
categorical (i.e., no histograms and statistics, just tables) if
<code>clusterwise=TRUE</code>. Note
that an error will be produced by factor type variables unless they
are declared as categorical here.</p>
</td></tr>
<tr><td><code id="cluster.varstats_+3A_catvar">catvar</code></td>
<td>
<p>vector of integers. Numbers of variables to be
categorised by proportional quantiles for table computation.
Recommended for all continuous variables.</p>
</td></tr>
<tr><td><code id="cluster.varstats_+3A_quantvar">quantvar</code></td>
<td>
<p>vector of integers. Variables for which means,
standard deviations and quantiles should be given out if
<code>clusterwise=TRUE</code>.</p>
</td></tr>
<tr><td><code id="cluster.varstats_+3A_catvarcats">catvarcats</code></td>
<td>
<p>integer. Number of categories used for
categorisation of variables specified in <code>quantvar</code>.</p>
</td></tr>
<tr><td><code id="cluster.varstats_+3A_proportions">proportions</code></td>
<td>
<p>logical. If <code>TRUE</code>, output tables contain
proportions, otherwise numbers of observations.</p>
</td></tr>
<tr><td><code id="cluster.varstats_+3A_projmethod">projmethod</code></td>
<td>
<p>one of <code>"none"</code>, <code>"dc"</code>, <code>"bc"</code>,
<code>"vbc"</code>, <code>"mvdc"</code>, <code>"adc"</code>, <code>"awc"</code> (recommended
if not <code>"none"</code>), <code>"arc"</code>, <code>"nc"</code>, <code>"wnc"</code>,
<code>"anc"</code>. Cluster validation projection method introduced in
Hennig (2004), passed on as <code>method</code> argument in
<code><a href="#topic+discrproj">discrproj</a></code>.</p>
</td></tr>
<tr><td><code id="cluster.varstats_+3A_minsize">minsize</code></td>
<td>
<p>integer. Projection is not carried out for clusters
with fewer points than this. (If this is chosen smaller, it may lead
to errors with some projection methods.)</p>
</td></tr>
<tr><td><code id="cluster.varstats_+3A_ask">ask</code></td>
<td>
<p>logical. If <code>TRUE</code>, <code>par(ask=TRUE)</code> is set in the
beginning to prompt the user before plots and <code>par(ask=FALSE)</code>
in the end.</p>
</td></tr>
<tr><td><code id="cluster.varstats_+3A_rangefactor">rangefactor</code></td>
<td>
<p>numeric. Factor by which to multiply the range for
projection plot ranges.</p>
</td></tr>
<tr><td><code id="cluster.varstats_+3A_x">x</code></td>
<td>
<p>an object of class <code>"varwisetables"</code>, output object of
<code>cluster.varstats</code>.</p>
</td></tr>
<tr><td><code id="cluster.varstats_+3A_digits">digits</code></td>
<td>
<p>integer. Number of digits after the decimal point to
print out.</p>
</td></tr>
<tr><td><code id="cluster.varstats_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"varwisetables"</code>, which is a
list with a table for each variable, giving (categorised) marginal
distributions by cluster.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en">https://www.unibo.it/sitoweb/christian.hennig/en</a></p>


<h3>References</h3>

<p>Hennig, C. (2004) Asymmetric linear dimension reduction for classification.
Journal of Computational and Graphical Statistics 13, 930-945 .
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(112233)
  options(digits=3)
  require(MASS)
  require(flexmix)
  data(Cars93)
  Cars934 &lt;- Cars93[,c(3,5,8,10)]
  cc &lt;-
    discrete.recode(Cars934,xvarsorted=FALSE,continuous=c(2,3),discrete=c(1,4))
  fcc &lt;- flexmix(cc$data~1,k=2,
  model=lcmixed(continuous=2,discrete=2,ppdim=c(6,3),diagonal=TRUE))
  cv &lt;-
    cluster.varstats(fcc@cluster,Cars934, contdata=Cars934[,c(2,3)],
    tablevar=c(1,4),catvar=c(2,3),quantvar=c(2,3),projmethod="awc",
    ask=FALSE)
  print(cv)
</code></pre>

<hr>
<h2 id='clusterbenchstats'>Run and validate many clusterings</h2><span id='topic+clusterbenchstats'></span><span id='topic+print.clusterbenchstats'></span>

<h3>Description</h3>

<p>This runs the methodology explained in Hennig (2019), Akhanli and
Hennig (2020). It runs a
user-specified set of clustering methods (CBI-functions, see
<code><a href="#topic+kmeansCBI">kmeansCBI</a></code>) with several numbers of clusters on a dataset,
and computes many cluster validation indexes. In order to explore the
variation of these indexes, random clusterings on the data are
generated, and validation indexes are standardised by use of the
random clusterings in order to make them comparable and differences
between values interpretable.
</p>
<p>The function <code><a href="#topic+print.valstat">print.valstat</a></code> can be used to provide
weights for the cluster
validation statistics, and will then compute a weighted validation index
that can be used to compare all clusterings.
</p>
<p>See the examples for how to get the indexes A1 and A2 from
Akhanli and Hennig (2020).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clusterbenchstats(data,G,diss = inherits(data, "dist"),
                                  scaling=TRUE, clustermethod,
                                  methodnames=clustermethod,
                              distmethod=rep(TRUE,length(clustermethod)),
                              ncinput=rep(TRUE,length(clustermethod)),
                              clustermethodpars,
                              npstats=FALSE,
                              useboot=FALSE,
                              bootclassif=NULL,
                              bootmethod="nselectboot",
                              bootruns=25,
                              trace=TRUE,
                              pamcrit=TRUE,snnk=2,
                              dnnk=2,
                              nnruns=100,kmruns=100,fnruns=100,avenruns=100,
                              multicore=FALSE,cores=detectCores()-1,
                              useallmethods=TRUE,
                              useallg=FALSE,...)

## S3 method for class 'clusterbenchstats'
print(x,...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clusterbenchstats_+3A_data">data</code></td>
<td>
<p>data matrix or <code>dist</code>-object.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_g">G</code></td>
<td>
<p>vector of integers. Numbers of clusters to consider.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_diss">diss</code></td>
<td>
<p>logical. If <code>TRUE</code>, the data matrix is assumed to be
a distance/dissimilariy matrix, otherwise it's observations times
variables.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_scaling">scaling</code></td>
<td>
<p>either a logical or a numeric vector of length equal to
the number of columns of <code>data</code>. If <code>FALSE</code>, data won't be
scaled, otherwise <code>scaling</code> is passed on to <code><a href="base.html#topic+scale">scale</a></code> as
argument<code>scale</code>.</p>
</td></tr> 
<tr><td><code id="clusterbenchstats_+3A_clustermethod">clustermethod</code></td>
<td>
<p>vector of strings specifying names of
CBI-functions (see <code><a href="#topic+kmeansCBI">kmeansCBI</a></code>). These are the
clustering methods to be applied.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_methodnames">methodnames</code></td>
<td>
<p>vector of strings with user-chosen names for
clustering methods, one for every method in
<code>clustermethod</code>. These can be used to distinguish different methods
run by the same CBI-function but with
different parameter values such as complete and average linkage for
<code><a href="#topic+hclustCBI">hclustCBI</a></code>.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_distmethod">distmethod</code></td>
<td>
<p>vector of logicals, of the same length as
<code>clustermethod</code>. <code>TRUE</code> means that the clustering method
operates on distances. If <code>diss=TRUE</code>, all entries have to be
<code>TRUE</code>. Otherwise, if an entry is true, the corresponding
method will be applied on <code>dist(data)</code>.</p>
</td></tr>  
<tr><td><code id="clusterbenchstats_+3A_ncinput">ncinput</code></td>
<td>
<p>vector of logicals, of the same length as
<code>clustermethod</code>. <code>TRUE</code> indicates that the corresponding
clustering method requires the number of clusters as input and will
not estimate the number of clusters itself. Only methods for which
this is <code>TRUE</code> can be used with <code>useboot=TRUE</code>.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_clustermethodpars">clustermethodpars</code></td>
<td>
<p>list of the same length as
<code>clustermethod</code>. Specifies parameters for all involved
clustering methods. Its jth entry is passed to clustermethod number
k. Can be an empty entry in case all defaults are used for a
clustering method. However, the last entry is not allowed to be
empty (you may just set a parameter of the last clustering method to
its default value if you don't want to specify anything else)! The
number of clusters does not need to be
specified here.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_npstats">npstats</code></td>
<td>
<p>logical. If <code>TRUE</code>, <code><a href="#topic+distrsimilarity">distrsimilarity</a></code>
is called and the two validity statistics computed there are
added. These require <code>diss=FALSE</code>.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_useboot">useboot</code></td>
<td>
<p>logical. If <code>TRUE</code>, a stability index (either
<code>nselectboot</code> or <code>prediction.strength</code>) will be involved.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_bootclassif">bootclassif</code></td>
<td>
<p>If <code>useboot=TRUE</code>, a vector of strings
indicating the
classification methods to be used with the stability index for the
different methods indicated in <code>clustermethods</code>, see the
<code>classification</code> argument of <code>nselectboot</code> and
<code>prediction.strength</code>.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_bootmethod">bootmethod</code></td>
<td>
<p>either <code>"nselectboot"</code> or
<code>"prediction.strength"</code>; stability index to be used if
<code>useboot=TRUE</code>.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_bootruns">bootruns</code></td>
<td>
<p>integer. Number of resampling runs. If
<code>useboot=TRUE</code>, passed on as <code>B</code> to
<code><a href="#topic+nselectboot">nselectboot</a></code> or
<code>M</code> to <code><a href="#topic+prediction.strength">prediction.strength</a></code>. Note that these are
applied to all <code>kmruns+nnruns+avenruns+fnruns</code> random
clusterings on top of the regular ones, which may take a lot of time
if <code>bootruns</code> and these values are chosen large.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_trace">trace</code></td>
<td>
<p>logical. If <code>TRUE</code>, some runtime information is
printed.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_pamcrit">pamcrit</code></td>
<td>
<p>logical. If <code>TRUE</code>, the average distance of points
to their respective cluster centroids is computed (criterion of the
PAM clustering method, validation criterion <code>pamc</code>); centroids
are chosen so that they minimise
this criterion for the given clustering. Passed on to
<code><a href="#topic+cqcluster.stats">cqcluster.stats</a></code>.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_snnk">snnk</code></td>
<td>
<p>integer. Number of neighbours used in coefficient of
variation of distance to nearest within cluster neighbour, the
<code>cvnnd</code>-statistic  (clusters
with <code>snnk</code> or fewer points are ignored for this). Passed on to
<code><a href="#topic+cqcluster.stats">cqcluster.stats</a></code> as argument <code>nnk</code>.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_dnnk">dnnk</code></td>
<td>
<p>integer. Number of nearest neighbors to use for
dissimilarity to the uniform in case that <code>npstats=TRUE</code>;
<code>nnk</code>-argument  to be passed on to <code><a href="#topic+distrsimilarity">distrsimilarity</a></code>.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_nnruns">nnruns</code></td>
<td>
<p>integer. Number of runs of <code><a href="#topic+stupidknn">stupidknn</a></code>
(random clusterings). With <code>useboot=TRUE</code> one may want to
choose this lower than the default for reasons of computation time.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_kmruns">kmruns</code></td>
<td>
<p>integer. Number of runs of
<code><a href="#topic+stupidkcentroids">stupidkcentroids</a></code> (random clusterings). With
<code>useboot=TRUE</code> one may want to
choose this lower than the default for reasons of computation time.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_fnruns">fnruns</code></td>
<td>
<p>integer. Number of runs of <code><a href="#topic+stupidkfn">stupidkfn</a></code>
(random clusterings).  With <code>useboot=TRUE</code> one may want to
choose this lower than the default for reasons of computation time.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_avenruns">avenruns</code></td>
<td>
<p>integer. Number of runs of <code><a href="#topic+stupidkaven">stupidkaven</a></code>
(random clusterings). With <code>useboot=TRUE</code> one may want to
choose this lower than the default for reasons of computation time.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_multicore">multicore</code></td>
<td>
<p>logical. If <code>TRUE</code>, parallel computing is used
through the function <code><a href="parallel.html#topic+mclapply">mclapply</a></code> from package
<code>parallel</code>; read warnings there if you intend to use this; it
won't work on Windows.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_cores">cores</code></td>
<td>
<p>integer. Number of cores for parallelisation.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_useallmethods">useallmethods</code></td>
<td>
<p>logical, to be passed on to
<code><a href="#topic+cgrestandard">cgrestandard</a></code>. If <code>FALSE</code>, only random clustering
results are used for standardisation. If
<code>TRUE</code>, clustering results from all methods are used.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_useallg">useallg</code></td>
<td>
<p>logical to be passed on to
<code><a href="#topic+cgrestandard">cgrestandard</a></code>. If <code>TRUE</code>, standardisation uses results
from all numbers of clusters in <code>G</code>. If <code>FALSE</code>,
standardisation of results for a specific number of cluster only
uses results from that number of clusters.</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_...">...</code></td>
<td>
<p>further arguments to be passed on to
<code><a href="#topic+cqcluster.stats">cqcluster.stats</a></code> through <code><a href="#topic+clustatsum">clustatsum</a></code> (no
effect in <code>print.clusterbenchstats</code>).</p>
</td></tr>
<tr><td><code id="clusterbenchstats_+3A_x">x</code></td>
<td>
<p>object of class <code>"clusterbenchstats"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The output of <code>clusterbenchstats</code> is a
big list of lists comprising lists <code>cm, stat, sim, qstat,
    sstat</code>
</p>
<table>
<tr><td><code>cm</code></td>
<td>
<p>output object of <code><a href="#topic+cluster.magazine">cluster.magazine</a></code>, see there
for details. Clustering of all methods and numbers of clusters on
the dataset <code>data</code>.</p>
</td></tr></table>
<p>. 
</p>
<table>
<tr><td><code>stat</code></td>
<td>
<p>object of class <code>"valstat"</code>, see
<code><a href="#topic+valstat.object">valstat.object</a></code> for details. Unstandardised cluster
validation statistics.</p>
</td></tr>
<tr><td><code>sim</code></td>
<td>
<p>output object of <code><a href="#topic+randomclustersim">randomclustersim</a></code>, see there.
validity indexes from random clusterings used for standardisation of
validation statistics on <code>data</code>.</p>
</td></tr>
<tr><td><code>qstat</code></td>
<td>
<p>object of class <code>"valstat"</code>, see
<code><a href="#topic+valstat.object">valstat.object</a></code> for details. Cluster validation
statistics standardised by random clusterings, output of
<code><a href="#topic+cgrestandard">cgrestandard</a></code> based on percentages, i.e., with
<code>percentage=TRUE</code>.</p>
</td></tr>
<tr><td><code>sstat</code></td>
<td>
<p>object of class <code>"valstat"</code>, see
<code><a href="#topic+valstat.object">valstat.object</a></code> for details. Cluster validation
statistics standardised by random clusterings, output of
<code><a href="#topic+cgrestandard">cgrestandard</a></code> based on mean and standard deviation
(called Z-score standardisation in Akhanli and Hennig (2020),
i.e., with <code>percentage=FALSE</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This may require a lot of computing time and also memory for datasets
that are not small, as most indexes require computation and storage of
distances. 
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2019) Cluster validation by measurement of clustering
characteristics relevant to the user. In C. H. Skiadas (ed.)
<em>Data Analysis and Applications 1: Clustering and Regression,
Modeling-estimating, Forecasting and Data Mining, Volume 2</em>, Wiley,
New York 1-24,
<a href="https://arxiv.org/abs/1703.09282">https://arxiv.org/abs/1703.09282</a>
</p>
<p>Akhanli, S. and Hennig, C. (2020) Calibrating and aggregating cluster
validity indexes for context-adapted comparison of clusterings.
<em>Statistics and Computing</em>, 30, 1523-1544,
<a href="https://link.springer.com/article/10.1007/s11222-020-09958-2">https://link.springer.com/article/10.1007/s11222-020-09958-2</a>, <a href="https://arxiv.org/abs/2002.01822">https://arxiv.org/abs/2002.01822</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+valstat.object">valstat.object</a></code>,
<code><a href="#topic+cluster.magazine">cluster.magazine</a></code>, <code><a href="#topic+kmeansCBI">kmeansCBI</a></code>,
<code><a href="#topic+cqcluster.stats">cqcluster.stats</a></code>, <code><a href="#topic+clustatsum">clustatsum</a></code>,
<code><a href="#topic+cgrestandard">cgrestandard</a></code>  
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
  set.seed(20000)
  options(digits=3)
  face &lt;- rFace(10,dMoNo=2,dNoEy=0,p=2)
  clustermethod=c("kmeansCBI","hclustCBI")
# A clustering method can be used more than once, with different
# parameters
  clustermethodpars &lt;- list()
  clustermethodpars[[2]] &lt;- list()
  clustermethodpars[[2]]$method &lt;- "average"
# Last element of clustermethodpars needs to have an entry!
  methodname &lt;- c("kmeans","average")
  cbs &lt;-  clusterbenchstats(face,G=2:3,clustermethod=clustermethod,
    methodname=methodname,distmethod=rep(FALSE,2),
    clustermethodpars=clustermethodpars,nnruns=1,kmruns=1,fnruns=1,avenruns=1)
  print(cbs)
  print(cbs$qstat,aggregate=TRUE,weights=c(1,0,0,0,0,1,0,1,0,1,0,1,0,0,1,1))
# The weights are weights for the validation statistics ordered as in
# cbs$qstat$statistics for computation of an aggregated index, see
# ?print.valstat.

# Now using bootstrap stability assessment as in Akhanli and Hennig (2020):
  bootclassif &lt;- c("centroid","averagedist")
  cbsboot &lt;- clusterbenchstats(face,G=2:3,clustermethod=clustermethod,
    methodname=methodname,distmethod=rep(FALSE,2),
    clustermethodpars=clustermethodpars,
    useboot=TRUE,bootclassif=bootclassif,bootmethod="nselectboot",
    bootruns=2,nnruns=1,kmruns=1,fnruns=1,avenruns=1,useallg=TRUE)
  print(cbsboot)
## Not run: 
# Index A1 in Akhanli and Hennig (2020) (need these weights choices):
  print(cbsboot$sstat,aggregate=TRUE,weights=c(1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0))
# Index A2 in Akhanli and Hennig (2020) (need these weights choices):
  print(cbsboot$sstat,aggregate=TRUE,weights=c(0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0))

## End(Not run)

# Results from nselectboot:
  plot(cbsboot$stat,cbsboot$sim,statistic="boot")
</code></pre>

<hr>
<h2 id='clusterboot'>Clusterwise cluster stability assessment by resampling</h2><span id='topic+clusterboot'></span><span id='topic+print.clboot'></span><span id='topic+plot.clboot'></span>

<h3>Description</h3>

<p>Assessment of the clusterwise stability of a clustering of data, which
can be cases*variables or dissimilarity data. The data is resampled
using several schemes (bootstrap, subsetting, jittering, replacement
of points by noise) and the Jaccard similarities of the original
clusters to the most similar clusters in the resampled data are
computed. The mean over these similarities is used as an index of the
stability of a cluster (other statistics can be computed as well). The
methods are described in Hennig (2007).
</p>
<p><code>clusterboot</code> is an integrated function that computes the
clustering as well, using interface functions for various
clustering methods implemented in R (several interface functions are
provided, but you can
implement further ones for your favourite clustering method). See the
documentation of the input parameter <code>clustermethod</code> below.
</p>
<p>Quite general clustering methods are possible, i.e. methods estimating
or fixing the number of clusters, methods producing overlapping
clusters or not assigning all cases to clusters (but declaring them as
&quot;noise&quot;). Fuzzy clusterings cannot be processed and have to be
transformed to crisp clusterings by the interface function.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clusterboot(data,B=100, distances=(inherits(data, "dist")),
                        bootmethod="boot",
                        bscompare=TRUE, 
                        multipleboot=FALSE,
                        jittertuning=0.05, noisetuning=c(0.05,4),
                        subtuning=floor(nrow(data)/2),
                        clustermethod,noisemethod=FALSE,count=TRUE,
                        showplots=FALSE,dissolution=0.5,
                        recover=0.75,seed=NULL,datatomatrix=TRUE,...)

## S3 method for class 'clboot'
print(x,statistics=c("mean","dissolution","recovery"),...)

## S3 method for class 'clboot'
plot(x,xlim=c(0,1),breaks=seq(0,1,by=0.05),...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clusterboot_+3A_data">data</code></td>
<td>
<p>by default something that can be coerced into a
(numerical) matrix (data frames with non-numerical data are allowed
when using <code>datatomatrix=FALSE</code>, see below). The data matrix -
either an <code>n*p</code>-data matrix (or data frame) or an
<code>n*n</code>-dissimilarity matrix (or
<code>dist</code>-object).</p>
</td></tr>
<tr><td><code id="clusterboot_+3A_b">B</code></td>
<td>
<p>integer. Number of resampling runs for each scheme, see
<code>bootmethod</code>.</p>
</td></tr>
<tr><td><code id="clusterboot_+3A_distances">distances</code></td>
<td>
<p>logical. If <code>TRUE</code>, the data is interpreted as
dissimilarity matrix. If <code>data</code> is a <code>dist</code>-object,
<code>distances=TRUE</code> automatically, otherwise
<code>distances=FALSE</code> by default. This means that you have to set
it to <code>TRUE</code> manually if <code>data</code> is a dissimilarity matrix.</p>
</td></tr>
<tr><td><code id="clusterboot_+3A_bootmethod">bootmethod</code></td>
<td>
<p>vector of strings, defining the methods used for
resampling. Possible methods:
</p>
<p><code>"boot"</code>: nonparametric bootstrap (precise behaviour is
controlled by parameters <code>bscompare</code> and
<code>multipleboot</code>).
</p>
<p><code>"subset"</code>: selecting random subsets from the dataset. Size
determined by <code>subtuning</code>.
</p>
<p><code>"noise"</code>: replacing a certain percentage of the points by
random noise, see <code>noisetuning</code>.
</p>
<p><code>"jitter"</code> add random noise to all points, see
<code>jittertuning</code>. (This didn't perform well in Hennig (2007),
but you may want to get your own experience.)
</p>
<p><code>"bojit"</code> nonparametric bootstrap first, and then adding
noise to the points, see <code>jittertuning</code>.
</p>
<p><strong>Important:</strong> only the methods <code>"boot"</code> and
<code>"subset"</code> work with dissimilarity data, or if
<code>datatomatrix=FALSE</code>!
</p>
<p>The results in Hennig (2007) indicate that <code>"boot"</code> is
generally informative and often quite similar to <code>"subset"</code> and
<code>"bojit"</code>, while <code>"noise"</code> sometimes provides different
information. Therefore the default (for <code>distances=FALSE</code>) is
to use <code>"boot"</code> and <code>"noise"</code>. However, some clustering
methods may have problems with multiple points, which can be solved
by using <code>"bojit"</code> or <code>"subset"</code> instead of <code>"boot"</code> or by
<code>multipleboot=FALSE</code> below.</p>
</td></tr>
<tr><td><code id="clusterboot_+3A_bscompare">bscompare</code></td>
<td>
<p>logical. If <code>TRUE</code>, multiple points in the
bootstrap sample are taken into account to compute the Jaccard
similarity to the original clusters (which are represented by their
&quot;bootstrap versions&quot;, i.e., the
points of the original cluster which also occur in the bootstrap
sample). If a point was drawn more than once, it is in the &quot;bootstrap
version&quot; of the original cluster more than once, too, if
<code>bscompare=TRUE</code>. Otherwise multiple points are
ignored for the computation of the Jaccard similarities. If
<code>multipleboot=FALSE</code>, it doesn't make a difference.</p>
</td></tr> 
<tr><td><code id="clusterboot_+3A_multipleboot">multipleboot</code></td>
<td>
<p>logical. If <code>FALSE</code>, all points drawn more
than once in the bootstrap draw are only used once in the bootstrap
samples.</p>
</td></tr>
<tr><td><code id="clusterboot_+3A_jittertuning">jittertuning</code></td>
<td>
<p>positive numeric. Tuning for the
<code>"jitter"</code>-method. The noise distribution for
jittering is a normal distribution with zero mean. The covariance
matrix has the same Eigenvectors as that of the original
data set, but the standard deviation along the principal directions is
determined by the <code>jittertuning</code>-quantile of the distances
between neighboring points projected along these directions.</p>
</td></tr>
<tr><td><code id="clusterboot_+3A_noisetuning">noisetuning</code></td>
<td>
<p>A vector of two positive numerics. Tuning for the
<code>"noise"</code>-method. The first component determines the
probability that a point is replaced by noise. Noise is generated by
a uniform distribution on a hyperrectangle along the principal
directions of the original data set, ranging from
<code>-noisetuning[2]</code> to <code>noisetuning[2]</code> times the standard
deviation of the data set along the respective direction. Note that
only points not replaced by noise are considered for the computation
of Jaccard similarities.</p>
</td></tr>
<tr><td><code id="clusterboot_+3A_subtuning">subtuning</code></td>
<td>
<p>integer. Size of subsets for <code>"subset"</code>.</p>
</td></tr>
<tr><td><code id="clusterboot_+3A_clustermethod">clustermethod</code></td>
<td>
<p>an interface function (the function name, not a
string containing the name, has to be provided!). This defines the
clustering method. See the &quot;Details&quot;-section for a list of available
interface functions and guidelines how to write your own ones. 
</p>
</td></tr>
<tr><td><code id="clusterboot_+3A_noisemethod">noisemethod</code></td>
<td>
<p>logical. If <code>TRUE</code>, the last cluster is
regarded as &quot;noise cluster&quot;, which means that for computing the Jaccard
similarity, it is not treated as a cluster. The noise cluster of
the original clustering is only compared with the noise cluster of
the clustering of the resampled data. This means that in the
<code>clusterboot</code>-output (and plot), if points were assigned to the
noise cluster, the last cluster number refers to it, and its
Jaccard similarity values refer to comparisons with estimated noise
components in resampled datasets only.
(Some cluster methods such as
<code><a href="tclust.html#topic+tclust">tclust</a></code> and
<code><a href="mclust.html#topic+mclustBIC">mclustBIC</a></code> produce such noise
components.)</p>
</td></tr>
<tr><td><code id="clusterboot_+3A_count">count</code></td>
<td>
<p>logical. If <code>TRUE</code>, the resampling runs are counted
on the screen.</p>
</td></tr>    
<tr><td><code id="clusterboot_+3A_showplots">showplots</code></td>
<td>
<p>logical. If <code>TRUE</code>, a plot of the first two
dimensions of the resampled data set (or the classical MDS solution
for dissimilarity data) is shown for every resampling run. The last
plot shows the original data set. Ignored if <code>datatomatrix=FALSE</code>.</p>
</td></tr>
<tr><td><code id="clusterboot_+3A_dissolution">dissolution</code></td>
<td>
<p>numeric between 0 and 1. If the Jaccard similarity
between the resampling version of the original cluster and the most
similar cluster on the resampled data is smaller or equal to this
value, the cluster is considered as &quot;dissolved&quot;. Numbers of
dissolved clusters are recorded.</p>
</td></tr>
<tr><td><code id="clusterboot_+3A_recover">recover</code></td>
<td>
<p>numeric between 0 and 1. If the Jaccard similarity
between the resampling version of the original cluster and the most
similar cluster on the resampled data is larger than  this
value, the cluster is considered as &quot;successfully recovered&quot;. Numbers of
recovered clusters are recorded.</p>
</td></tr>
<tr><td><code id="clusterboot_+3A_seed">seed</code></td>
<td>
<p>integer. Seed for random generator (fed into
<code>set.seed</code>) to make results reproducible. If <code>NULL</code>,
results depend on chance.</p>
</td></tr>
<tr><td><code id="clusterboot_+3A_datatomatrix">datatomatrix</code></td>
<td>
<p>logical. If <code>TRUE</code>, <code>data</code> is coerced
into a (numerical) matrix at the start of
<code>clusterboot</code>. <code>FALSE</code> may be chosen for mixed type data
including e.g. categorical factors (assuming that the chosen
<code>clustermethod</code> allows for this). This disables some features
of <code>clusterboot</code>, see parameters <code>bootmethod</code> and
<code>showplots</code>.</p>
</td></tr>  
<tr><td><code id="clusterboot_+3A_...">...</code></td>
<td>
<p>additional parameters for the clustermethods called by
<code>clusterboot</code>. No effect in <code>print.clboot</code> and
<code>plot.clboot</code>.</p>
</td></tr>
<tr><td><code id="clusterboot_+3A_x">x</code></td>
<td>
<p>object of class <code>clboot</code>.</p>
</td></tr>
<tr><td><code id="clusterboot_+3A_statistics">statistics</code></td>
<td>
<p>specifies in <code>print.clboot</code>,
which of the three clusterwise Jaccard
similarity statistics <code>"mean"</code>, <code>"dissolution"</code> (number of
times the cluster has been dissolved) and <code>"recovery"</code> (number
of times a cluster has been successfully recovered) is printed.</p>
</td></tr>
<tr><td><code id="clusterboot_+3A_xlim">xlim</code></td>
<td>
<p>transferred to <code>hist</code>.</p>
</td></tr>
<tr><td><code id="clusterboot_+3A_breaks">breaks</code></td>
<td>
<p>transferred to <code>hist</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Here are some guidelines for interpretation.
There is some theoretical justification to consider a Jaccard
similarity value smaller or equal to 0.5 as an indication of a
&quot;dissolved cluster&quot;, see Hennig (2008). Generally, a valid, stable
cluster should yield a mean Jaccard similarity value of 0.75 or more.
Between 0.6 and 0.75, clusters may be considered as indicating
patterns in the data, but which points exactly should belong to these
clusters is highly doubtful. Below average Jaccard values of 0.6, clusters
should not be trusted. &quot;Highly stable&quot; clusters should yield average
Jaccard similarities of 0.85 and above. All of this refers to
bootstrap; for the other resampling schemes it depends on the tuning
constants, though their default values should grant similar
interpretations in most cases.
</p>
<p>While <code>B=100</code> is recommended, smaller run numbers could give
quite informative results as well, if computation times become too high.
</p>
<p>Note that the stability of a cluster is assessed, but
stability is not the only important validity criterion - clusters
obtained by very inflexible clustering methods may be stable but not
valid, as discussed in Hennig (2007).
See <code><a href="#topic+plotcluster">plotcluster</a></code> for graphical cluster validation.
</p>
<p>Information about interface functions for clustering methods:
</p>
<p>The following interface functions are currently
implemented (in the present package; note that almost all of these
functions require the specification of some control parameters, so
if you use one of them, look up their common help page
<code><a href="#topic+kmeansCBI">kmeansCBI</a></code>) first:
</p>

<dl>
<dt>kmeansCBI</dt><dd><p>an interface to the function
<code><a href="stats.html#topic+kmeans">kmeans</a></code> for k-means clustering. This assumes a
cases*variables matrix as input.</p>
</dd>
<dt>hclustCBI</dt><dd><p>an interface to the function
<code>hclust</code> for agglomerative hierarchical clustering with
optional noise cluster. This
function produces a partition and assumes a cases*variables
matrix as input.</p>
</dd>
<dt>hclusttreeCBI</dt><dd><p>an interface to the function
<code>hclust</code> for agglomerative hierarchical clustering. This
function produces a tree (not only a partition; therefore the
number of clusters can be huge!) and assumes a cases*variables
matrix as input.</p>
</dd>
<dt>disthclustCBI</dt><dd><p>an interface to the function
<code>hclust</code> for agglomerative hierarchical clustering with
optional noise cluster. This
function produces a partition and assumes a dissimilarity
matrix as input.</p>
</dd>
<dt>noisemclustCBI</dt><dd><p>an interface to the function
<code><a href="mclust.html#topic+mclustBIC">mclustBIC</a></code> for normal mixture model based
clustering. This assumes a cases*variables matrix as
input. Warning: <code><a href="mclust.html#topic+mclustBIC">mclustBIC</a></code> sometimes has
problems with multiple
points. It is recommended to use this only together with
<code>multipleboot=FALSE</code>.</p>
</dd>
<dt>distnoisemclustCBI</dt><dd><p>an interface to the function
<code><a href="mclust.html#topic+mclustBIC">mclustBIC</a></code> for normal mixture model based
clustering. This assumes a dissimilarity matrix as input and
generates a data matrix by multidimensional scaling first.
Warning: <code><a href="mclust.html#topic+mclustBIC">mclustBIC</a></code> sometimes has
problems with multiple
points. It is recommended to use this only together with
<code>multipleboot=FALSE</code>.</p>
</dd>
<dt>claraCBI</dt><dd><p>an interface to the functions
<code><a href="cluster.html#topic+pam">pam</a></code> and <code><a href="cluster.html#topic+clara">clara</a></code>
for partitioning around medoids. This can be used with
cases*variables as well as dissimilarity matrices as input.</p>
</dd>
<dt>pamkCBI</dt><dd><p>an interface to the function
<code><a href="#topic+pamk">pamk</a></code> for partitioning around medoids. The number
of cluster is estimated by the average silhouette width.
This can be used with
cases*variables as well as dissimilarity matrices as input.</p>
</dd>



<dt>tclustCBI</dt><dd><p>an interface to the function
<code>tclust</code> in the tclust library for trimmed Gaussian 
clustering. This assumes a cases*variables matrix as input. Note
that this function is not currently provided because the tclust
package is only available in the CRAN archives, but the code is
in the Examples-section of the <code><a href="#topic+kmeansCBI">kmeansCBI</a></code>-help page.</p>
</dd>




<dt>dbscanCBI</dt><dd><p>an interface to the function
<code><a href="#topic+dbscan">dbscan</a></code> for density based 
clustering. This can be used with
cases*variables as well as dissimilarity matrices as input..</p>
</dd>
<dt>mahalCBI</dt><dd><p>an interface to the function
<code><a href="#topic+fixmahal">fixmahal</a></code> for fixed point
clustering. This assumes a cases*variables matrix as input.</p>
</dd>
<dt>mergenormCBI</dt><dd><p>an interface to the function
<code><a href="#topic+mergenormals">mergenormals</a></code> for clustering by merging Gaussian
mixture components.</p>
</dd>
<dt>speccCBI</dt><dd><p>an interface to the function
<code><a href="kernlab.html#topic+specc">specc</a></code> for spectral clustering.</p>
</dd>
</dl>

<p>You can write your own interface function. The first argument of an
interface function should preferably be a data matrix (of class
&quot;matrix&quot;, but it may be a symmetrical dissimilarity matrix). It can
be a data frame, but this restricts some of the functionality of
<code>clusterboot</code>, see above. Further
arguments can be tuning constants for the clustering method. The
output of an interface function should be a list containing (at
least) the following components:
</p>

<dl>
<dt>result</dt><dd><p>clustering result, usually a list with the full
output of the clustering method (the precise format doesn't
matter); whatever you want to use later.</p>
</dd>
<dt>nc</dt><dd><p>number of clusters. If some points don't belong to any
cluster but are declared as &quot;noise&quot;, <code>nc</code> includes the
noise cluster, and there should be another component
<code>nccl</code>, being the number of clusters not including the
noise cluster (note that it is not mandatory to define a noise
component if not all points are assigned to clusters, but if you
do it, the stability of the noise cluster is assessed as
well.)</p>
</dd>
<dt>clusterlist</dt><dd><p>this is a list consisting of a logical vectors
of length of the number of data points (<code>n</code>) for each cluster,
indicating whether a point is a member of this cluster
(<code>TRUE</code>) or not. If a noise cluster is included, it
should always be the last vector in this list.</p>
</dd>
<dt>partition</dt><dd><p>an integer vector of length <code>n</code>,
partitioning the data. If the method produces a partition, it
should be the clustering. This component is only used for plots,
so you could do something like <code>rep(1,n)</code> for
non-partitioning methods. If a noise cluster is included,
<code>nc=nccl+1</code> and the noise cluster is cluster no. <code>nc</code>.</p>
</dd>
<dt>clustermethod</dt><dd><p>a string indicating the clustering method.</p>
</dd>
</dl>
      


<h3>Value</h3>

<p><code>clusterboot</code> returns an object of class <code>"clboot"</code>, which
is a list with components
<code>result, partition, nc, clustermethod, B, noisemethod, bootmethod,
    multipleboot, dissolution, recover, bootresult, bootmean, bootbrd,
    bootrecover, jitterresult, jittermean, jitterbrd, jitterrecover,
    subsetresult, subsetmean, subsetbrd, subsetrecover, bojitresult,
    bojitmean, bojitbrd, bojitrecover, noiseresult, noisemean, 
    noisebrd, noiserecover</code>.
</p>
<table>
<tr><td><code>result</code></td>
<td>
<p>clustering result; full output of the selected
<code>clustermethod</code> for the original data set.</p>
</td></tr>
<tr><td><code>partition</code></td>
<td>
<p>partition parameter of the selected <code>clustermethod</code>
(note that this is only meaningful for partitioning clustering methods).</p>
</td></tr>
<tr><td><code>nc</code></td>
<td>
<p>number of clusters in original data (including noise
component if <code>noisemethod=TRUE</code>).</p>
</td></tr>
<tr><td><code>nccl</code></td>
<td>
<p>number of clusters in original data (not including noise
component if <code>noisemethod=TRUE</code>).</p>
</td></tr>
<tr><td><code>clustermethod</code>, <code>B</code>, <code>noisemethod</code>, <code>bootmethod</code>, <code>multipleboot</code>, <code>dissolution</code>, <code>recover</code></td>
<td>
<p>input parameters, see above.</p>
</td></tr>
<tr><td><code>bootresult</code></td>
<td>
<p>matrix of Jaccard similarities for
<code>bootmethod="boot"</code>. Rows correspond to clusters in the
original data set. Columns correspond to bootstrap runs.</p>
</td></tr>
<tr><td><code>bootmean</code></td>
<td>
<p>clusterwise means of the <code>bootresult</code>.</p>
</td></tr>
<tr><td><code>bootbrd</code></td>
<td>
<p>clusterwise number of times a cluster has been dissolved.</p>
</td></tr>
<tr><td><code>bootrecover</code></td>
<td>
<p>clusterwise number of times a cluster has been
successfully recovered.</p>
</td></tr>
<tr><td><code>subsetresult</code>, <code>subsetmean</code>, <code>etc.</code></td>
<td>
<p>same as <code>bootresult,
      bootmean, etc.</code>, but for the other resampling methods.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2007) Cluster-wise assessment of cluster
stability. <em>Computational Statistics and Data Analysis</em>,
52, 258-271.
</p>
<p>Hennig, C. (2008)  Dissolution point and isolation robustness:
robustness criteria for general cluster analysis methods.
<em>Journal of Multivariate Analysis</em> 99, 1154-1176.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dist">dist</a></code>,
interface functions:
<code><a href="#topic+kmeansCBI">kmeansCBI</a></code>, <code><a href="#topic+hclustCBI">hclustCBI</a></code>,
<code><a href="#topic+hclusttreeCBI">hclusttreeCBI</a></code>, <code><a href="#topic+disthclustCBI">disthclustCBI</a></code>,
<code><a href="#topic+noisemclustCBI">noisemclustCBI</a></code>, <code><a href="#topic+distnoisemclustCBI">distnoisemclustCBI</a></code>,
<code><a href="#topic+claraCBI">claraCBI</a></code>, <code><a href="#topic+pamkCBI">pamkCBI</a></code>,
<code><a href="#topic+dbscanCBI">dbscanCBI</a></code>, <code><a href="#topic+mahalCBI">mahalCBI</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  options(digits=3)
  set.seed(20000)
  face &lt;- rFace(50,dMoNo=2,dNoEy=0,p=2)
  cf1 &lt;- clusterboot(face,B=3,bootmethod=
          c("boot","noise","jitter"),clustermethod=kmeansCBI,
          krange=5,seed=15555)

  print(cf1)
  plot(cf1)


  cf2 &lt;- clusterboot(dist(face),B=3,bootmethod=
          "subset",clustermethod=disthclustCBI,
          k=5, cut="number", method="average", showplots=TRUE, seed=15555)
  print(cf2)
  d1 &lt;- c("a","b","a","c")
  d2 &lt;- c("a","a","a","b")
  dx &lt;- as.data.frame(cbind(d1,d2))
  cpx &lt;- clusterboot(dx,k=2,B=10,clustermethod=claraCBI,
          multipleboot=TRUE,usepam=TRUE,datatomatrix=FALSE)
  print(cpx)
</code></pre>

<hr>
<h2 id='cmahal'>Generation of tuning constant for Mahalanobis fixed point clusters.</h2><span id='topic+cmahal'></span>

<h3>Description</h3>

<p>Generates tuning constants <code>ca</code>
for <code><a href="#topic+fixmahal">fixmahal</a></code> dependent on
the number of points and variables of the current fixed point cluster
(FPC).
</p>
<p>This is experimental and only thought for use in <code><a href="#topic+fixmahal">fixmahal</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cmahal(n, p, nmin, cmin, nc1, c1 = cmin, q = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cmahal_+3A_n">n</code></td>
<td>
<p>positive integer. Number of points.</p>
</td></tr>
<tr><td><code id="cmahal_+3A_p">p</code></td>
<td>
<p>positive integer. Number of variables.</p>
</td></tr>
<tr><td><code id="cmahal_+3A_nmin">nmin</code></td>
<td>
<p>integer larger than 1. Smallest number of points for which
<code>ca</code> is computed. For smaller FPC sizes, <code>ca</code> is set to
the value for <code>nmin</code>.</p>
</td></tr>
<tr><td><code id="cmahal_+3A_cmin">cmin</code></td>
<td>
<p>positive number. Minimum value for <code>ca</code>.</p>
</td></tr>
<tr><td><code id="cmahal_+3A_nc1">nc1</code></td>
<td>
<p>positive integer. Number of points at which <code>ca=c1</code>.</p>
</td></tr>
<tr><td><code id="cmahal_+3A_c1">c1</code></td>
<td>
<p>positive numeric. Tuning constant for <code>cmahal</code>.
Value for <code>ca</code> for FPC size equal to <code>nc1</code>.</p>
</td></tr>
<tr><td><code id="cmahal_+3A_q">q</code></td>
<td>
<p>numeric between 0 and 1. 1 for steepest possible descent of
<code>ca</code> as function of the FPC size. Should presumably always be 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Some experiments suggest that the tuning constant <code>ca</code> should
decrease with increasing FPC size and increase with increasing
<code>p</code> in <code><a href="#topic+fixmahal">fixmahal</a></code>. This is to prevent too small
meaningless FPCs while maintaining the significant larger
ones. <code>cmahal</code> with <code>q=1</code> computes <code>ca</code> in such a way
that as long as <code>ca&gt;cmin</code>, the decrease in <code>n</code> is as steep
as possible in order to maintain the validity of the convergence
theorem in Hennig and Christlieb (2002).
</p>


<h3>Value</h3>

<p>A numeric vector of length <code>n</code>, giving the values for <code>ca</code>
for all FPC sizes smaller or equal to <code>n</code>.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>References</h3>

<p>Hennig, C. and Christlieb, N. (2002) Validating visual clusters in
large datasets: Fixed point clusters of spectral features,
<em>Computational Statistics and Data Analysis</em> 40, 723-739.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fixmahal">fixmahal</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  plot(1:100,cmahal(100,3,nmin=5,cmin=qchisq(0.99,3),nc1=90),
       xlab="FPC size", ylab="cmahal")
</code></pre>

<hr>
<h2 id='con.comp'>Connectivity components of an undirected graph</h2><span id='topic+con.comp'></span>

<h3>Description</h3>

<p>Computes the connectivity components of an undirected graph from a
matrix giving the edges.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>con.comp(comat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="con.comp_+3A_comat">comat</code></td>
<td>
<p>a symmetric logical or 0-1 matrix, where <code>comat[i,j]=TRUE</code>
means that there is an edge between vertices <code>i</code> and
<code>j</code>. The diagonal is ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The &quot;depth-first search&quot; algorithm of Cormen, Leiserson and Rivest
(1990, p. 477) is used.
</p>


<h3>Value</h3>

<p>An integer vector, giving the number of the connectivity component for
each vertice.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Cormen, T. H., Leiserson, C. E. and Rivest, R. L. (1990), <em>Introduction
to Algorithms</em>, Cambridge: MIT Press.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+hclust">hclust</a></code>, <code><a href="stats.html#topic+cutree">cutree</a></code> for cutted single linkage
trees (often equivalent).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(1000)
  x &lt;- rnorm(20)
  m &lt;- matrix(0,nrow=20,ncol=20)
  for(i in 1:20)
    for(j in 1:20)
      m[i,j] &lt;- abs(x[i]-x[j])
  d &lt;- m&lt;0.2
  cc &lt;- con.comp(d)
  max(cc) # number of connectivity components
  plot(x,cc)
  # The same should be produced by
  # cutree(hclust(as.dist(m),method="single"),h=0.2).
</code></pre>

<hr>
<h2 id='confusion'>Misclassification probabilities in mixtures</h2><span id='topic+confusion'></span>

<h3>Description</h3>

<p>Estimates a misclassification probability in a mixture distribution
between two mixture components from estimated posterior probabilities
regardless of component parameters, see Hennig (2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confusion(z,pro,i,j,adjustprobs=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confusion_+3A_z">z</code></td>
<td>
<p>matrix of posterior probabilities for observations (rows) to
belong to mixture components (columns), so entries need to sum up to
1 for each row.</p>
</td></tr>
<tr><td><code id="confusion_+3A_pro">pro</code></td>
<td>
<p>vector of component proportions, need to sum up to 1.</p>
</td></tr>
<tr><td><code id="confusion_+3A_i">i</code></td>
<td>
<p>integer. Component number.</p>
</td></tr>
<tr><td><code id="confusion_+3A_j">j</code></td>
<td>
<p>integer. Component number.</p>
</td></tr>
<tr><td><code id="confusion_+3A_adjustprobs">adjustprobs</code></td>
<td>
<p>logical. If <code>TRUE</code>, probabilities are
initially standardised so that those for components <code>i</code> and
<code>j</code> add up to one (i.e., if they were the only components).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Estimated probability that an observation generated by component
<code>j</code> is classified to component <code>i</code> by maximum a posteriori rule. 
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2010) Methods for merging Gaussian mixture components,
<em>Advances in Data Analysis and Classification</em>, 4, 3-34.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(12345)
  m &lt;- rpois(20,lambda=5)
  dim(m) &lt;- c(5,4)
  pro &lt;- apply(m,2,sum)
  pro &lt;- pro/sum(pro)
  m &lt;- m/apply(m,1,sum)
  round(confusion(m,pro,1,2),digits=2)
</code></pre>

<hr>
<h2 id='cov.wml'>Weighted Covariance Matrices (Maximum Likelihood)</h2><span id='topic+cov.wml'></span>

<h3>Description</h3>

<p>Returns a list containing estimates of the weighted covariance
matrix and the mean of the data, and optionally of the (weighted)
correlation matrix. The
covariance matrix is divided by the sum of the weights,
corresponding to <code>n</code> and the ML-estimator in the case of equal
weights, as opposed to <code>n-1</code> for <code><a href="stats.html#topic+cov.wt">cov.wt</a></code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cov.wml(x, wt = rep(1/nrow(x), nrow(x)), cor = FALSE, center = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cov.wml_+3A_x">x</code></td>
<td>
<p>a matrix or data frame.  As usual, rows are observations and
columns are variables.</p>
</td></tr>
<tr><td><code id="cov.wml_+3A_wt">wt</code></td>
<td>
<p>a non-negative and non-zero vector of weights for each
observation.  Its length must equal the number of rows of
<code>x</code>.</p>
</td></tr>
<tr><td><code id="cov.wml_+3A_cor">cor</code></td>
<td>
<p>A logical indicating whether the estimated correlation
weighted matrix will be returned as well.</p>
</td></tr>
<tr><td><code id="cov.wml_+3A_center">center</code></td>
<td>
<p>Either a logical or a numeric vector specifying the centers
to be used when computing covariances.  If <code>TRUE</code>, the
(weighted) mean of each variable is used, if '<code>FALSE</code>, zero is
used.  If <code>center</code> is numeric, its length must equal the
number of columns of <code>x</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following named components:
</p>
<table>
<tr><td><code>cov</code></td>
<td>
<p>the estimated (weighted) covariance matrix.</p>
</td></tr>
<tr><td><code>center</code></td>
<td>
<p>an estimate for the center (mean) of the data.</p>
</td></tr>
<tr><td><code>n.obs</code></td>
<td>
<p>the number of observations (rows) in <code>x</code>.</p>
</td></tr>
<tr><td><code>wt</code></td>
<td>
<p>the weights used in the estimation.  Only returned if given
as an argument.</p>
</td></tr>
<tr><td><code>cor</code></td>
<td>
<p>the estimated correlation matrix.  Only returned if &lsquo;cor&rsquo; is
&lsquo;TRUE&rsquo;.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+cov.wt">cov.wt</a></code>, <code><a href="stats.html#topic+cov">cov</a></code>, <code><a href="stats.html#topic+var">var</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  x &lt;- c(1,2,3,4,5,6,7,8,9,10)
  y &lt;- c(1,2,3,8,7,6,5,8,9,10)
  cov.wml(cbind(x,y),wt=c(0,0,0,1,1,1,1,1,0,0))
  cov.wt(cbind(x,y),wt=c(0,0,0,1,1,1,1,1,0,0))
</code></pre>

<hr>
<h2 id='cqcluster.stats'>Cluster validation statistics (version for use with clusterbenchstats</h2><span id='topic+cqcluster.stats'></span><span id='topic+summary.cquality'></span><span id='topic+print.summary.cquality'></span>

<h3>Description</h3>

<p>This is a more sophisticated version of <code><a href="#topic+cluster.stats">cluster.stats</a></code>
for use with <code><a href="#topic+clusterbenchstats">clusterbenchstats</a></code>, see Hennig (2017).
Computes a number of distance-based statistics, which can be used for cluster
validation, comparison between clusterings and decision about
the number of clusters: cluster sizes, cluster diameters,
average distances within and between clusters, cluster separation,
biggest within cluster gap, 
average silhouette widths, the Calinski and Harabasz index,
a Pearson version of
Hubert's gamma coefficient, the Dunn index, further statistics
introduced
in Hennig (2017) and two indexes
to assess the similarity of two clusterings, namely the corrected Rand
index and Meila's VI.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cqcluster.stats(d = NULL, clustering, alt.clustering = NULL,
                             noisecluster = FALSE, 
    silhouette = TRUE, G2 = FALSE, G3 = FALSE, wgap = TRUE, sepindex = TRUE, 
    sepprob = 0.1, sepwithnoise = TRUE, compareonly = FALSE, 
    aggregateonly = FALSE, 
    averagegap=FALSE, pamcrit=TRUE,
    dquantile=0.1,
    nndist=TRUE, nnk=2, standardisation="max", sepall=TRUE, maxk=10,
    cvstan=sqrt(length(clustering)))

## S3 method for class 'cquality'
summary(object,stanbound=TRUE,largeisgood=TRUE, ...)

## S3 method for class 'summary.cquality'
print(x, ...)

			      
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cqcluster.stats_+3A_d">d</code></td>
<td>
<p>a distance object (as generated by <code>dist</code>) or a distance
matrix between cases.</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_clustering">clustering</code></td>
<td>
<p>an integer vector of length of the number of cases,
which indicates a clustering. The clusters have to be numbered
from 1 to the number of clusters.</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_alt.clustering">alt.clustering</code></td>
<td>
<p>an integer vector such as for
<code>clustering</code>, indicating an alternative clustering. If provided, the
corrected Rand index and Meila's VI for <code>clustering</code>
vs. <code>alt.clustering</code> are computed.</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_noisecluster">noisecluster</code></td>
<td>
<p>logical. If <code>TRUE</code>, it is assumed that the
largest cluster number in <code>clustering</code> denotes a 'noise
class', i.e. points that do not belong to any cluster. These points
are not taken into account for the computation of all functions of
within and between cluster distances including the validation
indexes.</p>
</td></tr> 
<tr><td><code id="cqcluster.stats_+3A_silhouette">silhouette</code></td>
<td>
<p>logical. If <code>TRUE</code>, the silhouette statistics
are computed, which requires package <code>cluster</code>.</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_g2">G2</code></td>
<td>
<p>logical. If <code>TRUE</code>, Goodman and Kruskal's index G2
(cf. Gordon (1999), p. 62) is computed. This executes lots of
sorting algorithms and can be very slow (it has been improved
by R. Francois - thanks!)</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_g3">G3</code></td>
<td>
<p>logical. If <code>TRUE</code>, the index G3
(cf. Gordon (1999), p. 62) is computed. This executes <code>sort</code>
on all distances and can be extremely slow.</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_wgap">wgap</code></td>
<td>
<p>logical. If <code>TRUE</code>, the widest within-cluster gaps
(largest link in within-cluster minimum spanning tree) are
computed. This is used for finding a good number of clusters in
Hennig (2013). See also parameter <code>averagegap</code>.</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_sepindex">sepindex</code></td>
<td>
<p>logical. If <code>TRUE</code>, a separation index is
computed, defined based on the distances for every point to the
closest point not in the same cluster. The separation index is then
the mean of the smallest proportion <code>sepprob</code> of these. This
allows to formalise separation less sensitive to a single or a few
ambiguous points. The output component corresponding to this is
<code>sindex</code>, not <code>separation</code>! This is used for finding a
good number of clusters in Hennig (2013). See also parameter
<code>sepall</code>.</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_sepprob">sepprob</code></td>
<td>
<p>numerical between 0 and 1, see <code>sepindex</code>.</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_sepwithnoise">sepwithnoise</code></td>
<td>
<p>logical. If <code>TRUE</code> and <code>sepindex</code> and
<code>noisecluster</code> are both <code>TRUE</code>, the noise points are
incorporated as cluster in the separation index (<code>sepindex</code>)
computation. Also
they are taken into account for the computation for the minimum
cluster separation.</p>
</td></tr> 
<tr><td><code id="cqcluster.stats_+3A_compareonly">compareonly</code></td>
<td>
<p>logical. If <code>TRUE</code>, only the corrected Rand index
and Meila's VI are
computed and given out (this requires <code>alt.clustering</code> to be
specified).</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_aggregateonly">aggregateonly</code></td>
<td>
<p>logical. If <code>TRUE</code> (and not
<code>compareonly</code>), no clusterwise but only aggregated information
is given out (this cuts the size of the output down a bit).</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_averagegap">averagegap</code></td>
<td>
<p>logical. If <code>TRUE</code>, the average of the widest
within-cluster gaps over all clusters is given out; if
<code>FALSE</code>, the maximum is given out.</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_pamcrit">pamcrit</code></td>
<td>
<p>logical. If <code>TRUE</code>, the average distance of points
to their respective cluster centroids is computed (criterion of the
PAM clustering method); centroids are chosen so that they minimise
this criterion for the given clustering.</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_dquantile">dquantile</code></td>
<td>
<p>numerical between 0 and 1; quantile used for kernel
density estimator for density indexes, see Hennig (2019), Sec. 3.6.</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_nndist">nndist</code></td>
<td>
<p>logical. If <code>TRUE</code>, average distance to <code>nnk</code>th
nearest neighbour within cluster is computed.</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_nnk">nnk</code></td>
<td>
<p>integer. Number of neighbours used in average and
coefficient of
variation of distance to nearest within cluster neighbour (clusters
with <code>nnk</code> or fewer points are ignored for this).</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_standardisation">standardisation</code></td>
<td>
<p><code>"none"</code>, <code>"max"</code>, <code>"ave"</code>,
<code>"q90"</code>, or a number. See details.</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_sepall">sepall</code></td>
<td>
<p>logical. If <code>TRUE</code>, a fraction of smallest
<code>sepprob</code> distances to other clusters is used from every
cluster. Otherwise, a fraction of smallest <code>sepprob</code> distances
overall is used in the computation of <code>sindex</code>.</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_maxk">maxk</code></td>
<td>
<p>numeric. Parsimony is defined as the number of clusters
divided by <code>maxk</code>.</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_cvstan">cvstan</code></td>
<td>
<p>numeric. <code>cvnnd</code> is standardised by <code>cvstan</code>
if there is standardisation, see Details.</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_object">object</code></td>
<td>
<p>object of class <code>cquality</code>, output of <code>cqcluster.stats</code>.</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_x">x</code></td>
<td>
<p>object of class <code>cquality</code>, output of <code>cqcluster.stats</code>.</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_stanbound">stanbound</code></td>
<td>
<p>logical. If <code>TRUE</code>, all index values larger than
1 will be set to 1, and all values smaller than 0 will be set to 0.
This is for preparation in case of <code>largeisgood=TRUE</code> (if
values are already suitably standardised within
<code>cqcluster.stats</code>, it won't do harm and can do good).</p>
</td></tr>
<tr><td><code id="cqcluster.stats_+3A_largeisgood">largeisgood</code></td>
<td>
<p>logical. If <code>TRUE</code>, indexes <code>x</code> are
transformed to <code>1-x</code> in case that before transformation smaller
values indicate a better clustering (that's <code>average.within,
      mnnd, widestgap, within.cluster.ss, dindex, denscut, pamc,
      max.diameter, highdgap, cvnnd</code>. For this to make sense,
<code>cqcluster.stats</code> should be run with
<code>standardisation="max"</code> and <code>summary.cquality</code> with
<code>stanbound=TRUE</code>.</p>
</td></tr> 
<tr><td><code id="cqcluster.stats_+3A_...">...</code></td>
<td>
<p>no effect.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>standardisation</code>-parameter governs the standardisation of
the index values.
<code>standardisation="none"</code> means that unstandardised
raw values of indexes are given out. Otherwise, <code>entropy</code> will be
standardised by the
maximum possible value for the given number of clusters;
<code>within.cluster.ss</code> and <code>between.cluster.ss</code> will be
standardised by the overall sum of squares; <code>mnnd</code> will be
standardised by the maximum distance to the <code>nnk</code>th nearest
neighbour within cluster; <code>pearsongamma</code> will be standardised
by adding 1 and dividing by 2; <code>cvnn</code> will be standardised by
<code>cvstan</code> (the default is the possible maximum).
</p>
<p><code>standardisation</code> allows options for the standardisation of
<code>average.within, sindex, wgap, pamcrit, max.diameter,
  min.separation</code> and can be <code>"max"</code> (maximum distance),
<code>"ave"</code> (average distance), <code>q90</code> (0.9-quantile of
distances), or a positive number. <code>"max"</code> is the default and
standardises all the listed indexes into the range [0,1].</p>


<h3>Value</h3>

<p><code>cqcluster.stats</code> with <code>compareonly=FALSE</code> and
<code>aggregateonly=FALSE</code> returns a list of type
<code>cquality</code> containing the components
<code>n, cluster.number, cluster.size,  min.cluster.size, noisen,
    diameter,
    average.distance, median.distance, separation, average.toother,
    separation.matrix, ave.between.matrix, average.between, average.within,
    n.between, n.within, max.diameter, min.separation,
    within.cluster.ss, clus.avg.silwidths, avg.silwidth,
    g2, g3, pearsongamma, dunn, dunn2, entropy, wb.ratio, ch, cwidegap,
    widestgap, corrected.rand, vi, sindex, svec, psep, stan, nnk, mnnd,
    pamc, pamcentroids, dindex, denscut, highdgap, npenalty, dpenalty,
    withindensp, densoc, pdistto, pclosetomode, distto, percwdens,
    percdensoc, parsimony, cvnnd, cvnndc</code>. Some of these are
standardised, see Details. If
<code>compareonly=TRUE</code>, only <code>corrected.rand, vi</code> are given
out. If <code>aggregateonly=TRUE</code>, only <code>n, cluster.number,
    min.cluster.size, noisen, diameter,
    average.between, average.within,
    max.diameter, min.separation,
    within.cluster.ss, avg.silwidth,
    g2, g3, pearsongamma, dunn, dunn2, entropy, wb.ratio, ch, 
    widestgap, corrected.rand, vi, sindex, svec, psep, stan, nnk, mnnd,
    pamc, pamcentroids, dindex, denscut, highdgap, parsimony, cvnnd,
    cvnndc</code> are given out.
</p>
<p><code>summary.cquality</code> returns a list of type <code>summary.cquality</code>
with components <code>average.within,nnk,mnnd,
              avg.silwidth,
              widestgap,sindex,
              pearsongamma,entropy,pamc,
              within.cluster.ss,
              dindex,denscut,highdgap,
              parsimony,max.diameter,
              min.separation,cvnnd</code>. These are as documented below for
<code>cqcluster.stats</code>, but after transformation by <code>stanbound</code>
and <code>largeisgood</code>, see arguments.
</p>
<table>
<tr><td><code>n</code></td>
<td>
<p>number of points.</p>
</td></tr>
<tr><td><code>cluster.number</code></td>
<td>
<p>number of clusters.</p>
</td></tr>
<tr><td><code>cluster.size</code></td>
<td>
<p>vector of cluster sizes (number of points).</p>
</td></tr>
<tr><td><code>min.cluster.size</code></td>
<td>
<p>size of smallest cluster.</p>
</td></tr>
<tr><td><code>noisen</code></td>
<td>
<p>number of noise points, see argument <code>noisecluster</code>
(<code>noisen=0</code> if <code>noisecluster=FALSE</code>).</p>
</td></tr>
<tr><td><code>diameter</code></td>
<td>
<p>vector of cluster diameters (maximum within cluster
distances).</p>
</td></tr>
<tr><td><code>average.distance</code></td>
<td>
<p>vector of clusterwise
within cluster average distances.</p>
</td></tr>
<tr><td><code>median.distance</code></td>
<td>
<p>vector of clusterwise
within cluster distance medians.</p>
</td></tr>
<tr><td><code>separation</code></td>
<td>
<p>vector of clusterwise minimum distances of a point
in the cluster to a point of another cluster.</p>
</td></tr>
<tr><td><code>average.toother</code></td>
<td>
<p>vector of clusterwise average distances of a point
in the cluster to the points of other clusters.</p>
</td></tr>
<tr><td><code>separation.matrix</code></td>
<td>
<p>matrix of separation values between all pairs
of clusters.</p>
</td></tr>
<tr><td><code>ave.between.matrix</code></td>
<td>
<p>matrix of mean dissimilarities between
points of every pair of clusters.</p>
</td></tr>
<tr><td><code>avebetween</code></td>
<td>
<p>average distance between clusters.</p>
</td></tr>
<tr><td><code>avewithin</code></td>
<td>
<p>average distance within clusters (reweighted so
that every observation, rather than every distance, has the same weight).</p>
</td></tr>
<tr><td><code>n.between</code></td>
<td>
<p>number of distances between clusters.</p>
</td></tr>
<tr><td><code>n.within</code></td>
<td>
<p>number of distances within clusters.</p>
</td></tr>
<tr><td><code>maxdiameter</code></td>
<td>
<p>maximum cluster diameter.</p>
</td></tr>
<tr><td><code>minsep</code></td>
<td>
<p>minimum cluster separation.</p>
</td></tr>
<tr><td><code>withinss</code></td>
<td>
<p>a generalisation of the within clusters sum
of squares (k-means objective function), which is obtained if
<code>d</code> is a Euclidean distance matrix.  For general distance
measures, this is half
the sum of the within cluster squared dissimilarities divided by the
cluster size.</p>
</td></tr>
<tr><td><code>clus.avg.silwidths</code></td>
<td>
<p>vector of cluster average silhouette
widths. See
<code><a href="cluster.html#topic+silhouette">silhouette</a></code>.</p>
</td></tr>
<tr><td><code>asw</code></td>
<td>
<p>average silhouette
width. See <code><a href="cluster.html#topic+silhouette">silhouette</a></code>.</p>
</td></tr>
<tr><td><code>g2</code></td>
<td>
<p>Goodman and Kruskal's Gamma coefficient. See Milligan and
Cooper (1985), Gordon (1999, p. 62).</p>
</td></tr>
<tr><td><code>g3</code></td>
<td>
<p>G3 coefficient. See Gordon (1999, p. 62).</p>
</td></tr>
<tr><td><code>pearsongamma</code></td>
<td>
<p>correlation between distances and a
0-1-vector where 0 means same cluster, 1 means different clusters.
&quot;Normalized gamma&quot; in Halkidi et al. (2001).</p>
</td></tr>
<tr><td><code>dunn</code></td>
<td>
<p>minimum separation / maximum diameter. Dunn index, see
Halkidi et al. (2002).</p>
</td></tr>
<tr><td><code>dunn2</code></td>
<td>
<p>minimum average dissimilarity between two cluster /
maximum average within cluster dissimilarity, another version of
the family of Dunn indexes.</p>
</td></tr> 
<tr><td><code>entropy</code></td>
<td>
<p>entropy of the distribution of cluster memberships,
see Meila(2007).</p>
</td></tr>
<tr><td><code>wb.ratio</code></td>
<td>
<p><code>average.within/average.between</code>.</p>
</td></tr>
<tr><td><code>ch</code></td>
<td>
<p>Calinski and Harabasz index (Calinski and Harabasz 1974,
optimal in Milligan and Cooper 1985; generalised for dissimilarites
in Hennig and Liao 2013).</p>
</td></tr>
<tr><td><code>cwidegap</code></td>
<td>
<p>vector of widest within-cluster gaps.</p>
</td></tr>
<tr><td><code>widestgap</code></td>
<td>
<p>widest within-cluster gap or average of cluster-wise
widest within-cluster gap, depending on parameter <code>averagegap</code>.</p>
</td></tr>
<tr><td><code>corrected.rand</code></td>
<td>
<p>corrected Rand index (if <code>alt.clustering</code>
has been specified), see Gordon (1999, p. 198).</p>
</td></tr>
<tr><td><code>vi</code></td>
<td>
<p>variation of information (VI) index (if <code>alt.clustering</code>
has been specified), see Meila (2007).</p>
</td></tr>
<tr><td><code>sindex</code></td>
<td>
<p>separation index, see argument <code>sepindex</code>.</p>
</td></tr>
<tr><td><code>svec</code></td>
<td>
<p>vector of smallest closest distances of points to next
cluster that are used in the computation of <code>sindex</code> if
<code>sepall=TRUE</code>.</p>
</td></tr>
<tr><td><code>psep</code></td>
<td>
<p>vector of all closest distances of points to next
cluster.</p>
</td></tr>
<tr><td><code>stan</code></td>
<td>
<p>value by which som statistics were standardised, see
Details.</p>
</td></tr>
<tr><td><code>nnk</code></td>
<td>
<p>value of input parameter <code>nnk</code>.</p>
</td></tr>
<tr><td><code>mnnd</code></td>
<td>
<p>average distance to <code>nnk</code>th nearest neighbour within
cluster.</p>
</td></tr>
<tr><td><code>pamc</code></td>
<td>
<p>average distance to cluster centroid.</p>
</td></tr>
<tr><td><code>pamcentroids</code></td>
<td>
<p>index numbers of cluster centroids.</p>
</td></tr>
<tr><td><code>dindex</code></td>
<td>
<p>this index measures to what extent the density decreases
from the cluster mode to the outskirts; I-densdec in Sec. 3.6 of
Hennig (2019); low values are good.</p>
</td></tr>
<tr><td><code>denscut</code></td>
<td>
<p>this index measures whether cluster boundaries run
through density valleys; I-densbound in Sec. 3.6 of Hennig (2019); low
values are good.</p>
</td></tr>
<tr><td><code>highdgap</code></td>
<td>
<p>this measures whether there is a large within-cluster
gap with high density on both sides; I-highdgap in Sec. 3.6 of
Hennig (2019); low values are good.</p>
</td></tr>
<tr><td><code>npenalty</code></td>
<td>
<p>vector of penalties for all clusters that are used
in the computation of <code>denscut</code>, see Hennig (2019) (these are
sums of penalties over all points in the cluster).</p>
</td></tr>
<tr><td><code>depenalty</code></td>
<td>
<p>vector of penalties for all clusters that are used in
the computation of <code>dindex</code>, see Hennig (2019) (these are
sums of several penalties for density increase when going from the
mode outward in the cluster).</p>
</td></tr>
<tr><td><code>withindensp</code></td>
<td>
<p>distance-based kernel density values for all points
as computed in Sec. 3.6 of Hennig (2019).</p>
</td></tr>
<tr><td><code>densoc</code></td>
<td>
<p>contribution of points from other clusters than the one
to which a point is assigned to the density, for all points; called
<code>h_o</code> in Sec. 3.6 of Hennig (2019).</p>
</td></tr>
<tr><td><code>pdistto</code></td>
<td>
<p>list that for all clusters has a sequence of point
numbers. These are the points already incorporated in the sequence
of points constructed in the algorithm in Sec. 3.6 of Hennig (2019) to
which the next point to be joined is connected.</p>
</td></tr>
<tr><td><code>pclosetomode</code></td>
<td>
<p>list that for all clusters has a sequence of point
numbers. Sequence of points to be incorporated in the sequence
of points constructed in the algorithm in Sec. 3.6 of Hennig
(2019).</p>
</td></tr>
<tr><td><code>distto</code></td>
<td>
<p>list that for all clusters has a sequence of differences
between the standardised densities (see <code>percwdens</code>) at the new
point added and the point to which
it is connected (if this is positive, the penalty is this to the
square), in the algorithm in Sec. 3.6 of Hennig (2019).</p>
</td></tr>
<tr><td><code>percwdens</code></td>
<td>
<p>this is <code>withindensp</code> divided by its maximum.</p>
</td></tr>
<tr><td><code>percdensoc</code></td>
<td>
<p>this is <code>densoc</code> divided by the maximum of
<code>withindensp</code>, called <code>h_o^*</code> in Sec. 3.6 of Hennig (2019).</p>
</td></tr>
<tr><td><code>parsimony</code></td>
<td>
<p>number of clusters divided by <code>maxk</code>.</p>
</td></tr>
<tr><td><code>cvnnd</code></td>
<td>
<p>coefficient of variation of dissimilarities to
<code>nnk</code>th nearest within-cluster neighbour, measuring uniformity of
within-cluster densities, weighted over all clusters, see Sec. 3.7 of
Hennig (2019).</p>
</td></tr>
<tr><td><code>cvnndc</code></td>
<td>
<p>vector of cluster-wise coefficients of variation of
dissimilarities to <code>nnk</code>th nearest wthin-cluster neighbour as
required in computation of <code>cvnnd</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Because <code>cqcluster.stats</code> processes a full dissimilarity matrix, it
isn't suitable for large data sets. You may consider
<code><a href="#topic+distcritmulti">distcritmulti</a></code> in that case.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Akhanli, S. and Hennig, C. (2020) Calibrating and aggregating cluster
validity indexes for context-adapted comparison of clusterings.
<em>Statistics and Computing</em>, 30, 1523-1544,
<a href="https://link.springer.com/article/10.1007/s11222-020-09958-2">https://link.springer.com/article/10.1007/s11222-020-09958-2</a>, <a href="https://arxiv.org/abs/2002.01822">https://arxiv.org/abs/2002.01822</a>
</p>
<p>Calinski, T., and Harabasz, J. (1974) A Dendrite Method for Cluster 
Analysis, <em>Communications in Statistics</em>, 3, 1-27.
</p>
<p>Gordon, A. D. (1999) <em>Classification</em>, 2nd ed. Chapman and Hall.
</p>
<p>Halkidi, M., Batistakis, Y., Vazirgiannis, M. (2001) On Clustering
Validation Techniques, <em>Journal of Intelligent Information
Systems</em>, 17, 107-145.
</p>
<p>Hennig, C. and Liao, T. (2013) How to find an appropriate clustering
for mixed-type variables with application to socio-economic
stratification, <em>Journal of the Royal Statistical Society, Series
C Applied Statistics</em>, 62, 309-369.
</p>
<p>Hennig, C. (2013) How many bee species? A case study in
determining the number of clusters. In: Spiliopoulou, L. Schmidt-Thieme,
R. Janning (eds.):
&quot;Data Analysis, Machine Learning and Knowledge Discovery&quot;, Springer,
Berlin, 41-49.
</p>
<p>Hennig, C. (2019) Cluster validation by measurement of clustering
characteristics relevant to the user. In C. H. Skiadas (ed.)
<em>Data Analysis and Applications 1: Clustering and Regression,
Modeling-estimating, Forecasting and Data Mining, Volume 2</em>, Wiley,
New York 1-24,
<a href="https://arxiv.org/abs/1703.09282">https://arxiv.org/abs/1703.09282</a>
</p>
<p>Kaufman, L. and Rousseeuw, P.J. (1990). &quot;Finding Groups in Data:
An Introduction to Cluster Analysis&quot;. Wiley, New York.
</p>
<p>Meila, M. (2007) Comparing clusterings?an information based distance,
<em>Journal of Multivariate Analysis</em>, 98, 873-895.
</p>
<p>Milligan, G. W. and Cooper, M. C. (1985) An examination of procedures
for determining the number of clusters. <em>Psychometrika</em>, 50, 159-179.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cluster.stats">cluster.stats</a></code>,
<code><a href="cluster.html#topic+silhouette">silhouette</a></code>, <code><a href="stats.html#topic+dist">dist</a></code>, <code><a href="#topic+calinhara">calinhara</a></code>,
<code><a href="#topic+distcritmulti">distcritmulti</a></code>.
<code><a href="#topic+clusterboot">clusterboot</a></code> computes clusterwise stability statistics by
resampling.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
  set.seed(20000)
  options(digits=3)
  face &lt;- rFace(200,dMoNo=2,dNoEy=0,p=2)
  dface &lt;- dist(face)
  complete3 &lt;- cutree(hclust(dface),3)
  cqcluster.stats(dface,complete3,
                alt.clustering=as.integer(attr(face,"grouping")))
  
</code></pre>

<hr>
<h2 id='cvnn'>Cluster validation based on nearest neighbours</h2><span id='topic+cvnn'></span>

<h3>Description</h3>

<p>Cluster validity index based on nearest neighbours as defined in Liu
et al. (2013) with a correction explained in Halkidi et al. (2015).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvnn(d=NULL,clusterings,k=5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvnn_+3A_d">d</code></td>
<td>
<p>dissimilarity matrix or <code>dist</code>-object.</p>
</td></tr>
<tr><td><code id="cvnn_+3A_clusterings">clusterings</code></td>
<td>
<p>list of vectors of integers with length <code>=nrow(d)</code>;
indicating the cluster for each observation for several clusterings
(list elements) to be compared.</p>
</td></tr>
<tr><td><code id="cvnn_+3A_k">k</code></td>
<td>
<p>integer. Number of nearest neighbours.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with components (see Liu et al. (2013), Halkidi et al. (2015) for
details)
</p>
<table>
<tr><td><code>cvnnindex</code></td>
<td>
<p>vector of index values for the various clusterings,
see Liu et al. (2013), the lower the better.</p>
</td></tr>
<tr><td><code>sep</code></td>
<td>
<p>vector of separation values.</p>
</td></tr>
<tr><td><code>comp</code></td>
<td>
<p>vector of compactness values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Halkidi, M., Vazirgiannis, M. and Hennig, C. (2015) Method-independent
indices for cluster validation. In C. Hennig, M. Meila, F. Murtagh,
R. Rocci (eds.) <em>Handbook of Cluster Analysis</em>, CRC
Press/Taylor <code>&amp;</code> Francis, Boca Raton.
</p>
<p>Liu, Y, Li, Z., Xiong, H., Gao, X., Wu, J. and Wu, S. (2013)
Understanding and enhancement of internal clustering validation
measures. <em>IEEE Transactions on Cybernetics</em> 43, 982-994.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  options(digits=3)
  iriss &lt;- as.matrix(iris[c(1:10,51:55,101:105),-5])
  irisc &lt;- as.numeric(iris[c(1:10,51:55,101:105),5])
  print(cvnn(dist(iriss),list(irisc,rep(1:4,5))))
</code></pre>

<hr>
<h2 id='cweight'>Weight function for AWC</h2><span id='topic+cweight'></span>

<h3>Description</h3>

<p>For use in <code>awcoord</code> only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cweight(x, ca) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cweight_+3A_x">x</code></td>
<td>
<p>numerical.</p>
</td></tr>
<tr><td><code id="cweight_+3A_ca">ca</code></td>
<td>
<p>numerical.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ca/x</code> if smaller than 1, else 1.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+awcoord">awcoord</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  cweight(4,1)
</code></pre>

<hr>
<h2 id='dbscan'>DBSCAN density reachability and connectivity clustering</h2><span id='topic+dbscan'></span><span id='topic+print.dbscan'></span><span id='topic+plot.dbscan'></span><span id='topic+predict.dbscan'></span>

<h3>Description</h3>

<p>Generates a density based clustering of arbitrary shape as introduced
in Ester et al. (1996).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  dbscan(data, eps, MinPts = 5, scale = FALSE, method = c("hybrid", "raw",
    "dist"), seeds = TRUE, showplot = FALSE, countmode = NULL)
  ## S3 method for class 'dbscan'
print(x, ...)
  ## S3 method for class 'dbscan'
plot(x, data, ...)
  ## S3 method for class 'dbscan'
predict(object, data, newdata = NULL,
predict.max=1000, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dbscan_+3A_data">data</code></td>
<td>
<p>data matrix, data.frame, dissimilarity matrix or
<code>dist</code>-object. Specify <code>method="dist"</code> if the data should
be interpreted as dissimilarity matrix or object. Otherwise
Euclidean distances will be used.</p>
</td></tr>
<tr><td><code id="dbscan_+3A_eps">eps</code></td>
<td>
<p> Reachability distance, see Ester et al. (1996). </p>
</td></tr>
<tr><td><code id="dbscan_+3A_minpts">MinPts</code></td>
<td>
<p> Reachability minimum no. of points, see Ester et al. (1996). </p>
</td></tr>
<tr><td><code id="dbscan_+3A_scale">scale</code></td>
<td>
<p> scale the data if <code>TRUE</code>. </p>
</td></tr>
<tr><td><code id="dbscan_+3A_method">method</code></td>
<td>
<p> &quot;dist&quot; treats data as distance matrix (relatively fast
but memory expensive), &quot;raw&quot; treats data as raw data and avoids
calculating a distance matrix (saves memory but may be slow),
&quot;hybrid&quot; expects also raw data, but calculates partial distance
matrices (very fast with moderate memory requirements).</p>
</td></tr>
<tr><td><code id="dbscan_+3A_seeds">seeds</code></td>
<td>
<p>FALSE to not include the <code>isseed</code>-vector in the
<code>dbscan</code>-object.</p>
</td></tr>
<tr><td><code id="dbscan_+3A_showplot">showplot</code></td>
<td>
<p> 0 = no plot, 1 = plot per iteration, 2 = plot per
subiteration. </p>
</td></tr>
<tr><td><code id="dbscan_+3A_countmode">countmode</code></td>
<td>
<p> NULL or vector of point numbers at which to report
progress. </p>
</td></tr>
<tr><td><code id="dbscan_+3A_x">x</code></td>
<td>
<p>object of class <code>dbscan</code>.</p>
</td></tr>
<tr><td><code id="dbscan_+3A_object">object</code></td>
<td>
<p>object of class <code>dbscan</code>.</p>
</td></tr>
<tr><td><code id="dbscan_+3A_newdata">newdata</code></td>
<td>
<p> matrix or data.frame with raw data to predict. </p>
</td></tr>
<tr><td><code id="dbscan_+3A_predict.max">predict.max</code></td>
<td>
<p> max. batch size for predictions. </p>
</td></tr> 
<tr><td><code id="dbscan_+3A_...">...</code></td>
<td>
<p>Further arguments transferred to plot methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Clusters require a minimum no of points (MinPts) within a maximum distance
(eps) around one of its members (the seed).
Any point within eps around any point which satisfies the seed condition
is a cluster member (recursively).
Some points may not belong to any clusters (noise).
</p>
<p>We have clustered a 100.000 x 2 dataset in 40 minutes on a Pentium M 1600
MHz.
</p>
<p><code>print.dbscan</code> shows a statistic of the number of points
belonging to the clusters that are seeds and border points.
</p>
<p><code>plot.dbscan</code> distinguishes between seed and border points by
plot symbol.
</p>


<h3>Value</h3>

<p><code>predict.dbscan</code> gives out a vector of predicted clusters for the
points in <code>newdata</code>.
</p>
<p><code>dbscan</code> gives out 
an object of class 'dbscan' which is a LIST with components
</p>
<table>
<tr><td><code>cluster</code></td>
<td>
<p>integer vector coding cluster membership with noise
observations (singletons) coded as 0 </p>
</td></tr>
<tr><td><code>isseed</code></td>
<td>
<p>logical vector indicating whether a point is a seed (not
border, not noise)</p>
</td></tr>
<tr><td><code>eps</code></td>
<td>
<p>parameter eps</p>
</td></tr>
<tr><td><code>MinPts</code></td>
<td>
<p>parameter MinPts</p>
</td></tr>
</table>


<h3>Note</h3>

<p> this is a simplified version of the original algorithm (no K-D-trees
used), thus we have <code class="reqn">o(n^2)</code> instead of <code class="reqn">o(n*log(n))</code> </p>


<h3>Author(s)</h3>

<p>Jens Oehlschlaegel, based on a draft by Christian Hennig.</p>


<h3>References</h3>

<p> Martin Ester, Hans-Peter Kriegel, Joerg Sander, Xiaowei Xu
(1996). A Density-Based Algorithm for Discovering Clusters in Large Spatial
Databases with Noise. Institute for Computer Science, University of Munich.
Proceedings of 2nd International Conference on Knowledge Discovery and Data
Mining (KDD-96). </p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(665544)
  n &lt;- 600
  x &lt;- cbind(runif(10, 0, 10)+rnorm(n, sd=0.2), runif(10, 0, 10)+rnorm(n,
    sd=0.2))
  par(bg="grey40")
  ds &lt;- dbscan(x, 0.2)
# run with showplot=1 to see how dbscan works.
  ds
  plot(ds, x)

  x2 &lt;- matrix(0,nrow=4,ncol=2)
  x2[1,] &lt;- c(5,2)
  x2[2,] &lt;- c(8,3)
  x2[3,] &lt;- c(4,4)
  x2[4,] &lt;- c(9,9)
  predict(ds, x, x2)

  n &lt;- 600
  x &lt;- cbind((1:3)+rnorm(n, sd=0.2), (1:3)+rnorm(n, sd=0.2))

# Not run, but results from my machine are 0.105 - 0.068 - 0.255:
#  system.time(ds &lt;- dbscan(x, 0.3, countmode=NULL, method="raw"))[3] 
#  system.time(dsb &lt;- dbscan(x, 0.3, countmode=NULL, method="hybrid"))[3]
#  system.time(dsc &lt;- dbscan(dist(x), 0.3, countmode=NULL,
#    method="dist"))[3]
</code></pre>

<hr>
<h2 id='dipp.tantrum'>Simulates p-value for dip test</h2><span id='topic+dipp.tantrum'></span>

<h3>Description</h3>

<p>Simulates p-value for dip test (see <code><a href="diptest.html#topic+dip">dip</a></code>)
in the way suggested by Tantrum, Murua and Stuetzle (2003) from the
clostest unimodal distribution determined by kernel density estimation
with bandwith chosen so that the density just becomes unimodal. This is
less conservative (and in fact sometimes anti-conservative) than the
values from <code><a href="diptest.html#topic+dip.test">dip.test</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  dipp.tantrum(xdata,d,M=100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dipp.tantrum_+3A_xdata">xdata</code></td>
<td>
<p>numeric vector. One-dimensional dataset.</p>
</td></tr>
<tr><td><code id="dipp.tantrum_+3A_d">d</code></td>
<td>
<p>numeric. Value of dip statistic.</p>
</td></tr>
<tr><td><code id="dipp.tantrum_+3A_m">M</code></td>
<td>
<p>integer. Number of artificial datasets generated in order to
estimate the p-value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with components
</p>
<table>
<tr><td><code>p.value</code></td>
<td>
<p>approximated p-value.</p>
</td></tr>
<tr><td><code>bw</code></td>
<td>
<p>borderline unimodality bandwith in <code><a href="stats.html#topic+density">density</a></code>
with default settings.</p>
</td></tr>
<tr><td><code>dv</code></td>
<td>
<p>vector of dip statistic values from simulated artificial data.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>J. A. Hartigan and P. M. Hartigan (1985) The Dip Test of
Unimodality, <em>Annals of Statistics</em>, 13, 70-84.
</p>
<p>Tantrum, J., Murua, A. and Stuetzle, W. (2003) Assessment and 
Pruning of Hierarchical Model Based Clustering, <em>Proceedings of the 
ninth ACM SIGKDD international conference on Knowledge discovery and 
data mining</em>, Washington, D.C., 197-205.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># not run, requires package diptest
#  x &lt;- runif(100)
#  d &lt;- dip(x)
#  dt &lt;- dipp.tantrum(x,d,M=10)
</code></pre>

<hr>
<h2 id='diptest.multi'>Diptest for discriminant coordinate projection</h2><span id='topic+diptest.multi'></span>

<h3>Description</h3>

<p>Diptest (Hartigan and Hartigan, 1985, see <code><a href="diptest.html#topic+dip">dip</a></code>)
for data projected in discriminant coordinate separating optimally two
class means (see <code>discrcoord</code>) as suggested by Tantrum, Murua and
Stuetzle (2003).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  diptest.multi(xdata,class,pvalue="uniform",M=100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="diptest.multi_+3A_xdata">xdata</code></td>
<td>
<p>matrix. Potentially multidimensional dataset.</p>
</td></tr>
<tr><td><code id="diptest.multi_+3A_class">class</code></td>
<td>
<p>vector of integers giving class numbers for observations.</p>
</td></tr>
<tr><td><code id="diptest.multi_+3A_pvalue">pvalue</code></td>
<td>
<p><code>"uniform"</code> or <code>"tantrum"</code>. Defines whether
the p-value is computed from a uniform null model as suggested in
Hartigan and Hartigan (1985, using <code><a href="diptest.html#topic+dip.test">dip.test</a></code>) or as
suggested in Tantrum et al. (2003, using <code>dipp.tantrum</code>).</p>
</td></tr>
<tr><td><code id="diptest.multi_+3A_m">M</code></td>
<td>
<p>integer. Number of artificial datasets generated in order to
estimate the p-value if <code>pvalue="tantrum"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The resulting p-value.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>J. A. Hartigan and P. M. Hartigan (1985) The Dip Test of
Unimodality, <em>Annals of Statistics</em>, 13, 70-84.
</p>
<p>Tantrum, J., Murua, A. and Stuetzle, W. (2003) Assessment and 
Pruning of Hierarchical Model Based Clustering, <em>Proceedings of the 
ninth ACM SIGKDD international conference on Knowledge discovery and 
data mining</em>, Washington, D.C., 197-205.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  require(diptest)
  x &lt;- cbind(runif(100),runif(100))
  partition &lt;- 1+(x[,1]&lt;0.5)
  d1 &lt;- diptest.multi(x,partition)
  d2 &lt;- diptest.multi(x,partition,pvalue="tantrum",M=10)
</code></pre>

<hr>
<h2 id='discrcoord'>Discriminant coordinates/canonical variates</h2><span id='topic+discrcoord'></span>

<h3>Description</h3>

<p>Computes discriminant coordinates, sometimes referred to as &quot;canonical
variates&quot; as described in Seber (1984).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>discrcoord(xd, clvecd, pool = "n", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="discrcoord_+3A_xd">xd</code></td>
<td>
<p>the data matrix; a numerical object which can be coerced
to a matrix.</p>
</td></tr>
<tr><td><code id="discrcoord_+3A_clvecd">clvecd</code></td>
<td>
<p>integer vector of class numbers; length must equal
<code>nrow(xd)</code>.</p>
</td></tr>
<tr><td><code id="discrcoord_+3A_pool">pool</code></td>
<td>
<p>string. Determines how the within classes
covariance is pooled. &quot;n&quot; means that the class covariances are
weighted corresponding to the number of points in each class
(default). &quot;equal&quot; means that all classes get equal weight.</p>
</td></tr>
<tr><td><code id="discrcoord_+3A_...">...</code></td>
<td>
<p>no effect</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The matrix T (see Seber (1984), p. 270) is inverted by use of
<code><a href="#topic+tdecomp">tdecomp</a></code>, which can be expected to give
reasonable results for singular within-class covariance matrices.
</p>


<h3>Value</h3>

<p>List with the following components
</p>
<table>
<tr><td><code>ev</code></td>
<td>
<p>eigenvalues in descending order.</p>
</td></tr>
<tr><td><code>units</code></td>
<td>
<p>columns are coordinates of projection basis vectors.
New points <code>x</code> can be projected onto the projection basis vectors
by <code>x %*% units</code></p>
</td></tr>
<tr><td><code>proj</code></td>
<td>
<p>projections of <code>xd</code> onto <code>units</code>.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Seber, G. A. F. (1984). <em>Multivariate Observations</em>. New York: Wiley.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plotcluster">plotcluster</a></code> for straight forward discriminant plots.
</p>
<p><code><a href="#topic+batcoord">batcoord</a></code> for discriminating projections for two classes,
so that also the differences in variance are shown (<code>discrcoord</code> is
based only on differences in mean).
</p>
<p><code><a href="#topic+rFace">rFace</a></code> for generation of the example data used below.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(4634)
  face &lt;- rFace(600,dMoNo=2,dNoEy=0)
  grface &lt;- as.integer(attr(face,"grouping"))
  dcf &lt;- discrcoord(face,grface)
  plot(dcf$proj,col=grface)
  # ...done in one step by function plotcluster.
</code></pre>

<hr>
<h2 id='discrete.recode'>Recodes mixed variables dataset</h2><span id='topic+discrete.recode'></span>

<h3>Description</h3>

<p>Recodes a dataset with mixed continuous and categorical variables so
that the continuous variables come first and the categorical variables
have standard coding 1, 2, 3,... (in lexicographical ordering of
values coerced to strings).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  discrete.recode(x,xvarsorted=TRUE,continuous=0,discrete)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="discrete.recode_+3A_x">x</code></td>
<td>
<p>data matrix or data frame. The data need to be organised
case-wise, i.e., if there are categorical variables only, and 15
cases with values c(1,1,2) on the 3 variables, the data matrix needs
15 rows with values 1 1 2. (Categorical variables could take numbers
or strings or anything that can be coerced to factor levels as values.)</p>
</td></tr>
<tr><td><code id="discrete.recode_+3A_xvarsorted">xvarsorted</code></td>
<td>
<p>logical. If <code>TRUE</code>, the continuous variables
are assumed to be the first ones, and the categorical variables to
be behind them.</p>
</td></tr>
<tr><td><code id="discrete.recode_+3A_continuous">continuous</code></td>
<td>
<p>vector of integers giving positions of the
continuous variables. If <code>xvarsorted=TRUE</code>, a single integer,
number of continuous variables.</p>
</td></tr>
<tr><td><code id="discrete.recode_+3A_discrete">discrete</code></td>
<td>
<p>vector of integers giving positions of the
categorical variables (the variables need to be coded in such a way that
<code><a href="base.html#topic+data.matrix">data.matrix</a></code> converts them to something numeric). If
<code>xvarsorted=TRUE</code>, a single integer, number of categorical variables.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>data</code></td>
<td>
<p>data matrix with continuous variables first and
categorical variables in standard coding behind them.</p>
</td></tr>
<tr><td><code>ppdim</code></td>
<td>
<p>vector of categorical variable-wise numbers of
categories.</p>
</td></tr>
<tr><td><code>discretelevels</code></td>
<td>
<p>list of levels of the categorical variables
belonging to what is treated by <code>flexmixedruns</code> as category
1, 2, 3 etc.</p>
</td></tr> 
<tr><td><code>continuous</code></td>
<td>
<p>number of continuous variables.</p>
</td></tr>
<tr><td><code>discrete</code></td>
<td>
<p>number of categorical variables.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en">https://www.unibo.it/sitoweb/christian.hennig/en</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+lcmixed">lcmixed</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(776655)
  v1 &lt;- rnorm(20)
  v2 &lt;- rnorm(20)
  d1 &lt;- sample(c(2,4,6,8),20,replace=TRUE)
  d2 &lt;- sample(1:4,20,replace=TRUE)
  ldata &lt;- cbind(v1,d1,v2,d2)
  lc &lt;-
  discrete.recode(ldata,xvarsorted=FALSE,continuous=c(1,3),discrete=c(2,4))
  require(MASS)
  data(Cars93)
  Cars934 &lt;- Cars93[,c(3,5,8,10)]
  cc &lt;- discrete.recode(Cars934,xvarsorted=FALSE,continuous=c(2,3),discrete=c(1,4))
</code></pre>

<hr>
<h2 id='discrproj'>Linear dimension reduction for classification</h2><span id='topic+discrproj'></span>

<h3>Description</h3>

<p>An interface for ten methods of linear dimension reduction in order
to separate the groups optimally in the projected data. Includes
classical discriminant coordinates, methods to project differences in
mean and covariance structure, asymmetric methods (separation of a
homogeneous class from a heterogeneous one), local neighborhood-based
methods and methods based on robust covariance matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> discrproj(x, clvecd, method="dc", clnum=NULL, ignorepoints=FALSE,
           ignorenum=0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="discrproj_+3A_x">x</code></td>
<td>
<p>the data matrix; a numerical object which can be coerced
to a matrix.</p>
</td></tr>
<tr><td><code id="discrproj_+3A_clvecd">clvecd</code></td>
<td>
<p>vector of class numbers which can be coerced into
integers; length must equal
<code>nrow(xd)</code>.</p>
</td></tr>
<tr><td><code id="discrproj_+3A_method">method</code></td>
<td>
<p>one of
</p>

<dl>
<dt>&quot;dc&quot;</dt><dd><p>usual discriminant coordinates, see <code><a href="#topic+discrcoord">discrcoord</a></code>,</p>
</dd>
<dt>&quot;bc&quot;</dt><dd><p>Bhattacharyya coordinates, first coordinate showing
mean differences, second showing covariance matrix differences,
see <code><a href="#topic+batcoord">batcoord</a></code>,</p>
</dd>
<dt>&quot;vbc&quot;</dt><dd><p>variance dominated Bhattacharyya coordinates,
see <code><a href="#topic+batcoord">batcoord</a></code>,</p>
</dd>
<dt>&quot;mvdc&quot;</dt><dd><p>added meana and variance differences optimizing
coordinates, see <code><a href="#topic+mvdcoord">mvdcoord</a></code>,</p>
</dd>
<dt>&quot;adc&quot;</dt><dd><p>asymmetric discriminant coordinates, see
<code><a href="#topic+adcoord">adcoord</a></code>,</p>
</dd>
<dt>&quot;awc&quot;</dt><dd><p>asymmetric discriminant coordinates with weighted
observations, see <code><a href="#topic+awcoord">awcoord</a></code>,</p>
</dd>
<dt>&quot;arc&quot;</dt><dd><p>asymmetric discriminant coordinates with weighted
observations and robust MCD-covariance matrix,
see <code><a href="#topic+awcoord">awcoord</a></code>,</p>
</dd>
<dt>&quot;nc&quot;</dt><dd><p>neighborhood based coordinates,
see <code><a href="#topic+ncoord">ncoord</a></code>,</p>
</dd>
<dt>&quot;wnc&quot;</dt><dd><p>neighborhood based coordinates with weighted neighborhoods,
see <code><a href="#topic+ncoord">ncoord</a></code>,</p>
</dd>
<dt>&quot;anc&quot;</dt><dd><p>asymmetric neighborhood based coordinates,
see <code><a href="#topic+ancoord">ancoord</a></code>.</p>
</dd>
</dl>

<p>Note that &quot;bc&quot;, &quot;vbc&quot;, &quot;adc&quot;, &quot;awc&quot;, &quot;arc&quot; and &quot;anc&quot; assume that
there are only two classes.</p>
</td></tr>
<tr><td><code id="discrproj_+3A_clnum">clnum</code></td>
<td>
<p>integer. Number of the class which is attempted to plot
homogeneously by &quot;asymmetric methods&quot;, which are the methods
assuming that there are only two classes, as indicated above.</p>
</td></tr> 
<tr><td><code id="discrproj_+3A_ignorepoints">ignorepoints</code></td>
<td>
<p>logical. If <code>TRUE</code>, points with label
<code>ignorenum</code> in <code>clvecd</code> are ignored in the computation for
<code>method</code> and are only projected afterwards onto the resulting
units. If <code>pch=NULL</code>, the plot symbol for these points is &quot;N&quot;.</p>
</td></tr>
<tr><td><code id="discrproj_+3A_ignorenum">ignorenum</code></td>
<td>
<p>one of the potential values of the components of
<code>clvecd</code>. Only has effect if <code>ignorepoints=TRUE</code>, see above.</p>
</td></tr>
<tr><td><code id="discrproj_+3A_...">...</code></td>
<td>
<p>additional parameters passed to the
projection methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>discrproj</code> returns the output of the chosen projection method,
which is a list with at least the components <code>ev, units, proj</code>.
For detailed informations see the help pages of the projection methods.
</p>
<table>
<tr><td><code>ev</code></td>
<td>
<p>eigenvalues in descending order, usually indicating portion
of information in the corresponding direction.</p>
</td></tr>
<tr><td><code>units</code></td>
<td>
<p>columns are coordinates of projection basis vectors.
New points <code>x</code> can be projected onto the projection basis vectors
by <code>x %*% units</code></p>
</td></tr>
<tr><td><code>proj</code></td>
<td>
<p>projections of <code>xd</code> onto <code>units</code>.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>References</h3>

<p>Hennig, C. (2004) Asymmetric linear dimension reduction for classification.
Journal of Computational and Graphical Statistics 13, 930-945 .
</p>
<p>Hennig, C. (2005)  A method for visual cluster validation.  In:
Weihs, C. and Gaul, W. (eds.): Classification - The Ubiquitous
Challenge. Springer, Heidelberg 2005, 153-160.
</p>
<p>Seber, G. A. F. (1984). <em>Multivariate Observations</em>. New York: Wiley.
</p>
<p>Fukunaga (1990). <em>Introduction to Statistical Pattern
Recognition</em> (2nd ed.). Boston: Academic Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+discrcoord">discrcoord</a></code>, <code><a href="#topic+batcoord">batcoord</a></code>,
<code><a href="#topic+mvdcoord">mvdcoord</a></code>, <code><a href="#topic+adcoord">adcoord</a></code>,
<code><a href="#topic+awcoord">awcoord</a></code>, <code><a href="#topic+ncoord">ncoord</a></code>,
<code><a href="#topic+ancoord">ancoord</a></code>.
</p>
<p><code><a href="#topic+rFace">rFace</a></code> for generation of the example data used below.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(4634)
face &lt;- rFace(300,dMoNo=2,dNoEy=0,p=3)
grface &lt;- as.integer(attr(face,"grouping"))

# The abs in the following is there to unify the output,
# because eigenvectors are defined only up to their sign.
# Statistically it doesn't make sense to compute absolute values. 
round(abs(discrproj(face,grface, method="nc")$units),digits=2)
round(abs(discrproj(face,grface, method="wnc")$units),digits=2)
round(abs(discrproj(face,grface, clnum=1, method="arc")$units),digits=2)
</code></pre>

<hr>
<h2 id='distancefactor'>Factor for dissimilarity of mixed type data</h2><span id='topic+distancefactor'></span>

<h3>Description</h3>

<p>Computes a factor that can be used to standardise ordinal categorical
variables and binary dummy variables coding categories of nominal scaled
variables for Euclidean
dissimilarity computation in mixed type data. See Hennig and Liao (2013).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distancefactor(cat,n=NULL, catsizes=NULL,type="categorical",
               normfactor=2,qfactor=ifelse(type=="categorical",1/2,
                             1/(1+1/(cat-1))))

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distancefactor_+3A_cat">cat</code></td>
<td>
<p>integer. Number of categories of the variable to be standardised.
Note that for <code>type="categorical"</code> the number of categories of
the original variable is required, although the
<code>distancefactor</code> is used to standardise dummy
variables for the categories.</p>
</td></tr>
<tr><td><code id="distancefactor_+3A_n">n</code></td>
<td>
<p>integer. Number of data points.</p>
</td></tr>
<tr><td><code id="distancefactor_+3A_catsizes">catsizes</code></td>
<td>
<p>vector of integers giving numbers of observations per
category. One of <code>n</code> and <code>catsizes</code> must be supplied. If
<code>catsizes=NULL</code>, <code>rep(round(n/cat),cat)</code> is used (this may
be appropriate as well if numbers of observations of categories are
unequal, if the researcher decides that the dissimilarity measure
should not be influenced by empirical category sizes.</p>
</td></tr> 
<tr><td><code id="distancefactor_+3A_type">type</code></td>
<td>
<p><code>"categorical"</code> if the factor is used for dummy
variables belonging to a nominal variable, <code>"ordinal"</code> if the
factor is used for an ordinal variable ind standard Likert coding.</p>
</td></tr>
<tr><td><code id="distancefactor_+3A_normfactor">normfactor</code></td>
<td>
<p>numeric. Factor on which standardisation is based.
As a default, this is <code>E(X_1-X_2)^2=2</code> for independent unit
variance variables.</p>
</td></tr>
<tr><td><code id="distancefactor_+3A_qfactor">qfactor</code></td>
<td>
<p>numeric. Factor q in Hennig and Liao (2013) to
adjust for clumping effects due to discreteness.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A factor by which to multiply the variable in order to make it
comparable to a unit variance continuous variable when aggregated in
Euclidean fashion for dissimilarity computation, so that expected
effective difference between two realisations of the variable equals
<code>qfactor*normfactor</code>. 
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en">https://www.unibo.it/sitoweb/christian.hennig/en</a></p>


<h3>References</h3>

<p>Hennig, C. and Liao, T. (2013) How to find an appropriate clustering
for mixed-type variables with application to socio-economic
stratification, <em>Journal of the Royal Statistical Society, Series
C Applied Statistics</em>, 62, 309-369.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lcmixed">lcmixed</a></code>, <code><a href="cluster.html#topic+pam">pam</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(776655)
  d1 &lt;- sample(1:5,20,replace=TRUE)
  d2 &lt;- sample(1:4,20,replace=TRUE)
  ldata &lt;- cbind(d1,d2)
  lc &lt;- cat2bin(ldata,categorical=1)$data
  lc[,1:5] &lt;- lc[,1:5]*distancefactor(5,20,type="categorical")
  lc[,6] &lt;- lc[,6]*distancefactor(4,20,type="ordinal")
</code></pre>

<hr>
<h2 id='distcritmulti'>Distance based validity criteria for large data sets</h2><span id='topic+distcritmulti'></span>

<h3>Description</h3>

<p>Approximates average silhouette width or the Pearson version of
Hubert's gamma criterion by hacking the
dataset into pieces and averaging the subset-wise values, see Hennig
and Liao (2013).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distcritmulti(x,clustering,part=NULL,ns=10,criterion="asw",
                    fun="dist",metric="euclidean",
                     count=FALSE,seed=NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distcritmulti_+3A_x">x</code></td>
<td>
<p>cases times variables data matrix.</p>
</td></tr>
<tr><td><code id="distcritmulti_+3A_clustering">clustering</code></td>
<td>
<p>vector of integers indicating the clustering.</p>
</td></tr>
<tr><td><code id="distcritmulti_+3A_part">part</code></td>
<td>
<p>vector of integer subset sizes; sum should be smaller or
equal to the number of cases of <code>x</code>. If <code>NULL</code>, subset sizes are
chosen approximately equal.</p>
</td></tr> 
<tr><td><code id="distcritmulti_+3A_ns">ns</code></td>
<td>
<p>integer. Number of subsets, only used if <code>part==NULL</code>.</p>
</td></tr>
<tr><td><code id="distcritmulti_+3A_criterion">criterion</code></td>
<td>
<p><code>"asw"</code> or <code>"pearsongamma"</code>, specifies
whether the average silhouette width or the Pearson version of
Hubert's gamma is computed.</p>
</td></tr>
<tr><td><code id="distcritmulti_+3A_fun">fun</code></td>
<td>
<p><code>"dist"</code> or <code>"daisy"</code>, specifies
which function is used for computing dissimilarities.</p>
</td></tr>
<tr><td><code id="distcritmulti_+3A_metric">metric</code></td>
<td>
<p>passed on to <code><a href="stats.html#topic+dist">dist</a></code> (as argument
<code>method</code>) or <code><a href="cluster.html#topic+daisy">daisy</a></code> to determine which
dissimilarity is used.</p>
</td></tr>
<tr><td><code id="distcritmulti_+3A_count">count</code></td>
<td>
<p>logical. if <code>TRUE</code>, the subset number just processed
is printed.</p>
</td></tr>
<tr><td><code id="distcritmulti_+3A_seed">seed</code></td>
<td>
<p>integer, random seed. (If <code>NULL</code>, result depends on
random numbers.)</p>
</td></tr>
<tr><td><code id="distcritmulti_+3A_...">...</code></td>
<td>
<p>further arguments to be passed on to <code><a href="stats.html#topic+dist">dist</a></code> or
<code><a href="cluster.html#topic+daisy">daisy</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components <code>crit.overall,crit.sub,crit.sd,part</code>.
</p>
<table>
<tr><td><code>crit.overall</code></td>
<td>
<p>value of criterion.</p>
</td></tr>
<tr><td><code>crit.sub</code></td>
<td>
<p>vector of subset-wise criterion values.</p>
</td></tr>
<tr><td><code>crit.sd</code></td>
<td>
<p>standard deviation of <code>crit.sub</code>, can be used to
assess stability.</p>
</td></tr>
<tr><td><code>subsets</code></td>
<td>
<p>list of case indexes in subsets.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en">https://www.unibo.it/sitoweb/christian.hennig/en</a></p>


<h3>References</h3>

<p>Halkidi, M., Batistakis, Y., Vazirgiannis, M. (2001) On Clustering
Validation Techniques, <em>Journal of Intelligent Information
Systems</em>, 17, 107-145.
</p>
<p>Hennig, C. and Liao, T. (2013) How to find an appropriate clustering
for mixed-type variables with application to socio-economic
stratification, <em>Journal of the Royal Statistical Society, Series
C Applied Statistics</em>, 62, 309-369.
</p>
<p>Kaufman, L. and Rousseeuw, P.J. (1990). &quot;Finding Groups in Data:
An Introduction to Cluster Analysis&quot;. Wiley, New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cluster.stats">cluster.stats</a></code>, <code><a href="cluster.html#topic+silhouette">silhouette</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>    set.seed(20000)
    options(digits=3)
    face &lt;- rFace(50,dMoNo=2,dNoEy=0,p=2)
    clustering &lt;- as.integer(attr(face,"grouping"))
    distcritmulti(face,clustering,ns=3,seed=100000,criterion="pearsongamma")
</code></pre>

<hr>
<h2 id='distrsimilarity'>Similarity of within-cluster distributions to normal and uniform</h2><span id='topic+distrsimilarity'></span>

<h3>Description</h3>

<p>Two measures of dissimilarity between the within-cluster distributions of
a dataset and normal or uniform distribution. For the normal it's the
Kolmogorov dissimilarity between the Mahalanobis distances to the
center and a chi-squared distribution. For the uniform it is the
Kolmogorov distance between the distance to the kth nearest neighbour
and a Gamma distribution (this is based on Byers and Raftery (1998)).
The clusterwise values are aggregated by weighting with the cluster sizes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distrsimilarity(x,clustering,noisecluster = FALSE,
distribution=c("normal","uniform"),nnk=2,
largeisgood=FALSE,messages=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distrsimilarity_+3A_x">x</code></td>
<td>
<p>the data matrix; a numerical object which can be coerced
to a matrix.</p>
</td></tr>
<tr><td><code id="distrsimilarity_+3A_clustering">clustering</code></td>
<td>
<p>integer vector of class numbers; length must equal
<code>nrow(x)</code>, numbers must go from 1 to the number of clusters.</p>
</td></tr>
<tr><td><code id="distrsimilarity_+3A_noisecluster">noisecluster</code></td>
<td>
<p>logical. If <code>TRUE</code>, the cluster with the
largest number is ignored for the computations.</p>
</td></tr>
<tr><td><code id="distrsimilarity_+3A_distribution">distribution</code></td>
<td>
<p>vector of <code>"normal", "uniform"</code> or
both. Indicates which of the two dissimilarities is/are computed.</p>
</td></tr>
<tr><td><code id="distrsimilarity_+3A_nnk">nnk</code></td>
<td>
<p>integer. Number of nearest neighbors to use for
dissimilarity to the uniform.</p>
</td></tr>
<tr><td><code id="distrsimilarity_+3A_largeisgood">largeisgood</code></td>
<td>
<p>logical. If <code>TRUE</code>, dissimilarities are
transformed to <code>1-d</code> (this means that larger values indicate a
better fit).</p>
</td></tr>
<tr><td><code id="distrsimilarity_+3A_messages">messages</code></td>
<td>
<p>logical. If <code>TRUE</code>, warnings are given if
within-cluster covariance matrices are not invertible (in which case
all within-cluster Mahalanobis distances are set to zero).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with the following components
</p>
<table>
<tr><td><code>kdnorm</code></td>
<td>
<p>Kolmogorov distance between distribution of
within-cluster Mahalanobis
distances and appropriate chi-squared distribution, aggregated over
clusters (I am grateful to Agustin Mayo-Iscar for the idea).</p>
</td></tr>
<tr><td><code>kdunif</code></td>
<td>
<p>Kolmogorov distance between distribution of distances to
<code>nnk</code>th nearest within-cluster neighbor and appropriate
Gamma-distribution, see Byers and Raftery (1998), aggregated over
clusters.</p>
</td></tr>
<tr><td><code>kdnormc</code></td>
<td>
<p>vector of cluster-wise Kolmogorov distances between
distribution of within-cluster Mahalanobis
distances and appropriate chi-squared distribution.</p>
</td></tr>
<tr><td><code>kdunifc</code></td>
<td>
<p>vector of cluster-wise Kolmogorov distances between
distribution of distances to <code>nnk</code>th nearest within-cluster
neighbor and appropriate Gamma-distribution.</p>
</td></tr>
<tr><td><code>xmahal</code></td>
<td>
<p>vector of Mahalanobs distances to the respective cluster
center.</p>
</td></tr>
<tr><td><code>xdknn</code></td>
<td>
<p>vector of distance to <code>nnk</code>th nearest within-cluster
neighbor.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>It is very hard to capture similarity to a multivariate normal or
uniform in a single value, and both used here have their
shortcomings. Particularly, the dissimilarity to the uniform can still
indicate a good fit if there are holes or it's a uniform distribution
concentrated on several not connected sets.  
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Byers, S. and Raftery, A. E. (1998) Nearest-Neighbor Clutter
Removal for Estimating Features in Spatial Point Processes,
<em>Journal of the American Statistical Association</em>, 93, 577-584.
</p>
<p>Hennig, C. (2017) Cluster validation by measurement of clustering
characteristics relevant to the user. In C. H. Skiadas (ed.)
<em>Proceedings of ASMDA 2017</em>, 501-520,
<a href="https://arxiv.org/abs/1703.09282">https://arxiv.org/abs/1703.09282</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cqcluster.stats">cqcluster.stats</a></code>,<code><a href="#topic+cluster.stats">cluster.stats</a></code>
for more cluster validity statistics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(20000)
  options(digits=3)
  face &lt;- rFace(200,dMoNo=2,dNoEy=0,p=2)
  km3 &lt;- kmeans(face,3)
  distrsimilarity(face,km3$cluster) 
</code></pre>

<hr>
<h2 id='dridgeline'>Density along the ridgeline</h2><span id='topic+dridgeline'></span>

<h3>Description</h3>

<p>Computes the density of a two-component Gaussian mixture along the 
ridgeline (Ray and Lindsay, 2005), along which 
all its density extrema are located.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dridgeline(alpha=seq(0,1,0.001), prop,
                          mu1, mu2, Sigma1, Sigma2, showplot=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dridgeline_+3A_alpha">alpha</code></td>
<td>
<p>sequence of values between 0 and 1 for which the density
is computed.</p>
</td></tr>
<tr><td><code id="dridgeline_+3A_prop">prop</code></td>
<td>
<p>mixture proportion of first component.</p>
</td></tr>
<tr><td><code id="dridgeline_+3A_mu1">mu1</code></td>
<td>
<p>mean vector of component 1.</p>
</td></tr>
<tr><td><code id="dridgeline_+3A_mu2">mu2</code></td>
<td>
<p>mean vector of component 2.</p>
</td></tr>
<tr><td><code id="dridgeline_+3A_sigma1">Sigma1</code></td>
<td>
<p>covariance matrix of component 1.</p>
</td></tr>
<tr><td><code id="dridgeline_+3A_sigma2">Sigma2</code></td>
<td>
<p>covariance matrix of component 2.</p>
</td></tr>
<tr><td><code id="dridgeline_+3A_showplot">showplot</code></td>
<td>
<p>logical. If <code>TRUE</code>, the density is plotted
against <code>alpha</code>.</p>
</td></tr>
<tr><td><code id="dridgeline_+3A_...">...</code></td>
<td>
<p>further arguments to be passed on to plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of density values for values of <code>alpha</code>.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Ray, S. and Lindsay, B. G. (2005) The Topography of Multivariate 
Normal Mixtures, <em>Annals of Statistics</em>, 33, 2042-2065.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  q &lt;- dridgeline(seq(0,1,0.1),0.5,c(1,1),c(2,5),diag(2),diag(2))
</code></pre>

<hr>
<h2 id='dudahart2'>Duda-Hart test for splitting</h2><span id='topic+dudahart2'></span>

<h3>Description</h3>

<p>Duda-Hart test for whether a data set should be split into two
clusters. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  dudahart2(x,clustering,alpha=0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dudahart2_+3A_x">x</code></td>
<td>
<p>data matrix or data frame.</p>
</td></tr>
<tr><td><code id="dudahart2_+3A_clustering">clustering</code></td>
<td>
<p>vector of integers. Clustering into two clusters.</p>
</td></tr>
<tr><td><code id="dudahart2_+3A_alpha">alpha</code></td>
<td>
<p>numeric between 0 and 1. Significance level (recommended
to be small if this is used for estimating the number of clusters).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>p.value</code></td>
<td>
<p>p-value against null hypothesis of homogemeity.</p>
</td></tr>
<tr><td><code>dh</code></td>
<td>
<p>ratio of within-cluster sum of squares for two clusters and
overall sum of squares.</p>
</td></tr>
<tr><td><code>compare</code></td>
<td>
<p>critical value for <code>dh</code> at level <code>alpha</code>.</p>
</td></tr>
<tr><td><code>cluster1</code></td>
<td>
<p><code>FALSE</code> if the null hypothesis of homogemeity is
rejected.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>see above.</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p><code>1-alpha</code>-quantile of a standard Gaussian.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en">https://www.unibo.it/sitoweb/christian.hennig/en</a></p>


<h3>References</h3>

<p>Duda, R. O. and Hart, P. E. (1973) <em>Pattern Classification and
Scene Analysis</em>. Wiley, New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cluster.stats">cluster.stats</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  options(digits=2)
  set.seed(98765)
  iriss &lt;- iris[sample(150,20),-5]
  km &lt;- kmeans(iriss,2)
  dudahart2(iriss,km$cluster)
</code></pre>

<hr>
<h2 id='extract.mixturepars'>Extract parameters for certain components from mclust</h2><span id='topic+extract.mixturepars'></span>

<h3>Description</h3>

<p>Extracts parameters of certain mixture components from the output of
<code><a href="mclust.html#topic+summary.mclustBIC">summary.mclustBIC</a></code> and updates proportions so that
they sum up to 1. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  extract.mixturepars(mclustsum,compnumbers,noise=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract.mixturepars_+3A_mclustsum">mclustsum</code></td>
<td>
<p>output object of <code><a href="mclust.html#topic+summary.mclustBIC">summary.mclustBIC</a></code>.</p>
</td></tr>
<tr><td><code id="extract.mixturepars_+3A_compnumbers">compnumbers</code></td>
<td>
<p>vector of integers. Numbers of mixture components.</p>
</td></tr>
<tr><td><code id="extract.mixturepars_+3A_noise">noise</code></td>
<td>
<p>logical. Should be <code>TRUE</code> if a noise component was fitted by
<code><a href="mclust.html#topic+mclustBIC">mclustBIC</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object as component <code>parameters</code> of
<code><a href="mclust.html#topic+summary.mclustBIC">summary.mclustBIC</a></code>-output, but for specified
components only. (Orientation information from all components is kept.)
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(98765)
  require(mclust)
  iriss &lt;- iris[sample(150,20),-5]
  irisBIC &lt;- mclustBIC(iriss,G=5,modelNames="VEV")
  siris &lt;- summary(irisBIC,iriss)
  emp &lt;- extract.mixturepars(siris,2)
  emp$pro
  round(emp$mean,digits=1)
  emp$variance$modelName
  round(emp$variance$scale,digits=2)
  
</code></pre>

<hr>
<h2 id='findrep'>Finding representatives for cluster border</h2><span id='topic+findrep'></span>

<h3>Description</h3>

<p>Finds representative objects for the border of a cluster and the
within-cluster variance as defined in the framework of the <code><a href="#topic+cdbw">cdbw</a></code>
cluster validation index (and meant to be used in that context).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findrep(x,xcen,clustering,cluster,r,p=ncol(x),n=nrow(x),
                    nc=sum(clustering==cluster))

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findrep_+3A_x">x</code></td>
<td>
<p>matrix. Euclidean dataset.</p>
</td></tr>
<tr><td><code id="findrep_+3A_xcen">xcen</code></td>
<td>
<p>mean vector of cluster.</p>
</td></tr>
<tr><td><code id="findrep_+3A_clustering">clustering</code></td>
<td>
<p>vector of integers with length <code>=nrow(x)</code>;
indicating the cluster for each observation.</p>
</td></tr>
<tr><td><code id="findrep_+3A_cluster">cluster</code></td>
<td>
<p>integer. Number of cluster to be treated.</p>
</td></tr>
<tr><td><code id="findrep_+3A_r">r</code></td>
<td>
<p>integer. Number of representatives.</p>
</td></tr>
<tr><td><code id="findrep_+3A_p">p</code></td>
<td>
<p>integer. Number of dimensions.</p>
</td></tr>
<tr><td><code id="findrep_+3A_n">n</code></td>
<td>
<p>integer. Number of observations.</p>
</td></tr>
<tr><td><code id="findrep_+3A_nc">nc</code></td>
<td>
<p>integer. Number of observations in <code>cluster</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with components
</p>
<table>
<tr><td><code>repc</code></td>
<td>
<p>vector of index of representatives (out of all
observations).</p>
</td></tr>
<tr><td><code>repx</code></td>
<td>
<p>vector of index of representatives (out of only the
observations in <code>cluster</code>).</p>
</td></tr>
<tr><td><code>maxr</code></td>
<td>
<p>number of representatives (this can be smaller than
<code>r</code> if fewer pairwise different observations are in
<code>cluster</code>.</p>
</td></tr>
<tr><td><code>wvar</code></td>
<td>
<p>estimated average within-cluster squared distance to mean.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Halkidi, M. and Vazirgiannis, M. (2008) A density-based cluster
validity approach using multi-representatives. <em>Pattern
Recognition Letters</em> 29, 773-786.
</p>
<p>Halkidi, M., Vazirgiannis, M. and Hennig, C. (2015) Method-independent
indices for cluster validation. In C. Hennig, M. Meila, F. Murtagh,
R. Rocci (eds.) <em>Handbook of Cluster Analysis</em>, CRC
Press/Taylor <code>&amp;</code> Francis, Boca Raton.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cdbw">cdbw</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  options(digits=3)
  iriss &lt;- as.matrix(iris[c(1:5,51:55,101:105),-5])
  irisc &lt;- as.numeric(iris[c(1:5,51:55,101:105),5])
  findrep(iriss,colMeans(iriss),irisc,cluster=1,r=2)
</code></pre>

<hr>
<h2 id='fixmahal'>Mahalanobis Fixed Point Clusters</h2><span id='topic+fixmahal'></span><span id='topic+summary.mfpc'></span><span id='topic+plot.mfpc'></span><span id='topic+fpclusters.mfpc'></span><span id='topic+print.summary.mfpc'></span><span id='topic+print.mfpc'></span><span id='topic+fpmi'></span>

<h3>Description</h3>

<p>Computes Mahalanobis fixed point clusters (FPCs), i.e.,
subsets of the data, which consist exactly of the
non-outliers w.r.t. themselves, and may be interpreted
as generated from a homogeneous normal population.
FPCs may
overlap, are not necessarily exhausting and
do not need a specification of the number of clusters.
</p>
<p>Note that while <code>fixmahal</code> has lots of parameters, only one (or
few) of them have usually to be specified, cf. the examples. The
philosophy is to allow much flexibility, but to always provide 
sensible defaults.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fixmahal(dat, n = nrow(as.matrix(dat)), p = ncol(as.matrix(dat)), 
                      method = "fuzzy", cgen = "fixed",
                      ca = NA, ca2 = NA,
                      calpha = ifelse(method=="fuzzy",0.95,0.99),
                      calpha2 = 0.995,
                      pointit = TRUE, subset = n,
                      nc1 = 100+20*p,
                      startn = 18+p, mnc = floor(startn/2), 
                      mer = ifelse(pointit,0.1,0), 
                      distcut = 0.85, maxit = 5*n, iter = n*1e-5,
                      init.group = list(), 
                      ind.storage = TRUE, countmode = 100, 
                      plot = "none")


## S3 method for class 'mfpc'
summary(object, ...)

## S3 method for class 'summary.mfpc'
print(x, maxnc=30, ...)

## S3 method for class 'mfpc'
plot(x, dat, no, bw=FALSE, main=c("Representative FPC No. ",no),
                    xlab=NULL, ylab=NULL,
                    pch=NULL, col=NULL, ...)

## S3 method for class 'mfpc'
fpclusters(object, dat=NA, ca=object$ca, p=object$p, ...)

fpmi(dat, n = nrow(as.matrix(dat)), p = ncol(as.matrix(dat)),
                  gv, ca, ca2, method = "ml", plot,
                  maxit = 5*n, iter = n*1e-6) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fixmahal_+3A_dat">dat</code></td>
<td>
<p>something that can be coerced to a
numerical matrix or vector. Data matrix, rows are points, columns
are variables.
<code>fpclusters.rfpc</code>
does not need specification of <code>dat</code> if <code>fixmahal</code>
has been run with <code>ind.storage=TRUE</code>.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_n">n</code></td>
<td>
<p>optional positive integer.
Number of cases.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_p">p</code></td>
<td>
<p>optional positive integer.
Number of independent variables.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_method">method</code></td>
<td>
<p>a string. <code>method="classical"</code> means 0-1 weighting
of observations by Mahalanobis distances and use of the classical
normal covariance estimator. <code>method="ml"</code> uses the
ML-covariance estimator (division by <code>n</code> instead of <code>n-1</code>) 
This is used in Hennig and Christlieb (2002).
<code>method</code> can also be <code>"mcd"</code> or <code>"mve"</code>,
to enforce the use of robust centers and covariance matrices, see
<code><a href="MASS.html#topic+cov.rob">cov.rob</a></code>. This is experimental, not recommended at the
moment, may be very slowly and requires library <code>lqs</code>.
The default is
<code>method="fuzzy"</code>, where weighted means and covariance matrices
are used (Hennig, 2005).
The weights are computed by <code><a href="#topic+wfu">wfu</a></code>, i.e., a
function that is constant 1 for arguments smaller than <code>ca</code>, 0 for
arguments larger than <code>ca2</code> and continuously linear in between.
Convergence is only proven for <code>method="ml"</code> up to now.</p>
</td></tr>  
<tr><td><code id="fixmahal_+3A_cgen">cgen</code></td>
<td>
<p>optional string. <code>"fixed"</code> means that the same tuning
constant <code>ca</code> is used for all iterations. <code>"auto"</code> means
that <code>ca</code> is generated dependently on the size of the current data
subset in each iteration by <code><a href="#topic+cmahal">cmahal</a></code>. This is 
experimental.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_ca">ca</code></td>
<td>
<p>optional positive number. Tuning constant, specifying
required cluster
separation. By default determined as <code>calpha</code>-quantile of the
chisquared distribution with <code>p</code> degrees of freedom.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_ca2">ca2</code></td>
<td>
<p>optional positive number. Second tuning constant needed if
<code>method="fuzzy"</code>.
By default determined as <code>calpha2</code>-quantile of the
chisquared distribution with <code>p</code> degrees of freedom.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_calpha">calpha</code></td>
<td>
<p>number between 0 and 1. See <code>ca</code>.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_calpha2">calpha2</code></td>
<td>
<p>number between 0 and 1, larger than <code>calpha</code>.
See <code>ca2</code>.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_pointit">pointit</code></td>
<td>
<p>optional logical. If <code>TRUE</code>, <code>subset</code> fixed point
algorithms are started from initial configurations, which are built
around single points of the dataset, cf. <code><a href="#topic+mahalconf">mahalconf</a></code>.
Otherwise, initial configurations are only specified by
<code>init.group</code>.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_subset">subset</code></td>
<td>
<p>optional positive integer smaller or equal than <code>n</code>.
Initial configurations for the fixed point algorithm
(cf. <code><a href="#topic+mahalconf">mahalconf</a></code>) are built from
a subset of <code>subset</code> points from the data. No effect if
<code>pointit=FALSE</code>. Default: all points.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_nc1">nc1</code></td>
<td>
<p>optional positive integer. Tuning constant needed by
<code><a href="#topic+cmahal">cmahal</a></code> to generate <code>ca</code> automatically. Only 
needed for <code>cgen="auto"</code>.</p>
</td></tr> 
<tr><td><code id="fixmahal_+3A_startn">startn</code></td>
<td>
<p>optional positive integer. Size of the initial
configurations. The default value is chosen to prevent that small 
meaningless FPCs are found, but it should be decreased if 
clusters of size smaller than the default value are of interest.</p>
</td></tr> 
<tr><td><code id="fixmahal_+3A_mnc">mnc</code></td>
<td>
<p>optional positive integer. Minimum size of clusters
to be reported.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_mer">mer</code></td>
<td>
<p>optional nonnegative number. FPCs (groups of them,
respectively, see details)
are only reported as stable if the ratio 
of the number of their
findings to their number of points exceeds <code>mer</code>. This holds
under <code>pointit=TRUE</code> and <code>subset=n</code>. For <code>subset&lt;n</code>,
the ratio is adjusted, but for small <code>subset</code>, the results
may extremely vary and have to be taken with care.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_distcut">distcut</code></td>
<td>
<p>optional value between 0 and 1. A similarity
measure between FPCs, given in Hennig (2002), and the corresponding
Single Linkage groups of FPCs with similarity larger
than <code>distcut</code> are computed.
A single representative FPC is selected for each group.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_maxit">maxit</code></td>
<td>
<p>optional integer. Maximum number of iterations
per algorithm run (usually an FPC is found much earlier).</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_iter">iter</code></td>
<td>
<p>positive number. Algorithm stops when difference between
subsequent weight vectors is smaller than <code>iter</code>. Only needed
for <code>method="fuzzy"</code>.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_init.group">init.group</code></td>
<td>
<p>optional list of logical vectors of length
<code>n</code>.
Every vector indicates a starting configuration for the fixed
point algorithm. This can be used for datasets with high
dimension, where the vectors of <code>init.group</code> indicate cluster
candidates found by graphical inspection or background
knowledge, as in Hennig and Christlieb (2002).</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_ind.storage">ind.storage</code></td>
<td>
<p>optional logical. If <code>TRUE</code>,
then all indicator
vectors of found FPCs are given in the value of <code>fixmahal</code>.
May need lots of memory, but is a bit faster.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_countmode">countmode</code></td>
<td>
<p>optional positive integer. Every <code>countmode</code>
algorithm runs <code>fixmahal</code> shows a message.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_plot">plot</code></td>
<td>
<p>optional string. If <code>"start"</code>, you get a scatterplot
of the first two variables to highlight the initial configuration,
<code>"iteration"</code> generates such a plot at each iteration,
<code>"both"</code> does both (this may be very time consuming).
The default is <code>"none"</code>.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_object">object</code></td>
<td>
<p>object of class <code>mfpc</code>, output of <code>fixmahal</code>.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_x">x</code></td>
<td>
<p>object of class <code>mfpc</code>, output of <code>fixmahal</code>.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_maxnc">maxnc</code></td>
<td>
<p>positive integer. Maximum number of FPCs
to be reported.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_no">no</code></td>
<td>
<p>positive integer. Number of the representative FPC to
be plotted.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_bw">bw</code></td>
<td>
<p>optional logical. If <code>TRUE</code>, plot is black/white,
FPC is
indicated by different symbol. Else FPC is indicated red.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_main">main</code></td>
<td>
<p>plot title.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_xlab">xlab</code></td>
<td>
<p>label for x-axis. If <code>NULL</code>, a default text is used.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_ylab">ylab</code></td>
<td>
<p>label for y-axis. If <code>NULL</code>, a default text is used.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_pch">pch</code></td>
<td>
<p>plotting symbol, see <code><a href="graphics.html#topic+par">par</a></code>.
If <code>NULL</code>, the default is used.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_col">col</code></td>
<td>
<p>plotting color, see <code><a href="graphics.html#topic+par">par</a></code>.
If <code>NULL</code>, the default is used.</p>
</td></tr>
<tr><td><code id="fixmahal_+3A_gv">gv</code></td>
<td>
<p>logical vector (or, with <code>method="fuzzy"</code>,
vector of weights between 0 and 1) of length <code>n</code>.
Indicates the initial
configuration for the fixed point algorithm.</p>
</td></tr> 
<tr><td><code id="fixmahal_+3A_...">...</code></td>
<td>
<p>additional parameters to be passed to <code>plot</code>
(no effects elsewhere).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A (crisp) Mahalanobis FPC is a data subset 
that reproduces itself under the following operation: <br />
Compute mean and covariance matrix estimator for the data
subset, and compute all points of the dataset for which the squared
Mahalanobis distance is smaller than <code>ca</code>.<br />
Fixed points of this operation can be considered as clusters,
because they contain only
non-outliers (as defined by the above mentioned procedure) and all other
points are outliers w.r.t. the subset. <br />
The current default is to compute fuzzy Mahalanobis FPCs, where the
points in the subset have a membership weight between 0 and 1 and give
rise to weighted means and covariance matrices.
The new weights are then obtained by computing the weight function
<code><a href="#topic+wfu">wfu</a></code> of the squared Mahalanobis distances, i.e.,
full weight for squared distances smaller than <code>ca</code>, zero
weight for squared distances larger than <code>ca2</code> and 
decreasing weights (linear function of squared distances)
in between.<br />
A fixed point algorithm is started from the whole dataset,
algorithms are started from the subsets specified in
<code>init.group</code>, and further algorithms are started from further
initial configurations as explained under <code>subset</code> and in the
function <code><a href="#topic+mahalconf">mahalconf</a></code>. <br />
Usually some of the FPCs are unstable, and more than one FPC may
correspond to the same significant pattern in the data. Therefore the
number of FPCs is reduced: A similarity matrix is computed
between FPCs. Similarity between sets is defined as the ratio between
2 times size of
intersection and the sum of sizes of both sets. The Single Linkage
clusters (groups)
of level <code>distcut</code> are computed, i.e. the connectivity
components of the graph where edges are drawn between FPCs with
similarity larger than <code>distcut</code>. Groups of FPCs whose members
are found often enough (cf. parameter <code>mer</code>) are considered as
stable enough. A representative FPC is
chosen for every Single Linkage cluster of FPCs according to the
maximum expectation ratio <code>ser</code>. <code>ser</code> is the ratio between
the number of findings of an FPC and the number of points
of an FPC, adjusted suitably if <code>subset&lt;n</code>.
Usually only the representative FPCs of stable groups
are of interest. <br />
Default tuning constants are taken from Hennig (2005).<br />
Generally, the default settings are recommended for
<code>fixmahal</code>. For large datasets, the use of
<code>init.group</code> together with <code>pointit=FALSE</code>
is useful. Occasionally, <code>mnc</code> and <code>startn</code> may be chosen
smaller than the default,
if smaller clusters are of interest, but this may lead to too many
clusters. Decrease of
<code>ca</code> will often lead to too many clusters, even for homogeneous
data. Increase of <code>ca</code> will produce only very strongly
separated clusters. Both may be of interest occasionally.<br />
Singular covariance matrices during the iterations are handled by
<code><a href="#topic+solvecov">solvecov</a></code>.
</p>
<p><code>summary.mfpc</code> gives a summary about the representative FPCs of
stable groups.
</p>
<p><code>plot.mfpc</code> is a plot method for the representative FPC of stable
group no. <code>no</code>. It produces a scatterplot, where
the points belonging to the FPC are highlighted, the mean is and
for <code>p&lt;3</code> also the region of the FPC is shown. For <code>p&gt;=3</code>,
the optimal separating projection computed by <code><a href="#topic+batcoord">batcoord</a></code>
is shown.  
</p>
<p><code>fpclusters.mfpc</code> produces a list of indicator vectors for the
representative FPCs of stable groups.
</p>
<p><code>fpmi</code> is called by <code>fixmahal</code> for a single fixed point
algorithm and will usually not be executed alone.
</p>


<h3>Value</h3>

<p><code>fixmahal</code> returns an object of class <code>mfpc</code>. This is a list
containing the components <code>nc, g, means, covs, nfound, er, tsc,
    ncoll, skc, grto, imatrix, smatrix, stn, stfound, ser, sfpc, ssig,
    sto, struc, n, p, method, cgen, ca, ca2, cvec, calpha, pointit,
    subset, mnc, startn, mer, distcut</code>.
</p>
<p><code>summary.mfpc</code> returns an object of class <code>summary.mfpc</code>.
This is a list containing the components <code>means, covs, stn,
    stfound, sn, ser, tskip, skc, tsc, sim, ca, ca2, calpha, mer, method,
    cgen, pointit</code>.
</p>
<p><code>fpclusters.mfpc</code> returns a list of indicator vectors for the
representative FPCs of stable groups.
</p>
<p><code>fpmi</code> returns a list with the components <code>mg, covg, md,
    gv, coll, method, ca</code>.
</p>
<table>
<tr><td><code>nc</code></td>
<td>
<p>integer. Number of FPCs.</p>
</td></tr>
<tr><td><code>g</code></td>
<td>
<p>list of logical vectors. Indicator vectors of FPCs. <code>FALSE</code>
if <code>ind.storage=FALSE</code>.</p>
</td></tr>
<tr><td><code>means</code></td>
<td>
<p>list of numerical vectors. Means of FPCs. In
<code>summary.mfpc</code>, only for representative
FPCs of stable groups and sorted according to
<code>ser</code>.</p>
</td></tr>
<tr><td><code>covs</code></td>
<td>
<p>list of numerical matrices. Covariance matrices of FPCs. In
<code>summary.mfpc</code>, only for representative
FPCs of stable groups and sorted according to
<code>ser</code>.</p>
</td></tr>
<tr><td><code>nfound</code></td>
<td>
<p>vector of integers. Number of findings for the FPCs.</p>
</td></tr>
<tr><td><code>er</code></td>
<td>
<p>numerical vector. Ratio of number of findings of FPCs to their
size. Under <code>pointit=TRUE</code>,
this can be taken as a measure of stability of FPCs.</p>
</td></tr>
<tr><td><code>tsc</code></td>
<td>
<p>integer. Number of algorithm runs leading to too small or
too seldom found FPCs.</p>
</td></tr>
<tr><td><code>ncoll</code></td>
<td>
<p>integer. Number of algorithm runs where collinear
covariance matrices occurred.</p>
</td></tr>
<tr><td><code>skc</code></td>
<td>
<p>integer. Number of skipped clusters.</p>
</td></tr>
<tr><td><code>grto</code></td>
<td>
<p>vector of integers. Numbers of FPCs to which algorithm
runs led, which were started by <code>init.group</code>.</p>
</td></tr>
<tr><td><code>imatrix</code></td>
<td>
<p>vector of integers. Size of intersection between
FPCs. See <code><a href="#topic+sseg">sseg</a></code>.</p>
</td></tr>
<tr><td><code>smatrix</code></td>
<td>
<p>numerical vector. Similarities between
FPCs. See <code><a href="#topic+sseg">sseg</a></code>.</p>
</td></tr>
<tr><td><code>stn</code></td>
<td>
<p>integer. Number of representative FPCs of stable groups.
In <code>summary.mfpc</code>, sorted according to <code>ser</code>.</p>
</td></tr>
<tr><td><code>stfound</code></td>
<td>
<p>vector of integers. Number of findings of members of
all groups of FPCs. In <code>summary.mfpc</code>, sorted according to
<code>ser</code>.</p>
</td></tr>
<tr><td><code>ser</code></td>
<td>
<p>numerical vector. Ratio of number of findings of groups of
FPCs to their size. Under <code>pointit=TRUE</code>,
this can be taken as a measure of stability of FPCs. In
<code>summary.mfpc</code>, sorted from largest to smallest.</p>
</td></tr>
<tr><td><code>sfpc</code></td>
<td>
<p>vector of integers.
Numbers of representative FPCs of all groups.</p>
</td></tr>
<tr><td><code>ssig</code></td>
<td>
<p>vector of integers of length <code>stn</code>.
Numbers of representative FPCs of the stable groups.</p>
</td></tr>
<tr><td><code>sto</code></td>
<td>
<p>vector of integers. Numbers of groups ordered
according to largest <code>ser</code>.</p>
</td></tr>
<tr><td><code>struc</code></td>
<td>
<p>vector of integers. Number of group an FPC belongs to.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>cgen</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>ca</code></td>
<td>
<p>see arguments, if <code>cgen</code> has been <code>"fixed"</code>. Else
numerical vector of length <code>nc</code> (see below), giving the
final values of <code>ca</code> for all FPC. In <code>fpmi</code>, tuning
constant for the iterated FPC.</p>
</td></tr>
<tr><td><code>ca2</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>cvec</code></td>
<td>
<p>numerical vector of length <code>n</code> for
<code>cgen="auto"</code>. The values for the
tuning constant <code>ca</code> corresponding to the cluster sizes from
<code>1</code> to <code>n</code>.</p>
</td></tr>
<tr><td><code>calpha</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>pointit</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>subset</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>mnc</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>startn</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>mer</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>distcut</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>sn</code></td>
<td>
<p>vector of integers. Number of points of representative
FPCs.</p>
</td></tr>
<tr><td><code>tskip</code></td>
<td>
<p>integer. Number of algorithm runs leading to skipped FPCs.</p>
</td></tr>
<tr><td><code>sim</code></td>
<td>
<p>vector of integers. Size of intersections between
representative FPCs of stable groups. See <code><a href="#topic+sseg">sseg</a></code>.</p>
</td></tr>
<tr><td><code>mg</code></td>
<td>
<p>mean vector.</p>
</td></tr>
<tr><td><code>covg</code></td>
<td>
<p>covariance matrix.</p>
</td></tr>
<tr><td><code>md</code></td>
<td>
<p>Mahalanobis distances.</p>
</td></tr>
<tr><td><code>gv</code></td>
<td>
<p>logical (numerical, respectively, if <code>method="fuzzy"</code>)
indicator vector of iterated FPC.</p>
</td></tr>
<tr><td><code>coll</code></td>
<td>
<p>logical. <code>TRUE</code> means that singular covariance
matrices occurred during the iterations.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2002) Fixed point clusters for linear regression:
computation and comparison, <em>Journal of
Classification</em> 19, 249-276.
</p>
<p>Hennig, C. (2005) Fuzzy and Crisp Mahalanobis Fixed Point Clusters,
in Baier, D., Decker, R., and Schmidt-Thieme, L. (eds.):
<em>Data Analysis and Decision Support</em>. Springer, Heidelberg,
47-56.
</p>
<p>Hennig, C. and Christlieb, N. (2002) Validating visual clusters in
large datasets: Fixed point clusters of spectral features,
<em>Computational Statistics and Data Analysis</em> 40, 723-739.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fixreg">fixreg</a></code> for linear regression fixed point clusters.
</p>
<p><code><a href="#topic+mahalconf">mahalconf</a></code>, <code><a href="#topic+wfu">wfu</a></code>, <code><a href="#topic+cmahal">cmahal</a></code>
for computation of initial configurations, weights, tuning constants.  
</p>
<p><code><a href="#topic+sseg">sseg</a></code> for indexing the similarity/intersection vectors
computed by <code>fixmahal</code>.
</p>
<p><code><a href="#topic+batcoord">batcoord</a></code>, <code><a href="MASS.html#topic+cov.rob">cov.rob</a></code>, <code><a href="#topic+solvecov">solvecov</a></code>,
<code><a href="#topic+cov.wml">cov.wml</a></code>, <code><a href="#topic+plotcluster">plotcluster</a></code>
for computation of projections, (inverted)
covariance matrices, plotting.
</p>
<p><code><a href="#topic+rFace">rFace</a></code> for generation of example data, see below.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  options(digits=2)
  set.seed(20000)
  face &lt;- rFace(400,dMoNo=2,dNoEy=0, p=3)
  # The first example uses grouping information via init.group.
  initg &lt;- list()
  grface &lt;- as.integer(attr(face,"grouping"))
  for (i in 1:5) initg[[i]] &lt;- (grface==i)
  ff0 &lt;- fixmahal(face, pointit=FALSE, init.group=initg)
  summary(ff0)
  cff0 &lt;- fpclusters(ff0)
  plot(face, col=1+cff0[[1]])
  plot(face, col=1+cff0[[4]]) # Why does this come out as a cluster? 
  plot(ff0, face, 4) # A bit clearer...
  # Without grouping information, examples need more time:
  # ff1 &lt;- fixmahal(face)
  # summary(ff1)
  # cff1 &lt;- fpclusters(ff1)
  # plot(face, col=1+cff1[[1]])
  # plot(face, col=1+cff1[[6]]) # Why does this come out as a cluster? 
  # plot(ff1, face, 6) # A bit clearer...
  # ff2 &lt;- fixmahal(face,method="ml")
  # summary(ff2)
  # ff3 &lt;- fixmahal(face,method="ml",calpha=0.95,subset=50)
  # summary(ff3)
  ## ...fast, but lots of clusters. mer=0.3 may be useful here.
  # set.seed(3000)
  # face2 &lt;- rFace(400,dMoNo=2,dNoEy=0)
  # ff5 &lt;- fixmahal(face2)
  # summary(ff5)
  ## misses right eye of face data; with p=6,
  ## initial configurations are too large for 40 point clusters 
  # ff6 &lt;- fixmahal(face2, startn=30)
  # summary(ff6)
  # cff6 &lt;- fpclusters(ff6)
  # plot(face2, col=1+cff6[[3]])
  # plot(ff6, face2, 3)
  # x &lt;- c(1,2,3,6,6,7,8,120)
  # ff8 &lt;- fixmahal(x)
  # summary(ff8)
  # ...dataset a bit too small for the defaults...
  # ff9 &lt;- fixmahal(x, mnc=3, startn=3)
  # summary(ff9)
</code></pre>

<hr>
<h2 id='fixreg'>Linear Regression Fixed Point Clusters</h2><span id='topic+fixreg'></span><span id='topic+summary.rfpc'></span><span id='topic+plot.rfpc'></span><span id='topic+fpclusters.rfpc'></span><span id='topic+print.summary.rfpc'></span><span id='topic+print.rfpc'></span><span id='topic+rfpi'></span>

<h3>Description</h3>

<p>Computes linear regression fixed point clusters (FPCs), i.e.,
subsets of the data, which consist exactly of the
non-outliers w.r.t. themselves, and may be interpreted
as generated from a homogeneous linear regression relation
between independent and dependent variable.  FPCs may
overlap, are not necessarily exhausting and
do not need a specification of the number of clusters.
</p>
<p>Note that while <code>fixreg</code> has lots of parameters, only one (or
few) of them have usually to be specified, cf. the examples. The
philosophy is to allow much flexibility, but to always provide 
sensible defaults.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fixreg(indep=rep(1,n), dep, n=length(dep),
                    p=ncol(as.matrix(indep)),
                    ca=NA, mnc=NA, mtf=3, ir=NA, irnc=NA,
                    irprob=0.95, mncprob=0.5, maxir=20000, maxit=5*n,
                    distcut=0.85, init.group=list(), 
                    ind.storage=FALSE, countmode=100, 
                    plot=FALSE)

## S3 method for class 'rfpc'
summary(object, ...)

## S3 method for class 'summary.rfpc'
print(x, maxnc=30, ...)

## S3 method for class 'rfpc'
plot(x, indep=rep(1,n), dep, no, bw=TRUE,
                      main=c("Representative FPC No. ",no),
                      xlab="Linear combination of independents",
                      ylab=deparse(substitute(indep)),
                      xlim=NULL, ylim=range(dep), 
                      pch=NULL, col=NULL,...)

## S3 method for class 'rfpc'
fpclusters(object, indep=NA, dep=NA, ca=object$ca, ...)

rfpi(indep, dep, p, gv, ca, maxit, plot) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fixreg_+3A_indep">indep</code></td>
<td>
<p>numerical matrix or vector. Independent
variables.
Leave out for clustering one-dimensional data.
<code>fpclusters.rfpc</code>
does not need specification of <code>indep</code> if <code>fixreg</code>
was run with <code>ind.storage=TRUE</code>.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_dep">dep</code></td>
<td>
<p>numerical vector. Dependent variable.
<code>fpclusters.rfpc</code>
does not need specification of <code>dep</code> if <code>fixreg</code>
was run with <code>ind.storage=TRUE</code>.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_n">n</code></td>
<td>
<p>optional positive integer.
Number of cases.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_p">p</code></td>
<td>
<p>optional positive integer.
Number of independent variables.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_ca">ca</code></td>
<td>
<p>optional positive number. Tuning constant, specifying
required cluster
separation. By default determined automatically as a
function of <code>n</code> and <code>p</code>, see function <code><a href="#topic+can">can</a></code>,
Hennig (2002a).</p>
</td></tr>
<tr><td><code id="fixreg_+3A_mnc">mnc</code></td>
<td>
<p>optional positive integer. Minimum size of clusters
to be reported.
By default determined automatically as a function of
<code>mncprob</code>. See Hennig (2002a).</p>
</td></tr>
<tr><td><code id="fixreg_+3A_mtf">mtf</code></td>
<td>
<p>optional positive integer. FPCs must be found at
least <code>mtf</code> times to be reported by <code>summary.rfpc</code>.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_ir">ir</code></td>
<td>
<p>optional positive integer. Number of algorithm runs.
By default determined
automatically as a function of <code>n</code>, <code>p</code>, <code>irnc</code>,
<code>irprob</code>, <code>mtf</code>,
<code>maxir</code>. See function <code><a href="#topic+itnumber">itnumber</a></code> and Hennig (2002a).</p>
</td></tr>
<tr><td><code id="fixreg_+3A_irnc">irnc</code></td>
<td>
<p>optional positive integer. Size of the smallest
cluster to be found with
approximated probability <code>irprob</code>.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_irprob">irprob</code></td>
<td>
<p>optional value between 0 and 1. Approximated
probability for a cluster of size <code>irnc</code> to be found.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_mncprob">mncprob</code></td>
<td>
<p>optional value between 0 amd 1. Approximated
probability for a cluster of size <code>mnc</code> to be found.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_maxir">maxir</code></td>
<td>
<p>optional integer. Maximum number of algorithm runs.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_maxit">maxit</code></td>
<td>
<p>optional integer. Maximum number of iterations
per algorithm run (usually an FPC is found much earlier).</p>
</td></tr>
<tr><td><code id="fixreg_+3A_distcut">distcut</code></td>
<td>
<p>optional value between 0 and 1. A similarity
measure between FPCs, given in Hennig (2002a), and the corresponding
Single Linkage groups of FPCs with similarity larger
than <code>distcut</code> are computed.
A single representative FPC is selected for each group.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_init.group">init.group</code></td>
<td>
<p>optional list of logical vectors of length
<code>n</code>.
Every vector indicates a starting configuration for the fixed
point algorithm. This can be used for datasets with high
dimension, where the vectors of <code>init.group</code> indicate cluster
candidates found by graphical inspection or background
knowledge.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_ind.storage">ind.storage</code></td>
<td>
<p>optional logical. If <code>TRUE</code>,
then all indicator
vectors of found FPCs are given in the value of <code>fixreg</code>.
May need lots of memory, but is a bit faster.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_countmode">countmode</code></td>
<td>
<p>optional positive integer. Every <code>countmode</code>
algorithm runs <code>fixreg</code> shows a message.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_plot">plot</code></td>
<td>
<p>optional logical. If <code>TRUE</code>, you get a scatterplot
of first independent vs. dependent variable at each iteration.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_object">object</code></td>
<td>
<p>object of class <code>rfpc</code>, output of <code>fixreg</code>.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_x">x</code></td>
<td>
<p>object of class <code>rfpc</code>, output of <code>fixreg</code>.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_maxnc">maxnc</code></td>
<td>
<p>positive integer. Maximum number of FPCs
to be reported.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_no">no</code></td>
<td>
<p>positive integer. Number of the representative FPC to
be plotted.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_bw">bw</code></td>
<td>
<p>optional logical. If <code>TRUE</code>, plot is black/white,
FPC is
indicated by different symbol. Else FPC is indicated red.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_main">main</code></td>
<td>
<p>plot title.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_xlab">xlab</code></td>
<td>
<p>label for x-axis.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_ylab">ylab</code></td>
<td>
<p>label for y-axis.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_xlim">xlim</code></td>
<td>
<p>plotted range of x-axis. If <code>NULL</code>, the range of the
plotted linear combination of independent variables is used.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_ylim">ylim</code></td>
<td>
<p>plotted range of y-axis.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_pch">pch</code></td>
<td>
<p>plotting symbol, see <code><a href="graphics.html#topic+par">par</a></code>.
If <code>NULL</code>, the default is used.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_col">col</code></td>
<td>
<p>plotting color, see <code><a href="graphics.html#topic+par">par</a></code>.
If <code>NULL</code>, the default is used.</p>
</td></tr>
<tr><td><code id="fixreg_+3A_gv">gv</code></td>
<td>
<p>logical vector of length <code>n</code>. Indicates the initial
configuration for the fixed point algorithm.</p>
</td></tr> 
<tr><td><code id="fixreg_+3A_...">...</code></td>
<td>
<p>additional parameters to be passed to <code>plot</code>
(no effects elsewhere).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A linear regression FPC is a data subset 
that reproduces itself under the following operation: <br />
Compute linear regression and error variance estimator for the data
subset, and compute all points of the dataset for which the squared
residual is smaller than <code>ca</code> times the error variance.<br />
Fixed points of this operation can be considered as clusters,
because they contain only
non-outliers (as defined by the above mentioned procedure) and all other
points are outliers w.r.t. the subset. <br />
<code>fixreg</code> performs <code>ir</code> fixed point algorithms started from
random subsets of size <code>p+2</code> to look for
FPCs. Additionally an algorithm is started from the whole dataset,
and algorithms are started from the subsets specified in
<code>init.group</code>. <br />
Usually some of the FPCs are unstable, and more than one FPC may
correspond to the same significant pattern in the data. Therefore the
number of FPCs is reduced: FPCs with less than <code>mnc</code> points are
ignored. Then a similarity matrix is computed between the remaining
FPCs. Similarity between sets is defined as the ratio between
2 times size of
intersection and the sum of sizes of both sets. The Single Linkage
clusters (groups)
of level <code>distcut</code> are computed, i.e. the connectivity
components of the graph where edges are drawn between FPCs with
similarity larger than <code>distcut</code>. Groups of FPCs whose members
are found <code>mtf</code> times or more are considered as stable enough.
A representative FPC is
chosen for every Single Linkage cluster of FPCs according to the
maximum expectation ratio <code>ser</code>. <code>ser</code> is the ratio between
the number of findings of an FPC and the estimated
expectation of the number of findings of an FPC of this size,
called <em>expectation ratio</em> and
computed by <code><a href="#topic+clusexpect">clusexpect</a></code>.<br />
Usually only the representative FPCs of stable groups
are of interest. <br />
The choice of the involved tuning constants such as <code>ca</code> and
<code>ir</code> is discussed in detail in Hennig (2002a). Statistical theory
is presented in Hennig (2003).<br />
Generally, the default settings are recommended for
<code>fixreg</code>. In cases where they lead to a too large number of
algorithm runs (e.g., always for <code>p&gt;4</code>), the use of
<code>init.group</code> together with <code>mtf=1</code> and <code>ir=0</code>
is useful. Occasionally, <code>irnc</code> may be chosen
smaller than the default,
if smaller clusters are of interest, but this may lead to too many
clusters and too many algorithm runs. Decrease of
<code>ca</code> will often lead to too many clusters, even for homogeneous
data. Increase of <code>ca</code> will produce only very strongly
separated clusters. Both may be of interest occasionally.
</p>
<p><code>rfpi</code> is called by <code>fixreg</code> for a single fixed point
algorithm and will usually not be executed alone.
</p>
<p><code>summary.rfpc</code> gives a summary about the representative FPCs of
stable groups.
</p>
<p><code>plot.rfpc</code> is a plot method for the representative FPC of stable
group 
no. <code>no</code>. It produces a scatterplot of the linear combination of
independent variables determined by the regression coefficients of the
FPC vs. the dependent variable. The regression line and the region
of non-outliers determined by <code>ca</code> are plotted as well.
</p>
<p><code>fpclusters.rfpc</code> produces a list of indicator vectors for the
representative FPCs of stable groups.
</p>


<h3>Value</h3>

<p><code>fixreg</code> returns an object of class <code>rfpc</code>. This is a list
containing the components <code>nc, g, coefs, vars, nfound, er, tsc,
    ncoll, grto, imatrix, smatrix, stn, stfound, sfpc, ssig, sto, struc,
    n, p, ca, ir, mnc, mtf, distcut</code>.
</p>
<p><code>summary.rfpc</code> returns an object of class <code>summary.rfpc</code>.
This is a list containing the components <code>coefs, vars, stfound,
    stn, sn, ser, tsc, sim, ca, ir, mnc, mtf</code>.
</p>
<p><code>fpclusters.rfpc</code> returns a list of indicator vectors for the
representative FPCs of stable groups.
</p>
<p><code>rfpi</code> returns a list with the components <code>coef, var, g,
    coll, ca</code>.
</p>
<table>
<tr><td><code>nc</code></td>
<td>
<p>integer. Number of FPCs.</p>
</td></tr>
<tr><td><code>g</code></td>
<td>
<p>list of logical vectors. Indicator vectors of FPCs. <code>FALSE</code>
if <code>ind.storage=FALSE</code>.</p>
</td></tr>
<tr><td><code>coefs</code></td>
<td>
<p>list of numerical vectors. Regression coefficients of
FPCs. In <code>summary.rfpc</code>, only for representative
FPCs of stable groups and sorted according to
<code>stfound</code>.</p>
</td></tr>
<tr><td><code>vars</code></td>
<td>
<p>list of numbers. Error variances of FPCs. In
<code>summary.rfpc</code>, only for representative
FPCs of stable groups and sorted according to
<code>stfound</code>.</p>
</td></tr>
<tr><td><code>nfound</code></td>
<td>
<p>vector of integers. Number of findings for the FPCs.</p>
</td></tr>
<tr><td><code>er</code></td>
<td>
<p>numerical vector. Expectation ratios of FPCs. Can be
taken as a stability measure.</p>
</td></tr>
<tr><td><code>tsc</code></td>
<td>
<p>integer. Number of algorithm runs leading to too small or
too seldom found FPCs.</p>
</td></tr>
<tr><td><code>ncoll</code></td>
<td>
<p>integer. Number of algorithm runs where collinear
regressor matrices occurred.</p>
</td></tr>
<tr><td><code>grto</code></td>
<td>
<p>vector of integers. Numbers of FPCs to which algorithm
runs led, which were started by <code>init.group</code>.</p>
</td></tr>
<tr><td><code>imatrix</code></td>
<td>
<p>vector of integers. Size of intersection between
FPCs. See <code><a href="#topic+sseg">sseg</a></code>.</p>
</td></tr>
<tr><td><code>smatrix</code></td>
<td>
<p>numerical vector. Similarities between
FPCs. See <code><a href="#topic+sseg">sseg</a></code>.</p>
</td></tr>
<tr><td><code>stn</code></td>
<td>
<p>integer. Number of representative FPCs of stable groups. In
<code>summary.rfpc</code> sorted according to <code>stfound</code>.</p>
</td></tr>
<tr><td><code>stfound</code></td>
<td>
<p>vector of integers. Number of findings of members of
all groups of FPCs. In
<code>summary.rfpc</code> sorted according to <code>stfound</code>.</p>
</td></tr>
<tr><td><code>sfpc</code></td>
<td>
<p>vector of integers. Numbers of representative FPCs.</p>
</td></tr>
<tr><td><code>ssig</code></td>
<td>
<p>vector of integers. As <code>sfpc</code>, but only for stable
groups.</p>
</td></tr>
<tr><td><code>sto</code></td>
<td>
<p>vector of integers. Number of representative FPC of most,
2nd most, ..., often found group of FPCs.</p>
</td></tr>
<tr><td><code>struc</code></td>
<td>
<p>vector of integers. Number of group an FPC belongs to.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>ca</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>ir</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>mnc</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>mtf</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>distcut</code></td>
<td>
<p>see arguments.</p>
</td></tr>
<tr><td><code>sn</code></td>
<td>
<p>vector of integers. Number of points of representative
FPCs.</p>
</td></tr>
<tr><td><code>ser</code></td>
<td>
<p>numerical vector. Expectation ratio for stable groups.</p>
</td></tr>
<tr><td><code>sim</code></td>
<td>
<p>vector of integers. Size of intersections between
representative FPCs of stable groups. See <code><a href="#topic+sseg">sseg</a></code>.</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>vector of regression coefficients.</p>
</td></tr>
<tr><td><code>var</code></td>
<td>
<p>error variance.</p>
</td></tr>
<tr><td><code>g</code></td>
<td>
<p>logical indicator vector of iterated FPC.</p>
</td></tr>
<tr><td><code>coll</code></td>
<td>
<p>logical. <code>TRUE</code> means that singular covariance
matrices occurred during the iterations.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>References</h3>

<p>Hennig, C. (2002) Fixed point clusters for linear regression:
computation and comparison, <em>Journal of
Classification</em> 19, 249-276.
</p>
<p>Hennig, C. (2003) Clusters, outliers and regression:
fixed point clusters, <em>Journal of
Multivariate Analysis</em> 86, 183-212.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fixmahal">fixmahal</a></code> for fixed point clusters in the usual setup
(non-regression).
</p>
<p><code><a href="#topic+regmix">regmix</a></code> for clusterwiese linear regression by mixture
modeling ML.
</p>
<p><code><a href="#topic+can">can</a></code>, <code><a href="#topic+itnumber">itnumber</a></code> for computation of the default
settings.  
</p>
<p><code><a href="#topic+clusexpect">clusexpect</a></code> for estimation of the expected number of
findings of an FPC of given size.
</p>
<p><code><a href="#topic+itnumber">itnumber</a></code> for the generation of the number of fixed point
algorithms.
</p>
<p><code><a href="#topic+minsize">minsize</a></code> for the smallest FPC size to be found with a given
probability..
</p>
<p><code><a href="#topic+sseg">sseg</a></code> for indexing the similarity/intersection vectors
computed by <code>fixreg</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(190000)
options(digits=3)
data(tonedata)
attach(tonedata)
tonefix &lt;- fixreg(stretchratio,tuned,mtf=1,ir=20)
summary(tonefix)
# This is designed to have a fast example; default setting would be better.
# If you want to see more (and you have a bit more time),
# try out the following:
## Not run: 
 set.seed(1000)
 tonefix &lt;- fixreg(stretchratio,tuned)
# Default - good for these data
 summary(tonefix)
 plot(tonefix,stretchratio,tuned,1)
 plot(tonefix,stretchratio,tuned,2)
 plot(tonefix,stretchratio,tuned,3,bw=FALSE,pch=5) 
 toneclus &lt;- fpclusters(tonefix,stretchratio,tuned)
 plot(stretchratio,tuned,col=1+toneclus[[2]])
 tonefix2 &lt;- fixreg(stretchratio,tuned,distcut=1,mtf=1,countmode=50)
# Every found fixed point cluster is reported,
# no matter how instable it may be.
 summary(tonefix2)
 tonefix3 &lt;- fixreg(stretchratio,tuned,ca=7)
# ca defaults to 10.07 for these data.
 summary(tonefix3)
 subset &lt;- c(rep(FALSE,5),rep(TRUE,24),rep(FALSE,121))
 tonefix4 &lt;- fixreg(stretchratio,tuned,
                    mtf=1,ir=0,init.group=list(subset))
 summary(tonefix4)

## End(Not run)
</code></pre>

<hr>
<h2 id='flexmixedruns'>Fitting mixed Gaussian/multinomial mixtures with flexmix</h2><span id='topic+flexmixedruns'></span>

<h3>Description</h3>

<p><code>flexmixedruns</code> fits a latent class
mixture (clustering) model where some variables are continuous
and modelled within the mixture components by Gaussian distributions
and some variables are categorical and modelled within components by
independent multinomial distributions. The fit is by maximum
likelihood estimation computed with the EM-algorithm. The number of
components can be estimated by the BIC.
</p>
<p>Note that at least one categorical variable is needed, but it is
possible to use data without continuous variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>flexmixedruns(x,diagonal=TRUE,xvarsorted=TRUE,
                          continuous,discrete,ppdim=NULL,initial.cluster=NULL,
                          simruns=20,n.cluster=1:20,verbose=TRUE,recode=TRUE,
                          allout=TRUE,control=list(minprior=0.001),silent=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="flexmixedruns_+3A_x">x</code></td>
<td>
<p>data matrix or data frame. The data need to be organised
case-wise, i.e., if there are categorical variables only, and 15
cases with values c(1,1,2) on the 3 variables, the data matrix needs
15 rows with values 1 1 2. (Categorical variables could take numbers
or strings or anything that can be coerced to factor levels as values.)</p>
</td></tr>
<tr><td><code id="flexmixedruns_+3A_diagonal">diagonal</code></td>
<td>
<p>logical. If <code>TRUE</code>, Gaussian models are fitted
restricted to diagonal covariance matrices. Otherwise, covariance
matrices are unrestricted. <code>TRUE</code> is consistent with the
&quot;within class independence&quot; assumption for the multinomial
variables.</p>
</td></tr>
<tr><td><code id="flexmixedruns_+3A_xvarsorted">xvarsorted</code></td>
<td>
<p>logical. If <code>TRUE</code>, the continuous variables
are assumed to be the first ones, and the categorical variables to
be behind them.</p>
</td></tr>
<tr><td><code id="flexmixedruns_+3A_continuous">continuous</code></td>
<td>
<p>vector of integers giving positions of the
continuous variables. If <code>xvarsorted=TRUE</code>, a single integer,
number of continuous variables.</p>
</td></tr>
<tr><td><code id="flexmixedruns_+3A_discrete">discrete</code></td>
<td>
<p>vector of integers giving positions of the
categorical variables. If <code>xvarsorted=TRUE</code>, a single integer,
number of categorical variables.</p>
</td></tr>
<tr><td><code id="flexmixedruns_+3A_ppdim">ppdim</code></td>
<td>
<p>vector of integers specifying the number of (in the data)
existing categories for each categorical variable. If
<code>recode=TRUE</code>, this can be omitted and is computed
automatically.</p>
</td></tr>
<tr><td><code id="flexmixedruns_+3A_initial.cluster">initial.cluster</code></td>
<td>
<p>this corresponds to the <code>cluster</code>
parameter in <code>flexmix</code> and should only be specified if
<code>simruns=1</code> and <code>n.cluster</code> is a single number.
Either a matrix with <code>n.cluster</code> columns of initial cluster
membership probabilities for each observation; or a factor or
integer vector with the initial cluster assignments of
observations at the start of the EM algorithm. Default is
random assignment into <code>n.cluster</code> clusters.</p>
</td></tr>
<tr><td><code id="flexmixedruns_+3A_simruns">simruns</code></td>
<td>
<p>integer. Number of starts of the EM algorithm with
random initialisation in order to find a good global optimum.</p>
</td></tr>
<tr><td><code id="flexmixedruns_+3A_n.cluster">n.cluster</code></td>
<td>
<p>vector of integers, numbers of components (the optimum
one is found by minimising the BIC).</p>
</td></tr>
<tr><td><code id="flexmixedruns_+3A_verbose">verbose</code></td>
<td>
<p>logical. If <code>TRUE</code>, some information about the
different runs of the EM algorithm is given out.</p>
</td></tr>
<tr><td><code id="flexmixedruns_+3A_recode">recode</code></td>
<td>
<p>logical. If <code>TRUE</code>, the function
<code>discrete.recode</code> is applied in order to recode categorical
data so that the <code>lcmixed</code>-method can use it. Only set this
to <code>FALSE</code> if your data already has that format (even it that
case, <code>TRUE</code> doesn't do harm). If <code>recode=FALSE</code>, the
categorical variables are assumed to be coded 1,2,3,...</p>
</td></tr>
<tr><td><code id="flexmixedruns_+3A_allout">allout</code></td>
<td>
<p>logical. If <code>TRUE</code>, the regular
<code>flexmix</code>-output is given out for every single number of
clusters, which can create a huge output object.</p>
</td></tr>
<tr><td><code id="flexmixedruns_+3A_control">control</code></td>
<td>
<p>list of control parameters for <code>flexmix</code>, for
details see the help page of <code><a href="flexmix.html#topic+FLXcontrol-class">FLXcontrol-class</a></code>.</p>
</td></tr>
<tr><td><code id="flexmixedruns_+3A_silent">silent</code></td>
<td>
<p>logical. This is passed on to the
<code><a href="base.html#topic+try">try</a></code>-function. If <code>FALSE</code>, error messages from
failed runs of <code>flexmix</code> are suppressed. (The information that
a <code>flexmix</code>-error occurred is still given out if
<code>verbose=TRUE</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Sometimes flexmix produces errors because of degenerating covariance
matrices, too small clusters etc. <code>flexmixedruns</code> tolerates these
and treats them as non-optimal runs. (Higher <code>simruns</code> or
different <code>control</code> may be required to get a valid solution.)
</p>
<p>General documentation on flexmix can be found in 
Friedrich Leisch's &quot;FlexMix: A General Framework for Finite Mixture
Models and Latent Class Regression in R&quot;,
<a href="https://CRAN.R-project.org/package=flexmix">https://CRAN.R-project.org/package=flexmix</a>
</p>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>optsummary</code></td>
<td>
<p>summary object for <code>flexmix</code> object with
optimal number of components.</p>
</td></tr>
<tr><td><code>optimalk</code></td>
<td>
<p>optimal number of components.</p>
</td></tr>
<tr><td><code>errcount</code></td>
<td>
<p>vector with numbers of EM runs for each number of
components that led to flexmix errors.</p>
</td></tr>
<tr><td><code>flexout</code></td>
<td>
<p>if <code>allout=TRUE</code>, list of flexmix output objects
for all numbers of components, for details see the help page of
<code><a href="flexmix.html#topic+flexmix-class">flexmix-class</a></code>. Slots that can be used
include for example <code>cluster</code> and <code>components</code>. So
if <code>fo</code> is the <code>flexmixedruns</code>-output object,
<code>fo$flexout[[fo$optimalk]]@cluster</code> gives a component number
vector for the observations (maximum posterior rule), and
<code>fo$flexout[[fo$optimalk]]@components</code> gives the estimated
model parameters, which for <code>lcmixed</code> and therefore
<code>flexmixedruns</code> are called
</p>

<dl>
<dt>center</dt><dd><p>mean vector</p>
</dd>
<dt>cov</dt><dd><p>covariance matrix</p>
</dd>
<dt>pp</dt><dd><p>list of categorical variable-wise category probabilities</p>
</dd>
</dl>

<p>If <code>allout=FALSE</code>, only the flexmix output object for the
optimal number of components, i.e., the <code>[[fo$optimalk]]</code>
indexing above can then be omitted.
</p>
</td></tr>
<tr><td><code>bicvals</code></td>
<td>
<p>vector of values of the BIC for each number of
components.</p>
</td></tr>
<tr><td><code>ppdim</code></td>
<td>
<p>vector of categorical variable-wise numbers of
categories.</p>
</td></tr>
<tr><td><code>discretelevels</code></td>
<td>
<p>list of levels of the categorical variables
belonging to what is treated by <code>flexmixedruns</code> as category
1, 2, 3 etc.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en">https://www.unibo.it/sitoweb/christian.hennig/en</a></p>


<h3>References</h3>

<p>Hennig, C. and Liao, T. (2013) How to find an appropriate clustering
for mixed-type variables with application to socio-economic
stratification, <em>Journal of the Royal Statistical Society, Series
C Applied Statistics</em>, 62, 309-369.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lcmixed">lcmixed</a></code>, <code><a href="flexmix.html#topic+flexmix">flexmix</a></code>,
<code><a href="flexmix.html#topic+FLXcontrol-class">FLXcontrol-class</a></code>,
<code><a href="flexmix.html#topic+flexmix-class">flexmix-class</a></code>,
<code><a href="#topic+discrete.recode">discrete.recode</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>  options(digits=3)
  set.seed(776655)
  v1 &lt;- rnorm(100)
  v2 &lt;- rnorm(100)
  d1 &lt;- sample(1:5,100,replace=TRUE)
  d2 &lt;- sample(1:4,100,replace=TRUE)
  ldata &lt;- cbind(v1,v2,d1,d2)
  fr &lt;- flexmixedruns(ldata,
    continuous=2,discrete=2,simruns=2,n.cluster=2:3,allout=FALSE)
  print(fr$optimalk)
  print(fr$optsummary)
  print(fr$flexout@cluster)
  print(fr$flexout@components)
</code></pre>

<hr>
<h2 id='fpc-package'>fpc package overview</h2><span id='topic+fpc-package'></span>

<h3>Description</h3>

<p>Here is a list of the main functions in package fpc. Most other
functions are auxiliary functions for these.
</p>


<h3>Clustering methods</h3>


<dl>
<dt>dbscan</dt><dd><p>Computes DBSCAN density based clustering as introduced
in Ester et al. (1996).</p>
</dd>
<dt>fixmahal</dt><dd><p>Mahalanobis Fixed Point Clustering, Hennig and
Christlieb (2002), Hennig (2005).</p>
</dd>
<dt>fixreg</dt><dd><p>Regression Fixed Point Clustering, Hennig (2003).</p>
</dd>
<dt>flexmixedruns</dt><dd><p>This fits a latent class model to
data with mixed type continuous/nominal variables. Actually it
calls a method for <code><a href="flexmix.html#topic+flexmix">flexmix</a></code>.</p>
</dd>
<dt>mergenormals</dt><dd><p>Clustering by merging components of a Gaussian
mixture, see Hennig (2010).</p>
</dd>
<dt>regmix</dt><dd><p>ML-fit of a mixture of linear regression models, see
DeSarbo and Cron (1988).</p>
</dd>
</dl>


<h3>Cluster validity indexes and estimation of the number of clusters</h3>


<dl>
<dt>cluster.stats</dt><dd><p>This computes several cluster validity
statistics from a clustering and a dissimilarity matrix including
the Calinski-Harabasz index, the adjusted Rand index and other
statistics explained in Gordon (1999) as well as several
characterising
measures such as average between cluster and within cluster
dissimilarity and separation. See also <code><a href="#topic+calinhara">calinhara</a></code>,
<code><a href="#topic+dudahart2">dudahart2</a></code> for specific indexes, and a new version
<code><a href="#topic+cqcluster.stats">cqcluster.stats</a></code> that computes some more indexes and
statistics used for computing them. There's also
<code><a href="#topic+distrsimilarity">distrsimilarity</a></code>, which computes within-cluster
dissimilarity to the Gaussian and uniform distribution.</p>
</dd>
<dt>prediction.strength</dt><dd><p>Estimates the number of clusters by
computing the prediction strength of a
clustering of a dataset into different numbers of components for
various clustering methods, see
Tibshirani and Walther (2005). In fact, this is more flexible than
what is in the original paper, because it can use
point classification schemes that work better with clustering
methods other than k-means.</p>
</dd>
<dt>nselectboot</dt><dd><p>Estimates the number of clusters by bootstrap
stability selection, see Fang and Wang (2012). This is quite
flexible regarding clustering methods and point classification
schemes and also allows for dissimilarity data.</p>
</dd>
<dt>clusterbenchstats</dt><dd><p>This runs many clustering methods (to be
specifed by the user) with many numbers of clusters on a dataset
and produces standardised and comparable versions of many cluster
validity indexes (see Hennig 2019, Akhanli and Hennig 2020).
This is done by means of
producing random clusterings on the given data, see
<code><a href="#topic+stupidkcentroids">stupidkcentroids</a></code> and <code><a href="#topic+stupidknn">stupidknn</a></code>. It
allows to compare many 
clusterings based on many different potential desirable features
of a clustering. <code><a href="#topic+print.valstat">print.valstat</a></code> allows to compute an
aggregated index with user-specified weights.</p>
</dd>
</dl>


<h3>Cluster visualisation and validation</h3>


<dl>
<dt>clucols</dt><dd><p>Sets of colours and symbols useful for cluster plotting.</p>
</dd>
<dt>clusterboot</dt><dd><p>Cluster-wise stability assessment of a
clustering. Clusterings are performed on resampled data to see for
every cluster of the original dataset how well this is
reproduced. See Hennig (2007) for details.</p>
</dd>
<dt>cluster.varstats</dt><dd><p>Extracts variable-wise information for every
cluster in order to help with cluster interpretation.</p>
</dd>
<dt>plotcluster</dt><dd><p>Visualisation of a clustering or grouping in data
by various linear projection methods that optimise the separation
between clusters, or between a single cluster and the rest of the
data according to Hennig (2004) including classical methods such
as discriminant coordinates. This calls the function
<code><a href="#topic+discrproj">discrproj</a></code>, which is a bit more flexible but doesn't
produce a plot itself.</p>
</dd>
<dt>ridgeline.diagnosis</dt><dd><p>Plots and diagnostics for assessing
modality of Gaussian mixtures, see Ray and Lindsay (2005).</p>
</dd>
<dt>weightplots</dt><dd><p>Plots to diagnose component separation in
Gaussian mixtures, see Hennig (2010).</p>
</dd>
<dt>localshape</dt><dd><p>Local shape matrix, can be used for finding
clusters in connection with function <code>ics</code> in package
<code>ICS</code>, see Hennig's
discussion and rejoinder of Tyler et al. (2009).</p>
</dd>
</dl>


<h3>Useful wrapper functions for clustering methods</h3>


<dl>
<dt>kmeansCBI</dt><dd><p>This and other &quot;CBI&quot;-functions (see the
<code><a href="#topic+kmeansCBI">kmeansCBI</a></code>-help page) are unified wrappers for
various clustering methods in R that may be useful because they do
in one step for what you normally may need to do a bit more in R
(for example fitting a Gaussian mixture with noise component in
package mclust).</p>
</dd>
<dt>kmeansruns</dt><dd><p>This calls <code><a href="stats.html#topic+kmeans">kmeans</a></code> for the k-means
clustering method and includes estimation of the number of
clusters and finding an optimal solution from several starting
points.</p>
</dd>
<dt>pamk</dt><dd><p>This calls <code><a href="cluster.html#topic+pam">pam</a></code> and
<code><a href="cluster.html#topic+clara">clara</a></code> for the partitioning around medoids 
clustering method (Kaufman and Rouseeuw, 1990) and includes two
different ways of estimating the number of clusters.</p>
</dd>
</dl>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Akhanli, S. and Hennig, C. (2020) Calibrating and aggregating cluster
validity indexes for context-adapted comparison of clusterings.
<em>Statistics and Computing</em>, 30, 1523-1544,
<a href="https://link.springer.com/article/10.1007/s11222-020-09958-2">https://link.springer.com/article/10.1007/s11222-020-09958-2</a>,
<a href="https://arxiv.org/abs/2002.01822">https://arxiv.org/abs/2002.01822</a>
</p>
<p>DeSarbo, W. S. and Cron, W. L. (1988) A maximum likelihood methodology
for clusterwise linear regression, <em>Journal of
Classification</em> 5, 249-282.
</p>
<p>Ester, M., Kriegel, H.-P., Sander, J. and Xu, X.
(1996). A Density-Based Algorithm for Discovering Clusters in Large Spatial
Databases with Noise.
<em>Proceedings of 2nd International Conference on
Knowledge Discovery and Data
Mining (KDD-96).</em>
</p>
<p>Fang, Y. and Wang, J. (2012) Selection of the number of clusters via
the bootstrap method. <em>Computational Statistics and Data
Analysis</em>, 56, 468-477.
</p>
<p>Gordon, A. D. (1999) <em>Classification</em>, 2nd ed. Chapman and Hall.
</p>
<p>Hennig, C. (2003) Clusters, outliers and regression:
fixed point clusters, <em>Journal of
Multivariate Analysis</em> 86, 183-212.
</p>
<p>Hennig, C. (2004) Asymmetric linear dimension reduction for classification.
<em>Journal of Computational and Graphical Statistics</em>, 13, 930-945 .
</p>
<p>Hennig, C. (2005) Fuzzy and Crisp Mahalanobis Fixed Point Clusters,
in Baier, D., Decker, R., and Schmidt-Thieme, L. (eds.):
<em>Data Analysis and Decision Support</em>. Springer, Heidelberg,
47-56.
</p>
<p>Hennig, C. (2007) Cluster-wise assessment of cluster
stability. <em>Computational Statistics and Data Analysis</em>,
52, 258-271.
</p>
<p>Hennig, C. (2010) Methods for merging Gaussian mixture components,
<em>Advances in Data Analysis and Classification</em>, 4, 3-34.
</p>
<p>Hennig, C. (2019) Cluster validation by measurement of clustering
characteristics relevant to the user. In C. H. Skiadas (ed.)
<em>Data Analysis and Applications 1: Clustering and Regression,
Modeling-estimating, Forecasting and Data Mining, Volume 2</em>, Wiley,
New York 1-24,
<a href="https://arxiv.org/abs/1703.09282">https://arxiv.org/abs/1703.09282</a>
</p>
<p>Hennig, C. and Christlieb, N. (2002) Validating visual clusters in
large datasets: Fixed point clusters of spectral features,
<em>Computational Statistics and Data Analysis</em> 40, 723-739.
</p>
<p>Kaufman, L. and Rousseeuw, P.J. (1990). &quot;Finding Groups in Data:
An Introduction to Cluster Analysis&quot;. Wiley, New York.
</p>
<p>Ray, S. and Lindsay, B. G. (2005) The Topography of Multivariate 
Normal Mixtures, <em>Annals of Statistics</em>, 33, 2042-2065.
</p>
<p>Tibshirani, R. and Walther, G. (2005) Cluster Validation by 
Prediction Strength, <em>Journal of Computational and Graphical 
Statistics</em>, 14, 511-528.
</p>

<hr>
<h2 id='fpclusters'>Extracting clusters from fixed point cluster objects</h2><span id='topic+fpclusters'></span>

<h3>Description</h3>

<p><code>fpclusters</code> is a generic function which extracts the
representative fixed point clusters (FPCs)
from FPC objects generated by <code><a href="#topic+fixmahal">fixmahal</a></code> and
<code><a href="#topic+fixreg">fixreg</a></code>. For documentation and examples see
<code><a href="#topic+fixmahal">fixmahal</a></code> and  <code><a href="#topic+fixreg">fixreg</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fpclusters(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fpclusters_+3A_object">object</code></td>
<td>
<p>object of class <code>rfpc</code> or <code>mfpc</code>.</p>
</td></tr>
<tr><td><code id="fpclusters_+3A_...">...</code></td>
<td>
<p>further arguments depending on the method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of logical or numerical vectors indicating or giving the
weights of the cluster memberships.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fixmahal">fixmahal</a></code>, <code><a href="#topic+fixreg">fixreg</a></code></p>

<hr>
<h2 id='itnumber'>Number of regression fixed point cluster iterations</h2><span id='topic+itnumber'></span>

<h3>Description</h3>

<p>Computes the number of fixed point iterations needed by
<code><a href="#topic+fixreg">fixreg</a></code> to find <code>mtf</code> times
a fixed point cluster (FPC) of size
<code>cn</code> with an approximated probability of <code>prob</code>.
</p>
<p>Thought for use within <code><a href="#topic+fixreg">fixreg</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>itnumber(n, p, cn, mtf, prob = 0.95, maxir = 20000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="itnumber_+3A_n">n</code></td>
<td>
<p>positive integer. Total number of points.</p>
</td></tr>
<tr><td><code id="itnumber_+3A_p">p</code></td>
<td>
<p>positive integer. Number of independent variables.</p>
</td></tr>
<tr><td><code id="itnumber_+3A_cn">cn</code></td>
<td>
<p>positive integer smaller or equal to <code>n</code>.
Size of the FPC.</p>
</td></tr>
<tr><td><code id="itnumber_+3A_mtf">mtf</code></td>
<td>
<p>positive integer.</p>
</td></tr>
<tr><td><code id="itnumber_+3A_prob">prob</code></td>
<td>
<p>number between 0 and 1.</p>
</td></tr>
<tr><td><code id="itnumber_+3A_maxir">maxir</code></td>
<td>
<p>positive integer. <code>itnumber</code> is set to this value if
it would otherwise be larger.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The computation is based on the binomial distribution with probability
given by <code><a href="#topic+clusexpect">clusexpect</a></code> with <code>ir=1</code>.
</p>


<h3>Value</h3>

<p>An integer.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>References</h3>

<p>Hennig, C. (2002) Fixed point clusters for linear regression:
computation and comparison, <em>Journal of
Classification</em> 19, 249-276.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fixreg">fixreg</a></code>, <code><a href="#topic+clusexpect">clusexpect</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  itnumber(500,4,150,2)
</code></pre>

<hr>
<h2 id='jittervar'>Jitter variables in a data matrix</h2><span id='topic+jittervar'></span>

<h3>Description</h3>

<p>Jitters some variables in a data matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  jittervar(x,jitterv=NULL,factor=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jittervar_+3A_x">x</code></td>
<td>
<p>data matrix or data frame.</p>
</td></tr>
<tr><td><code id="jittervar_+3A_jitterv">jitterv</code></td>
<td>
<p>vector of numbers of variables to be jittered.</p>
</td></tr>
<tr><td><code id="jittervar_+3A_factor">factor</code></td>
<td>
<p>numeric. Passed on to <code><a href="base.html#topic+jitter">jitter</a></code>. See the
documentation there. The higher, the more jittering.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data matrix or data frame with jittered variables.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en">https://www.unibo.it/sitoweb/christian.hennig/en</a></p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+jitter">jitter</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(776655)
  v1 &lt;- rnorm(20)
  v2 &lt;- rnorm(20)
  d1 &lt;- sample(1:5,20,replace=TRUE)
  d2 &lt;- sample(1:4,20,replace=TRUE)
  ldata &lt;- cbind(v1,v2,d1,d2)
  jv &lt;- jittervar(ldata,jitterv=3:4)
</code></pre>

<hr>
<h2 id='kmeansCBI'>Interface functions for clustering methods</h2><span id='topic+kmeansCBI'></span><span id='topic+hclustCBI'></span><span id='topic+hclusttreeCBI'></span><span id='topic+disthclustCBI'></span><span id='topic+disthclusttreeCBI'></span><span id='topic+noisemclustCBI'></span><span id='topic+distnoisemclustCBI'></span><span id='topic+claraCBI'></span><span id='topic+pamkCBI'></span><span id='topic+dbscanCBI'></span><span id='topic+mahalCBI'></span><span id='topic+mergenormCBI'></span><span id='topic+speccCBI'></span><span id='topic+tclustCBI'></span><span id='topic+pdfclustCBI'></span><span id='topic+emskewCBI'></span><span id='topic+stupidkcentroidsCBI'></span><span id='topic+stupidknnCBI'></span><span id='topic+stupidkfnCBI'></span><span id='topic+stupidkavenCBI'></span>

<h3>Description</h3>

<p>These functions provide an interface to several clustering methods
implemented in R, for use together with the cluster stability
assessment in <code><a href="#topic+clusterboot">clusterboot</a></code> (as parameter
<code>clustermethod</code>; &quot;CBI&quot; stands for &quot;clusterboot interface&quot;).
In some situations it could make sense to use them to compute a
clustering even if you don't want to run <code>clusterboot</code>, because
some of the functions contain some additional features (e.g., normal
mixture model based clustering of dissimilarity matrices projected
into the Euclidean space by MDS or partitioning around medoids with
estimated number of clusters, noise/outlier identification in
hierarchical clustering).   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmeansCBI(data,krange,k,scaling=FALSE,runs=1,criterion="ch",...)

hclustCBI(data,k,cut="number",method,scaling=TRUE,noisecut=0,...)

hclusttreeCBI(data,minlevel=2,method,scaling=TRUE,...)

disthclustCBI(dmatrix,k,cut="number",method,noisecut=0,...)



noisemclustCBI(data,G,k,modelNames,nnk,hcmodel=NULL,Vinv=NULL,
                        summary.out=FALSE,...)

distnoisemclustCBI(dmatrix,G,k,modelNames,nnk,
                        hcmodel=NULL,Vinv=NULL,mdsmethod="classical",
                        mdsdim=4, summary.out=FALSE, points.out=FALSE,...)

claraCBI(data,k,usepam=TRUE,diss=inherits(data,"dist"),...)

pamkCBI(data,krange=2:10,k=NULL,criterion="asw", usepam=TRUE,
        scaling=FALSE,diss=inherits(data,"dist"),...)

tclustCBI(data,k,trim=0.05,...)

dbscanCBI(data,eps,MinPts,diss=inherits(data,"dist"),...)

mahalCBI(data,clustercut=0.5,...)

mergenormCBI(data, G=NULL, k=NULL, modelNames=NULL, nnk=0,
                         hcmodel = NULL,
                         Vinv = NULL, mergemethod="bhat",
                         cutoff=0.1,...)

speccCBI(data,k,...)

pdfclustCBI(data,...)



stupidkcentroidsCBI(dmatrix,k,distances=TRUE)

stupidknnCBI(dmatrix,k)

stupidkfnCBI(dmatrix,k)

stupidkavenCBI(dmatrix,k)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kmeansCBI_+3A_data">data</code></td>
<td>
<p>a numeric matrix. The data
matrix - usually a cases*variables-data matrix. <code>claraCBI</code>,
<code>pamkCBI</code> and <code>dbscanCBI</code> work with an
<code>n*n</code>-dissimilarity matrix as well, see parameter <code>diss</code>.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_dmatrix">dmatrix</code></td>
<td>
<p>a squared numerical dissimilarity matrix or a
<code>dist</code>-object.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_k">k</code></td>
<td>
<p>numeric, usually integer. In most cases, this is the number
of clusters for methods where this is fixed. For <code>hclustCBI</code>
and <code>disthclustCBI</code> see parameter <code>cut</code> below. Some
methods have a <code>k</code> parameter on top of a <code>G</code> or
<code>krange</code> parameter for compatibility; <code>k</code> in these cases
does not have to be specified but if it is, it is always a single
number of clusters and overwrites <code>G</code> and
<code>krange</code>.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_scaling">scaling</code></td>
<td>
<p>either a logical value or a numeric vector of length
equal to the number of variables. If <code>scaling</code> is a numeric
vector with length equal to the number of variables, then each
variable is divided by the corresponding value from <code>scaling</code>.
If <code>scaling</code> is <code>TRUE</code> then scaling is done by dividing
the (centered) variables by their root-mean-square, and if
<code>scaling</code> is <code>FALSE</code>, no scaling is done before execution.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_runs">runs</code></td>
<td>
<p>integer. Number of random initializations from which the
k-means algorithm is started.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_criterion">criterion</code></td>
<td>
<p><code>"ch"</code> or <code>"asw"</code>. Decides whether number
of clusters is estimated by the Calinski-Harabasz criterion or by the
average silhouette width.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_cut">cut</code></td>
<td>
<p>either &quot;level&quot; or &quot;number&quot;. This determines how
<code>cutree</code> is used to obtain a partition from a hierarchy
tree. <code>cut="level"</code> means that the tree is cut at a particular
dissimilarity level, <code>cut="number"</code> means that the tree is cut
in order to obtain a fixed number of clusters. The parameter
<code>k</code> specifies the number of clusters or the dissimilarity
level, depending on <code>cut</code>.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_method">method</code></td>
<td>
<p>method for hierarchical clustering, see the
documentation of <code><a href="stats.html#topic+hclust">hclust</a></code>.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_noisecut">noisecut</code></td>
<td>
<p>numeric. All clusters of size <code>&lt;=noisecut</code> in the
<code>disthclustCBI</code>/<code>hclustCBI</code>-partition are joined and declared as
noise/outliers.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_minlevel">minlevel</code></td>
<td>
<p>integer. <code>minlevel=1</code> means that all clusters in
the tree are given out by <code>hclusttreeCBI</code> or
<code>disthclusttreeCBI</code>, including one-point
clusters (but excluding the cluster with all
points). <code>minlevel=2</code> excludes the one-point clusters.
<code>minlevel=3</code> excludes the two-point cluster which has been
merged first, and increasing the value of <code>minlevel</code> by 1 in
all further steps means that the remaining earliest formed cluster
is excluded.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_g">G</code></td>
<td>
<p>vector of integers. Number of clusters or numbers of clusters
used by
<code><a href="mclust.html#topic+mclustBIC">mclustBIC</a></code>. If
<code>G</code> has more than one entry, the number of clusters is
estimated by the BIC.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_modelnames">modelNames</code></td>
<td>
<p>vector of string. Models for covariance matrices,
see documentation of
<code><a href="mclust.html#topic+mclustBIC">mclustBIC</a></code>.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_nnk">nnk</code></td>
<td>
<p>integer. Tuning constant for
<code><a href="prabclus.html#topic+NNclean">NNclean</a></code>, which is used to estimate the
initial noise for <code>noisemclustCBI</code> and
<code>distnoisemclustCBI</code>. See parameter <code>k</code> in the
documentation of <code><a href="prabclus.html#topic+NNclean">NNclean</a></code>. <code>nnk=0</code> means
that no noise component is fitted.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_hcmodel">hcmodel</code></td>
<td>
<p>string or <code>NULL</code>. Determines the initialization of
the EM-algorithm for
<code><a href="mclust.html#topic+mclustBIC">mclustBIC</a></code>.
Documented in <code><a href="mclust.html#topic+hc">hc</a></code>.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_vinv">Vinv</code></td>
<td>
<p>numeric. See documentation of
<code><a href="mclust.html#topic+mclustBIC">mclustBIC</a></code>.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_summary.out">summary.out</code></td>
<td>
<p>logical. If <code>TRUE</code>, the result of
<code><a href="mclust.html#topic+summary.mclustBIC">summary.mclustBIC</a></code> is added as component
<code>mclustsummary</code> to the output of <code>noisemclustCBI</code> and
<code>distnoisemclustCBI</code>.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_mdsmethod">mdsmethod</code></td>
<td>
<p>&quot;classical&quot;, &quot;kruskal&quot; or &quot;sammon&quot;. Determines the
multidimensional scaling method to compute Euclidean data from a
dissimilarity matrix. See <code><a href="stats.html#topic+cmdscale">cmdscale</a></code>,
<code><a href="MASS.html#topic+isoMDS">isoMDS</a></code> and <code><a href="MASS.html#topic+sammon">sammon</a></code>.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_mdsdim">mdsdim</code></td>
<td>
<p>integer. Dimensionality of MDS solution.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_points.out">points.out</code></td>
<td>
<p>logical. If <code>TRUE</code>, the matrix of MDS points
is added as component
<code>points</code> to the output of <code>noisemclustCBI</code>.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_usepam">usepam</code></td>
<td>
<p>logical. If <code>TRUE</code>, the function
<code><a href="cluster.html#topic+pam">pam</a></code> is used for clustering, otherwise
<code><a href="cluster.html#topic+clara">clara</a></code>. <code><a href="cluster.html#topic+pam">pam</a></code> is better,
<code><a href="cluster.html#topic+clara">clara</a></code> is faster.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_diss">diss</code></td>
<td>
<p>logical. If <code>TRUE</code>, <code>data</code> will be considered as
a dissimilarity matrix. In <code>claraCBI</code>, this requires
<code>usepam=TRUE</code>.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_krange">krange</code></td>
<td>
<p>vector of integers. Numbers of clusters to be compared.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_trim">trim</code></td>
<td>
<p>numeric between 0 and 1. Proportion of data points
trimmed, i.e., assigned to noise. See <code>tclust</code> in the tclust package.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_eps">eps</code></td>
<td>
<p>numeric. The radius of the neighborhoods to be considered
by <code><a href="#topic+dbscan">dbscan</a></code>.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_minpts">MinPts</code></td>
<td>
<p>integer. How many points have to be in a neighborhood so
that a point is considered to be a cluster seed? See documentation
of <code><a href="#topic+dbscan">dbscan</a></code>.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_clustercut">clustercut</code></td>
<td>
<p>numeric between 0 and 1. If <code><a href="#topic+fixmahal">fixmahal</a></code>
is used for fuzzy clustering, a crisp partition is generated and
points with cluster membership values above <code>clustercut</code> are
considered as members of the corresponding cluster.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_mergemethod">mergemethod</code></td>
<td>
<p>method for merging Gaussians, passed on as
<code>method</code> to <code><a href="#topic+mergenormals">mergenormals</a></code>.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_cutoff">cutoff</code></td>
<td>
<p>numeric between 0 and 1, tuning constant for
<code><a href="#topic+mergenormals">mergenormals</a></code>.</p>
</td></tr>








<tr><td><code id="kmeansCBI_+3A_distances">distances</code></td>
<td>
<p>logical (only for <code>stupidkcentroidsCBI</code>). If
<code>FALSE</code>, <code>dmatrix</code> is
interpreted as cases&amp;variables data matrix.</p>
</td></tr>
<tr><td><code id="kmeansCBI_+3A_...">...</code></td>
<td>
<p>further parameters to be transferred to the original
clustering functions (not required).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>All these functions call clustering methods implemented in R to
cluster data and to provide output in the format required by
<code><a href="#topic+clusterboot">clusterboot</a></code>. Here is a brief overview. For further
details see the help pages of the involved clustering methods.
</p>

<dl>
<dt>kmeansCBI</dt><dd><p>an interface to the function
<code><a href="#topic+kmeansruns">kmeansruns</a></code> calling <code><a href="stats.html#topic+kmeans">kmeans</a></code>
for k-means clustering. (<code><a href="#topic+kmeansruns">kmeansruns</a></code> allows the
specification of several random initializations of the
k-means algorithm and estimation of k by the Calinski-Harabasz
index or the average silhouette width.)</p>
</dd>
<dt>hclustCBI</dt><dd><p>an interface to the function
<code><a href="stats.html#topic+hclust">hclust</a></code> for agglomerative hierarchical clustering with
noise component (see parameter <code>noisecut</code> above). This
function produces a partition and assumes a cases*variables
matrix as input.</p>
</dd>
<dt>hclusttreeCBI</dt><dd><p>an interface to the function
<code>hclust</code> for agglomerative hierarchical clustering. This
function gives out all clusters belonging to the hierarchy
(upward from a certain level, see parameter <code>minlevel</code>
above).</p>
</dd>
<dt>disthclustCBI</dt><dd><p>an interface to the function
<code>hclust</code> for agglomerative hierarchical clustering with
noise component (see parameter <code>noisecut</code> above). This
function produces a partition and assumes a dissimilarity
matrix as input.</p>
</dd>





<dt>noisemclustCBI</dt><dd><p>an interface to the function
<code><a href="mclust.html#topic+mclustBIC">mclustBIC</a></code>, for normal mixture model based
clustering. Warning: <code><a href="mclust.html#topic+mclustBIC">mclustBIC</a></code> often
has problems with multiple
points. In <code><a href="#topic+clusterboot">clusterboot</a></code>, it is recommended to use
this together with <code>multipleboot=FALSE</code>.</p>
</dd>	
<dt>distnoisemclustCBI</dt><dd><p>an interface to the function
<code><a href="mclust.html#topic+mclustBIC">mclustBIC</a></code> for normal mixture model based
clustering. This assumes a dissimilarity matrix as input and
generates a data matrix by multidimensional scaling first.
Warning: <code><a href="mclust.html#topic+mclustBIC">mclustBIC</a></code> often has
problems with multiple
points. In <code><a href="#topic+clusterboot">clusterboot</a></code>, it is recommended to use
this together with <code>multipleboot=FALSE</code>.</p>
</dd>
<dt>claraCBI</dt><dd><p>an interface to the functions
<code><a href="cluster.html#topic+pam">pam</a></code> and <code><a href="cluster.html#topic+clara">clara</a></code>
for partitioning around medoids.</p>
</dd>
<dt>pamkCBI</dt><dd><p>an interface to the function
<code><a href="#topic+pamk">pamk</a></code> calling <code><a href="cluster.html#topic+pam">pam</a></code> for
partitioning around medoids. The number
of clusters is estimated by the Calinski-Harabasz index or by the
average silhouette width.</p>
</dd>
<dt>tclustCBI</dt><dd><p>an interface to the function
<code>tclust</code> in the tclust package for trimmed Gaussian 
clustering. This assumes a cases*variables matrix as input.</p>
</dd>










<dt>dbscanCBI</dt><dd><p>an interface to the function
<code><a href="#topic+dbscan">dbscan</a></code> for density based 
clustering.</p>
</dd>
<dt>mahalCBI</dt><dd><p>an interface to the function
<code><a href="#topic+fixmahal">fixmahal</a></code> for fixed point
clustering. This assumes a cases*variables matrix as input.</p>
</dd>
<dt>mergenormCBI</dt><dd><p>an interface to the function
<code><a href="#topic+mergenormals">mergenormals</a></code> for clustering by merging Gaussian
mixture components. Unlike <code><a href="#topic+mergenormals">mergenormals</a></code>, <code>mergenormCBI</code>
includes the computation of the initial Gaussian mixture.
This assumes a cases*variables matrix as input.
</p>
</dd>
<dt>speccCBI</dt><dd><p>an interface to the function
<code><a href="kernlab.html#topic+specc">specc</a></code> for spectral clustering. See
the <code><a href="kernlab.html#topic+specc">specc</a></code> help page for additional tuning
parameters. This assumes a cases*variables matrix as input.</p>
</dd>
<dt>pdfclustCBI</dt><dd><p>an interface to the function
<code><a href="pdfCluster.html#topic+pdfCluster">pdfCluster</a></code> for density-based clustering. See
the <code><a href="pdfCluster.html#topic+pdfCluster">pdfCluster</a></code> help page for additional tuning
parameters. This assumes a cases*variables matrix as input.</p>
</dd>








<dt>stupidkcentroidsCBI</dt><dd><p>an interface to the function
<code>stupidkcentroids</code> for random centroid-based clustering. See
the <code><a href="#topic+stupidkcentroids">stupidkcentroids</a></code> help page. This can have a
distance matrix as well as a cases*variables matrix as input, see
parameter <code>distances</code>.</p>
</dd>
<dt>stupidknnCBI</dt><dd><p>an interface to the function
<code>stupidknn</code> for random nearest neighbour clustering. See
the <code><a href="#topic+stupidknn">stupidknn</a></code> help page. This assumes a
distance matrix as input.</p>
</dd>
<dt>stupidkfnCBI</dt><dd><p>an interface to the function
<code>stupidkfn</code> for random farthest neighbour clustering. See
the <code><a href="#topic+stupidkfn">stupidkfn</a></code> help page. This assumes a
distance matrix as input.</p>
</dd>
<dt>stupidkavenCBI</dt><dd><p>an interface to the function
<code>stupidkaven</code> for random average dissimilarity clustering. See
the <code><a href="#topic+stupidkaven">stupidkaven</a></code> help page. This assumes a
distance matrix as input.</p>
</dd>
</dl>



<h3>Value</h3>

<p>All interface functions return a list with the following components
(there may be some more, see <code>summary.out</code> and <code>points.out</code>
above):
</p>
<table>
<tr><td><code>result</code></td>
<td>
<p>clustering result, usually a list with the full
output of the clustering method (the precise format doesn't
matter); whatever you want to use later.</p>
</td></tr>
<tr><td><code>nc</code></td>
<td>
<p>number of clusters. If some points don't belong to any
cluster, these are declared &quot;noise&quot;. <code>nc</code> includes the
&quot;noise cluster&quot;, and there should be another component
<code>nccl</code>, being the number of clusters not including the
noise cluster.</p>
</td></tr>
<tr><td><code>clusterlist</code></td>
<td>
<p>this is a list consisting of a logical vectors
of length of the number of data points (<code>n</code>) for each cluster,
indicating whether a point is a member of this cluster
(<code>TRUE</code>) or not. If a noise cluster is included, it
should always be the last vector in this list.</p>
</td></tr>
<tr><td><code>partition</code></td>
<td>
<p>an integer vector of length <code>n</code>,
partitioning the data. If the method produces a partition, it
should be the clustering. This component is only used for plots,
so you could do something like <code>rep(1,n)</code> for
non-partitioning methods. If a noise cluster is included,
<code>nc=nccl+1</code> and the noise cluster is cluster no. <code>nc</code>.</p>
</td></tr>
<tr><td><code>clustermethod</code></td>
<td>
<p>a string indicating the clustering method.</p>
</td></tr>      
</table>
<p>The output of some of the functions has further components:
</p>
<table>
<tr><td><code>nccl</code></td>
<td>
<p>see <code>nc</code> above.</p>
</td></tr>
<tr><td><code>nnk</code></td>
<td>
<p>by <code>noisemclustCBI</code> and <code>distnoisemclustCBI</code>,
see above.</p>
</td></tr>
<tr><td><code>initnoise</code></td>
<td>
<p>logical vector, indicating initially estimated noise by
<code><a href="prabclus.html#topic+NNclean">NNclean</a></code>, called by <code>noisemclustCBI</code>
and <code>distnoisemclustCBI</code>.</p>
</td></tr>
<tr><td><code>noise</code></td>
<td>
<p>logical. <code>TRUE</code> if points were classified as
noise/outliers by <code>disthclustCBI</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+clusterboot">clusterboot</a></code>, <code><a href="stats.html#topic+dist">dist</a></code>,
<code><a href="stats.html#topic+kmeans">kmeans</a></code>, <code><a href="#topic+kmeansruns">kmeansruns</a></code>, <code><a href="stats.html#topic+hclust">hclust</a></code>,
<code><a href="mclust.html#topic+mclustBIC">mclustBIC</a></code>, 
<code><a href="cluster.html#topic+pam">pam</a></code>,  <code><a href="#topic+pamk">pamk</a></code>,
<code><a href="cluster.html#topic+clara">clara</a></code>,
<code><a href="#topic+dbscan">dbscan</a></code>,
<code><a href="#topic+fixmahal">fixmahal</a></code>,
<code><a href="tclust.html#topic+tclust">tclust</a></code>, <code><a href="pdfCluster.html#topic+pdfCluster">pdfCluster</a></code>

</p>


<h3>Examples</h3>

<pre><code class='language-R'>  options(digits=3)
  set.seed(20000)
  face &lt;- rFace(50,dMoNo=2,dNoEy=0,p=2)
  dbs &lt;- dbscanCBI(face,eps=1.5,MinPts=4)
  dhc &lt;- disthclustCBI(dist(face),method="average",k=1.5,noisecut=2)
  table(dbs$partition,dhc$partition)
  dm &lt;- mergenormCBI(face,G=10,modelNames="EEE",nnk=2)
  dtc &lt;- tclustCBI(face,6,trim=0.1,restr.fact=500)
  table(dm$partition,dtc$partition)

</code></pre>

<hr>
<h2 id='kmeansruns'>k-means with estimating k and initialisations</h2><span id='topic+kmeansruns'></span>

<h3>Description</h3>

<p>This calls the function <code><a href="stats.html#topic+kmeans">kmeans</a></code> to perform a k-means
clustering, but initializes the k-means algorithm several times with
random points from the data set as means. Furthermore, it is more
robust against the occurrence of empty clusters in the algorithm and
it estimates the number of clusters by either the Calinski Harabasz
index (<code><a href="#topic+calinhara">calinhara</a></code>) or average silhouette width (see
<code><a href="cluster.html#topic+pam.object">pam.object</a></code>). The Duda-Hart test
(<code><a href="#topic+dudahart2">dudahart2</a></code>) is applied to decide whether there should be
more than one cluster (unless 1 is excluded as number of clusters).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmeansruns(data,krange=2:10,criterion="ch",
                       iter.max=100,runs=100,
                       scaledata=FALSE,alpha=0.001,
                       critout=FALSE,plot=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kmeansruns_+3A_data">data</code></td>
<td>
<p>A numeric matrix of data, or an object that can be coerced to
such a matrix (such as a numeric vector or a data frame with
all numeric columns). </p>
</td></tr>
<tr><td><code id="kmeansruns_+3A_krange">krange</code></td>
<td>
<p>integer vector. Numbers of clusters which are to be
compared by the average silhouette width criterion. Note: average
silhouette width and Calinski-Harabasz can't estimate number of
clusters <code>nc=1</code>. If 1 is included, a Duda-Hart test is applied
and 1 is estimated if this is not significant.</p>
</td></tr>
<tr><td><code id="kmeansruns_+3A_criterion">criterion</code></td>
<td>
<p>one of <code>"asw"</code> or <code>"ch"</code>. Determines
whether average silhouette width or Calinski-Harabasz is applied.</p>
</td></tr>
<tr><td><code id="kmeansruns_+3A_iter.max">iter.max</code></td>
<td>
<p>integer. The maximum number of iterations allowed.</p>
</td></tr>
<tr><td><code id="kmeansruns_+3A_runs">runs</code></td>
<td>
<p>integer. Number of starts of the k-means algorithm.</p>
</td></tr>
<tr><td><code id="kmeansruns_+3A_scaledata">scaledata</code></td>
<td>
<p>logical. If <code>TRUE</code>, the variables are centered
and scaled to unit variance before execution.</p>
</td></tr>
<tr><td><code id="kmeansruns_+3A_alpha">alpha</code></td>
<td>
<p>numeric between 0 and 1, tuning constant for
<code><a href="#topic+dudahart2">dudahart2</a></code> (only used for 1-cluster test).</p>
</td></tr>
<tr><td><code id="kmeansruns_+3A_critout">critout</code></td>
<td>
<p>logical. If <code>TRUE</code>, the criterion value is printed
out for every number of clusters.</p>
</td></tr>
<tr><td><code id="kmeansruns_+3A_plot">plot</code></td>
<td>
<p>logical. If <code>TRUE</code>, every clustering resulting from a
run of the algorithm is plotted.</p>
</td></tr>
<tr><td><code id="kmeansruns_+3A_...">...</code></td>
<td>
<p>further arguments to be passed on to <code><a href="stats.html#topic+kmeans">kmeans</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The output of the optimal run of the <code><a href="stats.html#topic+kmeans">kmeans</a></code>-function
with added components <code>bestk</code> and <code>crit</code>.
A list with components
</p>
<table>
<tr><td><code>cluster</code></td>
<td>
<p>A vector of integers indicating the cluster to which each
point is allocated.</p>
</td></tr> 
<tr><td><code>centers</code></td>
<td>
<p>A matrix of cluster centers.</p>
</td></tr>
<tr><td><code>withinss</code></td>
<td>
<p>The within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>The number of points in each cluster.</p>
</td></tr>
<tr><td><code>bestk</code></td>
<td>
<p>The optimal number of clusters.</p>
</td></tr>
<tr><td><code>crit</code></td>
<td>
<p>Vector with values of the <code>criterion</code> for all used numbers of
clusters (0 if number not tried).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Calinski, T., and Harabasz, J. (1974) A Dendrite Method for Cluster 
Analysis, <em>Communications in Statistics</em>, 3, 1-27.
</p>
<p>Duda, R. O. and Hart, P. E. (1973) <em>Pattern Classification and
Scene Analysis</em>. Wiley, New York.
</p>
<p>Hartigan, J. A. and Wong, M. A. (1979).  A K-means clustering
algorithm. <em>Applied Statistics</em>, 28, 100-108.
</p>
<p>Kaufman, L. and Rousseeuw, P.J. (1990). &quot;Finding Groups in Data:
An Introduction to Cluster Analysis&quot;. Wiley, New York.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+kmeans">kmeans</a></code>, <code><a href="#topic+pamk">pamk</a></code>,
<code><a href="#topic+calinhara">calinhara</a></code>, <code><a href="#topic+dudahart2">dudahart2</a></code>)  
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  options(digits=3)
  set.seed(20000)
  face &lt;- rFace(50,dMoNo=2,dNoEy=0,p=2)
  pka &lt;- kmeansruns(face,krange=1:5,critout=TRUE,runs=2,criterion="asw")
  pkc &lt;- kmeansruns(face,krange=1:5,critout=TRUE,runs=2,criterion="ch")
</code></pre>

<hr>
<h2 id='lcmixed'>flexmix method for mixed Gaussian/multinomial mixtures</h2><span id='topic+lcmixed'></span>

<h3>Description</h3>

<p><code>lcmixed</code> is a method for the
<code><a href="flexmix.html#topic+flexmix">flexmix</a></code>-function in package
<code>flexmix</code>. It provides the necessary information to run an
EM-algorithm for maximum likelihood estimation for a latent class
mixture (clustering) model where some variables are continuous
and modelled within the mixture components by Gaussian distributions
and some variables are categorical and modelled within components by
independent multinomial distributions. <code>lcmixed</code> can be called
within <code>flexmix</code>. The function <code><a href="#topic+flexmixedruns">flexmixedruns</a></code> is a wrapper
function that can be run to apply <code>lcmixed</code>.
</p>
<p>Note that at least one categorical variable is needed, but it is
possible to use data without continuous variable.
</p>
<p>There are further format restrictions to the data (see below in the
documentation of <code>continuous</code> and <code>discrete</code>), which
can be ignored when running <code>lcmixed</code> through
<code><a href="#topic+flexmixedruns">flexmixedruns</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lcmixed( formula = .~. , continuous, discrete, ppdim,
                     diagonal = TRUE, pred.ordinal=FALSE, printlik=FALSE )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lcmixed_+3A_formula">formula</code></td>
<td>
<p>a formula to specify response and explanatory
variables. For <code>lcmixed</code> this always has the form <code>x~1</code>,
where <code>x</code> is a matrix or data frome of all variables to be
involved, because regression and explanatory variables are not
implemented.</p>
</td></tr>
<tr><td><code id="lcmixed_+3A_continuous">continuous</code></td>
<td>
<p>number of continuous variables. Note that the
continuous variables always need to be the first variables in the
matrix or data frame.</p>
</td></tr>
<tr><td><code id="lcmixed_+3A_discrete">discrete</code></td>
<td>
<p>number of categorical variables. Always the last
variables in the matrix or data frame. Note that categorical
variables always must be coded as integers 1,2,3, etc. without
interruption.</p>
</td></tr>
<tr><td><code id="lcmixed_+3A_ppdim">ppdim</code></td>
<td>
<p>vector of integers specifying the number of (in the data)
existing categories for each categorical variable.</p>
</td></tr>
<tr><td><code id="lcmixed_+3A_diagonal">diagonal</code></td>
<td>
<p>logical. If <code>TRUE</code>, Gaussian models are fitted
restricted to diagonal covariance matrices. Otherwise, covariance
matrices are unrestricted. <code>TRUE</code> is consistent with the
&quot;within class independence&quot; assumption for the multinomial variables.</p>
</td></tr>
<tr><td><code id="lcmixed_+3A_pred.ordinal">pred.ordinal</code></td>
<td>
<p>logical. If <code>FALSE</code>, the within-component
predicted value for categorical variables is the probability mode,
otherwise it is the mean of the standard (1,2,3,...) scores, which
may be better for ordinal variables.</p>
</td></tr>
<tr><td><code id="lcmixed_+3A_printlik">printlik</code></td>
<td>
<p>logical. If <code>TRUE</code>, the loglikelihood is printed
out whenever computed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data need to be organised case-wise, i.e., if there are
categorical variables only, and 15 cases with values c(1,1,2) on the
3 variables, the data matrix needs 15 rows with values 1 1 2.
</p>
<p>General documentation on flexmix methods can be found in Chapter 4 of
Friedrich Leisch's &quot;FlexMix: A General Framework for Finite Mixture
Models and Latent Class Regression in R&quot;,
<a href="https://CRAN.R-project.org/package=flexmix">https://CRAN.R-project.org/package=flexmix</a>
</p>


<h3>Value</h3>

<p>An object of class <code>FLXMC</code> (not documented; only used
internally by <code>flexmix</code>).
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en">https://www.unibo.it/sitoweb/christian.hennig/en</a></p>


<h3>References</h3>

<p>Hennig, C. and Liao, T. (2013) How to find an appropriate clustering
for mixed-type variables with application to socio-economic
stratification, <em>Journal of the Royal Statistical Society, Series
C Applied Statistics</em>, 62, 309-369.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+flexmixedruns">flexmixedruns</a></code>, <code><a href="flexmix.html#topic+flexmix">flexmix</a></code>,
<code><a href="flexmix.html#topic+flexmix-class">flexmix-class</a></code>,
<code><a href="#topic+discrete.recode">discrete.recode</a></code>, which recodes a dataset into the format
required by <code>lcmixed</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(112233)
  options(digits=3)
  require(MASS)
  require(flexmix)
  data(Cars93)
  Cars934 &lt;- Cars93[,c(3,5,8,10)]
  cc &lt;-
  discrete.recode(Cars934,xvarsorted=FALSE,continuous=c(2,3),discrete=c(1,4))
  fcc &lt;- flexmix(cc$data~1,k=2,
  model=lcmixed(continuous=2,discrete=2,ppdim=c(6,3),diagonal=TRUE))
  summary(fcc)
</code></pre>

<hr>
<h2 id='localshape'>Local shape matrix</h2><span id='topic+localshape'></span>

<h3>Description</h3>

<p>This computes a matrix formalising 'local shape', i.e., aggregated
standardised variance/covariance in a Mahalanobis neighbourhood of the data
points. This can be used for finding clusters when used as one of the
covariance matrices in 
Invariant Coordinate Selection (function <code>ics</code> in package
<code>ICS</code>), see Hennig's
discussion and rejoinder of Tyler et al. (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  localshape(xdata,proportion=0.1,mscatter="mcd",mcdalpha=0.8,
                       covstandard="det")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="localshape_+3A_xdata">xdata</code></td>
<td>
<p>objects times variables data matrix.</p>
</td></tr>
<tr><td><code id="localshape_+3A_proportion">proportion</code></td>
<td>
<p>proportion of points to be considered as neighbourhood.</p>
</td></tr>
<tr><td><code id="localshape_+3A_mscatter">mscatter</code></td>
<td>
<p>&quot;mcd&quot; or &quot;cov&quot;; specified minimum covariance
determinant or
classical covariance matrix to be used for Mahalanobis distance
computation.</p>
</td></tr>
<tr><td><code id="localshape_+3A_mcdalpha">mcdalpha</code></td>
<td>
<p>if <code>mscatter="mcd"</code>, this is the alpha parameter
to be used by the MCD covariance matrix, i.e. one minus the
asymptotic breakdown point, see <code><a href="robustbase.html#topic+covMcd">covMcd</a></code>.</p>
</td></tr>
<tr><td><code id="localshape_+3A_covstandard">covstandard</code></td>
<td>
<p>one of &quot;trace&quot;, &quot;det&quot; or &quot;none&quot;, determining by
what constant the pointwise neighbourhood covariance matrices are
standardised. &quot;det&quot; makes the affine equivariant, as noted in the
discussion rejoinder of Tyler et al. (2009).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The local shape matrix.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en">https://www.unibo.it/sitoweb/christian.hennig/en</a></p>


<h3>References</h3>

<p>Tyler, D. E., Critchley, F., Duembgen, L., Oja, H. (2009)
Invariant coordinate selection (with discussion).
<em>Journal of the Royal Statistical Society, Series B</em>, 549-592.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  options(digits=3)
  data(iris)
  localshape(iris[,-5],mscatter="cov")
</code></pre>

<hr>
<h2 id='mahalanodisc'>Mahalanobis for AWC</h2><span id='topic+mahalanodisc'></span>

<h3>Description</h3>

<p>Vector of Mahalanobis distances or their root. For use in <code>awcoord</code> only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mahalanodisc(x2, mg, covg, modus="square") 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mahalanodisc_+3A_x2">x2</code></td>
<td>
<p>numerical data matrix.</p>
</td></tr>
<tr><td><code id="mahalanodisc_+3A_mg">mg</code></td>
<td>
<p>mean vector.</p>
</td></tr>
<tr><td><code id="mahalanodisc_+3A_covg">covg</code></td>
<td>
<p>covariance matrix.</p>
</td></tr>
<tr><td><code id="mahalanodisc_+3A_modus">modus</code></td>
<td>
<p>&quot;md&quot; (roots of Mahalanobis distances) or &quot;square&quot;
(original squared form of Mahalanobis distances).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The covariance matrix
is inverted by use of
<code><a href="#topic+solvecov">solvecov</a></code>, which can be expected to give
reasonable results for singular within-class covariance matrices.
</p>


<h3>Value</h3>

<p>vector of (rooted) Mahalanobis distances.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+awcoord">awcoord</a></code>, <code><a href="#topic+solvecov">solvecov</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  options(digits=3)
  x &lt;- cbind(rnorm(50),rnorm(50))
  mahalanodisc(x,c(0,0),cov(x))
  mahalanodisc(x,c(0,0),matrix(0,ncol=2,nrow=2))
</code></pre>

<hr>
<h2 id='mahalanofix'>Mahalanobis distances from center of indexed points</h2><span id='topic+mahalanofix'></span><span id='topic+mahalanofuz'></span>

<h3>Description</h3>

<p>Computes the vector of (classical or robust)
Mahalanobis distances of all points of <code>x</code>
to the center of the points indexed (or weighted)
by <code>gv</code>. The latter also determine
the covariance matrix.
</p>
<p>Thought for use within <code><a href="#topic+fixmahal">fixmahal</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mahalanofix(x, n = nrow(as.matrix(x)), p = ncol(as.matrix(x)), gv =
rep(1, times = n), cmax = 1e+10, method = "ml")

mahalanofuz(x, n = nrow(as.matrix(x)), p = ncol(as.matrix(x)),
                         gv = rep(1, times=n), cmax = 1e+10) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mahalanofix_+3A_x">x</code></td>
<td>
<p>a numerical data matrix, rows are points, columns are variables.</p>
</td></tr>
<tr><td><code id="mahalanofix_+3A_n">n</code></td>
<td>
<p>positive integer. Number of points.</p>
</td></tr>
<tr><td><code id="mahalanofix_+3A_p">p</code></td>
<td>
<p>positive integer. Number of variables.</p>
</td></tr>
<tr><td><code id="mahalanofix_+3A_gv">gv</code></td>
<td>
<p>for <code>mahalanofix</code>
a logical or 0-1 vector of length <code>n</code>. For <code>mahalanofuz</code> a
numerical vector with values between 0 and 1.</p>
</td></tr>
<tr><td><code id="mahalanofix_+3A_cmax">cmax</code></td>
<td>
<p>positive number. used in <code><a href="#topic+solvecov">solvecov</a></code> if
covariance matrix is singular.</p>
</td></tr>
<tr><td><code id="mahalanofix_+3A_method">method</code></td>
<td>
<p><code>"ml"</code>, <code>"classical"</code>,
<code>"mcd"</code> or <code>"mve"</code>. Method to compute the covariance
matrix estimator. See <code><a href="MASS.html#topic+cov.rob">cov.rob</a></code>, <code><a href="#topic+fixmahal">fixmahal</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code><a href="#topic+solvecov">solvecov</a></code> is used to invert the covariance matrix. The methods
<code>"mcd"</code> and <code>"mve"</code> in <code>mahalanofix</code> do not work properly
with point constellations with singular covariance matrices!
</p>


<h3>Value</h3>

<p>A list of the following components:
</p>
<table>
<tr><td><code>md</code></td>
<td>
<p>vector of Mahalanobis distances.</p>
</td></tr>
<tr><td><code>mg</code></td>
<td>
<p>mean of the points indexed by <code>gv</code>, weighted mean in
<code>mahalanofuz</code>.</p>
</td></tr>
<tr><td><code>covg</code></td>
<td>
<p>covariance matrix of the points indexed by <code>gv</code>,
weighted covariance matrix in <code>mahalanofuz</code>.</p>
</td></tr> 
<tr><td><code>covinv</code></td>
<td>
<p><code>covg</code> inverted by <code><a href="#topic+solvecov">solvecov</a></code>.</p>
</td></tr>
<tr><td><code>coll</code></td>
<td>
<p>logical. If <code>TRUE</code>, <code>covg</code> has been
(numerically) singular.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Methods <code>"mcd"</code> and <code>"mve"</code> require library <code>lqs</code>.</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+fixmahal">fixmahal</a></code>, <code><a href="#topic+solvecov">solvecov</a></code>, <code><a href="MASS.html#topic+cov.rob">cov.rob</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  x &lt;- c(1,2,3,4,5,6,7,8,9,10)
  y &lt;- c(1,2,3,8,7,6,5,8,9,10)
  mahalanofix(cbind(x,y),gv=c(0,0,0,1,1,1,1,1,0,0))
  mahalanofix(cbind(x,y),gv=c(0,0,0,1,1,1,1,0,0,0))
  mahalanofix(cbind(x,y),gv=c(0,0,0,1,1,1,1,1,0,0),method="mcd")
  mahalanofuz(cbind(x,y),gv=c(0,0,0.5,0.5,1,1,1,0.5,0.5,0))
</code></pre>

<hr>
<h2 id='mahalconf'>Mahalanobis fixed point clusters initial configuration</h2><span id='topic+mahalconf'></span>

<h3>Description</h3>

<p>Generates an initial configuration of <code>startn</code> points from
dataset <code>x</code> for the <code><a href="#topic+fixmahal">fixmahal</a></code>
fixed point iteration.
</p>
<p>Thought only for use within <code><a href="#topic+fixmahal">fixmahal</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mahalconf(x, no, startn, covall, plot)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mahalconf_+3A_x">x</code></td>
<td>
<p>numerical matrix. Rows are points, columns are variables.</p>
</td></tr>
<tr><td><code id="mahalconf_+3A_no">no</code></td>
<td>
<p>integer between 1 and <code>nrow(x)</code>. Number of the first
point of the configuration.</p>
</td></tr>
<tr><td><code id="mahalconf_+3A_startn">startn</code></td>
<td>
<p>integer between 1 and <code>nrow(x)</code>.</p>
</td></tr>
<tr><td><code id="mahalconf_+3A_covall">covall</code></td>
<td>
<p>covariance matrix for the computation of the first
Mahalanobis distances.</p>
</td></tr>
<tr><td><code id="mahalconf_+3A_plot">plot</code></td>
<td>
<p>a string. If equal to <code>"start"</code> or <code>"both"</code>,the
first two variables and the first <code>ncol(x)+1</code> points are plotted.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>mahalconf</code> first chooses the <code class="reqn">p</code> (number of variables)
nearest points to point no. <code>no</code> in terms of the Mahalanobis
distance w.r.t. <code>covall</code>, so that there are <code class="reqn">p+1</code> points.
In every further step, the covariance
matrix of the current configuration is computed and the nearest point
in terms of the new Mahalanobis distance is
added. <code><a href="#topic+solvecov">solvecov</a></code> is used to invert singular covariance
matrices. 
</p>


<h3>Value</h3>

<p>A logical vector of length <code>nrow(x)</code>.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+fixmahal">fixmahal</a></code>, <code><a href="#topic+solvecov">solvecov</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(4634)
  face &lt;- rFace(600,dMoNo=2,dNoEy=0,p=2)
  mahalconf(face,no=200,startn=20,covall=cov(face),plot="start")
</code></pre>

<hr>
<h2 id='mergenormals'>Clustering by merging Gaussian mixture components</h2><span id='topic+mergenormals'></span><span id='topic+summary.mergenorm'></span><span id='topic+print.summary.mergenorm'></span>

<h3>Description</h3>

<p>Clustering by merging Gaussian mixture components; computes all
methods introduced in Hennig (2010) from an initial mclust
clustering. See details section for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  mergenormals(xdata, mclustsummary=NULL, 
                         clustering, probs, muarray, Sigmaarray, z,
                         method=NULL, cutoff=NULL, by=0.005,
                         numberstop=NULL, renumber=TRUE, M=50, ...)

  ## S3 method for class 'mergenorm'
summary(object, ...)

  ## S3 method for class 'summary.mergenorm'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mergenormals_+3A_xdata">xdata</code></td>
<td>
<p>data (something that can be coerced into a matrix).</p>
</td></tr>
<tr><td><code id="mergenormals_+3A_mclustsummary">mclustsummary</code></td>
<td>
<p>output object from
<code><a href="mclust.html#topic+summary.mclustBIC">summary.mclustBIC</a></code> for <code>xdata</code>. Either
<code>mclustsummary</code> or all of <code>clustering</code>,
<code>probs</code>, <code>muarray</code>, <code>Sigmaarray</code> and <code>z</code> need
to be specified (the latter are obtained from <code>mclustsummary</code>
if they are not provided). I am not aware of restrictions of the
usage of
<code><a href="mclust.html#topic+mclustBIC">mclustBIC</a></code> to produce an initial clustering;
covariance matrix models can be restricted and a noise component can be
included if desired, although I have probably not tested all
possibilities.  
</p>
</td></tr>
<tr><td><code id="mergenormals_+3A_clustering">clustering</code></td>
<td>
<p>vector of integers. Initial assignment of data to
mixture components.</p>
</td></tr>
<tr><td><code id="mergenormals_+3A_probs">probs</code></td>
<td>
<p>vector of component proportions (for all components;
should sum up to one).</p>
</td></tr>
<tr><td><code id="mergenormals_+3A_muarray">muarray</code></td>
<td>
<p>matrix of component means (rows).</p>
</td></tr>
<tr><td><code id="mergenormals_+3A_sigmaarray">Sigmaarray</code></td>
<td>
<p>array of component covariance matrices (third
dimension refers to component number).</p>
</td></tr>
<tr><td><code id="mergenormals_+3A_z">z</code></td>
<td>
<p>matrix of observation- (row-)wise posterior probabilities of
belonging to the components (columns).</p>
</td></tr>
<tr><td><code id="mergenormals_+3A_method">method</code></td>
<td>
<p>one of <code>"bhat"</code>, <code>"ridge.uni"</code>,
<code>"ridge.ratio"</code>, <code>"demp"</code>, <code>"dipuni"</code>,
<code>"diptantrum"</code>, <code>"predictive"</code>. See details.</p>
</td></tr>
<tr><td><code id="mergenormals_+3A_cutoff">cutoff</code></td>
<td>
<p>numeric between 0 and 1. Tuning constant, see details
and Hennig (2010). If not specified, the default values given in (9)
in Hennig (2010) are used.</p>
</td></tr>
<tr><td><code id="mergenormals_+3A_by">by</code></td>
<td>
<p>real between 0 and 1. Interval width for density computation
along the ridgeline, used for methods <code>"ridge.uni"</code> and
<code>"ridge.ratio"</code>. Methods <code>"dipuni"</code> and
<code>"diptantrum"</code> require ridgeline computations and use it as well.</p>
</td></tr> 
<tr><td><code id="mergenormals_+3A_numberstop">numberstop</code></td>
<td>
<p>integer. If specified, <code>cutoff</code> is ignored and
components are merged until the number of clusters specified here is
reached.</p>
</td></tr>
<tr><td><code id="mergenormals_+3A_renumber">renumber</code></td>
<td>
<p>logical. If <code>TRUE</code> merged clusters are renumbered
from 1 to their number. If not, numbers of the original clustering
are used (numbers of components that were merged into others then
will not appear).</p>
</td></tr>
<tr><td><code id="mergenormals_+3A_m">M</code></td>
<td>
<p>integer. Number of times the dataset is divided into two
halves. Used if <code>method="predictive"</code>.</p>
</td></tr> 
<tr><td><code id="mergenormals_+3A_...">...</code></td>
<td>
<p>additional optional parameters to pass on to
<code>ridgeline.diagnosis</code> or <code>mixpredictive</code> (in
<code>mergenormals</code>).</p>
</td></tr>
<tr><td><code id="mergenormals_+3A_object">object</code></td>
<td>
<p>object of class <code>mergenorm</code>, output of
<code>mergenormals</code>.</p>
</td></tr>
<tr><td><code id="mergenormals_+3A_x">x</code></td>
<td>
<p>object of class <code>summary.mergenorm</code>, output of
<code>summary.mergenorm</code>.</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>Mixture components are merged in a hierarchical fashion. The merging
criterion is computed for all pairs of current clusters and the two
clusters with the highest criterion value (lowest, respectively, for
<code>method="predictive"</code>) are merged. Then criterion values are
recomputed for the merged cluster. Merging is continued until the
criterion value to merge is below (or above, for
<code>method="predictive"</code>) the cutoff value. Details are given in
Hennig (2010). The following criteria are offered, specified by the
<code>method</code>-argument.
</p>

<dl>
<dt>&quot;ridge.uni&quot;</dt><dd><p>components are only merged if their mixture is
unimodal according to Ray and Lindsay's (2005) ridgeline theory,
see <code><a href="#topic+ridgeline.diagnosis">ridgeline.diagnosis</a></code>. This ignores argument
<code>cutoff</code>.</p>
</dd>
<dt>&quot;ridge.ratio&quot;</dt><dd><p>ratio between density minimum between
components and minimum of density maxima according to Ray and
Lindsay's (2005) ridgeline theory, see
<code><a href="#topic+ridgeline.diagnosis">ridgeline.diagnosis</a></code>. </p>
</dd>      
<dt>&quot;bhat&quot;</dt><dd><p>Bhattacharyya upper bound on misclassification
probability between two components, see
<code><a href="#topic+bhattacharyya.matrix">bhattacharyya.matrix</a></code>.</p>
</dd>
<dt>&quot;demp&quot;</dt><dd><p>direct estimation of misclassification probability
between components, see Hennig (2010).</p>
</dd>
<dt>&quot;dipuni&quot;</dt><dd><p>this uses <code>method="ridge.ratio"</code> to decide
which clusters to merge but stops merging according to the p-value of
the dip test computed as in Hartigan and Hartigan (1985), see
<code><a href="diptest.html#topic+dip.test">dip.test</a></code>.</p>
</dd>
<dt>&quot;diptantrum&quot;</dt><dd><p>as <code>"dipuni"</code>, but p-value of dip test
computed as in Tantrum, Murua and Stuetzle (2003), see
<code><a href="#topic+dipp.tantrum">dipp.tantrum</a></code>.</p>
</dd>
<dt>&quot;predictive&quot;</dt><dd><p>this uses <code>method="demp"</code> to decide which
clusters to merge but stops merging according to the value of
prediction strength (Tibshirani and Walther, 2005) as computed in
<code><a href="#topic+mixpredictive">mixpredictive</a></code>.</p>
</dd>
</dl>



<h3>Value</h3>

<p><code>mergenormals</code> gives out an object of class <code>mergenorm</code>,
which is a List with components
</p>
<table>
<tr><td><code>clustering</code></td>
<td>
<p>integer vector. Final clustering.</p>
</td></tr>
<tr><td><code>clusternumbers</code></td>
<td>
<p>vector of numbers of remaining clusters. These
are given in terms of the original clusters even of
<code>renumber=TRUE</code>, in which case they may be needed to understand
the numbering of some further components, see below.</p>
</td></tr>
<tr><td><code>defunct.components</code></td>
<td>
<p>vector of numbers of components that were
&quot;merged away&quot;.</p>
</td></tr>
<tr><td><code>valuemerged</code></td>
<td>
<p>vector of values of the merging criterion (see
details) at which components were merged.</p>
</td></tr>
<tr><td><code>mergedtonumbers</code></td>
<td>
<p>vector of numbers of clusters to which the
original components were merged.</p>
</td></tr>
<tr><td><code>parameters</code></td>
<td>
<p>a list, if <code>mclustsummary</code> was provided. Entry
no. i refers to number i in <code>clusternumbers</code>. The list entry i
contains the parameters  of the original mixture components that
make up cluster i, as extracted by
<code><a href="#topic+extract.mixturepars">extract.mixturepars</a></code>.</p>
</td></tr>
<tr><td><code>predvalues</code></td>
<td>
<p>vector of prediction strength values for
clusternumbers from 1 to the number of components in the original
mixture, if <code>method=="predictive"</code>. See
<code><a href="#topic+mixpredictive">mixpredictive</a></code>.</p>
</td></tr>
<tr><td><code>orig.decisionmatrix</code></td>
<td>
<p>square matrix with entries giving the
original values of the merging criterion (see details) for every pair
of original mixture components.</p>
</td></tr>
<tr><td><code>new.decisionmatrix</code></td>
<td>
<p>square matrix as <code>orig.decisionmatrix</code>,
but with final entries; numbering of rows and columns corresponds to
<code>clusternumbers</code>; all entries corresponding to other rows and
columns can be ignored.</p>
</td></tr>
<tr><td><code>probs</code></td>
<td>
<p>final cluster values of <code>probs</code> (see arguments)
for merged components, generated by (potentially repeated) execution
of  <code><a href="#topic+mergeparameters">mergeparameters</a></code> out of the original
ones. Numbered according to <code>clusternumbers</code>.</p>
</td></tr>
<tr><td><code>muarray</code></td>
<td>
<p>final cluster means, analogous to <code>probs</code>.</p>
</td></tr>
<tr><td><code>Sigmaarray</code></td>
<td>
<p>final cluster covariance matrices, analogous to
<code>probs</code>.</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>final matrix of posterior probabilities of observations
belonging to the clusters, analogous to <code>probs</code>.</p>
</td></tr>
<tr><td><code>noise</code></td>
<td>
<p>logical. If <code>TRUE</code>, there was a noise component
fitted in the initial mclust clustering (see help for
<code>initialization</code> in <code><a href="mclust.html#topic+mclustBIC">mclustBIC</a></code>). In this
case, a cluster number 0 indicates noise. noise is ignored by the
merging methods and kept as it was originally.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>as above.</p>
</td></tr>  
<tr><td><code>cutoff</code></td>
<td>
<p>as above.</p>
</td></tr>
</table>
<p><code>summary.mergenorm</code> gives out a list with components
<code>clustering, clusternumbers, defunct.components, valuemerged,
    mergedtonumbers, predvalues, probs, muarray, Sigmaarray, z, noise,
    method, cutoff</code> as above, plus <code>onc</code> (original number of
components) and <code>mnc</code> (number of clusters after merging).
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>J. A. Hartigan and P. M. Hartigan (1985) The Dip Test of
Unimodality, <em>Annals of Statistics</em>, 13, 70-84.
</p>
<p>Hennig, C. (2010) Methods for merging Gaussian mixture components,
<em>Advances in Data Analysis and Classification</em>, 4, 3-34.
</p>
<p>Ray, S. and Lindsay, B. G. (2005) The Topography of Multivariate 
Normal Mixtures, <em>Annals of Statistics</em>, 33, 2042-2065.
</p>
<p>Tantrum, J., Murua, A. and Stuetzle, W. (2003) Assessment and 
Pruning of Hierarchical Model Based Clustering, <em>Proceedings of the 
ninth ACM SIGKDD international conference on Knowledge discovery and 
data mining</em>, Washington, D.C., 197-205.
</p>
<p>Tibshirani, R. and Walther, G. (2005) Cluster Validation by 
Prediction Strength, <em>Journal of Computational and Graphical 
Statistics</em>, 14, 511-528.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  require(mclust)
  require(MASS)
  options(digits=3)
  data(crabs)
  dc &lt;- crabs[,4:8]
  cm &lt;- mclustBIC(crabs[,4:8],G=9,modelNames="EEE")
  scm &lt;- summary(cm,crabs[,4:8])
  cmnbhat &lt;- mergenormals(crabs[,4:8],scm,method="bhat")
  summary(cmnbhat)
  cmndemp &lt;- mergenormals(crabs[,4:8],scm,method="demp")
  summary(cmndemp)
# Other methods take a bit longer, but try them!
# The values of by and M below are still chosen for reasonably fast execution.
# cmnrr &lt;- mergenormals(crabs[,4:8],scm,method="ridge.ratio",by=0.05)
# cmd &lt;- mergenormals(crabs[,4:8],scm,method="dip.tantrum",by=0.05)
# cmp &lt;- mergenormals(crabs[,4:8],scm,method="predictive",M=3)
</code></pre>

<hr>
<h2 id='mergeparameters'>New parameters from merging two Gaussian mixture components</h2><span id='topic+mergeparameters'></span>

<h3>Description</h3>

<p>Re-computes pointwise posterior probabilities, mean and covariance
matrix for a mixture component obtained by merging two mixture
components in a Gaussian mixture.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  mergeparameters(xdata, j1, j2, probs, muarray,Sigmaarray, z)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mergeparameters_+3A_xdata">xdata</code></td>
<td>
<p>data (something that can be coerced into a matrix).</p>
</td></tr>
<tr><td><code id="mergeparameters_+3A_j1">j1</code></td>
<td>
<p>integer. Number of first mixture component to be merged.</p>
</td></tr>
<tr><td><code id="mergeparameters_+3A_j2">j2</code></td>
<td>
<p>integer. Number of second mixture component to be merged.</p>
</td></tr>
<tr><td><code id="mergeparameters_+3A_probs">probs</code></td>
<td>
<p>vector of component proportions (for all components;
should sum up to one).</p>
</td></tr>
<tr><td><code id="mergeparameters_+3A_muarray">muarray</code></td>
<td>
<p>matrix of component means (rows).</p>
</td></tr>
<tr><td><code id="mergeparameters_+3A_sigmaarray">Sigmaarray</code></td>
<td>
<p>array of component covariance matrices (third
dimension refers to component number).</p>
</td></tr>
<tr><td><code id="mergeparameters_+3A_z">z</code></td>
<td>
<p>matrix of observation- (row-)wise posterior probabilities of
belonging to the components (columns).</p>
</td></tr> 
</table>


<h3>Value</h3>

<p>List with components
</p>
<table>
<tr><td><code>probs</code></td>
<td>
<p>see above; sum of probabilities for original components
<code>j1</code> and <code>j2</code> is now <code>probs[j1]</code>. Note that generally,
also for the further components, values for the merged component are
in place <code>j1</code> and values in place <code>j2</code> are not changed. This
means that in order to have only the information for the new mixture
after merging, the entries in places <code>j2</code> need to be suppressed.</p>
</td></tr>  
<tr><td><code>muarray</code></td>
<td>
<p>see above; weighted mean of means of component
<code>j1</code> and <code>j2</code> is now in place <code>j1</code>.</p>
</td></tr>
<tr><td><code>Sigmaarray</code></td>
<td>
<p>see above; weighted covariance matrix handled as
above.</p>
</td></tr> 
<tr><td><code>z</code></td>
<td>
<p>see above; original entries for columns <code>j1</code> and
<code>j2</code> are summed up and now in column <code>j1</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2010) Methods for merging Gaussian mixture components,
<em>Advances in Data Analysis and Classification</em>, 4, 3-34.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  options(digits=3)
  set.seed(98765)
  require(mclust)
  iriss &lt;- iris[sample(150,20),-5]
  irisBIC &lt;- mclustBIC(iriss)
  siris &lt;- summary(irisBIC,iriss)
  probs &lt;- siris$parameters$pro
  muarray &lt;- siris$parameters$mean
  Sigmaarray &lt;- siris$parameters$variance$sigma
  z &lt;- siris$z
  mpi &lt;- mergeparameters(iriss,1,2,probs,muarray,Sigmaarray,z)
  mpi$probs
  mpi$muarray
</code></pre>

<hr>
<h2 id='minsize'>Minimum size of regression fixed point cluster</h2><span id='topic+minsize'></span>

<h3>Description</h3>

<p>Computes the minimum size of a fixed point cluster (FPC) which is
found at least <code>mtf</code> times with approximated
probability <code>prob</code> by
<code>ir</code> fixed point iterations of <code><a href="#topic+fixreg">fixreg</a></code>.
</p>
<p>Thought for use within <code><a href="#topic+fixreg">fixreg</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>minsize(n, p, ir, mtf, prob = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="minsize_+3A_n">n</code></td>
<td>
<p>positive integer. Total number of points.</p>
</td></tr>
<tr><td><code id="minsize_+3A_p">p</code></td>
<td>
<p>positive integer. Number of independent variables.</p>
</td></tr>
<tr><td><code id="minsize_+3A_ir">ir</code></td>
<td>
<p>positive integer. Number of fixed point iterations.</p>
</td></tr>
<tr><td><code id="minsize_+3A_mtf">mtf</code></td>
<td>
<p>positive integer.</p>
</td></tr>
<tr><td><code id="minsize_+3A_prob">prob</code></td>
<td>
<p>numerical between 0 and 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The computation is based on the binomial distribution with probability
given by <code><a href="#topic+clusexpect">clusexpect</a></code> with <code>ir=1</code>.
</p>


<h3>Value</h3>

<p>An integer.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>References</h3>

<p>Hennig, C. (2002) Fixed point clusters for linear regression:
computation and comparison, <em>Journal of
Classification</em> 19, 249-276.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fixreg">fixreg</a></code>, <code><a href="#topic+clusexpect">clusexpect</a></code>,
<code><a href="#topic+itnumber">itnumber</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  minsize(500,4,7000,2)
</code></pre>

<hr>
<h2 id='mixdens'>Density of multivariate Gaussian mixture, mclust parameterisation</h2><span id='topic+mixdens'></span>

<h3>Description</h3>

<p>Computes density values for data from a mixture of multivariate Gaussian
distributions with parameters based on the way models are specified
and parameters are stored in package mclust.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  mixdens(modelName,data,parameters)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mixdens_+3A_modelname">modelName</code></td>
<td>
<p>an mclust model name.
See <code><a href="mclust.html#topic+mclustModelNames">mclustModelNames</a></code>.</p>
</td></tr>
<tr><td><code id="mixdens_+3A_data">data</code></td>
<td>
<p>data matrix; density values are computed for every
observation (row).</p>
</td></tr>
<tr><td><code id="mixdens_+3A_parameters">parameters</code></td>
<td>
<p>parameters of Gaussian mixture in the format used in
the output of <code><a href="mclust.html#topic+summary.mclustBIC">summary.mclustBIC</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of density values for the observations.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(98765)
  require(mclust)
  iriss &lt;- iris[sample(150,20),-5]
  irisBIC &lt;- mclustBIC(iriss)
  siris &lt;- summary(irisBIC,iriss)
  round(mixdens(siris$modelName,iriss,siris$parameters),digits=2)
</code></pre>

<hr>
<h2 id='mixpredictive'>Prediction strength of merged Gaussian mixture</h2><span id='topic+mixpredictive'></span>

<h3>Description</h3>

<p>Computes the prediction strength of clustering by
merging Gaussian mixture components, see <code><a href="#topic+mergenormals">mergenormals</a></code>.
The predictive strength is
defined according to Tibshirani and Walther (2005), carried out as
described in Hennig (2010), see details. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  mixpredictive(xdata, Gcomp, Gmix, M=50, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mixpredictive_+3A_xdata">xdata</code></td>
<td>
<p>data (something that can be coerced into a matrix).</p>
</td></tr>
<tr><td><code id="mixpredictive_+3A_gcomp">Gcomp</code></td>
<td>
<p>integer. Number of components of the underlying Gaussian mixture.</p>
</td></tr>
<tr><td><code id="mixpredictive_+3A_gmix">Gmix</code></td>
<td>
<p>integer. Number of clusters after merging Gaussian components.</p>
</td></tr>
<tr><td><code id="mixpredictive_+3A_m">M</code></td>
<td>
<p>integer. Number of times the dataset is divided into two
halves.</p>
</td></tr> 
<tr><td><code id="mixpredictive_+3A_...">...</code></td>
<td>
<p>further arguments that can potentially arrive in calls but
are currently not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The prediction strength for a certain number of clusters <code>Gmix</code> under a
random partition of the dataset in halves A and B is defined as
follows. Both halves are clustered with <code>Gmix</code>
clusters. Then the points of
A are classified to the clusters of B. This is done by use of the
maximum a posteriori rule for mixtures as in Hennig (2010),
differently from Tibshirani and Walther (2005). A pair of points A in
the same A-cluster is defined to be correctly predicted if both points
are classified into the same cluster on B. The same is done with the
points of B relative to the clustering on A. The prediction strength
for each of the clusterings is the minimum (taken over all clusters)
relative frequency of correctly predicted pairs of points of that
cluster. The final mean prediction strength statistic is the mean over
all 2M clusterings.
</p>


<h3>Value</h3>

<p>List with components
</p>
<table>
<tr><td><code>predcorr</code></td>
<td>
<p>vector of length <code>M</code> with relative frequencies of
correct predictions (clusterwise minimum).</p>
</td></tr>
<tr><td><code>mean.pred</code></td>
<td>
<p>mean of <code>predcorr</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2010) Methods for merging Gaussian mixture components,
<em>Advances in Data Analysis and Classification</em>, 4, 3-34.
</p>
<p>Tibshirani, R. and Walther, G. (2005) Cluster Validation by 
Prediction Strength, <em>Journal of Computational and Graphical 
Statistics</em>, 14, 511-528.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prediction.strength">prediction.strength</a></code> for Tibshirani and Walther's
original method.
<code><a href="#topic+mergenormals">mergenormals</a></code> for the clustering method applied here.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(98765)
  iriss &lt;- iris[sample(150,20),-5]
  mp &lt;- mixpredictive(iriss,2,2,M=2)
</code></pre>

<hr>
<h2 id='mvdcoord'>Mean/variance differences discriminant coordinates</h2><span id='topic+mvdcoord'></span>

<h3>Description</h3>

<p>Discriminant projections as defined in Young, Marco and Odell (1987).
The principle is to maximize the projection of a matrix consisting of
the differences between the means of all classes and the first mean
and the differences between the covariance matrices of all classes and
the forst covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mvdcoord(xd, clvecd, clnum=1, sphere="mcd", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mvdcoord_+3A_xd">xd</code></td>
<td>
<p>the data matrix; a numerical object which can be coerced
to a matrix.</p>
</td></tr>
<tr><td><code id="mvdcoord_+3A_clvecd">clvecd</code></td>
<td>
<p>integer vector of class numbers; length must equal
<code>nrow(xd)</code>.</p>
</td></tr>
<tr><td><code id="mvdcoord_+3A_clnum">clnum</code></td>
<td>
<p>integer. Number of the class to which all differences are
computed.</p>
</td></tr>
<tr><td><code id="mvdcoord_+3A_sphere">sphere</code></td>
<td>
<p>a covariance matrix or one of
&quot;mve&quot;, &quot;mcd&quot;, &quot;classical&quot;, &quot;none&quot;. The matrix used for sphering the
data. &quot;mcd&quot; and &quot;mve&quot; are robust covariance matrices as implemented
in <code><a href="MASS.html#topic+cov.rob">cov.rob</a></code>. &quot;classical&quot; refers to the classical
covariance matrix. &quot;none&quot; means no sphering and use of the raw
data.</p>
</td></tr>
<tr><td><code id="mvdcoord_+3A_...">...</code></td>
<td>
<p>no effect</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with the following components
</p>
<table>
<tr><td><code>ev</code></td>
<td>
<p>eigenvalues in descending order.</p>
</td></tr>
<tr><td><code>units</code></td>
<td>
<p>columns are coordinates of projection basis vectors.
New points <code>x</code> can be projected onto the projection basis vectors
by <code>x %*% units</code></p>
</td></tr>
<tr><td><code>proj</code></td>
<td>
<p>projections of <code>xd</code> onto <code>units</code>.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Young, D. M., Marco, V. R. and Odell, P. L. (1987). Quadratic
discrimination: some results on optimal low-dimensional
representation, <em>Journal of Statistical Planning and Inference</em>,
17, 307-319.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plotcluster">plotcluster</a></code> for straight forward discriminant plots.
<code><a href="#topic+discrproj">discrproj</a></code> for alternatives.
<code><a href="#topic+rFace">rFace</a></code> for generation of the example data used below.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(4634)
  face &lt;- rFace(300,dMoNo=2,dNoEy=0,p=3)
  grface &lt;- as.integer(attr(face,"grouping"))
  mcf &lt;- mvdcoord(face,grface)
  plot(mcf$proj,col=grface)
  # ...done in one step by function plotcluster.
</code></pre>

<hr>
<h2 id='ncoord'>Neighborhood based discriminant coordinates</h2><span id='topic+ncoord'></span>

<h3>Description</h3>

<p>Neighborhood based discriminant coordinates as defined in Hastie and
Tibshirani (1996) and a robustified version as defined in Hennig (2003).
The principle is to maximize the projection of a between
classes covariance matrix, which is defined by averaging the
between classes covariance matrices in the neighborhoods of all points.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ncoord(xd, clvecd, nn=50, weighted=FALSE,
                    sphere="mcd", orderall=TRUE, countmode=1000, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ncoord_+3A_xd">xd</code></td>
<td>
<p>the data matrix; a numerical object which can be coerced
to a matrix.</p>
</td></tr>
<tr><td><code id="ncoord_+3A_clvecd">clvecd</code></td>
<td>
<p>integer vector of class numbers; length must equal
<code>nrow(xd)</code>.</p>
</td></tr>
<tr><td><code id="ncoord_+3A_nn">nn</code></td>
<td>
<p>integer. Number of points which belong to the neighborhood
of each point (including the point itself).</p>
</td></tr>
<tr><td><code id="ncoord_+3A_weighted">weighted</code></td>
<td>
<p>logical. <code>FALSE</code> corresponds to the original
method of Hastie and Tibshirani (1996). If <code>TRUE</code>,
the between classes
covariance matrices B are weighted by w/trace B, where w is some
weight depending on the sizes of the
classes in the neighborhood. Division by trace B reduces the effect
of outliers. <code>TRUE</code> cooresponds to WNC as defined in Hennig
(2003).</p>
</td></tr>
<tr><td><code id="ncoord_+3A_sphere">sphere</code></td>
<td>
<p>a covariance matrix or one of
&quot;mve&quot;, &quot;mcd&quot;, &quot;classical&quot;, &quot;none&quot;. The matrix used for sphering the
data. &quot;mcd&quot; and &quot;mve&quot; are robust covariance matrices as implemented
in <code><a href="MASS.html#topic+cov.rob">cov.rob</a></code>. &quot;classical&quot; refers to the classical
covariance matrix. &quot;none&quot; means no sphering and use of the raw
data.</p>
</td></tr>
<tr><td><code id="ncoord_+3A_orderall">orderall</code></td>
<td>
<p>logical. By default, the neighborhoods are computed by
ordering all points each time. If <code>FALSE</code>, the neighborhoods
are computed by selecting <code>nn</code> times the nearest point from the
remaining points, which may be faster sometimes.</p>
</td></tr>
<tr><td><code id="ncoord_+3A_countmode">countmode</code></td>
<td>
<p>optional positive integer. Every <code>countmode</code>
algorithm runs <code>ncoord</code> shows a message.</p>
</td></tr>
<tr><td><code id="ncoord_+3A_...">...</code></td>
<td>
<p>no effect</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with the following components
</p>
<table>
<tr><td><code>ev</code></td>
<td>
<p>eigenvalues in descending order.</p>
</td></tr>
<tr><td><code>units</code></td>
<td>
<p>columns are coordinates of projection basis vectors.
New points <code>x</code> can be projected onto the projection basis vectors
by <code>x %*% units</code></p>
</td></tr>
<tr><td><code>proj</code></td>
<td>
<p>projections of <code>xd</code> onto <code>units</code>.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hastie, T. and Tibshirani, R.  (1996). Discriminant adaptive nearest
neighbor classification. <em>IEEE Transactions on Pattern Analysis
and Machine Intelligence</em> 18, 607-616. 
</p>
<p>Hennig, C. (2004) Asymmetric linear dimension reduction for classification.
Journal of Computational and Graphical Statistics 13, 930-945 .
</p>
<p>Hennig, C. (2005)  A method for visual cluster validation.  In:
Weihs, C. and Gaul, W. (eds.): Classification - The Ubiquitous
Challenge. Springer, Heidelberg 2005, 153-160.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plotcluster">plotcluster</a></code> for straight forward discriminant plots.
<code><a href="#topic+discrproj">discrproj</a></code> for alternatives.
<code><a href="#topic+rFace">rFace</a></code> for generation of the example data used below.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(4634)
  face &lt;- rFace(600,dMoNo=2,dNoEy=0)
  grface &lt;- as.integer(attr(face,"grouping"))
  ncf &lt;- ncoord(face,grface)
  plot(ncf$proj,col=grface)
  ncf2 &lt;- ncoord(face,grface,weighted=TRUE)
  plot(ncf2$proj,col=grface)
  # ...done in one step by function plotcluster.
</code></pre>

<hr>
<h2 id='neginc'>Neg-entropy normality index for cluster validation</h2><span id='topic+neginc'></span>

<h3>Description</h3>

<p>Cluster validity index based on the neg-entropy distances of
within-cluster distributions to normal distribution, see
Lago-Fernandez and Corbacho (2010). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neginc(x,clustering)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neginc_+3A_x">x</code></td>
<td>
<p>something that can be coerced into a numerical
matrix. Euclidean dataset.</p>
</td></tr>
<tr><td><code id="neginc_+3A_clustering">clustering</code></td>
<td>
<p>vector of integers with length <code>=nrow(x)</code>;
indicating the cluster for each observation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Index value, see
Lago-Fernandez and Corbacho (2010). The lower (i.e., the more
negative) the better. 
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Lago-Fernandez, L. F. and Corbacho, F. (2010) Normality-based
validation for crisp clustering. <em>Pattern Recognition</em> 43, 782-795.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  options(digits=3)
  iriss &lt;- as.matrix(iris[c(1:10,51:55,101:105),-5])
  irisc &lt;- as.numeric(iris[c(1:10,51:55,101:105),5])
  neginc(iriss,irisc)
</code></pre>

<hr>
<h2 id='nselectboot'>Selection of the number of clusters via bootstrap</h2><span id='topic+nselectboot'></span>

<h3>Description</h3>

<p>Selection of the number of clusters via bootstrap as explained in Fang
and Wang (2012). Several times 2 bootstrap samples are drawn from the
data and the number of clusters is chosen by optimising an instability
estimation from these pairs.
</p>
<p>In principle all clustering methods can be used that have a
CBI-wrapper, see <code><a href="#topic+clusterboot">clusterboot</a></code>,
<code><a href="#topic+kmeansCBI">kmeansCBI</a></code>. However, the currently implemented
classification methods are not necessarily suitable for all of them,
see argument <code>classification</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nselectboot(data,B=50,distances=inherits(data,"dist"),
                        clustermethod=NULL,
                        classification="averagedist",centroidname = NULL,
                        krange=2:10, count=FALSE,nnk=1,
                        largeisgood=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nselectboot_+3A_data">data</code></td>
<td>
<p>something that can be coerced into a matrix. The data
matrix - either an <code>n*p</code>-data matrix (or data frame) or an
<code>n*n</code>-dissimilarity matrix (or <code>dist</code>-object).</p>
</td></tr>
<tr><td><code id="nselectboot_+3A_b">B</code></td>
<td>
<p>integer. Number of resampling runs.</p>
</td></tr>
<tr><td><code id="nselectboot_+3A_distances">distances</code></td>
<td>
<p>logical. If <code>TRUE</code>, the data is interpreted as
dissimilarity matrix. If <code>data</code> is a <code>dist</code>-object,
<code>distances=TRUE</code> automatically, otherwise
<code>distances=FALSE</code> by default. This means that you have to set
it to <code>TRUE</code> manually if <code>data</code> is a dissimilarity matrix.</p>
</td></tr>
<tr><td><code id="nselectboot_+3A_clustermethod">clustermethod</code></td>
<td>
<p>an interface function (the function name, not a
string containing the name, has to be provided!). This defines the
clustering method. See the &quot;Details&quot;-section of <code><a href="#topic+clusterboot">clusterboot</a></code>
and <code><a href="#topic+kmeansCBI">kmeansCBI</a></code> for the format. Clustering methods for
<code>nselectboot</code> must have a <code>k</code>-argument for the number of
clusters and must otherwise follow the specifications in
<code><a href="#topic+clusterboot">clusterboot</a></code>. Note that <code>nselectboot</code> won't work
with CBI-functions that implicitly already estimate the number of
clusters such as <code><a href="#topic+pamkCBI">pamkCBI</a></code>; use <code><a href="#topic+claraCBI">claraCBI</a></code>
if you want to run it for pam/clara clustering. 
</p>
</td></tr>
<tr><td><code id="nselectboot_+3A_classification">classification</code></td>
<td>
<p>string.
This determines how non-clustered points are classified to given
clusters. Options are explained in <code><a href="#topic+classifdist">classifdist</a></code> (if
<code>distances=TRUE</code>) and <code><a href="#topic+classifnp">classifnp</a></code> (otherwise).
Certain classification methods are connected to certain clustering
methods. <code>classification="averagedist"</code> is recommended for
average linkage, <code>classification="centroid"</code> is recommended for
k-means, clara and pam (with distances it will work with
<code><a href="#topic+claraCBI">claraCBI</a></code> only), <code>classification="knn"</code> with
<code>nnk=1</code> is recommended for single linkage and
<code>classification="qda"</code> is recommended for Gaussian mixtures
with flexible covariance matrices.  
</p>
</td></tr>
<tr><td><code id="nselectboot_+3A_centroidname">centroidname</code></td>
<td>
<p>string. Indicates the name of the component of
<code>CBIoutput$result</code> that contains the cluster centroids in case of
<code>classification="centroid"</code>, where <code>CBIoutput</code> is the
output object of <code>clustermethod</code>. If <code>clustermethod</code> is
<code>kmeansCBI</code> or <code>claraCBI</code>, centroids are recognised
automatically if <code>centroidname=NULL</code>. If
<code>centroidname=NULL</code> and <code>distances=FALSE</code>, cluster means
are computed as the cluster centroids.</p>
</td></tr>  
<tr><td><code id="nselectboot_+3A_krange">krange</code></td>
<td>
<p>integer vector; numbers of clusters to be tried.</p>
</td></tr>
<tr><td><code id="nselectboot_+3A_count">count</code></td>
<td>
<p>logical. If <code>TRUE</code>, numbers of clusters and
bootstrap runs are printed.</p>
</td></tr>
<tr><td><code id="nselectboot_+3A_nnk">nnk</code></td>
<td>
<p>number of nearest neighbours if
<code>classification="knn"</code>, see  <code><a href="#topic+classifdist">classifdist</a></code> (if
<code>distances=TRUE</code>) and <code><a href="#topic+classifnp">classifnp</a></code> (otherwise).</p>
</td></tr>
<tr><td><code id="nselectboot_+3A_largeisgood">largeisgood</code></td>
<td>
<p>logical. If <code>TRUE</code>, output component
<code>stabk</code> is taken as one minus the original instability value
so that larger values of <code>stabk</code> are better.</p>
</td></tr>
<tr><td><code id="nselectboot_+3A_...">...</code></td>
<td>
<p>arguments to be passed on to the clustering method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>nselectboot</code> returns a list with components
<code>kopt,stabk,stab</code>.
</p>
<table>
<tr><td><code>kopt</code></td>
<td>
<p>optimal number of clusters.</p>
</td></tr>
<tr><td><code>stabk</code></td>
<td>
<p>mean instability values for numbers of clusters (or one
minus this if <code>largeisgood=TRUE</code>).</p>
</td></tr>
<tr><td><code>stab</code></td>
<td>
<p>matrix of instability values for all bootstrap runs and
numbers of clusters.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Fang, Y. and Wang, J. (2012) Selection of the number of clusters via
the bootstrap method. <em>Computational Statistics and Data
Analysis</em>, 56, 468-477.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+classifdist">classifdist</a></code>, <code><a href="#topic+classifnp">classifnp</a></code>,
<code><a href="#topic+clusterboot">clusterboot</a></code>,<code><a href="#topic+kmeansCBI">kmeansCBI</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
  set.seed(20000)
  face &lt;- rFace(50,dMoNo=2,dNoEy=0,p=2)
  nselectboot(dist(face),B=2,clustermethod=disthclustCBI,
   method="average",krange=5:7)
  nselectboot(dist(face),B=2,clustermethod=claraCBI,
   classification="centroid",krange=5:7)
  nselectboot(face,B=2,clustermethod=kmeansCBI,
   classification="centroid",krange=5:7)
# Of course use larger B in a real application.
</code></pre>

<hr>
<h2 id='pamk'>Partitioning around medoids with estimation of number of clusters</h2><span id='topic+pamk'></span>

<h3>Description</h3>

<p>This calls the function <code><a href="cluster.html#topic+pam">pam</a></code> or
<code><a href="cluster.html#topic+clara">clara</a></code> to perform a
partitioning around medoids clustering with the number of clusters
estimated by optimum average silhouette width (see
<code><a href="cluster.html#topic+pam.object">pam.object</a></code>) or Calinski-Harabasz
index (<code><a href="#topic+calinhara">calinhara</a></code>). The Duda-Hart test
(<code><a href="#topic+dudahart2">dudahart2</a></code>) is applied to decide whether there should be
more than one cluster (unless 1 is excluded as number of clusters or
data are dissimilarities).  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pamk(data,krange=2:10,criterion="asw", usepam=TRUE,
     scaling=FALSE, alpha=0.001, diss=inherits(data, "dist"),
     critout=FALSE, ns=10, seed=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pamk_+3A_data">data</code></td>
<td>
<p>a data matrix or data frame or something that can be
coerced into a matrix, or dissimilarity matrix or
object. See <code><a href="cluster.html#topic+pam">pam</a></code> for more information.</p>
</td></tr>
<tr><td><code id="pamk_+3A_krange">krange</code></td>
<td>
<p>integer vector. Numbers of clusters which are to be
compared by the average silhouette width criterion. Note: average
silhouette width and Calinski-Harabasz can't estimate number of
clusters <code>nc=1</code>. If 1 is included, a Duda-Hart test is applied
and 1 is estimated if this is not significant.</p>
</td></tr>
<tr><td><code id="pamk_+3A_criterion">criterion</code></td>
<td>
<p>one of <code>"asw"</code>, <code>"multiasw"</code> or
<code>"ch"</code>. Determines whether average silhouette width (as given
out by  <code><a href="cluster.html#topic+pam">pam</a></code>/<code><a href="cluster.html#topic+clara">clara</a></code>, or
as computed by <code><a href="#topic+distcritmulti">distcritmulti</a></code> if <code>"multiasw"</code> is
specified; recommended for large data sets with <code>usepam=FALSE</code>)
or Calinski-Harabasz is applied. Note that the original
Calinski-Harabasz index is not defined for dissimilarities; if
dissimilarity data is run with <code>criterion="ch"</code>, the
dissimilarity-based generalisation in Hennig and Liao (2013) is
used.</p>
</td></tr>
<tr><td><code id="pamk_+3A_usepam">usepam</code></td>
<td>
<p>logical. If <code>TRUE</code>, <code><a href="cluster.html#topic+pam">pam</a></code> is
used, otherwise <code><a href="cluster.html#topic+clara">clara</a></code> (recommended for large
datasets with 2,000 or more observations; dissimilarity matrices can
not be used with <code><a href="cluster.html#topic+clara">clara</a></code>).</p>
</td></tr>
<tr><td><code id="pamk_+3A_scaling">scaling</code></td>
<td>
<p>either a logical value or a numeric vector of length
equal to the number of variables. If <code>scaling</code> is a numeric
vector with length equal to the number of variables, then each
variable is divided by the corresponding value from <code>scaling</code>.
If <code>scaling</code> is <code>TRUE</code> then scaling is done by dividing
the (centered) variables by their root-mean-square, and if
<code>scaling</code> is <code>FALSE</code>, no scaling is done.</p>
</td></tr>
<tr><td><code id="pamk_+3A_alpha">alpha</code></td>
<td>
<p>numeric between 0 and 1, tuning constant for
<code><a href="#topic+dudahart2">dudahart2</a></code> (only used for 1-cluster test).</p>
</td></tr> 
<tr><td><code id="pamk_+3A_diss">diss</code></td>
<td>
<p>logical flag: if <code>TRUE</code> (default for <code>dist</code> or
<code>dissimilarity</code>-objects), then <code>data</code> will be considered
as a dissimilarity matrix (and the potential number of clusters 1
will be ignored).  If <code>FALSE</code>, then <code>data</code> will
be considered as a matrix of observations by variables.</p>
</td></tr>
<tr><td><code id="pamk_+3A_critout">critout</code></td>
<td>
<p>logical. If <code>TRUE</code>, the criterion value is printed
out for every number of clusters.</p>
</td></tr>
<tr><td><code id="pamk_+3A_ns">ns</code></td>
<td>
<p>passed on to <code><a href="#topic+distcritmulti">distcritmulti</a></code> if
<code>criterion="multiasw"</code>.</p>
</td></tr>
<tr><td><code id="pamk_+3A_seed">seed</code></td>
<td>
<p>passed on to <code><a href="#topic+distcritmulti">distcritmulti</a></code> if
<code>criterion="multiasw"</code>.</p>
</td></tr>
<tr><td><code id="pamk_+3A_...">...</code></td>
<td>
<p>further arguments to be transferred to
<code><a href="cluster.html#topic+pam">pam</a></code> or <code><a href="cluster.html#topic+clara">clara</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>pamobject</code></td>
<td>
<p>The output of the optimal run of the
<code><a href="cluster.html#topic+pam">pam</a></code>-function.</p>
</td></tr>
<tr><td><code>nc</code></td>
<td>
<p>the optimal number of clusters.</p>
</td></tr>
<tr><td><code>crit</code></td>
<td>
<p>vector of criterion values for numbers of
clusters. <code>crit[1]</code> is the p-value of the Duda-Hart test
if 1 is in <code>krange</code> and <code>diss=FALSE</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p><code><a href="cluster.html#topic+clara">clara</a></code> and <code><a href="cluster.html#topic+pam">pam</a></code>
can handle <code>NA</code>-entries (see their documentation) but
<code><a href="#topic+dudahart2">dudahart2</a></code> cannot. Therefore <code>NA</code> should not occur
if 1 is in <code>krange</code>.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Calinski, R. B., and Harabasz, J. (1974) A Dendrite Method for Cluster 
Analysis, <em>Communications in Statistics</em>, 3, 1-27.
</p>
<p>Duda, R. O. and Hart, P. E. (1973) <em>Pattern Classification and
Scene Analysis</em>. Wiley, New York.
</p>
<p>Hennig, C. and Liao, T. (2013) How to find an appropriate clustering
for mixed-type variables with application to socio-economic
stratification, <em>Journal of the Royal Statistical Society, Series
C Applied Statistics</em>, 62, 309-369.
</p>
<p>Kaufman, L. and Rousseeuw, P.J. (1990). &quot;Finding Groups in Data:
An Introduction to Cluster Analysis&quot;. Wiley, New York.
</p>


<h3>See Also</h3>

<p><code><a href="cluster.html#topic+pam">pam</a></code>, <code><a href="cluster.html#topic+clara">clara</a></code>
<code><a href="#topic+distcritmulti">distcritmulti</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  options(digits=3)
  set.seed(20000)
  face &lt;- rFace(50,dMoNo=2,dNoEy=0,p=2)
  pk1 &lt;- pamk(face,krange=1:5,criterion="asw",critout=TRUE)
  pk2 &lt;- pamk(face,krange=1:5,criterion="multiasw",ns=2,critout=TRUE)
# "multiasw" is better for larger data sets, use larger ns then.
  pk3 &lt;- pamk(face,krange=1:5,criterion="ch",critout=TRUE)
</code></pre>

<hr>
<h2 id='piridge'>Ridgeline Pi-function</h2><span id='topic+piridge'></span>

<h3>Description</h3>

<p>The Pi-function is given in (6) in Ray and Lindsay, 2005. Equating it
to the mixture proportion yields locations of two-component Gaussian
mixture density extrema. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>piridge(alpha, mu1, mu2, Sigma1, Sigma2, showplot=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="piridge_+3A_alpha">alpha</code></td>
<td>
<p>sequence of values between 0 and 1 for which the Pi-function
is computed.</p>
</td></tr>
<tr><td><code id="piridge_+3A_mu1">mu1</code></td>
<td>
<p>mean vector of component 1.</p>
</td></tr>
<tr><td><code id="piridge_+3A_mu2">mu2</code></td>
<td>
<p>mean vector of component 2.</p>
</td></tr>
<tr><td><code id="piridge_+3A_sigma1">Sigma1</code></td>
<td>
<p>covariance matrix of component 1.</p>
</td></tr>
<tr><td><code id="piridge_+3A_sigma2">Sigma2</code></td>
<td>
<p>covariance matrix of component 2.</p>
</td></tr>
<tr><td><code id="piridge_+3A_showplot">showplot</code></td>
<td>
<p>logical. If <code>TRUE</code>, the Pi-function is plotted
against <code>alpha</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of values of the Pi-function for values of <code>alpha</code>.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Ray, S. and Lindsay, B. G. (2005) The Topography of Multivariate 
Normal Mixtures, <em>Annals of Statistics</em>, 33, 2042-2065.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  q &lt;- piridge(seq(0,1,0.1),c(1,1),c(2,5),diag(2),diag(2))
</code></pre>

<hr>
<h2 id='piridge.zeroes'>Extrema of two-component Gaussian mixture</h2><span id='topic+piridge.zeroes'></span>

<h3>Description</h3>

<p>By use of the Pi-function in Ray and Lindsay, 2005, locations of 
two-component Gaussian mixture density extrema or saddlepoints are computed. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>piridge.zeroes(prop, mu1, mu2, Sigma1, Sigma2, alphamin=0,
                          alphamax=1,by=0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="piridge.zeroes_+3A_prop">prop</code></td>
<td>
<p>proportion of mixture component 1.</p>
</td></tr>
<tr><td><code id="piridge.zeroes_+3A_mu1">mu1</code></td>
<td>
<p>mean vector of component 1.</p>
</td></tr>
<tr><td><code id="piridge.zeroes_+3A_mu2">mu2</code></td>
<td>
<p>mean vector of component 2.</p>
</td></tr>
<tr><td><code id="piridge.zeroes_+3A_sigma1">Sigma1</code></td>
<td>
<p>covariance matrix of component 1.</p>
</td></tr>
<tr><td><code id="piridge.zeroes_+3A_sigma2">Sigma2</code></td>
<td>
<p>covariance matrix of component 2.</p>
</td></tr>
<tr><td><code id="piridge.zeroes_+3A_alphamin">alphamin</code></td>
<td>
<p>minimum alpha value.</p>
</td></tr>
<tr><td><code id="piridge.zeroes_+3A_alphamax">alphamax</code></td>
<td>
<p>maximum alpha value.</p>
</td></tr>
<tr><td><code id="piridge.zeroes_+3A_by">by</code></td>
<td>
<p>interval between alpha-values where to look for extrema.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with components
</p>
<table>
<tr><td><code>number.zeroes</code></td>
<td>
<p>number of zeroes of Pi-function, i.e.,
extrema or saddlepoints of density.</p>
</td></tr>
<tr><td><code>estimated.roots</code></td>
<td>
<p>estimated <code>alpha</code>-values at which extrema
or saddlepoints occur.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Ray, S. and Lindsay, B. G. (2005) The Topography of Multivariate 
Normal Mixtures, <em>Annals of Statistics</em>, 33, 2042-2065.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  q &lt;- piridge.zeroes(0.2,c(1,1),c(2,5),diag(2),diag(2),by=0.1)
</code></pre>

<hr>
<h2 id='plot.valstat'>Simulation-standardised plot and print of cluster validation statistics</h2><span id='topic+plot.valstat'></span><span id='topic+print.valstat'></span>

<h3>Description</h3>

<p>Visualisation and print function for cluster validation output
compared to results
on simulated random clusterings. The print method can also be used to
compute and print an aggregated cluster validation index.
</p>
<p>Unlike for many other plot methods, the additional arguments
of <code>plot.valstat</code> are essential. <code>print.valstat</code> should make
good sense with the defaults, but for computing the aggregate index
need to be set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'valstat'
plot(x,simobject=NULL,statistic="sindex",
                            xlim=NULL,ylim=c(0,1),
                            nmethods=length(x)-5,
                            col=1:nmethods,cex=1,pch=c("c","f","a","n"),
                            simcol=rep(grey(0.7),4),
                         shift=c(-0.1,-1/3,1/3,0.1),include.othernc=NULL,...)


## S3 method for class 'valstat'
print(x,statistics=x$statistics,
                          nmethods=length(x)-5,aggregate=FALSE,
                          weights=NULL,digits=2,
                          include.othernc=NULL,...)

			      
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.valstat_+3A_x">x</code></td>
<td>
<p>object of class <code>"valstat"</code>, such as sublists
<code>stat, qstat, sstat</code> of <code><a href="#topic+clusterbenchstats">clusterbenchstats</a></code>-output.</p>
</td></tr>
<tr><td><code id="plot.valstat_+3A_simobject">simobject</code></td>
<td>
<p>list of simulation results as produced by
<code><a href="#topic+randomclustersim">randomclustersim</a></code> and documented there; typically sublist
<code>sim</code> of <code><a href="#topic+clusterbenchstats">clusterbenchstats</a></code>-output.</p>
</td></tr>  
<tr><td><code id="plot.valstat_+3A_statistic">statistic</code></td>
<td>
<p>one of <code>"avewithin","mnnd","variation",
      "diameter","gap","sindex","minsep","asw","dindex","denscut",
      "highdgap","pg","withinss","entropy","pamc","kdnorm","kdunif","dmode"</code>;
validation statistic to be plotted.</p>
</td></tr>
<tr><td><code id="plot.valstat_+3A_xlim">xlim</code></td>
<td>
<p>passed on to <code>plot</code>. Default is the range of all
involved numbers of clusters, minimum minus 0.5 to maximum plus
0.5.</p>
</td></tr>
<tr><td><code id="plot.valstat_+3A_ylim">ylim</code></td>
<td>
<p>passed on to <code>plot</code>.</p>
</td></tr>    
<tr><td><code id="plot.valstat_+3A_nmethods">nmethods</code></td>
<td>
<p>integer. Number of clustering methods to involve
(these are those from number 1 to <code>nmethods</code> specified in
<code>x$name</code>).</p>
</td></tr>
<tr><td><code id="plot.valstat_+3A_col">col</code></td>
<td>
<p>colours used for the different clustering methods.</p>
</td></tr>
<tr><td><code id="plot.valstat_+3A_cex">cex</code></td>
<td>
<p>passed on to <code>plot</code>.</p>
</td></tr>
<tr><td><code id="plot.valstat_+3A_pch">pch</code></td>
<td>
<p>vector of symbols for random clustering results from
<code><a href="#topic+stupidkcentroids">stupidkcentroids</a></code>, <code><a href="#topic+stupidkfn">stupidkfn</a></code>,
<code><a href="#topic+stupidkaven">stupidkaven</a></code>, <code><a href="#topic+stupidknn">stupidknn</a></code>.
To be passed on to <code>plot</code>.</p>
</td></tr>
<tr><td><code id="plot.valstat_+3A_simcol">simcol</code></td>
<td>
<p>vector of colours used for random clustering results in
order <code><a href="#topic+stupidkcentroids">stupidkcentroids</a></code>, <code><a href="#topic+stupidkfn">stupidkfn</a></code>,
<code><a href="#topic+stupidkaven">stupidkaven</a></code>, <code><a href="#topic+stupidknn">stupidknn</a></code>.</p>
</td></tr>
<tr><td><code id="plot.valstat_+3A_shift">shift</code></td>
<td>
<p>numeric vector. Indicates the amount to which the results
from  <code><a href="#topic+stupidkcentroids">stupidkcentroids</a></code>, <code><a href="#topic+stupidkfn">stupidkfn</a></code>,
<code><a href="#topic+stupidkaven">stupidkaven</a></code>, <code><a href="#topic+stupidknn">stupidknn</a></code> are
plotted to the right of their respective
number of clusters (negative numbers plot to the left).</p>
</td></tr>
<tr><td><code id="plot.valstat_+3A_include.othernc">include.othernc</code></td>
<td>
<p>this indicates whether methods should be
included that estimated their number of clusters themselves and gave
a result outside the standard range as given by <code>x$minG</code>
and <code>x$maxG</code>. If not <code>NULL</code>, this is a
list of integer vectors of length 2. The first
number is
the number of the clustering method (the order is determined by
argument <code>x$name</code>), the second number is the
number of clusters for those methods that estimate the number of
clusters themselves and estimated a number outside the standard
range. Normally what will be used here, if not <code>NULL</code>, is the
output parameter
<code>cm$othernc</code> of <code><a href="#topic+clusterbenchstats">clusterbenchstats</a></code>, see also
<code><a href="#topic+cluster.magazine">cluster.magazine</a></code>.</p>
</td></tr>
<tr><td><code id="plot.valstat_+3A_statistics">statistics</code></td>
<td>
<p>vector of character strings specifying the
validation statistics that will be included in the output (unless
you want to restrict the output for some reason, the default should
be fine.</p>
</td></tr>
<tr><td><code id="plot.valstat_+3A_aggregate">aggregate</code></td>
<td>
<p>logical. If <code>TRUE</code>, an aggegate validation
statistic will be computed as the weighted mean of the involved
statistic. This requires <code>weights</code> to be set. In order for this
to make sense, values of the validation statistics should be
comparable, which is achieved by standardisation in
<code><a href="#topic+clusterbenchstats">clusterbenchstats</a></code>. Accordingly, <code>x</code> should
be the <code>qstat</code> or <code>sstat</code>-component of the
<code><a href="#topic+clusterbenchstats">clusterbenchstats</a></code>-output rather than the
<code>stat</code>-component.</p>
</td></tr>
<tr><td><code id="plot.valstat_+3A_weights">weights</code></td>
<td>
<p>vector of numericals. Weights for computation of the
aggregate statistic in case that <code>aggregate=TRUE</code>. The order of
clustering methods corresponding to the weight vector is given by
<code>x$name</code>.</p>
</td></tr>
<tr><td><code id="plot.valstat_+3A_digits">digits</code></td>
<td>
<p>minimal number of significant digits, passed on to
<code><a href="base.html#topic+print.table">print.table</a></code>.</p>
</td></tr>
<tr><td><code id="plot.valstat_+3A_...">...</code></td>
<td>
<p>no effect.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Whereas <code>print.valstat</code>, at least with <code>aggregate=TRUE</code>
makes more sense for the <code>qstat</code> or <code>sstat</code>-component of the
<code><a href="#topic+clusterbenchstats">clusterbenchstats</a></code>-output rather than the
<code>stat</code>-component, <code>plot.valstat</code> should be run with the
<code>stat</code>-component if <code>simobject</code> is specified, because the
simulated cluster validity statistics are unstandardised and need to
be compared with unstandardised values on the dataset of interest.
</p>
<p><code>print.valstat</code> will print all values for all validation indexes
and the aggregated index (in case of <code>aggregate=TRUE</code> and set
<code>weights</code> will be printed last.
</p>


<h3>Value</h3>

<p><code>print.valstats</code> returns the results table as invisible object.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2019) Cluster validation by measurement of clustering
characteristics relevant to the user. In C. H. Skiadas (ed.)
<em>Data Analysis and Applications 1: Clustering and Regression,
Modeling-estimating, Forecasting and Data Mining, Volume 2</em>, Wiley,
New York 1-24,
<a href="https://arxiv.org/abs/1703.09282">https://arxiv.org/abs/1703.09282</a>
</p>
<p>Akhanli, S. and Hennig, C. (2020) Calibrating and aggregating cluster
validity indexes for context-adapted comparison of clusterings.
<em>Statistics and Computing</em>, 30, 1523-1544,
<a href="https://link.springer.com/article/10.1007/s11222-020-09958-2">https://link.springer.com/article/10.1007/s11222-020-09958-2</a>, <a href="https://arxiv.org/abs/2002.01822">https://arxiv.org/abs/2002.01822</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+clusterbenchstats">clusterbenchstats</a></code>, <code><a href="#topic+valstat.object">valstat.object</a></code>, 
<code><a href="#topic+cluster.magazine">cluster.magazine</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
  set.seed(20000)
  options(digits=3)
  face &lt;- rFace(10,dMoNo=2,dNoEy=0,p=2)
  clustermethod=c("kmeansCBI","hclustCBI","hclustCBI")
  clustermethodpars &lt;- list()
  clustermethodpars[[2]] &lt;- clustermethodpars[[3]] &lt;- list()
  clustermethodpars[[2]]$method &lt;- "ward.D2"
  clustermethodpars[[3]]$method &lt;- "single"
  methodname &lt;- c("kmeans","ward","single")
  cbs &lt;-  clusterbenchstats(face,G=2:3,clustermethod=clustermethod,
    methodname=methodname,distmethod=rep(FALSE,3),
    clustermethodpars=clustermethodpars,nnruns=2,kmruns=2,fnruns=2,avenruns=2)
  plot(cbs$stat,cbs$sim)
  plot(cbs$stat,cbs$sim,statistic="dindex")
  plot(cbs$stat,cbs$sim,statistic="avewithin")
  pcbs &lt;- print(cbs$sstat,aggregate=TRUE,weights=c(1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0))
# Some of the values are "NaN" because due to the low number of runs of
# the stupid clustering methods there is no variation. If this happens
# in a real application, nnruns etc. should be chosen higher than 2.
# Also useallg=TRUE in clusterbenchstats may help.
#
# Finding the best aggregated value:
  mpcbs &lt;- as.matrix(pcbs[[17]][,-1])
  which(mpcbs==max(mpcbs),arr.ind=TRUE)
# row=1 refers to the first clustering method kmeansCBI,
# col=2 refers to the second number of clusters, which is 3 in g=2:3.
</code></pre>

<hr>
<h2 id='plotcluster'>Discriminant projection plot.</h2><span id='topic+plotcluster'></span>

<h3>Description</h3>

<p>Plots to distinguish given classes by ten available projection
methods. Includes classical discriminant
coordinates, methods to project differences in
mean and covariance structure, asymmetric methods (separation of a
homogeneous class from a heterogeneous one), local neighborhood-based
methods and methods based on robust covariance matrices.
One-dimensional data is plotted against the cluster number.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotcluster(x, clvecd, clnum=NULL,
            method=ifelse(is.null(clnum),"dc","awc"),
            bw=FALSE,
            ignorepoints=FALSE, ignorenum=0, pointsbyclvecd=TRUE,
            xlab=NULL, ylab=NULL,
            pch=NULL, col=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotcluster_+3A_x">x</code></td>
<td>
<p>the data matrix; a numerical object which can be coerced
to a matrix.</p>
</td></tr>
<tr><td><code id="plotcluster_+3A_clvecd">clvecd</code></td>
<td>
<p>vector of class numbers which can be coerced into
integers; length must equal
<code>nrow(xd)</code>.</p>
</td></tr>
<tr><td><code id="plotcluster_+3A_method">method</code></td>
<td>
<p>one of
</p>

<dl>
<dt>&quot;dc&quot;</dt><dd><p>usual discriminant coordinates, see <code><a href="#topic+discrcoord">discrcoord</a></code>,</p>
</dd>
<dt>&quot;bc&quot;</dt><dd><p>Bhattacharyya coordinates, first coordinate showing
mean differences, second showing covariance matrix differences,
see <code><a href="#topic+batcoord">batcoord</a></code>,</p>
</dd>
<dt>&quot;vbc&quot;</dt><dd><p>variance dominated Bhattacharyya coordinates,
see <code><a href="#topic+batcoord">batcoord</a></code>,</p>
</dd>
<dt>&quot;mvdc&quot;</dt><dd><p>added mean and variance differences optimizing
coordinates, see <code><a href="#topic+mvdcoord">mvdcoord</a></code>,</p>
</dd>
<dt>&quot;adc&quot;</dt><dd><p>asymmetric discriminant coordinates, see
<code><a href="#topic+adcoord">adcoord</a></code>,</p>
</dd>
<dt>&quot;awc&quot;</dt><dd><p>asymmetric discriminant coordinates with weighted
observations, see <code><a href="#topic+awcoord">awcoord</a></code>,</p>
</dd>
<dt>&quot;arc&quot;</dt><dd><p>asymmetric discriminant coordinates with weighted
observations and robust MCD-covariance matrix,
see <code><a href="#topic+awcoord">awcoord</a></code>,</p>
</dd>
<dt>&quot;nc&quot;</dt><dd><p>neighborhood based coordinates,
see <code><a href="#topic+ncoord">ncoord</a></code>,</p>
</dd>
<dt>&quot;wnc&quot;</dt><dd><p>neighborhood based coordinates with weighted neighborhoods,
see <code><a href="#topic+ncoord">ncoord</a></code>,</p>
</dd>
<dt>&quot;anc&quot;</dt><dd><p>asymmetric neighborhood based coordinates,
see <code><a href="#topic+ancoord">ancoord</a></code>.</p>
</dd>
</dl>

<p>Note that &quot;bc&quot;, &quot;vbc&quot;, &quot;adc&quot;, &quot;awc&quot;, &quot;arc&quot; and &quot;anc&quot; assume that
there are only two classes.</p>
</td></tr>
<tr><td><code id="plotcluster_+3A_clnum">clnum</code></td>
<td>
<p>integer. Number of the class which is attempted to plot
homogeneously by &quot;asymmetric methods&quot;, which are the methods
assuming that there are only two classes, as indicated above.
<code>clnum</code> is ignored for methods &quot;dc&quot; and &quot;nc&quot;.</p>
</td></tr>
<tr><td><code id="plotcluster_+3A_bw">bw</code></td>
<td>
<p>logical. If <code>TRUE</code>, the classes are distinguished by
symbols, and the default color is black/white.
If <code>FALSE</code>, the classes are distinguished by
colors, and the default symbol is <code>pch=1</code>.</p>
</td></tr>
<tr><td><code id="plotcluster_+3A_ignorepoints">ignorepoints</code></td>
<td>
<p>logical. If <code>TRUE</code>, points with label
<code>ignorenum</code> in <code>clvecd</code> are ignored in the computation for
<code>method</code> and are only projected afterwards onto the resulting
units. If <code>pch=NULL</code>, the plot symbol for these points is &quot;N&quot;.</p>
</td></tr>
<tr><td><code id="plotcluster_+3A_ignorenum">ignorenum</code></td>
<td>
<p>one of the potential values of the components of
<code>clvecd</code>. Only has effect if <code>ignorepoints=TRUE</code>, see above.</p>
</td></tr>
<tr><td><code id="plotcluster_+3A_pointsbyclvecd">pointsbyclvecd</code></td>
<td>
<p>logical. If <code>TRUE</code> and <code>pch=NULL</code>
and/or <code>col=NULL</code>, some hopefully suitable
plot symbols (numbers and letters) and colors are chosen to
distinguish the values of <code>clvecd</code>, starting with &quot;1&quot;/&quot;black&quot;
for the cluster with the smallest <code>clvecd</code>-code (note that
colors for clusters with numbers larger than minimum number
<code>+3</code> are drawn at random from all available colors).
<code>FALSE</code> produces
potentially less reasonable (but nonrandom) standard colors and symbols if
<code>method</code> is &quot;dc&quot; or &quot;nc&quot;, and will only distinguish whether
<code>clvecd=clnum</code> or not for the other methods.</p>
</td></tr> 
<tr><td><code id="plotcluster_+3A_xlab">xlab</code></td>
<td>
<p>label for x-axis. If <code>NULL</code>, a default text is used.</p>
</td></tr>
<tr><td><code id="plotcluster_+3A_ylab">ylab</code></td>
<td>
<p>label for y-axis. If <code>NULL</code>, a default text is used.</p>
</td></tr>
<tr><td><code id="plotcluster_+3A_pch">pch</code></td>
<td>
<p>plotting symbol, see <code><a href="graphics.html#topic+par">par</a></code>.
If <code>NULL</code>, the default is used.</p>
</td></tr>
<tr><td><code id="plotcluster_+3A_col">col</code></td>
<td>
<p>plotting color, see <code><a href="graphics.html#topic+par">par</a></code>.
If <code>NULL</code>, the default is used.</p>
</td></tr>
<tr><td><code id="plotcluster_+3A_...">...</code></td>
<td>
<p>additional parameters passed to <code>plot</code> or the
projection methods.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For some of the asymmetric methods, the area in the plot
occupied by the &quot;homogeneous class&quot; (see <code>clnum</code> above) may be
very small, and it may make sense to run <code>plotcluster</code> a second
time specifying plot parameters <code>xlim</code> and <code>ylim</code> in a
suitable way. It often makes sense to magnify the plot region
containing the homogeneous class in this way
so that its separation from the rest can be
seen more clearly.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>References</h3>

<p>Hennig, C. (2004) Asymmetric linear dimension reduction for classification.
Journal of Computational and Graphical Statistics 13, 930-945 .
</p>
<p>Hennig, C. (2005)  A method for visual cluster validation.  In:
Weihs, C. and Gaul, W. (eds.): Classification - The Ubiquitous
Challenge. Springer, Heidelberg 2005, 153-160.
</p>
<p>Seber, G. A. F. (1984). <em>Multivariate Observations</em>. New York: Wiley.
</p>
<p>Fukunaga (1990). <em>Introduction to Statistical Pattern
Recognition</em> (2nd ed.). Boston: Academic Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+discrcoord">discrcoord</a></code>, <code><a href="#topic+batcoord">batcoord</a></code>,
<code><a href="#topic+mvdcoord">mvdcoord</a></code>, <code><a href="#topic+adcoord">adcoord</a></code>,
<code><a href="#topic+awcoord">awcoord</a></code>, <code><a href="#topic+ncoord">ncoord</a></code>,
<code><a href="#topic+ancoord">ancoord</a></code>.
</p>
<p><code><a href="#topic+discrproj">discrproj</a></code> is an interface to all these projection methods.
</p>
<p><code><a href="#topic+rFace">rFace</a></code> for generation of the example data used below.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(4634)
face &lt;- rFace(300,dMoNo=2,dNoEy=0)
grface &lt;- as.integer(attr(face,"grouping"))
plotcluster(face,grface)
plotcluster(face,grface==1)
plotcluster(face,grface, clnum=1, method="vbc")
</code></pre>

<hr>
<h2 id='prediction.strength'>Prediction strength for estimating number of clusters</h2><span id='topic+prediction.strength'></span><span id='topic+print.predstr'></span>

<h3>Description</h3>

<p>Computes the prediction strength of a clustering of a dataset into
different numbers of components. The prediction strength is
defined according to Tibshirani and Walther (2005), who recommend to
choose as optimal number of cluster the largest number of clusters
that leads to a prediction strength above 0.8 or 0.9. See details.
</p>
<p>Various clustering methods can be used, see argument
<code>clustermethod</code>. In  Tibshirani and Walther (2005), only
classification to the nearest centroid is discussed, but more methods
are offered here, see argument <code>classification</code>.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  prediction.strength(xdata, Gmin=2, Gmax=10, M=50,
                      clustermethod=kmeansCBI,
                                classification="centroid", centroidname = NULL,
                                cutoff=0.8,nnk=1,
                      distances=inherits(xdata,"dist"),count=FALSE,...)
  ## S3 method for class 'predstr'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prediction.strength_+3A_xdata">xdata</code></td>
<td>
<p>data (something that can be coerced into a matrix).</p>
</td></tr>
<tr><td><code id="prediction.strength_+3A_gmin">Gmin</code></td>
<td>
<p>integer. Minimum number of clusters. Note that the
prediction strength for 1 cluster is trivially 1, which is
automatically included if <code>GMin&gt;1</code>. Therefore <code>GMin&lt;2</code> is
useless.</p>
</td></tr>  
<tr><td><code id="prediction.strength_+3A_gmax">Gmax</code></td>
<td>
<p>integer. Maximum number of clusters.</p>
</td></tr>
<tr><td><code id="prediction.strength_+3A_m">M</code></td>
<td>
<p>integer. Number of times the dataset is divided into two
halves.</p>
</td></tr>
<tr><td><code id="prediction.strength_+3A_clustermethod">clustermethod</code></td>
<td>
<p>an interface function (the function name, not a
string containing the name, has to be provided!). This defines the
clustering method. See the &quot;Details&quot;-section of <code><a href="#topic+clusterboot">clusterboot</a></code>
and <code><a href="#topic+kmeansCBI">kmeansCBI</a></code> for the format. Clustering methods for
<code>prediction.strength</code> must have a <code>k</code>-argument for the number of
clusters, must operate on n times p data matrices
and must otherwise follow the specifications in
<code><a href="#topic+clusterboot">clusterboot</a></code>
Note that <code>prediction.strength</code> won't work
with CBI-functions that implicitly already estimate the number of
clusters such as <code><a href="#topic+pamkCBI">pamkCBI</a></code>; use <code><a href="#topic+claraCBI">claraCBI</a></code>
if you want to run it for pam/clara clustering.</p>
</td></tr>
<tr><td><code id="prediction.strength_+3A_classification">classification</code></td>
<td>
<p>string.
This determines how non-clustered points are classified to given
clusters. Options are explained in <code><a href="#topic+classifnp">classifnp</a></code> and
<code><a href="#topic+classifdist">classifdist</a></code>, the latter for dissimilarity data.
Certain classification methods are connected to certain clustering
methods. <code>classification="averagedist"</code> is recommended for
average linkage, <code>classification="centroid"</code> is recommended for
k-means, clara and pam (with distances it will work with
<code><a href="#topic+claraCBI">claraCBI</a></code> only), <code>classification="knn"</code> with
<code>nnk=1</code> is recommended for single linkage and
<code>classification="qda"</code> is recommended for Gaussian mixtures
with flexible covariance matrices. 
</p>
</td></tr>
<tr><td><code id="prediction.strength_+3A_centroidname">centroidname</code></td>
<td>
<p>string. Indicates the name of the component of
<code>CBIoutput$result</code> that contains the cluster centroids in case of
<code>classification="centroid"</code>, where <code>CBIoutput</code> is the
output object of <code>clustermethod</code>. If <code>clustermethod</code> is
<code>kmeansCBI</code> or <code>claraCBI</code>, centroids are recognised
automatically if <code>centroidname=NULL</code>. If
<code>centroidname=NULL</code> and <code>distances=FALSE</code>, cluster means
are computed as the cluster centroids.</p>
</td></tr>  
<tr><td><code id="prediction.strength_+3A_cutoff">cutoff</code></td>
<td>
<p>numeric between 0 and 1. The optimal number of clusters
is the maximum one with prediction strength above <code>cutoff</code>.</p>
</td></tr>
<tr><td><code id="prediction.strength_+3A_nnk">nnk</code></td>
<td>
<p>number of nearest neighbours if
<code>classification="knn"</code>, see <code><a href="#topic+classifnp">classifnp</a></code>.</p>
</td></tr>
<tr><td><code id="prediction.strength_+3A_distances">distances</code></td>
<td>
<p>logical. If <code>TRUE</code>, data will be interpreted as
dissimilarity matrix, passed on to clustering methods as
<code>"dist"</code>-object, and <code><a href="#topic+classifdist">classifdist</a></code> will be used for
classification.</p>
</td></tr>   
<tr><td><code id="prediction.strength_+3A_count">count</code></td>
<td>
<p>logical. <code>TRUE</code> will print current number of
clusters and simulation run number on the screen.</p>
</td></tr>
<tr><td><code id="prediction.strength_+3A_x">x</code></td>
<td>
<p>object of class <code>predstr</code>.</p>
</td></tr>
<tr><td><code id="prediction.strength_+3A_...">...</code></td>
<td>
<p>arguments to be passed on to the clustering method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The prediction strength for a certain number of clusters k under a
random partition of the dataset in halves A and B is defined as
follows. Both halves are clustered with k clusters. Then the points of
A are classified to the clusters of B. In the original paper
this is done by assigning every
observation in A to the closest cluster centroid in B (corresponding
to <code>classification="centroid"</code>), but other methods are possible,
see <code><a href="#topic+classifnp">classifnp</a></code>. A pair of points A in
the same A-cluster is defined to be correctly predicted if both points
are classified into the same cluster on B. The same is done with the
points of B relative to the clustering on A. The prediction strength
for each of the clusterings is the minimum (taken over all clusters)
relative frequency of correctly predicted pairs of points of that
cluster. The final mean prediction strength statistic is the mean over
all 2M clusterings.
</p>


<h3>Value</h3>

<p><code>prediction.strength</code> gives out an object of class
<code>predstr</code>, which is a
list with components
</p>
<table>
<tr><td><code>predcorr</code></td>
<td>
<p>list of vectors of length <code>M</code> with relative
frequencies of correct predictions (clusterwise minimum). Every list
entry refers to a certain number of clusters.</p>
</td></tr>
<tr><td><code>mean.pred</code></td>
<td>
<p>means of <code>predcorr</code> for all numbers of
clusters.</p>
</td></tr>
<tr><td><code>optimalk</code></td>
<td>
<p>optimal number of clusters.</p>
</td></tr>
<tr><td><code>cutoff</code></td>
<td>
<p>see above.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a string identifying the clustering method.</p>
</td></tr>
<tr><td><code>Gmax</code></td>
<td>
<p>see above.</p>
</td></tr>
<tr><td><code>M</code></td>
<td>
<p>see above.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Tibshirani, R. and Walther, G. (2005) Cluster Validation by 
Prediction Strength, <em>Journal of Computational and Graphical 
Statistics</em>, 14, 511-528.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kmeansCBI">kmeansCBI</a></code>, <code><a href="#topic+classifnp">classifnp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  options(digits=3)
  set.seed(98765)
  iriss &lt;- iris[sample(150,20),-5]
  prediction.strength(iriss,2,3,M=3)
  prediction.strength(iriss,2,3,M=3,clustermethod=claraCBI)
# The examples are fast, but of course M should really be larger.
</code></pre>

<hr>
<h2 id='randcmatrix'>Random partition matrix</h2><span id='topic+randcmatrix'></span>

<h3>Description</h3>

<p>For use within <code>regmix</code>. Generates a random
0-1-matrix with <code>n</code> rows
and <code>cln</code> columns so that every row contains exactly one one and
every columns contains at least <code>p+3</code> ones.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>randcmatrix(n,cln,p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="randcmatrix_+3A_n">n</code></td>
<td>
<p>positive integer. Number of rows.</p>
</td></tr>
<tr><td><code id="randcmatrix_+3A_cln">cln</code></td>
<td>
<p>positive integer. Number of columns.</p>
</td></tr>
<tr><td><code id="randcmatrix_+3A_p">p</code></td>
<td>
<p>positive integer. See above.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>n*cln</code>-matrix.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+regmix">regmix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(111)
randcmatrix(10,2,1)
</code></pre>

<hr>
<h2 id='randconf'>Generate a sample indicator vector</h2><span id='topic+randconf'></span>

<h3>Description</h3>

<p>Generates a logical vector of length <code>n</code> with <code>p TRUE</code>s.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>randconf(n, p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="randconf_+3A_n">n</code></td>
<td>
<p>positive integer.</p>
</td></tr>
<tr><td><code id="randconf_+3A_p">p</code></td>
<td>
<p>positive integer.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A logical vector.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+sample">sample</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  randconf(10,3)
</code></pre>

<hr>
<h2 id='randomclustersim'>Simulation of validity indexes based on random clusterings</h2><span id='topic+randomclustersim'></span>

<h3>Description</h3>

<p>For a given dataset this simulates random clusterings using
<code><a href="#topic+stupidkcentroids">stupidkcentroids</a></code>, <code><a href="#topic+stupidknn">stupidknn</a></code>,
<code><a href="#topic+stupidkfn">stupidkfn</a></code>, and <code><a href="#topic+stupidkaven">stupidkaven</a></code>. It then
computes and stores a set of cluster validity indexes for every
clustering. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  randomclustersim(datadist,datanp=NULL,npstats=FALSE,useboot=FALSE,
                      bootmethod="nselectboot",
                      bootruns=25, 
                      G,nnruns=100,kmruns=100,fnruns=100,avenruns=100,
                      nnk=4,dnnk=2,
                      pamcrit=TRUE, 
                      multicore=FALSE,cores=detectCores()-1,monitor=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="randomclustersim_+3A_datadist">datadist</code></td>
<td>
<p>distances on which validation-measures are based, <code>dist</code>
object or distance matrix.</p>
</td></tr>
<tr><td><code id="randomclustersim_+3A_datanp">datanp</code></td>
<td>
<p>optional observations times variables data matrix, see
<code>npstats</code>.</p>
</td></tr> 
<tr><td><code id="randomclustersim_+3A_npstats">npstats</code></td>
<td>
<p>logical. If <code>TRUE</code>, <code><a href="#topic+distrsimilarity">distrsimilarity</a></code>
is called and the two statistics computed there are added to the
output. These are based on <code>datanp</code> and require <code>datanp</code>
to be specified.</p>
</td></tr>
<tr><td><code id="randomclustersim_+3A_useboot">useboot</code></td>
<td>
<p>logical. If <code>TRUE</code>, a stability index (either
<code>nselectboot</code> or <code>prediction.strength</code>) will be involved.</p>
</td></tr>
<tr><td><code id="randomclustersim_+3A_bootmethod">bootmethod</code></td>
<td>
<p>either <code>"nselectboot"</code> or
<code>"prediction.strength"</code>; stability index to be used if
<code>useboot=TRUE</code>.</p>
</td></tr>
<tr><td><code id="randomclustersim_+3A_bootruns">bootruns</code></td>
<td>
<p>integer. Number of resampling runs. If
<code>useboot=TRUE</code>, passed on as <code>B</code> to <code>nselectboot</code> or
<code>M</code> to <code>prediction.strength</code>.</p>
</td></tr>
<tr><td><code id="randomclustersim_+3A_g">G</code></td>
<td>
<p>vector of integers. Numbers of clusters to consider.</p>
</td></tr>
<tr><td><code id="randomclustersim_+3A_nnruns">nnruns</code></td>
<td>
<p>integer. Number of runs of <code><a href="#topic+stupidknn">stupidknn</a></code>.</p>
</td></tr>
<tr><td><code id="randomclustersim_+3A_kmruns">kmruns</code></td>
<td>
<p>integer. Number of runs of <code><a href="#topic+stupidkcentroids">stupidkcentroids</a></code>.</p>
</td></tr>
<tr><td><code id="randomclustersim_+3A_fnruns">fnruns</code></td>
<td>
<p>integer. Number of runs of <code><a href="#topic+stupidkfn">stupidkfn</a></code>.</p>
</td></tr>
<tr><td><code id="randomclustersim_+3A_avenruns">avenruns</code></td>
<td>
<p>integer. Number of runs of <code><a href="#topic+stupidkaven">stupidkaven</a></code>.</p>
</td></tr>
<tr><td><code id="randomclustersim_+3A_nnk">nnk</code></td>
<td>
<p><code>nnk</code>-argument to be passed on to
<code><a href="#topic+cqcluster.stats">cqcluster.stats</a></code>.</p>
</td></tr> 
<tr><td><code id="randomclustersim_+3A_dnnk">dnnk</code></td>
<td>
<p><code>nnk</code>-argument to be passed on to
<code><a href="#topic+distrsimilarity">distrsimilarity</a></code>.</p>
</td></tr>
<tr><td><code id="randomclustersim_+3A_pamcrit">pamcrit</code></td>
<td>
<p><code>pamcrit</code>-argument to be passed on to
<code><a href="#topic+cqcluster.stats">cqcluster.stats</a></code>.</p>
</td></tr> 
<tr><td><code id="randomclustersim_+3A_multicore">multicore</code></td>
<td>
<p>logical. If <code>TRUE</code>, parallel computing is used
through the function <code><a href="parallel.html#topic+mclapply">mclapply</a></code> from package
<code>parallel</code>; read warnings there if you intend to use this; it
won't work on Windows.</p>
</td></tr>
<tr><td><code id="randomclustersim_+3A_cores">cores</code></td>
<td>
<p>integer. Number of cores for parallelisation.</p>
</td></tr>
<tr><td><code id="randomclustersim_+3A_monitor">monitor</code></td>
<td>
<p>logical. If <code>TRUE</code>, it will print some runtime
information.</p>
</td></tr> 
</table>


<h3>Value</h3>

<p>List with components
</p>
<table>
<tr><td><code>nn</code></td>
<td>
<p>list, indexed by number of clusters. Every entry is
a data frame with <code>nnruns</code> observations for every simulation
run of <code><a href="#topic+stupidknn">stupidknn</a></code>. The variables of the data frame are
<code>avewithin, mnnd,
      cvnnd, maxdiameter, widestgap, sindex, minsep, asw, dindex,
      denscut, highdgap, pearsongamma, withinss, entropy</code>, if
<code>pamcrit=TRUE</code> also <code>pamc</code>, if <code>npstats=TRUE</code> also
<code>kdnorm, kdunif</code>. All these are cluster validation indexes;
documented as values of <code><a href="#topic+clustatsum">clustatsum</a></code>.</p>
</td></tr>
<tr><td><code>fn</code></td>
<td>
<p>list, indexed by number of clusters. Every entry is
a data frame with <code>fnruns</code> observations for every simulation
run of <code><a href="#topic+stupidkfn">stupidkfn</a></code>. The variables of the data frame are
<code>avewithin, mnnd,
      cvnnd, maxdiameter, widestgap, sindex, minsep, asw, dindex,
      denscut, highdgap, pearsongamma, withinss, entropy</code>, if
<code>pamcrit=TRUE</code> also <code>pamc</code>, if <code>npstats=TRUE</code> also
<code>kdnorm, kdunif</code>. All these are cluster validation indexes;
documented as values of <code><a href="#topic+clustatsum">clustatsum</a></code>.</p>
</td></tr>
<tr><td><code>aven</code></td>
<td>
<p>list, indexed by number of clusters. Every entry is
a data frame with <code>avenruns</code> observations for every simulation
run of <code><a href="#topic+stupidkaven">stupidkaven</a></code>. The variables of the data frame are
<code>avewithin, mnnd,
      cvnnd, maxdiameter, widestgap, sindex, minsep, asw, dindex,
      denscut, highdgap, pearsongamma, withinss, entropy</code>, if
<code>pamcrit=TRUE</code> also <code>pamc</code>, if <code>npstats=TRUE</code> also
<code>kdnorm, kdunif</code>. All these are cluster validation indexes;
documented as values of <code><a href="#topic+clustatsum">clustatsum</a></code>.</p>
</td></tr>
<tr><td><code>km</code></td>
<td>
<p>list, indexed by number of clusters. Every entry is
a data frame with <code>kmruns</code> observations for every simulation
run of <code><a href="#topic+stupidkcentroids">stupidkcentroids</a></code>. The variables of the data
frame are <code>avewithin, mnnd,
      cvnnd, maxdiameter, widestgap, sindex, minsep, asw, dindex,
      denscut, highdgap, pearsongamma, withinss, entropy</code>, if
<code>pamcrit=TRUE</code> also <code>pamc</code>, if <code>npstats=TRUE</code> also
<code>kdnorm, kdunif</code>. All these are cluster validation indexes;
documented as values of <code><a href="#topic+clustatsum">clustatsum</a></code>.</p>
</td></tr>
<tr><td><code>nnruns</code></td>
<td>
<p>number of involved runs of <code><a href="#topic+stupidknn">stupidknn</a></code>,</p>
</td></tr>
<tr><td><code>fnruns</code></td>
<td>
<p>number of involved runs of <code><a href="#topic+stupidkfn">stupidkfn</a></code>,</p>
</td></tr>
<tr><td><code>avenruns</code></td>
<td>
<p>number of involved runs of <code><a href="#topic+stupidkaven">stupidkaven</a></code>,</p>
</td></tr>
<tr><td><code>kmruns</code></td>
<td>
<p>number of involved runs of <code><a href="#topic+stupidkcentroids">stupidkcentroids</a></code>,</p>
</td></tr>
<tr><td><code>boot</code></td>
<td>
<p>if <code>useboot=TRUE</code>, stability value; <code>stabk</code> for
method <code><a href="#topic+nselectboot">nselectboot</a></code>; <code>mean.pred</code> for method
<code><a href="#topic+prediction.strength">prediction.strength</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2019) Cluster validation by measurement of clustering
characteristics relevant to the user. In C. H. Skiadas (ed.)
<em>Data Analysis and Applications 1: Clustering and Regression,
Modeling-estimating, Forecasting and Data Mining, Volume 2</em>, Wiley,
New York 1-24,
<a href="https://arxiv.org/abs/1703.09282">https://arxiv.org/abs/1703.09282</a>
</p>
<p>Akhanli, S. and Hennig, C. (2020) Calibrating and aggregating cluster
validity indexes for context-adapted comparison of clusterings.
<em>Statistics and Computing</em>, 30, 1523-1544,
<a href="https://link.springer.com/article/10.1007/s11222-020-09958-2">https://link.springer.com/article/10.1007/s11222-020-09958-2</a>, <a href="https://arxiv.org/abs/2002.01822">https://arxiv.org/abs/2002.01822</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stupidkcentroids">stupidkcentroids</a></code>, <code><a href="#topic+stupidknn">stupidknn</a></code>, <code><a href="#topic+stupidkfn">stupidkfn</a></code>, <code><a href="#topic+stupidkaven">stupidkaven</a></code>, <code><a href="#topic+clustatsum">clustatsum</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(20000)
  options(digits=3)
  face &lt;- rFace(10,dMoNo=2,dNoEy=0,p=2)
  rmx &lt;- randomclustersim(dist(face),datanp=face,npstats=TRUE,G=2:3,
    nnruns=2,kmruns=2, fnruns=1,avenruns=1,nnk=2)
## Not run: 
  rmx$km # Produces slightly different but basically identical results on ATLAS

## End(Not run)
  rmx$aven
  rmx$fn
  rmx$nn
  
</code></pre>

<hr>
<h2 id='regmix'>Mixture Model ML for Clusterwise Linear Regression</h2><span id='topic+regmix'></span><span id='topic+regem'></span>

<h3>Description</h3>

<p>Computes an ML-estimator for clusterwise linear regression under a
regression mixture model with Normal errors. Parameters are
proportions, regression coefficients and error variances, all
independent of the values of the independent variable, and all may
differ for different clusters. Computation is by the EM-algorithm. The
number of clusters is estimated via the Bayesian Information Criterion
(BIC). Note that package <code>flexmix</code> has more sophisticated tools to do the
same thing and is recommended. The functions are kept in here only for
compatibility reasons.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regmix(indep, dep, ir=1, nclust=1:7, icrit=1.e-5, minsig=1.e-6, warnings=FALSE)

regem(indep, dep, m, cln, icrit=1.e-5, minsig=1.e-6, warnings=FALSE) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regmix_+3A_indep">indep</code></td>
<td>
<p>numerical matrix or vector. Independent
variables.</p>
</td></tr>
<tr><td><code id="regmix_+3A_dep">dep</code></td>
<td>
<p>numerical vector. Dependent variable.</p>
</td></tr>
<tr><td><code id="regmix_+3A_ir">ir</code></td>
<td>
<p>positive integer.
Number of iteration runs for every number of clusters.</p>
</td></tr>
<tr><td><code id="regmix_+3A_nclust">nclust</code></td>
<td>
<p>vector of positive integers.
Numbers of clusters.</p>
</td></tr>
<tr><td><code id="regmix_+3A_icrit">icrit</code></td>
<td>
<p>positive numerical. Stopping criterion for the iterations
(difference of loglikelihoods).</p>
</td></tr>
<tr><td><code id="regmix_+3A_minsig">minsig</code></td>
<td>
<p>positive numerical. Minimum value for the variance
parameters (likelihood is unbounded if variances are allowed to
converge to 0).</p>
</td></tr>
<tr><td><code id="regmix_+3A_warnings">warnings</code></td>
<td>
<p>logical. If <code>TRUE</code>, warnings are given during the
EM iteration in case of collinear regressors, too small mixture
components and error variances
smaller than minimum. In the former two cases, the algorithm is
terminated without a result, but an optimal solution is still
computed from other algorithm runs (if there are others). In the
latter case, the corresponding variance is set to the minimum.</p>
</td></tr>
<tr><td><code id="regmix_+3A_cln">cln</code></td>
<td>
<p>positive integer. (Single) number of clusters.</p>
</td></tr>
<tr><td><code id="regmix_+3A_m">m</code></td>
<td>
<p>matrix of positive numericals. Number of columns must be
<code>cln</code>. Number of rows must be number of data points. Columns
must add up to 1. Initial configuration for the EM iteration in
terms of a probabilty vector for every point which gives its degree
of membership to every cluster. As generated by <code><a href="#topic+randcmatrix">randcmatrix</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The result of the EM iteration depends on the initial configuration,
which is generated randomly by <code><a href="#topic+randcmatrix">randcmatrix</a></code> for <code>regmix</code>.
<code>regmix</code> calls <code>regem</code>. To provide the initial configuration
manually, use parameter <code>m</code> of
<code>regem</code> directly. Take a look at the example about how to
generate <code>m</code> if you want to specify initial parameters.
</p>
<p>The original paper DeSarbo and Cron (1988) suggests the AIC for
estimating the number of clusters. The use of the BIC is advocated by
Wedel and DeSarbo (1995). The BIC is defined here as
<code>2*loglik - log(n)*((p+3)*cln-1)</code>, <code>p</code> being the number of
independent variables, i.e., the larger the better.
</p>
<p>See the entry for the input parameter <code>warnings</code> for the
treatment of several numerical problems.
</p>


<h3>Value</h3>

<p><code>regmix</code> returns a list
containing the components <code>clnopt, loglik, bic,
    coef, var, eps, z, g</code>.
</p>
<p><code>regem</code>  returns a list
containing the components <code>loglik, 
    coef, var, z, g, warn</code>.
</p>
<table>
<tr><td><code>clnopt</code></td>
<td>
<p>optimal number of clusters according to the BIC.</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>loglikelihood for the optimal model.</p>
</td></tr>
<tr><td><code>bic</code></td>
<td>
<p>vector of BIC values for all numbers of clusters in <code>nclust</code>.</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>matrix of regression coefficients. First row: intercept
parameter. Second row: parameter of first independent variable and
so on. Columns corresponding to clusters.</p>
</td></tr>
<tr><td><code>var</code></td>
<td>
<p>vector of error variance estimators for the clusters.</p>
</td></tr>
<tr><td><code>eps</code></td>
<td>
<p>vector of cluster proportion estimators.</p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p>matrix of estimated a posteriori probabilities of the points
(rows) to be generated by the clusters (columns). Compare input
argument <code>m</code>.</p>
</td></tr>
<tr><td><code>g</code></td>
<td>
<p>integer vector of estimated cluster numbers for the points
(via argmax over <code>z</code>).</p>
</td></tr>
<tr><td><code>warn</code></td>
<td>
<p>logical. <code>TRUE</code> if one of the estimated clusters has
too few points and/or collinear regressors.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>References</h3>

<p>DeSarbo, W. S. and Cron, W. L. (1988) A maximum likelihood methodology
for clusterwise linear regression, <em>Journal of
Classification</em> 5, 249-282.
</p>
<p>Wedel, M. and DeSarbo, W. S. (1995) A mixture likelihood approach for
generalized linear models, <em>Journal of
Classification</em> 12, 21-56.
</p>


<h3>See Also</h3>

<p>Regression mixtures can also (and probably better) be computed with the
flexmix package, see <code><a href="flexmix.html#topic+flexmix">flexmix</a></code>. (When I first write
the <code>regmix</code>-function, <code>flexmix</code> didn't exist.)
</p>
<p><code><a href="#topic+fixreg">fixreg</a></code> for fixed point clusters for clusterwise linear regression.
</p>
<p><code><a href="mclust.html#topic+mclustBIC">EMclust</a></code> for Normal mixture model fitting (non-regression).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# This apparently gives slightly different
# but data-analytically fine results
# on some versions of R.
set.seed(12234)
data(tonedata)
attach(tonedata)
rmt1 &lt;- regmix(stretchratio,tuned,nclust=1:2)
# nclust=1:2 makes the example fast;
# a more serious application would rather use the default.
rmt1$g
round(rmt1$bic,digits=2)
# start with initial parameter values
cln &lt;- 3
n &lt;- 150
initcoef &lt;- cbind(c(2,0),c(0,1),c(0,2.5))
initvar &lt;- c(0.001,0.0001,0.5)
initeps &lt;- c(0.4,0.3,0.3)
# computation of m from initial parameters
m &lt;- matrix(nrow=n, ncol=cln)
stm &lt;- numeric(0)
for (i in 1:cln)
  for (j in 1:n){
    m[j,i] &lt;- initeps[i]*dnorm(tuned[j],mean=initcoef[1,i]+
              initcoef[2,i]*stretchratio[j], sd=sqrt(initvar[i]))
  }
  for (j in 1:n){
    stm[j] &lt;- sum(m[j,])
    for (i in 1:cln)
      m[j,i] &lt;- m[j,i]/stm[j]
  } 
rmt2 &lt;- regem(stretchratio, tuned, m, cln)

## End(Not run)
</code></pre>

<hr>
<h2 id='rFace'>&quot;Face-shaped&quot; clustered benchmark datasets</h2><span id='topic+rFace'></span>

<h3>Description</h3>

<p>Generates &quot;face-shaped&quot; clustered benchmark datasets.
This is based on a collaboration with Martin Maechler.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rFace(n, p = 6, nrep.top = 2, smile.coef = 0.6, dMoNo = 1.2, dNoEy = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rFace_+3A_n">n</code></td>
<td>
<p>integer greater or equal to 10. Number of points.</p>
</td></tr>
<tr><td><code id="rFace_+3A_p">p</code></td>
<td>
<p>integer greater or equal to 2. Dimension.</p>
</td></tr>
<tr><td><code id="rFace_+3A_nrep.top">nrep.top</code></td>
<td>
<p>integer. Number of repetitions of the hair-top point.</p>
</td></tr>
<tr><td><code id="rFace_+3A_smile.coef">smile.coef</code></td>
<td>
<p>numeric. Coefficient for quadratic term used for
generation of mouth-points. Positive values=&gt;smile.</p>
</td></tr>
<tr><td><code id="rFace_+3A_dmono">dMoNo</code></td>
<td>
<p>number. Distance from mouth to nose.</p>
</td></tr>
<tr><td><code id="rFace_+3A_dnoey">dNoEy</code></td>
<td>
<p>number. Minimum vertical distance from mouth to eyes.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function generates a nice benchmark example for cluster
analysis. 
There are six &quot;clusters&quot; in this data, of which the first five are
clearly homogeneous patterns, but with different distributional
shapes and different qualities of separation. The clusters are
distinguished only in the first two dimensions. The attribute
<code>grouping</code> is a factor giving the cluster numbers, see below.
The sixth group of
points corresponds to some hairs, and is rather a collection of
outliers than a cluster in itself. This group contains
<code>nrep.top+2</code> points. Of the remaining points, 20% belong to
cluster 1, the chin (quadratic function plus noise).
10% belong to cluster 2, the right eye (Gaussian). 30% belong to
cluster 3, the mouth (Gaussian/squared Gaussian). 
20% belong to cluster 4, the nose (Gaussian/gamma), and
20% belong to cluster 5, the left eye (uniform).
</p>
<p>The distributions of the further
variables are homogeneous over
all points. The third dimension is exponentially distributed, the
fourth dimension is Cauchy distributed, all further distributions are
Gaussian.
</p>
<p>Please consider the source code for exact generation of the clusters.
</p>


<h3>Value</h3>

<p>An <code>n</code> times <code>p</code> numeric matrix with attributes
</p>
<table>
<tr><td><code>grouping</code></td>
<td>
<p>a factor giving the cluster memberships of the points.</p>
</td></tr>
<tr><td><code>indexlist</code></td>
<td>
<p>a list of six vectors containing the indices of points
belonging to the six groups.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(4634)
  face &lt;- rFace(600,dMoNo=2,dNoEy=0)
  grface &lt;- as.integer(attr(face,"grouping"))
  plot(face, col = grface)
#  pairs(face, col = grface, main ="rFace(600,dMoNo=2,dNoEy=0)")
</code></pre>

<hr>
<h2 id='ridgeline'>Ridgeline computation</h2><span id='topic+ridgeline'></span>

<h3>Description</h3>

<p>Computes <code class="reqn">(\alpha*\Sigma_1^{-1}+(1-\alpha)*\Sigma_2^{-1})^{-1}* 
    \alpha*(\Sigma_1^{-1}*\mu_1)+(1-\alpha)*(\Sigma_2^{-1}*\mu_2)</code>
as required for the
computation of the ridgeline (Ray and Lindsay, 2005) to find
all density extrema of a two-component Gaussian mixture with
mean vectors mu1 and mu2 and covariance matrices Sigma1, Sigma2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgeline(alpha, mu1, mu2, Sigma1, Sigma2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgeline_+3A_alpha">alpha</code></td>
<td>
<p>numeric between 0 and 1.</p>
</td></tr>
<tr><td><code id="ridgeline_+3A_mu1">mu1</code></td>
<td>
<p>mean vector of component 1.</p>
</td></tr>
<tr><td><code id="ridgeline_+3A_mu2">mu2</code></td>
<td>
<p>mean vector of component 2.</p>
</td></tr>
<tr><td><code id="ridgeline_+3A_sigma1">Sigma1</code></td>
<td>
<p>covariance matrix of component 1.</p>
</td></tr>
<tr><td><code id="ridgeline_+3A_sigma2">Sigma2</code></td>
<td>
<p>covariance matrix of component 2.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector. See above.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Ray, S. and Lindsay, B. G. (2005) The Topography of Multivariate 
Normal Mixtures, <em>Annals of Statistics</em>, 33, 2042-2065.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ridgeline(0.5,c(1,1),c(2,5),diag(2),diag(2))
</code></pre>

<hr>
<h2 id='ridgeline.diagnosis'>Ridgeline plots, ratios and unimodality</h2><span id='topic+ridgeline.diagnosis'></span>

<h3>Description</h3>

<p>Computes ridgeline ratios and unimodality checks for pairs of components
given the parameters of a Gaussian mixture. Produces ridgeline plots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ridgeline.diagnosis (propvector,muarray,Sigmaarray,
                                k=length(propvector),
                                ipairs="all", compute.ratio=TRUE,by=0.001,
                                ratiocutoff=NULL,ridgelineplot="matrix")

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgeline.diagnosis_+3A_propvector">propvector</code></td>
<td>
<p>vector of component proportions. Length must be
number of components, and must sum up to 1.</p>
</td></tr>
<tr><td><code id="ridgeline.diagnosis_+3A_muarray">muarray</code></td>
<td>
<p>matrix of component means (different components are in
different columns).</p>
</td></tr>
<tr><td><code id="ridgeline.diagnosis_+3A_sigmaarray">Sigmaarray</code></td>
<td>
<p>three dimensional array with component covariance
matrices (the third dimension refers to components).</p>
</td></tr>
<tr><td><code id="ridgeline.diagnosis_+3A_k">k</code></td>
<td>
<p>integer. Number of components.</p>
</td></tr>
<tr><td><code id="ridgeline.diagnosis_+3A_ipairs">ipairs</code></td>
<td>
<p><code>"all"</code> or list of vectors of two integers. If
<code>ipairs="all"</code>, computations are carried out for all pairs of
components. Otherwise, ipairs gives the pairs of components for
which computations are carried out.</p>
</td></tr>
<tr><td><code id="ridgeline.diagnosis_+3A_compute.ratio">compute.ratio</code></td>
<td>
<p>logical. If <code>TRUE</code>, a matrix of ridgeline
ratios is computed, see Hennig (2010a).</p>
</td></tr>
<tr><td><code id="ridgeline.diagnosis_+3A_by">by</code></td>
<td>
<p>real between 0 and 1. Interval width for density computation
along the ridgeline.</p>
</td></tr>
<tr><td><code id="ridgeline.diagnosis_+3A_ratiocutoff">ratiocutoff</code></td>
<td>
<p>real between 0 and 1. If not <code>NULL</code>, the
<code>connection.matrix</code> (see below) is computed by checking whether
ridgeline ratios between components are below <code>ratiocutoff</code>.</p>
</td></tr>
<tr><td><code id="ridgeline.diagnosis_+3A_ridgelineplot">ridgelineplot</code></td>
<td>
<p>one of <code>"none"</code>, <code>"matrix"</code>,
<code>"pairwise"</code>. If <code>"matrix"</code>, a matrix of pairwise
ridgeline plots (see Hennig 2010b) will be plotted. If
<code>"pairwise"</code>, pairwise ridgeline plots are plotted (you may
want to set <code>par(ask=TRUE)</code> to see them all). No plotting if
<code>"none"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>merged.clusters</code></td>
<td>
<p>vector of integers, stating for every mixture
component the number of the cluster of components that would be merged
by merging connectivity components of the graph specified by
<code>connection.matrix</code>.</p>
</td></tr>
<tr><td><code>connection.matrix</code></td>
<td>
<p>zero-one matrix, in which a one means that the
mixture of the corresponding pair of components of the original
mixture is either unimodel (if <code>ratiocutoff=NULL</code>) or that their
ridgeline ratio is above <code>ratiocutoff</code>. If <code>ipairs!="all"</code>,
ignored pairs always have 0 in this matrix, same for
<code>ratio.matrix</code>.</p>
</td></tr>
<tr><td><code>ratio.matrix</code></td>
<td>
<p>matrix with entries between 0 und 1, giving the
ridgeline ratio, which is the density minimum of the mixture of the
corresponding pair of components along the ridgeline divided by the
minimum of the two maxima closest to the beginning and the end of the
ridgeline.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2010a) Methods for merging Gaussian mixture components,
<em>Advances in Data Analysis and Classification</em>, 4, 3-34.
</p>
<p>Hennig, C. (2010b) Ridgeline plot and clusterwise stability as tools
for merging Gaussian mixture components. To appear in
<em>Classification as a Tool for Research</em>, Proceedings of IFCS
2009.
</p>
<p>Ray, S. and Lindsay, B. G. (2005) The Topography of Multivariate 
Normal Mixtures, <em>Annals of Statistics</em>, 33, 2042-2065.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeline">ridgeline</a></code>, <code><a href="#topic+dridgeline">dridgeline</a></code>,
<code><a href="#topic+piridge">piridge</a></code>, <code><a href="#topic+piridge.zeroes">piridge.zeroes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  muarray &lt;- cbind(c(0,0),c(0,0.1),c(10,10))
  sigmaarray &lt;- array(c(diag(2),diag(2),diag(2)),dim=c(2,2,3))
  rd &lt;-
  ridgeline.diagnosis(c(0.5,0.3,0.2),muarray,sigmaarray,ridgelineplot="matrix",by=0.1)
  # Much slower but more precise with default by=0.001.
</code></pre>

<hr>
<h2 id='simmatrix'>Extracting intersections between clusters from fpc-object</h2><span id='topic+simmatrix'></span>

<h3>Description</h3>

<p>Extracts the information about the size of the intersections
between representative
Fixed Point Clusters (FPCs) of stable groups from the output of
the FPC-functions <code><a href="#topic+fixreg">fixreg</a></code> and <code><a href="#topic+fixmahal">fixmahal</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simmatrix(fpcobj)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simmatrix_+3A_fpcobj">fpcobj</code></td>
<td>
<p>an object of class <code>rfpc</code> or <code>mfpc</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A non-negative real-valued vector giving the number of points in
the intersections of the representative FPCs of stable groups.
</p>


<h3>Note</h3>

<p>The intersection between representative FPCs no. <code>i</code> and
<code>j</code> is at position <code><a href="#topic+sseg">sseg</a>(i,j)</code>.</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fixmahal">fixmahal</a></code>,
<code><a href="#topic+fixreg">fixreg</a></code>,
<code><a href="#topic+sseg">sseg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(190000)
data(tonedata)
# Note: If you do not use the installed package, replace this by
# tonedata &lt;- read.table("(path/)tonedata.txt", header=TRUE)
attach(tonedata)
tonefix &lt;- fixreg(stretchratio,tuned,mtf=1,ir=20)
simmatrix(tonefix)[sseg(2,3)]
</code></pre>

<hr>
<h2 id='solvecov'>Inversion of (possibly singular) symmetric matrices</h2><span id='topic+solvecov'></span>

<h3>Description</h3>

<p>Tries to invert a matrix by <code>solve</code>. If this fails because of
singularity, an
eigenvector decomposition is computed, and eigenvalues below
<code>1/cmax</code> are replaced by <code>1/cmax</code>, i.e., <code>cmax</code> will be
the corresponding eigenvalue of the inverted matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>solvecov(m, cmax = 1e+10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="solvecov_+3A_m">m</code></td>
<td>
<p>a numeric symmetric matrix.</p>
</td></tr>
<tr><td><code id="solvecov_+3A_cmax">cmax</code></td>
<td>
<p>a positive value, see above.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>inv</code></td>
<td>
<p>the inverted matrix</p>
</td></tr>
<tr><td><code>coll</code></td>
<td>
<p><code>TRUE</code> if <code>solve</code> failed because of singularity.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+solve">solve</a></code>, <code><a href="base.html#topic+eigen">eigen</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  x &lt;- c(1,0,0,1,0,1,0,0,1)
  dim(x) &lt;- c(3,3)
  solvecov(x)
</code></pre>

<hr>
<h2 id='sseg'>Position in a similarity vector</h2><span id='topic+sseg'></span>

<h3>Description</h3>

<p><code>sseg(i,j)</code> gives the position of the similarity of objects
<code>i</code> and <code>j</code> in the similarity vectors produced by
<code>fixreg</code> and <code>fixmahal</code>.
<code>sseg</code> should only be used as an auxiliary function in
<code>fixreg</code> and <code>fixmahal</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sseg(i, j)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sseg_+3A_i">i</code></td>
<td>
<p>positive integer.</p>
</td></tr>
<tr><td><code id="sseg_+3A_j">j</code></td>
<td>
<p>positive integer.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A positive integer.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>sseg(3,4)
</code></pre>

<hr>
<h2 id='stupidkaven'>Stupid average dissimilarity random clustering</h2><span id='topic+stupidkaven'></span>

<h3>Description</h3>

<p>Picks k random starting points from given dataset to initialise k
clusters. Then, one by one, the point not yet assigned to any cluster
with smallest average dissimilarity to the points of any already
existing cluster is assigned to that
cluster, until all points are assigned. This is a random versione of
average linkage clustering, see
Akhanli and Hennig (2020).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  stupidkaven(d,k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stupidkaven_+3A_d">d</code></td>
<td>
<p><code>dist</code>-object or dissimilarity matrix.</p>
</td></tr>
<tr><td><code id="stupidkaven_+3A_k">k</code></td>
<td>
<p>integer. Number of clusters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The clustering vector (values 1 to <code>k</code>, length number of objects
behind <code>d</code>), 
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Akhanli, S. and Hennig, C. (2020) Calibrating and aggregating cluster
validity indexes for context-adapted comparison of clusterings.
<em>Statistics and Computing</em>, 30, 1523-1544,
<a href="https://link.springer.com/article/10.1007/s11222-020-09958-2">https://link.springer.com/article/10.1007/s11222-020-09958-2</a>, <a href="https://arxiv.org/abs/2002.01822">https://arxiv.org/abs/2002.01822</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stupidkcentroids">stupidkcentroids</a></code>, <code><a href="#topic+stupidknn">stupidknn</a></code>, <code><a href="#topic+stupidkfn">stupidkfn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(20000)
  options(digits=3)
  face &lt;- rFace(200,dMoNo=2,dNoEy=0,p=2)
  stupidkaven(dist(face),3) 
</code></pre>

<hr>
<h2 id='stupidkcentroids'>Stupid k-centroids random clustering</h2><span id='topic+stupidkcentroids'></span>

<h3>Description</h3>

<p>Picks k random centroids from given dataset and assigns every point to
closest centroid. This is called stupid k-centroids in Hennig (2019).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  stupidkcentroids(xdata, k, distances = inherits(xdata, "dist"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stupidkcentroids_+3A_xdata">xdata</code></td>
<td>
<p>cases*variables data, <code>dist</code>-object or dissimilarity
matrix, see <code>distances</code>.</p>
</td></tr>
<tr><td><code id="stupidkcentroids_+3A_k">k</code></td>
<td>
<p>integer. Number of clusters.</p>
</td></tr>
<tr><td><code id="stupidkcentroids_+3A_distances">distances</code></td>
<td>
<p>logical. If <code>TRUE</code>, <code>xdata</code> is interpreted
as distances.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>partition</code></td>
<td>
<p>vector if integers 1 to <code>k</code>, of length equal to
number of objects, indicates to which cluster an object belongs.</p>
</td></tr>
<tr><td><code>centroids</code></td>
<td>
<p>vector of integers of length <code>k</code>, indicating the
centroids of the clusters (observation number).</p>
</td></tr>
<tr><td><code>distances</code></td>
<td>
<p>as argument <code>distances</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2019) Cluster validation by measurement of clustering
characteristics relevant to the user. In C. H. Skiadas (ed.)
<em>Data Analysis and Applications 1: Clustering and Regression,
Modeling-estimating, Forecasting and Data Mining, Volume 2</em>, Wiley,
New York 1-24,
<a href="https://arxiv.org/abs/1703.09282">https://arxiv.org/abs/1703.09282</a>
</p>
<p>Akhanli, S. and Hennig, C. (2020) Calibrating and aggregating cluster
validity indexes for context-adapted comparison of clusterings.
<em>Statistics and Computing</em>, 30, 1523-1544,
<a href="https://link.springer.com/article/10.1007/s11222-020-09958-2">https://link.springer.com/article/10.1007/s11222-020-09958-2</a>, <a href="https://arxiv.org/abs/2002.01822">https://arxiv.org/abs/2002.01822</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stupidknn">stupidknn</a></code>, <code><a href="#topic+stupidkfn">stupidkfn</a></code>, <code><a href="#topic+stupidkaven">stupidkaven</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(20000)
  options(digits=3)
  face &lt;- rFace(200,dMoNo=2,dNoEy=0,p=2)
  stupidkcentroids(dist(face),3) 
</code></pre>

<hr>
<h2 id='stupidkfn'>Stupid farthest neighbour random clustering</h2><span id='topic+stupidkfn'></span>

<h3>Description</h3>

<p>Picks k random starting points from given dataset to initialise k
clusters. Then, one by one, a point not yet assigned to any cluster
is assigned to that
cluster, until all points are assigned. The point/cluster pair to be
used is picked according to the smallest distance of a point to the
farthest point to it in any of the already existing clusters as in
complete linkage clustering, see
Akhanli and Hennig (2020).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  stupidkfn(d,k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stupidkfn_+3A_d">d</code></td>
<td>
<p><code>dist</code>-object or dissimilarity matrix.</p>
</td></tr>
<tr><td><code id="stupidkfn_+3A_k">k</code></td>
<td>
<p>integer. Number of clusters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The clustering vector (values 1 to <code>k</code>, length number of objects
behind <code>d</code>), 
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Akhanli, S. and Hennig, C. (2020) Calibrating and aggregating cluster
validity indexes for context-adapted comparison of clusterings.
<em>Statistics and Computing</em>, 30, 1523-1544,
<a href="https://link.springer.com/article/10.1007/s11222-020-09958-2">https://link.springer.com/article/10.1007/s11222-020-09958-2</a>, <a href="https://arxiv.org/abs/2002.01822">https://arxiv.org/abs/2002.01822</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stupidkcentroids">stupidkcentroids</a></code>, <code><a href="#topic+stupidknn">stupidknn</a></code>, <code><a href="#topic+stupidkaven">stupidkaven</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(20000)
  options(digits=3)
  face &lt;- rFace(200,dMoNo=2,dNoEy=0,p=2)
  stupidkfn(dist(face),3) 
</code></pre>

<hr>
<h2 id='stupidknn'>Stupid nearest neighbour random clustering</h2><span id='topic+stupidknn'></span>

<h3>Description</h3>

<p>Picks k random starting points from given dataset to initialise k
clusters. Then, one by one, the point not yet assigned to any cluster
that is closest to an already assigned point is assigned to that
cluster, until all points are assigned. This is called stupid nearest
neighbour clustering in Hennig (2019).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  stupidknn(d,k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stupidknn_+3A_d">d</code></td>
<td>
<p><code>dist</code>-object or dissimilarity matrix.</p>
</td></tr>
<tr><td><code id="stupidknn_+3A_k">k</code></td>
<td>
<p>integer. Number of clusters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The clustering vector (values 1 to <code>k</code>, length number of objects
behind <code>d</code>), 
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2019) Cluster validation by measurement of clustering
characteristics relevant to the user. In C. H. Skiadas (ed.)
<em>Data Analysis and Applications 1: Clustering and Regression,
Modeling-estimating, Forecasting and Data Mining, Volume 2</em>, Wiley,
New York 1-24,
<a href="https://arxiv.org/abs/1703.09282">https://arxiv.org/abs/1703.09282</a>
</p>
<p>Akhanli, S. and Hennig, C. (2020) Calibrating and aggregating cluster
validity indexes for context-adapted comparison of clusterings.
<em>Statistics and Computing</em>, 30, 1523-1544,
<a href="https://link.springer.com/article/10.1007/s11222-020-09958-2">https://link.springer.com/article/10.1007/s11222-020-09958-2</a>, <a href="https://arxiv.org/abs/2002.01822">https://arxiv.org/abs/2002.01822</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stupidkcentroids">stupidkcentroids</a></code>, <code><a href="#topic+stupidkfn">stupidkfn</a></code>, <code><a href="#topic+stupidkaven">stupidkaven</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(20000)
  options(digits=3)
  face &lt;- rFace(200,dMoNo=2,dNoEy=0,p=2)
  stupidknn(dist(face),3) 
</code></pre>

<hr>
<h2 id='tdecomp'>Root of singularity-corrected eigenvalue decomposition</h2><span id='topic+tdecomp'></span>

<h3>Description</h3>

<p>Computes transposed eigenvectors of matrix <code>m</code> times diagonal of
square root of eigenvalues so that eigenvalues smaller than 1e-6 are
set to 1e-6.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  tdecomp(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tdecomp_+3A_m">m</code></td>
<td>
<p>a symmetric matrix of minimum format 2*2.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Thought for use in <code>discrcoord</code> only.</p>


<h3>Value</h3>

<p>a matrix.
</p>


<h3>Note</h3>

<p>Thought for use within <code><a href="#topic+discrcoord">discrcoord</a></code> only.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(10)
y &lt;- rnorm(10)
z &lt;- cov(cbind(x,y))
round(tdecomp(z),digits=2)
</code></pre>

<hr>
<h2 id='tonedata'>Tone perception data</h2><span id='topic+tonedata'></span>

<h3>Description</h3>

<p>The tone perception data stem
from an experiment of Cohen (1980) and have been analyzed in de Veaux
(1989).
A pure fundamental tone was played to a
trained musician. Electronically generated overtones were added, determined 
by a stretching ratio of <code>stretchratio</code>. <code>stretchratio=2.0</code>
corresponds to the harmonic pattern
usually heard in traditional definite pitched instruments. The musician was
asked to tune an adjustable tone to the octave above the fundamental tone.
<code>tuned</code> gives the ratio of the adjusted tone to the fundamental,
i.e. <code>tuned=2.0</code> would be the correct tuning for all
<code>stretchratio</code>-values.
The data analyzed here belong to 150 trials 
with the same musician. In the original study, there were four further
musicians. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(tonedata)</code></pre>


<h3>Format</h3>

<p>A data frame with 2 variables <code>stretchratio</code> and
<code>tuned</code> and 150 cases.</p>


<h3>Source</h3>

<p>Cohen, E. A. (1980) <em>Inharmonic tone
perception</em>. Unpublished Ph.D. dissertation, Stanford University</p>


<h3>References</h3>

<p>de Veaux, R. D. (1989) Mixtures of Linear Regressions,
<em>Computational Statistics and Data Analysis</em> 8, 227-245.
</p>

<hr>
<h2 id='unimodal.ind'>Is a fitted denisity unimodal or not?</h2><span id='topic+unimodal.ind'></span>

<h3>Description</h3>

<p>Checks whether a series of fitted density values (such as given out as
<code>y</code>-component of  <code><a href="stats.html#topic+density">density</a></code>) is unimodal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  unimodal.ind(y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unimodal.ind_+3A_y">y</code></td>
<td>
<p>numeric vector of fitted density values in order of
increasing x-values such as given out as
<code>y</code>-component of  <code><a href="stats.html#topic+density">density</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Logical. <code>TRUE</code> if unimodal.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>unimodal.ind(c(1,3,3,4,2,1,0,0))
</code></pre>

<hr>
<h2 id='valstat.object'>Cluster validation statistics - object</h2><span id='topic+valstat.object'></span>

<h3>Description</h3>

<p>The objects of class <code>"valstat"</code> store cluster validation
statistics from various clustering methods run with various numbers of
clusters.
</p>


<h3>Value</h3>

<p>A legitimate <code>valstat</code> object is a list. The format of the list
relies on the number of involved clustering methods, <code>nmethods</code>,
say, i.e., the length
of the <code>method</code>-component explained below. The first
<code>nmethods</code> elements of the <code>valstat</code>-list are just
numbered. These are themselves lists that are numbered between 1 and
the <code>maxG</code>-component defined below. Element <code>[[i]][[j]]</code>
refers to the clustering from clustering method number i with number
of clusters j. Every such element is a list
with components 
<code>avewithin, mnnd, cvnnd, maxdiameter, widestgap, sindex, minsep,
  asw, dindex, denscut, highdgap, pearsongamma, withinss, entropy</code>:
Further optional components are <code>pamc, kdnorm, kdunif,
    dmode, aggregated</code>. All these are cluster validation indexes, as
follows.
</p>
<table>
<tr><td><code>avewithin</code></td>
<td>
<p>average distance within clusters (reweighted so
that every observation, rather than every distance, has the same weight).</p>
</td></tr>
<tr><td><code>mnnd</code></td>
<td>
<p>average distance to <code>nnk</code>th nearest neighbour within
cluster. (<code>nnk</code> is a parameter of
<code><a href="#topic+cqcluster.stats">cqcluster.stats</a></code>, default 2.)</p>
</td></tr>
<tr><td><code>cvnnd</code></td>
<td>
<p>coefficient of variation of dissimilarities to
<code>nnk</code>th nearest wthin-cluster neighbour, measuring uniformity of
within-cluster densities, weighted over all clusters, see Sec. 3.7 of
Hennig (2019). (<code>nnk</code> is a parameter of
<code><a href="#topic+cqcluster.stats">cqcluster.stats</a></code>, default 2.)</p>
</td></tr>
<tr><td><code>maxdiameter</code></td>
<td>
<p>maximum cluster diameter.</p>
</td></tr>
<tr><td><code>widestgap</code></td>
<td>
<p>widest within-cluster gap or average of cluster-wise
widest within-cluster gap, depending on parameter <code>averagegap</code>
of <code><a href="#topic+cqcluster.stats">cqcluster.stats</a></code>, default <code>FALSE</code>.</p>
</td></tr>
<tr><td><code>sindex</code></td>
<td>
<p>separation index. Defined based on the distances for
every point to the
closest point not in the same cluster. The separation index is then
the mean of the smallest proportion <code>sepprob</code> (parameter of
<code><a href="#topic+cqcluster.stats">cqcluster.stats</a></code>, default 0.1) of these. See Hennig (2019).</p>
</td></tr>
<tr><td><code>minsep</code></td>
<td>
<p>minimum cluster separation.</p>
</td></tr>
<tr><td><code>asw</code></td>
<td>
<p>average silhouette
width. See <code><a href="cluster.html#topic+silhouette">silhouette</a></code>.</p>
</td></tr>
<tr><td><code>dindex</code></td>
<td>
<p>this index measures to what extent the density decreases
from the cluster mode to the outskirts; I-densdec in Sec. 3.6 of
Hennig (2019); low values are good.</p>
</td></tr>
<tr><td><code>denscut</code></td>
<td>
<p>this index measures whether cluster boundaries run
through density valleys; I-densbound in Sec. 3.6 of Hennig (2019); low
values are good.</p>
</td></tr>
<tr><td><code>highdgap</code></td>
<td>
<p>this measures whether there is a large within-cluster
gap with high density on both sides; I-highdgap in Sec. 3.6 of
Hennig (2019); low values are good.</p>
</td></tr>
<tr><td><code>pearsongamma</code></td>
<td>
<p>correlation between distances and a
0-1-vector where 0 means same cluster, 1 means different clusters.
&quot;Normalized gamma&quot; in Halkidi et al. (2001).</p>
</td></tr>
<tr><td><code>withinss</code></td>
<td>
<p>a generalisation of the within clusters sum
of squares (k-means objective function), which is obtained if
<code>d</code> is a Euclidean distance matrix.  For general distance
measures, this is half
the sum of the within cluster squared dissimilarities divided by the
cluster size.</p>
</td></tr>
<tr><td><code>entropy</code></td>
<td>
<p>entropy of the distribution of cluster memberships,
see Meila(2007).</p>
</td></tr>
<tr><td><code>pamc</code></td>
<td>
<p>average distance to cluster centroid, which is the
observation that minimises this average distance.</p>
</td></tr>
<tr><td><code>kdnorm</code></td>
<td>
<p>Kolmogorov distance between distribution of
within-cluster Mahalanobis
distances and appropriate chi-squared distribution, aggregated over
clusters (I am grateful to Agustin Mayo-Iscar for the idea).</p>
</td></tr>
<tr><td><code>kdunif</code></td>
<td>
<p>Kolmogorov distance between distribution of distances to
<code>dnnk</code>th nearest within-cluster neighbor and appropriate
Gamma-distribution, see Byers and Raftery (1998), aggregated over
clusters. <code>dnnk</code> is parameter <code>nnk</code> of
<code><a href="#topic+distrsimilarity">distrsimilarity</a></code>, corresponding to <code>dnnk</code> of
<code><a href="#topic+clusterbenchstats">clusterbenchstats</a></code>.</p>
</td></tr>
<tr><td><code>dmode</code></td>
<td>
<p>aggregated density mode index equal to
<code>0.75*dindex+0.25*highdgap</code> after standardisation by <code><a href="#topic+cgrestandard">cgrestandard</a></code>.</p>
</td></tr>
</table>
<p>Furthermore, a <code>valstat</code> object
has the following list components:   
</p>
<table>
<tr><td><code>maxG</code></td>
<td>
<p>maximum number of clusters.</p>
</td></tr>
<tr><td><code>minG</code></td>
<td>
<p>minimum number of clusters (list entries below that number
are empty lists).</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>vector of names (character strings) of clustering
CBI-functions, see <code><a href="#topic+kmeansCBI">kmeansCBI</a></code>.</p>
</td></tr>
<tr><td><code>name</code></td>
<td>
<p>vector of names (character strings) of clustering
methods. These can be user-chosen names (see argument
<code>methodsnames</code> in <code><a href="#topic+clusterbenchstats">clusterbenchstats</a></code>) and may
distinguish different methods run by the same CBI-function but with
different parameter values such as complete and average linkage for
<code><a href="#topic+hclustCBI">hclustCBI</a></code>.</p>
</td></tr>
<tr><td><code>statistics</code></td>
<td>
<p>vector of names (character strings) of cluster
validation indexes.</p>
</td></tr> 
</table>


<h3>GENERATION</h3>

<p>These objects are generated as part of the
<code><a href="#topic+clusterbenchstats">clusterbenchstats</a></code>-output.
</p>


<h3>METHODS</h3>

<p>The <code>valstat</code> class has methods for the following generic functions:
<code>print</code>, <code>plot</code>, see <code><a href="#topic+plot.valstat">plot.valstat</a></code>.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2019) Cluster validation by measurement of clustering
characteristics relevant to the user. In C. H. Skiadas (ed.)
<em>Data Analysis and Applications 1: Clustering and Regression,
Modeling-estimating, Forecasting and Data Mining, Volume 2</em>, Wiley,
New York 1-24,
<a href="https://arxiv.org/abs/1703.09282">https://arxiv.org/abs/1703.09282</a>
</p>
<p>Akhanli, S. and Hennig, C. (2020) Calibrating and aggregating cluster
validity indexes for context-adapted comparison of clusterings.
<em>Statistics and Computing</em>, 30, 1523-1544,
<a href="https://link.springer.com/article/10.1007/s11222-020-09958-2">https://link.springer.com/article/10.1007/s11222-020-09958-2</a>, <a href="https://arxiv.org/abs/2002.01822">https://arxiv.org/abs/2002.01822</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+clusterbenchstats">clusterbenchstats</a></code>,
<code><a href="#topic+plot.valstat">plot.valstat</a></code>.
</p>

<hr>
<h2 id='weightplots'>Ordered posterior plots</h2><span id='topic+weightplots'></span>

<h3>Description</h3>

<p>Ordered posterior plots for Gaussian mixture components, see Hennig (2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  weightplots(z, clusternumbers="all", clustercol=2,
                        allcol=grey(0.2+((1:ncol(z))-1)*
                          0.6/(ncol(z)-1)),
                        lty=rep(1,ncol(z)),clusterlwd=3,
                        legendposition="none",
                        weightcutoff=0.01,ask=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weightplots_+3A_z">z</code></td>
<td>
<p>matrix with rows corresponding to observations and columns
corresponding to mixture components. Entries are probabilities that
an observation has been generated by a mixture component. These will
normally be estimated a posteriori probabilities, as generated as
component <code>z</code> of the output object from
<code><a href="mclust.html#topic+summary.mclustBIC">summary.mclustBIC</a></code>.</p>
</td></tr>
<tr><td><code id="weightplots_+3A_clusternumbers">clusternumbers</code></td>
<td>
<p><code>"all"</code> or vector of integers. Numbers of
components for which plots are drawn.</p>
</td></tr>
<tr><td><code id="weightplots_+3A_clustercol">clustercol</code></td>
<td>
<p>colour used for the main components for which a
plot is drawn.</p>
</td></tr>
<tr><td><code id="weightplots_+3A_allcol">allcol</code></td>
<td>
<p>colours used for respective other components in plots in
which they are not main components.</p>
</td></tr>
<tr><td><code id="weightplots_+3A_lty">lty</code></td>
<td>
<p>line types for components.</p>
</td></tr>
<tr><td><code id="weightplots_+3A_clusterlwd">clusterlwd</code></td>
<td>
<p>numeric. Line width for main component.</p>
</td></tr>
<tr><td><code id="weightplots_+3A_legendposition">legendposition</code></td>
<td>
<p><code>"none"</code> or vector with two coordinates in
the plot, where a legend should be printed.</p>
</td></tr>
<tr><td><code id="weightplots_+3A_weightcutoff">weightcutoff</code></td>
<td>
<p>numeric between 0 and 1. Observations are only taken
into account for which the posterior probability for the main
component is larger than this.</p>
</td></tr>
<tr><td><code id="weightplots_+3A_ask">ask</code></td>
<td>
<p>logical. If <code>TRUE</code>, it sets <code>par(ask=TRUE)</code> in
the beginning and <code>par(ask=FALSE)</code> after all plots were showed.</p>
</td></tr>
<tr><td><code id="weightplots_+3A_...">...</code></td>
<td>
<p>further parameters to be passed on to <code><a href="graphics.html#topic+legend">legend</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Shows posterior probabilities for observations belonging to all
mixture components on the y-axis, with points ordered by posterior
probability for main component.
</p>


<h3>Value</h3>

<p>Invisible matrix of posterior probabilities <code>z</code> from
<code>mclustsummary</code>.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2010) Methods for merging Gaussian mixture components,
<em>Advances in Data Analysis and Classification</em>, 4, 3-34.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  require(mclust)
  require(MASS)
  data(crabs)
  dc &lt;- crabs[,4:8]
  cm &lt;- mclustBIC(crabs[,4:8],G=9,modelNames="EEE")
  scm &lt;- summary(cm,crabs[,4:8])
  weightplots(scm$z,clusternumbers=1:3,ask=FALSE)
  weightplots(scm$z,clusternumbers=1:3,allcol=1:9, ask=FALSE,
              legendposition=c(5,0.7))
# Remove ask=FALSE to have time to watch the plots.
</code></pre>

<hr>
<h2 id='wfu'>Weight function (for Mahalabobis distances)</h2><span id='topic+wfu'></span>

<h3>Description</h3>

<p>Function of the elements of <code>md</code>, which is 1 for arguments smaller
than <code>ca</code>, 0 for arguments larger than <code>ca2</code> and linear
(default: continuous) in between.
</p>
<p>Thought for use in <code>fixmahal</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wfu(md, ca, ca2, a1 = 1/(ca - ca2), a0 = -a1 * ca2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wfu_+3A_md">md</code></td>
<td>
<p>vector of positive numericals.</p>
</td></tr>
<tr><td><code id="wfu_+3A_ca">ca</code></td>
<td>
<p>positive numerical.</p>
</td></tr>
<tr><td><code id="wfu_+3A_ca2">ca2</code></td>
<td>
<p>positive numerical.</p>
</td></tr>
<tr><td><code id="wfu_+3A_a1">a1</code></td>
<td>
<p>numerical. Slope.</p>
</td></tr>
<tr><td><code id="wfu_+3A_a0">a0</code></td>
<td>
<p>numerical. Intercept.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of numericals between 0 and 1.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+fixmahal">fixmahal</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  md &lt;- seq(0,10,by=0.1)
  round(wfu(md,ca=5,ca2=8),digits=2)
</code></pre>

<hr>
<h2 id='xtable'>Partition crosstable with empty clusters</h2><span id='topic+xtable'></span>

<h3>Description</h3>

<p>This produces a crosstable between two integer vectors (partitions) of
the same length with a given maximum vector entry <code>k</code> so that the
size of the table is <code>k*k</code> with zeroes for missing entries
between 1 and <code>k</code> (the command <code><a href="base.html#topic+table">table</a></code> does pretty
much the same thing but will leave out missing entries). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xtable(c1,c2,k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xtable_+3A_c1">c1</code></td>
<td>
<p>vector of integers.</p>
</td></tr>
<tr><td><code id="xtable_+3A_c2">c2</code></td>
<td>
<p>vector of integers of same length as <code>c1</code>.</p>
</td></tr>
<tr><td><code id="xtable_+3A_k">k</code></td>
<td>
<p>integer. Must be larger or equal to maximum entry in
<code>c1</code> and <code>c2</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of dimensions <code>c(k,k)</code>. Entry <code>[i,j]</code> gives the
number of places in which <code>c1==i &amp; c2==j</code>. 
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+table">table</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  c1 &lt;- 1:3
  c2 &lt;- c(1,1,2)
  xtable(c1,c2,3)
</code></pre>

<hr>
<h2 id='zmisclassification.matrix'>Matrix of misclassification probabilities between mixture components</h2><span id='topic+zmisclassification.matrix'></span>

<h3>Description</h3>

<p>Matrix of misclassification probabilities in a mixture distribution
between two mixture components from estimated posterior probabilities
regardless of component parameters, see Hennig (2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>zmisclassification.matrix(z,pro=NULL,clustering=NULL,
                                      ipairs="all",symmetric=TRUE,
                                      stat="max")

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="zmisclassification.matrix_+3A_z">z</code></td>
<td>
<p>matrix of posterior probabilities for observations (rows) to
belong to mixture components (columns), so entries need to sum up to
1 for each row.</p>
</td></tr>
<tr><td><code id="zmisclassification.matrix_+3A_pro">pro</code></td>
<td>
<p>vector of component proportions, need to sum up to
1. Computed from <code>z</code> as default.</p>
</td></tr>
<tr><td><code id="zmisclassification.matrix_+3A_clustering">clustering</code></td>
<td>
<p>vector of integers giving the estimated mixture
components for every observation. Computed from <code>z</code> as
default.</p>
</td></tr>
<tr><td><code id="zmisclassification.matrix_+3A_ipairs">ipairs</code></td>
<td>
<p><code>"all"</code> or list of vectors of two integers. If
<code>ipairs="all"</code>, computations are carried out for all pairs of
components. Otherwise, ipairs gives the pairs of components for
which computations are carried out.</p>
</td></tr>
<tr><td><code id="zmisclassification.matrix_+3A_symmetric">symmetric</code></td>
<td>
<p>logical. If <code>TRUE</code>, the matrix is symmetrised,
see parameter <code>stat</code>.</p>
</td></tr>
<tr><td><code id="zmisclassification.matrix_+3A_stat">stat</code></td>
<td>
<p><code>"max"</code> or <code>"mean"</code>. The statistic by which the
two misclassification probabilities are aggregated if
<code>symmetric=TRUE</code>.</p>
</td></tr> 
</table>


<h3>Value</h3>

<p>A matrix with the (symmetrised, if required) misclassification
probabilities between each pair of mixture components. If
<code>symmetric=FALSE</code>, matrix entry <code>[i,j]</code> is the estimated
probability that an observation generated by component
<code>j</code> is classified to component <code>i</code> by maximum a posteriori rule.
</p>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>References</h3>

<p>Hennig, C. (2010) Methods for merging Gaussian mixture components,
<em>Advances in Data Analysis and Classification</em>, 4, 3-34.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+confusion">confusion</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(12345)
  m &lt;- rpois(20,lambda=5)
  dim(m) &lt;- c(5,4)
  m &lt;- m/apply(m,1,sum)
  round(zmisclassification.matrix(m,symmetric=FALSE),digits=2) 
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
