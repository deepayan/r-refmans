<!DOCTYPE html><html><head><title>Help for package ANN2</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ANN2}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ANN'><p>Rcpp module exposing C++ class ANN</p></a></li>
<li><a href='#autoencoder'><p>Train an Autoencoding Neural Network</p></a></li>
<li><a href='#compression_plot'><p>Compression plot</p></a></li>
<li><a href='#decode'><p>Decoding step</p></a></li>
<li><a href='#encode'><p>Encoding step</p></a></li>
<li><a href='#neuralnetwork'><p>Train a Neural Network</p></a></li>
<li><a href='#plot.ANN'><p>Plot training and validation loss</p></a></li>
<li><a href='#predict.ANN'><p>Make predictions for new data</p></a></li>
<li><a href='#print.ANN'><p>Print ANN</p></a></li>
<li><a href='#read_ANN'><p>Read ANN object from file</p></a></li>
<li><a href='#reconstruct'><p>Reconstruct data using trained ANN object of type autoencoder</p></a></li>
<li><a href='#reconstruction_plot'><p>Reconstruction plot</p></a></li>
<li><a href='#setActivParams'><p>Check user input related to activation functions</p></a></li>
<li><a href='#setData'><p>Check the input data</p></a></li>
<li><a href='#setLossParams'><p>Check user input related to loss function</p></a></li>
<li><a href='#setMeta'><p>Sets list with meta info, to be used in other checks</p></a></li>
<li><a href='#setNetworkParams'><p>Check user input related to network structure</p></a></li>
<li><a href='#setOptimParams'><p>Check user input related to optimizer</p></a></li>
<li><a href='#setTrainParams'><p>Check user input related to training</p></a></li>
<li><a href='#train'><p>Continue training of a Neural Network</p></a></li>
<li><a href='#write_ANN'><p>Write ANN object to file</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Artificial Neural Networks for Anomaly Detection</td>
</tr>
<tr>
<td>Version:</td>
<td>2.3.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-11-29</td>
</tr>
<tr>
<td>Author:</td>
<td>Bart Lammers</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Bart Lammers &lt;bart.f.lammers@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Training of neural networks for classification and regression tasks
    using mini-batch gradient descent. Special features include a function for 
    training autoencoders, which can be used to detect anomalies, and some 
    related plotting functions. Multiple activation functions are supported, 
    including tanh, relu, step and ramp. For the use of the step and ramp 
    activation functions in detecting anomalies using autoencoders, see 
    Hawkins et al. (2002) &lt;<a href="https://doi.org/10.1007%2F3-540-46145-0_17">doi:10.1007/3-540-46145-0_17</a>&gt;. Furthermore, 
    several loss functions are supported, including robust ones such as Huber 
    and pseudo-Huber loss, as well as L1 and L2 regularization. The possible 
    options for optimization algorithms are RMSprop, Adam and SGD with momentum.
    The package contains a vectorized C++ implementation that facilitates 
    fast training through mini-batch learning.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a> | file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/bflammers/ANN2">https://github.com/bflammers/ANN2</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++11</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.18), reshape2 (&ge; 1.4.3), ggplot2 (&ge; 3.0.0),
viridisLite (&ge; 0.3.0), methods</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, testthat</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-11-30 22:49:28 UTC; bartlammers</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-12-01 10:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='ANN'>Rcpp module exposing C++ class ANN</h2><span id='topic+ANN'></span><span id='topic+Rcpp_ANN-class'></span>

<h3>Description</h3>

<p>C++ class ANN is the work horse of this package</p>

<hr>
<h2 id='autoencoder'>Train an Autoencoding Neural Network</h2><span id='topic+autoencoder'></span>

<h3>Description</h3>

<p>Construct and train an Autoencoder by setting the target variables equal 
to the input variables. The number of nodes in the middle layer should be 
smaller than the number of input variables in X in order to create a 
bottleneck layer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autoencoder(
  X,
  hidden.layers,
  standardize = TRUE,
  loss.type = "squared",
  huber.delta = 1,
  activ.functions = "tanh",
  step.H = 5,
  step.k = 100,
  optim.type = "sgd",
  learn.rates = 1e-04,
  L1 = 0,
  L2 = 0,
  sgd.momentum = 0.9,
  rmsprop.decay = 0.9,
  adam.beta1 = 0.9,
  adam.beta2 = 0.999,
  n.epochs = 100,
  batch.size = 32,
  drop.last = TRUE,
  val.prop = 0.1,
  verbose = TRUE,
  random.seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autoencoder_+3A_x">X</code></td>
<td>
<p>matrix with explanatory variables</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_hidden.layers">hidden.layers</code></td>
<td>
<p>vector specifying the number of nodes in each layer. The
number of hidden layers in the network is implicitly defined by the length of
this vector. Set <code>hidden.layers</code> to <code>NA</code> for a network with no hidden 
layers</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_standardize">standardize</code></td>
<td>
<p>logical indicating if X and Y should be standardized before
training the network. Recommended to leave at <code>TRUE</code> for faster
convergence.</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_loss.type">loss.type</code></td>
<td>
<p>which loss function should be used. Options are &quot;squared&quot;, 
&quot;absolute&quot;, &quot;huber&quot; and &quot;pseudo-huber&quot;</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_huber.delta">huber.delta</code></td>
<td>
<p>used only in case of loss functions &quot;huber&quot; and &quot;pseudo-huber&quot;.
This parameter controls the cut-off point between quadratic and absolute loss.</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_activ.functions">activ.functions</code></td>
<td>
<p>character vector of activation functions to be used in 
each hidden layer. Possible options are 'tanh', 'sigmoid', 'relu', 'linear', 
'ramp' and 'step'. Should be either the size of the number of hidden layers
or equal to one. If a single activation type is specified, this type will be 
broadcasted across the hidden layers.</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_step.h">step.H</code></td>
<td>
<p>number of steps of the step activation function. Only applicable 
if activ.functions includes 'step'</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_step.k">step.k</code></td>
<td>
<p>parameter controlling the smoothness of the step activation 
function. Larger values lead to a less smooth step function. Only applicable 
if activ.functions includes 'step'.</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_optim.type">optim.type</code></td>
<td>
<p>type of optimizer to use for updating the parameters. Options 
are 'sgd', 'rmsprop' and 'adam'. SGD is implemented with momentum.</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_learn.rates">learn.rates</code></td>
<td>
<p>the size of the steps to make in gradient descent. If set 
too large, the optimization might not converge to optimal values. If set too 
small, convergence will be slow. Should be either the size of the number of 
hidden layers plus one or equal to one. If a single learn rate is specified, 
this learn rate will be broadcasted across the layers.</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_l1">L1</code></td>
<td>
<p>L1 regularization. Non-negative number. Set to zero for no regularization.</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_l2">L2</code></td>
<td>
<p>L2 regularization. Non-negative number. Set to zero for no regularization.</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_sgd.momentum">sgd.momentum</code></td>
<td>
<p>numeric value specifying how much momentum should be
used. Set to zero for no momentum, otherwise a value between zero and one.</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_rmsprop.decay">rmsprop.decay</code></td>
<td>
<p>level of decay in the rms term. Controls the strength
of the exponential decay of the squared gradients in the term that scales the
gradient before the parameter update. Common values are 0.9, 0.99 and 0.999</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_adam.beta1">adam.beta1</code></td>
<td>
<p>level of decay in the first moment estimate (the mean). 
The recommended value is 0.9</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_adam.beta2">adam.beta2</code></td>
<td>
<p>level of decay in the second moment estimate (the uncentered
variance). The recommended value is 0.999</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_n.epochs">n.epochs</code></td>
<td>
<p>the number of epochs to train. One epoch is a single iteration 
through the training data.</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_batch.size">batch.size</code></td>
<td>
<p>the number of observations to use in each batch. Batch learning
is computationally faster than stochastic gradient descent. However, large
batches might not result in optimal learning, see Efficient Backprop by LeCun 
for details.</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_drop.last">drop.last</code></td>
<td>
<p>logical. Only applicable if the size of the training set is not
perfectly devisible by the batch size. Determines if the last chosen observations
should be discarded (in the current epoch) or should constitute a smaller batch. 
Note that a smaller batch leads to a noisier approximation of the gradient.</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_val.prop">val.prop</code></td>
<td>
<p>proportion of training data to use for tracking the loss on a 
validation set during training. Useful for assessing the training process and
identifying possible overfitting. Set to zero for only tracking the loss on the 
training data.</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_verbose">verbose</code></td>
<td>
<p>logical indicating if additional information should be printed</p>
</td></tr>
<tr><td><code id="autoencoder_+3A_random.seed">random.seed</code></td>
<td>
<p>optional seed for the random number generator</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A function for training Autoencoders. During training, the network will learn a
generalised representation of the data (generalised since the middle layer
acts as a bottleneck, resulting in reproduction of only the most
important features of the data). As such, the network models the normal state 
of the data and therefore has a denoising property. This property can be 
exploited to detect anomalies by comparing input to reconstruction. If the 
difference (the reconstruction error) is large, the observation is a possible 
anomaly.
</p>


<h3>Value</h3>

<p>An <code>ANN</code> object. Use function <code>plot(&lt;object&gt;)</code> to assess
loss on training and optionally validation data during training process. Use
function <code>predict(&lt;object&gt;, &lt;newdata&gt;)</code> for prediction.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Autoencoder example
X &lt;- USArrests
AE &lt;- autoencoder(X, c(10,2,10), loss.type = 'pseudo-huber',
                  activ.functions = c('tanh','linear','tanh'),
                  batch.size = 8, optim.type = 'adam',
                  n.epochs = 1000, val.prop = 0)

# Plot loss during training
plot(AE)

# Make reconstruction and compression plots
reconstruction_plot(AE, X)
compression_plot(AE, X)

# Reconstruct data and show states with highest anomaly scores
recX &lt;- reconstruct(AE, X)
sort(recX$anomaly_scores, decreasing = TRUE)[1:5]

</code></pre>

<hr>
<h2 id='compression_plot'>Compression plot</h2><span id='topic+compression_plot'></span><span id='topic+compression_plot.ANN'></span>

<h3>Description</h3>

<p>plot compressed observation in pairwise dimensions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compression_plot(object, ...)

## S3 method for class 'ANN'
compression_plot(object, X, colors = NULL, jitter = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compression_plot_+3A_object">object</code></td>
<td>
<p>autoencoder object of class <code>ANN</code></p>
</td></tr>
<tr><td><code id="compression_plot_+3A_...">...</code></td>
<td>
<p>arguments to be passed to <code>jitter()</code></p>
</td></tr>
<tr><td><code id="compression_plot_+3A_x">X</code></td>
<td>
<p>data matrix with original values to be compressed and plotted</p>
</td></tr>
<tr><td><code id="compression_plot_+3A_colors">colors</code></td>
<td>
<p>optional vector of discrete colors</p>
</td></tr>
<tr><td><code id="compression_plot_+3A_jitter">jitter</code></td>
<td>
<p>logical specifying whether to apply jitter to the compressed 
values. Especially useful whith step activation function that clusters the 
compressions and reconstructions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Matrix plot of pairwise dimensions
</p>


<h3>Value</h3>

<p>Plots
</p>

<hr>
<h2 id='decode'>Decoding step</h2><span id='topic+decode'></span><span id='topic+decode.ANN'></span>

<h3>Description</h3>

<p>Decompress low-dimensional representation resulting from the nodes
of the middle layer. Output are the reconstructed inputs to function <code>encode()</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decode(object, ...)

## S3 method for class 'ANN'
decode(object, compressed, compression.layer = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="decode_+3A_object">object</code></td>
<td>
<p>Object of class <code>ANN</code></p>
</td></tr>
<tr><td><code id="decode_+3A_...">...</code></td>
<td>
<p>arguments to be passed down</p>
</td></tr>
<tr><td><code id="decode_+3A_compressed">compressed</code></td>
<td>
<p>Compressed data</p>
</td></tr>
<tr><td><code id="decode_+3A_compression.layer">compression.layer</code></td>
<td>
<p>Integer specifying which hidden layer is the 
compression layer. If NULL this parameter is inferred from the structure 
of the network (hidden layer with smallest number of nodes)</p>
</td></tr>
</table>

<hr>
<h2 id='encode'>Encoding step</h2><span id='topic+encode'></span><span id='topic+encode.ANN'></span>

<h3>Description</h3>

<p>Compress data according to trained replicator or autoencoder.
Outputs are the activations of the nodes in the middle layer for each 
observation in <code>newdata</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>encode(object, ...)

## S3 method for class 'ANN'
encode(object, newdata, compression.layer = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="encode_+3A_object">object</code></td>
<td>
<p>Object of class <code>ANN</code></p>
</td></tr>
<tr><td><code id="encode_+3A_...">...</code></td>
<td>
<p>arguments to be passed down</p>
</td></tr>
<tr><td><code id="encode_+3A_newdata">newdata</code></td>
<td>
<p>Data to compress</p>
</td></tr>
<tr><td><code id="encode_+3A_compression.layer">compression.layer</code></td>
<td>
<p>Integer specifying which hidden layer is the 
compression layer. If NULL this parameter is inferred from the structure 
of the network (hidden layer with smallest number of nodes)</p>
</td></tr>
</table>

<hr>
<h2 id='neuralnetwork'>Train a Neural Network</h2><span id='topic+neuralnetwork'></span>

<h3>Description</h3>

<p>Construct and train a Multilayer Neural Network for regression or 
classification
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neuralnetwork(
  X,
  y,
  hidden.layers,
  regression = FALSE,
  standardize = TRUE,
  loss.type = "log",
  huber.delta = 1,
  activ.functions = "tanh",
  step.H = 5,
  step.k = 100,
  optim.type = "sgd",
  learn.rates = 1e-04,
  L1 = 0,
  L2 = 0,
  sgd.momentum = 0.9,
  rmsprop.decay = 0.9,
  adam.beta1 = 0.9,
  adam.beta2 = 0.999,
  n.epochs = 100,
  batch.size = 32,
  drop.last = TRUE,
  val.prop = 0.1,
  verbose = TRUE,
  random.seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neuralnetwork_+3A_x">X</code></td>
<td>
<p>matrix with explanatory variables</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_y">y</code></td>
<td>
<p>matrix with dependent variables. For classification this should be 
a one-columns matrix containing the classes - classes will be one-hot encoded.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_hidden.layers">hidden.layers</code></td>
<td>
<p>vector specifying the number of nodes in each layer. The
number of hidden layers in the network is implicitly defined by the length of
this vector. Set <code>hidden.layers</code> to <code>NA</code> for a network with no hidden 
layers</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_regression">regression</code></td>
<td>
<p>logical indicating regression or classification. In case of 
TRUE (regression), the activation function in the last hidden layer will be the 
linear activation function (identity function). In case of FALSE (classification), 
the activation function in the last hidden layer will be the softmax, and the 
log loss function should be used.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_standardize">standardize</code></td>
<td>
<p>logical indicating if X and Y should be standardized before
training the network. Recommended to leave at <code>TRUE</code> for faster
convergence.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_loss.type">loss.type</code></td>
<td>
<p>which loss function should be used. Options are &quot;log&quot;,
&quot;squared&quot;, &quot;absolute&quot;, &quot;huber&quot; and &quot;pseudo-huber&quot;. The log loss function should 
be used for classification (regression = FALSE), and ONLY for classification.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_huber.delta">huber.delta</code></td>
<td>
<p>used only in case of loss functions &quot;huber&quot; and &quot;pseudo-huber&quot;.
This parameter controls the cut-off point between quadratic and absolute loss.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_activ.functions">activ.functions</code></td>
<td>
<p>character vector of activation functions to be used in 
each hidden layer. Possible options are 'tanh', 'sigmoid', 'relu', 'linear', 
'ramp' and 'step'. Should be either the size of the number of hidden layers
or equal to one. If a single activation type is specified, this type will be 
broadcasted across the hidden layers.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_step.h">step.H</code></td>
<td>
<p>number of steps of the step activation function. Only applicable 
if activ.functions includes 'step'.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_step.k">step.k</code></td>
<td>
<p>parameter controlling the smoothness of the step activation 
function. Larger values lead to a less smooth step function. Only applicable 
if activ.functions includes 'step'.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_optim.type">optim.type</code></td>
<td>
<p>type of optimizer to use for updating the parameters. Options 
are 'sgd', 'rmsprop' and 'adam'. SGD is implemented with momentum.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_learn.rates">learn.rates</code></td>
<td>
<p>the size of the steps to make in gradient descent. If set 
too large, the optimization might not converge to optimal values. If set too 
small, convergence will be slow. Should be either the size of the number of 
hidden layers plus one or equal to one. If a single learn rate is specified, 
this learn rate will be broadcasted across the layers.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_l1">L1</code></td>
<td>
<p>L1 regularization. Non-negative number. Set to zero for no regularization.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_l2">L2</code></td>
<td>
<p>L2 regularization. Non-negative number. Set to zero for no regularization.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_sgd.momentum">sgd.momentum</code></td>
<td>
<p>numeric value specifying how much momentum should be
used. Set to zero for no momentum, otherwise a value between zero and one.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_rmsprop.decay">rmsprop.decay</code></td>
<td>
<p>level of decay in the rms term. Controls the strength
of the exponential decay of the squared gradients in the term that scales the
gradient before the parameter update. Common values are 0.9, 0.99 and 0.999.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_adam.beta1">adam.beta1</code></td>
<td>
<p>level of decay in the first moment estimate (the mean). 
The recommended value is 0.9.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_adam.beta2">adam.beta2</code></td>
<td>
<p>level of decay in the second moment estimate (the uncentered
variance). The recommended value is 0.999.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_n.epochs">n.epochs</code></td>
<td>
<p>the number of epochs to train. One epoch is a single iteration 
through the training data.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_batch.size">batch.size</code></td>
<td>
<p>the number of observations to use in each batch. Batch learning
is computationally faster than stochastic gradient descent. However, large
batches might not result in optimal learning, see Efficient Backprop by LeCun 
for details.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_drop.last">drop.last</code></td>
<td>
<p>logical. Only applicable if the size of the training set is not 
perfectly devisible by the batch size. Determines if the last chosen observations
should be discarded (in the current epoch) or should constitute a smaller batch. 
Note that a smaller batch leads to a noisier approximation of the gradient.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_val.prop">val.prop</code></td>
<td>
<p>proportion of training data to use for tracking the loss on a 
validation set during training. Useful for assessing the training process and
identifying possible overfitting. Set to zero for only tracking the loss on the 
training data.</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_verbose">verbose</code></td>
<td>
<p>logical indicating if additional information should be printed</p>
</td></tr>
<tr><td><code id="neuralnetwork_+3A_random.seed">random.seed</code></td>
<td>
<p>optional seed for the random number generator</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A genereric function for training Neural Networks for classification and
regression problems. Various types of activation and loss functions are
supported, as well as  L1 and L2 regularization. Possible optimizer include
SGD (with or without momentum), RMSprop and Adam.
</p>


<h3>Value</h3>

<p>An <code>ANN</code> object. Use function <code>plot(&lt;object&gt;)</code> to assess
loss on training and optionally validation data during training process. Use
function <code>predict(&lt;object&gt;, &lt;newdata&gt;)</code> for prediction.
</p>


<h3>References</h3>

<p>LeCun, Yann A., et al. &quot;Efficient backprop.&quot; Neural networks:
Tricks of the trade. Springer Berlin Heidelberg, 2012. 9-48.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example on iris dataset
# Prepare test and train sets
random_draw &lt;- sample(1:nrow(iris), size = 100)
X_train     &lt;- iris[random_draw, 1:4]
y_train     &lt;- iris[random_draw, 5]
X_test      &lt;- iris[setdiff(1:nrow(iris), random_draw), 1:4]
y_test      &lt;- iris[setdiff(1:nrow(iris), random_draw), 5]

# Train neural network on classification task
NN &lt;- neuralnetwork(X = X_train, y = y_train, hidden.layers = c(5, 5),
                    optim.type = 'adam', learn.rates = 0.01, val.prop = 0)

# Plot the loss during training
plot(NN)

# Make predictions
y_pred &lt;- predict(NN, newdata = X_test)

# Plot predictions
correct &lt;- (y_test == y_pred$predictions)
plot(X_test, pch = as.numeric(y_test), col = correct + 2)

</code></pre>

<hr>
<h2 id='plot.ANN'>Plot training and validation loss</h2><span id='topic+plot.ANN'></span>

<h3>Description</h3>

<p><code>plot</code> Generate plots of the loss against epochs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ANN'
plot(x, max.points = 1000, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.ANN_+3A_x">x</code></td>
<td>
<p>Object of class <code>ANN</code></p>
</td></tr>
<tr><td><code id="plot.ANN_+3A_max.points">max.points</code></td>
<td>
<p>Maximum number of points to plot, set to NA, NULL or Inf to
include all points in the plot</p>
</td></tr>
<tr><td><code id="plot.ANN_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to plot</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A generic function for plot loss of neural net
</p>


<h3>Value</h3>

<p>Plots
</p>

<hr>
<h2 id='predict.ANN'>Make predictions for new data</h2><span id='topic+predict.ANN'></span>

<h3>Description</h3>

<p><code>predict</code> Predict class or value for new data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ANN'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.ANN_+3A_object">object</code></td>
<td>
<p>Object of class <code>ANN</code></p>
</td></tr>
<tr><td><code id="predict.ANN_+3A_newdata">newdata</code></td>
<td>
<p>Data to make predictions on</p>
</td></tr>
<tr><td><code id="predict.ANN_+3A_...">...</code></td>
<td>
<p>further arguments (not in use)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A genereric function for training neural nets
</p>


<h3>Value</h3>

<p>A list with predicted classes for classification and fitted probabilities
</p>

<hr>
<h2 id='print.ANN'>Print ANN</h2><span id='topic+print.ANN'></span>

<h3>Description</h3>

<p>Print info on trained Neural Network
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ANN'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.ANN_+3A_x">x</code></td>
<td>
<p>Object of class <code>ANN</code></p>
</td></tr>
<tr><td><code id="print.ANN_+3A_...">...</code></td>
<td>
<p>Further arguments</p>
</td></tr>
</table>

<hr>
<h2 id='read_ANN'>Read ANN object from file</h2><span id='topic+read_ANN'></span>

<h3>Description</h3>

<p>Deserialize ANN object from binary file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_ANN(file)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read_ANN_+3A_file">file</code></td>
<td>
<p>character specifying file path</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class ANN
</p>

<hr>
<h2 id='reconstruct'>Reconstruct data using trained ANN object of type autoencoder</h2><span id='topic+reconstruct'></span>

<h3>Description</h3>

<p><code>reconstruct</code> takes new data as input and reconstructs the observations using
a trained replicator or autoencoder object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reconstruct(object, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reconstruct_+3A_object">object</code></td>
<td>
<p>Object of class <code>ANN</code> created with <code>autoencoder()</code></p>
</td></tr>
<tr><td><code id="reconstruct_+3A_x">X</code></td>
<td>
<p>data matrix to reconstruct</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A genereric function for training neural nets
</p>


<h3>Value</h3>

<p>Reconstructed observations and anomaly scores (reconstruction errors)
</p>

<hr>
<h2 id='reconstruction_plot'>Reconstruction plot</h2><span id='topic+reconstruction_plot'></span><span id='topic+reconstruction_plot.ANN'></span>

<h3>Description</h3>

<p>plots original and reconstructed data points in a single plot with connecting 
lines between original value and corresponding reconstruction
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reconstruction_plot(object, ...)

## S3 method for class 'ANN'
reconstruction_plot(object, X, colors = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reconstruction_plot_+3A_object">object</code></td>
<td>
<p>autoencoder object of class <code>ANN</code></p>
</td></tr>
<tr><td><code id="reconstruction_plot_+3A_...">...</code></td>
<td>
<p>arguments to be passed down</p>
</td></tr>
<tr><td><code id="reconstruction_plot_+3A_x">X</code></td>
<td>
<p>data matrix with original values to be reconstructed and plotted</p>
</td></tr>
<tr><td><code id="reconstruction_plot_+3A_colors">colors</code></td>
<td>
<p>optional vector of discrete colors. The reconstruction errors
are are used as color if this argument is not specified</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Matrix plot of pairwise dimensions
</p>


<h3>Value</h3>

<p>Plots
</p>

<hr>
<h2 id='setActivParams'>Check user input related to activation functions</h2><span id='topic+setActivParams'></span>

<h3>Description</h3>

<p>Set and check activation parameter list
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setActivParams(activ.functions, step.H, step.k, meta)
</code></pre>

<hr>
<h2 id='setData'>Check the input data</h2><span id='topic+setData'></span>

<h3>Description</h3>

<p>Set and check data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setData(X, y, regression, y_names = NULL)
</code></pre>

<hr>
<h2 id='setLossParams'>Check user input related to loss function</h2><span id='topic+setLossParams'></span>

<h3>Description</h3>

<p>Set and check loss parameter list
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setLossParams(loss.type, huber.delta, meta)
</code></pre>

<hr>
<h2 id='setMeta'>Sets list with meta info, to be used in other checks</h2><span id='topic+setMeta'></span>

<h3>Description</h3>

<p>Set network meta info
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setMeta(data, hidden.layers, regression, autoencoder)
</code></pre>

<hr>
<h2 id='setNetworkParams'>Check user input related to network structure</h2><span id='topic+setNetworkParams'></span>

<h3>Description</h3>

<p>Set and check network parameter list
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setNetworkParams(hidden.layers, standardize, verbose, meta)
</code></pre>

<hr>
<h2 id='setOptimParams'>Check user input related to optimizer</h2><span id='topic+setOptimParams'></span>

<h3>Description</h3>

<p>Set and check optimizer parameter list
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setOptimParams(
  optim.type,
  learn.rates,
  L1,
  L2,
  sgd.momentum,
  rmsprop.decay,
  adam.beta1,
  adam.beta2,
  meta
)
</code></pre>

<hr>
<h2 id='setTrainParams'>Check user input related to training</h2><span id='topic+setTrainParams'></span>

<h3>Description</h3>

<p>Set training parameters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setTrainParams(n.epochs, batch.size, val.prop, drop.last, random.seed, data)
</code></pre>

<hr>
<h2 id='train'>Continue training of a Neural Network</h2><span id='topic+train'></span>

<h3>Description</h3>

<p>Continue training of a neural network object returned by <code>neuralnetwork()</code> 
or <code>autoencoder()</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train(
  object,
  X,
  y = NULL,
  n.epochs = 100,
  batch.size = 32,
  drop.last = TRUE,
  val.prop = 0.1,
  random.seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train_+3A_object">object</code></td>
<td>
<p>object of class <code>ANN</code> produced by <code>neuralnetwork()</code> 
or <code>autoencoder()</code></p>
</td></tr>
<tr><td><code id="train_+3A_x">X</code></td>
<td>
<p>matrix with explanatory variables</p>
</td></tr>
<tr><td><code id="train_+3A_y">y</code></td>
<td>
<p>matrix with dependent variables. Not required if object is an autoencoder</p>
</td></tr>
<tr><td><code id="train_+3A_n.epochs">n.epochs</code></td>
<td>
<p>the number of epochs to train. This parameter largely determines
the training time (one epoch is a single iteration through the training data).</p>
</td></tr>
<tr><td><code id="train_+3A_batch.size">batch.size</code></td>
<td>
<p>the number of observations to use in each batch. Batch learning
is computationally faster than stochastic gradient descent. However, large
batches might not result in optimal learning, see Efficient Backprop by Le Cun 
for details.</p>
</td></tr>
<tr><td><code id="train_+3A_drop.last">drop.last</code></td>
<td>
<p>logical. Only applicable if the size of the training set is not 
perfectly devisible by the batch size. Determines if the last chosen observations
should be discarded (in the current epoch) or should constitute a smaller batch. 
Note that a smaller batch leads to a noisier approximation of the gradient.</p>
</td></tr>
<tr><td><code id="train_+3A_val.prop">val.prop</code></td>
<td>
<p>proportion of training data to use for tracking the loss on a 
validation set during training. Useful for assessing the training process and
identifying possible overfitting. Set to zero for only tracking the loss on the 
training data.</p>
</td></tr>
<tr><td><code id="train_+3A_random.seed">random.seed</code></td>
<td>
<p>optional seed for the random number generator</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A new validation set is randomly chosen. This can result in irregular jumps
in the plot given by <code>plot.ANN()</code>.
</p>


<h3>Value</h3>

<p>An <code>ANN</code> object. Use function <code>plot(&lt;object&gt;)</code> to assess
loss on training and optionally validation data during training process. Use
function <code>predict(&lt;object&gt;, &lt;newdata&gt;)</code> for prediction.
</p>


<h3>References</h3>

<p>LeCun, Yann A., et al. &quot;Efficient backprop.&quot; Neural networks:
Tricks of the trade. Springer Berlin Heidelberg, 2012. 9-48.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Train a neural network on the iris dataset
X &lt;- iris[,1:4]
y &lt;- iris$Species
NN &lt;- neuralnetwork(X, y, hidden.layers = 10, sgd.momentum = 0.9, 
                    learn.rates = 0.01, val.prop = 0.3, n.epochs = 100)

# Plot training and validation loss during training
plot(NN)

# Continue training for 1000 epochs
train(NN, X, y, n.epochs = 200, val.prop = 0.3)

# Again plot the loss - note the jump in the validation loss at the 100th epoch
# This is due to the random selection of a new validation set
plot(NN)
</code></pre>

<hr>
<h2 id='write_ANN'>Write ANN object to file</h2><span id='topic+write_ANN'></span>

<h3>Description</h3>

<p>Serialize ANN object to binary file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_ANN(object, file)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write_ANN_+3A_object">object</code></td>
<td>
<p>Object of class <code>ANN</code></p>
</td></tr>
<tr><td><code id="write_ANN_+3A_file">file</code></td>
<td>
<p>character specifying file path</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
