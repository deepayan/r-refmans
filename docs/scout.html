<!DOCTYPE html><html><head><title>Help for package scout</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {scout}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#crossProdLasso'><p>Performs the lasso on the cross product matrices X'X and X'y</p></a></li>
<li><a href='#cv.scout'><p>Perform cross-validation for covariance-regularized regression, aka the Scout.</p></a></li>
<li><a href='#predict.scoutobject'><p>Prediction function for covariance-regularized regression, aka the Scout.</p></a></li>
<li><a href='#print.cvobject'><p>Print function for scout</p></a></li>
<li><a href='#print.scoutobject'><p>Print function for scout</p></a></li>
<li><a href='#scout'><p>Covariance-regularized regression, aka the Scout.</p></a></li>
<li><a href='#scout-package'>
<p>Implements covariance-regularized regression, aka the Scout Method.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Implements the Scout Method for Covariance-Regularized
Regression</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2015-07-09</td>
</tr>
<tr>
<td>Author:</td>
<td>Daniela M. Witten and Robert Tibshirani</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Daniela M. Witten &lt;dwitten@uw.edu&gt;</td>
</tr>
<tr>
<td>Imports:</td>
<td>glasso, stats, grDevices, graphics</td>
</tr>
<tr>
<td>Suggests:</td>
<td>lars</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements the Scout method for regression, described in "Covariance-regularized regression and classification for high-dimensional problems", by Witten and Tibshirani (2008),  Journal of the Royal Statistical Society, Series B 71(3): 615-636. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2015-07-09 16:41:41 UTC; dwitten</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2015-07-10 20:41:38</td>
</tr>
</table>
<hr>
<h2 id='crossProdLasso'>Performs the lasso on the cross product matrices X'X and X'y</h2><span id='topic+crossProdLasso'></span>

<h3>Description</h3>

<p>Perform L1-regularized regression of y onto X using only
the cross-product matrices X'X and X'y. In the case of
covariance-regularized regression, this is useful if you would like to
try out something other than L1 or L2 regularization of the inverse
covariance matrix.
</p>
<p>Suppose you
use your own method to regularize X'X. Then let Sigma denote your estimate of the population covariance matrix.  Now say you want to minimize
beta' Sigma  beta - 2 beta' X'y + lambda ||beta||_1
in order to get the regression estimate beta, which maximizes the
second scout criterion when an L_1 penalty is used. You can do this by
calling crossProdLasso(Sigma, X'y,rho).
</p>
<p>If you run crossProdLasso(X'X,X'y,rho) then it should give the same
result as lars(X,y)
</p>
<p>Notice that the xtx that you pass into this function must be POSITIVE
SEMI DEFINITE (or positive definite) or the problem is not convex and
the algorithm will not converge.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crossProdLasso(xtx,xty,rho,thr=1e-4,maxit=100,beta.init=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crossProdLasso_+3A_xtx">xtx</code></td>
<td>
<p>A pxp matrix, which should be an estimate of a covariance
matrix. This matrix must be POSITIVE
SEMI DEFINITE (or positive definite) or the problem is not convex and
the algorithm will not converge.</p>
</td></tr>
<tr><td><code id="crossProdLasso_+3A_xty">xty</code></td>
<td>
<p>A px1 vector, which is generally obtained via X'y.</p>
</td></tr>
<tr><td><code id="crossProdLasso_+3A_rho">rho</code></td>
<td>
<p>Must be non-negative; the regularization parameter you are
using. </p>
</td></tr>
<tr><td><code id="crossProdLasso_+3A_thr">thr</code></td>
<td>
<p>Convergence threshold.</p>
</td></tr>
<tr><td><code id="crossProdLasso_+3A_maxit">maxit</code></td>
<td>
<p>How many iterations to perform?</p>
</td></tr>
<tr><td><code id="crossProdLasso_+3A_beta.init">beta.init</code></td>
<td>
<p>If you're running this over a range of rho values,
then set beta.init equal to the solution you got for a previous rho
value. It will speed things up.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If your xtx is simply X'X for some X, and your xty is simple X'y
with some y, then the results will be the same as running lars on data
(X,y) for a single shrinkage parameter value.
</p>
<p>Note that when you use the scout function with p2=1, the crossProdLasso
function is
called internally to give the regression coefficients, after the
regularized inverse covariance matrix is estimated. It is provided
here in case it is useful to the user in other settings.
</p>


<h3>Value</h3>

<table>
<tr><td><code>beta</code></td>
<td>
<p>A px1 vector with the regression coefficients.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The FORTRAN code that this function links to was kindly written
and provided by Jerry Friedman.
</p>


<h3>Author(s)</h3>

<p>FORTRAN code by Jerry Friedman.   R interface by Daniela M. Witten and Robert Tibshirani</p>


<h3>References</h3>

<p>Witten, DM and Tibshirani, R (2008) Covariance-regularized
regression and classification for high-dimensional problems. Journal
of the Royal Statistical Society, Series B 71(3): 615-636. &lt;http://www-stat.stanford.edu/~dwitten&gt;</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
#data(diabetes)
#attach(diabetes)
x2 &lt;- matrix(rnorm(10*20),ncol=20)
y &lt;- rnorm(10)
# First, let's do scout(2,1) the usual way).
scout.out &lt;- scout(x2,y,p1=2,p2=1)
print(scout.out)



# Now, suppose I want to do develop a covariance-regularized regression
# method as in Section 3.2 of Witten and Tibshirani (2008). It will work
# like this:
# 1. Develop some positive definite estimate of Sigma
# 2. Find \beta by minimize \beta^T \Sigma \beta - 2 \beta^T X^T y +
# \lamda ||\beta||_1
# 3. Re-scale \beta.

# Step 1:
regcovx &lt;- cov(x2)*(abs(cov(x2))&gt;.005) + diag(ncol(x2))*.01

# Step 2:
betahat &lt;-  crossProdLasso(regcovx, cov(x2,y), rho=.02)$beta
# Step 3:
betahat.sc &lt;- betahat*lsfit(x2%*%betahat, y, intercept=FALSE)$coef
print(betahat.sc)

# Try a different value of rho:
betahat2 &lt;- crossProdLasso(regcovx,cov(x2,y),rho=.04,beta.init=betahat)$beta
plot(betahat,betahat2, xlab="rho=.02",ylab="rho=.04")
#detach(diabetes)
</code></pre>

<hr>
<h2 id='cv.scout'>Perform cross-validation for covariance-regularized regression, aka the Scout.</h2><span id='topic+cv.scout'></span>

<h3>Description</h3>

<p>This function returns cross-validation error rates for a range
of lambda1 and lambda2 values, and also makes beautiful CV plots if
plot=TRUE.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.scout(x, y, K= 10,
  lam1s=seq(0.001,.2,len=10),lam2s=seq(0.001,.2,len=10),p1=2,p2=1,
  trace = TRUE, plot=TRUE,plotSE=FALSE,rescale=TRUE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.scout_+3A_x">x</code></td>
<td>
<p>A matrix of predictors, where the rows are the samples and
the columns are the predictors</p>
</td></tr>
<tr><td><code id="cv.scout_+3A_y">y</code></td>
<td>
<p>A matrix of observations, where length(y) should equal
nrow(x)</p>
</td></tr>
<tr><td><code id="cv.scout_+3A_k">K</code></td>
<td>
<p>Number of cross-validation folds to be performed; default is
10</p>
</td></tr>
<tr><td><code id="cv.scout_+3A_lam1s">lam1s</code></td>
<td>
<p>The (vector of) tuning parameters for regularization of the
covariance matrix. Can be NULL if p1=NULL, since then no covariance
regularization is taking place. If p1=1 and nrow(x)&lt;ncol(x), then the no value in lam1s
should be smaller than 1e-3, because this will cause graphical lasso
to take too long. Also, if ncol(x)&gt;500 then we really do not
recommend using p1=1, as graphical lasso can be uncomfortably slow.</p>
</td></tr>
<tr><td><code id="cv.scout_+3A_lam2s">lam2s</code></td>
<td>
<p>The (vector of) tuning parameters for the $L_1$ regularization of
the regression coefficients, using the regularized covariacne
matrix. Can be NULL if p2=NULL. (If p2=NULL, then non-zero lam2s
have no effect). A value of 0 will result in no
regularization. </p>
</td></tr>
<tr><td><code id="cv.scout_+3A_p1">p1</code></td>
<td>
<p>The $L_p$ penalty for the covariance regularization. Must be
one of 1, 2, or NULL. NULL corresponds to no covariance
regularization. </p>
</td></tr>
<tr><td><code id="cv.scout_+3A_p2">p2</code></td>
<td>
<p>The $L_p$ penalty for the estimation of the regression
coefficients based on the regularized covariance matrix. Must be one
of 1 (for $L_1$ regularization) or NULL (for no regularization).</p>
</td></tr>
<tr><td><code id="cv.scout_+3A_trace">trace</code></td>
<td>
<p>Print out progress as we go? Default is TRUE.</p>
</td></tr>
<tr><td><code id="cv.scout_+3A_plot">plot</code></td>
<td>
<p>If TRUE (by default), makes beautiful CV plots.</p>
</td></tr>
<tr><td><code id="cv.scout_+3A_plotse">plotSE</code></td>
<td>
<p>Should those beautiful CV plots also display std error
bars for the CV? Default is FALSE</p>
</td></tr>
<tr><td><code id="cv.scout_+3A_rescale">rescale</code></td>
<td>
<p>Scout rescales coefficients, by default, in order to
avoid over-shrinkage</p>
</td></tr>
<tr><td><code id="cv.scout_+3A_...">...</code></td>
<td>
<p>Additional parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Pass in a data matrix x and a vector of outcomes y; it will perform
(10-fold) cross-validation over a range of lambda1 and lambda2
values. By default, Scout(2,1) is performed.
</p>


<h3>Value</h3>

<table>
<tr><td><code>folds</code></td>
<td>
<p>The indices of the members of the K test sets are
returned.</p>
</td></tr>
<tr><td><code>cv</code></td>
<td>
<p>A matrix of average cross-validation errors is returned.</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>A matrix containing the standard errors of the
elements in &quot;cv&quot;, the matrix of average cross-validation errors.</p>
</td></tr>
<tr><td><code>bestlam1</code></td>
<td>
<p>Best value of lam1 found via cross-validation.</p>
</td></tr>
<tr><td><code>bestlam2</code></td>
<td>
<p>Best value fo lam2 found via cross-validation.</p>
</td></tr>
<tr><td><code>lam1s</code></td>
<td>
<p>Values of lam1 considered.</p>
</td></tr>
<tr><td><code>lam2s</code></td>
<td>
<p>Values of lam2 considered.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Daniela M. Witten and Robert Tibshirani</p>


<h3>References</h3>

<p>Witten, DM and Tibshirani, R (2008) Covariance-regularized
regression and classification for high-dimensional problems. Journal
of the Royal Statistical Society, Series B 71(3): 615-636. &lt;http://www-stat.stanford.edu/~dwitten&gt;</p>


<h3>See Also</h3>

<p><a href="#topic+scout">scout</a>, <a href="#topic+predict.scoutobject">predict.scoutobject</a> </p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lars)
data(diabetes)
attach(diabetes)
par(mfrow=c(2,1))
par(mar=c(2,2,2,2))
## Not run: cv.sc &lt;- cv.scout(x2,y,p1=2,p2=1)
## Not run: print(cv.sc)
## Not run: cv.la &lt;- cv.lars(x2,y)
## Not run: print(c("Lars minimum CV is ", min(cv.la$cv)))
## Not run: print(c("Scout(2,1) minimum CV is ", min(cv.sc$cv)))
detach(diabetes)
</code></pre>

<hr>
<h2 id='predict.scoutobject'>Prediction function for covariance-regularized regression, aka the Scout.</h2><span id='topic+predict.scoutobject'></span>

<h3>Description</h3>

<p>A function to perform prediction, using an x matrix and the output of
the &quot;scout&quot; function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'scoutobject'
predict(object, newx, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.scoutobject_+3A_object">object</code></td>
<td>
<p>The results of a call to the &quot;scout&quot; function. The
coefficients that are part of this object will be used for
making predictions.</p>
</td></tr>
<tr><td><code id="predict.scoutobject_+3A_newx">newx</code></td>
<td>
<p>The new x at which predictions should be made. Can be a
vector of length ncol(x), where x is the data on which scout.obj was
created, or a matrix with ncol(x) columns.</p>
</td></tr>
<tr><td><code id="predict.scoutobject_+3A_...">...</code></td>
<td>
<p>Additional arguments to predict</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>yhat</code></td>
<td>
<p>If newx was a vector, then a  matrix will be returned,
with dimension length(lam1s)xlength(lam2s) (where lam1s and lam2s
are attributes of scout.obj). The (i,j) element of this matrix will
correspond to tuning parameter values (lam1s[i], lam2s[j]). If newx
is a matrix, then an array of dimension
nrow(newx)xlength(lam1s)xlength(lam2s) will be returned.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Daniela M. Witten and Robert Tibshirani</p>


<h3>References</h3>

<p>Witten, DM and Tibshirani, R (2008) Covariance-regularized
regression and classification for high-dimensional problems.  Journal
of the Royal Statistical Society, Series B 71(3): 615-636. &lt;http://www-stat.stanford.edu/~dwitten&gt;</p>


<h3>See Also</h3>

<p><a href="#topic+scout">scout</a>, <a href="#topic+cv.scout">cv.scout</a> </p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lars)
data(diabetes)
attach(diabetes)
# Split data into training and test set
training &lt;- sample(nrow(x2),floor(nrow(x2)/2))
xtrain &lt;- x2[training,]
ytrain &lt;- y[training]
xtest &lt;- x2[-training,]
ytest &lt;- y[-training]
# Done splitting data into training and test set
# Do cross-validation to determine best tuning parameter values for Scout(1,1)
## Not run: cv.out &lt;- cv.scout(xtrain,ytrain,p1=1,p2=1, lam1s=seq(0.001,.15,len=5),K=4)
## Not run: print(cv.out)
# Done cross-validation
## Fit Model
#scout.object &lt;- scout(xtrain,ytrain,p1=1,p2=1,lam1s=cv.out$bestlam1,lam2s=cv.out$bestlam2)
#print(scout.object)
## Done Fitting Model
## Predict on test data, and report MSE
#yhats &lt;- predict(scout.object,xtest)
#print(mean((yhats-ytest)^2))
detach(diabetes)
</code></pre>

<hr>
<h2 id='print.cvobject'>Print function for scout</h2><span id='topic+print.cvobject'></span>

<h3>Description</h3>

<p>A function to print CV output for scout
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cvobject'
print(x,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.cvobject_+3A_x">x</code></td>
<td>
<p>The results of a call to the &quot;cv.scout&quot; function.</p>
</td></tr>
<tr><td><code id="print.cvobject_+3A_...">...</code></td>
<td>
<p>Additional arguments; ignored.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Daniela M. Witten and Robert Tibshirani</p>


<h3>References</h3>

<p>Witten, DM and Tibshirani, R (2008) Covariance-regularized
regression and classification for high-dimensional problems.  Journal
of the Royal Statistical Society, Series B 71(3): 615-636.</p>


<h3>See Also</h3>

<p><a href="#topic+scout">scout</a>, <a href="#topic+cv.scout">cv.scout</a> </p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lars)
data(diabetes)
attach(diabetes)
# Split data into training and test set
training &lt;- sample(nrow(x2),floor(nrow(x2)/2))
xtrain &lt;- x2[training,]
ytrain &lt;- y[training]
# Done splitting data into training and test set
# Do cross-validation to determine best tuning parameter values for Scout(1,1)
## Not run: cv.out &lt;- cv.scout(xtrain,ytrain,p1=1,p2=1, lam1s=seq(0.001,0.1), K=4)
## Not run: print(cv.out)
# Done cross-validation
detach(diabetes)
</code></pre>

<hr>
<h2 id='print.scoutobject'>Print function for scout</h2><span id='topic+print.scoutobject'></span>

<h3>Description</h3>

<p>A function to print scout output
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'scoutobject'
print(x,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.scoutobject_+3A_x">x</code></td>
<td>
<p>The results of a call to the &quot;scout&quot; function.</p>
</td></tr>
<tr><td><code id="print.scoutobject_+3A_...">...</code></td>
<td>
<p>additional arguments; these are ignored.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Daniela M. Witten and Robert Tibshirani</p>


<h3>References</h3>

<p>Witten, DM and Tibshirani, R (2008) Covariance-regularized
regression and classification for high-dimensional problems.  Journal
of the Royal Statistical Society, Series B 71(3): 615-636. &lt;http://www-stat.stanford.edu/~dwitten&gt;</p>


<h3>See Also</h3>

<p><a href="#topic+scout">scout</a>, <a href="#topic+cv.scout">cv.scout</a> </p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lars)
data(diabetes)
attach(diabetes)
# Split data into training and test set
training &lt;- sample(nrow(x2),floor(nrow(x2)/2))
xtrain &lt;- x2[training,]
ytrain &lt;- y[training]
xtest &lt;- x2[-training,]
ytest &lt;- y[-training]
# Done splitting data into training and test set
# Fit Model
scout.object &lt;- scout(xtrain,ytrain,p1=1,p2=1,lam1s=c(0.001,0.1),lam2s=c(0.01,0.2))
print(scout.object)
# Done Fitting Model
detach(diabetes)
</code></pre>

<hr>
<h2 id='scout'>Covariance-regularized regression, aka the Scout.</h2><span id='topic+scout'></span>

<h3>Description</h3>

<p>The main function of the &quot;scout&quot; package. Performs
covariance-regularized regression. Required inputs are an x matrix of
features (the columns are the features) and a y vector of
observations. By default, Scout(2,1) is performed; however, $p_1$ and
$p_2$ can be specified (in which case Scout($p_1$, $p_2$) is
performed). Also, by default Scout is performed over a grid of lambda1
and lambda2 values, but a different grid of values (or individual
values, rather than an entire grid) can be specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scout(x,y,newx,p1=2,p2=1,lam1s=seq(.001,.2,len=10),lam2s=seq(.001,.2,len=10),
   rescale=TRUE, trace=TRUE,standardize=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scout_+3A_x">x</code></td>
<td>
<p>A matrix of predictors, where the rows are the samples and
the columns are the predictors</p>
</td></tr>
<tr><td><code id="scout_+3A_y">y</code></td>
<td>
<p>A matrix of observations, where length(y) should equal
nrow(x)</p>
</td></tr>
<tr><td><code id="scout_+3A_newx">newx</code></td>
<td>
<p>An *optional* argument, consisting of a matrix with
ncol(x) columns, at which one wishes to make predictions for each
(lam1,lam2) pair.</p>
</td></tr>
<tr><td><code id="scout_+3A_p1">p1</code></td>
<td>
<p>The $L_p$ penalty for the covariance regularization. Must be
one of 1, 2, or NULL. NULL corresponds to no covariance
regularization.  WARNING: When p1=1, and ncol(x)&gt;500, Scout can be
SLOW. We recommend that for very large data sets, you use Scout with
p1=2. Also, when ncol(x)&gt;nrow(x) and p1=1, then very small values of
lambda1 (lambda1 &lt; 1e-4) will cause problems with graphical lasso,
and so those values will be automatically increased to 1e-4. </p>
</td></tr>
<tr><td><code id="scout_+3A_p2">p2</code></td>
<td>
<p>The $L_p$ penalty for the estimation of the regression
coefficients based on the regularized covariance matrix. Must be one
of 1 (for $L_1$ regularization) or NULL (for no regularization).</p>
</td></tr>
<tr><td><code id="scout_+3A_lam1s">lam1s</code></td>
<td>
<p>The (vector of) tuning parameters for regularization of the
covariance matrix. Can be NULL if p1=NULL, since then no covariance
regularization is taking place. If p1=1 and nrow(x)&lt;ncol(x), then the no value in lam1s
should be smaller than 1e-3, because this will cause graphical lasso
to take too long. Also, if ncol(x)&gt;500 then we really do not
recommend using p1=1, as graphical lasso can be uncomfortably slow.</p>
</td></tr>
<tr><td><code id="scout_+3A_lam2s">lam2s</code></td>
<td>
<p>The (vector of) tuning parameters for the $L_1$ regularization of
the regression coefficients, using the regularized covariance
matrix. Can be NULL if p2=NULL. (If p2=NULL, then non-zero lam2s
have no effect). A value of 0 will result in no
regularization. </p>
</td></tr>
<tr><td><code id="scout_+3A_rescale">rescale</code></td>
<td>
<p>Should coefficients beta obtained by
covariance-regularized regression be re-scaled by a constant, given
by regressing $y$ onto $x beta$? This is done in Witten and
Tibshirani (2008) and is important for good performance. Default is
TRUE.</p>
</td></tr>
<tr><td><code id="scout_+3A_trace">trace</code></td>
<td>
<p>Print out progress? Prints out each time a lambda1 is
completed. This is a good idea, especially when
ncol(x) is large.</p>
</td></tr>
<tr><td><code id="scout_+3A_standardize">standardize</code></td>
<td>
<p>Should the columns of x be scaled to have standard deviation
1, and should y be scaled to have standard deviation 1, before
covariance-regularized regression is performed? This affects the
meaning of the penalties that are applied. In general,
standardization should be performed. Default is TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>intercepts</code></td>
<td>
<p>Returns a matrix of intercepts, of dimension length(lam1s)xlength(lam2s)</p>
</td></tr>
<tr><td><code>coefficients</code></td>
<td>
<p>Returns an array of coefficients, of dimension
length(lam1s)xlength(lam2s)xncol(x).</p>
</td></tr>
<tr><td><code>p1</code></td>
<td>
<p>p1 value used</p>
</td></tr>
<tr><td><code>p2</code></td>
<td>
<p>p2 value used</p>
</td></tr>
<tr><td><code>lam1s</code></td>
<td>
<p>lam1s used</p>
</td></tr>
<tr><td><code>lam2s</code></td>
<td>
<p>lam2s used</p>
</td></tr>
</table>


<h3>Note</h3>

<p>When p1=1 and ncol(x)&gt;500 or so, then Scout can be very slow!!
Please use p1=2 when ncol(x) is large.
</p>


<h3>Author(s)</h3>

<p>Daniela M. Witten and Robert Tibshirani</p>


<h3>References</h3>

<p>Witten, DM and Tibshirani, R (2008) Covariance-regularized
regression and classification for high-dimensional problems. Journal
of the Royal Statistical Society, Series B 71(3): 615-636. &lt;http://www-stat.stanford.edu/~dwitten&gt;</p>


<h3>See Also</h3>

<p><a href="#topic+predict.scoutobject">predict.scoutobject</a>, <a href="#topic+cv.scout">cv.scout</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lars)
data(diabetes)
attach(diabetes)
scout.out &lt;- scout(x2,y,p1=2,p2=1)
print(scout.out)
detach(diabetes)
</code></pre>

<hr>
<h2 id='scout-package'>
Implements covariance-regularized regression, aka the Scout Method.
</h2><span id='topic+scout-package'></span><span id='topic+scout-package'></span>

<h3>Description</h3>

<p> Functions for implementing covariance-regularize
regression.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> scout</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2008-11-20</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;=2) </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>The main function is &quot;scout&quot;, which takes in a data matrix x and an
outcome vector y and estimates regression coefficients for Scout(2,1)
for a range of tuning parameter values. Alternatively one can specify
other tuning parameter values  and one can also perform
Scout(1,1), Scout(2,.), or Scout(1,.). Cross-validation and prediction
functions also are available.
</p>


<h3>Author(s)</h3>

<p>Daniela Witten and Robert Tibshirani
</p>
<p>Maintainer: Daniela Witten &lt;dwitten@stanford.edu&gt;
</p>


<h3>References</h3>

<p>Witten and Tibshirani (2008) Covariance-regularized regression and
classification for high-dimensional problems. Journal of the Royal
Statistical Society, Series B 71(3): 615-636.
</p>


<h3>See Also</h3>

<p>&lt;http://www-stat.stanford.edu/~dwitten&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lars)
data(diabetes)
attach(diabetes)
## Not run: cv.out &lt;- cv.scout(x2,y,p1=1,p2=1,K=3)
## Not run: print(cv.out)
## Not run: out &lt;- scout(x2,y,p1=1,p2=1,lam1=cv.out$bestlam1,lam2=cv.out$bestlam2)
## Not run: coef &lt;- out$coef[1,1,]
detach(diabetes)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
