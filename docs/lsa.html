<!DOCTYPE html><html><head><title>Help for package lsa</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {lsa}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#alnumx'><p>Regular expression for removal of non-alphanumeric characters (saving special characters)</p></a></li>
<li><a href='#as.textmatrix'><p>Display a latent semantic space generated by Latent Semantic Analysis (LSA)</p></a></li>
<li><a href='#associate'><p>Find close terms in a textmatrix</p></a></li>
<li><a href='#corpora'><p>Corpora (Essay Scoring)</p></a></li>
<li><a href='#cosine'><p>Cosine Measure (Matrices)</p></a></li>
<li><a href='#dimcalc'><p>Dimensionality Calculation Routines (LSA)</p></a></li>
<li><a href='#fold_in'><p>Ex-post folding-in of textmatrices into an existing latent semantic space</p></a></li>
<li><a href='#lsa'><p>Create a vector space with Latent Semantic Analysis (LSA)</p></a></li>
<li><a href='#print.textmatrix'><p>Print a textmatrix (Matrices)</p></a></li>
<li><a href='#query'><p>Query (Matrices)</p></a></li>
<li><a href='#sample.textmatrix'><p>Create a random sample of files</p></a></li>
<li><a href='#specialchars'><p>List of special character html entities and their character replacement</p></a></li>
<li><a href='#stopwords'><p>Stopwordlists in German, English, Dutch, French, Polish, and Arab</p></a></li>
<li><a href='#summary.textmatrix'><p>Summary of a textmatrix (Matrices)</p></a></li>
<li><a href='#textmatrix'><p>Textmatrix (Matrices)</p></a></li>
<li><a href='#triples'><p>Bind Triples to a Textmatrix</p></a></li>
<li><a href='#weightings'><p>Weighting Schemes (Matrices)</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Latent Semantic Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>0.73.3</td>
</tr>
<tr>
<td>Author:</td>
<td>Fridolin Wild</td>
</tr>
<tr>
<td>Description:</td>
<td>The basic idea of latent semantic analysis (LSA) is, 
  that text do have a higher order (=latent semantic) structure which, 
  however, is obscured by word usage (e.g. through the use of synonyms 
  or polysemy). By using conceptual indices that are derived statistically 
  via a truncated singular value decomposition (a two-mode factor analysis) 
  over a given document-term matrix, this variability problem can be overcome. </td>
</tr>
<tr>
<td>Depends:</td>
<td>SnowballC</td>
</tr>
<tr>
<td>Suggests:</td>
<td>tm</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Fridolin Wild &lt;wild@brookes.ac.uk&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>BuildResaveData:</td>
<td>no</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-05-09 06:24:58 UTC; hornik</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-05-09 06:58:26 UTC</td>
</tr>
</table>
<hr>
<h2 id='alnumx'>Regular expression for removal of non-alphanumeric characters (saving special characters)</h2><span id='topic+alnumx'></span>

<h3>Description</h3>

<p>This character string contains a regular expression for use in <code>gsub</code> deployed in <code>textvector</code> that identifies all alphanumeric characters (including language specific special characters not included in <code>[:alnum:]</code>, currently only the ones found in German and Polish.
You can use this expression by loading it with <code>data(alnumx)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   data(alnumx)
</code></pre>


<h3>Format</h3>

<p>Vector of type character.</p>


<h3>Author(s)</h3>

<p> Fridolin Wild <a href="mailto:f.wild@open.ac.uk">f.wild@open.ac.uk</a></p>

<hr>
<h2 id='as.textmatrix'>Display a latent semantic space generated by Latent Semantic Analysis (LSA)</h2><span id='topic+as.textmatrix'></span>

<h3>Description</h3>

<p>Returns a latent semantic space (created by createLSAspace) in
textmatrix format: rows are terms, columns are documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   as.textmatrix( LSAspace )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.textmatrix_+3A_lsaspace">LSAspace</code></td>
<td>
<p>a latent semantic space generated by createLSAspace.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To allow comparisons between terms and documents, the internal
format of the latent semantic space needs to be converted to
a classical document-term matrix (just like the ones generated by
<code>textmatrix()</code> that are of class &lsquo;textmatrix&rsquo;).
</p>
<p>Remark: There are other ways to compare documents and terms using 
the partial matrices from an LSA space directly. See (Berry, 1995)
for more information.
</p>


<h3>Value</h3>

<table>
<tr><td><code>textmatrix</code></td>
<td>
<p>a textmatrix representation of the latent semantic space.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Fridolin Wild <a href="mailto:f.wild@open.ac.uk">f.wild@open.ac.uk</a> </p>


<h3>References</h3>

<p>Berry, M., Dumais, S., and O'Brien, G (1995) <em>Using Linear Algebra for Intelligent Information Retrieval</em>. In: SIAM Review, Vol. 37(4), pp.573&ndash;595. </p>


<h3>See Also</h3>

 <p><code><a href="#topic+textmatrix">textmatrix</a></code>, <code><a href="#topic+lsa">lsa</a></code>, <code><a href="#topic+fold_in">fold_in</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
# create some files
td = tempfile()
dir.create(td)
write( c("dog", "cat", "mouse"), file=paste(td, "D1", sep="/"))
write( c("hamster", "mouse", "sushi"), file=paste(td, "D2", sep="/"))
write( c("dog", "monster", "monster"), file=paste(td, "D3", sep="/"))
write( c("dog", "mouse", "dog"), file=paste(td, "D4", sep="/"))

# read files into a document-term matrix
myMatrix = textmatrix(td, minWordLength=1)

# create the latent semantic space
myLSAspace = lsa(myMatrix, dims=dimcalc_raw()) 

# display it as a textmatrix again
round(as.textmatrix(myLSAspace),2) # should give the original

# create the latent semantic space
myLSAspace = lsa(myMatrix, dims=dimcalc_share()) 

# display it as a textmatrix again
myNewMatrix = as.textmatrix(myLSAspace) 
myNewMatrix # should look be different!

# compare two terms with the cosine measure
cosine(myNewMatrix["dog",], myNewMatrix["cat",])

# compare two documents with pearson
cor(myNewMatrix[,1], myNewMatrix[,2], method="pearson")

# clean up
unlink(td, recursive=TRUE)

</code></pre>

<hr>
<h2 id='associate'>Find close terms in a textmatrix</h2><span id='topic+associate'></span>

<h3>Description</h3>

<p>Returns those terms above a threshold
close to the input term, sorted in descending 
order of their closeness. Alternatively,
all terms and their closeness value can
be returned sorted descending.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>associate(textmatrix, term, measure = "cosine", threshold = 0.7)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="associate_+3A_textmatrix">textmatrix</code></td>
<td>
<p>A document-term matrix.</p>
</td></tr>
<tr><td><code id="associate_+3A_term">term</code></td>
<td>
<p>The stimulus 'word'.</p>
</td></tr>
<tr><td><code id="associate_+3A_measure">measure</code></td>
<td>
<p>The closeness measure to choose (Pearson, Spearman, Cosine)</p>
</td></tr>
<tr><td><code id="associate_+3A_threshold">threshold</code></td>
<td>
<p>Terms being closer than this threshold are going to be returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Internally, a complete term-to-term similarity table
is calculated, denoting the closeness (calculated with 
the specified measure) in its cells. All
terms being close above this specified threshold are returned,
sorted by their closeness value. Select a threshold of 0 to
get all terms. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>termlist</code></td>
<td>
<p>A named vector of closeness values (terms as labels, sorted in descending order).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Fridolin Wild <a href="mailto:f.wild@open.ac.uk">f.wild@open.ac.uk</a> </p>


<h3>See Also</h3>

 <p><code><a href="#topic+textmatrix">textmatrix</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
# create some files
td = tempfile()
dir.create(td)
write( c("dog", "cat", "mouse"), file=paste(td, "D1", sep="/"))
write( c("hamster", "mouse", "sushi"), file=paste(td, "D2", sep="/"))
write( c("dog", "monster", "monster"), file=paste(td, "D3", sep="/"))
write( c("dog", "mouse", "dog"), file=paste(td, "D4", sep="/"))

# create matrices
myMatrix = textmatrix(td, minWordLength=1)
myLSAspace = lsa(myMatrix, dims=dimcalc_share()) 
myNewMatrix = as.textmatrix(myLSAspace) 

# calc associations for mouse
associate(myNewMatrix, "mouse")

# clean up
unlink(td, recursive=TRUE)

</code></pre>

<hr>
<h2 id='corpora'>Corpora (Essay Scoring)</h2><span id='topic+corpus_training'></span><span id='topic+corpus_essays'></span><span id='topic+corpus_scores'></span>

<h3>Description</h3>

<p>This data sets contain example corpora for essay scoring.
A training textmatrix contains files to construct a 
latent semantic space apt for grading student essays
provided in the essay textmatrix. In a separate data
set, the original human scores are noted down with
which the student essays were graded by a human
assessor. The corpora (and human scores) can be loaded by
calling <code>data(corpus_training)</code>, <code>data(corpus_essays)</code>, or
<code>data(corpus_scores)</code>. The objects must already 
exist <em>before</em> being handed over to e.g. <code>lsa()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   data(corpus_training)
   data(corpus_essays)
   data(corpus_scores)
</code></pre>


<h3>Format</h3>

<p> Corpora: textmatrix; Scores: table.</p>


<h3>Author(s)</h3>

<p> Fridolin Wild <a href="mailto:f.wild@open.ac.uk">f.wild@open.ac.uk</a> </p>

<hr>
<h2 id='cosine'>Cosine Measure (Matrices)</h2><span id='topic+cosine'></span>

<h3>Description</h3>

<p>Calculates the cosine measure between two vectors or between all column vectors of a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cosine(x, y = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cosine_+3A_x">x</code></td>
<td>
<p>A vector or a matrix (e.g., a document-term matrix).</p>
</td></tr>
<tr><td><code id="cosine_+3A_y">y</code></td>
<td>
<p>Optional: a vector with compatible dimensions to <code>x</code>. If &lsquo;NULL&rsquo;, all column vectors of <code>x</code> are correlated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>cosine()</code> calculates a similarity matrix between all column
vectors of a matrix <code>x</code>. This matrix might be a document-term
matrix, so columns would be expected to be documents and
rows to be terms.
</p>
<p>When executed on two vectors <code>x</code> and <code>y</code>, 
<code>cosine()</code> calculates the cosine similarity between them.
</p>


<h3>Value</h3>

<p>Returns a <code class="reqn">n*n</code> similarity matrix of cosine values, comparing all 
<code class="reqn">n</code> column vectors against each other. Executed on two vectors, their 
cosine similarity value is returned.
</p>


<h3>Note</h3>

<p>The cosine measure is nearly identical with the pearson correlation 
coefficient (besides a constant factor) <code>cor(method="pearson")</code>. 
For an investigation on the differences in the context of textmining see
(Leydesdorff, 2005).
</p>


<h3>Author(s)</h3>

<p> Fridolin Wild <a href="mailto:f.wild@open.ac.uk">f.wild@open.ac.uk</a> </p>


<h3>References</h3>

<p>Leydesdorff, L. (2005) <em>Similarity Measures, Author Cocitation Analysis,and Information Theory</em>. In: JASIST 56(7), pp.769-772.</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+cor">cor</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
## the cosinus measure between two vectors

vec1 = c( 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0 )
vec2 = c( 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0 )
cosine(vec1,vec2) 


## the cosine measure for all document vectors of a matrix

vec3 = c( 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0 )
matrix = cbind(vec1,vec2, vec3)
cosine(matrix)


</code></pre>

<hr>
<h2 id='dimcalc'>Dimensionality Calculation Routines (LSA)</h2><span id='topic+dimcalc'></span><span id='topic+dimcalc_share'></span><span id='topic+dimcalc_ndocs'></span><span id='topic+dimcalc_kaiser'></span><span id='topic+dimcalc_raw'></span><span id='topic+dimcalc_fraction'></span>

<h3>Description</h3>

<p>Methods for choosing a &lsquo;good&rsquo; number of singular values for the dimensionality reduction in LSA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   dimcalc_share(share=0.5)
   dimcalc_ndocs(ndocs)
   dimcalc_kaiser()
   dimcalc_raw()
   dimcalc_fraction(frac=(1/50))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dimcalc_+3A_share">share</code></td>
<td>
<p>Optional: a fraction of the sum of the selected singular values to the sum of all singular values (default: 0.5). Only needed by <code>dimcalc\_share</code>.</p>
</td></tr>
<tr><td><code id="dimcalc_+3A_frac">frac</code></td>
<td>
<p>Optional: a fraction of the number of the singular values to be used (default: 1/50th).</p>
</td></tr>
<tr><td><code id="dimcalc_+3A_ndocs">ndocs</code></td>
<td>
<p>Optional: the number of documents (only needed for <code>dimcalc\_ndocs()</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In an LSA process, the diagonal matrix of the singular value decomposition is 
usually reduced to a specific number of dimensions (also &lsquo;factors&rsquo; or &lsquo;singular values&rsquo;).
</p>
<p>The functions <code>dimcalc\_share()</code>, <code>dimcalc\_ndocs()</code>, <code>dimcalc\_kaiser()</code>
and also the redundant function <code>dimcalc\_raw()</code> offer methods to calculate a useful
number of singular values (based on the distribution and values of the given sequence 
of singular values).
</p>
<p>All of them are tightly coupled to the core LSA functions: they generates 
a function to be executed by the calling (higher-level) 
function <code>lsa()</code>. The output function contains only one parameter, 
namely <code>s</code>, which is expected to be the sequence of singular values. 
In <code>lsa()</code>, the code returned is executed, the mandatory 
singular values are provided as a parameter within <code>lsa()</code>.
</p>
<p>The dimensionality calculation methods, however, can still be called directly
by adding a second, separate parameter set: e.g.
</p>
<p><code>dimcalc\_share(share=0.2)(mysingularvalues)</code>
</p>
<p>The method <code>dimcalc\_share()</code> finds the first position in the descending sequence of 
singular values <code>s</code> where their sum (divided by the sum of all 
values) meets or exceeds the specified share.
</p>
<p>The method <code>dimcalc\_ndocs()</code> calculates the first position in the descending sequence
of singular values where their sum meets or exceeds the number of documents.
</p>
<p>The method <code>dimcalc\_kaiser()</code> calculates the number of singular values according to the 
Kaiser-Criterium, i.e. from the descending order of values all values 
with <code>s[n] &gt; 1</code> will be taken. The number of dimensions is returned
accordingly.
</p>
<p>The method <code>dimcalc_fraction()</code> returns the specified share of the
number of singular values. Per default, 1/50th of the available values
will be used and the determined number of singular values will be returned.
</p>
<p>The method <code>dimcalc\_raw()</code> return the maximum number of singular values (= the length 
of <code>s</code>). It is here only for completeness.
</p>


<h3>Value</h3>

<p>Returns a function that takes the singular values as a parameter to
return the recommended number of dimensions. The expected parameter 
of this function is 
</p>
<table>
<tr><td><code>s</code></td>
<td>
<p>A sequence of singular values (as produced by the SVD). Only needed when calling the dimensionality calculation routines directly.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Fridolin Wild <a href="mailto:f.wild@open.ac.uk">f.wild@open.ac.uk</a> </p>


<h3>References</h3>

<p>Wild, F., Stahl, C., Stermsek, G., Neumann, G., Penya, Y. (2005) <em>Parameters Driving Effectiveness of Automated Essay Scoring with LSA</em>. In: Proceedings of the 9th CAA, pp.485-494, Loughborough </p>


<h3>See Also</h3>

<p><code><a href="#topic+lsa">lsa</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
## create some data 
vec1 = c( 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0 )
vec2 = c( 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0 )
vec3 = c( 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0 )
matrix = cbind(vec1,vec2, vec3)
s = svd(matrix)$d

# standard share of 0.5
dimcalc_share()(s) 

# specific share of 0.9
dimcalc_share(share=0.9)(s) 

# meeting the number of documents (here: 3)
n = ncol(matrix)
dimcalc_ndocs(n)(s)

</code></pre>

<hr>
<h2 id='fold_in'>Ex-post folding-in of textmatrices into an existing latent semantic space</h2><span id='topic+fold_in'></span>

<h3>Description</h3>

<p>Additional documents can be mapped into a pre-exisiting
latent semantic space without influencing the factor
distribution of the space. Applied, when additional documents 
must not influence the calculated existing latent semantic 
factor structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fold_in( docvecs, LSAspace )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fold_in_+3A_lsaspace">LSAspace</code></td>
<td>
<p>a latent semantic space generated by createLSAspace.</p>
</td></tr>
<tr><td><code id="fold_in_+3A_docvecs">docvecs</code></td>
<td>
<p>a textmatrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To keep additional documents from influencing the factor distribution
calculated previously from a particular text basis, they can be folded-in 
after the singular value decomposition performed in <code>lsa()</code>.
</p>
<p>Background Information:
For folding-in, a pseudo document vector <code>mi</code> of the new documents 
is calculated into as shown in the equations (1) and (2) (cf. Berry et al., 1995):
</p>
<p>(1) <code class="reqn">\hat{d} = v^T T_k S_k^{-1}</code>
</p>
<p>(2) <code class="reqn">\hat{m} = T_k S_k \hat{d}</code>
</p>
<p>The document vector <code class="reqn">v^T</code> in equation~(1) is identical to an additional 
column of an input textmatrix <code class="reqn">M</code> with the term frequencies of the 
essay to be folded-in. <code class="reqn">T_k</code> and <code class="reqn">S_k</code> are the truncated matrices 
from the SVD applied through <code>lsa()</code> on a given text 
collection to construct the latent semantic space. The resulting vector
<code class="reqn">\hat{m}</code> from equation~(2) is identical to an additional column in the
textmatrix representation of the latent semantic space (as produced by 
<code>as.textmatrix()</code>). Be careful when using weighting schemes: you
may want to use the global weights of the training textmatrix also for
your new data that you fold-in!
</p>


<h3>Value</h3>

<table>
<tr><td><code>textmatrix</code></td>
<td>
<p>a textmatrix representation of the additional documents in the latent semantic space.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Fridolin Wild <a href="mailto:f.wild@open.ac.uk">f.wild@open.ac.uk</a> </p>


<h3>See Also</h3>

 <p><code><a href="#topic+textmatrix">textmatrix</a></code>, <code><a href="#topic+lsa">lsa</a></code>, <code><a href="#topic+as.textmatrix">as.textmatrix</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
# create a first textmatrix with some files
td = tempfile()
dir.create(td)
write( c("dog", "cat", "mouse"), file=paste(td, "D1", sep="/") )
write( c("hamster", "mouse", "sushi"), file=paste(td, "D2", sep="/") )
write( c("dog", "monster", "monster"), file=paste(td, "D3", sep="/") )
matrix1 = textmatrix(td, minWordLength=1)
unlink(td, recursive=TRUE)

# create a second textmatrix with some more files
td = tempfile()
dir.create(td)
write( c("cat", "mouse", "mouse"), file=paste(td, "A1", sep="/") )
write( c("nothing", "mouse", "monster"), file=paste(td, "A2", sep="/") )
write( c("cat", "monster", "monster"), file=paste(td, "A3", sep="/") )
matrix2 = textmatrix(td, vocabulary=rownames(matrix1), minWordLength=1)
unlink(td, recursive=TRUE)

# create an LSA space from matrix1
space1 = lsa(matrix1, dims=dimcalc_share())
as.textmatrix(space1)

# fold matrix2 into the space generated by matrix1
fold_in( matrix2, space1)

</code></pre>

<hr>
<h2 id='lsa'>Create a vector space with Latent Semantic Analysis (LSA)</h2><span id='topic+lsa'></span>

<h3>Description</h3>

<p>Calculates a latent semantic space from a given document-term matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   lsa( x, dims=dimcalc_share() )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lsa_+3A_x">x</code></td>
<td>
<p>a document-term matrix (recommeded to be of class textmatrix), containing documents in 
colums, terms in rows and occurrence frequencies in the cells.</p>
</td></tr>
<tr><td><code id="lsa_+3A_dims">dims</code></td>
<td>
<p>either the number of dimensions or a configuring function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>LSA combines the classical vector space model &mdash; well known in 
textmining &mdash; with a Singular Value Decomposition (SVD), a two-mode 
factor analysis. Thereby, bag-of-words representations of texts can 
be mapped into a modified vector space that is assumed to reflect 
semantic structure.
</p>
<p>With <code>lsa()</code> a new latent semantic space can
be constructed over a given document-term matrix. To ease
comparisons of terms and documents with common
correlation measures, the space can be converted into
a textmatrix of the same format as <code>y</code> 
by calling <code>as.textmatrix()</code>.
</p>
<p>To add more documents or queries to this latent semantic
space in order to keep them from influencing the original 
factor distribution (i.e., the latent semantic structure calculated
from a primary text corpus), they can be &lsquo;folded-in&rsquo; later on 
(with the function <code>fold_in()</code>).
</p>
<p>Background information (see also Deerwester et al., 1990): 
</p>
<p>A document-term matrix <code class="reqn">M</code> is constructed 
with <code>textmatrix()</code> from a given text base of <code class="reqn">n</code> documents 
containing <code class="reqn">m</code> terms.
This matrix <code class="reqn">M</code> of the size <code class="reqn">m \times n</code> is then decomposed via a
singular value decomposition into: term vector matrix <code class="reqn">T</code> (constituting 
left singular vectors), the document vector matrix <code class="reqn">D</code> (constituting 
right singular vectors) being both orthonormal, and the diagonal matrix 
<code class="reqn">S</code> (constituting singular values). 
</p>
<p><code class="reqn">M = TSD^T</code>
</p>
<p>These matrices are then reduced to the given number of dimensions <code class="reqn">k=dims</code>
to result into truncated matrices <code class="reqn">T_{k}</code>, <code class="reqn">S_{k}</code> and <code class="reqn">D_{k}</code>
&mdash; the latent semantic space. 
</p>
<p><code class="reqn">M_k = \sum\limits_{i=1}^k t_i \cdot s_i \cdot d_i^T</code>
</p>
<p>If these matrices <code class="reqn">T_k, S_k, D_k</code> were multiplied, they would give a new
matrix <code class="reqn">M_k</code> (of the same format as <code class="reqn">M</code>, i.e., rows are the
same terms, columns are the same documents), which is the least-squares best 
fit approximation of <code class="reqn">M</code> with <code class="reqn">k</code> singular values.
</p>
<p>In the case of folding-in, i.e., multiplying new documents into a given
latent semantic space, the matrices <code class="reqn">T_k</code> and <code class="reqn">S_k</code> remain unchanged
and an additional <code class="reqn">D_k</code> is created (without replacing the old one).
All three are multiplied together to return a (new and appendable)
document-term matrix <code class="reqn">\hat{M}</code> in the term-order of <code class="reqn">M</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>LSAspace</code></td>
<td>
<p>a list with components (<code class="reqn">T_k, S_k, D_k</code>), representing the latent semantic space.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Fridolin Wild <a href="mailto:fridolin.wild@wu-wien.ac.at">fridolin.wild@wu-wien.ac.at</a> </p>


<h3>References</h3>

<p>Deerwester, S., Dumais, S., Furnas, G., Landauer, T., and Harshman, R. (1990) <em>Indexing by Latent Semantic Analysis</em>. In: Journal of the American Society for Information Science 41(6), pp. 391&ndash;407.
</p>
<p>Landauer, T., Foltz, P., and Laham, D. (1998) <em>Introduction to Latent Semantic Analysis</em>. In: Discourse Processes 25, pp. 259&ndash;284.
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+as.textmatrix">as.textmatrix</a></code>, <code><a href="#topic+fold_in">fold_in</a></code>, <code><a href="#topic+textmatrix">textmatrix</a></code>, <code><a href="#topic+gw_idf">gw_idf</a></code>, <code><a href="#topic+dimcalc_share">dimcalc_share</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# create some files
td = tempfile()
dir.create(td)
write( c("dog", "cat", "mouse"), file=paste(td, "D1", sep="/") )
write( c("ham", "mouse", "sushi"), file=paste(td, "D2", sep="/") )
write( c("dog", "pet", "pet"), file=paste(td, "D3", sep="/") )

# LSA
data(stopwords_en)
myMatrix = textmatrix(td, stopwords=stopwords_en)
myMatrix = lw_logtf(myMatrix) * gw_idf(myMatrix)
myLSAspace = lsa(myMatrix, dims=dimcalc_share())
as.textmatrix(myLSAspace)

# clean up
unlink(td, recursive=TRUE)

</code></pre>

<hr>
<h2 id='print.textmatrix'>Print a textmatrix (Matrices)</h2><span id='topic+print.textmatrix'></span>

<h3>Description</h3>

<p>Display a one screen short version of a textmatrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'textmatrix'
print( x, bag_lines, bag_cols, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.textmatrix_+3A_x">x</code></td>
<td>
<p>A textmatrix.</p>
</td></tr>
<tr><td><code id="print.textmatrix_+3A_bag_lines">bag_lines</code></td>
<td>
<p>The number of lines per bag.</p>
</td></tr>
<tr><td><code id="print.textmatrix_+3A_bag_cols">bag_cols</code></td>
<td>
<p>The number of columns per bag.</p>
</td></tr>
<tr><td><code id="print.textmatrix_+3A_...">...</code></td>
<td>
<p>Arguments to be passed on.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Document-term matrices are often very large and
cannot be displayed completely on one screen. 
Therefore, the textmatrix print method displays
only clippings (&lsquo;bags&rsquo;) from this matrix.
</p>
<p>Clippings are taken vertically and horizontally 
from beginning, middle, and end of the matrix.
<code>bag\_lines</code> lines and <code>bag\_cols</code> columns 
are printed to the screen.
</p>
<p>To keep document titles from blowing up the display,
the legend is printed below, referencing the symbols
used in the table.
</p>


<h3>Author(s)</h3>

<p> Fridolin Wild <a href="mailto:f.wild@open.ac.uk">f.wild@open.ac.uk</a> </p>


<h3>See Also</h3>

 <p><code><a href="#topic+textmatrix">textmatrix</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
# fake a matrix
m = matrix(ncol=800, nrow=400)
m[1:length(m)] = 1:length(m)
colnames(m) = paste("D",1:ncol(m),sep="")
rownames(m) = paste("W",1:nrow(m),sep="")
class(m) = "textmatrix"

# show a short form of the matrix
print(m, bag_cols=5)

</code></pre>

<hr>
<h2 id='query'>Query (Matrices)</h2><span id='topic+query'></span>

<h3>Description</h3>

<p>Create a query in the format of a given textmatrix. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>query ( qtext, termlist, stemming=FALSE, language="german" )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="query_+3A_termlist">termlist</code></td>
<td>
<p>the termlist of the background latent-semantic space.</p>
</td></tr>
<tr><td><code id="query_+3A_language">language</code></td>
<td>
<p>specifies a language for stemming / stop-word-removal.</p>
</td></tr>
<tr><td><code id="query_+3A_stemming">stemming</code></td>
<td>
<p>boolean, specifies whether all terms will be reduced to their wordstems.</p>
</td></tr>
<tr><td><code id="query_+3A_qtext">qtext</code></td>
<td>
<p>the query string, words are separated by blanks.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create queries, i.e., an additional term vector to be used for query-to-document
comparisons, in the format of a given textmatrix.
</p>


<h3>Value</h3>

<table>
<tr><td><code>query</code></td>
<td>
<p>returns the query vector (based on the given vocabulary) as matrix.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Fridolin Wild <a href="mailto:f.wild@open.ac.uk">f.wild@open.ac.uk</a> </p>


<h3>See Also</h3>

<p><code><a href="SnowballC.html#topic+wordStem">wordStem</a></code>, <code><a href="#topic+textmatrix">textmatrix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# prepare some files
td = tempfile()
dir.create(td)
write( c("dog", "cat", "mouse"), file=paste(td,"D1", sep="/") )
write( c("hamster", "mouse", "sushi"), file=paste(td,"D2", sep="/") )
write( c("dog", "monster", "monster"), file=paste(td,"D3", sep="/") )

# demonstrate generation of a query
dtm = textmatrix(td)
query("monster", rownames(dtm))
query("monster dog", rownames(dtm))

# clean up
unlink(td, TRUE)

</code></pre>

<hr>
<h2 id='sample.textmatrix'>Create a random sample of files</h2><span id='topic+sample.textmatrix'></span>

<h3>Description</h3>

<p>Creates a subset of the documents
of a corpus to help reduce a corpus in size
through random sampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   sample.textmatrix(textmatrix, samplesize, index.return=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample.textmatrix_+3A_textmatrix">textmatrix</code></td>
<td>
<p>A document-term matrix.</p>
</td></tr>
<tr><td><code id="sample.textmatrix_+3A_samplesize">samplesize</code></td>
<td>
<p>Desired number of files</p>
</td></tr>
<tr><td><code id="sample.textmatrix_+3A_index.return">index.return</code></td>
<td>
<p>if set to true, the positions of the subset in the original column vectors will be returned as well.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Often a corpus is so big that it cannot be processed
in memory. One technique to reduce the size is to
select a subset of the documents randomly, assuming
that through the random selection the nature of
the term sets and distributions will not be changed. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>filelist</code></td>
<td>
<p>a list of filenames of the documents in the corpus.). </p>
</td></tr>
<tr><td><code>ix</code></td>
<td>
<p>If index.return is set to true, a list is returned; <code>x</code> contains
the filenames and <code>ix</code> contains the position of the sample files in the
original filelist.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Fridolin Wild <a href="mailto:f.wild@open.ac.uk">f.wild@open.ac.uk</a> </p>


<h3>See Also</h3>

 <p><code><a href="#topic+textmatrix">textmatrix</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
# create some files
td = tempfile()
dir.create(td)
write( c("dog", "cat", "mouse"), file=paste(td, "D1", sep="/"))
write( c("hamster", "mouse", "sushi"), file=paste(td, "D2", sep="/"))
write( c("dog", "monster", "monster"), file=paste(td, "D3", sep="/"))
write( c("dog", "mouse", "dog"), file=paste(td, "D4", sep="/"))

# create matrices
myMatrix = textmatrix(td, minWordLength=1)

sample(myMatrix, 3)

# clean up
unlink(td, recursive=TRUE)

</code></pre>

<hr>
<h2 id='specialchars'>List of special character html entities and their character replacement</h2><span id='topic+specialchars'></span>

<h3>Description</h3>

<p>This list contains entities (<code>specialchars$entities</code>) and their replacement character (<code>specialchars$replacement</code>), as used by textvector to cleanup html code: for example, this is used to replace the html entity \&amp;auml\; with the character ae. You can use this data set with <code>data(specialchars)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   data(specialchars)
</code></pre>


<h3>Format</h3>

<p>list of language specific html entities and their replacement</p>


<h3>Author(s)</h3>

<p> Fridolin Wild <a href="mailto:f.wild@open.ac.uk">f.wild@open.ac.uk</a></p>

<hr>
<h2 id='stopwords'>Stopwordlists in German, English, Dutch, French, Polish, and Arab</h2><span id='topic+stopwords_de'></span><span id='topic+stopwords_en'></span><span id='topic+stopwords_nl'></span><span id='topic+stopwords_fr'></span><span id='topic+stopwords_pl'></span><span id='topic+stopwords_ar'></span>

<h3>Description</h3>

<p>This data sets contain very common lists of words that want to be ignored when 
building up a document-term matrix. The stop word lists can be loaded by
calling <code>data(stopwords_en)</code>, <code>data(stopwords_de)</code>, 
<code>data(stopwords_nl)</code>, <code>data(stopwords_ar)</code>, etc. The objects <code>stopwords_de</code>, <code>stopwords_en</code>, <code>stopwords_nl</code>, <code>stopwords_ar</code>, etc. must already exist <em>before</em> being handed over to <code>textmatrix()</code>.
</p>
<p>The French stopword list has been combined by Haykel Demnati
by integrating the lists from rank.nl (www.rank.nl/stopwors/french.html), 
the one from the CLEF team at the University of Neuchatel 
(http://members.unine.ch/jacques.savoy/clef/frenchST.txt), 
and the one prepared by Jean VÃ©ronis
(http://sites.univ-provence.fr/veronis/data/antidico.txt).
</p>
<p>The Polish stopword list has been contributed by Grazyna Paliwoda-Pekosz, Cracow University of Economics and is taken from the Polish Wikipedia.
</p>
<p>The Arab stopword list has been contributed by Marwa Naili, Tunisia. The list is based on the stopword lists by Shereen Khoja and by Siham Boulaknadel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   data(stopwords_de)
   stopwords_de

   data(stopwords_en)
   stopwords_en

   data(stopwords_nl)
   stopwords_nl

   data(stopwords_fr)
   stopwords_fr

   data(stopwords_ar)
   stopwords_ar
</code></pre>


<h3>Format</h3>

<p>A vector containing 424 English, 370 German, 260 Dutch, 890 French stop, or 434 Arab words (e.g. 'he', 'she', 'a').</p>


<h3>Author(s)</h3>

<p> Fridolin Wild <a href="mailto:fridolin.wild@wu-wien.ac.at">fridolin.wild@wu-wien.ac.at</a>, Marco Kalz <a href="mailto:marco.kalz@ou.nl">marco.kalz@ou.nl</a> (for Dutch), Haykel Demnati <a href="mailto:Haykel.Demnati@isg.rnu.tn">Haykel.Demnati@isg.rnu.tn</a> (for French),
Marwa Naili <a href="mailto:naili.maroua@gmail.com">naili.maroua@gmail.com</a> (for Arab) </p>

<hr>
<h2 id='summary.textmatrix'>Summary of a textmatrix (Matrices)</h2><span id='topic+summary.textmatrix'></span>

<h3>Description</h3>

<p>Return a summary with some statistical infos about a given textmatrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'textmatrix'
summary( object, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.textmatrix_+3A_object">object</code></td>
<td>
<p>A textmatrix.</p>
</td></tr>
<tr><td><code id="summary.textmatrix_+3A_...">...</code></td>
<td>
<p>Arguments to be passed on</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns some statistical infos about the textmatrix <code>x</code>:
number of terms, number of documents, maximum length of a term, 
number of values not 0, number of terms containing strange
characters.
</p>


<h3>Value</h3>

<table>
<tr><td><code>matrix</code></td>
<td>
<p>Returns a matrix.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Fridolin Wild <a href="mailto:f.wild@open.ac.uk">f.wild@open.ac.uk</a> </p>


<h3>See Also</h3>

 <p><code><a href="#topic+textmatrix">textmatrix</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
# fake a matrix
m = matrix(ncol=800, nrow=400)
m[1:length(m)] = 1:length(m)
colnames(m) = paste("D",1:ncol(m),sep="")
rownames(m) = paste("W",1:nrow(m),sep="")
class(m) = "textmatrix"

# show a short form of the matrix
summary(m)

</code></pre>

<hr>
<h2 id='textmatrix'>Textmatrix (Matrices)</h2><span id='topic+textmatrix'></span><span id='topic+textvector'></span>

<h3>Description</h3>

<p>Creates a document-term matrix from all textfiles in a given directory.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textmatrix( mydir, stemming=FALSE, language="english",
   minWordLength=2, maxWordLength=FALSE, minDocFreq=1, 
   maxDocFreq=FALSE, minGlobFreq=FALSE, maxGlobFreq=FALSE, 
   stopwords=NULL, vocabulary=NULL, phrases=NULL, 
   removeXML=FALSE, removeNumbers=FALSE)
textvector( file, stemming=FALSE, language="english", 
   minWordLength=2, maxWordLength=FALSE, minDocFreq=1, 
   maxDocFreq=FALSE, stopwords=NULL, vocabulary=NULL, 
   phrases=NULL, removeXML=FALSE, removeNumbers=FALSE )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textmatrix_+3A_file">file</code></td>
<td>
<p>filename (may include path).</p>
</td></tr>
<tr><td><code id="textmatrix_+3A_mydir">mydir</code></td>
<td>
<p>the directory path (e.g., <code>"corpus/texts/"</code>); may be single files/directories or a vector of files/directories.</p>
</td></tr>
<tr><td><code id="textmatrix_+3A_stemming">stemming</code></td>
<td>
<p>boolean indicating whether to reduce all terms to their wordstem.</p>
</td></tr>
<tr><td><code id="textmatrix_+3A_language">language</code></td>
<td>
<p>specifies language for the stemming / stop-word-removal.</p>
</td></tr>
<tr><td><code id="textmatrix_+3A_minwordlength">minWordLength</code></td>
<td>
<p>words with less than minWordLength characters will be ignored.</p>
</td></tr>
<tr><td><code id="textmatrix_+3A_maxwordlength">maxWordLength</code></td>
<td>
<p>words with more than maxWordLength characters will be ignored; per default set to <code>FALSE</code> to use no upper boundary.</p>
</td></tr>
<tr><td><code id="textmatrix_+3A_mindocfreq">minDocFreq</code></td>
<td>
<p>words of a document appearing less than minDocFreq within that document will be ignored.</p>
</td></tr>
<tr><td><code id="textmatrix_+3A_maxdocfreq">maxDocFreq</code></td>
<td>
<p>words of a document appearing more often than maxDocFreq within that document will be ignored; per default set to <code>FALSE</code> to use no upper boundary for document frequencies.</p>
</td></tr>
<tr><td><code id="textmatrix_+3A_minglobfreq">minGlobFreq</code></td>
<td>
<p>words which appear in less than minGlobFreq documents will be ignored.</p>
</td></tr>
<tr><td><code id="textmatrix_+3A_maxglobfreq">maxGlobFreq</code></td>
<td>
<p>words which appear in more than maxGlobFreq documents will be ignored.</p>
</td></tr>
<tr><td><code id="textmatrix_+3A_stopwords">stopwords</code></td>
<td>
<p>a stopword list that contains terms the will be ignored.</p>
</td></tr>
<tr><td><code id="textmatrix_+3A_vocabulary">vocabulary</code></td>
<td>
<p>a character vector containing the words: only words in this term list will be used for building the matrix (&lsquo;controlled vocabulary&rsquo;).</p>
</td></tr>
<tr><td><code id="textmatrix_+3A_removexml">removeXML</code></td>
<td>
<p>if set to <code>TRUE</code>, XML tags (elements, attributes, some special characters) will be removed.</p>
</td></tr>
<tr><td><code id="textmatrix_+3A_removenumbers">removeNumbers</code></td>
<td>
<p>if set to <code>TRUE</code>, terms that consist only out of numbers will be removed.</p>
</td></tr>
<tr><td><code id="textmatrix_+3A_phrases">phrases</code></td>
<td>
<p>not implemented, yet.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>All documents in the specified directory are read and a matrix is composed.
The matrix contains in every cell the exact number of appearances (i.e., the term frequency) 
of every word for all documents. If specified, simple text preprocessing mechanisms
are applied (stemming, stopword filtering, wordlength cutoffs).
</p>
<p>Stemming thereby uses Porter's snowball stemmer (from package <code>SnowballC</code>).
</p>
<p>There are two stopword lists included (for english and for german), which
are loaded on demand into the variables <code>stopwords_de</code> and 
<code>stopwords_en</code>. They can be activated by calling <code>data(stopwords_de)</code>
or <code>data(stopwords_en)</code>. Attention: the stopword lists have
to be already loaded when <code>textmatrix()</code> is called.
</p>
<p><code>textvector()</code> is a support function that creates a list of
term-in-document occurrences.
</p>
<p>For every generated matrix, an own environment is added as an attribute which
holds the triples that are stored by <code>setTriple()</code> and can be
retrieved with <code>getTriple()</code>.
</p>
<p>If the language is set to &quot;arabic&quot;, special characters for the Buckwalter 
transliteration will be kept.
</p>


<h3>Value</h3>

<table>
<tr><td><code>textmatrix</code></td>
<td>
<p>the document-term matrix (incl. row and column names).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Fridolin Wild <a href="mailto:f.wild@open.ac.uk">f.wild@open.ac.uk</a> </p>


<h3>See Also</h3>

 <p><code><a href="SnowballC.html#topic+wordStem">wordStem</a></code>, <code><a href="#topic+stopwords_de">stopwords_de</a></code>, <code><a href="#topic+stopwords_en">stopwords_en</a></code>, <code><a href="#topic+setTriple">setTriple</a></code>, <code><a href="#topic+getTriple">getTriple</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
# create some files
td = tempfile()
dir.create(td)
write( c("dog", "cat", "mouse"), file=paste(td, "D1", sep="/") )
write( c("hamster", "mouse", "sushi"), file=paste(td, "D2", sep="/") )
write( c("dog", "monster", "monster"), file=paste(td, "D3", sep="/") )

# read them, create a document-term matrix
textmatrix(td)

# read them, drop german stopwords
data(stopwords_de)
textmatrix(td, stopwords=stopwords_de)

# read them based on a controlled vocabulary
voc = c("dog", "mouse")
textmatrix(td, vocabulary=voc, minWordLength=1)

# clean up
unlink(td, recursive=TRUE)

</code></pre>

<hr>
<h2 id='triples'>Bind Triples to a Textmatrix</h2><span id='topic+getTriple'></span><span id='topic+setTriple'></span><span id='topic+delTriple'></span><span id='topic+getSubjectId'></span>

<h3>Description</h3>

<p>Allows to store, manage and retrieve SPO-triples (subject, predicate, object)
bound to the document columns of a document term matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getTriple( M, subject, predicate )
setTriple( M, subject, predicate, object )
delTriple( M, subject, predicate, object )
getSubjectId( M, subject )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="triples_+3A_m">M</code></td>
<td>
<p>the document term matrix (see <code>textmatrix</code>).</p>
</td></tr>
<tr><td><code id="triples_+3A_subject">subject</code></td>
<td>
<p>column number or column name (e.g., <code>"doc3"</code> or <code>3</code>).</p>
</td></tr>
<tr><td><code id="triples_+3A_predicate">predicate</code></td>
<td>
<p>predicate of the triple sentence (e.g., <code>"has\_category"</code> or <code>"has\_grade"</code>).</p>
</td></tr>
<tr><td><code id="triples_+3A_object">object</code></td>
<td>
<p>value of the triple sentence (e.g., <code>"14"</code> or <code>14</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SPO-Triples are simple facts of the uniform structure
(subject, predicate, object). A subject is typically
a document in the given document-term matrix M,
i.e. its document title (as in the column names) or its 
column position. A key-value pair (the <span class="option">predicate</span> 
and the <span class="option">object</span>) can be bound to this <span class="option">subject</span>.
</p>
<p>This can be used, for example, to store classification
information about the documents of the text base used.
</p>
<p>The triple data is stored in the environment of M
constructed by <code>textmatrix()</code>.
</p>
<p>Whenever a matrix has to be used which has not been generated by 
this function, its class should be set to 'textmatrix' and an 
environment has to be added manually via:
</p>
<p><code>class(mymatrix) = "textmatrix"</code>
</p>
<p><code>environment(mymatrix) = new.env()</code>
</p>
<p>Alternatively, <code>as.matrix()</code> can be used to convert a
matrix to a textmatrix. To spare memory, the manual method
might be of advantage.
</p>
<p>In <code>getTriple()</code>, the arguments <span class="option">subject</span> and <span class="option">predicate</span>
are optional.
</p>


<h3>Value</h3>

<table>
<tr><td><code>textmatrix</code></td>
<td>
<p>the document-term matrix (including row and column names).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Fridolin Wild <a href="mailto:f.wild@open.ac.uk">f.wild@open.ac.uk</a> </p>


<h3>See Also</h3>

 <p><code><a href="#topic+textmatrix">textmatrix</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
    x = matrix(2,2,3) # we fake a document term matrix
    rownames(x) = c("dog","mouse") # fake term names
    colnames(x) = c("doc1","doc2","doc3") # fake doc titles
    class(x) = "textmatrix" # usually done by textmatrix()
    environment(x) = new.env() # usually done by textmatrix()
    
    setTriple(x, "doc1", "has_category", "15")
    setTriple(x, "doc2", "has_category", "7")
    setTriple(x, "doc1", "has_grade", "5")
    setTriple(x, "doc1", "has_category", "11")
    
    getTriple(x, "doc1")
    getTriple(x, "doc1")[[2]]
    getTriple(x, "doc1", "has_category") # -&gt; [1] "15" "11"
    
    delTriple(x, "doc1", "has_category", "15")
    getTriple(x, "doc1", "has_category") # -&gt; [1] "11"
    
</code></pre>

<hr>
<h2 id='weightings'>Weighting Schemes (Matrices)</h2><span id='topic+lw_tf'></span><span id='topic+lw_logtf'></span><span id='topic+lw_bintf'></span><span id='topic+gw_normalisation'></span><span id='topic+gw_idf'></span><span id='topic+gw_gfidf'></span><span id='topic+entropy'></span><span id='topic+gw_entropy'></span>

<h3>Description</h3>

<p>Calculates a weighted document-term matrix according to the chosen local and/or global weighting scheme.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    lw_tf(m)
    lw_logtf(m)
    lw_bintf(m)
    gw_normalisation(m)
    gw_idf(m)
    gw_gfidf(m)
    entropy(m)
    gw_entropy(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weightings_+3A_m">m</code></td>
<td>
<p>a document-term matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When combining a local and a global weighting scheme to be applied on a 
given textmatrix <code>m</code> via <code class="reqn">dtm = lw(m) \cdot gw(m)</code>, where
</p>

<ul>
<li> <p><code class="reqn">m</code> is the given document-term matrix,
</p>
</li>
<li> <p><code class="reqn">lw(m)</code> is one of the local weight functions <code>lw\_tf()</code>, <code>lw\_logtf()</code>, <code>lw\_bintf()</code>, and
</p>
</li>
<li> <p><code class="reqn">gw(m)</code> is one of the global weight functions <code>gw\_normalisation()</code>, <code>gw\_idf()</code>, <code>gw\_gfidf()</code>, <code>entropy()</code>, <code>gw\_entropy()</code>.
</p>
</li></ul>

<p>This set of weighting schemes includes the local weightings (lw)
raw, log, binary and the global weightings (gw) normalisation, two versions of the 
inverse document frequency (idf), and entropy in both the original Shannon as well as 
in a slightly modified, more common version:
</p>
<p><code>lw\_tf()</code> returns a completely unmodified <code class="reqn">n \times m</code> matrix (placebo function).
</p>
<p><code>lw\_logtf()</code> returns the logarithmised <code class="reqn">n \times m</code> matrix. <code class="reqn">log(m_{i,j}+1)</code> is applied on every cell.
</p>
<p><code>lw\_bintf()</code> returns binary values of the <code class="reqn">n \times m</code> matrix. Every cell is assigned 1, iff the term frequency is not equal to 0.
</p>
<p><code>gw\_normalisation()</code> returns a normalised <code class="reqn">n \times m</code> matrix. Every cell equals 1 divided by the square root of the document vector length.
</p>
<p><code>gw\_idf()</code> returns the inverse document frequency in a <code class="reqn">n \times m</code> matrix. Every cell is 1 plus the logarithmus of the number of documents divided by the number of documents where the term appears.
</p>
<p><code>gw\_gfidf()</code> returns the global frequency multiplied with idf. Every cell equals the sum of the frequencies of one term divided by the number of documents where the term shows up.
</p>
<p><code>entropy()</code> returns the entropy (as defined by Shannon).
</p>
<p><code>gw\_entropy()</code> returns one plus entropy.
</p>
<p>Be careful when folding in data into an existing lsa space: you may want to 
weight an additional textmatrix based on the same vocabulary with the global 
weights of the training data (not the new data)!
</p>


<h3>Value</h3>

<p>Returns the weighted textmatrix of the same size and format as the input matrix.
</p>


<h3>Author(s)</h3>

<p> Fridolin Wild <a href="mailto:f.wild@open.ac.uk">f.wild@open.ac.uk</a> </p>


<h3>References</h3>

<p>Dumais, S. (1992) <em>Enhancing Performance in Latent Semantic Indexing (LSI) Retrieval</em>. Technical Report, Bellcore.
</p>
<p>Nakov, P., Popova, A., and Mateev, P. (2001) <em>Weight functions impact on LSA performance</em>. In: Proceedings of the Recent Advances in Natural language processing, Bulgaria, pp.187-193.
</p>
<p>Shannon, C. (1948) <em>A Mathematical Theory of Communication</em>. In: The Bell System Technical Journal 27(July), pp.379&ndash;423.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## use the logarithmised term frequency as local weight and 
## the inverse document frequency as global weight.

vec1 = c( 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0 )
vec2 = c( 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0 )
vec3 = c( 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0 )
matrix = cbind(vec1,vec2, vec3)
weighted = lw_logtf(matrix)*gw_idf(matrix)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
