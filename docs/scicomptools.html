<!DOCTYPE html><html><head><title>Help for package scicomptools</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {scicomptools}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#drive_toc'><p>Identify all Folders within Specified Google Drive Folder</p></a></li>
<li><a href='#read_xl_format'><p>Read Formatting of All Sheets in an Excel Workbook</p></a></li>
<li><a href='#read_xl_sheets'><p>Read All Sheets from an Excel Workbook</p></a></li>
<li><a href='#stat_extract'><p>Extract Summary Statistics from Model Fit Object</p></a></li>
<li><a href='#token_check'><p>Check Token Status</p></a></li>
<li><a href='#wd_loc'><p>Define Local or Remote Working Directories</p></a></li>
<li><a href='#word_cloud_plot'><p>Text Mine a Given Column and Create a Word Cloud</p></a></li>
<li><a href='#word_cloud_prep'><p>Perform Text Mining of a Given Column</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Tools Developed by the NCEAS Scientific Computing Support Team</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Angel Chen &lt;anchen@nceas.ucsb.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Set of tools to import, summarize, wrangle, and visualize data. 
    These functions were originally written based on the needs of the 
    various synthesis working groups that were supported by the 
    National Center for Ecological Analysis and Synthesis (NCEAS). 
    These tools are meant to be useful inside and 
    outside of the context for which they were designed. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/BSD-3-Clause">BSD_3_clause</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/NCEAS/scicomptools">https://github.com/NCEAS/scicomptools</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/NCEAS/scicomptools/issues">https://github.com/NCEAS/scicomptools/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Imports:</td>
<td>data.tree, dplyr, gitcreds, googledrive, ggplot2, ggwordcloud,
magrittr, methods, purrr, readxl, stringr, SemNetCleaner,
tibble, tidyr, tidytext, tidyxl</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, nlme, rmarkdown, RRPP</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-06-05 17:06:29 UTC; chen</td>
</tr>
<tr>
<td>Author:</td>
<td>Julien Brun <a href="https://orcid.org/0000-0002-7751-6238"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Angel Chen <a href="https://orcid.org/0000-0003-3515-6710"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre] (angelchen7.github.io),
  Gabriel Antunes Daldegan
    <a href="https://orcid.org/0000-0001-5345-4880"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Gabe De La Rosa [ctb] (www.gabrieldelarosa.com/),
  Kara Koenig <a href="https://orcid.org/0000-0002-6371-7821"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Nicholas J Lyon <a href="https://orcid.org/0000-0003-3905-1078"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut] (njlyon0.github.io),
  Kendall Miller [aut],
  Timothy D Nguyen [aut] (www.linkedin.com/in/timothy-d-nguyen),
  National Science Foundation [fnd] (NSF 1929393, 09/01/2019 -
    08/31/2024),
  University of California, Santa Barbara [cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-06-06 07:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='drive_toc'>Identify all Folders within Specified Google Drive Folder</h2><span id='topic+drive_toc'></span>

<h3>Description</h3>

<p>Identifies all sub-folders within a user-supplied Drive folder (typically the top-level URL). Also allows for exclusion of folders by name; useful if a &quot;Backups&quot; or &quot;Archive&quot; folder is complex and a table of contents is unwanted for that folder(s).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>drive_toc(url = NULL, ignore_names = NULL, quiet = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="drive_toc_+3A_url">url</code></td>
<td>
<p>(drive_id) Google Drive folder link modified by 'googledrive::as_id' to be a true &quot;Drive ID&quot; (e.g., 'url = as_id(&quot;url text&quot;)')</p>
</td></tr>
<tr><td><code id="drive_toc_+3A_ignore_names">ignore_names</code></td>
<td>
<p>(character) Vector of name(s) of folder(s) to be excluded from list of folders</p>
</td></tr>
<tr><td><code id="drive_toc_+3A_quiet">quiet</code></td>
<td>
<p>(logical) Whether to message which folder it is currently listing (defaults to 'FALSE'). Complex folder structures will take time to fully process but the informative per-folder message provides solace that this function has not stopped working</p>
</td></tr>
</table>


<h3>Value</h3>

<p>(node / R6) Special object class used by the 'data.tree' package
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
## Not run: 
# Supply a single Google Drive folder link to identify all its sub-folders 
drive_toc(url = googledrive::as_id("https://drive.google.com/drive/u/0/folders/your-folder"))

## End(Not run)

</code></pre>

<hr>
<h2 id='read_xl_format'>Read Formatting of All Sheets in an Excel Workbook</h2><span id='topic+read_xl_format'></span>

<h3>Description</h3>

<p>Retrieves all sheets of a Microsoft Excel workbook and identifies the formatting of each value (including column headers and blank cells).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_xl_format(file_name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read_xl_format_+3A_file_name">file_name</code></td>
<td>
<p>(character) Name of (and path to) the Excel workbook</p>
</td></tr>
</table>


<h3>Value</h3>

<p>(data frame) One row per cell in the dataframe with a column for each type of relevant formatting and its 'address' within the original Excel workbook
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Identify the formatting of every cell in all sheets of an Excel file
read_xl_format(file_name = system.file("extdata", "excel_book.xlsx", package = "scicomptools"))

</code></pre>

<hr>
<h2 id='read_xl_sheets'>Read All Sheets from an Excel Workbook</h2><span id='topic+read_xl_sheets'></span>

<h3>Description</h3>

<p>Retrieves all of the sheets in a given Microsoft Excel workbook and stores them as elements in a list. Note that the guts of this function were created by the developers of 'readxl::read_excel()' and we merely created a wrapper function to invoke their work more easily.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_xl_sheets(file_name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read_xl_sheets_+3A_file_name">file_name</code></td>
<td>
<p>(character) Name of (and path to) the Excel workbook</p>
</td></tr>
</table>


<h3>Value</h3>

<p>(list) One tibble per sheet in the Excel workbook stored as separate elements in a list
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Read in each sheet as an element in a list
read_xl_sheets(file_name = system.file("extdata", "excel_book.xlsx", package = "scicomptools"))

</code></pre>

<hr>
<h2 id='stat_extract'>Extract Summary Statistics from Model Fit Object</h2><span id='topic+stat_extract'></span>

<h3>Description</h3>

<p>Accepts model fit object and extracts core statistical information. This includes P value, test statistic, degrees of freedom, etc. Currently accepts the following model types: 'stats::t.test', 'stats::lm', 'stats_nls', 'nlme::lme', 'lmerTest::lmer', 'ecodist::MRM', or 'RRPP::trajectory.analysis'
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stat_extract(mod_fit = NULL, traj_angle = "deg")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stat_extract_+3A_mod_fit">mod_fit</code></td>
<td>
<p>(lme, trajectory.analysis) Model fit object of supported class (see function description text)</p>
</td></tr>
<tr><td><code id="stat_extract_+3A_traj_angle">traj_angle</code></td>
<td>
<p>(character) Either &quot;deg&quot; or &quot;rad&quot; for whether trajectory analysis angle information should be extracted in degrees or radians. Only required if model is trajectory analysis</p>
</td></tr>
</table>


<h3>Value</h3>

<p>(data.frame) Dataframe of core summary statistics for the given model
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create some example data
x &lt;- c(3.5, 2.1, 7.5, 5.6, 3.3, 6.0, 5.6)
y &lt;- c(2.3, 4.7, 7.8, 9.1, 4.5, 3.6, 5.1)

# Fit a linear model
mod &lt;- lm(y ~ x)

# Extract the relevant information
stat_extract(mod_fit = mod)

</code></pre>

<hr>
<h2 id='token_check'>Check Token Status</h2><span id='topic+token_check'></span>

<h3>Description</h3>

<p>To make some direct-from-API workflows functional (e.g., Qualtrics surveys, etc.). It is necessary to quickly test whether a given R session &quot;knows&quot; the API token. This function returns an error if the specified token type isn't found and prints a message if one is found
</p>


<h3>Usage</h3>

<pre><code class='language-R'>token_check(api = "qualtrics", secret = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="token_check_+3A_api">api</code></td>
<td>
<p>(character) API the token is for (currently only supports &quot;qualtrics&quot; and &quot;github&quot;)</p>
</td></tr>
<tr><td><code id="token_check_+3A_secret">secret</code></td>
<td>
<p>(logical) Whether to include the token character string in the success message. FALSE prints the token, TRUE keeps it secret but returns a success message</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, called for side effects
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Check whether a GitHub token is attached or not
token_check(api = "github", secret = TRUE)

## End(Not run)
## Not run: 
# Check whether a Qualtrics token is attached or not
token_check(api = "qualtrics", secret = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='wd_loc'>Define Local or Remote Working Directories</h2><span id='topic+wd_loc'></span>

<h3>Description</h3>

<p>While working on the same script both in a remote server and locally on your home computer, defining file paths can be unwieldy and may even require duplicate scripts&ndash;one for each location&ndash;that require maintenance in parallel. This function allows you to define whether you are working locally or not and specify the path to use in either case.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wd_loc(local = TRUE, local_path = getwd(), remote_path = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wd_loc_+3A_local">local</code></td>
<td>
<p>(logical) Whether you are working locally or on a remote server</p>
</td></tr>
<tr><td><code id="wd_loc_+3A_local_path">local_path</code></td>
<td>
<p>(character) File path to use if 'local' is 'TRUE' (defaults to 'getwd()')</p>
</td></tr>
<tr><td><code id="wd_loc_+3A_remote_path">remote_path</code></td>
<td>
<p>(character) File path to use if 'local' is 'FALSE'</p>
</td></tr>
</table>


<h3>Value</h3>

<p>(character) Either the entry of 'local_path' or 'remote_path' depending on whether 'local' is set as true or false
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Set two working directory paths to toggle between

# If you are working in your local computer, set `local` to "TRUE"
wd_loc(local = TRUE,
       local_path = file.path("local path"),
       remote_path = file.path("path on server"))
       
# If you are working in a remote server, set `local` to "FALSE"
wd_loc(local = FALSE,
       local_path = file.path("local path"),
       remote_path = file.path("path on server"))
      
</code></pre>

<hr>
<h2 id='word_cloud_plot'>Text Mine a Given Column and Create a Word Cloud</h2><span id='topic+word_cloud_plot'></span>

<h3>Description</h3>

<p>Mines a user-defined column of text and creates a word cloud from the identified words and bigrams.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>word_cloud_plot(
  data = NULL,
  text_column = NULL,
  word_count = 50,
  known_bigrams = c("working group")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="word_cloud_plot_+3A_data">data</code></td>
<td>
<p>dataframe containing at least one column</p>
</td></tr>
<tr><td><code id="word_cloud_plot_+3A_text_column">text_column</code></td>
<td>
<p>character, name of column in dataframe given to 'data' that contains the text to be mined</p>
</td></tr>
<tr><td><code id="word_cloud_plot_+3A_word_count">word_count</code></td>
<td>
<p>numeric, number of words to be returned (counts from most to least frequent)</p>
</td></tr>
<tr><td><code id="word_cloud_plot_+3A_known_bigrams">known_bigrams</code></td>
<td>
<p>character vector, all bigrams (two-word phrases) to be mined before mining for single words</p>
</td></tr>
</table>


<h3>Value</h3>

<p>dataframe of one column (named 'word') that can be used for word cloud creation. One row per bigram supplied in 'known_bigrams' or single word (not including &quot;stop words&quot;)
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create a dataframe containing some example text
text &lt;- data.frame(article_num = 1:6,
                   article_title = c("Why pigeons are the best birds",
                                     "10 ways to show your pet budgie love",
                                     "Should you feed ducks at the park?",
                                     "Locations and tips for birdwatching",
                                     "How to tell which pet bird is right for you",
                                     "Do birds make good pets?"))
                                     
# Prepare the dataframe for word cloud plotting              
word_cloud_prep(data = text, text_column = "article_title")

# Plot the word cloud
word_cloud_plot(data = text, text_column = "article_title")

</code></pre>

<hr>
<h2 id='word_cloud_prep'>Perform Text Mining of a Given Column</h2><span id='topic+word_cloud_prep'></span>

<h3>Description</h3>

<p>Mines a user-defined column to create a dataframe that is ready for creating a word cloud. It also identifies any user-defined &quot;bigrams&quot; (i.e., two-word phrases) supplied as a vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>word_cloud_prep(
  data = NULL,
  text_column = NULL,
  word_count = 50,
  known_bigrams = c("working group")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="word_cloud_prep_+3A_data">data</code></td>
<td>
<p>(dataframe) Data object containing at least one column</p>
</td></tr>
<tr><td><code id="word_cloud_prep_+3A_text_column">text_column</code></td>
<td>
<p>(character) Name of column in dataframe given to 'data' that contains the text to be mined</p>
</td></tr>
<tr><td><code id="word_cloud_prep_+3A_word_count">word_count</code></td>
<td>
<p>(numeric) Number of words to be returned (counts from most to least frequent)</p>
</td></tr>
<tr><td><code id="word_cloud_prep_+3A_known_bigrams">known_bigrams</code></td>
<td>
<p>(character) Vector of all bigrams (two-word phrases) to be mined before mining for single words</p>
</td></tr>
</table>


<h3>Value</h3>

<p>dataframe of one column (named 'word') that can be used for word cloud creation. One row per bigram supplied in 'known_bigrams' or single word (not including &quot;stop words&quot;)
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create a dataframe containing some example text
text &lt;- data.frame(article_num = 1:6,
                   article_title = c("Why pigeons are the best birds",
                                     "10 ways to show your pet budgie love",
                                     "Should you feed ducks at the park?",
                                     "Locations and tips for birdwatching",
                                     "How to tell which pet bird is right for you",
                                     "Do birds make good pets?"))
                                     
# Prepare the dataframe for word cloud plotting              
word_cloud_prep(data = text, text_column = "article_title")

# Plot the word cloud
word_cloud_plot(data = text, text_column = "article_title")

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
