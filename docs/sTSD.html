<!DOCTYPE html><html lang="en"><head><title>Help for package sTSD</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sTSD}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#adf_lag_select'><p>Identify Optimal Lag Order Selection for (Augmented) Dickey-Fuller Tests</p></a></li>
<li><a href='#exCopdab'><p>Dynamic Foreign Policy Behavior (COPDAB)</p></a></li>
<li><a href='#lag_suggests'><p>Suggested Lags for Your Time Series</p></a></li>
<li><a href='#money_demand'><p>Quarterly Money Demand in the United States</p></a></li>
<li><a href='#sadf_test'><p>Simulate a (Augmented) Dickey-Fuller Test to Assess Unit Root in a Time Series</p></a></li>
<li><a href='#sim_df_mod'><p>Simulate an Individual (Augmented) Dickey-Fuller Model</p></a></li>
<li><a href='#sim_ts'><p>Simulate a Time Series</p></a></li>
<li><a href='#skpss_test'><p>Simulate a KPSS Test to Assess Unit Root in a Time Series</p></a></li>
<li><a href='#spp_test'><p>Simulate a Phillips-Perron Test to Assess Unit Root in a Time Series</p></a></li>
<li><a href='#tbills'><p>Daily maturity rates for U.S. Treasury Bills</p></a></li>
<li><a href='#ur_summary'><p>Summarize Unit Root Test Simulations</p></a></li>
<li><a href='#USDICE'><p>Quarterly disposable income and personal consumption expenditures in the United States</p></a></li>
<li><a href='#USDSEK'><p>The USD/SEK Exchange Rate</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Simulate Time Series Diagnostics</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Steven Miller &lt;steve@svmiller.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>These are tools that allow users to do time series diagnostics, primarily
    tests of unit root, by way of simulation. While there is nothing necessarily
    wrong with the received wisdom of critical values generated decades ago, 
    simulation provides its own perks. Not only is simulation broadly informative
    as to what these various test statistics do and what are their plausible 
    values, simulation provides more flexibility for assessing unit root by way
    of different thresholds or different hypothesized distributions.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-27 08:35:37 UTC; steve</td>
</tr>
<tr>
<td>Author:</td>
<td>Steven Miller <a href="https://orcid.org/0000-0003-4072-6263"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-27 08:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='adf_lag_select'>Identify Optimal Lag Order Selection for (Augmented) Dickey-Fuller Tests</h2><span id='topic+adf_lag_select'></span>

<h3>Description</h3>

<p><code>adf_lag_select()</code> runs a series of (Augmented) Dickey-Fuller
tests and returns information that may (or may not) be useful in identify
a potential lag order for unit root tests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adf_lag_select(x, min_lag = 0, max_lag = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="adf_lag_select_+3A_x">x</code></td>
<td>
<p>a vector</p>
</td></tr>
<tr><td><code id="adf_lag_select_+3A_min_lag">min_lag</code></td>
<td>
<p>the minimum lag order to use. Defaults to 0.</p>
</td></tr>
<tr><td><code id="adf_lag_select_+3A_max_lag">max_lag</code></td>
<td>
<p>the maximum lag order to use. Defaults to Schwert's (1989) upper lag.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function removes missing values from the vector before calculating test
statistics.
</p>
<p>The lower bound lag order suggested by Schwert (1989) and the default suggested
by Said and Dickey (1984) do not meaningfully separate from each other until
the length of the series reaches 127. Under those conditions, if the <code>note</code>
column returned by this function for a finite series does not identify the
Said and Dickey (1984) default, but identifies the Schwert (1989) lower
bound, interpret the latter as the former.
</p>


<h3>Value</h3>

<p><code>adf_lag_select()</code> returns a list of length 3. The first element
in the list is a data frame of a series of (Augmented) Dickey-Fuller tests
for no-drift, no-trend. The second is a data frame of a series of (Augmented)
Dickey-Fuller tests for drift, no trend. The third is a data frame of a series
of (Augmented) Dickey-Fuller tests for a drift and trend. Each data frame has
the following columns communicating the following information.
</p>

<ol>
<li><p> The lag order
</p>
</li>
<li><p> The (A)DF statistic for the lag order.
</p>
</li>
<li><p> The Akaike information criterion for the model.
</p>
</li>
<li><p> Schwartz' (Bayesian) criteron for the model.
</p>
</li>
<li><p> The absolute value of the last lagged first difference in the model.
</p>
</li>
<li><p> The &quot;modified&quot; Akaike information criterion for the model.
</p>
</li>
<li><p> The &quot;modified&quot; Schwarz' (Bayesian) criterion for the model.
</p>
</li>
<li><p> A note indicating if the lag was suggested by Schwert (1989) or Said and Dickey (1984)
</p>
</li></ol>



<h3>Author(s)</h3>

<p>Steven V. Miller
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- head(tbills$tb3m, 500)
adf_lag_select(x)

</code></pre>

<hr>
<h2 id='exCopdab'>Dynamic Foreign Policy Behavior (COPDAB)</h2><span id='topic+exCopdab'></span>

<h3>Description</h3>

<p>A data frame on monthly dyadic foreign policy behavior from 1948 to 1978 for
select dyads, using COPDAB data. The data offer the opportunity for a basic
replication of Lebo and Moore (2003).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>exCopdab
</code></pre>


<h3>Format</h3>

<p>A data frame with 372 observations on the following 12 variables.
</p>

<dl>
<dt><code>ym</code></dt><dd><p>a year-month indicator, in the format of YYMM</p>
</dd>
<dt><code>eg2is</code></dt><dd><p>an estimate of the dyadic foreign policy behavior of Egypt to Israel</p>
</dd>
<dt><code>is2eg</code></dt><dd><p>an estimate of the dyadic foreign policy behavior of Israel to Egypt</p>
</dd>
<dt><code>us2ussr</code></dt><dd><p>an estimate of the dyadic foreign policy behavior of the U.S. to the Soviet Union</p>
</dd>
<dt><code>ussr2us</code></dt><dd><p>an estimate of the dyadic foreign policy behavior of the Soviet Union to the U.S.</p>
</dd>
<dt><code>us2fra</code></dt><dd><p>an estimate of the dyadic foreign policy behavior of the U.S. to France</p>
</dd>
<dt><code>fra2us</code></dt><dd><p>an estimate of the dyadic foreign policy behavior of France to the U.S.</p>
</dd>
<dt><code>us2is</code></dt><dd><p>an estimate of the dyadic foreign policy behavior of the U.S. to Israel</p>
</dd>
<dt><code>is2us</code></dt><dd><p>an estimate of the dyadic foreign policy behavior of Israel to the U.S.</p>
</dd>
<dt><code>suez</code></dt><dd><p>a dummy variable indicating if the observation corresponds with the Suez Crisis</p>
</dd>
<dt><code>sixday</code></dt><dd><p>a dummy variable indicating if the observation corresponds with the Six-Day War</p>
</dd>
<dt><code>yomk</code></dt><dd><p>a dummy variable indicating if the observation corresponds with the Yom Kippur War</p>
</dd>
</dl>



<h3>Details</h3>

<p>Lebo and Moore (2003, 22-24) will offer more context about how these
variables are coded. Important details for replication from scratch are assuredly
lost to history, but the authors are clear about what they're doing and the
procedure they used to weight fundamentally ordinal data to create some kind
of continuous estimate. Context clues offer more information as well.
</p>


<h3>References</h3>

<p>Lebo, Matthew J. and Will H. Moore. 2003. &quot;Dynamic Foreign Policy Behavior.&quot;
<em>Journal of Conflict Resolution</em> 47(1): 13-32.
</p>

<hr>
<h2 id='lag_suggests'>Suggested Lags for Your Time Series</h2><span id='topic+lag_suggests'></span>

<h3>Description</h3>

<p>A data frame on various suggestions for lags for your time series, given
the length of your time series. You are not compelled to use these. These
are just suggestions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lag_suggests
</code></pre>


<h3>Format</h3>

<p>A data frame with 1000 observations on the following 4 variables.
</p>

<dl>
<dt><code>n</code></dt><dd><p>an integer corresponding with an assumed length of your time series</p>
</dd>
<dt><code>schwert_ub</code></dt><dd><p>the upper bound lag order suggested by Schwert (1989) for a time series of that length</p>
</dd>
<dt><code>schwert_lb</code></dt><dd><p>the lower bound lag order suggested by Schwert (1989) for a time series of that length</p>
</dd>
<dt><code>qiuetal2013</code></dt><dd><p>the suggested lag order from Qiu et al. (2013)</p>
</dd>
<dt><code>sd84</code></dt><dd><p>the suggested lag order from Said and Dickey (1984)</p>
</dd>
</dl>



<h3>Details</h3>

<p>The lower bound lag order suggested by Schwert (1989) and the default
suggested by Said and Dickey (1984) do not meaningfully separate from each
other until the length of the series reaches 127. You should think long and
hard about doing any of this if your time series is so finite that it has
fewer than 25 observations.
</p>
<p>The Qiu et al. (2013) suggestion is the default lag if you're using
the <span class="pkg">aTSA</span> package. It is almost equivalent to the Schwert (1989) lower
bound, except the length of the series is raised to 2/9 and not 2/8. The two
do not meaningfully separate until the length of the series reaches 5,720
observations (which is when the difference between two reaches two lags of
separation).
</p>


<h3>References</h3>

<p>Qiu, D., Q. Shao, and L. Yang. 2013. &quot;Efficient Inference for Autoregressive
Coefficients in the Presence of Trends.&quot; <em>Jounal of Multivariate Analysis</em>
114: 40&ndash;53.
</p>
<p>Said, Said E. and David A. Dickey. 1984. &quot;Testing for Unit Roots in
Autoregressive-Moving Average Models of Unknown Order.&quot; <em>Biometrika</em> 71(3):
599-607.
</p>
<p>Schwert, G. William. 1989. &quot;Tests for Unit Roots: A Monte Carlo Investigation&quot;.
<em>Journal of Business &amp; Economic Statistics</em> 7(2): 147&ndash;59.
</p>

<hr>
<h2 id='money_demand'>Quarterly Money Demand in the United States</h2><span id='topic+money_demand'></span>

<h3>Description</h3>

<p>A data frame of quarterly indicators useful for modeling the demand for money
in the United States. Data go from the first quarter of 1960 to the third
quarter of 2024.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>money_demand
</code></pre>


<h3>Format</h3>

<p>A data frame with 259 observations on the following 6 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>a date</p>
</dd>
<dt><code>m1</code></dt><dd><p>so-called 'narrow' money (M1) in supply, in billions, not seasonally adjusted</p>
</dd>
<dt><code>m2</code></dt><dd><p>monetary supply (M2), in billions, not seasonally adjusted</p>
</dd>
<dt><code>gnpdef</code></dt><dd><p>an implicit price deflator for gross national product (index, 2017 = 100)</p>
</dd>
<dt><code>ffer</code></dt><dd><p>the federal funds effective rate</p>
</dd>
<dt><code>rgnp</code></dt><dd><p>real gross national product (in 2017 dollars)</p>
</dd>
<dt><code>pcepi</code></dt><dd><p>the chain-type price index (index, 2017 == 100)</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data come by way of <span class="pkg">fredr</span> call. Be mindful of changes in the
definition of the money supply, especially as they manifest in May 2020.
Subject domain expertise goes a long way here. The &quot;M2&quot; indicator is the &quot;M1&quot;
indicator with small-time deposits that are &quot;close substitutes&quot; for M1.
</p>

<hr>
<h2 id='sadf_test'>Simulate a (Augmented) Dickey-Fuller Test to Assess Unit Root in a Time Series</h2><span id='topic+sadf_test'></span>

<h3>Description</h3>

<p><code>sadf_test()</code> provides a simulation approach to assessing
unit root in a time series by way of the (Augmented) Dickey-Fuller test. It
takes a vector and performs three (Augmented) Dickey-Fuller tests (no drift,
no trend; drift, no trend; drift and trend) and calculates tau statistics as
one normally would. Rather than interpolate or approximate a  <em>p</em>-value, it
simulates some user-specified number of (Augmented) Dickey-Fuller tests of
either a known, non-stationary time series or a known, white-noise time series
matching the length of the time series the user provides. This allows the
user to make assessments of non-stationarity or stationarity by way of
simulation rather than approximation from received critical values by way of
books or tables some years out of date.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sadf_test(x, n_lags = NULL, n_sims = 1000, sim_hyp = "nonstationary")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sadf_test_+3A_x">x</code></td>
<td>
<p>a vector</p>
</td></tr>
<tr><td><code id="sadf_test_+3A_n_lags">n_lags</code></td>
<td>
<p>defaults to NULL, but must be 0 or a positive integer. This
argument determines the number of lagged first differences to include in the
estimation procedure. Recall that the test statistic (tau) is still the
t-statistic for the <em>level</em> value of the vector at t-1, whether the constant
(drift) and time trend is included or not. If this value is 0, the procedure
is the classic Dickey-Fuller test. If this value is greater than 0, this is
the &quot;augmented&quot; Dickey-Fuller test, so-called because it is &quot;augmented&quot; by
the number of lagged first differences to assess higher-order AR processes.
If no argument is specified, the default lag is Schwert's suggested lower
bound. The <code>lag_suggests</code> data provides more information about these
suggested lags.</p>
</td></tr>
<tr><td><code id="sadf_test_+3A_n_sims">n_sims</code></td>
<td>
<p>the number of simulations for calculating an interval or
distribution of test statistics for assessing stationarity or
non-stationarity. Defaults to 1,000.</p>
</td></tr>
<tr><td><code id="sadf_test_+3A_sim_hyp">sim_hyp</code></td>
<td>
<p>can be either &quot;stationary&quot; or &quot;nonstationary&quot;. If
&quot;stationary&quot;, the function runs (A)DF tests on simulated stationary
(pure white noise) data. This allows the user to assess
compatibility/plausibility of the test statistic against a distribution of
test statistics that are known to be pure white noise (in expectation). If
&quot;nonstationary&quot; (default), the function generates three different data sets of
a pure random walk, a random walk with a drift, and a random walk with a
drift and trend. It then runs (A)DF tests on all those. This allows the user
to assess the compatibility/plausibility of their test statistics with data
that are known to be nonstationary in some form.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Dickey-Fuller and its &quot;augmented&quot; corollary are curious statistical
procedures, even if the underlying concept is straightforward. I have seen
various implementations of these procedures use slightly different
terminology to describe its procedure, though this particular implementation
will impose nomenclature in which the classic Dickey-Fuller procedure that
assumes just the AR(1) process is one in which <code>n_lags</code> is 0. The
addition of lags (of first differences) is what ultimately makes the
Dickey-Fuller procedure to be &quot;augmented.&quot;
</p>
<p>The function employs the default suggested by Schwert (1989) for the number
of lagged first differences to include in this procedure. Schwert (1989)
recommends taking the length of the series and dividing it by 100 before
raising that number to the power of 1/4. Thereafter, multiply it by 12 and
round down the number to the nearest integer. There are other suggested
defaults you can consider. <code>adf.test</code> in <span class="pkg">aTSA</span> takes the length of
the series, divides it by 100 and raises it to the power of 2/9. It
multiplies that by 4 and floors the result. <code>adf.test</code> in <span class="pkg">tseries</span>
subtracts 1 from the length of the series before raising it to the power of
1/3 (flooring that result as well). The Examples section will show you how
you can do this.
</p>
<p>This function specifies three different types of tests: 1) no drift, no trend,
2) drift, no trend, and 3) drift and trend. In the language of the <code>lm()</code>
function, the first is <code>lm(y ~ ly - 1)</code> where <code>y</code> is the value of <code>y</code> and
<code>ly</code> is its first-order lag. The second test is <code>lm(y ~ ly)</code>, intuitively
suggesting the <em>y</em>-intercept in this equation is the &quot;drift&quot;. The third would
be <code>lm(y ~ ly + t)</code> with <code>t</code> being a simple integer that increases by 1 for
each observation (i.e. a time-trend).
</p>
<p>None of this is meant to discourage the use of Fuller (1976) or its various
reproductions for the sake of diagnosing stationarity or non-stationary, and
I will confess their expertise on these matters outpaces mine. Consider the
justification for this function to be largely philosophical and/or
experimental. Why not simulate it? It's not like time or computing power are
huge issues anymore.
</p>
<p>This is always awkwardly stated, but it's a good reminder that the classic
Dickey-Fuller statistics are mostly intended to come back negative. That's
not always the case, to be clear, but it is the intended case. You assess the
statistic by &quot;how negative&quot; it is. Stationary time series will produce test
statistics more negative (&quot;smaller&quot;) than those produced by non-stationary
time series. In a way, this makes the hypotheses implicitly one-tailed (to
use that language).
</p>
<p>This function removes missing values from the vector before calculating test
statistics.
</p>


<h3>Value</h3>

<p><code>sadf_test()</code> returns a list of length 3. The first element
in the list is a matrix of tau statistics calculated by the test. The second
element is a data frame of the simulated tau statistics of either a known
white-noise time series or three different non-stationary time series
(pure random walk, random walk with drift, random walk with drift and trend).
The third element contains some attributes about the procedure for
post-processing.
</p>


<h3>Author(s)</h3>

<p>Steven V. Miller
</p>


<h3>References</h3>

<p>Schwert, G. William. 1989. &quot;Tests for Unit Roots: A Monte Carlo Investigation.&quot;
<em>Journal of Business &amp; Economic Statistics</em> 7(2): 147&ndash;159.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
y &lt;- na.omit(USDSEK[1:500,])$close # there is one missing value here. n = 499.


sadf_test(y, n_sims = 25) # Doing 25, just to make it quick


</code></pre>

<hr>
<h2 id='sim_df_mod'>Simulate an Individual (Augmented) Dickey-Fuller Model</h2><span id='topic+sim_df_mod'></span>

<h3>Description</h3>

<p><code>sim_df_mod()</code> is designed as a helper function, to be used
internally in this package in <code>sadf_test()</code>. But, you can use it here to
simulate a time series and perform a(n Augmented) Dickey-Fuller test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim_df_mod(x, ts_type, df_lags = NULL, classic_df = FALSE, wn = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sim_df_mod_+3A_x">x</code></td>
<td>
<p>a numeric vector corresponding to a series to replicate/simulate</p>
</td></tr>
<tr><td><code id="sim_df_mod_+3A_ts_type">ts_type</code></td>
<td>
<p>a type of time-series to simulate (either 'ndnt', 'dnt', or 'dt')</p>
</td></tr>
<tr><td><code id="sim_df_mod_+3A_df_lags">df_lags</code></td>
<td>
<p>a numeric vector for the number of lags to calculate for the test.</p>
</td></tr>
<tr><td><code id="sim_df_mod_+3A_classic_df">classic_df</code></td>
<td>
<p>logical, defaults to FALSE. If FALSE, the function calculates
an &quot;Augmented&quot; Dickey-Fuller test on a simulated series with the number of lagged
first differences requested in the <code>df_lags</code> argument. If <code>TRUE</code>, the classic
Dickey-Fuller test is executed without the lagged first differences.</p>
</td></tr>
<tr><td><code id="sim_df_mod_+3A_wn">wn</code></td>
<td>
<p>logical, defaults to FALSE. If FALSE, generates a random
walk of some description for a DF/ADF test. If TRUE, series to be simulated
for a DF/ADF test is white noise.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>classic_df = TRUE</code> suppresses the need to specify <code>df_lags = 0</code>, but
<code>df_lags</code> cannot be 0 if <code>classic_df = FALSE</code>.
</p>
<p>This might change in future iterations, but it's worth clarifying the values
assigned to the parameters of a drift and trend. The drift is randomly
generated from a Rademacher distribution for both the times series with drift
and drift-and-trend. The series with a deterministic trend divides the value
from the Rademacher distribution by 10. My rationale is largely based on what
I've seen other pedagogical guides do, the extent to which they talk about
simulating values for these types of random walks.
</p>


<h3>Value</h3>

<p><code>sim_df_mod()</code> returns the output of a linear model (with class
<code>lm</code>) that performs a(n Augmented) Dickey-Fuller test on a simulated time
series. This is mostly for internal use, but it might pique the user's
interest to see such a test in action independent of simulated summaries
generated by <code>sadf_test()</code>.
</p>


<h3>Author(s)</h3>

<p>Steven V. Miller
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(8675309) # don't want new numbers in documentation every time...

sim_df_mod(rnorm(25), ts_type = 'ndnt', classic_df = TRUE)

sim_df_mod(rnorm(25), ts_type = 'ndnt', df_lags = 2, classic_df = FALSE)

</code></pre>

<hr>
<h2 id='sim_ts'>Simulate a Time Series</h2><span id='topic+sim_ts'></span>

<h3>Description</h3>

<p><code>sim_ts()</code> is mostly a helper function, to be used
internally in this package, but you can use it here to simulate a time series.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim_ts(n, b0 = 0, bt = 0, rho = 1, white_noise = FALSE, rsd = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sim_ts_+3A_n">n</code></td>
<td>
<p>a numeric vector for the length of the series</p>
</td></tr>
<tr><td><code id="sim_ts_+3A_b0">b0</code></td>
<td>
<p>a numeric vector for a potential drift in the series. Defaults to 0</p>
</td></tr>
<tr><td><code id="sim_ts_+3A_bt">bt</code></td>
<td>
<p>a numeric vector for a potential trend in the series. Defaults to 0.</p>
</td></tr>
<tr><td><code id="sim_ts_+3A_rho">rho</code></td>
<td>
<p>a numeric vector for the simple autoregressive parameter. Defaults to 1.</p>
</td></tr>
<tr><td><code id="sim_ts_+3A_white_noise">white_noise</code></td>
<td>
<p>= logical, defaults to FALSE. If FALSE, generates a random
walk. If TRUE, series is white noise.</p>
</td></tr>
<tr><td><code id="sim_ts_+3A_rsd">rsd</code></td>
<td>
<p>the standard deviation for a normal distribution to be simulated. Defaults to 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>sim_ts()</code> returns a numeric vector of a simulated time series
that would follow the user's input.
</p>


<h3>Author(s)</h3>

<p>Steven V. Miller
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(8675309) # don't want new numbers in documentation every time...

sim_ts(25)

sim_ts(25, b0 = 1)

sim_ts(25, b0 = 1, bt = .05)

</code></pre>

<hr>
<h2 id='skpss_test'>Simulate a KPSS Test to Assess Unit Root in a Time Series</h2><span id='topic+skpss_test'></span>

<h3>Description</h3>

<p><code>skpss_test()</code> provides a simulation approach to assessing
unit root in a time series by way of KPSS test, first proposed by Kwiatkowski
et al. (1992). It takes a vector and extracts the residuals from two models to
assess stationarity around a level or trend, producing a KPSS test statistic
(eta). Rather than interpolate or approximate a <em>p</em>-value, it simulates
some user-specified number of KPSS of either a known, stationary time series
(default) or a known, non-stationary time series matching the length of the
time series the user provides. This allows the user to make assessments of
non-stationarity or stationarity by way of simulation rather than
approximation from received critical values by way of various books/tables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>skpss_test(x, lag_short = TRUE, n_sims = 1000, sim_hyp = "stationary")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="skpss_test_+3A_x">x</code></td>
<td>
<p>a vector</p>
</td></tr>
<tr><td><code id="skpss_test_+3A_lag_short">lag_short</code></td>
<td>
<p>logical, defaults to <code>TRUE</code>. If <code>TRUE</code>, the
&quot;short-term&quot; lag is used for the KPSS test. If <code>FALSE</code>, the
&quot;long-term&quot; lag is used. These lags are those suggested by Schwert (1989).</p>
</td></tr>
<tr><td><code id="skpss_test_+3A_n_sims">n_sims</code></td>
<td>
<p>the number of simulations for calculating an interval or
distribution of test statistics for assessing stationarity or
non-stationarity. Defaults to 1,000.</p>
</td></tr>
<tr><td><code id="skpss_test_+3A_sim_hyp">sim_hyp</code></td>
<td>
<p>can be either &quot;stationary&quot; or &quot;nonstationary&quot;. If
&quot;stationary&quot; (default), the function runs KPSS tests on simulated stationary
(pure white noise) data. This allows the user to assess
compatibility/plausibility of the test statistic against a distribution of
test statistics that are known to be pure white noise (in expectation). If
&quot;nonstationary&quot;, the simulations are conducted on two different random walks.
The &quot;trend&quot; test includes a level drawn from a Rademacher distribution with
a time trend of that level, divided by 10.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Recall that this procedure defaults from almost every other unit root test
by making stationarity a null hypothesis. Non-stationarity is the alternative
hypothesis.
</p>
<p>As of writing, the default lags are those suggested by Schwert (1989) to
apply to the Bartlett kernel generating the KPSS test statistic (eta).
</p>
<p><span class="pkg">aTSA</span> has a particularly interesting approach to this test that draws on
on insights seemingly proposed by Hobijn et al. (2004). Future updates may
include those, but this function, as is, performs calculations of eta
identical to what <span class="pkg">tseries</span> or <span class="pkg">urca</span> would produce. Right now,
I don't have the time or energy to get into the weeds of what Hobijn et al.
(2004) are doing or what <span class="pkg">aTSA</span> is implementing, but it seems pretty cool.
</p>
<p>This function removes missing values from the vector before calculating test
statistics.
</p>


<h3>Value</h3>

<p><code>skpss_test()</code> returns a list of length 3. The first element
in the list is a matrix of eta statistics calculated by the test. The first
of those is the level statistic and the second of those is the trend
statistic. The second element is a data frame of the simulated eta statistics,
where the type of simulation (level, trend) is communicated in the <code>cat</code> column.
The third element contains some attributes about the procedure for
post-processing.
</p>


<h3>Author(s)</h3>

<p>Steven V. Miller
</p>


<h3>References</h3>

<p>Hobijn, Bart, Philip Hans Franses, and Marius Ooms. 2004. &quot;Generalizations of
the KPSS-test for Stationarity&quot;. <em>Statistica Neerlandica</em> 58(4): 483&ndash;502.
</p>
<p>Kwiatkowski, Denis, Peter C.B. Phillips, Peter Schmidt, and Yongcheol Shin.
1992. &quot;Testing the Null Hypothesis of Stationarity Against the Alternative
of a Unit Root: How Sure Are We that Economic Time Series Have a Unit Root?&quot;
<em>Journal of Econometrics</em> 54(1-3): 159&ndash;78.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- USDSEK$close[1:500] # note: one missing obs here; becomes series of 499

skpss_test(x, n_sims = 25) # make it quick...


</code></pre>

<hr>
<h2 id='spp_test'>Simulate a Phillips-Perron Test to Assess Unit Root in a Time Series</h2><span id='topic+spp_test'></span>

<h3>Description</h3>

<p><code>spp_test()</code> provides a simulation approach to assessing
unit root in a time series by way of the Phillips-Perron test. It takes a
vector and performs three Phillips-Perron tests (no drift, no trend; drift, no
trend; drift and trend) and calculates both rho and tau statistics as one
normally would. Rather than interpolate or approximate a <em>p</em>-value, it
simulates some user-specified number of Phillips-Perron tests of either a
known, non-stationary time series or a known, white-noise time series
matching the length of the time series the user provides. This allows the
user to make assessments of non-stationarity or stationarity by way of
simulation rather than approximation from received critical values by way of
various books/tables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spp_test(x, lag_short = TRUE, n_sims = 1000, sim_hyp = "nonstationary")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spp_test_+3A_x">x</code></td>
<td>
<p>a vector</p>
</td></tr>
<tr><td><code id="spp_test_+3A_lag_short">lag_short</code></td>
<td>
<p>logical, defaults to <code>TRUE</code>. If <code>TRUE</code>, the
&quot;short-term&quot; lag is used for the Phillips-Perron test. If <code>FALSE</code>, the
&quot;long-term&quot; lag is used.</p>
</td></tr>
<tr><td><code id="spp_test_+3A_n_sims">n_sims</code></td>
<td>
<p>the number of simulations for calculating an interval or
distribution of test statistics for assessing stationarity or
non-stationarity. Defaults to 1,000.</p>
</td></tr>
<tr><td><code id="spp_test_+3A_sim_hyp">sim_hyp</code></td>
<td>
<p>can be either &quot;stationary&quot; or &quot;nonstationary&quot;. If
&quot;stationary&quot;, the function runs Phillips-Perron tests on simulated stationary
(pure white noise) data. This allows the user to assess
compatibility/plausibility of the test statistic against a distribution of
test statistics that are known to be pure white noise (in expectation). If
&quot;nonstationary&quot; (default), the function generates three different data sets
of a pure random walk, a random walk with a drift, and a random walk with a
drift and trend. It then runs Phillips-Perron tests on all those. This allows
the user to assess the compatibility/plausibility of their test statistics
with data that are known to be nonstationary in some form.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Some knowledge of Augmented Dickey-Fuller and the Phillips-Perron
procedure is assumed here. Generally, the Phillips-Perron test purports to
build on the Augmented Dickey-Fuller (ADF) procedure through two primary
means. The first is relaxing the need to be as deliberate about lag structures
as ADF is, typically asking for some kind of long- or short-term lag for the
bandwidth/kernel. The second, related to this, is that its nonparametric
procedure makes it robust to various forms of heteroskedasticity in the error
term.
</p>
<p>The short-term and long-term lags follow the convention introduced in the
Phillips-Perron test. The short-term lag uses the default number of
Newey-West lags, defined as the floor of 4*(n/100)^.25 where <code>n</code> is the length
of the time series. The long-term lag substitutes 4 for 12 in this equation.
We have Schwert (1989) to thank for these defaults. Future iterations of this
function may permit manual lag selection, but this is fine for now.
</p>
<p>This function specifies three different types of tests: 1) no drift, no trend,
2) drift, no trend, and 3) drift and trend. In the language of the <code>lm()</code>
function, the first is <code>lm(y ~ ly - 1)</code> where <code>y</code> is the value of <code>y</code> and
<code>ly</code> is its first-order lag. The second test is <code>lm(y ~ ly)</code>, intuitively
suggesting the <em>y</em>-intercept in this equation is the &quot;drift&quot;. The third would
be <code>lm(y ~ ly + t)</code> with <code>t</code> being a simple integer that increases by 1 for
each observation (i.e. a time-trend).
</p>
<p>There are two types of statistics in the Phillips-Perron test: rho and tau.
Of the two, tau is the more intuitive statistic and compares favorably to its
corollary statistic in the Augmented Dickey-Fuller test. It's why you'll
typically see tau reported as the statistic of interest in other
implementations. rho has its utility for more advanced diagnostics, though.
Both are calculated in this function, though tau is the default statistic.
</p>
<p>None of this is meant to discourage the use of Fuller (1976) or its various
reproductions for the sake of diagnosing stationarity or non-stationary, and
I will confess their expertise on these matters outpaces mine. Consider the
justification for this function to be largely philosophical and/or
experimental. Why not simulate it? It's not like time or computing power are
huge issues anymore.
</p>
<p>This is always awkwardly stated, but it's a good reminder that the classic
Dickey-Fuller statistics are mostly intended to come back negative. That's
not always the case, to be clear, but it is the intended case. You assess the
statistic by &quot;how negative&quot; it is. Stationary time series will produce test
statistics more negative (&quot;smaller&quot;) than those produced by non-stationary
time series. In a way, this makes the hypotheses implicitly one-tailed (to
use that language).
</p>
<p>This function removes missing values from the vector before calculating test
statistics.
</p>


<h3>Value</h3>

<p><code>spp_test()</code> returns a list of length 3. The first element
in the list is a matrix of rho statistics and tau statistics calculated by
the Phillips-Perron test. The second element is a data frame of the simulated
rho and tau statistics of either a known white-noise time series or three
different non-stationary time series (pure random walk, random walk with
drift, random walk with drift and trend). The third element is some
attributes about the procedure for post-processing.
</p>


<h3>Author(s)</h3>

<p>Steven V. Miller
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
a &lt;- rnorm(25) # white noise
b &lt;- cumsum(a) # random walk

spp_test(a, n_sims = 25)
spp_test(b, n_sims = 25)

</code></pre>

<hr>
<h2 id='tbills'>Daily maturity rates for U.S. Treasury Bills</h2><span id='topic+tbills'></span>

<h3>Description</h3>

<p>A data frame on daily (when applicable/available) U.S. Treasury Bill rates.
These are the yield received for investing in a government-issued treasury
security that has a maturity of a given period of time (three months, six
months, or a year).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tbills
</code></pre>


<h3>Format</h3>

<p>A data frame with 17,741 observations on the following 4 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>a date</p>
</dd>
<dt><code>tb3m</code></dt><dd><p>the three-month treasury bill rate</p>
</dd>
<dt><code>tb6m</code></dt><dd><p>the six-month treasury bill rate</p>
</dd>
<dt><code>tb1y</code></dt><dd><p>the one-year treasury bill rate</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data come by way of <span class="pkg">fredr</span> call. The one-year (52-week) treasury
bill rate was discontinued in 2001 and re-introduced in 2008. Be mindful of
that gap in the series.
</p>

<hr>
<h2 id='ur_summary'>Summarize Unit Root Test Simulations</h2><span id='topic+ur_summary'></span>

<h3>Description</h3>

<p><code>ur_summary()</code> provides a summary of the unit root tests
included in this package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ur_summary(obj, pp_stat = "tau", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ur_summary_+3A_obj">obj</code></td>
<td>
<p>the object to be summarized, of class 'spp_test'</p>
</td></tr>
<tr><td><code id="ur_summary_+3A_pp_stat">pp_stat</code></td>
<td>
<p>a statistic to be summarized: either &quot;tau&quot; or &quot;rho&quot;. Applicable
only to Phillips-Perron tests generated by functions in this package.</p>
</td></tr>
<tr><td><code id="ur_summary_+3A_...">...</code></td>
<td>
<p>additional argument, currently ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function makes ample use of the &quot;attributes&quot; element in the
list produced by the unit root simulations.
</p>


<h3>Value</h3>

<p><code>ur_summary()</code> produces console output that offers a summary
assessment about the presence of a unit root based on your simulations.
</p>


<h3>Author(s)</h3>

<p>Steven V. Miller
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
A &lt;- spp_test(money_demand$ffer, n_sims = 100)
ur_summary(A)


</code></pre>

<hr>
<h2 id='USDICE'>Quarterly disposable income and personal consumption expenditures in the United States</h2><span id='topic+USDICE'></span>

<h3>Description</h3>

<p>A data frame on personal consumption expenditures and disposable personal income in the United States.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>USDICE
</code></pre>


<h3>Format</h3>

<p>A data frame with 299 observations on the following 4 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>a date</p>
</dd>
<dt><code>pce</code></dt><dd><p>personal consumption expenditures, seasonally adjusted, in billions</p>
</dd>
<dt><code>dpi</code></dt><dd><p>disposable personal income, seasonally adjusted, in billions</p>
</dd>
<dt><code>pira</code></dt><dd><p>personal income receipts on assets (personal dividend income), in billions</p>
</dd>
<dt><code>cpiu</code></dt><dd><p>consumer price index for all urban consumers (all items in U.S. city average)</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data come by way of <span class="pkg">fredr</span> call. Data are quarterly. Personal
consumption expenditure. disposable personal income, and personal dividend
income are not inflation-adjusted. The data on the consumer price index allow
for such inflation adjustment to &quot;real&quot; dollars based on researcher discretion.
</p>

<hr>
<h2 id='USDSEK'>The USD/SEK Exchange Rate</h2><span id='topic+USDSEK'></span>

<h3>Description</h3>

<p>A data frame on the USD/SEK exchange rate (i.e. how many Swedish crowns does
one dollar get you).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>USDSEK
</code></pre>


<h3>Format</h3>

<p>A data frame with 3905 observations on the following 2 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>a date</p>
</dd>
<dt><code>close</code></dt><dd><p>the exchange rate at the close of trading</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data come by way of <span class="pkg">quantmod</span>.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
