<!DOCTYPE html><html lang="en"><head><title>Help for package ReSurv</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ReSurv}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#data_generator'><p>Individual data generator</p></a></li>
<li><a href='#IndividualDataPP'><p>Individual Data Pre-Processing</p></a></li>
<li><a href='#install_pyresurv'><p>Install Python Environment for ReSurv</p></a></li>
<li><a href='#ooslkh'><p>Compute the out-of-sample likelihood</p></a></li>
<li><a href='#ooslkh.default'><p>Compute the out-of-the sample likelihood</p></a></li>
<li><a href='#ooslkh.ReSurvFit'><p>Compute the out-of-the sample likelihood</p></a></li>
<li><a href='#pkg.env'><p>Helper functions</p></a></li>
<li><a href='#plot.ReSurvFit'><p>Plot for machine learning models</p></a></li>
<li><a href='#plot.ReSurvPredict'><p>Plot of the development factors</p></a></li>
<li><a href='#predict.ReSurvFit'><p>Predict IBNR frequency</p></a></li>
<li><a href='#print.summaryReSurvPredict'><p>Print summary of IBNR predictions</p></a></li>
<li><a href='#ReSurv'><p>Fit <code>ReSurv</code> models on the individual data.</p></a></li>
<li><a href='#ReSurv.default'><p>Fit <code>ReSurv</code> models on the individual data.</p></a></li>
<li><a href='#ReSurv.IndividualDataPP'><p>Fit <code>ReSurv</code> models on the individual data.</p></a></li>
<li><a href='#ReSurvCV'><p>K fold cross-validation of a <code>ReSurv</code> model.</p></a></li>
<li><a href='#ReSurvCV.default'><p>K fold cross-validation of ReSurv model.</p></a></li>
<li><a href='#ReSurvCV.IndividualDataPP'><p>K fold cross-validation of ReSurv model.</p></a></li>
<li><a href='#summary.ReSurvPredict'><p>Summary of IBNR predictions</p></a></li>
<li><a href='#survival_crps'><p>Survival continuously ranked probability score.</p></a></li>
<li><a href='#survival_crps.default'><p>Survival continuously ranked probability score.</p></a></li>
<li><a href='#survival_crps.ReSurvFit'><p>Survival continuously ranked probability score.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Machine Learning Models for Predicting Claim Counts</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Prediction of claim counts using the feature based development factors introduced in the manuscript Hiabu M., Hofman E. and Pittarello G. (2023) &lt;<a href="https://doi.org/10.48550%2FarXiv.2312.14549">doi:10.48550/arXiv.2312.14549</a>&gt;. 
             Implementation of Neural Networks, Extreme Gradient Boosting, 
             and Cox model with splines to optimise the partial log-likelihood of proportional hazard models.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/edhofman/ReSurv">https://github.com/edhofman/ReSurv</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/edhofman/ReSurv/issues">https://github.com/edhofman/ReSurv/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>tidyverse</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, dplyr, dtplyr, fastDummies, forecast, data.table,
purrr, tidyr, tibble, ggplot2, survival, reshape2, bshazard,
SynthETIC, rpart, reticulate, xgboost, SHAPforxgboost</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>Python (&gt;= 3.8.0)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-11-14 08:40:52 UTC; gpitt</td>
</tr>
<tr>
<td>Author:</td>
<td>Emil Hofman [aut, cre, cph],
  Gabriele Pittarello
    <a href="https://orcid.org/0000-0003-3360-5826"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cph],
  Munir Hiabu <a href="https://orcid.org/0000-0001-5846-667X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Emil Hofman &lt;emil_hofman@hotmail.dk&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-11-14 16:00:10 UTC</td>
</tr>
</table>
<hr>
<h2 id='data_generator'>Individual data generator</h2><span id='topic+data_generator'></span>

<h3>Description</h3>

<p>This function generates the monthly individual claims data in the accompanying methodological paper using the <code>SynthETIC</code> package.
This simple function allows to simulate from a sand-box to test out the <code>ReSurv</code> approach.
Some parameters of the simulation can be changed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_generator(
  ref_claim = 2e+05,
  time_unit = 1/360,
  years = 4,
  random_seed = 1964,
  period_exposure = 200,
  period_frequency = 0.2,
  scenario = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="data_generator_+3A_ref_claim">ref_claim</code></td>
<td>
<p><code>integer</code>, reference claim size.</p>
</td></tr>
<tr><td><code id="data_generator_+3A_time_unit">time_unit</code></td>
<td>
<p><code>numeric</code>, output time unit.</p>
</td></tr>
<tr><td><code id="data_generator_+3A_years">years</code></td>
<td>
<p><code>integer</code>, number of years to be simulated.</p>
</td></tr>
<tr><td><code id="data_generator_+3A_random_seed">random_seed</code></td>
<td>
<p><code>integer</code>, random seed for replicable code.</p>
</td></tr>
<tr><td><code id="data_generator_+3A_period_exposure">period_exposure</code></td>
<td>
<p><code>integer</code>, volume (number of policies) underwritten each period.</p>
</td></tr>
<tr><td><code id="data_generator_+3A_period_frequency">period_frequency</code></td>
<td>
<p><code>numeric</code>, expected frequency in each period.</p>
</td></tr>
<tr><td><code id="data_generator_+3A_scenario">scenario</code></td>
<td>
<p><code>character</code> or <code>numeric</code>, one of the scenarios described in the accompanying manuscript. Possible choices are
'alpha' (0), 'beta' (1), 'gamma'(2), 'delta'(3),'epsilon'(4). Our simulated data are constituted of a mix of short tail claims (<code>claim_type 0</code>) and claims with longer resolution (<code>claim_type 1</code>).
We chose the parameter of the simulator to resemble a mix of property damage (<code>claim_type 0</code>) and bodily injuries (<code>claim_type 1</code>). each scenario has distinctive characteristics.
Scenario Alpha is a mix of <code>claim_type 0</code> and  <code>claim_type 1</code> with same number of claims volume at each accident period.
Differently from scenario Alpha, in scenario Beta the volumes of <code>claim_type 1</code> are decreasing in the most recent accident periods.
In scenario Gamma we add an interaction between <code>claim_type 1</code> and accident period: in a real world setting this can be motivated by a change in consumer behavior or company policies resulted in different reporting patterns over time.
In scenario Delta, we introduce a seasonality effect dependent on the accident period for <code>claim_type 0</code> and <code>claim_type 1</code>.
In the real word, scenario Delta resembles seasonal changes in the workforce composition. Scenario Epsilon does not satisfy the proportionality assumption.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Individual claims data. It contains the following columns:
</p>

<ul>
<li><p><code>claim_number</code>: Policy ID.
</p>
</li>
<li><p><code>claim_type</code>: Type of claim. It can be either 0 or 1.
</p>
</li>
<li><p><code>AP</code>: Accident period
</p>
</li>
<li><p><code>RP</code>: Reporting period.
</p>
</li></ul>



<h3>References</h3>

<p>Avanzi, B., Taylor, G., Wang, M., &amp; Wong, B. (2021). SynthETIC: an individual insurance claim simulator with feature control. Insurance: Mathematics and Economics, 100, 296-308.
</p>
<p>Hiabu, M., Hofman, E., &amp; Pittarello, G. (2023). A machine learning approach based on survival analysis for IBNR frequencies in non-life reserving. arXiv preprint arXiv:2312.14549.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>input_data_0 &lt;- data_generator(
random_seed = 1964,
scenario = "alpha",
time_unit = 1,
years = 2,
period_exposure = 100)




</code></pre>

<hr>
<h2 id='IndividualDataPP'>Individual Data Pre-Processing</h2><span id='topic+IndividualDataPP'></span>

<h3>Description</h3>

<p>This function pre-processes the data for the application of a <code>ReSurv</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IndividualDataPP(
  data,
  id = NULL,
  continuous_features = NULL,
  categorical_features = NULL,
  accident_period,
  calendar_period,
  input_time_granularity = "months",
  output_time_granularity = "quarters",
  years = NULL,
  calendar_period_extrapolation = FALSE,
  continuous_features_spline = NULL,
  degrees_cf = 3,
  degrees_of_freedom_cf = 4,
  degrees_cp = 3,
  degrees_of_freedom_cp = 4
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="IndividualDataPP_+3A_data">data</code></td>
<td>
<p><code>data.frame</code>, for the individual reserving. The number of development periods can be larger than the number of accident periods.</p>
</td></tr>
<tr><td><code id="IndividualDataPP_+3A_id">id</code></td>
<td>
<p><code>character</code>, <code>data</code> column that contains the policy identifier. If <code>NULL</code> (default), we assume that each row is an observation. We assume that each observation can only have one reporting time, if not null we take the reporting time of the first row for each <code>id</code>.</p>
</td></tr>
<tr><td><code id="IndividualDataPP_+3A_continuous_features">continuous_features</code></td>
<td>
<p><code>character</code>, continuous features columns to be scaled.</p>
</td></tr>
<tr><td><code id="IndividualDataPP_+3A_categorical_features">categorical_features</code></td>
<td>
<p><code>character</code>, categorical features columns to be one-hot encoded.</p>
</td></tr>
<tr><td><code id="IndividualDataPP_+3A_accident_period">accident_period</code></td>
<td>
<p><code>character</code>, it contains the name of the column in data corresponding to the accident period.</p>
</td></tr>
<tr><td><code id="IndividualDataPP_+3A_calendar_period">calendar_period</code></td>
<td>
<p><code>character</code>, it contains the name of the column in data corresponding to the calendar period.</p>
</td></tr>
<tr><td><code id="IndividualDataPP_+3A_input_time_granularity">input_time_granularity</code></td>
<td>
<p><code>character</code>, time unit of the input data. Granularity supported:
</p>

<ul>
<li><p><code>"days"</code>: the input data are daily.
</p>
</li>
<li><p><code>"months"</code>: the input data are monthly.
</p>
</li>
<li><p><code>"quarters"</code>: the input data are quarterly
</p>
</li>
<li><p><code>"years"</code>: the input data are yearly.
</p>
</li></ul>

<p>Default to <code>months</code>.</p>
</td></tr>
<tr><td><code id="IndividualDataPP_+3A_output_time_granularity">output_time_granularity</code></td>
<td>
<p><code>character</code>, time unit of the output data. The granularity supported is the same as for the input data:
</p>

<ul>
<li><p><code>"days"</code>: the output data will be on a daily scale.
</p>
</li>
<li><p><code>"months"</code>: the output data will be on a monthly scale.
</p>
</li>
<li><p><code>"quarters"</code>: the output data will be on a quarterly scale.
</p>
</li>
<li><p><code>"years"</code>: the output data will be on yearly scale.
</p>
</li></ul>

<p>The output granularity must be bigger than the input granularity.
Also, the output granularity must be consistent with the input granularity, meaning that the time conversion must be possible.
E.g., it is possible to group quarters to years. It is not possible to group quarters to semesters.
Default to <code>quarters</code>.</p>
</td></tr>
<tr><td><code id="IndividualDataPP_+3A_years">years</code></td>
<td>
<p><code>numeric</code>, number of development years in the study.</p>
</td></tr>
<tr><td><code id="IndividualDataPP_+3A_calendar_period_extrapolation">calendar_period_extrapolation</code></td>
<td>
<p><code>character</code>, whether a spline for calendar extrapolation should be considered in the cox model fit.
Default is 'FALSE'.</p>
</td></tr>
<tr><td><code id="IndividualDataPP_+3A_continuous_features_spline">continuous_features_spline</code></td>
<td>
<p><code>logical</code>, weather a spline for smoothing continuous features should be added.</p>
</td></tr>
<tr><td><code id="IndividualDataPP_+3A_degrees_cf">degrees_cf</code></td>
<td>
<p><code>numeric</code>, degrees of the spline for smoothing continuous features.</p>
</td></tr>
<tr><td><code id="IndividualDataPP_+3A_degrees_of_freedom_cf">degrees_of_freedom_cf</code></td>
<td>
<p><code>numeric</code>, degrees of freedom of the splines for smoothing continuous features.</p>
</td></tr>
<tr><td><code id="IndividualDataPP_+3A_degrees_cp">degrees_cp</code></td>
<td>
<p><code>numeric</code>, degrees of the spline for smoothing the calendar period effect.</p>
</td></tr>
<tr><td><code id="IndividualDataPP_+3A_degrees_of_freedom_cp">degrees_of_freedom_cp</code></td>
<td>
<p><code>numeric</code>, degrees of freedom of the splines for smoothing the calendar period effect.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The input <code>accident_period</code> is coded as <code>AP_i</code>. The input development periods are derived as <code>DP_i</code>=<code>calendar_period</code>-<code>accident_period</code>+1.
</p>
<p>The reverse time development factors are <code>DP_rev_i</code> = <code>DP_max</code>-<code>DP_i</code>, where <code>DP_max</code> is the maximum number of development times: <code>DP_i</code> <code class="reqn">=1,\ldots,</code><code>DP_max</code>. Given the parameter <code>years</code>, <code>DP_max</code> is derived internally from our package.
</p>
<p>As for the truncation time, <code>TR_i</code> = <code>AP_i</code>-1.
</p>
<p><code>AP_i</code>, <code>DP_i</code>, <code>DP_rev_i</code> and <code>TR_i</code> are converted to <code>AP_o</code>, <code>DP_o</code>, <code>DP_rev_o</code> and <code>TR_o</code> (from the <code>input_time_granularity</code> to the <code>output_time_granularity</code>) using a multiplicative conversion factor. E.g., <code>AP_o</code> = <code>AP_i</code> * <code class="reqn">CF</code>.
</p>
<p>The conversion factor is computed as
</p>
<p><code class="reqn">CF=\frac{{\nu}^i}{({\nu}^o)^{-1}}</code>,
</p>
<p>where <code class="reqn">{\nu}^i</code> and <code class="reqn">{\nu}^o</code> are the fraction of a year corresponding to <code>input_time_granularity</code> and <code>output_time_granularity</code>. <code class="reqn">{\nu}^i</code> and <code class="reqn">{\nu}^o</code> take values <code>1/360, 1/12, 1/4, 1/2, 1</code> for <code>"days", "months", "quarters", "semesters", "years"</code> respectively.
We will have <code>RP_o</code> = <code>AP_o</code> + <code>DP_o</code>.
</p>


<h3>Value</h3>

<p><code>IndividualDataPP</code> object. A list containing
</p>

<ul>
<li><p><code>full.data</code>: <code>data.frame</code>. The input data after pre-processing.
</p>
</li>
<li><p><code>starting.data</code>: <code>data.frame</code>. The input data as they were provided from the user.
</p>
</li>
<li><p><code>training.data</code>: <code>data.frame</code>. The input data pre-processed for training.
</p>
</li>
<li><p><code>conversion_factor</code>: <code>numeric</code>. The conversion factor for going from input granularity to output granularity. E.g, the conversion factor for going from months to quarters is 1/3.
</p>
</li>
<li><p><code>string_formula_i</code>: <code>character</code>. The <code>survival</code> formula to model the data in input granularity.
</p>
</li>
<li><p><code>string_formula_o</code>: <code>character</code>. The <code>survival</code> formula to model the in data output granularity.
</p>
</li>
<li><p><code>continuous_features</code>: <code>character</code>. The continuous features names as provided from the user.
</p>
</li>
<li><p><code>categorical_features</code>: <code>character</code>. The categorical features names as provided from the user.
</p>
</li>
<li><p><code>calendar_period_extrapolation</code>: <code>logical</code>. The value specifying if a calendar period component is extrapolated.
</p>
</li>
<li><p><code>years</code>: <code>numeric</code>. Total number of development years in the data. Default is NULL and computed automatically from the data.
</p>
</li>
<li><p><code>accident_period</code>: <code>character</code>. Accident period column name.
</p>
</li>
<li><p><code>calendar_period</code>: <code>character</code>. Calendar_period column name.
</p>
</li>
<li><p><code>input_time_granularity</code>: <code>character</code>. Input time granularity.
</p>
</li>
<li><p><code>output_time_granularity</code>: <code>character</code>. Output time granularity.
</p>
</li></ul>

<p>After pre-processing, we provide a standard encoding for the time components. This regards the output in <code>training.data</code> and <code>full.data</code>.
In the <code>ReSurv</code> notation:
</p>

<ul>
<li><p><code>AP_i</code>: Input granularity accident period.
</p>
</li>
<li><p><code>AP_o</code>: Output granularity accident period.
</p>
</li>
<li><p><code>DP_i</code>: Input granularity development period in forward time.
</p>
</li>
<li><p><code>DP_rev_i</code>: Input granularity development period in reverse time.
</p>
</li>
<li><p><code>DP_rev_o</code>: Output granularity development period in reverse time.
</p>
</li>
<li><p><code>TR_i</code>: Input granularity truncation time.
</p>
</li>
<li><p><code>TR_o</code>: Output granularity truncation time.
</p>
</li>
<li><p><code>I</code>: event indicator, under this framework is equal to one for each entry. 
</p>
</li></ul>



<h3>References</h3>

<p>Munir, H., Emil, H., &amp; Gabriele, P. (2023). A machine learning approach based on survival analysis for IBNR frequencies in non-life reserving. arXiv preprint arXiv:2312.14549.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
input_data_0 &lt;- data_generator(
random_seed = 1964,
scenario = "alpha",
time_unit = 1,
years = 2,
period_exposure = 100)

individual_data &lt;- IndividualDataPP(data = input_data_0,
categorical_features = "claim_type",
continuous_features = "AP",
accident_period = "AP",
calendar_period = "RP",
input_time_granularity = "years",
output_time_granularity = "years",
years = 2)




</code></pre>

<hr>
<h2 id='install_pyresurv'>Install Python Environment for ReSurv</h2><span id='topic+install_pyresurv'></span>

<h3>Description</h3>

<p>Install a Python environment that allows the user to apply the Neural Network (NN) models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>install_pyresurv(
  ...,
  envname = "pyresurv",
  new_env = identical(envname, "pyresurv")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="install_pyresurv_+3A_...">...</code></td>
<td>
<p>Additional arguments for 'virtualenv_create'.</p>
</td></tr>
<tr><td><code id="install_pyresurv_+3A_envname">envname</code></td>
<td>
<p>'character'. Name of the environment created. Default 'pyresurv'.</p>
</td></tr>
<tr><td><code id="install_pyresurv_+3A_new_env">new_env</code></td>
<td>
<p>'logical'. If 'TRUE', any existing Python virtual environment and/or 'conda' environment specified by 'envname' is deleted first.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value.
</p>

<hr>
<h2 id='ooslkh'>Compute the out-of-sample likelihood</h2><span id='topic+ooslkh'></span>

<h3>Description</h3>

<p>When the lower triangle data are available, this method computes the likelihood on the lower triangle.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ooslkh(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ooslkh_+3A_object">object</code></td>
<td>
<p><code>ReSurvFit</code> object.</p>
</td></tr>
<tr><td><code id="ooslkh_+3A_...">...</code></td>
<td>
<p>Other arguments to pass to ooslkh.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>numeric</code>, out-of-sample likelihood.
</p>

<hr>
<h2 id='ooslkh.default'>Compute the out-of-the sample likelihood</h2><span id='topic+ooslkh.default'></span>

<h3>Description</h3>

<p>When the lower triangle data are available, this method computes the likelihood on the lower triangle.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
ooslkh(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ooslkh.default_+3A_object">object</code></td>
<td>
<p><code>ReSurvFit</code> object.</p>
</td></tr>
<tr><td><code id="ooslkh.default_+3A_...">...</code></td>
<td>
<p>Other arguments to pass to ooslkh.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>numeric</code>, out-of-sample likelihood.
</p>

<hr>
<h2 id='ooslkh.ReSurvFit'>Compute the out-of-the sample likelihood</h2><span id='topic+ooslkh.ReSurvFit'></span>

<h3>Description</h3>

<p>When the lower triangle data are available, this method computes the likelihood on the lower triangle.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ReSurvFit'
ooslkh(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ooslkh.ReSurvFit_+3A_object">object</code></td>
<td>
<p><code>ReSurvFit</code> object.</p>
</td></tr>
<tr><td><code id="ooslkh.ReSurvFit_+3A_...">...</code></td>
<td>
<p>Other arguments to pass to ooslkh.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>numeric</code>, out-of-sample likelihood.
</p>

<hr>
<h2 id='pkg.env'>Helper functions</h2><span id='topic+pkg.env'></span>

<h3>Description</h3>

<p>This script contains the utils functions that are used in ReSurv.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pkg.env
</code></pre>


<h3>Format</h3>

<p>An object of class <code>environment</code> of length 60.
</p>

<hr>
<h2 id='plot.ReSurvFit'>Plot for machine learning models</h2><span id='topic+plot.ReSurvFit'></span>

<h3>Description</h3>

<p>This function plots the mean absolute SHAP values for the ReSurv fits of machine learning models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ReSurvFit'
plot(x, nsamples = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.ReSurvFit_+3A_x">x</code></td>
<td>
<p><code>ReSurvFit</code> x.</p>
</td></tr>
<tr><td><code id="plot.ReSurvFit_+3A_nsamples">nsamples</code></td>
<td>
<p><code>integer</code>, number of observations to sample for neural networks features importance plot.</p>
</td></tr>
<tr><td><code id="plot.ReSurvFit_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ggplot2</code> of the SHAP values for an <code>"XGB"</code> model or a <code>"NN"</code> model.
</p>

<hr>
<h2 id='plot.ReSurvPredict'>Plot of the development factors</h2><span id='topic+plot.ReSurvPredict'></span>

<h3>Description</h3>

<p>Plots the development factors by group code.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ReSurvPredict'
plot(
  x,
  granularity = "input",
  group_code = 1,
  color_par = "royalblue",
  linewidth_par = 2.5,
  ylim_par = NULL,
  ticks_by_par = NULL,
  base_size_par = NULL,
  title_par = NULL,
  x_text_par = NULL,
  plot.title.size_par = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.ReSurvPredict_+3A_x">x</code></td>
<td>
<p>&quot;ReSurvPredict&quot; object specifying hazard and development factors.</p>
</td></tr>
<tr><td><code id="plot.ReSurvPredict_+3A_granularity">granularity</code></td>
<td>
<p><code>character</code>, either <code>"input"</code> for <code>input_time_granularity</code> or <code>"output"</code> for <code>output_time_granularity</code>.</p>
</td></tr>
<tr><td><code id="plot.ReSurvPredict_+3A_group_code">group_code</code></td>
<td>
<p><code>numeric</code>: Identifier for the group that will be plotted. Default is 1. The code identifiers can be find in the <code>ReSurvPredict$long_triangle_format_out</code> list. Depending on the granularity of interest, it will be either in <code>ReSurvPredict$long_triangle_format_out$input_granularity</code> for <code>input_time_granularity</code> or <code>ReSurvPredict$long_triangle_format_out$output_granularity</code> for <code>output_time_granularity</code>.</p>
</td></tr>
<tr><td><code id="plot.ReSurvPredict_+3A_color_par">color_par</code></td>
<td>
<p><code>character</code>: <code>ggplot2</code> Colour of the line plot. Default is <code>'royalblue'</code>. Optional.</p>
</td></tr>
<tr><td><code id="plot.ReSurvPredict_+3A_linewidth_par">linewidth_par</code></td>
<td>
<p><code>numeric</code>: Line plot width. Optional.</p>
</td></tr>
<tr><td><code id="plot.ReSurvPredict_+3A_ylim_par">ylim_par</code></td>
<td>
<p><code>numeric</code>: Highest intercept on the y-axis (development factors). The default is the highest predicted development factor. Optional.</p>
</td></tr>
<tr><td><code id="plot.ReSurvPredict_+3A_ticks_by_par">ticks_by_par</code></td>
<td>
<p><code>numeric</code>: gap between each x-axis label (development period). Default is 2. Optional.</p>
</td></tr>
<tr><td><code id="plot.ReSurvPredict_+3A_base_size_par">base_size_par</code></td>
<td>
<p><code>numeric</code>: base size of the plot. Default is 5. See <code>base_size</code> in the <code>?theme_bw</code> documentation. Optional.</p>
</td></tr>
<tr><td><code id="plot.ReSurvPredict_+3A_title_par">title_par</code></td>
<td>
<p><code>character</code>: Title of the plot. Optional.</p>
</td></tr>
<tr><td><code id="plot.ReSurvPredict_+3A_x_text_par">x_text_par</code></td>
<td>
<p><code>character</code>: Text on the x-axis. Optional.</p>
</td></tr>
<tr><td><code id="plot.ReSurvPredict_+3A_plot.title.size_par">plot.title.size_par</code></td>
<td>
<p><code>numeric</code>: size of the plot title. Default is 20. See <code>size</code> in the <code>?element_text</code> documentation. Optional.</p>
</td></tr>
<tr><td><code id="plot.ReSurvPredict_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to Plot. Optional.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ggplot2</code> of the development factors
</p>

<hr>
<h2 id='predict.ReSurvFit'>Predict IBNR frequency</h2><span id='topic+predict.ReSurvFit'></span>

<h3>Description</h3>

<p>This function predicts the results from the ReSurv fits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ReSurvFit'
predict(
  object,
  newdata = NULL,
  grouping_method = "probability",
  check_value = 1.85,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.ReSurvFit_+3A_object">object</code></td>
<td>
<p><code>ResurvFit</code> object specifying start time, end time and status.</p>
</td></tr>
<tr><td><code id="predict.ReSurvFit_+3A_newdata">newdata</code></td>
<td>
<p><code>IndividualDataPP</code> object that contains new data to predict.</p>
</td></tr>
<tr><td><code id="predict.ReSurvFit_+3A_grouping_method">grouping_method</code></td>
<td>
<p><code>character</code>, use probability or exposure approach to group from input to output development factors. Choice between:
</p>

<ul>
<li><p><code>"exposure"</code>
</p>
</li>
<li><p><code>"probability"</code>
</p>
</li></ul>

<p>Default is <code>"exposure"</code>.</p>
</td></tr>
<tr><td><code id="predict.ReSurvFit_+3A_check_value">check_value</code></td>
<td>
<p><code>numeric</code>, check hazard value on initial granularity, if above threshold we increase granularity to try and adjust the development factor.</p>
</td></tr>
<tr><td><code id="predict.ReSurvFit_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass to the predict function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Predictions for the <code>ReSurvFit</code> model. It includes
</p>

<ul>
<li><p><code>ReSurvFit</code>: Fitted <code>ReSurv</code> model.
</p>
</li>
<li><p><code>long_triangle_format_out</code>: <code>data.frame</code>. Predicted development factors and IBNR claim counts for each feature combination in long format.
</p>

<ul>
<li><p><code>input_granularity</code>: <code>data.frame</code>. Predictions for each feature combination in long format for <code>input_time_granularity</code>.
</p>

<ul>
<li><p><code>AP_i</code>: Accident period, <code>input_time_granularity</code>.
</p>
</li>
<li><p><code>DP_i</code>: Development period, <code>input_time_granularity</code>.
</p>
</li>
<li><p><code>f_i</code>: Predicted development factors, <code>input_time_granularity</code>.
</p>
</li>
<li><p><code>group_i</code>: Group code, <code>input_time_granularity</code>. This associates to each feature combination an identifier.
</p>
</li>
<li><p><code>expected_counts</code>: Expected counts, <code>input_time_granularity</code>.
</p>
</li>
<li><p><code>IBNR</code>: Predicted IBNR claim counts, <code>input_time_granularity</code>.
</p>
</li></ul>

</li>
<li><p><code>output_granularity</code>: <code>data.frame</code>. Predictions for each feature combination in long format for <code>output_time_granularity</code>.
</p>

<ul>
<li><p><code>AP_o</code>: Accident period, <code>output_time_granularity</code>.
</p>
</li>
<li><p><code>DP_o</code>: Development period, <code>output_time_granularity</code>.
</p>
</li>
<li><p><code>f_o</code>: Predicted development factors, <code>output_time_granularity</code>.
</p>
</li>
<li><p><code>group_o</code>: Group code, <code>output_time_granularity</code>. This associates to each feature combination an identifier.
</p>
</li>
<li><p><code>expected_counts</code>: Expected counts, <code>output_time_granularity</code>.
</p>
</li>
<li><p><code>IBNR</code>: Predicted IBNR claim counts, <code>output_time_granularity</code>.
</p>
</li></ul>

</li></ul>

</li>
<li><p><code>lower_triangle</code>: Predicted lower triangle.
</p>

<ul>
<li><p><code>input_granularity</code>: <code>data.frame</code>. Predicted lower triangle for <code>input_time_granularity</code>.
</p>
</li>
<li><p><code>output_granularity</code>: <code>data.frame</code>. Predicted lower triangle for <code>output_time_granularity</code>.
</p>
</li></ul>

</li>
<li><p><code>predicted_counts</code>: <code>numeric</code>. Predicted total frequencies.
</p>
</li>
<li><p><code>grouping_method</code>: <code>character</code>. Chosen grouping method.
</p>
</li></ul>


<hr>
<h2 id='print.summaryReSurvPredict'>Print summary of IBNR predictions</h2><span id='topic+print.summaryReSurvPredict'></span>

<h3>Description</h3>

<p>Gives overview of IBNr predictions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'summaryReSurvPredict'
print(x, digits = max(3L, getOption("digits") - 3L), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.summaryReSurvPredict_+3A_x">x</code></td>
<td>
<p>&quot;ReSurvPredict&quot; object specifying hazard and development factors.</p>
</td></tr>
<tr><td><code id="print.summaryReSurvPredict_+3A_digits">digits</code></td>
<td>
<p><code>numeric</code>, number of digits to print for IBNR and Likelihood.</p>
</td></tr>
<tr><td><code id="print.summaryReSurvPredict_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to print.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>print of summary of predictions
</p>

<hr>
<h2 id='ReSurv'>Fit <code>ReSurv</code> models on the individual data.</h2><span id='topic+ReSurv'></span>

<h3>Description</h3>

<p>This function fits and computes the reserves for the <code>ReSurv</code> models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ReSurv(
  IndividualDataPP,
  hazard_model = "COX",
  tie = "efron",
  baseline = "spline",
  continuous_features_scaling_method = "minmax",
  random_seed = 1,
  hparameters = list(),
  percentage_data_training = 0.8,
  grouping_method = "exposure",
  check_value = 1.85
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ReSurv_+3A_individualdatapp">IndividualDataPP</code></td>
<td>
<p>IndividualDataPP object to use for the <code>ReSurv</code> fit.</p>
</td></tr>
<tr><td><code id="ReSurv_+3A_hazard_model">hazard_model</code></td>
<td>
<p><code>character</code>, hazard model supported from our package, must be provided as a string. The model can be chosen from:
</p>

<ul>
<li><p><code>"COX"</code>: Standard Cox model for the hazard.
</p>
</li>
<li><p><code>"NN"</code>: Deep Survival Neural Network.
</p>
</li>
<li><p><code>"XGB"</code>: eXtreme Gradient Boosting.
</p>
</li></ul>
</td></tr>
<tr><td><code id="ReSurv_+3A_tie">tie</code></td>
<td>
<p>ties handling, default is the Efron approach.</p>
</td></tr>
<tr><td><code id="ReSurv_+3A_baseline">baseline</code></td>
<td>
<p>handling the baseline hazard. Default is a spline.</p>
</td></tr>
<tr><td><code id="ReSurv_+3A_continuous_features_scaling_method">continuous_features_scaling_method</code></td>
<td>
<p>method to preprocess the features</p>
</td></tr>
<tr><td><code id="ReSurv_+3A_random_seed">random_seed</code></td>
<td>
<p><code>integer</code>, random seed set for reproducibility</p>
</td></tr>
<tr><td><code id="ReSurv_+3A_hparameters">hparameters</code></td>
<td>
<p><code>list</code>, hyperparameters for the machine learning models. It will be disregarded for the cox approach.</p>
</td></tr>
<tr><td><code id="ReSurv_+3A_percentage_data_training">percentage_data_training</code></td>
<td>
<p><code>numeric</code>, percentage of data used for training on the upper triangle.</p>
</td></tr>
<tr><td><code id="ReSurv_+3A_grouping_method">grouping_method</code></td>
<td>
<p><code>character</code>, use probability or exposure approach to group from input to output development factors. Choice between:
</p>

<ul>
<li><p><code>"exposure"</code>
</p>
</li>
<li><p><code>"probability"</code>
</p>
</li></ul>

<p>Default is <code>"exposure"</code>.</p>
</td></tr>
<tr><td><code id="ReSurv_+3A_check_value">check_value</code></td>
<td>
<p><code>numeric</code>, check hazard value on initial granularity, if above threshold we increase granularity to try and adjust the development factor.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model fit uses the theoretical framework of Hiabu et al. (2023), that relies on the
correspondence between hazard models and development factors:
</p>
<p>To be completed with final notation of the paper.
</p>
<p>The <code>ReSurv</code> package assumes proportional hazard models.
Given an i.i.d. sample <code class="reqn">\left\{y_i,x_i\right\}_{i=1, \ldots, n}</code> the individual hazard at time <code class="reqn">t</code> is:
</p>
<p><code class="reqn">\lambda_i(t)=\lambda_0(t)e^{y_i(x_i)}</code>
</p>
<p>Composed of a baseline <code class="reqn">\lambda_0(t)</code> and a proportional effect <code class="reqn">e^{y_i(x_i)}</code>.
</p>
<p>Currently, the implementation allows to optimize the partial likelihood (concerning the proportional effects) using one of the following statistical learning approaches:
</p>

<ul>
<li><p><a href="https://github.com/therneau/survival">COX</a>
</p>
</li>
<li><p><a href="https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-018-0482-1">Neural Networks</a>
</p>
</li>
<li><p><a href="https://xgboost.readthedocs.io/en/stable/">eXtreme Gradient Boosting</a>
</p>
</li></ul>



<h3>Value</h3>

<p><code>ReSurv</code> fit. A list containing
</p>

<ul>
<li><p><code>model.out</code>: <code>list</code> containing the pre-processed covariates data for the fit (<code>data</code>) and the basic model output (<code>model.out</code>;COX, XGB or NN).
</p>
</li>
<li><p><code>is_lkh</code>: <code>numeric</code> Training negative log likelihood.
</p>
</li>
<li><p><code>os_lkh</code>:  <code>numeric</code> Validation  negative log likelihood. Not available for COX.
</p>
</li>
<li><p><code>hazard_frame</code>: <code>data.frame</code> containing the fitted hazard model with the corresponding covariates. It contains:
</p>

<ul>
<li><p><code>expg</code>: fitted risk score.
</p>
</li>
<li><p><code>baseline</code>: fitted baseline.
</p>
</li>
<li><p><code>hazard</code>: fitted hazard rate (<code>expg</code>*<code>baseline</code>).
</p>
</li>
<li><p><code>f_i</code>: fitted development factors.
</p>
</li>
<li><p><code>cum_f_i</code>: fitted cumulative development factors.
</p>
</li>
<li><p><code>S_i</code>:fitted survival function.
</p>
</li>
<li><p><code>S_i_lag</code>:fitted survival function (lag version, for further information see <code>?dplyr::lag</code>).
</p>
</li>
<li><p><code>S_i_lead</code>:fitted survival function (lead version, for further information see <code>?dplyr::lead</code>).
</p>
</li></ul>

</li>
<li><p><code>hazard_model</code>: <code>string</code> chosen hazard model (COX, NN or XGB)
</p>
</li>
<li><p><code>IndividualDataPP</code>: starting <code>IndividualDataPP</code> object.
</p>
</li></ul>



<h3>References</h3>

<p>Munir, H., Emil, H., &amp; Gabriele, P. (2023). A machine learning approach based on survival analysis for IBNR frequencies in non-life reserving. arXiv preprint arXiv:2312.14549.
</p>
<p>Therneau, T. M., &amp; Lumley, T. (2015). Package ‘survival’. R Top Doc, 128(10), 28-33.
</p>
<p>Katzman, J. L., Shaham, U., Cloninger, A., Bates, J., Jiang, T., &amp; Kluger, Y. (2018). DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network. BMC medical research methodology, 18(1), 1-12.
</p>
<p>Chen, T., He, T., Benesty, M., &amp; Khotilovich, V. (2019). Package ‘xgboost’. R version, 90, 1-66.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
input_data_0 &lt;- data_generator(
random_seed = 1964,
scenario = "alpha",
time_unit = 1,
years = 4,
period_exposure = 100)

individual_data &lt;- IndividualDataPP(data = input_data_0,
categorical_features = "claim_type",
continuous_features = "AP",
accident_period = "AP",
calendar_period = "RP",
input_time_granularity = "years",
output_time_granularity = "years",
years=4)


resurv_fit_cox &lt;- ReSurv(individual_data,
hazard_model = "COX")




</code></pre>

<hr>
<h2 id='ReSurv.default'>Fit <code>ReSurv</code> models on the individual data.</h2><span id='topic+ReSurv.default'></span>

<h3>Description</h3>

<p>This function fits and computes the reserves for the <code>ReSurv</code> models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
ReSurv(
  IndividualDataPP,
  hazard_model = "COX",
  tie = "efron",
  baseline = "spline",
  continuous_features_scaling_method = "minmax",
  random_seed = 1,
  hparameters = list(),
  percentage_data_training = 0.8,
  grouping_method = "exposure",
  check_value = 1.85
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ReSurv.default_+3A_individualdatapp">IndividualDataPP</code></td>
<td>
<p>IndividualDataPP object to use for the <code>ReSurv</code> fit.</p>
</td></tr>
<tr><td><code id="ReSurv.default_+3A_hazard_model">hazard_model</code></td>
<td>
<p><code>character</code>, hazard model supported from our package, must be provided as a string. The model can be chosen from:
</p>

<ul>
<li><p><code>"COX"</code>: Standard Cox model for the hazard.
</p>
</li>
<li><p><code>"NN"</code>: Deep Survival Neural Network.
</p>
</li>
<li><p><code>"XGB"</code>: eXtreme Gradient Boosting.
</p>
</li></ul>
</td></tr>
<tr><td><code id="ReSurv.default_+3A_tie">tie</code></td>
<td>
<p>ties handling, default is the Efron approach.</p>
</td></tr>
<tr><td><code id="ReSurv.default_+3A_baseline">baseline</code></td>
<td>
<p>handling the baseline hazard. Default is a spline.</p>
</td></tr>
<tr><td><code id="ReSurv.default_+3A_continuous_features_scaling_method">continuous_features_scaling_method</code></td>
<td>
<p>method to preprocess the features</p>
</td></tr>
<tr><td><code id="ReSurv.default_+3A_random_seed">random_seed</code></td>
<td>
<p><code>integer</code>, random seed set for reproducibility</p>
</td></tr>
<tr><td><code id="ReSurv.default_+3A_hparameters">hparameters</code></td>
<td>
<p><code>list</code>, hyperparameters for the machine learning models. It will be disregarded for the cox approach.</p>
</td></tr>
<tr><td><code id="ReSurv.default_+3A_percentage_data_training">percentage_data_training</code></td>
<td>
<p><code>numeric</code>, percentage of data used for training on the upper triangle.</p>
</td></tr>
<tr><td><code id="ReSurv.default_+3A_grouping_method">grouping_method</code></td>
<td>
<p><code>character</code>, use probability or exposure approach to group from input to output development factors. Choice between:
</p>

<ul>
<li><p><code>"exposure"</code>
</p>
</li>
<li><p><code>"probability"</code>
</p>
</li></ul>

<p>Default is <code>"exposure"</code>.</p>
</td></tr>
<tr><td><code id="ReSurv.default_+3A_check_value">check_value</code></td>
<td>
<p><code>numeric</code>, check hazard value on initial granularity, if above threshold we increase granularity to try and adjust the development factor.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model fit uses the theoretical framework of Hiabu et al. (2023), that relies on the
correspondence between hazard models and development factors:
</p>
<p>To be completed with final notation of the paper.
</p>
<p>The <code>ReSurv</code> package assumes proportional hazard models.
Given an i.i.d. sample <code class="reqn">\left\{y_i,x_i\right\}_{i=1, \ldots, n}</code> the individual hazard at time <code class="reqn">t</code> is:
</p>
<p><code class="reqn">\lambda_i(t)=\lambda_0(t)e^{y_i(x_i)}</code>
</p>
<p>Composed of a baseline <code class="reqn">\lambda_0(t)</code> and a proportional effect <code class="reqn">e^{y_i(x_i)}</code>.
</p>
<p>Currently, the implementation allows to optimize the partial likelihood (concerning the proportional effects) using one of the following statistical learning approaches:
</p>

<ul>
<li><p><a href="https://github.com/therneau/survival">COX</a>
</p>
</li>
<li><p><a href="https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-018-0482-1">Neural Networks</a>
</p>
</li>
<li><p><a href="https://xgboost.readthedocs.io/en/stable/">eXtreme Gradient Boosting</a>
</p>
</li></ul>



<h3>Value</h3>

<p><code>ReSurv</code> fit. A list containing
</p>

<ul>
<li><p><code>model.out</code>: <code>list</code> containing the pre-processed covariates data for the fit (<code>data</code>) and the basic model output (<code>model.out</code>;COX, XGB or NN).
</p>
</li>
<li><p><code>is_lkh</code>: <code>numeric</code> Training negative log likelihood.
</p>
</li>
<li><p><code>os_lkh</code>:  <code>numeric</code> Validation  negative log likelihood. Not available for COX.
</p>
</li>
<li><p><code>hazard_frame</code>: <code>data.frame</code> containing the fitted hazard model with the corresponding covariates. It contains:
</p>

<ul>
<li><p><code>expg</code>: fitted risk score.
</p>
</li>
<li><p><code>baseline</code>: fitted baseline.
</p>
</li>
<li><p><code>hazard</code>: fitted hazard rate (<code>expg</code>*<code>baseline</code>).
</p>
</li>
<li><p><code>f_i</code>: fitted development factors.
</p>
</li>
<li><p><code>cum_f_i</code>: fitted cumulative development factors.
</p>
</li>
<li><p><code>S_i</code>:fitted survival function.
</p>
</li>
<li><p><code>S_i_lag</code>:fitted survival function (lag version, for further information see <code>?dplyr::lag</code>).
</p>
</li>
<li><p><code>S_i_lead</code>:fitted survival function (lead version, for further information see <code>?dplyr::lead</code>).
</p>
</li></ul>

</li>
<li><p><code>hazard_model</code>: <code>string</code> chosen hazard model (COX, NN or XGB)
</p>
</li>
<li><p><code>IndividualDataPP</code>: starting <code>IndividualDataPP</code> object.
</p>
</li></ul>



<h3>References</h3>

<p>Pittarello, G., Hiabu, M., &amp; Villegas, A. M. (2023). Chain Ladder Plus: a versatile approach for claims reserving. arXiv preprint arXiv:2301.03858.
</p>
<p>Therneau, T. M., &amp; Lumley, T. (2015). Package ‘survival’. R Top Doc, 128(10), 28-33.
</p>
<p>Katzman, J. L., Shaham, U., Cloninger, A., Bates, J., Jiang, T., &amp; Kluger, Y. (2018). DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network. BMC medical research methodology, 18(1), 1-12.
</p>
<p>Chen, T., He, T., Benesty, M., &amp; Khotilovich, V. (2019). Package ‘xgboost’. R version, 90, 1-66.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
input_data_0 &lt;- data_generator(
random_seed = 1964,
scenario = "alpha",
time_unit = 1,
years = 4,
period_exposure = 100)

individual_data &lt;- IndividualDataPP(data = input_data_0,
categorical_features = "claim_type",
continuous_features = "AP",
accident_period = "AP",
calendar_period = "RP",
input_time_granularity = "years",
output_time_granularity = "years",
years=4)


resurv_fit_cox &lt;- ReSurv(individual_data,
hazard_model = "COX")





</code></pre>

<hr>
<h2 id='ReSurv.IndividualDataPP'>Fit <code>ReSurv</code> models on the individual data.</h2><span id='topic+ReSurv.IndividualDataPP'></span>

<h3>Description</h3>

<p>This function fits and computes the reserves for the <code>ReSurv</code> models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'IndividualDataPP'
ReSurv(
  IndividualDataPP,
  hazard_model = "COX",
  tie = "efron",
  baseline = "spline",
  continuous_features_scaling_method = "minmax",
  random_seed = 1,
  hparameters = list(),
  percentage_data_training = 0.8,
  grouping_method = "exposure",
  check_value = 1.85
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ReSurv.IndividualDataPP_+3A_individualdatapp">IndividualDataPP</code></td>
<td>
<p>IndividualDataPP object to use for the <code>ReSurv</code> fit.</p>
</td></tr>
<tr><td><code id="ReSurv.IndividualDataPP_+3A_hazard_model">hazard_model</code></td>
<td>
<p><code>character</code>, hazard model supported from our package, must be provided as a string. The model can be chosen from:
</p>

<ul>
<li><p><code>"COX"</code>: Standard Cox model for the hazard.
</p>
</li>
<li><p><code>"NN"</code>: Deep Survival Neural Network.
</p>
</li>
<li><p><code>"XGB"</code>: eXtreme Gradient Boosting.
</p>
</li></ul>
</td></tr>
<tr><td><code id="ReSurv.IndividualDataPP_+3A_tie">tie</code></td>
<td>
<p>ties handling, default is the Efron approach.</p>
</td></tr>
<tr><td><code id="ReSurv.IndividualDataPP_+3A_baseline">baseline</code></td>
<td>
<p>handling the baseline hazard. Default is a spline.</p>
</td></tr>
<tr><td><code id="ReSurv.IndividualDataPP_+3A_continuous_features_scaling_method">continuous_features_scaling_method</code></td>
<td>
<p>method to preprocess the features</p>
</td></tr>
<tr><td><code id="ReSurv.IndividualDataPP_+3A_random_seed">random_seed</code></td>
<td>
<p><code>integer</code>, random seed set for reproducibility</p>
</td></tr>
<tr><td><code id="ReSurv.IndividualDataPP_+3A_hparameters">hparameters</code></td>
<td>
<p><code>list</code>, hyperparameters for the machine learning models. It will be disregarded for the cox approach.</p>
</td></tr>
<tr><td><code id="ReSurv.IndividualDataPP_+3A_percentage_data_training">percentage_data_training</code></td>
<td>
<p><code>numeric</code>, percentage of data used for training on the upper triangle.</p>
</td></tr>
<tr><td><code id="ReSurv.IndividualDataPP_+3A_grouping_method">grouping_method</code></td>
<td>
<p><code>character</code>, use probability or exposure approach to group from input to output development factors. Choice between:
</p>

<ul>
<li><p><code>"exposure"</code>
</p>
</li>
<li><p><code>"probability"</code>
</p>
</li></ul>

<p>Default is <code>"exposure"</code>.</p>
</td></tr>
<tr><td><code id="ReSurv.IndividualDataPP_+3A_check_value">check_value</code></td>
<td>
<p><code>numeric</code>, check hazard value on initial granularity, if above threshold we increase granularity to try and adjust the development factor.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model fit uses the theoretical framework of Hiabu et al. (2023), that relies on the
correspondence between hazard models and development factors:
</p>
<p>To be completed with final notation of the paper.
</p>
<p>The <code>ReSurv</code> package assumes proportional hazard models.
Given an i.i.d. sample <code class="reqn">\left\{y_i,x_i\right\}_{i=1, \ldots, n}</code> the individual hazard at time <code class="reqn">t</code> is:
</p>
<p><code class="reqn">\lambda_i(t)=\lambda_0(t)e^{y_i(x_i)}</code>
</p>
<p>Composed of a baseline <code class="reqn">\lambda_0(t)</code> and a proportional effect <code class="reqn">e^{y_i(x_i)}</code>.
</p>
<p>Currently, the implementation allows to optimize the partial likelihood (concerning the proportional effects) using one of the following statistical learning approaches:
</p>

<ul>
<li><p><a href="https://github.com/therneau/survival">COX</a>
</p>
</li>
<li><p><a href="https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-018-0482-1">Neural Networks</a>
</p>
</li>
<li><p><a href="https://xgboost.readthedocs.io/en/stable/">eXtreme Gradient Boosting</a>
</p>
</li></ul>



<h3>Value</h3>

<p><code>ReSurv</code> fit. A list containing
</p>

<ul>
<li><p><code>model.out</code>: <code>list</code> containing the pre-processed covariates data for the fit (<code>data</code>) and the basic model output (<code>model.out</code>;COX, XGB or NN).
</p>
</li>
<li><p><code>is_lkh</code>: <code>numeric</code> Training negative log likelihood.
</p>
</li>
<li><p><code>os_lkh</code>:  <code>numeric</code> Validation  negative log likelihood. Not available for COX.
</p>
</li>
<li><p><code>hazard_frame</code>: <code>data.frame</code> containing the fitted hazard model with the corresponding covariates. It contains:
</p>

<ul>
<li><p><code>expg</code>: fitted risk score.
</p>
</li>
<li><p><code>baseline</code>: fitted baseline.
</p>
</li>
<li><p><code>hazard</code>: fitted hazard rate (<code>expg</code>*<code>baseline</code>).
</p>
</li>
<li><p><code>f_i</code>: fitted development factors.
</p>
</li>
<li><p><code>cum_f_i</code>: fitted cumulative development factors.
</p>
</li>
<li><p><code>S_i</code>:fitted survival function.
</p>
</li>
<li><p><code>S_i_lag</code>:fitted survival function (lag version, for further information see <code>?dplyr::lag</code>).
</p>
</li>
<li><p><code>S_i_lead</code>:fitted survival function (lead version, for further information see <code>?dplyr::lead</code>).
</p>
</li></ul>

</li>
<li><p><code>hazard_model</code>: <code>string</code> chosen hazard model (COX, NN or XGB)
</p>
</li>
<li><p><code>IndividualDataPP</code>: starting <code>IndividualDataPP</code> object.
</p>
</li></ul>



<h3>References</h3>

<p>Pittarello, G., Hiabu, M., &amp; Villegas, A. M. (2023). Chain Ladder Plus: a versatile approach for claims reserving. arXiv preprint arXiv:2301.03858.
</p>
<p>Therneau, T. M., &amp; Lumley, T. (2015). Package ‘survival’. R Top Doc, 128(10), 28-33.
</p>
<p>Katzman, J. L., Shaham, U., Cloninger, A., Bates, J., Jiang, T., &amp; Kluger, Y. (2018). DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network. BMC medical research methodology, 18(1), 1-12.
</p>
<p>Chen, T., He, T., Benesty, M., &amp; Khotilovich, V. (2019). Package ‘xgboost’. R version, 90, 1-66.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
input_data_0 &lt;- data_generator(
random_seed = 1964,
scenario = "alpha",
time_unit = 1,
years = 4,
period_exposure = 100)

individual_data &lt;- IndividualDataPP(data = input_data_0,
categorical_features = "claim_type",
continuous_features = "AP",
accident_period = "AP",
calendar_period = "RP",
input_time_granularity = "years",
output_time_granularity = "years",
years=4)


resurv_fit_cox &lt;- ReSurv(individual_data,
hazard_model = "COX")





</code></pre>

<hr>
<h2 id='ReSurvCV'>K fold cross-validation of a <code>ReSurv</code> model.</h2><span id='topic+ReSurvCV'></span>

<h3>Description</h3>

<p>This function computes a K fold cross-validation of a pre-specified machine learning model supported from the <code>ReSurv</code> package for a given grid of hyperparameters.
The hyperparameters to be tested are provided in a list, namely <code>hparameters_grid</code>.
Conversely, the parameters for the models run are provided separately as arguments and they are specific for each machine learning model support from.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ReSurvCV(
  IndividualDataPP,
  model,
  hparameters_grid,
  folds,
  random_seed,
  continuous_features_scaling_method = "minmax",
  print_every_n = 1L,
  nrounds = NULL,
  early_stopping_rounds = NULL,
  epochs = 1,
  parallel = FALSE,
  ncores = 1,
  num_workers = 0,
  verbose = FALSE,
  verbose.cv = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ReSurvCV_+3A_individualdatapp">IndividualDataPP</code></td>
<td>
<p><code>IndividualDataPP</code> object to use for the <code>ReSurv</code> fit cross-validation.</p>
</td></tr>
<tr><td><code id="ReSurvCV_+3A_model">model</code></td>
<td>
<p><code>character</code>, machine learning for cross validation.</p>
</td></tr>
<tr><td><code id="ReSurvCV_+3A_hparameters_grid">hparameters_grid</code></td>
<td>
<p><code>list</code>, grid of the hyperparameters to cross-validate.</p>
</td></tr>
<tr><td><code id="ReSurvCV_+3A_folds">folds</code></td>
<td>
<p><code>integer</code>, number of folds (i.e. K).</p>
</td></tr>
<tr><td><code id="ReSurvCV_+3A_random_seed">random_seed</code></td>
<td>
<p><code>integer</code>, random seed for making the code reproducible.</p>
</td></tr>
<tr><td><code id="ReSurvCV_+3A_continuous_features_scaling_method">continuous_features_scaling_method</code></td>
<td>
<p><code>character</code>, method for scaling continuous features.</p>
</td></tr>
<tr><td><code id="ReSurvCV_+3A_print_every_n">print_every_n</code></td>
<td>
<p><code>integer</code>, specific to the <code>XGB</code> approach, see <code>xgboost::xgb.train</code> documentation.</p>
</td></tr>
<tr><td><code id="ReSurvCV_+3A_nrounds">nrounds</code></td>
<td>
<p><code>integer</code>, specific to <code>XGB</code>, max number of boosting iterations.</p>
</td></tr>
<tr><td><code id="ReSurvCV_+3A_early_stopping_rounds">early_stopping_rounds</code></td>
<td>
<p><code>integer</code>, specific to the <code>XGB</code> approach, see <code>xgboost::xgb.train</code> documentation.</p>
</td></tr>
<tr><td><code id="ReSurvCV_+3A_epochs">epochs</code></td>
<td>
<p><code>integer</code>, specific to the <code>NN</code> approach, epochs to be checked.</p>
</td></tr>
<tr><td><code id="ReSurvCV_+3A_parallel">parallel</code></td>
<td>
<p><code>logical</code>, specific to the <code>NN</code> approach, whether to use parallel computing.</p>
</td></tr>
<tr><td><code id="ReSurvCV_+3A_ncores">ncores</code></td>
<td>
<p><code>integer</code>, specific to <code>NN</code>, max number of cores used.</p>
</td></tr>
<tr><td><code id="ReSurvCV_+3A_num_workers">num_workers</code></td>
<td>
<p><code>numeric</code>, number of workers for the <code>NN</code> approach, multi-process data loading with the specified number of loader worker processes.</p>
</td></tr>
<tr><td><code id="ReSurvCV_+3A_verbose">verbose</code></td>
<td>
<p><code>logical</code>, whether messages from the machine learning models must be printed.</p>
</td></tr>
<tr><td><code id="ReSurvCV_+3A_verbose.cv">verbose.cv</code></td>
<td>
<p><code>logical</code>, whether messages from cross-validation must be printed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Best <code>ReSurv</code> model fit. The output is different depending on the machine learning approach that is required for cross-validation. A list containing:
</p>

<ul>
<li><p><code>out.cv</code>: <code>data.frame</code>, total output of the cross-validation (all the input parameters combinations). 
</p>
</li>
<li><p><code>out.cv.best.oos</code>:  <code>data.frame</code>, combination with the best out of sample likelihood. 
</p>
</li></ul>

<p>For XGB the columns in <code>out.cv</code> and <code>out.cv.best.oos</code> are the hyperparameters <code>booster</code>, <code>eta</code>, <code>max_depth</code>, <code>subsample</code>, <code>alpha</code>, <code>lambda</code>, <code>min_child_weight</code>. They also contain the metrics <code>train.lkh</code>, <code>test.lkh</code>, and the computational time <code>time</code>. For NN the columns in <code>out.cv</code> and <code>out.cv.best.oos</code> are the hyperparameters <code>num_layers</code>, <code>optim</code>, <code>activation</code>, <code>lr</code>, <code>xi</code>, <code>eps</code>, <code>tie</code>, <code>batch_size</code>, <code>early_stopping</code>, <code>patience</code>, <code>node</code> train.lkh test.lkh. They also contain the metrics <code>train.lkh</code>, <code>test.lkh</code>, and the computational time <code>time</code>.
</p>


<h3>References</h3>

<p>Munir, H., Emil, H., &amp; Gabriele, P. (2023). A machine learning approach based on survival analysis for IBNR frequencies in non-life reserving. arXiv preprint arXiv:2312.14549.
</p>

<hr>
<h2 id='ReSurvCV.default'>K fold cross-validation of ReSurv model.</h2><span id='topic+ReSurvCV.default'></span>

<h3>Description</h3>

<p>This function computes a K fold cross-validation of a pre-specified ReSurv model for a given grid of parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
ReSurvCV(
  IndividualDataPP,
  model,
  hparameters_grid,
  folds,
  random_seed,
  continuous_features_scaling_method = "minmax",
  print_every_n = 1L,
  nrounds = NULL,
  early_stopping_rounds = NULL,
  epochs = 1,
  parallel = FALSE,
  ncores = 1,
  num_workers = 0,
  verbose = FALSE,
  verbose.cv = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ReSurvCV.default_+3A_individualdatapp">IndividualDataPP</code></td>
<td>
<p><code>IndividualDataPP</code> object to use for the <code>ReSurv</code> fit cross-validation.</p>
</td></tr>
<tr><td><code id="ReSurvCV.default_+3A_model">model</code></td>
<td>
<p><code>character</code>, machine learning for cross validation.</p>
</td></tr>
<tr><td><code id="ReSurvCV.default_+3A_hparameters_grid">hparameters_grid</code></td>
<td>
<p><code>list</code>, grid of the hyperparameters to cross-validate.</p>
</td></tr>
<tr><td><code id="ReSurvCV.default_+3A_folds">folds</code></td>
<td>
<p><code>integer</code>, number of folds (i.e. K).</p>
</td></tr>
<tr><td><code id="ReSurvCV.default_+3A_random_seed">random_seed</code></td>
<td>
<p><code>integer</code>, random seed for making the code reproducible.</p>
</td></tr>
<tr><td><code id="ReSurvCV.default_+3A_continuous_features_scaling_method">continuous_features_scaling_method</code></td>
<td>
<p><code>character</code>, method for scaling continuous features.</p>
</td></tr>
<tr><td><code id="ReSurvCV.default_+3A_print_every_n">print_every_n</code></td>
<td>
<p><code>integer</code>, specific to the <code>XGB</code> approach, see <code>xgboost::xgb.train</code> documentation.</p>
</td></tr>
<tr><td><code id="ReSurvCV.default_+3A_nrounds">nrounds</code></td>
<td>
<p><code>integer</code>, specific to <code>XGB</code>, max number of boosting iterations.</p>
</td></tr>
<tr><td><code id="ReSurvCV.default_+3A_early_stopping_rounds">early_stopping_rounds</code></td>
<td>
<p><code>integer</code>, specific to the <code>XGB</code> approach, see <code>xgboost::xgb.train</code> documentation.</p>
</td></tr>
<tr><td><code id="ReSurvCV.default_+3A_epochs">epochs</code></td>
<td>
<p><code>integer</code>, specific to the <code>NN</code> approach, epochs to be checked.</p>
</td></tr>
<tr><td><code id="ReSurvCV.default_+3A_parallel">parallel</code></td>
<td>
<p><code>logical</code>, specific to the <code>NN</code> approach, whether to use parallel computing.</p>
</td></tr>
<tr><td><code id="ReSurvCV.default_+3A_ncores">ncores</code></td>
<td>
<p><code>integer</code>, specific to <code>NN</code>, max number of cores used.</p>
</td></tr>
<tr><td><code id="ReSurvCV.default_+3A_num_workers">num_workers</code></td>
<td>
<p><code>numeric</code>, number of workers for the <code>NN</code> approach, multi-process data loading with the specified number of loader worker processes.</p>
</td></tr>
<tr><td><code id="ReSurvCV.default_+3A_verbose">verbose</code></td>
<td>
<p><code>logical</code>, whether messages from the machine learning models must be printed.</p>
</td></tr>
<tr><td><code id="ReSurvCV.default_+3A_verbose.cv">verbose.cv</code></td>
<td>
<p><code>logical</code>, whether messages from cross-validation must be printed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Best <code>ReSurv</code> model fit. The output is different depending on the machine learning approach that is required for cross-validation. A list containing:
</p>

<ul>
<li><p><code>out.cv</code>: <code>data.frame</code>, total output of the cross-validation (all the input parameters combinations). 
</p>
</li>
<li><p><code>out.cv.best.oos</code>:  <code>data.frame</code>, combination with the best out of sample likelihood. 
</p>
</li></ul>

<p>For XGB the columns in <code>out.cv</code> and <code>out.cv.best.oos</code> are the hyperparameters <code>booster</code>, <code>eta</code>, <code>max_depth</code>, <code>subsample</code>, <code>alpha</code>, <code>lambda</code>, <code>min_child_weight</code>. They also contain the metrics <code>train.lkh</code>, <code>test.lkh</code>, and the computational time <code>time</code>. For NN the columns in <code>out.cv</code> and <code>out.cv.best.oos</code> are the hyperparameters <code>num_layers</code>, <code>optim</code>, <code>activation</code>, <code>lr</code>, <code>xi</code>, <code>eps</code>, <code>tie</code>, <code>batch_size</code>, <code>early_stopping</code>, <code>patience</code>, <code>node</code> train.lkh test.lkh. They also contain the metrics <code>train.lkh</code>, <code>test.lkh</code>, and the computational time <code>time</code>.
</p>


<h3>References</h3>

<p>Munir, H., Emil, H., &amp; Gabriele, P. (2023). A machine learning approach based on survival analysis for IBNR frequencies in non-life reserving. arXiv preprint arXiv:2312.14549.
</p>

<hr>
<h2 id='ReSurvCV.IndividualDataPP'>K fold cross-validation of ReSurv model.</h2><span id='topic+ReSurvCV.IndividualDataPP'></span>

<h3>Description</h3>

<p>This function computes a K fold cross-validation of a pre-specified ReSurv model for a given grid of parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'IndividualDataPP'
ReSurvCV(
  IndividualDataPP,
  model,
  hparameters_grid,
  folds,
  random_seed,
  continuous_features_scaling_method = "minmax",
  print_every_n = 1L,
  nrounds = NULL,
  early_stopping_rounds = NULL,
  epochs = NULL,
  parallel = FALSE,
  ncores = 1,
  num_workers = 0,
  verbose = FALSE,
  verbose.cv = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ReSurvCV.IndividualDataPP_+3A_individualdatapp">IndividualDataPP</code></td>
<td>
<p><code>IndividualDataPP</code> object to use for the <code>ReSurv</code> fit cross-validation.</p>
</td></tr>
<tr><td><code id="ReSurvCV.IndividualDataPP_+3A_model">model</code></td>
<td>
<p><code>character</code>, machine learning for cross validation.</p>
</td></tr>
<tr><td><code id="ReSurvCV.IndividualDataPP_+3A_hparameters_grid">hparameters_grid</code></td>
<td>
<p><code>list</code>, grid of the hyperparameters to cross-validate.</p>
</td></tr>
<tr><td><code id="ReSurvCV.IndividualDataPP_+3A_folds">folds</code></td>
<td>
<p><code>integer</code>, number of folds (i.e. K).</p>
</td></tr>
<tr><td><code id="ReSurvCV.IndividualDataPP_+3A_random_seed">random_seed</code></td>
<td>
<p><code>integer</code>, random seed for making the code reproducible.</p>
</td></tr>
<tr><td><code id="ReSurvCV.IndividualDataPP_+3A_continuous_features_scaling_method">continuous_features_scaling_method</code></td>
<td>
<p><code>character</code>, method for scaling continuous features.</p>
</td></tr>
<tr><td><code id="ReSurvCV.IndividualDataPP_+3A_print_every_n">print_every_n</code></td>
<td>
<p><code>integer</code>, specific to the <code>XGB</code> approach, see <code>xgboost::xgb.train</code> documentation.</p>
</td></tr>
<tr><td><code id="ReSurvCV.IndividualDataPP_+3A_nrounds">nrounds</code></td>
<td>
<p><code>integer</code>, specific to <code>XGB</code>, max number of boosting iterations.</p>
</td></tr>
<tr><td><code id="ReSurvCV.IndividualDataPP_+3A_early_stopping_rounds">early_stopping_rounds</code></td>
<td>
<p><code>integer</code>, specific to the <code>XGB</code> approach, see <code>xgboost::xgb.train</code> documentation.</p>
</td></tr>
<tr><td><code id="ReSurvCV.IndividualDataPP_+3A_epochs">epochs</code></td>
<td>
<p><code>integer</code>, specific to the <code>NN</code> approach, epochs to be checked.</p>
</td></tr>
<tr><td><code id="ReSurvCV.IndividualDataPP_+3A_parallel">parallel</code></td>
<td>
<p><code>logical</code>, specific to the <code>NN</code> approach, whether to use parallel computing.</p>
</td></tr>
<tr><td><code id="ReSurvCV.IndividualDataPP_+3A_ncores">ncores</code></td>
<td>
<p><code>integer</code>, specific to <code>NN</code>, max number of cores used.</p>
</td></tr>
<tr><td><code id="ReSurvCV.IndividualDataPP_+3A_num_workers">num_workers</code></td>
<td>
<p><code>numeric</code>, number of workers for the <code>NN</code> approach, multi-process data loading with the specified number of loader worker processes.</p>
</td></tr>
<tr><td><code id="ReSurvCV.IndividualDataPP_+3A_verbose">verbose</code></td>
<td>
<p><code>logical</code>, whether messages from the machine learning models must be printed.</p>
</td></tr>
<tr><td><code id="ReSurvCV.IndividualDataPP_+3A_verbose.cv">verbose.cv</code></td>
<td>
<p><code>logical</code>, whether messages from cross-validation must be printed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Best <code>ReSurv</code> model fit. The output is different depending on the machine learning approach that is required for cross-validation. A list containing:
</p>

<ul>
<li><p><code>out.cv</code>: <code>data.frame</code>, total output of the cross-validation (all the input parameters combinations). 
</p>
</li>
<li><p><code>out.cv.best.oos</code>:  <code>data.frame</code>, combination with the best out of sample likelihood. 
</p>
</li></ul>

<p>For XGB the columns in <code>out.cv</code> and <code>out.cv.best.oos</code> are the hyperparameters <code>booster</code>, <code>eta</code>, <code>max_depth</code>, <code>subsample</code>, <code>alpha</code>, <code>lambda</code>, <code>min_child_weight</code>. They also contain the metrics <code>train.lkh</code>, <code>test.lkh</code>, and the computational time <code>time</code>. For NN the columns in <code>out.cv</code> and <code>out.cv.best.oos</code> are the hyperparameters <code>num_layers</code>, <code>optim</code>, <code>activation</code>, <code>lr</code>, <code>xi</code>, <code>eps</code>, <code>tie</code>, <code>batch_size</code>, <code>early_stopping</code>, <code>patience</code>, <code>node</code> train.lkh test.lkh. They also contain the metrics <code>train.lkh</code>, <code>test.lkh</code>, and the computational time <code>time</code>.
</p>

<hr>
<h2 id='summary.ReSurvPredict'>Summary of IBNR predictions</h2><span id='topic+summary.ReSurvPredict'></span>

<h3>Description</h3>

<p>Gives overview of IBNR predictions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ReSurvPredict'
summary(object, granularity = "input", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.ReSurvPredict_+3A_object">object</code></td>
<td>
<p>&quot;ReSurvPredict&quot; object specifying hazard and development factors.</p>
</td></tr>
<tr><td><code id="summary.ReSurvPredict_+3A_granularity">granularity</code></td>
<td>
<p><code>character</code>, specify if which granularity the summary should be on.
</p>

<ul>
<li><p><code>"input"</code>
</p>
</li>
<li><p><code>"output"</code>
</p>
</li></ul>

<p>Default is <code>"input"</code>.</p>
</td></tr>
<tr><td><code id="summary.ReSurvPredict_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to summary.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Summary of predictions
</p>

<hr>
<h2 id='survival_crps'>Survival continuously ranked probability score.</h2><span id='topic+survival_crps'></span>

<h3>Description</h3>

<p>Return the Survival Continuously Ranked Probability Score (SCRPS) of a <code>ReSurv</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survival_crps(ReSurvFit, user_data_set = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="survival_crps_+3A_resurvfit">ReSurvFit</code></td>
<td>
<p>ReSurvFit object to use for the score computation.</p>
</td></tr>
<tr><td><code id="survival_crps_+3A_user_data_set">user_data_set</code></td>
<td>
<p>data.frame provided from the user to compute the survival CRPS, optional.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model fit uses the theoretical framework of Hiabu et al. (2023), that relies on the
</p>


<h3>Value</h3>

<p>Survival CRPS, <code>data.table</code> that contains the CRPS (<code>crps</code>) for each observation (<code>id</code>).
</p>


<h3>References</h3>

<p>Pittarello, G., Hiabu, M., &amp; Villegas, A. M. (2023). Chain Ladder Plus: a versatile approach for claims reserving. arXiv preprint arXiv:2301.03858.
</p>
<p>Therneau, T. M., &amp; Lumley, T. (2015). Package ‘survival’. R Top Doc, 128(10), 28-33.
</p>
<p>Katzman, J. L., Shaham, U., Cloninger, A., Bates, J., Jiang, T., &amp; Kluger, Y. (2018). DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network. BMC medical research methodology, 18(1), 1-12.
</p>
<p>Chen, T., He, T., Benesty, M., &amp; Khotilovich, V. (2019). Package ‘xgboost’. R version, 90, 1-66.
</p>

<hr>
<h2 id='survival_crps.default'>Survival continuously ranked probability score.</h2><span id='topic+survival_crps.default'></span>

<h3>Description</h3>

<p>Return the Survival Continuously Ranked Probability Score (SCRPS) of a <code>ReSurv</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
survival_crps(ReSurvFit, user_data_set = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="survival_crps.default_+3A_resurvfit">ReSurvFit</code></td>
<td>
<p>ReSurvFit object to use for the score computation.</p>
</td></tr>
<tr><td><code id="survival_crps.default_+3A_user_data_set">user_data_set</code></td>
<td>
<p>data.frame provided from the user to compute the survival CRPS, optional.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model fit uses the theoretical framework of Hiabu et al. (2023), that relies on the
</p>


<h3>Value</h3>

<p>Survival CRPS, <code>data.table</code> that contains the CRPS (<code>crps</code>) for each observation (<code>id</code>).
</p>


<h3>References</h3>

<p>Pittarello, G., Hiabu, M., &amp; Villegas, A. M. (2023). Chain Ladder Plus: a versatile approach for claims reserving. arXiv preprint arXiv:2301.03858.
</p>
<p>Therneau, T. M., &amp; Lumley, T. (2015). Package ‘survival’. R Top Doc, 128(10), 28-33.
</p>
<p>Katzman, J. L., Shaham, U., Cloninger, A., Bates, J., Jiang, T., &amp; Kluger, Y. (2018). DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network. BMC medical research methodology, 18(1), 1-12.
</p>
<p>Chen, T., He, T., Benesty, M., &amp; Khotilovich, V. (2019). Package ‘xgboost’. R version, 90, 1-66.
</p>

<hr>
<h2 id='survival_crps.ReSurvFit'>Survival continuously ranked probability score.</h2><span id='topic+survival_crps.ReSurvFit'></span>

<h3>Description</h3>

<p>Return the Survival Continuously Ranked Probability Score (SCRPS) of a <code>ReSurv</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ReSurvFit'
survival_crps(ReSurvFit, user_data_set = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="survival_crps.ReSurvFit_+3A_resurvfit">ReSurvFit</code></td>
<td>
<p>ReSurvFit object to use for the score computation.</p>
</td></tr>
<tr><td><code id="survival_crps.ReSurvFit_+3A_user_data_set">user_data_set</code></td>
<td>
<p>data.frame provided from the user to compute the survival CRPS, optional.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model fit uses the theoretical framework of Hiabu et al. (2023), that relies on the
</p>


<h3>Value</h3>

<p>Survival CRPS, <code>data.table</code> that contains the CRPS (<code>crps</code>) for each observation (<code>id</code>).
</p>


<h3>References</h3>

<p>Pittarello, G., Hiabu, M., &amp; Villegas, A. M. (2023). Chain Ladder Plus: a versatile approach for claims reserving. arXiv preprint arXiv:2301.03858.
</p>
<p>Therneau, T. M., &amp; Lumley, T. (2015). Package ‘survival’. R Top Doc, 128(10), 28-33.
</p>
<p>Katzman, J. L., Shaham, U., Cloninger, A., Bates, J., Jiang, T., &amp; Kluger, Y. (2018). DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network. BMC medical research methodology, 18(1), 1-12.
</p>
<p>Chen, T., He, T., Benesty, M., &amp; Khotilovich, V. (2019). Package ‘xgboost’. R version, 90, 1-66.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
