<!DOCTYPE html><html><head><title>Help for package survey</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {survey}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#anova.svyglm'>
<p>Model comparison for glms.</p></a></li>
<li><a href='#api'><p>Student performance in California schools</p></a></li>
<li><a href='#as.fpc'><p> Package sample and population size data</p></a></li>
<li><a href='#as.svrepdesign'><p>Convert a survey design to use replicate weights</p></a></li>
<li><a href='#as.svydesign2'><p>Update to the new survey design format</p></a></li>
<li><a href='#barplot.svystat'><p> Barplots and Dotplots</p></a></li>
<li><a href='#bootweights'><p>Compute survey bootstrap weights</p></a></li>
<li><a href='#brrweights'><p>Compute replicate weights</p></a></li>
<li><a href='#calibrate'><p>Calibration (GREG) estimators</p></a></li>
<li><a href='#compressWeights'><p>Compress replicate weight matrix</p></a></li>
<li><a href='#confint.svyglm'><p>Confidence intervals for regression parameters</p></a></li>
<li><a href='#crowd'><p>Household crowding</p></a></li>
<li><a href='#dimnames.DBIsvydesign'><p>Dimensions of survey designs</p></a></li>
<li><a href='#election'><p>US 2004 presidential election data at state or county level</p></a></li>
<li><a href='#estweights'><p>Estimated weights for missing data</p></a></li>
<li><a href='#fpc'><p>Small survey example</p></a></li>
<li><a href='#ftable.svystat'><p>Lay out tables of survey statistics</p></a></li>
<li><a href='#hadamard'><p>Hadamard matrices</p></a></li>
<li><a href='#hospital'><p>Sample of obstetric hospitals</p></a></li>
<li><a href='#HR'><p>Wrappers for specifying PPS designs</p></a></li>
<li><a href='#make.calfun'><p>Calibration metrics</p></a></li>
<li><a href='#marginpred'>
<p>Standardised predictions (predictive margins) for regression models.</p></a></li>
<li><a href='#mu284'><p>Two-stage sample from MU284</p></a></li>
<li><a href='#myco'>
<p>Association between leprosy and BCG vaccination</p></a></li>
<li><a href='#newsvyquantile'>
<p>Quantiles under complex sampling.</p></a></li>
<li><a href='#nhanes'>
<p>Cholesterol data from a US survey</p></a></li>
<li><a href='#nonresponse'><p>Experimental: Construct non-response weights</p></a></li>
<li><a href='#oldsvyquantile'><p>Deprecated implementation of quantiles</p></a></li>
<li><a href='#open.DBIsvydesign'><p>Open and close DBI connections</p></a></li>
<li><a href='#paley'><p>Paley-type Hadamard matrices</p></a></li>
<li><a href='#pchisqsum'><p>Distribution of quadratic forms</p></a></li>
<li><a href='#poisson_sampling'>
<p>Specify Poisson sampling design</p></a></li>
<li><a href='#postStratify'><p>Post-stratify a survey</p></a></li>
<li><a href='#psrsq'>
<p>Pseudo-Rsquareds</p></a></li>
<li><a href='#rake'><p>Raking of replicate weight design</p></a></li>
<li><a href='#regTermTest'><p>Wald test for a term in a regression model</p></a></li>
<li><a href='#salamander'><p>Salamander mating data set from McCullagh and Nelder (1989)</p></a></li>
<li><a href='#scd'><p>Survival in cardiac arrest</p></a></li>
<li><a href='#SE'><p>Extract standard errors</p></a></li>
<li><a href='#smoothArea'><p>Small area estimation via basic area level model</p></a></li>
<li><a href='#smoothUnit'><p>Smooth via basic unit level model</p></a></li>
<li><a href='#stratsample'>
<p>Take a stratified sample</p></a></li>
<li><a href='#subset.survey.design'><p>Subset of survey</p></a></li>
<li><a href='#surveyoptions'><p>Options for the survey package</p></a></li>
<li><a href='#surveysummary'><p>Summary statistics for sample surveys</p></a></li>
<li><a href='#svrepdesign'><p>Specify survey design with replicate weights</p></a></li>
<li><a href='#svrVar'><p>Compute variance from replicates</p></a></li>
<li><a href='#svy.varcoef'><p>Sandwich variance estimator for glms</p></a></li>
<li><a href='#svyby'><p>Survey statistics on subsets</p></a></li>
<li><a href='#svycdf'><p>Cumulative Distribution Function</p></a></li>
<li><a href='#svyciprop'><p>Confidence intervals for proportions</p></a></li>
<li><a href='#svycontrast'><p>Linear and nonlinearconstrasts of survey statistics</p></a></li>
<li><a href='#svycoplot'><p>Conditioning plots of survey data</p></a></li>
<li><a href='#svycoxph'><p>Survey-weighted Cox models.</p></a></li>
<li><a href='#svyCprod'><p>Computations for survey variances</p></a></li>
<li><a href='#svycralpha'>
<p>Cronbach's alpha</p></a></li>
<li><a href='#svydesign'><p>Survey sample analysis.</p></a></li>
<li><a href='#svyfactanal'>
<p>Factor analysis in complex surveys (experimental).</p></a></li>
<li><a href='#svyglm'><p>Survey-weighted generalised linear models.</p></a></li>
<li><a href='#svygofchisq'>
<p>Test of fit to known probabilities</p></a></li>
<li><a href='#svyhist'><p>Histograms and boxplots</p></a></li>
<li><a href='#svyivreg'>
<p>Two-stage least-squares for instrumental variable regression</p></a></li>
<li><a href='#svykappa'><p>Cohen's kappa for agreement</p></a></li>
<li><a href='#svykm'><p>Estimate survival function.</p></a></li>
<li><a href='#svyloglin'><p>Loglinear models</p></a></li>
<li><a href='#svylogrank'>
<p>Compare survival distributions</p></a></li>
<li><a href='#svymle'><p>Maximum pseudolikelihood estimation in complex surveys</p></a></li>
<li><a href='#svynls'>
<p>Probability-weighted nonlinear least squares</p></a></li>
<li><a href='#svyolr'><p>Proportional odds and related models</p></a></li>
<li><a href='#svyplot'><p>Plots for survey data</p></a></li>
<li><a href='#svyprcomp'>
<p>Sampling-weighted principal component analysis</p></a></li>
<li><a href='#svypredmeans'>
<p>Predictive marginal means</p></a></li>
<li><a href='#svyqqplot'>
<p>Quantile-quantile plots for survey data</p></a></li>
<li><a href='#svyranktest'>
<p>Design-based rank tests</p></a></li>
<li><a href='#svyratio'><p>Ratio estimation</p></a></li>
<li><a href='#svyrecvar'><p>Variance estimation for multistage surveys</p></a></li>
<li><a href='#svyscoretest'>
<p>Score tests in survey regression models</p></a></li>
<li><a href='#svysmooth'><p>Scatterplot smoothing and density estimation</p></a></li>
<li><a href='#svystandardize'>
<p>Direct standardization within domains</p></a></li>
<li><a href='#svysurvreg'>
<p>Fit accelerated failure models to survey data</p></a></li>
<li><a href='#svytable'><p>Contingency tables for survey data</p></a></li>
<li><a href='#svyttest'><p>Design-based t-test</p></a></li>
<li><a href='#trimWeights'>
<p>Trim sampling weights</p></a></li>
<li><a href='#twophase'><p>Two-phase designs</p></a></li>
<li><a href='#update.survey.design'><p> Add variables to a survey design</p></a></li>
<li><a href='#weights.survey.design'><p>Survey design weights</p></a></li>
<li><a href='#with.svyimputationList'><p>Analyse multiple imputations</p></a></li>
<li><a href='#withPV.survey.design'>
<p>Analyse plausible values in surveys</p></a></li>
<li><a href='#withReplicates'><p>Compute variances by replicate weighting</p></a></li>
<li><a href='#xdesign'>
<p>Crossed effects and other sparse correlations</p></a></li>
<li><a href='#yrbs'>
<p>One variable from the Youth Risk Behaviors Survey, 2015.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Analysis of Complex Survey Samples</td>
</tr>
<tr>
<td>Description:</td>
<td>Summary statistics, two-sample tests, rank tests, generalised linear models, cumulative link models, Cox models, loglinear models, and general maximum pseudolikelihood estimation for multistage stratified, cluster-sampled, unequally weighted survey samples. Variances by Taylor series linearisation or replicate weights. Post-stratification, calibration, and raking. Two-phase subsampling designs. Graphics. PPS sampling without replacement. Small-area estimation.</td>
</tr>
<tr>
<td>Version:</td>
<td>4.4-2</td>
</tr>
<tr>
<td>Author:</td>
<td>Thomas Lumley, Peter Gao, Ben Schneider</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>"Thomas Lumley" &lt;t.lumley@auckland.ac.nz&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0), grid, methods, Matrix, survival</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, graphics, splines, lattice, minqa, numDeriv, mitools
(&ge; 2.4), Rcpp (&ge; 0.12.8)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>R.rsp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>foreign, MASS, KernSmooth, hexbin, RSQLite, quantreg,
parallel, CompQuadForm, DBI, AER, SUMMER (&ge; 1.4.0), R.rsp</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://r-survey.r-forge.r-project.org/survey/">http://r-survey.r-forge.r-project.org/survey/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-20 00:51:41 UTC; tlum005</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-20 15:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='anova.svyglm'>
Model comparison for glms.
</h2><span id='topic+anova.svyglm'></span><span id='topic+AIC.svyglm'></span><span id='topic+BIC.svyglm'></span><span id='topic+extractAIC.svyglm'></span><span id='topic+extractAIC.svrepglm'></span><span id='topic+anova.svycoxph'></span>

<h3>Description</h3>

<p>A method for the <code><a href="stats.html#topic+anova">anova</a></code> function, for use on
<code><a href="#topic+svyglm">svyglm</a></code> and <code><a href="#topic+svycoxph">svycoxph</a></code> objects.  With a single model argument it produces a sequential anova table, with two arguments it compares the two models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'svyglm'
anova(object, object2 = NULL, test = c("F", "Chisq"), 
 method = c("LRT", "Wald"), tolerance = 1e-05, ..., force = FALSE)
## S3 method for class 'svycoxph'
anova(object, object2=NULL,test=c("F","Chisq"),
 method=c("LRT","Wald"),tolerance=1e-5,...,force=FALSE)
## S3 method for class 'svyglm'
AIC(object,...,k=2, null_has_intercept=TRUE)
## S3 method for class 'svyglm'
BIC(object,...,maximal)
## S3 method for class 'svyglm'
extractAIC(fit,scale,k=2,..., null_has_intercept=TRUE)
## S3 method for class 'svrepglm'
extractAIC(fit,scale,k=2,..., null_has_intercept=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="anova.svyglm_+3A_object">object</code>, <code id="anova.svyglm_+3A_fit">fit</code></td>
<td>

<p>A <code><a href="#topic+svyglm">svyglm</a></code> or<code><a href="#topic+svycoxph">svycoxph</a></code>  object.
</p>
</td></tr>
<tr><td><code id="anova.svyglm_+3A_object2">object2</code></td>
<td>

<p>Optionally, another <code><a href="#topic+svyglm">svyglm</a></code> or <code><a href="#topic+svycoxph">svycoxph</a></code> object.
</p>
</td></tr>
<tr><td><code id="anova.svyglm_+3A_test">test</code></td>
<td>

<p>Use (linear combination of) F or chi-squared distributions for p-values. F is usually preferable.
</p>
</td></tr>
<tr><td><code id="anova.svyglm_+3A_method">method</code></td>
<td>

<p>Use weighted deviance difference (LRT) or Wald tests to compare models
</p>
</td></tr>
<tr><td><code id="anova.svyglm_+3A_tolerance">tolerance</code></td>
<td>

<p>For models that are not symbolically nested, the tolerance for deciding that a term is common to the models.
</p>
</td></tr>
<tr><td><code id="anova.svyglm_+3A_...">...</code></td>
<td>

<p>For <code>AIC</code> and <code>BIC</code>, optionally more <code>svyglm</code> objects
</p>
</td></tr>
<tr><td><code id="anova.svyglm_+3A_scale">scale</code></td>
<td>
<p>not used</p>
</td></tr>
<tr><td><code id="anova.svyglm_+3A_null_has_intercept">null_has_intercept</code></td>
<td>
<p>Does the null model for AIC have an intercept or
not? Must be <code>FALSE</code> if any of the models are intercept-only. </p>
</td></tr>
<tr><td><code id="anova.svyglm_+3A_force">force</code></td>
<td>

<p>Force the tests to be done by explicit projection even if the models
are symbolically nested (eg, for debugging)
</p>
</td></tr>
<tr><td><code id="anova.svyglm_+3A_maximal">maximal</code></td>
<td>
<p>A <code>svyglm</code> model that <code>object</code> (and ... if supplied) are nested in.</p>
</td></tr>
<tr><td><code id="anova.svyglm_+3A_k">k</code></td>
<td>
<p>Multiplier for effective df in AIC. Usually 2. There is no choice of <code>k</code> that will give BIC</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The reference distribution for the LRT depends on the misspecification effects for the parameters being tested (Rao and Scott, 1984). If the models are symbolically nested, so that the relevant parameters can be identified just by manipulating the model formulas, <code>anova</code> is equivalent to <code><a href="#topic+regTermTest">regTermTest</a></code>.If the models are nested but not symbolically nested, more computation using the design matrices is needed to determine the projection matrix on to the parameters being tested.   In the examples below, <code>model1</code> and <code>model2</code> are symbolically nested in <code>model0</code> because <code>model0</code> can be obtained just by deleting terms from the formulas.  On the other hand, <code>model2</code> is nested in <code>model1</code> but not symbolically nested: knowing that the model is nested requires knowing what design matrix columns are produced by <code>stype</code> and <code>as.numeric(stype)</code>.  Other typical examples of models that are nested but not symbolically nested are linear and spline models for a continuous covariate, or models with categorical versions of a variable at different resolutions (eg, smoking yes/no or smoking never/former/current). 
</p>
<p>A saddlepoint approximation is used for the LRT with numerator df greater than 1.
</p>
<p><code>AIC</code> is defined using the Rao-Scott approximation to the weighted
loglikelihood (Lumley and Scott, 2015). It replaces the usual penalty term p, which is the null expectation of the log likelihood ratio, by the trace of the generalised design effect matrix, which is the expectation under complex sampling. For computational reasons everything is scaled so the weights sum to the sample size. 
</p>
<p><code>BIC</code> is a BIC for the (approximate) multivariate Gaussian models
on regression coefficients from the maximal model implied by each
submodel (ie, the models that say some coefficients in the maximal model
are zero) (Lumley and Scott, 2015). It corresponds to comparing the models with a Wald test and replacing the sample size in the penalty by an effective sample size.
For computational reasons, the models must not only be nested, the names of the coefficients must match.
</p>
<p><code>extractAIC</code> for a model with a Gaussian link uses the actual AIC based on maximum likelihood estimation of the variance parameter as well as the regression parameters.
</p>


<h3>Value</h3>

<p>Object of class <code>seqanova.svyglm</code> if one model is given, otherwise of class <code>regTermTest</code> or <code>regTermTestLRT</code>
</p>


<h3>Note</h3>

<p>At the moment, <code>AIC</code> works only for models including an intercept.
</p>


<h3>References</h3>

<p>Rao, JNK, Scott, AJ (1984) &quot;On Chi-squared Tests For Multiway Contingency Tables with Proportions Estimated From Survey Data&quot; Annals of Statistics 12:46-60.
</p>
<p>Lumley, T., &amp; Scott, A. (2014). &quot;Tests for Regression Models Fitted to Survey Data&quot;. Australian and New Zealand Journal of Statistics, 56 (1), 1-14. 
</p>
<p>Lumley T, Scott AJ (2015)
&quot;AIC and BIC for modelling with complex survey data&quot; J Surv Stat
Methodol 3 (1): 1-18. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+regTermTest">regTermTest</a></code>, <code><a href="#topic+pchisqsum">pchisqsum</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus2&lt;-svydesign(id=~dnum+snum, weights=~pw, data=apiclus2)

model0&lt;-svyglm(I(sch.wide=="Yes")~ell+meals+mobility, design=dclus2, family=quasibinomial())
model1&lt;-svyglm(I(sch.wide=="Yes")~ell+meals+mobility+as.numeric(stype), 
     design=dclus2, family=quasibinomial())
model2&lt;-svyglm(I(sch.wide=="Yes")~ell+meals+mobility+stype, design=dclus2, family=quasibinomial())

anova(model2)	
anova(model0,model2)					     		    
anova(model1, model2)

anova(model1, model2, method="Wald")

AIC(model0,model1, model2)
BIC(model0, model2,maximal=model2)

## AIC for linear model is different because it considers the variance
## parameter

model0&lt;-svyglm(api00~ell+meals+mobility, design=dclus2)
model1&lt;-svyglm(api00~ell+meals+mobility+as.numeric(stype), 
     design=dclus2)
model2&lt;-svyglm(api00~ell+meals+mobility+stype, design=dclus2)
modelnull&lt;-svyglm(api00~1, design=dclus2)

AIC(model0, model1, model2)

AIC(model0, model1, model2,modelnull, null_has_intercept=FALSE)

## from ?twophase
data(nwtco)
dcchs&lt;-twophase(id=list(~seqno,~seqno), strata=list(NULL,~rel),
        subset=~I(in.subcohort | rel), data=nwtco)
a&lt;-svycoxph(Surv(edrel,rel)~factor(stage)+factor(histol)+I(age/12), design=dcchs)
b&lt;-update(a, .~.-I(age/12)+poly(age,3))
## not symbolically nested models
anova(a,b)

</code></pre>

<hr>
<h2 id='api'>Student performance in California schools</h2><span id='topic+api'></span><span id='topic+apipop'></span><span id='topic+apiclus1'></span><span id='topic+apiclus2'></span><span id='topic+apistrat'></span><span id='topic+apisrs'></span>

<h3>Description</h3>

<p>The Academic Performance Index is computed for all California schools
based on standardised testing of students. The data sets contain
information for all schools with at least 100 students and for various
probability samples of the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(api)
</code></pre>


<h3>Format</h3>

<p>The full population data in <code>apipop</code> are a data frame with 6194 observations on the following 37 variables.
</p>

<dl>
<dt>cds</dt><dd><p>Unique identifier</p>
</dd>
<dt>stype</dt><dd><p>Elementary/Middle/High School</p>
</dd>
<dt>name</dt><dd><p>School name (15 characters)</p>
</dd>
<dt>sname</dt><dd><p>School name (40 characters)</p>
</dd>
<dt>snum</dt><dd><p>School number</p>
</dd>
<dt>dname</dt><dd><p>District name</p>
</dd>
<dt>dnum</dt><dd><p>District number</p>
</dd>
<dt>cname</dt><dd><p>County name</p>
</dd>
<dt>cnum</dt><dd><p>County number</p>
</dd>
<dt>flag</dt><dd><p>reason for missing data</p>
</dd>
<dt>pcttest</dt><dd><p>percentage of students tested</p>
</dd>
<dt>api00</dt><dd><p>API in 2000</p>
</dd>
<dt>api99</dt><dd><p>API in 1999</p>
</dd>
<dt>target</dt><dd><p>target for change in API</p>
</dd>
<dt>growth</dt><dd><p>Change in API</p>
</dd>
<dt>sch.wide</dt><dd><p>Met school-wide growth target?</p>
</dd>
<dt>comp.imp</dt><dd><p>Met Comparable Improvement target</p>
</dd>
<dt>both</dt><dd><p>Met both targets</p>
</dd>
<dt>awards</dt><dd><p>Eligible for awards program</p>
</dd>
<dt>meals</dt><dd><p>Percentage of students eligible for subsidized meals</p>
</dd>
<dt>ell</dt><dd><p>&lsquo;English Language Learners&rsquo; (percent)</p>
</dd>
<dt>yr.rnd</dt><dd><p>Year-round school</p>
</dd>
<dt>mobility</dt><dd><p>percentage of students for whom this is the first
year at the school</p>
</dd>
<dt>acs.k3</dt><dd><p>average class size years K-3</p>
</dd>
<dt>acs.46</dt><dd><p>average class size years 4-6</p>
</dd>
<dt>acs.core</dt><dd><p>Number of core academic courses</p>
</dd>
<dt>pct.resp</dt><dd><p>percent where parental education level is known</p>
</dd>
<dt>not.hsg</dt><dd><p>percent parents not high-school graduates</p>
</dd>
<dt>hsg</dt><dd><p>percent parents who are high-school graduates</p>
</dd>
<dt>some.col</dt><dd><p>percent parents with some college</p>
</dd>
<dt>col.grad</dt><dd><p>percent parents with college degree</p>
</dd>
<dt>grad.sch</dt><dd><p>percent parents with postgraduate education</p>
</dd>
<dt>avg.ed</dt><dd><p>average parental education level</p>
</dd>
<dt>full</dt><dd><p>percent fully qualified teachers</p>
</dd>
<dt>emer</dt><dd><p>percent teachers with emergency qualifications</p>
</dd>
<dt>enroll</dt><dd><p>number of students enrolled</p>
</dd>
<dt>api.stu</dt><dd><p>number of students tested.</p>
</dd>
</dl>

<p>The other data sets contain additional variables <code>pw</code> for
sampling weights and <code>fpc</code> to compute finite population
corrections to variance.
</p>


<h3>Details</h3>

<p><code>apipop</code> is the entire population, <code>apisrs</code> is a simple random sample,
<code>apiclus1</code> is a cluster sample of school districts, <code>apistrat</code> is
a sample stratified by <code>stype</code>, and <code>apiclus2</code> is a two-stage
cluster sample of schools within districts.  The sampling weights in
<code>apiclus1</code> are incorrect (the weight should be 757/15) but are as 
obtained from UCLA.
</p>


<h3>Source</h3>

<p>Data were obtained from the survey sampling help pages of UCLA
Academic Technology Services; these pages are no longer on line. 
</p>


<h3>References</h3>

<p>The API program has been discontinued at the end of 2018, and the archive page at the California Department of Education is now gone. The Wikipedia article has links to past material at the Internet Archive. <a href="https://en.wikipedia.org/wiki/Academic_Performance_Index_(California_public_schools)">https://en.wikipedia.org/wiki/Academic_Performance_Index_(California_public_schools)</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(survey)
data(api)
mean(apipop$api00)
sum(apipop$enroll, na.rm=TRUE)

#stratified sample
dstrat&lt;-svydesign(id=~1,strata=~stype, weights=~pw, data=apistrat, fpc=~fpc)
summary(dstrat)
svymean(~api00, dstrat)
svytotal(~enroll, dstrat, na.rm=TRUE)

# one-stage cluster sample
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)
summary(dclus1)
svymean(~api00, dclus1)
svytotal(~enroll, dclus1, na.rm=TRUE)

# two-stage cluster sample
dclus2&lt;-svydesign(id=~dnum+snum, fpc=~fpc1+fpc2, data=apiclus2)
summary(dclus2)
svymean(~api00, dclus2)
svytotal(~enroll, dclus2, na.rm=TRUE)

# two-stage `with replacement'
dclus2wr&lt;-svydesign(id=~dnum+snum, weights=~pw, data=apiclus2)
summary(dclus2wr)
svymean(~api00, dclus2wr)
svytotal(~enroll, dclus2wr, na.rm=TRUE)


# convert to replicate weights
rclus1&lt;-as.svrepdesign(dclus1)
summary(rclus1)
svymean(~api00, rclus1)
svytotal(~enroll, rclus1, na.rm=TRUE)

# post-stratify on school type
pop.types&lt;-xtabs(~stype, data=apipop)

rclus1p&lt;-postStratify(rclus1, ~stype, pop.types)
dclus1p&lt;-postStratify(dclus1, ~stype, pop.types)
summary(dclus1p)
summary(rclus1p)

svymean(~api00, dclus1p)
svytotal(~enroll, dclus1p, na.rm=TRUE)

svymean(~api00, rclus1p)
svytotal(~enroll, rclus1p, na.rm=TRUE)

</code></pre>

<hr>
<h2 id='as.fpc'> Package sample and population size data</h2><span id='topic+as.fpc'></span>

<h3>Description</h3>

<p>This function creates an object to store the number of clusters sampled
within each stratum (at each stage of multistage sampling) and the
number of clusters available in the population.  It is called by
<code>svydesign</code>, not directly by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.fpc(df, strata, ids,pps=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.fpc_+3A_df">df</code></td>
<td>
<p>A data frame or matrix with population size information</p>
</td></tr>
<tr><td><code id="as.fpc_+3A_strata">strata</code></td>
<td>
<p>A data frame giving strata at each stage</p>
</td></tr>
<tr><td><code id="as.fpc_+3A_ids">ids</code></td>
<td>
<p>A data frame giving cluster ids at each stage</p>
</td></tr>
<tr><td><code id="as.fpc_+3A_pps">pps</code></td>
<td>
<p>if <code>TRUE</code>, fpc information may vary within a stratum
and must be specified as a proportion rather than a population sizes</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The population size information may be specified as the number of
clusters in the population or as the proportion of clusters sampled.
</p>


<h3>Value</h3>

<p>An object of class <code>survey_fpc</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+svydesign">svydesign</a></code>,<code><a href="#topic+svyrecvar">svyrecvar</a></code></p>

<hr>
<h2 id='as.svrepdesign'>Convert a survey design to use replicate weights</h2><span id='topic+as.svrepdesign'></span><span id='topic+as.svrepdesign.default'></span><span id='topic+as.svrepdesign.svyimputationList'></span>

<h3>Description</h3>

<p>Creates a replicate-weights survey design object from a traditional
strata/cluster survey design object. <code>JK1</code> and <code>JKn</code> are
jackknife methods, <code>BRR</code> is Balanced Repeated Replicates and
<code>Fay</code> is Fay's modification of this, <code>bootstrap</code> is Canty
and Davison's bootstrap, <code>subbootstrap</code> is Rao and Wu's
<code class="reqn">(n-1)</code> bootstrap, and <code>mrbbootstrap</code> is Preston's multistage
rescaled bootstrap.  With a <code>svyimputationList</code> object, the same
replicate weights will be used for each imputation if the sampling
weights are all the same and <code>separate.replicates=FALSE</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.svrepdesign(design,...)
## Default S3 method:
as.svrepdesign(design, type=c("auto", "JK1", "JKn", "BRR", "bootstrap",
   "subbootstrap","mrbbootstrap","Fay"),
   fay.rho = 0, fpc=NULL,fpctype=NULL,..., compress=TRUE, 
   mse=getOption("survey.replicates.mse"))
## S3 method for class 'svyimputationList'
as.svrepdesign(design, type=c("auto", "JK1", "JKn", "BRR", "bootstrap",
   "subbootstrap","mrbbootstrap","Fay"),
   fay.rho = 0, fpc=NULL,fpctype=NULL, separate.replicates=FALSE, ..., compress=TRUE, 
   mse=getOption("survey.replicates.mse"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.svrepdesign_+3A_design">design</code></td>
<td>
<p>Object of class <code>survey.design</code> or
<code>svyimputationList</code>. Must not have been
post-stratified/raked/calibrated in R</p>
</td></tr>
<tr><td><code id="as.svrepdesign_+3A_type">type</code></td>
<td>
<p>Type of replicate weights. <code>"auto"</code> uses JKn for
stratified, JK1 for unstratified designs</p>
</td></tr>
<tr><td><code id="as.svrepdesign_+3A_fay.rho">fay.rho</code></td>
<td>
<p>Tuning parameter for Fay's variance method </p>
</td></tr>
<tr><td><code id="as.svrepdesign_+3A_fpc">fpc</code>, <code id="as.svrepdesign_+3A_fpctype">fpctype</code>, <code id="as.svrepdesign_+3A_...">...</code></td>
<td>
<p>Passed to <code>jk1weights</code>, <code>jknweights</code>,
<code>brrweights</code>, <code>bootweights</code>, <code>subbootweights</code>, or
<code>mrbweights</code>.</p>
</td></tr>
<tr><td><code id="as.svrepdesign_+3A_separate.replicates">separate.replicates</code></td>
<td>
<p>Compute replicate weights separately for
each design (useful for the bootstrap types, which are not deterministic</p>
</td></tr>
<tr><td><code id="as.svrepdesign_+3A_compress">compress</code></td>
<td>
<p>Use a compressed representation of the replicate
weights matrix.</p>
</td></tr>
<tr><td><code id="as.svrepdesign_+3A_mse">mse</code></td>
<td>
<p>if <code>TRUE</code>, compute variances from sums of squares around
the point estimate, rather than the mean of the replicates</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>svyrep.design</code>.
</p>


<h3>References</h3>

<p>Canty AJ, Davison AC. (1999) Resampling-based variance
estimation for labour force surveys. The Statistician 48:379-391
</p>
<p>Judkins, D. (1990), &quot;Fay's Method for Variance Estimation,&quot; Journal of Official Statistics, 6, 223-239.
</p>
<p>Preston J. (2009) Rescaled bootstrap for stratified multistage sampling. Survey Methodology 35(2) 227-234
</p>
<p>Rao JNK, Wu CFJ. Bootstrap inference for sample surveys. Proc Section
on Survey Research Methodology. 1993 (866&ndash;871)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+brrweights">brrweights</a></code>, <code><a href="#topic+svydesign">svydesign</a></code>,
<code><a href="#topic+svrepdesign">svrepdesign</a></code>, <code><a href="#topic+bootweights">bootweights</a></code>, <code><a href="#topic+subbootweights">subbootweights</a></code>, <code><a href="#topic+mrbweights">mrbweights</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(scd)
scddes&lt;-svydesign(data=scd, prob=~1, id=~ambulance, strata=~ESA,
nest=TRUE, fpc=rep(5,6))
scdnofpc&lt;-svydesign(data=scd, prob=~1, id=~ambulance, strata=~ESA,
nest=TRUE)

# convert to BRR replicate weights
scd2brr &lt;- as.svrepdesign(scdnofpc, type="BRR")
scd2fay &lt;- as.svrepdesign(scdnofpc, type="Fay",fay.rho=0.3)
# convert to JKn weights 
scd2jkn &lt;- as.svrepdesign(scdnofpc, type="JKn")

# convert to JKn weights with finite population correction
scd2jknf &lt;- as.svrepdesign(scddes, type="JKn")

## with user-supplied hadamard matrix
scd2brr1 &lt;- as.svrepdesign(scdnofpc, type="BRR", hadamard.matrix=paley(11))

svyratio(~alive, ~arrests, design=scd2brr)
svyratio(~alive, ~arrests, design=scd2brr1)
svyratio(~alive, ~arrests, design=scd2fay)
svyratio(~alive, ~arrests, design=scd2jkn)
svyratio(~alive, ~arrests, design=scd2jknf)

data(api)
## one-stage cluster sample
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)
## convert to JK1 jackknife
rclus1&lt;-as.svrepdesign(dclus1)
## convert to bootstrap
bclus1&lt;-as.svrepdesign(dclus1,type="bootstrap", replicates=100)

svymean(~api00, dclus1)
svytotal(~enroll, dclus1)

svymean(~api00, rclus1)
svytotal(~enroll, rclus1)

svymean(~api00, bclus1)
svytotal(~enroll, bclus1)

dclus2&lt;-svydesign(id = ~dnum + snum, fpc = ~fpc1 + fpc2, data = apiclus2)
mrbclus2&lt;-as.svrepdesign(dclus2, type="mrb",replicates=100)
svytotal(~api00+stype, dclus2)
svytotal(~api00+stype, mrbclus2)
</code></pre>

<hr>
<h2 id='as.svydesign2'>Update to the new survey design format</h2><span id='topic+as.svydesign2'></span><span id='topic+.svycheck'></span>

<h3>Description</h3>

<p>The structure of survey design objects changed in version 2.9, to allow
standard errors based on multistage sampling. <code>as.svydesign</code> converts an
object to the new structure and  <code>.svycheck</code> warns if an object
does not have the new structure.
</p>
<p>You can set <code>options(survey.want.obsolete=TRUE)</code> to suppress the
warnings produced by <code>.svycheck</code> and
<code>options(survey.ultimate.cluster=TRUE)</code> to always compute
variances based on just the first stage of sampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.svydesign2(object)
.svycheck(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.svydesign2_+3A_object">object</code></td>
<td>
<p>produced by <code>svydesign</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>survey.design2</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+svydesign">svydesign</a></code>, <code><a href="#topic+svyrecvar">svyrecvar</a></code></p>

<hr>
<h2 id='barplot.svystat'> Barplots and Dotplots </h2><span id='topic+barplot.svystat'></span><span id='topic+barplot.svrepstat'></span><span id='topic+barplot.svyby'></span><span id='topic+dotchart'></span><span id='topic+dotchart.svystat'></span><span id='topic+dotchart.svrepstat'></span><span id='topic+dotchart.svyby'></span>

<h3>Description</h3>

<p>Draws a barplot or dotplot based on results from a survey analysis. The default
barplot method already works for results from <code><a href="#topic+svytable">svytable</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'svystat'
barplot(height, ...)
## S3 method for class 'svrepstat'
barplot(height, ...)
## S3 method for class 'svyby'
barplot(height,beside=TRUE, ...)

## S3 method for class 'svystat'
dotchart(x,...,pch=19)
## S3 method for class 'svrepstat'
dotchart(x,...,pch=19)
## S3 method for class 'svyby'
dotchart(x,...,pch=19)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="barplot.svystat_+3A_height">height</code>, <code id="barplot.svystat_+3A_x">x</code></td>
<td>
<p>Analysis result </p>
</td></tr>
<tr><td><code id="barplot.svystat_+3A_beside">beside</code></td>
<td>
<p>Grouped, rather than stacked, bars</p>
</td></tr>
<tr><td><code id="barplot.svystat_+3A_...">...</code></td>
<td>
<p> Arguments to <code><a href="graphics.html#topic+barplot">barplot</a></code> or <code>dotchart</code> </p>
</td></tr>
<tr><td><code id="barplot.svystat_+3A_pch">pch</code></td>
<td>
<p>Overrides the default in <code>dotchart.default</code></p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
data(api)
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)

a&lt;-svymean(~stype, dclus1)
barplot(a)
barplot(a, names.arg=c("Elementary","High","Middle"), col="purple", 
  main="Proportions of school level")

b&lt;-svyby(~enroll+api.stu, ~stype, dclus1, svymean)
barplot(b,beside=TRUE,legend=TRUE)
dotchart(b)

</code></pre>

<hr>
<h2 id='bootweights'>Compute survey bootstrap weights </h2><span id='topic+bootweights'></span><span id='topic+subbootweights'></span><span id='topic+mrbweights'></span><span id='topic+bootstratum'></span>

<h3>Description</h3>

<p>Bootstrap weights for infinite populations ('with replacement' sampling) are created by sampling with
replacement from the PSUs in each stratum. <code>subbootweights()</code>
samples <code>n-1</code> PSUs from the <code>n</code> available (Rao and Wu),
<code>bootweights</code> samples <code>n</code> (Canty and Davison). 
</p>
<p>For multistage designs or those with large sampling fractions,
<code>mrbweights</code> implements Preston's multistage rescaled
bootstrap. The multistage rescaled bootstrap is still useful for
single-stage designs with small sampling fractions, where it reduces
to a half-sample replicate method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootweights(strata, psu, replicates = 50, fpc = NULL,
         fpctype = c("population", "fraction", "correction"),
         compress = TRUE)
subbootweights(strata, psu, replicates = 50, compress = TRUE)
mrbweights(clusters, stratas, fpcs, replicates=50, 
         multicore=getOption("survey.multicore"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootweights_+3A_strata">strata</code></td>
<td>
<p>Identifier for sampling strata (top level only)</p>
</td></tr>
<tr><td><code id="bootweights_+3A_stratas">stratas</code></td>
<td>
<p>data frame of strata for all stages of sampling</p>
</td></tr>
<tr><td><code id="bootweights_+3A_psu">psu</code></td>
<td>
<p>Identifier for primary sampling units</p>
</td></tr>
<tr><td><code id="bootweights_+3A_clusters">clusters</code></td>
<td>
<p>data frame of identifiers for sampling units at each stage</p>
</td></tr>
<tr><td><code id="bootweights_+3A_replicates">replicates</code></td>
<td>
<p>Number of bootstrap replicates</p>
</td></tr>
<tr><td><code id="bootweights_+3A_fpc">fpc</code></td>
<td>
<p>Finite population correction (top level only) </p>
</td></tr>
<tr><td><code id="bootweights_+3A_fpctype">fpctype</code></td>
<td>
<p>Is <code>fpc</code> the population size, sampling fraction,
or 1-sampling fraction?</p>
</td></tr>
<tr><td><code id="bootweights_+3A_fpcs">fpcs</code></td>
<td>
<p><code>survey_fpc</code> object with population and sample size at each stage</p>
</td></tr>
<tr><td><code id="bootweights_+3A_compress">compress</code></td>
<td>
<p>Should the replicate weights be compressed?</p>
</td></tr>
<tr><td><code id="bootweights_+3A_multicore">multicore</code></td>
<td>
<p>Use the <code>multicore</code> package to generate the replicates in parallel</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A set of replicate weights
</p>


<h3>warning</h3>

<p>With <code>multicore=TRUE</code> the resampling procedure does not
use the current random seed, so the results cannot be exactly
reproduced even by using <code>set.seed()</code></p>


<h3>Note</h3>

<p>These bootstraps are strictly appropriate only when the first stage of
sampling is a simple or stratified random sample of PSUs with or
without replacement, and not (eg) for PPS sampling.  The functions
will not enforce simple random sampling, so they can be used
(approximately) for data that have had non-response corrections and
other weight adjustments.  It is preferable to apply these adjustments
after creating the bootstrap replicate weights, but that may not be
possible with public-use data.
</p>


<h3>References</h3>

<p>Canty AJ, Davison AC. (1999) Resampling-based variance
estimation for labour force surveys. The Statistician 48:379-391
</p>
<p>Judkins, D. (1990), &quot;Fay's Method for Variance Estimation&quot; Journal of Official Statistics, 6, 223-239.
</p>
<p>Preston J. (2009) Rescaled bootstrap for stratified multistage sampling. Survey Methodology 35(2) 227-234
</p>
<p>Rao JNK, Wu CFJ. Bootstrap inference for sample surveys. Proc Section
on Survey Research Methodology. 1993 (866&ndash;871)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+as.svrepdesign">as.svrepdesign</a></code></p>

<hr>
<h2 id='brrweights'>Compute replicate weights </h2><span id='topic+jk1weights'></span><span id='topic+jknweights'></span><span id='topic+brrweights'></span>

<h3>Description</h3>

<p>Compute replicate weights from a survey design. These functions are
usually called from <code><a href="#topic+as.svrepdesign">as.svrepdesign</a></code> rather than directly
by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>brrweights(strata, psu, match = NULL,
              small = c("fail","split","merge"),
              large = c("split", "merge", "fail"),
              fay.rho=0, only.weights=FALSE,
              compress=TRUE, hadamard.matrix=NULL)
jk1weights(psu,fpc=NULL,
              fpctype=c("population","fraction","correction"),
              compress=TRUE)
jknweights(strata,psu, fpc=NULL,
              fpctype=c("population","fraction","correction"),
              compress=TRUE,
              lonely.psu=getOption("survey.lonely.psu"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="brrweights_+3A_strata">strata</code></td>
<td>
<p>Stratum identifiers </p>
</td></tr>
<tr><td><code id="brrweights_+3A_psu">psu</code></td>
<td>
<p>PSU (cluster) identifier </p>
</td></tr>
<tr><td><code id="brrweights_+3A_match">match</code></td>
<td>
<p>Optional variable to use in matching. </p>
</td></tr>
<tr><td><code id="brrweights_+3A_small">small</code></td>
<td>
<p>How to handle strata with only one PSU</p>
</td></tr>
<tr><td><code id="brrweights_+3A_large">large</code></td>
<td>
<p>How to handle strata with more than two PSUs</p>
</td></tr>
<tr><td><code id="brrweights_+3A_fpc">fpc</code></td>
<td>
<p>Optional population (stratum) size or finite population correction </p>
</td></tr>
<tr><td><code id="brrweights_+3A_fpctype">fpctype</code></td>
<td>
<p>How <code>fpc</code> is coded.</p>
</td></tr>
<tr><td><code id="brrweights_+3A_fay.rho">fay.rho</code></td>
<td>
<p>Parameter for Fay's extended BRR method</p>
</td></tr>
<tr><td><code id="brrweights_+3A_only.weights">only.weights</code></td>
<td>
<p>If <code>TRUE</code> return only the matrix of
replicate weights</p>
</td></tr>
<tr><td><code id="brrweights_+3A_compress">compress</code></td>
<td>
<p>If <code>TRUE</code>, store the replicate weights in
compressed form</p>
</td></tr>
<tr><td><code id="brrweights_+3A_hadamard.matrix">hadamard.matrix</code></td>
<td>
<p>Optional user-supplied Hadamard matrix for
<code>brrweights</code></p>
</td></tr>
<tr><td><code id="brrweights_+3A_lonely.psu">lonely.psu</code></td>
<td>
<p>Handling of non-certainty single-PSU strata</p>
</td></tr>
</table>


<h3>Details</h3>

<p>JK1 and JKn are  jackknife schemes for unstratified and stratified
designs respectively.  The finite population correction may be
specified as a single number, a vector with one entry per stratum, or
a vector with one entry per observation (constant within strata). 
When <code>fpc</code> is a vector with one entry per stratum it may not have
names that differ from the stratum identifiers (it may have no names,
in which case it must be in the same order as
<code>unique(strata)</code>). To specify population stratum sizes use
<code>fpctype="population"</code>, to specify sampling fractions use
<code>fpctype="fraction"</code> and to specify the correction directly use
<code>fpctype="correction"</code>
</p>
<p>The only reason not to use <code>compress=TRUE</code> is that it is new and
there is a greater possibility of bugs.  It reduces the number of
rows of the replicate weights matrix from the number of observations
to the number of PSUs.
</p>
<p>In BRR variance estimation each stratum is split in two to give
half-samples. Balanced replicated weights are needed, where
observations in two different strata end up in the same half stratum
as often as in different half-strata.BRR, strictly speaking, is
defined only when each stratum has exactly
two PSUs.  A stratum with one PSU can be merged with another such
stratum, or can be split to appear in both half samples with half
weight.  The latter approach is appropriate for a PSU that was
deterministically sampled.
</p>
<p>A stratum with more than two PSUs can be split into multiple smaller
strata each with two PSUs or the PSUs can be merged to give two
superclusters within the stratum.
</p>
<p>When merging small strata or grouping PSUs in large strata the
<code>match</code> variable is used to sort PSUs before merging, to give
approximate matching on this variable.
</p>
<p>If you want more control than this you should probably construct your
own weights using the Hadamard matrices produced by <code><a href="#topic+hadamard">hadamard</a></code>
</p>


<h3>Value</h3>

<p>For <code>brrweights</code> with <code>only.weights=FALSE</code> a list with elements
</p>
<table>
<tr><td><code>weights</code></td>
<td>
<p>two-column matrix indicating the weight for each
half-stratum in one particular set  of split samples</p>
</td></tr>
<tr><td><code>wstrata</code></td>
<td>
<p>New stratum variable incorporating merged or split strata</p>
</td></tr>
<tr><td><code>strata</code></td>
<td>
<p>Original strata for distinct PSUs</p>
</td></tr>
<tr><td><code>psu</code></td>
<td>
<p>Distinct PSUs</p>
</td></tr>
<tr><td><code>npairs</code></td>
<td>
<p>Dimension of Hadamard matrix used in BRR construction</p>
</td></tr>
<tr><td><code>sampler</code></td>
<td>
<p>function returning replicate weights</p>
</td></tr>
<tr><td><code>compress</code></td>
<td>
<p>Indicates whether the <code>sampler</code> returns per PSU
or per observation weights</p>
</td></tr>
</table>
<p>For <code>jk1weights</code> and <code>jknweights</code> a data frame of replicate
weights and the <code>scale</code> and <code>rscale</code> arguments to <code><a href="#topic+svrVar">svrVar</a></code>.
</p>


<h3>References</h3>

<p>Levy and Lemeshow &quot;Sampling of Populations&quot;. Wiley.
</p>
<p>Shao and Tu &quot;The Jackknife and Bootstrap&quot;. Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hadamard">hadamard</a></code>, <code><a href="#topic+as.svrepdesign">as.svrepdesign</a></code>,
<code><a href="#topic+svrVar">svrVar</a></code>, <code><a href="#topic+surveyoptions">surveyoptions</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(scd)
scdnofpc&lt;-svydesign(data=scd, prob=~1, id=~ambulance, strata=~ESA,
nest=TRUE)

## convert to BRR replicate weights
scd2brr &lt;- as.svrepdesign(scdnofpc, type="BRR")
svymean(~alive, scd2brr)
svyratio(~alive, ~arrests, scd2brr)

## with user-supplied hadamard matrix
scd2brr1 &lt;- as.svrepdesign(scdnofpc, type="BRR", hadamard.matrix=paley(11))
svymean(~alive, scd2brr1)
svyratio(~alive, ~arrests, scd2brr1)


</code></pre>

<hr>
<h2 id='calibrate'>Calibration (GREG) estimators</h2><span id='topic+calibrate.survey.design2'></span><span id='topic+calibrate.svyrep.design'></span><span id='topic+calibrate'></span><span id='topic+calibrate.twophase'></span><span id='topic+grake'></span><span id='topic+cal_names'></span>

<h3>Description</h3>

<p>Calibration, generalized raking, or GREG estimators generalise post-stratification and
raking by calibrating a sample to the marginal totals of
variables in a linear regression model.  This function reweights the
survey design and adds additional information that is used by
<code>svyrecvar</code> to reduce the estimated standard errors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calibrate(design,...)
## S3 method for class 'survey.design2'
calibrate(design, formula, population,
       aggregate.stage=NULL, stage=0, variance=NULL,
       bounds=c(-Inf,Inf), calfun=c("linear","raking","logit"),
       maxit=50,epsilon=1e-7,verbose=FALSE,force=FALSE,trim=NULL,
       bounds.const=FALSE, sparse=FALSE,...)
## S3 method for class 'svyrep.design'
calibrate(design, formula, population,compress=NA,
       aggregate.index=NULL, variance=NULL, bounds=c(-Inf,Inf),
       calfun=c("linear","raking","logit"),
       maxit=50, epsilon=1e-7, verbose=FALSE,force=FALSE,trim=NULL,
       bounds.const=FALSE, sparse=FALSE,...)
## S3 method for class 'twophase'
calibrate(design, phase=2,formula, population,
       calfun=c("linear","raking","logit","rrz"),...)
grake(mm,ww,calfun,eta=rep(0,NCOL(mm)),bounds,population,epsilon, 
  verbose,maxit,variance=NULL)
cal_names(formula,design,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calibrate_+3A_design">design</code></td>
<td>
<p>Survey design object</p>
</td></tr>
<tr><td><code id="calibrate_+3A_formula">formula</code></td>
<td>
<p>Model formula for calibration model, or list of
formulas for each margin</p>
</td></tr>
<tr><td><code id="calibrate_+3A_population">population</code></td>
<td>
<p>Vectors of population column totals for the model matrix in the
calibration model, or list of such vectors for each
cluster, or list of tables for each margin. Required except for two-phase designs</p>
</td></tr>
<tr><td><code id="calibrate_+3A_compress">compress</code></td>
<td>
<p>compress the resulting replicate weights if
<code>TRUE</code> or if <code>NA</code> and weights were previously compressed</p>
</td></tr>
<tr><td><code id="calibrate_+3A_stage">stage</code></td>
<td>
<p>See Details below</p>
</td></tr>
<tr><td><code id="calibrate_+3A_variance">variance</code></td>
<td>
<p>Coefficients for variance in calibration model (heteroskedasticity parameters) (see
Details below)</p>
</td></tr>
<tr><td><code id="calibrate_+3A_aggregate.stage">aggregate.stage</code></td>
<td>
<p>An integer. If not <code>NULL</code>, make calibration weights
constant within sampling units at this stage.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_aggregate.index">aggregate.index</code></td>
<td>
<p>A vector or one-sided formula. If not <code>NULL</code>, make calibration weights
constant within levels of this variable</p>
</td></tr>
<tr><td><code id="calibrate_+3A_bounds">bounds</code></td>
<td>
<p>Bounds for the calibration weights, optional
except for <code>calfun="logit"</code></p>
</td></tr>
<tr><td><code id="calibrate_+3A_bounds.const">bounds.const</code></td>
<td>
<p>Should be <code>TRUE</code> if <code>bounds</code> have been spcified as constant values rather than multiplicative values</p>
</td></tr>
<tr><td><code id="calibrate_+3A_trim">trim</code></td>
<td>
<p>Weights outside this range will be trimmed to these bounds.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_...">...</code></td>
<td>
<p>Options for other methods</p>
</td></tr>
<tr><td><code id="calibrate_+3A_calfun">calfun</code></td>
<td>
<p>Calibration function: see below</p>
</td></tr>
<tr><td><code id="calibrate_+3A_maxit">maxit</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code id="calibrate_+3A_epsilon">epsilon</code></td>
<td>
<p>Tolerance in matching population total. Either a single
number or a vector of the same length as <code>population</code></p>
</td></tr>
<tr><td><code id="calibrate_+3A_verbose">verbose</code></td>
<td>
<p>Print lots of uninteresting information</p>
</td></tr>
<tr><td><code id="calibrate_+3A_force">force</code></td>
<td>
<p>Return an answer even if the specified accuracy was not achieved</p>
</td></tr>
<tr><td><code id="calibrate_+3A_phase">phase</code></td>
<td>
<p>Phase of a two-phase design to calibrate (only
<code>phase=2</code> currently implemented.)</p>
</td></tr>
<tr><td><code id="calibrate_+3A_mm">mm</code></td>
<td>
<p>Model matrix</p>
</td></tr>
<tr><td><code id="calibrate_+3A_ww">ww</code></td>
<td>
<p>Vector of weights</p>
</td></tr>
<tr><td><code id="calibrate_+3A_eta">eta</code></td>
<td>
<p>Starting values for iteration</p>
</td></tr>
<tr><td><code id="calibrate_+3A_sparse">sparse</code></td>
<td>
<p>Use sparse matrices for faster computation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>formula</code> argument specifies a model matrix, and the
<code>population</code> argument is the population column sums of this
matrix. The function <code>cal_names</code> shows what the column names of
this model matrix will be. 
</p>
<p>For the important special case where the calibration totals are (possibly
overlapping) marginal tables of factor variables, as in classical
raking, the <code>formula</code> and <code>population</code> arguments may be
lists in the same format as the input to <code><a href="#topic+rake">rake</a></code>.
</p>
<p>If the <code>population</code> argument has a names attribute it will be
checked against the names produced by <code>model.matrix(formula)</code> and
reordered if necessary.  This protects against situations where the
(locale-dependent) ordering of factor levels is not what you expected.
</p>
<p>Numerical instabilities may result if the sampling weights in the
<code>design</code> object are wrong by multiple orders of magnitude. The
code now attempts to rescale the weights first, but it is better for
the user to ensure that the scale is reasonable. 
</p>
<p>The <code>calibrate</code> function implements linear, bounded linear,
raking, bounded raking, and logit calibration functions. All except
unbounded linear calibration use the Newton-Raphson algorithm
described by Deville et al (1993). This algorithm is exposed for other
uses in the <code>grake</code> function.  Unbounded linear calibration uses
an algorithm that is less sensitive to collinearity. The calibration
function may be specified as a string naming one of the three built-in
functions or as an object of class <code>calfun</code>, allowing
user-defined functions. See <code><a href="#topic+make.calfun">make.calfun</a></code> for details.
</p>
<p>The <code>bounds</code> argument can be specified as global upper and lower bounds e.g
<code>bounds=c(0.5, 2)</code> or as a list with lower and upper vectors e.g. 
<code>bounds=list(lower=lower, upper=upper)</code>. This allows for individual 
boundary constraints for each unit. The lower and upper vectors must be 
the same length as the input data. The bounds can be specified as multiplicative 
values or constant values. If constant, <code>bounds.const</code> must be set to <code>TRUE</code>.
</p>
<p>Calibration with bounds, or on highly collinear data, may fail. If
<code>force=TRUE</code> the approximately calibrated design object will
still be returned (useful for examining why it failed). A failure in
calibrating a set of replicate weights when the sampling weights were
successfully calibrated will give only a warning, not an error.
</p>
<p>When calibration to the desired set of bounds is not possible, another option is
to trim weights. To do this set <code>bounds</code> to a looser set of bounds
for which calibration is achievable and set <code>trim</code> to the tighter
bounds. Weights outside the bounds will be trimmed to the bounds, and
the excess weight distributed over other observations in proportion to
their sampling weight (and so this may put some other observations
slightly over the trimming bounds). The projection matrix used in computing
standard errors is based on the feasible bounds specified by the
<code>bounds</code> argument.  See also <code><a href="#topic+trimWeights">trimWeights</a></code>,
which trims the final weights in a design object rather than the
calibration adjustments.
</p>
<p>For two-phase designs <code>calfun="rrz"</code> estimates the sampling
probabilities using logistic regression as described by Robins et al
(1994). <code><a href="#topic+estWeights">estWeights</a></code> will do the same thing.
</p>
<p>Calibration may result in observations within the last-stage sampling
units having unequal weight even though they necessarily are sampled
together.  Specifying <code>aggegrate.stage</code> ensures that the
calibration weight adjustments are constant within sampling units at
the specified stage; if the original sampling weights were equal the
final weights will also be equal.  The algorithm is as described by
Vanderhoeft (2001, section III.D). Specifying <code>aggregate.index</code>
does the same thing for replicate weight designs; a warning will be
given if the original weights are not constant within levels of
<code>aggregate.index</code>.
</p>
<p>In a model with two-stage sampling, population totals may be available
for the PSUs actually sampled, but not for the whole population.  In
this situation, calibrating within each PSU reduces with second-stage
contribution to variance. This generalizes to multistage sampling.
The <code>stage</code> argument specifies which stage of sampling the totals
refer to.  Stage 0 is full population totals, stage 1 is totals for
PSUs, and so on.  The default, <code>stage=NULL</code> is interpreted as
stage 0 when a single population vector is supplied and stage 1 when a
list is supplied. Calibrating to PSU totals will fail (with a message
about an exactly singular matrix) for PSUs that have fewer
observations than the number of calibration variables.
</p>
<p>The variance in the calibration model may depend on covariates.  If <code>variance=NULL</code> the
calibration model has constant variance.  If <code>variance</code> is not <code>NULL</code>
it specifies a linear combination of the columns of the model matrix
and the calibration variance is proportional to that linear combination.
Alternatively <code>variance</code> can be specified as a vector of values the
same length as the input data specifying a heteroskedasticity parameter
for each unit.
</p>
<p>The design matrix specified by formula (after any aggregation) must be
of full rank, with one exception. If the population total for a column
is zero and all the observations are zero the column will be
ignored. This allows the use of factors where the population happens
to have no observations at some level.
</p>
<p>In a two-phase design, <code>population</code> may be omitted when
<code>phase=2</code>, to specify calibration to the phase-one sample. If the
two-phase design object was constructed using the more memory-efficient
<code>method="approx"</code> argument to <code><a href="#topic+twophase">twophase</a></code>, calibration of the first
phase of sampling to the population is not supported.
</p>


<h3>Value</h3>

<p>A survey design object.
</p>


<h3>References</h3>

<p>Breslow NE, Lumley T, Ballantyne CM, Chambless LE, Kulich M. Using the
whole cohort in the analysis of case-cohort data. Am J Epidemiol.
2009;169(11):1398-1405. doi:10.1093/aje/kwp055
</p>
<p>Deville J-C, Sarndal C-E, Sautory O (1993) Generalized Raking
Procedures in Survey Sampling. JASA 88:1013-1020
</p>
<p>Kalton G, Flores-Cervantes I (2003) &quot;Weighting methods&quot; J Official
Stat 19(2) 81-97
</p>
<p>Lumley T, Shaw PA, Dai JY (2011) &quot;Connections between survey calibration estimators and semiparametric models for incomplete data&quot; International Statistical Review. 79:200-220. (with discussion 79:221-232)
</p>
<p>Sarndal C-E, Swensson B, Wretman J. &quot;Model Assisted Survey
Sampling&quot;. Springer. 1991.
</p>
<p>Rao JNK, Yung W, Hidiroglou MA (2002)   Estimating equations for the
analysis of survey data using poststratification information. Sankhya
64 Series A Part 2, 364-378.
</p>
<p>Robins JM, Rotnitzky A, Zhao LP. (1994) Estimation of regression
coefficients when some regressors are not always observed. Journal of
the American Statistical Association, 89, 846-866.
</p>
<p>Vanderhoeft C (2001) Generalized Calibration at Statistics
Belgium. Statistics Belgium Working Paper No 3.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+postStratify">postStratify</a></code>, <code><a href="#topic+rake">rake</a></code> for other ways
to use auxiliary information
</p>
<p><code><a href="#topic+twophase">twophase</a></code> and <code>vignette("epi")</code> for an example of calibration in two-phase designs
</p>
<p><code>survey/tests/kalton.R</code> for examples replicating those in Kalton &amp; Flores-Cervantes (2003)
</p>
<p><code><a href="#topic+make.calfun">make.calfun</a></code> for user-defined calibration distances.
</p>
<p><code><a href="#topic+trimWeights">trimWeights</a></code> to trim final weights rather than calibration adjustments.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)

cal_names(~stype, dclus1)

pop.totals&lt;-c(`(Intercept)`=6194, stypeH=755, stypeM=1018)

## For a single factor variable this is equivalent to
## postStratify

(dclus1g&lt;-calibrate(dclus1, ~stype, pop.totals))

svymean(~api00, dclus1g)
svytotal(~enroll, dclus1g)
svytotal(~stype, dclus1g)

## Make weights constant within school district
(dclus1agg&lt;-calibrate(dclus1, ~stype, pop.totals, aggregate=1))
svymean(~api00, dclus1agg)
svytotal(~enroll, dclus1agg)
svytotal(~stype, dclus1agg)


## Now add sch.wide
cal_names(~stype+sch.wide, dclus1)
(dclus1g2 &lt;- calibrate(dclus1, ~stype+sch.wide, c(pop.totals, sch.wideYes=5122)))

svymean(~api00, dclus1g2)
svytotal(~enroll, dclus1g2)
svytotal(~stype, dclus1g2)

## Finally, calibrate on 1999 API and school type

cal_names(~stype+api99, dclus1)
(dclus1g3 &lt;- calibrate(dclus1, ~stype+api99, c(pop.totals, api99=3914069)))

svymean(~api00, dclus1g3)
svytotal(~enroll, dclus1g3)
svytotal(~stype, dclus1g3)


## Same syntax with replicate weights
rclus1&lt;-as.svrepdesign(dclus1)

(rclus1g3 &lt;- calibrate(rclus1, ~stype+api99, c(pop.totals, api99=3914069)))

svymean(~api00, rclus1g3)
svytotal(~enroll, rclus1g3)
svytotal(~stype, rclus1g3)

(rclus1agg3 &lt;- calibrate(rclus1, ~stype+api99, c(pop.totals,api99=3914069), aggregate.index=~dnum))

svymean(~api00, rclus1agg3)
svytotal(~enroll, rclus1agg3)
svytotal(~stype, rclus1agg3)


###
## Bounded weights
range(weights(dclus1g3)/weights(dclus1))
dclus1g3b &lt;- calibrate(dclus1, ~stype+api99, c(pop.totals, api99=3914069),bounds=c(0.6,1.6))
range(weights(dclus1g3b)/weights(dclus1))

svymean(~api00, dclus1g3b)
svytotal(~enroll, dclus1g3b)
svytotal(~stype, dclus1g3b)

## Individual boundary constraints as constant values
# the first weight will be bounded at 40, the rest free to move
bnds &lt;- list(
  lower = rep(-Inf, nrow(apiclus1)), 
  upper = c(40, rep(Inf, nrow(apiclus1)-1))) 
head(weights(dclus1g3))
dclus1g3b1 &lt;- calibrate(dclus1, ~stype+api99, c(pop.totals, api99=3914069), 
  bounds=bnds, bounds.const=TRUE)
head(weights(dclus1g3b1))
svytotal(~api.stu, dclus1g3b1)

## trimming
dclus1tr &lt;- calibrate(dclus1, ~stype+api99, c(pop.totals, api99=3914069), 
   bounds=c(0.5,2), trim=c(2/3,3/2))
svymean(~api00+api99+enroll, dclus1tr)
svytotal(~stype,dclus1tr)
range(weights(dclus1tr)/weights(dclus1))

rclus1tr &lt;- calibrate(rclus1, ~stype+api99, c(pop.totals, api99=3914069),
   bounds=c(0.5,2), trim=c(2/3,3/2))
svymean(~api00+api99+enroll, rclus1tr)
svytotal(~stype,rclus1tr)

## Input in the same format as rake() for classical raking
pop.table &lt;- xtabs(~stype+sch.wide,apipop)
pop.table2 &lt;- xtabs(~stype+comp.imp,apipop)
dclus1r&lt;-rake(dclus1, list(~stype+sch.wide, ~stype+comp.imp),
               list(pop.table, pop.table2))
gclus1r&lt;-calibrate(dclus1, formula=list(~stype+sch.wide, ~stype+comp.imp), 
     population=list(pop.table, pop.table2),calfun="raking")
svymean(~api00+stype, dclus1r)
svymean(~api00+stype, gclus1r)


## generalised raking
dclus1g3c &lt;- calibrate(dclus1, ~stype+api99, c(pop.totals,
    api99=3914069), calfun="raking")
range(weights(dclus1g3c)/weights(dclus1))

(dclus1g3d &lt;- calibrate(dclus1, ~stype+api99, c(pop.totals,
    api99=3914069), calfun=cal.logit, bounds=c(0.5,2.5)))
range(weights(dclus1g3d)/weights(dclus1))



## Ratio estimators are calibration estimators
dstrat&lt;-svydesign(id=~1,strata=~stype, weights=~pw, data=apistrat, fpc=~fpc)
svytotal(~api.stu,dstrat)

common&lt;-svyratio(~api.stu, ~enroll, dstrat, separate=FALSE)
predict(common, total=3811472)

pop&lt;-3811472
## equivalent to (common) ratio estimator
dstratg1&lt;-calibrate(dstrat,~enroll-1, pop, variance=1)
svytotal(~api.stu, dstratg1)

# Alternatively specifying the heteroskedasticity parameters directly
dstratgh &lt;- calibrate(dstrat,~enroll-1, pop, variance=apistrat$enroll)
svytotal(~api.stu, dstratgh)

</code></pre>

<hr>
<h2 id='compressWeights'>Compress replicate weight matrix</h2><span id='topic+compressWeights'></span><span id='topic+compressWeights.default'></span><span id='topic+compressWeights.repweights_compressed'></span><span id='topic++5B.repweights_compressed'></span><span id='topic+dim.repweights_compressed'></span><span id='topic+dimnames.repweights_compressed'></span><span id='topic+as.matrix.repweights_compressed'></span><span id='topic+as.matrix.repweights'></span><span id='topic+as.vector.repweights_compressed'></span><span id='topic+compressWeights.svyrep.design'></span>

<h3>Description</h3>

<p>Many replicate weight matrices have redundant rows, such as when
weights are the same for all observations in a PSU.  This function
produces a compressed form. Methods for <code>as.matrix</code> and
<code>as.vector</code> extract and expand the weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compressWeights(rw, ...)
## S3 method for class 'svyrep.design'
compressWeights(rw,...)
## S3 method for class 'repweights_compressed'
as.matrix(x,...)
## S3 method for class 'repweights_compressed'
as.vector(x,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compressWeights_+3A_rw">rw</code></td>
<td>
<p>A set of replicate weights or a <code>svyrep.design</code> object</p>
</td></tr>
<tr><td><code id="compressWeights_+3A_x">x</code></td>
<td>
<p>A compressed set of replicate weights</p>
</td></tr>
<tr><td><code id="compressWeights_+3A_...">...</code></td>
<td>
<p>For future expansion</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>repweights_compressed</code> or a
<code>svyrep.design</code> object with <code>repweights</code> element of class <code>repweights_compressed</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+jknweights">jknweights</a></code>,<code><a href="#topic+as.svrepdesign">as.svrepdesign</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)
rclus1c&lt;-as.svrepdesign(dclus1,compress=TRUE)
rclus1&lt;-as.svrepdesign(dclus1,compress=FALSE)
</code></pre>

<hr>
<h2 id='confint.svyglm'>Confidence intervals for regression parameters </h2><span id='topic+confint.svyglm'></span>

<h3>Description</h3>

<p>Computes confidence intervals for regression parameters in
<code><a href="#topic+svyglm">svyglm</a></code> objects. The default is a Wald-type confidence
interval, adding and subtracting a multiple of the standard error. The
<code>method="likelihood"</code> is an interval based on inverting the Rao-Scott
likelihood ratio test. That is, it is an interval where the working
model deviance is lower than the threshold for the Rao-Scott test at the
specified level.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'svyglm'
confint(object, parm, level = 0.95, method = c("Wald", "likelihood"), ddf = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confint.svyglm_+3A_object">object</code></td>
<td>
<p><code>svyglm</code> object</p>
</td></tr>
<tr><td><code id="confint.svyglm_+3A_parm">parm</code></td>
<td>
<p>numeric or character vector indicating which parameters to
construct intervals for.</p>
</td></tr>
<tr><td><code id="confint.svyglm_+3A_level">level</code></td>
<td>
<p>desired coverage</p>
</td></tr>
<tr><td><code id="confint.svyglm_+3A_method">method</code></td>
<td>
<p>See description above </p>
</td></tr>
<tr><td><code id="confint.svyglm_+3A_ddf">ddf</code></td>
<td>
<p>Denominator degrees of freedom for <code>"likelihood"</code>
method, to use a t distribution rather than norma. If <code>NULL</code>,
use <code>object$df.residual</code></p>
</td></tr>
<tr><td><code id="confint.svyglm_+3A_...">...</code></td>
<td>
<p>for future expansion</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of confidence intervals
</p>


<h3>References</h3>

<p>J. N. K. Rao and Alistair J. Scott (1984) On Chi-squared Tests For
Multiway Contigency Tables with Proportions Estimated From Survey
Data. Annals of Statistics 12:46-60
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+confint">confint</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus2&lt;-svydesign(id=~dnum+snum, fpc=~fpc1+fpc2, data=apiclus2)

m&lt;-svyglm(I(comp.imp=="Yes")~stype*emer+ell, design=dclus2, family=quasibinomial)
confint(m)
confint(m, method="like",ddf=NULL, parm=c("ell","emer"))

</code></pre>

<hr>
<h2 id='crowd'>Household crowding</h2><span id='topic+crowd'></span>

<h3>Description</h3>

<p>A tiny dataset from the VPLX manual.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(crowd)</code></pre>


<h3>Format</h3>

<p>A data frame with 6 observations on the following 5 variables.
</p>

<dl>
<dt>rooms</dt><dd><p>Number of rooms in the house</p>
</dd>
<dt>person</dt><dd><p>Number of people in the household</p>
</dd>
<dt>weight</dt><dd><p>Sampling weight</p>
</dd>
<dt>cluster</dt><dd><p>Cluster number</p>
</dd>
<dt>stratum</dt><dd><p>Stratum number</p>
</dd>
</dl>



<h3>Source</h3>

<p>Manual for VPLX, Census Bureau.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(crowd)

## Example 1-1
i1.1&lt;-as.svrepdesign(svydesign(id=~cluster, weight=~weight,data=crowd))
i1.1&lt;-update(i1.1, room.ratio=rooms/person,
overcrowded=factor(person&gt;rooms))
svymean(~rooms+person+room.ratio,i1.1)
svytotal(~rooms+person+room.ratio,i1.1)
svymean(~rooms+person+room.ratio,subset(i1.1,overcrowded==TRUE))
svytotal(~rooms+person+room.ratio,subset(i1.1,overcrowded==TRUE))

## Example 1-2
i1.2&lt;-as.svrepdesign(svydesign(id=~cluster,weight=~weight,strata=~stratum, data=crowd))
svymean(~rooms+person,i1.2)
svytotal(~rooms+person,i1.2)

</code></pre>

<hr>
<h2 id='dimnames.DBIsvydesign'>Dimensions of survey designs</h2><span id='topic+dimnames.DBIsvydesign'></span><span id='topic+dimnames.survey.design'></span><span id='topic+dimnames.svyrep.design'></span><span id='topic+dimnames.twophase'></span><span id='topic+dimnames.svyimputationList'></span><span id='topic+dim.DBIsvydesign'></span><span id='topic+dim.survey.design'></span><span id='topic+dim.twophase'></span><span id='topic+dim.svyimputationList'></span><span id='topic+dim.svyrep.design'></span>

<h3>Description</h3>

<p><code>dimnames</code> returns variable names and row names for the data
variables in a design object and <code>dim</code> returns dimensions.
For multiple imputation designs there is a third dimension giving the
number of imputations.  For database-backed designs the second dimension
includes variables defined by <code>update</code>.  The first dimension
excludes observations with zero weight.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survey.design'
dim(x)
## S3 method for class 'svyimputationList'
dim(x)
## S3 method for class 'survey.design'
dimnames(x)
## S3 method for class 'DBIsvydesign'
dimnames(x)
## S3 method for class 'svyimputationList'
dimnames(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dimnames.DBIsvydesign_+3A_x">x</code></td>
<td>
<p>Design object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of numbers for <code>dim</code>, a list of vectors of strings for <code>dimnames</code>.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+update.DBIsvydesign">update.DBIsvydesign</a></code>, <code><a href="#topic+with.svyimputationList">with.svyimputationList</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus1 &lt;- svydesign(ids=~dnum,weights=~pw,data=apiclus1,fpc=~fpc)
dim(dclus1)
dimnames(dclus1)
colnames(dclus1)
</code></pre>

<hr>
<h2 id='election'>US 2004 presidential election data at state or county level</h2><span id='topic+election'></span><span id='topic+election_pps'></span><span id='topic+election_jointprob'></span><span id='topic+election_jointHR'></span><span id='topic+election_insample'></span>

<h3>Description</h3>

<p>A sample of voting data from US states or counties (depending on data
availability), sampled with probability proportional to number of votes. The sample was drawn using Tille's splitting method, implemented in the &quot;sampling&quot; package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(election)</code></pre>


<h3>Format</h3>

<p><code>election</code> is a data frame with 4600 observations on the following 8 variables.
</p>

<dl>
<dt><code>County</code></dt><dd><p>A factor specifying the state or country</p>
</dd>
<dt><code>TotPrecincts</code></dt><dd><p>Number of precincts in the state or county</p>
</dd>
<dt><code>PrecinctsReporting</code></dt><dd><p>Number of precincts supplying data</p>
</dd>
<dt><code>Bush</code></dt><dd><p>Votes for George W. Bush</p>
</dd>
<dt><code>Kerry</code></dt><dd><p>Votes for John Kerry</p>
</dd>
<dt><code>Nader</code></dt><dd><p>Votes for Ralph Nader</p>
</dd>
<dt><code>votes</code></dt><dd><p>Total votes for those three candidates</p>
</dd>
<dt><code>p</code></dt><dd><p>Sampling probability, proportional to <code>votes</code></p>
</dd>
</dl>

<p><code>election_pps</code> is a sample of 40 counties or states taken with
probability proportional to the number of votes. It includes the
additional column <code>wt</code> with the sampling weights. 
</p>
<p><code>election_insample</code> indicates which rows of <code>election</code> were sampled.
</p>
<p><code>election_jointprob</code> are the pairwise sampling probabilities and
<code>election_jointHR</code> are approximate pairwise sampling probabilities using
the Hartley-Rao approximation.
</p>


<h3>Source</h3>

<p>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(election)
## high positive correlation between totals
plot(Bush~Kerry,data=election,log="xy")
## high negative correlation between proportions
plot(I(Bush/votes)~I(Kerry/votes), data=election)

## Variances without replacement
## Horvitz-Thompson type
dpps_br&lt;- svydesign(id=~1,  fpc=~p, data=election_pps, pps="brewer")
dpps_ov&lt;- svydesign(id=~1,  fpc=~p, data=election_pps, pps="overton")
dpps_hr&lt;- svydesign(id=~1,  fpc=~p, data=election_pps, pps=HR(sum(election$p^2)/40))
dpps_hr1&lt;- svydesign(id=~1, fpc=~p, data=election_pps, pps=HR())
dpps_ht&lt;- svydesign(id=~1,  fpc=~p, data=election_pps, pps=ppsmat(election_jointprob))
## Yates-Grundy type
dpps_yg&lt;- svydesign(id=~1,  fpc=~p, data=election_pps, pps=ppsmat(election_jointprob),variance="YG")
dpps_hryg&lt;- svydesign(id=~1,  fpc=~p, data=election_pps, pps=HR(sum(election$p^2)/40),variance="YG")

## The with-replacement approximation
dppswr &lt;-svydesign(id=~1, probs=~p, data=election_pps)

svytotal(~Bush+Kerry+Nader, dpps_ht)
svytotal(~Bush+Kerry+Nader, dpps_yg)
svytotal(~Bush+Kerry+Nader, dpps_hr)
svytotal(~Bush+Kerry+Nader, dpps_hryg)
svytotal(~Bush+Kerry+Nader, dpps_hr1)
svytotal(~Bush+Kerry+Nader, dpps_br)
svytotal(~Bush+Kerry+Nader, dpps_ov)
svytotal(~Bush+Kerry+Nader, dppswr)
</code></pre>

<hr>
<h2 id='estweights'>Estimated weights for missing data</h2><span id='topic+estWeights'></span><span id='topic+estWeights.twophase'></span><span id='topic+estWeights.data.frame'></span>

<h3>Description</h3>

<p>Creates or adjusts a two-phase survey design object using a logistic
regression model for second-phase sampling probability.  This function
should be particularly useful in reweighting to account for missing data.  </p>


<h3>Usage</h3>

<pre><code class='language-R'>estWeights(data,formula,...)
## S3 method for class 'twophase'
estWeights(data,formula=NULL, working.model=NULL,...)
## S3 method for class 'data.frame'
estWeights(data,formula=NULL, working.model=NULL,
      subset=NULL, strata=NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estweights_+3A_data">data</code></td>
<td>
<p>twophase design object or data frame</p>
</td></tr>
<tr><td><code id="estweights_+3A_formula">formula</code></td>
<td>
<p>Predictors for estimating weights</p>
</td></tr>
<tr><td><code id="estweights_+3A_working.model">working.model</code></td>
<td>
<p>Model fitted to complete (ie phase 1) data</p>
</td></tr>
<tr><td><code id="estweights_+3A_subset">subset</code></td>
<td>
<p>Subset of data frame with complete data (ie phase 1).
If <code>NULL</code> use all complete cases</p>
</td></tr>
<tr><td><code id="estweights_+3A_strata">strata</code></td>
<td>
<p>Stratification (if any) of phase 2 sampling</p>
</td></tr>
<tr><td><code id="estweights_+3A_...">...</code></td>
<td>
<p>for future expansion</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>data</code> is a data frame, <code>estWeights</code> first creates a
two-phase design object. The <code>strata</code> argument is used only to
compute finite population corrections, the same variables must be
included in <code>formula</code> to compute stratified sampling probabilities.
</p>
<p>With a two-phase design object, <code>estWeights</code> estimates the sampling
probabilities using logistic regression as described by Robins et al
(1994) and adds information to the object to enable correct sandwich
standard errors to be computed.
</p>
<p>An alternative to specifying <code>formula</code> is to specify
<code>working.model</code>. The estimating functions from this model will be
used as predictors of the sampling probabilities, which will increase
efficiency to the extent that the working model and the model of
interest estimate the same parameters (Kulich &amp; Lin 2004).
</p>
<p>The effect on a two-phase design object is very similar to
<code><a href="#topic+calibrate">calibrate</a></code>, and is identical when <code>formula</code>
specifies a saturated model.
</p>


<h3>Value</h3>

<p>A two-phase survey design object.
</p>


<h3>References</h3>

<p>Breslow NE, Lumley T, Ballantyne CM, Chambless LE, Kulich M. (2009) Using the Whole Cohort in the Analysis of Case-Cohort Data.  Am J Epidemiol. 2009 Jun 1;169(11):1398-405.
</p>
<p>Robins JM, Rotnitzky A, Zhao LP. (1994) Estimation of regression
coefficients when some regressors are not always observed. Journal of
the American Statistical Association, 89, 846-866.
</p>
<p>Kulich M, Lin DY (2004). Improving the Efficiency of Relative-Risk
Estimation in Case-Cohort Studies. Journal of the American Statistical Association, Vol. 99,  pp.832-844 
</p>
<p>Lumley T, Shaw PA, Dai JY (2011) &quot;Connections between survey calibration estimators and semiparametric models for incomplete data&quot; International Statistical Review. 79:200-220. (with discussion 79:221-232)
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+postStratify">postStratify</a></code>,
<code><a href="#topic+calibrate">calibrate</a></code>, <code><a href="#topic+twophase">twophase</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(airquality)

## ignoring missingness, using model-based standard error
summary(lm(log(Ozone)~Temp+Wind, data=airquality))

## Without covariates to predict missingness we get
## same point estimates, but different (sandwich) standard errors
daq&lt;-estWeights(airquality, formula=~1,subset=~I(!is.na(Ozone)))
summary(svyglm(log(Ozone)~Temp+Wind,design=daq))

## Reweighting based on weather, month
d2aq&lt;-estWeights(airquality, formula=~Temp+Wind+Month,
                 subset=~I(!is.na(Ozone)))
summary(svyglm(log(Ozone)~Temp+Wind,design=d2aq))

</code></pre>

<hr>
<h2 id='fpc'>Small survey example</h2><span id='topic+fpc'></span>

<h3>Description</h3>

<p>The <code>fpc</code> data frame has 8 rows and 6 columns. It is artificial
data to illustrate survey sampling estimators.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(fpc)</code></pre>


<h3>Format</h3>

<p>This data frame contains the following columns:
</p>

<dl>
<dt>stratid</dt><dd><p>Stratum ids</p>
</dd>
<dt>psuid</dt><dd><p>Sampling unit ids</p>
</dd>
<dt>weight</dt><dd><p>Sampling weights</p>
</dd>
<dt>nh</dt><dd><p>number sampled per stratum</p>
</dd>
<dt>Nh</dt><dd><p>population size per stratum</p>
</dd>
<dt>x</dt><dd><p>data</p>
</dd>
</dl>



<h3>Source</h3>

<p><code style="white-space: pre;">&#8288;https://www.stata-press.com/data/r7/fpc.dta&#8288;</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(fpc)
fpc


withoutfpc&lt;-svydesign(weights=~weight, ids=~psuid, strata=~stratid, variables=~x, 
   data=fpc, nest=TRUE)

withoutfpc
svymean(~x, withoutfpc)

withfpc&lt;-svydesign(weights=~weight, ids=~psuid, strata=~stratid,
fpc=~Nh, variables=~x, data=fpc, nest=TRUE)

withfpc
svymean(~x, withfpc)

## Other equivalent forms 
withfpc&lt;-svydesign(prob=~I(1/weight), ids=~psuid, strata=~stratid,
fpc=~Nh, variables=~x, data=fpc, nest=TRUE)

svymean(~x, withfpc)

withfpc&lt;-svydesign(weights=~weight, ids=~psuid, strata=~stratid,
fpc=~I(nh/Nh), variables=~x, data=fpc, nest=TRUE)

svymean(~x, withfpc)

withfpc&lt;-svydesign(weights=~weight, ids=~interaction(stratid,psuid),
strata=~stratid, fpc=~I(nh/Nh), variables=~x, data=fpc)

svymean(~x, withfpc)

withfpc&lt;-svydesign(ids=~psuid, strata=~stratid, fpc=~Nh,
 variables=~x,data=fpc,nest=TRUE)

svymean(~x, withfpc)

withfpc&lt;-svydesign(ids=~psuid, strata=~stratid,
fpc=~I(nh/Nh), variables=~x, data=fpc, nest=TRUE)

svymean(~x, withfpc)



</code></pre>

<hr>
<h2 id='ftable.svystat'>Lay out tables of survey statistics</h2><span id='topic+ftable.svystat'></span><span id='topic+ftable.svrepstat'></span><span id='topic+ftable.svyby'></span>

<h3>Description</h3>

<p>Reformat the output of survey computations to a table.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'svystat'
ftable(x, rownames,...)
## S3 method for class 'svrepstat'
ftable(x, rownames,...)
## S3 method for class 'svyby'
ftable(x,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ftable.svystat_+3A_x">x</code></td>
<td>
<p>Output of functions such as <code>svymean</code>,<code>svrepmean</code>, <code>svyby</code></p>
</td></tr>
<tr><td><code id="ftable.svystat_+3A_rownames">rownames</code></td>
<td>
<p>List of vectors of strings giving dimension names for
the resulting table (see examples)</p>
</td></tr>
<tr><td><code id="ftable.svystat_+3A_...">...</code></td>
<td>
<p>Arguments for future expansion</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"ftable"</code>
</p>


<h3>See Also</h3>

 <p><code><a href="stats.html#topic+ftable">ftable</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)

a&lt;-svymean(~interaction(stype,comp.imp), design=dclus1)
b&lt;-ftable(a, rownames=list(stype=c("E","H","M"),comp.imp=c("No","Yes")))
b

a&lt;-svymean(~interaction(stype,comp.imp), design=dclus1, deff=TRUE)
b&lt;-ftable(a, rownames=list(stype=c("E","H","M"),comp.imp=c("No","Yes")))
round(100*b,1)

rclus1&lt;-as.svrepdesign(dclus1)
a&lt;-svytotal(~interaction(stype,comp.imp), design=rclus1)
b&lt;-ftable(a, rownames=list(stype=c("E","H","M"),comp.imp=c("No","Yes")))
b
round(b)

a&lt;-svyby(~api99 + api00, ~stype + sch.wide, rclus1, svymean, keep.var=TRUE)
ftable(a)
print(ftable(a),digits=2)

b&lt;-svyby(~api99 + api00, ~stype + sch.wide, rclus1, svymean, keep.var=TRUE, deff=TRUE)
print(ftable(b),digits=2)

d&lt;-svyby(~api99 + api00, ~stype + sch.wide, rclus1, svymean, keep.var=TRUE, vartype=c("se","cvpct"))
round(ftable(d),1)

</code></pre>

<hr>
<h2 id='hadamard'>Hadamard matrices </h2><span id='topic+hadamard'></span>

<h3>Description</h3>

<p>Returns a Hadamard matrix of dimension larger than the argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hadamard(n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hadamard_+3A_n">n</code></td>
<td>
<p>lower bound for size </p>
</td></tr>
</table>


<h3>Details</h3>

<p>For most <code>n</code> the matrix comes from <code><a href="#topic+paley">paley</a></code>. The
<code class="reqn">36\times 36</code> matrix is from Plackett and Burman (1946)
and the <code class="reqn">28\times 28</code> is from Sloane's library of Hadamard
matrices.
</p>
<p>Matrices of dimension every multiple of 4 are thought to exist, but
this function doesn't know about all of them, so it will sometimes
return matrices that are larger than necessary.  The excess is at most
4 for <code class="reqn">n&lt;180</code> and at most 5% for <code class="reqn">n&gt;100</code>.
</p>


<h3>Value</h3>

<p>A Hadamard matrix
</p>


<h3>Note</h3>

<p>Strictly speaking, a Hadamard matrix has entries +1 and -1 rather
than 1 and 0, so <code>2*hadamard(n)-1</code> is a Hadamard matrix</p>


<h3>References</h3>

<p>Sloane NJA. A Library of Hadamard Matrices <a href="http://neilsloane.com/hadamard/">http://neilsloane.com/hadamard/</a>
</p>
<p>Plackett RL, Burman JP. (1946) The Design of Optimum Multifactorial Experiments
Biometrika, Vol. 33, No. 4  pp. 305-325 
</p>
<p>Cameron PJ (2005) Hadamard Matrices
<a href="http://designtheory.org/library/encyc/topics/had.pdf">http://designtheory.org/library/encyc/topics/had.pdf</a>. In: The
Encyclopedia of Design Theory <a href="http://designtheory.org/library/encyc/">http://designtheory.org/library/encyc/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+brrweights">brrweights</a></code>, <code><a href="#topic+paley">paley</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
par(mfrow=c(2,2))
## Sylvester-type
image(hadamard(63),main=quote("Sylvester: "*64==2^6))
## Paley-type
image(hadamard(59),main=quote("Paley: "*60==59+1))
## from NJ Sloane's library
image(hadamard(27),main=quote("Stored: "*28))
## For n=90 we get 96 rather than the minimum possible size, 92.
image(hadamard(90),main=quote("Constructed: "*96==2^3%*%(11+1)))

par(mfrow=c(1,1))
plot(2:150,sapply(2:150,function(i) ncol(hadamard(i))),type="S",
     ylab="Matrix size",xlab="n",xlim=c(1,150),ylim=c(1,150))
abline(0,1,lty=3)
lines(2:150, 2:150-(2:150 %% 4)+4,col="purple",type="S",lty=2)
legend(c(x=10,y=140),legend=c("Actual size","Minimum possible size"),
     col=c("black","purple"),bty="n",lty=c(1,2))

</code></pre>

<hr>
<h2 id='hospital'>Sample of obstetric hospitals </h2><span id='topic+hospital'></span>

<h3>Description</h3>

<p>The <code>hospital</code> data frame has 15 rows and 5 columns. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(hospital)</code></pre>


<h3>Format</h3>

<p>This data frame contains the following columns:
</p>

<dl>
<dt>hospno</dt><dd><p>Hospital id</p>
</dd>
<dt>oblevel</dt><dd><p>level of obstetric care</p>
</dd>
<dt>weighta</dt><dd><p>Weights, as given by the original reference</p>
</dd>
<dt>tothosp</dt><dd><p>total hospitalisations</p>
</dd>
<dt>births</dt><dd><p>births</p>
</dd>
<dt>weightats</dt><dd><p>Weights, as given in the source</p>
</dd>
</dl>



<h3>Source</h3>

<p>Previously at <code style="white-space: pre;">&#8288;http://www.ats.ucla.edu/stat/books/sop/hospsamp.dta&#8288;</code>
</p>


<h3>References</h3>

<p>Levy and Lemeshow. &quot;Sampling of Populations&quot; (3rd edition). Wiley.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hospital)
hospdes&lt;-svydesign(strata=~oblevel, id=~hospno, weights=~weighta,
fpc=~tothosp, data=hospital)
hosprep&lt;-as.svrepdesign(hospdes)

svytotal(~births, design=hospdes)
svytotal(~births, design=hosprep)

</code></pre>

<hr>
<h2 id='HR'>Wrappers for specifying PPS designs</h2><span id='topic+HR'></span><span id='topic+ppsmat'></span><span id='topic+ppscov'></span>

<h3>Description</h3>

<p>The Horvitz-Thompson estimator and the Hartley-Rao approximation require information in addition to the sampling probabilities for sampled individuals.  These functions allow this information to be supplied.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HR(psum=NULL, strata = NULL)
ppsmat(jointprob, tolerance = 1e-04)
ppscov(probcov, weighted=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HR_+3A_psum">psum</code></td>
<td>
<p> The sum of squared sampling probabilities for the population, divided by the sample size, as a single number or as a vector for stratified sampling
</p>
</td></tr>
<tr><td><code id="HR_+3A_strata">strata</code></td>
<td>

<p>Stratum labels, of the same length as <code>psum</code>, if <code>psum</code> is a vector
</p>
</td></tr>
<tr><td><code id="HR_+3A_jointprob">jointprob</code></td>
<td>
<p>Matrix of pairwise sampling probabilities for the sampled individuals</p>
</td></tr>
<tr><td><code id="HR_+3A_tolerance">tolerance</code></td>
<td>
<p>Tolerance for deciding that the covariance of sampling indicators is zero</p>
</td></tr>
<tr><td><code id="HR_+3A_probcov">probcov</code></td>
<td>
<p>Covariance of the sampling indicators (often written 'Delta'), or weighted covariance if <code>weighted=TRUE</code></p>
</td></tr>
<tr><td><code id="HR_+3A_weighted">weighted</code></td>
<td>
<p>If <code>TRUE</code>, the <code>probcov</code> argument is the covariance divided by pairwise sampling probabilities</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>HR</code>,<code>ppsmat</code>, <code>ppsdelta</code>, or <code>ppsdcheck</code> suitable for supplying as the <code>pps</code> argument to <code><a href="#topic+svydesign">svydesign</a></code>.
</p>


<h3>See Also</h3>

<p><a href="#topic+election">election</a> for examples of PPS designs
</p>


<h3>Examples</h3>

<pre><code class='language-R'>HR(0.1)
</code></pre>

<hr>
<h2 id='make.calfun'>Calibration metrics</h2><span id='topic+make.calfun'></span><span id='topic+cal.linear'></span><span id='topic+cal.raking'></span><span id='topic+cal.logit'></span><span id='topic+cal.sinh'></span>

<h3>Description</h3>

<p>Create calibration metric for use  in <code><a href="#topic+calibrate">calibrate</a></code>. The
function <code>F</code> is the link function described in section 2 of
Deville et al. To create a new calibration metric, specify <code class="reqn">F-1</code> and its
derivative. The package provides <code>cal.linear</code>, <code>cal.raking</code>,
<code>cal.logit</code>, which are standard, and <code>cal.sinh</code> from the
<code>CALMAR2</code> macro, for which <code>F</code> is the derivative of the inverse hyperbolic
sine. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.calfun(Fm1, dF, name)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.calfun_+3A_fm1">Fm1</code></td>
<td>
<p>Function <code class="reqn">F-1</code> taking a vector <code>u</code> and a
vector of length 2, <code>bounds</code>.</p>
</td></tr>
<tr><td><code id="make.calfun_+3A_df">dF</code></td>
<td>
<p>Derivative of <code>Fm1</code> wrt <code>u</code>: arguments <code>u</code>
and <code>bounds</code> </p>
</td></tr>
<tr><td><code id="make.calfun_+3A_name">name</code></td>
<td>
<p>Character string to use as name </p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"calfun"</code>
</p>


<h3>References</h3>

<p>Deville J-C, Sarndal C-E, Sautory O (1993) Generalized Raking
Procedures in Survey Sampling. JASA 88:1013-1020
</p>
<p>Deville J-C, Sarndal C-E (1992) Calibration Estimators in Survey
Sampling. JASA 87: 376-382
</p>


<h3>See Also</h3>

<p><code><a href="#topic+calibrate">calibrate</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>str(cal.linear)
cal.linear$Fm1
cal.linear$dF

hellinger &lt;- make.calfun(Fm1=function(u, bounds)  ((1-u/2)^-2)-1,
                    dF= function(u, bounds) (1-u/2)^-3 ,
                    name="hellinger distance")

hellinger

data(api)
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)

svymean(~api00,calibrate(dclus1, ~api99, pop=c(6194, 3914069),
         calfun=hellinger))

svymean(~api00,calibrate(dclus1, ~api99, pop=c(6194, 3914069),
         calfun=cal.linear))

svymean(~api00,calibrate(dclus1, ~api99, pop=c(6194,3914069),
          calfun=cal.raking))
</code></pre>

<hr>
<h2 id='marginpred'>
Standardised predictions (predictive margins) for regression models.
</h2><span id='topic+marginpred'></span><span id='topic+marginpred.svycoxph'></span><span id='topic+marginpred.svykmlist'></span><span id='topic+marginpred.svyglm'></span>

<h3>Description</h3>

<p>Reweights the design (using <code><a href="#topic+calibrate">calibrate</a></code>) so that the adjustment variables are uncorrelated
with the variables in the model, and then performs predictions by
calling <code>predict</code>.  When the adjustment model is saturated this is
equivalent to direct standardization on the adjustment variables. 
</p>
<p>The <code>svycoxph</code> and <code>svykmlist</code> methods return survival curves.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>marginpred(model, adjustfor, predictat, ...)
## S3 method for class 'svycoxph'
marginpred(model, adjustfor, predictat, se=FALSE, ...)
## S3 method for class 'svykmlist'
marginpred(model, adjustfor, predictat, se=FALSE, ...)
## S3 method for class 'svyglm'
marginpred(model, adjustfor, predictat,  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="marginpred_+3A_model">model</code></td>
<td>

<p>A regression model object of a class that has a <code>marginpred</code> method
</p>
</td></tr>
<tr><td><code id="marginpred_+3A_adjustfor">adjustfor</code></td>
<td>

<p>Model formula specifying adjustment variables, which must be in the
design object of the model
</p>
</td></tr>
<tr><td><code id="marginpred_+3A_predictat">predictat</code></td>
<td>

<p>A data frame giving values of the variables in <code>model</code> to
predict at</p>
</td></tr>
<tr><td><code id="marginpred_+3A_se">se</code></td>
<td>
<p>Estimate standard errors for the survival curve (uses a lot
of memory if the sample size is large)</p>
</td></tr>
<tr><td><code id="marginpred_+3A_...">...</code></td>
<td>
<p>Extra arguments, passed to the <code>predict</code> method for <code>model</code></p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+svypredmeans">svypredmeans</a></code> for the method of Graubard and Korn implemented in SUDAAN.
</p>
<p><code><a href="#topic+calibrate">calibrate</a></code>
</p>
<p><code><a href="#topic+predict.svycoxph">predict.svycoxph</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate data with apparent group effect from confounding
set.seed(42)
df&lt;-data.frame(x=rnorm(100))
df$time&lt;-rexp(100)*exp(df$x-1)
df$status&lt;-1
df$group&lt;-(df$x+rnorm(100))&gt;0
des&lt;-svydesign(id=~1,data=df)
newdf&lt;-data.frame(group=c(FALSE,TRUE), x=c(0,0))

## Cox model
m0&lt;-svycoxph(Surv(time,status)~group,design=des)
m1&lt;-svycoxph(Surv(time,status)~group+x,design=des)
## conditional predictions, unadjusted and adjusted
cpred0&lt;-predict(m0, type="curve", newdata=newdf, se=TRUE)
cpred1&lt;-predict(m1, type="curve", newdata=newdf, se=TRUE)
## adjusted marginal prediction
mpred&lt;-marginpred(m0, adjustfor=~x, predictat=newdf, se=TRUE)

plot(cpred0)
lines(cpred1[[1]],col="red")
lines(cpred1[[2]],col="red")
lines(mpred[[1]],col="blue")
lines(mpred[[2]],col="blue")

## Kaplan--Meier
s2&lt;-svykm(Surv(time,status&gt;0)~group, design=des)
p2&lt;-marginpred(s2, adjustfor=~x, predictat=newdf,se=TRUE)
plot(s2)
lines(p2[[1]],col="green")
lines(p2[[2]],col="green")

## logistic regression
logisticm &lt;- svyglm(group~time, family=quasibinomial, design=des)
newdf$time&lt;-c(0.1,0.8)
logisticpred &lt;- marginpred(logisticm, adjustfor=~x, predictat=newdf)
</code></pre>

<hr>
<h2 id='mu284'>Two-stage sample from MU284</h2><span id='topic+mu284'></span>

<h3>Description</h3>

<p>The MU284 population comes from Sarndal et al, and the complete data are
available from Statlib. These data are a two-stage sample from the
population, analyzed on page 143 of the book.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(mu284)</code></pre>


<h3>Format</h3>

<p>A data frame with 15 observations on the following 5 variables.
</p>

<dl>
<dt><code>id1</code></dt><dd><p>identifier for PSU</p>
</dd>
<dt><code>n1</code></dt><dd><p>number of PSUs in population</p>
</dd>
<dt><code>id2</code></dt><dd><p>identifier for second-stage unit</p>
</dd>
<dt><code>y1</code></dt><dd><p>variable to be analysed</p>
</dd>
<dt><code>n2</code></dt><dd><p>number of second-stage units in this PSU</p>
</dd>
</dl>



<h3>Source</h3>

<p>Carl Erik Sarndal, Bengt Swensson, Jan Wretman. (1991) &quot;Model Assisted
Survey Sampling&quot; Springer.
</p>
<p>(downloaded from StatLib, which is no longer active)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mu284)
(dmu284&lt;-svydesign(id=~id1+id2,fpc=~n1+n2, data=mu284))
(ytotal&lt;-svytotal(~y1, dmu284))
vcov(ytotal)
</code></pre>

<hr>
<h2 id='myco'>
Association between leprosy and BCG vaccination
</h2><span id='topic+myco'></span>

<h3>Description</h3>

<p>These data are in a paper by JNK Rao and colleagues, on score tests
for complex survey data.  External information (not further specified) suggests
the functional form for the <code>Age</code> variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("myco")</code></pre>


<h3>Format</h3>

<p>A data frame with 516 observations on the following 6 variables.
</p>

<dl>
<dt><code>Age</code></dt><dd><p>Age in years at the midpoint of six age strata</p>
</dd>
<dt><code>Scar</code></dt><dd><p>Presence of a BCG vaccination scar</p>
</dd>
<dt><code>n</code></dt><dd><p>Sampled number of cases (and thus controls) in the age stratum</p>
</dd>
<dt><code>Ncontrol</code></dt><dd><p>Number of non-cases in the population</p>
</dd>
<dt><code>wt</code></dt><dd><p>Sampling weight</p>
</dd>
<dt><code>leprosy</code></dt><dd><p>case status 0/1</p>
</dd>
</dl>



<h3>Details</h3>

<p>The data are a simulated stratified case-control study drawn from a
population study conducted in a region of Malawi (Clayton and Hills,
1993, Table 18.1). The goal was to examine whether BCG vaccination against
tuberculosis protects against leprosy (the causative agents are both species of
_Mycobacterium_). Rao et al have a typographical error: the number of
non-cases in the population in the 25-30 age stratum is given as 4981
but 5981 matches both the computational output and the data as given by Clayton
and Hills.
</p>


<h3>Source</h3>

<p>JNK Rao, AJ Scott, and Skinner, C. (1998). QUASI-SCORE TESTS WITH SURVEY
DATA. Statistica Sinica, 8(4), 1059-1070.
</p>
<p>Clayton, D., &amp; Hills, M. (1993). Statistical Models in Epidemiology. OUP
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(myco)
dmyco&lt;-svydesign(id=~1, strata=~interaction(Age,leprosy),weights=~wt,data=myco)

m_full&lt;-svyglm(leprosy~I((Age+7.5)^-2)+Scar, family=quasibinomial, design=dmyco)
m_age&lt;-svyglm(leprosy~I((Age+7.5)^-2), family=quasibinomial, design=dmyco)
anova(m_full,m_age)

## unweighted model does not match
m_full
glm(leprosy~I((Age+7.5)^-2)+Scar, family=binomial, data=myco)

</code></pre>

<hr>
<h2 id='newsvyquantile'>
Quantiles under complex sampling.
</h2><span id='topic+svyquantile'></span><span id='topic+svyquantile.survey.design'></span><span id='topic+svyquantile.svyrep.design'></span>

<h3>Description</h3>

<p>Estimates quantiles and confidence intervals for them. This function was
completely re-written for version 4.1 of the survey package, and has a
wider range of ways to define the quantile. See the vignette for a list of them. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svyquantile(x, design, quantiles, ...)
## S3 method for class 'survey.design'
svyquantile(x, design, quantiles, alpha = 0.05,
                      interval.type = c("mean", "beta","xlogit", "asin","score"),
                      na.rm = FALSE,  ci=TRUE, se = ci,
                      qrule=c("math","school","shahvaish","hf1","hf2","hf3",
		      "hf4","hf5","hf6","hf7","hf8","hf9"),
                      df = NULL, ...)
## S3 method for class 'svyrep.design'
svyquantile(x, design, quantiles, alpha = 0.05,
                      interval.type = c("mean", "beta","xlogit", "asin","quantile"),
                      na.rm = FALSE, ci = TRUE, se=ci,
                      qrule=c("math","school","shahvaish","hf1","hf2","hf3",
		      "hf4","hf5","hf6","hf7","hf8","hf9"),
                      df = NULL, return.replicates=FALSE,...)		      
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="newsvyquantile_+3A_x">x</code></td>
<td>

<p>A one-sided formula describing variables to be used
</p>
</td></tr>
<tr><td><code id="newsvyquantile_+3A_design">design</code></td>
<td>

<p>Design object
</p>
</td></tr>
<tr><td><code id="newsvyquantile_+3A_quantiles">quantiles</code></td>
<td>

<p>Numeric vector specifying which quantiles are requested
</p>
</td></tr>
<tr><td><code id="newsvyquantile_+3A_alpha">alpha</code></td>
<td>
<p>Specified confidence interval coverage</p>
</td></tr>
<tr><td><code id="newsvyquantile_+3A_interval.type">interval.type</code></td>
<td>
<p>See Details below</p>
</td></tr>
<tr><td><code id="newsvyquantile_+3A_na.rm">na.rm</code></td>
<td>
<p>Remove missing values?</p>
</td></tr>
<tr><td><code id="newsvyquantile_+3A_ci">ci</code>, <code id="newsvyquantile_+3A_se">se</code></td>
<td>
<p>Return an estimated confidence interval and standard error?</p>
</td></tr>
<tr><td><code id="newsvyquantile_+3A_qrule">qrule</code></td>
<td>
<p>Rule for defining the quantiles: either a character string specifying one of the built-in rules, or a function</p>
</td></tr>
<tr><td><code id="newsvyquantile_+3A_df">df</code></td>
<td>
<p>Degrees of freedom for confidence interval estimation: <code>NULL</code> specifies <code>degf(design)</code></p>
</td></tr>
<tr><td><code id="newsvyquantile_+3A_return.replicates">return.replicates</code></td>
<td>
<p>Return replicate estimates of the quantile (only for <code>interval.type="quantile"</code>)</p>
</td></tr>
<tr><td><code id="newsvyquantile_+3A_...">...</code></td>
<td>

<p>For future expansion
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>p</code>th quantile is defined as the value where the estimated cumulative
distribution function is equal to <code>p</code>. As with quantiles in
unweighted data, this definition only pins down the quantile to an
interval between two observations, and a rule is needed to interpolate.
The default is the mathematical definition, the lower end of the
quantile interval; <code>qrule="school"</code> uses the midpoint of the
quantile interval; <code>"hf1"</code> to <code>"hf9"</code> are weighted analogues of
<code>type=1</code> to <code>9</code> in <code><a href="stats.html#topic+quantile">quantile</a></code>. See the vignette
&quot;Quantile rules&quot; for details and for how to write your own. 
</p>
<p>By default, confidence intervals are estimated using Woodruff's (1952) method,
which involves computing the quantile, estimating a confidence interval
for the proportion of observations below the quantile, and then
transforming that interval using the estimated CDF.  In that context,
the <code>interval.type</code> argument specifies how the confidence interval
for the proportion is computed, matching <code><a href="#topic+svyciprop">svyciprop</a></code>. In
contrast to <code>oldsvyquantile</code>, <code>NaN</code> is returned if a confidence
interval endpoint on the probability scale falls outside <code>[0,1]</code>.
</p>
<p>There are two exceptions. For <code>svydesign</code> objects,
<code>interval.type="score"</code> asks for the Francisco &amp; Fuller confidence
interval based on inverting a score test. According to Dorfmann &amp;
Valliant, this interval has inferior performance to the <code>"beta"</code>
and <code>"logit"</code> intervals; it is provided for compatibility.
</p>
<p>For replicate-weight designs, <code>interval.type="quantile"</code> ask for an
interval based directly on the replicates of the quantile. This interval
is not valid for jackknife-type replicates, though it should perform well for
bootstrap-type replicates, BRR, and SDR.
</p>
<p>The <code>df</code> argument specifies degrees of freedom for a t-distribution
approximation to distributions of means. The default is the design degrees of
freedom. Specify <code>df=Inf</code> to use a Normal distribution (eg, for compatibility).
</p>
<p>When the standard error is requested, it is estimated by dividing the
confidence interval length by the number of standard errors in a t
confidence interval with the specified <code>alpha</code>. For example, with
<code>alpha=0.05</code> and <code>df=Inf</code> the standard error is estimated as the confidence
interval length divided by <code>2*1.96</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>"newsvyquantile"</code>, except that with a
replicate-weights design and <code>interval.type="quantile"</code> and
<code>return.replicates=TRUE</code> it's an object of class <code>"svrepstat"</code>
</p>


<h3>References</h3>

<p>Dorfman A, Valliant R (1993) Quantile variance estimators in complex
surveys. Proceedings of the ASA Survey Research Methods Section. 1993: 866-871
</p>
<p>Francisco CA, Fuller WA (1986) Estimation of the distribution
function with a complex survey. Technical Report, Iowa State
University.
</p>
<p>Hyndman, R. J. and Fan, Y. (1996) Sample quantiles in statistical packages,
The American Statistician 50, 361-365.
</p>
<p>Shah BV, Vaish AK (2006) Confidence Intervals for Quantile Estimation
from Complex Survey Data. Proceedings of the Section on Survey
Research Methods. 
</p>
<p>Woodruff RS (1952) Confidence intervals for medians and other
position measures. JASA 57, 622-627.
</p>


<h3>See Also</h3>

<p><code>vignette("qrule", package = "survey")</code>
<code><a href="#topic+oldsvyquantile">oldsvyquantile</a></code>	 
<code><a href="stats.html#topic+quantile">quantile</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
## population
quantile(apipop$api00,c(.25,.5,.75))

## one-stage cluster sample
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)
rclus1&lt;-as.svrepdesign(dclus1)
bclus1&lt;-as.svrepdesign(dclus1,type="boot")


svyquantile(~api00, dclus1, c(.25,.5,.75))
svyquantile(~api00, dclus1, c(.25,.5,.75),interval.type="beta")

svyquantile(~api00, rclus1, c(.25,.5,.75))
svyquantile(~api00, rclus1, c(.25,.5,.75),interval.type="quantile")
svyquantile(~api00, bclus1, c(.25,.5,.75),interval.type="quantile")

svyquantile(~api00+ell, dclus1, c(.25,.5,.75), qrule="math")
svyquantile(~api00+ell, dclus1, c(.25,.5,.75), qrule="school")
svyquantile(~api00+ell, dclus1, c(.25,.5,.75), qrule="hf8")

</code></pre>

<hr>
<h2 id='nhanes'>
Cholesterol data from a US survey
</h2><span id='topic+nhanes'></span>

<h3>Description</h3>

<p>Data extracted from NHANES 2009-2010 on high cholesterol.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(nhanes)</code></pre>


<h3>Format</h3>

<p>A data frame with 8591 observations on the following 7 variables.
</p>

<dl>
<dt><code>SDMVPSU</code></dt><dd><p>Primary sampling units</p>
</dd>
<dt><code>SDMVSTRA</code></dt><dd><p>Sampling strata</p>
</dd>
<dt><code>WTMEC2YR</code></dt><dd><p>Sampling weights</p>
</dd>
<dt><code>HI_CHOL</code></dt><dd><p>Numeric vector: 1 for total cholesterol over
240mg/dl, 0 under 240mg/dl</p>
</dd>
<dt><code>race</code></dt><dd><p>1=Hispanic, 2=non-Hispanic white, 3=non-Hispanic
black, 4=other</p>
</dd>
<dt><code>agecat</code></dt><dd><p>Age group <code>(0,19]</code> <code>(19,39]</code> <code>(39,59]</code> <code>(59,Inf]</code></p>
</dd>
<dt><code>RIAGENDR</code></dt><dd><p>Gender: 1=male, 2=female</p>
</dd>
</dl>



<h3>Source</h3>

<p>Previously at  <code style="white-space: pre;">&#8288;https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=laboratory&amp;CycleBeginYear=2009&#8288;</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(nhanes)
design &lt;- svydesign(id=~SDMVPSU, strata=~SDMVSTRA, weights=~WTMEC2YR, nest=TRUE,data=nhanes)
design
</code></pre>

<hr>
<h2 id='nonresponse'>Experimental: Construct non-response weights</h2><span id='topic+nonresponse'></span><span id='topic+sparseCells'></span><span id='topic+neighbours'></span><span id='topic+joinCells'></span><span id='topic+weights.nonresponse'></span><span id='topic+print.nonresponse'></span><span id='topic+print.nonresponseSubset'></span><span id='topic++5B.nonresponse'></span>

<h3>Description</h3>

<p>Functions to simplify the construction of non-reponse weights by
combining strata with small numbers or large weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nonresponse(sample.weights, sample.counts, population)
sparseCells(object, count=0,totalweight=Inf, nrweight=1.5)
neighbours(index,object)
joinCells(object,a,...)
## S3 method for class 'nonresponse'
weights(object,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nonresponse_+3A_sample.weights">sample.weights</code></td>
<td>
<p>table of sampling weight by stratifying variables</p>
</td></tr>
<tr><td><code id="nonresponse_+3A_sample.counts">sample.counts</code></td>
<td>
<p>table of sample counts by stratifying variables</p>
</td></tr>
<tr><td><code id="nonresponse_+3A_population">population</code></td>
<td>
<p>table of population size by stratifying variables</p>
</td></tr>
<tr><td><code id="nonresponse_+3A_object">object</code></td>
<td>
<p>object of class <code>"nonresponse"</code></p>
</td></tr>
<tr><td><code id="nonresponse_+3A_count">count</code></td>
<td>
<p>Cells with fewer sampled units than this are &quot;sparse&quot;</p>
</td></tr>
<tr><td><code id="nonresponse_+3A_nrweight">nrweight</code></td>
<td>
<p>Cells with higher non-response weight than this are &quot;sparse&quot;</p>
</td></tr>
<tr><td><code id="nonresponse_+3A_totalweight">totalweight</code></td>
<td>
<p>Cells with average sampling weight times
non-response weight higher than this are &quot;sparse&quot;</p>
</td></tr>
<tr><td><code id="nonresponse_+3A_index">index</code></td>
<td>
<p>Number of a cell whose neighbours are to be found</p>
</td></tr>
<tr><td><code id="nonresponse_+3A_a">a</code>, <code id="nonresponse_+3A_...">...</code></td>
<td>
<p>Cells to join</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When a stratified survey is conducted with imperfect response it is
desirable to rescale the sampling weights to reflect the nonresponse.
If some strata have small sample size, high non-response, or already
had high sampling weights it may be desirable to get less variable
non-response weights by averaging non-response across strata.
Suitable strata to collapse may be similar on the stratifying
variables and/or on the level of non-response.
</p>
<p><code>nonresponse()</code> combines stratified tables of population size,
sample size, and sample weight into an object. <code>sparseCells</code>
identifies cells that may need combining. <code>neighbours</code> describes the
cells adjacent to a specified cell, and <code>joinCells</code> collapses
the specified cells.  When the collapsing is complete, use
<code>weights()</code> to extract the nonresponse weights.
</p>


<h3>Value</h3>

<p><code>nonresponse</code> and <code>joinCells</code> return objects of class <code>"nonresponse"</code>,
<code>neighbours</code> and <code>sparseCells</code> return objects of class <code>"nonresponseSubset"</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
## pretend the sampling was stratified on three variables
poptable&lt;-xtabs(~sch.wide+comp.imp+stype,data=apipop)
sample.count&lt;-xtabs(~sch.wide+comp.imp+stype,data=apiclus1)
sample.weight&lt;-xtabs(pw~sch.wide+comp.imp+stype, data=apiclus1)

## create a nonresponse object
nr&lt;-nonresponse(sample.weight,sample.count, poptable)

## sparse cells
sparseCells(nr)

## Look at neighbours
neighbours(3,nr)
neighbours(11,nr)

## Collapse some contiguous cells
nr1&lt;-joinCells(nr,3,5,7)

## sparse cells now
sparseCells(nr1)
nr2&lt;-joinCells(nr1,3,11,8)

nr2

## one relatively sparse cell
sparseCells(nr2)
## but nothing suitable to join it to
neighbours(3,nr2)

## extract the weights
weights(nr2)
</code></pre>

<hr>
<h2 id='oldsvyquantile'>Deprecated implementation of quantiles</h2><span id='topic+oldsvyquantile'></span><span id='topic+oldsvyquantile.survey.design'></span><span id='topic+oldsvyquantile.svyrep.design'></span><span id='topic+print.svyquantile'></span>

<h3>Description</h3>

<p>Compute quantiles for data from complex surveys. <code>oldsvyquantile</code>
is the version of the function from before version 4.1 of the package,
available for backwards compatibility. See <code><a href="#topic+svyquantile">svyquantile</a></code>
for the current version
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survey.design'
oldsvyquantile(x, design, quantiles, alpha=0.05,
   ci=FALSE, method = "linear", f = 1,
   interval.type=c("Wald","score","betaWald"), na.rm=FALSE,se=ci,
   ties=c("discrete","rounded"), df=NULL,...)
## S3 method for class 'svyrep.design'
oldsvyquantile(x, design, quantiles,
   method ="linear", interval.type=c("probability","quantile"), f = 1,
   return.replicates=FALSE, ties=c("discrete","rounded"),na.rm=FALSE,
   alpha=0.05,df=NULL,...)
 </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="oldsvyquantile_+3A_x">x</code></td>
<td>
<p>A formula, vector or matrix</p>
</td></tr>
<tr><td><code id="oldsvyquantile_+3A_design">design</code></td>
<td>
<p><code>survey.design</code> or <code>svyrep.design</code> object</p>
</td></tr>
<tr><td><code id="oldsvyquantile_+3A_quantiles">quantiles</code></td>
<td>
<p>Quantiles to estimate</p>
</td></tr>
<tr><td><code id="oldsvyquantile_+3A_method">method</code></td>
<td>
<p>see <code><a href="stats.html#topic+approxfun">approxfun</a></code></p>
</td></tr>
<tr><td><code id="oldsvyquantile_+3A_f">f</code></td>
<td>
<p>see <code><a href="stats.html#topic+approxfun">approxfun</a></code></p>
</td></tr>
<tr><td><code id="oldsvyquantile_+3A_ci">ci</code></td>
<td>
<p>Compute a confidence interval? (relatively slow; needed for <code><a href="#topic+svyby">svyby</a></code>)</p>
</td></tr>
<tr><td><code id="oldsvyquantile_+3A_se">se</code></td>
<td>
<p>Compute standard errors from the confidence interval length?</p>
</td></tr>
<tr><td><code id="oldsvyquantile_+3A_alpha">alpha</code></td>
<td>
<p>Level for confidence interval</p>
</td></tr>
<tr><td><code id="oldsvyquantile_+3A_interval.type">interval.type</code></td>
<td>
<p>See Details below</p>
</td></tr>
<tr><td><code id="oldsvyquantile_+3A_ties">ties</code></td>
<td>
<p>See Details below</p>
</td></tr>
<tr><td><code id="oldsvyquantile_+3A_df">df</code></td>
<td>
<p>Degrees of freedom for a t-distribution. <code>Inf</code> requests a Normal distribution,
<code>NULL</code> uses <code><a href="#topic+degf">degf</a></code>. Not relevant for <code>type="betaWald"</code></p>
</td></tr>
<tr><td><code id="oldsvyquantile_+3A_return.replicates">return.replicates</code></td>
<td>
<p>Return the replicate means?</p>
</td></tr>
<tr><td><code id="oldsvyquantile_+3A_na.rm">na.rm</code></td>
<td>
<p>Remove <code>NA</code>s?</p>
</td></tr>
<tr><td><code id="oldsvyquantile_+3A_...">...</code></td>
<td>
<p>arguments for future expansion</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The definition of the CDF and thus of the quantiles is ambiguous in
the presence of ties.  With <code>ties="discrete"</code> the data are
treated as genuinely discrete, so the CDF has vertical steps at tied
observations. With <code>ties="rounded"</code> all the weights for tied
observations are summed and the CDF interpolates linearly between
distinct observed values, and so is a continuous function.  Combining
<code>interval.type="betaWald"</code> and <code>ties="discrete"</code> is (close
to) the proposal of Shah and Vaish(2006) used in some versions of SUDAAN.
</p>
<p>Interval estimation for quantiles is complicated, because the
influence function is not continuous.  Linearisation cannot be used
directly, and computing the variance of replicates is valid only for
some designs (eg BRR, but not jackknife). The <code>interval.type</code>
option controls how the intervals are computed.
</p>
<p>For <code>survey.design</code> objects the default is
<code>interval.type="Wald"</code>. A 95% Wald confidence interval is
constructed for the proportion below the estimated quantile. The
inverse of the estimated CDF is used to map this to a confidence
interval for the quantile. This is the method of Woodruff
(1952). For <code>"betaWald"</code> the same procedure is used, but the
confidence interval for the proportion is computed using the exact
binomial cdf with an effective sample size proposed by Korn &amp;
Graubard (1998).
</p>
<p>If <code>interval.type="score"</code> we use a method described by Binder
(1991) and due originally to Francisco and Fuller (1986), which
corresponds to inverting a robust score test.  At the upper and lower
limits of the confidence interval, a test of the null hypothesis that
the cumulative distribution function is equal to the target quantile
just rejects.  This was the default before version 2.9. It is much
slower than <code>"Wald"</code>, and Dorfman &amp; Valliant (1993) suggest it is
not any more accurate.
</p>
<p>Standard errors are computed from these confidence intervals by
dividing the confidence interval length by <code>2*qnorm(alpha/2)</code>.
</p>
<p>For replicate-weight designs, ordinary replication-based standard errors
are valid for BRR and Fay's method, and for some bootstrap-based
designs, but not for jackknife-based designs.
<code>interval.type="quantile"</code> gives these replication-based
standard errors.  The default, <code>interval.type="probability"</code>
computes confidence on the probability scale and then transforms
back to quantiles, the equivalent of <code>interval.type="Wald"</code> for
<code>survey.design</code> objects (with <code>alpha=0.05</code>).
</p>
<p>There is a <code>confint</code> method for <code>svyquantile</code> objects; it
simply extracts the pre-computed confidence interval.
</p>


<h3>Value</h3>

<p>returns a list whose first component is the quantiles and second
component is the confidence intervals. For replicate weight designs,
returns an object of class <code>svyrepstat</code>.
</p>


<h3>Author(s)</h3>

<p>Thomas Lumley</p>


<h3>References</h3>

<p>Binder DA (1991) Use of estimating functions for interval estimation
from complex surveys. <em>Proceedings of the ASA Survey Research
Methods Section</em>  1991: 34-42
</p>
<p>Dorfman A, Valliant R (1993) Quantile variance estimators in complex
surveys. Proceedings of the ASA Survey Research Methods Section. 1993: 866-871
</p>
<p>Korn EL, Graubard BI. (1998) Confidence Intervals For Proportions With
Small Expected Number of Positive Counts Estimated From Survey
Data. Survey Methodology 23:193-201.
</p>
<p>Francisco CA, Fuller WA (1986) Estimation of the distribution
function with a complex survey. Technical Report, Iowa State
University.
</p>
<p>Shao J, Tu D (1995) <em>The Jackknife and Bootstrap</em>. Springer.
</p>
<p>Shah BV, Vaish AK (2006) Confidence Intervals for Quantile Estimation
from Complex Survey Data. Proceedings of the Section on Survey
Research Methods. 
</p>
<p>Woodruff RS (1952) Confidence intervals for medians and other
position measures. JASA 57, 622-627.  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+svykm">svykm</a></code> for quantiles of survival curves
</p>
<p><code><a href="#topic+svyciprop">svyciprop</a></code> for confidence intervals on proportions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data(api)
  ## population
  quantile(apipop$api00,c(.25,.5,.75))

  ## one-stage cluster sample
  dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)
  oldsvyquantile(~api00, dclus1, c(.25,.5,.75),ci=TRUE)
  oldsvyquantile(~api00, dclus1, c(.25,.5,.75),ci=TRUE,interval.type="betaWald")
  oldsvyquantile(~api00, dclus1, c(.25,.5,.75),ci=TRUE,df=NULL)

  dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)
  (qapi&lt;-oldsvyquantile(~api00, dclus1, c(.25,.5,.75),ci=TRUE, interval.type="score"))
  SE(qapi)

  #stratified sample
  dstrat&lt;-svydesign(id=~1, strata=~stype, weights=~pw, data=apistrat, fpc=~fpc)
  oldsvyquantile(~api00, dstrat, c(.25,.5,.75),ci=TRUE)

  #stratified sample, replicate weights
  # interval="probability" is necessary for jackknife weights
  rstrat&lt;-as.svrepdesign(dstrat)
  oldsvyquantile(~api00, rstrat, c(.25,.5,.75), interval.type="probability")


  # BRR method
  data(scd)
  repweights&lt;-2*cbind(c(1,0,1,0,1,0), c(1,0,0,1,0,1), c(0,1,1,0,0,1),
              c(0,1,0,1,1,0))
  scdrep&lt;-svrepdesign(data=scd, type="BRR", repweights=repweights)
  oldsvyquantile(~arrests+alive, design=scdrep, quantile=0.5, interval.type="quantile")
  oldsvyquantile(~arrests+alive, design=scdrep, quantile=0.5, interval.type="quantile",df=NULL)

 </code></pre>

<hr>
<h2 id='open.DBIsvydesign'>Open and close DBI connections </h2><span id='topic+open.DBIsvydesign'></span><span id='topic+close.DBIsvydesign'></span>

<h3>Description</h3>

<p>A database-backed survey design object contains a connection to a
database.  This connection will be broken if the object is saved and
reloaded, and the connection should ideally be closed with <code>close</code>
before quitting R (although it doesn't matter for SQLite
connections). The connection can be reopened with <code>open</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'DBIsvydesign'
open(con, ...)
## S3 method for class 'DBIsvydesign'
close(con, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="open.DBIsvydesign_+3A_con">con</code></td>
<td>
<p>Object of class <code>DBIsvydesign</code> </p>
</td></tr>
<tr><td><code id="open.DBIsvydesign_+3A_...">...</code></td>
<td>
<p>Other options, to be passed to <code>dbConnect</code> or
<code>dbDisconnect</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The same survey design object with the connection opened or closed.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+svydesign">svydesign</a></code>
</p>
<p>DBI package </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(RSQLite)
dbclus1&lt;-svydesign(id=~dnum, weights=~pw, fpc=~fpc,
data="apiclus1",dbtype="SQLite",
dbname=system.file("api.db",package="survey"))

dbclus1
close(dbclus1)
dbclus1
try(svymean(~api00, dbclus1))

dbclus1&lt;-open(dbclus1)
open(dbclus1)
svymean(~api00, dbclus1)

## End(Not run)
</code></pre>

<hr>
<h2 id='paley'>Paley-type Hadamard matrices</h2><span id='topic+paley'></span><span id='topic+is.hadamard'></span>

<h3>Description</h3>

<p>Computes a Hadamard matrix of dimension <code class="reqn">(p+1)\times 2^k</code>, where p is a prime,
and p+1 is a multiple of 4, using the Paley construction. Used by <code><a href="#topic+hadamard">hadamard</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>paley(n, nmax = 2 * n, prime=NULL, check=!is.null(prime))

is.hadamard(H, style=c("0/1","+-"), full.orthogonal.balance=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="paley_+3A_n">n</code></td>
<td>
<p>Minimum size for matrix</p>
</td></tr>
<tr><td><code id="paley_+3A_nmax">nmax</code></td>
<td>
<p>Maximum size for matrix. Ignored if <code>prime</code> is specified.</p>
</td></tr>
<tr><td><code id="paley_+3A_prime">prime</code></td>
<td>
<p>Optional. A prime at least as large as
<code>n</code>,  such that <code>prime+1</code> is divisible by 4.</p>
</td></tr>
<tr><td><code id="paley_+3A_check">check</code></td>
<td>
<p>Check that the resulting matrix is of Hadamard type</p>
</td></tr>
<tr><td><code id="paley_+3A_h">H</code></td>
<td>
<p>Matrix</p>
</td></tr>
<tr><td><code id="paley_+3A_style">style</code></td>
<td>
<p><code>"0/1"</code> for a matrix of 0s and 1s, <code>"+-"</code> for a
matrix of <code class="reqn">\pm 1</code>.</p>
</td></tr>
<tr><td><code id="paley_+3A_full.orthogonal.balance">full.orthogonal.balance</code></td>
<td>
<p>Require full orthogonal balance?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Paley construction gives a Hadamard matrix of order p+1 if p is
prime and p+1 is a multiple of 4.  This is then expanded to order
<code class="reqn">(p+1)\times 2^k</code> using the Sylvester construction.
</p>
<p><code>paley</code> knows primes up to 7919.  The user can specify a prime
with the <code>prime</code> argument, in which case a matrix of order
<code class="reqn">p+1</code> is constructed.
</p>
<p>If <code>check=TRUE</code> the code uses <code>is.hadamard</code> to check that
the resulting matrix really is of Hadamard type, in the same way as in
the example below. As this test takes <code class="reqn">n^3</code> time it is
preferable to just be sure that <code>prime</code> really is prime.
</p>
<p>A Hadamard matrix including a row of 1s gives BRR designs where the
average of the replicates for a linear statistic is exactly the full
sample estimate. This property is called full orthogonal balance.
</p>


<h3>Value</h3>

<p>For <code>paley</code>, a matrix of zeros and ones, or <code>NULL</code> if no matrix smaller than
<code>nmax</code> can be found.
</p>
<p>For <code>is.hadamard</code>, <code>TRUE</code> if <code>H</code> is a Hadamard matrix.
</p>


<h3>References</h3>

<p>Cameron PJ (2005) Hadamard Matrices. In: The
Encyclopedia of Design Theory 
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+hadamard">hadamard</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
M&lt;-paley(11)

is.hadamard(M)
## internals of is.hadamard(M)
H&lt;-2*M-1
## HH^T is diagonal for any Hadamard matrix
H%*%t(H)

</code></pre>

<hr>
<h2 id='pchisqsum'>Distribution of quadratic forms </h2><span id='topic+pchisqsum'></span><span id='topic+pFsum'></span>

<h3>Description</h3>

<p>The distribution of a quadratic form in p standard Normal variables is
a linear combination of p chi-squared distributions with 1df.   When
there is uncertainty about the variance, a reasonable model for the
distribution is a linear combination of F distributions with the same
denominator. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pchisqsum(x, df, a, lower.tail = TRUE, 
   method = c("satterthwaite", "integration","saddlepoint"))
pFsum(x, df, a, ddf=Inf,lower.tail = TRUE, 
   method = c("saddlepoint","integration","satterthwaite"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pchisqsum_+3A_x">x</code></td>
<td>
<p>Observed values</p>
</td></tr>
<tr><td><code id="pchisqsum_+3A_df">df</code></td>
<td>
<p>Vector of degrees of freedom</p>
</td></tr>
<tr><td><code id="pchisqsum_+3A_a">a</code></td>
<td>
<p>Vector of coefficients </p>
</td></tr>
<tr><td><code id="pchisqsum_+3A_ddf">ddf</code></td>
<td>
<p>Denominator degrees of freedom</p>
</td></tr>
<tr><td><code id="pchisqsum_+3A_lower.tail">lower.tail</code></td>
<td>
<p> lower or upper  tail? </p>
</td></tr>
<tr><td><code id="pchisqsum_+3A_method">method</code></td>
<td>
<p>See Details below</p>
</td></tr>
<tr><td><code id="pchisqsum_+3A_...">...</code></td>
<td>
<p>arguments to <code>pchisqsum</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>"satterthwaite"</code> method uses Satterthwaite's
approximation, and this is also used as a fallback for the other
methods. The accuracy is usually good, but is more variable depending
on <code>a</code> than the other methods and is anticonservative in the
right tail (eg for upper tail probabilities less than <code>10^-5</code>).
The Satterthwaite approximation requires all <code>a&gt;0</code>.
</p>
<p><code>"integration"</code> requires the <code>CompQuadForm</code> package. For
<code>pchisqsum</code> it uses Farebrother's algorithm if all
<code>a&gt;0</code>. For <code>pFsum</code> or when some <code>a&lt;0</code> it inverts the
characteristic function using the algorithm of Davies (1980).
These algorithms are highly accurate for the lower tail probability, but they obtain the upper tail probability by subtraction from 1 and so fail completely when the upper tail probability is comparable to machine epsilon or smaller. 
</p>
<p>If  the <code>CompQuadForm</code> package is not present, a warning is given
and the saddlepoint approximation is used. 
</p>
<p><code>"saddlepoint"</code> uses Kuonen's saddlepoint approximation. This
is moderately accurate even very far out in the upper tail or with some
<code>a=0</code> and does not require any additional packages. The relative error
in the right tail is uniformly bounded for all <code>x</code> and decreases as p
increases. This method is implemented in pure R and so is slower than
the <code>"integration"</code>  method. 
</p>
<p>The distribution in <code>pFsum</code> is standardised so that a likelihood
ratio test can use the same <code>x</code> value as in <code>pchisqsum</code>.
That is, the linear combination of chi-squareds is multiplied by
<code>ddf</code> and then divided by an independent chi-squared with
<code>ddf</code> degrees of freedom.
</p>


<h3>Value</h3>

<p>Vector of cumulative probabilities
</p>


<h3>References</h3>

<p>Chen, T., &amp; Lumley T. (2019). Numerical evaluation of methods approximating
the distribution of a large quadratic form in normal variables.
Computational Statistics and Data Analysis, 139, 75-81.
</p>
<p>Davies RB (1973). &quot;Numerical inversion of a characteristic function&quot;
Biometrika 60:415-7
</p>
<p>Davies RB (1980) &quot;Algorithm AS 155: The Distribution of a Linear Combination of chi-squared Random Variables&quot;
Applied Statistics,Vol. 29, No. 3 (1980), pp. 323-333
</p>
<p>P. Duchesne, P. Lafaye de Micheaux (2010) &quot;Computing the distribution
of quadratic forms: Further comparisons between the Liu-Tang-Zhang
approximation and exact methods&quot;, Computational Statistics and Data
Analysis, Volume 54, (2010), 858-862
</p>
<p>Farebrother R.W. (1984)  &quot;Algorithm AS 204: The distribution of a
Positive Linear Combination of chi-squared random variables&quot;. Applied
Statistics Vol. 33, No. 3 (1984), p. 332-339
</p>
<p>Kuonen D (1999) Saddlepoint Approximations for Distributions of
Quadratic Forms in Normal Variables. Biometrika, Vol. 86, No. 4
(Dec., 1999), pp. 929-935 
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+pchisq">pchisq</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 2.7*rnorm(1001)^2+rnorm(1001)^2+0.3*rnorm(1001)^2
x.thin&lt;-sort(x)[1+(0:50)*20]
p.invert&lt;-pchisqsum(x.thin,df=c(1,1,1),a=c(2.7,1,.3),method="int" ,lower=FALSE)
p.satt&lt;-pchisqsum(x.thin,df=c(1,1,1),a=c(2.7,1,.3),method="satt",lower=FALSE)
p.sadd&lt;-pchisqsum(x.thin,df=c(1,1,1),a=c(2.7,1,.3),method="sad",lower=FALSE)

plot(p.invert, p.satt,type="l",log="xy")
abline(0,1,lty=2,col="purple")
plot(p.invert, p.sadd,type="l",log="xy")
abline(0,1,lty=2,col="purple")

pchisqsum(20, df=c(1,1,1),a=c(2.7,1,.3), lower.tail=FALSE,method="sad")
pFsum(20, df=c(1,1,1),a=c(2.7,1,.3), ddf=49,lower.tail=FALSE,method="sad")
pFsum(20, df=c(1,1,1),a=c(2.7,1,.3), ddf=1000,lower.tail=FALSE,method="sad")

</code></pre>

<hr>
<h2 id='poisson_sampling'>
Specify Poisson sampling design
</h2><span id='topic+poisson_sampling'></span>

<h3>Description</h3>

<p>Specify a design where units are sampled independently from the population, with known probabilities. This design is often used theoretically, but is rarely used in practice because the sample size is variable. This function calls <code><a href="#topic+ppscov">ppscov</a></code> to specify a sparse sampling covariance matrix. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>poisson_sampling(p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="poisson_sampling_+3A_p">p</code></td>
<td>

<p>Vector of sampling probabilities
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>ppsdcheck</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ppscov">ppscov</a></code>, <code><a href="#topic+svydesign">svydesign</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
apipop$prob&lt;-with(apipop, 200*api00/sum(api00))
insample&lt;-as.logical(rbinom(nrow(apipop),1,apipop$prob))
apipois&lt;-apipop[insample,]
despois&lt;-svydesign(id=~1, prob=~prob, pps=poisson_sampling(apipois$prob), data=apipois)

svytotal(~api00, despois)

## SE formula
sqrt(sum( (apipois$api00*weights(despois))^2*(1-apipois$prob)))
</code></pre>

<hr>
<h2 id='postStratify'>Post-stratify a survey </h2><span id='topic+postStratify'></span><span id='topic+postStratify.twophase'></span><span id='topic+postStratify.svyrep.design'></span><span id='topic+postStratify.survey.design'></span>

<h3>Description</h3>

<p>Post-stratification adjusts the sampling and replicate weights so that
the joint distribution of a set of post-stratifying variables matches
the known population joint distribution. Use <code><a href="#topic+rake">rake</a></code> when
the full joint distribution is not available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>postStratify(design, strata, population, partial = FALSE, ...)
## S3 method for class 'svyrep.design'
postStratify(design, strata, population, partial = FALSE, compress=NULL,...)
## S3 method for class 'survey.design'
postStratify(design, strata, population, partial = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="postStratify_+3A_design">design</code></td>
<td>
<p>A survey design with replicate weights</p>
</td></tr>
<tr><td><code id="postStratify_+3A_strata">strata</code></td>
<td>
<p>A formula or data frame of post-stratifying variables, which must not contain missing values. </p>
</td></tr>
<tr><td><code id="postStratify_+3A_population">population</code></td>
<td>
<p>A <code><a href="base.html#topic+table">table</a></code>, <code><a href="stats.html#topic+xtabs">xtabs</a></code> or <code>data.frame</code>
with population frequencies </p>
</td></tr>
<tr><td><code id="postStratify_+3A_partial">partial</code></td>
<td>
<p>if <code>TRUE</code>, ignore population strata not present in
the sample</p>
</td></tr>
<tr><td><code id="postStratify_+3A_compress">compress</code></td>
<td>
<p>Attempt to compress the replicate weight matrix? When
<code>NULL</code> will attempt to compress if the original weight matrix
was compressed</p>
</td></tr>
<tr><td><code id="postStratify_+3A_...">...</code></td>
<td>
<p>arguments for future expansion</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>population</code> totals can be specified as a table with the
strata variables in the margins, or as a data frame where one column
lists frequencies and the other columns list the unique combinations
of strata variables (the format produced by <code>as.data.frame</code>
acting on a <code>table</code> object).   A table must have named dimnames
to indicate the variable names.
</p>
<p>Compressing the replicate weights will take time and may even
increase memory use if there is actually little redundancy in the
weight matrix (in particular if the post-stratification variables have
many values and cut  across PSUs).
</p>
<p>If a <code>svydesign</code> object is to be converted to a replication
design the post-stratification should be performed after conversion.
</p>
<p>The variance estimate for replication designs follows the same
procedure as Valliant (1993) described for estimating totals. Rao et
al (2002) describe this procedure for estimating functions (and also
the GREG or g-calibration procedure, see <code><a href="#topic+calibrate">calibrate</a></code>)
</p>


<h3>Value</h3>

<p>A new survey design object.
</p>


<h3>Note</h3>

<p>If the sampling weights are already post-stratified there will be no
change in point estimates after <code>postStratify</code> but the standard
error estimates will decrease to correctly reflect the post-stratification.
</p>


<h3>References</h3>

<p>Valliant R (1993) Post-stratification and conditional variance
estimation. JASA 88: 89-96  
</p>
<p>Rao JNK, Yung W, Hidiroglou MA (2002)   Estimating equations for the
analysis of survey data using poststratification information. Sankhya
64 Series A Part 2, 364-378.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rake">rake</a></code>, <code><a href="#topic+calibrate">calibrate</a></code> for other things to do
with auxiliary information
</p>
<p><code><a href="#topic+compressWeights">compressWeights</a></code> for information on compressing weights</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)
rclus1&lt;-as.svrepdesign(dclus1)

svymean(~api00, rclus1)
svytotal(~enroll, rclus1)

# post-stratify on school type
pop.types &lt;- data.frame(stype=c("E","H","M"), Freq=c(4421,755,1018))
#or: pop.types &lt;- xtabs(~stype, data=apipop)
#or: pop.types &lt;- table(stype=apipop$stype)

rclus1p&lt;-postStratify(rclus1, ~stype, pop.types)
summary(rclus1p)
svymean(~api00, rclus1p)
svytotal(~enroll, rclus1p)


## and for svydesign objects
dclus1p&lt;-postStratify(dclus1, ~stype, pop.types)
summary(dclus1p)
svymean(~api00, dclus1p)
svytotal(~enroll, dclus1p)
</code></pre>

<hr>
<h2 id='psrsq'>
Pseudo-Rsquareds
</h2><span id='topic+psrsq'></span>

<h3>Description</h3>

<p>Compute the Nagelkerke and Cox&ndash;Snell pseudo-rsquared statistics, primarily for logistic regression. A generic function with methods for <code>glm</code> and <code><a href="#topic+svyglm">svyglm</a></code>.  The method for <code>svyglm</code> objects uses the design-based estimators described by Lumley (2017)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>psrsq(object, method = c("Cox-Snell", "Nagelkerke"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="psrsq_+3A_object">object</code></td>
<td>

<p>A regression model (<code>glm</code> or <code>svyglm</code>)
</p>
</td></tr>
<tr><td><code id="psrsq_+3A_method">method</code></td>
<td>

<p>Which statistic to compute
</p>
</td></tr>
<tr><td><code id="psrsq_+3A_...">...</code></td>
<td>

<p>For future expansion
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric value
</p>


<h3>References</h3>

<p>Lumley T (2017) &quot;Pseudo-R2 statistics under complex sampling&quot; Australian and New Zealand Journal of Statistics DOI: 10.1111/anzs.12187 (preprint: <a href="https://arxiv.org/abs/1701.07745">https://arxiv.org/abs/1701.07745</a>)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+AIC.svyglm">AIC.svyglm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus2&lt;-svydesign(id=~dnum+snum, weights=~pw, data=apiclus2)

model1&lt;-svyglm(I(sch.wide=="Yes")~ell+meals+mobility+as.numeric(stype), 
     design=dclus2, family=quasibinomial())

psrsq(model1, type="Nagelkerke")

</code></pre>

<hr>
<h2 id='rake'>Raking of replicate weight design</h2><span id='topic+rake'></span>

<h3>Description</h3>

<p>Raking uses iterative post-stratification to match marginal
distributions of a survey sample to known population margins. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rake(design, sample.margins, population.margins, control = list(maxit =
10, epsilon = 1, verbose=FALSE), compress=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rake_+3A_design">design</code></td>
<td>
<p>A survey object </p>
</td></tr>
<tr><td><code id="rake_+3A_sample.margins">sample.margins</code></td>
<td>
<p>list of formulas or data frames describing
sample margins, which must not contain missing values</p>
</td></tr>
<tr><td><code id="rake_+3A_population.margins">population.margins</code></td>
<td>
<p>list of tables or data frames
describing corresponding population margins </p>
</td></tr>
<tr><td><code id="rake_+3A_control">control</code></td>
<td>
<p><code>maxit</code> controls the number of
iterations. Convergence is declared if the maximum change in a table
entry is less than  <code>epsilon</code>. If <code>epsilon&lt;1</code> it is
taken to be a fraction of the total sampling weight. </p>
</td></tr>
<tr><td><code id="rake_+3A_compress">compress</code></td>
<td>
<p>If <code>design</code> has replicate weights, attempt to
compress the new replicate weight matrix? When <code>NULL</code>, will
attempt to compress if the original weight matrix was compressed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>sample.margins</code> should be in a format suitable for <code><a href="#topic+postStratify">postStratify</a></code>.
</p>
<p>Raking (aka iterative proportional fitting) is known to converge for
any table without zeros, and for any table with zeros for which there
is a joint distribution with the given margins and the same pattern of
zeros.  The &lsquo;margins&rsquo; need not be one-dimensional.
</p>
<p>The algorithm works by repeated calls to <code><a href="#topic+postStratify">postStratify</a></code>
(iterative proportional fitting), which is efficient for large
multiway tables. For small tables <code><a href="#topic+calibrate">calibrate</a></code> will be
faster, and also allows raking to population totals for continuous
variables, and raking with bounded weights. 
</p>


<h3>Value</h3>

<p>A raked survey design. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+postStratify">postStratify</a></code>, <code><a href="#topic+compressWeights">compressWeights</a></code>
</p>
<p><code><a href="#topic+calibrate">calibrate</a></code> for other ways to use auxiliary information.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus1 &lt;- svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)
rclus1 &lt;- as.svrepdesign(dclus1)

svymean(~api00, rclus1)
svytotal(~enroll, rclus1)

## population marginal totals for each stratum
pop.types &lt;- data.frame(stype=c("E","H","M"), Freq=c(4421,755,1018))
pop.schwide &lt;- data.frame(sch.wide=c("No","Yes"), Freq=c(1072,5122))

rclus1r &lt;- rake(rclus1, list(~stype,~sch.wide), list(pop.types, pop.schwide))

svymean(~api00, rclus1r)
svytotal(~enroll, rclus1r)

## marginal totals correspond to population
xtabs(~stype, apipop)
svytable(~stype, rclus1r, round=TRUE)
xtabs(~sch.wide, apipop)
svytable(~sch.wide, rclus1r, round=TRUE)

## joint totals don't correspond 
xtabs(~stype+sch.wide, apipop)
svytable(~stype+sch.wide, rclus1r, round=TRUE)

## Do it for a design without replicate weights
dclus1r&lt;-rake(dclus1, list(~stype,~sch.wide), list(pop.types, pop.schwide))

svymean(~api00, dclus1r)
svytotal(~enroll, dclus1r)

## compare to raking with calibrate()
dclus1gr&lt;-calibrate(dclus1, ~stype+sch.wide, pop=c(6194, 755,1018,5122),
           calfun="raking")
svymean(~stype+api00, dclus1r)
svymean(~stype+api00, dclus1gr)

## compare to joint post-stratification
## (only possible if joint population table is known)
##
pop.table &lt;- xtabs(~stype+sch.wide,apipop)
rclus1ps &lt;- postStratify(rclus1, ~stype+sch.wide, pop.table)
svytable(~stype+sch.wide, rclus1ps, round=TRUE)

svymean(~api00, rclus1ps)
svytotal(~enroll, rclus1ps)

## Example of raking with partial joint distributions
pop.imp&lt;-data.frame(comp.imp=c("No","Yes"),Freq=c(1712,4482))
dclus1r2&lt;-rake(dclus1, list(~stype+sch.wide, ~comp.imp),
               list(pop.table, pop.imp))
svymean(~api00, dclus1r2)

## compare to calibrate() syntax with tables
dclus1r2&lt;-calibrate(dclus1, formula=list(~stype+sch.wide, ~comp.imp),
               population=list(pop.table, pop.imp),calfun="raking")
svymean(~api00, dclus1r2)


</code></pre>

<hr>
<h2 id='regTermTest'>Wald test for a term in a regression model</h2><span id='topic+regTermTest'></span><span id='topic+print.regTermTest'></span>

<h3>Description</h3>

<p>Provides Wald test and working Wald and working likelihood ratio (Rao-Scott) test of the
hypothesis that all coefficients associated with a particular
regression term are zero (or have some other specified
values). Particularly useful as a substitute for <code><a href="stats.html#topic+anova">anova</a></code>
when not fitting by maximum likelihood.  </p>


<h3>Usage</h3>

<pre><code class='language-R'>regTermTest(model, test.terms, null=NULL,df=NULL,
method=c("Wald","WorkingWald","LRT"), lrt.approximation="saddlepoint")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regTermTest_+3A_model">model</code></td>
<td>
<p>A model object with <code><a href="stats.html#topic+coef">coef</a></code> and <code><a href="stats.html#topic+vcov">vcov</a></code> methods</p>
</td></tr>
<tr><td><code id="regTermTest_+3A_test.terms">test.terms</code></td>
<td>
<p>Character string or one-sided formula giving name of
term or terms to test</p>
</td></tr>
<tr><td><code id="regTermTest_+3A_null">null</code></td>
<td>
<p>Null hypothesis values for parameters. Default is zeros</p>
</td></tr>
<tr><td><code id="regTermTest_+3A_df">df</code></td>
<td>
<p>Denominator degrees of freedom for an F test. If
<code>NULL</code> these are estimated from the model. Use <code>Inf</code> for a
chi-squared test.</p>
</td></tr>
<tr><td><code id="regTermTest_+3A_method">method</code></td>
<td>
<p>If <code>"Wald"</code>, the Wald-type test; if <code>"LRT"</code>
the Rao-Scott test based on the estimated log likelihood ratio; If
<code>"WorkingWald"</code> the Wald-type test using the variance matrix
under simple random sampling</p>
</td></tr>
<tr><td><code id="regTermTest_+3A_lrt.approximation">lrt.approximation</code></td>
<td>
<p>method for approximating the distribution of
the LRT and Working Wald statistic; see <code><a href="#topic+pchisqsum">pchisqsum</a></code>.  </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Wald test uses a chisquared or F distribution.  The two
working-model tests come from the (misspecified) working model where the
observations are independent and the weights are frequency weights. For
categorical data, this is just the model fitted to the estimated
population crosstabulation. The Rao-Scott LRT statistic is the likelihood
ratio statistic in this model.  The working Wald test statistic is the Wald statistic
in this model. The working-model tests do not have a chi-squared
sampling distribution: we use a linear combination of chi-squared or F
distributions as in <code><a href="#topic+pchisqsum">pchisqsum</a></code>. I believe the working Wald
test is what SUDAAN refers to as a
&quot;Satterthwaite adjusted Wald test&quot;.
</p>
<p>To match other software you will typically need to use <code>lrt.approximation="satterthwaite"</code>
</p>


<h3>Value</h3>

<p>An object of class <code>regTermTest</code> or <code>regTermTestLRT</code>.
</p>


<h3>Note</h3>

<p>The <code>"LRT"</code> method will not work if the model had starting values supplied for the regression coefficients. Instead, fit the two models separately and use <code>anova(model1, model2, force=TRUE)</code>
</p>


<h3>References</h3>

<p>Rao, JNK, Scott, AJ (1984) &quot;On Chi-squared Tests For Multiway Contingency Tables with Proportions Estimated From Survey Data&quot; Annals of Statistics 12:46-60.
</p>
<p>Lumley T, Scott A (2012) &quot;Partial likelihood ratio tests for the Cox model under complex sampling&quot; Statistics in Medicine 17 JUL 2012. DOI: 10.1002/sim.5492
</p>
<p>Lumley T, Scott A (2014) &quot;Tests for Regression Models Fitted to Survey Data&quot; Australian and New Zealand Journal of Statistics 56:1-14 DOI: 10.1111/anzs.12065
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+anova">anova</a></code>, <code><a href="stats.html#topic+vcov">vcov</a></code>, <code><a href="stats.html#topic+contrasts">contrasts</a></code>,<code><a href="#topic+pchisqsum">pchisqsum</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'> data(esoph)
 model1 &lt;- glm(cbind(ncases, ncontrols) ~ agegp + tobgp * 
     alcgp, data = esoph, family = binomial())
 anova(model1)

 regTermTest(model1,"tobgp")
 regTermTest(model1,"tobgp:alcgp")
 regTermTest(model1, ~alcgp+tobgp:alcgp)


 data(api)
 dclus2&lt;-svydesign(id=~dnum+snum, weights=~pw, data=apiclus2)
 model2&lt;-svyglm(I(sch.wide=="Yes")~ell+meals+mobility, design=dclus2, family=quasibinomial())
 regTermTest(model2, ~ell)
 regTermTest(model2, ~ell,df=NULL)
 regTermTest(model2, ~ell, method="LRT", df=Inf)
 regTermTest(model2, ~ell+meals, method="LRT", df=NULL)

 regTermTest(model2, ~ell+meals, method="WorkingWald", df=NULL)



</code></pre>

<hr>
<h2 id='salamander'>Salamander mating data set from McCullagh and Nelder (1989)</h2><span id='topic+salamander'></span>

<h3>Description</h3>

<p>This data set presents the outcome of three experiments
conducted at the University of Chicago in 1986 to study interbreeding
between populations of mountain dusky salamanders (McCullagh and
Nelder, 1989, Section 14.5). The analysis here is from Lumley (1998,
section 5.3)</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(salamander)</code></pre>


<h3>Format</h3>

<p>A data frame with the following columns:
</p>

<dl>
<dt>Mate</dt><dd><p>Whether the salamanders mated (1) or did not mate (0).</p>
</dd>
<dt>Cross</dt><dd><p>Cross between female and male type. A factor with four levels: <code>R/R</code>,<code>R/W</code>,<code>W/R</code>, and <code>W/W</code>. The type of the female salamander is listed first and the male is listed second. Rough Butt is represented by R and White Side is represented by W. For example, <code>Cross=W/R</code> indicates a White Side female was crossed with a Rough Butt male.</p>
</dd>
<dt>Male</dt><dd><p>Identification number of the male salamander. A factor.</p>
</dd>
<dt>Female</dt><dd><p>Identification number of the female salamander. A factor.</p>
</dd>
</dl>



<h3>References</h3>

<p>McCullagh P. and Nelder, J. A. (1989)  <em>Generalized
Linear Models</em>.  Chapman and Hall/CRC.
Lumley T (1998) PhD thesis, University of Washington
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(salamander)
salamander$mixed&lt;-with(salamander, Cross=="W/R" | Cross=="R/W")
salamander$RWvsWR&lt;-with(salamander,  ifelse(mixed,
          ((Cross=="R/W")-(Cross=="W/R"))/2,
          0))
xsalamander&lt;-xdesign(id=list(~Male, ~Female), data=salamander,
    overlap="unbiased")

## Adjacency matrix
## Blocks 1 and 2 are actually the same salamanders, but
## it's traditional to pretend they are independent.
image(xsalamander$adjacency)

## R doesn't allow family=binomial(identity)
success &lt;- svyglm(Mate~mixed+RWvsWR, design=xsalamander,
    family=quasi(link="identity", variance="mu(1-mu)"))
summary(success)
</code></pre>

<hr>
<h2 id='scd'>Survival in cardiac arrest</h2><span id='topic+scd'></span>

<h3>Description</h3>

<p>These data are from Section 12.2 of Levy and Lemeshow.  They describe
(a possibly apocryphal) study of survival in out-of-hospital cardiac
arrest. Two out of five ambulance stations were sampled from each of
three emergency service areas.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(scd)</code></pre>


<h3>Format</h3>

<p>This data frame contains the following columns:
</p>

<dl>
<dt>ESA</dt><dd><p>Emergency Service Area (strata)</p>
</dd>
<dt>ambulance</dt><dd><p>Ambulance station (PSU)</p>
</dd>
<dt>arrests</dt><dd><p>estimated number of cardiac arrests</p>
</dd>
<dt>alive</dt><dd><p>number reaching hospital alive</p>
</dd>
</dl>



<h3>Source</h3>

<p>Levy and Lemeshow. &quot;Sampling of Populations&quot; (3rd edition). Wiley.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(scd)

## survey design objects
scddes&lt;-svydesign(data=scd, prob=~1, id=~ambulance, strata=~ESA,
nest=TRUE, fpc=rep(5,6))
scdnofpc&lt;-svydesign(data=scd, prob=~1, id=~ambulance, strata=~ESA,
nest=TRUE)

# convert to BRR replicate weights
scd2brr &lt;- as.svrepdesign(scdnofpc, type="BRR")
# or to Rao-Wu bootstrap
scd2boot &lt;- as.svrepdesign(scdnofpc, type="subboot")

# use BRR replicate weights from Levy and Lemeshow
repweights&lt;-2*cbind(c(1,0,1,0,1,0), c(1,0,0,1,0,1), c(0,1,1,0,0,1),
c(0,1,0,1,1,0))
scdrep&lt;-svrepdesign(data=scd, type="BRR", repweights=repweights)

# ratio estimates
svyratio(~alive, ~arrests, design=scddes)
svyratio(~alive, ~arrests, design=scdnofpc)
svyratio(~alive, ~arrests, design=scd2brr)
svyratio(~alive, ~arrests, design=scd2boot)
svyratio(~alive, ~arrests, design=scdrep)

# or a logistic regression
summary(svyglm(cbind(alive,arrests-alive)~1, family=quasibinomial, design=scdnofpc))
summary(svyglm(cbind(alive,arrests-alive)~1, family=quasibinomial, design=scdrep))

# Because no sampling weights are given, can't compute design effects
# without replacement: use deff="replace"

svymean(~alive+arrests, scddes, deff=TRUE)
svymean(~alive+arrests, scddes, deff="replace")

</code></pre>

<hr>
<h2 id='SE'>Extract standard errors </h2><span id='topic+SE'></span><span id='topic+SE.default'></span><span id='topic+SE.svrepstat'></span>

<h3>Description</h3>

<p>Extracts standard errors from an object. The default method is for
objects with a <code><a href="stats.html#topic+vcov">vcov</a></code> method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SE(object, ...)
## Default S3 method:
SE(object,...)
## S3 method for class 'svrepstat'
SE(object,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SE_+3A_object">object</code></td>
<td>
<p>An object</p>
</td></tr>
<tr><td><code id="SE_+3A_...">...</code></td>
<td>
<p>Arguments for future expansion </p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of standard errors.
</p>


<h3>See Also</h3>

 <p><code><a href="stats.html#topic+vcov">vcov</a></code></p>

<hr>
<h2 id='smoothArea'>Small area estimation via basic area level model</h2><span id='topic+svysmoothArea'></span>

<h3>Description</h3>

<p>Generates small area estimates  by smoothing direct estimates using an
area level model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svysmoothArea(
  formula,
  domain,
  design = NULL,
  adj.mat = NULL,
  X.domain = NULL,
  direct.est = NULL,
  domain.size = NULL,
  transform = c("identity", "logit", "log"),
  pc.u = 1,
  pc.alpha = 0.01,
  pc.u.phi = 0.5,
  pc.alpha.phi = 2/3,
  level = 0.95,
  n.sample = 250,
  var.tol = 1e-10,
  return.samples = FALSE,...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smoothArea_+3A_formula">formula</code></td>
<td>
<p>An object of class 'formula' describing the model to be fitted.
If direct.est is specified, the right hand side of the formula is not necessary.</p>
</td></tr>
<tr><td><code id="smoothArea_+3A_domain">domain</code></td>
<td>
<p>One-sided formula specifying factors containing domain labels</p>
</td></tr>
<tr><td><code id="smoothArea_+3A_design">design</code></td>
<td>
<p>An object of class &quot;svydesign&quot; containing the data for the model</p>
</td></tr>
<tr><td><code id="smoothArea_+3A_adj.mat">adj.mat</code></td>
<td>
<p>Adjacency matrix with rownames matching the domain labels. If set to <code>NULL</code>, the IID spatial effect will be used.</p>
</td></tr>
<tr><td><code id="smoothArea_+3A_x.domain">X.domain</code></td>
<td>
<p>Data frame of areal covariates. One of the column names needs to match the name of the domain variable, in order to be linked to the data input. Currently only supporting time-invariant covariates.</p>
</td></tr>
<tr><td><code id="smoothArea_+3A_direct.est">direct.est</code></td>
<td>
<p>Data frame of direct estimates, with first column containing the domain variable, second column containing direct estimate, and third column containing the variance of direct estimate.</p>
</td></tr>
<tr><td><code id="smoothArea_+3A_domain.size">domain.size</code></td>
<td>
<p>Data frame of domain sizes. One of the column names needs to match the name of the <code>domain</code> variable, in order to be linked to the data input and there must be a column names 'size' containing domain sizes.</p>
</td></tr>
<tr><td><code id="smoothArea_+3A_transform">transform</code></td>
<td>
<p>Optional transformation applied to the direct estimates before fitting area level model. The default option is no transformation, but logit and log are implemented.</p>
</td></tr>
<tr><td><code id="smoothArea_+3A_pc.u">pc.u</code></td>
<td>
<p>Hyperparameter U for the PC prior on precisions. See the INLA documentation for more details on the parameterization.</p>
</td></tr>
<tr><td><code id="smoothArea_+3A_pc.alpha">pc.alpha</code></td>
<td>
<p>Hyperparameter alpha for the PC prior on precisions.</p>
</td></tr>
<tr><td><code id="smoothArea_+3A_pc.u.phi">pc.u.phi</code></td>
<td>
<p>Hyperparameter U for the PC prior on the mixture probability phi in BYM2 model.</p>
</td></tr>
<tr><td><code id="smoothArea_+3A_pc.alpha.phi">pc.alpha.phi</code></td>
<td>
<p>Hyperparameter alpha for the PC prior on the mixture probability phi in BYM2 model.</p>
</td></tr>
<tr><td><code id="smoothArea_+3A_level">level</code></td>
<td>
<p>The specified level for the posterior credible intervals</p>
</td></tr>
<tr><td><code id="smoothArea_+3A_n.sample">n.sample</code></td>
<td>
<p>Number of draws from posterior used to compute summaries</p>
</td></tr>
<tr><td><code id="smoothArea_+3A_var.tol">var.tol</code></td>
<td>
<p>Tolerance parameter; if variance of an area's direct estimator is below this value, that direct estimator is dropped from model</p>
</td></tr>
<tr><td><code id="smoothArea_+3A_return.samples">return.samples</code></td>
<td>
<p>If TRUE, return matrix of posterior samples of
area level quantities</p>
</td></tr>
<tr><td><code id="smoothArea_+3A_...">...</code></td>
<td>
<p>for future methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The basic area level model is a Bayesian version of the Fay-Herriot
model (Fay &amp; Herriot,1979). It treats direct estimates of small area quantities as response
data and explicitly models differences between areas using covariate
information and random effects. The Fay-Herriot model can be viewed as a
two-stage model: in the first stage, a sampling model represents the
sampling variability of a direct estimator and in the second stage, a
linking model describes the between area differences in small area
quantities. More detail is given in section 4 of Mercer et al (2015).
</p>


<h3>Value</h3>

<p>A <code>svysae</code> object
</p>


<h3>References</h3>

<p>Fay, Robert E., and Roger A. Herriot. (1979). Estimates of Income for
Small Places: An Application of James-Stein Procedures to Census
Data. Journal of the American Statistical Association 74 (366a):
269-77. 
</p>
<p>Mercer LD, Wakefield J, Pantazis A, Lutambi AM, Masanja H, Clark
S. Space-Time Smoothing of Complex Survey Data: Small Area
Estimation for Child Mortality. Ann Appl Stat. 2015
Dec;9(4):1889-1905. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959836/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959836/</a>
</p>


<h3>See Also</h3>

<p>The <code>survey-sae</code> vignette</p>


<h3>Examples</h3>

<pre><code class='language-R'>## artificial data from SUMMER package
## Uses too many cores for a CRAN example

## Not run: 
 hasSUMMER&lt;-tryCatch({
   data("DemoData2",package="SUMMER")
   data("DemoMap2", package="SUMMER")
  }, error=function(e) FALSE)

if (!isFALSE(hasSUMMER)){
 library(survey)
 des0 &lt;- svydesign(ids = ~clustid+id, strata = ~strata,
                  weights = ~weights, data = DemoData2, nest = TRUE)
 Xmat &lt;- aggregate(age~region, data = DemoData2, FUN = mean)

 cts.cov.res &lt;- svysmoothArea(tobacco.use ~ age, 
                          domain = ~region,
                          design = des0,
                          adj.mat = DemoMap2$Amat, 
                          X.domain = Xmat,
                          pc.u = 1,
                          pc.alpha = 0.01,
                          pc.u.phi = 0.5,
                          pc.alpha.phi = 2/3)
 print(cts.cov.res)
 plot(cts.cov.res)
}

## End(Not run)
</code></pre>

<hr>
<h2 id='smoothUnit'>Smooth via basic unit level model</h2><span id='topic+svysmoothUnit'></span>

<h3>Description</h3>

<p>Generates small area estimates by smoothing direct estimates using a basic
unit level model. This model assumes sampling is ignorable (no selection
bias). It's a Bayesian linear (<code>family="gaussian"</code>) or generalised linear
(<code>family="binomial"</code>) mixed model for the unit-level data with
individual-level covariates and area-level random effects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svysmoothUnit(
  formula,
  domain,
  design,
  family = c("gaussian", "binomial"),
  X.pop = NULL,
  adj.mat = NULL,
  domain.size = NULL,
  pc.u = 1,
  pc.alpha = 0.01,
  pc.u.phi = 0.5,
  pc.alpha.phi = 2/3,
  level = 0.95,
  n.sample = 250,
  return.samples = FALSE,
  X.pop.weights = NULL,...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smoothUnit_+3A_formula">formula</code></td>
<td>
<p>An object of class 'formula' describing the model to be fitted.</p>
</td></tr>
<tr><td><code id="smoothUnit_+3A_domain">domain</code></td>
<td>
<p>One-sided formula specifying factors containing domain labels</p>
</td></tr>
<tr><td><code id="smoothUnit_+3A_design">design</code></td>
<td>
<p>An object of class &quot;survey.design&quot; containing the data for the model</p>
</td></tr>
<tr><td><code id="smoothUnit_+3A_family">family</code></td>
<td>
<p>of the response variable, currently supports 'binomial' (default with logit link function) or 'gaussian'.</p>
</td></tr>
<tr><td><code id="smoothUnit_+3A_x.pop">X.pop</code></td>
<td>
<p>Data frame of population unit-level covariates. One of the column name needs to match the domain specified, in order to be linked to the data input. Currently only supporting time-invariant covariates.</p>
</td></tr>
<tr><td><code id="smoothUnit_+3A_adj.mat">adj.mat</code></td>
<td>
<p>Adjacency matrix with rownames matching the domain labels. If set to NULL, the IID spatial effect will be used.</p>
</td></tr>
<tr><td><code id="smoothUnit_+3A_domain.size">domain.size</code></td>
<td>
<p>Data frame of domain sizes. One of the column names needs to match the name of the domain variable, in order to be linked to the data input and there must be a column names 'size' containing domain sizes. The default option is no transformation, but logit and log are implemented.</p>
</td></tr>
<tr><td><code id="smoothUnit_+3A_pc.u">pc.u</code></td>
<td>
<p>Hyperparameter U for the PC prior on precisions. See the INLA documentation for more details on the parameterization.</p>
</td></tr>
<tr><td><code id="smoothUnit_+3A_pc.alpha">pc.alpha</code></td>
<td>
<p>Hyperparameter alpha for the PC prior on precisions.</p>
</td></tr>
<tr><td><code id="smoothUnit_+3A_pc.u.phi">pc.u.phi</code></td>
<td>
<p>Hyperparameter U for the PC prior on the mixture probability phi in BYM2 model.</p>
</td></tr>
<tr><td><code id="smoothUnit_+3A_pc.alpha.phi">pc.alpha.phi</code></td>
<td>
<p>Hyperparameter alpha for the PC prior on the mixture probability phi in BYM2 model.</p>
</td></tr>
<tr><td><code id="smoothUnit_+3A_level">level</code></td>
<td>
<p>The specified level for the posterior credible intervals</p>
</td></tr>
<tr><td><code id="smoothUnit_+3A_n.sample">n.sample</code></td>
<td>
<p>Number of draws from posterior used to compute
summaries</p>
</td></tr>
<tr><td><code id="smoothUnit_+3A_return.samples">return.samples</code></td>
<td>
<p>If TRUE, return matrix of posterior samples of area level quantities</p>
</td></tr>
<tr><td><code id="smoothUnit_+3A_x.pop.weights">X.pop.weights</code></td>
<td>
<p>Optional vector of weights to use when aggregating
unit level predictions</p>
</td></tr>
<tr><td><code id="smoothUnit_+3A_...">...</code></td>
<td>
<p>for future expansion</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>svysae</code> object
</p>


<h3>References</h3>

<p>Battese, G. E., Harter, R. M., &amp; Fuller, W. A. (1988). An Error-Components Model for Prediction of County Crop Areas Using Survey and Satellite Data. Journal of the American Statistical Association, 83(401), 28-36.
</p>


<h3>See Also</h3>

<p>The <code>survey-sae</code> vignette</p>

<hr>
<h2 id='stratsample'>
Take a stratified sample
</h2><span id='topic+stratsample'></span>

<h3>Description</h3>

<p>This function takes a stratified sample without replacement from a data set. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stratsample(strata, counts)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stratsample_+3A_strata">strata</code></td>
<td>

<p>Vector of stratum identifiers; will be coerced to character
</p>
</td></tr>
<tr><td><code id="stratsample_+3A_counts">counts</code></td>
<td>

<p>named vector of stratum sample sizes, with names corresponding to the values of <code>as.character(strata)</code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of indices into <code>strata</code> giving the sample
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+sample">sample</a></code>
</p>
<p>The &quot;sampling&quot; package has many more sampling algorithms.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(api)
  s&lt;-stratsample(apipop$stype, c("E"=5,"H"=4,"M"=2))
  table(apipop$stype[s])
</code></pre>

<hr>
<h2 id='subset.survey.design'>Subset of survey</h2><span id='topic+subset.survey.design'></span><span id='topic+subset.svyrep.design'></span><span id='topic++5B.survey.design'></span>

<h3>Description</h3>

<p>Restrict a survey design to a subpopulation, keeping the original design
information about number of clusters, strata.   If the design has no
post-stratification or calibration data the subset will use
proportionately less memory.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survey.design'
subset(x, subset, ...)
## S3 method for class 'svyrep.design'
subset(x, subset, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="subset.survey.design_+3A_x">x</code></td>
<td>
<p>A survey design object</p>
</td></tr>
<tr><td><code id="subset.survey.design_+3A_subset">subset</code></td>
<td>
<p>An expression specifying the subpopulation</p>
</td></tr>
<tr><td><code id="subset.survey.design_+3A_...">...</code></td>
<td>
<p>Arguments not used by this method</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new survey design object
</p>


<h3>See Also</h3>

<p><code><a href="#topic+svydesign">svydesign</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(fpc)
dfpc&lt;-svydesign(id=~psuid,strat=~stratid,weight=~weight,data=fpc,nest=TRUE)
dsub&lt;-subset(dfpc,x&gt;4)
summary(dsub)
svymean(~x,design=dsub)

## These should give the same domain estimates and standard errors
svyby(~x,~I(x&gt;4),design=dfpc, svymean)
summary(svyglm(x~I(x&gt;4)+0,design=dfpc))

data(api)
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)
rclus1&lt;-as.svrepdesign(dclus1)
svymean(~enroll, subset(dclus1, sch.wide=="Yes" &amp; comp.imp=="Yes"))
svymean(~enroll, subset(rclus1, sch.wide=="Yes" &amp; comp.imp=="Yes"))

</code></pre>

<hr>
<h2 id='surveyoptions'>Options for the survey package</h2><span id='topic+surveyoptions'></span><span id='topic+survey.lonely.psu'></span><span id='topic+survey.ultimate.cluster'></span><span id='topic+survey.adjust.domain.lonely'></span><span id='topic+survey.want.obsolete'></span><span id='topic+survey.drop.replicates'></span><span id='topic+survey.multicore'></span><span id='topic+survey.replicates.mse'></span><span id='topic+survey.use_rcpp'></span>

<h3>Description</h3>

<p>This help page documents the options that control the behaviour of
the survey package.
</p>


<h3>Details</h3>

<p>All the options for the survey package have names beginning with
&quot;survey&quot;.  Four of them control standard error estimation.
</p>
<p><code>options("survey.replicates.mse")</code> controls the default in
<code>svrepdesign</code> and <code>as.svrepdesign</code> for computing
variances. When <code>options("survey.replicates.mse")</code> is
<code>TRUE</code>, the default is to create replicate weight designs that
compute variances centered at the point estimate, rather than at the
mean of the replicates.  The option can be overridden by specifying
the <code>mse</code> argument explicitly in <code>svrepdesign</code> and
<code>as.svrepdesign</code>. The default is <code>FALSE</code>.
</p>
<p>When <code>options("survey.ultimate.cluster")</code> is <code>TRUE</code>,
standard error estimation is based on independence of PSUs at the
first stage of sampling, without using any information about
subsequent stages. When <code>FALSE</code>, finite population corrections
and variances are estimated recursively. See <code><a href="#topic+svyrecvar">svyrecvar</a></code>
for more information. This option makes no difference unless
first-stage finite population corrections are specified, in which
case setting the option to <code>TRUE</code> gives the wrong answer for a
multistage study. The only reason to use <code>TRUE</code> is for
compatibility with other software that gives the wrong answer.
</p>
<p>Handling of strata with a single PSU that are not certainty PSUs is
controlled by <code>options("survey.lonely.psu")</code>. The default
setting is <code>"fail"</code>, which gives an error. Use <code>"remove"</code>
to ignore that PSU for variance computation, <code>"adjust"</code> to
center the stratum at the population mean rather than the stratum
mean, and <code>"average"</code> to replace the variance contribution of
the stratum by the average variance contribution across strata. As
of version 3.4-2 <code>as.svrepdesign</code> also uses this option.
</p>
<p>The variance formulas for domain estimation give well-defined,
positive results when a stratum contains only one PSU with
observations in the domain, but are not unbiased.  If
<code>options("survey.adjust.domain.lonely")</code> is <code>TRUE</code> and
<code>options("survey.lonely.psu")</code> is <code>"average"</code> or
<code>"adjust"</code> the same adjustment for lonely PSUs will be used
within a domain. Note that this adjustment is not available for
replicate-weight designs, nor (currently) for raked,
post-stratified, or calibrated designs.
</p>
<p>The fourth option is <code>options("survey.want.obsolete")</code>. This
controls the warnings about using the deprecated pre-2.9.0 survey
design objects.
</p>
<p>The behaviour of replicate-weight designs for self-representing
strata is controlled by <code>options("survey.drop.replicates")</code>.
When <code>TRUE</code>, various optimizations are used that take advantage
of the fact that these strata do not contribute to the variance.
The only reason ever to use <code>FALSE</code> is if there is a bug in
the code for these optimizations.
</p>
<p>The fifth option controls the use of multiple processors with the
<code>multicore</code> package. This option should not affect the values
computed by any of the survey functions.  If <code>TRUE</code>, all
functions that are able to use multiple processors will do so by
default. Using multiple processors may speed up calculations, but
need not, especially if the computer is short on memory. The best
strategy is probably to experiment with explicitly requesting
<code>multicore=TRUE</code> in functions that support it, to see if there
is an increase in speed before setting the global option.
</p>
<p><code>survey.use_rcpp</code> controls whether the new C++ code for
standard errors is used (vs the old R code). The factory setting is
<code>TRUE</code> and the only reason to use <code>FALSE</code> is for comparisons.
</p>

<hr>
<h2 id='surveysummary'>Summary statistics for sample surveys</h2><span id='topic+svymean'></span><span id='topic+svymean.survey.design'></span><span id='topic+svymean.survey.design2'></span><span id='topic+svymean.svyrep.design'></span><span id='topic+svymean.twophase'></span><span id='topic+svytotal'></span><span id='topic+svytotal.twophase'></span><span id='topic+svytotal.survey.design'></span><span id='topic+svytotal.survey.design2'></span><span id='topic+svytotal.svyrep.design'></span><span id='topic+svyvar'></span><span id='topic+svyvar.survey.design'></span><span id='topic+svyvar.svyrep.design'></span><span id='topic+coef.svystat'></span><span id='topic+vcov.svystat'></span><span id='topic+coef.svrepstat'></span><span id='topic+vcov.svrepstat'></span><span id='topic+cv.svyratio'></span><span id='topic+cv.svrepratio'></span><span id='topic+cv.svrepstat'></span><span id='topic+cv.svystat'></span><span id='topic+cv.default'></span><span id='topic+cv'></span><span id='topic+deff'></span><span id='topic+deff.default'></span><span id='topic+confint.svystat'></span><span id='topic+confint.svrepstat'></span><span id='topic+make.formula'></span>

<h3>Description</h3>

<p>Compute means, variances, ratios and totals for data from complex surveys.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survey.design'
svymean(x, design, na.rm=FALSE,deff=FALSE,influence=FALSE,...) 
## S3 method for class 'survey.design2'
svymean(x, design, na.rm=FALSE,deff=FALSE,influence=FALSE,...) 
## S3 method for class 'twophase'
svymean(x, design, na.rm=FALSE,deff=FALSE,...) 
## S3 method for class 'svyrep.design'
svymean(x, design, na.rm=FALSE, rho=NULL,
  return.replicates=FALSE, deff=FALSE,...) 
## S3 method for class 'survey.design'
svyvar(x, design, na.rm=FALSE,...) 
## S3 method for class 'svyrep.design'
svyvar(x, design, na.rm=FALSE, rho=NULL,
   return.replicates=FALSE,...,estimate.only=FALSE) 
## S3 method for class 'survey.design'
svytotal(x, design, na.rm=FALSE,deff=FALSE,influence=FALSE,...) 
## S3 method for class 'survey.design2'
svytotal(x, design, na.rm=FALSE,deff=FALSE,influence=FALSE,...) 
## S3 method for class 'twophase'
svytotal(x, design, na.rm=FALSE,deff=FALSE,...) 
## S3 method for class 'svyrep.design'
svytotal(x, design, na.rm=FALSE, rho=NULL,
   return.replicates=FALSE, deff=FALSE,...)
## S3 method for class 'svystat'
coef(object,...)
## S3 method for class 'svrepstat'
coef(object,...)
## S3 method for class 'svystat'
vcov(object,...)
## S3 method for class 'svrepstat'
vcov(object,...)
## S3 method for class 'svystat'
confint(object,  parm, level = 0.95,df =Inf,...)
## S3 method for class 'svrepstat'
confint(object,  parm, level = 0.95,df =Inf,...)
cv(object,...)
deff(object, quietly=FALSE,...)
make.formula(names)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="surveysummary_+3A_x">x</code></td>
<td>
<p>A formula, vector or matrix</p>
</td></tr>
<tr><td><code id="surveysummary_+3A_design">design</code></td>
<td>
<p><code>survey.design</code> or <code>svyrep.design</code> object</p>
</td></tr>
<tr><td><code id="surveysummary_+3A_na.rm">na.rm</code></td>
<td>
<p>Should cases with missing values be dropped?</p>
</td></tr>
<tr><td><code id="surveysummary_+3A_influence">influence</code></td>
<td>
<p>Should a matrix of influence functions be returned
(primarily to support <code><a href="#topic+svyby">svyby</a></code>)</p>
</td></tr>
<tr><td><code id="surveysummary_+3A_rho">rho</code></td>
<td>
<p>parameter for Fay's variance estimator in a BRR design</p>
</td></tr>
<tr><td><code id="surveysummary_+3A_return.replicates">return.replicates</code></td>
<td>
<p>Return the replicate means/totals?</p>
</td></tr>
<tr><td><code id="surveysummary_+3A_deff">deff</code></td>
<td>
<p>Return the design effect (see below)</p>
</td></tr>
<tr><td><code id="surveysummary_+3A_object">object</code></td>
<td>
<p>The result of one of the other survey summary functions</p>
</td></tr>
<tr><td><code id="surveysummary_+3A_quietly">quietly</code></td>
<td>
<p>Don't warn when there is no design effect computed</p>
</td></tr>
<tr><td><code id="surveysummary_+3A_estimate.only">estimate.only</code></td>
<td>
<p>Don't compute standard errors (useful when
<code>svyvar</code> is used to estimate the design effect)</p>
</td></tr>
<tr><td><code id="surveysummary_+3A_parm">parm</code></td>
<td>
<p>a specification of which parameters are to be given
confidence intervals, either a vector of numbers or a vector of
names. If missing, all parameters are considered.</p>
</td></tr>
<tr><td><code id="surveysummary_+3A_level">level</code></td>
<td>
<p>the confidence level required.</p>
</td></tr>
<tr><td><code id="surveysummary_+3A_df">df</code></td>
<td>
<p>degrees of freedom for t-distribution in confidence
interval, use <code>degf(design)</code> for number of PSUs minus number of
strata</p>
</td></tr>
<tr><td><code id="surveysummary_+3A_...">...</code></td>
<td>
<p>additional arguments to  methods,not currently
used</p>
</td></tr>
<tr><td><code id="surveysummary_+3A_names">names</code></td>
<td>
<p>vector of character strings</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions perform weighted estimation, with each observation being
weighted by the inverse of its sampling probability.  Except for the
table functions, these also give precision estimates that incorporate
the effects of stratification and clustering.
</p>
<p>Factor variables are converted to sets of indicator variables for each
category in computing means and totals. Combining this with the
<code><a href="base.html#topic+interaction">interaction</a></code> function, allows crosstabulations. See
<code><a href="#topic+ftable.svystat">ftable.svystat</a></code> for formatting the output.
</p>
<p>With <code>na.rm=TRUE</code>, all cases with missing data are removed. With
<code>na.rm=FALSE</code> cases with missing data are not removed and so will
produce missing results.  When using replicate weights and
<code>na.rm=FALSE</code> it may be useful to set
<code>options(na.action="na.pass")</code>, otherwise all replicates with any
missing results will be discarded.
</p>
<p>The <code>svytotal</code> and <code>svreptotal</code> functions estimate a
population total.  Use <code>predict</code> on <code><a href="#topic+svyratio">svyratio</a></code> and
<code><a href="#topic+svyglm">svyglm</a></code>, to get ratio or regression estimates of totals.
</p>
<p><code>svyvar</code> estimates the population variance. The object returned
includes the full matrix of estimated population variances and
covariances, but by default only the diagonal elements are printed. To
display the whole matrix use <code>as.matrix(v)</code> or <code>print(v,
covariance=TRUE)</code>.
</p>
<p>The design effect compares the variance of a mean or total to the
variance from a study of the same size using simple random sampling
without replacement. Note that the design effect will be incorrect if
the weights have been rescaled so that they are not reciprocals of
sampling probabilities.  To obtain an estimate of the design effect
comparing to simple random sampling with replacement, which does not
have this requirement, use <code>deff="replace"</code>. This with-replacement
design effect is the square of Kish's &quot;deft&quot;.
</p>
<p>The design effect for a subset of a design conditions on the size of
the subset. That is, it compares the variance of the estimate to the
variance of an estimate based on a simple random sample of the same
size as the subset, taken from the subpopulation. So, for example,
under stratified random sampling the design effect in a subset
consisting of a single stratum will be 1.0.
</p>
<p>The <code>cv</code> function computes the coefficient of variation of a
statistic such as ratio, mean or total. The default method is for any
object with methods for <code><a href="#topic+SE">SE</a></code> and <code>coef</code>.
</p>
<p><code>make.formula</code> makes a formula from a vector of names.  This is
useful because formulas as the best way to specify variables to the
survey functions.
</p>


<h3>Value</h3>

<p>Objects of class <code>"svystat"</code> or <code>"svrepstat"</code>,
which are vectors with a <code>"var"</code> attribute giving the variance
and a <code>"statistic"</code> attribute giving the name of the
statistic.
</p>
<p>These objects have methods for <code>vcov</code>, <code>SE</code>, <code>coef</code>,
<code>confint</code>, <code>svycontrast</code>.
</p>


<h3>Author(s)</h3>

<p>Thomas Lumley</p>


<h3>See Also</h3>

 <p><code><a href="#topic+svydesign">svydesign</a></code>, <code><a href="#topic+as.svrepdesign">as.svrepdesign</a></code>,
<code><a href="#topic+svrepdesign">svrepdesign</a></code> for constructing design objects.
</p>
<p><code><a href="#topic+degf">degf</a></code> to extract degrees of freedom from a design.
</p>
<p><code><a href="#topic+svyquantile">svyquantile</a></code>  for quantiles
</p>
<p><code><a href="#topic+ftable.svystat">ftable.svystat</a></code> for more attractive tables
</p>
<p><code><a href="#topic+svyciprop">svyciprop</a></code> for more accurate confidence intervals for
proportions near 0 or 1.
</p>
<p><code><a href="#topic+svyttest">svyttest</a></code> for comparing two means.
</p>
<p><code><a href="#topic+svycontrast">svycontrast</a></code> for linear and nonlinear functions of estimates.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data(api)

  ## one-stage cluster sample
  dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)

  svymean(~api00, dclus1, deff=TRUE)
  svymean(~factor(stype),dclus1)
  svymean(~interaction(stype, comp.imp), dclus1)
  svyquantile(~api00, dclus1, c(.25,.5,.75))
  svytotal(~enroll, dclus1, deff=TRUE)
  svyratio(~api.stu, ~enroll, dclus1)

  v&lt;-svyvar(~api00+api99, dclus1)
  v
  print(v, cov=TRUE)
  as.matrix(v)


  # replicate weights - jackknife (this is slower)
  dstrat&lt;-svydesign(id=~1,strata=~stype, weights=~pw,
        data=apistrat, fpc=~fpc)
  jkstrat&lt;-as.svrepdesign(dstrat)

  svymean(~api00, jkstrat)
  svymean(~factor(stype),jkstrat)
  svyvar(~api00+api99,jkstrat)

  svyquantile(~api00, jkstrat, c(.25,.5,.75))
  svytotal(~enroll, jkstrat)
  svyratio(~api.stu, ~enroll, jkstrat)

  # coefficients of variation
  cv(svytotal(~enroll,dstrat))
  cv(svyratio(~api.stu, ~enroll, jkstrat))

  # extracting information from the results
  coef(svytotal(~enroll,dstrat))
  vcov(svymean(~api00+api99,jkstrat))
  SE(svymean(~enroll, dstrat))
  confint(svymean(~api00+api00, dclus1))
  confint(svymean(~api00+api00, dclus1), df=degf(dclus1))

  # Design effect
  svymean(~api00, dstrat, deff=TRUE)
  svymean(~api00, dstrat, deff="replace")
  svymean(~api00, jkstrat, deff=TRUE)
  svymean(~api00, jkstrat, deff="replace")
 (a&lt;-svytotal(~enroll, dclus1, deff=TRUE))
  deff(a)

## weights that are *already* calibrated to population size
sum(weights(dclus1))
nrow(apipop)
cdclus1&lt;- svydesign(id=~dnum, weights=~pw, data=apiclus1,
fpc=~fpc,calibrate.formula=~1)
SE(svymean(~enroll, dclus1))
## not equal to SE(mean)
SE(svytotal(~enroll, dclus1))/nrow(apipop)
## equal to SE(mean)
SE(svytotal(~enroll, cdclus1))/nrow(apipop)

 </code></pre>

<hr>
<h2 id='svrepdesign'>Specify survey design with replicate weights</h2><span id='topic+svrepdesign'></span><span id='topic+svrepdesign.default'></span><span id='topic+svrepdesign.imputationList'></span><span id='topic+svrepdesign.character'></span><span id='topic++5B.svyrep.design'></span><span id='topic+image.svyrep.design'></span><span id='topic+print.svyrep.design'></span><span id='topic+model.frame.svyrep.design'></span><span id='topic+summary.svyrep.design'></span><span id='topic+print.summary.svyrep.design'></span>

<h3>Description</h3>

<p>Some recent large-scale surveys specify replication weights rather than
the sampling design (partly for privacy reasons).  This function specifies the
data structure for such a survey. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svrepdesign(variables , repweights , weights, data, degf=NULL,...)
## Default S3 method:
svrepdesign(variables = NULL, repweights = NULL, weights = NULL, 
   data = NULL, degf=NULL, type = c("BRR", "Fay", "JK1","JKn","bootstrap",
   "ACS","successive-difference","JK2","other"),
   combined.weights=TRUE, rho = NULL, bootstrap.average=NULL,
   scale=NULL, rscales=NULL,fpc=NULL, fpctype=c("fraction","correction"),
   mse=getOption("survey.replicates.mse"),...)
## S3 method for class 'imputationList'
svrepdesign(variables=NULL,
repweights,weights,data, degf=NULL,
   mse=getOption("survey.replicates.mse"),...)
## S3 method for class 'character'
svrepdesign(variables=NULL,repweights=NULL,
weights=NULL,data=NULL, degf=NULL,
type=c("BRR","Fay","JK1", "JKn","bootstrap","ACS","successive-difference","JK2","other"),
combined.weights=TRUE, rho=NULL, bootstrap.average=NULL, scale=NULL,rscales=NULL,
fpc=NULL,fpctype=c("fraction","correction"),mse=getOption("survey.replicates.mse"),
 dbtype="SQLite", dbname,...) 

## S3 method for class 'svyrep.design'
image(x, ...,
				 col=grey(seq(.5,1,length=30)), type.=c("rep","total"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svrepdesign_+3A_variables">variables</code></td>
<td>
<p>formula or data frame specifying variables to include in the design (default is all) </p>
</td></tr>
<tr><td><code id="svrepdesign_+3A_repweights">repweights</code></td>
<td>
<p>formula or data frame specifying replication weights, or character string specifying a regular expression that matches the names of the replication weight variables </p>
</td></tr>
<tr><td><code id="svrepdesign_+3A_weights">weights</code></td>
<td>
<p>sampling weights </p>
</td></tr>
<tr><td><code id="svrepdesign_+3A_data">data</code></td>
<td>
<p>data frame to look up variables in formulas, or character
string giving name of database table</p>
</td></tr>
<tr><td><code id="svrepdesign_+3A_degf">degf</code></td>
<td>
<p>Design degrees of freedom; use <code>NULL</code> to have the
function work this out for you</p>
</td></tr>
<tr><td><code id="svrepdesign_+3A_type">type</code></td>
<td>
<p>Type of replication weights</p>
</td></tr>
<tr><td><code id="svrepdesign_+3A_combined.weights">combined.weights</code></td>
<td>
<p><code>TRUE</code> if the <code>repweights</code> already
include the sampling weights. This is usually the case.</p>
</td></tr>
<tr><td><code id="svrepdesign_+3A_rho">rho</code></td>
<td>
<p>Shrinkage factor for weights in Fay's method</p>
</td></tr>
<tr><td><code id="svrepdesign_+3A_bootstrap.average">bootstrap.average</code></td>
<td>
<p>For <code>type="bootstrap"</code>, if the bootstrap
weights have been averaged, gives the number of iterations averaged over</p>
</td></tr>
<tr><td><code id="svrepdesign_+3A_scale">scale</code>, <code id="svrepdesign_+3A_rscales">rscales</code></td>
<td>
<p>Scaling constant for variance, see Details
below</p>
</td></tr>
<tr><td><code id="svrepdesign_+3A_fpc">fpc</code>, <code id="svrepdesign_+3A_fpctype">fpctype</code></td>
<td>
<p>Finite population correction information</p>
</td></tr>
<tr><td><code id="svrepdesign_+3A_mse">mse</code></td>
<td>
<p>If <code>TRUE</code>, compute variances based on sum of squares
around the point estimate, rather than the mean of the replicates</p>
</td></tr>
<tr><td><code id="svrepdesign_+3A_dbname">dbname</code></td>
<td>
<p>name of database, passed to <code>DBI::dbConnect()</code></p>
</td></tr>
<tr><td><code id="svrepdesign_+3A_dbtype">dbtype</code></td>
<td>
<p>Database driver: see Details</p>
</td></tr>
<tr><td><code id="svrepdesign_+3A_x">x</code></td>
<td>
<p>survey design with replicate weights</p>
</td></tr>
<tr><td><code id="svrepdesign_+3A_...">...</code></td>
<td>
<p>Other arguments to <code><a href="graphics.html#topic+image">image</a></code></p>
</td></tr>
<tr><td><code id="svrepdesign_+3A_col">col</code></td>
<td>
<p>Colors</p>
</td></tr>
<tr><td><code id="svrepdesign_+3A_type.">type.</code></td>
<td>
<p><code>"rep"</code> for only the replicate weights, <code>"total"</code> for the replicate and sampling weights combined.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the BRR method, the dataset is split into halves, and the
difference between halves is used to estimate the variance. In Fay's
method, rather than removing observations from half the sample they
are given weight <code>rho</code> in one half-sample and <code>2-rho</code> in the
other.  The ideal BRR analysis is restricted to a design where each
stratum has two PSUs, however, it has been used in a much wider class
of surveys. The <code>scale</code> and <code>rscales</code>
arguments will be ignored (with a warning) if they are specified.
</p>
<p>The JK1 and JKn types are both jackknife estimators deleting one
cluster at a time. JKn is designed for stratified and JK1 for
unstratified designs.
</p>
<p>The successive-difference weights in the American Community Survey
automatically use <code>scale = 4/ncol(repweights)</code> and <code>rscales=rep(1,
   ncol(repweights))</code>. This can be specified as <code>type="ACS"</code> or
<code>type="successive-difference"</code>. The <code>scale</code> and <code>rscales</code>
arguments will be ignored (with a warning) if they are specified.
</p>
<p>JK2 weights (<code>type="JK2"</code>), as in the California Health Interview
Survey, automatically use <code>scale=1</code>,  <code>rscales=rep(1, ncol(repweights))</code>.
The <code>scale</code> and <code>rscales</code>
arguments will be ignored (with a warning) if they are specified.
</p>
<p>Averaged bootstrap weights (&quot;mean bootstrap&quot;) are used for some
surveys from Statistics Canada. Yee et al (1999) describe their
construction and use for one such survey.
</p>
<p>The variance is computed as the sum of squared deviations of the
replicates from their mean.  This may be rescaled: <code>scale</code> is an
overall multiplier and <code>rscales</code> is a vector of
replicate-specific multipliers for the squared deviations. That is,
<code>rscales</code> should have one entry for each column of <code>repweights</code>
If thereplication weights incorporate the sampling weights
(<code>combined.weights=TRUE</code>) or for <code>type="other"</code> these must
be specified, otherwise they can be guessed from the weights.
</p>
<p>A finite population correction may be specified for <code>type="other"</code>,
<code>type="JK1"</code> and <code>type="JKn"</code>.  <code>fpc</code> must be a vector
with one entry for each replicate. To specify sampling fractions use
<code>fpctype="fraction"</code> and to specify the correction directly use
<code>fpctype="correction"</code>
</p>
<p>The design degrees of freedom are returned by <code><a href="#topic+degf">degf</a></code>. By
default they are computed from the numerical rank of the
repweights. This is slow for very large data sets and you can specify
a value instead. 
</p>
<p><code>repweights</code> may be a character string giving a regular expression
for the replicate weight variables. For example, in the
California Health Interview Survey public-use data, the sampling weights are
<code>"rakedw0"</code> and the replicate weights are <code>"rakedw1"</code> to
<code>"rakedw80"</code>.  The regular expression <code>"rakedw[1-9]"</code>
matches the replicate weight variables (and not the sampling weight
variable).
</p>
<p><code>data</code> may be a character string giving the name of a table or view
in a relational database that can be accessed through the <code>DBI</code> 
interface. For DBI interfaces <code>dbtype</code> should be the name of the database
driver and <code>dbname</code> should be the name by which the driver identifies
the specific database (eg file name for SQLite).
</p>
<p>The appropriate database interface package must already be loaded (eg
<code>RSQLite</code> for SQLite).  The survey design
object will contain the replicate weights, but actual variables will
be loaded from the database only as needed.  Use
<code><a href="#topic+close.DBIsvydesign">close</a></code> to close the database connection and
<code><a href="#topic+open.DBIsvydesign">open</a></code> to reopen the connection, eg, after
loading a saved object.
</p>
<p>The database interface does not attempt to modify the underlying
database and so can be used with read-only permissions on the database.
</p>
<p>To generate your own replicate weights either use
<code><a href="#topic+as.svrepdesign">as.svrepdesign</a></code> on a <code>survey.design</code> object, or see
<code><a href="#topic+brrweights">brrweights</a></code>, <code><a href="#topic+bootweights">bootweights</a></code>,
<code><a href="#topic+jk1weights">jk1weights</a></code> and <code><a href="#topic+jknweights">jknweights</a></code>
</p>
<p>The <code>model.frame</code> method extracts the observed data.
</p>


<h3>Value</h3>

<p>Object of class <code>svyrep.design</code>, with methods for <code>print</code>,
<code>summary</code>, <code>weights</code>, <code>image</code>.
</p>


<h3>Note</h3>

<p>To use replication-weight analyses on a survey specified by
sampling design, use <code>as.svrepdesign</code> to convert it. </p>


<h3>References</h3>

<p>Levy and Lemeshow. &quot;Sampling of Populations&quot;. Wiley.
</p>
<p>Shao and Tu. &quot;The Jackknife and Bootstrap.&quot; Springer.
</p>
<p>Yee et al (1999). Bootstrat Variance Estimation for the National
Population Health Survey. Proceedings of the ASA Survey Research
Methodology Section. <a href="https://web.archive.org/web/20151110170959/http://www.amstat.org/sections/SRMS/Proceedings/papers/1999_136.pdf">https://web.archive.org/web/20151110170959/http://www.amstat.org/sections/SRMS/Proceedings/papers/1999_136.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+as.svrepdesign">as.svrepdesign</a></code>, <code><a href="#topic+svydesign">svydesign</a></code>,
<code><a href="#topic+brrweights">brrweights</a></code>, <code>bootweights</code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(scd)
# use BRR replicate weights from Levy and Lemeshow
repweights&lt;-2*cbind(c(1,0,1,0,1,0), c(1,0,0,1,0,1), c(0,1,1,0,0,1),
c(0,1,0,1,1,0))
scdrep&lt;-svrepdesign(data=scd, type="BRR", repweights=repweights, combined.weights=FALSE)
svyratio(~alive, ~arrests, scdrep)


## Not run: 
## Needs RSQLite
library(RSQLite)
db_rclus1&lt;-svrepdesign(weights=~pw, repweights="wt[1-9]+", type="JK1", scale=(1-15/757)*14/15,
data="apiclus1rep",dbtype="SQLite", dbname=system.file("api.db",package="survey"), combined=FALSE)
svymean(~api00+api99,db_rclus1)

summary(db_rclus1)

## closing and re-opening a connection
close(db_rclus1)
db_rclus1
try(svymean(~api00+api99,db_rclus1))
db_rclus1&lt;-open(db_rclus1)
svymean(~api00+api99,db_rclus1)




## End(Not run)

</code></pre>

<hr>
<h2 id='svrVar'>Compute variance from replicates </h2><span id='topic+svrVar'></span>

<h3>Description</h3>

<p>Compute an appropriately scaled empirical variance estimate from
replicates.  The <code>mse</code> argument specifies whether the sums of
squares should be centered at the point estimate (<code>mse=TRUE</code>) or
the mean of the replicates. It is usually taken from the <code>mse</code>
component of the design object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svrVar(thetas, scale, rscales, na.action=getOption("na.action"), 
  mse=getOption("survey.replicates.mse"),coef)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svrVar_+3A_thetas">thetas</code></td>
<td>
<p>matrix whose rows are replicates (or a vector of replicates)</p>
</td></tr>
<tr><td><code id="svrVar_+3A_scale">scale</code></td>
<td>
<p>Overall scaling factor</p>
</td></tr>
<tr><td><code id="svrVar_+3A_rscales">rscales</code></td>
<td>
<p>Scaling factor for each squared deviation </p>
</td></tr>
<tr><td><code id="svrVar_+3A_na.action">na.action</code></td>
<td>
<p>How to handle replicates where the statistic could
not be estimated</p>
</td></tr>
<tr><td><code id="svrVar_+3A_mse">mse</code></td>
<td>
<p>if <code>TRUE</code>, center at the point estimated, if
<code>FALSE</code> center at the mean of the replicates</p>
</td></tr>
<tr><td><code id="svrVar_+3A_coef">coef</code></td>
<td>
<p>The point estimate, required only if <code>mse==TRUE</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>covariance matrix.  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+svrepdesign">svrepdesign</a></code>, <code><a href="#topic+as.svrepdesign">as.svrepdesign</a></code>,
<code><a href="#topic+brrweights">brrweights</a></code>,
<code><a href="#topic+jk1weights">jk1weights</a></code>, <code><a href="#topic+jknweights">jknweights</a></code></p>

<hr>
<h2 id='svy.varcoef'>Sandwich variance estimator for glms</h2><span id='topic+svy.varcoef'></span>

<h3>Description</h3>

<p>Computes the sandwich variance estimator for a generalised linear model fitted to data from a complex sample survey. Designed to be used internally by <code><a href="#topic+svyglm">svyglm</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svy.varcoef(glm.object, design)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svy.varcoef_+3A_glm.object">glm.object</code></td>
<td>
<p>A <code><a href="stats.html#topic+glm">glm</a></code> object</p>
</td></tr>
<tr><td><code id="svy.varcoef_+3A_design">design</code></td>
<td>
<p>A <code>survey.design</code> object </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A variance matrix
</p>


<h3>Author(s)</h3>

<p> Thomas Lumley</p>


<h3>See Also</h3>

<p><code><a href="#topic+svyglm">svyglm</a></code>,<code><a href="#topic+svydesign">svydesign</a></code>, <code><a href="#topic+svyCprod">svyCprod</a></code> </p>

<hr>
<h2 id='svyby'>Survey statistics on subsets</h2><span id='topic+svyby'></span><span id='topic+svybys'></span><span id='topic+svyby.default'></span><span id='topic+SE.svyby'></span><span id='topic+deff.svyby'></span><span id='topic+coef.svyby'></span><span id='topic+confint.svyby'></span><span id='topic+unwtd.count'></span><span id='topic+svyby.survey.design2'></span>

<h3>Description</h3>

<p>Compute survey statistics on subsets of a survey defined by factors. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svyby(formula, by ,design,...)
## Default S3 method:
svyby(formula, by, design, FUN, ..., deff=FALSE,keep.var = TRUE,
keep.names = TRUE,verbose=FALSE, vartype=c("se","ci","ci","cv","cvpct","var"),
 drop.empty.groups=TRUE, covmat=FALSE, return.replicates=FALSE,
 na.rm.by=FALSE, na.rm.all=FALSE, stringsAsFactors=TRUE,
multicore=getOption("survey.multicore"))
## S3 method for class 'survey.design2'
svyby(formula, by, design, FUN, ..., deff=FALSE,keep.var = TRUE,
keep.names = TRUE,verbose=FALSE, vartype=c("se","ci","ci","cv","cvpct","var"),
 drop.empty.groups=TRUE, covmat=FALSE, influence=covmat, 
 na.rm.by=FALSE, na.rm.all=FALSE, stringsAsFactors=TRUE,
 multicore=getOption("survey.multicore"))

## S3 method for class 'svyby'
SE(object,...)
## S3 method for class 'svyby'
deff(object,...)
## S3 method for class 'svyby'
coef(object,...)
## S3 method for class 'svyby'
confint(object,  parm, level = 0.95,df =Inf,...)
unwtd.count(x, design, ...)
svybys(formula,  bys,  design, FUN, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyby_+3A_formula">formula</code>, <code id="svyby_+3A_x">x</code></td>
<td>
<p>A formula specifying the variables to pass to
<code>FUN</code> (or a matrix, data frame, or vector)</p>
</td></tr>
<tr><td><code id="svyby_+3A_by">by</code></td>
<td>
<p>A formula specifying factors that define subsets, or a list
of factors.</p>
</td></tr>
<tr><td><code id="svyby_+3A_design">design</code></td>
<td>
<p>A <code>svydesign</code> or <code>svrepdesign</code> object</p>
</td></tr>
<tr><td><code id="svyby_+3A_fun">FUN</code></td>
<td>
<p>A function taking a formula and survey design object as its
first two arguments.</p>
</td></tr>
<tr><td><code id="svyby_+3A_...">...</code></td>
<td>
<p>Other arguments to <code>FUN</code>. NOTE: if any of the
names of these are partial matches to <code>formula</code>,<code>by</code>,
or <code>design</code>, you must specify the  <code>formula</code>,<code>by</code>,
or <code>design</code> argument by name, not just by position.
</p>
</td></tr>
<tr><td><code id="svyby_+3A_deff">deff</code></td>
<td>
<p>Request a design effect from <code>FUN</code></p>
</td></tr>
<tr><td><code id="svyby_+3A_keep.var">keep.var</code></td>
<td>
<p>If <code>FUN</code> returns a <code>svystat</code> object, extract
standard errors from it</p>
</td></tr>
<tr><td><code id="svyby_+3A_keep.names">keep.names</code></td>
<td>
<p>Define row names based on the subsets</p>
</td></tr>
<tr><td><code id="svyby_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, print a label for each subset as it is
processed.</p>
</td></tr>
<tr><td><code id="svyby_+3A_vartype">vartype</code></td>
<td>
<p>Report variability as one or more of
standard error, confidence interval, coefficient of
variation,  percent coefficient of variation, or variance</p>
</td></tr>
<tr><td><code id="svyby_+3A_drop.empty.groups">drop.empty.groups</code></td>
<td>
<p>If <code>FALSE</code>, report <code>NA</code> for empty
groups, if <code>TRUE</code> drop them from the output</p>
</td></tr>
<tr><td><code id="svyby_+3A_na.rm.by">na.rm.by</code></td>
<td>
<p>If true, omit groups defined by <code>NA</code> values of the
<code>by</code> variables</p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="svyby_+3A_na.rm.all">na.rm.all</code></td>
<td>
<p>If true, check for groups with no non-missing
observations for variables defined by <code>formula</code> and treat these groups
as empty. Doesn't make much sense without <code>na.rm=TRUE</code></p>
</td></tr>
<tr><td><code id="svyby_+3A_covmat">covmat</code></td>
<td>
<p>If <code>TRUE</code>, compute covariances between estimates for
different subsets. Allows <code><a href="#topic+svycontrast">svycontrast</a></code> to be used on
output. Requires that <code>FUN</code> supports either
<code>return.replicates=TRUE</code> or <code>influence=TRUE</code></p>
</td></tr>
<tr><td><code id="svyby_+3A_return.replicates">return.replicates</code></td>
<td>
<p>Only for replicate-weight designs. If
<code>TRUE</code>, return all the replicates as the &quot;replicates&quot; attribute of the result</p>
</td></tr>
<tr><td><code id="svyby_+3A_influence">influence</code></td>
<td>
<p>Return the influence functions of the result</p>
</td></tr>
<tr><td><code id="svyby_+3A_multicore">multicore</code></td>
<td>
<p>Use <code>multicore</code> package to distribute subsets over
multiple processors?</p>
</td></tr>
<tr><td><code id="svyby_+3A_stringsasfactors">stringsAsFactors</code></td>
<td>
<p>Convert any string variables in <code>formula</code>
to factors before calling <code>FUN</code>, so that the factor levels will
be the same in all groups (See Note below). Potentially slow.</p>
</td></tr>
<tr><td><code id="svyby_+3A_parm">parm</code></td>
<td>
<p>a specification of which parameters are to be given
confidence intervals, either a vector of numbers or a vector of
names. If missing, all parameters are considered.</p>
</td></tr>
<tr><td><code id="svyby_+3A_level">level</code></td>
<td>
<p>the confidence level required.</p>
</td></tr>
<tr><td><code id="svyby_+3A_df">df</code></td>
<td>
<p>degrees of freedom for t-distribution in confidence
interval, use <code>degf(design)</code> for number of PSUs minus number of
strata</p>
</td></tr>
<tr><td><code id="svyby_+3A_object">object</code></td>
<td>
<p>An object of class <code>"svyby"</code></p>
</td></tr>
<tr><td><code id="svyby_+3A_bys">bys</code></td>
<td>
<p>one-sided formula with each term specifying a grouping
(rather than being combined to give a grouping </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The variance type &quot;ci&quot; asks for confidence intervals, which are produced
by <code>confint</code>. In some cases additional options to <code>FUN</code> will
be needed to produce confidence intervals, for example,
<code>svyquantile</code> needs <code>ci=TRUE</code> or <code>keep.var=FALSE</code>.
</p>
<p><code>unwtd.count</code> is designed to be passed to <code>svyby</code> to report
the number of non-missing observations in each subset. Observations
with exactly zero weight will also be counted as missing, since that's
how subsets are implemented for some designs.
</p>
<p>Parallel processing with <code>multicore=TRUE</code> is useful only for
fairly large problems and on computers with sufficient memory. The
<code>multicore</code> package is incompatible with some GUIs, although the
Mac Aqua GUI appears to be safe.
</p>
<p>The variant <code>svybys</code> creates a separate table for each term in
<code>bys</code> rather than creating a joint table. 
</p>


<h3>Value</h3>

<p>An object of class <code>"svyby"</code>: a data frame showing the factors and the results of <code>FUN</code>.
</p>
<p>For <code>unwtd.count</code>, the unweighted number of non-missing observations in the data matrix specified by <code>x</code> for the design. 
</p>


<h3>Note</h3>

<p>The function works by making a lot of calls of the form
<code>FUN(formula, subset(design, by==i))</code>, where <code>formula</code> is
re-evaluated in each subset, so it is unwise to use data-dependent
terms in <code>formula</code>.  In particular, <code>svyby(~factor(a), ~b,
    design=d, svymean)</code>, will create factor variables whose levels are
only those values of <code>a</code>  present in each subset. If <code>a</code>
is a character variable then <code>svyby(~a, ~b,
    design=d, svymean)</code> creates factor variables implicitly and so has
the same problem.  Either use
<code><a href="#topic+update.survey.design">update.survey.design</a></code> to add variables to the design
object instead or specify the levels explicitly in the call to
<code>factor</code>. The <code>stringsAsFactors=TRUE</code> option converts
all character variables to factors, which can be slow, set it to
<code>FALSE</code> if you have predefined factors where necessary.
</p>


<h3>Note</h3>

<p> Asking for a design effect (<code>deff=TRUE</code>) from a function
that does not produce one will cause an error or incorrect formatting
of the output. The same will occur with <code>keep.var=TRUE</code> if the
function does not compute a standard error.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+svytable">svytable</a></code> and <code><a href="#topic+ftable.svystat">ftable.svystat</a></code> for
contingency tables, <code><a href="#topic+ftable.svyby">ftable.svyby</a></code> for pretty-printing of <code>svyby</code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)

svyby(~api99, ~stype, dclus1, svymean)
svyby(~api99, ~stype, dclus1, svyquantile, quantiles=0.5,ci=TRUE,vartype="ci")
## without ci=TRUE svyquantile does not compute standard errors
svyby(~api99, ~stype, dclus1, svyquantile, quantiles=0.5, keep.var=FALSE)
svyby(~api99, list(school.type=apiclus1$stype), dclus1, svymean)
svyby(~api99+api00, ~stype, dclus1, svymean, deff=TRUE,vartype="ci")
svyby(~api99+api00, ~stype+sch.wide, dclus1, svymean, keep.var=FALSE)
## report raw number of observations
svyby(~api99+api00, ~stype+sch.wide, dclus1, unwtd.count, keep.var=FALSE)

rclus1&lt;-as.svrepdesign(dclus1)

svyby(~api99, ~stype, rclus1, svymean)
svyby(~api99, ~stype, rclus1, svyquantile, quantiles=0.5)
svyby(~api99, list(school.type=apiclus1$stype), rclus1, svymean, vartype="cv")
svyby(~enroll,~stype, rclus1,svytotal, deff=TRUE)
svyby(~api99+api00, ~stype+sch.wide, rclus1, svymean, keep.var=FALSE)
##report raw number of observations
svyby(~api99+api00, ~stype+sch.wide, rclus1, unwtd.count, keep.var=FALSE)

## comparing subgroups using covmat=TRUE
mns&lt;-svyby(~api99, ~stype, rclus1, svymean,covmat=TRUE)
vcov(mns)
svycontrast(mns, c(E = 1, M = -1))

str(svyby(~api99, ~stype, rclus1, svymean,return.replicates=TRUE))

tots&lt;-svyby(~enroll, ~stype, dclus1, svytotal,covmat=TRUE)
vcov(tots)
svycontrast(tots, quote(E/H))


## comparing subgroups uses the delta method unless replicates are present
meanlogs&lt;-svyby(~log(enroll),~stype,svymean, design=rclus1,covmat=TRUE)
svycontrast(meanlogs, quote(exp(E-H)))
meanlogs&lt;-svyby(~log(enroll),~stype,svymean, design=rclus1,covmat=TRUE,return.replicates=TRUE)
svycontrast(meanlogs, quote(exp(E-H)))


## extractor functions
(a&lt;-svyby(~enroll, ~stype, rclus1, svytotal, deff=TRUE, verbose=TRUE, 
  vartype=c("se","cv","cvpct","var")))
deff(a)
SE(a)
cv(a)
coef(a)
confint(a, df=degf(rclus1))

## ratio estimates
svyby(~api.stu, by=~stype, denominator=~enroll, design=dclus1, svyratio)

ratios&lt;-svyby(~api.stu, by=~stype, denominator=~enroll, design=dclus1, svyratio,covmat=TRUE)
vcov(ratios)

## empty groups
svyby(~api00,~comp.imp+sch.wide,design=dclus1,svymean)
svyby(~api00,~comp.imp+sch.wide,design=dclus1,svymean,drop.empty.groups=FALSE)

## Multiple tables
svybys(~api00,~comp.imp+sch.wide,design=dclus1,svymean)



</code></pre>

<hr>
<h2 id='svycdf'>Cumulative Distribution Function</h2><span id='topic+svycdf'></span><span id='topic+print.svycdf'></span><span id='topic+plot.svycdf'></span>

<h3>Description</h3>

<p>Estimates the population cumulative distribution function for specified
variables.  In contrast to <code><a href="#topic+svyquantile">svyquantile</a></code>, this does not do
any interpolation: the result is a right-continuous step function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svycdf(formula, design, na.rm = TRUE,...)
## S3 method for class 'svycdf'
print(x,...)
## S3 method for class 'svycdf'
plot(x,xlab=NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svycdf_+3A_formula">formula</code></td>
<td>
<p>one-sided formula giving variables from the design object </p>
</td></tr>
<tr><td><code id="svycdf_+3A_design">design</code></td>
<td>
<p>survey design object </p>
</td></tr>
<tr><td><code id="svycdf_+3A_na.rm">na.rm</code></td>
<td>
<p>remove missing data (case-wise deletion)?</p>
</td></tr>
<tr><td><code id="svycdf_+3A_...">...</code></td>
<td>
<p>other arguments to <code><a href="stats.html#topic+plot.stepfun">plot.stepfun</a></code></p>
</td></tr>
<tr><td><code id="svycdf_+3A_x">x</code></td>
<td>
<p>object of class <code>svycdf</code></p>
</td></tr>
<tr><td><code id="svycdf_+3A_xlab">xlab</code></td>
<td>
<p>a vector of x-axis labels or <code>NULL</code> for the default labels</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>svycdf</code>, which is a list of step functions (of
class <code><a href="stats.html#topic+stepfun">stepfun</a></code>)
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+svyquantile">svyquantile</a></code>, <code><a href="#topic+svyhist">svyhist</a></code>, <code><a href="stats.html#topic+plot.stepfun">plot.stepfun</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dstrat &lt;- svydesign(id = ~1, strata = ~stype, weights = ~pw, data = apistrat, 
    fpc = ~fpc)
cdf.est&lt;-svycdf(~enroll+api00+api99, dstrat)
cdf.est
## function
cdf.est[[1]]
## evaluate the function
cdf.est[[1]](800)
cdf.est[[2]](800)

## compare to population and sample CDFs.
opar&lt;-par(mfrow=c(2,1))
cdf.pop&lt;-ecdf(apipop$enroll)
cdf.samp&lt;-ecdf(apistrat$enroll)
plot(cdf.pop,main="Population vs sample", xlab="Enrollment")
lines(cdf.samp,col.points="red")

plot(cdf.pop, main="Population vs estimate", xlab="Enrollment")
lines(cdf.est[[1]],col.points="red")

par(opar)
</code></pre>

<hr>
<h2 id='svyciprop'>Confidence intervals for proportions </h2><span id='topic+svyciprop'></span>

<h3>Description</h3>

<p>Computes confidence intervals for proportions using methods that may be
more accurate near 0 and 1 than simply using <code>confint(svymean())</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svyciprop(formula, design, method = c("logit", "likelihood", "asin", "beta",
"mean","xlogit"), level = 0.95, df=degf(design),...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyciprop_+3A_formula">formula</code></td>
<td>
<p>Model formula specifying a single binary variable</p>
</td></tr>
<tr><td><code id="svyciprop_+3A_design">design</code></td>
<td>
<p> survey design object</p>
</td></tr>
<tr><td><code id="svyciprop_+3A_method">method</code></td>
<td>
<p> See Details below. Partial matching is done on the argument.</p>
</td></tr>
<tr><td><code id="svyciprop_+3A_level">level</code></td>
<td>
<p>Confidence level for interval</p>
</td></tr>
<tr><td><code id="svyciprop_+3A_df">df</code></td>
<td>
<p>denominator degrees of freedom, for all methods except
<code>"beta"</code>.  Use <code>Inf</code> for confidence intervals based on a
Normal distribution, and for <code>"likelihood"</code> and <code>"logit"</code>
use <code>NULL</code> for the default method in glms (currently
<code>degf(design)-1</code>, but this may be improved in the future)</p>
</td></tr>
<tr><td><code id="svyciprop_+3A_...">...</code></td>
<td>
<p>For <code>"mean"</code> and <code>"asin"</code>, this is passed to <code><a href="#topic+confint.svystat">confint.svystat</a></code> </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>"logit"</code> method fits a logistic regression model and computes a
Wald-type interval on the log-odds scale, which is then transformed to
the probability scale.
</p>
<p>The <code>"likelihood"</code> method uses the (Rao-Scott) scaled chi-squared distribution
for the loglikelihood from a binomial distribution.
</p>
<p>The <code>"asin"</code> method uses the variance-stabilising transformation
for the binomial distribution, the arcsine square root, and then
back-transforms the interval to the probability scale
</p>
<p>The <code>"beta"</code> method uses the incomplete beta function as in
<code><a href="stats.html#topic+binom.test">binom.test</a></code>, with an effective sample size based on the
estimated variance of the proportion. (Korn and Graubard, 1998)
</p>
<p>The <code>"xlogit"</code> method uses a logit transformation of the mean and
then back-transforms to the probablity scale. This appears to be the
method used by SUDAAN and SPSS COMPLEX SAMPLES. 
</p>
<p>The <code>"mean"</code> method is a Wald-type interval on the probability
scale, the same as <code>confint(svymean())</code>
</p>
<p>All methods undercover for probabilities close enough to zero or one,
but <code>"beta"</code>, <code>"likelihood"</code>, <code>"logit"</code>, and <code>"logit"</code> are noticeably
better than the other two. None of the methods will work when the
observed proportion is exactly 0 or 1.
</p>
<p>The <code>confint</code> method extracts the confidence interval; the
<code>vcov</code> and <code>SE</code> methods just report the variance or standard
error of the mean. 
</p>


<h3>Value</h3>

<p>The point estimate of the proportion, with the confidence interval as
an attribute
</p>


<h3>References</h3>

<p>Rao, JNK, Scott, AJ (1984) &quot;On Chi-squared Tests For Multiway
Contingency Tables with Proportions Estimated From Survey Data&quot; Annals
of Statistics 12:46-60.
</p>
<p>Korn EL, Graubard BI. (1998) Confidence Intervals For Proportions With
Small Expected Number of Positive Counts Estimated From Survey
Data. Survey Methodology 23:193-201.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+svymean">svymean</a></code>, <code><a href="#topic+yrbs">yrbs</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus1&lt;-svydesign(id=~dnum, fpc=~fpc, data=apiclus1)

svyciprop(~I(ell==0), dclus1, method="li")
svyciprop(~I(ell==0), dclus1, method="lo")
svyciprop(~I(ell==0), dclus1, method="as")
svyciprop(~I(ell==0), dclus1, method="be")
svyciprop(~I(ell==0), dclus1, method="me")
svyciprop(~I(ell==0), dclus1, method="xl")

## reproduces Stata svy: mean
svyciprop(~I(ell==0), dclus1, method="me", df=degf(dclus1))
## reproduces Stata svy: prop
svyciprop(~I(ell==0), dclus1, method="lo", df=degf(dclus1))


rclus1&lt;-as.svrepdesign(dclus1)
svyciprop(~I(emer==0), rclus1, method="li")
svyciprop(~I(emer==0), rclus1, method="lo")
svyciprop(~I(emer==0), rclus1, method="as")
svyciprop(~I(emer==0), rclus1, method="be")
svyciprop(~I(emer==0), rclus1, method="me")


</code></pre>

<hr>
<h2 id='svycontrast'>Linear and nonlinearconstrasts of survey statistics </h2><span id='topic+svycontrast'></span>

<h3>Description</h3>

<p>Computes linear or nonlinear contrasts of estimates produced by survey
functions (or any object with <code>coef</code> and <code>vcov</code> methods).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svycontrast(stat, contrasts, add=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svycontrast_+3A_stat">stat</code></td>
<td>
<p>object of class <code>svrepstat</code> or <code>svystat</code> </p>
</td></tr>
<tr><td><code id="svycontrast_+3A_contrasts">contrasts</code></td>
<td>
<p>A vector or list of vectors of coefficients, or a
call or list of calls </p>
</td></tr>
<tr><td><code id="svycontrast_+3A_add">add</code></td>
<td>
<p>keep all the coefficients of the input in the output?</p>
</td></tr>
<tr><td><code id="svycontrast_+3A_...">...</code></td>
<td>
<p>For future expansion</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>contrasts</code> is a list, the element names are used as
names for the returned statistics.
</p>
<p>If an element of <code>contrasts</code> is shorter than <code>coef(stat)</code> and has names, the
names are used to match up the vectors and the remaining elements of
<code>contrasts</code> are assumed to be zero. If the names are not legal
variable names (eg <code>0.1</code>) they must be quoted (eg <code>"0.1"</code>)
</p>
<p>If <code>contrasts</code> is a <code>"call"</code> or list of <code>"call"s</code>, and
<code>stat</code> is a <code>svrepstat</code> object including replicates, the
replicates are transformed and used to compute the variance. If
<code>stat</code> is a <code>svystat</code> object or a <code>svrepstat</code> object
without replicates, the delta-method is used to compute variances, and
the calls must use only functions that <code><a href="stats.html#topic+deriv">deriv</a></code> knows how
to differentiate. If the names are not legal variable names they must
be quoted with backticks (eg <code>`0.1`</code>).
</p>
<p>If <code>stats</code> is a <code>svyvar</code> object, the estimates are elements of a matrix and the names are the row and column names pasted together with a colon. 
</p>


<h3>Value</h3>

<p>Object of class <code>svrepstat</code> or <code>svystat</code> or <code>svyvar</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+regTermTest">regTermTest</a></code>, <code>svyglm</code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)

a &lt;- svytotal(~api00+enroll+api99, dclus1)
svycontrast(a, list(avg=c(0.5,0,0.5), diff=c(1,0,-1)))
## if contrast vectors have names, zeroes may be omitted
svycontrast(a, list(avg=c(api00=0.5,api99=0.5), diff=c(api00=1,api99=-1)))

## nonlinear contrasts
svycontrast(a, quote(api00/api99))
svyratio(~api00, ~api99, dclus1)

## Example: standardised skewness coefficient
moments&lt;-svymean(~I(api00^3)+I(api00^2)+I(api00), dclus1)
svycontrast(moments, 
quote((`I(api00^3)`-3*`I(api00^2)`*`I(api00)`+ 3*`I(api00)`*`I(api00)`^2-`I(api00)`^3)/
      (`I(api00^2)`-`I(api00)`^2)^1.5))

## Example: geometric means
## using delta method
meanlogs &lt;- svymean(~log(api00)+log(api99), dclus1)
svycontrast(meanlogs,
    list(api00=quote(exp(`log(api00)`)), api99=quote(exp(`log(api99)`))))

## using delta method
rclus1&lt;-as.svrepdesign(dclus1)
meanlogs &lt;- svymean(~log(api00)+log(api99), rclus1)
svycontrast(meanlogs,
    list(api00=quote(exp(`log(api00)`)),
api99=quote(exp(`log(api99)`))))

## why is add=TRUE useful?

(totals&lt;-svyby(~enroll,~stype,design=dclus1,svytotal,covmat=TRUE))
totals1&lt;-svycontrast(totals, list(total=c(1,1,1)), add=TRUE)

svycontrast(totals1, list(quote(E/total), quote(H/total), quote(M/total)))


totals2&lt;-svycontrast(totals, list(total=quote(E+H+M)), add=TRUE)
all.equal(as.matrix(totals1),as.matrix(totals2))

## more complicated svyby
means &lt;- svyby(~api00+api99, ~stype+sch.wide, design=dclus1, svymean,covmat=TRUE)
svycontrast(means, quote(`E.No:api00`-`E.No:api99`))
svycontrast(means, quote(`E.No:api00`/`E.No:api99`))

## transforming replicates
meanlogs_r &lt;- svymean(~log(api00)+log(api99), rclus1, return.replicates=TRUE)
svycontrast(meanlogs_r,
    list(api00=quote(exp(`log(api00)`)), api99=quote(exp(`log(api99)`))))

## converting covariances to correlations
vmat &lt;-svyvar(~api00+ell,dclus1)
print(vmat,cov=TRUE)
cov2cor(as.matrix(vmat))[1,2]
svycontrast(vmat, quote(`api00:ell`/sqrt(`api00:api00`*`ell:ell`)))

</code></pre>

<hr>
<h2 id='svycoplot'>Conditioning plots of survey data </h2><span id='topic+svycoplot'></span>

<h3>Description</h3>

<p>Draws conditioned scatterplots ('Trellis' plots) of survey data using
hexagonal binning or transparency.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svycoplot(formula, design, style = c("hexbin", "transparent"), basecol =
"black", alpha = c(0, 0.8),hexscale=c("relative","absolute"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svycoplot_+3A_formula">formula</code></td>
<td>
<p>A graph formula suitable for <code><a href="lattice.html#topic+xyplot">xyplot</a></code></p>
</td></tr>
<tr><td><code id="svycoplot_+3A_design">design</code></td>
<td>
<p>A survey design object </p>
</td></tr>
<tr><td><code id="svycoplot_+3A_style">style</code></td>
<td>
<p>Hexagonal binning or transparent color?</p>
</td></tr>
<tr><td><code id="svycoplot_+3A_basecol">basecol</code></td>
<td>
<p>The fully opaque 'base' color for creating transparent
colors. This may also be a function; see <code><a href="#topic+svyplot">svyplot</a></code> for details</p>
</td></tr>
<tr><td><code id="svycoplot_+3A_alpha">alpha</code></td>
<td>
<p>Minimum and maximum opacity </p>
</td></tr>
<tr><td><code id="svycoplot_+3A_hexscale">hexscale</code></td>
<td>
<p>Scale hexagons separate for each panel (relative) or
across all panels (absolute)</p>
</td></tr>
<tr><td><code id="svycoplot_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code>grid.hexagons</code> or <code><a href="lattice.html#topic+xyplot">xyplot</a></code> </p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>trellis</code>
</p>


<h3>Note</h3>

<p>As with all 'Trellis' graphs, this function creates an object but does
not draw the graph. When used inside a function or non-interactively
you need to <code>print()</code> the result to create the graph.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+svyplot">svyplot</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus2&lt;-svydesign(id=~dnum+snum,  weights=~pw,
                    data=apiclus2, fpc=~fpc1+fpc2)

svycoplot(api00~api99|sch.wide*comp.imp, design=dclus2, style="hexbin")
svycoplot(api00~api99|sch.wide*comp.imp, design=dclus2, style="hexbin", hexscale="absolute")

svycoplot(api00~api99|sch.wide, design=dclus2, style="trans")

svycoplot(api00~meals|stype,design=dclus2,
  style="transparent",
  basecol=function(d) c("darkred","purple","forestgreen")[as.numeric(d$stype)],
  alpha=c(0,1)) 
</code></pre>

<hr>
<h2 id='svycoxph'>Survey-weighted Cox models.</h2><span id='topic+svycoxph'></span><span id='topic+svycoxph.survey.design2'></span><span id='topic+svycoxph.survey.design'></span><span id='topic+svycoxph.svyrep.design'></span><span id='topic+predict.svycoxph'></span><span id='topic+extractAIC.svycoxph'></span><span id='topic+AIC.svycoxph'></span>

<h3>Description</h3>

<p>Fit a proportional hazards model to data from a complex survey design.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svycoxph(formula, design,subset=NULL, rescale=TRUE, ...)
## S3 method for class 'svycoxph'
predict(object, newdata, se=FALSE,
    type=c("lp", "risk", "terms","curve"),...)
## S3 method for class 'svycoxph'
AIC(object, ..., k = 2)    
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svycoxph_+3A_formula">formula</code></td>
<td>
<p>Model formula. Any <code>cluster()</code> terms will be ignored.</p>
</td></tr>
<tr><td><code id="svycoxph_+3A_design">design</code></td>
<td>
 <p><code>survey.design</code> object. Must contain all variables
in the formula</p>
</td></tr>
<tr><td><code id="svycoxph_+3A_subset">subset</code></td>
<td>
<p>Expression to select a subpopulation</p>
</td></tr>
<tr><td><code id="svycoxph_+3A_rescale">rescale</code></td>
<td>
<p>Rescale weights to improve numerical stability</p>
</td></tr>
<tr><td><code id="svycoxph_+3A_object">object</code></td>
<td>
<p>A <code>svycoxph</code> object</p>
</td></tr>
<tr><td><code id="svycoxph_+3A_newdata">newdata</code></td>
<td>
<p>New data for prediction</p>
</td></tr>
<tr><td><code id="svycoxph_+3A_se">se</code></td>
<td>
<p>Compute standard errors? This takes a lot of memory for
<code>type="curve"</code></p>
</td></tr>
<tr><td><code id="svycoxph_+3A_type">type</code></td>
<td>
<p>&quot;curve&quot; does predicted survival curves. The other values
are passed to <code>predict.coxph()</code></p>
</td></tr>
<tr><td><code id="svycoxph_+3A_...">...</code></td>
<td>
<p>For <code>AIC</code>, more models to compare the AIC of. For <code>svycoxph</code>,
other arguments passed to <code>coxph</code>. </p>
</td></tr>
<tr><td><code id="svycoxph_+3A_k">k</code></td>
<td>
<p>The penalty per parameter that would be used under independent sampling: AIC has <code>k=2</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The main difference between <code>svycoxph</code> function and the <code>robust=TRUE</code>
option to <code><a href="survival.html#topic+coxph">coxph</a></code> in the
survival package is that this function accounts for the reduction in
variance from stratified sampling and the increase in variance from
having only a small number of clusters.
</p>
<p>Note that <code>strata</code> terms in the model formula describe subsets that
have a separate baseline hazard function and need not have anything to
do with the stratification of the sampling.
</p>
<p>The <code>AIC</code> method uses the same approach as <code><a href="#topic+AIC.svyglm">AIC.svyglm</a></code>,
though the relevance of the criterion this optimises is a bit less clear
than for generalised linear models. 
</p>
<p>The standard errors for predicted survival curves are available only by linearization, not
by replicate weights (at the moment). Use
<code><a href="#topic+withReplicates">withReplicates</a></code> to get standard errors with replicate
weights. Predicted survival curves are not available for stratified
Cox models.
</p>
<p>The standard errors use the delta-method approach of Williams (1995)
for the Nelson-Aalen estimator, modified to handle the Cox model
following Tsiatis (1981). The standard errors agree closely with
<code>survfit.coxph</code> for independent sampling when the model fits
well, but are larger when the model fits poorly.  I believe the
standard errors are equivalent to those of Lin (2000), but I don't
know of any implementation that would allow a check.
</p>


<h3>Value</h3>

<p>An object of class <code>svycoxph</code> for <code>svycoxph</code>, an object of
class <code>svykm</code> or <code>svykmlist</code> for <code>predict(,type="curve")</code>.
</p>


<h3>Warning</h3>

<p>The standard error calculation for survival curves uses memory
proportional to the sample size times the square of the number of events.
</p>


<h3>Author(s)</h3>

<p>Thomas Lumley</p>


<h3>References</h3>

<p>Binder DA. (1992) Fitting Cox's proportional hazards models
from survey data. Biometrika 79: 139-147
</p>
<p>Lin D-Y (2000) On fitting Cox's proportional hazards model to survey data. Biometrika 87: 37-47
</p>
<p>Tsiatis AA (1981) A Large Sample Study of Cox's Regression Model. Annals
of Statistics 9(1) 93-108
</p>
<p>Williams RL (1995) &quot;Product-Limit Survival Functions with Correlated 
Survival Times&quot; Lifetime Data Analysis 1: 171&ndash;186
</p>


<h3>See Also</h3>

  <p><code><a href="survival.html#topic+coxph">coxph</a></code>, <code><a href="survival.html#topic+predict.coxph">predict.coxph</a></code>
</p>
<p><code><a href="#topic+svykm">svykm</a></code> for estimation of Kaplan-Meier survival curves and
for methods that operate on survival curves.
</p>
<p><code><a href="#topic+regTermTest">regTermTest</a></code> for Wald and (Rao-Scott) likelihood ratio tests for one or more parameters.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Somewhat unrealistic example of nonresponse bias.
data(pbc, package="survival")

pbc$randomized&lt;-with(pbc, !is.na(trt) &amp; trt&gt;0)
biasmodel&lt;-glm(randomized~age*edema,data=pbc,family=binomial)
pbc$randprob&lt;-fitted(biasmodel)
if (is.null(pbc$albumin)) pbc$albumin&lt;-pbc$alb ##pre2.9.0

dpbc&lt;-svydesign(id=~1, prob=~randprob, strata=~edema, data=subset(pbc,randomized))
rpbc&lt;-as.svrepdesign(dpbc)

(model&lt;-svycoxph(Surv(time,status&gt;0)~log(bili)+protime+albumin,design=dpbc))

svycoxph(Surv(time,status&gt;0)~log(bili)+protime+albumin,design=rpbc)

s&lt;-predict(model,se=TRUE, type="curve",
     newdata=data.frame(bili=c(3,9), protime=c(10,10), albumin=c(3.5,3.5)))
plot(s[[1]],ci=TRUE,col="sienna")
lines(s[[2]], ci=TRUE,col="royalblue")
quantile(s[[1]], ci=TRUE)
confint(s[[2]], parm=365*(1:5))
</code></pre>

<hr>
<h2 id='svyCprod'>Computations for survey variances</h2><span id='topic+svyCprod'></span><span id='topic+onestage'></span><span id='topic+onestrat'></span>

<h3>Description</h3>

<p>Computes the sum of products needed for the variance of survey sample
estimators.  <code>svyCprod</code> is used for survey design objects from
before version 2.9, <code>onestage</code> is called by <code><a href="#topic+svyrecvar">svyrecvar</a></code>
for post-2.9 design objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svyCprod(x, strata, psu, fpc, nPSU,certainty=NULL, postStrata=NULL,
      lonely.psu=getOption("survey.lonely.psu"))
onestage(x, strata, clusters, nPSU, fpc,
      lonely.psu=getOption("survey.lonely.psu"),stage=0,cal)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyCprod_+3A_x">x</code></td>
<td>
<p>A vector or matrix</p>
</td></tr>
<tr><td><code id="svyCprod_+3A_strata">strata</code></td>
<td>
<p>A vector of stratum indicators (may be <code>NULL</code> for <code>svyCprod</code>)</p>
</td></tr>
<tr><td><code id="svyCprod_+3A_psu">psu</code></td>
<td>
<p>A vector of cluster indicators (may be <code>NULL</code>)</p>
</td></tr>
<tr><td><code id="svyCprod_+3A_clusters">clusters</code></td>
<td>
<p>A vector of cluster indicators </p>
</td></tr>
<tr><td><code id="svyCprod_+3A_fpc">fpc</code></td>
<td>
<p>A data frame (<code>svyCprod</code>) or vector (<code>onestage</code>)
of population stratum sizes, or <code>NULL</code></p>
</td></tr>
<tr><td><code id="svyCprod_+3A_npsu">nPSU</code></td>
<td>
<p>Table (<code>svyprod</code>) or vector (<code>onestage</code>)
of original sample stratum sizes (or <code>NULL</code>)</p>
</td></tr>
<tr><td><code id="svyCprod_+3A_certainty">certainty</code></td>
<td>
<p>logical vector with stratum names as names. If
<code>TRUE</code> and that stratum has a single PSU it is a certainty PSU</p>
</td></tr>
<tr><td><code id="svyCprod_+3A_poststrata">postStrata</code></td>
<td>
<p>Post-stratification variables</p>
</td></tr>
<tr><td><code id="svyCprod_+3A_lonely.psu">lonely.psu</code></td>
<td>
<p>One of <code>"remove"</code>, <code>"adjust"</code>,
<code>"fail"</code>, <code>"certainty"</code>, <code>"average"</code>. See Details
below</p>
</td></tr>
<tr><td><code id="svyCprod_+3A_stage">stage</code></td>
<td>
<p>Used internally to track the depth of recursion</p>
</td></tr>
<tr><td><code id="svyCprod_+3A_cal">cal</code></td>
<td>
<p>Used to pass calibration information at stages below the population</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The observations for each cluster are added, then centered within each
stratum and the outer product is taken of the row vector resulting for
each cluster.  This is added within strata, multiplied by a
degrees-of-freedom correction and by a finite population correction (if
supplied) and added across strata.  
</p>
<p>If there are fewer clusters (PSUs) in a stratum than in the original
design extra rows of zeroes are added to <code>x</code> to allow the correct
subpopulation variance to be computed.
</p>
<p>See <code><a href="#topic+postStratify">postStratify</a></code> for information about
post-stratification adjustments.
</p>
<p>The variance formula gives 0/0 if a stratum contains only one sampling
unit. If the <code>certainty</code> argument specifies that this is a PSU
sampled with probability 1 (a &quot;certainty&quot; PSU) then it does not
contribute to the variance (this is correct only when there is no 
subsampling within the PSU &ndash; otherwise it should be defined as a 
pseudo-stratum).  If <code>certainty</code> is <code>FALSE</code> for
this stratum or is not supplied the result depends on <code>lonely.psu</code>.
</p>
<p>The options are <code>"fail"</code> to give an error, <code>"remove"</code> or
<code>"certainty"</code> to give a variance contribution of 0 for the stratum,
<code>"adjust"</code> to center the stratum at the grand mean rather than the
stratum mean, and <code>"average"</code> to assign strata with one PSU the
average variance contribution from strata with more than one PSU.  The
choice is controlled by setting <code>options(survey.lonely.psu)</code>. If
this is not done the factory default is <code>"fail"</code>. Using
<code>"adjust"</code> is conservative, and it would often be better to combine
strata in some intelligent way. The properties of <code>"average"</code> have
not been investigated thoroughly, but it may be useful when the lonely
PSUs are due to a few strata having PSUs missing completely at random.
</p>
<p>The <code>"remove"</code>and <code>"certainty"</code> options give the same result,
but <code>"certainty"</code> is intended for situations where there is only
one PSU in the population stratum, which is sampled with certainty (also
called &lsquo;self-representing&rsquo; PSUs or strata). With <code>"certainty"</code> no
warning is generated for strata with only one PSU.  Ordinarily,
<code>svydesign</code> will detect certainty PSUs, making this option
unnecessary.
</p>
<p>For strata with a single PSU in a subset (domain) the variance formula
gives a value that is well-defined and positive, but not typically
correct. If <code>options("survey.adjust.domain.lonely")</code> is <code>TRUE</code>
and <code>options("survey.lonely.psu")</code> is <code>"adjust"</code> or
<code>"average"</code>, and no post-stratification or G-calibration has been
done, strata with a single PSU in a subset will be treated like those
with a single PSU in the sample.  I am not aware of any theoretical
study of this procedure, but it should at least be conservative.
</p>


<h3>Value</h3>

<p>A covariance matrix
</p>


<h3>Author(s)</h3>

<p>Thomas Lumley</p>


<h3>References</h3>

<p>Binder, David A. (1983).  On the variances of asymptotically normal estimators from complex surveys.  International Statistical Review, 51, 279- 292. </p>


<h3>See Also</h3>

<p><code><a href="#topic+svydesign">svydesign</a></code>, <code><a href="#topic+svyrecvar">svyrecvar</a></code>, <code><a href="#topic+surveyoptions">surveyoptions</a></code>, <code><a href="#topic+postStratify">postStratify</a></code> </p>

<hr>
<h2 id='svycralpha'>
Cronbach's alpha
</h2><span id='topic+svycralpha'></span>

<h3>Description</h3>

<p>Compute Cronbach's alpha coefficient of reliability from survey data.  The formula is equation (2) of Cronbach (1951) only with design-based estimates of the variances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svycralpha(formula, design, na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svycralpha_+3A_formula">formula</code></td>
<td>

<p>One-sided formula giving the variables that make up the total score
</p>
</td></tr>
<tr><td><code id="svycralpha_+3A_design">design</code></td>
<td>

<p>survey design object
</p>
</td></tr>
<tr><td><code id="svycralpha_+3A_na.rm">na.rm</code></td>
<td>

<p><code>TRUE</code> to remove missing values
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A number
</p>


<h3>References</h3>

<p>Cronbach LJ (1951). &quot;Coefficient alpha and the internal structure of tests&quot;. Psychometrika. 16 (3): 297-334. doi:10.1007/bf02310555.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dstrat&lt;-svydesign(id = ~1, strata = ~stype, weights = ~pw, data = apistrat, 
    fpc = ~fpc)
svycralpha(~ell+mobility+avg.ed+emer+meals, dstrat)    
</code></pre>

<hr>
<h2 id='svydesign'>Survey sample analysis.</h2><span id='topic+svydesign'></span><span id='topic+svydesign.default'></span><span id='topic+svydesign.imputationList'></span><span id='topic+svydesign.character'></span><span id='topic+na.omit.survey.design'></span><span id='topic+na.exclude.survey.design'></span><span id='topic+na.fail.survey.design'></span>

<h3>Description</h3>

<p>Specify a complex survey design.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svydesign(ids, probs=NULL, strata = NULL, variables = NULL, fpc=NULL,
data = NULL, nest = FALSE, check.strata = !nest, weights=NULL,pps=FALSE,...)
## Default S3 method:
svydesign(ids, probs=NULL, strata = NULL, variables = NULL,
  fpc=NULL,data = NULL, nest = FALSE, check.strata = !nest, weights=NULL,
  pps=FALSE,calibrate.formula=NULL,variance=c("HT","YG"),...)
## S3 method for class 'imputationList'
svydesign(ids, probs = NULL, strata = NULL, variables = NULL, 
    fpc = NULL, data, nest = FALSE, check.strata = !nest, weights = NULL, pps=FALSE,
     calibrate.formula=NULL,...)
## S3 method for class 'character'
svydesign(ids, probs = NULL, strata = NULL, variables = NULL, 
    fpc = NULL, data, nest = FALSE, check.strata = !nest, weights = NULL, pps=FALSE,
    calibrate.formula=NULL,dbtype = "SQLite", dbname, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svydesign_+3A_ids">ids</code></td>
<td>
<p>Formula or data frame specifying cluster ids from largest
level to smallest level, <code>~0</code> or <code>~1</code> is a formula for no clusters.</p>
</td></tr>
<tr><td><code id="svydesign_+3A_probs">probs</code></td>
<td>
<p>Formula or data frame specifying cluster sampling probabilities</p>
</td></tr>
<tr><td><code id="svydesign_+3A_strata">strata</code></td>
<td>
<p>Formula or vector specifying strata, use <code>NULL</code> for no strata</p>
</td></tr>
<tr><td><code id="svydesign_+3A_variables">variables</code></td>
<td>
<p>Formula or data frame specifying the variables
measured in the survey. If <code>NULL</code>, the <code>data</code> argument is
used.</p>
</td></tr>
<tr><td><code id="svydesign_+3A_fpc">fpc</code></td>
<td>
<p>Finite population correction: see Details below</p>
</td></tr>
<tr><td><code id="svydesign_+3A_weights">weights</code></td>
<td>
<p>Formula or vector specifying sampling weights as an
alternative to <code>prob</code></p>
</td></tr>
<tr><td><code id="svydesign_+3A_data">data</code></td>
<td>
<p>Data frame to look up variables in the formula
arguments, or database table name, or <code>imputationList</code> object, see below</p>
</td></tr>
<tr><td><code id="svydesign_+3A_nest">nest</code></td>
<td>
<p>If <code>TRUE</code>, relabel cluster ids to enforce nesting
within strata</p>
</td></tr>
<tr><td><code id="svydesign_+3A_check.strata">check.strata</code></td>
<td>
<p>If <code>TRUE</code>, check that clusters are nested in
strata</p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="svydesign_+3A_pps">pps</code></td>
<td>
<p><code>"brewer"</code> to use Brewer's approximation for PPS 
sampling without replacement. <code>"overton"</code> to use
Overton's approximation. An object of class <code><a href="#topic+HR">HR</a></code> to use the Hartley-Rao approximation. An
object of class <code><a href="#topic+ppsmat">ppsmat</a></code> to use the Horvitz-Thompson
estimator.</p>
</td></tr>
<tr><td><code id="svydesign_+3A_calibrate.formula">calibrate.formula</code></td>
<td>
<p>model formula specifying how the weights are
*already* calibrated (raked, post-stratified).</p>
</td></tr>
<tr><td><code id="svydesign_+3A_dbtype">dbtype</code></td>
<td>
<p>name of database driver to pass to <code>dbDriver</code></p>
</td></tr>
<tr><td><code id="svydesign_+3A_dbname">dbname</code></td>
<td>
<p>name of database (eg file name for SQLite)</p>
</td></tr>
<tr><td><code id="svydesign_+3A_variance">variance</code></td>
<td>
<p>For <code>pps</code> without replacement, use <code>variance="YG"</code> for the Yates-Grundy estimator instead of the Horvitz-Thompson estimator</p>
</td></tr>
<tr><td><code id="svydesign_+3A_...">...</code></td>
<td>
<p>for future expansion</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>svydesign</code> object combines a data frame and all the survey
design information needed to analyse it.  These objects are used by
the survey modelling and summary functions.  The
<code>id</code> argument is always required, the <code>strata</code>,
<code>fpc</code>, <code>weights</code> and <code>probs</code> arguments are
optional. If these variables are specified they must not have any
missing values.
</p>
<p>By default, <code>svydesign</code> assumes that all PSUs, even those in
different strata, have a unique value of the <code>id</code>
variable. This allows some data errors to be detected. If your PSUs
reuse the same identifiers across strata then set <code>nest=TRUE</code>.
</p>
<p>The finite population correction (fpc) is used to reduce the variance when
a substantial fraction of the total population of interest has been
sampled. It may not be appropriate if the target of inference is the
process generating the data rather than the statistics of a
particular finite population.
</p>
<p>The finite population correction can be specified either as the total
population size in each stratum or as the fraction of the total
population that has been sampled. In either case the relevant
population size is the sampling units.  That is, sampling 100 units
from a population stratum of size 500 can be specified as 500 or as
100/500=0.2.  The exception is for PPS sampling without replacement, where the
sampling probability (which will be different for each PSU) must be used.
</p>
<p>If population sizes are specified but not sampling probabilities or
weights, the sampling probabilities will be computed from the
population sizes assuming simple random sampling within strata. 
</p>
<p>For multistage sampling the <code>id</code> argument should specify a
formula with the cluster identifiers at each stage.  If subsequent
stages are stratified <code>strata</code> should also be specified as a
formula with stratum identifiers at each stage.  The population size
for each level of sampling should also be specified in <code>fpc</code>.
If <code>fpc</code> is not specified then sampling is assumed to be with
replacement at the top level and only the first stage of cluster is
used in computing variances. If <code>fpc</code> is specified but for fewer
stages than <code>id</code>, sampling is assumed to be complete for
subsequent stages.   The variance calculations for
multistage sampling assume simple or stratified random sampling
within clusters at each stage except possibly the last.
</p>
<p>For PPS sampling without replacement it is necessary to specify the
probabilities for each stage of sampling using the <code>fpc</code>
arguments, and an overall <code>weight</code> argument should not be
given. At the moment, multistage or stratified PPS sampling without
replacement is supported only with <code>pps="brewer"</code>, or by
giving the full joint probability matrix using
<code><a href="#topic+ppsmat">ppsmat</a></code>. [Cluster sampling is supported by all
methods, but not subsampling within clusters].  
</p>
<p>The <code>dim</code>, <code>"["</code>, <code>"[&lt;-"</code> and na.action methods for
<code>survey.design</code> objects operate on the dataframe specified by
<code>variables</code> and ensure that the design information is properly
updated to correspond to the new data frame.  With the <code>"[&lt;-"</code>
method the new value can be a <code>survey.design</code> object instead of a
data frame, but only the data frame is used. See also
<code><a href="#topic+subset.survey.design">subset.survey.design</a></code> for a simple way to select
subpopulations.
</p>
<p>The <code>model.frame</code> method extracts the observed data.
</p>
<p>If the strata with only one PSU are not self-representing (or they are,
but <code>svydesign</code> cannot tell based on <code>fpc</code>) then the handling
of these strata for variance computation is determined by
<code>options("survey.lonely.psu")</code>.  See <code><a href="#topic+svyCprod">svyCprod</a></code> for
details.
</p>
<p><code>data</code> may be a character string giving the name of a table or view
in a relational database that can be accessed through the <code>DBI</code>
interfaces. For DBI interfaces <code>dbtype</code> should be the name of the database
driver and <code>dbname</code> should be the name by which the driver identifies
the specific database (eg file name for SQLite).
</p>
<p>The appropriate database interface package must already be loaded (eg
<code>RSQLite</code> for SQLite).  The survey design
object will contain only the design meta-data, and actual variables will
be loaded from the database as needed.  Use
<code><a href="#topic+close.DBIsvydesign">close</a></code> to close the database connection and
<code><a href="#topic+open.DBIsvydesign">open</a></code> to reopen the connection, eg, after
loading a saved object.
</p>
<p>The database interface does not attempt to modify the underlying
database and so can be used with read-only permissions on the database.
</p>
<p>If <code>data</code> is an <code>imputationList</code> object (from the &quot;mitools&quot;
package), <code>svydesign</code> will return a <code>svyimputationList</code> object
containing a set of designs. Use <code><a href="#topic+with.svyimputationList">with.svyimputationList</a></code> to
do analyses on these designs and <code>MIcombine</code> to combine the results.
</p>


<h3>Value</h3>

<p>An object of class <code>survey.design</code>.
</p>


<h3>Author(s)</h3>

<p>Thomas Lumley</p>


<h3>See Also</h3>

<p><code><a href="#topic+as.svrepdesign">as.svrepdesign</a></code> for converting to replicate weight designs,
<code><a href="#topic+subset.survey.design">subset.survey.design</a></code> for domain estimates,
<code><a href="#topic+update.survey.design">update.survey.design</a></code> to add variables.
</p>
<p><code>mitools</code> package for using multiple imputations
</p>
<p><code><a href="#topic+svyrecvar">svyrecvar</a></code> for details of variance estimation
</p>
<p><code><a href="#topic+election">election</a></code> for examples of PPS sampling without replacement.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(api)
# stratified sample
dstrat&lt;-svydesign(id=~1,strata=~stype, weights=~pw, data=apistrat, fpc=~fpc)
# one-stage cluster sample
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)
# two-stage cluster sample: weights computed from population sizes.
dclus2&lt;-svydesign(id=~dnum+snum, fpc=~fpc1+fpc2, data=apiclus2)

## multistage sampling has no effect when fpc is not given, so
## these are equivalent.
dclus2wr&lt;-svydesign(id=~dnum+snum, weights=weights(dclus2), data=apiclus2)
dclus2wr2&lt;-svydesign(id=~dnum, weights=weights(dclus2), data=apiclus2)

## syntax for stratified cluster sample
##(though the data weren't really sampled this way)
svydesign(id=~dnum, strata=~stype, weights=~pw, data=apistrat,
nest=TRUE)

## PPS sampling without replacement
data(election)
dpps&lt;- svydesign(id=~1, fpc=~p, data=election_pps, pps="brewer")

##database example: requires RSQLite
## Not run: 
library(RSQLite)
dbclus1&lt;-svydesign(id=~dnum, weights=~pw, fpc=~fpc,
data="apiclus1",dbtype="SQLite", dbname=system.file("api.db",package="survey"))


## End(Not run)

## pre-calibrated weights
cdclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc,
  calibration.formula=~1)

</code></pre>

<hr>
<h2 id='svyfactanal'>
Factor analysis in complex surveys (experimental).
</h2><span id='topic+svyfactanal'></span>

<h3>Description</h3>

<p>This function fits a factor analysis model or SEM,  by maximum weighted likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svyfactanal(formula, design, factors, 
   n = c("none", "sample", "degf","effective", "min.effective"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyfactanal_+3A_formula">formula</code></td>
<td>

<p>Model formula specifying the variables to use
</p>
</td></tr>
<tr><td><code id="svyfactanal_+3A_design">design</code></td>
<td>

<p>Survey design object
</p>
</td></tr>
<tr><td><code id="svyfactanal_+3A_factors">factors</code></td>
<td>

<p>Number of factors to estimate
</p>
</td></tr>
<tr><td><code id="svyfactanal_+3A_n">n</code></td>
<td>

<p>Sample size to be used for testing: see below</p>
</td></tr>
<tr><td><code id="svyfactanal_+3A_...">...</code></td>
<td>

<p>Other arguments to pass to <code><a href="stats.html#topic+factanal">factanal</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The population covariance matrix is estimated by <code><a href="#topic+svyvar">svyvar</a></code>
and passed to <code><a href="stats.html#topic+factanal">factanal</a></code>
</p>
<p>Although fitting these models requires only the estimated covariance
matrix, inference requires a sample size.  With <code>n="sample"</code>, the sample size is taken to be
the number of observations; with <code>n="degf"</code>, the survey degrees of
freedom as returned by <code><a href="#topic+degf">degf</a></code>.  Using <code>"sample"</code>
corresponds to standardizing weights to have mean 1, and is known to
result in anti-conservative tests.
</p>
<p>The other two methods estimate an effective sample size for each
variable as the sample size where the standard error of a variance of a
Normal distribution would match the design-based standard error
estimated by <code><a href="#topic+svyvar">svyvar</a></code>. With <code>n="min.effective"</code> the
minimum sample size across the variables is used; with
<code>n="effective"</code> the harmonic mean is used.  For <code>svyfactanal</code>
the test of model adequacy is optional, and the default choice,
<code>n="none"</code>, does not do the test.
</p>


<h3>Value</h3>

<p>An object of class <code>factanal</code>
</p>


<h3>References</h3>

<p>.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+factanal">factanal</a></code>
</p>
<p>The <code>lavaan.survey</code> package fits structural equation models to complex samples using similar techniques.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)

svyfactanal(~api99+api00+hsg+meals+ell+emer, design=dclus1, factors=2)

svyfactanal(~api99+api00+hsg+meals+ell+emer, design=dclus1, factors=2, n="effective")

##Population dat for comparison
factanal(~api99+api00+hsg+meals+ell+emer, data=apipop, factors=2)

</code></pre>

<hr>
<h2 id='svyglm'>Survey-weighted generalised linear models.</h2><span id='topic+svyglm'></span><span id='topic+svyglm.survey.design'></span><span id='topic+svyglm.svyrep.design'></span><span id='topic+summary.svyglm'></span><span id='topic+summary.svrepglm'></span><span id='topic+vcov.svyglm'></span><span id='topic+residuals.svyglm'></span><span id='topic+residuals.svrepglm'></span><span id='topic+predict.svyglm'></span><span id='topic+predict.svrepglm'></span><span id='topic+coef.svyglm'></span>

<h3>Description</h3>

<p>Fit a generalised linear model to data from a complex survey design,
with inverse-probability weighting and design-based standard errors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survey.design'
svyglm(formula, design, subset=NULL,
family=stats::gaussian(),start=NULL, rescale=TRUE, ..., deff=FALSE,
 influence=FALSE)
## S3 method for class 'svyrep.design'
svyglm(formula, design, subset=NULL,
family=stats::gaussian(),start=NULL, rescale=NULL, ..., rho=NULL,
return.replicates=FALSE, na.action,multicore=getOption("survey.multicore"))
## S3 method for class 'svyglm'
summary(object, correlation = FALSE, df.resid=NULL,
...)
## S3 method for class 'svyglm'
predict(object,newdata=NULL,total=NULL,
                         type=c("link","response","terms"),
                         se.fit=(type != "terms"),vcov=FALSE,...)
## S3 method for class 'svrepglm'
predict(object,newdata=NULL,total=NULL,
                         type=c("link","response","terms"),
                         se.fit=(type != "terms"),vcov=FALSE,
                         return.replicates=!is.null(object$replicates),...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyglm_+3A_formula">formula</code></td>
<td>
<p>Model formula</p>
</td></tr>
<tr><td><code id="svyglm_+3A_design">design</code></td>
<td>
<p>Survey design from <code><a href="#topic+svydesign">svydesign</a></code> or <code><a href="#topic+svrepdesign">svrepdesign</a></code>. Must contain all variables
in the formula</p>
</td></tr>
<tr><td><code id="svyglm_+3A_subset">subset</code></td>
<td>
<p>Expression to select a subpopulation</p>
</td></tr>
<tr><td><code id="svyglm_+3A_family">family</code></td>
<td>
<p><code>family</code> object for <code>glm</code></p>
</td></tr>
<tr><td><code id="svyglm_+3A_start">start</code></td>
<td>
<p>Starting values for the coefficients (needed for some
uncommon link/family combinations)</p>
</td></tr>
<tr><td><code id="svyglm_+3A_rescale">rescale</code></td>
<td>
<p>Rescaling of weights, to improve numerical stability. The default
rescales weights to sum to the sample size. Use <code>FALSE</code> to not
rescale weights. For replicate-weight designs, use <code>TRUE</code> to
rescale weights to sum to 1, as was the case before version 3.34.</p>
</td></tr>
<tr><td><code id="svyglm_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code>glm</code> or
<code>summary.glm</code> </p>
</td></tr>
<tr><td><code id="svyglm_+3A_rho">rho</code></td>
<td>
<p>For replicate BRR designs, to specify the parameter for
Fay's variance method, giving weights of <code>rho</code> and <code>2-rho</code></p>
</td></tr>
<tr><td><code id="svyglm_+3A_return.replicates">return.replicates</code></td>
<td>
<p>Return the replicates as the <code>replicates</code> component of the
result? (for <code>predict</code>, only possible if they
were computed in the <code>svyglm</code> fit)</p>
</td></tr>
<tr><td><code id="svyglm_+3A_deff">deff</code></td>
<td>
<p>Estimate the design effects</p>
</td></tr>
<tr><td><code id="svyglm_+3A_influence">influence</code></td>
<td>
<p>Return influence functions</p>
</td></tr>
<tr><td><code id="svyglm_+3A_object">object</code></td>
<td>
<p>A <code>svyglm</code> object</p>
</td></tr>
<tr><td><code id="svyglm_+3A_correlation">correlation</code></td>
<td>
<p>Include the correlation matrix of parameters?</p>
</td></tr>
<tr><td><code id="svyglm_+3A_na.action">na.action</code></td>
<td>
<p>Handling of NAs</p>
</td></tr>
<tr><td><code id="svyglm_+3A_multicore">multicore</code></td>
<td>
<p>Use the <code>multicore</code> package to distribute
replicates across processors?</p>
</td></tr>	      
<tr><td><code id="svyglm_+3A_df.resid">df.resid</code></td>
<td>
<p>Optional denominator degrees of freedom for Wald
tests</p>
</td></tr>
<tr><td><code id="svyglm_+3A_newdata">newdata</code></td>
<td>
<p>new data frame for prediction</p>
</td></tr>
<tr><td><code id="svyglm_+3A_total">total</code></td>
<td>
<p>population size when predicting population total</p>
</td></tr>
<tr><td><code id="svyglm_+3A_type">type</code></td>
<td>
<p>linear predictor (<code>link</code>) or response</p>
</td></tr>
<tr><td><code id="svyglm_+3A_se.fit">se.fit</code></td>
<td>
<p>if <code>TRUE</code>, return variances of predictions</p>
</td></tr>
<tr><td><code id="svyglm_+3A_vcov">vcov</code></td>
<td>
<p>if <code>TRUE</code> and <code>se=TRUE</code> return full
variance-covariance matrix of predictions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For binomial and Poisson families use <code>family=quasibinomial()</code>
and <code>family=quasipoisson()</code> to avoid a warning about non-integer
numbers of successes.  The &lsquo;quasi&rsquo; versions of the family objects give
the same point estimates and standard errors and do not give the
warning.
</p>
<p>If <code>df.resid</code> is not specified the df for the null model is
computed by <code><a href="#topic+degf">degf</a></code> and the residual df computed by
subtraction.  This is recommended by Korn and Graubard and is correct
for PSU-level covariates but is potentially very conservative for
individual-level covariates. To get tests based on a Normal distribution
use <code>df.resid=Inf</code>, and to use number of PSUs-number of strata,
specify <code>df.resid=degf(design)</code>.
</p>
<p>Parallel processing with <code>multicore=TRUE</code> is helpful only for
fairly large data sets and on computers with sufficient memory. It may
be incompatible with GUIs, although the Mac Aqua GUI appears to be safe.
</p>
<p><code>predict</code> gives fitted values and sampling variability for specific new
values of covariates.  When <code>newdata</code> are the population mean it
gives the regression estimator of the mean, and when <code>newdata</code> are
the population totals and <code>total</code> is specified it gives the
regression estimator of the population total.  Regression estimators of
mean and total can also be obtained with <code><a href="#topic+calibrate">calibrate</a></code>.
</p>


<h3>Value</h3>

 <p><code>svyglm</code> returns an object of class <code>svyglm</code>.  The
<code>predict</code> method returns an object of class <code>svystat</code></p>


<h3>Note</h3>

<p><code>svyglm</code> always returns 'model-robust' standard errors; the
Horvitz-Thompson-type standard errors used everywhere in the survey
package are a generalisation of the model-robust 'sandwich' estimators.
In particular, a quasi-Poisson <code>svyglm</code> will return correct
standard errors for relative risk regression models. 
</p>


<h3>Note</h3>

<p>This function does not return the same standard error estimates for the
regression estimator of population mean and total as some textbooks, or
SAS.  However, it does give the same standard error estimator as
estimating the mean or total with calibrated weights.
</p>
<p>In particular, under simple random sampling with or without replacement
there is a simple rescaling of the mean squared residual to estimate the
mean squared error of the regression estimator.   The standard error
estimate produced by <code>predict.svyglm</code> has very similar
(asymptotically identical) expected
value to the textbook estimate, and has the advantage of being
applicable when the supplied <code>newdata</code> are not the population mean
of the predictors. The difference is small when the sample size is large, but can be
appreciable for small samples.
</p>
<p>You can obtain the other standard error estimator by calling
<code>predict.svyglm</code> with the covariates set to their estimated (rather
than true) population mean values.
</p>


<h3>Author(s)</h3>

<p>Thomas Lumley</p>


<h3>References</h3>

<p>Lumley T, Scott A (2017) &quot;Fitting Regression Models to Survey Data&quot;
Statistical Science 32: 265-278
</p>


<h3>See Also</h3>

 
<p><code><a href="stats.html#topic+glm">glm</a></code>, which is used to do most of the work.
</p>
<p><code><a href="#topic+regTermTest">regTermTest</a></code>, for multiparameter tests
</p>
<p><code><a href="#topic+calibrate">calibrate</a></code>, for an alternative way to specify regression
estimators of population totals or means
</p>
<p><code><a href="#topic+svyttest">svyttest</a></code> for one-sample and two-sample t-tests.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data(api)


  dstrat&lt;-svydesign(id=~1,strata=~stype, weights=~pw, data=apistrat, fpc=~fpc)
  dclus2&lt;-svydesign(id=~dnum+snum, weights=~pw, data=apiclus2)
  rstrat&lt;-as.svrepdesign(dstrat)
  rclus2&lt;-as.svrepdesign(dclus2)

  summary(svyglm(api00~ell+meals+mobility, design=dstrat))
  summary(svyglm(api00~ell+meals+mobility, design=dclus2))
  summary(svyglm(api00~ell+meals+mobility, design=rstrat))
  summary(svyglm(api00~ell+meals+mobility, design=rclus2))

  ## use quasibinomial, quasipoisson to avoid warning messages
  summary(svyglm(sch.wide~ell+meals+mobility, design=dstrat,
        family=quasibinomial()))


  ## Compare regression and ratio estimation of totals
  api.ratio &lt;- svyratio(~api.stu,~enroll, design=dstrat)
  pop&lt;-data.frame(enroll=sum(apipop$enroll, na.rm=TRUE))
  npop &lt;- nrow(apipop)
  predict(api.ratio, pop$enroll)

  ## regression estimator is less efficient
  api.reg &lt;- svyglm(api.stu~enroll, design=dstrat)
  predict(api.reg, newdata=pop, total=npop)
  ## same as calibration estimator
  svytotal(~api.stu, calibrate(dstrat, ~enroll, pop=c(npop, pop$enroll)))

  ## svyglm can also reproduce the ratio estimator
  api.reg2 &lt;- svyglm(api.stu~enroll-1, design=dstrat,
                    family=quasi(link="identity",var="mu"))
  predict(api.reg2, newdata=pop, total=npop)

  ## higher efficiency by modelling variance better
  api.reg3 &lt;- svyglm(api.stu~enroll-1, design=dstrat,
                    family=quasi(link="identity",var="mu^3"))
  predict(api.reg3, newdata=pop, total=npop)
  ## true value
  sum(apipop$api.stu)

 </code></pre>

<hr>
<h2 id='svygofchisq'>
Test of fit to known probabilities
</h2><span id='topic+svygofchisq'></span>

<h3>Description</h3>

<p>A Rao-Scott-type version of the chi-squared test for goodness of fit to prespecified proportions.  The test statistic is the chi-squared statistic applied to the estimated population table, and the reference distribution is a Satterthwaite approximation: the test statistic divided by the estimated scale is compared to a chi-squared distribution with the estimated df.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svygofchisq(formula, p, design, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svygofchisq_+3A_formula">formula</code></td>
<td>

<p>Formula specifying a single factor variable
</p>
</td></tr>
<tr><td><code id="svygofchisq_+3A_p">p</code></td>
<td>

<p>Vector of probabilities for the categories of the factor, in the correct order (will be rescaled to sum to 1)
</p>
</td></tr>
<tr><td><code id="svygofchisq_+3A_design">design</code></td>
<td>

<p>Survey design object
</p>
</td></tr>
<tr><td><code id="svygofchisq_+3A_...">...</code></td>
<td>

<p>Other arguments to pass to <code><a href="#topic+svytotal">svytotal</a></code>, such as <code>na.rm</code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>htest</code>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+chisq.test">chisq.test</a></code>, <code><a href="#topic+svychisq">svychisq</a></code>, <code><a href="#topic+pchisqsum">pchisqsum</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus2&lt;-svydesign(id=~dnum+snum, fpc=~fpc1+fpc2, data=apiclus2)

true_p &lt;- table(apipop$stype)

svygofchisq(~stype,dclus2,p=true_p)
svygofchisq(~stype,dclus2,p=c(1/3,1/3,1/3))

</code></pre>

<hr>
<h2 id='svyhist'>Histograms and boxplots</h2><span id='topic+svyhist'></span><span id='topic+svyboxplot'></span>

<h3>Description</h3>

<p>Histograms and boxplots weighted by the sampling weights. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svyhist(formula, design, breaks = "Sturges",
      include.lowest = TRUE, right = TRUE, xlab = NULL,
       main = NULL, probability = TRUE, freq = !probability, ...)
svyboxplot(formula, design, all.outliers=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyhist_+3A_formula">formula</code></td>
<td>
<p>One-sided formula for <code>svyhist</code>, two-sided for <code>svyboxplot</code></p>
</td></tr>
<tr><td><code id="svyhist_+3A_design">design</code></td>
<td>
<p>A survey design object</p>
</td></tr>
<tr><td><code id="svyhist_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="svyhist_+3A_main">main</code></td>
<td>
<p>Main title</p>
</td></tr>
<tr><td><code id="svyhist_+3A_probability">probability</code>, <code id="svyhist_+3A_freq">freq</code></td>
<td>
<p>Y-axis is probability density or frequency</p>
</td></tr>
<tr><td><code id="svyhist_+3A_all.outliers">all.outliers</code></td>
<td>
<p>Show all outliers in the boxplot, not just extremes</p>
</td></tr>
<tr><td><code id="svyhist_+3A_breaks">breaks</code>, <code id="svyhist_+3A_include.lowest">include.lowest</code>, <code id="svyhist_+3A_right">right</code></td>
<td>
<p>As for <code><a href="graphics.html#topic+hist">hist</a></code></p>
</td></tr>
<tr><td><code id="svyhist_+3A_...">...</code></td>
<td>
<p>Other arguments to <code><a href="graphics.html#topic+hist">hist</a></code> or <code><a href="graphics.html#topic+bxp">bxp</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The histogram breakpoints are computed as if the sample were a
simple random sample of the same size.
</p>
<p>The grouping variable in <code>svyboxplot</code>, if present, must be a factor.
</p>
<p>The boxplot whiskers go to the maximum and minimum observations or to
1.5 interquartile ranges beyond the end of the box, whichever is
closer. The maximum and minimum are plotted as outliers if they are
beyond the ends of the whiskers, but other outlying points are not
plotted unless <code>all.outliers=TRUE</code>.  <code>svyboxplot</code>
requires a two-sided formula; use <code>variable~1</code> for a single boxplot.
</p>


<h3>Value</h3>

<p>As for <code>hist</code>, except that when <code>probability=FALSE</code>, the return value includes a component
<code>count_scale</code> giving a scale factor between density and
counts, assuming equal bin widths. 
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+svyplot">svyplot</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dstrat &lt;- svydesign(id = ~1, strata = ~stype, weights = ~pw, data = apistrat, 
    fpc = ~fpc)
opar&lt;-par(mfrow=c(1,3))
svyhist(~enroll, dstrat, main="Survey weighted",col="purple",ylim=c(0,1.3e-3))
hist(apistrat$enroll,  main="Sample unweighted",col="purple",prob=TRUE,ylim=c(0,1.3e-3))
hist(apipop$enroll,  main="Population",col="purple",prob=TRUE,ylim=c(0,1.3e-3))

par(mfrow=c(1,1))
svyboxplot(enroll~stype,dstrat,all.outliers=TRUE)
svyboxplot(enroll~1,dstrat)
par(opar)
</code></pre>

<hr>
<h2 id='svyivreg'>
Two-stage least-squares for instrumental variable regression
</h2><span id='topic+svyivreg'></span>

<h3>Description</h3>

<p>Estimates regressions with endogenous covariates using two-stage least squares. The function uses <code>ivreg</code> from the <code>AER</code> package for the main computations, and follows the syntax of that function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svyivreg(formula, design, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyivreg_+3A_formula">formula</code></td>
<td>
<p>formula specification(s) of the regression
relationship and the instruments. See Details for details</p>
</td></tr>
<tr><td><code id="svyivreg_+3A_design">design</code></td>
<td>

<p>A survey design object
</p>
</td></tr>
<tr><td><code id="svyivreg_+3A_...">...</code></td>
<td>

<p>For future expansion
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Regressors and instruments for <code>svyivreg</code> are specified
in a formula with two parts on the right-hand side, e.g., <code>y ~ x1
     + x2 | z1 + z2 + z3</code>, where <code>x1</code> and <code>x2</code> are the regressors and
<code>z1</code>, <code>z2</code>, and <code>z3</code> are the instruments. Note that exogenous
regressors have to be included as instruments for themselves. For
example, if there is one exogenous regressor <code>ex</code> and one
endogenous regressor <code>en</code> with instrument <code>in</code>, the appropriate
formula would be <code>y ~ ex + en | ex + in</code>. Equivalently, this can
be specified as <code>y ~ ex + en | . - en + in</code>, i.e., by providing an
update formula with a <code>.</code> in the second part of the formula. </p>


<h3>Value</h3>

<p>An object of class <code>svyivreg</code>
</p>


<h3>References</h3>

<p><a href="https://notstatschat.rbind.io/2019/07/16/adding-new-functions-to-the-survey-package/">https://notstatschat.rbind.io/2019/07/16/adding-new-functions-to-the-survey-package/</a>
</p>


<h3>See Also</h3>

<p><code><a href="AER.html#topic+ivreg">ivreg</a></code></p>

<hr>
<h2 id='svykappa'>Cohen's kappa for agreement</h2><span id='topic+svykappa'></span>

<h3>Description</h3>

<p>Computes the unweighted kappa measure of agreement between two raters
and the standard error. The measurements must both be factor variables
in the survey design object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svykappa(formula, design, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svykappa_+3A_formula">formula</code></td>
<td>
<p>one-sided formula giving two measurements</p>
</td></tr>
<tr><td><code id="svykappa_+3A_design">design</code></td>
<td>
<p>survey design object</p>
</td></tr>
<tr><td><code id="svykappa_+3A_...">...</code></td>
<td>
<p>passed to <code>svymean</code> internally
(such as <code>return.replicates</code> or <code>influence</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>svystat</code>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+svycontrast">svycontrast</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)
svykappa(~comp.imp+sch.wide, dclus1)

dclus1&lt;-update(dclus1, stypecopy=stype)
svykappa(~stype+stypecopy,dclus1)


(kappas&lt;-svyby(~comp.imp+sch.wide,~stype,design=dclus1, svykappa, covmat=TRUE))
svycontrast(kappas, quote(E/H))

</code></pre>

<hr>
<h2 id='svykm'>Estimate survival function. </h2><span id='topic+svykm'></span><span id='topic+plot.svykm'></span><span id='topic+plot.svykmlist'></span><span id='topic+lines.svykm'></span><span id='topic+quantile.svykm'></span><span id='topic+confint.svykm'></span>

<h3>Description</h3>

<p>Estimates the survival function using a weighted Kaplan-Meier
estimator. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svykm(formula, design,se=FALSE, ...)
## S3 method for class 'svykm'
plot(x,xlab="time",ylab="Proportion surviving",
  ylim=c(0,1),ci=NULL,lty=1,...)
## S3 method for class 'svykm'
lines(x,xlab="time",type="s",ci=FALSE,lty=1,...)
## S3 method for class 'svykmlist'
plot(x, pars=NULL, ci=FALSE,...)
## S3 method for class 'svykm'
quantile(x, probs=c(0.75,0.5,0.25),ci=FALSE,level=0.95,...)
## S3 method for class 'svykm'
confint(object,parm,level=0.95,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svykm_+3A_formula">formula</code></td>
<td>
<p>Two-sided formula. The response variable should be a right-censored
<code>Surv</code> object</p>
</td></tr>
<tr><td><code id="svykm_+3A_design">design</code></td>
<td>
<p>survey design object</p>
</td></tr>
<tr><td><code id="svykm_+3A_se">se</code></td>
<td>
<p>Compute standard errors? This is slow for moderate to large
data sets</p>
</td></tr>
<tr><td><code id="svykm_+3A_...">...</code></td>
<td>
<p>in <code>plot</code> and <code>lines</code> methods, graphical
parameters </p>
</td></tr>
<tr><td><code id="svykm_+3A_x">x</code></td>
<td>
<p>a <code>svykm</code> or <code>svykmlist</code> object</p>
</td></tr>
<tr><td><code id="svykm_+3A_xlab">xlab</code>, <code id="svykm_+3A_ylab">ylab</code>, <code id="svykm_+3A_ylim">ylim</code>, <code id="svykm_+3A_type">type</code></td>
<td>
<p>as for <code>plot</code></p>
</td></tr>
<tr><td><code id="svykm_+3A_lty">lty</code></td>
<td>
<p>Line type, see <code><a href="graphics.html#topic+par">par</a></code></p>
</td></tr>
<tr><td><code id="svykm_+3A_ci">ci</code></td>
<td>
<p>Plot (or return, for<code>quantile</code>) the confidence interval</p>
</td></tr>
<tr><td><code id="svykm_+3A_pars">pars</code></td>
<td>
<p>A list of vectors of graphical parameters for the
separate curves in a <code>svykmlist</code> object</p>
</td></tr>
<tr><td><code id="svykm_+3A_object">object</code></td>
<td>
<p>A <code>svykm</code> object</p>
</td></tr>
<tr><td><code id="svykm_+3A_parm">parm</code></td>
<td>
<p>vector of times to report confidence intervals</p>
</td></tr>
<tr><td><code id="svykm_+3A_level">level</code></td>
<td>
<p>confidence level</p>
</td></tr>
<tr><td><code id="svykm_+3A_probs">probs</code></td>
<td>
<p>survival probabilities for computing survival quantiles
(note that these are the complement of the usual
<code><a href="stats.html#topic+quantile">quantile</a></code> input, so 0.9 means 90% surviving, not 90% dead)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When standard errors are computed, the survival curve is
actually the Aalen (hazard-based) estimator rather than the
Kaplan-Meier estimator. 
</p>
<p>The standard error computations use memory proportional to the sample
size times the square of the number of events. This can be a lot.
</p>
<p>In the case of equal-probability cluster sampling without replacement
the computations are essentially the same as those of Williams (1995),
and the same linearization strategy is used for other designs.
</p>
<p>Confidence intervals are computed on the log(survival) scale,
following the default in <code>survival</code> package, which was based on
simulations by Link(1984).
</p>
<p>Confidence intervals for quantiles use Woodruff's method: the interval
is the intersection of the horizontal line at the specified quantile
with the pointwise confidence band around the survival curve.
</p>


<h3>Value</h3>

<p>For <code>svykm</code>, an object of class <code>svykm</code> for a single curve or <code>svykmlist</code>
for multiple curves.
</p>


<h3>References</h3>

<p>Link, C. L. (1984). Confidence intervals for the survival function using
Cox's proportional hazards model with covariates. Biometrics 40,
601-610.
</p>
<p>Williams RL (1995) &quot;Product-Limit Survival Functions with Correlated 
Survival Times&quot; Lifetime Data Analysis 1: 171&ndash;186
</p>
<p>Woodruff RS (1952) Confidence intervals for medians and other
position measures. JASA 57, 622-627.  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.svycoxph">predict.svycoxph</a></code> for survival curves from a Cox model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(pbc, package="survival")
pbc$randomized &lt;- with(pbc, !is.na(trt) &amp; trt&gt;0)
biasmodel&lt;-glm(randomized~age*edema,data=pbc)
pbc$randprob&lt;-fitted(biasmodel)

dpbc&lt;-svydesign(id=~1, prob=~randprob, strata=~edema, data=subset(pbc,randomized))

s1&lt;-svykm(Surv(time,status&gt;0)~1, design=dpbc)
s2&lt;-svykm(Surv(time,status&gt;0)~I(bili&gt;6), design=dpbc)

plot(s1)
plot(s2)
plot(s2, lwd=2, pars=list(lty=c(1,2),col=c("purple","forestgreen")))

quantile(s1, probs=c(0.9,0.75,0.5,0.25,0.1))

s3&lt;-svykm(Surv(time,status&gt;0)~I(bili&gt;6), design=dpbc,se=TRUE)
plot(s3[[2]],col="purple")

confint(s3[[2]], parm=365*(1:5))
quantile(s3[[1]], ci=TRUE)

</code></pre>

<hr>
<h2 id='svyloglin'>Loglinear models </h2><span id='topic+svyloglin'></span><span id='topic+anova.svyloglin'></span><span id='topic+update.svyloglin'></span><span id='topic+coef.svyloglin'></span><span id='topic+print.anova.svyloglin'></span>

<h3>Description</h3>

<p>Fit and compare hierarchical loglinear models for complex survey data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svyloglin(formula, design, ...)
## S3 method for class 'svyloglin'
update(object,formula,...)
## S3 method for class 'svyloglin'
anova(object,object1,...,integrate=FALSE)
## S3 method for class 'anova.svyloglin'
print(x,pval=c("F","saddlepoint","lincom","chisq"),...)
## S3 method for class 'svyloglin'
coef(object,...,intercept=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyloglin_+3A_formula">formula</code></td>
<td>
<p>Model formula</p>
</td></tr>
<tr><td><code id="svyloglin_+3A_design">design</code></td>
<td>
<p>survey design object</p>
</td></tr>
<tr><td><code id="svyloglin_+3A_object">object</code>, <code id="svyloglin_+3A_object1">object1</code></td>
<td>
<p>loglinear model from <code>svyloglin</code></p>
</td></tr>
<tr><td><code id="svyloglin_+3A_pval">pval</code></td>
<td>
<p>p-value approximation: see Details</p>
</td></tr>
<tr><td><code id="svyloglin_+3A_integrate">integrate</code></td>
<td>
<p>Compute the exact asymptotic p-value (slow)?</p>
</td></tr>
<tr><td><code id="svyloglin_+3A_...">...</code></td>
<td>
<p>not used </p>
</td></tr>
<tr><td><code id="svyloglin_+3A_intercept">intercept</code></td>
<td>
<p>Report the intercept?</p>
</td></tr>
<tr><td><code id="svyloglin_+3A_x">x</code></td>
<td>
<p>anova object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The loglinear model is fitted to a multiway table with probabilities
estimated by <code><a href="#topic+svymean">svymean</a></code> and with the sample size equal to the
observed sample size, treating the resulting table as if it came from iid
multinomial sampling, as described by Rao and Scott. The
variance-covariance matrix does not include the intercept term, and so
by default neither does the <code>coef</code> method.  A Newton-Raphson
algorithm is used, rather than iterative proportional fitting, so
starting values are not needed.
</p>
<p>The <code>anova</code> method computes the quantities that would be the score
(Pearson) and likelihood ratio chi-squared statistics if the data were
an iid sample. It computes four p-values for each of these, based on the
exact asymptotic distribution (see <code><a href="#topic+pchisqsum">pchisqsum</a></code>), a
saddlepoint approximateion to this distribution, a scaled
chi-squared distribution, and a scaled F-distribution.  When testing the
two-way interaction model against the main-effects model in a two-way
table the score statistic and p-values match the Rao-Scott tests
computed by <code><a href="#topic+svychisq">svychisq</a></code>.
</p>
<p>The <code>anova</code> method can only compare two models if they are for
exactly the same multiway table (same variables and same order). The
<code>update</code> method will help with this. It is also much faster to use
<code>update</code> than <code>svyloglin</code> for a large data set: its time
complexity depends only on the size of the model, not on the size of the
data set.
</p>
<p>It is not possible to fit a model using a variable created inline, eg
<code>I(x&lt;10)</code>, since the multiway table is based on all variables used
in the formula. 
</p>


<h3>Value</h3>

<p>Object of class <code>"svyloglin"</code>
</p>


<h3>References</h3>

<p>Rao, JNK, Scott, AJ (1984) &quot;On Chi-squared Tests For Multiway Contingency Tables with Proportions Estimated From Survey Data&quot; Annals of Statistics 12:46-60.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+svychisq">svychisq</a></code>, <code><a href="#topic+svyglm">svyglm</a></code>,<code><a href="#topic+pchisqsum">pchisqsum</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'> data(api)
 dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)
 a&lt;-svyloglin(~stype+comp.imp,dclus1)
 b&lt;-update(a,~.^2)
 an&lt;-anova(a,b)
 an
 print(an, pval="saddlepoint")

 ## Wald test
 regTermTest(b, ~stype:comp.imp)

 ## linear-by-linear association
 d&lt;-update(a,~.+as.numeric(stype):as.numeric(comp.imp))
 an1&lt;-anova(a,d)
 an1


</code></pre>

<hr>
<h2 id='svylogrank'>
Compare survival distributions
</h2><span id='topic+svylogrank'></span>

<h3>Description</h3>

<p>Computes a weighted version of the logrank test for comparing two or more
survival distributions.  The generalization to complex samples is based
on the characterization of the logrank test as the score test in a Cox model.
Under simple random sampling with replacement, this function with
<code>rho=0</code> and <code>gamma=0</code> is almost identical to the robust score test
in the survival package. The <code>rho=0</code> and <code>gamma=0</code> version was
proposed by Rader (2014).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svylogrank(formula, design, rho=0,gamma=0,method=c("small","large","score"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svylogrank_+3A_formula">formula</code></td>
<td>

<p>Model formula with a single predictor. The predictor must be a factor if it has more than two levels.
</p>
</td></tr>
<tr><td><code id="svylogrank_+3A_design">design</code></td>
<td>

<p>A survey design object
</p>
</td></tr>
<tr><td><code id="svylogrank_+3A_rho">rho</code>, <code id="svylogrank_+3A_gamma">gamma</code></td>
<td>
<p>Coefficients for the Harrington/Fleming G-rho-gamma
tests. The default is the logrank test, <code>rho=1</code> gives a
generalised Wilcoxon test</p>
</td></tr>
<tr><td><code id="svylogrank_+3A_method">method</code></td>
<td>
<p><code>"small"</code> works faster when a matrix with dimension
number of events by number of people fits easily in memory;
<code>"large"</code> works faster for large data sets;  <code>"score"</code> works
by brute-force construction of an expanded data set, and is for debugging</p>
</td></tr>
<tr><td><code id="svylogrank_+3A_...">...</code></td>
<td>

<p>for future expansion.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector containing the z-statistic for comparing each level of the variable to the lowest, the chisquared statistic for the logrank test, and the p-value.
</p>


<h3>References</h3>

<p>Rader, Kevin Andrew. 2014. Methods for Analyzing Survival and
Binary Data in Complex Surveys. Doctoral dissertation, Harvard
University.<a href="http://nrs.harvard.edu/urn-3:HUL.InstRepos:12274283">http://nrs.harvard.edu/urn-3:HUL.InstRepos:12274283</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+svykm">svykm</a></code>, <code><a href="#topic+svycoxph">svycoxph</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("survival")
data(nwtco)
## stratified on case status
dcchs&lt;-twophase(id=list(~seqno,~seqno), strata=list(NULL,~rel),
         subset=~I(in.subcohort | rel), data=nwtco, method="simple")
svylogrank(Surv(edrel,rel)~factor(stage),design=dcchs)

data(pbc, package="survival")
pbc$randomized &lt;- with(pbc, !is.na(trt) &amp; trt&gt;0)
biasmodel&lt;-glm(randomized~age*edema,data=pbc)
pbc$randprob&lt;-fitted(biasmodel)
dpbc&lt;-svydesign(id=~1, prob=~randprob, strata=~edema, data=subset(pbc,randomized))

svylogrank(Surv(time,status==2)~trt,design=dpbc)

svylogrank(Surv(time,status==2)~trt,design=dpbc,rho=1)

rpbc&lt;-as.svrepdesign(dpbc)
svylogrank(Surv(time,status==2)~trt,design=rpbc)

</code></pre>

<hr>
<h2 id='svymle'>Maximum pseudolikelihood estimation in complex surveys</h2><span id='topic+svymle'></span><span id='topic+print.svymle'></span><span id='topic+coef.svymle'></span><span id='topic+summary.svymle'></span><span id='topic+vcov.svymle'></span>

<h3>Description</h3>

<p>Maximises a user-specified likelihood parametrised by multiple linear
predictors to data from a complex sample survey and computes the
sandwich variance estimator of the coefficients. Note that this function
maximises an estimated population likelihood, it is not the sample MLE.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svymle(loglike, gradient = NULL, design, formulas, start = NULL, control
= list(), na.action="na.fail", method=NULL, lower=NULL,upper=NULL,influence=FALSE,...)
## S3 method for class 'svymle'
summary(object, stderr=c("robust", "model"),...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svymle_+3A_loglike">loglike</code></td>
<td>
<p>vectorised loglikelihood function</p>
</td></tr>
<tr><td><code id="svymle_+3A_gradient">gradient</code></td>
<td>
<p>Derivative of <code>loglike</code>. Required for variance computation and helpful for fitting</p>
</td></tr>
<tr><td><code id="svymle_+3A_design">design</code></td>
<td>
<p> a <code>survey.design</code> object </p>
</td></tr>
<tr><td><code id="svymle_+3A_formulas">formulas</code></td>
<td>
<p>A list of formulas specifying the variable and linear predictors: see Details below</p>
</td></tr>
<tr><td><code id="svymle_+3A_start">start</code></td>
<td>
<p>Starting values for parameters</p>
</td></tr>
<tr><td><code id="svymle_+3A_control">control</code></td>
<td>
<p>control options for the optimiser: see the help page
for the optimiser you are using.</p>
</td></tr>
<tr><td><code id="svymle_+3A_lower">lower</code>, <code id="svymle_+3A_upper">upper</code></td>
<td>
<p>Parameter bounds for <code>bobyqa</code></p>
</td></tr>
<tr><td><code id="svymle_+3A_influence">influence</code></td>
<td>
<p>Return the influence functions (primarily for svyby)</p>
</td></tr>
<tr><td><code id="svymle_+3A_na.action">na.action</code></td>
<td>
<p>Handling of <code>NA</code>s</p>
</td></tr>
<tr><td><code id="svymle_+3A_method">method</code></td>
<td>
<p><code>"nlm"</code> to use <code>nlm</code>,<code>"uobyqa"</code> or
<code>"bobyqa"</code> to use those optimisers from the <code>minqa</code>
package;  otherwise passed to <code><a href="stats.html#topic+optim">optim</a></code></p>
</td></tr>
<tr><td><code id="svymle_+3A_...">...</code></td>
<td>
<p>Arguments to <code>loglike</code> and <code>gradient</code> that are
not to be optimised over.</p>
</td></tr>
<tr><td><code id="svymle_+3A_object">object</code></td>
<td>
<p><code>svymle</code> object</p>
</td></tr>
<tr><td><code id="svymle_+3A_stderr">stderr</code></td>
<td>
<p>Choice of standard error estimator. The default is a
standard sandwich estimator. See Details below.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Optimization is done by <code><a href="stats.html#topic+nlm">nlm</a></code> by default or if
<code>method=="nlm"</code>. Otherwise <code><a href="stats.html#topic+optim">optim</a></code> is used and <code>method</code>
specifies the method and <code>control</code> specifies control parameters.
</p>
<p>The <code>design</code> object contains all the data and design information
from the survey, so all the formulas refer to variables in this object.
The <code>formulas</code> argument needs to specify the response variable and
a linear predictor for each freely varying argument of <code>loglike</code>.
</p>
<p>Consider for example the <code><a href="stats.html#topic+dnorm">dnorm</a></code> function, with arguments
<code>x</code>, <code>mean</code>, <code>sd</code> and <code>log</code>, and suppose we want to
estimate the mean of <code>y</code> as a linear function of a variable
<code>z</code>, and to estimate a constant standard deviation.  The <code>log</code>
argument must be fixed at <code>FALSE</code> to get the loglikelihood.  A
<code>formulas</code> argument would be <code>list(~y, mean=~z, sd=~1)</code>. Note
that the data variable <code>y</code> must be the first argument to
<code>dnorm</code> and the first formula and that all the other formulas are
labelled.  It is also permitted to have the data variable as the
left-hand side of one of the formulas: eg <code>list( mean=y~z, sd=~1)</code>.
</p>
<p>The two optimisers from the <code>minqa</code> package do not use any
derivatives to be specified for optimisation, but they do assume
that the function is smooth enough for a quadratic approximation, ie,
that two derivatives exist.
</p>
<p>The usual variance estimator for MLEs in a survey sample is a &lsquo;sandwich&rsquo;
variance that requires the score vector and the information matrix. It
requires only sampling assumptions to be valid (though some model
assumptions are required for it to be useful). This is the
<code>stderr="robust"</code> option, which is available only when the <code>gradient</code>
argument was specified.
</p>
<p>If the model is correctly specified and the sampling is at random
conditional on variables in the model then standard errors based on just
the information matrix will be approximately valid.  In particular, for
independent sampling where weights and strata depend on variables in the
model the <code>stderr="model"</code> should work fairly well.
</p>


<h3>Value</h3>

<p>An object of class <code>svymle</code>
</p>


<h3>Author(s)</h3>

<p>Thomas Lumley</p>


<h3>See Also</h3>

<p><code><a href="#topic+svydesign">svydesign</a></code>, <code><a href="#topic+svyglm">svyglm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
 data(api)

 dstrat&lt;-svydesign(id=~1, strata=~stype, weight=~pw, fpc=~fpc, data=apistrat)

 ## fit with glm
 m0 &lt;- svyglm(api00~api99+ell,family="gaussian",design=dstrat)
 ## fit as mle (without gradient)
 m1 &lt;- svymle(loglike=dnorm,gradient=NULL, design=dstrat, 
    formulas=list(mean=api00~api99+ell, sd=~1),
    start=list(c(80,1,0),c(20)), log=TRUE)
 ## with gradient
 gr&lt;- function(x,mean,sd,log){
	 dm&lt;-2*(x - mean)/(2*sd^2)
	 ds&lt;-(x-mean)^2*(2*(2 * sd))/(2*sd^2)^2 - sqrt(2*pi)/(sd*sqrt(2*pi))
         cbind(dm,ds)
      }
 m2 &lt;- svymle(loglike=dnorm,gradient=gr, design=dstrat, 
    formulas=list(mean=api00~api99+ell, sd=~1),
    start=list(c(80,1,0),c(20)), log=TRUE, method="BFGS")

 summary(m0)
 summary(m1,stderr="model")
 summary(m2)

 ## Using offsets
 m3 &lt;- svymle(loglike=dnorm,gradient=gr, design=dstrat, 
    formulas=list(mean=api00~api99+offset(ell)+ell, sd=~1),
    start=list(c(80,1,0),c(20)), log=TRUE, method="BFGS")


## demonstrating multiple linear predictors

 m3 &lt;- svymle(loglike=dnorm,gradient=gr, design=dstrat, 
    formulas=list(mean=api00~api99+offset(ell)+ell, sd=~stype),
    start=list(c(80,1,0),c(20,0,0)), log=TRUE, method="BFGS")



 ## More complicated censored lognormal data example
 ## showing that the response variable can be multivariate

 data(pbc, package="survival")
 pbc$randomized &lt;- with(pbc, !is.na(trt) &amp; trt&gt;0)
 biasmodel&lt;-glm(randomized~age*edema,data=pbc)
 pbc$randprob&lt;-fitted(biasmodel)
 dpbc&lt;-svydesign(id=~1, prob=~randprob, strata=~edema,
    data=subset(pbc,randomized))


## censored logNormal likelihood
 lcens&lt;-function(x,mean,sd){
    ifelse(x[,2]==1,
           dnorm(log(x[,1]),mean,sd,log=TRUE),
           pnorm(log(x[,1]),mean,sd,log=TRUE,lower.tail=FALSE)
           )
 }

 gcens&lt;- function(x,mean,sd){

        dz&lt;- -dnorm(log(x[,1]),mean,sd)/pnorm(log(x[,1]),mean,sd,lower.tail=FALSE)

        dm&lt;-ifelse(x[,2]==1,
                   2*(log(x[,1]) - mean)/(2*sd^2),
                   dz*-1/sd)
        ds&lt;-ifelse(x[,2]==1,
                   (log(x[,1])-mean)^2*(2*(2 * sd))/(2*sd^2)^2 - sqrt(2*pi)/(sd*sqrt(2*pi)),
                   ds&lt;- dz*-(log(x[,1])-mean)/(sd*sd))
        cbind(dm,ds)      
 }

m&lt;-svymle(loglike=lcens, gradient=gcens, design=dpbc, method="newuoa",
      formulas=list(mean=I(cbind(time,status&gt;0))~bili+protime+albumin,
                    sd=~1),
         start=list(c(10,0,0,0),c(1)))

summary(m)

## the same model, but now specifying the lower bound of zero on the
## log standard deviation

mbox&lt;-svymle(loglike=lcens, gradient=gcens, design=dpbc, method="bobyqa",
      formulas=list(mean=I(cbind(time,status&gt;0))~bili+protime+albumin, sd=~1),
      lower=list(c(-Inf,-Inf,-Inf,-Inf),0), upper=Inf,
      start=list(c(10,0,0,0),c(1)))


## The censored lognormal model is now available in svysurvreg()

summary(svysurvreg(Surv(time,status&gt;0)~bili+protime+albumin,
        design=dpbc,dist="lognormal"))

## compare svymle scale value after log transformation
svycontrast(m, quote(log(`sd.(Intercept)`)))



</code></pre>

<hr>
<h2 id='svynls'>
Probability-weighted nonlinear least squares
</h2><span id='topic+svynls'></span>

<h3>Description</h3>

<p>Fits a nonlinear model by probability-weighted least squares.  Uses
<code>nls</code> to do the fitting, but estimates design-based standard errors with either
linearisation or replicate weights. See <code><a href="stats.html#topic+nls">nls</a></code> for 
documentation of model specification and fitting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svynls(formula, design, start, weights=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svynls_+3A_formula">formula</code></td>
<td>

<p>Nonlinear model specified as a formula; see <code><a href="stats.html#topic+nls">nls</a></code>
</p>
</td></tr>
<tr><td><code id="svynls_+3A_design">design</code></td>
<td>

<p>Survey design object
</p>
</td></tr>
<tr><td><code id="svynls_+3A_start">start</code></td>
<td>
<p>starting values, passed to <code><a href="stats.html#topic+nls">nls</a></code></p>
</td></tr>
<tr><td><code id="svynls_+3A_weights">weights</code></td>
<td>

<p>Non-sampling weights, eg precision weights to give more efficient estimation in the presence of heteroscedasticity.
</p>
</td></tr>
<tr><td><code id="svynls_+3A_...">...</code></td>
<td>

<p>Other arguments to <code>nls</code> (especially, <code>start</code>). Also
supports <code>return.replicates</code> for replicate-weight designs and
<code>influence</code> for other designs. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>svynls</code>. The fitted <code>nls</code> object is
included as the <code>fit</code> element.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+svymle">svymle</a></code> for maximum likelihood with linear predictors on
one or more parameters
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(2020-4-3)
x&lt;-rep(seq(0,50,1),10)
y&lt;-((runif(1,10,20)*x)/(runif(1,0,10)+x))+rnorm(510,0,1)

pop_model&lt;-nls(y~a*x/(b+x), start=c(a=15,b=5))

df&lt;-data.frame(x=x,y=y)
df$p&lt;-ifelse((y-fitted(pop_model))*(x-mean(x))&gt;0, .4,.1)

df$strata&lt;-ifelse(df$p==.4,"a","b")

in_sample&lt;-stratsample(df$strata, round(table(df$strat)*c(0.4,0.1)))

sdf&lt;-df[in_sample,]
des&lt;-svydesign(id=~1, strata=~strata, prob=~p, data=sdf)
pop_model
(biased_sample&lt;-nls(y~a*x/(b+x),data=sdf, start=c(a=15,b=5)))
(corrected &lt;- svynls(y~a*x/(b+x), design=des, start=c(a=15,b=5)))
</code></pre>

<hr>
<h2 id='svyolr'>Proportional odds and related models </h2><span id='topic+svyolr'></span><span id='topic+svyolr.survey.design2'></span><span id='topic+svyolr.svyrep.design'></span><span id='topic+predict.svyolr'></span>

<h3>Description</h3>

<p>Fits cumulative link models: proportional odds, probit, complementary
log-log, and cauchit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svyolr(formula, design, ...)
## S3 method for class 'survey.design2'
svyolr(formula, design, start, subset=NULL,...,
    na.action = na.omit,method = c("logistic", "probit", "cloglog", "cauchit"))
## S3 method for class 'svyrep.design'
svyolr(formula,design,subset=NULL,...,return.replicates=FALSE, 
    multicore=getOption("survey.multicore"))
## S3 method for class 'svyolr'
predict(object, newdata, type = c("class", "probs"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyolr_+3A_formula">formula</code></td>
<td>
<p>Formula: the response must be a factor with at least
three levels</p>
</td></tr>
<tr><td><code id="svyolr_+3A_design">design</code></td>
<td>
<p>survey design object </p>
</td></tr>
<tr><td><code id="svyolr_+3A_subset">subset</code></td>
<td>
<p>subset of the design to use; <code>NULL</code> for all of it</p>
</td></tr>
<tr><td><code id="svyolr_+3A_...">...</code></td>
<td>
<p>dots</p>
</td></tr>
<tr><td><code id="svyolr_+3A_start">start</code></td>
<td>
<p>Optional starting values for optimization</p>
</td></tr>
<tr><td><code id="svyolr_+3A_na.action">na.action</code></td>
<td>
<p>handling of missing values</p>
</td></tr>
<tr><td><code id="svyolr_+3A_multicore">multicore</code></td>
<td>
<p>Use <code>multicore</code> package to distribute computation of replicates across multiple
processors?</p>
</td></tr>			   
<tr><td><code id="svyolr_+3A_method">method</code></td>
<td>
<p>Link function</p>
</td></tr>
<tr><td><code id="svyolr_+3A_return.replicates">return.replicates</code></td>
<td>
<p>return the individual replicate-weight
estimates</p>
</td></tr>
<tr><td><code id="svyolr_+3A_object">object</code></td>
<td>
<p>object of class <code>svyolr</code></p>
</td></tr>
<tr><td><code id="svyolr_+3A_newdata">newdata</code></td>
<td>
<p>new data for predictions</p>
</td></tr>
<tr><td><code id="svyolr_+3A_type">type</code></td>
<td>
<p>return vector of most likely class or matrix of probabilities</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>svyolr</code>
</p>


<h3>Author(s)</h3>

<p>The code is based closely on polr() from the MASS package of
Venables and Ripley.</p>


<h3>See Also</h3>

<p><code><a href="#topic+svyglm">svyglm</a></code>, <code><a href="#topic+regTermTest">regTermTest</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)
dclus1&lt;-update(dclus1, mealcat=cut(meals,c(0,25,50,75,100)))

m&lt;-svyolr(mealcat~avg.ed+mobility+stype, design=dclus1)
m

## Use regTermTest for testing multiple parameters
regTermTest(m, ~avg.ed+stype, method="LRT")

## predictions
summary(predict(m, newdata=apiclus2))
summary(predict(m, newdata=apiclus2, type="probs"))
</code></pre>

<hr>
<h2 id='svyplot'>Plots for survey data </h2><span id='topic+svyplot'></span><span id='topic+svyplot.default'></span>

<h3>Description</h3>

<p>Because observations in survey samples may represent very different
numbers of units in the population ordinary plots can be misleading.
The <code>svyplot</code> function produces scatterplots adjusted in various ways
for sampling weights. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svyplot(formula, design,...)
## Default S3 method:
svyplot(formula, design, style = c("bubble", "hex", "grayhex","subsample","transparent"),
sample.size = 500, subset = NULL, legend = 1, inches = 0.05,
amount=NULL, basecol="black",
alpha=c(0, 0.8),xbins=30,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyplot_+3A_formula">formula</code></td>
<td>
<p>A model formula</p>
</td></tr>
<tr><td><code id="svyplot_+3A_design">design</code></td>
<td>
<p> A survey object (svydesign or svrepdesign)</p>
</td></tr>
<tr><td><code id="svyplot_+3A_style">style</code></td>
<td>
<p>See Details below</p>
</td></tr>
<tr><td><code id="svyplot_+3A_sample.size">sample.size</code></td>
<td>
<p>For <code>style="subsample"</code></p>
</td></tr>
<tr><td><code id="svyplot_+3A_subset">subset</code></td>
<td>
<p>expression using variables in the design object</p>
</td></tr>
<tr><td><code id="svyplot_+3A_legend">legend</code></td>
<td>
<p>For <code>style="hex"</code> or <code>"grayhex"</code></p>
</td></tr>
<tr><td><code id="svyplot_+3A_inches">inches</code></td>
<td>
<p>Scale for bubble plots</p>
</td></tr>
<tr><td><code id="svyplot_+3A_amount">amount</code></td>
<td>
<p>list with <code>x</code> and <code>y</code> components for amount of
jittering to use in subsample plots, or <code>NULL</code> for the default
amount</p>
</td></tr>
<tr><td><code id="svyplot_+3A_basecol">basecol</code></td>
<td>
<p>base color for transparent plots, or a function to
compute the color (see below), or color for bubble plots</p>
</td></tr>
<tr><td><code id="svyplot_+3A_alpha">alpha</code></td>
<td>
<p>minimum and maximum opacity for transparent plots</p>
</td></tr>
<tr><td><code id="svyplot_+3A_xbins">xbins</code></td>
<td>
<p>Number of (x-axis) bins for hexagonal binning</p>
</td></tr>
<tr><td><code id="svyplot_+3A_...">...</code></td>
<td>
<p>Passed to <code>plot</code> methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Bubble plots are scatterplots with circles whose area is proportional
to the sampling weight.  The two &quot;hex&quot; styles produce hexagonal
binning scatterplots, and require the <code>hexbin</code> package from
Bioconductor. The &quot;transparent&quot; style plots points with opacity
proportional to sampling weight.
</p>
<p>The <code>subsample</code> method uses the sampling weights to create a
sample from approximately the population distribution and passes this to <code><a href="base.html#topic+plot">plot</a></code>
</p>
<p>Bubble plots are suited to small surveys, hexagonal binning and
transparency to large surveys where plotting all the points would
result in too much overlap.
</p>
<p><code>basecol</code> can be a function taking one data frame argument, which
will be passed the data frame of variables from the survey object.
This could be memory-intensive for large data sets.
</p>


<h3>Value</h3>

<p>None
</p>


<h3>References</h3>

<p>Korn EL, Graubard BI (1998) &quot;Scatterplots with Survey Data&quot; The American Statistician 52: 58-69
</p>
<p>Lumley T, Scott A (2017) &quot;Fitting Regression Models to Survey Data&quot;
Statistical Science 32: 265-278
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+symbols">symbols</a></code> for other options (such as colour) for bubble
plots.
</p>
<p><code><a href="#topic+svytable">svytable</a></code> for plots of discrete data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dstrat&lt;-svydesign(id=~1,strata=~stype, weights=~pw, data=apistrat, fpc=~fpc)

svyplot(api00~api99, design=dstrat, style="bubble")
svyplot(api00~api99, design=dstrat, style="transparent",pch=19)

## these two require the hexbin package 
svyplot(api00~api99, design=dstrat, style="hex", xlab="1999 API",ylab="2000 API")
svyplot(api00~api99, design=dstrat, style="grayhex",legend=0)


dclus2&lt;-svydesign(id=~dnum+snum,  weights=~pw,
                    data=apiclus2, fpc=~fpc1+fpc2)
svyplot(api00~api99, design=dclus2, style="subsample")
svyplot(api00~api99, design=dclus2, style="subsample",
          amount=list(x=25,y=25))

svyplot(api00~api99, design=dstrat,
  basecol=function(df){c("goldenrod","tomato","sienna")[as.numeric(df$stype)]},
  style="transparent",pch=19,alpha=c(0,1))
legend("topleft",col=c("goldenrod","tomato","sienna"), pch=19, legend=c("E","H","M"))

## For discrete data, estimate a population table and plot the table.
plot(svytable(~sch.wide+comp.imp+stype,design=dstrat))
fourfoldplot(svytable(~sch.wide+comp.imp+stype,design=dstrat,round=TRUE))


## To draw on a hexbin plot you need grid graphics, eg,
library(grid)
h&lt;-svyplot(api00~api99, design=dstrat, style="hex", xlab="1999 API",ylab="2000 API")
s&lt;-svysmooth(api00~api99,design=dstrat)
grid.polyline(s$api99$x,s$api99$y,vp=h$plot.vp@hexVp.on,default.units="native", 
   gp=gpar(col="red",lwd=2))
</code></pre>

<hr>
<h2 id='svyprcomp'>
Sampling-weighted principal component analysis
</h2><span id='topic+svyprcomp'></span><span id='topic+biplot.svyprcomp'></span>

<h3>Description</h3>

<p>Computes principal components using the sampling weights. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svyprcomp(formula, design, center = TRUE, scale. = FALSE, tol = NULL, scores = FALSE, ...)
## S3 method for class 'svyprcomp'
biplot(x, cols=c("black","darkred"),xlabs=NULL,
   weight=c("transparent","scaled","none"),
  max.alpha=0.5,max.cex=0.5,xlim=NULL,ylim=NULL,pc.biplot=FALSE,
  expand=1,xlab=NULL,ylab=NULL, arrow.len=0.1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyprcomp_+3A_formula">formula</code></td>
<td>

<p>model formula describing variables to be used
</p>
</td></tr>
<tr><td><code id="svyprcomp_+3A_design">design</code></td>
<td>

<p>survey design object.
</p>
</td></tr>
<tr><td><code id="svyprcomp_+3A_center">center</code></td>
<td>

<p>Center data before analysis?
</p>
</td></tr>
<tr><td><code id="svyprcomp_+3A_scale.">scale.</code></td>
<td>

<p>Scale to unit variance before analysis?
</p>
</td></tr>
<tr><td><code id="svyprcomp_+3A_tol">tol</code></td>
<td>

<p>Tolerance for omitting components from the results; a proportion of the standard deviation of the first component.  The default is to keep all components.
</p>
</td></tr>
<tr><td><code id="svyprcomp_+3A_scores">scores</code></td>
<td>

<p>Return scores on each component? These are needed for <code>biplot</code>.
</p>
</td></tr>
<tr><td><code id="svyprcomp_+3A_x">x</code></td>
<td>

<p>A <code>svyprcomp</code> object
</p>
</td></tr>
<tr><td><code id="svyprcomp_+3A_cols">cols</code></td>
<td>

<p>Base colors for observations and variables respectively
</p>
</td></tr>
<tr><td><code id="svyprcomp_+3A_xlabs">xlabs</code></td>
<td>

<p>Formula, or character vector, giving labels for each observation
</p>
</td></tr>
<tr><td><code id="svyprcomp_+3A_weight">weight</code></td>
<td>

<p>How to display the sampling weights: <code>"scaled"</code> changes the size of the point label, <code>"transparent"</code> uses opacity proportional to sampling weight, <code>"none"</code> changes neither. 
</p>
</td></tr>
<tr><td><code id="svyprcomp_+3A_max.alpha">max.alpha</code></td>
<td>

<p>Opacity for the largest sampling weight, or for all points if <code>weight!="transparent"</code>
</p>
</td></tr>
<tr><td><code id="svyprcomp_+3A_max.cex">max.cex</code></td>
<td>

<p>Character size (as a multiple of <code>par("cex")</code>) for the largest sampling weight, or for all points if <code>weight!="scaled"</code>
</p>
</td></tr>
<tr><td><code id="svyprcomp_+3A_xlim">xlim</code>, <code id="svyprcomp_+3A_ylim">ylim</code>, <code id="svyprcomp_+3A_xlab">xlab</code>, <code id="svyprcomp_+3A_ylab">ylab</code></td>
<td>
<p>Graphical parameters</p>
</td></tr>
<tr><td><code id="svyprcomp_+3A_expand">expand</code>, <code id="svyprcomp_+3A_arrow.len">arrow.len</code></td>
<td>
<p>See <code><a href="stats.html#topic+biplot">biplot</a></code></p>
</td></tr>
<tr><td><code id="svyprcomp_+3A_pc.biplot">pc.biplot</code></td>
<td>
<p>See <code>link{biplot.prcomp}</code></p>
</td></tr>
<tr><td><code id="svyprcomp_+3A_...">...</code></td>
<td>
	    
<p>Other arguments to <code><a href="stats.html#topic+prcomp">prcomp</a></code>, or graphical parameters for <code>biplot</code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>svyprcomp</code> returns an object of class <code>svyprcomp</code>, similar to
class <code>prcomp</code> but including design information
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+prcomp">prcomp</a></code>, <code><a href="stats.html#topic+biplot.prcomp">biplot.prcomp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus2&lt;-svydesign(id=~dnum+snum, fpc=~fpc1+fpc2, data=apiclus2)

pc &lt;- svyprcomp(~api99+api00+ell+hsg+meals+emer, design=dclus2,scale=TRUE,scores=TRUE)
pc
biplot(pc, xlabs=~dnum, weight="none")

biplot(pc, xlabs=~dnum,max.alpha=1)

biplot(pc, weight="scaled",max.cex=1.5, xlabs=~dnum)

</code></pre>

<hr>
<h2 id='svypredmeans'>
Predictive marginal means
</h2><span id='topic+svypredmeans'></span>

<h3>Description</h3>

<p>Predictive marginal means for a generalised linear model, using the method of Korn and Graubard (1999) and matching the results of SUDAAN. The predictive marginal mean for one level of a factor is the probability-weighted average of the fitted values for the model on new data where all the observations are set to that level of the factor but have whatever values of adjustment variables they really have.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svypredmeans(adjustmodel, groupfactor, predictat=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svypredmeans_+3A_adjustmodel">adjustmodel</code></td>
<td>

<p>A generalised linear model fit by <code><a href="#topic+svyglm">svyglm</a></code> with the adjustment variable but without the factor for which predictive means are wanted
</p>
</td></tr>
<tr><td><code id="svypredmeans_+3A_groupfactor">groupfactor</code></td>
<td>

<p>A one-sided formula specifying the factor for which predictive means are wanted. Can use, eg, <code>~interaction(race,sex)</code> for combining variables.
This does not have to be a factor, but it will be modelled linearly if it isn't</p>
</td></tr>
<tr><td><code id="svypredmeans_+3A_predictat">predictat</code></td>
<td>
<p>A vector of the values of <code>groupfactor</code> where you want predictions.  If <code>groupfactor</code> is a factor, these must be values in the data, but if it is numeric you can interpolate/extrapolate</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>svystat</code> with the predictive marginal means and their covariance matrix. 
</p>


<h3>Note</h3>

<p>It is possible to supply an adjustment model with only an intercept, but the results are then the same as <code><a href="#topic+svymean">svymean</a></code>
</p>
<p>It makes no sense to have a variable in the adjustment model that is part of the grouping factor, and will give an error message or <code>NA</code>.
</p>


<h3>References</h3>

<p>Graubard B, Korn E (1999) &quot;Predictive Margins with Survey Data&quot; Biometrics 55:652-659
</p>
<p>Bieler, Brown, Williams, &amp; Brogan (2010) &quot;Estimating Model-Adjusted Risks, Risk Differences, and Risk Ratios From Complex Survey Data&quot; Am J Epi DOI: 10.1093/aje/kwp440</p>


<h3>See Also</h3>

<p><code><a href="#topic+svyglm">svyglm</a></code>
</p>
<p>Worked example using National Health Interview Survey data: <a href="https://gist.github.com/tslumley/2e74cd0ac12a671d2724">https://gist.github.com/tslumley/2e74cd0ac12a671d2724</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(nhanes)
nhanes_design &lt;- svydesign(id=~SDMVPSU, strata=~SDMVSTRA, weights=~WTMEC2YR, nest=TRUE,data=nhanes)
agesexmodel&lt;-svyglm(HI_CHOL~agecat+RIAGENDR, design=nhanes_design,family=quasibinomial)
## high cholesterol by race/ethnicity, adjusted for demographic differences
means&lt;-svypredmeans(agesexmodel, ~factor(race))
means
## relative risks compared to non-Hispanic white
svycontrast(means,quote(`1`/`2`))
svycontrast(means,quote(`3`/`2`))

data(api)
dstrat&lt;-svydesign(id=~1,strata=~stype, weights=~pw, data=apistrat, fpc=~fpc)
demog_model &lt;- svyglm(api00~mobility+ell+hsg+meals, design=dstrat)
svypredmeans(demog_model,~enroll, predictat=c(100,300,1000,3000))

</code></pre>

<hr>
<h2 id='svyqqplot'>
Quantile-quantile plots for survey data
</h2><span id='topic+svyqqplot'></span><span id='topic+svyqqmath'></span>

<h3>Description</h3>

<p>Quantile-quantile plots either against a specified distribution function or comparing two variables from the same or different designs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svyqqplot(formula, design, designx = NULL, na.rm = TRUE, qrule = "hf8",
    xlab = NULL, ylab = NULL, ...)
svyqqmath(x, design, null=qnorm, na.rm=TRUE, xlab="Expected",ylab="Observed",...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyqqplot_+3A_x">x</code>, <code id="svyqqplot_+3A_formula">formula</code></td>
<td>

<p>A one-sided formula for <code>svyqqmath</code> or a two-sided formula for <code>svyqqplot</code>
</p>
</td></tr>
<tr><td><code id="svyqqplot_+3A_design">design</code></td>
<td>

<p>Survey design object to look up variables
</p>
</td></tr>
<tr><td><code id="svyqqplot_+3A_designx">designx</code></td>
<td>

<p>Survey design object to look up the RHS variable in <code>svyqqplot</code>, if
different from the LHS variable
</p>
</td></tr>
<tr><td><code id="svyqqplot_+3A_null">null</code></td>
<td>
<p>Quantile function to compare the data quantiles to</p>
</td></tr>
<tr><td><code id="svyqqplot_+3A_na.rm">na.rm</code></td>
<td>

<p>Remove missing values
</p>
</td></tr>
<tr><td><code id="svyqqplot_+3A_qrule">qrule</code></td>
<td>

<p>How to define quantiles for <code>svyqqplot</code> &ndash; see
<code><a href="#topic+svyquantile">svyquantile</a></code> for possible values
</p>
</td></tr>
<tr><td><code id="svyqqplot_+3A_xlab">xlab</code>, <code id="svyqqplot_+3A_ylab">ylab</code></td>
<td>

<p>Passed to <code>plot</code>. For <code>svyqqplot</code>, if these are  <code>NULL</code>
they are replaced by the variable names
</p>
</td></tr>
<tr><td><code id="svyqqplot_+3A_...">...</code></td>
<td>

<p>Graphical options to be passed to <code>plot</code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+quantile">quantile</a></code>
<code><a href="stats.html#topic+qqnorm">qqnorm</a></code>
<code><a href="stats.html#topic+qqplot">qqplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)

dstrat&lt;-svydesign(id=~1,strata=~stype, weights=~pw, data=apistrat,
fpc=~fpc)

svyqqmath(~api99, design=dstrat)
svyqqplot(api00~api99, design=dstrat)

dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)
opar&lt;-par(mfrow=c(1,2))

## sample distributions very different
qqplot(apiclus1$enroll, apistrat$enroll); abline(0,1)

## estimated population distributions much more similar
svyqqplot(enroll~enroll, design=dstrat,designx=dclus1,qrule=survey:::qrule_hf8); abline(0,1)
par(opar)

</code></pre>

<hr>
<h2 id='svyranktest'>
Design-based rank tests
</h2><span id='topic+svyranktest'></span>

<h3>Description</h3>

<p>Design-based versions of k-sample rank tests.  The built-in tests are
all for location hypotheses, but the user could specify others.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svyranktest(formula, design, 
  test = c("wilcoxon", "vanderWaerden", "median","KruskalWallis"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyranktest_+3A_formula">formula</code></td>
<td>

<p>Model formula <code>y~g</code> for outcome variable <code>y</code> and group <code>g</code>
</p>
</td></tr>
<tr><td><code id="svyranktest_+3A_design">design</code></td>
<td>

<p>A survey design object
</p>
</td></tr>
<tr><td><code id="svyranktest_+3A_test">test</code></td>
<td>

<p>Which rank test to use: Wilcoxon, van der Waerden's normal-scores
test, Mood's test for the median, or a function <code>f(r,N)</code> where
<code>r</code> is the rank and <code>N</code> the estimated population
size. &quot;KruskalWallis&quot; is a synonym for &quot;wilcoxon&quot; for more than two groups.
</p>
</td></tr>
<tr><td><code id="svyranktest_+3A_...">...</code></td>
<td>
<p> for future expansion</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These tests are for the null hypothesis that the population or superpopulation distributions of the response variable are different between groups, targeted at population or superpopulation alternatives. 
The 'ranks' are defined as quantiles of the pooled distribution of the variable, so they do not just go from 1 to N; the null hypothesis does not depend on the weights, but the ranks do.   
</p>
<p>The tests reduce to the usual Normal approximations to the usual rank tests under iid sampling.  Unlike the traditional rank tests, they are not exact in small samples. 
</p>


<h3>Value</h3>

<p>Object of class <code>htest</code>
</p>
<p>Note that with more than two groups the <code>statistic</code> element of the return value holds the numerator degrees of freedom and the <code>parameter</code> element holds the test statistic. 
</p>


<h3>References</h3>

<p>Lumley, T., &amp; Scott, A. J. (2013). Two-sample rank tests under complex sampling. BIOMETRIKA, 100 (4), 831-842. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+svyttest">svyttest</a></code>, <code><a href="#topic+svylogrank">svylogrank</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(api)
dclus1&lt;-svydesign(id=~dnum, weights=~pw, fpc=~fpc, data=apiclus1)

svyranktest(ell~comp.imp, dclus1)
svyranktest(ell~comp.imp, dclus1, test="median")


svyranktest(ell~stype, dclus1)
svyranktest(ell~stype, dclus1, test="median")

str(svyranktest(ell~stype, dclus1))

## upper quartile
svyranktest(ell~comp.imp, dclus1, test=function(r,N) as.numeric(r&gt;0.75*N))


quantiletest&lt;-function(p){
	  rval&lt;-function(r,N) as.numeric(r&gt;(N*p))
	  attr(rval,"name")&lt;-paste(p,"quantile")
	  rval
	}
svyranktest(ell~comp.imp, dclus1, test=quantiletest(0.5))
svyranktest(ell~comp.imp, dclus1, test=quantiletest(0.75))

## replicate weights

rclus1&lt;-as.svrepdesign(dclus1)
svyranktest(ell~stype, rclus1)


</code></pre>

<hr>
<h2 id='svyratio'>Ratio estimation</h2><span id='topic+svyratio'></span><span id='topic+print.svyratio'></span><span id='topic+print.svyratio_separate'></span><span id='topic+svyratio.svyrep.design'></span><span id='topic+svyratio.survey.design'></span><span id='topic+svyratio.survey.design2'></span><span id='topic+svyratio.twophase'></span><span id='topic+coef.svyratio'></span><span id='topic+SE.svyratio'></span><span id='topic+predict.svyratio'></span><span id='topic+predict.svyratio_separate'></span><span id='topic+confint.svyratio'></span>

<h3>Description</h3>

<p>Ratio estimation and estimates of totals based on ratios for complex
survey samples. Estimating domain (subpopulation) means can be done
more easily with <code><a href="#topic+svymean">svymean</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survey.design2'
svyratio(numerator=formula, denominator,
   design,separate=FALSE, na.rm=FALSE,formula, covmat=FALSE,
   deff=FALSE,influence=FALSE,...)
## S3 method for class 'svyrep.design'
svyratio(numerator=formula, denominator, design,
   na.rm=FALSE,formula, covmat=FALSE,return.replicates=FALSE,deff=FALSE, ...)
## S3 method for class 'twophase'
svyratio(numerator=formula, denominator, design,
    separate=FALSE, na.rm=FALSE,formula,...)
## S3 method for class 'svyratio'
predict(object, total, se=TRUE,...)
## S3 method for class 'svyratio_separate'
predict(object, total, se=TRUE,...)
## S3 method for class 'svyratio'
SE(object,...,drop=TRUE)
## S3 method for class 'svyratio'
coef(object,...,drop=TRUE)
## S3 method for class 'svyratio'
confint(object,  parm, level = 0.95,df =Inf,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyratio_+3A_numerator">numerator</code>, <code id="svyratio_+3A_formula">formula</code></td>
<td>
<p>formula, expression, or data frame giving numerator variable(s)</p>
</td></tr>
<tr><td><code id="svyratio_+3A_denominator">denominator</code></td>
<td>
<p>formula, expression, or data frame giving
denominator variable(s) </p>
</td></tr>
<tr><td><code id="svyratio_+3A_design">design</code></td>
<td>
<p>survey design object</p>
</td></tr>
<tr><td><code id="svyratio_+3A_object">object</code></td>
<td>
<p>result of <code>svyratio</code></p>
</td></tr>
<tr><td><code id="svyratio_+3A_total">total</code></td>
<td>
<p>vector of population totals for the denominator variables in
<code>object</code>, or list of vectors of 
population stratum totals if <code>separate=TRUE</code></p>
</td></tr>
<tr><td><code id="svyratio_+3A_se">se</code></td>
<td>
<p>Return standard errors?</p>
</td></tr>
<tr><td><code id="svyratio_+3A_separate">separate</code></td>
<td>
<p>Estimate ratio separately for strata</p>
</td></tr>
<tr><td><code id="svyratio_+3A_na.rm">na.rm</code></td>
<td>
<p>Remove missing values?</p>
</td></tr>
<tr><td><code id="svyratio_+3A_covmat">covmat</code></td>
<td>
<p>Compute the full variance-covariance matrix of the
ratios</p>
</td></tr>
<tr><td><code id="svyratio_+3A_deff">deff</code></td>
<td>
<p>Compute design effects</p>
</td></tr>
<tr><td><code id="svyratio_+3A_return.replicates">return.replicates</code></td>
<td>
<p>Return replicate estimates of ratios</p>
</td></tr>
<tr><td><code id="svyratio_+3A_influence">influence</code></td>
<td>
<p>Return influence functions</p>
</td></tr>
<tr><td><code id="svyratio_+3A_drop">drop</code></td>
<td>
<p>Return a vector rather than a matrix</p>
</td></tr>
<tr><td><code id="svyratio_+3A_parm">parm</code></td>
<td>
<p>a specification of which parameters are to be given
confidence intervals, either a vector of numbers or a vector of
names. If missing, all parameters are considered.</p>
</td></tr>
<tr><td><code id="svyratio_+3A_level">level</code></td>
<td>
<p>the confidence level required.</p>
</td></tr>
<tr><td><code id="svyratio_+3A_df">df</code></td>
<td>
<p>degrees of freedom for t-distribution in confidence
interval, use <code>degf(design)</code> for number of PSUs minus number of
strata</p>
</td></tr>  <tr><td><code id="svyratio_+3A_...">...</code></td>
<td>
<p>Other unused arguments for other methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The separate ratio estimate of a total is the sum of ratio estimates
in each stratum.  If the stratum totals supplied in the <code>total</code>
argument and the strata in the design object both have names these
names will be matched. If they do not have names it is important that
the sample totals are supplied in the correct order, the same order
as shown in the output of <code>summary(design)</code>.
</p>
<p>When <code>design</code> is a two-phase design, stratification will be on
the second phase.
</p>


<h3>Value</h3>

<p><code>svyratio</code> returns an object of class <code>svyratio</code>. The
<code>predict</code> method returns a matrix of population totals and
optionally a matrix of standard errors.
</p>


<h3>Author(s)</h3>

<p>Thomas Lumley</p>


<h3>References</h3>

<p>Levy and Lemeshow. &quot;Sampling of Populations&quot; (3rd edition). Wiley</p>


<h3>See Also</h3>

<p><code><a href="#topic+svydesign">svydesign</a></code>
</p>
<p><code><a href="#topic+svymean">svymean</a></code> for estimating proportions and domain means
</p>
<p><code><a href="#topic+calibrate">calibrate</a></code> for estimators related to the separate ratio estimator.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(scd)

## survey design objects
scddes&lt;-svydesign(data=scd, prob=~1, id=~ambulance, strata=~ESA,
nest=TRUE, fpc=rep(5,6))
scdnofpc&lt;-svydesign(data=scd, prob=~1, id=~ambulance, strata=~ESA,
nest=TRUE)

# convert to BRR replicate weights
scd2brr &lt;- as.svrepdesign(scdnofpc, type="BRR")

# use BRR replicate weights from Levy and Lemeshow
repweights&lt;-2*cbind(c(1,0,1,0,1,0), c(1,0,0,1,0,1), c(0,1,1,0,0,1),
c(0,1,0,1,1,0))
scdrep&lt;-svrepdesign(data=scd, type="BRR", repweights=repweights)

# ratio estimates
svyratio(~alive, ~arrests, design=scddes)
svyratio(~alive, ~arrests, design=scdnofpc)
svyratio(~alive, ~arrests, design=scd2brr)
svyratio(~alive, ~arrests, design=scdrep)


data(api)
dstrat&lt;-svydesign(id=~1,strata=~stype, weights=~pw, data=apistrat, fpc=~fpc)

## domain means are ratio estimates, but available directly
svyratio(~I(api.stu*(comp.imp=="Yes")), ~as.numeric(comp.imp=="Yes"), dstrat)
svymean(~api.stu, subset(dstrat, comp.imp=="Yes"))

## separate and combined ratio estimates of total
(sep&lt;-svyratio(~api.stu,~enroll, dstrat,separate=TRUE))
(com&lt;-svyratio(~api.stu, ~enroll, dstrat))

stratum.totals&lt;-list(E=1877350, H=1013824, M=920298)

predict(sep, total=stratum.totals)
predict(com, total=sum(unlist(stratum.totals)))

SE(com)
coef(com)
coef(com, drop=FALSE)
confint(com)
</code></pre>

<hr>
<h2 id='svyrecvar'>Variance estimation for multistage surveys</h2><span id='topic+svyrecvar'></span><span id='topic+multistage'></span><span id='topic+multistage_rcpp'></span>

<h3>Description</h3>

<p>Compute the variance of a total under multistage sampling, using a
recursive descent algorithm.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svyrecvar(x, clusters, stratas,fpcs, postStrata = NULL,
lonely.psu = getOption("survey.lonely.psu"),
one.stage=getOption("survey.ultimate.cluster"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyrecvar_+3A_x">x</code></td>
<td>
<p>Matrix of data or estimating functions</p>
</td></tr>
<tr><td><code id="svyrecvar_+3A_clusters">clusters</code></td>
<td>
<p>Data frame or matrix with cluster ids for each stage</p>
</td></tr>
<tr><td><code id="svyrecvar_+3A_stratas">stratas</code></td>
<td>
<p>Strata for each stage </p>
</td></tr>
<tr><td><code id="svyrecvar_+3A_fpcs">fpcs</code></td>
<td>
<p>Information on population and sample size for each stage,
created by <code><a href="#topic+as.fpc">as.fpc</a></code></p>
</td></tr>
<tr><td><code id="svyrecvar_+3A_poststrata">postStrata</code></td>
<td>
<p>post-stratification information as created by
<code><a href="#topic+postStratify">postStratify</a></code> or <code><a href="#topic+calibrate">calibrate</a></code> </p>
</td></tr>
<tr><td><code id="svyrecvar_+3A_lonely.psu">lonely.psu</code></td>
<td>
<p>How to handle strata with a single PSU</p>
</td></tr>
<tr><td><code id="svyrecvar_+3A_one.stage">one.stage</code></td>
<td>
<p>If <code>TRUE</code>, compute a one-stage
(ultimate-cluster) estimator</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The main use of this function is to compute the variance of the sum
of a set of estimating functions under multistage sampling.  The
sampling is assumed to be simple or stratified random sampling within
clusters at each stage except perhaps the last stage.  The variance of
a statistic is computed from the variance of estimating functions as
described by Binder (1983).
</p>
<p>Use <code>one.stage=FALSE</code> for compatibility with other software that
does not perform multi-stage calculations, and set
<code>options(survey.ultimate.cluster=TRUE)</code> to make this the default.
</p>
<p>The idea of a recursive algorithm is due to Bellhouse (1985).
Texts such as Cochran (1977) and Sarndal et al (1991) describe the
decomposition of the variance into a single-stage between-cluster
estimator and a within-cluster estimator, and this is applied recursively.
</p>
<p>If <code>one.stage</code> is a positive integer it specifies the number of
stages of sampling to use in the recursive estimator.
</p>
<p>If <code>pps="brewer"</code>, standard errors are estimated using Brewer's
approximation for PPS without replacement, option 2 of those described
by Berger (2004). The <code>fpc</code> argument must then be specified in
terms of sampling fractions, not population sizes (or omitted, but
then the <code>pps</code> argument would have no effect and the
with-replacement standard errors would be correct).
</p>


<h3>Value</h3>

<p>A covariance matrix
</p>


<h3>Note</h3>

<p>A simple set of finite population corrections will only be exactly
correct when each successive stage uses simple or stratified random
sampling without replacement.  A correction under general unequal
probability sampling (eg PPS) would require joint inclusion probabilities (or,
at least, sampling probabilities for units not included in the sample),
information not generally available.
</p>
<p>The quality of Brewer's approximation is excellent in Berger's
simulations, but the accuracy may vary depending on the sampling
algorithm used.
</p>


<h3>References</h3>

<p>Bellhouse DR (1985) Computing Methods for Variance Estimation in Complex Surveys.
Journal of Official Statistics. Vol.1, No.3, 1985
</p>
<p>Berger, Y.G. (2004), A Simple Variance Estimator for Unequal
Probability Sampling Without Replacement. Journal of Applied
Statistics, 31, 305-315.
</p>
<p>Binder, David A. (1983).  On the variances of asymptotically normal
estimators from complex surveys.  International Statistical Review,
51, 279-292.
</p>
<p>Brewer KRW (2002) Combined Survey Sampling Inference (Weighing Basu's
Elephants)  [Chapter 9]
</p>
<p>Cochran, W. (1977)  Sampling Techniques. 3rd edition. Wiley.
</p>
<p>Sarndal C-E, Swensson B, Wretman J (1991) Model Assisted Survey
Sampling. Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+svrVar">svrVar</a></code> for replicate weight designs
</p>
<p><code><a href="#topic+svyCprod">svyCprod</a></code> for a description of how variances are
estimated at each stage
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mu284)
dmu284&lt;-svydesign(id=~id1+id2,fpc=~n1+n2, data=mu284)
svytotal(~y1, dmu284)


data(api)
# two-stage cluster sample
dclus2&lt;-svydesign(id=~dnum+snum, fpc=~fpc1+fpc2, data=apiclus2)
summary(dclus2)
svymean(~api00, dclus2)
svytotal(~enroll, dclus2,na.rm=TRUE)

# bootstrap for multistage sample
mrbclus2&lt;-as.svrepdesign(dclus2, type="mrb", replicates=100)
svytotal(~enroll, mrbclus2, na.rm=TRUE)

# two-stage `with replacement'
dclus2wr&lt;-svydesign(id=~dnum+snum, weights=~pw, data=apiclus2)
summary(dclus2wr)
svymean(~api00, dclus2wr)
svytotal(~enroll, dclus2wr,na.rm=TRUE)


</code></pre>

<hr>
<h2 id='svyscoretest'>
Score tests in survey regression models
</h2><span id='topic+svyscoretest'></span><span id='topic+svyscoretest.svyglm'></span>

<h3>Description</h3>

<p>Performs two versions of the efficient score test. These are the same
for a single parameter. In the <code>working</code> score test, different
parameters are weighted according to the inverse of the estimated population Fisher
information. In the <code>pseudoscore</code> test, parameters are weighted according to the
inverse of their estimated covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svyscoretest(model, drop.terms=NULL, add.terms=NULL,
method=c("working","pseudoscore","individual"),ddf=NULL,
lrt.approximation = "satterthwaite", ...)
## S3 method for class 'svyglm'
svyscoretest(model, drop.terms=NULL, add.terms=NULL,
method=c("working","pseudoscore","individual"), ddf=NULL,
lrt.approximation = "satterthwaite",fullrank=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyscoretest_+3A_model">model</code></td>
<td>

<p>A model of a class having a <code>svyscoretest</code> method (currently
just <code>svyglm</code>)
</p>
</td></tr>
<tr><td><code id="svyscoretest_+3A_drop.terms">drop.terms</code></td>
<td>

<p>Model formula giving terms to remove from <code>model</code>
</p>
</td></tr>
<tr><td><code id="svyscoretest_+3A_add.terms">add.terms</code></td>
<td>
<p>Model formula giving terms to add to <code>model</code> </p>
</td></tr>
<tr><td><code id="svyscoretest_+3A_method">method</code></td>
<td>

<p>The type of score test to use. For a single parameter they are
equivalent. To report tests for each column separately use <code>individual</code>
</p>
</td></tr>
<tr><td><code id="svyscoretest_+3A_ddf">ddf</code></td>
<td>
<p>denominator degrees of freedom for an F or linear combination
of F distributions. Use <code>Inf</code> to get chi-squared
distributions. <code>NULL</code> asks for the model residual degrees of
freedom, which is conservative. </p>
</td></tr>
<tr><td><code id="svyscoretest_+3A_lrt.approximation">lrt.approximation</code></td>
<td>

<p>For the working score, the method for computing/approximating the null
distribution: see <code><a href="#topic+pchisqsum">pchisqsum</a></code>
</p>
</td></tr>
<tr><td><code id="svyscoretest_+3A_fullrank">fullrank</code></td>
<td>
<p>If <code>FALSE</code> and <code>method="individual"</code>, keep
even linearly dependent columns of the
efficient score</p>
</td></tr>
<tr><td><code id="svyscoretest_+3A_...">...</code></td>
<td>

<p>for future expansion
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>working</code> score test will be asymptotically equivalent to the
Rao-Scott likelihood ratio test computed by <code><a href="#topic+regTermTest">regTermTest</a></code>
and <code><a href="#topic+anova.svyglm">anova.svyglm</a></code>. The paper by Rao, Scott and Skinner calls this
a &quot;naive&quot; score test. The null distribution is a linear combination
of chi-squared (or F) variables. 
</p>
<p>The <code>pseudoscore</code> test will be
asymptotically equivalent to the Wald test computed by
<code><a href="#topic+regTermTest">regTermTest</a></code>; it has a chi-squared (or F) null
distribution.
</p>
<p>If <code>ddf</code> is negative or zero, which can happen with large numbers
of predictors and small numbers of PSUs, it will be changed to 1 with a warning.
</p>


<h3>Value</h3>

<p>For &quot;pseudoscore&quot; and &quot;working&quot; score methods, a named vector with the test
statistic, degrees of freedom, and p-value. For &quot;individual&quot; an object
of class &quot;svystat&quot;
</p>


<h3>References</h3>

<p>JNK Rao, AJ Scott, and C
Rao, J., Scott, A., &amp; Skinner, C. (1998). QUASI-SCORE TESTS WITH SURVEY
DATA. Statistica Sinica, 8(4), 1059-1070.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+regTermTest">regTermTest</a></code>, <code><a href="#topic+anova.svyglm">anova.svyglm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(myco)
dmyco&lt;-svydesign(id=~1, strata=~interaction(Age,leprosy),weights=~wt,data=myco)

m_full&lt;-svyglm(leprosy~I((Age+7.5)^-2)+Scar, family=quasibinomial, design=dmyco)
svyscoretest(m_full, ~Scar)

svyscoretest(m_full,add.terms= ~I((Age+7.5)^-2):Scar)
svyscoretest(m_full,add.terms= ~factor(Age), method="pseudo")
svyscoretest(m_full,add.terms= ~factor(Age),method="individual",fullrank=FALSE)

svyscoretest(m_full,add.terms= ~factor(Age),method="individual")

</code></pre>

<hr>
<h2 id='svysmooth'>Scatterplot smoothing and density estimation</h2><span id='topic+svysmooth'></span><span id='topic+svysmooth.default'></span><span id='topic+plot.svysmooth'></span><span id='topic+print.svysmooth'></span><span id='topic+lines.svysmooth'></span><span id='topic+make.panel.svysmooth'></span>

<h3>Description</h3>

<p>Scatterplot smoothing and density estimation for probability-weighted
data. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svysmooth(formula, design, ...)
## Default S3 method:
svysmooth(formula, design, method = c("locpoly", "quantreg"), 
    bandwidth = NULL, quantile, df = 4, ...)
## S3 method for class 'svysmooth'
plot(x, which=NULL, type="l", xlabs=NULL, ylab=NULL,...)
## S3 method for class 'svysmooth'
lines(x,which=NULL,...)
make.panel.svysmooth(design,bandwidth=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svysmooth_+3A_formula">formula</code></td>
<td>
<p>One-sided formula for density estimation, two-sided for smoothing</p>
</td></tr>
<tr><td><code id="svysmooth_+3A_design">design</code></td>
<td>
<p>Survey design object</p>
</td></tr>
<tr><td><code id="svysmooth_+3A_method">method</code></td>
<td>
<p>local polynomial smoothing for the mean or regression
splines for quantiles</p>
</td></tr>
<tr><td><code id="svysmooth_+3A_bandwidth">bandwidth</code></td>
<td>
<p>Smoothing bandwidth for &quot;locpoly&quot; or <code>NULL</code> for automatic choice</p>
</td></tr>
<tr><td><code id="svysmooth_+3A_quantile">quantile</code></td>
<td>
<p>quantile to be estimated for &quot;quantreg&quot;</p>
</td></tr>
<tr><td><code id="svysmooth_+3A_df">df</code></td>
<td>
<p>Degrees of freedom for &quot;quantreg&quot;</p>
</td></tr>
<tr><td><code id="svysmooth_+3A_which">which</code></td>
<td>
<p>Which plots to show (default is all)</p>
</td></tr>
<tr><td><code id="svysmooth_+3A_type">type</code></td>
<td>
<p>as for <code>plot</code></p>
</td></tr>
<tr><td><code id="svysmooth_+3A_xlabs">xlabs</code></td>
<td>
<p>Optional vector of x-axis labels</p>
</td></tr>
<tr><td><code id="svysmooth_+3A_ylab">ylab</code></td>
<td>
<p>Optional y-axis label</p>
</td></tr>
<tr><td><code id="svysmooth_+3A_...">...</code></td>
<td>
<p>More arguments</p>
</td></tr>
<tr><td><code id="svysmooth_+3A_x">x</code></td>
<td>
<p>Object of class <code>svysmooth</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>svysmooth</code> does one-dimensional smoothing. If <code>formula</code> has
multiple predictor variables a separate one-dimensional smooth is
performed for each one. 
</p>
<p>For <code>method="locpoly"</code> the extra arguments are passed to
<code>locpoly</code> from the KernSmooth package, for
<code>method="quantreg"</code> they are passed to <code>rq</code> from the
quantreg package.  The automatic choice of bandwidth for
<code>method="locpoly"</code> uses the default settings for <code>dpik</code> and
<code>dpill</code> in the KernSmooth package.
</p>
<p><code>make.panel.svysmooth()</code> makes a function that plots points and
draws a weighted smooth curve through them, a weighted replacement for
<code><a href="graphics.html#topic+panel.smooth">panel.smooth</a></code> that can be passed to functions such as
<code><a href="stats.html#topic+termplot">termplot</a></code> or <code><a href="stats.html#topic+plot.lm">plot.lm</a></code>.  The resulting function has a <code>span</code> argument that will set the bandwidth; if this is not specified the automatic choice will be used.
</p>


<h3>Value</h3>

<p>An object of class <code>svysmooth</code>, a list of lists, each with <code>x</code> and <code>y</code> components.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+svyhist">svyhist</a></code> for histograms</p>


<h3>Examples</h3>

<pre><code class='language-R'> data(api)
 dstrat&lt;-svydesign(id=~1,strata=~stype, weights=~pw, data=apistrat, fpc=~fpc)

 smth&lt;-svysmooth(api00~api99+ell,dstrat)
 dens&lt;-svysmooth(~api99, dstrat,bandwidth=30)
 dens1&lt;-svysmooth(~api99, dstrat)
 qsmth&lt;-svysmooth(api00~ell,dstrat, quantile=0.75, df=3,method="quantreg")

 plot(smth)
 plot(smth, which="ell",lty=2,ylim=c(500,900))
 lines(qsmth, col="red")

 svyhist(~api99,design=dstrat)
 lines(dens,col="purple",lwd=3)
 lines(dens1, col="forestgreen",lwd=2)

 m&lt;-svyglm(api00~sin(api99/100)+stype, design=dstrat)
 termplot(m, data=model.frame(dstrat), partial.resid=TRUE, se=TRUE,
  smooth=make.panel.svysmooth(dstrat))
</code></pre>

<hr>
<h2 id='svystandardize'>
Direct standardization within domains
</h2><span id='topic+svystandardize'></span>

<h3>Description</h3>

<p>In health surveys it is often of interest to standardize domains to have the same distribution of, eg, age as in a target population.  The operation is similar to post-stratification, except that the totals for the domains are fixed at the current estimates, not at known population values.  This function matches the estimates produced by the (US) National Center for Health Statistics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svystandardize(design, by, over, population, excluding.missing = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svystandardize_+3A_design">design</code></td>
<td>

<p>survey design object  
</p>
</td></tr>
<tr><td><code id="svystandardize_+3A_by">by</code></td>
<td>

<p>A one-sided formula specifying the variables whose distribution will be standardised
</p>
</td></tr>
<tr><td><code id="svystandardize_+3A_over">over</code></td>
<td>

<p>A one-sided formula specifying the domains within which the
standardisation will occur, or <code>~1</code> to use the whole population.
</p>
</td></tr>
<tr><td><code id="svystandardize_+3A_population">population</code></td>
<td>

<p>Desired population totals or proportions for the levels of combinations of variables in <code>by</code>
</p>
</td></tr>
<tr><td><code id="svystandardize_+3A_excluding.missing">excluding.missing</code></td>
<td>

<p>Optionally, a one-sided formula specifying variables whose missing values should be dropped before calculating the domain totals. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new survey design object of the same type as the input.
</p>


<h3>Note</h3>

<p>The standard error estimates do not exactly match the NCHS estimates
</p>


<h3>References</h3>

<p>National Center for Health Statistics <code style="white-space: pre;">&#8288;https://www.cdc.gov/nchs/tutorials/NHANES/NHANESAnalyses/agestandardization/age_standardization_intro.htm&#8288;</code></p>


<h3>See Also</h3>

<p><code><a href="#topic+postStratify">postStratify</a></code>, <code><a href="#topic+svyby">svyby</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## matches http://www.cdc.gov/nchs/data/databriefs/db92_fig1.png
data(nhanes)
popage &lt;- c( 55901 , 77670 , 72816 , 45364 )
design&lt;-svydesign(id=~SDMVPSU, strata=~SDMVSTRA, weights=~WTMEC2YR, data=nhanes, nest=TRUE)
stdes&lt;-svystandardize(design, by=~agecat, over=~race+RIAGENDR, 
   population=popage, excluding.missing=~HI_CHOL)
svyby(~HI_CHOL, ~race+RIAGENDR, svymean, design=subset(stdes,
agecat!="(0,19]"))


data(nhanes)
nhanes_design &lt;- svydesign(ids = ~ SDMVPSU, strata = ~ SDMVSTRA, 
                        weights = ~ WTMEC2YR, nest = TRUE, data = nhanes)

## These are the same
nhanes_adj &lt;- svystandardize(update(nhanes_design, all_adults = "1"),
                 by = ~ agecat, over = ~ all_adults,
                 population = c(55901, 77670, 72816, 45364), 
                 excluding.missing = ~ HI_CHOL)
svymean(~I(HI_CHOL == 1), nhanes_adj, na.rm = TRUE)		 

nhanes_adj &lt;- svystandardize(nhanes_design,
                 by = ~ agecat, over = ~ 1,
                 population = c(55901, 77670, 72816, 45364), 
                 excluding.missing = ~ HI_CHOL)
svymean(~I(HI_CHOL == 1), nhanes_adj, na.rm = TRUE)		 

</code></pre>

<hr>
<h2 id='svysurvreg'>
Fit accelerated failure models to survey data
</h2><span id='topic+svysurvreg'></span><span id='topic+svysurvreg.survey.design'></span>

<h3>Description</h3>

<p>This function calls <code>survreg</code> from the 'survival' package to fit accelerated failure (accelerated life) models to complex survey data, and then computes correct standard errors by linearisation.  It has the same arguments as <code>survreg</code>, except that the second argument is <code>design</code> rather than <code>data</code>.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survey.design'
svysurvreg(formula, design, weights=NULL, subset=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svysurvreg_+3A_formula">formula</code></td>
<td>

<p>Model formula
</p>
</td></tr>
<tr><td><code id="svysurvreg_+3A_design">design</code></td>
<td>

<p>Survey design object, including two-phase designs
</p>
</td></tr>
<tr><td><code id="svysurvreg_+3A_weights">weights</code></td>
<td>

<p>Additional weights to multiply by the sampling weights. No, I don't know why you'd want to do that.
</p>
</td></tr>
<tr><td><code id="svysurvreg_+3A_subset">subset</code></td>
<td>

<p>subset to use in fitting (if needed)
</p>
</td></tr>
<tr><td><code id="svysurvreg_+3A_...">...</code></td>
<td>

<p>Other arguments of <code>survreg</code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>svysurvreg</code>, with the same structure as a <code>survreg</code> object but with <code>NA</code> for the loglikelihood.
</p>


<h3>Note</h3>

<p>The <code>residuals</code> method is identical to that for <code>survreg</code> objects except the <code>weighted</code> option defaults to <code>TRUE</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 data(pbc, package="survival")
 pbc$randomized &lt;- with(pbc, !is.na(trt) &amp; trt&gt;0)
 biasmodel&lt;-glm(randomized~age*edema,data=pbc)
 pbc$randprob&lt;-fitted(biasmodel)
 dpbc&lt;-svydesign(id=~1, prob=~randprob, strata=~edema,
    data=subset(pbc,randomized))

 model &lt;- svysurvreg(Surv(time, status&gt;0)~bili+protime+albumin, design=dpbc, dist="weibull")
summary(model)

</code></pre>

<hr>
<h2 id='svytable'>Contingency tables for survey data</h2><span id='topic+svreptable'></span><span id='topic+svytable'></span><span id='topic+svytable.svyrep.design'></span><span id='topic+svytable.survey.design'></span><span id='topic+svychisq'></span><span id='topic+svychisq.survey.design'></span><span id='topic+svychisq.svyrep.design'></span><span id='topic+summary.svytable'></span><span id='topic+print.summary.svytable'></span><span id='topic+summary.svreptable'></span><span id='topic+degf'></span><span id='topic+degf.svyrep.design'></span><span id='topic+degf.survey.design2'></span><span id='topic+degf.twophase'></span>

<h3>Description</h3>

<p>Contingency tables and chisquared tests of association for survey data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survey.design'
svytable(formula, design, Ntotal = NULL, round = FALSE,...)
## S3 method for class 'svyrep.design'
svytable(formula, design,
		Ntotal = sum(weights(design, "sampling")), round = FALSE,...)
## S3 method for class 'survey.design'
svychisq(formula, design, 
   statistic = c("F",  "Chisq","Wald","adjWald","lincom",
   "saddlepoint","wls-score"),na.rm=TRUE,...)
## S3 method for class 'svyrep.design'
svychisq(formula, design, 
   statistic = c("F",  "Chisq","Wald","adjWald","lincom",
   "saddlepoint","wls-score"),na.rm=TRUE,...)
## S3 method for class 'svytable'
summary(object, 
   statistic = c("F","Chisq","Wald","adjWald","lincom","saddlepoint"),...)
degf(design, ...)
## S3 method for class 'survey.design2'
degf(design, ...)
## S3 method for class 'svyrep.design'
degf(design, tol=1e-5,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svytable_+3A_formula">formula</code></td>
<td>
<p>Model formula specifying margins for the table (using <code>+</code> only)</p>
</td></tr>
<tr><td><code id="svytable_+3A_design">design</code></td>
<td>
<p>survey object</p>
</td></tr>
<tr><td><code id="svytable_+3A_statistic">statistic</code></td>
<td>
<p>See Details below</p>
</td></tr>
<tr><td><code id="svytable_+3A_ntotal">Ntotal</code></td>
<td>
<p>A population total or set of population stratum totals
to normalise to.</p>
</td></tr>
<tr><td><code id="svytable_+3A_round">round</code></td>
<td>
<p>Should the table entries be rounded to the nearest
integer?</p>
</td></tr>
<tr><td><code id="svytable_+3A_na.rm">na.rm</code></td>
<td>
<p>Remove missing values</p>
</td></tr>
<tr><td><code id="svytable_+3A_object">object</code></td>
<td>
<p>Output from <code>svytable</code></p>
</td></tr>
<tr><td><code id="svytable_+3A_...">...</code></td>
<td>
<p>For <code>svytable</code> these are passed to <code>xtabs</code>. Use
<code>exclude=NULL</code>, <code>na.action=na.pass</code> to include <code>NA</code>s
in the table</p>
</td></tr>
<tr><td><code id="svytable_+3A_tol">tol</code></td>
<td>
<p>Tolerance for <code><a href="base.html#topic+qr">qr</a></code> in computing the matrix rank</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>svytable</code> function computes a weighted crosstabulation. This
is especially useful for producing graphics. It is sometimes easier
to use <code><a href="#topic+svytotal">svytotal</a></code> or <code><a href="#topic+svymean">svymean</a></code>, which also
produce standard errors, design effects, etc.
</p>
<p>The frequencies in the table can be normalised to some convenient total
such as 100 or 1.0 by specifying the <code>Ntotal</code> argument.  If the
formula has a left-hand side the mean or sum of this variable rather
than the frequency is tabulated.
</p>
<p>The <code>Ntotal</code> argument can be either a single number or a data
frame whose first column gives the (first-stage) sampling strata and
second column the population size in each stratum.  In this second case
the <code>svytable</code> command performs &lsquo;post-stratification&rsquo;: tabulating
and scaling to the population within strata and then adding up the
strata.
</p>
<p>As with other <code>xtabs</code> objects, the output of <code>svytable</code> can be
processed by <code>ftable</code> for more attractive display. The
<code>summary</code> method for <code>svytable</code> objects calls <code>svychisq</code>
for a test of independence.
</p>
<p><code>svychisq</code> computes first and second-order Rao-Scott corrections to
the Pearson chisquared test, and two Wald-type tests.
</p>
<p>The default (<code>statistic="F"</code>) is the Rao-Scott second-order
correction.  The p-values are computed with a Satterthwaite
approximation to the distribution and with denominator degrees of
freedom as recommended by Thomas and Rao (1990). The alternative
<code>statistic="Chisq"</code> adjusts the Pearson chisquared statistic by a
design effect estimate and then compares it to the chisquared
distribution it would have under simple random sampling.
</p>
<p>The <code>statistic="Wald"</code> test is that proposed by Koch et al (1975)
and used by the SUDAAN software package. It is a Wald test based on the
differences between the observed cells counts and those expected under
independence. The adjustment given by <code>statistic="adjWald"</code> reduces
the statistic when the number of PSUs is small compared to the number of
degrees of freedom of the test. Thomas and Rao (1987) compare these
tests and find the adjustment benefical.
</p>
<p><code>statistic="lincom"</code> replaces the numerator of the Rao-Scott F with
the exact asymptotic distribution, which is a linear combination of
chi-squared variables (see <code><a href="#topic+pchisqsum">pchisqsum</a></code>, and
<code>statistic="saddlepoint"</code> uses a saddlepoint approximation to this
distribution.  The <code>CompQuadForm</code> package is needed for
<code>statistic="lincom"</code> but not for
<code>statistic="saddlepoint"</code>. The saddlepoint approximation is
especially useful when the p-value is very small (as in large-scale
multiple testing problems).
</p>
<p><code>statistic="wls-score"</code> is an experimental implementation of the
weighted least squares score test of Lipsitz et al (2015). It is not
identical to that paper, for example, I think the denominator degrees
of freedom need to be reduced by JK for a JxK table, not (J-1)(K-1). And
it's very close to the &quot;adjWald&quot; test.
</p>
<p>For designs using replicate weights the code is essentially the same as
for designs with sampling structure, since the necessary variance
computations are done by the appropriate methods of
<code><a href="#topic+svytotal">svytotal</a></code> and <code><a href="#topic+svymean">svymean</a></code>.  The exception is that
the degrees of freedom is computed as one less than the rank of the
matrix of replicate weights (by <code>degf</code>).
</p>
<p>At the moment, <code>svychisq</code> works only for 2-dimensional tables.
</p>


<h3>Value</h3>

<p>The table commands return an <code>xtabs</code> object, <code>svychisq</code>
returns a <code>htest</code> object.
</p>


<h3>Note</h3>

<p>Rao and Scott (1984) leave open one computational issue. In
computing &lsquo;generalised design effects&rsquo; for these tests, should the
variance under simple random sampling be estimated using the observed
proportions or the the predicted proportions under the null
hypothesis? <code>svychisq</code> uses the observed proportions, following
simulations by Sribney (1998), and the choices made in Stata</p>


<h3>References</h3>

<p>Davies RB (1973). &quot;Numerical inversion of a characteristic function&quot;
Biometrika 60:415-7
</p>
<p>P. Duchesne, P. Lafaye de Micheaux (2010) &quot;Computing the distribution of
quadratic forms: Further comparisons between the Liu-Tang-Zhang
approximation and exact methods&quot;, Computational Statistics and Data
Analysis, Volume 54,  858-862
</p>
<p>Koch, GG, Freeman, DH, Freeman, JL (1975) &quot;Strategies in the
multivariate analysis of data from complex surveys&quot; International
Statistical Review 43: 59-78
</p>
<p>Stuart R. Lipsitz, Garrett M. Fitzmaurice, Debajyoti Sinha, Nathanael
Hevelone, Edward Giovannucci, and Jim C. Hu (2015) &quot;Testing for
independence in JxK contingency tables with complex sample survey data&quot;
Biometrics 71(3): 832-840
</p>
<p>Rao, JNK, Scott, AJ (1984) &quot;On Chi-squared Tests For Multiway
Contigency Tables with Proportions Estimated From Survey Data&quot;  Annals
of Statistics 12:46-60.
</p>
<p>Sribney WM (1998) &quot;Two-way contingency tables for survey or clustered
data&quot; Stata Technical Bulletin 45:33-49.
</p>
<p>Thomas, DR, Rao, JNK (1987) &quot;Small-sample comparison of level and power
for simple goodness-of-fit statistics under cluster sampling&quot; JASA 82:630-636
</p>


<h3>See Also</h3>

<p><code><a href="#topic+svytotal">svytotal</a></code> and <code><a href="#topic+svymean">svymean</a></code> report totals
and proportions by category for factor variables.
</p>
<p>See <code><a href="#topic+svyby">svyby</a></code> and <code><a href="#topic+ftable.svystat">ftable.svystat</a></code> to construct
more complex tables of summary statistics.
</p>
<p>See <code><a href="#topic+svyloglin">svyloglin</a></code> for loglinear models.
</p>
<p>See <code><a href="#topic+regTermTest">regTermTest</a></code> for Rao-Scott tests in regression models.
</p>
<p>See  <a href="https://notstatschat.rbind.io/2019/06/08/design-degrees-of-freedom-brief-note/">https://notstatschat.rbind.io/2019/06/08/design-degrees-of-freedom-brief-note/</a> for an explanation of the design degrees of freedom with replicate weights.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(api)
  xtabs(~sch.wide+stype, data=apipop)

  dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)
  summary(dclus1)

  (tbl &lt;- svytable(~sch.wide+stype, dclus1))
  plot(tbl)
  fourfoldplot(svytable(~sch.wide+comp.imp+stype,design=dclus1,round=TRUE), conf.level=0)

  svychisq(~sch.wide+stype, dclus1)
  summary(tbl, statistic="Chisq")
  svychisq(~sch.wide+stype, dclus1, statistic="adjWald")

  rclus1 &lt;- as.svrepdesign(dclus1)
  summary(svytable(~sch.wide+stype, rclus1))
  svychisq(~sch.wide+stype, rclus1, statistic="adjWald")

</code></pre>

<hr>
<h2 id='svyttest'>Design-based t-test</h2><span id='topic+svyttest'></span><span id='topic+confint.svyttest'></span>

<h3>Description</h3>

<p>One-sample or two-sample t-test.  This function is a wrapper for
<code><a href="#topic+svymean">svymean</a></code> in the one-sample case and for
<code><a href="#topic+svyglm">svyglm</a></code> in the two-sample case. Degrees of freedom are
<code>degf(design)-1</code> for the one-sample test and <code>degf(design)-2</code>
for the two-sample case.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svyttest(formula, design,  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svyttest_+3A_formula">formula</code></td>
<td>
<p>Formula, <code>outcome~group</code> for two-sample,
<code>outcome~0</code> or <code>outcome~1</code> for one-sample. The <code>group</code> variable 
must be a factor or character with two levels, or be coded 0/1 or 1/2</p>
</td></tr>
<tr><td><code id="svyttest_+3A_design">design</code></td>
<td>
<p>survey design object</p>
</td></tr>
<tr><td><code id="svyttest_+3A_...">...</code></td>
<td>
<p>for methods </p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>htest</code>
</p>


<h3>See Also</h3>

 <p><code><a href="stats.html#topic+t.test">t.test</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus2&lt;-svydesign(id=~dnum+snum, fpc=~fpc1+fpc2, data=apiclus2)
tt&lt;-svyttest(enroll~comp.imp, dclus2)
tt
confint(tt, level=0.9)

svyttest(enroll~I(stype=="E"),dclus2)

svyttest(I(api00-api99)~0, dclus2)

</code></pre>

<hr>
<h2 id='trimWeights'>
Trim sampling weights
</h2><span id='topic+trimWeights'></span><span id='topic+trimWeights.svyrep.design'></span><span id='topic+trimWeights.survey.design2'></span>

<h3>Description</h3>

<p>Trims very high or very low sampling weights to reduce the influence of outlying observations. In a replicate-weight design object, the replicate weights are also trimmed. The total amount trimmed is divided among the observations that were not trimmed, so that the total weight remains the same.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trimWeights(design, upper = Inf, lower = -Inf, ...)
## S3 method for class 'survey.design2'
trimWeights(design, upper = Inf, lower = -Inf, strict=FALSE,...)
## S3 method for class 'svyrep.design'
trimWeights(design, upper = Inf, lower = -Inf,
strict=FALSE, compress=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trimWeights_+3A_design">design</code></td>
<td>

<p>A survey design object
</p>
</td></tr>
<tr><td><code id="trimWeights_+3A_upper">upper</code></td>
<td>

<p>Upper bound for weights
</p>
</td></tr>
<tr><td><code id="trimWeights_+3A_lower">lower</code></td>
<td>

<p>Lower bound for weights
</p>
</td></tr>
<tr><td><code id="trimWeights_+3A_strict">strict</code></td>
<td>

<p>The reapportionment of the &lsquo;trimmings&rsquo; from the weights can push
other weights over the limits. If <code>trim=TRUE</code> the function
repeats the trimming iteratively to prevent this. For
replicate-weight designs <code>strict</code> applies only to the trimming of the sampling  weights.
</p>
</td></tr>
<tr><td><code id="trimWeights_+3A_compress">compress</code></td>
<td>

<p>Compress the replicate weights after trimming.
</p>
</td></tr>
<tr><td><code id="trimWeights_+3A_...">...</code></td>
<td>

<p>Other arguments for future expansion
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new survey design object with trimmed weights.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+calibrate">calibrate</a></code> has a <code>trim</code> option for trimming the
calibration adjustments.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)

pop.totals&lt;-c(`(Intercept)`=6194, stypeH=755, stypeM=1018,
api99=3914069)
dclus1g&lt;-calibrate(dclus1, ~stype+api99, pop.totals)

summary(weights(dclus1g))
dclus1t&lt;-trimWeights(dclus1g,lower=20, upper=45)
summary(weights(dclus1t))
dclus1tt&lt;-trimWeights(dclus1g, lower=20, upper=45,strict=TRUE)
summary(weights(dclus1tt))


svymean(~api99+api00+stype, dclus1g)
svymean(~api99+api00+stype, dclus1t)
svymean(~api99+api00+stype, dclus1tt)
</code></pre>

<hr>
<h2 id='twophase'>Two-phase designs</h2><span id='topic+twophase'></span><span id='topic+twophasevar'></span><span id='topic+twophase2var'></span><span id='topic++5B.twophase'></span><span id='topic+subset.twophase'></span><span id='topic+print.twophase'></span><span id='topic+summary.twophase'></span><span id='topic+print.summary.twophase'></span><span id='topic+model.frame.twophase'></span><span id='topic+na.fail.twophase'></span><span id='topic+na.omit.twophase'></span><span id='topic+na.exclude.twophase'></span><span id='topic+svyrecvar.phase1'></span><span id='topic+multistage.phase1'></span><span id='topic+onestage.phase1'></span><span id='topic+onestrat.phase1'></span>

<h3>Description</h3>

<p>In a two-phase design a sample is taken from a population and a
subsample taken from the sample, typically stratified by variables not
known for the whole population.  The second phase can use any design 
supported for single-phase sampling. The first phase must currently
be one-stage element  or cluster sampling
</p>


<h3>Usage</h3>

<pre><code class='language-R'>twophase(id, strata = NULL, probs = NULL, weights = NULL, fpc = NULL,
subset, data, method=c("full","approx","simple"), pps=NULL)
twophasevar(x,design)
twophase2var(x,design)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="twophase_+3A_id">id</code></td>
<td>
<p>list of two formulas for sampling unit identifiers</p>
</td></tr>
<tr><td><code id="twophase_+3A_strata">strata</code></td>
<td>
<p>list of two formulas (or <code>NULL</code>s) for stratum identifies</p>
</td></tr>
<tr><td><code id="twophase_+3A_probs">probs</code></td>
<td>
<p> list of two formulas (or <code>NULL</code>s) for sampling probabilities</p>
</td></tr>
<tr><td><code id="twophase_+3A_weights">weights</code></td>
<td>
<p>Only for <code>method="approx"</code>, list of two formulas (or <code>NULL</code>s) for sampling weights</p>
</td></tr>
<tr><td><code id="twophase_+3A_fpc">fpc</code></td>
<td>
<p>list of two formulas (or <code>NULL</code>s) for finite
population corrections</p>
</td></tr>
<tr><td><code id="twophase_+3A_subset">subset</code></td>
<td>
<p>formula specifying which observations are selected in
phase 2</p>
</td></tr>
<tr><td><code id="twophase_+3A_data">data</code></td>
<td>
<p>Data frame will all data for phase 1 and 2</p>
</td></tr>
<tr><td><code id="twophase_+3A_method">method</code></td>
<td>
<p><code>"full"</code> requires (much) more memory, but gives unbiased
variance estimates for general multistage designs at both phases.
<code>"simple"</code> or <code>"approx"</code> uses the standard error calculation from
version 3.14 and earlier,  which uses much less memory and is correct for designs with simple
random sampling at phase one and stratified random sampling at phase two.
</p>
</td></tr>
<tr><td><code id="twophase_+3A_pps">pps</code></td>
<td>
<p>With <code>method="full"</code>, an optional list of two PPS
specifications for <code><a href="#topic+svydesign">svydesign</a></code>. At the moment, the
phase-one element must be <code>NULL</code></p>
</td></tr>
<tr><td><code id="twophase_+3A_x">x</code></td>
<td>
<p>probability-weighted estimating functions</p>
</td></tr>
<tr><td><code id="twophase_+3A_design">design</code></td>
<td>
<p>two-phase design</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The population for the second phase is the first-phase sample. If the
second phase sample uses stratified (multistage cluster) sampling
without replacement and all the stratum and sampling unit identifier
variables are available for the whole first-phase sample it is
possible to estimate the sampling probabilities/weights and the
finite population correction. These would then  be specified as
<code>NULL</code>.
</p>
<p>Two-phase case-control and case-cohort studies in biostatistics will
typically have simple random sampling with replacement as the first
stage. Variances given here may differ slightly from those in the
biostatistics literature where a model-based estimator of the
first-stage variance would typically be used.
</p>
<p>Variance computations are based on the conditioning argument in
Section 9.3 of Sarndal et al. Method <code>"full"</code> corresponds exactly
to the formulas in that reference. Method <code>"simple"</code> or
<code>"approx"</code> (the two are the same) uses less time and memory but
is exact only for some special cases. The most important special case
is the two-phase epidemiologic designs where phase 1 is simple random
sampling from an infinite population and phase 2 is stratified random
sampling.  See the <code>tests</code> directory for a worked example. The
only disadvantage of method=&quot;simple&quot; in these cases is that
standardization of margins (<code><a href="#topic+marginpred">marginpred</a></code>) is not available.
</p>
<p>For <code>method="full"</code>, sampling probabilities must be available for
each stage of sampling, within each phase.  For multistage sampling
this requires specifying either <code>fpc</code> or <code>probs</code> as a
formula with a term for each stage of sampling.  If no <code>fpc</code> or
<code>probs</code> are specified at phase 1 it is treated as simple random
sampling from an infinite population, and population totals will not
be correctly estimated, but means, quantiles, and regression models
will be correct.
</p>
<p>The <code>pps</code> argument allows for PPS sampling at phase two (or
eventually at phase one), and also for Poisson sampling at phase two
as a model for non-response. 
</p>


<h3>Value</h3>

<p><code>twophase</code> returns an object of class <code>twophase2</code> (for
<code>method="full"</code>) or <code>twophase</code>.  The structure of
<code>twophase2</code> objects may change as unnecessary components are removed.
</p>
<p><code>twophase2var</code> and <code>twophasevar</code> return a variance matrix with an attribute
containing the separate phase 1 and phase 2 contributions to the variance.
</p>


<h3>References</h3>

<p>Sarndal CE, Swensson B, Wretman J (1992) &quot;Model Assisted Survey Sampling&quot;
Springer.
</p>
<p>Breslow NE and Chatterjee N, Design and analysis of two-phase
studies with binary outcome applied to Wilms tumour prognosis.  &quot;Applied
Statistics&quot;  48:457-68, 1999
</p>
<p>Breslow N, Lumley T, Ballantyne CM, Chambless LE, Kulick M. (2009)
Improved Horvitz-Thompson estimation of model parameters from two-phase
stratified samples: applications in epidemiology. Statistics in
Biosciences. doi 10.1007/s12561-009-9001-6
</p>
<p>Lin, DY and Ying, Z (1993). Cox regression with incomplete covariate measurements.
&quot;Journal of the American Statistical Association&quot; 88: 1341-1349.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+svydesign">svydesign</a></code>, <code><a href="#topic+svyrecvar">svyrecvar</a></code> for multi*stage*
sampling
</p>
<p><code><a href="#topic+calibrate">calibrate</a></code> for calibration (GREG) estimators.
</p>
<p><code><a href="#topic+estWeights">estWeights</a></code> for two-phase designs for missing data.
</p>
<p>The &quot;epi&quot; and &quot;phase1&quot; vignettes for examples and technical details.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> ## two-phase simple random sampling.
 data(pbc, package="survival")
 pbc$randomized&lt;-with(pbc, !is.na(trt) &amp; trt&gt;0)
 pbc$id&lt;-1:nrow(pbc)
 d2pbc&lt;-twophase(id=list(~id,~id), data=pbc, subset=~randomized)
 svymean(~bili, d2pbc)

 ## two-stage sampling as two-phase
 data(mu284)
 ii&lt;-with(mu284, c(1:15, rep(1:5,n2[1:5]-3)))
 mu284.1&lt;-mu284[ii,]
 mu284.1$id&lt;-1:nrow(mu284.1)
 mu284.1$sub&lt;-rep(c(TRUE,FALSE),c(15,34-15))
 dmu284&lt;-svydesign(id=~id1+id2,fpc=~n1+n2, data=mu284)
 ## first phase cluster sample, second phase stratified within cluster
 d2mu284&lt;-twophase(id=list(~id1,~id),strata=list(NULL,~id1),
                     fpc=list(~n1,NULL),data=mu284.1,subset=~sub)
 svytotal(~y1, dmu284)
 svytotal(~y1, d2mu284)
 svymean(~y1, dmu284)
 svymean(~y1, d2mu284)

 ## case-cohort design: this example requires R 2.2.0 or later
 library("survival")
 data(nwtco)

 ## stratified on case status
 dcchs&lt;-twophase(id=list(~seqno,~seqno), strata=list(NULL,~rel),
         subset=~I(in.subcohort | rel), data=nwtco)
 svycoxph(Surv(edrel,rel)~factor(stage)+factor(histol)+I(age/12), design=dcchs)

 ## Using survival::cch 
 subcoh &lt;- nwtco$in.subcohort
 selccoh &lt;- with(nwtco, rel==1|subcoh==1)
 ccoh.data &lt;- nwtco[selccoh,]
 ccoh.data$subcohort &lt;- subcoh[selccoh]
 cch(Surv(edrel, rel) ~ factor(stage) + factor(histol) + I(age/12), data =ccoh.data,
        subcoh = ~subcohort, id=~seqno, cohort.size=4028, method="LinYing")


 ## two-phase case-control
 ## Similar to Breslow &amp; Chatterjee, Applied Statistics (1999) but with
 ## a slightly different version of the data set
 
 nwtco$incc2&lt;-as.logical(with(nwtco, ifelse(rel | instit==2,1,rbinom(nrow(nwtco),1,.1))))
 dccs2&lt;-twophase(id=list(~seqno,~seqno),strata=list(NULL,~interaction(rel,instit)),
    data=nwtco, subset=~incc2)
 dccs8&lt;-twophase(id=list(~seqno,~seqno),strata=list(NULL,~interaction(rel,stage,instit)),
    data=nwtco, subset=~incc2)
 summary(glm(rel~factor(stage)*factor(histol),data=nwtco,family=binomial()))
 summary(svyglm(rel~factor(stage)*factor(histol),design=dccs2,family=quasibinomial()))
 summary(svyglm(rel~factor(stage)*factor(histol),design=dccs8,family=quasibinomial()))

 ## Stratification on stage is really post-stratification, so we should use calibrate()
 gccs8&lt;-calibrate(dccs2, phase=2, formula=~interaction(rel,stage,instit))
 summary(svyglm(rel~factor(stage)*factor(histol),design=gccs8,family=quasibinomial()))

 ## For this saturated model calibration is equivalent to estimating weights.
 pccs8&lt;-calibrate(dccs2, phase=2,formula=~interaction(rel,stage,instit), calfun="rrz")
 summary(svyglm(rel~factor(stage)*factor(histol),design=pccs8,family=quasibinomial()))

 ## Since sampling is SRS at phase 1 and stratified RS at phase 2, we
 ## can use method="simple" to save memory.
 dccs8_simple&lt;-twophase(id=list(~seqno,~seqno),strata=list(NULL,~interaction(rel,stage,instit)),
    data=nwtco, subset=~incc2,method="simple")
 summary(svyglm(rel~factor(stage)*factor(histol),design=dccs8_simple,family=quasibinomial()))



</code></pre>

<hr>
<h2 id='update.survey.design'> Add variables to a survey design</h2><span id='topic+update.survey.design'></span><span id='topic+update.twophase'></span><span id='topic+update.svyrep.design'></span><span id='topic+update.DBIsvydesign'></span>

<h3>Description</h3>

<p>Update the data variables in a survey design, either with a formula for a new set of variables or with an expression for variables to be added.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survey.design'
update(object, ...)
## S3 method for class 'twophase'
update(object, ...)
## S3 method for class 'svyrep.design'
update(object, ...)
## S3 method for class 'DBIsvydesign'
update(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update.survey.design_+3A_object">object</code></td>
<td>
<p>a survey design object</p>
</td></tr>
<tr><td><code id="update.survey.design_+3A_...">...</code></td>
<td>
<p>Arguments <code>tag=expr</code> add a new variable <code>tag</code>
computed by evaluating <code>expr</code> in the survey data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Database-backed objects may not have write access to the database and so
<code>update</code> does not attempt to modify the database.  The expressions
are stored and are evaluated when the data is loaded.
</p>
<p>If a set of new variables will be used extensively it may be more efficient to
modify the database, either with SQL queries from the R interface or
separately. One useful intermediate approach is to create a table with
the new variables and a view that joins this table to the table of
existing variables.
</p>
<p>There is now a base-R function <code><a href="base.html#topic+transform">transform</a></code> for adding new
variables to a data frame, so I have added <code>transform</code> as a synonym for
<code>update</code> for survey objects.
</p>


<h3>Value</h3>

<p>A survey design object
</p>


<h3>See Also</h3>

<p><code><a href="#topic+svydesign">svydesign</a></code>, <code><a href="#topic+svrepdesign">svrepdesign</a></code>, <code><a href="#topic+twophase">twophase</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(api)
dstrat&lt;-svydesign(id=~1,strata=~stype, weights=~pw, data=apistrat,
fpc=~fpc)
dstrat&lt;-update(dstrat, apidiff=api00-api99)
svymean(~api99+api00+apidiff, dstrat)
</code></pre>

<hr>
<h2 id='weights.survey.design'>Survey design weights</h2><span id='topic+weights.survey.design'></span><span id='topic+weights.svyrep.design'></span><span id='topic+weights.survey_fpc'></span>

<h3>Description</h3>

<p>Extract weights from a survey design object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survey.design'
weights(object, ...)
## S3 method for class 'svyrep.design'
weights(object,
type=c("replication","sampling","analysis"), ...)
## S3 method for class 'survey_fpc'
weights(object,final=TRUE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weights.survey.design_+3A_object">object</code></td>
<td>
<p>Survey design object</p>
</td></tr>
<tr><td><code id="weights.survey.design_+3A_type">type</code></td>
<td>
<p>Type of weights: <code>"analysis"</code> combines sampling and
replication weights.</p>
</td></tr>
<tr><td><code id="weights.survey.design_+3A_final">final</code></td>
<td>
<p>If <code>FALSE</code> return a data frame with sampling
weights at each stage of sampling.</p>
</td></tr>
<tr><td><code id="weights.survey.design_+3A_...">...</code></td>
<td>
<p>Other arguments ignored </p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector or matrix of weights
</p>


<h3>See Also</h3>

<p><code><a href="#topic+svydesign">svydesign</a></code>, <code><a href="#topic+svrepdesign">svrepdesign</a></code>,
<code><a href="#topic+as.fpc">as.fpc</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(scd)


scddes&lt;-svydesign(data=scd, prob=~1, id=~ambulance, strata=~ESA,
                 nest=TRUE, fpc=rep(5,6))
repweights&lt;-2*cbind(c(1,0,1,0,1,0), c(1,0,0,1,0,1), c(0,1,1,0,0,1), c(0,1,0,1,1,0))
scdrep&lt;-svrepdesign(data=scd, type="BRR", repweights=repweights)

weights(scdrep)
weights(scdrep, type="sampling")
weights(scdrep, type="analysis")
weights(scddes)

</code></pre>

<hr>
<h2 id='with.svyimputationList'>Analyse multiple imputations</h2><span id='topic+with.svyimputationList'></span><span id='topic+subset.svyimputationList'></span>

<h3>Description</h3>

<p>Performs a survey analysis on each of the designs in a
<code>svyimputationList</code> objects and returns a list of results suitable
for <code>MIcombine</code>. The analysis may be specified as an expression or
as a function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'svyimputationList'
with(data, expr, fun, ...,multicore=getOption("survey.multicore"))
## S3 method for class 'svyimputationList'
subset(x, subset,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="with.svyimputationList_+3A_data">data</code>, <code id="with.svyimputationList_+3A_x">x</code></td>
<td>
<p>A <code>svyimputationList</code> object </p>
</td></tr>
<tr><td><code id="with.svyimputationList_+3A_expr">expr</code></td>
<td>
<p>An expression giving a survey analysis</p>
</td></tr>
<tr><td><code id="with.svyimputationList_+3A_fun">fun</code></td>
<td>
<p>A function taking a survey design object as its argument </p>
</td></tr>
<tr><td><code id="with.svyimputationList_+3A_...">...</code></td>
<td>
<p>for future expansion </p>
</td></tr>
<tr><td><code id="with.svyimputationList_+3A_multicore">multicore</code></td>
<td>
<p>Use <code>multicore</code> package to distribute imputed data sets over multiple processors?</p>
</td></tr>
<tr><td><code id="with.svyimputationList_+3A_subset">subset</code></td>
<td>
<p>An logical expression specifying the subset</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of the results from applying the analysis to each design object.
</p>


<h3>See Also</h3>

<p><code>MIcombine</code>, in the <code>mitools</code> package </p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mitools)
data.dir&lt;-system.file("dta",package="mitools")
files.men&lt;-list.files(data.dir,pattern="m.\\.dta$",full=TRUE)
men&lt;-imputationList(lapply(files.men, foreign::read.dta,
	warn.missing.labels=FALSE))
files.women&lt;-list.files(data.dir,pattern="f.\\.dta$",full=TRUE)
women&lt;-imputationList(lapply(files.women, foreign::read.dta,
	warn.missing.labels=FALSE))
men&lt;-update(men, sex=1)
women&lt;-update(women,sex=0)
all&lt;-rbind(men,women)

designs&lt;-svydesign(id=~id, strata=~sex, data=all)
designs

results&lt;-with(designs, svymean(~drkfre))

MIcombine(results)

summary(MIcombine(results))

repdesigns&lt;-as.svrepdesign(designs, type="boot", replicates=50)
MIcombine(with(repdesigns, svymean(~drkfre)))

</code></pre>

<hr>
<h2 id='withPV.survey.design'>
Analyse plausible values in surveys
</h2><span id='topic+withPV.survey.design'></span>

<h3>Description</h3>

<p>Repeats an analysis for each of a set of 'plausible values' in a survey data set, returning a list suitable for <code>mitools::MIcombine</code>. The default method works for both standard and replicate-weight designs but not for two-phase designs. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survey.design'
withPV(mapping, data, action, rewrite=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="withPV.survey.design_+3A_mapping">mapping</code></td>
<td>

<p>A formula or list of formulas describing each variable in the analysis that has plausible values. The left-hand side of the formula is the name to use in the analysis; the right-hand side gives the names in the dataset.
</p>
</td></tr>
<tr><td><code id="withPV.survey.design_+3A_data">data</code></td>
<td>

<p>A survey design object, as created by <code>svydesign</code> or <code>svrepdesign</code>
</p>
</td></tr>
<tr><td><code id="withPV.survey.design_+3A_action">action</code></td>
<td>

<p>With <code>rewrite=TRUE</code>, a function taking a survey design object as
its only argument, or a quoted expression.  With <code>rewrite=TRUE</code>
a function taking a survey design object as its only argument, or a
quoted expression with <code>.DESIGN</code> referring to the survey design object to be used.
</p>
</td></tr>
<tr><td><code id="withPV.survey.design_+3A_rewrite">rewrite</code></td>
<td>

<p>Rewrite <code>action</code> before evaluating it (versus constructing new data
sets)
</p>
</td></tr>
<tr><td><code id="withPV.survey.design_+3A_...">...</code></td>
<td>

<p>For methods
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of the results returned by each evaluation of <code>action</code>, with the call as an attribute.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+with.svyimputationList">with.svyimputationList</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require(mitools)){
data(pisamaths, package="mitools")
des&lt;-svydesign(id=~SCHOOLID+STIDSTD, strata=~STRATUM, nest=TRUE,
	weights=~W_FSCHWT+condwt, data=pisamaths)

oo&lt;-options(survey.lonely.psu="remove")

results&lt;-withPV(list(maths~PV1MATH+PV2MATH+PV3MATH+PV4MATH+PV5MATH),
   data=des,
   action=quote(svyglm(maths~ST04Q01*(PCGIRLS+SMRATIO)+MATHEFF+OPENPS, design=des)),
   rewrite=TRUE)

summary(MIcombine(results))
options(oo)
}
</code></pre>

<hr>
<h2 id='withReplicates'>Compute variances by replicate weighting</h2><span id='topic+withReplicates'></span><span id='topic+withReplicates.svyrep.design'></span><span id='topic+withReplicates.svrepvar'></span><span id='topic+withReplicates.svrepstat'></span><span id='topic+withReplicates.svyimputationList'></span><span id='topic+vcov.svyrep.design'></span>

<h3>Description</h3>

<p>Given a function or expression computing a statistic based on sampling
weights, <code>withReplicates</code> evaluates the statistic and produces a
replicate-based estimate of variance. <code>vcov.svrep.design</code> produces
the variance estimate from a set of replicates and the design object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>withReplicates(design, theta,..., return.replicates=FALSE)
## S3 method for class 'svyrep.design'
withReplicates(design, theta, rho = NULL, ..., 
     scale.weights=FALSE, return.replicates=FALSE)
## S3 method for class 'svrepvar'
withReplicates(design, theta,  ...,  return.replicates=FALSE)
## S3 method for class 'svrepstat'
withReplicates(design, theta,  ...,  return.replicates=FALSE)
## S3 method for class 'svyimputationList'
withReplicates(design, theta,  ...,  return.replicates=FALSE)
## S3 method for class 'svyrep.design'
vcov(object, replicates, centre,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="withReplicates_+3A_design">design</code></td>
<td>
<p>A survey design with replicate weights (eg from <code><a href="#topic+svrepdesign">svrepdesign</a></code>) or a suitable object with replicate parameter estimates</p>
</td></tr>
<tr><td><code id="withReplicates_+3A_theta">theta</code></td>
<td>
<p>A function or expression: see Details below</p>
</td></tr>
<tr><td><code id="withReplicates_+3A_rho">rho</code></td>
<td>
<p>If <code>design</code> uses BRR weights, <code>rho</code> optionally
specifies the parameter for Fay's variance estimator.</p>
</td></tr>
<tr><td><code id="withReplicates_+3A_...">...</code></td>
<td>
<p>Other arguments to <code>theta</code></p>
</td></tr>
<tr><td><code id="withReplicates_+3A_scale.weights">scale.weights</code></td>
<td>
<p>Divide the probability weights by their sum (can
help with overflow problems)</p>
</td></tr>
<tr><td><code id="withReplicates_+3A_return.replicates">return.replicates</code></td>
<td>
<p>Return the replicate estimates as well as
the variance?</p>
</td></tr>
<tr><td><code id="withReplicates_+3A_object">object</code></td>
<td>
<p>The replicate-weights design object used to create the replicates</p>
</td></tr>
<tr><td><code id="withReplicates_+3A_replicates">replicates</code></td>
<td>
<p>A set of replicates</p>
</td></tr>
<tr><td><code id="withReplicates_+3A_centre">centre</code></td>
<td>
<p>The centering value for variance calculation. If
<code>object$mse</code> is <code>TRUE</code> 
this is the result of estimation using the sampling weights, and
must be supplied.  If  <code>object$mse</code> is <code>FALSE</code> the
mean of the replicates is used and this argument is silently ignored.
</p>
</td></tr></table>


<h3>Details</h3>

<p>The method for <code>svyrep.design</code> objects evaluates a function or
expression using the sampling weights and then each set of replicate
weights.  The method for <code>svrepvar</code> objects evaluates the function
or expression on an estimated population covariance matrix and its
replicates, to simplify multivariate statistics such as structural
equation models.
</p>
<p>For the <code>svyrep.design</code> method, if <code>theta</code> is a function its first argument will be a vector of
weights and the second argument will be a data frame containing the
variables from the design object.   If it is an expression, the sampling weights will be available as the
variable <code>.weights</code>.  Variables in the design object will also
be in scope.  It is possible to use global variables in the
expression, but unwise, as they may be masked by local variables
inside <code>withReplicates</code>.
</p>
<p>For the <code>svrepvar</code> method a function will get the covariance
matrix as its first argument, and an expression will be evaluated with
<code>.replicate</code> set to the variance matrix.
</p>
<p>For the <code>svrepstat</code> method a function will get the point estimate, and an expression will be evaluated with
<code>.replicate</code> set to each replicate.  The method can only be used
when the <code>svrepstat</code> object includes replicates.
</p>
<p>The <code>svyimputationList</code> method runs <code>withReplicates</code> on each imputed design (which must be replicate-weight designs).
</p>


<h3>Value</h3>

<p>If <code>return.replicates=FALSE</code>,  the weighted statistic, with the
variance matrix as the <code>"var"</code> attribute. If
<code>return.replicates=TRUE</code>, a list with elements <code>theta</code> for
the usual return value and <code>replicates</code> for the replicates.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+svrepdesign">svrepdesign</a></code>, <code><a href="#topic+as.svrepdesign">as.svrepdesign</a></code>, <code><a href="#topic+svrVar">svrVar</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(scd)
repweights&lt;-2*cbind(c(1,0,1,0,1,0), c(1,0,0,1,0,1), c(0,1,1,0,0,1),
c(0,1,0,1,1,0))
scdrep&lt;-svrepdesign(data=scd, type="BRR", repweights=repweights)

a&lt;-svyratio(~alive, ~arrests, design=scdrep)
print(a$ratio)
print(a$var)
withReplicates(scdrep, quote(sum(.weights*alive)/sum(.weights*arrests)))
withReplicates(scdrep, function(w,data)
sum(w*data$alive)/sum(w*data$arrests))

data(api)
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)
rclus1&lt;-as.svrepdesign(dclus1)
varmat&lt;-svyvar(~api00+api99+ell+meals+hsg+mobility,rclus1,return.replicates=TRUE)
withReplicates(varmat, quote( factanal(covmat=.replicate, factors=2)$unique) )


data(nhanes)
nhanesdesign &lt;- svydesign(id=~SDMVPSU, strata=~SDMVSTRA, weights=~WTMEC2YR, nest=TRUE,data=nhanes)
logistic &lt;- svyglm(HI_CHOL~race+agecat+RIAGENDR, design=as.svrepdesign(nhanesdesign),
family=quasibinomial, return.replicates=TRUE)
fitted&lt;-predict(logistic, return.replicates=TRUE, type="response")
sensitivity&lt;-function(pred,actual) mean(pred&gt;0.1 &amp; actual)/mean(actual)
withReplicates(fitted, sensitivity, actual=logistic$y)

## Not run: 
library(quantreg)
data(api)
## one-stage cluster sample
dclus1&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1, fpc=~fpc)
## convert to bootstrap
bclus1&lt;-as.svrepdesign(dclus1,type="bootstrap", replicates=100)

## median regression
withReplicates(bclus1, quote(coef(rq(api00~api99, tau=0.5, weights=.weights))))

## End(Not run)


## pearson correlation
dstrat &lt;- svydesign(id=~1,strata=~stype, weights=~pw, data=apistrat, fpc=~fpc)
bstrat&lt;- as.svrepdesign(dstrat,type="subbootstrap")

v &lt;- svyvar(~api00+api99, bstrat, return.replicates=TRUE)
vcor&lt;-cov2cor(as.matrix(v))[2,1]
vreps&lt;-v$replicates
correps&lt;-apply(vreps,1, function(v) v[2]/sqrt(v[1]*v[4]))

vcov(bstrat,correps, centre=vcor)


</code></pre>

<hr>
<h2 id='xdesign'>
Crossed effects and other sparse correlations
</h2><span id='topic+xdesign'></span>

<h3>Description</h3>

<p>Defines a design object with multiple dimensions of correlation:
observations that share any of the <code>id</code> variables are correlated,
or you can supply an adjacency matrix or Matrix to specify which are
correlated. Supports crossed designs (eg multiple raters of multiple
objects) and non-nested observational correlation (eg observations
sharing primary school or secondary school). Has methods for
<code>svymean</code>, <code>svytotal</code>, <code>svyglm</code> (so far). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xdesign(id = NULL, strata = NULL, weights = NULL, data, fpc = NULL,
adjacency = NULL, overlap = c("unbiased", "positive"), allow.non.binary = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xdesign_+3A_id">id</code></td>
<td>

<p>list of formulas specifying cluster identifiers for each clustering dimension (or <code>NULL</code>)
</p>
</td></tr>
<tr><td><code id="xdesign_+3A_strata">strata</code></td>
<td>

<p>Not implemented
</p>
</td></tr>
<tr><td><code id="xdesign_+3A_weights">weights</code></td>
<td>

<p>model formula specifying (sampling) weights
</p>
</td></tr>
<tr><td><code id="xdesign_+3A_data">data</code></td>
<td>

<p>data frame containing all the variables
</p>
</td></tr>
<tr><td><code id="xdesign_+3A_fpc">fpc</code></td>
<td>

<p>Not implemented
</p>
</td></tr>
<tr><td><code id="xdesign_+3A_adjacency">adjacency</code></td>
<td>

<p>Adjacency matrix or Matrix indicating which pairs of observations
are correlated
</p>
</td></tr>
<tr><td><code id="xdesign_+3A_overlap">overlap</code></td>
<td>

<p>See details below
</p>
</td></tr>
<tr><td><code id="xdesign_+3A_allow.non.binary">allow.non.binary</code></td>
<td>

<p>If <code>FALSE</code> check that <code>adjacency</code> is a binary 0/1 or
<code>TRUE</code>/<code>FALSE</code> matrix or Matrix.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Subsetting for these objects actually drops observations; it is not
equivalent to just setting weights to zero as for survey
designs. So, for example, a subset of a balanced design will not be
a balanced design.
</p>
<p>The <code>overlap</code> option controls double-counting of some variance
terms. Suppose there are two clustering dimensions, <code>~a</code> and
<code>~b</code>. If we compute variance matrices clustered on <code>a</code> and
clustered on <code>b</code> and add them, observations that share both
<code>a</code> and <code>b</code> will be counted twice, giving a positively
biased estimator.  We can subtract off a variance matrix clustered
on combinations of <code>a</code> and <code>b</code> to give an unbiased
variance estimator.  However, the unbiased estimator is not
guaranteed to be positive definite. In the references, Miglioretti
and Heagerty use the <code>overlap="positive"</code> estimator and Cameron
et al use the <code>overlap="unbiased"</code> estimator. 
</p>


<h3>Value</h3>

<p>An object of class <code>xdesign</code>
</p>


<h3>References</h3>

<p>Miglioretti D, Heagerty PJ (2007) Marginal modeling of nonnested
multilevel data using standard software. Am J Epidemiol 165(4):453-63
</p>
<p>Cameron, A. C., Gelbach, J. B., &amp; Miller, D. L. (2011). Robust Inference
With Multiway Clustering. Journal of Business &amp; Economic Statistics,
29(2), 238-249.
</p>
<p><a href="https://notstatschat.rbind.io/2021/09/18/crossed-clustering-and-parallel-invention/">https://notstatschat.rbind.io/2021/09/18/crossed-clustering-and-parallel-invention/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+salamander">salamander</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>

## With one clustering dimension, is close to the with-replacement
##   survey estimator, but not identical unless clusters are equal size
data(api)
dclus1r&lt;-svydesign(id=~dnum, weights=~pw, data=apiclus1)
xclus1&lt;-xdesign(id=list(~dnum), weights=~pw, data=apiclus1)
xclus1

svymean(~enroll,dclus1r)
svymean(~enroll,xclus1)

data(salamander)
xsalamander&lt;-xdesign(id=list(~Male, ~Female), data=salamander,
    overlap="unbiased")
xsalamander
degf(xsalamander)

</code></pre>

<hr>
<h2 id='yrbs'>
One variable from the Youth Risk Behaviors Survey, 2015.
</h2><span id='topic+yrbs'></span>

<h3>Description</h3>

<p>Design information from the Youth Risk Behaviors Survey (YRBS), together
with the single variable &lsquo;Never/Rarely wore bike helmet&rsquo;.  Used as an
analysis example by CDC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("yrbs")</code></pre>


<h3>Format</h3>

<p>A data frame with 15624 observations on the following 4 variables.
</p>

<dl>
<dt><code>weight</code></dt><dd><p>sampling weights</p>
</dd>
<dt><code>stratum</code></dt><dd><p>sampling strata</p>
</dd>
<dt><code>psu</code></dt><dd><p>primary sampling units</p>
</dd>
<dt><code>qn8</code></dt><dd><p>1=Yes, 2=No</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="https://ftp.cdc.gov/pub/Data/YRBS/2015smy/">https://ftp.cdc.gov/pub/Data/YRBS/2015smy/</a> for files
</p>


<h3>References</h3>

<p>Centers for Disease Control and Prevention (2016) Software for Analysis
of YRBS Data. [CRAN doesn't believe the URL is valid]
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(yrbs)

yrbs_design &lt;- svydesign(id=~psu, weight=~weight, strata=~stratum,
data=yrbs)
yrbs_design &lt;- update(yrbs_design, qn8yes=2-qn8)

ci &lt;- svyciprop(~qn8yes, yrbs_design, na.rm=TRUE, method="xlogit")
ci

## to print more digits: matches SUDAAN and SPSS exactly, per table 3 of reference
coef(ci)
SE(ci)
attr(ci,"ci")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
