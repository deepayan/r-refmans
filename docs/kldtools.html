<!DOCTYPE html><html lang="en"><head><title>Help for package kldtools</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {kldtools}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#kldtools'><p>Kullback-Leibler Divergence (KLD) and Turing's perspective</p>
estimator</a></li>
<li><a href='#ksboot'><p>Bootstrapping based on the Kolmogorov-Smirnov test</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Kullback-Leibler Divergence and Other Tools to Analyze
Frequencies</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-11-15</td>
</tr>
<tr>
<td>Description:</td>
<td>Most importantly, calculates Kullback-Leibler Divergence (KLD), Turing's perspective estimator
 and their confidence intervals.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-11-15 11:26:17 UTC; alexey</td>
</tr>
<tr>
<td>Author:</td>
<td>Alexey Shipunov [cre],
  Kateryna Krykoniuk [aut],
  Jasjeet Sekhon [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Alexey Shipunov &lt;dactylorhiza@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-11-15 12:10:29 UTC</td>
</tr>
</table>
<hr>
<h2 id='kldtools'>Kullback-Leibler Divergence (KLD) and Turing's perspective
estimator</h2><span id='topic+kldtools'></span>

<h3>Description</h3>

<p>Calculates three estimators of Kullback-Leibler Divergence
(KLD): KLD, symmetrized KLD and Turing's perspective KLD along with their
confidence intervals.</p>


<h3>Usage</h3>

<pre><code class='language-R'>
kldtools(x, y, threshold=0.975)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kldtools_+3A_x">x</code></td>
<td>
<p>The first vector of frequencies with a length of 2 or more</p>
</td></tr>
<tr><td><code id="kldtools_+3A_y">y</code></td>
<td>
<p>The second vector of frequencies with a length of 2 or more</p>
</td></tr>
<tr><td><code id="kldtools_+3A_threshold">threshold</code></td>
<td>
<p>The threshold for declaring statistical significance,
the default is 0.975 (5%)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function computes three estimators of Kullback-Leibler Divergence
(KLD): KLD, symmetrized KLD and Turing's perspective KLD, based on Zhang
(2017). It compares two empirical, discrete distributions with the
confidence interval estimate. The limiting distribution in this method is
normal. Simply speaking, these estimators measure the difference between
two probability distributions.
</p>
<p>More specifically, the function calculates the confidence intervals for
these KLD estimators with the measure of standard deviation (sd), for
which the equation is different from that of the most known theories in
the field (for more detail, see Zhang 2017, Section 5.3):
</p>
<p>sd^2 = g(t(v))*sum(v)*g(v)
</p>
<p>However, our function uses a corrected formula of the vector g(v) in  the
above-given formula, the inconsistency in which has been identified by
comparing the results obtained from the application of this formula  and
its simplified version for k = 2 (Zhang 2017: 187).
</p>
<p>A deeper enquiry into this problem has led to detecting the formatting
error in formula (5.94) of the book (Zhang 2017: 185): it is missing the
elements for the p distribution. We would like to thank Dr. Jialin Zhang for
verifying the correct formula of the vector in the calculation of the
variance  of the KLD plug-in estimator.
</p>
<p>Also, note that if there are only two elements in probability
distributions, an infinite bias emerges (Zhang &amp; Grabchak 2014), which
makes the  estimation less reliable.
</p>
<p>The measure of symmetrized KLD (S) is calculated with the following
formula:
</p>
<p>S = S(p, q) = 1/2*(KLD(p||q) + KLD(q||p)
</p>
<p>Although similar to Jensen-Shannon Divergence (JSD), this measure is
different in that it is not a smoothed version of divergence.
</p>
<p>Please note that the discussed estimators (which are based on empirical
data) allow for a negative value of KLD (i.e. in the values of the CIs
and Turing's perspective estimator), despite the proven fact that
theoretical KLD should always take a non-negative value. Even though
precluded by Zhang's (2017: 152, Theorem 5.1) theory, it is possible to
apply these tests, since, in fact, they compare the absolute values of
fluctuation of the properly normalized empirical counterparts of KLD
around a theoretical value of KLD with the quantile of normal 
distribution.
</p>
<p>The application areas for these three estimators might be different. By
way of suggestion, the KLD estimator is appropriate for a study of
differences between distributions in systems (samples), whose design
criteria differ. On the other hand, symmetrized KLD may be more suitable
for the systems (samples) with a more similar design.
</p>
<p>In addition, the differences between KLD and symmetrized KLD can inform
researchers on a degree of symmetry between the systems (samples): the
larger the  difference between these measures, the greater the asymmetry
between their distributions.
</p>
<p>Finally, Turing's perspective estimator is believed to yield more precise
results and is appropriate for the data containing zeros (i.e. only in
the q distribution), which are handled with the augmentation added to the
formula of standard deviation (i.e. to the vector g(v) in the
above-mentioned formula of sd).
</p>


<h3>Value</h3>

<p>The list with the following components: &quot;KLD&quot; stands for the measure of
KLD; &quot;KLD.s&quot; for the measure of symmetrized KLD; &quot;Turing&quot; for the measure
of Turing's perspective KLD - and &quot;*sd&quot; for their respective standard
deviations; and &quot;*ci.left&quot; for the lower and &quot;*ci.right&quot; for the upper
limits of their respective confidence intervals - &quot;*CI&quot;.
</p>


<h3>Author(s)</h3>

<p>Kateryna Krykoniuk, Alexey Shipunov</p>


<h3>References</h3>

<p>Zhang, Zh. &amp; Grabchak, M. (2014). Nonparametric estimation of
Kullback-Leibler divergence. Neural computation 26 (11), pp. 2570-2593.
DOI: 10.1162/NECO_a_00646
</p>
<p>Zhang, Zh. (2017). Statistical Implications of Turing's Formula. Newark:
John Wiley &amp; Sons, Incorporated.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data &lt;- data.frame(V1=c(1213, 57683, 74466, 44419, 17481, 3403, 42252, 7045,
 29445, 15004, 21337, 1892, 21861, 238, 26574, 17579),
 V2=c(3185, 29692, 12570, 26081, 4992, 1659, 16592, 1748, 37583, 6751, 10188,
 355, 8116, 9, 5064, 1846))

kldtools(data$V1, data$V2)

</code></pre>

<hr>
<h2 id='ksboot'>Bootstrapping based on the Kolmogorov-Smirnov test</h2><span id='topic+ksboot'></span>

<h3>Description</h3>

<p>Performs bootstrapping with the Kolmogorov-Smirnov test to
estimate differences between frequencies</p>


<h3>Usage</h3>

<pre><code class='language-R'>
ksboot(x, y, nboots=1000, alternative=c("two.sided", "less", "greater"))

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ksboot_+3A_x">x</code></td>
<td>
<p>The first vector</p>
</td></tr>
<tr><td><code id="ksboot_+3A_y">y</code></td>
<td>
<p>The second vector</p>
</td></tr>
<tr><td><code id="ksboot_+3A_nboots">nboots</code></td>
<td>
<p>The number of bootstraps to perform</p>
</td></tr>
<tr><td><code id="ksboot_+3A_alternative">alternative</code></td>
<td>
<p>The type of alternative hypothesis (the default is
&quot;two.sided&quot;)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This bootstrap version of the Kolmogorov-Smirnov test is suitable for
estimating not only continuous but also frequency distributions. This is
because bootstrap theories suggest that the asymptotic theory of
estimates (which is built on the bootstrapping data) is, in a sense,
similar to the asymptotic theory of large data sets. Hence, although the
Kolmogorov-Smirnov test is initially designed for continuous
distributions, in bootstrapping, it is possible to apply this method to
discrete random variables, for which an empirical distribution function
is built on the observed frequencies (see Abadie 2002 for an example).
</p>


<h3>Value</h3>

<p>The list with the following components: &quot;ksboot.pvalue&quot; for the bootstrap
p-value of the Kolmogorov-Smirnov test, calculated for the null hypothesis
that the probability densities of two compared distributions are the
same; &quot;nboots&quot; for the number of the completed bootstraps.
</p>


<h3>Author(s)</h3>

<p>Jasjeet S. Sekhon, Alexey Shipunov</p>


<h3>References</h3>

<p>Abadie A. 2002. Bootstrap tests for distributional treatment effects in
instrumental variable models. Journal of the American statistical
Association. 97: 284-292.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+ks.test">ks.test</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
data &lt;- stack(data.frame(V1=c(1213, 57683, 74466, 44419, 17481, 3403, 42252, 7045,
 29445, 15004, 21337, 1892, 21861, 238, 26574, 17579),
 V2=c(3185, 29692, 12570, 26081, 4992, 1659, 16592, 1748, 37583, 6751, 10188, 355,
 8116, 9, 5064, 1846)))

ksboot(data$values[data$ind == "V1"], data$values[data$ind == "V2"])

pairwise.table(function(i, j)
 suppressWarnings(ksboot(data$values[as.integer(data$ind) == i],
 data$values[as.integer(data$ind) == j])$ksboot.pvalue),
 levels(data$ind), p.adjust.method="bonferroni")

pairwise.table(function(i, j)
 suppressWarnings(ksboot(data$values[as.integer(data$ind) == i],
 data$values[as.integer(data$ind) == j])$ksboot.pvalue),
 levels(data$ind), p.adjust.method="none")

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
