<!DOCTYPE html><html><head><title>Help for package sambia</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sambia}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#costing'><p>Predicting outcomes using Costing.</p></a></li>
<li><a href='#genSample'><p>Generate synthetic observations using inverse-probability weights</p></a></li>
<li><a href='#IPbag'><p>Predicting outcomes using non-parametric Inverse-Probability bagging</p></a></li>
<li><a href='#ipOversampling'><p>Plain replication of each observation by inverse-probability weights</p></a></li>
<li><a href='#rejSamp'><p>Rejection Sampling is a method used in sambia's function 'costing' (Krautenbacher et al, 2017).</p></a></li>
<li><a href='#smoteMod'><p>smoteMod is a modified version of the 'synthetic minority oversampling technique to generate new data.</p></a></li>
<li><a href='#smoteNew'><p>smoteNew is a necessary function that modifies the SMOTE algorithm.</p></a></li>
<li><a href='#synthIPbag'><p>Train a classifier via synthetic observations using inverse-probability weights</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>A Collection of Techniques Correcting for Sample Selection Bias</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Author:</td>
<td>Norbert Krautenbacher, Kevin Strauss, Maximilian Mandl, Christiane Fuchs</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Norbert Krautenbacher &lt;norbert.krautenbacher@tum.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>A collection of various techniques correcting statistical models for sample selection bias is provided. In particular, the resampling-based methods "stochastic inverse-probability oversampling" and "parametric inverse-probability bagging" are placed at the disposal which generate synthetic observations for correcting classifiers for biased samples resulting from stratified random sampling. For further information, see the article Krautenbacher, Theis, and Fuchs (2017) &lt;<a href="https://doi.org/10.1155%2F2017%2F7847531">doi:10.1155/2017/7847531</a>&gt;. The methods may be used for further purposes where weighting and generation of new observations is needed.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1.9000</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-06-04 14:15:39 UTC; norbertkrautenbacher</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, mvtnorm, dplyr, smotefamily, e1071, ranger, pROC, FNN</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-06-06 11:27:57 UTC</td>
</tr>
</table>
<hr>
<h2 id='costing'>Predicting outcomes using Costing.</h2><span id='topic+costing'></span>

<h3>Description</h3>

<p>This method trains classifiers by correcting them for sample selection bias via Costing, a method
proposed in Zadrozny et al. (2003) . This method is
similar to sambia's IP bagging function in terms of resampling from the learning data and aggregation of the learned
algorithms. It differs in the implementation of resampling. Observations from the original data enters a resampled
data set only once at most.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>costing(..., learner, list.train.learner, list.predict.learner, n.bs,
  mod = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="costing_+3A_...">...</code></td>
<td>
<p>see the parameter rejSamp() of package sambia.</p>
</td></tr>
<tr><td><code id="costing_+3A_learner">learner</code></td>
<td>
<p>a character indicating which classifier is used to train. Note: set learner='rangerTree'
if random forest should be applied as in Krautenbacher et al. (2017), i.e. the correction step is incorporated in the inherent random forest resampling procedure.</p>
</td></tr>
<tr><td><code id="costing_+3A_list.train.learner">list.train.learner</code></td>
<td>
<p>a list of parameters specific to the classifier that will be trained. Note that the
parameter 'data' need not to be provided in this list since the training data which the model will learn on is already attained by new sampled data produced by the method rejSamp().</p>
</td></tr>
<tr><td><code id="costing_+3A_list.predict.learner">list.predict.learner</code></td>
<td>
<p>a list of parameters specifiying how to predict new data given the trained model.
(This trained model is uniquely determined by parameters 'learner' and 'list.train.learner'</p>
</td></tr>
<tr><td><code id="costing_+3A_n.bs">n.bs</code></td>
<td>
<p>number of bootstramp samples.</p>
</td></tr>
<tr><td><code id="costing_+3A_mod">mod</code></td>
<td>
<p>If mod = TRUE: strategy for always having (at least) two outcome classes in subsets.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Norbert Krautenbacher, Kevin Strauss, Maximilian Mandl, Christiane Fuchs
</p>


<h3>References</h3>

<p>Zadrozny, B., Langford, J., &amp; Abe, N. (2003, November). 
Cost-sensitive learning by cost-proportionate example weighting. 
In Data Mining, 2003. ICDM 2003. Third IEEE International Conference on (pp. 435-442). IEEE.
</p>
<p>Krautenbacher, N., Theis, F. J., &amp; Fuchs, C. (2017). Correcting Classifiers for Sample Selection Bias in Two-Phase Case-Control Studies. Computational and mathematical methods in medicine, 2017.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## simulate data for a population
require(pROC)

set.seed(1342334)
N = 100000
x1 &lt;- rnorm(N, mean=0, sd=1) 
x2 &lt;- rt(N, df=25)
x3 &lt;- x1 + rnorm(N, mean=0, sd=.6)
x4 &lt;- x2 + rnorm(N, mean=0, sd=1.3)
x5 &lt;- rbinom(N, 1, prob=.6)
x6 &lt;- rnorm(N, 0, sd = 1) # noise not known as variable
x7 &lt;- x1*x5 # interaction
x &lt;- cbind(x1, x2, x3, x4, x5, x6, x7)

## stratum variable (covariate)
xs &lt;- c(rep(1,0.1*N), rep(0,(1-0.1)*N))

## effects
beta &lt;- c(-1, 0.2, 0.4, 0.4, 0.5, 0.5, 0.6)
beta0 &lt;- -2

## generate binary outcome
linpred.slopes &lt;-  log(0.5)*xs + c(x %*% beta)
eta &lt;-  beta0 + linpred.slopes

p &lt;- 1/(1+exp(-eta)) # this is the probability P(Y=1|X), we want the binary outcome however:
y&lt;-rbinom(n=N, size=1, prob=p) #

population &lt;- data.frame(y,xs,x)

#### draw "given" data set for training
sel.prob &lt;- rep(1,N)
sel.prob[population$xs == 1] &lt;- 9
sel.prob[population$y == 1] &lt;- 8
sel.prob[population$y == 1 &amp; population$xs == 1] &lt;- 150
ind &lt;- sample(1:N, 200, prob = sel.prob)

data = population[ind, ]

## calculate weights from original numbers for xs and y
w.matrix &lt;- table(population$y, population$xs)/table(data$y, data$xs)
w &lt;- rep(NA, nrow(data))
w[data$y==0 &amp; data$xs ==0] &lt;- w.matrix[1,1]
w[data$y==1 &amp; data$xs ==0] &lt;- w.matrix[2,1]
w[data$y==0 &amp; data$xs ==1] &lt;- w.matrix[1,2]
w[data$y==1 &amp; data$xs ==1] &lt;- w.matrix[2,2]

### draw a test data set
newdata = population[sample(1:N, size=200 ), ]

n.bs = 20
pred_nb &lt;- sambia::costing(data = data, weights = w,
  learner='naiveBayes', list.train.learner = list(formula=formula(y~.)),
  list.predict.learner = list(newdata=newdata, type="raw"),n.bs = n.bs, mod=TRUE)
roc(newdata$y, pred_nb, direction="&lt;")
</code></pre>

<hr>
<h2 id='genSample'>Generate synthetic observations using inverse-probability weights</h2><span id='topic+genSample'></span>

<h3>Description</h3>

<p>This method corrects a given data set for sample selection bias by
generating new observations via Stochastic inverse-probability
oversampling or parametric inverse-probability sampling using
inverse-probability weights and information on covariance structure of the given strata (Krautenbacher et al, 2017).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>genSample(data, strata.variables = NULL, stratum = NULL, weights = rep(1,
  nrow(data)), distr = "mvnorm", type = c("parIP", "stochIP"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="genSample_+3A_data">data</code></td>
<td>
<p>a data frame containing the observations rowwise, along with their corresponding categorical strata feature.</p>
</td></tr>
<tr><td><code id="genSample_+3A_strata.variables">strata.variables</code></td>
<td>
<p>a character vector of the names determined by the categorical stratum features.</p>
</td></tr>
<tr><td><code id="genSample_+3A_stratum">stratum</code></td>
<td>
<p>a numerical vector of the length of the number of rows of the data specifying 
the stratum ID. Either 'strata.variables' or 'stratum' has to be provided.
This vector will not be included as a column in the resulting data set.</p>
</td></tr>
<tr><td><code id="genSample_+3A_weights">weights</code></td>
<td>
<p>a numerical vector whose length must coincide with the number of the rows of data. The i-th value contains the inverse-probability e.g. determines how often the i-th observation of data shall be replicated.</p>
</td></tr>
<tr><td><code id="genSample_+3A_distr">distr</code></td>
<td>
<p>character object that describes the distribution</p>
</td></tr>
<tr><td><code id="genSample_+3A_type">type</code></td>
<td>
<p>character which decides which method is used to correct a given data set for sample selection bias. Stochastic Inverse-Probabiltiy oversampling is applied if type = 'stochIP' or Parametric Inverse-Probability Bagging if type = 'parIP'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>$data data frame containing synthetic data which is corrected for sample selection bias by generating new observations via Stochastic inverse-probability oversampling or parametric inverse-probability oversampling.
</p>
<p>$orig.data original data frame which shall to corrected
</p>
<p>$stratum vector containing the stratum of each observation
</p>
<p>$method a character indicating which method was used. If method = 'stochIP' then Stochastic Inverse-Probabiltiy oversampling was used, 
if method = 'parIP' the Parametric Inverse-Probability sampling was used.
</p>
<p>$strata.tbl a data frame containing all variables and their feature occurences
</p>
<p>$N number of rows in data
</p>
<p>$n number of rows in original data
</p>


<h3>Author(s)</h3>

<p>Norbert Krautenbacher, Kevin Strauss, Maximilian Mandl, Christiane Fuchs
</p>


<h3>References</h3>

<p>Krautenbacher, N., Theis, F. J., &amp; Fuchs, C. (2017). Correcting Classifiers for Sample Selection Bias in Two-Phase Case-Control Studies. Computational and mathematical methods in medicine, 2017.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## simulate data for a population
require(pROC)

set.seed(1342334)
N = 100000
x1 &lt;- rnorm(N, mean=0, sd=1) 
x2 &lt;- rt(N, df=25)
x3 &lt;- x1 + rnorm(N, mean=0, sd=.6)
x4 &lt;- x2 + rnorm(N, mean=0, sd=1.3)
x5 &lt;- rbinom(N, 1, prob=.6)
x6 &lt;- rnorm(N, 0, sd = 1) # noise not known as variable
x7 &lt;- x1*x5 # interaction
x &lt;- cbind(x1, x2, x3, x4, x5, x6, x7)

## stratum variable (covariate)
xs &lt;- c(rep(1,0.1*N), rep(0,(1-0.1)*N))

## effects
beta &lt;- c(-1, 0.2, 0.4, 0.4, 0.5, 0.5, 0.6)
beta0 &lt;- -2

## generate binary outcome
linpred.slopes &lt;-  log(0.5)*xs + c(x %*% beta)
eta &lt;-  beta0 + linpred.slopes

p &lt;- 1/(1+exp(-eta)) # this is the probability P(Y=1|X), we want the binary outcome however:
y&lt;-rbinom(n=N, size=1, prob=p) #

population &lt;- data.frame(y,xs,x)

#### draw "given" data set 
sel.prob &lt;- rep(1,N)
sel.prob[population$xs == 1] &lt;- 9
sel.prob[population$y == 1] &lt;- 8
sel.prob[population$y == 1 &amp; population$xs == 1] &lt;- 150
ind &lt;- sample(1:N, 200, prob = sel.prob)

data = population[ind, ]

## calculate weights from original numbers for xs and y
w.matrix &lt;- table(population$y, population$xs)/table(data$y, data$xs)
w &lt;- rep(NA, nrow(data))
w[data$y==0 &amp; data$xs ==0] &lt;- w.matrix[1,1]
w[data$y==1 &amp; data$xs ==0] &lt;- w.matrix[2,1]
w[data$y==0 &amp; data$xs ==1] &lt;- w.matrix[1,2]
w[data$y==1 &amp; data$xs ==1] &lt;- w.matrix[2,2]
## parametric IP bootstrap sample
sample1 &lt;- sambia::genSample(data=data, strata.variables = c('y', 'xs'),
                          weights = w, type='parIP')
## stochastic IP oversampling; treating 'y' and 'xs' as usual input variable
## but using strata info unambiguously defined by the weights w                        
sample2 &lt;- sambia::genSample(data=data,
                            weights = w, type='stochIP', stratum= round(w))
</code></pre>

<hr>
<h2 id='IPbag'>Predicting outcomes using non-parametric Inverse-Probability bagging</h2><span id='topic+IPbag'></span>

<h3>Description</h3>

<p>This method trains classifiers by correcting them for sample selection bias via non-parametric
inverse-probability bagging. This method fits classifiers from different resampled data whose observations
are increased per stratum to correct for the bias in the original sample. The so attained ensemble of predictors
is aggregated by averaging.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IPbag(..., learner, list.train.learner, list.predict.learner, n.bs)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IPbag_+3A_...">...</code></td>
<td>
<p>see the parameter ipOversampling() of package sambia.</p>
</td></tr>
<tr><td><code id="IPbag_+3A_learner">learner</code></td>
<td>
<p>a character indicating which classifier is used to train. Note: set learner='rangerTree'
if random forest should be applied as in Krautenbacher et al. (2017), i.e. the correction step is incorporated in the inherent random forest resampling procedure.</p>
</td></tr>
<tr><td><code id="IPbag_+3A_list.train.learner">list.train.learner</code></td>
<td>
<p>a list of parameters specific to the classifier that will be trained. Note that the
parameter 'data' need not to be provided in this list since the training data which the model will learn on is already attained by new sampled data produced by the method genSample().</p>
</td></tr>
<tr><td><code id="IPbag_+3A_list.predict.learner">list.predict.learner</code></td>
<td>
<p>a list of parameters specifiying how to predict new data given the learned model.
(This learned model is uniquely determined by parameters 'learner' and 'list.train.learner').</p>
</td></tr>
<tr><td><code id="IPbag_+3A_n.bs">n.bs</code></td>
<td>
<p>number of bootstramp samples.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Norbert Krautenbacher, Kevin Strauss, Maximilian Mandl, Christiane Fuchs
</p>


<h3>References</h3>

<p>Krautenbacher, N., Theis, F. J., &amp; Fuchs, C. (2017). Correcting Classifiers for Sample Selection Bias in Two-Phase Case-Control Studies. Computational and mathematical methods in medicine, 2017.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## simulate data for a population
require(pROC)

set.seed(1342334)
N = 100000
x1 &lt;- rnorm(N, mean=0, sd=1) 
x2 &lt;- rt(N, df=25)
x3 &lt;- x1 + rnorm(N, mean=0, sd=.6)
x4 &lt;- x2 + rnorm(N, mean=0, sd=1.3)
x5 &lt;- rbinom(N, 1, prob=.6)
x6 &lt;- rnorm(N, 0, sd = 1) # noise not known as variable
x7 &lt;- x1*x5 # interaction
x &lt;- cbind(x1, x2, x3, x4, x5, x6, x7)

## stratum variable (covariate)
xs &lt;- c(rep(1,0.1*N), rep(0,(1-0.1)*N))

## effects
beta &lt;- c(-1, 0.2, 0.4, 0.4, 0.5, 0.5, 0.6)
beta0 &lt;- -2

## generate binary outcome
linpred.slopes &lt;-  log(0.5)*xs + c(x %*% beta)
eta &lt;-  beta0 + linpred.slopes

p &lt;- 1/(1+exp(-eta)) # this is the probability P(Y=1|X), we want the binary outcome however:
y&lt;-rbinom(n=N, size=1, prob=p) #

population &lt;- data.frame(y,xs,x)

#### draw "given" data set for training
sel.prob &lt;- rep(1,N)
sel.prob[population$xs == 1] &lt;- 9
sel.prob[population$y == 1] &lt;- 8
sel.prob[population$y == 1 &amp; population$xs == 1] &lt;- 150
ind &lt;- sample(1:N, 200, prob = sel.prob)

data = population[ind, ]

## calculate weights from original numbers for xs and y
w.matrix &lt;- table(population$y, population$xs)/table(data$y, data$xs)
w &lt;- rep(NA, nrow(data))
w[data$y==0 &amp; data$xs ==0] &lt;- w.matrix[1,1]
w[data$y==1 &amp; data$xs ==0] &lt;- w.matrix[2,1]
w[data$y==0 &amp; data$xs ==1] &lt;- w.matrix[1,2]
w[data$y==1 &amp; data$xs ==1] &lt;- w.matrix[2,2]

### draw a test data set
newdata = population[sample(1:N, size=200 ), ]

n.bs = 5
pred_nb &lt;- sambia::IPbag(data = data, weights = w,
           learner='naiveBayes', list.train.learner = list(formula=formula(y~.)),
           list.predict.learner = list(newdata=newdata, type="raw"),
           n.bs = n.bs)
roc(newdata$y, pred_nb, direction="&lt;")
</code></pre>

<hr>
<h2 id='ipOversampling'>Plain replication of each observation by inverse-probability weights</h2><span id='topic+ipOversampling'></span>

<h3>Description</h3>

<p>This method corrects for the sample selection bias by the plain replication of each observation in the sample according to its IP weight,
i.e. in a stratified random sample one replicates an observation of stratum h by the factor w_h.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ipOversampling(data, weights, normalize = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ipOversampling_+3A_data">data</code></td>
<td>
<p>a data frame containing the observations rowwise, along with their corresponding categorical strata feature(s).</p>
</td></tr>
<tr><td><code id="ipOversampling_+3A_weights">weights</code></td>
<td>
<p>a numerical vector whose length must coincide with the number of the rows of data. The i-th value contains the inverse-probability e.g. determines how often the i-th observation of data shall be replicated.</p>
</td></tr>
<tr><td><code id="ipOversampling_+3A_normalize">normalize</code></td>
<td>
<p>If weight vector should be normalized, i.e. the smallest entry of the vector will be set to 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the numeric vector contains numbers which are not natural but real, they will be rounded.
</p>


<h3>Author(s)</h3>

<p>Norbert Krautenbacher, Kevin Strauss, Maximilian Mandl, Christiane Fuchs
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(smotefamily)
library(sambia)
data.example &lt;- sample_generator(100,ratio = 0.80)
result &lt;- gsub('n','0',data.example[,'result'])
result &lt;- gsub('p','1',result)
data.example[,'result'] &lt;- as.numeric(result)
weights &lt;- data.example[,'result']
weights &lt;- ifelse(weights==1,1,4)
sample &lt;- sambia::ipOversampling(data.example,weights)
</code></pre>

<hr>
<h2 id='rejSamp'>Rejection Sampling is a method used in sambia's function 'costing' (Krautenbacher et al, 2017).</h2><span id='topic+rejSamp'></span>

<h3>Description</h3>

<p>Rejection Sampling is a method used in sambias costing function. It is sampling scheme that allows us
to draw examples independently from a distribution X, given examples drawn independently from distribution Y.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rejSamp(data, weights)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rejSamp_+3A_data">data</code></td>
<td>
<p>a data frame containing the observations rowwise, along with their corresponding categorical strata feature</p>
</td></tr>
<tr><td><code id="rejSamp_+3A_weights">weights</code></td>
<td>
<p>a numerical vector whose length must coincide with the number of the rows of data. The i-th value contains the inverse-probability e.g. determines how often the i-th observation of data shall be replicated.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Norbert Krautenbacher, Kevin Strauss, Maximilian Mandl, Christiane Fuchs
</p>


<h3>References</h3>

<p>Krautenbacher, N., Theis, F. J., &amp; Fuchs, C. (2017). Correcting Classifiers for Sample Selection Bias in Two-Phase Case-Control Studies. Computational and mathematical methods in medicine, 2017.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(smotefamily)
library(sambia)
data.example &lt;- sample_generator(100,ratio = 0.80)
result &lt;- gsub('n','0',data.example[,'result'])
result &lt;- gsub('p','1',result)
data.example[,'result'] &lt;- as.numeric(result)
weights &lt;- data.example[,'result']
weights &lt;- ifelse(weights==1,1,4)
rej.sample &lt;- sambia:::rejSamp(data=data.example, weights = weights)
</code></pre>

<hr>
<h2 id='smoteMod'>smoteMod is a modified version of the 'synthetic minority oversampling technique to generate new data.</h2><span id='topic+smoteMod'></span>

<h3>Description</h3>

<p>This method adapts SMOTE to the context of stratified random samples. Rather than enlarging only the
minority class, smoteMod generates synthetic data for all strata with a weight bigger than 1.
Note: this function has to apply SMOTE H-1 times:
1. subsample data by smallest stratum and a stratum to oversample
2. oversample with modified SMOTE function according to weight of the stratum
3. do this for the other H-2 to subsamples
4. build new data set with strata where H-1 strata contain synthetic data (stratum with smallest weight remains as is)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smoteMod(data.x, stratum, weights, data.y = NULL, K)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smoteMod_+3A_data.x">data.x</code></td>
<td>
<p>A data frame or matrix of numeric-attributed dataset</p>
</td></tr>
<tr><td><code id="smoteMod_+3A_stratum">stratum</code></td>
<td>
<p>a numerical vector of the same length as the number of the rows of data. Depending on the number of strata variables and their number of exposures each such combination is assigned to a numeric class id. The i-th entry of stratum contains the class id (and therefore class belonging) of the i-th row (=observation) of data.</p>
</td></tr>
<tr><td><code id="smoteMod_+3A_weights">weights</code></td>
<td>
<p>a numerical vector whose length must coincide with the number of the rows of data. The i-th value contains the inverse-probability e.g. determines how often the i-th observation of data shall be replicated.</p>
</td></tr>
<tr><td><code id="smoteMod_+3A_data.y">data.y</code></td>
<td>
<p>A vector of a target class attribute corresponding to a dataset data.x.</p>
</td></tr>
<tr><td><code id="smoteMod_+3A_k">K</code></td>
<td>
<p>The number of nearest neighbors during sampling process</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Norbert Krautenbacher, Kevin Strauss, Maximilian Mandl, Christiane Fuchs
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## simulate data for a population
require(pROC)

set.seed(1342334)
N = 100000
x1 &lt;- rnorm(N, mean=0, sd=1) 
x2 &lt;- rt(N, df=25)
x3 &lt;- x1 + rnorm(N, mean=0, sd=.6)
x4 &lt;- x2 + rnorm(N, mean=0, sd=1.3)
x5 &lt;- rbinom(N, 1, prob=.6)
x6 &lt;- rnorm(N, 0, sd = 1) # noise not known as variable
x7 &lt;- x1*x5 # interaction
x &lt;- cbind(x1, x2, x3, x4, x5, x6, x7)

## stratum variable (covariate)
xs &lt;- c(rep(1,0.1*N), rep(0,(1-0.1)*N))

## effects
beta &lt;- c(-1, 0.2, 0.4, 0.4, 0.5, 0.5, 0.6)
beta0 &lt;- -2

## generate binary outcome
linpred.slopes &lt;-  log(0.5)*xs + c(x %*% beta)
eta &lt;-  beta0 + linpred.slopes

p &lt;- 1/(1+exp(-eta)) # this is the probability P(Y=1|X), we want the binary outcome however:
y&lt;-rbinom(n=N, size=1, prob=p) #

population &lt;- data.frame(y,xs,x)

#### draw "given" data set for training
sel.prob &lt;- rep(1,N)
sel.prob[population$xs == 1] &lt;- 9
sel.prob[population$y == 1] &lt;- 8
sel.prob[population$y == 1 &amp; population$xs == 1] &lt;- 150
ind &lt;- sample(1:N, 200, prob = sel.prob)

data = population[ind, ]

## calculate weights from original numbers for xs and y
w.matrix &lt;- table(population$y, population$xs)/table(data$y, data$xs)
w &lt;- rep(NA, nrow(data))
w[data$y==0 &amp; data$xs ==0] &lt;- w.matrix[1,1]
w[data$y==1 &amp; data$xs ==0] &lt;- w.matrix[2,1]
w[data$y==0 &amp; data$xs ==1] &lt;- w.matrix[1,2]
w[data$y==1 &amp; data$xs ==1] &lt;- w.matrix[2,2]

### draw a test data set
newdata = population[sample(1:N, size=200 ), ]

K = 5
genData = smoteMod(data.x = data[ , -which(colnames(data) %in% c('y', 'xs'))] , 
stratum = w, data.y = data$y, weights = w, K=K)
</code></pre>

<hr>
<h2 id='smoteNew'>smoteNew is a necessary function that modifies the SMOTE algorithm.</h2><span id='topic+smoteNew'></span>

<h3>Description</h3>

<p>smoteNewis a necessary function that modifies the SMOTE algorithm in the following ways: (1) correct bug in original
smotefamily::SMOTE() function and (2) lets the user specifiy which class to be oversampled.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smoteNew(data.x, data.y, K, dup_size = 0, class.to.oversample)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smoteNew_+3A_data.x">data.x</code></td>
<td>
<p>A data frame or matrix of numeric-attributed dataset</p>
</td></tr>
<tr><td><code id="smoteNew_+3A_data.y">data.y</code></td>
<td>
<p>A vector of a target class attribute corresponding to a dataset X</p>
</td></tr>
<tr><td><code id="smoteNew_+3A_k">K</code></td>
<td>
<p>The number of nearest neighbors during sampling process</p>
</td></tr>
<tr><td><code id="smoteNew_+3A_dup_size">dup_size</code></td>
<td>
<p>The number or vector representing the desired times of synthetic minority instances over the original number of majority instances</p>
</td></tr>
<tr><td><code id="smoteNew_+3A_class.to.oversample">class.to.oversample</code></td>
<td>
<p>Class to be oversampled</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Norbert Krautenbacher, Kevin Strauss, Maximilian Mandl, Christiane Fuchs
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(smotefamily)
library(sambia)
data.example = sample_generator(10000,ratio = 0.80)
genData = sambia:::smoteNew(data.example[,-3],data.example[,3],K = 5,class.to.oversample = 'p')
</code></pre>

<hr>
<h2 id='synthIPbag'>Train a classifier via synthetic observations using inverse-probability weights</h2><span id='topic+synthIPbag'></span>

<h3>Description</h3>

<p>This method trains classifiers by correcting them for sample selection bias via stochastic
inverse-probability oversampling or parametric inverse-probability bagging (Krautenbacher et al 2017). Classifiers are trained from
differently resampled data whose observations are weighted by inverse-probability weights per stratum to correct for the bias in the original
sample. The so attained ensemble of predictors is aggregated by averaging.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>synthIPbag(..., learner, list.train.learner, list.predict.learner, n.bs)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="synthIPbag_+3A_...">...</code></td>
<td>
<p>see the parameter genSample() of package sambia.</p>
</td></tr>
<tr><td><code id="synthIPbag_+3A_learner">learner</code></td>
<td>
<p>a character indicating which classifier is used to train. Note: set learner='rangerTree'
if random forest should be applied as in Krautenbacher et al. (2017), i.e. the correction step is incorporated 
in the inherent random forest resampling procedure.</p>
</td></tr>
<tr><td><code id="synthIPbag_+3A_list.train.learner">list.train.learner</code></td>
<td>
<p>a list of parameters specific to the classifier that will be trained. Note that the
parameter 'data' need not to be provided in this list since the training data which the model will learn
on is already attained by new sampled data produced by the method genSample().</p>
</td></tr>
<tr><td><code id="synthIPbag_+3A_list.predict.learner">list.predict.learner</code></td>
<td>
<p>a list of parameters specifiying how to predict new data given the trained model.</p>
</td></tr>
<tr><td><code id="synthIPbag_+3A_n.bs">n.bs</code></td>
<td>
<p>number of bootstramp samples. This trained model is uniquely determined by parameters 'learner' and 'list.train.learner'.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Norbert Krautenbacher, Kevin Strauss, Maximilian Mandl, Christiane Fuchs
</p>


<h3>References</h3>

<p>Krautenbacher, N., Theis, F. J., &amp; Fuchs, C. (2017). Correcting Classifiers for Sample Selection Bias in Two-Phase Case-Control Studies. Computational and mathematical methods in medicine, 2017.
</p>
<p>Krautenbacher, N., Theis, F. J., &amp; Fuchs, C. (2017). Correcting Classifiers for Sample Selection Bias in Two-Phase Case-Control Studies.
Computational and mathematical methods in medicine, 2017.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## simulate data for a population
require(pROC)

set.seed(1342334)
N = 100000
x1 &lt;- rnorm(N, mean=0, sd=1) 
x2 &lt;- rt(N, df=25)
x3 &lt;- x1 + rnorm(N, mean=0, sd=.6)
x4 &lt;- x2 + rnorm(N, mean=0, sd=1.3)
x5 &lt;- rbinom(N, 1, prob=.6)
x6 &lt;- rnorm(N, 0, sd = 1) # noise not known as variable
x7 &lt;- x1*x5 # interaction
x &lt;- cbind(x1, x2, x3, x4, x5, x6, x7)

## stratum variable (covariate)
xs &lt;- c(rep(1,0.1*N), rep(0,(1-0.1)*N))

## effects
beta &lt;- c(-1, 0.2, 0.4, 0.4, 0.5, 0.5, 0.6)
beta0 &lt;- -2

## generate binary outcome
linpred.slopes &lt;-  log(0.5)*xs + c(x %*% beta)
eta &lt;-  beta0 + linpred.slopes

p &lt;- 1/(1+exp(-eta)) # this is the probability P(Y=1|X), we want the binary outcome however:
y&lt;-rbinom(n=N, size=1, prob=p) #

population &lt;- data.frame(y,xs,x)

#### draw "given" data set for training
sel.prob &lt;- rep(1,N)
sel.prob[population$xs == 1] &lt;- 9
sel.prob[population$y == 1] &lt;- 8
sel.prob[population$y == 1 &amp; population$xs == 1] &lt;- 150
ind &lt;- sample(1:N, 200, prob = sel.prob)

data = population[ind, ]

## calculate weights from original numbers for xs and y
w.matrix &lt;- table(population$y, population$xs)/table(data$y, data$xs)
w &lt;- rep(NA, nrow(data))
w[data$y==0 &amp; data$xs ==0] &lt;- w.matrix[1,1]
w[data$y==1 &amp; data$xs ==0] &lt;- w.matrix[2,1]
w[data$y==0 &amp; data$xs ==1] &lt;- w.matrix[1,2]
w[data$y==1 &amp; data$xs ==1] &lt;- w.matrix[2,2]

### draw a test data set
newdata = population[sample(1:N, size=200 ), ]

n.bs = 10
## glm
pred_glm &lt;- sambia::synthIPbag(data = data, weights = w, type='parIP',
                              strata.variables = c('y', 'xs'), learner='glm', 
                              list.train.learner = list(formula=formula(y~.),family="binomial"),
                              list.predict.learner = list(newdata=newdata, type="response"),
                              n.bs = n.bs)
roc(newdata$y, pred_glm, direction = "&lt;")

## random forest
pred_rf &lt;- sambia::synthIPbag(data = data, weights = w, type='parIP',
                             strata.variables = c('y','xs'), learner='rangerTree', 
                             list.train.learner = list(formula=formula(as.factor(y)~.)),
                             list.predict.learner = list(data=newdata),
                             n.bs = n.bs)
roc(newdata$y, pred_rf, direction = "&lt;")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
