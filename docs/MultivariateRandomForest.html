<!DOCTYPE html><html><head><title>Help for package MultivariateRandomForest</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {MultivariateRandomForest}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#build_forest_predict'><p>Prediction using Random Forest or Multivariate Random Forest</p></a></li>
<li><a href='#build_single_tree'><p>Model of a single tree of Random Forest or Multivariate Random Forest</p></a></li>
<li><a href='#CrossValidation'><p>Generate training and testing samples for cross validation</p></a></li>
<li><a href='#Imputation'><p>Imputation of a numerical vector</p></a></li>
<li><a href='#Node_cost'><p>Information Gain</p></a></li>
<li><a href='#predicting'><p>Prediction of testing sample in a node</p></a></li>
<li><a href='#single_tree_prediction'><p>Prediction of Testing Samples for single tree</p></a></li>
<li><a href='#split_node'><p>Splitting Criteria of all the nodes of the tree</p></a></li>
<li><a href='#splitt2'><p>Split of the Parent node</p></a></li>
<li><a href='#variable_importance_measure'>
<p>Calculates variable Importance of a Regression Tree Model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Models Multivariate Cases Using Random Forests</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2017-04-05</td>
</tr>
<tr>
<td>Author:</td>
<td>Raziur Rahman</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Raziur Rahman &lt;razeeebuet@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Models and predicts multiple output features in single random forest considering the 
    linear relation among the output features, see details in Rahman et al (2017)&lt;<a href="https://doi.org/10.1093%2Fbioinformatics%2Fbtw765">doi:10.1093/bioinformatics/btw765</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, bootstrap, stats</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2017-05-01 00:21:14 UTC; Raziur_Rahman</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2017-05-01 10:20:31 UTC</td>
</tr>
</table>
<hr>
<h2 id='build_forest_predict'>Prediction using Random Forest or Multivariate Random Forest</h2><span id='topic+build_forest_predict'></span>

<h3>Description</h3>

<p>Builds Model of Random Forest or Multivariate Random Forest (when the number of output features &gt; 1) using training samples 
and generates the prediction of testing samples using the inferred model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_forest_predict(trainX, trainY, n_tree, m_feature, min_leaf, testX)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build_forest_predict_+3A_trainx">trainX</code></td>
<td>
<p>Input Feature matrix of M x N, M is the number of training samples and N is the number of input features</p>
</td></tr>
<tr><td><code id="build_forest_predict_+3A_trainy">trainY</code></td>
<td>
<p>Output Response matrix of M x T, M is the number of training samples and T is the number of ouput features</p>
</td></tr>
<tr><td><code id="build_forest_predict_+3A_n_tree">n_tree</code></td>
<td>
<p>Number of trees in the forest, which must be positive integer</p>
</td></tr>
<tr><td><code id="build_forest_predict_+3A_m_feature">m_feature</code></td>
<td>
<p>Number of randomly selected features considered for a split in each regression tree node, which must be positive integer and less than N (number of input features)</p>
</td></tr>
<tr><td><code id="build_forest_predict_+3A_min_leaf">min_leaf</code></td>
<td>
<p>Minimum number of samples in the leaf node. If a node has less than or equal to min_leaf samples,
then there will be no splitting in that node and this node will be considered as a leaf node. Valid input is positive integer, which is less than or
equal to M (number of training samples)</p>
</td></tr>
<tr><td><code id="build_forest_predict_+3A_testx">testX</code></td>
<td>
<p>Testing samples of size Q x N, where Q is the number of testing samples and N is the number of features 
(Same number of features as training samples)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Random Forest regression refers to ensembles of regression trees where a set of n_tree un-pruned regression
trees are generated based on bootstrap sampling from the original training data. For each node, the optimal
feature for node splitting is selected from a random set of m_feature from the total N features. The selection
of the feature for node splitting from a random set of features decreases the correlation between different
trees and thus the average prediction of multiple regression trees is expected to have lower variance than
individual regression trees. Larger m_feature can improve the predictive capability of individual trees but can also
increase the correlation between trees and void any gains from averaging multiple predictions. The bootstrap
resampling of the data for training each tree also increases the variation between the trees.
</p>
<p>In a node with training predictor features (X) and output feature vectors (Y), node splitting is done
with the aim of selecting a feature from a random set of m_feature and threshold z to partition the node 
into two child nodes, left node (with samples &lt; z) and right node (with samples &gt;=z). In multivariate trees (MRF) 
node cost is measured as the sum of squares of the Mahalanobis 
distance where as in univariate trees (RF) node cost is measured as the Euclidean distance.
</p>
<p>After the Model of the forest is built using training Input features (trainX) and output feature matrix (trainY),
the Model is used to generate the prediction of output features (testY) for the testing samples (testX).
</p>


<h3>Value</h3>

<p>Prediction result of the Testing samples
</p>


<h3>References</h3>

<p>[Random Forest] Breiman, Leo. &quot;Random forests.&quot; Machine learning 45.1 (2001): 5-32.
</p>
<p>[Multivariate Random Forest] Segal, Mark, and Yuanyuan Xiao. &quot;Multivariate random forests.&quot; 
Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 1.1 (2011): 80-87.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(MultivariateRandomForest)
#Input and Output Feature Matrix of random data (created using runif)
trainX=matrix(runif(50*100),50,100) 
trainY=matrix(runif(50*5),50,5) 
n_tree=2
m_feature=5
min_leaf=5
testX=matrix(runif(10*100),10,100) 
#Prediction size is 10 x 5, where 10 is the number 
#of testing samples and 5 is the number of output features
Prediction=build_forest_predict(trainX, trainY, n_tree, m_feature, min_leaf, testX)

</code></pre>

<hr>
<h2 id='build_single_tree'>Model of a single tree of Random Forest or Multivariate Random Forest</h2><span id='topic+build_single_tree'></span>

<h3>Description</h3>

<p>Build a Univariate Regression Tree (for generation of Random Forest (RF) ) or Multivariate Regression Tree ( for generation of Multivariate Random Forest (MRF) ) using the training samples,
which is used for the prediction of testing samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_single_tree(X, Y, m_feature, min_leaf, Inv_Cov_Y, Command)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build_single_tree_+3A_x">X</code></td>
<td>
<p>Input Feature matrix of M x N, M is the number of training samples and N is the number of input features</p>
</td></tr>
<tr><td><code id="build_single_tree_+3A_y">Y</code></td>
<td>
<p>Output Feature matrix of M x T, M is the number of training samples and T is the number of ouput features</p>
</td></tr>
<tr><td><code id="build_single_tree_+3A_m_feature">m_feature</code></td>
<td>
<p>Number of randomly selected features considered for a split in each regression tree node, which must be positive integer and less than N (number of input features)</p>
</td></tr>
<tr><td><code id="build_single_tree_+3A_min_leaf">min_leaf</code></td>
<td>
<p>Minimum number of samples in the leaf node, which must be positive integer and less than or equal to M (number of training samples)</p>
</td></tr>
<tr><td><code id="build_single_tree_+3A_inv_cov_y">Inv_Cov_Y</code></td>
<td>
<p>Inverse of Covariance matrix of Output Response matrix for MRF(Input [0 0;0 0] for RF)</p>
</td></tr>
<tr><td><code id="build_single_tree_+3A_command">Command</code></td>
<td>
<p>1 for univariate Regression Tree (corresponding to RF) and 2 for Multivariate Regression Tree (corresponding to MRF)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The regression tree structure is represented as a list of lists. For a non-leaf node, it contains the splitting criteria 
(feature for split and threshold) and for a leaf node, it contains the output responses for the samples contained in the leaf node.
</p>


<h3>Value</h3>

<p>Model of a single regression tree (Univariate or Multivariate Regression Tree). An example of the list of the non-leaf node:
</p>
<table>
<tr><td><code>Flag for determining whether the node is leaf node or branch node. 0 means branch node and 1 means leaf node.</code></td>
<td>
<p>1</p>
</td></tr>
<tr><td><code>Index of samples for the left node</code></td>
<td>
<p>int [1:34] 1 2 4 5 ...</p>
</td></tr>
<tr><td><code>Index of samples for the right node</code></td>
<td>
<p>int [1:16] 3 6 9 ...</p>
</td></tr>
<tr><td><code>Feature for split</code></td>
<td>
<p>int 34</p>
</td></tr>
<tr><td><code>Threshold values for split</code>, <code>average them</code></td>
<td>
<p>num [1:3] 0.655 0.526 0.785</p>
</td></tr>
<tr><td><code>List number for the left and right nodes</code></td>
<td>
<p>num [1:2] 2 3</p>
</td></tr>
</table>
<p>An example of the list of the leaf node:
</p>
<table>
<tr><td><code>Output responses</code></td>
<td>
<p>num[1:4,1:5] 0.0724 0.1809 0.0699 ...</p>
</td></tr>
</table>

<hr>
<h2 id='CrossValidation'>Generate training and testing samples for cross validation</h2><span id='topic+CrossValidation'></span>

<h3>Description</h3>

<p>Generates Cross Validation Input Matrices and Output Vectors for training and testing, where number of folds in cross validation is user defined.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CrossValidation(X, Y, F)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CrossValidation_+3A_x">X</code></td>
<td>
<p>M x N Input matrix, M is the number of samples and N is the number of features</p>
</td></tr>
<tr><td><code id="CrossValidation_+3A_y">Y</code></td>
<td>
<p>output responses as column vector</p>
</td></tr>
<tr><td><code id="CrossValidation_+3A_f">F</code></td>
<td>
<p>Number of Folds</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with the following components: 
</p>
<table>
<tr><td><code>TrainingData</code></td>
<td>
<p>List of matrices where each matrix contains a fold of Cross Validation Training Data, 
where the number of matrices is equal to F</p>
</td></tr>
<tr><td><code>TestingData</code></td>
<td>
<p>List of matrices where each matrix contains a fold of Cross Validation Testing Data, 
where the number of matrices is equal to F</p>
</td></tr>
<tr><td><code>OutputTrain</code></td>
<td>
<p>List of matrices where each matrix contains a fold of Cross Validation Training Output Feature Data, 
where the number of matrices is equal to F</p>
</td></tr>
<tr><td><code>OutputTest</code></td>
<td>
<p>List of matrices where each matrix contains a fold of Cross Validation Testing Output Feature Data, 
where the number of matrices is equal to F</p>
</td></tr>
<tr><td><code>FoldedIndex</code></td>
<td>
<p>Index of Different Folds. (e.g., for Sample Index 1:6 and 3 fold, FoldedIndex are [1 2 3 4], [1 2 5 6], [3 4 5 6])</p>
</td></tr>
</table>

<hr>
<h2 id='Imputation'>Imputation of a numerical vector</h2><span id='topic+Imputation'></span>

<h3>Description</h3>

<p>Imputes the values of the vector that are NaN
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Imputation(XX)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Imputation_+3A_xx">XX</code></td>
<td>
<p>a vector of size N x 1</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If a value is missing, it will be replaced by an imputed value that is an average of previous and 
next value. If previous or next value is also missing, the closest value is used as the imputed value.
</p>


<h3>Value</h3>

<p>Imputed vector of size N x 1
</p>

<hr>
<h2 id='Node_cost'>Information Gain</h2><span id='topic+Node_cost'></span>

<h3>Description</h3>

<p>Compute the cost function of a tree node
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Node_cost(y, Inv_Cov_Y, Command)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Node_cost_+3A_y">y</code></td>
<td>
<p>Output Features for the samples of the node</p>
</td></tr>
<tr><td><code id="Node_cost_+3A_inv_cov_y">Inv_Cov_Y</code></td>
<td>
<p>Inverse of Covariance matrix of Output Response matrix for MRF(Input [0 0;0 0] for RF)</p>
</td></tr>
<tr><td><code id="Node_cost_+3A_command">Command</code></td>
<td>
<p>1 for univariate Regression Tree (corresponding to RF) and 2 for Multivariate Regression Tree (corresponding to MRF)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In multivariate trees (MRF) node cost is measured as the sum of squares of the Mahalanobis distance to capture the correlations in 
the data whereas in univariate trees node cost is measured as the sum of Euclidean distance square. Mahalanobis Distance captures 
the distance of the sample point from the mean of the node along the principal component axes.
</p>


<h3>Value</h3>

<p>cost or entropy of samples in a node of a tree
</p>


<h3>References</h3>

<p>Segal, Mark, and Yuanyuan Xiao. &quot;Multivariate random forests.&quot; 
Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 1.1 (2011): 80-87.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(MultivariateRandomForest)
y=matrix(runif(10*2),10,2)
Inv_Cov_Y=solve(cov(y))
Command=2
#Command=2 for MRF and 1 for RF
#This function calculates information gain of a node
Cost=Node_cost(y,Inv_Cov_Y,Command)
</code></pre>

<hr>
<h2 id='predicting'>Prediction of testing sample in a node</h2><span id='topic+predicting'></span>

<h3>Description</h3>

<p>Provides the value of a testing sample in a node which refers to which child node it will go using the splitting criteria 
of the tree node or prediction results if the node is a leaf.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predicting(Single_Model, i, X_test, Variable_number)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predicting_+3A_single_model">Single_Model</code></td>
<td>
<p>Model of a particular tree</p>
</td></tr>
<tr><td><code id="predicting_+3A_i">i</code></td>
<td>
<p>Number of split. Used as an index, which indicates where in the list the splitting
criteria of this split has been stored.</p>
</td></tr>
<tr><td><code id="predicting_+3A_x_test">X_test</code></td>
<td>
<p>Testing samples of size Q x N, Q is the number of testing samples and N is the number of features (same order and
size used as training)</p>
</td></tr>
<tr><td><code id="predicting_+3A_variable_number">Variable_number</code></td>
<td>
<p>Number of Output Features</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function considers the output at a particular node. If the node is a leaf, the average of output responses 
is returned as prediction result. For a non-leaf node, the direction of left or right node is decided based on 
the node threshold and splitting feature value.
</p>


<h3>Value</h3>

<p>Prediction result of a testing samples in a node
</p>

<hr>
<h2 id='single_tree_prediction'>Prediction of Testing Samples for single tree</h2><span id='topic+single_tree_prediction'></span>

<h3>Description</h3>

<p>Predicts the output responses of testing samples based on the input regression tree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>single_tree_prediction(Single_Model, X_test, Variable_number)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="single_tree_prediction_+3A_single_model">Single_Model</code></td>
<td>
<p>Random Forest or Multivariate Random Forest Model of a particular tree</p>
</td></tr>
<tr><td><code id="single_tree_prediction_+3A_x_test">X_test</code></td>
<td>
<p>Testing samples of size Q x N, Q is the number of testing samples and N is the number of features (same order and
size used as training)</p>
</td></tr>
<tr><td><code id="single_tree_prediction_+3A_variable_number">Variable_number</code></td>
<td>
<p>Number of Output Features</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A regression tree model contains splitting criteria for all the splits in the tree and output responses of training 
samples in the leaf nodes. A testing sample using these criteria will reach a leaf node and the average of the 
Output response vectors in the leaf node is considered as the prediction of the testing sample.
</p>


<h3>Value</h3>

<p>Prediction result of the Testing samples for a particular tree
</p>

<hr>
<h2 id='split_node'>Splitting Criteria of all the nodes of the tree</h2><span id='topic+split_node'></span>

<h3>Description</h3>

<p>Stores the Splitting criteria of all the nodes of a tree in a list
</p>


<h3>Usage</h3>

<pre><code class='language-R'>split_node(X, Y, m_feature, Index, i, model, min_leaf, Inv_Cov_Y, Command)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="split_node_+3A_x">X</code></td>
<td>
<p>Input Training matrix of size M x N, M is the number of training samples and N is the number of features</p>
</td></tr>
<tr><td><code id="split_node_+3A_y">Y</code></td>
<td>
<p>Output Training response of size M x T, M is the number of samples and T is the number of output responses</p>
</td></tr>
<tr><td><code id="split_node_+3A_m_feature">m_feature</code></td>
<td>
<p>Number of randomly selected features considered for a split in each regression tree node</p>
</td></tr>
<tr><td><code id="split_node_+3A_index">Index</code></td>
<td>
<p>Index of training samples</p>
</td></tr>
<tr><td><code id="split_node_+3A_i">i</code></td>
<td>
<p>Number of split. Used as an index, which indicates where in the list the splitting
criteria of this split will be stored.</p>
</td></tr>
<tr><td><code id="split_node_+3A_model">model</code></td>
<td>
<p>A list of lists with the spliting criteria of all the node splits. In each iteration,
a new list is included with the spliting criteria of the new split of a node.</p>
</td></tr>
<tr><td><code id="split_node_+3A_min_leaf">min_leaf</code></td>
<td>
<p>Minimum number of samples in the leaf node. If a node has less than or, equal to min_leaf samples,
then there will be no splitting in that node and the node is a leaf node. Valid input is a positive integer and less than or equal to M (number of training samples)</p>
</td></tr>
<tr><td><code id="split_node_+3A_inv_cov_y">Inv_Cov_Y</code></td>
<td>
<p>Inverse of Covariance matrix of Output Response matrix for MRF(Give Zero for RF)</p>
</td></tr>
<tr><td><code id="split_node_+3A_command">Command</code></td>
<td>
<p>1 for univariate Regression Tree (corresponding to RF) and 2 for Multivariate Regression Tree (corresponding to MRF)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the splitting criteria of a node and stores the information in a list format.
If the node is a parent node, then indices of left and right nodes and feature number and threshold value
of the feature for the split are stored. If the node is a leaf, the output feature matrix of the samples
for the node are stored as a list.
</p>


<h3>Value</h3>

<p>Model: A list of lists with the splitting criteria of all the split of the nodes. In each iteration, the Model is
updated with a new list that includes the splitting criteria of the new split of a node.
</p>

<hr>
<h2 id='splitt2'>Split of the Parent node</h2><span id='topic+splitt2'></span>

<h3>Description</h3>

<p>Split of the training samples of the parent node into the child nodes based on the feature and threshold that produces the minimum cost
</p>


<h3>Usage</h3>

<pre><code class='language-R'>splitt2(X, Y, m_feature, Index, Inv_Cov_Y, Command, ff)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="splitt2_+3A_x">X</code></td>
<td>
<p>Input Training matrix of size M x N, M is the number of training samples and N is the number of features</p>
</td></tr>
<tr><td><code id="splitt2_+3A_y">Y</code></td>
<td>
<p>Output Training response of size M x T, M is the number of samples and T is the number of output responses</p>
</td></tr>
<tr><td><code id="splitt2_+3A_m_feature">m_feature</code></td>
<td>
<p>Number of randomly selected features considered for a split in each regression tree node.</p>
</td></tr>
<tr><td><code id="splitt2_+3A_index">Index</code></td>
<td>
<p>Index of training samples</p>
</td></tr>
<tr><td><code id="splitt2_+3A_inv_cov_y">Inv_Cov_Y</code></td>
<td>
<p>Inverse of Covariance matrix of Output Response matrix for MRF (Input [0 0; 0 0] for RF)</p>
</td></tr>
<tr><td><code id="splitt2_+3A_command">Command</code></td>
<td>
<p>1 for univariate Regression Tree (corresponding to RF) and 2 for Multivariate Regression Tree (corresponding to MRF)</p>
</td></tr>
<tr><td><code id="splitt2_+3A_ff">ff</code></td>
<td>
<p>Vector of m_feature from all features of X. This varies with each split</p>
</td></tr>
</table>


<h3>Details</h3>

<p>At each node of a regression a tree, a fixed number of features (m_feature) are selected randomly to be 
considered for generating the split. Node cost for all selected features along with possible n-1 thresholds for 
n samples are considered to select the feature and threshold with minimum cost.
</p>


<h3>Value</h3>

<p>List with the following components:
</p>
<table>
<tr><td><code>index_left</code></td>
<td>
<p>Index of the samples that are in the left node after splitting</p>
</td></tr>
<tr><td><code>index_right</code></td>
<td>
<p>Index of the samples that are in the right node after splitting</p>
</td></tr>
<tr><td><code>which_feature</code></td>
<td>
<p>The number of the feature that produces the minimum splitting cost</p>
</td></tr>
<tr><td><code>threshold_feature</code></td>
<td>
<p>The threshold value for the node split. 
A feature value less than or equal to the threshold will go to the left node and it will go to the right node otherwise.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library(MultivariateRandomForest)
X=matrix(runif(20*100),20,100)
Y=matrix(runif(20*3),20,3)
m_feature=5
Index=1:20
Inv_Cov_Y=solve(cov(Y))
ff2 = ncol(X) # number of features
ff =sort(sample(ff2, m_feature)) 
Command=2#MRF, as number of output feature is greater than 1
Split_criteria=splitt2(X,Y,m_feature,Index,Inv_Cov_Y,Command,ff) 
</code></pre>

<hr>
<h2 id='variable_importance_measure'>
Calculates variable Importance of a Regression Tree Model
</h2><span id='topic+variable_importance_measure'></span>

<h3>Description</h3>

<p>Number of times a variable has been picked in the branch nodes of a (single) regression tree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>variable_importance_measure(Model_VIM,NumVariable)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="variable_importance_measure_+3A_model_vim">Model_VIM</code></td>
<td>
<p> Regression Tree model in which the variable importance is measured</p>
</td></tr>
<tr><td><code id="variable_importance_measure_+3A_numvariable">NumVariable</code></td>
<td>
<p> Number of variables in the training or testing matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In time of calculating node cost of a tree of a random forest, a user defined number of variables are randomly picked. Among this, the best variable is chosen for the node using the node cost. While an important variable for a model will always come out as the best.
This function calculates the number of times a variable has been picked in the regression tree. It has been done by checking which variables are picked, how many times, in the branch nodes of the model.
</p>


<h3>Value</h3>

<p>Vector of size (1 x NumVariable), showing the number of repetition of variables (serially) in the branch nodes of the model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(MultivariateRandomForest)
trainX=matrix(runif(50*100),50,100)
trainY=matrix(runif(50*5),50,5) 
n_tree=2
m_feature=5
min_leaf=5
testX=matrix(runif(10*100),10,100) 

theta &lt;- function(trainX){trainX}
results &lt;- bootstrap::bootstrap(1:nrow(trainX),n_tree,theta) 
b=results$thetastar

Variable_number=ncol(trainY)
if (Variable_number&gt;1){
  Command=2
}else if(Variable_number==1){
  Command=1
} 
NumVariable=ncol(trainX)
NumRepeatation=matrix(rep(0,n_tree*NumVariable),nrow=n_tree)

for (i in 1:n_tree){
  Single_Model=NULL
  X=trainX[ b[ ,i],  ]
  Y=matrix(trainY[ b[ ,i],  ],ncol=Variable_number)
  Inv_Cov_Y = solve(cov(Y)) # calculate the V inverse
  if (Command==1){
    Inv_Cov_Y=matrix(rep(0,4),ncol=2)
  }
  Single_Model=build_single_tree(X, Y, m_feature, min_leaf,Inv_Cov_Y,Command)
  NumRepeatation[i,]=variable_importance_measure(Single_Model,NumVariable)
}
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
