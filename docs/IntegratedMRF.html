<!DOCTYPE html><html lang="en"><head><title>Help for package IntegratedMRF</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {IntegratedMRF}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#build_forest_predict'><p>Prediction using Random Forest or Multivariate Random Forest</p></a></li>
<li><a href='#build_single_tree'><p>Model of a single tree of Random Forest or Multivariate Random Forest</p></a></li>
<li><a href='#Combination'><p>Weights for combination of predictions from different data subtypes using Least Square Regression based on various error estimation techniques</p></a></li>
<li><a href='#CombPredict'><p>Integrated Prediction of Testing samples using Combination Weights from integrated RF or MRF model</p></a></li>
<li><a href='#CombPredictSpecific'><p>Prediction for testing samples using specific combination weights from integrated RF or MRF model</p></a></li>
<li><a href='#CrossValidation'><p>Generate training and testing samples for cross validation</p></a></li>
<li><a href='#Dream_Dataset'><p>NCI-Dream Drug Sensitivity Prediction Challenge Dataset</p></a></li>
<li><a href='#error_calculation'><p>Error calculation for integrated model</p></a></li>
<li><a href='#Imputation'><p>Imputation of a numerical vector</p></a></li>
<li><a href='#IntegratedPrediction'><p>Integrated Prediction of Testing samples from integrated RF or MRF model</p></a></li>
<li><a href='#Node_cost'><p>Information Gain</p></a></li>
<li><a href='#predicting'><p>Prediction of testing sample in a node</p></a></li>
<li><a href='#single_tree_prediction'><p>Prediction of Testing Samples for single tree</p></a></li>
<li><a href='#split_node'><p>Splitting Criteria of all the nodes of the tree</p></a></li>
<li><a href='#splitt'><p>Split of the Parent node</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Integrated Prediction using Uni-Variate and Multivariate Random
Forests</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.9</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-07-05</td>
</tr>
<tr>
<td>Author:</td>
<td>Raziur Rahman, Ranadip Pal </td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Raziur Rahman &lt;razeeebuet@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>An implementation of a framework for drug sensitivity prediction from various genetic characterizations using ensemble approaches. Random Forests or Multivariate Random Forest predictive models can be generated from each genetic characterization that are then combined using a Least Square Regression approach. It also provides options for the use of different error estimation approaches of Leave-one-out, Bootstrap, N-fold cross validation and 0.632+Bootstrap along with generation of prediction confidence interval using Jackknife-after-Bootstrap approach. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.4), bootstrap, ggplot2, utils, stats, limSolve,
MultivariateRandomForest</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-07-05 20:10:58 UTC; Raziur_Rahman</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-07-05 20:30:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='build_forest_predict'>Prediction using Random Forest or Multivariate Random Forest</h2><span id='topic+build_forest_predict'></span>

<h3>Description</h3>

<p>Builds Model of Random Forest or Multivariate Random Forest (when the number of output features &gt; 1) using training samples 
and generates the prediction of testing samples using the inferred model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_forest_predict(trainX, trainY, n_tree, m_feature, min_leaf, testX)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="build_forest_predict_+3A_trainx">trainX</code></td>
<td>
<p>Input Feature matrix of M x N, M is the number of training samples and N is the number of input features</p>
</td></tr>
<tr><td><code id="build_forest_predict_+3A_trainy">trainY</code></td>
<td>
<p>Output Response matrix of M x T, M is the number of training samples and T is the number of ouput features</p>
</td></tr>
<tr><td><code id="build_forest_predict_+3A_n_tree">n_tree</code></td>
<td>
<p>Number of trees in the forest, which must be positive integer</p>
</td></tr>
<tr><td><code id="build_forest_predict_+3A_m_feature">m_feature</code></td>
<td>
<p>Number of randomly selected features considered for a split in each regression tree node, which must be positive integer and less than N (number of input features)</p>
</td></tr>
<tr><td><code id="build_forest_predict_+3A_min_leaf">min_leaf</code></td>
<td>
<p>Minimum number of samples in the leaf node. If a node has less than or equal to min_leaf samples,
then there will be no splitting in that node and this node will be considered as a leaf node. Valid input is positive integer, which is less than or
equal to M (number of training samples)</p>
</td></tr>
<tr><td><code id="build_forest_predict_+3A_testx">testX</code></td>
<td>
<p>Testing samples of size Q x N, where Q is the number of testing samples and N is the number of features 
(Same number of features as training samples)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Random Forest regression refers to ensembles of regression trees where a set of n_tree un-pruned regression
trees are generated based on bootstrap sampling from the original training data. For each node, the optimal
feature for node splitting is selected from a random set of m_feature from the total N features. The selection
of the feature for node splitting from a random set of features decreases the correlation between different
trees and thus the average prediction of multiple regression trees is expected to have lower variance than
individual regression trees. Larger m_feature can improve the predictive capability of individual trees but can also
increase the correlation between trees and void any gains from averaging multiple predictions. The bootstrap
resampling of the data for training each tree also increases the variation between the trees.
</p>
<p>In a node with training predictor features (X) and output feature vectors (Y), node splitting is done
with the aim of selecting a feature from a random set of m_feature and threshold z to partition the node 
into two child nodes, left node (with samples &lt; z) and right node (with samples &gt;=z). In multivariate trees (MRF) 
node cost is measured as the sum of squares of the Mahalanobis 
distance where as in univariate trees (RF) node cost is measured as the Euclidean distance.
</p>
<p>After the Model of the forest is built using training Input features (trainX) and output feature matrix (trainY),
the Model is used to generate the prediction of output features (testY) for the testing samples (testX).
</p>


<h3>Value</h3>

<p>Prediction result of the Testing samples
</p>


<h3>References</h3>

<p>[Random Forest] Breiman, Leo. &quot;Random forests.&quot; Machine learning 45.1 (2001): 5-32.
</p>
<p>[Multivariate Random Forest] Segal, Mark, and Yuanyuan Xiao. &quot;Multivariate random forests.&quot; 
Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 1.1 (2011): 80-87.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(IntegratedMRF)
#Input and Output Feature Matrix of random data (created using runif)
trainX=matrix(runif(50*100),50,100) 
trainY=matrix(runif(50*5),50,5) 
n_tree=2
m_feature=5
min_leaf=5
testX=matrix(runif(10*100),10,100) 
#Prediction size is 10 x 5, where 10 is the number 
#of testing samples and 5 is the number of output features
Prediction=build_forest_predict(trainX, trainY, n_tree, m_feature, min_leaf, testX)
</code></pre>

<hr>
<h2 id='build_single_tree'>Model of a single tree of Random Forest or Multivariate Random Forest</h2><span id='topic+build_single_tree'></span>

<h3>Description</h3>

<p>Build a Univariate Regression Tree (for generation of Random Forest (RF) ) or Multivariate Regression Tree ( for generation of Multivariate Random Forest (MRF) ) using the training samples,
which is used for the prediction of testing samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_single_tree(X, Y, m_feature, min_leaf, Inv_Cov_Y, Command)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="build_single_tree_+3A_x">X</code></td>
<td>
<p>Input Feature matrix of M x N, M is the number of training samples and N is the number of input features</p>
</td></tr>
<tr><td><code id="build_single_tree_+3A_y">Y</code></td>
<td>
<p>Output Feature matrix of M x T, M is the number of training samples and T is the number of ouput features</p>
</td></tr>
<tr><td><code id="build_single_tree_+3A_m_feature">m_feature</code></td>
<td>
<p>Number of randomly selected features considered for a split in each regression tree node, which must be positive integer and less than N (number of input features)</p>
</td></tr>
<tr><td><code id="build_single_tree_+3A_min_leaf">min_leaf</code></td>
<td>
<p>Minimum number of samples in the leaf node, which must be positive integer and less than or equal to M (number of training samples)</p>
</td></tr>
<tr><td><code id="build_single_tree_+3A_inv_cov_y">Inv_Cov_Y</code></td>
<td>
<p>Inverse of Covariance matrix of Output Response matrix for MRF(Input [0 0;0 0] for RF)</p>
</td></tr>
<tr><td><code id="build_single_tree_+3A_command">Command</code></td>
<td>
<p>1 for univariate Regression Tree (corresponding to RF) and 2 for Multivariate Regression Tree (corresponding to MRF)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The regression tree structure is represented as a list of lists. For a non-leaf node, it contains the splitting criteria 
(feature for split and threshold) and for a leaf node, it contains the output responses for the samples contained in the leaf node.
</p>


<h3>Value</h3>

<p>Model of a single regression tree (Univariate or Multivariate Regression Tree). An example of the list of the non-leaf node:
</p>
<table role = "presentation">
<tr><td><code>Flag for determining node status; leaf node (1) or branch node (0)</code></td>
<td>
<p>1</p>
</td></tr>
<tr><td><code>Index of samples for the left node</code></td>
<td>
<p>int [1:34] 1 2 4 5 ...</p>
</td></tr>
<tr><td><code>Index of samples for the right node</code></td>
<td>
<p>int [1:16] 3 6 9 ...</p>
</td></tr>
<tr><td><code>Feature for split</code></td>
<td>
<p>int 34</p>
</td></tr>
<tr><td><code>Threshold values for split</code>, <code>average them</code></td>
<td>
<p>num [1:3] 0.655 0.526 0.785</p>
</td></tr>
<tr><td><code>List number for the left and right nodes</code></td>
<td>
<p>num [1:2] 2 3</p>
</td></tr>
</table>
<p>An example of the list of the leaf node:
</p>
<table role = "presentation">
<tr><td><code>Output responses</code></td>
<td>
<p>num[1:4,1:5] 0.0724 0.1809 0.0699 ...</p>
</td></tr>
</table>

<hr>
<h2 id='Combination'>Weights for combination of predictions from different data subtypes using Least Square Regression based on various error estimation techniques</h2><span id='topic+Combination'></span>

<h3>Description</h3>

<p>Calculates combination weights for different subtypes of dataset combinations to generate integrated Random Forest (RF) or Multivariate Random Forest (MRF) model based on different error estimation models such as Bootstrap, 0.632+ Bootstrap, N-fold cross validation or Leave one out.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Combination(finalX, finalY_train, Cell, finalY_train_cell, n_tree, m_feature,
  min_leaf, Confidence_Level)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Combination_+3A_finalx">finalX</code></td>
<td>
<p>List of Matrices where each matrix represent a specific data subtype (such as genomic characterizations for
drug sensitivity prediction). Each subtype can have different types of features. For example, if there are three subtypes containing
100, 200 and 250 features respectively,  finalX will be a list containing 3 matrices of sizes M x 100, M x 200 and M x 250
where M is the number of Samples.</p>
</td></tr>
<tr><td><code id="Combination_+3A_finaly_train">finalY_train</code></td>
<td>
<p>A M x T matrix of output features for training samples, where M is number of samples and T is the number of output features.
The dataset is assumed to contain no missing values. If there are missing values, an imputation method should be applied before using the function.
A function 'Imputation' is included within the package.</p>
</td></tr>
<tr><td><code id="Combination_+3A_cell">Cell</code></td>
<td>
<p>It contains a list of samples (the samples can be represented either numerically by indices or by names) for each data subtype.
For the example of 3 data subtypes, it will be a list containing 3 arrays where each array contains the sample information for each data subtype.</p>
</td></tr>
<tr><td><code id="Combination_+3A_finaly_train_cell">finalY_train_cell</code></td>
<td>
<p>Sample names of output features for training samples</p>
</td></tr>
<tr><td><code id="Combination_+3A_n_tree">n_tree</code></td>
<td>
<p>Number of trees in the forest, which must be positive integer</p>
</td></tr>
<tr><td><code id="Combination_+3A_m_feature">m_feature</code></td>
<td>
<p>Number of randomly selected features considered for a split in each regression tree node, Valid Input is a positive integer, which is less than N (which is equal to number of input features for the smallest genomic characterization)</p>
</td></tr>
<tr><td><code id="Combination_+3A_min_leaf">min_leaf</code></td>
<td>
<p>Minimum number of samples in the leaf node, which must be positive integer and less than or equal to M (number of training samples)</p>
</td></tr>
<tr><td><code id="Combination_+3A_confidence_level">Confidence_Level</code></td>
<td>
<p>Confidence level for calculation of confidence interval (User Defined), which must be between 0 and 100</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function takes all the subtypes of dataset in matrix format and its corresponding sample information.
For calculation purpose, we have considered the data of the samples that are common in all the subtypes and output training responses.
For example, consider a dataset of 3 sub-types with different number of samples and features, with indices of samples in subtype 1, 2, 3  and output feature matrix
is 1:10, 3:15, 5:16 and 5:11 respectively. Thus, features of sample index 5:10 (common to all subtypes and output feature matrix) of all subtypes and output feature
matrix will be selected and considered for all calculations.
</p>
<p>For M x N dataset, N number of bootstrap sampling sets are considered. For each bootstrap sampling set and each subtype, a Random Forest (RF)
or, Multivariate Random Forest (MRF) model is generated, which is used for calculating the prediction performance for out-of-bag samples.
The prediction performance for each subtype of the dataset is based on the averaging over different bootstrap training sets.
The combination weights (regression coefficients) for each combination of subtypes are generated using least Square Regression from the
individual subtype predictions and used later to calculate mean absolute error, mean square error and correlation coefficient between
predicted and actual values.
</p>
<p>For N-fold cross validation error estimation with M cell lines, N models are generated for each subtype of dataset, where for each partition (M/N)*(N-1) cell
lines are used for training and the remaining cell lines are used to
estimate errors and combination weights for different data subtype combinations.
</p>
<p>In 0.632 Bootstrap error estimation, bootstrap and re-substitution error estimates are combined based on
0.632xBootstrap Error + 0.368xRe-substitution Error. While 0.632+ Bootstrap error estimation considers the overfitting of re-substitution error
with no information error rate <code class="reqn">\gamma</code>. An estimate of <code class="reqn">\gamma</code> is obtained by permuting the responses <code class="reqn">y[i]</code> and predictors <code class="reqn">x[j]</code>.
</p>
<p style="text-align: center;"><code class="reqn">\gamma=sum(sum(error(x[j],y[i]),j=1,m),i=1,m)/m^2</code>
</p>

<p>The relative overfitting rate is defined as <code class="reqn">R=(Bootstrap Error-Resubstitution Error)/(\gamma-Resubstitution Error)</code> and weight distribution
between bootstrap error and Re-substitution Error is defined as <code class="reqn">w=0.632/(1-0.368*R)</code>. So, 0.632+ Bootstrap error is equal to
<code class="reqn">(1-w)*Bootstrap Error+w*Resubstitution Error</code>.
These prediction results are then used to compute the errors and combination weights for different data subtype combinations.
</p>
<p>Confidence Interval has been calculated using Jackkniffe-After-Bootstrap Approach and prediction result of bootstrap error estimation.
</p>
<p>For leave-one-out error estimation using M cell lines, M models are generated for each subtype of dataset, which are then used to
calculate the errors and combination weights for different data subtype combinations.
</p>


<h3>Value</h3>

<p>List with the following components:
</p>
<table role = "presentation">
<tr><td><code>BSP_coeff</code></td>
<td>
<p>Combination weights using Bootstrap Error Estimation Model, where index is in list format.
If the number of genomic characterizations or subtypes of dataset is 5, there will be 2^5-1=31 list of weights</p>
</td></tr>
<tr><td><code>Nfold_coeff</code></td>
<td>
<p>Combination weights using N fold cross validation Error Estimation Model, where index is in list format.
If the number of genomic characterizations or subtypes of dataset is 5, there will be 2^5-1=31 list of weights</p>
</td></tr>
<tr><td><code>BSP632plus_coeff</code></td>
<td>
<p>Combination weights using 0.632+ Bootstrap Error Estimation Model, where index is in list format.
If the number of genomic characterizations or subtypes of dataset is 5, there will be 2^5-1=31 list of weights</p>
</td></tr>
<tr><td><code>LOO_coeff</code></td>
<td>
<p>Combination weights using Leave-One-Out Error Estimation Model, where index is in list format.
If the number of genomic characterizations or subtypes of dataset is 5, there will be 2^5-1=31 list of weights</p>
</td></tr>
<tr><td><code>Error</code></td>
<td>
<p>Matrix of Mean Absolute Error, Mean Square Error and correlation between actual and predicted responses for integrated model based
on Bootstrap, N fold cross validation, 0.632+ Bootstrap and Leave-one-out error estimation sampling techniques for the integrated model
containing all the data subtypes</p>
</td></tr>
<tr><td><code>Confidence Interval</code></td>
<td>
<p>Low and High confidence interval for a user defined confidence level for the drug using Jackknife-After-Bootstrap Approach in a list</p>
</td></tr>
<tr><td><code>BSP_error_all_mae</code></td>
<td>
<p>Bootstrap Mean Absolute Errors (MAE) for all combinations of the dataset subtypes. Size C x R, where C is the number of
combinations and R is the number of output responses. C is in decreasing order, which means first value is combination of all subtypes
and next ones are in decreasing order. For example, if a dataset has 3 subtypes, then C is equal to 2^3-1=7.  The ordering of C is the combination of
subtypes [1 2 3], [1 2], [1 3], [2 3], [1], [2], [3] </p>
</td></tr>
<tr><td><code>Nfold_error_all_mae</code></td>
<td>
<p>N fold cross validation Mean Absolute Errors (MAE) for all combinations of the dataset subtypes. Size C x R, where C is the number of
combinations and R is the number of output responses. C is in decreasing order, which means first value is combination of all subtypes
and next ones are in decreasing order. For example, if a dataset has 3 subtypes, then C is equal to 2^3-1=7.  The ordering of C is the combination of
subtypes [1 2 3], [1 2], [1 3], [2 3], [1], [2], [3] </p>
</td></tr>
<tr><td><code>BSP632plus_error_all_mae</code></td>
<td>
<p>0.632+ Bootstrap Mean Absolute Errors (MAE) for all combinations of the dataset subtypes. Size C x R, where C is the number of
combinations and R is the number of output responses. C is in decreasing order, which means first value is combination of all subtypes
and next ones are in decreasing order. For example, if a dataset has 3 subtypes, then C is equal to 2^3-1=7.  The ordering of C is the combination of
subtypes [1 2 3], [1 2], [1 3], [2 3], [1], [2], [3] </p>
</td></tr>
<tr><td><code>LOO_error_all_mae</code></td>
<td>
<p>Leave One Out Mean Absolute Errors (MAE) for all combinations of the dataset subtypes. Size C x R, where C is the number of
combinations and R is the number of output responses. C is in decreasing order, which means first value is combination of all subtypes
and next ones are in decreasing order. For example, if a dataset has 3 subtypes, then C is equal to 2^3-1=7.  The ordering of C is the combination of
subtypes [1 2 3], [1 2], [1 3], [2 3], [1], [2], [3] </p>
</td></tr>
</table>
<p>The function also returns figures of different error estimations in .tiff format
</p>


<h3>References</h3>

<p>Wan, Qian, and Ranadip Pal. &quot;An ensemble based top performing approach for NCI-DREAM drug sensitivity prediction challenge.&quot; PloS one 9.6 (2014): e101183.
</p>
<p>Rahman, Raziur, John Otridge, and Ranadip Pal. &quot;IntegratedMRF: random forest-based framework for integrating prediction from different data types.&quot; Bioinformatics (Oxford, England) (2017).
</p>
<p>Efron, Bradley, and Robert Tibshirani. &quot;Improvements on cross-validation: the 632+ bootstrap method.&quot; Journal of the American Statistical Association 92.438 (1997): 548-560.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(IntegratedMRF)
data(Dream_Dataset)
Tree=1
Feature=1
Leaf=5
Confidence=80
finalX=Dream_Dataset[[1]]
Cell=Dream_Dataset[[2]]
Y_train_Dream=Dream_Dataset[[3]]
Y_train_cell=Dream_Dataset[[4]]
Y_test=Dream_Dataset[[5]]
Y_test_cell=Dream_Dataset[[6]]
Drug=c(1,2,3)
Y_train_Drug=matrix(Y_train_Dream[,Drug],ncol=length(Drug))
Result=Combination(finalX,Y_train_Drug,Cell,Y_train_cell,Tree,Feature,Leaf,Confidence)

</code></pre>

<hr>
<h2 id='CombPredict'>Integrated Prediction of Testing samples using Combination Weights from integrated RF or MRF model</h2><span id='topic+CombPredict'></span>

<h3>Description</h3>

<p>Generates Random Forest or Multivariate Random Forest model for each subtype of dataset and predicts testing samples using the generated models.
Subsequently, the prediction for different subtypes of dataset are combined using the Combination weights generated from 'Combination' function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CombPredict(finalX, finalY_train, Cell, finalY_train_cell, finalY_test_cell,
  n_tree, m_feature, min_leaf, Coeff)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CombPredict_+3A_finalx">finalX</code></td>
<td>
<p>List of Matrices where each matrix represents a specific data subtype (such as genomic characterizations for
drug sensitivity prediction). Each subtype can have different types of features. For example, if there are three subtypes containing
100, 200 and 250 features respectively,  finalX will be a list containing 3 matrices of sizes M x 100, M x 200 and M x 250
where M is the number of Samples.</p>
</td></tr>
<tr><td><code id="CombPredict_+3A_finaly_train">finalY_train</code></td>
<td>
<p>A M x T matrix of output features for training samples, where M is number of samples and
T is the number of output features. The dataset is assumed to contain no missing values. If there are missing values, an imputation method
should be applied before using the function. A function 'Imputation' is included within the package.</p>
</td></tr>
<tr><td><code id="CombPredict_+3A_cell">Cell</code></td>
<td>
<p>It contains a list of samples (the samples can be represented either numerically by indices or by names) for each data subtype.
For the example of 3 data subtypes, it will be a list containing 3 arrays where each array contains the sample information for each data subtype.</p>
</td></tr>
<tr><td><code id="CombPredict_+3A_finaly_train_cell">finalY_train_cell</code></td>
<td>
<p>Cell lines of output features for training samples</p>
</td></tr>
<tr><td><code id="CombPredict_+3A_finaly_test_cell">finalY_test_cell</code></td>
<td>
<p>Cell lines of output features for testing samples</p>
</td></tr>
<tr><td><code id="CombPredict_+3A_n_tree">n_tree</code></td>
<td>
<p>number of trees in the forest, which must be positive integer</p>
</td></tr>
<tr><td><code id="CombPredict_+3A_m_feature">m_feature</code></td>
<td>
<p>Number of randomly selected features considered for a split in each regression tree node, which must be positive integer</p>
</td></tr>
<tr><td><code id="CombPredict_+3A_min_leaf">min_leaf</code></td>
<td>
<p>minimum number of samples in the leaf node, which must be positive integer and less than or equal to M (number of training samples)</p>
</td></tr>
<tr><td><code id="CombPredict_+3A_coeff">Coeff</code></td>
<td>
<p>Combination Weights. The user can supply the weights based on either Bootstrap, Re-substitution, 0.632Bootstrap or Leave-one-out
error estimation approaches.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Input matrix and output response of training samples have been used to build Random Forest or Multivariate Random Forest model for each subtype of
a dataset. These models are used to calculate prediction of
testing samples for each subtype separately. Subsequently Combination Weights (different errors have different combination weights
and the user should select the one to be used) are used to integrate the predictions from data subtypes.
Note that the combination weights are linear regression coefficients generated using the training samples.
</p>
<p>The specific set of combination weights to be used for testing samples will depend on the number of data subtypes available
for the testing samples. Note that not all subtype information maybe available for all samples.
As an example with three data subtypes, a testing sample with all subtype data available will use
the combination weights corresponding to Serial [1 2 3] where if subtype 3 is not available, the function will
using the combination weights corresponding to Serial [1 2].
</p>


<h3>Value</h3>

<p>Final Prediction of testing samples based on provided testing sample names.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(IntegratedMRF)
data(Dream_Dataset)
Tree=1
Feature=1
Leaf=10
Confidence=80
finalX=Dream_Dataset[[1]]
Cell=Dream_Dataset[[2]]
Y_train_Dream=Dream_Dataset[[3]]
Y_train_cell=Dream_Dataset[[4]]
Y_test=Dream_Dataset[[5]]
Y_test_cell=Dream_Dataset[[6]]
Drug=1
Y_train_Drug=matrix(Y_train_Dream[,Drug],ncol=length(Drug))
Result=Combination(finalX,Y_train_Drug,Cell,Y_train_cell,Tree,Feature,Leaf,Confidence)

CombPredict(finalX,Y_train_Drug,Cell,Y_train_cell,Y_test_cell,Tree,Feature,Leaf,Result[[1]])
</code></pre>

<hr>
<h2 id='CombPredictSpecific'>Prediction for testing samples using specific combination weights from integrated RF or MRF model</h2><span id='topic+CombPredictSpecific'></span>

<h3>Description</h3>

<p>Generates Random Forest (One Output Feature) or Multivariate Random Forest (More than One Output Feature)
model for each subtype of dataset and predicts testing samples using these models. The predictions are
combined using the specific combination weights provided by the user. For the input combination weights,
the testing cell lines should have the subtype data corresponding to the non-zero weight subtypes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CombPredictSpecific(finalX, finalY_train, Cell, finalY_train_cell,
  finalY_test_cell, n_tree, m_feature, min_leaf, Coeff)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CombPredictSpecific_+3A_finalx">finalX</code></td>
<td>
<p>List of Matrices where each matrix represent a specific data subtype (such as genomic characterizations for
drug sensitivity prediction). Each subtype can have different types of features. For example, if there are three subtypes containing
100, 200 and 250 features respectively,  finalX will be a list containing 3 matrices of sizes M x 100, M x 200 and M x 250
where M is the number of Samples.</p>
</td></tr>
<tr><td><code id="CombPredictSpecific_+3A_finaly_train">finalY_train</code></td>
<td>
<p>A M x T matrix of output features for training samples, where M is the number of samples and T is the number of output features.
The dataset is assumed to contain no missing values. If there are missing values, an imputation method should be applied before using the function.
A function 'Imputation' is included within the package.</p>
</td></tr>
<tr><td><code id="CombPredictSpecific_+3A_cell">Cell</code></td>
<td>
<p>It contains a list of samples (the samples can be represented either numerically by indices or by names) for each data subtype.
For the example of 3 data subtypes, it will be a list containing 3 arrays where each array contains the sample information for each data subtype.</p>
</td></tr>
<tr><td><code id="CombPredictSpecific_+3A_finaly_train_cell">finalY_train_cell</code></td>
<td>
<p>Sample names of output features for training samples</p>
</td></tr>
<tr><td><code id="CombPredictSpecific_+3A_finaly_test_cell">finalY_test_cell</code></td>
<td>
<p>Sample names of output features for testing samples (All these testing samples
must have features for each subtypes of dataset)</p>
</td></tr>
<tr><td><code id="CombPredictSpecific_+3A_n_tree">n_tree</code></td>
<td>
<p>Number of trees in the forest, which must be positive integer</p>
</td></tr>
<tr><td><code id="CombPredictSpecific_+3A_m_feature">m_feature</code></td>
<td>
<p>Number of randomly selected features considered for a split in each regression tree node, which must be a positive integer</p>
</td></tr>
<tr><td><code id="CombPredictSpecific_+3A_min_leaf">min_leaf</code></td>
<td>
<p>Minimum number of samples in the leaf node, which must be a positive integer less than or equal to M (number of training samples)</p>
</td></tr>
<tr><td><code id="CombPredictSpecific_+3A_coeff">Coeff</code></td>
<td>
<p>Combination Weights (user defined or some combination weights generated using the 'Combination' function).
The size must be C, which is equal to the number of subtypes of dataset given in finalX.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Input feature matrix and output feature matrix have been used to generate Random Forest (One Output Feature)
or Multivariate Random Forest (More than One Output Feature) model for each subtype of dataset separately.
The prediction of testing samples using each subtype trained model is generated. The predictions are combined
using the specific combination weights provided by the user. For the input combination weights, the testing cell lines
should have the subtype data corresponding to the non-zero weight subtypes. For instance, if combination weights is
[0.6 0.3 0 0.1], then the subtype 1, 2 and 4 needs to be present for the testing samples. Furthermore, all the features
should be present for the required subtypes for the testing samples.
</p>


<h3>Value</h3>

<p>Final Prediction of testing samples based on provided testing sample names
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(IntegratedMRF)
data(Dream_Dataset)
Tree=1
Feature=1
Leaf=10
Confidence=80
finalX=Dream_Dataset[[1]]
Cell=Dream_Dataset[[2]]
Y_train_Dream=Dream_Dataset[[3]]
Y_train_cell=Dream_Dataset[[4]]
Y_test=Dream_Dataset[[5]]
Y_test_cell=Dream_Dataset[[6]]
Drug=1
Y_train_Drug=matrix(Y_train_Dream[,Drug],ncol=length(Drug))
Result=Combination(finalX,Y_train_Drug,Cell,Y_train_cell,Tree,Feature,Leaf,Confidence)

CombPredictSpecific(finalX,Y_train_Drug,Cell,Y_train_cell,Y_test_cell,Tree,
        Feature,Leaf,runif(length(Cell)*1))
</code></pre>

<hr>
<h2 id='CrossValidation'>Generate training and testing samples for cross validation</h2><span id='topic+CrossValidation'></span>

<h3>Description</h3>

<p>Generates Cross Validation Input Matrices and Output Vectors for training and testing, where number of folds in cross validation is user defined.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CrossValidation(X, Y, F)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CrossValidation_+3A_x">X</code></td>
<td>
<p>M x N Input matrix, M is the number of samples and N is the number of features</p>
</td></tr>
<tr><td><code id="CrossValidation_+3A_y">Y</code></td>
<td>
<p>output response as column vector</p>
</td></tr>
<tr><td><code id="CrossValidation_+3A_f">F</code></td>
<td>
<p>Number of Folds</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with the following components: 
</p>
<table role = "presentation">
<tr><td><code>TrainingData</code></td>
<td>
<p>List of matrices where each matrix contains a fold of Cross Validation Training Data, 
where the number of matrices is equal to F</p>
</td></tr>
<tr><td><code>TestingData</code></td>
<td>
<p>List of matrices where each matrix contains a fold of Cross Validation Testing Data, 
where the number of matrices is equal to F</p>
</td></tr>
<tr><td><code>OutputTrain</code></td>
<td>
<p>List of matrices where each matrix contains a fold of Cross Validation Training Output Feature Data, 
where the number of matrices is equal to F</p>
</td></tr>
<tr><td><code>OutputTest</code></td>
<td>
<p>List of matrices where each matrix contains a fold of Cross Validation Testing Output Feature Data, 
where the number of matrices is equal to F</p>
</td></tr>
<tr><td><code>FoldedIndex</code></td>
<td>
<p>Index of Different Folds. (e.g., for Sample Index 1:6 and 3 fold, FoldedIndex are [1 2 3 4], [1 2 5 6], [3 4 5 6])</p>
</td></tr>
</table>

<hr>
<h2 id='Dream_Dataset'>NCI-Dream Drug Sensitivity Prediction Challenge Dataset</h2><span id='topic+Dream_Dataset'></span>

<h3>Description</h3>

<p>A demo dataset of different genomic characterizations and drug sensitivity selected from NCI-Dream
Drug Sensitivity Prediction Challenge dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Dream_Dataset
</code></pre>


<h3>Format</h3>

<p>A list of 6 variables containing genomic characterizations and drug sensitivity:
</p>

<dl>
<dt>finalX_Dream</dt><dd><p>List of 5 Matrices where the matrices represent different 
genomic characterizations of Gene Expression, Methylation, RNA sequencing, Reverse Phase Protein Array (RPPA) and 
Copy Number Variation (CNV). 1000 predictor features for each subtype is included to satisfy package size limitations.</p>
</dd>
<dt>Cell_line_Index_Dream</dt><dd><p>List of Cell Line names for each genomic charcterization</p>
</dd>
<dt>finalY_train_Dream</dt><dd><p>Drug Sensitivity of training samples (35) for 31 drugs provided for 
NCI-Dream Drug Sensitivity Prediction Challenge</p>
</dd>
<dt>finalY_train_cell_Dream</dt><dd><p>Cell line names of the training samples</p>
</dd>
<dt>finalY_test_Dream</dt><dd><p>Drug Sensitivity of testing samples (18) for 31 drugs provided for 
NCI-Dream Drug Sensitivity Prediction Challenge Dataset</p>
</dd>
<dt>finalY_test_cell_Dream</dt><dd><p>Cell line names of the testing samples</p>
</dd>
</dl>


<h3>Source</h3>

<p><a href="https://www.synapse.org/#!Synapse:syn2785778/wiki/70252">https://www.synapse.org/#!Synapse:syn2785778/wiki/70252</a>
</p>


<h3>References</h3>

<p>Costello, James C., et al. &quot;A community effort to assess and improve drug sensitivity prediction algorithms.&quot; Nature biotechnology 32.12 (2014): 1202-1212.
</p>

<hr>
<h2 id='error_calculation'>Error calculation for integrated model</h2><span id='topic+error_calculation'></span>

<h3>Description</h3>

<p>Combines Prediction from different data subtypes through Least Square Regression and computes Mean Absolute Error, 
Mean Square Error and Pearson Correlation Coefficient between Integrated Prediction and Original Output feature.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>error_calculation(final_pred, final_actual)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="error_calculation_+3A_final_pred">final_pred</code></td>
<td>
<p>A n x p matrix of predicted features, where n is the number of samples and p is the number of data subtypes with prediction</p>
</td></tr>
<tr><td><code id="error_calculation_+3A_final_actual">final_actual</code></td>
<td>
<p>A n x 1 vector of original output responses</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If final_pred is a vector, it refers to the prediction result for one subtype of dataset and this function will return 
Mean Absolute Error, Mean Square Error and Pearson Correlation Coefficient between predicted and Original Output response. 
If final_pred is a matrix containing prediction results for more than one subtype of dataset, Least Square 
Regression will be used to calculate the weights for combining the predictions and generate an integrated prediction of size n x 1. 
Subsequently, Mean Absolute Error, Mean Square Error and Pearson Correlation Coefficient between 
Integrated Prediction and Original Output responses are calculated.
</p>


<h3>Value</h3>

<p>List with the following components: 
</p>
<table role = "presentation">
<tr><td><code>Integrated Prediction</code></td>
<td>
<p>Integrated Prediction based on combining predictions from data subtypes using Least Square Regression</p>
</td></tr>
<tr><td><code>error_mae</code></td>
<td>
<p>Mean Absolute Error between Integrated Prediction and Original Output Responses</p>
</td></tr>
<tr><td><code>error_mse</code></td>
<td>
<p>Mean Square Error between Integrated Prediction and Original Output Responses</p>
</td></tr>
<tr><td><code>error_corr</code></td>
<td>
<p>Pearson Correlation Coefficient between Integrated Prediction and Original Output Responses</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code>lsei</code>
</p>

<hr>
<h2 id='Imputation'>Imputation of a numerical vector</h2><span id='topic+Imputation'></span>

<h3>Description</h3>

<p>Imputes the values of the vector that are NaN
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Imputation(XX)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Imputation_+3A_xx">XX</code></td>
<td>
<p>a vector of size N x 1</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If a value is missing, it will be replaced by an imputed value that is an average of previous and 
next value. If previous or next value is also missing, the closest value is used as the imputed value.
</p>


<h3>Value</h3>

<p>Imputed vector of size N x 1
</p>

<hr>
<h2 id='IntegratedPrediction'>Integrated Prediction of Testing samples from integrated RF or MRF model</h2><span id='topic+IntegratedPrediction'></span>

<h3>Description</h3>

<p>Generates Random Forest or Multivariate Random Forest model for each subtype of dataset and predicts testing samples using the generated models.
Subsequently, the prediction for different subtypes of dataset are combined using the Combination weights generated from
Integrated Model which is based on Bootstrap error estimate
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IntegratedPrediction(finalX, finalY_train, Cell, finalY_train_cell,
  finalY_test_cell, n_tree, m_feature, min_leaf)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="IntegratedPrediction_+3A_finalx">finalX</code></td>
<td>
<p>List of Matrices where each matrix represent a specific data subtype (such as genomic characterizations for
drug sensitivity prediction). Each subtype can have different types of features. For example, if there are three subtypes containing
100, 200 and 250 features respectively,  finalX will be a list containing 3 matrices of sizes M x 100, M x 200 and M x 250
where M is the number of Samples.</p>
</td></tr>
<tr><td><code id="IntegratedPrediction_+3A_finaly_train">finalY_train</code></td>
<td>
<p>A M x T matrix of output features for training samples, where M is number of samples and
T is the number of output features. The dataset is assumed to contain no missing values. If there are missing values, an imputation method
should be applied before using the function. A function 'Imputation' is included within the package.</p>
</td></tr>
<tr><td><code id="IntegratedPrediction_+3A_cell">Cell</code></td>
<td>
<p>It contains a list of samples (the samples can be represented either numerically by indices or by names) for each data subtype.
For the example of 3 data subtypes, it will be a list containing 3 arrays where each array contains the sample information for each data subtype.</p>
</td></tr>
<tr><td><code id="IntegratedPrediction_+3A_finaly_train_cell">finalY_train_cell</code></td>
<td>
<p>Cell lines of output features for training samples</p>
</td></tr>
<tr><td><code id="IntegratedPrediction_+3A_finaly_test_cell">finalY_test_cell</code></td>
<td>
<p>Cell lines of output features for testing samples</p>
</td></tr>
<tr><td><code id="IntegratedPrediction_+3A_n_tree">n_tree</code></td>
<td>
<p>number of trees in the forest, which must be positive integer</p>
</td></tr>
<tr><td><code id="IntegratedPrediction_+3A_m_feature">m_feature</code></td>
<td>
<p>Number of randomly selected features considered for a split in each regression tree node, which must be positive integer</p>
</td></tr>
<tr><td><code id="IntegratedPrediction_+3A_min_leaf">min_leaf</code></td>
<td>
<p>minimum number of samples in the leaf node, which must be positive integer and less than or equal to M (number of training samples)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Input matrix and output response of training samples have been used to build Random Forest or Multivariate Random Forest model for each subtype of
a dataset. These models are used to calculate prediction of
testing samples for each subtype separately. Subsequently Combination Weights are used to integrate the predictions from data subtypes.
</p>
<p>Combination Weight Generation: For M x N dataset, N number of bootstrap sampling sets are considered. For each bootstrap sampling set and each subtype, a Random Forest (RF)
or, Multivariate Random Forest (MRF) model is generated, which is used for calculating the prediction performance for out-of-bag samples.
The prediction performance for each dataset subtypes is based on the averaging over different bootstrap training sets.
The combination weights (regression coefficients) for each combination of subtypes are generated using least Square Regression from the
individual subtype predictions and used to integrate the predictions from data subtypes.
</p>
<p>The specific set of combination weights to be used for testing samples will depend on the number of data subtypes available
for the testing samples. Note that not all subtype information maybe available for all samples.
As an example with three data subtypes, a testing sample with all subtype data available will use
the combination weights corresponding to Serial [1 2 3] where as if subtype 3 is not available, the function will
use the combination weights corresponding to Serial [1 2].
</p>


<h3>Value</h3>

<p>Final Prediction of testing samples based on provided testing sample names.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(IntegratedMRF)
data(Dream_Dataset)
Tree=1
Feature=1
Leaf=10
finalX=Dream_Dataset[[1]]
Cell=Dream_Dataset[[2]]
Y_train_Dream=Dream_Dataset[[3]]
Y_train_cell=Dream_Dataset[[4]]
Y_test=Dream_Dataset[[5]]
Y_test_cell=Dream_Dataset[[6]]
Drug=c(1,2,3)
Y_train_Drug=matrix(Y_train_Dream[,Drug],ncol=length(Drug))
IntegratedPrediction(finalX,Y_train_Drug,Cell,Y_train_cell,Y_test_cell,Tree,Feature,Leaf)

</code></pre>

<hr>
<h2 id='Node_cost'>Information Gain</h2><span id='topic+Node_cost'></span>

<h3>Description</h3>

<p>Compute the cost function of a tree node
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Node_cost(y, Inv_Cov_Y, Command)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Node_cost_+3A_y">y</code></td>
<td>
<p>Output Features for the samples of the node</p>
</td></tr>
<tr><td><code id="Node_cost_+3A_inv_cov_y">Inv_Cov_Y</code></td>
<td>
<p>Inverse of Covariance matrix of Output Response matrix for MRF(Input [0 0;0 0] for RF)</p>
</td></tr>
<tr><td><code id="Node_cost_+3A_command">Command</code></td>
<td>
<p>1 for univariate Regression Tree (corresponding to RF) and 2 for Multivariate Regression Tree (corresponding to MRF)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In multivariate trees (MRF) node cost is measured as the sum of squares of the Mahalanobis distance to capture the correlations in 
the data whereas in univariate trees node cost is measured as the sum of Euclidean distance square. Mahalanobis Distance captures 
the distance of the sample point from the mean of the node along the principal component axes.
</p>


<h3>Value</h3>

<p>cost or entropy of samples in a node of a tree
</p>


<h3>References</h3>

<p>Segal, Mark, and Yuanyuan Xiao. &quot;Multivariate random forests.&quot; 
Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 1.1 (2011): 80-87.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(IntegratedMRF)
y=matrix(runif(10*2),10,2)
Inv_Cov_Y=solve(cov(y))
Command=2
#Command=2 for MRF and 1 for RF
#This function calculates information gain of a node
Cost=Node_cost(y,Inv_Cov_Y,Command)
</code></pre>

<hr>
<h2 id='predicting'>Prediction of testing sample in a node</h2><span id='topic+predicting'></span>

<h3>Description</h3>

<p>Provides the value of a testing sample in a node that refers to which child node it will go to using the splitting criteria 
of the tree node or the prediction results if the node is a leaf.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predicting(Single_Model, i, X_test, Variable_number)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predicting_+3A_single_model">Single_Model</code></td>
<td>
<p>Model of a particular tree</p>
</td></tr>
<tr><td><code id="predicting_+3A_i">i</code></td>
<td>
<p>Number of splits. Used as an index, which indicates where in the list the splitting
criteria of this split has been stored.</p>
</td></tr>
<tr><td><code id="predicting_+3A_x_test">X_test</code></td>
<td>
<p>Testing samples of size 1 x N, 1 is the number of testing samples and N is the number of features (same order and
size used as training)</p>
</td></tr>
<tr><td><code id="predicting_+3A_variable_number">Variable_number</code></td>
<td>
<p>Number of Output Features</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function considers the output at a particular node. If the node is a leaf, the average of output responses 
is returned as prediction result. For a non-leaf node, the direction of left or right node is decided based on 
the node threshold and splitting feature value.
</p>


<h3>Value</h3>

<p>Prediction result of a testing samples in a node
</p>

<hr>
<h2 id='single_tree_prediction'>Prediction of Testing Samples for single tree</h2><span id='topic+single_tree_prediction'></span>

<h3>Description</h3>

<p>Predicts the output responses of testing samples based on the input regression tree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>single_tree_prediction(Single_Model, X_test, Variable_number)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="single_tree_prediction_+3A_single_model">Single_Model</code></td>
<td>
<p>Random Forest or Multivariate Random Forest Model of a particular tree</p>
</td></tr>
<tr><td><code id="single_tree_prediction_+3A_x_test">X_test</code></td>
<td>
<p>Testing samples of size Q x N, Q is the number of testing samples and N is the number of features (same order and
size used as training)</p>
</td></tr>
<tr><td><code id="single_tree_prediction_+3A_variable_number">Variable_number</code></td>
<td>
<p>Number of Output Features</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A regression tree model contains splitting criteria for all the splits in the tree and output responses of training 
samples in the leaf nodes. A testing sample using these criteria will reach a leaf node and the average of the 
Output response vectors in the leaf node is considered as the prediction of the testing sample.
</p>


<h3>Value</h3>

<p>Prediction result of the Testing samples for a particular tree
</p>

<hr>
<h2 id='split_node'>Splitting Criteria of all the nodes of the tree</h2><span id='topic+split_node'></span>

<h3>Description</h3>

<p>Stores the Splitting criteria of all the nodes of a tree in a list
</p>


<h3>Usage</h3>

<pre><code class='language-R'>split_node(X, Y, m_feature, Index, i, model, min_leaf, Inv_Cov_Y, Command)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="split_node_+3A_x">X</code></td>
<td>
<p>Input Training matrix of size M x N, M is the number of training samples and N is the number of features</p>
</td></tr>
<tr><td><code id="split_node_+3A_y">Y</code></td>
<td>
<p>Output Training response of size M x T, M is the number of samples and T is the number of output responses</p>
</td></tr>
<tr><td><code id="split_node_+3A_m_feature">m_feature</code></td>
<td>
<p>Number of randomly selected features considered for a split in each regression tree node</p>
</td></tr>
<tr><td><code id="split_node_+3A_index">Index</code></td>
<td>
<p>Index of training samples</p>
</td></tr>
<tr><td><code id="split_node_+3A_i">i</code></td>
<td>
<p>Number of split. Used as an index, which indicates where in the list the splitting
criteria of this split will be stored.</p>
</td></tr>
<tr><td><code id="split_node_+3A_model">model</code></td>
<td>
<p>A list of lists with the spliting criteria of all the node splits. In each iteration,
a new list is included with the spliting criteria of the new split of a node.</p>
</td></tr>
<tr><td><code id="split_node_+3A_min_leaf">min_leaf</code></td>
<td>
<p>Minimum number of samples in the leaf node. If a node has less than or, equal to min_leaf samples,
then there will be no splitting in that node and the node is a leaf node. Valid input is a positive integer and less than or equal to M (number of training samples)</p>
</td></tr>
<tr><td><code id="split_node_+3A_inv_cov_y">Inv_Cov_Y</code></td>
<td>
<p>Inverse of Covariance matrix of Output Response matrix for MRF (Input [0 0; 0 0] for RF)</p>
</td></tr>
<tr><td><code id="split_node_+3A_command">Command</code></td>
<td>
<p>1 for univariate Regression Tree (corresponding to RF) and 2 for Multivariate Regression Tree (corresponding to MRF)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the splitting criteria of a node and stores the information in a list format. 
If the node is a parent node, then indices of left and right nodes and feature number and threshold value 
of the feature for the split are stored. If the node is a leaf, the output feature matrix of the samples 
for the node are stored as a list.
</p>


<h3>Value</h3>

<p>model: A list of lists with the splitting criteria of all the split of the nodes. In each iteration, the Model is 
updated with a new list that includes the splitting criteria of the new split of a node.
</p>

<hr>
<h2 id='splitt'>Split of the Parent node</h2><span id='topic+splitt'></span>

<h3>Description</h3>

<p>Split of the training samples of the parent node into the child nodes based on the feature and threshold that produces the minimum cost
</p>


<h3>Usage</h3>

<pre><code class='language-R'>splitt(X, Y, m_feature, Index, Inv_Cov_Y, Command, ff)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="splitt_+3A_x">X</code></td>
<td>
<p>Input Training matrix of size M x N, M is the number of training samples and N is the number of features</p>
</td></tr>
<tr><td><code id="splitt_+3A_y">Y</code></td>
<td>
<p>Output Training response of size M x T, M is the number of samples and T is the number of output responses</p>
</td></tr>
<tr><td><code id="splitt_+3A_m_feature">m_feature</code></td>
<td>
<p>Number of randomly selected features considered for a split in each regression tree node.</p>
</td></tr>
<tr><td><code id="splitt_+3A_index">Index</code></td>
<td>
<p>Index of training samples</p>
</td></tr>
<tr><td><code id="splitt_+3A_inv_cov_y">Inv_Cov_Y</code></td>
<td>
<p>Inverse of Covariance matrix of Output Response matrix for MRF (Input [0 0; 0 0] for RF)</p>
</td></tr>
<tr><td><code id="splitt_+3A_command">Command</code></td>
<td>
<p>1 for univariate Regression Tree (corresponding to RF) and 2 for Multivariate Regression Tree (corresponding to MRF)</p>
</td></tr>
<tr><td><code id="splitt_+3A_ff">ff</code></td>
<td>
<p>Vector of m_feature from all features of X. This varies with each split</p>
</td></tr>
</table>


<h3>Details</h3>

<p>At each node of a regression a tree, a fixed number of features (m_feature) are selected randomly to be 
considered for generating the split. Node cost for all selected features along with possible n-1 thresholds for 
n samples are considered to select the feature and threshold with minimum cost.
</p>


<h3>Value</h3>

<p>List with the following components:
</p>
<table role = "presentation">
<tr><td><code>index_left</code></td>
<td>
<p>Index of the samples that are in the left node after splitting</p>
</td></tr>
<tr><td><code>index_right</code></td>
<td>
<p>Index of the samples that are in the right node after splitting</p>
</td></tr>
<tr><td><code>which_feature</code></td>
<td>
<p>The number of the feature that produces the minimum splitting cost</p>
</td></tr>
<tr><td><code>threshold_feature</code></td>
<td>
<p>The threshold value for the node split. 
A feature value less than or equal to the threshold will go to the left node and it will go to the right node otherwise.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library(IntegratedMRF)
X=matrix(runif(20*100),20,100)
Y=matrix(runif(20*3),20,3)
m_feature=5
Index=1:20
Inv_Cov_Y=solve(cov(Y))
ff2 = ncol(X) # number of features
ff =sort(sample(ff2, m_feature)) 
Command=2#MRF, as number of output feature is greater than 1
Split_criteria=splitt(X,Y,m_feature,Index,Inv_Cov_Y,Command,ff) 
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
