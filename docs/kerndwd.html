<!DOCTYPE html><html><head><title>Help for package kerndwd</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {kerndwd}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#kerndwd-package'>
<p>Kernel Distance Weighted Discrimination</p></a></li>
<li><a href='#BUPA'><p>BUPA's liver disorders data</p></a></li>
<li><a href='#cv.kerndwd'><p>cross-validation</p></a></li>
<li><a href='#kerndwd'><p>solve Linear DWD and Kernel DWD</p></a></li>
<li><a href='#kernel functions'><p>Kernel Functions</p></a></li>
<li><a href='#plot.cv.kerndwd'><p>plot the cross-validation curve</p></a></li>
<li><a href='#plot.kerndwd'><p>plot coefficients</p></a></li>
<li><a href='#predict.kerndwd'><p>predict class labels for new observations</p></a></li>
<li><a href='#tunedwd'><p>fast tune procedure for DWD</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Distance Weighted Discrimination (DWD) and Kernel Methods</td>
</tr>
<tr>
<td>Version:</td>
<td>2.0.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-08-27</td>
</tr>
<tr>
<td>Author:</td>
<td>Boxiang Wang &lt;boxiang-wang@uiowa.edu&gt;, Hui Zou &lt;hzou@stat.umn.edu&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Boxiang Wang &lt;boxiang-wang@uiowa.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>A novel implementation that solves the linear distance weighted discrimination and the kernel distance weighted discrimination. Reference: Wang and Zou (2018) &lt;<a href="https://doi.org/10.1111%2Frssb.12244">doi:10.1111/rssb.12244</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>methods</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics, grDevices, stats, utils</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-09-01 14:53:24 UTC; boxiangw</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-09-03 22:22:23 UTC</td>
</tr>
</table>
<hr>
<h2 id='kerndwd-package'>
Kernel Distance Weighted Discrimination
</h2><span id='topic+kerndwd-package'></span>

<h3>Description</h3>

<p>Extremely novel efficient procedures for solving linear generalized DWD and kernel generalized DWD in reproducing kernel Hilbert spaces for classification. The algorithm is based on the majorization-minimization (MM) principle to compute the entire solution path at a given fine grid of regularization parameters.<br />
</p>


<h3>Details</h3>

<p>Suppose <code>x</code> is predictor and <code>y</code> is a binary response. The package computes the entire solution path over a grid of <code>lambda</code> values.
</p>
<p>The main functions of the package <code><a href="#topic+kerndwd">kerndwd</a></code> include:<br />
<code>kerndwd</code><br />
<code>cv.kerndwd</code><br />
<code>tunedwd</code><br />
<code>predict.kerndwd</code><br />
<code>plot.kerndwd</code><br />
<code>plot.cv.kerndwd</code><br />
</p>


<h3>Author(s)</h3>

<p>Boxiang Wang and Hui Zou<br />
Maintainer: Boxiang Wang  <a href="mailto:boxiang-wang@uiowa.edu">boxiang-wang@uiowa.edu</a></p>


<h3>References</h3>

<p>Wang, B. and Zou, H. (2018)
&ldquo;Another Look at Distance Weighted Discrimination,&quot; 
<em>Journal of Royal Statistical Society, Series B</em>, <b>80</b>(1), 177&ndash;198. <br />
<a href="https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12244">https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12244</a><br />
Karatzoglou, A., Smola, A., Hornik, K., and Zeileis, A. (2004)
&ldquo;kernlab &ndash; An S4 Package for Kernel Methods in R&quot;, 
<em>Journal of Statistical Software</em>, <b>11</b>(9), 1&ndash;20.<br />
<a href="https://www.jstatsoft.org/v11/i09/paper">https://www.jstatsoft.org/v11/i09/paper</a><br />
Marron, J.S., Todd, M.J., Ahn, J. (2007)
&ldquo;Distance-Weighted Discrimination&quot;&quot;, 
<em>Journal of the American Statistical Association</em>, <b>102</b>(408), 1267&ndash;1271.<br />
<a href="https://www.tandfonline.com/doi/abs/10.1198/016214507000001120">https://www.tandfonline.com/doi/abs/10.1198/016214507000001120</a><br />
</p>

<hr>
<h2 id='BUPA'>BUPA's liver disorders data</h2><span id='topic+BUPA'></span>

<h3>Description</h3>

<p>BUPA's liver disorders data: 345 male individuals' blood test result and liver disorder status. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(BUPA)
</code></pre>


<h3>Details</h3>

<p>This data set consists of 345 observations and 6 predictors representing the blood test result liver disorder status of 345 patients. The three predictors are mean corpuscular volume (MCV), alkaline phosphotase (ALKPHOS), alamine aminotransferase (SGPT), aspartate aminotransferase (SGOT), gamma-glutamyl transpeptidase (GAMMAGT), and the number of alcoholic beverage drinks per day (DRINKS). 
</p>


<h3>Value</h3>

<p>A list with the following elements:
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>A numerical matrix for predictors: 345 rows and 6 columns; each row corresponds to a patient.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>A numeric vector of length 305 representing the liver disorder status.</p>
</td></tr>
</table>


<h3>Source</h3>

<p>The data set is available for download from UCI machine learning repository. <br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load data set
data(BUPA)

# the number of samples predictors
dim(BUPA$X)

# the number of samples for each class
sum(BUPA$y == -1) 
sum(BUPA$y == 1)
</code></pre>

<hr>
<h2 id='cv.kerndwd'>cross-validation</h2><span id='topic+cv.kerndwd'></span>

<h3>Description</h3>

<p>Carry out a cross-validation for <code><a href="#topic+kerndwd">kerndwd</a></code> to find optimal values of the tuning parameter <code>lambda</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.kerndwd(x, y, kern, lambda, nfolds=5, foldid, wt, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.kerndwd_+3A_x">x</code></td>
<td>
<p>A matrix of predictors, i.e., the matrix <code>x</code> used in <code><a href="#topic+kerndwd">kerndwd</a></code>.</p>
</td></tr>
<tr><td><code id="cv.kerndwd_+3A_y">y</code></td>
<td>
<p>A vector of binary class labels, i.e., the <code>y</code> used in <code><a href="#topic+kerndwd">kerndwd</a></code>. <code>y</code> has to be two levels.</p>
</td></tr>
<tr><td><code id="cv.kerndwd_+3A_kern">kern</code></td>
<td>
<p>A kernel function.</p>
</td></tr>
<tr><td><code id="cv.kerndwd_+3A_lambda">lambda</code></td>
<td>
<p>A user specified <code>lambda</code> candidate sequence for cross-validation.</p>
</td></tr>
<tr><td><code id="cv.kerndwd_+3A_nfolds">nfolds</code></td>
<td>
<p>The number of folds. Default value is 5. The allowable range is from 3 to the sample size.</p>
</td></tr>
<tr><td><code id="cv.kerndwd_+3A_foldid">foldid</code></td>
<td>
<p>An optional vector with values between 1 and <code>nfold</code>, representing the fold indices for each observation. If supplied, <code>nfold</code> can be missing.</p>
</td></tr>
<tr><td><code id="cv.kerndwd_+3A_wt">wt</code></td>
<td>
<p>A vector of length <code class="reqn">n</code> for weight factors. When <code>wt</code> is missing or <code>wt=NULL</code>, an unweighted DWD is fitted. </p>
</td></tr> 
<tr><td><code id="cv.kerndwd_+3A_...">...</code></td>
<td>
<p>Other arguments being passed to <code><a href="#topic+kerndwd">kerndwd</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the mean cross-validation error and the standard error by fitting <code><a href="#topic+kerndwd">kerndwd</a></code> with every fold excluded alternatively. This function is modified based on the <code>cv</code> function from the <code>glmnet</code> package.
</p>


<h3>Value</h3>

<p>A <code><a href="#topic+cv.kerndwd">cv.kerndwd</a></code> object including the cross-validation results is return..
</p>
<table>
<tr><td><code>lambda</code></td>
<td>
<p>The <code>lambda</code> sequence used in <code><a href="#topic+kerndwd">kerndwd</a></code>.</p>
</td></tr>
<tr><td><code>cvm</code></td>
<td>
<p>A vector of length <code>length(lambda)</code>: mean cross-validated error.</p>
</td></tr>
<tr><td><code>cvsd</code></td>
<td>
<p>A vector of length <code>length(lambda)</code>: estimates of standard error of <code>cvm</code>.</p>
</td></tr>
<tr><td><code>cvupper</code></td>
<td>
<p>The upper curve: <code>cvm + cvsd</code>.</p>
</td></tr>
<tr><td><code>cvlower</code></td>
<td>
<p>The lower curve: <code>cvm - cvsd</code>.</p>
</td></tr>
<tr><td><code>lambda.min</code></td>
<td>
<p>The <code>lambda</code> incurring the minimum cross validation error <code>cvm</code>.</p>
</td></tr>
<tr><td><code>lambda.1se</code></td>
<td>
<p>The largest value of <code>lambda</code> such that error is within one standard error of the minimum.</p>
</td></tr>
<tr><td><code>cvm.min</code></td>
<td>
<p>The cross-validation error corresponding to <code>lambda.min</code>, i.e., the least error.</p>
</td></tr>
<tr><td><code>cvm.1se</code></td>
<td>
<p>The cross-validation error corresponding to <code>lambda.1se</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Boxiang Wang and Hui Zou<br />
Maintainer: Boxiang Wang  <a href="mailto:boxiang-wang@uiowa.edu">boxiang-wang@uiowa.edu</a></p>


<h3>References</h3>

<p>Wang, B. and Zou, H. (2018)
&ldquo;Another Look at Distance Weighted Discrimination,&quot; 
<em>Journal of Royal Statistical Society, Series B</em>, <b>80</b>(1), 177&ndash;198. <br />
<a href="https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12244">https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12244</a><br />
Friedman, J., Hastie, T., and Tibshirani, R. (2010), &quot;Regularization paths for generalized linear models via coordinate descent,&quot; <em>Journal of Statistical Software</em>, <b>33</b>(1), 1&ndash;22.<br />
<a href="https://www.jstatsoft.org/v33/i01/paper">https://www.jstatsoft.org/v33/i01/paper</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kerndwd">kerndwd</a></code> and <code><a href="#topic+plot.cv.kerndwd">plot.cv.kerndwd</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
data(BUPA)
BUPA$X = scale(BUPA$X, center=TRUE, scale=TRUE)
lambda = 10^(seq(3, -3, length.out=10))
kern = rbfdot(sigma=sigest(BUPA$X))
m.cv = cv.kerndwd(BUPA$X, BUPA$y, kern, qval=1, lambda=lambda, eps=1e-5, maxit=1e5)
m.cv$lambda.min
</code></pre>

<hr>
<h2 id='kerndwd'>solve Linear DWD and Kernel DWD</h2><span id='topic+kerndwd'></span>

<h3>Description</h3>

<p>Fit the linear generalized distance weighted discrimination (DWD) model and the generalized DWD on Reproducing kernel Hilbert space. The solution path is computed at a grid of values of tuning parameter <code>lambda</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>kerndwd(x, y, kern, lambda, qval=1, wt, eps=1e-05, maxit=1e+05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kerndwd_+3A_x">x</code></td>
<td>
<p>A numerical matrix with <code class="reqn">N</code> rows and <code class="reqn">p</code> columns for predictors.</p>
</td></tr>
<tr><td><code id="kerndwd_+3A_y">y</code></td>
<td>
<p>A vector of length <code class="reqn">N</code> for binary responses. The element of <code>y</code> is either -1 or 1.</p>
</td></tr>
<tr><td><code id="kerndwd_+3A_kern">kern</code></td>
<td>
<p>A kernel function; see <code><a href="#topic+dots">dots</a></code>.</p>
</td></tr>
<tr><td><code id="kerndwd_+3A_lambda">lambda</code></td>
<td>
<p>A user supplied <code>lambda</code> sequence.</p>
</td></tr>
<tr><td><code id="kerndwd_+3A_qval">qval</code></td>
<td>
<p>The exponent index of the generalized DWD. Default value is 1.</p>
</td></tr>
<tr><td><code id="kerndwd_+3A_wt">wt</code></td>
<td>
<p>A vector of length <code class="reqn">n</code> for weight factors. When <code>wt</code> is missing or <code>wt=NULL</code>, an unweighted DWD is fitted. </p>
</td></tr> 
<tr><td><code id="kerndwd_+3A_eps">eps</code></td>
<td>
<p>The algorithm stops when (i.e. <code class="reqn">\sum_j(\beta_j^{new}-\beta_j^{old})^2</code> is less than <code>eps</code>, where <code class="reqn">j=0,\ldots, p</code>. Default value is <code>1e-5</code>.</p>
</td></tr>
<tr><td><code id="kerndwd_+3A_maxit">maxit</code></td>
<td>
<p>The maximum of iterations allowed. Default is 1e5.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suppose that the generalized DWD loss is <code class="reqn">V_q(u)=1-u</code> if <code class="reqn">u \le q/(q+1)</code> and <code class="reqn">\frac{1}{u^q}\frac{q^q}{(q+1)^{(q+1)}}</code> if <code class="reqn">u &gt; q/(q+1)</code>. The value of <code class="reqn">\lambda</code>, i.e., <code>lambda</code>, is user-specified. 
</p>
<p>In the linear case (<code>kern</code> is the inner product and N &gt; p), the <code><a href="#topic+kerndwd">kerndwd</a></code> fits a linear DWD by minimizing the L2 penalized DWD loss function,
</p>
<p style="text-align: center;"><code class="reqn">\frac{1}{N}\sum_{i=1}^n V_q(y_i(\beta_0 + X_i'\beta)) + \lambda \beta' \beta.</code>
</p>
 
<p>If a linear DWD is fitted when N &lt; p, a kernel DWD with the linear kernel is actually solved. In such case, the coefficient <code class="reqn">\beta</code> can be obtained from <code class="reqn">\beta = X'\alpha.</code> 
</p>
<p>In the kernel case, the <code><a href="#topic+kerndwd">kerndwd</a></code> fits a kernel DWD by minimizing
</p>
<p style="text-align: center;"><code class="reqn">\frac{1}{N}\sum_{i=1}^n V_q(y_i(\beta_0 + K_i' \alpha)) + \lambda \alpha' K \alpha,</code>
</p>

<p>where <code class="reqn">K</code> is the kernel matrix and <code class="reqn">K_i</code> is the ith row. 
</p>
<p>The weighted linear DWD and the weighted kernel DWD are formulated as follows,
</p>
<p style="text-align: center;"><code class="reqn">\frac{1}{N}\sum_{i=1}^n w_i \cdot V_q(y_i(\beta_0 + X_i'\beta)) + \lambda \beta' \beta,</code>
</p>

<p style="text-align: center;"><code class="reqn">\frac{1}{N}\sum_{i=1}^n w_i \cdot V_q(y_i(\beta_0 + K_i' \alpha)) + \lambda \alpha' K \alpha,</code>
</p>

<p>where <code class="reqn">w_i</code> is the ith element of <code>wt</code>. The choice of weight factors can be seen in the reference below.
</p>


<h3>Value</h3>

<p>An object with S3 class <code><a href="#topic+kerndwd">kerndwd</a></code>.
</p>
<table>
<tr><td><code>alpha</code></td>
<td>
<p>A matrix of DWD coefficients at each <code>lambda</code> value. The dimension is <code>(p+1)*length(lambda)</code> in the linear case and <code>(N+1)*length(lambda)</code> in the kernel case.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>The <code>lambda</code> sequence.</p>
</td></tr>
<tr><td><code>npass</code></td>
<td>
<p>Total number of MM iterations for all lambda values. </p>
</td></tr>
<tr><td><code>jerr</code></td>
<td>
<p>Warnings and errors; 0 if none.</p>
</td></tr>
<tr><td><code>info</code></td>
<td>
<p>A list including parameters of the loss function, <code>eps</code>, <code>maxit</code>, <code>kern</code>, and <code>wt</code> if a weight vector was used.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The call that produced this object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Boxiang Wang and Hui Zou<br />
Maintainer: Boxiang Wang  <a href="mailto:boxiang-wang@uiowa.edu">boxiang-wang@uiowa.edu</a></p>


<h3>References</h3>

<p>Wang, B. and Zou, H. (2018)
&ldquo;Another Look at Distance Weighted Discrimination,&quot; 
<em>Journal of Royal Statistical Society, Series B</em>, <b>80</b>(1), 177&ndash;198. <br />
<a href="https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12244">https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12244</a><br />
Karatzoglou, A., Smola, A., Hornik, K., and Zeileis, A. (2004)
&ldquo;kernlab &ndash; An S4 Package for Kernel Methods in R&quot;, 
<em>Journal of Statistical Software</em>, <b>11</b>(9), 1&ndash;20.<br />
<a href="https://www.jstatsoft.org/v11/i09/paper">https://www.jstatsoft.org/v11/i09/paper</a><br />
Friedman, J., Hastie, T., and Tibshirani, R. (2010), &quot;Regularization paths for generalized
linear models via coordinate descent,&quot; <em>Journal of Statistical Software</em>, <b>33</b>(1), 1&ndash;22.<br />
<a href="https://www.jstatsoft.org/v33/i01/paper">https://www.jstatsoft.org/v33/i01/paper</a><br />
Marron, J.S., Todd, M.J., and Ahn, J. (2007)
&ldquo;Distance-Weighted Discrimination&quot;&quot;, 
<em>Journal of the American Statistical Association</em>, <b>102</b>(408), 1267&ndash;1271.<br />
<a href="https://www.tandfonline.com/doi/abs/10.1198/016214507000001120">https://www.tandfonline.com/doi/abs/10.1198/016214507000001120</a><br />
Qiao, X., Zhang, H., Liu, Y., Todd, M., Marron, J.S. (2010)
&ldquo;Weighted distance weighted discrimination and its asymptotic properties&quot;, 
<em>Journal of the American Statistical Association</em>, <b>105</b>(489), 401&ndash;414.<br />
<a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2010.tm08487">https://www.tandfonline.com/doi/abs/10.1198/jasa.2010.tm08487</a><br />
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.kerndwd">predict.kerndwd</a></code>, <code><a href="#topic+plot.kerndwd">plot.kerndwd</a></code>, and <code><a href="#topic+cv.kerndwd">cv.kerndwd</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(BUPA)
# standardize the predictors
BUPA$X = scale(BUPA$X, center=TRUE, scale=TRUE)

# a grid of tuning parameters
lambda = 10^(seq(3, -3, length.out=10))

# fit a linear DWD
kern = vanilladot()
DWD_linear = kerndwd(BUPA$X, BUPA$y, kern,
  qval=1, lambda=lambda, eps=1e-5, maxit=1e5)

# fit a DWD using Gaussian kernel
kern = rbfdot(sigma=1)
DWD_Gaussian = kerndwd(BUPA$X, BUPA$y, kern,
  qval=1, lambda=lambda, eps=1e-5, maxit=1e5)

# fit a weighted kernel DWD
kern = rbfdot(sigma=1)
weights = c(1, 2)[factor(BUPA$y)]
DWD_wtGaussian = kerndwd(BUPA$X, BUPA$y, kern,
  qval=1, lambda=lambda, wt = weights, eps=1e-5, maxit=1e5)
</code></pre>

<hr>
<h2 id='kernel+20functions'>Kernel Functions</h2><span id='topic+kern'></span><span id='topic+dots'></span><span id='topic+rbfdot'></span><span id='topic+polydot'></span><span id='topic+vanilladot'></span><span id='topic+laplacedot'></span><span id='topic+besseldot'></span><span id='topic+anovadot'></span><span id='topic+splinedot'></span><span id='topic+rbfkernel-class'></span><span id='topic+polykernel-class'></span><span id='topic+vanillakernel-class'></span><span id='topic+anovakernel-class'></span><span id='topic+besselkernel-class'></span><span id='topic+laplacekernel-class'></span><span id='topic+splinekernel-class'></span><span id='topic+sigest'></span>

<h3>Description</h3>

<p>Kernel functions provided in the R package <code>kernlab</code>. Details can be seen in the reference below.<br />
The Gaussian RBF kernel <code class="reqn">k(x,x') = \exp(-\sigma \|x - x'\|^2)</code> <br />
The Polynomial kernel <code class="reqn">k(x,x') = (scale &lt;x, x'&gt; + offset)^{degree}</code><br />
The Linear kernel <code class="reqn">k(x,x') = &lt;x, x'&gt;</code><br />
The Laplacian kernel <code class="reqn">k(x,x') = \exp(-\sigma \|x - x'\|)</code> <br />
The Bessel kernel <code class="reqn">k(x,x') = (- \mathrm{Bessel}_{(\nu+1)}^n \sigma \|x - x'\|^2)</code> <br />
The ANOVA RBF kernel <code class="reqn">k(x,x') = \sum_{1\leq i_1 \ldots &lt; i_D \leq N} 
  \prod_{d=1}^D k(x_{id}, {x'}_{id})</code> where k(x, x) is a Gaussian RBF kernel. <br />
The Spline kernel <code class="reqn"> \prod_{d=1}^D 1 + x_i x_j + x_i x_j \min(x_i,
    x_j)  - \frac{x_i + x_j}{2} \min(x_i,x_j)^2 +
    \frac{\min(x_i,x_j)^3}{3}</code>.
The parameter <code>sigma</code> used in <code>rbfdot</code> can be selected by <code>sigest()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbfdot(sigma = 1)
polydot(degree = 1, scale = 1, offset = 1)
vanilladot()
laplacedot(sigma = 1)
besseldot(sigma = 1, order = 1, degree = 1)
anovadot(sigma = 1, degree = 1)
splinedot()
sigest(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernel+2B20functions_+3A_sigma">sigma</code></td>
<td>
<p>The inverse kernel width used by the Gaussian, the
Laplacian, the Bessel, and the ANOVA kernel.</p>
</td></tr>
<tr><td><code id="kernel+2B20functions_+3A_degree">degree</code></td>
<td>
<p>The degree of the polynomial, bessel or ANOVA
kernel function. This has to be an positive integer.</p>
</td></tr>
<tr><td><code id="kernel+2B20functions_+3A_scale">scale</code></td>
<td>
<p>The scaling parameter of the polynomial kernel function.</p>
</td></tr>
<tr><td><code id="kernel+2B20functions_+3A_offset">offset</code></td>
<td>
<p>The offset used in a polynomial kernel.</p>
</td></tr>
<tr><td><code id="kernel+2B20functions_+3A_order">order</code></td>
<td>
<p>The order of the Bessel function to be used as a kernel.</p>
</td></tr>
<tr><td><code id="kernel+2B20functions_+3A_x">x</code></td>
<td>
<p>The design matrix used in <code>kerndwd</code> when <code>sigest</code> is called to estimate <code>sigma</code> in <code>rbfdot()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These R functions and descriptions are directly duplicated and/or adapted from the R package <code>kernlab</code>.</p>


<h3>Value</h3>

<p>Return an S4 object of class <code>kernel</code> which can be used as the argument of <code>kern</code> when fitting a <code><a href="#topic+kerndwd">kerndwd</a></code> model.
</p>


<h3>References</h3>

<p>Wang, B. and Zou, H. (2018)
&ldquo;Another Look at Distance Weighted Discrimination,&quot; 
<em>Journal of Royal Statistical Society, Series B</em>, <b>80</b>(1), 177&ndash;198. <br />
<a href="https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12244">https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12244</a><br />
</p>
<p>Karatzoglou, A., Smola, A., Hornik, K., and Zeileis, A. (2004)
&ldquo;kernlab &ndash; An S4 Package for Kernel Methods in R&quot;, 
<em>Journal of Statistical Software</em>, <b>11</b>(9), 1&ndash;20.<br />
<a href="https://www.jstatsoft.org/v11/i09/paper">https://www.jstatsoft.org/v11/i09/paper</a><br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(BUPA)
# generate a linear kernel
kfun = vanilladot()

# generate a Laplacian kernel function with sigma = 1
kfun = laplacedot(sigma=1)

# generate a Gaussian kernel function with sigma estimated by sigest()
kfun = rbfdot(sigma=sigest(BUPA$X))

# set kern=kfun when fitting a kerndwd object
data(BUPA)
BUPA$X = scale(BUPA$X, center=TRUE, scale=TRUE)
lambda = 10^(seq(-3, 3, length.out=10))
m1 = kerndwd(BUPA$X, BUPA$y, kern=kfun,
  qval=1, lambda=lambda, eps=1e-5, maxit=1e5)
</code></pre>

<hr>
<h2 id='plot.cv.kerndwd'>plot the cross-validation curve</h2><span id='topic+plot.cv.kerndwd'></span>

<h3>Description</h3>

<p>Plot cross-validation error curves with the upper and lower standard deviations versus log <code>lambda</code> values.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv.kerndwd'
plot(x, sign.lambda, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.cv.kerndwd_+3A_x">x</code></td>
<td>
<p>A fitted <code><a href="#topic+cv.kerndwd">cv.kerndwd</a></code> object.</p>
</td></tr>
<tr><td><code id="plot.cv.kerndwd_+3A_sign.lambda">sign.lambda</code></td>
<td>
<p>Against <code>log(lambda)</code> (default) or its negative if <code>sign.lambda=-1</code>.</p>
</td></tr>
<tr><td><code id="plot.cv.kerndwd_+3A_...">...</code></td>
<td>
<p>Other graphical parameters being passed to <code>plot</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function plots the cross-validation error curves. This function is modified based on the <code>plot.cv</code> function of the <code>glmnet</code> package.
</p>


<h3>Author(s)</h3>

<p>Boxiang Wang and Hui Zou<br />
Maintainer: Boxiang Wang  <a href="mailto:boxiang-wang@uiowa.edu">boxiang-wang@uiowa.edu</a></p>


<h3>References</h3>

<p>Wang, B. and Zou, H. (2018)
&ldquo;Another Look at Distance Weighted Discrimination,&quot; 
<em>Journal of Royal Statistical Society, Series B</em>, <b>80</b>(1), 177&ndash;198. <br />
<a href="https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12244">https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12244</a><br />
</p>
<p>Friedman, J., Hastie, T., and Tibshirani, R. (2010), &quot;Regularization paths for generalized
linear models via coordinate descent,&quot; <em>Journal of Statistical Software</em>, <b>33</b>(1), 1&ndash;22.<br />
<a href="https://www.jstatsoft.org/v33/i01/paper">https://www.jstatsoft.org/v33/i01/paper</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.kerndwd">cv.kerndwd</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
data(BUPA)
BUPA$X = scale(BUPA$X, center=TRUE, scale=TRUE)
lambda = 10^(seq(-3, 3, length.out=10))
kern = rbfdot(sigma=sigest(BUPA$X))
m.cv = cv.kerndwd(BUPA$X, BUPA$y, kern,
  qval=1, lambda=lambda, eps=1e-5, maxit=1e5)
m.cv
</code></pre>

<hr>
<h2 id='plot.kerndwd'>plot coefficients</h2><span id='topic+plot.kerndwd'></span>

<h3>Description</h3>

<p>Plot the solution paths for a fitted <code><a href="#topic+kerndwd">kerndwd</a></code> object.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'kerndwd'
plot(x, color=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.kerndwd_+3A_x">x</code></td>
<td>
<p>A fitted &ldquo;<code><a href="#topic+kerndwd">kerndwd</a></code>&quot;&quot; model.</p>
</td></tr>
<tr><td><code id="plot.kerndwd_+3A_color">color</code></td>
<td>
<p>If <code>TRUE</code>, plots the curves with rainbow colors; otherwise, with gray colors (default).</p>
</td></tr>
<tr><td><code id="plot.kerndwd_+3A_...">...</code></td>
<td>
<p>Other graphical parameters to <code>plot</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Plots the solution paths as a coefficient profile plot. This function is modified based on the <code>plot</code> function from the <code>glmnet</code> package.
</p>


<h3>Author(s)</h3>

<p>Boxiang Wang and Hui Zou<br />
Maintainer: Boxiang Wang  <a href="mailto:boxiang-wang@uiowa.edu">boxiang-wang@uiowa.edu</a></p>


<h3>References</h3>

<p>Wang, B. and Zou, H. (2018)
&ldquo;Another Look at Distance Weighted Discrimination,&quot; 
<em>Journal of Royal Statistical Society, Series B</em>, <b>80</b>(1), 177&ndash;198. <br />
<a href="https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12244">https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12244</a><br />
Friedman, J., Hastie, T., and Tibshirani, R. (2010), &quot;Regularization paths for generalized linear models via coordinate descent,&quot; <em>Journal of Statistical Software</em>, <b>33</b>(1), 1&ndash;22.<br />
<a href="https://www.jstatsoft.org/v33/i01/paper">https://www.jstatsoft.org/v33/i01/paper</a>
</p>


<h3>See Also</h3>

<p><code>kerndwd</code>, <code>predict.kerndwd</code>, <code>coef.kerndwd</code>, <code>plot.kerndwd</code>, and <code>cv.kerndwd</code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(BUPA)
BUPA$X = scale(BUPA$X, center=TRUE, scale=TRUE)
lambda = 10^(seq(-3, 3, length.out=10))
kern = rbfdot(sigma=sigest(BUPA$X))
m1 = kerndwd(BUPA$X, BUPA$y, kern, qval=1, 
  lambda=lambda, eps=1e-5, maxit=1e5)
plot(m1, color=TRUE)
</code></pre>

<hr>
<h2 id='predict.kerndwd'>predict class labels for new observations</h2><span id='topic+predict.kerndwd'></span>

<h3>Description</h3>

<p>Predict the binary class labels or the fitted values of an <code><a href="#topic+kerndwd">kerndwd</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'kerndwd'
predict(object, kern, x, newx, type=c("class", "link"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.kerndwd_+3A_object">object</code></td>
<td>
<p>A fitted <code><a href="#topic+kerndwd">kerndwd</a></code> object.</p>
</td></tr>
<tr><td><code id="predict.kerndwd_+3A_kern">kern</code></td>
<td>
<p>The kernel function used when fitting the <code><a href="#topic+kerndwd">kerndwd</a></code> object.</p>
</td></tr>
<tr><td><code id="predict.kerndwd_+3A_x">x</code></td>
<td>
<p>The predictor matrix, i.e., the <code>x</code> matrix used when fitting the <code><a href="#topic+kerndwd">kerndwd</a></code> object.</p>
</td></tr>
<tr><td><code id="predict.kerndwd_+3A_newx">newx</code></td>
<td>
<p>A matrix of new values for <code>x</code> at which predictions are to be made. We note that <code>newx</code> must be a matrix, <code>predict</code> function does not accept a vector or other formats of <code>newx</code>.</p>
</td></tr>
<tr><td><code id="predict.kerndwd_+3A_type">type</code></td>
<td>
<p><code>"class"</code> or <code>"link"</code>? <code>"class"</code> produces the predicted binary class labels and <code>"link"</code> returns the fitted values. Default is <code>"class"</code>.</p>
</td></tr>
<tr><td><code id="predict.kerndwd_+3A_...">...</code></td>
<td>
<p>Not used. Other arguments to <code>predict</code>.</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>If <code>"type"</code> is <code>"class"</code>, the function returns the predicted class labels. If <code>"type"</code> is <code>"link"</code>, the result is <code class="reqn">\beta_0 + x_i'\beta</code> for the linear case and <code class="reqn">\beta_0 + K_i'\alpha</code> for the kernel case.</p>


<h3>Value</h3>

<p>Returns either the predicted class labels or the fitted values, depending on the choice of <code>type</code>.</p>


<h3>Author(s)</h3>

<p>Boxiang Wang and Hui Zou<br />
Maintainer: Boxiang Wang  <a href="mailto:boxiang-wang@uiowa.edu">boxiang-wang@uiowa.edu</a></p>


<h3>References</h3>

<p>Wang, B. and Zou, H. (2018)
&ldquo;Another Look at Distance Weighted Discrimination,&quot; 
<em>Journal of Royal Statistical Society, Series B</em>, <b>80</b>(1), 177&ndash;198. <br />
<a href="https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12244">https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12244</a><br />
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kerndwd">kerndwd</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(BUPA)
BUPA$X = scale(BUPA$X, center=TRUE, scale=TRUE)
lambda = 10^(seq(-3, 3, length.out=10))
kern = rbfdot(sigma=sigest(BUPA$X))
m1 = kerndwd(BUPA$X, BUPA$y, kern,
  qval=1, lambda=lambda, eps=1e-5, maxit=1e5)
predict(m1, kern, BUPA$X, tail(BUPA$X))
</code></pre>

<hr>
<h2 id='tunedwd'>fast tune procedure for DWD</h2><span id='topic+tunedwd'></span>

<h3>Description</h3>

<p>A fast implementaiton of cross-validation for <code><a href="#topic+kerndwd">kerndwd</a></code> to find the optimal values of the tuning parameter <code>lambda</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>tunedwd(x, y, kern, lambda, qvals=1, eps=1e-5, maxit=1e+5, nfolds=5, foldid=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tunedwd_+3A_x">x</code></td>
<td>
<p>A matrix of predictors, i.e., the matrix <code>x</code> used in <code><a href="#topic+kerndwd">kerndwd</a></code>.</p>
</td></tr>
<tr><td><code id="tunedwd_+3A_y">y</code></td>
<td>
<p>A vector of binary class labels, i.e., the <code>y</code> used in <code><a href="#topic+kerndwd">kerndwd</a></code>. <code>y</code> has two levels.</p>
</td></tr>
<tr><td><code id="tunedwd_+3A_kern">kern</code></td>
<td>
<p>A kernel function.</p>
</td></tr>
<tr><td><code id="tunedwd_+3A_lambda">lambda</code></td>
<td>
<p>A user specified <code>lambda</code> candidate sequence for cross-validation.</p>
</td></tr>
<tr><td><code id="tunedwd_+3A_qvals">qvals</code></td>
<td>
<p>A vector containing the index of the generalized DWD. Default value is 1.</p>
</td></tr>
<tr><td><code id="tunedwd_+3A_eps">eps</code></td>
<td>
<p>The algorithm stops when (i.e. <code class="reqn">\sum_j(\beta_j^{new}-\beta_j^{old})^2</code> is less than <code>eps</code>, where <code class="reqn">j=0,\ldots, p</code>. Default value is <code>1e-5</code>.</p>
</td></tr>
<tr><td><code id="tunedwd_+3A_maxit">maxit</code></td>
<td>
<p>The maximum of iterations allowed. Default is 1e5.</p>
</td></tr>
<tr><td><code id="tunedwd_+3A_nfolds">nfolds</code></td>
<td>
<p>The number of folds. Default value is 5. The allowable range is from 3 to the sample size.</p>
</td></tr>
<tr><td><code id="tunedwd_+3A_foldid">foldid</code></td>
<td>
<p>An optional vector with values between 1 and <code>nfold</code>, representing the fold indices for each observation. If supplied, <code>nfold</code> can be missing.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function returns the best tuning parameters
<code>q</code> and <code>lambda</code> by cross-validation. An efficient tune method is employed to accelerate the algorithm.
</p>


<h3>Value</h3>

<p>A <code>tunedwd.kerndwd</code> object including the cross-validation results is return.
</p>
<table>
<tr><td><code>lam.tune</code></td>
<td>
<p>The optimal <code>lambda</code> value.</p>
</td></tr>
<tr><td><code>q.tune</code></td>
<td>
<p>The optimal <code>q</code> value.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Boxiang Wang and Hui Zou<br />
Maintainer: Boxiang Wang  <a href="mailto:boxiang-wang@uiowa.edu">boxiang-wang@uiowa.edu</a></p>


<h3>References</h3>

<p>Wang, B. and Zou, H. (2018)
&ldquo;Another Look at Distance Weighted Discrimination,&quot; 
<em>Journal of Royal Statistical Society, Series B</em>, <b>80</b>(1), 177&ndash;198. <br />
<a href="https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12244">https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12244</a><br />
Friedman, J., Hastie, T., and Tibshirani, R. (2010), &quot;Regularization paths for generalized linear models via coordinate descent,&quot; <em>Journal of Statistical Software</em>, <b>33</b>(1), 1&ndash;22.<br />
<a href="https://www.jstatsoft.org/v33/i01/paper">https://www.jstatsoft.org/v33/i01/paper</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kerndwd">kerndwd</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
data(BUPA)
BUPA$X = scale(BUPA$X, center=TRUE, scale=TRUE)
lambda = 10^(seq(-3, 3, length.out=10))
kern = rbfdot(sigma=sigest(BUPA$X))
ret = tunedwd(BUPA$X, BUPA$y, kern, qvals=c(1,2,10), lambda=lambda, eps=1e-5, maxit=1e5)
ret
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
