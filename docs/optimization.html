<!DOCTYPE html><html lang="en"><head><title>Help for package optimization</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {optimization}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#optimization-package'>
<p>Flexible Optimization of Complex Loss Functions with State and Parameter</p>
Space Constraints</a></li>
<li><a href='#optim_nm'>
<p>Optimization with Nelder-Mead</p></a></li>
<li><a href='#optim_sa'>
<p>Flexible Optimization with Simulated Annealing</p></a></li>
<li><a href='#plot.optim_nmsa'>
<p>Plot an optim_nmsa Object</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Flexible Optimization of Complex Loss Functions with State and
Parameter Space Constraints</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0-9</td>
</tr>
<tr>
<td>Description:</td>
<td>Flexible optimizer with numerous input specifications for detailed
  parameterisation. Designed for complex loss functions with state and 
  parameter space constraints. Visualization tools for validation and analysis
  of the convergence are included.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.0), Rcpp (&ge; 0.12.12)</td>
</tr>
<tr>
<td>Imports:</td>
<td>colorspace</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>R.rsp</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>R.rsp</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/kaihusmann/optimization">https://github.com/kaihusmann/optimization</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/kaihusmann/optimization/issues">https://github.com/kaihusmann/optimization/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-02-15 12:44:19 UTC; khusman1</td>
</tr>
<tr>
<td>Author:</td>
<td>Kai Husmann [aut, cre],
  Alexander Lange [aut],
  Nordwestdeutsche Forstliche Versuchsanstalt (NW-FVA) [cph, fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kai Husmann &lt;kai.husmann@uni-goettingen.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-02-15 14:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='optimization-package'>
Flexible Optimization of Complex Loss Functions with State and Parameter
Space Constraints
</h2><span id='topic+optimization-package'></span>

<h3>Description</h3>

<p>Flexible optimizer with numerous input specifications for detailed
  parameterisation. Designed for complex loss functions with state and 
  parameter space constraints. Visualization tools for validation and analysis
  of the convergence are included.
</p>


<h3>Details</h3>

<p>Package: optimization
Type:    Package
Version: 1.0-6
Date:    2017-09-23
License: GPL-2
</p>


<h3>Author(s)</h3>

<p>NA
</p>
<p>Maintainer: NA
</p>


<h3>References</h3>

<p>Corana, A., Marchesi, M., Martini, C. and Ridella, S. (1987), Minimizing Multimodal Functions of Continuous Variables with the 'Simulated Annealing' Algorithm. ACM Transactions on Mathematical Software, 13(3):262-280.
</p>
<p>Gao, F. and Han, L. (2012). Implementing the nelder-mead simplex algorithm with adaptive parameters. Computational Optimization and Applications, 51(1):259 277.
</p>
<p>Geiger, C. and Kanzow, C. (1999). Das nelder-mead-verfahren. Numerische Verfahren zur Loesung unregestrierter Optimierungsaufgaben.
</p>
<p>Kirkpatrick, S., Gelatt, C. D. and Vecchi, M. P. (1983). Optimization by Simulated Annealing. Science, 220(4598): 671-680.
</p>
<p>Nelder, J. and Mead, R. (1965). A simplex method for function minimization. Computer Journal, 7(4).
</p>
<p>Pronzato, L., Walter, E., Venot, A. and Lebruchec, J.-F. (1984). A general-purpose global optimizer: Implementation and applications. Mathematics and Computers in Simulation, 26(5):412-422.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+optim_nm">optim_nm</a></code>, <code><a href="#topic+optim_sa">optim_sa</a></code>, <code><a href="stats.html#topic+optim">optim</a></code>,  <code><a href="base.html#topic+plot">plot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>hi &lt;- function(x){(x[1]**2 + x[2] - 11)**2 + (x[1] + x[2]**2 -7)**2}
optim_nm(fun = hi, k = 2)
optim_sa(fun = hi, start = c(runif(2, min = -1, max = 1)),
  trace = FALSE,
  lower = c(-4, -4),
  upper = c(4, 4),
  control = list(dyn_rf = FALSE,
    rf = 1.2,
    t0 = 10,
    nlimit = 100,
    r = 0.6,
    t_min = 0.1
  )
)
</code></pre>

<hr>
<h2 id='optim_nm'>
Optimization with Nelder-Mead
</h2><span id='topic+optim_nm'></span>

<h3>Description</h3>

<p>This function contains a direct search algorithm, to minimize or maximize an objective function with respect to their
input parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  optim_nm(fun, k = 0, start, maximum = FALSE, trace = FALSE,
           alpha = 1, beta = 2, gamma = 1/2, delta = 1/2,
           tol = 0.00001, exit = 500, edge = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="optim_nm_+3A_fun">fun</code></td>
<td>

<p>Function to minimize or maximize. It should return a single scalar value.
</p>
</td></tr>
<tr><td><code id="optim_nm_+3A_k">k</code></td>
<td>

<p>Number of parameters of the objective function.
</p>
</td></tr>
<tr><td><code id="optim_nm_+3A_start">start</code></td>
<td>

<p>Optional vector with starting values. Number of values must be equal to <code>k</code>. The initial simplex is constructed around this start vector.
</p>
</td></tr>
<tr><td><code id="optim_nm_+3A_maximum">maximum</code></td>
<td>

<p>Logical. The default is FALSE.
</p>
</td></tr>
<tr><td><code id="optim_nm_+3A_trace">trace</code></td>
<td>

<p>Logical. If TRUE, interim results are stored. Necessary for the plot function. Default is FALSE.
</p>
</td></tr>
<tr><td><code id="optim_nm_+3A_alpha">alpha</code></td>
<td>

<p>A positive scalar which indicates the size of the reflected simplex. The value 1 leads to a reflected simplex of the same size as the former iteration.
</p>
</td></tr>
<tr><td><code id="optim_nm_+3A_beta">beta</code></td>
<td>

<p>A positive scalar which indicates the size of the expended simplex. It is usually twice as high as <code>alpha</code>. It must be higher than <code>alpha</code>.
</p>
</td></tr>
<tr><td><code id="optim_nm_+3A_gamma">gamma</code></td>
<td>

<p>A positive scalar which indicates the size of either the outside contracted simplex or inside contracted simplex. It
is usually half as high as <code>alpha</code>. It must be smaller than <code>alpha</code>.
</p>
</td></tr>
<tr><td><code id="optim_nm_+3A_delta">delta</code></td>
<td>

<p>A positive scalar which indicates the size of the shrinked simplex. It is usually half as high as alpha. It must be smaller than <code>alpha</code>.
</p>
</td></tr>
<tr><td><code id="optim_nm_+3A_tol">tol</code></td>
<td>

<p>A positive scalar describing the tolerance at which the distances in between the function responses of the simplex vertices are close enough to zero to terminate the algorithm.
</p>
</td></tr>
<tr><td><code id="optim_nm_+3A_exit">exit</code></td>
<td>

<p>A positive scalar giving the maximum number of iterations the algorithm is allowed to take. It is used to prevent infinite loops. In case of optimizing functions with higher dimensions it is quite likely that the algorithm needs more than 500 iterations. The value should therefore be adjusted to the specific optimization problem.
</p>
</td></tr>
<tr><td><code id="optim_nm_+3A_edge">edge</code></td>
<td>

<p>A positive scalar providing the edge length of the initial simplex. It is useful to adjust the edge length if the initial guess is close to the global optimum or if the parameter space of the loss function is relatively small.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Nelder-Mead method is a comparatively simple heuristic optimization algorithm. It is, However, useful for relatively simple optimization problems without many local minima and low dimensions(n &lt; 10). Nevertheless, the speed and accuracy are rather useful for simple problems. Moreover, the Nelder-Mead is able to optimize functions without derivatives. The handling of the optimization function is quite easy, because there are only few parameters to adjust.
</p>


<h3>Value</h3>

<p>The output is a nmsa_optim object with following entries:
</p>

<dl>
<dt><code>par</code></dt><dd>
<p>Function parameters after optimization.
</p>
</dd>
<dt><code>function_value</code></dt><dd>
<p>Function response after optimization.
</p>
</dd>
<dt><code>trace</code></dt><dd>
<p>Matrix with interim results. NULL if <code>trace</code> was not activated.
</p>
</dd>
<dt><code>fun</code></dt><dd>
<p>The loss function.
</p>
</dd>
<dt><code>start</code></dt><dd>
<p>The initial function parameters.
</p>
</dd>
<dt><code>lower</code></dt><dd>
<p>The lower boundaries of the function parameters.
</p>
</dd>
<dt><code>upper</code></dt><dd>
<p>The upper boundaries of the function parameters.
</p>
</dd>
<dt><code>control</code></dt><dd>
<p>The number of parameters and iterations of the algorithm.
</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexander Lange
</p>


<h3>References</h3>

<p>Gao, F. and Han, L. (2012). Implementing the nelder-mead simplex algorithm
with adaptive parameters. Computational Optimization and Applications, 51(1):259
277.
</p>
<p>Geiger, C. and Kanzow, C. (1999). Das Nelder-Mead-Verfahren. Numerische
Verfahren zur Loesung unregestrierter Optimierungsaufgaben.
</p>
<p>Nelder, J. and Mead, R. (1965). A simplex method for function minimization.
Computer Journal, 7(4).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+optim_sa">optim_sa</a></code>, <code><a href="stats.html#topic+optim">optim</a></code>, <code><a href="#topic+plot.optim_nmsa">plot.optim_nmsa</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#####  Rosenbrock function
# minimum at f(1,1) = 0
   B &lt;- function(x){
    100*(x[2]-x[1]^2)^2+(1-x[1])^2
  }

##### Minimization with an initial guess at c(-2.048, 2.048)
  optim_nm(B, start = c(-2.048, 2.048))

#####  Himmelblau's function
# minimum at f(3,2) = 0
# f(-2.805, -3.1313) = 0
# f(-3.779, -3.283) = 0
#f(3.5844, -1.848) = 0
  H &lt;- function(x){
    (x[1]^2+x[2]-11)^2+(x[1]+x[2]^2-7)^2
  }

##### Minimization with defined number of parameters
  optim_nm(fun = H, k = 2)

##### Colville function with 4 parameters
  co &lt;- function(x){
    x1 &lt;- x[1]
    x2 &lt;- x[2]
    x3 &lt;- x[3]
    x4 &lt;- x[4]

    term1 &lt;- 100 * (x1^2 - x2)^2
    term2 &lt;- (x1 - 1)^2
    term3 &lt;- (x3-1)^2
    term4 &lt;- 90 * (x3^2 - x4)^2
    term5 &lt;- 10.1 * ((x2 - 1)^2 + (x4 - 1)^2)
    term6 &lt;- 19.8 * (x2 - 1)*(x4-1)

     y &lt;- term1 + term2 + term3 + term4 + term5 + term6
  }

  optim_nm(co, k = 4)

#### Minimization with trace
  Output &lt;- optim_nm(H, k = 2, trace = TRUE)
  plot(Output)
  plot(Output, 'contour')

</code></pre>

<hr>
<h2 id='optim_sa'>
Flexible Optimization with Simulated Annealing
</h2><span id='topic+optim_sa'></span>

<h3>Description</h3>

<p>Random search optimization method with systematic component that searches the global optimum. The loss function is allowed to be non-linear, non-differentiable and multimodal. Undefined responses are allowed as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optim_sa(fun, start, maximization = FALSE, trace = FALSE,
         lower, upper, control = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="optim_sa_+3A_fun">fun</code></td>
<td>

<p>Loss function to be optimized. It must return a scalar value. The variables must be assigned as a vector. See 'details'.
</p>
</td></tr>
<tr><td><code id="optim_sa_+3A_start">start</code></td>
<td>

<p>Vector of initial values for the function variables. Must be of same length as the variables vector of the loss function. The response of the initial variables combination must be defined (NA or NaN responses are not allowed).
</p>
</td></tr>
<tr><td><code id="optim_sa_+3A_maximization">maximization</code></td>
<td>

<p>Logical. Default is FALSE.
</p>
</td></tr>
<tr><td><code id="optim_sa_+3A_trace">trace</code></td>
<td>

<p>Logical. If TRUE, interim results are stored. Necessary for the plot function. Default is FALSE.
</p>
</td></tr>
<tr><td><code id="optim_sa_+3A_lower">lower</code></td>
<td>

<p>Vector of lower boundaries for the function variables. Must be of same length as the variables vector of the function.
</p>
</td></tr>
<tr><td><code id="optim_sa_+3A_upper">upper</code></td>
<td>

<p>Vector of upper boundaries for the function variables. Must be of same length as the variables vector of the function.
</p>
</td></tr>
<tr><td><code id="optim_sa_+3A_control">control</code></td>
<td>

<p>List with optional further arguments to modify the optimization specifically to the loss function:
</p>

<dl>
<dt><code>vf</code></dt><dd>
<p>Function that determines the variation of the function variables for the next iteration. The variation function is allowed to depend on the vector of variables of the current iteration, the vector of random factors <code>rf</code> and the temperature of the current iteration. Default is a uniform distributed random number with relative range <code>rf</code>.
</p>
</dd>
<dt><code>rf</code></dt><dd>
<p>Numeric vector. Random factor vector that determines the variation of the random number of <code>vf</code> in relation to the dimension of the function variables for the following iteration. Default is 1. If <code>dyn_rf</code> is enabled, the <code>rf</code> change dynamically over time.
</p>
</dd>
<dt><code>dyn_rf</code></dt><dd>
<p>Logical. <code>rf</code> change dynamically over time to ensure increasing precision with increasing number of iterations. Default is TRUE, see 'details'.
</p>
</dd>
<dt><code>t0</code></dt><dd>
<p>Numeric. Initial temperature. Default is 1000.
</p>
</dd>
<dt><code>nlimit</code></dt><dd>
<p>Integer. Maximum number of iterations of the inner loop. Default is 100.
</p>
</dd>
<dt><code>r</code></dt><dd>
<p>Numeric. Temperature reduction in the outer loop. Default is 0.6.
</p>
</dd>
<dt><code>k</code></dt><dd>
<p>Numeric. Constant for the Metropolis function. Default is 1.
</p>
</dd>
<dt><code>t_min</code></dt><dd>
<p>Numeric. Temperature where outer loop stops. Default is 0.1.
</p>
</dd>
<dt><code>maxgood</code></dt><dd>
<p>Integer. Break criterion to improve the algorithm performance. Maximum number of loss function improvements in the inner loop. Breaks the inner loop. Default is 100.
</p>
</dd>
<dt><code>stopac</code></dt><dd>
<p>Integer. Break criterion to improve the algorithm performance. Maximum number of repetitions where the loss improvement is lower than <code>ac_acc</code>. Breaks the inner loop. Default is 30.
</p>
</dd>
<dt><code>ac_acc</code></dt><dd>
<p>Numeric. Accuracy of the <code>stopac</code> break criterion in relation to the response. Default is 1/10000 of the function value at initial variables combination.
</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Details</h3>

<p>Simulated Annealing is an optimization algorithm for solving complex functions that may have several optima. The method is composed of a random and a systematic component. Basically, it randomly modifies the variables combination <code>n_limit</code> times to compare their response values. Depending on the temperature and the constant <code>k</code>, there is also a likelihood of choosing variables combinations with worse response. There is thus a time-decreasing likelihood of leaving local optima. The Simulated Annealing Optimization method is therefore advantageous for multimodal functions. Undefined response values (NA) are allowed as well. This can be useful for loss functions with variables restrictions. The high number of parameters allows a very flexible parameterization. <code>optim_sa</code> is able to solve mathematical formulas as well as complex rule sets.
</p>
<p>The performance therefore highly depends on the settings. It is indispensable to parameterize the algorithm carefully. The control list is pre-parameterized for loss functions of medium complexity. To improve the performance, the settings should be changed when solving relatively simple functions (e. g. three dimensional multimodal functions). For complex functions the settings should be changed to improve the accuracy. Most important parameters are <code>nlimit</code>,  <code>r</code> and <code>t0</code>.
</p>
<p>The dynamic <code>rf</code> adjustment depends on the number of loss function calls which are out of the variables boundaries as well as the temperature of the current iteration. The obligatory decreasing <code>rf</code> ensures a relatively wide search grid at the beginning of the optimization process that shrinks over time. It thus automatically adjusts for the trade-off between range of the search grid and accuracy. See Pronzato (1984) for more details. It is sometimes useful to disable the dynamic <code>rf</code> changing when the most performant <code>rf</code> are known. As <code>dyn_rf</code> usually improves the performance as well as the accuracy, the default is TRUE.
</p>


<h3>Value</h3>

<p>The output is a nmsa_optim list object with following entries:
</p>

<dl>
<dt><code>par</code></dt><dd>
<p>Function variables after optimization.
</p>
</dd>
<dt><code>function_value</code></dt><dd>
<p>Loss function response after optimization.
</p>
</dd>
<dt><code>trace</code></dt><dd>
<p>Matrix with interim results. NULL if <code>trace</code> was not activated.
</p>
</dd>
<dt><code>fun</code></dt><dd>
<p>The loss function.
</p>
</dd>
<dt><code>start</code></dt><dd>
<p>The initial function variables.
</p>
</dd>
<dt><code>lower</code></dt><dd>
<p>The lower boundaries of the function variables.
</p>
</dd>
<dt><code>upper</code></dt><dd>
<p>The upper boundaries of the function variables.
</p>
</dd>
<dt><code>control</code></dt><dd>
<p>Control arguments, see 'details'.
</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Kai Husmann
</p>


<h3>References</h3>

<p>Corana, A., Marchesi, M., Martini, C. and Ridella, S. (1987), Minimizing Multimodal Functions of Continuous Variables with the 'Simulated Annealing' Algorithm. ACM Transactions on Mathematical Software, 13(3):262-280.
</p>
<p>Kirkpatrick, S., Gelatt, C. D. and Vecchi, M. P. (1983). Optimization by Simulated Annealing. Science, 220(4598):671-680.
</p>
<p>Pronzato, L., Walter, E., Venot, A. and Lebruchec, J.-F. (1984). A general-purpose global optimizer: Implementation and applications. Mathematics and Computers in Simulation, 26(5):412-422.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+optim_nm">optim_nm</a></code>, <code><a href="stats.html#topic+optim">optim</a></code>, <code><a href="#topic+plot.optim_nmsa">plot.optim_nmsa</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##### Rosenbrock function
# minimum at f(1,1) = 0
ro &lt;- function(x){
  100*(x[2]-x[1]^2)^2+(1-x[1])^2
}

# Random start values. Example arguments for the relatively simple Rosenbrock function.
ro_sa &lt;- optim_sa(fun = ro,
                  start = c(runif(2, min = -1, max = 1)),
                  lower = c(-5, -5),
                  upper = c(5, 5),
                  trace = TRUE,
                  control = list(t0 = 100,
                            nlimit = 550,
                            t_min = 0.1,
                            dyn_rf = FALSE,
                            rf = 1,
                            r = 0.7
                  )
         )


# Visual inspection.
plot(ro_sa)
plot(ro_sa, type = "contour")


##### Holder table function

# 4 minima at
  #f(8.055, 9.665) = -19.2085
  #f(-8.055, 9.665) = -19.2085
  #f(8.055, -9.665) = -19.2085
  #f(-8.055, -9.665) = -19.2085

ho &lt;- function(x){
  x1 &lt;- x[1]
  x2 &lt;- x[2]

  fact1 &lt;- sin(x1) * cos(x2)
  fact2 &lt;- exp(abs(1 - sqrt(x1^2 + x2^2) / pi))
  y &lt;- -abs(fact1 * fact2)
}

# Random start values. Example arguments for the relatively complex Holder table function.
optim_sa(fun = ho,
         start = c(1, 1),
         lower = c(-10, -10),
         upper = c(10, 10),
         trace = TRUE,
         control = list(dyn_rf = FALSE,
                        rf = 1.6,
                        t0 = 10,
                        nlimit = 200,
                        r = 0.6,
                        t_min = 0.1
         )
)

</code></pre>

<hr>
<h2 id='plot.optim_nmsa'>
Plot an optim_nmsa Object
</h2><span id='topic+plot.optim_nmsa'></span>

<h3>Description</h3>

<p>Creates convergence or contour plots for visual inspection of the optimization result. Note that 'trace' must be activated for this function. <br />
In case of a bivariate optimization, the 'contour' plot gives an overview of the parameter development over time in the entire state space. This is useful for the evaluation of the algorithm settings and therefore helps improving the performance. The development of the response can be visualized via the 'convergence' plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'optim_nmsa'
plot(x, type = 'convergence', lower = NA, upper = NA, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.optim_nmsa_+3A_x">x</code></td>
<td>

<p>Object of type 'optim_nmsa' to be plotted. The 'trace' entry must not be empty.
</p>
</td></tr>
<tr><td><code id="plot.optim_nmsa_+3A_type">type</code></td>
<td>

<p>Character string which determines the plot type. Either 'convergence' or 'contour' is possible.
</p>
</td></tr>
<tr><td><code id="plot.optim_nmsa_+3A_lower">lower</code></td>
<td>

<p>Vector containing the lower limits of the variables in the plot. Only useful for 'contour' plots.
</p>
</td></tr>
<tr><td><code id="plot.optim_nmsa_+3A_upper">upper</code></td>
<td>

<p>Vector containing the upper limits of the variables in the plot. Only useful for 'contour' plots.
</p>
</td></tr>
<tr><td><code id="plot.optim_nmsa_+3A_...">...</code></td>
<td>

<p>Further arguments for the generic plot function.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kai Husmann, Alexander Lange
</p>


<h3>See Also</h3>

<p><code><a href="#topic+optim_nm">optim_nm</a></code>, <code><a href="#topic+optim_sa">optim_sa</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># S3 method for class 'optim_nlme'

# Himmelblau's function
hi &lt;- function(x){(x[1]**2 + x[2] - 11)**2 + (x[1] + x[2]**2 -7)**2}

out_nm &lt;- optim_nm(hi, k = 2, trace = TRUE)
out_sa &lt;- optim_sa(fun = hi, start = c(runif(2, min = -1, max = 1)),
                   trace = TRUE, lower = c(-4, -4) ,upper=c(4, 4),
                   control = list(t0 = 1000, nlimit = 1500,r = 0.8))

# Examples for optimization results via 'Nelder-Mead' method.
plot(out_nm)
plot(out_nm, type = "contour", lower = c(-4, -4), upper = c(4, 4))

# Examples for optimization results via 'Simulated Annealing' method.
plot(out_sa)
plot(out_sa, type = "contour")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
