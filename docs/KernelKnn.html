<!DOCTYPE html><html><head><title>Help for package KernelKnn</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {KernelKnn}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Boston'>
<p>Boston Housing Data    (Regression)</p></a></li>
<li><a href='#class_folds'><p>stratified folds (in classification)                      [ detailed information about class_folds in the FeatureSelection package ]</p></a></li>
<li><a href='#distMat.KernelKnn'><p>kernel k-nearest-neighbors using a distance matrix</p></a></li>
<li><a href='#distMat.knn.index.dist'><p>indices and distances of k-nearest-neighbors using a distance matrix</p></a></li>
<li><a href='#FUN_kernels'><p>performs kernel smoothing using a bandwidth. Besides using a kernel there is also the option to combine kernels</p></a></li>
<li><a href='#func_categorical_preds'><p>OPTION to convert categorical features TO either numeric [ if levels more than 32] OR to dummy variables [ if levels less than 32 ]</p></a></li>
<li><a href='#func_shuffle'><p>shuffle data</p></a></li>
<li><a href='#func_tbl'><p>this function returns a table of probabilities for each label</p></a></li>
<li><a href='#func_tbl_dist'><p>this function returns the probabilities in case of classification</p></a></li>
<li><a href='#FUNCTION_weights'><p>this function is used as a kernel-function-identifier [ takes the distances and a weights-kernel (in form of a function) and returns weights ]</p></a></li>
<li><a href='#ionosphere'>
<p>Johns Hopkins University Ionosphere database   (binary classification)</p></a></li>
<li><a href='#KernelKnn'><p>kernel k-nearest-neighbors</p></a></li>
<li><a href='#KernelKnnCV'><p>kernel-k-nearest-neighbors using cross-validation</p></a></li>
<li><a href='#knn.index.dist'><p>indices and distances of k-nearest-neighbors</p></a></li>
<li><a href='#normalized'><p>this function normalizes the data</p></a></li>
<li><a href='#regr_folds'><p>create folds (in regression)                                           [ detailed information about class_folds in the FeatureSelection package ]</p></a></li>
<li><a href='#switch.ops'><p>Arithmetic operations on lists</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Kernel k Nearest Neighbors</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-01-06</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mlampros/KernelKnn/issues">https://github.com/mlampros/KernelKnn/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mlampros/KernelKnn">https://github.com/mlampros/KernelKnn</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Extends the simple k-nearest neighbors algorithm by incorporating numerous kernel functions and a variety of distance metrics. The package takes advantage of 'RcppArmadillo' to speed up the calculation of distances between observations.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>libarmadillo: apt-get install -y libarmadillo-dev
(deb), libblas: apt-get install -y libblas-dev (deb),
liblapack: apt-get install -y liblapack-dev (deb),
libarpack++2: apt-get install -y libarpack++2-dev (deb),
gfortran: apt-get install -y gfortran (deb)</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R(&ge; 2.10.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.5)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, covr, knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-01-06 06:46:13 UTC; lampros</td>
</tr>
<tr>
<td>Author:</td>
<td>Lampros Mouselimis
    <a href="https://orcid.org/0000-0002-8024-1546"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre],
  Matthew Parks [ctb] (Github Contributor)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Lampros Mouselimis &lt;mouselimislampros@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-01-06 18:51:00 UTC</td>
</tr>
</table>
<hr>
<h2 id='Boston'>
Boston Housing Data    (Regression)
</h2><span id='topic+Boston'></span>

<h3>Description</h3>

<p>housing values in suburbs of Boston
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Boston)</code></pre>


<h3>Format</h3>

<p>A data frame with 506 Instances and 14 attributes (including the class attribute, &quot;medv&quot;)
</p>

<dl>
<dt><code>crim</code></dt><dd><p>per capita crime rate by town</p>
</dd>
<dt><code>zn</code></dt><dd><p>proportion of residential land zoned for lots over 25,000 sq.ft.</p>
</dd>
<dt><code>indus</code></dt><dd><p>proportion of non-retail business acres per town</p>
</dd>
<dt><code>chas</code></dt><dd><p>Charles River dummy variable (= 1 if tract bounds)</p>
</dd>
<dt><code>nox</code></dt><dd><p>nitric oxides concentration (parts per 10 million)</p>
</dd>
<dt><code>rm</code></dt><dd><p>average number of rooms per dwelling</p>
</dd>
<dt><code>age</code></dt><dd><p>proportion of owner-occupied units built prior to 1940</p>
</dd>
<dt><code>dis</code></dt><dd><p>weighted distances to five Boston employment centres</p>
</dd>
<dt><code>rad</code></dt><dd><p>index of accessibility to radial highways</p>
</dd>
<dt><code>tax</code></dt><dd><p>full-value property-tax rate per $10,000</p>
</dd>
<dt><code>ptratio</code></dt><dd><p>pupil-teacher ratio by town</p>
</dd>
<dt><code>black</code></dt><dd><p>1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town</p>
</dd>
<dt><code>lstat</code></dt><dd><p>percentage of lower status of the population</p>
</dd>
<dt><code>medv</code></dt><dd><p>Median value of owner-occupied homes in $1000's</p>
</dd>
</dl>



<h3>Source</h3>

<p>This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.
</p>
<p>Creator:  Harrison, D. and Rubinfeld, D.L. 'Hedonic prices and the demand for clean air', J. Environ. Economics &amp; Management, vol.5, 81-102, 1978.
</p>


<h3>References</h3>

<p>https://archive.ics.uci.edu/ml/datasets/Housing
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Boston)

X = Boston[, -ncol(Boston)]

y = Boston[, ncol(Boston)]
</code></pre>

<hr>
<h2 id='class_folds'>stratified folds (in classification)                      [ detailed information about class_folds in the FeatureSelection package ]</h2><span id='topic+class_folds'></span>

<h3>Description</h3>

<p>this function creates stratified folds in binary and multiclass classification
</p>


<h3>Usage</h3>

<pre><code class='language-R'>class_folds(folds, RESP)
</code></pre>

<hr>
<h2 id='distMat.KernelKnn'>kernel k-nearest-neighbors using a distance matrix</h2><span id='topic+distMat.KernelKnn'></span>

<h3>Description</h3>

<p>kernel k-nearest-neighbors using a distance matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distMat.KernelKnn(
  DIST_mat,
  TEST_indices = NULL,
  y,
  k = 5,
  h = 1,
  weights_function = NULL,
  regression = F,
  threads = 1,
  extrema = F,
  Levels = NULL,
  minimize = T
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distMat.KernelKnn_+3A_dist_mat">DIST_mat</code></td>
<td>
<p>a distance matrix (square matrix) having a <em>diagonal</em> filled with either zero's (<em>0</em>) or NA's (<em>missing values</em>)</p>
</td></tr>
<tr><td><code id="distMat.KernelKnn_+3A_test_indices">TEST_indices</code></td>
<td>
<p>a numeric vector specifying the indices of the test data in the distance matrix (row-wise or column-wise). If the parameter equals NULL then no test data is included in the distance matrix</p>
</td></tr>
<tr><td><code id="distMat.KernelKnn_+3A_y">y</code></td>
<td>
<p>a numeric vector (in classification the labels must be numeric from 1:Inf). It is assumed that if the <em>TEST_indices</em> is not NULL then the length of <em>y</em> equals to the rows of the train data <em>( nrow(DIST_mat) - length(TEST_indices) )</em>, otherwise  <em>length(y) == nrow(DIST_mat)</em>.</p>
</td></tr>
<tr><td><code id="distMat.KernelKnn_+3A_k">k</code></td>
<td>
<p>an integer specifying the k-nearest-neighbors</p>
</td></tr>
<tr><td><code id="distMat.KernelKnn_+3A_h">h</code></td>
<td>
<p>the bandwidth (applicable if the weights_function is not NULL, defaults to 1.0)</p>
</td></tr>
<tr><td><code id="distMat.KernelKnn_+3A_weights_function">weights_function</code></td>
<td>
<p>there are various ways of specifying the kernel function. See the details section.</p>
</td></tr>
<tr><td><code id="distMat.KernelKnn_+3A_regression">regression</code></td>
<td>
<p>a boolean (TRUE,FALSE) specifying if regression or classification should be performed</p>
</td></tr>
<tr><td><code id="distMat.KernelKnn_+3A_threads">threads</code></td>
<td>
<p>the number of cores to be used in parallel (openmp will be employed)</p>
</td></tr>
<tr><td><code id="distMat.KernelKnn_+3A_extrema">extrema</code></td>
<td>
<p>if TRUE then the minimum and maximum values from the k-nearest-neighbors will be removed (can be thought as outlier removal)</p>
</td></tr>
<tr><td><code id="distMat.KernelKnn_+3A_levels">Levels</code></td>
<td>
<p>a numeric vector. In case of classification the unique levels of the response variable are necessary</p>
</td></tr>
<tr><td><code id="distMat.KernelKnn_+3A_minimize">minimize</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then lower values will be considered as relevant for the k-nearest search, otherwise higher values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes a distance matrix (square matrix where the diagonal is filled with <em>0</em> or <em>NA</em>) as input. If the <em>TEST_indices</em> parameter is NULL then the predictions for the train data will be returned, whereas if the <em>TEST_indices</em> parameter is not NULL then the predictions for the test data will be returned.
There are three possible ways to specify the weights function, 1st option : if the weights_function is NULL then a simple k-nearest-neighbor is performed. 2nd option : the weights_function is one of 'uniform', 'triangular', 'epanechnikov', 'biweight', 'triweight', 'tricube', 'gaussian', 'cosine', 'logistic', 'gaussianSimple', 'silverman', 'inverse', 'exponential'. The 2nd option can be extended by combining kernels from the existing ones (adding or multiplying). For instance, I can multiply the tricube with the gaussian kernel by giving 'tricube_gaussian_MULT' or I can add the previously mentioned kernels by giving 'tricube_gaussian_ADD'. 3rd option : a user defined kernel function
</p>


<h3>Value</h3>

<p>a vector (if regression is TRUE), or a data frame with class probabilities (if regression is FALSE)
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Boston)

X = Boston[, -ncol(Boston)]
y = Boston[, ncol(Boston)]

dist_obj = dist(X)

dist_mat = as.matrix(dist_obj)

out = distMat.KernelKnn(dist_mat, TEST_indices = NULL, y, k = 5, regression = TRUE)

</code></pre>

<hr>
<h2 id='distMat.knn.index.dist'>indices and distances of k-nearest-neighbors using a distance matrix</h2><span id='topic+distMat.knn.index.dist'></span>

<h3>Description</h3>

<p>indices and distances of k-nearest-neighbors using a distance matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distMat.knn.index.dist(
  DIST_mat,
  TEST_indices = NULL,
  k = 5,
  threads = 1,
  minimize = T
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distMat.knn.index.dist_+3A_dist_mat">DIST_mat</code></td>
<td>
<p>a distance matrix (square matrix) having a diagonal filled with either zero's (<em>0</em>) or NA's (<em>missing values</em>)</p>
</td></tr>
<tr><td><code id="distMat.knn.index.dist_+3A_test_indices">TEST_indices</code></td>
<td>
<p>a numeric vector specifying the indices of the test data in the distance matrix (row-wise or column-wise). If the parameter equals NULL then no test data is included in the distance matrix</p>
</td></tr>
<tr><td><code id="distMat.knn.index.dist_+3A_k">k</code></td>
<td>
<p>an integer specifying the k-nearest-neighbors</p>
</td></tr>
<tr><td><code id="distMat.knn.index.dist_+3A_threads">threads</code></td>
<td>
<p>the number of cores to be used in parallel (openmp will be employed)</p>
</td></tr>
<tr><td><code id="distMat.knn.index.dist_+3A_minimize">minimize</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then lower values will be considered as relevant for the k-nearest search, otherwise higher values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes a number of arguments and it returns the indices and distances of the k-nearest-neighbors for each observation. If TEST_indices is NULL then the indices-distances for the DIST_mat be returned, whereas if TEST_indices is not NULL then the indices-distances for the test data only will be returned.
</p>


<h3>Value</h3>

<p>a list of length 2. The first sublist returns the indices and the second the distances of the k nearest neighbors for each observation.
If TEST_indices is NULL the number of rows of each sublist equals the number of rows in the DIST_mat data. If TEST_indices is not NULL the number of rows of each sublist equals the length of the input TEST_indices.
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Boston)

X = Boston[, -ncol(Boston)]

dist_obj = dist(X)

dist_mat = as.matrix(dist_obj)

out = distMat.knn.index.dist(dist_mat, TEST_indices = NULL, k = 5)

</code></pre>

<hr>
<h2 id='FUN_kernels'>performs kernel smoothing using a bandwidth. Besides using a kernel there is also the option to combine kernels</h2><span id='topic+FUN_kernels'></span>

<h3>Description</h3>

<p>performs kernel smoothing using a bandwidth. Besides using a kernel there is also the option to combine kernels
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FUN_kernels(kernel, W, h)
</code></pre>

<hr>
<h2 id='func_categorical_preds'>OPTION to convert categorical features TO either numeric [ if levels more than 32] OR to dummy variables [ if levels less than 32 ]</h2><span id='topic+func_categorical_preds'></span>

<h3>Description</h3>

<p>OPTION to convert categorical features TO either numeric [ if levels more than 32] OR to dummy variables [ if levels less than 32 ]
</p>


<h3>Usage</h3>

<pre><code class='language-R'>func_categorical_preds(prepr_categ)
</code></pre>

<hr>
<h2 id='func_shuffle'>shuffle data</h2><span id='topic+func_shuffle'></span>

<h3>Description</h3>

<p>this function shuffles the items of a vector
</p>


<h3>Usage</h3>

<pre><code class='language-R'>func_shuffle(vec, times = 10)
</code></pre>

<hr>
<h2 id='func_tbl'>this function returns a table of probabilities for each label</h2><span id='topic+func_tbl'></span>

<h3>Description</h3>

<p>this function returns a table of probabilities for each label
</p>


<h3>Usage</h3>

<pre><code class='language-R'>func_tbl(DF, W, labels)
</code></pre>

<hr>
<h2 id='func_tbl_dist'>this function returns the probabilities in case of classification</h2><span id='topic+func_tbl_dist'></span>

<h3>Description</h3>

<p>this function returns the probabilities in case of classification
</p>


<h3>Usage</h3>

<pre><code class='language-R'>func_tbl_dist(DF, Levels)
</code></pre>

<hr>
<h2 id='FUNCTION_weights'>this function is used as a kernel-function-identifier [ takes the distances and a weights-kernel (in form of a function) and returns weights ]</h2><span id='topic+FUNCTION_weights'></span>

<h3>Description</h3>

<p>this function is used as a kernel-function-identifier [ takes the distances and a weights-kernel (in form of a function) and returns weights ]
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FUNCTION_weights(W_dist_matrix, weights_function, eps = 1e-06)
</code></pre>

<hr>
<h2 id='ionosphere'>
Johns Hopkins University Ionosphere database   (binary classification)
</h2><span id='topic+ionosphere'></span>

<h3>Description</h3>

<p>This radar data was collected by a system in Goose Bay, Labrador.
This radar data was collected by a system in Goose Bay, Labrador. This system consists of a phased array of 16 high-frequency
antennas with a total transmitted power on the order of 6.4 kilowatts. See the paper for more details.  The targets were free
electrons in the ionosphere. &quot;Good&quot; radar returns are those showing evidence of some type of structure in the ionosphere.
&quot;Bad&quot; returns are those that do not; their signals pass through the ionosphere. Received signals were processed using an autocorrelation
function whose arguments are the time of a pulse and the pulse number. There were 17 pulse numbers for the Goose Bay system.
Instances in this databse are described by 2 attributes per pulse number, corresponding to the complex values returned by the
function resulting from the complex electromagnetic signal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ionosphere)</code></pre>


<h3>Format</h3>

<p>A data frame with 351 Instances and 35 attributes (including the class attribute, &quot;class&quot;)
</p>


<h3>Details</h3>

<p>Sigillito, V. G., Wing, S. P., Hutton, L. V., Baker, K. B. (1989). Classification of radar returns from the ionosphere using neural networks.
Johns Hopkins APL Technical Digest, 10, 262-266.
</p>
<p>They investigated using backprop and the perceptron training algorithm on this database. Using the first 200 instances for training, which
were carefully split almost 50 percent positive and 50 percent negative, they found that a &quot;linear&quot; perceptron attained 90.7 percent,
a &quot;non-linear&quot; perceptron attained 92 percent, and backprop an average of over 96 percent accuracy on the remaining 150 test instances,
consisting of 123 &quot;good&quot; and only 24 &quot;bad&quot; instances.  (There was a counting error or some mistake somewhere; there are a total of 351 rather
than 350 instances in this domain.) Accuracy on &quot;good&quot; instances was much higher than for &quot;bad&quot; instances.  Backprop was tested with several
different numbers of hidden units (in [0,15]) and incremental results were also reported (corresponding to how well the different variants of
backprop did after a periodic number of epochs). David Aha (aha@ics.uci.edu) briefly investigated this database. He found that nearest neighbor
attains an accuracy of 92.1 percent, that Ross Quinlan's C4 algorithm attains 94.0 percent (no windowing), and that IB3 (Aha &amp; Kibler, IJCAI-1989)
attained 96.7 percent (parameter settings: 70 percent and 80 percent for acceptance and dropping respectively).
</p>


<h3>Source</h3>

<p>Donor: Vince Sigillito (vgs@aplcen.apl.jhu.edu)
</p>
<p>Date: 1989
</p>
<p>Source: Space Physics Group
</p>
<p>Applied Physics Laboratory
</p>
<p>Johns Hopkins University
</p>
<p>Johns Hopkins Road
</p>
<p>Laurel, MD 20723
</p>


<h3>References</h3>

<p>https://archive.ics.uci.edu/ml/datasets/Ionosphere
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(ionosphere)

X = ionosphere[, -ncol(ionosphere)]

y = ionosphere[, ncol(ionosphere)]
</code></pre>

<hr>
<h2 id='KernelKnn'>kernel k-nearest-neighbors</h2><span id='topic+KernelKnn'></span>

<h3>Description</h3>

<p>This function utilizes kernel k nearest neighbors to predict new observations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KernelKnn(
  data,
  TEST_data = NULL,
  y,
  k = 5,
  h = 1,
  method = "euclidean",
  weights_function = NULL,
  regression = F,
  transf_categ_cols = F,
  threads = 1,
  extrema = F,
  Levels = NULL,
  p = k
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KernelKnn_+3A_data">data</code></td>
<td>
<p>a data frame or matrix</p>
</td></tr>
<tr><td><code id="KernelKnn_+3A_test_data">TEST_data</code></td>
<td>
<p>a data frame or matrix (it can be also NULL)</p>
</td></tr>
<tr><td><code id="KernelKnn_+3A_y">y</code></td>
<td>
<p>a numeric vector (in classification the labels must be numeric from 1:Inf)</p>
</td></tr>
<tr><td><code id="KernelKnn_+3A_k">k</code></td>
<td>
<p>an integer specifying the k-nearest-neighbors</p>
</td></tr>
<tr><td><code id="KernelKnn_+3A_h">h</code></td>
<td>
<p>the bandwidth (applicable if the weights_function is not NULL, defaults to 1.0)</p>
</td></tr>
<tr><td><code id="KernelKnn_+3A_method">method</code></td>
<td>
<p>a string specifying the method. Valid methods are 'euclidean', 'manhattan', 'chebyshev', 'canberra', 'braycurtis', 'pearson_correlation', 'simple_matching_coefficient', 'minkowski' (by default the order 'p' of the minkowski parameter equals k), 'hamming', 'mahalanobis', 'jaccard_coefficient', 'Rao_coefficient'</p>
</td></tr>
<tr><td><code id="KernelKnn_+3A_weights_function">weights_function</code></td>
<td>
<p>there are various ways of specifying the kernel function. See the details section.</p>
</td></tr>
<tr><td><code id="KernelKnn_+3A_regression">regression</code></td>
<td>
<p>a boolean (TRUE,FALSE) specifying if regression or classification should be performed</p>
</td></tr>
<tr><td><code id="KernelKnn_+3A_transf_categ_cols">transf_categ_cols</code></td>
<td>
<p>a boolean (TRUE, FALSE) specifying if the categorical columns should be converted to numeric or to dummy variables</p>
</td></tr>
<tr><td><code id="KernelKnn_+3A_threads">threads</code></td>
<td>
<p>the number of cores to be used in parallel (openmp will be employed)</p>
</td></tr>
<tr><td><code id="KernelKnn_+3A_extrema">extrema</code></td>
<td>
<p>if TRUE then the minimum and maximum values from the k-nearest-neighbors will be removed (can be thought as outlier removal)</p>
</td></tr>
<tr><td><code id="KernelKnn_+3A_levels">Levels</code></td>
<td>
<p>a numeric vector. In case of classification the unique levels of the response variable are necessary</p>
</td></tr>
<tr><td><code id="KernelKnn_+3A_p">p</code></td>
<td>
<p>a numeric value specifying the 'minkowski' order, i.e. if 'method' is set to 'minkowski'. This parameter defaults to 'k'</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes a number of arguments and it returns the predicted values. If TEST_data is NULL then the predictions for the train data will be returned, whereas if TEST_data is not NULL then the predictions for the TEST_data will be returned.
There are three possible ways to specify the weights function, 1st option : if the weights_function is NULL then a simple k-nearest-neighbor is performed. 2nd option : the weights_function is one of 'uniform', 'triangular', 'epanechnikov', 'biweight', 'triweight', 'tricube', 'gaussian', 'cosine', 'logistic', 'gaussianSimple', 'silverman', 'inverse', 'exponential'. The 2nd option can be extended by combining kernels from the existing ones (adding or multiplying). For instance, I can multiply the tricube with the gaussian kernel by giving 'tricube_gaussian_MULT' or I can add the previously mentioned kernels by giving 'tricube_gaussian_ADD'. 3rd option : a user defined kernel function
</p>


<h3>Value</h3>

<p>a vector (if regression is TRUE), or a data frame with class probabilities (if regression is FALSE)
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Boston)

X = Boston[, -ncol(Boston)]
y = Boston[, ncol(Boston)]

out = KernelKnn(X, TEST_data = NULL, y, k = 5, method = 'euclidean', regression = TRUE)

</code></pre>

<hr>
<h2 id='KernelKnnCV'>kernel-k-nearest-neighbors using cross-validation</h2><span id='topic+KernelKnnCV'></span>

<h3>Description</h3>

<p>This function performs kernel k nearest neighbors regression and classification using cross validation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KernelKnnCV(
  data,
  y,
  k = 5,
  folds = 5,
  h = 1,
  method = "euclidean",
  weights_function = NULL,
  regression = F,
  transf_categ_cols = F,
  threads = 1,
  extrema = F,
  Levels = NULL,
  seed_num = 1,
  p = k
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KernelKnnCV_+3A_data">data</code></td>
<td>
<p>a data frame or matrix</p>
</td></tr>
<tr><td><code id="KernelKnnCV_+3A_y">y</code></td>
<td>
<p>a numeric vector (in classification the labels must be numeric from 1:Inf)</p>
</td></tr>
<tr><td><code id="KernelKnnCV_+3A_k">k</code></td>
<td>
<p>an integer specifying the k-nearest-neighbors</p>
</td></tr>
<tr><td><code id="KernelKnnCV_+3A_folds">folds</code></td>
<td>
<p>the number of cross validation folds (must be greater than 1)</p>
</td></tr>
<tr><td><code id="KernelKnnCV_+3A_h">h</code></td>
<td>
<p>the bandwidth (applicable if the weights_function is not NULL, defaults to 1.0)</p>
</td></tr>
<tr><td><code id="KernelKnnCV_+3A_method">method</code></td>
<td>
<p>a string specifying the method. Valid methods are 'euclidean', 'manhattan', 'chebyshev', 'canberra', 'braycurtis', 'pearson_correlation', 'simple_matching_coefficient', 'minkowski' (by default the order 'p' of the minkowski parameter equals k), 'hamming', 'mahalanobis', 'jaccard_coefficient', 'Rao_coefficient'</p>
</td></tr>
<tr><td><code id="KernelKnnCV_+3A_weights_function">weights_function</code></td>
<td>
<p>there are various ways of specifying the kernel function. See the details section.</p>
</td></tr>
<tr><td><code id="KernelKnnCV_+3A_regression">regression</code></td>
<td>
<p>a boolean (TRUE,FALSE) specifying if regression or classification should be performed</p>
</td></tr>
<tr><td><code id="KernelKnnCV_+3A_transf_categ_cols">transf_categ_cols</code></td>
<td>
<p>a boolean (TRUE, FALSE) specifying if the categorical columns should be converted to numeric or to dummy variables</p>
</td></tr>
<tr><td><code id="KernelKnnCV_+3A_threads">threads</code></td>
<td>
<p>the number of cores to be used in parallel (openmp will be employed)</p>
</td></tr>
<tr><td><code id="KernelKnnCV_+3A_extrema">extrema</code></td>
<td>
<p>if TRUE then the minimum and maximum values from the k-nearest-neighbors will be removed (can be thought as outlier removal)</p>
</td></tr>
<tr><td><code id="KernelKnnCV_+3A_levels">Levels</code></td>
<td>
<p>a numeric vector. In case of classification the unique levels of the response variable are necessary</p>
</td></tr>
<tr><td><code id="KernelKnnCV_+3A_seed_num">seed_num</code></td>
<td>
<p>a numeric value specifying the seed of the random number generator</p>
</td></tr>
<tr><td><code id="KernelKnnCV_+3A_p">p</code></td>
<td>
<p>a numeric value specifying the 'minkowski' order, i.e. if 'method' is set to 'minkowski'. This parameter defaults to 'k'</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes a number of arguments (including the number of cross-validation-folds) and it returns predicted values and indices for each fold.
There are three possible ways to specify the weights function, 1st option : if the weights_function is NULL then a simple k-nearest-neighbor is performed. 2nd option : the weights_function is one of 'uniform', 'triangular', 'epanechnikov', 'biweight', 'triweight', 'tricube', 'gaussian', 'cosine', 'logistic', 'gaussianSimple', 'silverman', 'inverse', 'exponential'. The 2nd option can be extended by combining kernels from the existing ones (adding or multiplying). For instance, I can multiply the tricube with the gaussian kernel by giving 'tricube_gaussian_MULT' or I can add the previously mentioned kernels by giving 'tricube_gaussian_ADD'. 3rd option : a user defined kernel function
</p>


<h3>Value</h3>

<p>a list of length 2. The first sublist is a list of predictions (the length of the list equals the number of the folds). The second sublist is a list with the indices for each fold.
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
data(ionosphere)

X = ionosphere[, -c(2, ncol(ionosphere))]
y = as.numeric(ionosphere[, ncol(ionosphere)])

out = KernelKnnCV(X, y, k = 5, folds = 3, regression = FALSE, Levels = unique(y))

## End(Not run)
</code></pre>

<hr>
<h2 id='knn.index.dist'>indices and distances of k-nearest-neighbors</h2><span id='topic+knn.index.dist'></span>

<h3>Description</h3>

<p>This function returns the k nearest indices and distances of each observation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>knn.index.dist(
  data,
  TEST_data = NULL,
  k = 5,
  method = "euclidean",
  transf_categ_cols = F,
  threads = 1,
  p = k
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="knn.index.dist_+3A_data">data</code></td>
<td>
<p>a data.frame or matrix</p>
</td></tr>
<tr><td><code id="knn.index.dist_+3A_test_data">TEST_data</code></td>
<td>
<p>a data.frame or matrix (it can be also NULL)</p>
</td></tr>
<tr><td><code id="knn.index.dist_+3A_k">k</code></td>
<td>
<p>an integer specifying the k-nearest-neighbors</p>
</td></tr>
<tr><td><code id="knn.index.dist_+3A_method">method</code></td>
<td>
<p>a string specifying the method. Valid methods are 'euclidean', 'manhattan', 'chebyshev', 'canberra', 'braycurtis', 'pearson_correlation', 'simple_matching_coefficient', 'minkowski' (by default the order 'p' of the minkowski parameter equals k), 'hamming', 'mahalanobis', 'jaccard_coefficient', 'Rao_coefficient'</p>
</td></tr>
<tr><td><code id="knn.index.dist_+3A_transf_categ_cols">transf_categ_cols</code></td>
<td>
<p>a boolean (TRUE, FALSE) specifying if the categorical columns should be converted to numeric or to dummy variables</p>
</td></tr>
<tr><td><code id="knn.index.dist_+3A_threads">threads</code></td>
<td>
<p>the number of cores to be used in parallel (openmp will be employed)</p>
</td></tr>
<tr><td><code id="knn.index.dist_+3A_p">p</code></td>
<td>
<p>a numeric value specifying the 'minkowski' order, i.e. if 'method' is set to 'minkowski'. This parameter defaults to 'k'</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes a number of arguments and it returns the indices and distances of the k-nearest-neighbors for each observation. If TEST_data is NULL then the indices-distances for the train data will be returned, whereas if TEST_data is not NULL then the indices-distances for the TEST_data will be returned.
</p>


<h3>Value</h3>

<p>a list of length 2. The first sublist returns the indices and the second the distances of the k nearest neighbors for each observation.
If TEST_data is NULL the number of rows of each sublist equals the number of rows in the train data. If TEST_data is not NULL the number of rows of each sublist equals the number of rows in the TEST data.
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Boston)

X = Boston[, -ncol(Boston)]

out = knn.index.dist(X, TEST_data = NULL, k = 4, method = 'euclidean', threads = 1)

</code></pre>

<hr>
<h2 id='normalized'>this function normalizes the data</h2><span id='topic+normalized'></span>

<h3>Description</h3>

<p>this function normalizes the data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalized(x)
</code></pre>

<hr>
<h2 id='regr_folds'>create folds (in regression)                                           [ detailed information about class_folds in the FeatureSelection package ]</h2><span id='topic+regr_folds'></span>

<h3>Description</h3>

<p>this function creates both stratified and non-stratified folds in regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regr_folds(folds, RESP)
</code></pre>

<hr>
<h2 id='switch.ops'>Arithmetic operations on lists</h2><span id='topic+switch.ops'></span>

<h3>Description</h3>

<p>Arithmetic operations on lists
</p>


<h3>Usage</h3>

<pre><code class='language-R'>switch.ops(LST, MODE = "ADD")
</code></pre>


<h3>References</h3>

<p>https://www.cs.toronto.edu/~duvenaud/cookbook/
</p>
<p>https://raw.githubusercontent.com/duvenaud/phd-thesis/master/kernels.pdf
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
