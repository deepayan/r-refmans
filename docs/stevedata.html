<!DOCTYPE html><html><head><title>Help for package stevedata</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {stevedata}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#af_crime93'><p>Statewide Crime Data (1993)</p></a></li>
<li><a href='#aluminum_premiums'><p>LME Aluminum Premiums Data</p></a></li>
<li><a href='#anes_partytherms'><p>Major Party (Democrat, Republican) Thermometer Index Data (1978-2012)</p></a></li>
<li><a href='#anes_prochoice'><p>Abortion Attitudes (ANES, 2012)</p></a></li>
<li><a href='#anes_vote84'><p>Simple Data for a Simple Model of Individual Voter Turnout (ANES, 1984)</p></a></li>
<li><a href='#Arca'><p>NYSE Arca Steel Index data, 2017–present</p></a></li>
<li><a href='#arcticseaice'><p>Arctic Sea Ice Extent Data, 1901-2015</p></a></li>
<li><a href='#arg_tariff'><p>Simple Mean Tariff Rate for Argentina</p></a></li>
<li><a href='#asn_stats'><p>Aviation Safety Network Statistics, 1942-2019</p></a></li>
<li><a href='#CFT15'><p>Randomization Inference in the Regression Discontinuity Design: An Application to Party Advantages in the U.S. Senate</p></a></li>
<li><a href='#clemson_temps'><p>Daily Clemson Temperature Data</p></a></li>
<li><a href='#co2emissions'><p>Carbon Dioxide Emissions Data</p></a></li>
<li><a href='#coffee_imports'><p>Coffee Imports for Select Importing Countries</p></a></li>
<li><a href='#coffee_price'><p>The Primary Commodity Price for Coffee (Arabica, Robustas)</p></a></li>
<li><a href='#commodity_prices'><p>Select World Bank Commodity Price Data (Monthly)</p></a></li>
<li><a href='#CP77'><p>Education Expenditure Data (Chatterjee and Price, 1977)</p></a></li>
<li><a href='#Datasaurus'><p>The Datasaurus Dozen</p></a></li>
<li><a href='#Dee04'><p>Are There Civics Returns to Education?</p></a></li>
<li><a href='#DJIA'><p>Dow Jones Industrial Average, 1885-Present</p></a></li>
<li><a href='#DST'><p>Casualties/Fatalities in the U.S. for Drunk-Driving, Suicide, and Terrorism</p></a></li>
<li><a href='#eight_schools'><p>The Effect of Special Preparation on SAT-V Scores in Eight Randomized Experiments</p></a></li>
<li><a href='#election_turnout'><p>State-Level Education and Voter Turnout in 2016</p></a></li>
<li><a href='#eq_passengercars'><p>Export Quality Data for Passenger Cars, 1963-2014</p></a></li>
<li><a href='#ESS10NO'><p>Norwegian Attitudes toward European Integration (2021-2022)</p></a></li>
<li><a href='#ESS9GB'><p>British Attitudes Toward Immigration (2018-19)</p></a></li>
<li><a href='#ESSBE5'><p>Trust in the Police in Belgium (European Social Survey, Round 5)</p></a></li>
<li><a href='#eurostat_codes'><p>Eurostat Country Codes</p></a></li>
<li><a href='#eustates'><p>EU Member States (Current as of 2019)</p></a></li>
<li><a href='#fakeAPI'><p>Hypothetical (Fake) Data on Academic Performance</p></a></li>
<li><a href='#fakeHappiness'><p>Fake Data on Happiness</p></a></li>
<li><a href='#fakeLogit'><p>Fake Data for a Logistic Regression</p></a></li>
<li><a href='#fakeTSCS'><p>Fake Data for a Time-Series Cross-Section</p></a></li>
<li><a href='#fakeTSD'><p>Fake Data for a Time-Series</p></a></li>
<li><a href='#ghp100k'><p>Gun Homicide Rate per 100,000 People, by Country</p></a></li>
<li><a href='#GHR04'><p>Comparative Public Health: The Political Economy of Human Misery and Well-Being</p></a></li>
<li><a href='#gss_abortion'><p>Abortion Opinions in the General Social Survey</p></a></li>
<li><a href='#gss_spending'><p>Attitudes Toward National Spending in the General Social Survey (2018)</p></a></li>
<li><a href='#gss_wages'><p>The Gender Pay Gap in the General Social Survey</p></a></li>
<li><a href='#Guber99'><p>School Expenditures and Test Scores for 50 States, 1994-95</p></a></li>
<li><a href='#illiteracy30'><p>Illiteracy in the Population 10 Years Old and Over, 1930</p></a></li>
<li><a href='#inglehart03'><p>&quot;How Solid is Mass Support for Democracy&mdash;And How Can We Measure It?&quot;</p></a></li>
<li><a href='#Lipset59'><p>Democracy and Economic Development (Around) 1949-50</p></a></li>
<li><a href='#LOTI'><p>Land-Ocean Temperature Index, 1880-2022</p></a></li>
<li><a href='#LTPT'><p>Long-Term Price Trends for Computers, TVs, and Related Items</p></a></li>
<li><a href='#LTWT'><p>&quot;Let Them Watch TV&quot;</p></a></li>
<li><a href='#min_wage'><p>History of Federal Minimum Wage Rates Under the Fair Labor Standards Act, 1938-2009</p></a></li>
<li><a href='#mm_mlda'><p>Minimum Legal Drinking Age Fatalities Data</p></a></li>
<li><a href='#mm_nhis'><p>Data from the 2009 National Health Interview Survey (NHIS)</p></a></li>
<li><a href='#mm_randhie'><p>Data from the RAND Health Insurance Experiment (HIE)</p></a></li>
<li><a href='#mvprod'><p>Motor Vehicle Production by Country, 1950-2019</p></a></li>
<li><a href='#nesarc_drinkspd'><p>The Usual Daily Drinking Habits of Americans (NESARC, 2001-2)</p></a></li>
<li><a href='#Newhouse77'><p>Medical-Care Expenditure: A Cross-National Survey (Newhouse, 1977)</p></a></li>
<li><a href='#ODGI'><p>Ozone Depleting Gas Index Data, 1992-2022</p></a></li>
<li><a href='#OODTPT'><p>Data for &quot;Optimal Obfuscation: Democracy and Trade Policy Transparency&quot;</p></a></li>
<li><a href='#PPGE'><p>Partisan Politics in the Global Economy</p></a></li>
<li><a href='#PRDEG'><p>Property Rights, Democracy, and Economic Growth</p></a></li>
<li><a href='#Presidents'><p>U.S. Presidents and Their Terms in Office</p></a></li>
<li><a href='#pwt_sample'><p>Penn World Table (10.0) Macroeconomic Data for Select Countries, 1950-2019</p></a></li>
<li><a href='#quartets'><p>Anscombe's (1973) Quartets</p></a></li>
<li><a href='#recessions'><p>United States Recessions, 1855-present</p></a></li>
<li><a href='#SBCD'><p>Systemic Banking Crises Database II</p></a></li>
<li><a href='#scb_regions'><p>Region Codes in the Central Bureau of Statistics (&quot;Statistiska centralbyrån&quot;) in Sweden</p></a></li>
<li><a href='#SCP16'><p>South Carolina County GOP/Democratic Primary Data, 2016</p></a></li>
<li><a href='#sealevels'><p>Global Average Absolute Sea Level Change, 1880–2015</p></a></li>
<li><a href='#so2concentrations'><p>Sulfur Dioxide Emissions, 1980-2020</p></a></li>
<li><a href='#steves_clothes'><p>Steve's (Professional) Clothes, as of March 20, 2022</p></a></li>
<li><a href='#sugar_price'><p>IMF Primary Commodity Price Data for Sugar</p></a></li>
<li><a href='#sweden_counties'><p>The Counties of Sweden</p></a></li>
<li><a href='#thatcher_approval'><p>Margaret Thatcher Satisfaction Ratings, 1980-1990</p></a></li>
<li><a href='#therms'><p>Thermometer Ratings for Donald Trump and Barack Obama</p></a></li>
<li><a href='#turnips'><p>Turnip prices in Animal Crossing (New Horizons)</p></a></li>
<li><a href='#TV16'><p>The Individual Correlates of the Trump Vote in 2016</p></a></li>
<li><a href='#ukg_eeri'><p>United Kingdom Effective Exchange Rate Index Data, 1990-2022</p></a></li>
<li><a href='#uniondensity'><p>Cross-National Rates of Trade Union Density</p></a></li>
<li><a href='#usa_chn_gdp_forecasts'><p>United States-China GDP and GDP Forecasts, 1960-2050</p></a></li>
<li><a href='#usa_computers'><p>Percentage of U.S. Households with Computer Access, by Year</p></a></li>
<li><a href='#usa_migration'><p>U.S. Inbound/Outbound Migration Data, 1990-2017</p></a></li>
<li><a href='#usa_states'><p>State Abbreviations, Names, and Regions/Divisions</p></a></li>
<li><a href='#usa_tradegdp'><p>U.S. Trade and GDP, 1790-2018</p></a></li>
<li><a href='#voteincome'><p>Sample Turnout and Demographic Data from the 2000 Current Population Survey</p></a></li>
<li><a href='#wbd_example'><p>A Simple Panel drawn from World Bank Open Data</p></a></li>
<li><a href='#wvs_ccodes'><p>Syncing Word Values Survey Country Codes with CoW Codes</p></a></li>
<li><a href='#wvs_immig'><p>Attitudes about Immigration in the World Values Survey</p></a></li>
<li><a href='#wvs_justifbribe'><p>Attitudes about the Justifiability of Bribe-Taking in the World Values Survey</p></a></li>
<li><a href='#wvs_usa_abortion'><p>Attitudes on the Justifiability of Abortion in the United States (World Values Survey, 1982-2011)</p></a></li>
<li><a href='#wvs_usa_educat'><p>Education Categories for the United States in the World Values Survey</p></a></li>
<li><a href='#wvs_usa_regions'><p>Region Categories for the United States in the World Values Survey</p></a></li>
<li><a href='#yugo_sales'><p>Yugo Sales in the United States, 1985-1992</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Steve's Toy Data for Teaching About a Variety of Methodological,
Social, and Political Topics</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Steve Miller &lt;steven.v.miller@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>This is a collection of various kinds of data with broad uses for teaching. 
    My students, and academics like me who teach the same topics I teach, should find 
    this useful if their teaching workflow is also built around the R programming 
    language. The applications are multiple but mostly cluster on topics of statistical
    methodology, international relations, and political economy.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>LazyDataCompression:</td>
<td>xz</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://svmiller.com/stevedata/">http://svmiller.com/stevedata/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/svmiller/stevedata/issues/">https://github.com/svmiller/stevedata/issues/</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, tibble, tools, testthat</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-25 11:04:29 UTC; steve</td>
</tr>
<tr>
<td>Author:</td>
<td>Steve Miller <a href="https://orcid.org/0000-0003-4072-6263"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-25 11:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='af_crime93'>Statewide Crime Data (1993)</h2><span id='topic+af_crime93'></span>

<h3>Description</h3>

<p>These data are in Table 9.1 of the 3rd edition of Agresti and Finlay's
*Statistical Methods for the Social Sciences*. The data are from
*Statistical Abstract of the United States* and most variables were measured in 1993.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>af_crime93
</code></pre>


<h3>Format</h3>

<p>A data frame with 51 observations on the following 8 variables.
</p>

<dl>
<dt><code>state</code></dt><dd><p>a character vector for the state</p>
</dd>
<dt><code>violent</code></dt><dd><p>a numeric vector for the violent crime rate (per 100,000 people in population)</p>
</dd>
<dt><code>murder</code></dt><dd><p>a numeric vector for the murder rate (per 100,000 people in population)</p>
</dd>
<dt><code>poverty</code></dt><dd><p>a numeric vector for the  percent with income below the poverty level</p>
</dd>
<dt><code>single</code></dt><dd><p>a numeric vector for the percent of families headed by a single parent</p>
</dd>
<dt><code>metro</code></dt><dd><p>a numeric vector for the percent of population in metropolitan areas</p>
</dd>
<dt><code>white</code></dt><dd><p>a numeric vector for the percentage of the state that is white</p>
</dd>
<dt><code>highschool</code></dt><dd><p>a numeric vector for the percent of state that graduated from high school</p>
</dd>
</dl>



<h3>Details</h3>

<p>The data are from Statistical Abstract of the United States and most variables were measured in 1993. These data should
result in regressions that would flunk a Breusch-Pagan test for heteroskedasticity.
</p>


<h3>References</h3>

<p>Agresti, Alan and Barbara Finley. 1997. <em>Statistical Methods for the Social Sciences</em>. Prentice Hall. (3rd Edition)
</p>

<hr>
<h2 id='aluminum_premiums'>LME Aluminum Premiums Data</h2><span id='topic+aluminum_premiums'></span>

<h3>Description</h3>

<p>A near daily data set on the price of aluminum premiums (USD/MT) for LME
in the U.S., Western Europe, East Asia, and Southeast Asia. I like these
data as illustrative of some of the shortsightedness of the aluminum tariffs
that Donald Trump announced in March 2018. The tariffs had no discernible
effect on manufacturing employment or earnings, but they created a supply shock
that made aluminum more expensive.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aluminum_premiums
</code></pre>


<h3>Format</h3>

<p>A data frame with 2,812 observations on the following 3 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>a date</p>
</dd>
<dt><code>group</code></dt><dd><p>a factor with levels of <code>East Asia</code>, <code>Southeast Asia</code>,
<code>United States</code>, and <code>Western Europe</code></p>
</dd>
<dt><code>price</code></dt><dd><p>a numeric vector for the price of the LME aluminum premium</p>
</dd>
</dl>



<h3>Details</h3>

<p>LME aluminum premiums (monthly contracts going out to 15 months) work
alongside LME aluminum contracts to allow market participants to hedge the all-in
price and physically deliver or receive premium aluminum warrants in
non-queued LME premium warehouses.
</p>

<hr>
<h2 id='anes_partytherms'>Major Party (Democrat, Republican) Thermometer Index Data (1978-2012)</h2><span id='topic+anes_partytherms'></span>

<h3>Description</h3>

<p>A data set on thermometer ratings for the Democratic party, Republican party,
&quot;both major parties&quot;, and a major party thermometer index from the
American National Election Studies (1978-2012).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>anes_partytherms
</code></pre>


<h3>Format</h3>

<p>A data frame with 33830 observations on the following 19 variables.
</p>

<dl>
<dt><code>year</code></dt><dd><p>the survey year</p>
</dd>
<dt><code>uid</code></dt><dd><p>a unique identifier for each respondent, taken directly from the time-series files for potential merging</p>
</dd>
<dt><code>stateabb</code></dt><dd><p>the two-character abbreviation for the state of residence for the respondent</p>
</dd>
<dt><code>therm_dem</code></dt><dd><p>the respondent's thermometer rating of the Democratic party</p>
</dd>
<dt><code>therm_gop</code></dt><dd><p>the respondent's thermometer rating of the Republican party</p>
</dd>
<dt><code>therm_bmp</code></dt><dd><p>the respondent's thermometer rating of &quot;both major parties&quot;</p>
</dd>
<dt><code>mpti</code></dt><dd><p>the &quot;major party thermometer index&quot; score for the respondent. See details for more.</p>
</dd>
<dt><code>age</code></dt><dd><p>the age of the respondent</p>
</dd>
<dt><code>educat</code></dt><dd><p>the education-level of the respondent. 1 = 8 grades or less.
2 = high school, no diploma. 3 = high school diploma. 4 = high school &quot;plus non-academic training&quot;.
5 = Some college, no degree (includes AA holders). 6 = BA-level degree. 7 = advanced degree, including Bachelor of Laws degrees.</p>
</dd>
<dt><code>urbanism</code></dt><dd><p>1 = central cities. 2 = suburban areas. 3 = rural/small towns/outlying areas.</p>
</dd>
<dt><code>pid7</code></dt><dd><p>1 = Strong Democrat. 2 = Weak Democrat. 3 = Independent, lean Democrat. 4 = Independent. 5 = Independent, lean Republican.
6 = Weak Republican. 7 = Strong Republican</p>
</dd>
<dt><code>incomeperc</code></dt><dd><p>respondent's household income percentile. 1 = 0-16 percentile. 2 = 17-33. 3 = 34-67. 4 = 68-95. 5 = 96-100.</p>
</dd>
<dt><code>race4</code></dt><dd><p>respondent's race-ethnicity summary. 1 = White, non-hispanic. 2 = Black, non-hispanic. 3 = Hispanic. 4 = Other.</p>
</dd>
<dt><code>unemployed</code></dt><dd><p>a binary numeric vector for if the respondent is temporarily unemployed. </p>
</dd>
<dt><code>polint</code></dt><dd><p>the respondent's self-reported interest in public affairs. 1 = Hardly at all. 2 = Only now and then. 3 = Some of the time. 4 = Most of the time.</p>
</dd>
<dt><code>distrust_govt</code></dt><dd><p>the respondent's self-reported (dis)trust in the federal government's ability to do what's right. 1 = Just about always (trust the government).
2 = Most of the time. 3 = Some of the time. 4 = None of the time/never.</p>
</dd>
<dt><code>govt_crooked</code></dt><dd><p>the respondent's assessment of how many government officials are crooked. 1 = Hardly any. 2 = Not many. 3 = Quite a few; quite a lot.</p>
</dd>
<dt><code>govt_waste</code></dt><dd><p>the respondent's assessment of how much the government wastes in tax money. 1 = Not very much. 2 = Some. 3 = A lot.</p>
</dd>
<dt><code>govt_biginterests</code></dt><dd><p>the respondent's assessment of whether the government is run by a few big interests. 0 = Run for the benefit of all people.
1 = Run by a few big interests.</p>
</dd>
</dl>



<h3>Details</h3>

<p>The major party thermometer index is calculated as the thermometer rating for the
Democratic party minus the thermometer rating for the Republican party. 100 is then added to that difference, which is then divided by 2.
Fractional results are rounded to the next highest integer. Also note the coding of the &quot;government distrust&quot; measures. These are reverse-coded
from their original scales.
</p>


<h3>Source</h3>

<p>Data come from ANES's time series file.
</p>

<hr>
<h2 id='anes_prochoice'>Abortion Attitudes (ANES, 2012)</h2><span id='topic+anes_prochoice'></span>

<h3>Description</h3>

<p>A simple data set for in-class illustration about how to estimate and interpret
interactive relationships. The data here are deliberately minimal for that end.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>anes_prochoice
</code></pre>


<h3>Format</h3>

<p>A data frame with 5914 observations on the following 14 variables.
</p>

<dl>
<dt><code>version</code></dt><dd><p>version identifier from ANES</p>
</dd>
<dt><code>caseid</code></dt><dd><p>time-series case identifier from ANES</p>
</dd>
<dt><code>health</code></dt><dd><p>oppose/&quot;NFNO&quot;/favor [0:2] abortion if pregnancy would hurt woman</p>
</dd>
<dt><code>fatal</code></dt><dd><p>oppose/&quot;NFNO&quot;/favor [0:2] abortion if pregnancy would cause woman to die</p>
</dd>
<dt><code>incest</code></dt><dd><p>oppose/&quot;NFNO&quot;/favor [0:2] abortion if pregnancy was caused by incest</p>
</dd>
<dt><code>rape</code></dt><dd><p>oppose/&quot;NFNO&quot;/favor [0:2] abortion if pregnancy was caused by rape</p>
</dd>
<dt><code>bd</code></dt><dd><p>oppose/&quot;NFNO&quot;/favor [0:2] abortion if fetus would be born with serious birth defect</p>
</dd>
<dt><code>fin</code></dt><dd><p>oppose/&quot;NFNO&quot;/favor [0:2] abortion if having child would impose financial hardship</p>
</dd>
<dt><code>sex</code></dt><dd><p>oppose/&quot;NFNO&quot;/favor [0:2] abortion if the child will not be the sex the woman wants</p>
</dd>
<dt><code>choice</code></dt><dd><p>oppose/&quot;NFNO&quot;/favor [0:2] abortion if woman chooses to have one</p>
</dd>
<dt><code>pid</code></dt><dd><p>respondent's partisanship [0:2] (Democrat, Independent, Republican)</p>
</dd>
<dt><code>knowspeaker</code></dt><dd><p>was the respondent able to correctly identify  the Speaker of the House (John Boehner)</p>
</dd>
<dt><code>addchoice</code></dt><dd><p>an additive scale of the abortion scores [0:16]</p>
</dd>
<dt><code>lchoice</code></dt><dd><p>a continuous latent scale of pro-choice scores (from a simple graded response model)</p>
</dd>
</dl>



<h3>Details</h3>

<p>&quot;NFNO&quot; = &quot;Neither Favor Nor Oppose&quot;
</p>


<h3>Source</h3>

<p>Data come from ANES's (2012) time series.
</p>

<hr>
<h2 id='anes_vote84'>Simple Data for a Simple Model of Individual Voter Turnout (ANES, 1984)</h2><span id='topic+anes_vote84'></span>

<h3>Description</h3>

<p>This is a simple data set for estimating a simple model on voter turnout from
the 1984 American National Election Studies (ANES) 1984 time-series.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>anes_vote84
</code></pre>


<h3>Format</h3>

<p>A data frame with 2257 observations on the following 9 variables.
</p>

<dl>
<dt><code>uid</code></dt><dd><p>a unique identifier for the respondent</p>
</dd>
<dt><code>stateabb</code></dt><dd><p>the state where the respondent lives (as an abbreviation)</p>
</dd>
<dt><code>vote</code></dt><dd><p>whether the respondent voted (1 = yes; 0 = no)</p>
</dd>
<dt><code>age</code></dt><dd><p>the age of the respondent</p>
</dd>
<dt><code>educ</code></dt><dd><p>the education-level of the respondent. See details section for more.</p>
</dd>
<dt><code>female</code></dt><dd><p>whether the respondent is a woman (1 = female; 0 = male)</p>
</dd>
<dt><code>south</code></dt><dd><p>does the respondent live in the south (1 = yes; 0 = no)</p>
</dd>
<dt><code>polint</code></dt><dd><p>the political interest of the respondent in the campaigns (-1 = not much interested; 0 = somewhat interested; 1 = very much interested)</p>
</dd>
<dt><code>govrace</code></dt><dd><p>did the respondent's state have a gubernatorial election that same November (1 = yes; 0 = no)</p>
</dd>
</dl>



<h3>Details</h3>

<p>The <code>vote</code> variable is deliberately coded where those with a value of 1 are respondents
who said they voted and the ANES was able to confirm that with voter registration records. There are purportedly
85 responses in this raw variable where the respondent said they voted, but this could not be confirmed from
registration records. Those cases are recorded as <code>NA</code>. The <code>educ</code> variable ranges from 1 (finished 8th
grade or less than that) to 10 (respondent holds an advanced degree). The <code>uid</code> variable is a simple sequence variable
ranging from 1 to 2257 and is calculated on the original 1984 time-series study (May 3, 1999 version)
before other recoding was done. This should allow some reproducibility for an interested user.
</p>


<h3>Source</h3>

<p>Data come from ANES's (1984) time series.
</p>

<hr>
<h2 id='Arca'>NYSE Arca Steel Index data, 2017–present</h2><span id='topic+Arca'></span>

<h3>Description</h3>

<p>Daily data on the NYSE Arca Steel Index. These data are useful for me in
teaching how Trump's 2018 steel tariffs didn't do much good for the steel industry.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Arca
</code></pre>


<h3>Format</h3>

<p>A data frame with 966 observations on the following 6 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>the date</p>
</dd>
<dt><code>close</code></dt><dd><p>the closing price</p>
</dd>
<dt><code>open</code></dt><dd><p>the opening price</p>
</dd>
<dt><code>high</code></dt><dd><p>the daily high in that day's trading</p>
</dd>
<dt><code>low</code></dt><dd><p>the daily low in that day's trading</p>
</dd>
</dl>



<h3>Details</h3>

<p>These data are taken from <code>investing.com</code>.
See: <a href="https://www.investing.com/indices/arca-steel-historical-data">https://www.investing.com/indices/arca-steel-historical-data</a>
</p>

<hr>
<h2 id='arcticseaice'>Arctic Sea Ice Extent Data, 1901-2015</h2><span id='topic+arcticseaice'></span>

<h3>Description</h3>

<p>This data set from Connelly et al. (2017) measures the Arctic sea
ice extent in 10^6 square kilometers. It includes lower bounds and
upper bounds on annual averages.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>arcticseaice
</code></pre>


<h3>Format</h3>

<p>A data frame with 115 observations on the following 4 variables.
</p>

<dl>
<dt><code>year</code></dt><dd><p>the year</p>
</dd>
<dt><code>value</code></dt><dd><p>the annual Arctic sea ice extent (in 10^6 sq km)</p>
</dd>
<dt><code>ub</code></dt><dd><p>The upper bound of the value, provided by Connelly et al.</p>
</dd>
<dt><code>lb</code></dt><dd><p>The lower bound of the value, provided by Connelly et al.</p>
</dd>
</dl>



<h3>Details</h3>

<p>This is for illustration of climate change for my intro students.
Connelly et al. (2017) are in part a methodological paper. The data I present
here are from the &quot;rescaled (unadjusted T)&quot; data in the second sheet from
their replication files.
</p>


<h3>References</h3>

<p>Connolly et al. (2017), &rdquo;Re-calibration of Arctic sea ice extent
datasets using Arctic surface air temperature records&rdquo;.
*Hydrological Sciences Journal* 62(8): 1317&ndash;40.
</p>

<hr>
<h2 id='arg_tariff'>Simple Mean Tariff Rate for Argentina</h2><span id='topic+arg_tariff'></span>

<h3>Description</h3>

<p>Simple mean tariff rate for Argentina, starting in 1980.
The goal is to keep these data current.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>arg_tariff
</code></pre>


<h3>Format</h3>

<p>A data frame with three variables:
</p>

<dl>
<dt><code>country</code></dt><dd><p>country name (Argentina)</p>
</dd>
<dt><code>year</code></dt><dd><p>the year</p>
</dd>
<dt><code>tariffrate</code></dt><dd><p>the simple mean tariff rate for Argentina on all products (as a percentage)</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data come from various sources. World Bank estimates are used for
1980-1984 and 2010-2018, but see also Lora's (2012) report for the Inter-American
Development Bank. The 1980-1984 estimates are actually means for 1980-1 and 1982-4 via
Laird and Nogues' (1989) article in the World Bank Economic Review.
</p>

<hr>
<h2 id='asn_stats'>Aviation Safety Network Statistics, 1942-2019</h2><span id='topic+asn_stats'></span>

<h3>Description</h3>

<p>These are yearly counts on air accidents and fatalities, including measures for
corporate jet accidents and hijackings. The hijackings are of particular interest
to me, at least from a historical terrorism perspective.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>asn_stats
</code></pre>


<h3>Format</h3>

<p>A data frame with 78 observations on the following 7 variables.
</p>

<dl>
<dt><code>year</code></dt><dd><p>numeric vector for the year</p>
</dd>
<dt><code>airacc</code></dt><dd><p>a numeric vector for the number of airliner accidents</p>
</dd>
<dt><code>airfatal</code></dt><dd><p>a numeric vector for the number of fatalities from airliner accidents</p>
</dd>
<dt><code>corpjetacc</code></dt><dd><p>a numeric vector for the number of corporate jet accidents</p>
</dd>
<dt><code>corpjetfatal</code></dt><dd><p>a numeric vector for the number of fatalities from corporate jet accidents</p>
</dd>
<dt><code>hijack</code></dt><dd><p>a numeric vector for the number of hijackings/skyjackings</p>
</dd>
<dt><code>hijackfatal</code></dt><dd><p>a numeric vector for the number of fatalities from hijackings/skyjackings</p>
</dd>
</dl>



<h3>Details</h3>

<p>All fatality estimates exclude ground fatalities. All accidents are hull-loss accidents.
The airliner figures are for those flights with at least 14 passengers.
Check <a href="https://aviation-safety.net/statistics/period/stats.php?cat=H2">https://aviation-safety.net/statistics/period/stats.php?cat=H2</a> for more.
</p>


<h3>Source</h3>

<p>Aviation Safety Network, a service provided by the Flight Safety Foundation.
</p>

<hr>
<h2 id='CFT15'>Randomization Inference in the Regression Discontinuity Design: An Application to Party Advantages in the U.S. Senate</h2><span id='topic+CFT15'></span>

<h3>Description</h3>

<p>This is the replication data for &quot;Randomization Inference in the Regression
Discontinuity Design: An Application to Party Advantages in the U.S. Senate&quot;,
published in 2015 in <em>Journal of Causal Inference</em>. I use these data to
teach about regression discontinuity designs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CFT15
</code></pre>


<h3>Format</h3>

<p>A data frame with 1390 observations on the following 9 variables.
</p>

<dl>
<dt><code>state</code></dt><dd><p>a numeric vector for the state. This is ultimately a categorical variable.</p>
</dd>
<dt><code>year</code></dt><dd><p>a numeric vector for the year of the election.</p>
</dd>
<dt><code>vote</code></dt><dd><p>a numeric vector for the Democratic vote share in the <em>next</em> election (i.e. six years later).</p>
</dd>
<dt><code>margin</code></dt><dd><p>a numeric vector for the Democratic party's margin of victory in the statewide election. This is the running variable, in RDD parlance.</p>
</dd>
<dt><code>class</code></dt><dd><p>a numeric vector for the class to which each Senate seat belongs.</p>
</dd>
<dt><code>termshouse</code></dt><dd><p>a numeric vector for the Democratic candidate's cumulative number of terms previously served in the U.S. House.</p>
</dd>
<dt><code>termssenate</code></dt><dd><p>a numeric vector for the Democratic candidate's cumulative number of terms previously served in the U.S. Senate.</p>
</dd>
<dt><code>population</code></dt><dd><p>a numeric vector for the population of the Senate seat's state.</p>
</dd>
<dt><code>treatment</code></dt><dd><p>a numeric vector that is 1 if <code>margin</code> &gt; 0 and is 0 if <code>margin</code> &lt; 0.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Cattaneo, Matias D. and Brigham R. Frandsen and Rocio Titiunik. 2015. &quot;Randomization Inference in the Regression Discontinuity Design: An Application to Party Advantages in the U.S. Senate&quot;. <em>Journal of Causal Inference</em> 3(1): 1&ndash;24.
</p>


<h3>References</h3>

<p>Cattaneo, Matias D. and Brigham R. Frandsen and Rocio Titiunik. 2015. &quot;Randomization Inference in the Regression Discontinuity Design: An Application to Party Advantages in the U.S. Senate&quot;. <em>Journal of Causal Inference</em> 3(1): 1&ndash;24.
</p>
<p>Calonico, Sebastian and Matias D. Cattaneo and Max H. Farrell and Rocio Titiunik. 2017. &quot;<code>rdrobust</code>: Software for regression-discontinuity designs&quot;. <em>The Stata Journal</em> 17(2):372&ndash;404.
</p>

<hr>
<h2 id='clemson_temps'>Daily Clemson Temperature Data</h2><span id='topic+clemson_temps'></span>

<h3>Description</h3>

<p>This data set contains daily temperatures (highs and lows) for Clemson, South Carolina
from Jan. 1, 1930 to the end of the most recent calendar year.
The goal is to update this periodically with new data for
as long as I live in this town.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clemson_temps
</code></pre>


<h3>Format</h3>

<p>A data frame with 33,148 observations on the following 3 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>the date</p>
</dd>
<dt><code>tmin</code></dt><dd><p>the daily low, adjusted to Fahrenheit</p>
</dd>
<dt><code>tmax</code></dt><dd><p>the daily high, adjusted to Fahrenheit</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data obtained from NOAA, via the <span class="pkg">rnoaa</span> package. The station identifier is <code>GHCND:USC00381770</code> for added context.
The call from <span class="pkg">rnoaa</span> returns these values initially as Celsius*10. I don't know why NOAA does it this way, but there you go.
</p>

<hr>
<h2 id='co2emissions'>Carbon Dioxide Emissions Data</h2><span id='topic+co2emissions'></span>

<h3>Description</h3>

<p>This is a sample data set, cobbled from various sources, about
carbon dioxide emissions in the history of the planet from 800,000 BCE to
the most recently concluded calendar year. I use this for a data visualization
example for a lecture on climate change and international politics.
Data communicate yearly averages/estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>co2emissions
</code></pre>


<h3>Format</h3>

<p>A data frame with 3,099 observations on the following 2 variables.
</p>

<dl>
<dt><code>year</code></dt><dd><p>the year (negative values = BCE)</p>
</dd>
<dt><code>value</code></dt><dd><p>estimated carbon dioxide emissions (in ppm)</p>
</dd>
</dl>



<h3>Details</h3>

<p>The data come from many sources. Before 0 CE, the data come from 10 sources
described by the Environmental Protection Agency (&quot;Climate Change Indicators:
Atmospheric Concentrations of Greenhouse Gases&quot;). Observations from 0 CE to
2014 come from Meinshausen et al. (2017)
doi: <a href="https://doi.org/10.5194/gmd-10-2057-2017">10.5194/gmd-10-2057-2017</a>. Observations from 2015 forward come
from NASA (&quot;Vital Signs&quot;).
</p>


<h3>References</h3>

<p>EPICA Dome C and Vostok Station, Antarctica: approximately 796,562 BCE to 1813 CE
Lüthi, D., M. Le Floch, B. Bereiter, T. Blunier, J.-M. Barnola, U. Siegenthaler, D. Raynaud,
J. Jouzel, H. Fischer, K. Kawamura, and T.F. Stocker. 2008. High-resolution carbon dioxide
concentration record 650,000–800,000 years before present. Nature 453:379–382.
</p>
<p>Law Dome, Antarctica, 75-year smoothed: approximately 1010 CE to 1975 CE
Etheridge, D.M., L.P. Steele, R.L. Langenfelds, R.J. Francey, J.-M. Barnola,
and V.I. Morgan. 1998. Historical CO2 records from the Law Dome DE08, DE08-2,
and DSS ice cores. In: Trends: A compendium of data on global change.
Oak Ridge, TN: U.S. Department of Energy.
</p>
<p>Siple Station, Antarctica: approximately 1744 CE to 1953 CE Neftel, A.,
H. Friedli, E. Moor, H. Lötscher, H. Oeschger, U. Siegenthaler, and B. Stauffer.
1994. Historical carbon dioxide record from the Siple Station ice core.
In: Trends: A compendium of data on global change.
Oak Ridge, TN: U.S. Department of Energy.
</p>
<p>Mauna Loa, Hawaii: 1959 CE to 2015 CE NOAA
(National Oceanic and Atmospheric Administration). 2016.
Annual mean carbon dioxide concentrations for Mauna Loa, Hawaii.
</p>
<p>Barrow, Alaska: 1974 CE to 2014 CE Cape Matatula, American Samoa:
1976 CE to 2014 CE South Pole, Antarctica: 1976 CE to 2014 CE NOAA
(National Oceanic and Atmospheric Administration). 2016.
Monthly mean carbon dioxide concentrations for Barrow, Alaska; Cape Matatula,
American Samoa; and the South Pole.
</p>
<p>Cape Grim, Australia: 1992 CE to 2006 CE Shetland Islands, Scotland:
1993 CE to 2002 CE Steele, L.P., P.B. Krummel, and R.L. Langenfelds. 2007.
Atmospheric CO2 concentrations (ppmv) derived from flask air samples collected at
Cape Grim, Australia, and Shetland Islands, Scotland. Commonwealth Scientific
and Industrial Research Organisation.
</p>
<p>Lampedusa Island, Italy: 1993 CE to 2000 CE Chamard, P., L. Ciattaglia, A. di Sarra,
and F. Monteleone. 2001. Atmospheric carbon dioxide record from flask measurements
at Lampedusa Island. In: Trends: A compendium of data on global change.
Oak Ridge, TN: U.S. Department of Energy.
</p>
<p>Meinshausen, M., Vogel, E., Nauels, A., Lorbacher, K., Meinshausen,
N., Etheridge, D. M., Fraser, P. J., Montzka, S. A., Rayner, P. J., Trudinger,
C. M., Krummel, P. B., Beyerle, U., Canadell, J. G., Daniel, J. S., Enting,
I. G., Law, R. M., Lunder, C. R., O'Doherty, S., Prinn, R. G., Reimann, S., Rubino,
M., Velders, G. J. M., Vollmer, M. K., Wang, R. H. J., and Weiss, R.:
Historical greenhouse gas concentrations for climate modelling (CMIP6),
Geosci. Model Dev., 10, 2057-2116, 2017.
</p>

<hr>
<h2 id='coffee_imports'>Coffee Imports for Select Importing Countries</h2><span id='topic+coffee_imports'></span>

<h3>Description</h3>

<p>A simple panel on coffee imports for importing countries.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coffee_imports
</code></pre>


<h3>Format</h3>

<p>A data frame with 4530 observations on the following 4 variables.
</p>

<dl>
<dt><code>country</code></dt><dd><p>a character vector for the country</p>
</dd>
<dt><code>member</code></dt><dd><p>a numeric vector indicating whether the importer is or is not a member of the International Coffee Organization</p>
</dd>
<dt><code>year</code></dt><dd><p>a numeric vector for the year</p>
</dd>
<dt><code>value</code></dt><dd><p>a numeric vector for the coffee imports for all select importing countries (in thousand 60-kg bags)</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data come from the International Coffee Organization, of which I
feel I should be a member.
</p>
<p>Observations for the People's Republic of China are removed because those can
be obtained by adding together the values for &quot;Macao&quot;, &quot;Hong Kong&quot;, and
&quot;China (Mainland)&quot;.
</p>
<p>The user may want to be mindful about when 0s in the value data are actually
communicating that the entry did not exist at the time, or no longer exists.
For example, there is no independent Armenia in 1990 (and whatever imports
Armenia had are lumped into the USSR value for 1990). Likewise, the 0s for
the USSR in 1992 are communicating the USSR no longer exists that year and you
should instead look into one of the constituent republics for the information
you want. You may want to benchmark this information to some kind of state
system membership data.
</p>

<hr>
<h2 id='coffee_price'>The Primary Commodity Price for Coffee (Arabica, Robustas)</h2><span id='topic+coffee_price'></span>

<h3>Description</h3>

<p>This is primary commodity price data for coffee (Arabica, Robustas)
from 1980 to the present. I manually update these data since FRED's
coverage since 2017 has been spotty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coffee_price
</code></pre>


<h3>Format</h3>

<p>A data frame with the following 3 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>the date (year-month)</p>
</dd>
<dt><code>arabica</code></dt><dd><p>the price (monthly average) of mild Arabica, via
International Coffee Organization data, in nominal US cents per pound</p>
</dd>
<dt><code>robustas</code></dt><dd><p>the price (monthly average) of Robustas,
via International Coffee Organization data, in nominal US cents per pound</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data come from International Monetary Fund (Primary Commodity Prices) and
International Coffee Organization. The IMF adds these prices are global and the
New York cash price, ex-dock
</p>

<hr>
<h2 id='commodity_prices'>Select World Bank Commodity Price Data (Monthly)</h2><span id='topic+commodity_prices'></span>

<h3>Description</h3>

<p>A data set on select, monthly commodity prices made available by the
World Bank in its so-called &quot;pink sheet.&quot; These data are potentially
useful for applications on data gathering, inflation adjustments,
indexing, cointegration, general economic riff-raff, and more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>commodity_prices
</code></pre>


<h3>Format</h3>

<p>A data frame with the following 11 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>a date</p>
</dd>
<dt><code>oil_brent</code></dt><dd><p>crude oil, UK Brent 38' API ($/bbl)</p>
</dd>
<dt><code>oil_dubai</code></dt><dd><p>crude oil, Dubai Fateh 32 API for years 1985-present; 1960-84 refer to Saudi Arabian Light, 34' API ($/bbl).</p>
</dd>
<dt><code>coffee_arabica</code></dt><dd><p>coffee (ICO), International Coffee Organization indicator price, other mild Arabicas, average New York and Bremen/Hamburg markets, ex-dock ($/kg)</p>
</dd>
<dt><code>coffee_robustas</code></dt><dd><p>coffee (ICO), International Coffee Organization indicator price, Robustas, average New York and Le Havre/Marseilles markets, ex-dock ($/kg)</p>
</dd>
<dt><code>tea_columbo</code></dt><dd><p>tea (Colombo auctions), Sri Lankan origin, all tea, arithmetic average of weekly quotes ($/kg).</p>
</dd>
<dt><code>tea_kolkata</code></dt><dd><p>tea (Kolkata auctions), leaf, include excise duty, arithmetic average of weekly quotes ($/kg).</p>
</dd>
<dt><code>tea_mombasa</code></dt><dd><p>tea (Mombasa/Nairobi auctions), African origin, all tea, arithmetic average of weekly quotes ($/kg).</p>
</dd>
<dt><code>sugar_eu</code></dt><dd><p>sugar (EU), European Union negotiated import price for raw unpackaged sugar from African, Caribbean and Pacific (ACP) under Lome Conventions, c.I.f. European ports ($/kg)</p>
</dd>
<dt><code>sugar_us</code></dt><dd><p>sugar (United States), nearby futures contract, c.i.f. ($/kg)</p>
</dd>
<dt><code>sugar_world</code></dt><dd><p>sugar (World), International Sugar Agreement (ISA) daily price, raw,  f.o.b. and stowed at greater Caribbean ports ($/kg).</p>
</dd>
</dl>



<h3>Details</h3>

<p>All data are in nominal USD. Adjust (to taste) accordingly.
</p>
<p>Data compiled by the World Bank for its historical data on commodity prices.
The oil price data come from a combination of sources, supposedly Bloomberg,
Energy Intelligence Group (EIG), Organization of Petroleum Exporting Countries
(OPEC), and the World Bank. Data on coffee prices come from Bloomberg, Complete
Coffee Coverage, the International Coffee Organization, Thomson Reuters Datastream,
and the World Bank. Data on tea prices for Colombo auctions come the from
International Tea Committee, Tea Broker's Association of London, Tea Exporters
Association Sri Lanka, and the World Bank. Data on tea prices for Kolkata
auctions come from the International Tea Committee, Tea Board India, Tea
Broker's Association of London, and the World Bank. Tea prices
for Mombasa/Nairobi auctions come from African Tea Brokers Limited, International
Tea Committee, Tea Broker's Association of London, and the World Bank. EU sugar
price data come from International Monetary Fund,  World Bank. Sugar price
data for the United States come from Bloomberg and World Bank. Global sugar
price data come from Bloomberg, International Sugar Organization, Thomson Reuters
Datastream, and the World Bank.
</p>
<p>This data set effectively deprecates the <code>sugar_price</code> and <code>coffee_price</code>
data sets in this package. Both may be removed at a later point.
</p>

<hr>
<h2 id='CP77'>Education Expenditure Data (Chatterjee and Price, 1977)</h2><span id='topic+CP77'></span>

<h3>Description</h3>

<p>This is a simple data set provided by Chatterjee and Price (1977, p. 108)
that serves as a known example of heteroscedasticity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CP77
</code></pre>


<h3>Format</h3>

<p>A data frame with 50 observations on the following 6 variables.
</p>

<dl>
<dt><code>state</code></dt><dd><p>a character vector for the state</p>
</dd>
<dt><code>region</code></dt><dd><p>a character vector for the Census region</p>
</dd>
<dt><code>urbanpop</code></dt><dd><p>a numeric vector for the number of residents (per thousand) living in urban areas in 1970</p>
</dd>
<dt><code>incpc</code></dt><dd><p>a numeric vector for income per capita in 1973</p>
</dd>
<dt><code>pop</code></dt><dd><p>a numeric vector for residents (per thousand) under 18 years of age in 1974</p>
</dd>
<dt><code>edexppc</code></dt><dd><p>a numeric vector for per capita public school expenditures in a state, projected for 1975.</p>
</dd>
</dl>



<h3>Details</h3>

<p>I copied these data from the <code>robustbase</code> package.
I just didn't want to make my students install it.
Note: I'm pretty sure &quot;NB&quot; was suppose to be &quot;NE&quot; and that
&quot;DY&quot; is supposed to be &quot;KY&quot;. I made those changes.
</p>


<h3>References</h3>

<p>P. J. Rousseeuw and A. M. Leroy (1987) Robust Regression and Outlier Detection; Wiley, p.110, table 16.
</p>

<hr>
<h2 id='Datasaurus'>The Datasaurus Dozen</h2><span id='topic+Datasaurus'></span>

<h3>Description</h3>

<p>An illustrative exercise in never trusting the summary statistics without
also visualizing them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Datasaurus
</code></pre>


<h3>Format</h3>

<p>A data frame with 1,846 observations on the following 3 variables.
</p>

<dl>
<dt><code>dataset</code></dt><dd><p>the particular data set, one of 12</p>
</dd>
<dt><code>x</code></dt><dd><p>a random variable</p>
</dd>
<dt><code>y</code></dt><dd><p>another random variable</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data were created by Alberto Cairo to illustrate you should always
visualize your data beyond the summary statistics. These are 12 data sets,
in long form, each with a mean of <code>x</code> about 54.26, a mean of <code>y</code>
about 47.83. The standard deviation for <code>x</code> is about 16.76 and the
standard deviation of <code>y</code> is about 26.93. <code>x</code> and <code>y</code> will
correlate weakly, about -.06.
</p>


<h3>Author(s)</h3>

<p>Alberto Cairo, Justin Matejka, George Fitzmaurice
</p>


<h3>References</h3>

<p>Cairo, Alberto. 2016. &ldquo;Download the Datasaurus: Never trust
summary statistics alone; always visualize your data&rdquo;.
<em>URL:</em> <a href="http://www.thefunctionalart.com/2016/08/download-datasaurus-never-trust-summary.html">http://www.thefunctionalart.com/2016/08/download-datasaurus-never-trust-summary.html</a>
</p>
<p>Matejka, Justin and George Fitzmaurice. 2017. &ldquo;Same Stats, Different Graphs: Generating Datasets
with Varied Appearance and Identical Statistics through Simulated Annealing.&rdquo;
<em>ACM SIGCHI Conference on Human Factors in Computing Systems</em>.
</p>

<hr>
<h2 id='Dee04'>Are There Civics Returns to Education?</h2><span id='topic+Dee04'></span>

<h3>Description</h3>

<p>This should be a data set for a (partial?) replication of
Dee's (2004) article on the purported civics returns to education. I use
these data for in-class illustration about instrumental variable analyses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Dee04
</code></pre>


<h3>Format</h3>

<p>A data frame with 9227 observations on the following 8 variables.
</p>

<dl>
<dt><code>schoolid</code></dt><dd><p>a numeric vector that should be understood as categorical</p>
</dd>
<dt><code>hispanic</code></dt><dd><p>a numeric vector for if the person is Hispanic</p>
</dd>
<dt><code>college</code></dt><dd><p>a numeric vector for if the person went to college</p>
</dd>
<dt><code>black</code></dt><dd><p>a numeric vector for if the person is black</p>
</dd>
<dt><code>otherrace</code></dt><dd><p>a numeric vector for if the person is another race</p>
</dd>
<dt><code>female</code></dt><dd><p>a numeric vector for if the person is a woman</p>
</dd>
<dt><code>register</code></dt><dd><p>a numeric vector for if the person is registered to vote</p>
</dd>
<dt><code>distance</code></dt><dd><p>a numeric vector for the distance to college</p>
</dd>
</dl>



<h3>Details</h3>

<p>I should note I acquired this data set in Mexico City sitting on a two-week program at IPSA-FLACSO Mexico Summer School in 2019. The sample size here (9,227) is about two thousand short of what Dee reports in his article. It'll do, though.
</p>


<h3>References</h3>

<p>Dee, Thomas S. 2004. &quot;Are there civics returns to education?&quot; <em>Journal of Public Economics</em> 88: 1697&ndash;1720
</p>

<hr>
<h2 id='DJIA'>Dow Jones Industrial Average, 1885-Present</h2><span id='topic+DJIA'></span>

<h3>Description</h3>

<p>This data set contains the value of the Dow Jones Industrial Average
on daily close for all available dates (to the best of my knowledge) from 1885
to the most recently concluded calendar year. Extensions shouldn't be too
difficult with existing packages.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DJIA
</code></pre>


<h3>Format</h3>

<p>A data frame with 36951 observations on the following 2 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>the date</p>
</dd>
<dt><code>value</code></dt><dd><p>the value of the the Dow Jones Industrial Average at daily close</p>
</dd>
</dl>



<h3>Details</h3>

<p>Observations before October 7, 1896 are from the single Dow Jones Average.
Observations from October 7, 1896 to July 30, 1914 are from the first DJIA.
Observations before the 1914 closure of the first DJIA in July 1914 come from MeasuringWorth.
Observations from its reopening in Dec. 12, 1914 to January 28, 1985 come from
Pinnacle Systems. Observations from January 29, 1985 to the most recent observation come
from a <code>quantmod</code> call.
</p>


<h3>References</h3>

<p>Samuel H. Williamson, 'Daily Closing Value of the Dow Jones Average, 1885 to Present,' MeasuringWorth, 2019.
</p>
<p>Jeffrey A. Ryan and Joshua M. Ulrich, '<code>quantmod</code>: Quantitative Financial Modelling Framework,' 2018.
</p>

<hr>
<h2 id='DST'>Casualties/Fatalities in the U.S. for Drunk-Driving, Suicide, and Terrorism</h2><span id='topic+DST'></span>

<h3>Description</h3>

<p>These are fatalities (and, in the case of terrorism, casualties as well) for
drunk-driving, suicide, and acts of terrorism in the U.S. spanning 1970 to 2018.
Only one of these is sufficiently important to command public attention despite
being the least severe public bad. Do you want to guess which one?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DST
</code></pre>


<h3>Format</h3>

<p>A data frame with 49 observations on the following 5 variables.
</p>

<dl>
<dt><code>year</code></dt><dd><p>the year</p>
</dd>
<dt><code>nkill</code></dt><dd><p>a numeric vector for the number killed in acts of terrorism</p>
</dd>
<dt><code>terrtotal</code></dt><dd><p>a numeric vector for the number killed or wounded in acts of terrorism</p>
</dd>
<dt><code>suicides</code></dt><dd><p>a numeric vector for the number of suicides</p>
</dd>
<dt><code>ddfat</code></dt><dd><p>a numeric vector for the number of drunk-driving fatalities</p>
</dd>
</dl>



<h3>Details</h3>

<p>Following my own work in <em>Political Research Quarterly</em>, terror
incidents with unknown fatalities or number wounded were imputed to be 1.
In those cases, the GTD has reason to believe at least one person died or was
wounded, but doesn't know how many. GTD is weird about 1993, so perhaps treat
those observations with some care (though it does well to capture the WTC
bombing that year). Suicides include only those who passed, not those who
survived a suicide attempt. Drunk-driving fatalities seem to include those
who were killed in a drunk-driving accident despite not being drunk themselves.
</p>


<h3>Source</h3>

<p>Global Terrorism Database (Sept. 2019 update), Centers for Disease Control,
U.S. Department of Transportation
</p>

<hr>
<h2 id='eight_schools'>The Effect of Special Preparation on SAT-V Scores in Eight Randomized Experiments</h2><span id='topic+eight_schools'></span>

<h3>Description</h3>

<p>You've all seen these before. These are the &quot;eight schools&quot; that everyone gets when
being introduced to Bayesian programming. Here are the full data for your consideration,
which you can use instead of awkwardly searching where the data are and copy-pasting
them as a list. Every damn time, Steve.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eight_schools
</code></pre>


<h3>Format</h3>

<p>A data frame with 8 observations on the following 6 variables.
</p>

<dl>
<dt><code>school</code></dt><dd><p>a letter denoting the school</p>
</dd>
<dt><code>num_treat</code></dt><dd><p>the number of students in the school receiving the treatment</p>
</dd>
<dt><code>num_control</code></dt><dd><p>the number of students in the school in the control group</p>
</dd>
<dt><code>est</code></dt><dd><p>the estimated treatment effect</p>
</dd>
<dt><code>se</code></dt><dd><p>the standard error of the effect estimate</p>
</dd>
<dt><code>rvar</code></dt><dd><p>the residual variance</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data copy-pasted from Table 1 in Rubin (1981).
</p>


<h3>References</h3>

<p>Rubin, Donald B. 1981. &quot;Estimation in Parallel Randomized Experiments.&quot; <em>Journal of Educational Statistics</em> 6(4): 377-401.
</p>

<hr>
<h2 id='election_turnout'>State-Level Education and Voter Turnout in 2016</h2><span id='topic+election_turnout'></span>

<h3>Description</h3>

<p>A simple data set on education and state-level (+ DC) turnout in the 2016
presidential election. This is inspired by what Pollock (2012) does in his book.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>election_turnout
</code></pre>


<h3>Format</h3>

<p>A data frame with 51 observations on the following 13 variables.
</p>

<dl>
<dt><code>year</code></dt><dd><p>the year of the presidential election (2016)</p>
</dd>
<dt><code>state</code></dt><dd><p>the state abbreviation</p>
</dd>
<dt><code>region</code></dt><dd><p>the state's Census region</p>
</dd>
<dt><code>division</code></dt><dd><p>the state's Census division</p>
</dd>
<dt><code>turnoutho</code></dt><dd><p>voter turnout for the highest office as percent of voting-eligible population (VEP)</p>
</dd>
<dt><code>perhsed</code></dt><dd><p>the percentage of the state that completed high school</p>
</dd>
<dt><code>percoled</code></dt><dd><p>the percentage of the state that completed college</p>
</dd>
<dt><code>gdppercap</code></dt><dd><p>an estimate of the state's GDP per capita</p>
</dd>
<dt><code>ss</code></dt><dd><p>is it a &ldquo;swing state?&rdquo;</p>
</dd>
<dt><code>trumpw</code></dt><dd><p>did Trump win the state?</p>
</dd>
<dt><code>trumpshare</code></dt><dd><p>the share of the vote Trump received</p>
</dd>
<dt><code>sunempr</code></dt><dd><p>the state-level unemployment rate entering Nov. 2016</p>
</dd>
<dt><code>sunempr12md</code></dt><dd><p>the state-level unemployment rate (12-month difference) entering Nov. 2016. Higher values indicate the unemployment rate is increasing entering Nov. 2016 relative to what it was entering Nov. 2015.</p>
</dd>
<dt><code>gdp</code></dt><dd><p>an estimate of the state's GDP</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data were created in early 2017 for an upper-division course on
quantitative methods. Educational attainment and division/region data come from the
Census. Voter turnout/share data come from the Elections Project at George Mason University.
GDP per capita estimates come from Bureau of Economic Analysis. Unemployment data come from
the Bureau of Labor Statistics and code to generate it was derived from a forthcoming
publication of mine.
</p>

<hr>
<h2 id='eq_passengercars'>Export Quality Data for Passenger Cars, 1963-2014</h2><span id='topic+eq_passengercars'></span>

<h3>Description</h3>

<p>Data from the International Monetary Fund for the export quality and
unit/trade value of passenger cars for all available countries and years
from 1963 to 2014.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eq_passengercars
</code></pre>


<h3>Format</h3>

<p>A data frame with 60424 observations on the following 6 variables.
</p>

<dl>
<dt><code>country</code></dt><dd><p>a character vector for the country/area.</p>
</dd>
<dt><code>ccode</code></dt><dd><p>a numeric vector for the Correlates of War country code.</p>
</dd>
<dt><code>category</code></dt><dd><p>a factor with levels <code>Export Quality Index</code>,
<code>Export quality 95 percent interval - lower bound</code>,
<code>Export quality 95 percent interval - upper bound</code>
<code>Unit value of exports</code>, <code>Unit value 95 percent interval - lower bound</code>,
<code>Unit value 95 percent interval - upper bound</code>,
<code>Trade value of exports</code></p>
</dd>
<dt><code>type</code></dt><dd><p>a factor with levels <code>51. Transport equipment, Passenger cars</code>.
This is a constant. I just felt like making it a factor.</p>
</dd>
<dt><code>year</code></dt><dd><p>a numeric vector for the year</p>
</dd>
<dt><code>value</code></dt><dd><p>a numeric vector for the value of the particular category.</p>
</dd>
</dl>


<hr>
<h2 id='ESS10NO'>Norwegian Attitudes toward European Integration (2021-2022)</h2><span id='topic+ESS10NO'></span>

<h3>Description</h3>

<p>This is a simple data set to illustrate the use of sampling weights from
the European Social Survey.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ESS10NO
</code></pre>


<h3>Format</h3>

<p>A data frame with 1,411 observations on the following 24 variables.
</p>

<dl>
<dt><code>cntry</code></dt><dd><p>a character vector with Norway's two-character ISO code</p>
</dd>
<dt><code>idno</code></dt><dd><p>a numeric identifier for the individual respondent</p>
</dd>
<dt><code>region</code></dt><dd><p>a character for one of six regions recorded by the European Social Survey</p>
</dd>
<dt><code>inwds</code></dt><dd><p>a date-time vector for the start of the interview</p>
</dd>
<dt><code>inwde</code></dt><dd><p>a date-time vector for the end of the interview</p>
</dd>
<dt><code>dweight</code></dt><dd><p>a design weight</p>
</dd>
<dt><code>pspwght</code></dt><dd><p>a post-stratification weight, including the design weight</p>
</dd>
<dt><code>pweight</code></dt><dd><p>a population size weight</p>
</dd>
<dt><code>anweight</code></dt><dd><p>an analysis weight</p>
</dd>
<dt><code>prob</code></dt><dd><p>the sampling probability</p>
</dd>
<dt><code>stratum</code></dt><dd><p>the sampling stratum</p>
</dd>
<dt><code>psu</code></dt><dd><p>the primary sampling unit</p>
</dd>
<dt><code>eu_vote</code></dt><dd><p>a character vector indicating how a respondent would vote if given a vote on joining the European Union</p>
</dd>
<dt><code>brnnorge</code></dt><dd><p>a dummy variable indicating whether respondent was born in Norway or not</p>
</dd>
<dt><code>agea</code></dt><dd><p>a numeric vector for the respondent's age in years</p>
</dd>
<dt><code>imbgeco</code></dt><dd><p>a numeric vector for if respondent thinks immigrants are generally good or bad for Norway's economy. Higher values = good</p>
</dd>
<dt><code>imueclt</code></dt><dd><p>a numeric vector for if respondent thinks immigrants enrich or undermine Norway's culture. Higher values = enrich more than undermine</p>
</dd>
<dt><code>imwbcnt</code></dt><dd><p>a numeric vector for if respondent thinks immigrants make Norway a better place to live. Higher values = better place to live</p>
</dd>
<dt><code>female</code></dt><dd><p>a numeric vector for whether the respondent is a woman</p>
</dd>
<dt><code>eduyrs</code></dt><dd><p>a numeric vector for total years of education for the respondent</p>
</dd>
<dt><code>uempla</code></dt><dd><p>a numeric vector for whether the respondent is currently unemployed but seeking work</p>
</dd>
<dt><code>polint</code></dt><dd><p>a dummy variable indicating political interest. 1 = very or quite interested. 0 = hardly or not at all interested.</p>
</dd>
<dt><code>hinctnta</code></dt><dd><p>a numeric vector for household income in deciles</p>
</dd>
<dt><code>lrscale</code></dt><dd><p>a numeric vector for the ideology of the respondent on an 11-point [0:10] scale</p>
</dd>
</dl>



<h3>Details</h3>

<p>You'll want to convert the <code>eu_vote</code> variable into something usable.
Possible values include &quot;Remain Outside&quot;, &quot;Join EU&quot;, &quot;Don't Know&quot;, &quot;Not Eligible&quot;,
&quot;Blank Ballot&quot;, &quot;Refuse to Answer&quot;, &quot;Wouldn't Vote&quot;. Perhaps it's reasonable
to make this a dummy variable comparing those who want to join versus those
who want Norway to remain outside the European Union.
</p>
<p>The data are edition 2.2 of the 10th round of European Social Survey, which
was released for public consumption on 21 December 2022.
</p>


<h3>Source</h3>

<p>European Social Survey, Round 10
</p>

<hr>
<h2 id='ESS9GB'>British Attitudes Toward Immigration (2018-19)</h2><span id='topic+ESS9GB'></span>

<h3>Description</h3>

<p>This is a replication data originally set to accompany a blog post and
presentation to students at the University of Nottingham in March 2020.
However, COVID-19 led to the cancellation of the talk.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ESS9GB
</code></pre>


<h3>Format</h3>

<p>A data frame with 1,905 observations on the following 19 variables.
</p>

<dl>
<dt><code>name</code></dt><dd><p>a character for the name of the survey</p>
</dd>
<dt><code>essround</code></dt><dd><p>a numeric for the ESS round</p>
</dd>
<dt><code>edition</code></dt><dd><p>a character for the particular edition of the ESS round</p>
</dd>
<dt><code>idno</code></dt><dd><p>a numeric/unique identifier</p>
</dd>
<dt><code>cntry</code></dt><dd><p>a character vector for the country (i.e. the UK)</p>
</dd>
<dt><code>region</code></dt><dd><p>a character vector for the region of the UK the respondent lives</p>
</dd>
<dt><code>brncntr</code></dt><dd><p>a numeric vector for if the respondent was born in the UK</p>
</dd>
<dt><code>stintrvw</code></dt><dd><p>a Date for the interview start date</p>
</dd>
<dt><code>endintrvw</code></dt><dd><p>a Date for the interview end date</p>
</dd>
<dt><code>imbgeco</code></dt><dd><p>a numeric vector for if respondent thinks immigrants are generally good or bad for UK's economy. Higher values = good</p>
</dd>
<dt><code>imueclt</code></dt><dd><p>a numeric vector for if respondent thinks immigrants enrich or undermine UK's culture. Higher values = enrich more than undermine</p>
</dd>
<dt><code>imwbcnt</code></dt><dd><p>a numeric vector for if respondent thinks immigrants make UK a better place to live. Higher values = better place to live</p>
</dd>
<dt><code>immigsent</code></dt><dd><p>a numeric vector for immigration sentiment (i.e. <code>imbgeco</code> +
<code>imueclt</code> + <code>imwbcnt</code>). Higher values = more pro-immigration sentiment</p>
</dd>
<dt><code>agea</code></dt><dd><p>a numeric vector for the respondent's age in years</p>
</dd>
<dt><code>female</code></dt><dd><p>a numeric vector for whether the respondent is a woman</p>
</dd>
<dt><code>eduyrs</code></dt><dd><p>a numeric vector for total years of education for the respondent</p>
</dd>
<dt><code>uempla</code></dt><dd><p>a numeric vector for whether the respondent is currently unemployed but seeking work</p>
</dd>
<dt><code>hinctnta</code></dt><dd><p>a numeric vector for household income in deciles</p>
</dd>
<dt><code>lrscale</code></dt><dd><p>a numeric vector for the ideology of the respondent on an 11-point [0:10] scale</p>
</dd>
</dl>



<h3>Details</h3>

<p>See accompanying blog post at <a href="http://svmiller.com/blog/2020/03/what-explains-british-attitudes-toward-immigration-a-pedagogical-example/">http://svmiller.com/blog/2020/03/what-explains-british-attitudes-toward-immigration-a-pedagogical-example/</a>.
</p>


<h3>Source</h3>

<p>European Social Survey, Round 9
</p>

<hr>
<h2 id='ESSBE5'>Trust in the Police in Belgium (European Social Survey, Round 5)</h2><span id='topic+ESSBE5'></span>

<h3>Description</h3>

<p>This is a sample data set cobbled from the fifth round of
European Social Survey data for Belgium. It offers a means to do a
basic replication of some of Chapter 5 of The SAGE Handbook of
Regression Analysis and Causal Inference.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ESSBE5
</code></pre>


<h3>Format</h3>

<p>A data frame with 1704 observations on the following 10 variables.
</p>

<dl>
<dt><code>essround</code></dt><dd><p>a numeric for the ESS round</p>
</dd>
<dt><code>edition</code></dt><dd><p>a character for the edition number of the fifth round</p>
</dd>
<dt><code>idno</code></dt><dd><p>a numeric id number</p>
</dd>
<dt><code>cntry</code></dt><dd><p>a character vector for the country (i.e. Belgium, or <code>BE</code>)</p>
</dd>
<dt><code>trstplc</code></dt><dd><p>a numeric vector for trust in the police on an 11-point scale. Higher values indicate more trust. 0 = &quot;no trust at all&quot;. 10 = &quot;complete trust&quot;</p>
</dd>
<dt><code>agea</code></dt><dd><p>a numeric vector for the respondent's age</p>
</dd>
<dt><code>female</code></dt><dd><p>a numeric vector for whether the respondent is a woman or not.</p>
</dd>
<dt><code>eduyrs</code></dt><dd><p>a numeric vector for years of education.</p>
</dd>
<dt><code>hincfel</code></dt><dd><p>a numeric vector for the respondent's feeling about their household income. 1 = &quot;living comfortably&quot;, 2 = &quot;coping on present income&quot;, 3 = &quot;difficult on present income&quot;, 4 = &quot;very difficult on present income&quot;</p>
</dd>
<dt><code>plcpvcr</code></dt><dd><p>a numeric vector for how successful police are at preventing crimes in a country on an 11-point scale. 0 = &quot;extremely unsuccessful&quot;. 10  = &quot;extremely successful.&quot;</p>
</dd>
</dl>



<h3>Details</h3>

<p>See Chapter 5 of The SAGE Handbook of Regression Analysis and Causal Inference for more information.
</p>


<h3>Source</h3>

<p>European Social Survey (Round 5)
</p>

<hr>
<h2 id='eurostat_codes'>Eurostat Country Codes</h2><span id='topic+eurostat_codes'></span>

<h3>Description</h3>

<p>A data set taken from Eurostat's glossary on codes and country classifications.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eurostat_codes
</code></pre>


<h3>Format</h3>

<p>A data frame on the following 3 variables.
</p>

<dl>
<dt><code>country</code></dt><dd><p>an English country/territorial unit name</p>
</dd>
<dt><code>iso2c</code></dt><dd><p>a two-character code for the country/territorial unit</p>
</dd>
<dt><code>cat</code></dt><dd><p>a category indicator for the country/territorial unit. See Details section for more.</p>
</dd>
</dl>



<h3>Details</h3>

<p>The ISO two-character code for Kosovo is not &quot;XK&quot;. XK is a &quot;user assigned&quot;
ISO 3166 code that is not used by the International Organization for
Standardization, but is nevertheless in wide use by entities like the European
Commission. To the best of my knowledge, Kosovo's official ISO classification
is still what it was when it was a subdivision of Serbia/Yugoslavia.
</p>
<p>A glossary on Eurostat provides the following category entries included in
this data frame. &quot;EU&quot; is an European Union member. &quot;EFTA&quot; are countries
outside the European Union, but still included in the free trade agreement.
&quot;UK&quot; is the United Kingdom, because they left. &quot;EUCC&quot; is a category for
European Union candidate countries. &quot;PC&quot; are potential candidates. European
Union expansion led to the delineation of neighboring states to &quot;South&quot; and
&quot;East&quot; as part of the European Neighbourhood Policy (ENP). &quot;OEC&quot; stands for
&quot;Other European Countries&quot;, but is effectively a simple indicator for Russia.
</p>

<hr>
<h2 id='eustates'>EU Member States (Current as of 2019)</h2><span id='topic+eustates'></span>

<h3>Description</h3>

<p>European Union membership by accession date
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eustates
</code></pre>


<h3>Format</h3>

<p>A data frame with 28 observations on the following 3 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>a date indicating accession</p>
</dd>
<dt><code>country</code></dt><dd><p>a character vector for the country</p>
</dd>
<dt><code>iso2c</code></dt><dd><p>a character vector for iso2c</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data come from the European Union's website.
</p>

<hr>
<h2 id='fakeAPI'>Hypothetical (Fake) Data on Academic Performance</h2><span id='topic+fakeAPI'></span>

<h3>Description</h3>

<p>This is a hypothetical universe of schools in a given territorial unit,
patterned off the <code>apipop</code> data available in the <code>survey</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fakeAPI
</code></pre>


<h3>Format</h3>

<p>A data frame with 10000 observations on the following 8 variables.
</p>

<dl>
<dt><code>uid</code></dt><dd><p>a numeric vector as a unique identifier for schools</p>
</dd>
<dt><code>schooltype</code></dt><dd><p>a character vector for school type. E = elementary school. M = middle school. H = high school</p>
</dd>
<dt><code>county</code></dt><dd><p>a character vector for the county, named after an Ohio State All-American. &ldquo;County&rdquo; incidence is weighted by how many All-American honors the Ohio State player had. It's my fake data. You make your own if you have a problem with it.</p>
</dd>
<dt><code>community</code></dt><dd><p>a character vector for the school's community, either rural, suburban, or urban.</p>
</dd>
<dt><code>api</code></dt><dd><p>a numeric vector vector an academic performance index for the school</p>
</dd>
<dt><code>meals</code></dt><dd><p>a numeric vector for the percentage of school students eligible for subsidized meals</p>
</dd>
<dt><code>colgrad</code></dt><dd><p>a numeric vector for the percentage of school parents with college degrees</p>
</dd>
<dt><code>fullqual</code></dt><dd><p>a numeric vector for the percentage of the school with teachers that are fully qualified</p>
</dd>
<dt><code>sbase</code></dt><dd><p>a numeric vector for some base differences between schools, patterned off the school type means for <code>api00</code> in the <code>apipop</code> data.</p>
</dd>
<dt><code>cbase</code></dt><dd><p>a numeric vector for some base differences between counties, randomly drawn from a uniform distribution</p>
</dd>
<dt><code>e</code></dt><dd><p>a numeric vector for random errors</p>
</dd>
</dl>



<h3>Details</h3>

<p>These data were generated for a blog post on my website.
</p>


<h3>References</h3>

<p>Miller, Steven V. 2020. &quot;Some Parlor Tricks with Survey-Type Analyses in R.&quot; URL: <a href="http://svmiller.com/blog/2020/08/some-parlor-tricks-with-survey-type-analyses-in-r/">http://svmiller.com/blog/2020/08/some-parlor-tricks-with-survey-type-analyses-in-r/</a>
</p>

<hr>
<h2 id='fakeHappiness'>Fake Data on Happiness</h2><span id='topic+fakeHappiness'></span>

<h3>Description</h3>

<p>This is a toy (&quot;fake&quot;) data set I might use to illustrate the so-called
curvilinear effect of age on happiness.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fakeHappiness
</code></pre>


<h3>Format</h3>

<p>A data frame with 1000 observations on the following 8 variables.
</p>

<dl>
<dt><code>age</code></dt><dd><p>a numeric vector for age.</p>
</dd>
<dt><code>female</code></dt><dd><p>a numeric that equals 1 if the respondent is a woman</p>
</dd>
<dt><code>collegeed</code></dt><dd><p>a numeric vector that equals 1 if the respondent says s/he has a college degree</p>
</dd>
<dt><code>famincr</code></dt><dd><p>a numeric vector for the respondent's household income. Ranges from 1 to 12.</p>
</dd>
<dt><code>bornagain</code></dt><dd><p>a numeric vector for whether the respondent self-identifies as a born-again Christian.</p>
</dd>
<dt><code>e</code></dt><dd><p>random noise, generated from a normal distribution with a mean of 0 and a standard deviation of 3</p>
</dd>
<dt><code>happy</code></dt><dd><p>an arbitrary happiness variable. See details for its construction</p>
</dd>
<dt><code>z_happy</code></dt><dd><p>the same arbitrary happiness variable, scaled to have a mean of 0 and a standard deviation of 1. This makes it seem more &quot;latent&quot;.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data are randomly sampled from the <code>TV16</code> data set in the same package
for the age, female, college education, family income, and born-again variables.
Thereafter, I created an arbitrary &quot;happiness&quot; variable that is equal to
<code>100 -  .95*age + .01*(age^2) + .25*female + .05*famincr + .1*bornagain + e</code>. The
data are not supposed to be realistic, per se. They're supposed to be functional for this
purpose.
</p>

<hr>
<h2 id='fakeLogit'>Fake Data for a Logistic Regression</h2><span id='topic+fakeLogit'></span>

<h3>Description</h3>

<p>This is a simple fake data set to illustrate a logistic regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fakeLogit
</code></pre>


<h3>Format</h3>

<p>A data frame with 10000 observations on the following 2 variables.
</p>

<dl>
<dt><code>x</code></dt><dd><p>a five-item functionally ordered categorical variable</p>
</dd>
<dt><code>y</code></dt><dd><p>a binary variable that is either 0 or 1</p>
</dd>
</dl>



<h3>Details</h3>

<p>The data are generated such that the outcome <code>y</code> is a logistic
function of the <code>x</code> variable and come from a <code>rbinom()</code> call. The
estimated natural logged odds of <code>y</code> when <code>x</code> is 0 is -2.8. Each
unit increase in <code>x</code> is simulated to increase the natural logged odds of
<code>y</code> by 1.4. This example is very much patterned off a similar fake data
set that Pollock (2012) uses to teach about logistic regression. In his case,
<code>x</code> is a stand-in for hypothetical education categories and <code>y</code> is
whether this fake person voted or not.
</p>

<hr>
<h2 id='fakeTSCS'>Fake Data for a Time-Series Cross-Section</h2><span id='topic+fakeTSCS'></span>

<h3>Description</h3>

<p>This is a toy (i.e. &quot;fake&quot;) data set created by the <code>fabricatr</code> package.
There are 100 observations for 25 hypothetical countries. The outcome <code>y</code>
is a linear function of a baseline for each hypothetical country, plus a yearly
growth trend as well as varying growth errors for each country. <code>x1</code> is
supposed to have a linear effect of .5 on <code>y</code>, all things considered.
<code>x2</code> is supposed to have a linear effect of 1 on <code>y</code> for each unit
change in <code>x2</code>, all things considered.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fakeTSCS
</code></pre>


<h3>Format</h3>

<p>A data frame with 2500 observations on the following 8 variables.
</p>

<dl>
<dt><code>year</code></dt><dd><p>a numeric vector for the year</p>
</dd>
<dt><code>country</code></dt><dd><p>a character vector for the country</p>
</dd>
<dt><code>y</code></dt><dd><p>a numeric vector for the outcome.</p>
</dd>
<dt><code>x1</code></dt><dd><p>a continuous variable</p>
</dd>
<dt><code>x2</code></dt><dd><p>a binary variable</p>
</dd>
<dt><code>base</code></dt><dd><p>a numeric vector for the baseline starting point for each country</p>
</dd>
<dt><code>growth_units</code></dt><dd><p>a numeric vector for the growth units for each country</p>
</dd>
<dt><code>growth_error</code></dt><dd><p>a numeric vector for the growth errors for each country</p>
</dd>
</dl>



<h3>Details</h3>

<p><code>x1</code> is generated by a normal distribution with a mean of 5 and a standard
deviation of 2. <code>x2</code> is drawn from a Bernoulli distribution with a
probability of .5 of observing a 1.
</p>

<hr>
<h2 id='fakeTSD'>Fake Data for a Time-Series</h2><span id='topic+fakeTSD'></span>

<h3>Description</h3>

<p>This is a toy (i.e. &quot;fake&quot;) data set created by the <code>fabricatr</code> package.
There are 100 observations. The outcome <code>y</code> is a linear function
of <code>20 + (.25 * year) + .(25 * x1) + (1 * x2) + e</code>. This clearly implies
some autocorrelation in the data. I.e. it's a time-series.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fakeTSD
</code></pre>


<h3>Format</h3>

<p>A data frame with 100 observations on the following 5 variables.
</p>

<dl>
<dt><code>year</code></dt><dd><p>the year</p>
</dd>
<dt><code>y</code></dt><dd><p>an outcome</p>
</dd>
<dt><code>x1</code></dt><dd><p>a continuous variable</p>
</dd>
<dt><code>x2</code></dt><dd><p>a binary variable</p>
</dd>
<dt><code>e</code></dt><dd><p>randomly generated errors</p>
</dd>
</dl>



<h3>Details</h3>

<p>Errors are random-normal with a mean of 0 and a standard deviation of 1.
<code>x1</code> is generated by a normal distribution with a mean of 5 and a standard
deviation of 2. <code>x2</code> is drawn from a Bernoulli distribution with a
probability of .5 of observing a 1.
</p>

<hr>
<h2 id='ghp100k'>Gun Homicide Rate per 100,000 People, by Country</h2><span id='topic+ghp100k'></span>

<h3>Description</h3>

<p>This is the yearly rate of gun homicides per 100,000 people in the population,
selecting on &quot;Western&quot; countries of interest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ghp100k
</code></pre>


<h3>Format</h3>

<p>A data frame with 561 observations on the following 3 variables.
</p>

<dl>
<dt><code>country</code></dt><dd><p>the country</p>
</dd>
<dt><code>year</code></dt><dd><p>the year</p>
</dd>
<dt><code>value</code></dt><dd><p>a numeric vector for the estimated rate of gun homicide per 100,000 people</p>
</dd>
</dl>



<h3>Details</h3>

<p>The reported, or calculated annual crude rate of completed,
intentional homicide committed with a firearm, per 100,000 population,
in years descending.
</p>
<p>Where a jurisdiction's published count of 'annual homicide' includes cases
of attempted (uncompleted) homicide, these figures have been disaggregated
wherever possible.
</p>
<p>In the United States, this category is confused by inaccurate and conflicting
data published, suppressed or labeled as unreliable by the Centers for Disease
Control and Prevention (CDC) and the Federal Bureau of Investigation (FBI).
Suppression can result in zero values where in fact homicides did occur.
</p>
<p>Incomplete classification by local agencies can also result in a significant
proportion of events being categorized as 'unknown cause' or similar.
</p>
<p>Before quoting these datasets, please follow the citation links for a description
of the considerable differences between them and the reasons for data suppression.
</p>
<p>Where a rate is calculated by <code>GunPolicy.org</code>, a matched population estimate is also cited.
</p>


<h3>Source</h3>

<p><a href="https://www.gunpolicy.org">https://www.gunpolicy.org</a>
</p>

<hr>
<h2 id='GHR04'>Comparative Public Health: The Political Economy of Human Misery and Well-Being</h2><span id='topic+GHR04'></span>

<h3>Description</h3>

<p>This is a data set for replicating Ghobarah et al. (2004), a reduced form of
what they make available on Dataverse for replication. Variables have been
renamed for legibility.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GHR04
</code></pre>


<h3>Format</h3>

<p>A data frame with 182 observations on the following 15 variables.
</p>

<dl>
<dt><code>country</code></dt><dd><p>a character vector denoting a country name</p>
</dd>
<dt><code>iso3c</code></dt><dd><p>a three-character ISO code for the country</p>
</dd>
<dt><code>pubhlthexppgdp</code></dt><dd><p>a numeric vector for public health expenditures as a percentage of GDP</p>
</dd>
<dt><code>totexphlth</code></dt><dd><p>a numeric vector for total expenditures on health</p>
</dd>
<dt><code>hale</code></dt><dd><p>a numeric vector for health adjusted life expectancy (in years)</p>
</dd>
<dt><code>log_gdppc</code></dt><dd><p>a numeric vector for (log-transformed) GDP per capita</p>
</dd>
<dt><code>gini</code></dt><dd><p>a numeric vector for income inequality</p>
</dd>
<dt><code>log_educ</code></dt><dd><p>a numeric vector for (log-transformed) educational attainment</p>
</dd>
<dt><code>log_vanhanen</code></dt><dd><p>a numeric vector for (log-transformed) racial-linguistic-religious heterogeneity</p>
</dd>
<dt><code>rivalry</code></dt><dd><p>a dummy variable indicating the presence of an enduring international rivalry for the country</p>
</dd>
<dt><code>polity</code></dt><dd><p>a numeric vector communicating a Polity score, as a measure of the democratic nature of the country's regime</p>
</dd>
<dt><code>prvhlthexpgdp</code></dt><dd><p>a numeric vector for private spending on health as a percentage of GDP</p>
</dd>
<dt><code>urban_growth</code></dt><dd><p>a numeric vector for the pace of urbanization</p>
</dd>
<dt><code>cwdeaths</code></dt><dd><p>a numeric vector for civil war deaths</p>
</dd>
<dt><code>contig_cw</code></dt><dd><p>a dummy variable communicating whether there is a civil war in a geographically contiguous territory</p>
</dd>
</dl>



<h3>Details</h3>

<p>The three-character ISO code is the only new addition to the data. I
add this because the country names they have in the data are not neat and may
lead users astray if they wanted to search for a specific observation. The
ISO code for Yugoslavia (Serbia and Montenegro) around this time was &quot;SCG&quot;.
</p>
<p>The data the authors make available come with no .do file to indicate what
exactly they used. Some forensic work based on the descriptive statistics they
mention led to this reduced form of their data, which almost perfectly replicates
their results. The differences are typically in the hundredths, and often in
the thousandths, and should be considered &quot;good enough&quot; for replication
purposes. The only real confusion on my end is why I ended up with one more
observation than they report in their analyses. This suggests one (or more?)
of their variables they use has an 'NA', but I have no way of knowing what it
could be.
</p>


<h3>Source</h3>

<p>Ghobarah, Hazem Adam, Paul Huth, and Bruce Russett. 2004. &quot;Comparative Public Health: The Political Economy of Human Misery and Well-Being&quot; International Studies Quarterly 48: 73-94
</p>

<hr>
<h2 id='gss_abortion'>Abortion Opinions in the General Social Survey</h2><span id='topic+gss_abortion'></span>

<h3>Description</h3>

<p>This is a toy data set derived from the General Social Survey that I intend
to use for several purposes. First, the battery of abortion items can serve as
toy data to illustrate mixed effects modeling as equivalent to a
one-parameter (Rasch) model. Second, I include some covariates to also do some
basic regressions. I think abortion opinions are useful learning tools for
statistical inference for college students. Third, there's a time-series component
as well for understanding how abortion attitudes have changed over time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gss_abortion
</code></pre>


<h3>Format</h3>

<p>A data frame with 64,814 observations on the following 18 variables.
</p>

<dl>
<dt><code>id</code></dt><dd><p>a unique respondent identifier</p>
</dd>
<dt><code>year</code></dt><dd><p>the survey year</p>
</dd>
<dt><code>age</code></dt><dd><p>the respondent's age in years</p>
</dd>
<dt><code>race</code></dt><dd><p>the respondent's race, as character variable</p>
</dd>
<dt><code>sex</code></dt><dd><p>the respondent's gender, as character variable</p>
</dd>
<dt><code>hispaniccat</code></dt><dd><p>the respondent's Hispanic ethnicity, as character variable</p>
</dd>
<dt><code>educ</code></dt><dd><p>how many years the respondent spent in school</p>
</dd>
<dt><code>partyid</code></dt><dd><p>the respondent's party identification, as character variable</p>
</dd>
<dt><code>relactiv</code></dt><dd><p>the self-reported religious activity of the respondent on a 1:11 scale</p>
</dd>
<dt><code>abany</code></dt><dd><p>a binary variable that equals 1 if the respondent thinks abortion
should be legal for any reason. 0 indicates no support for abortion for any reason.</p>
</dd>
<dt><code>abdefect</code></dt><dd><p>a numeric vector that equals 1 if the respondent thinks
abortion should be legal if there is a serious defect in the fetus.
0 indicates no support for abortion in this circumstance.</p>
</dd>
<dt><code>abnomore</code></dt><dd><p>a numeric vector that equals 1 if the respondent
thinks abortion should be legal if a woman is pregnant but wants no
more children. 0 indicates no support for abortion in this circumstance.</p>
</dd>
<dt><code>abhlth</code></dt><dd><p>a numeric vector that equals 1 if the respondent
thinks abortion should be legal if a pregnant woman's health is in danger.
0 indicates no support for abortion in this circumstance.</p>
</dd>
<dt><code>abpoor</code></dt><dd><p>a numeric vector that equals 1 if the respondent
thinks abortion should be legal if a pregnant woman is poor and cannot afford
more children. 0 indicates no support for abortion in this circumstance.</p>
</dd>
<dt><code>abrape</code></dt><dd><p>a numeric vector that equals 1 if the respondent thinks
abortion should be legal if the woman became pregnant because of a rape.
0 indicates no support for abortion in this circumstance.</p>
</dd>
<dt><code>absingle</code></dt><dd><p>a numeric vector that equals 1 if the respondent thinks
abortion should be legal if a pregnant woman is single and does not want to
marry the man who impregnated her. 0 indicates no support for abortion
in this circumstance.</p>
</dd>
<dt><code>pid</code></dt><dd><p><code>partyid</code> recoded so that 7 = NA</p>
</dd>
<dt><code>hispanic</code></dt><dd><p>a dummy variable that equals 1 if the respondent is any way Hispanic</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data include all General Social Survey observations from 1972 to 2018 for
these variables. Be mindful of missing data.
</p>

<hr>
<h2 id='gss_spending'>Attitudes Toward National Spending in the General Social Survey (2018)</h2><span id='topic+gss_spending'></span>

<h3>Description</h3>

<p>This is a toy data set that collects attitudes on toward national spending for
various things in the General Social Survey for 2018. I use these data for
in-class illustration about ordinal variables and ordinal models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gss_spending
</code></pre>


<h3>Format</h3>

<p>A data frame with 2348 observations on the following 33 variables.
</p>

<dl>
<dt><code>year</code></dt><dd><p>a numeric constant for the GSS survey year (2018)</p>
</dd>
<dt><code>id</code></dt><dd><p>a unique identifier for the survey respondent</p>
</dd>
<dt><code>age</code></dt><dd><p>a numeric vector for the age of the respondent (min: 18, max: 89)</p>
</dd>
<dt><code>sex</code></dt><dd><p>a numeric vector for the respondent's sex (1 = female, 0 = male)</p>
</dd>
<dt><code>educ</code></dt><dd><p>a numeric vector for the highest year of school completed (min: 0, max: 20)</p>
</dd>
<dt><code>degree</code></dt><dd><p>a numeric vector for the respondent's highest degree (0 = did not graduate high school, 1 = high school, 2 = junior college, 3 = bachelor degree, 4 = graduate degree)</p>
</dd>
<dt><code>race</code></dt><dd><p>a numeric vector for the respondent's race (1 = white, 2 = black, 3 = other)</p>
</dd>
<dt><code>rincom16</code></dt><dd><p>a numeric vector for the respondent's yearly income (min: 1 (under $1,000), max: 26 ($170,000 or over))</p>
</dd>
<dt><code>partyid</code></dt><dd><p>a numeric vector for the respondent's party identification on the familiar seven-point scale. NOTE: D to R partisanship in this variable goes from 0 to 6. 7 = supporters of other parties. You may want to recode this if you want an interval-level measure of partisanship.</p>
</dd>
<dt><code>polviews</code></dt><dd><p>a numeric vector for the respondent's ideology (min: 1 (extremely liberal), max: 7 (extremely conservative))</p>
</dd>
<dt><code>xnorcsiz</code></dt><dd><p>a numeric vector for the NORC size code. This is a measure of what kind of area in which the respondent took the survey (i.e. lives). 1 = city, greater than 250k residents. 2 = city, between 50k-250k residents. 3 = suburbs of a large city. 4 = suburbs of a medium-sized city. 5 = unincorporated area of a large city. 6 = unincorporated area of a medium city. 7 = city, between 10-50k residents. 8 = town, greater than 2,500 residents. 9 = smaller areas. 10 = open country.</p>
</dd>
<dt><code>news</code></dt><dd><p>a numeric vector for how often the respondent reads the newspapers. 1 = everyday. 2 = a few times a week. 3 = once a week. 4 = less than once a week. 5 = never.</p>
</dd>
<dt><code>wrkstat</code></dt><dd><p>a numeric vector for the respondent's work status. 1 = working full-time. 2 = working part-time. 3 = temporarily not working. 4 = unemployed/laid off. 5 = retired. 6 = in school. 7 = house-keeping work. 8 = other.</p>
</dd>
<dt><code>natspac</code></dt><dd><p>a numeric vector for attitudes toward spending on the space program. See details below for this variable and all other variables beginning with <code>nat</code>.</p>
</dd>
<dt><code>natenvir</code></dt><dd><p>a numeric vector for attitudes toward spending on improving/protecting the environment.</p>
</dd>
<dt><code>natheal</code></dt><dd><p>a numeric vector for attitudes toward spending on improving/protecting the nation's health.</p>
</dd>
<dt><code>natcity</code></dt><dd><p>a numeric vector for attitudes toward spending on solving the big city's problems.</p>
</dd>
<dt><code>natcrime</code></dt><dd><p>a numeric vector for attitudes toward spending on halting the &quot;rising crime rate.&quot; This question is subtly hilarious.</p>
</dd>
<dt><code>natdrug</code></dt><dd><p>a numeric vector for attitudes toward spending on dealing with drug addiction.</p>
</dd>
<dt><code>nateduc</code></dt><dd><p>a numeric vector for attitudes toward spending on improving the nation's education system.</p>
</dd>
<dt><code>natrace</code></dt><dd><p>a numeric vector for attitudes toward spending on improving the condition of black people.</p>
</dd>
<dt><code>natarms</code></dt><dd><p>a numeric vector for attitudes toward spending on the military/armaments/defense.</p>
</dd>
<dt><code>nataid</code></dt><dd><p>a numeric vector for attitudes toward spending on foreign aid.</p>
</dd>
<dt><code>natfare</code></dt><dd><p>a numeric vector for attitudes toward spending on welfare.</p>
</dd>
<dt><code>natroad</code></dt><dd><p>a numeric vector for attitudes toward spending on highways and bridges.</p>
</dd>
<dt><code>natsoc</code></dt><dd><p>a numeric vector for attitudes toward spending on social security.</p>
</dd>
<dt><code>natmass</code></dt><dd><p>a numeric vector for attitudes toward spending on mass transportation.</p>
</dd>
<dt><code>natpark</code></dt><dd><p>a numeric vector for attitudes toward spending on parks and recreation.</p>
</dd>
<dt><code>natchld</code></dt><dd><p>a numeric vector for attitudes toward spending on assistance for child care.</p>
</dd>
<dt><code>natsci</code></dt><dd><p>a numeric vector for attitudes toward spending on scientific research.</p>
</dd>
<dt><code>natenrgy</code></dt><dd><p>a numeric vector for attitudes toward spending on alternative sources of energy.</p>
</dd>
<dt><code>sumnat</code></dt><dd><p>a numeric vector for the sum total of responses to all the aforementioned spending variables (i.e. those that begin with <code>nat</code>). This creates an interval-ish measure with a nice and mostly normal distribution.</p>
</dd>
<dt><code>sumnatsoc</code></dt><dd><p>a numeric vector for the sum of all responses toward various &quot;social&quot; prompts (i.e. <code>natenvir</code>, <code>natheal</code>, <code>natdrug</code>, <code>nateduc</code>, <code>natrace</code>, <code>natfare</code>, <code>natroad</code>, <code>natmass</code>, <code>natpark</code>,  <code>natsoc</code>, <code>natchld</code>). This creates an interval-ish measure with a mostly normal (but small left skew) distribution.</p>
</dd>
</dl>



<h3>Details</h3>

<p>For all the variables beginning with <code>nat</code>, note that I rescaled the original data so that -1 = respondent thinks country is spending too much on this topic, 0 = respondent thinks country is spending &quot;about (the) right&quot; amount, and 1 = respondent thinks country is spending too little on this topic. I do this to facilitate reading each <code>nat</code> prompt as increasing support for more spending (the extent to which increasing values means the respondent thinks the country spends too little on a given prompt). I think this is more intuitive.
</p>
<p>Also, the <code>natspac</code>, <code>natenvir</code>, <code>natheal</code>, <code>natcity</code>, <code>natcrime</code>, <code>natdrug</code>, <code>nateduc</code>, <code>natrace</code>, <code>natarms</code>, <code>nataid</code>, and <code>natfare</code> have &quot;alternate&quot; prompts in later GSS waves in which a subset of respondents get a slightly different prompt. For example, one set of respondents for <code>natcity</code> gets a prompt of &quot;Solving the problems of the big cities&quot; (the legacy prompt) whereas another set of respondents gets a prompt of &quot;Assistance to big cities&quot; (typically noted as &quot;version y&quot; in the GSS). I, perhaps problematically if I were interested in publishing analyses on these data, combine both prompts into a single variable. I don't think it's a huge problem for what I want the data to do, but FYI.
</p>


<h3>Source</h3>

<p>General Social Survey, 2018
</p>

<hr>
<h2 id='gss_wages'>The Gender Pay Gap in the General Social Survey</h2><span id='topic+gss_wages'></span>

<h3>Description</h3>

<p>Wage data from the General Social Survey (1974-2018) to illustrate
wage discrepancies by gender (while also considering respondent occupation, age, and education).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gss_wages
</code></pre>


<h3>Format</h3>

<p>A data frame with 11 variables:
</p>

<dl>
<dt><code>year</code></dt><dd><p>the survey year</p>
</dd>
<dt><code>realrinc</code></dt><dd><p>the respondent's base income (in constant 1986 USD)</p>
</dd>
<dt><code>age</code></dt><dd><p>the respondent's age in years</p>
</dd>
<dt><code>occ10</code></dt><dd><p>respondent's occupation code (2010)</p>
</dd>
<dt><code>occrecode</code></dt><dd><p>recode of the occupation code into one of 11 main categories</p>
</dd>
<dt><code>prestg10</code></dt><dd><p>respondent's occupational prestige score (2010)</p>
</dd>
<dt><code>childs</code></dt><dd><p>number of children (0-8)</p>
</dd>
<dt><code>wrkstat</code></dt><dd><p>the work status of the respondent (full-time, part-time, temporarily not working, unemployed (laid off), retired, school, housekeeper, other)</p>
</dd>
<dt><code>gender</code></dt><dd><p>respondent's gender (male or female)</p>
</dd>
<dt><code>educcat</code></dt><dd><p>respondent's degree level (Less Than High School, High School, Junior College, Bachelor, or Graduate)</p>
</dd>
<dt><code>maritalcat</code></dt><dd><p>respondent's marital status (Married, Widowed, Divorced, Separated, Never Married)</p>
</dd>
</dl>



<h3>Details</h3>

<p>For further details, see <a href="https://gssdataexplorer.norc.org">https://gssdataexplorer.norc.org</a>. Consult <a href="https://census.gov">https://census.gov</a> for more information about occupation codes.
</p>

<hr>
<h2 id='Guber99'>School Expenditures and Test Scores for 50 States, 1994-95</h2><span id='topic+Guber99'></span>

<h3>Description</h3>

<p>A data set for a canonical case of a Simpson's paradox, useful for in-class
instruction on the topic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Guber99
</code></pre>


<h3>Format</h3>

<p>A data frame with 50 observations on the following 8 variables.
</p>

<dl>
<dt><code>state</code></dt><dd><p>a character vector for the state</p>
</dd>
<dt><code>expendpp</code></dt><dd><p>a numeric vector for the current expenditure per pupil in average daily attendance in public elementary and secondary schools, 1994-95 (in thousands of dollars)</p>
</dd>
<dt><code>ptratio</code></dt><dd><p>a numeric vector for the average pupil/teacher ratio in public elementary and secondary schools, Fall 1994</p>
</dd>
<dt><code>tsalary</code></dt><dd><p>a numeric vector for the estimated average annual salary of teachers in public elementary and secondary schools, 1994-95 (in thousands of dollars)</p>
</dd>
<dt><code>perctakers</code></dt><dd><p>a numeric vector for the percentage of all eligible students taking the SAT, 1994-95</p>
</dd>
<dt><code>verbal</code></dt><dd><p>a numeric vector for the average verbal SAT score, 1994-95</p>
</dd>
<dt><code>math</code></dt><dd><p>a numeric vector for the average math SAT score, 1994-95</p>
</dd>
<dt><code>total</code></dt><dd><p>a numeric vector for the average total SAT score, 1994-95</p>
</dd>
</dl>



<h3>References</h3>

<p>Guber, Deborah Lynne. 1999. &quot;Getting What You Pay For: The Debate Over Equity in Public School Expenditures.&quot; <em>Journal of Statistics Education</em> 7(2).
</p>

<hr>
<h2 id='illiteracy30'>Illiteracy in the Population 10 Years Old and Over, 1930</h2><span id='topic+illiteracy30'></span>

<h3>Description</h3>

<p>This is perhaps the canonical data set for illustrating the ecological fallacy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>illiteracy30
</code></pre>


<h3>Format</h3>

<p>A data frame with 40 observations on the following 11 variables.
</p>

<dl>
<dt><code>state</code></dt><dd><p>a character for the state</p>
</dd>
<dt><code>pop</code></dt><dd><p>a numeric vector for the total population</p>
</dd>
<dt><code>pop_il</code></dt><dd><p>a numeric vector for the total population that is illiterate</p>
</dd>
<dt><code>nwhite</code></dt><dd><p>a numeric vector for the total native white population</p>
</dd>
<dt><code>nwhite_il</code></dt><dd><p>a numeric vector for the total native white population that is illiterate</p>
</dd>
<dt><code>fpwhite</code></dt><dd><p>a numeric vector for the total white population with &quot;foreign or mixed parentage&quot;</p>
</dd>
<dt><code>fpwhite_il</code></dt><dd><p>a numeric vector for the total white population with &quot;foreign or mixed parentage&quot; that is illiterate</p>
</dd>
<dt><code>fbwhite</code></dt><dd><p>a numeric vector for the total foreign-born white population</p>
</dd>
<dt><code>fbwhite_il</code></dt><dd><p>a numeric vector for the total foreign-born white population that is illiterate</p>
</dd>
<dt><code>black</code></dt><dd><p>a numeric vector for the total black population.</p>
</dd>
<dt><code>black_il</code></dt><dd><p>a numeric vector for the total black population that is illiterate</p>
</dd>
</dl>



<h3>Details</h3>

<p>All population totals reflect those 10 years or older. The 1930 Census (along with Robinson (1950))
uses &quot;negro&quot; in lieu of black, but the variable names here eschew that older label. Note that some states
are not yet states in the 1930 Census.
</p>


<h3>Source</h3>

<p>U.S. Census Bureau (1933). Fifteenth Census of the United States: 1930. Population, Volume II.
</p>


<h3>References</h3>

<p>Grotenhuis, Manfred Te, Rob Eisinga, and SV Subramanian. 2011. &quot;Robinson's Ecological Correlations and the Behavior of Individuals: methodological corrections.&quot; <em>Internatoinal Journal of Epidemiology</em> 40(4): 1123-25.
</p>
<p>Robinson, WS. 1950. &quot;Ecological Correlations and the Behavior of Individuals.&quot; <em>American Sociological Review</em> 15(3): 351&ndash;57.
</p>

<hr>
<h2 id='inglehart03'>&quot;How Solid is Mass Support for Democracy&mdash;And How Can We Measure It?&quot;</h2><span id='topic+inglehart03'></span>

<h3>Description</h3>

<p>A data set based on summary information provided in Inglehart's (2003)
article in *PS: Political Science &amp; Politics*. These data would be from
the article itself and only indirectly from the raw World or European
Values Survey.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inglehart03
</code></pre>


<h3>Format</h3>

<p>A data frame with 77 observations on the following 4 variables.
</p>

<dl>
<dt><code>state_year</code></dt><dd><p>the state year and survey year, as provided in the article</p>
</dd>
<dt><code>havedem</code></dt><dd><p>the percentage of respondents saying having a democratic political system is &quot;very good&quot; or &quot;good&quot;</p>
</dd>
<dt><code>strongleader</code></dt><dd><p>the percentage of respondents saying having a strong leader unencumbered by elections or parliaments is &quot;very good&quot; or &quot;good&quot;</p>
</dd>
<dt><code>muslim</code></dt><dd><p>a dummy variable that equals 1 if Inglehart codes the state as being a &quot;predominently Islamic society&quot;</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data manually entered based on Table 1 and Table 2 in Inglehart's (2003) article.
</p>


<h3>References</h3>

<p>Inglehart, Ronald. 2003. &quot;How Solid is Mass Support for Democracy&mdash;And How Can We Measure It?&quot; *PS: Political Science &amp; Politics* 36(1): 51&ndash;57.
</p>

<hr>
<h2 id='Lipset59'>Democracy and Economic Development (Around) 1949-50</h2><span id='topic+Lipset59'></span>

<h3>Description</h3>

<p>A data set on democracy and economic development for 48 countries that Lipset
(1959) first described.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Lipset59
</code></pre>


<h3>Format</h3>

<p>A data frame with 48 observations on the following 11 variables.
</p>

<dl>
<dt><code>country</code></dt><dd><p>a character country for an English country name</p>
</dd>
<dt><code>cat</code></dt><dd><p>a category for the country by their region and level of democracy</p>
</dd>
<dt><code>iso3c</code></dt><dd><p>a three-character ISO code</p>
</dd>
<dt><code>wbgdp2011est</code></dt><dd><p>an estimated gross domestic product in 2011 USD</p>
</dd>
<dt><code>wbpopest</code></dt><dd><p>an estimated population size</p>
</dd>
<dt><code>unpop</code></dt><dd><p>a population size (in thousands)</p>
</dd>
<dt><code>uninc</code></dt><dd><p>a national income (in millions)</p>
</dd>
<dt><code>unincpc</code></dt><dd><p>a national income per capita</p>
</dd>
<dt><code>xm_qudsest</code></dt><dd><p>a &quot;Quick UDS&quot; estimate of democracy on a latent scale (see details)</p>
</dd>
<dt><code>v2x_polyarchy</code></dt><dd><p>the Varieties of Democracy &quot;polyarchy&quot; estimate (see details)</p>
</dd>
<dt><code>polity2</code></dt><dd><p>the <code>polity2</code> score from the Polity project (see details)</p>
</dd>
</dl>



<h3>Details</h3>

<p>The three variables with the prefix of <code>un</code> nominally come from the
United Nations Statistical Division for 1949/1950, but are actually retrieved
from Andic and Peacock (1961). Andic and Peacock (1961) note you should be
skeptical of Soviet-style calculations of national income and thus don't
include it in the data they make available.
</p>
<p>Anything else is explicitly benchmarked to 1950 as a referent year. The GDP
and population estimates come by way of Anders et al. (2020). You can manually
create your own GDP per capita variable here because the GDP is demarcated
in dollars and the population size is in units of 1. Take one and divide it
over the other.
</p>
<p>The democracy variables are all unique in their own way. The &quot;Quick UDS&quot;
estimates are generated to be latent and, globally, have a mean that
approximates 0 and a standard deviation that approximates 1. In the regression
context, that would mean a coefficient would communicate something like a
magnitude change across a standard deviation on the scale. The &quot;polyarchy&quot;
estimate has a theoretical minimum of 0 and a theoretical maximum of 1. In
the regression context, that would mean a coefficient communicates a min/max
effect. The Polity project estimate comes from a usual scale of -10 to 10 and
a regression coefficient communicates something much less exotic. It's a unit
change on this scale.
</p>
<p>In all cases, higher values of democracy = more &quot;democraticness&quot;, for lack
of a better term.
</p>


<h3>References</h3>

<p>Anders, Therese, Christopher J. Fariss, and Jonathan N. Markowitz. 2020.
&quot;Bread Before Guns or Butter: Introducing Surplus Domestic Product (SDP)&quot;
<em>International Studies Quarterly</em> 64(2): 392&ndash;405.
</p>
<p>Andic, Suphan and Alan T. Peacock. 1961. &quot;The International Distribution of
Income, 1949 and 1957.&quot; *Journal of the Royal Statistical Society*. Series A
(General) 124(2): 206-218.
</p>
<p>Coppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg,
Jan Teorell, David Altman, Michael Bernhard, M. Steven Fish, Adam Glynn,
Allen Hicken, Anna Luhrmann, Kyle L. Marquardt, Kelly McMann, Pamela
Paxton, Daniel Pemstein, Brigitte Seim, Rachel Sigman, Svend-Erik
Skaaning, Jeffrey Staton, Agnes Cornell, Lisa Gastaldi, Haakon Gjerlow,
Valeriya Mechkova, Johannes von Romer, Aksel Sundtrom, Eitan Tzelgov,
Luca Uberti, Yi-ting Wang, Tore Wig, and Daniel Ziblatt. 2020.
&quot;V-Dem Codebook v10&quot; Varieties of Democracy (V-Dem) Project.
</p>
<p>Lipset, Seymour Martin. 1959. &quot;Some Social Requisites of Democracy:
Economic Development and Political Legitimacy&quot; *American Political Science
Review* 53(1): 69-105.
</p>
<p>Marshall, Monty G., Ted Robert Gurr, and Keith Jaggers. 2017.
&quot;Polity IV Project: Political Regime Characteristics and Transitions,
1800-2017.&quot; Center for Systemic Peace.
</p>
<p>Marquez, Xavier, &quot;A Quick Method for Extending the Unified Democracy
Scores&quot; (March 23, 2016).  doi: <a href="https://doi.org/10.2139/ssrn.2753830">10.2139/ssrn.2753830</a>
</p>
<p>Pemstein, Daniel, Stephen Meserve, and James Melton. 2010. &quot;Democratic
Compromise: A Latent Variable Analysis of Ten Measures of Regime Type.&quot;
*Political Analysis* 18(4): 426-449.
</p>

<hr>
<h2 id='LOTI'>Land-Ocean Temperature Index, 1880-2022</h2><span id='topic+LOTI'></span>

<h3>Description</h3>

<p>These data contain monthly mean temperature anomalies expressed as deviations
from the corresponding 1951-1980 means. They are useful for showing
how we can measure climate change.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LOTI
</code></pre>


<h3>Format</h3>

<p>A data frame with 1,716 observations on the following 2 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>a date, mostly to contain information for the year and month</p>
</dd>
<dt><code>value</code></dt><dd><p>the mean temperature anomaly as deviation from corresponding 1951-1980 mean</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data are updated through most recent month, at least for last time I updated it.
Data represent combined land-surface air and sea-surface water temperature anomalies.
Of note: the day value in the <code>date</code> column has no real value.
It was just a way of combining data that are aggregated by year and month.
</p>


<h3>Source</h3>

<p>National Aeronautics and Space Administration's Goddard Institute for Space Studies.
</p>

<hr>
<h2 id='LTPT'>Long-Term Price Trends for Computers, TVs, and Related Items</h2><span id='topic+LTPT'></span>

<h3>Description</h3>

<p>These data are a monthly time-series of changes in the consumer price index
relative to a Dec. 1997 starting date for televisions, computers, and related
items. I use this as in-class illustration that globalization has made
consumer electronics cheaper across the board for Americans.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LTPT
</code></pre>


<h3>Format</h3>

<p>A data frame with 1,704 observations on the following 3 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>a date</p>
</dd>
<dt><code>category</code></dt><dd><p>the particular category (e.g. all items, televisions, etc.)</p>
</dd>
<dt><code>value</code></dt><dd><p>the consumer price index (Dec. 1997 = 100)</p>
</dd>
</dl>



<h3>Details</h3>

<p>This is a web-scraping job from the U.S. Bureau of Labor Statistics. Post is titled &quot;Long-term price trends for computers, TVs, and related items&quot; and was published on Oct. 13, 2015.
</p>


<h3>Source</h3>

<p>U.S. Bureau of Labor Statistics.
</p>

<hr>
<h2 id='LTWT'>&quot;Let Them Watch TV&quot;</h2><span id='topic+LTWT'></span>

<h3>Description</h3>

<p>&quot;Let Them Watch TV&quot;: These data contain price indices for various items for the general
urban consumer. Categories include medical services, college tuition, college textbooks,
child care, housing, food and beverages, all items (i.e. general CPI),
new vehicles, apparel, and televisions. The base period in value was originally
the 1982-4 average, but I converted the base period to January 2000. I use these
data for in-class discussion about how liberalized trade has made consumer
electronics (like TVs) fractions of their past prices. Yet, young adults face
mounting costs for college, child-raising, and health care that government
policy has failed to address.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LTWT
</code></pre>


<h3>Format</h3>

<p>A data frame with 2377 observations on the following 3 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>a date</p>
</dd>
<dt><code>category</code></dt><dd><p>a factor for the particular category</p>
</dd>
<dt><code>value</code></dt><dd><p>the price index. Base: January 2000</p>
</dd>
</dl>



<h3>Details</h3>

<p>Inspiration comes from a blog post titled &quot;Chart of the day (century?): Price changes 1997 to 2017&quot;, which was published by the American Enterprise Institute on Feb. 2, 2018.
</p>


<h3>Source</h3>

<p>Bureau of Labor Statistics, via the <code>blscrapeR</code> package.
</p>

<hr>
<h2 id='min_wage'>History of Federal Minimum Wage Rates Under the Fair Labor Standards Act, 1938-2009</h2><span id='topic+min_wage'></span>

<h3>Description</h3>

<p>A data set on the various federal minimum wage rates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>min_wage
</code></pre>


<h3>Format</h3>

<p>A data frame with 23 observations on the following 5 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>a date for when a new minimum wage was introduced</p>
</dd>
<dt><code>wage</code></dt><dd><p>the (nominal) value of the wage</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data come from the Department of Labor. Wages are taken from wage adjustments
from the 1938 act.
</p>


<h3>Source</h3>

<p>Department of Labor
</p>

<hr>
<h2 id='mm_mlda'>Minimum Legal Drinking Age Fatalities Data</h2><span id='topic+mm_mlda'></span>

<h3>Description</h3>

<p>These are data you can use to replicate the regression discontinuity design
analyses throughout Chapter 4 of <em>Mastering 'Metrics</em>.
Original analyses come from Carpenter and Dobkin (2009, 2011).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mm_mlda
</code></pre>


<h3>Format</h3>

<p>A data frame with 50 observations on the following 19 variables.
</p>

<dl>
<dt><code>agecell</code></dt><dd><p>a numeric</p>
</dd>
<dt><code>all</code></dt><dd><p>a numeric</p>
</dd>
<dt><code>allfitted</code></dt><dd><p>a numeric</p>
</dd>
<dt><code>internal</code></dt><dd><p>a numeric</p>
</dd>
<dt><code>internalfitted</code></dt><dd><p>a numeric</p>
</dd>
<dt><code>external</code></dt><dd><p>a numeric</p>
</dd>
<dt><code>externalfitted</code></dt><dd><p>a numeric</p>
</dd>
<dt><code>alcohol</code></dt><dd><p>a numeric</p>
</dd>
<dt><code>alcoholfitted</code></dt><dd><p>a numeric</p>
</dd>
<dt><code>homicide</code></dt><dd><p>a numeric</p>
</dd>
<dt><code>homicidefitted</code></dt><dd><p>a numeric</p>
</dd>
<dt><code>suicide</code></dt><dd><p>a numeric</p>
</dd>
<dt><code>suicidefitted</code></dt><dd><p>a numeric</p>
</dd>
<dt><code>mva</code></dt><dd><p>a numeric</p>
</dd>
<dt><code>mvafitted</code></dt><dd><p>a numeric</p>
</dd>
<dt><code>drugs</code></dt><dd><p>a numeric</p>
</dd>
<dt><code>drugsfitted</code></dt><dd><p>a numeric</p>
</dd>
<dt><code>externalother</code></dt><dd><p>a numeric</p>
</dd>
<dt><code>externalotherfitted</code></dt><dd><p>a numeric</p>
</dd>
</dl>



<h3>Details</h3>

<p>These data are not well-documented. You guys are on your own here. Good luck.
</p>


<h3>References</h3>

<p>Carpenter, Christopher and Carlos Dobkin. 2009. &quot;The Effect of Alcohol Consumption on Mortality: Regression Discontinuity Evidence from the Minimum Drinking Age&quot;. <em>American Economic Journal: Applied Economics</em> 1(1): 164&ndash;182.
</p>
<p>Carpenter, Christopher and Carloss Dobkin. 2011. &quot;The Minimum Legal Drinking Age and Public Health&quot;. <em>Journal of Economic Perspectives</em> 25(2): 133&ndash;156.
</p>

<hr>
<h2 id='mm_nhis'>Data from the 2009 National Health Interview Survey (NHIS)</h2><span id='topic+mm_nhis'></span>

<h3>Description</h3>

<p>These are data from the 2009 NHIS survey. People who have read
<em>Mastering 'Metrics</em> should recognize these data. They're
featured prominently in that book and the authors' discussion of
random assignment and experiments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mm_nhis
</code></pre>


<h3>Format</h3>

<p>A data frame with 18790 observations on the following 10 variables.
</p>

<dl>
<dt><code>fml</code></dt><dd><p>is the respondent a woman?</p>
</dd>
<dt><code>hi</code></dt><dd><p>a numeric vector for whether respondent has at least some health insurance</p>
</dd>
<dt><code>hlth</code></dt><dd><p>a numeric vector for a health index, broadly understood</p>
</dd>
<dt><code>nwhite</code></dt><dd><p>is the respondent not white?</p>
</dd>
<dt><code>age</code></dt><dd><p>the respondent's age in years</p>
</dd>
<dt><code>yedu</code></dt><dd><p>the respondent's total years of education</p>
</dd>
<dt><code>famsize</code></dt><dd><p>the size of the respondent's family</p>
</dd>
<dt><code>empl</code></dt><dd><p>is the respondent employed</p>
</dd>
<dt><code>inc</code></dt><dd><p>the respondent's household/family income</p>
</dd>
<dt><code>perweight</code></dt><dd><p>a numeric vector for weight</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data are already cleaned in a way that facilitates an easy
replication of Table 1.1 in <em>Mastering 'Metrics</em>. Check
the book's website for more information.
</p>


<h3>Source</h3>

<p>National Health Interview Survey (2009).
</p>

<hr>
<h2 id='mm_randhie'>Data from the RAND Health Insurance Experiment (HIE)</h2><span id='topic+mm_randhie'></span>

<h3>Description</h3>

<p>These are data from the RAND Health Insurance Experiment (HIE).
People who have read <em>Mastering 'Metrics</em> should recognize these data. They're
featured prominently in that book and the authors' discussion of
random assignment and experiments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mm_randhie
</code></pre>


<h3>Format</h3>

<p>The data are a list of two data frames (or &quot;tibbles&quot;). The first is the baseline data.
</p>

<dl>
<dt><code>plantype</code></dt><dd><p>the plan coverage of the respondent, as a factor</p>
</dd>
<dt><code>age</code></dt><dd><p>the age of the respondent</p>
</dd>
<dt><code>blackhisp</code></dt><dd><p>whether the respondent is not white</p>
</dd>
<dt><code>cholest</code></dt><dd><p>the cholesterol level of the respondent (in <code>mg/dl</code>)</p>
</dd>
<dt><code>educper</code></dt><dd><p>the education-level of the respondent</p>
</dd>
<dt><code>female</code></dt><dd><p>whether the respondent is a woman</p>
</dd>
<dt><code>ghindx</code></dt><dd><p>a general health index</p>
</dd>
<dt><code>hosp</code></dt><dd><p>was the respondent hospitalized last year?</p>
</dd>
<dt><code>income1cpi</code></dt><dd><p>the family/household income of the respondent, adjusted for inflation</p>
</dd>
<dt><code>mhi</code></dt><dd><p>a mental health index</p>
</dd>
<dt><code>systol</code></dt><dd><p>the systolic blood pressure level of the respondent (in <code>mm HG</code>)</p>
</dd>
</dl>

<p>The second is the outcome data.
</p>

<dl>
<dt><code>plantype</code></dt><dd><p>the plan coverage of the respondent, as a factor</p>
</dd>
<dt><code>ftf</code></dt><dd><p>the number of face-to-face visits for the respondent</p>
</dd>
<dt><code>out_inf</code></dt><dd><p>the total of out-patient expenses for the respondent</p>
</dd>
<dt><code>totadm</code></dt><dd><p>the number of hospital admissions for the respondent</p>
</dd>
<dt><code>tot_inf</code></dt><dd><p>the total health expenses for the respondent</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data are already cleaned in a way that facilitates an easy
replication of Table 1.3 and a partial replication of
Table 1.4 in <em>Mastering 'Metrics</em>. Check
the book's website for more information. I want to note
that my treatment of the data leans heavily on Jeff Arnold's
treatment of it. Check <a href="https://jrnold.github.io/masteringmetrics/">https://jrnold.github.io/masteringmetrics/</a> for
more information. Future updates to the data may pursue a more exhaustive
replication. I will only note these data are a mess and the authors of
<em>Mastering 'Metrics</em> do not do a great job annotating code.
</p>


<h3>Source</h3>

<p>RAND Health Insurance Experiment.
</p>

<hr>
<h2 id='mvprod'>Motor Vehicle Production by Country, 1950-2019</h2><span id='topic+mvprod'></span>

<h3>Description</h3>

<p>Data, largely from Organisation Internationale des Constructeurs d'Automobiles (OICA),
on motor vehicle production in various countries (and the world totals) from 1950 to
2019 at various intervals. Tallies include production of passenger cars, light
commercial vehicles, minibuses, trucks, buses and coaches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mvprod
</code></pre>


<h3>Format</h3>

<p>A data frame with three variables
</p>

<dl>
<dt><code>country</code></dt><dd><p>the country's name</p>
</dd>
<dt><code>year</code></dt><dd><p>the year</p>
</dd>
<dt><code>value</code></dt><dd><p>the total motor vehicles produced that year</p>
</dd>
</dl>



<h3>Details</h3>

<p>This is a Wikipedia web-scraping job. See:
<a href="https://en.wikipedia.org/wiki/List_of_countries_by_motor_vehicle_production">https://en.wikipedia.org/wiki/List_of_countries_by_motor_vehicle_production</a>
</p>


<h3>Source</h3>

<p>Organisation Internationale des Constructeurs d'Automobiles (OICA)
</p>

<hr>
<h2 id='nesarc_drinkspd'>The Usual Daily Drinking Habits of Americans (NESARC, 2001-2)</h2><span id='topic+nesarc_drinkspd'></span>

<h3>Description</h3>

<p>This toy data set is loosely modified from Wave I of the NESARC data set.
Here, my main interest is the number of drinks consumed on a usual day drinking
alcohol in the past 12 months, according to respondents in the nationally
representative survey of 43,093 Americans.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nesarc_drinkspd
</code></pre>


<h3>Format</h3>

<p>A data frame with 43093 observations on the following 8 variables.
</p>

<dl>
<dt><code>idnum</code></dt><dd><p>a numeric vector and sequence from 1 to the number of rows in the data</p>
</dd>
<dt><code>ethrace2a</code></dt><dd><p>a numeric vector for the ethnicity/race. 1 = White, not Hispanic. 2 = Black, not Hispanic. 3 = AI/AN. 4 = Asian, Native Hawaiian, Pacific Islander. 5 = Hispanic or Latino.</p>
</dd>
<dt><code>region</code></dt><dd><p>a numeric vector for the Census region. 1 = Northeast. 2 = Midwest. 3 = South. 4 = West</p>
</dd>
<dt><code>age</code></dt><dd><p>a numeric vector for age in years</p>
</dd>
<dt><code>sex</code></dt><dd><p>a numeric vector for sex. 1 = female. 0 = male</p>
</dd>
<dt><code>marital</code></dt><dd><p>a numeric vector for marital status. 1 = married. 2 = living with someone as married. 3 = widowed. 4 = divorced. 5 = separated. 6 = never married</p>
</dd>
<dt><code>educ</code></dt><dd><p>a numeric vector for education level, recoded from <code>s1q6a</code> in the original data. 1 = did not make it to/finish high school. 2 = high school graduate or equivalency. 3 = some college, but no four-year degree. 4 = four-year college degree or more.</p>
</dd>
<dt><code>s2aq8b</code></dt><dd><p>a numeric vector for the number of drinks of any alcohol consumed on days drinking alcohol in the past 12 months. This variable is &ldquo;as-is&rdquo; from the original data set.</p>
</dd>
</dl>



<h3>Details</h3>

<p>You will not want to use the <code>s2aq8b</code> variable without recoding it first.
Those who cannot recall how much they typically drink (i.e. true &ldquo;don't knows&rdquo;
or missing info) are coded as 99. Non-drinkers are coded as <code>NA</code> in the <code>s2aq8b</code>
variable and should be recoded as 0. Any value between 1 and 98 in the variable represents the,
for lack of better term, &ldquo;true&rdquo; number of alcoholic drinks a respondent says s/he typically
consumes on a day drinking alcohol in the past 12 months, though this is evidently preposterous
as a count variable. A person drinking 42 alcoholic drinks a day would not be alive to tell you
they did this. The researcher may want to employ some sensible right censoring here.
</p>


<h3>Source</h3>

<p>National Epidemiologic Survey on Alcohol and Related Conditions (NESARC)—Wave 1 (2001–2002)
</p>

<hr>
<h2 id='Newhouse77'>Medical-Care Expenditure: A Cross-National Survey (Newhouse, 1977)</h2><span id='topic+Newhouse77'></span>

<h3>Description</h3>

<p>These are the data in Newhouse's (1977) simple OLS model from 1977. In
his case, he's trying to explain medical care expenditures as a function of GDP
per capita for these countries. It's probably the easiest OLS model I can find
in print because Newhouse helpfully provides all the data in one simple table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Newhouse77
</code></pre>


<h3>Format</h3>

<p>A data frame with 13 observations on the following 5 variables.
</p>

<dl>
<dt><code>country</code></dt><dd><p>a character vector for the country</p>
</dd>
<dt><code>year</code></dt><dd><p>a numeric vector for the year</p>
</dd>
<dt><code>gdppc</code></dt><dd><p>a numeric vector for the per capita GDP in USD</p>
</dd>
<dt><code>medsharegdp</code></dt><dd><p>a numeric vector for the medical care share as percentage of GDP</p>
</dd>
<dt><code>medexppc</code></dt><dd><p>a numeric vector for per capita medical care expenditure (in USD)</p>
</dd>
</dl>



<h3>Details</h3>

<p>Table 1 in Newhouse (1977) is well-annotated with background information.
</p>


<h3>References</h3>

<p>Newhouse, Joseph P. 1977. &quot;Medical-Care Expenditure: A Cross-National Survey.&quot; <em>Journal of Human Resources</em> 12(1): 115-125.
</p>

<hr>
<h2 id='ODGI'>Ozone Depleting Gas Index Data, 1992-2022</h2><span id='topic+ODGI'></span>

<h3>Description</h3>

<p>The NOAA Earth System Research Laboratory has an &quot;ozone depleting gas index&quot;
(ODGI) data set from 1992 to 2018. This dataset summarizes Table 1 and Table 2 from its website.
The primary interest here (for my purposes) is the ODGI indices
(including the new 2012 measure). The data set includes constituent greenhouse
gases/chlorines as well in parts per trillion. The primary use here is for
in-class illustration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ODGI
</code></pre>


<h3>Format</h3>

<p>A data frame with 62 observations on the following 16 variables.
</p>

<dl>
<dt><code>year</code></dt><dd><p>the year</p>
</dd>
<dt><code>cat</code></dt><dd><p>categorical variable for the Antarctic or Mid-Latitudes measurements</p>
</dd>
<dt><code>cfc12</code></dt><dd><p>CFC-12 concentration in parts per trillion</p>
</dd>
<dt><code>cfc11</code></dt><dd><p>CFC-11 concentration in parts per trillion</p>
</dd>
<dt><code>ch3cl</code></dt><dd><p>chloromethane concentration in parts per trillion</p>
</dd>
<dt><code>ch3br</code></dt><dd><p>bromomethane concentration in parts per trillion</p>
</dd>
<dt><code>ccl4</code></dt><dd><p>carbon tetrachloride concentration in parts per trillion</p>
</dd>
<dt><code>ch3ccl3</code></dt><dd><p>methyl chloroform concentration in parts per trillion</p>
</dd>
<dt><code>halons</code></dt><dd><p>aggregate concentration in parts per trillion of H-1211, H-1301 and H-2402</p>
</dd>
<dt><code>cfc113</code></dt><dd><p>trichlorotrifluoroethane concentration in parts per trillion</p>
</dd>
<dt><code>hcfcs</code></dt><dd><p>aggregate concentration in parts per trillion of HCFC-22, HCFC-141b, and HCFC-142b</p>
</dd>
<dt><code>wmo_minor</code></dt><dd><p>aggregate concentration in parts per trillion of CFC-114, CFC-115, halon 2402 and halon 1201</p>
</dd>
<dt><code>sum</code></dt><dd><p>the sum of all greenhouse gas concentration measurements</p>
</dd>
<dt><code>eesc</code></dt><dd><p>includes consideration of lag times for transport and mixing associated with transport. New as of 2012</p>
</dd>
<dt><code>odgi_old</code></dt><dd><p>old greenhouse gas index, no longer supported as of 2012</p>
</dd>
<dt><code>odgi_new</code></dt><dd><p>new greenhouse gas index, as of 2012</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="https://gml.noaa.gov/odgi/">https://gml.noaa.gov/odgi/</a>
</p>

<hr>
<h2 id='OODTPT'>Data for &quot;Optimal Obfuscation: Democracy and Trade Policy Transparency&quot;</h2><span id='topic+OODTPT'></span>

<h3>Description</h3>

<p>A data set for replicating an argument about the relationship between democracy
and tariffs/non-tariff trade barriers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>OODTPT
</code></pre>


<h3>Format</h3>

<p>A data frame with 75 observations on the following 16 variables.
</p>

<dl>
<dt><code>country</code></dt><dd><p>a character vector for the country</p>
</dd>
<dt><code>isocode</code></dt><dd><p>a character vector for the three-character ISO code of the country</p>
</dd>
<dt><code>tariff</code></dt><dd><p>the mean statutory most favored nation tariff rate</p>
</dd>
<dt><code>corecov</code></dt><dd><p>the core non-tariff barrier coverage ratio</p>
</dd>
<dt><code>qualcov</code></dt><dd><p>the quality non-tariff barrier coverate ratio</p>
</dd>
<dt><code>polity</code></dt><dd><p>the familiar [-10,10] Polity measure of democracy</p>
</dd>
<dt><code>iec</code></dt><dd><p>the index of electorcal competitiveness from the World Bank</p>
</dd>
<dt><code>lngdppc</code></dt><dd><p>real GDP per capita in 1995 dollars</p>
</dd>
<dt><code>lngdp</code></dt><dd><p>real GDP in 1995 dollars</p>
</dd>
<dt><code>lnexpgdp</code></dt><dd><p>export dependence (i.e. export/GDP ratio)</p>
</dd>
<dt><code>reer</code></dt><dd><p>real effective exchange rate</p>
</dd>
<dt><code>growth</code></dt><dd><p>GDP per capita growth rate</p>
</dd>
<dt><code>dimpgdp</code></dt><dd><p>the change in the import/GDP ratio over the past three years</p>
</dd>
<dt><code>lngovcons</code></dt><dd><p>the log of country's government consumption spending as a percentage of GDP</p>
</dd>
<dt><code>gatt</code></dt><dd><p>a dummy variable for GATT membership</p>
</dd>
<dt><code>avgtar</code></dt><dd><p>the country's average most favored nation tariff rate</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data downloaded Joshua Alley's Github repository on simple cross-sectional
OLS models. These were originally two separate Stata files that I merged into one.
Please read the Kono (2006) article for more information.
</p>


<h3>References</h3>

<p>Kono, Daniel. 2006. &quot;Optimal Obfuscation: Democracy and Trade Policy Transparency&quot;
*American Political Science Review* 100(3): 369-384.
</p>

<hr>
<h2 id='PPGE'>Partisan Politics in the Global Economy</h2><span id='topic+PPGE'></span>

<h3>Description</h3>

<p>A data set on government spending in select rich countries as a function of
trade/GDP, financial openness, and the state-year-level engagement in trade
unions. The data offer a means to quasi-replicate Garrett's (1998) argument
about left-wing governments' ability to stem the tide of globalization's
effect on decreased government spending.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PPGE
</code></pre>


<h3>Format</h3>

<p>A data frame with the following 9 variables.
</p>

<dl>
<dt><code>country</code></dt><dd><p>a character vector for the country</p>
</dd>
<dt><code>iso3c</code></dt><dd><p>a character vector for the three-character country ISO code</p>
</dd>
<dt><code>year</code></dt><dd><p>the year</p>
</dd>
<dt><code>govtspendgdp</code></dt><dd><p>total government spending over GDP</p>
</dd>
<dt><code>tradegdp</code></dt><dd><p>the volume of trade over GDP</p>
</dd>
<dt><code>kaopen</code></dt><dd><p>an index measuring a country's degree of capital account openness</p>
</dd>
<dt><code>ka_open</code></dt><dd><p>an alternate index measuring a country's degree of capital account openness, normalized to be between 0 and 1</p>
</dd>
<dt><code>v2catrauni</code></dt><dd><p>an estimate of a country's engagement in independent trade unions, generated by way of a Bayesian item response model</p>
</dd>
<dt><code>v2catrauni_ord</code></dt><dd><p>an estimate of a country's engagement in independent trade unions, on ordinal scale. See details.</p>
</dd>
</dl>



<h3>Details</h3>

<p>The data are an unbalanced panel because of data missingness primarily
affecting Switzerland (which would only appear in the panel in earnest starting
in the mid-1990s). The Netherlands has some missing data in the mid-1970s. Spain
and Portugal appear at the start of the panel, though the transition to democracy
for both wouldn't start until 1974/1975. The data also have some obvious
COVID weirdness for 2020. Perhaps an honest re-assessment of Garrett (1998)
may want to drop Switzerland altogether, ignore 2020, and lop a few years off
the Spanish and Portuguese panel. What you do with the Netherlands is up to you.
</p>
<p>Briefly: the government spending/GDP data come from the International
Monetary Fund. The trade/GDP data come from the World Bank's API.
The financial openness indicators come by way of the Chinn-Ito index. The
engagement in trade unions data are from the Varieties of Democracy project.
The ordinal measure of the trade union estimates communicate what percentage
of the population is active in independent trade unions. Values include 0)
virtually no one 1) a small share of the population (less than 5
moderate share of the population (about 5 to 15
the population (about 16 
(about 26
</p>


<h3>References</h3>

<p>Coppedge, Michael, John Gerring, Carl Henrik Knutsen, Staffan I. Lindberg, Jan Teorell,
Nazifa Alizada, David Altman, Michael Bernhard, Agnes Cornell, M. Steven Fish, Lisa
Gastaldi, Haakon Gjerløw, Adam Glynn, Sandra Grahn, Allen Hicken, Garry Hindle, Nina
Ilchenko, Katrin Kinzelbach, Joshua Krusell, Kyle L. Marquardt, Kelly McMann, Valeriya
Mechkova, Juraj Medzihorsky, Pamela Paxton, Daniel Pemstein, Josefine Pernes, Oskar
Rydén, Johannes von Römer, Brigitte Seim, Rachel Sigman, Svend-Erik Skaaning, Jeffrey
Staton, Aksel Sundström, Eitan Tzelgov, Yi-ting Wang, Tore Wig, Steven Wilson and Daniel
Ziblatt. 2022. &quot;V-Dem [Country-Year/Country-Date] Dataset v12&quot; Varieties of Democracy
(V-Dem) Project. doi: <a href="https://doi.org/10.23696/vdemds22">10.23696/vdemds22</a>
</p>
<p>Chinn, Menzie D. and Hiro Ito. 2006. &quot;What Matters for Financial Development?
Capital Controls, Institutions, and Interactions.&quot; *Journal of Development
Economics* 81(1): 163&ndash;192.
</p>
<p>Garrett, Geoffrey. 1998. *Partisan Politics in the Global Economy* New York,
NY: Cambridge University Press.
</p>

<hr>
<h2 id='PRDEG'>Property Rights, Democracy, and Economic Growth</h2><span id='topic+PRDEG'></span>

<h3>Description</h3>

<p>A data set for replicating David Leblang's (1996) analysis on property rights,
democracy, and economic growth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PRDEG
</code></pre>


<h3>Format</h3>

<p>A data frame with 147 observations on the following 10 variables.
</p>

<dl>
<dt><code>levine</code></dt><dd><p>a numeric vector that servies as a cross-section identifier</p>
</dd>
<dt><code>country</code></dt><dd><p>a character vector for the country</p>
</dd>
<dt><code>decade</code></dt><dd><p>a numeric vector for a decade</p>
</dd>
<dt><code>private</code></dt><dd><p>a numeric vector for credit allocated to private enterprise</p>
</dd>
<dt><code>rgdp</code></dt><dd><p>a numeric vector for the initial level of real per capita GDP</p>
</dd>
<dt><code>democ</code></dt><dd><p>a numeric vector for the level of democracy</p>
</dd>
<dt><code>pri</code></dt><dd><p>a numeric vector for primary school attainment</p>
</dd>
<dt><code>sec</code></dt><dd><p>a numeric vector for secondary school attainment</p>
</dd>
<dt><code>grow</code></dt><dd><p>a numeric vector for per capita growth rate</p>
</dd>
<dt><code>xcontrol</code></dt><dd><p>a numeric vector for exchange controls</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data come Joshua Alley's Github repository on cross-sectional OLS
regressions. Please read David Leblang's (1996) article for some more detail
about the variables included in the model.
</p>


<h3>References</h3>

<p>Leblang, David. 1996. &quot;Property Rights, Democracy, and Economic Growth.&quot; 49(1): 5-26.
</p>

<hr>
<h2 id='Presidents'>U.S. Presidents and Their Terms in Office</h2><span id='topic+Presidents'></span>

<h3>Description</h3>

<p>This should be self-evident. Here are all U.S. presidents who have completed
their terms in office (i.e. excluding the current one).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Presidents
</code></pre>


<h3>Format</h3>

<p>A data frame with 45 observations on the following 3 variables.
</p>

<dl>
<dt><code>president</code></dt><dd><p>the president</p>
</dd>
<dt><code>start</code></dt><dd><p>the start date of the term, as a date</p>
</dd>
<dt><code>end</code></dt><dd><p>the end date of the term, as a date</p>
</dd>
</dl>



<h3>Details</h3>

<p>I scraped this from <a href="https://www.presidentsusa.net/presvplist.html">https://www.presidentsusa.net/presvplist.html</a>.
Data frame is capital-P &quot;Presidents&quot; to avoid a conflict with the
<code>presidents</code> data frame from the <code>datasets</code> package.
</p>

<hr>
<h2 id='pwt_sample'>Penn World Table (10.0) Macroeconomic Data for Select Countries, 1950-2019</h2><span id='topic+pwt_sample'></span>

<h3>Description</h3>

<p>These are some macroeconomic data for 21 select (rich) countries. I've used
these data before to discuss issues of grouping and skew in cross-sectional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pwt_sample
</code></pre>


<h3>Format</h3>

<p>A data frame with 1470 observations on the following 11 variables.
</p>

<dl>
<dt><code>country</code></dt><dd><p>the country name</p>
</dd>
<dt><code>isocode</code></dt><dd><p>The country's ISO code</p>
</dd>
<dt><code>year</code></dt><dd><p>a numeric vector for the year</p>
</dd>
<dt><code>pop</code></dt><dd><p>Population in millions</p>
</dd>
<dt><code>hc</code></dt><dd><p>Index of human capital per person, based on years of schooling and returns to education</p>
</dd>
<dt><code>rgdpna</code></dt><dd><p>Real GDP at constant 2011 national prices (in million 2017 USD)</p>
</dd>
<dt><code>rgdpo</code></dt><dd><p>Output-side real GDP at chained PPPs (in million 2017 USD)</p>
</dd>
<dt><code>rgdpe</code></dt><dd><p>Expenditure-side real GDP at chained PPPs (in million 2017 USD)</p>
</dd>
<dt><code>labsh</code></dt><dd><p>Share of labor compensation in GDP at current national prices</p>
</dd>
<dt><code>avh</code></dt><dd><p>Average annual hours worked by persons engaged.</p>
</dd>
<dt><code>emp</code></dt><dd><p>Number of persons engaged (in millions)</p>
</dd>
<dt><code>rnna</code></dt><dd><p>Capital stock at constant 2017 national prices (in million 2017 USD)</p>
</dd>
</dl>



<h3>Source</h3>

<p>Taken from the <code>pwt10</code> package. See: <a href="https://www.rug.nl/ggdc/">https://www.rug.nl/ggdc/</a>
</p>

<hr>
<h2 id='quartets'>Anscombe's (1973) Quartets</h2><span id='topic+quartets'></span>

<h3>Description</h3>

<p>These are four x-y data sets, combined into a long format, which have the
same traditional statistical properties (mean, variance, correlation,
regression line, etc.). However, they look quite different.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quartets
</code></pre>


<h3>Format</h3>

<p>A data frame with 44 observations on the following 3 variables.
</p>

<dl>
<dt><code>group</code></dt><dd><p>a categorical identifier for the quartet</p>
</dd>
<dt><code>x</code></dt><dd><p>a continuous variable</p>
</dd>
<dt><code>y</code></dt><dd><p>a continuous variable</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data come default in R, but I elected to change the format to be
a bit more accessible.
</p>


<h3>References</h3>

<p>Anscombe, Francis J. (1973). &quot;Graphs in Statistical Analysis.&quot; <em>The American Statistician</em> 27: 17–21.
</p>

<hr>
<h2 id='recessions'>United States Recessions, 1855-present</h2><span id='topic+recessions'></span>

<h3>Description</h3>

<p>Data on U.S. recessions, past to present. Data include information on contraction,
expansion, and cycle.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>recessions
</code></pre>


<h3>Format</h3>

<p>A data frame with 35 observations on the following 8 variables.
</p>

<dl>
<dt><code>peak</code></dt><dd><p>the year-month of the peak, as a date</p>
</dd>
<dt><code>trough</code></dt><dd><p>the year-month of the trough, as a date</p>
</dd>
<dt><code>peakq</code></dt><dd><p>the peak quarter</p>
</dd>
<dt><code>troughq</code></dt><dd><p>the trough quarter</p>
</dd>
<dt><code>p2t</code></dt><dd><p>peak to trough (in months)</p>
</dd>
<dt><code>prev_t2p</code></dt><dd><p>previous trough to this peak (in months)</p>
</dd>
<dt><code>tfpt</code></dt><dd><p>trough from previous trough (in months)</p>
</dd>
<dt><code>pfpp</code></dt><dd><p>peak from previous peak (in months)</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data come from via scraping job of <a href="https://www.nber.org/research/data/us-business-cycle-expansions-and-contractions">https://www.nber.org/research/data/us-business-cycle-expansions-and-contractions</a>
</p>


<h3>Source</h3>

<p>National Bureau of Economic Research (NBER)
</p>

<hr>
<h2 id='SBCD'>Systemic Banking Crises Database II</h2><span id='topic+SBCD'></span>

<h3>Description</h3>

<p>A data set on banking, currency, debt, and debt-restructuring crises from
1970 to 2017.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SBCD
</code></pre>


<h3>Format</h3>

<p>A data frame with 574 observations on the following 4 variables.
</p>

<dl>
<dt><code>country</code></dt><dd><p>the country, as it appears in the data</p>
</dd>
<dt><code>type</code></dt><dd><p>the type of crisis, entered here as &quot;banking&quot;, &quot;currency&quot;, &quot;debt&quot;, or &quot;debtrestructuring&quot;</p>
</dd>
<dt><code>year</code></dt><dd><p>the year of the crisis</p>
</dd>
<dt><code>month</code></dt><dd><p>the month the crisis started, if known</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data are cobbled from the second and third sheets of the spreadsheet
the authors provide. Country names are as entered in their spreadsheet.
Liberia has an &quot;NA&quot; in the raw data for sovereign debt restructuring and I
don't know why. I elect to keep it.
</p>


<h3>References</h3>

<p>Laeven, Luc and Fabian Valencia. 2020. &quot;Systemic Banking Crises
Database II&quot;. *IMF Economic Review* 68: 307&ndash;361.
</p>

<hr>
<h2 id='scb_regions'>Region Codes in the Central Bureau of Statistics (&quot;Statistiska centralbyrån&quot;) in Sweden</h2><span id='topic+scb_regions'></span>

<h3>Description</h3>

<p>This is a simple data set for matching region codes to the names of
territorial units in Sweden, at least recorded/cataloged by the Central
Bureau of Statistics in Sweden.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scb_regions
</code></pre>


<h3>Format</h3>

<p>A data frame with 312 observations on the following 2 variables.
</p>

<dl>
<dt><code>region</code></dt><dd><p>an intuitive name for a territorial unit/&quot;region&quot; in Sweden</p>
</dd>
<dt><code>region_code</code></dt><dd><p>an alpha-numeric code coinciding with the territorial unit/&quot;region&quot;</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data were manually derived from first gathering everything the Central Bureau
of Statistics had to offer. Its intended use is alongside the <span class="pkg">pxweb</span>
package. May it allow for more focused uses of the package without having
to rely on the interactive component to do all the heavy-lifting.
</p>

<hr>
<h2 id='SCP16'>South Carolina County GOP/Democratic Primary Data, 2016</h2><span id='topic+SCP16'></span>

<h3>Description</h3>

<p>County-level data on vote share and various background/demographic
information for the 2016 South Carolina GOP/Democratic primaries.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SCP16
</code></pre>


<h3>Format</h3>

<p>A data frame with 46 observations on the following 15 variables.
</p>

<dl>
<dt><code>county</code></dt><dd><p>the county</p>
</dd>
<dt><code>clinton</code></dt><dd><p>Hillary Clinton's county-level vote share in the 2016 party primary</p>
</dd>
<dt><code>sanders</code></dt><dd><p>Bernie Sanders' county-level vote share in the 2016 party primary</p>
</dd>
<dt><code>trump</code></dt><dd><p>Donald Trump's county-level vote share in the 2016 party primary</p>
</dd>
<dt><code>cruz</code></dt><dd><p>Ted Cruz' county-level vote share in the 2016 party primary</p>
</dd>
<dt><code>rubio</code></dt><dd><p>Marco Rubio's county-level vote share in the 2016 party primary</p>
</dd>
<dt><code>percapinc</code></dt><dd><p>A county-level estimate for per capita income</p>
</dd>
<dt><code>medhouseinc</code></dt><dd><p>A county-level estimate for the median household income</p>
</dd>
<dt><code>medfaminc</code></dt><dd><p>A county-level estimate for the median family income</p>
</dd>
<dt><code>illiteracy</code></dt><dd><p>An estimate of the percent of the county lacking &quot;basic&quot; prose literacy skills</p>
</dd>
<dt><code>perblack</code></dt><dd><p>Percentage of the county that is black</p>
</dd>
<dt><code>population</code></dt><dd><p>An estimate of the county-level population</p>
</dd>
<dt><code>romneyshare2012</code></dt><dd><p>Mitt Romney's vote share at the county-level from the 2012 general election</p>
</dd>
<dt><code>perhsgrad</code></dt><dd><p>Percentage of the county whose residents 25 years and older have at least a high school education</p>
</dd>
<dt><code>unemployment</code></dt><dd><p>Unemployment rate for the county for January 2016</p>
</dd>
</dl>



<h3>Details</h3>

<p>The illiteracy estimate comes from a Department of Education report from 2003.
The unemployment rate data come from the Bureau of Labor Statistics. A Github repository contains
more information: <a href="https://github.com/svmiller/sc-primary-2016">https://github.com/svmiller/sc-primary-2016</a>.
</p>

<hr>
<h2 id='sealevels'>Global Average Absolute Sea Level Change, 1880–2015</h2><span id='topic+sealevels'></span>

<h3>Description</h3>

<p>These data describe how sea level has changed over time, in both
relative and absolute terms. Absolute sea level change refers to
the height of the ocean surface regardless of whether nearby
land is rising or falling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sealevels
</code></pre>


<h3>Format</h3>

<p>A data frame with 136 observations on the following 5 variables.
</p>

<dl>
<dt><code>year</code></dt><dd><p>the year</p>
</dd>
<dt><code>adjlev</code></dt><dd><p>adjusted sea level (in inches)</p>
</dd>
<dt><code>lb</code></dt><dd><p>the lower bound of the estimate (in inches)</p>
</dd>
<dt><code>ub</code></dt><dd><p>the upper bound of the estimate (in inches)</p>
</dd>
<dt><code>adjlev_noaa</code></dt><dd><p>NOAA's adjusted sea level (in inches)</p>
</dd>
</dl>



<h3>Source</h3>

<p>Environmental Protection Agency (&quot;Climate Change Indicators: Sea Level&quot;)
</p>


<h3>References</h3>

<p>CSIRO (Commonwealth Scientific and Industrial Research Organisation).
2015 update to data originally published in: Church, J.A., and N.J. White. 2011.
Sea-level rise from the late 19th to the early 21st century. Surv. Geophys.
32:585–602.
</p>
<p>NOAA (National Oceanic and Atmospheric Administration). 2016.
Laboratory for Satellite Altimetry: Sea level rise. Accessed June 2016.
</p>

<hr>
<h2 id='so2concentrations'>Sulfur Dioxide Emissions, 1980-2020</h2><span id='topic+so2concentrations'></span>

<h3>Description</h3>

<p>This data set contains yearly observations by the Environmental Protection Agency
on the concentration of sulfur dioxide in parts per billion, based on 32 sites.
I use this for in-class illustration. Note that the national standard is
75 parts per billion. Data are the national trend.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>so2concentrations
</code></pre>


<h3>Format</h3>

<p>A data frame with the following 4 variables.
</p>

<dl>
<dt><code>year</code></dt><dd><p>the year</p>
</dd>
<dt><code>value</code></dt><dd><p>the mean concentration of sulfur dioxide in the air
based on 32 trend sites, in parts per billion</p>
</dd>
<dt><code>ub</code></dt><dd><p>the lower bound of the value (10th percentile)</p>
</dd>
<dt><code>lb</code></dt><dd><p>the upper bound of the value (90th percentile)</p>
</dd>
</dl>



<h3>Source</h3>

<p>Environmental Protection Agency (&quot;Sulfur Dioxide Trends&quot;)
</p>

<hr>
<h2 id='steves_clothes'>Steve's (Professional) Clothes, as of March 20, 2022</h2><span id='topic+steves_clothes'></span>

<h3>Description</h3>

<p>I cobbled together this data set of the professional clothes
(polos, long-sleeve dress shirts, pants) in my closet, largely for
illustration on the origins of apparel in the U.S. for an intro lecture on trade.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>steves_clothes
</code></pre>


<h3>Format</h3>

<p>A data frame with 86 observations on the following 4 variables.
</p>

<dl>
<dt><code>type</code></dt><dd><p>Type of clothing</p>
</dd>
<dt><code>brand</code></dt><dd><p>The brand of clothing (e.g. Apt. 9, Saddlebred)</p>
</dd>
<dt><code>color</code></dt><dd><p>the color (and/or pattern) of the article of clothing</p>
</dd>
<dt><code>origin</code></dt><dd><p>The country that produced the garment.</p>
</dd>
</dl>



<h3>Details</h3>

<p>If you must know, I do most of my clothes shopping at major retailers
in the U.S. This is mostly Belk, J.C. Penney, and Kohl's. If that's you as well, the
odds are good the distribution of my clothes will closely resemble yours. A recent
move I made resulted in me donating a fair bit of my short-sleeved polo shirts. I did
not buy any new shirts, though. Thus, I copied that information from a previous version
of the data.
</p>


<h3>Source</h3>

<p>Steve's closet. Hey, that's me!
</p>

<hr>
<h2 id='sugar_price'>IMF Primary Commodity Price Data for Sugar</h2><span id='topic+sugar_price'></span>

<h3>Description</h3>

<p>This is primary commodity price data for sugar globally,
in the United States, and in Europe for every month from 1980 to (roughly)
the present. Prices are nominal U.S. cents per pound and are
not seasonally adjusted (&quot;NSA&quot;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sugar_price
</code></pre>


<h3>Format</h3>

<p>A data frame with 1,316 observations on the following 3 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>a date</p>
</dd>
<dt><code>category</code></dt><dd><p>the category (either the U.S., global, or Europe)</p>
</dd>
<dt><code>value</code></dt><dd><p>the price of sugar in U.S. cents per pound (NSA, nominal)</p>
</dd>
</dl>



<h3>Details</h3>

<p>The price data for Europe do not appear to be updated as regularly as the
global and U.S. prices. Thus, the last month in the data for Europe are June 2017.
For that reason, I elected to make a data set of these data for posterity's sake.
</p>


<h3>Source</h3>

<p>International Monetary Fund
</p>

<hr>
<h2 id='sweden_counties'>The Counties of Sweden</h2><span id='topic+sweden_counties'></span>

<h3>Description</h3>

<p>A simple data set on Sweden's counties.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sweden_counties
</code></pre>


<h3>Format</h3>

<p>A data frame with 21 observations on the following 6 variables.
</p>

<dl>
<dt><code>iso</code></dt><dd><p>the ISO 3166-2 code for the county</p>
</dd>
<dt><code>nuts</code></dt><dd><p>the Nomenclature of Territorial Units for Statistics (NUTS) code for the county</p>
</dd>
<dt><code>county</code></dt><dd><p>the name of the county, in Swedish</p>
</dd>
<dt><code>centre</code></dt><dd><p>the administrative centre, or centres, of the county</p>
</dd>
<dt><code>area</code></dt><dd><p>the size of the county in square kilometers</p>
</dd>
<dt><code>pop2019</code></dt><dd><p>the size of the county in 2019</p>
</dd>
</dl>



<h3>Details</h3>

<p>This is a simple Wikipedia scrape job from 7 November 2022.
</p>

<hr>
<h2 id='thatcher_approval'>Margaret Thatcher Satisfaction Ratings, 1980-1990</h2><span id='topic+thatcher_approval'></span>

<h3>Description</h3>

<p>A data set on satisfaction/dissatisfaction ratings during Margaret Thatcher's tenure as prime minister.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>thatcher_approval
</code></pre>


<h3>Format</h3>

<p>A data frame with 125 observations on the following 8 variables.
</p>

<dl>
<dt><code>poll_date</code></dt><dd><p>the effective &quot;date&quot; of the public opinion poll</p>
</dd>
<dt><code>date</code></dt><dd><p>a date for the poll, to make for easier plotting</p>
</dd>
<dt><code>govt_sat</code></dt><dd><p>the percentage of respondents saying they were satisfied with the government</p>
</dd>
<dt><code>govt_dis</code></dt><dd><p>the percentage of respondents saying they were dissatisfied with the government</p>
</dd>
<dt><code>thatcher_sat</code></dt><dd><p>the percentage of respondents saying they were satisfied with Margaret Thatcher</p>
</dd>
<dt><code>thatcher_dis</code></dt><dd><p>the percentage of respondents saying they were dissatisfied with Margaret Thatcher</p>
</dd>
<dt><code>opp_sat</code></dt><dd><p>the percentage of respondents saying they were satisfied with the leader of the opposition</p>
</dd>
<dt><code>opp_dis</code></dt><dd><p>the percentage of respondents saying they were dissatisfied with the leader of the opposition</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data come from Ipsos. &quot;Leader of the opposition&quot; was typically named in the exact poll. In the
lifetime of this series, the leader of the opposition was James Callaghan until Nov. 10 1980. Thereafter, it
was Michael Foot until Oct. 2 1983. Neil Kinnock replaces him for the duration of this series. Interpret
&quot;leader of the opposition&quot; with that in mind.
</p>
<p>The date variable is, again, for simple convenience to make for easier plotting. In the absence of a specific
day provided by Ipsos, the poll benchmarks to the first of the month. In the case of a known period of days, it
benchmarks to the first day provided.
</p>

<hr>
<h2 id='therms'>Thermometer Ratings for Donald Trump and Barack Obama</h2><span id='topic+therms'></span>

<h3>Description</h3>

<p>A data set on thermometer ratings for Donald Trump and Barack Obama in 2020.
I use these data for in-class illustration of
central limit theorem. Basically: the sampling distribution of a
population is normal, even if the underlying population is decidedly
not.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>therms
</code></pre>


<h3>Format</h3>

<p>A data frame with 3080 observations on the following 2 variables.
</p>

<dl>
<dt><code>fttrump1</code></dt><dd><p>a thermometer rating for Donald Trump [0:100]</p>
</dd>
<dt><code>ftobama1</code></dt><dd><p>a thermometer rating for Barack Obama [0:100]</p>
</dd>
</dl>



<h3>Details</h3>

<p>The survey period was April 10-18, 2020 and was done entirely online.
</p>


<h3>Source</h3>

<p>American National Election Studies (ANES) Exploratory Testing Survey (ETS)
</p>

<hr>
<h2 id='turnips'>Turnip prices in Animal Crossing (New Horizons)</h2><span id='topic+turnips'></span>

<h3>Description</h3>

<p>A data set on turnip prices from my experience with Animal Crossing (New Horizons)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>turnips
</code></pre>


<h3>Format</h3>

<p>A data frame with the following 3 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>a date</p>
</dd>
<dt><code>time</code></dt><dd><p>a character vector referring to the particular time period of observation</p>
</dd>
<dt><code>price</code></dt><dd><p>a numeric vector for the price of turnips, in bells</p>
</dd>
</dl>



<h3>Details</h3>

<p>Sunday prices are set for purchase and do not fluctuate. Timmy and Tommy do not accept turnips on Sunday either. Daily prices
fluctuate both at opening on Nook's Cranny and at noon. This amounts to three time periods in the data. &quot;5:00 a.m.&quot; is reserved only for
Sunday purchases (i.e. when Daisy Mae arrives on the island). 8:00 a.m. is the morning price because that is when Nook's Cranny opens.
12:00 p.m. is when the price changes for the day.
</p>
<p>Explanations for missing dates: Timmy and Tommy were renovating the shop on May 6, 2021. My wife was diagnosed with
cancer and my mother in law went to the hospital on the afternoon of Dec. 27, 2021. I did not get to play the game on
Jan. 9, 2022 because of errands I was running for my wife. I plain forgot to check on Feb. 7, 2022.
</p>

<hr>
<h2 id='TV16'>The Individual Correlates of the Trump Vote in 2016</h2><span id='topic+TV16'></span>

<h3>Description</h3>

<p>These data come from the 2016 CCES and allow interested students to model the
individual correlates of the Trump vote in 2016. Code/analysis heavily indebted
to a 2017 analysis I did on my blog (see references).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TV16
</code></pre>


<h3>Format</h3>

<p>A data frame with 64600 observations on the following 21 variables.
</p>

<dl>
<dt><code>uid</code></dt><dd><p>a numeric vector, a unique identifier for the respondent as they first appear in the CCES data.</p>
</dd>
<dt><code>state</code></dt><dd><p>a character vector for the state in which the respondent resides</p>
</dd>
<dt><code>votetrump</code></dt><dd><p>a numeric that equals 1 if the respondent voted says s/he voted for Trump in 2016.</p>
</dd>
<dt><code>age</code></dt><dd><p>a numeric vector for age that is roughly calculated as 2016 - <code>birthyr</code>, as it's coded in the CCES data.</p>
</dd>
<dt><code>female</code></dt><dd><p>a numeric that equals 1 if the respondent is a woman</p>
</dd>
<dt><code>collegeed</code></dt><dd><p>a numeric vector that equals 1 if the respondent says s/he has a college degree</p>
</dd>
<dt><code>racef</code></dt><dd><p>a character vector for the race of the respondent</p>
</dd>
<dt><code>famincr</code></dt><dd><p>a numeric vector for the respondent's household income. Ranges from 1 (Less than $10,000) to 12 ($150,000 or more).</p>
</dd>
<dt><code>ideo</code></dt><dd><p>a numeric vector for the respondent's ideology on a liberal-conservative discrete scale. 1 = very liberal. 5 = very conservative.</p>
</dd>
<dt><code>pid7na</code></dt><dd><p>a numeric vector for the respondent's partisanship on the familiar 1-7 scale. 1 = Strong Democrat. 7 = Strong Republican. Other party supporters (e.g. libertarians) are coded as NA.</p>
</dd>
<dt><code>bornagain</code></dt><dd><p>a numeric vector for whether the respondent self-identifies as a born-again Christian.</p>
</dd>
<dt><code>religimp</code></dt><dd><p>a numeric vector for the importance of religion to the respondent. 1 = not at all important. 4 = very important.</p>
</dd>
<dt><code>churchatd</code></dt><dd><p>a numeric vector for the extent of church attendance for the respondent. 1 = never. 6 = more than once a week.</p>
</dd>
<dt><code>prayerfreq</code></dt><dd><p>a numeric vector for the frequency of prayer for the respondent. 1 = never. 7 = several times a day.</p>
</dd>
<dt><code>angryracism</code></dt><dd><p>a numeric vector for how angry the respondent is that racism exists. 1 = strongly agree (i.e. is angry racism exists). 5 = strongly disagree.</p>
</dd>
<dt><code>whiteadv</code></dt><dd><p>a numeric vector for agreement with statement that white people have advantages over others in the U.S. 1 = strongly agree. 5 = strongly disagree.</p>
</dd>
<dt><code>fearraces</code></dt><dd><p>a numeric vector for agreement with statement that the respondent fears other races. 1 = strongly disagree. 5 = strongly agree.</p>
</dd>
<dt><code>racerare</code></dt><dd><p>a numeric vector for agreement with statement that racism is rare in the U.S. 1 = strongly disagree. 5 = strongly agree.</p>
</dd>
<dt><code>lrelig</code></dt><dd><p>a numeric vector that serves as a latent estimate for religiosity from the <code>bornagain</code>, <code>religimp</code>, <code>churchatd</code>, and <code>prayerfreq</code> variables. Higher values = more religiosity. </p>
</dd>
<dt><code>lcograc</code></dt><dd><p>a numeric vector that serves as a latent estimate for cognitive racism. This is derived from the <code>racerare</code> and <code>whiteadv</code> variables.</p>
</dd>
<dt><code>lemprac</code></dt><dd><p>a numeric vector that serves as a latent estimate for empathetic racism. This is derived from the <code>fearraces</code> and <code>angryracism</code> variables.</p>
</dd>
</dl>



<h3>Details</h3>

<p>The latent estimates for religiosity, cognitive racism, and empathetic
racism come from a graded response model estimated in <code>mirt</code>. The concepts of
&quot;cognitive racism&quot; and &quot;empathetic racism&quot; come from DeSante and Smith.
</p>


<h3>Source</h3>

<p>Cooperative Congressional Election Study, 2016
</p>


<h3>References</h3>

<p><a href="http://svmiller.com/blog/2017/04/age-income-racism-partisanship-trump-vote-2016/">http://svmiller.com/blog/2017/04/age-income-racism-partisanship-trump-vote-2016/</a>
</p>
<p><a href="https://github.com/svmiller/2016-cces-trump-vote/blob/master/1-2016-cces-trump.R">https://github.com/svmiller/2016-cces-trump-vote/blob/master/1-2016-cces-trump.R</a>
</p>

<hr>
<h2 id='ukg_eeri'>United Kingdom Effective Exchange Rate Index Data, 1990-2022</h2><span id='topic+ukg_eeri'></span>

<h3>Description</h3>

<p>This is a (near) daily data set on the effective exchange rate index for
the United Kingdom's pound sterling from 1990 onward. The data are
indexed, such that 100 equals the monthly average in January 2005. This
is useful for illustrating devaluations of the pound after Black Wednesday,
the financial crisis, and, more recently, the UK's separation from the
European Union.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ukg_eeri
</code></pre>


<h3>Format</h3>

<p>A data frame with 8318 observations on the following 2 variables.
</p>

<dl>
<dt><code>date</code></dt><dd><p>a date</p>
</dd>
<dt><code>value</code></dt><dd><p>a numeric vector for the effective exchange rate index (Jan. 2005 = 100)</p>
</dd>
</dl>



<h3>Details</h3>

<p>Credit to the Bank of England for making these data readily available and
accessible. The Bank of England's website (<a href="https://www.bankofengland.co.uk/">https://www.bankofengland.co.uk/</a>) has
these data with a code of <code>XUDLBK67</code>.
</p>


<h3>Source</h3>

<p>Bank of England
</p>

<hr>
<h2 id='uniondensity'>Cross-National Rates of Trade Union Density</h2><span id='topic+uniondensity'></span>

<h3>Description</h3>

<p>Cross-national data on relative size of the trade unions and predictors in 20 countries.
This is a data set of interest to replicating Western and Jackman (1994),
who themselves were addressing a debate between Wallerstein and Stephens on
which of two highly correlated predictors explains trade union density.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>uniondensity
</code></pre>


<h3>Format</h3>

<p>A data frame with 20 observations on the following 5 variables.
</p>

<dl>
<dt><code>country</code></dt><dd><p>a character vector for the country</p>
</dd>
<dt><code>union</code></dt><dd><p>a numeric vector for the percentage of the total number of
wage and salary earners plus the unemployed who are union members, measured
between 1975 and 1980, with most of the data drawn from 1979.</p>
</dd>
<dt><code>left</code></dt><dd><p>a numeric vector tapping the extent to which parties of
the left have controlled governments since 1919, due to Wilensky (1981).</p>
</dd>
<dt><code>size</code></dt><dd><p>a numeric vector measuring the log of labor force size,
defined as the number of wage and salary earners, plus the unemployed.</p>
</dd>
<dt><code>concen</code></dt><dd><p>a numeric vector measuring the percentage of employment,
shipments, or production accounted for by the four largest enterprises in
a particular industry, averaged over industries (with weights proportional
to the size of the industry) and the resulting measure is normalized such
that the United States scores a 1.0, and is due to Pryor (1973).
Some of the scores on this variable are imputed using procedures
described in Stephens and Wallerstein (1991, 945).</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data documentation are derived from Simon Jackman's <code>pscl</code> package.
I just tidied up the presentation a bit.
</p>


<h3>Source</h3>

<p>Pryor, Frederic. 1973. Property and Industrial Organization in Communist and Capitalist Countries. Bloomington: Indiana University Press.
</p>
<p>Stephens, John and Michael Wallerstein. 1991. Industrial Concentration, Country Size and Trade Union Membership. American Political Science Review 85:941-953.
</p>
<p>Western, Bruce and Simon Jackman. 1994. Bayesian Inference for Comparative Research. American Political Science Review 88:412-423.
</p>
<p>Wilensky, Harold L. 1981. Leftism, Catholicism, Democratic Corporatism: The Role of Political Parties in Recemt Welfare State Development. In The Development of Welfare States in Europe and America, ed. Peter Flora and Arnold J. Heidenheimer. New Brunswick: Transaction Books.
</p>


<h3>References</h3>

<p>Jackman, Simon. 2009. Bayesian Analysis for the Social Sciences. Wiley: Hoboken, New Jersey.
</p>

<hr>
<h2 id='usa_chn_gdp_forecasts'>United States-China GDP and GDP Forecasts, 1960-2050</h2><span id='topic+usa_chn_gdp_forecasts'></span>

<h3>Description</h3>

<p>This is a toy data set to examine the time in which we should expect China
to overtake the United States in total gross domestic product (GDP),
given current trends. It includes an OECD long-term GDP forecast from 2014,
and forecasts from the <code>forecast</code> and <code>prophet</code> packages in R.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>usa_chn_gdp_forecasts
</code></pre>


<h3>Format</h3>

<p>A data frame with 182 observations on the following 12 variables.
</p>

<dl>
<dt><code>country</code></dt><dd><p>a character vector (United States, China)</p>
</dd>
<dt><code>year</code></dt><dd><p>a numeric vector for the year</p>
</dd>
<dt><code>p_gdp</code></dt><dd><p>y-hats (forecasted GDP) from a <code>prophet</code> forecast</p>
</dd>
<dt><code>p_lo80</code></dt><dd><p>lower bound (80%) of y-hats (forecasted GDP) from a <code>prophet</code> forecast</p>
</dd>
<dt><code>p_hi80</code></dt><dd><p>upper bound (80%) of y-hats (forecasted GDP) from a <code>prophet</code> forecast</p>
</dd>
<dt><code>gdp</code></dt><dd><p>observed GDP, made available to the World Bank and OECD national accounts data. Available from 1960 to 2019.</p>
</dd>
<dt><code>f_gdp</code></dt><dd><p>forecasted GDP from 2020 to 2050, from the <code>forecast</code> package</p>
</dd>
<dt><code>f_lo80</code></dt><dd><p>lower bound (80%) forecasted GDP from 2018 to 2050, from the <code>forecast</code> package</p>
</dd>
<dt><code>f_hi80</code></dt><dd><p>upper bound (80%) forecasted GDP from 2018 to 2050, from the <code>forecast</code> package</p>
</dd>
<dt><code>f_lo95</code></dt><dd><p>lower bound (95%) forecasted GDP from 2018 to 2050, from the <code>forecast</code> package</p>
</dd>
<dt><code>f_hi95</code></dt><dd><p>upper bound (95%) forecasted GDP from 2018 to 2050, from the <code>forecast</code> package</p>
</dd>
<dt><code>oecd_ltgdpf</code></dt><dd><p>long-term GDP forecast from the OECD via the OECD Outlook No 95 - May 2014</p>
</dd>
</dl>



<h3>Details</h3>

<p>Forecasts from the <code>forecast</code> package and <code>prophet</code>
package are rudimentary and bare minimum forecasts based on previous values
to that point. Notice the <code>forecast</code> forecasts have a prefix of
<code>f_</code> and the <code>prophet</code> forecasts have a prefix of
<code>p_</code>. Forecasts are not meant to be exhaustive (clearly), only
illustrative for in-class discussion about the &quot;Rise of China.&quot; Forecasts
made in R on Nov. 20, 2020.
</p>


<h3>Source</h3>

<p>OECD Outlook No 95 - May 2014 - Long-term baseline projections provided
by Organisation for Economic Co-operation and Development (OECD)
</p>

<hr>
<h2 id='usa_computers'>Percentage of U.S. Households with Computer Access, by Year</h2><span id='topic+usa_computers'></span>

<h3>Description</h3>

<p>This is a simple and regrettably incomplete time-series on the percentage
of U.S. households with access to a computer, by year.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>usa_computers
</code></pre>


<h3>Format</h3>

<p>A data frame with 19 observations on the following 2 variables.
</p>

<dl>
<dt><code>year</code></dt><dd><p>the year</p>
</dd>
<dt><code>value</code></dt><dd><p>the estimated percentage of households with access to a computer</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data are spotty and regrettably this is not a perfect time-series.
However, it is useful for an in-class exercise to show that the proliferation of
household computers (over time) in the United States comes in part because of
globalization. Use it for that purpose. The data are reasonably faithful, but don't
treat it as gospel. Exact sourcing available upon request.
</p>


<h3>Source</h3>

<p>Various: U.S. Census Bureau, Current Population Survey, and
American Community Survey
</p>

<hr>
<h2 id='usa_migration'>U.S. Inbound/Outbound Migration Data, 1990-2017</h2><span id='topic+usa_migration'></span>

<h3>Description</h3>

<p>This data set contains counts/estimates for the number of inbound migrants
in the U.S as well as outbound migrants of American origin to other countries
from 1990 to 2017.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>usa_migration
</code></pre>


<h3>Format</h3>

<p>A data frame with 3535 observations on the following 5 variables.
</p>

<dl>
<dt><code>year</code></dt><dd><p>a numeric vector for 1990, 1995, 2000, 2005, 2010, 2015, 2017</p>
</dd>
<dt><code>country</code></dt><dd><p>a character vector/constant for the United States</p>
</dd>
<dt><code>category</code></dt><dd><p>a character vector for whether the <code>count</code> is inbound to the U.S. from the <code>area</code> variable or outbound (i.e. American expats) to the  <code>area</code>  variable in a given year.</p>
</dd>
<dt><code>area</code></dt><dd><p>a character vector for the area of origin (if <code>category</code> == &quot;Inbound&quot;) or destination for American migrants (if <code>category</code> == &quot;Outbound&quot;)</p>
</dd>
<dt><code>count</code></dt><dd><p>a numeric vector for the count of inbound/outbound migrants</p>
</dd>
</dl>



<h3>Details</h3>

<p>&quot;Cote d'Ivoire&quot;, &quot;Curacao&quot;, and &quot;Reunion&quot; originally had UTF-8
characters, which were removed for maximal compliance with CRAN. CRAN
raises a note for every non-ASCII character it sees.
</p>


<h3>Source</h3>

<p>United Nations Population Division (DESA)
</p>

<hr>
<h2 id='usa_states'>State Abbreviations, Names, and Regions/Divisions</h2><span id='topic+usa_states'></span>

<h3>Description</h3>

<p>A simple data set from <code>state.abb</code>, <code>state.name</code>, <code>state.region</code>, and
<code>state.division</code> (+ District of Columbia). I'd rather just have all these
in one place.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>usa_states
</code></pre>


<h3>Format</h3>

<p>A data frame with 51 observations on the following 4 variables.
</p>

<dl>
<dt><code>stateabb</code></dt><dd><p>the state abbreviation</p>
</dd>
<dt><code>statename</code></dt><dd><p>the state's name</p>
</dd>
<dt><code>region</code></dt><dd><p>the state's Census region</p>
</dd>
<dt><code>division</code></dt><dd><p>the state's Census division</p>
</dd>
</dl>


<hr>
<h2 id='usa_tradegdp'>U.S. Trade and GDP, 1790-2018</h2><span id='topic+usa_tradegdp'></span>

<h3>Description</h3>

<p>A yearly data set on U.S. trade and GDP from 1790 to 2018. Data also include
a population variable to facilitate per capita adjustments, if the user
sees it useful.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>usa_tradegdp
</code></pre>


<h3>Format</h3>

<p>A data frame with 229 observations on the following 5 variables.
</p>

<dl>
<dt><code>year</code></dt><dd><p>the year</p>
</dd>
<dt><code>gdpb</code></dt><dd><p>U.S. GDP (nominal, in billions)</p>
</dd>
<dt><code>pop</code></dt><dd><p>Population of the U.S. (in thousands)</p>
</dd>
<dt><code>impo</code></dt><dd><p>The value of U.S. imports (in billions)</p>
</dd>
<dt><code>expo</code></dt><dd><p>The value of U.S. exports (in billions)</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data come from various sources (see, especially:
<a href="https://econdataus.com/tradeall.html">https://econdataus.com/tradeall.html</a>). Post-1989 data come from the
U.S. Census Bureau. 2018 GDP comes from the IMF. 2018 population estimate
comes from the U.S. Census Bureau.
</p>

<hr>
<h2 id='voteincome'>Sample Turnout and Demographic Data from the 2000 Current Population Survey</h2><span id='topic+voteincome'></span>

<h3>Description</h3>

<p>A data set on turnout and demographic data from the 2000 Current Population Survey. This is a basic
port of the <code>voteincome</code> data from the <span class="pkg">Zelig</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>voteincome
</code></pre>


<h3>Format</h3>

<p>A data frame with 1500 observations on the following 7 variables.
</p>

<dl>
<dt><code>state</code></dt><dd><p>a character variable for the state, either Arkansas (AK) or South Carolina (SC)</p>
</dd>
<dt><code>year</code></dt><dd><p>a numeric constant for the year (2000)</p>
</dd>
<dt><code>vote</code></dt><dd><p>a dummy variable for whether the person voted (1) or did not vote</p>
</dd>
<dt><code>income</code></dt><dd><p>a numeric variable for income ranging from 4 (less than $5000) to 17 (greater than $75000)</p>
</dd>
<dt><code>education</code></dt><dd><p>a numeric variable for educational attainment
ranging from 1 (less than high school education) to 4 (more than college education)</p>
</dd>
<dt><code>age</code></dt><dd><p>a numeric variable for the respondent's age in years,ranging from 18 to 85</p>
</dd>
<dt><code>female</code></dt><dd><p>a dummy variable for whether the respondent is a woman (1) or a man (0)</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data come from the 2000 Current Population Survey by way of the <span class="pkg">Zelig</span> package. Data should
not be used for inferential applications, only for pedagogical purposes. See the appropriate CPS codebook
for more information on variable coding (especially for income and education). In all likelihood, age is
right-censored as well.
</p>

<hr>
<h2 id='wbd_example'>A Simple Panel drawn from World Bank Open Data</h2><span id='topic+wbd_example'></span>

<h3>Description</h3>

<p>A simple data set drawn from World Bank Open Data. I'll use it to illustrate
some merge issues you might encounter in panel data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wbd_example
</code></pre>


<h3>Format</h3>

<p>A data frame with 4537 observations on the following 7 variables.
</p>

<dl>
<dt><code>country</code></dt><dd><p>an English name for the country/territorial unit</p>
</dd>
<dt><code>iso2c</code></dt><dd><p>the two-character ISO code for the country/territorial unit</p>
</dd>
<dt><code>iso3c</code></dt><dd><p>the three-character ISO code for the country/territorial unit</p>
</dd>
<dt><code>year</code></dt><dd><p>the year of observation</p>
</dd>
<dt><code>rgdppc</code></dt><dd><p>the real GDP per capita of the country/territorial unit in that year</p>
</dd>
<dt><code>lifeexp</code></dt><dd><p>the average life expectancy at birth for men and women for the country that year</p>
</dd>
<dt><code>hci</code></dt><dd><p>the human capital index [0-1] for the country that year</p>
</dd>
</dl>



<h3>Details</h3>

<p>The idea for this data comes by way of a student encounter where we noticed
this issue. Data were further generated by the wonderful <span class="pkg">WDI</span> package.
The underlying data come from the World Bank national accounts (GDP per
capita), World Bank analyst estimates (human capital index), or the United
Nations Population Division (life expectancy at birth).
</p>

<hr>
<h2 id='wvs_ccodes'>Syncing Word Values Survey Country Codes with CoW Codes</h2><span id='topic+wvs_ccodes'></span>

<h3>Description</h3>

<p>A simple data set that syncs World Values Survey country codes (<code>s003</code>)
with corresponding country codes from the Correlates of War state system
membership data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wvs_ccodes
</code></pre>


<h3>Format</h3>

<p>A data frame with 112 observations on the following 3 variables.
</p>

<dl>
<dt><code>s003</code></dt><dd><p>the World Values Survey country code</p>
</dd>
<dt><code>country</code></dt><dd><p>a character vector for the corresponding country name</p>
</dd>
<dt><code>ccode</code></dt><dd><p>the equivalent country code from the Correlates of War state system membership data</p>
</dd>
</dl>



<h3>Details</h3>

<p><a href="http://svmiller.com/blog/2015/06/syncing-word-values-survey-country-codes-with-cow-codes/">http://svmiller.com/blog/2015/06/syncing-word-values-survey-country-codes-with-cow-codes/</a>
</p>

<hr>
<h2 id='wvs_immig'>Attitudes about Immigration in the World Values Survey</h2><span id='topic+wvs_immig'></span>

<h3>Description</h3>

<p>A data set on attitudes about immigration for all observations in
the third to sixth wave of the World Values Survey. I use these
data for in-class illustration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wvs_immig
</code></pre>


<h3>Format</h3>

<p>A data frame with 310,388 observations on the following 6 variables.
</p>

<dl>
<dt><code>s002</code></dt><dd><p>the World Values Survey wave</p>
</dd>
<dt><code>s003</code></dt><dd><p>the World Values Survey country code</p>
</dd>
<dt><code>country</code></dt><dd><p>the country name</p>
</dd>
<dt><code>s020</code></dt><dd><p>the survey year</p>
</dd>
<dt><code>uid</code></dt><dd><p>a unique identifier for the survey respondent</p>
</dd>
<dt><code>e143</code></dt><dd><p>an attitude about immigration policy in the World Values Survey</p>
</dd>
</dl>



<h3>Details</h3>

<p>1 = &quot;let anyone come&quot;. 2 = &quot;as long as jobs are available&quot;. 3 = &quot;strict limits&quot;.
4 = &quot;Prohibit people from coming&quot; for the <code>e143</code> variable. See <code>?wvs_ccodes</code> for
more information about naming/identifying countries.
</p>

<hr>
<h2 id='wvs_justifbribe'>Attitudes about the Justifiability of Bribe-Taking in the World Values Survey</h2><span id='topic+wvs_justifbribe'></span>

<h3>Description</h3>

<p>A data set on attitudes about the justifiability of bribe-taking for all
observations in the third to sixth wave of the World Values Survey. I use these
data for in-class illustration about seemingly interval-level, but information-poor
measurements.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wvs_justifbribe
</code></pre>


<h3>Format</h3>

<p>A data frame with 348532 observations on the following 6 variables.
</p>

<dl>
<dt><code>s002</code></dt><dd><p>the World Values Survey wave</p>
</dd>
<dt><code>s003</code></dt><dd><p>the World Values Survey country code</p>
</dd>
<dt><code>country</code></dt><dd><p>the country name</p>
</dd>
<dt><code>s020</code></dt><dd><p>the survey year</p>
</dd>
<dt><code>uid</code></dt><dd><p>a unique identifier for the survey respondent</p>
</dd>
<dt><code>f117</code></dt><dd><p>an attitude about the justifiability of bribe-taking in the World Values Survey</p>
</dd>
</dl>



<h3>Details</h3>

<p>1 = &quot;never justifiable&quot;. 10 = &quot;always justifiable&quot;. Increasing values on this
1-10 scale imply increasing permissiveness for the respondent toward this
particular/blatant form of corruption.
</p>

<hr>
<h2 id='wvs_usa_abortion'>Attitudes on the Justifiability of Abortion in the United States (World Values Survey, 1982-2011)</h2><span id='topic+wvs_usa_abortion'></span>

<h3>Description</h3>

<p>A data set on attitudes about the justifiability of abortion in the United States
based on World Values Survey responses recorded across six waves (from 1982
to 2011). I assembled this data frame probably around 2014 and routinely use it
for in-class illustration about regression, post-estimation simulation, quantities of
interest, and how to think about modeling a dependent variable that is on a
1-10 scale, but has curious heaping patterns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wvs_usa_abortion
</code></pre>


<h3>Format</h3>

<p>A data frame with 10387 observations on the following 16 variables.
</p>

<dl>
<dt><code>wvsccode</code></dt><dd><p>the country code for the United States (a numeric constant)</p>
</dd>
<dt><code>wave</code></dt><dd><p>the survey wave</p>
</dd>
<dt><code>year</code></dt><dd><p>the survey year corresponding to the survey wave</p>
</dd>
<dt><code>aj</code></dt><dd><p>the justifiability of abortion on a 1-10 scale (1 = never justifiable; 10 = always justifiable)</p>
</dd>
<dt><code>age</code></dt><dd><p>the age of the respondent in years</p>
</dd>
<dt><code>collegeed</code></dt><dd><p>a dummy variable that equals 1 if the respondent graduated from college</p>
</dd>
<dt><code>female</code></dt><dd><p>a dummy variable that equals 1 if the respondent is a woman</p>
</dd>
<dt><code>unemployed</code></dt><dd><p>a dummy variable that equals 1 if the respondent is unemployed</p>
</dd>
<dt><code>ideology</code></dt><dd><p>the ideological self-placement of the respondent on a 1-10 scale (1 = furthest to the left; 10 = furthest to the right)</p>
</dd>
<dt><code>satisfinancial</code></dt><dd><p>the respondent's financial satisfaction with his/her life (1 = most dissatisfied; 10 = most satisfied)</p>
</dd>
<dt><code>postma4</code></dt><dd><p>the post-materialist index for the respondent (-1 = materialist; 0 = mixed, 1 = post-materialist)</p>
</dd>
<dt><code>cai</code></dt><dd><p>the child autonomy index, which ranges from -2 to 2</p>
</dd>
<dt><code>trustmostpeople</code></dt><dd><p>can most people be trusted (1) or &quot;(you) never can be too careful&quot; (0)</p>
</dd>
<dt><code>godimportant</code></dt><dd><p>the importance of God to the respondent on a 1-10 scale (1 = God is not at all important; 10 = God is most important)</p>
</dd>
<dt><code>respectauthority</code></dt><dd><p>would more respect for authority be a welcome change to the United States?</p>
</dd>
<dt><code>nationalpride</code></dt><dd><p>a dummy that equals 1 if the respondent is very proud to be an American.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data come from the World Values Survey. Note that the college education variable
is curiously <code>NA</code> until the third survey wave. The child autonomy index ranges from
-2 to 2 where increasing values indicate that children should learn determination and
independence over obedience and religious faith. The <code>respectauthority</code> variable is coded
where -1  means the respondent believes greater respect for authority in the United States
as a future change to the country would be a bad thing. 0 means the respondent doesn't mind
such a change. 1 = the respondent believes it would be a good thing.
</p>

<hr>
<h2 id='wvs_usa_educat'>Education Categories for the United States in the World Values Survey</h2><span id='topic+wvs_usa_educat'></span>

<h3>Description</h3>

<p>This is a simple data set that summarizes what the education codes are in the World Values Survey for the United States.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wvs_usa_educat
</code></pre>


<h3>Format</h3>

<p>A data frame with 42 observations the following 6 variables.
</p>

<dl>
<dt><code>x025</code></dt><dd><p>the numeric code for supposedly the highest educational level attained</p>
</dd>
<dt><code>x025cswvs</code></dt><dd><p>the numeric code for supposedly the education-level attained by the respondent, with country-specific categories</p>
</dd>
<dt><code>n</code></dt><dd><p>the number of observations in the World Values Survey with that unique <code>x025cswvs</code> code</p>
</dd>
<dt><code>x025cswvsmeaning</code></dt><dd><p>the meaning behind the unique <code>x025cswvs</code> code</p>
</dd>
<dt><code>x025meaning</code></dt><dd><p>the meaning behind the unique <code>x025</code> code</p>
</dd>
<dt><code>educat</code></dt><dd><p>a standardized categorical variable corresponding with that unique <code>x025cswvs</code> code</p>
</dd>
</dl>



<h3>Details</h3>

<p>Observations taken from the combined seven waves of survey data made available by the World Values Survey, but isolated to
just the United States. The World Values Survey unfortunately did not collect information about the education-level of the respondent
in the 1981 and 1990 waves. These education categories feature in the Miller and Davis (2020) article in <em>Journal of
Ethnicity, and Politics</em>, albeit before the release of the seventh wave.
</p>


<h3>References</h3>

<p>Miller, Steven V. and Nicholas T. Davis. Forthcoming. &quot;The Effect of White Social Prejudice on Support for American Democracy.&quot;
<em>Journal of Race, Ethnicity, and Politics</em>.
</p>

<hr>
<h2 id='wvs_usa_regions'>Region Categories for the United States in the World Values Survey</h2><span id='topic+wvs_usa_regions'></span>

<h3>Description</h3>

<p>This is a simple data set that summarizes what the region codes are in the World Values Survey for the United States.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wvs_usa_regions
</code></pre>


<h3>Format</h3>

<p>A data frame with 63 observations the following 6 variables.
</p>

<dl>
<dt><code>x048wvs</code></dt><dd><p>the numeric code for supposedly the region in which the interview was conducted</p>
</dd>
<dt><code>x048wvsmeaning</code></dt><dd><p>the meaning behind the unique <code>x048wvs</code> code</p>
</dd>
<dt><code>stateabb</code></dt><dd><p>the corresponding state abbreviation (if available) for the unique <code>x048wvs</code> code</p>
</dd>
<dt><code>statename</code></dt><dd><p>the corresponding state abbreviation (if available) for the unique <code>x048wvs</code> code</p>
</dd>
<dt><code>division</code></dt><dd><p>the corresponding division for the unique <code>x048wvs</code> code</p>
</dd>
<dt><code>region</code></dt><dd><p>the corresponding region for the unique <code>x048wvs</code> code</p>
</dd>
</dl>



<h3>Details</h3>

<p>The region codes are a mess. Some of these are informed guesses. For example, I assume &quot;Northwest&quot; means
&quot;Pacific&quot; and that Idaho was not included in that category. I make a similar assumption that &quot;Rocky Mountain state&quot;
means &quot;Mountain&quot;.
</p>

<hr>
<h2 id='yugo_sales'>Yugo Sales in the United States, 1985-1992</h2><span id='topic+yugo_sales'></span>

<h3>Description</h3>

<p>A data set on Yugo sales against two competing models in the United States
from 1985 to 1992.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>yugo_sales
</code></pre>


<h3>Format</h3>

<p>A data frame with 24 observations on the following 3 variables.
</p>

<dl>
<dt><code>year</code></dt><dd><p>the year</p>
</dd>
<dt><code>car</code></dt><dd><p>the car type, either the Hyundai Excel, Yugo, or Toyota Tercel</p>
</dd>
<dt><code>sales</code></dt><dd><p>the number of units sold in the United States</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data come from a website then known as <code>carsalesbase.com</code>. I'm
aware the inclusion of the Tercel is questionable since the third generation
of Tercels were quite different from the first and second generations.
However, I use these data to illustrate how poorly the Yugo fared against
competing models, including the first and second generation Tercels. I think
the inclusion is fair for that purpose.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
