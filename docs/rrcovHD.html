<!DOCTYPE html><html><head><title>Help for package rrcovHD</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rrcovHD}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#CSimca'>
<p>Classification in high dimensions based on the (classical) SIMCA method</p></a></li>
<li><a href='#CSimca-class'><p>Class <code>"CSimca"</code> - classification in high dimensions based on the (classical) SIMCA method</p></a></li>
<li><a href='#getWeight-methods'><p>Accessor methods to the essential slots of <code>Outlier</code> and its subclasses</p></a></li>
<li><a href='#kibler'>
<p>1985 Auto Imports Database</p></a></li>
<li><a href='#Outlier-class'><p>Class <code>"Outlier"</code> &ndash; a base class for outlier identification</p></a></li>
<li><a href='#OutlierMahdist'>
<p>Outlier identification using robust (mahalanobis) distances based on robust multivariate location and covariance matrix</p></a></li>
<li><a href='#OutlierMahdist-class'><p>Class <code>OutlierMahdist</code> - Outlier identification using</p>
robust (mahalanobis) distances based on robust multivariate
location and covariance matrix</a></li>
<li><a href='#OutlierPCDist'>
<p>Outlier identification in high dimensions using the PCDIST algorithm</p></a></li>
<li><a href='#OutlierPCDist-class'><p>Class <code>"OutlierPCDist"</code> - Outlier identification in high dimensions using using the PCDIST algorithm</p></a></li>
<li><a href='#OutlierPCOut'>
<p>Outlier identification in high dimensions using the PCOUT algorithm</p></a></li>
<li><a href='#OutlierPCOut-class'><p>Class <code>"OutlierPCOut"</code> - Outlier identification in high dimensions using using the PCOUT algorithm</p></a></li>
<li><a href='#OutlierSign1'>
<p>Outlier identification in high dimensions using the SIGN1 algorithm</p></a></li>
<li><a href='#OutlierSign1-class'><p>Class <code>"OutlierSign1"</code> - Outlier identification in high dimensions using the SIGN1 algorithm</p></a></li>
<li><a href='#OutlierSign2'>
<p>Outlier identification in high dimensions using the SIGN2 algorithm</p></a></li>
<li><a href='#OutlierSign2-class'><p>Class <code>"OutlierSign2"</code> - Outlier identification in high dimensions using the SIGN2 algorithm</p></a></li>
<li><a href='#PredictSimca-class'><p>Class <code>"PredictSimca"</code> - prediction of <code>"Simca"</code> objects</p></a></li>
<li><a href='#PredictSosDisc-class'><p>Class <code>"PredictSosDisc"</code> - prediction of <code>"SosDisc"</code> objects</p></a></li>
<li><a href='#RSimca'>
<p>Robust classification in high dimensions based on the SIMCA method</p></a></li>
<li><a href='#RSimca-class'><p>Class <code>"RSimca" - robust classification in high dimensions based on the SIMCA method</code></p></a></li>
<li><a href='#Simca-class'><p>Class <code>"Simca"</code> - virtual base class for all classic and robust SIMCA</p>
classes representing classification in high dimensions based on the SIMCA method</a></li>
<li><a href='#SosDisc-class'>
<p>Class <code>"SosDisc"</code> - virtual base class for all classic and robust SosDisc</p>
classes representing the results of the robust and sparse multigroup classification
by the optimal scoring approach</a></li>
<li><a href='#SosDiscClassic-class'><p>Class <code>SosDiscClassic</code> - sparse multigroup classification by the optimal scoring approach</p></a></li>
<li><a href='#SosDiscRobust'>
<p>Robust and sparse multigroup classification by the optimal scoring approach</p></a></li>
<li><a href='#SosDiscRobust-class'><p>Class <code>SosDiscRobust</code> - robust and sparse multigroup classification by the optimal scoring approach</p></a></li>
<li><a href='#SPcaGrid'><p>Sparse Robust Principal Components based on Projection Pursuit (PP): GRID search Algorithm</p></a></li>
<li><a href='#SPcaGrid-class'><p>Class <code>SPcaGrid</code> - Sparse Robust PCA using PP - GRID search Algorithm</p></a></li>
<li><a href='#SummarySimca-class'><p>Class <code>"SummarySimca"</code> - summary of <code>"Simca"</code> objects</p></a></li>
<li><a href='#SummarySosDisc-class'><p>Class <code>"SummarySosDisc"</code> - summary of <code>"SosDisc"</code> objects</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Date:</td>
<td>2024-02-01</td>
</tr>
<tr>
<td>Title:</td>
<td>Robust Multivariate Methods for High Dimensional Data</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3-0</td>
</tr>
<tr>
<td>VersionNote:</td>
<td>Released 0.2-7 on 2021-04-23 on CRAN</td>
</tr>
<tr>
<td>Description:</td>
<td>Robust multivariate methods for high dimensional data including
        outlier detection (Filzmoser and Todorov (2013) &lt;<a href="https://doi.org/10.1016%2Fj.ins.2012.10.017">doi:10.1016/j.ins.2012.10.017</a>&gt;), 
        robust sparse PCA (Croux et al. (2013) &lt;<a href="https://doi.org/10.1080%2F00401706.2012.727746">doi:10.1080/00401706.2012.727746</a>&gt;, Todorov and Filzmoser (2013) &lt;<a href="https://doi.org/10.1007%2F978-3-642-33042-1_31">doi:10.1007/978-3-642-33042-1_31</a>&gt;), 
        robust PLS (Todorov and Filzmoser (2014) &lt;<a href="https://doi.org/10.17713%2Fajs.v43i4.44">doi:10.17713/ajs.v43i4.44</a>&gt;), 
        and robust sparse classification (Ortner et al. (2020) &lt;<a href="https://doi.org/10.1007%2Fs10618-019-00666-8">doi:10.1007/s10618-019-00666-8</a>&gt;).</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Valentin Todorov &lt;valentin.todorov@chello.at&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>rrcov (&ge; 1.3-7), robustbase (&ge; 0.92-1), methods</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats4, pls, spls, pcaPP, robustHD, Rcpp</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/valentint/rrcovHD">https://github.com/valentint/rrcovHD</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/valentint/rrcovHD/issues">https://github.com/valentint/rrcovHD/issues</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-04 15:30:24 UTC; valen</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Author:</td>
<td>Valentin Todorov <a href="https://orcid.org/0000-0003-4215-0245"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-04 16:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='CSimca'>
Classification in high dimensions based on the (classical) SIMCA method
</h2><span id='topic+CSimca'></span><span id='topic+CSimca.formula'></span><span id='topic+CSimca.default'></span>

<h3>Description</h3>

<p>CSimca performs the (classical) SIMCA method. This method classifies
a data matrix x with a known group structure. To reduce the dimension on 
each group a PCA analysis is performed. Afterwards a classification
rule is developped to determine the assignment of new observations. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CSimca(x, ...)
## Default S3 method:
CSimca(x, grouping, prior=proportions, k, kmax = ncol(x), 
    tol = 1.0e-4, trace=FALSE, ...)
## S3 method for class 'formula'
CSimca(formula, data = NULL, ..., subset, na.action)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CSimca_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>y~x</code>, it describes the response
and the predictors. The formula can be more complicated, such as
<code>y~log(x)+z</code> etc (see <code><a href="stats.html#topic+formula">formula</a></code> for more details).
The response should
be a factor representing the response variable, or any vector
that can be coerced to such (such as a logical variable).</p>
</td></tr>
<tr><td><code id="CSimca_+3A_data">data</code></td>
<td>
<p>an optional data frame (or similar: see
<code><a href="stats.html#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.</p>
</td></tr>
<tr><td><code id="CSimca_+3A_subset">subset</code></td>
<td>
<p>an optional vector used to select rows (observations) of the
data matrix <code>x</code>.</p>
</td></tr>
<tr><td><code id="CSimca_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen
when the data contain <code>NA</code>s.  The default is set by
the <code>na.action</code> setting of <code><a href="base.html#topic+options">options</a></code>, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The default is <code><a href="stats.html#topic+na.omit">na.omit</a></code>.</p>
</td></tr>
<tr><td><code id="CSimca_+3A_x">x</code></td>
<td>
<p>a matrix or data frame containing the explanatory variables (training set). </p>
</td></tr>
<tr><td><code id="CSimca_+3A_grouping">grouping</code></td>
<td>
<p>grouping variable:  a factor specifying the class for each observation.</p>
</td></tr>
<tr><td><code id="CSimca_+3A_prior">prior</code></td>
<td>
<p>prior probabilities, default to the class proportions for the training set.</p>
</td></tr>
<tr><td><code id="CSimca_+3A_tol">tol</code></td>
<td>
<p>tolerance</p>
</td></tr>
<tr><td><code id="CSimca_+3A_k">k</code></td>
<td>
<p>number of principal components to compute. If <code>k</code> is missing, 
or <code>k = 0</code>, the algorithm itself will determine the number of 
components by finding such <code>k</code> that <code class="reqn">l_k/l_1 &gt;= 10.E-3</code> and 
<code class="reqn">\Sigma_{j=1}^k l_j/\Sigma_{j=1}^r l_j &gt;= 0.8</code>. 
It is preferable to investigate the scree plot in order to choose the number 
of components and then run again. Default is <code>k=0</code>. </p>
</td></tr>
<tr><td><code id="CSimca_+3A_kmax">kmax</code></td>
<td>
<p>maximal number of principal components to compute.
Default is <code>kmax=10</code>. If <code>k</code> is provided, <code>kmax</code> 
does not need to be specified, unless <code>k</code> is larger than 10.</p>
</td></tr>
<tr><td><code id="CSimca_+3A_trace">trace</code></td>
<td>
<p>whether to print intermediate results. Default is <code>trace = FALSE</code></p>
</td></tr>
<tr><td><code id="CSimca_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>CSimca</code>, serving as a constructor for objects of class <code><a href="#topic+CSimca-class">CSimca-class</a></code> 
is a generic function with &quot;formula&quot; and &quot;default&quot; methods.
</p>
<p>SIMCA is a two phase procedure consisting of PCA performed on each group 
separately for dimension reduction followed by classification rules built 
in the lower dimensional space (note that the dimension in 
each group can be different). In original SIMCA new observations are 
classified by means of their deviations from the different PCA models.
Here (and also in the robust versions implemented in this package) the classification
rules will be obtained using two popular distances arising from PCA - 
orthogonal distances (OD) and score distances (SD). For the definition of these distances,
the definition of the cutoff values and the standartization of the distances see 
Vanden Branden K, Hubert M (2005) and Todorov and Filzmoser (2009).
</p>


<h3>Value</h3>

<p>An S4 object of class <code><a href="#topic+CSimca-class">CSimca-class</a></code> which is a subclass of of the 
virtual class <code><a href="#topic+Simca-class">Simca-class</a></code>. 
</p>


<h3>Author(s)</h3>

<p> Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>Vanden Branden K, Hubert M (2005) Robust classification in high 
dimensions based on the SIMCA method. Chemometrics and 
Intellegent Laboratory Systems 79:10&ndash;21
</p>
<p>Todorov V &amp; Filzmoser P (2009),
An Object Oriented Framework for Robust Multivariate Analysis.
<em>Journal of Statistical Software</em>, <b>32</b>(3), 1&ndash;47,
<a href="https://doi.org/10.18637/jss.v032.i03">doi:10.18637/jss.v032.i03</a>.   
</p>
<p>Todorov V &amp; Filzmoser P (2014),
Software Tools for Robust Analysis of High-Dimensional Data.
<em>Austrian Journal of Statistics</em>, <b>43</b>(4),  255&ndash;266,
<a href="https://doi.org/10.17713/ajs.v43i4.44">doi:10.17713/ajs.v43i4.44</a>.   
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(pottery)
dim(pottery)        # 27 observations in 2 classes, 6 variables
head(pottery)

## Build the SIMCA model. Use RSimca for a robust version
cs &lt;- CSimca(origin~., data=pottery)
cs
summary(cs)


## generate a sample from the pottery data set -
##  this will be the "new" data to be predicted
smpl &lt;- sample(1:nrow(pottery), 5)
test &lt;- pottery[smpl, -7]          # extract the test sample. Remove the last (grouping) variable
print(test)


## predict new data
pr &lt;- predict(cs, newdata=test)

pr@classification 
</code></pre>

<hr>
<h2 id='CSimca-class'>Class <code>"CSimca"</code> - classification in high dimensions based on the (classical) SIMCA method</h2><span id='topic+CSimca-class'></span>

<h3>Description</h3>

<p>The class <code>CSimca</code> represents the SIMCA algorithm for classification in high dimensions. 
The objects of class <code>CSImca</code> contain the results
of the SIMCA method.
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("CSImca", ...)</code> but the 
usual way of creating <code>CSimca</code> objects is a call to the function
<code>CSimca()</code> which serves as a constructor.
</p>


<h3>Slots</h3>


<dl>
<dt><code>call</code>:</dt><dd><p>the (matched) function call.</p>
</dd>
<dt><code>prior</code>:</dt><dd><p>prior probabilities used, default to group proportions</p>
</dd>
<dt><code>counts</code>:</dt><dd><p>number of observations in each class</p>
</dd>
<dt><code>pcaobj</code>:</dt><dd><p>A list of Pca objects - one for each group</p>
</dd>
<dt><code>k</code>:</dt><dd><p>Object of class <code>"numeric"</code> number of (choosen) principal components </p>
</dd>
<dt><code>flag</code>:</dt><dd><p>Object of class <code>"Uvector"</code> The observations whose score distance is larger 
than cutoff.sd or whose orthogonal distance is larger than cutoff.od can be considered 
as outliers and receive a flag equal to zero.
The regular observations receive a flag 1 </p>
</dd>
<dt><code>X</code>:</dt><dd><p>the training data set (same as the input parameter x of the constructor function)</p>
</dd>
<dt><code>grp</code>:</dt><dd><p>grouping variable:  a factor specifying the class for each observation.</p>
</dd>   
</dl>



<h3>Extends</h3>

<p>Class <code>"<a href="#topic+Simca-class">Simca</a>"</code>, directly.
</p>


<h3>Methods</h3>

<p>No methods defined with class &quot;CSimca&quot; in the signature.
</p>


<h3>Author(s)</h3>

<p> Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>Vanden Branden K, Hubert M (2005) Robust classification in high 
dimensions based on the SIMCA method. Chemometrics and 
Intellegent Laboratory Systems 79:10&ndash;21
</p>
<p>Todorov V &amp; Filzmoser P (2009),
An Object Oriented Framework for Robust Multivariate Analysis.
<em>Journal of Statistical Software</em>, <b>32</b>(3), 1&ndash;47,
<a href="https://doi.org/10.18637/jss.v032.i03">doi:10.18637/jss.v032.i03</a>.   
</p>
<p>Todorov V &amp; Filzmoser P (2014),
Software Tools for Robust Analysis of High-Dimensional Data.
<em>Austrian Journal of Statistics</em>, <b>43</b>(4),  255&ndash;266,
<a href="https://doi.org/10.17713/ajs.v43i4.44">doi:10.17713/ajs.v43i4.44</a>.     
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("CSimca")
</code></pre>

<hr>
<h2 id='getWeight-methods'>Accessor methods to the essential slots of <code><a href="#topic+Outlier-class">Outlier</a></code> and its subclasses</h2><span id='topic+getWeight-methods'></span><span id='topic+getWeight'></span><span id='topic+getOutliers-methods'></span><span id='topic+getOutliers'></span><span id='topic+getClassLabels-methods'></span><span id='topic+getClassLabels'></span><span id='topic+getCutoff-methods'></span><span id='topic+getCutoff'></span>

<h3>Description</h3>

<p>Accessor methods to the essential slots of <code><a href="#topic+Outlier-class">Outlier</a></code> and its subclasses
</p>


<h3>Methods</h3>


<dl>
<dt>obj = &quot;Outlier&quot;</dt><dd><p>generic functions - see <code>getWeight</code>, <code>getOutliers</code>, <code>getClassLabels</code>, <code>getCutoff</code></p>
</dd>
</dl>

<hr>
<h2 id='kibler'>
1985 Auto Imports Database
</h2><span id='topic+kibler'></span><span id='topic+kibler.orig'></span>

<h3>Description</h3>

<p>The original data set <code>kibler.orig</code> consists of three types of entities: 
(a) the specification of an auto in terms of various characteristics, 
(b) its assigned insurance risk rating and
(c) its normalized losses in use as compared to other cars.  
</p>
<p>The second rating corresponds to the degree to which the auto is more 
risky than its price indicates. Cars are initially assigned a risk 
factor symbol associated with its price.   Then, if it is more 
risky (or less), this symbol is adjusted by moving it up (or down) 
the scale.  Actuarians call this process &quot;symboling&quot;.  A value 
of +3 indicates that the auto is risky, -3 that it is probably pretty safe.
</p>
<p>The third factor is the relative average loss payment per insured 
vehicle year.  This value is normalized for all autos within a particular 
size classification (two-door small, station wagons, sports/speciality, etc...), 
and represents the average loss per car per year.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(kibler)</code></pre>


<h3>Format</h3>

<p>A data frame with 195 observations on the following 14 variables. The original 
data set (also available as <code>kibler.orig</code>) contains 205 cases and 
26 variables of which 15 continuous, 1 integer and 10 nominal. The 
non-numeric variables and variables with many missing values were removed. 
Cases with missing values were removed too. 
</p>

<dl>
<dt><code>symboling</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>wheel-base</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>length</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>width</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>height</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>curb-weight</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>bore</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>stroke</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>compression-ratio</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>horsepower</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>peak-rpm</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>city-mpg</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>highway-mpg</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>price</code></dt><dd><p>a numeric vector</p>
</dd>
</dl>



<h3>Details</h3>

<p>The original data set contains 205 cases and 26 variables of which 15 continuous, 1 integer and 10 nominal. The non-numeric variables and
variables with many missing values were removed. Cases with missing values were removed too. Thus the data set remains with 195 cases and 14 variables.
</p>


<h3>Source</h3>

<p>www.cs.umb.edu/~rickb/files/UCI/
</p>


<h3>References</h3>

<p>Kibler, D., Aha, D.W. and Albert, M. (1989).  Instance-based prediction
of real-valued attributes.  <em>Computational Intelligence</em>, Vo.l 5,
51-57. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(kibler)
x.sd &lt;- apply(kibler,2,sd)
xsd &lt;- sweep(kibler, 2, x.sd, "/", check.margin = FALSE)
apply(xsd, 2, sd)

x.mad &lt;- apply(kibler, 2, mad)
xmad &lt;- sweep(kibler, 2, x.mad, "/", check.margin = FALSE)
apply(xmad, 2, mad)

x.qn &lt;- apply(kibler, 2, Qn)
xqn &lt;- sweep(kibler, 2, x.qn, "/", check.margin = FALSE)
apply(xqn, 2, Qn)


## Display the scree plot of the classical and robust PCA
screeplot(PcaClassic(xsd))
screeplot(PcaGrid(xqn))

#########################################
##
## DD-plots
##
## Not run: 
usr &lt;- par(mfrow=c(2,2))
plot(SPcaGrid(xsd, lambda=0, method="sd", k=4), main="Standard PCA")    # standard
plot(SPcaGrid(xqn, lambda=0, method="Qn", k=4))                         # robust, non-sparse

plot(SPcaGrid(xqn, lambda=1,43, method="sd", k=4), main="Stdandard sparse PCA")  # sparse
plot(SPcaGrid(xqn, lambda=2.36, method="Qn", k=4), main="Robust sparse PCA")     # robust sparse
par(usr)

#########################################
##  Table 2 in Croux et al
##  - to compute EV=Explained variance and Cumulative EV we
##      need to get all 14 eigenvalues
##
rpca &lt;- SPcaGrid(xqn, lambda=0, k=14)
srpca &lt;- SPcaGrid(xqn, lambda=2.36, k=14)
tab &lt;- cbind(round(getLoadings(rpca)[,1:4], 2), round(getLoadings(srpca)[,1:4], 2))

vars1 &lt;- getEigenvalues(rpca);  vars1 &lt;- vars1/sum(vars1)
vars2 &lt;- getEigenvalues(srpca); vars2 &lt;- vars2/sum(vars2)
cvars1 &lt;- cumsum(vars1)
cvars2 &lt;- cumsum(vars2)
ev &lt;- round(c(vars1[1:4], vars2[1:4]),2)
cev &lt;- round(c(cvars1[1:4], cvars2[1:4]),2)
rbind(tab, ev, cev)

## End(Not run)

</code></pre>

<hr>
<h2 id='Outlier-class'>Class <code>"Outlier"</code> &ndash; a base class for outlier identification</h2><span id='topic+Outlier-class'></span><span id='topic+getClassLabels+2COutlier-method'></span><span id='topic+getDistance+2COutlier-method'></span><span id='topic+getFlag+2COutlier-method'></span><span id='topic+getOutliers+2COutlier-method'></span><span id='topic+getWeight+2COutlier-method'></span><span id='topic+plot+2COutlier+2Cmissing-method'></span><span id='topic+show+2COutlier-method'></span>

<h3>Description</h3>

<p>The class <code>Outlier</code> represents the results of outlier identification. 
</p>


<h3>Objects from the Class</h3>

<p>A virtual Class: No objects may be created from it.</p>


<h3>Slots</h3>


<dl>
<dt><code>call</code>:</dt><dd><p>Object of class <code>"language"</code></p>
</dd>
<dt><code>counts</code>:</dt><dd><p>Number of observations in each class</p>
</dd>
<dt><code>grp</code>:</dt><dd><p>Grouping variable</p>
</dd>
<dt><code>wt</code>:</dt><dd><p>Vector of weights</p>
</dd>
<dt><code>flag</code>:</dt><dd><p>0/1 flags identifying the outliers</p>
</dd>
<dt><code>method</code>:</dt><dd><p>A character string specifying the method used to 
identify the outliers. In case of <code><a href="#topic+OutlierMahdist-class">OutlierMahdist</a></code> 
class this is the name of the robust estimator
of multivariate location and covariance matrix used</p>
</dd>
<dt><code>singularity</code>:</dt><dd><p>a list with singularity 
information for the covariance matrix (or <code>NULL</code> 
if not singular)</p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>getClassLabels</dt><dd><p>Returns a vector with indices for a given class</p>
</dd>
<dt>getDistance</dt><dd><p>Returns a vector containing the computed distances </p>
</dd>
<dt>getFlag</dt><dd><p>Returns the flags identifying the outliers</p>
</dd>
<dt>getOutliers</dt><dd><p>Returns a vector with the indices of the identified outliers</p>
</dd>
<dt>getWeight</dt><dd><p>Returns a vector of weights</p>
</dd>
<dt>plot</dt><dd></dd>
<dt>show</dt><dd></dd>
</dl>



<h3>Author(s)</h3>

<p> Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> </p>


<h3>References</h3>

<p>Todorov V &amp; Filzmoser P (2009).
An Object Oriented Framework for Robust Multivariate Analysis.
<em>Journal of Statistical Software</em>, <b>32</b>(3), 1&ndash;47.
<a href="https://doi.org/10.18637/jss.v032.i03">doi:10.18637/jss.v032.i03</a>.   
</p>
<p>Filzmoser P &amp; Todorov V (2013).
Robust tools for the imperfect world,
<em>Information Sciences</em> <b>245</b>, 4&ndash;20.
<a href="https://doi.org/10.1016/j.ins.2012.10.017">doi:10.1016/j.ins.2012.10.017</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("Outlier")
</code></pre>

<hr>
<h2 id='OutlierMahdist'>
Outlier identification using robust (mahalanobis) distances based on robust multivariate location and covariance matrix 
</h2><span id='topic+OutlierMahdist'></span><span id='topic+OutlierMahdist.formula'></span><span id='topic+OutlierMahdist.default'></span>

<h3>Description</h3>

<p>This function uses the Mahalanobis distance as a basis for 
multivariate outlier detection. The standard method for multivariate 
outlier detection is robust estimation of the parameters in the 
Mahalanobis distance and the comparison with a critical value 
of the Chi2 distribution (Rousseeuw and Van Zomeren, 1990). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    OutlierMahdist(x, ...)
    ## Default S3 method:
OutlierMahdist(x, grouping, control, trace=FALSE, ...)
    ## S3 method for class 'formula'
OutlierMahdist(formula, data, ..., subset, na.action)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="OutlierMahdist_+3A_formula">formula</code></td>
<td>
<p>a formula with no response variable, referring only to
numeric variables.</p>
</td></tr>
<tr><td><code id="OutlierMahdist_+3A_data">data</code></td>
<td>
<p>an optional data frame (or similar: see
<code><a href="stats.html#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.</p>
</td></tr>
<tr><td><code id="OutlierMahdist_+3A_subset">subset</code></td>
<td>
<p>an optional vector used to select rows (observations) of the
data matrix <code>x</code>.</p>
</td></tr>
<tr><td><code id="OutlierMahdist_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen
when the data contain <code>NA</code>s.  The default is set by
the <code>na.action</code> setting of <code><a href="base.html#topic+options">options</a></code>, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The default is <code><a href="stats.html#topic+na.omit">na.omit</a></code>.</p>
</td></tr>
<tr><td><code id="OutlierMahdist_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="OutlierMahdist_+3A_x">x</code></td>
<td>
<p>a matrix or data frame. </p>
</td></tr>
<tr><td><code id="OutlierMahdist_+3A_grouping">grouping</code></td>
<td>
<p>grouping variable:  a factor specifying the class for each observation.</p>
</td></tr>
<tr><td><code id="OutlierMahdist_+3A_control">control</code></td>
<td>
<p> a control object (S4) for one of the available control classes,
e.g. <code><a href="rrcov.html#topic+CovControlMcd-class">CovControlMcd-class</a></code>, <code><a href="rrcov.html#topic+CovControlOgk-class">CovControlOgk-class</a></code>,
<code><a href="rrcov.html#topic+CovControlSest-class">CovControlSest-class</a></code>, etc.,
containing estimation options. The class of this object defines
which estimator will be used. Alternatively a character string can be specified
which names the estimator - one of auto, sde, mcd, ogk, m, mve, sfast, surreal,
bisquare, rocke. If 'auto' is specified or the argument is missing, the
function will select the estimator (see below for details)</p>
</td></tr>
<tr><td><code id="OutlierMahdist_+3A_trace">trace</code></td>
<td>
<p>whether to print intermediate results. Default is <code>trace = FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the data set consists of two or more classes 
(specified by the grouping variable <code>grouping</code>) the proposed method iterates
through the classes present in the data, separates each class from the rest and
identifies the outliers relative to this class, thus treating both types of outliers,
the mislabeled and the abnormal samples in a homogenous way.
</p>
<p>The estimation method is selected by the control object <code>control</code>. 
If a character string naming an estimator is specified, a
new control object will be created and used (with default estimation options).
If this argument is missing or a character string
'auto' is specified, the function will select the robust estimator
according to the size of the dataset - for details see <code><a href="rrcov.html#topic+CovRobust">CovRobust</a></code>.
</p>


<h3>Value</h3>

<p>An S4 object of class <code><a href="#topic+OutlierMahdist-class">OutlierMahdist</a></code> which 
is a subclass of the virtual class <code><a href="#topic+Outlier-class">Outlier</a></code>.
</p>


<h3>Author(s)</h3>

<p> Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>P. J. Rousseeuw and B. C. Van Zomeren (1990). 
Unmasking multivariate outliers and leverage points. 
<em>Journal of the American Statistical Association</em>. 
Vol. 85(411), pp. 633-651.
</p>
<p>P. J. Rousseeuw and A. M. Leroy (1987).
<em>Robust Regression and Outlier Detection.</em> Wiley.
</p>
<p>P. J. Rousseeuw and K. van Driessen (1999)
A fast algorithm for the minimum covariance determinant estimator.
<em>Technometrics</em> <b>41</b>, 212&ndash;223.
</p>
<p>Todorov V &amp; Filzmoser P (2009).
An Object Oriented Framework for Robust Multivariate Analysis.
<em>Journal of Statistical Software</em>, <b>32</b>(3), 1&ndash;47,
<a href="https://doi.org/10.18637/jss.v032.i03">doi:10.18637/jss.v032.i03</a>.   
</p>
<p>Filzmoser P &amp; Todorov V (2013).
Robust tools for the imperfect world,
<em>Information Sciences</em> <b>245</b>, 4&ndash;20.
<a href="https://doi.org/10.1016/j.ins.2012.10.017">doi:10.1016/j.ins.2012.10.017</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(hemophilia)
obj &lt;- OutlierMahdist(gr~.,data=hemophilia)
obj

getDistance(obj)            # returns an array of distances
getClassLabels(obj, 1)      # returns an array of indices for a given class
getCutoff(obj)              # returns an array of cutoff values (for each class, usually equal)
getFlag(obj)                #  returns an 0/1 array of flags
plot(obj, class=2)          # standard plot function
</code></pre>

<hr>
<h2 id='OutlierMahdist-class'>Class <code>OutlierMahdist</code> - Outlier identification using 
robust (mahalanobis) distances based on robust multivariate 
location and covariance matrix</h2><span id='topic+OutlierMahdist-class'></span><span id='topic+getCutoff+2COutlierMahdist-method'></span><span id='topic+getDistance+2COutlierMahdist-method'></span>

<h3>Description</h3>

<p>Holds the results of outlier identification using robust mahalanobis 
distances computed by robust multivarite location and covarince matrix.   
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("OutlierMahdist", ...)</code> but the
usual way of creating <code>OutlierMahdist</code> objects is a call to the function
<code>OutlierMahdist()</code> which serves as a constructor.
</p>


<h3>Slots</h3>


<dl>
<dt><code>covobj</code>:</dt><dd><p>A list containing the robust estimates 
of multivariate location and covariance matrix for each class</p>
</dd>
<dt><code>call</code>:</dt><dd><p>Object of class <code>"language"</code></p>
</dd>
<dt><code>counts</code>:</dt><dd><p>Number of observations in each class</p>
</dd>
<dt><code>grp</code>:</dt><dd><p>Grouping variable</p>
</dd>
<dt><code>wt</code>:</dt><dd><p>Weights</p>
</dd>
<dt><code>flag</code>:</dt><dd><p>0/1 flags identifying the outliers</p>
</dd>
<dt><code>method</code>:</dt><dd><p>Method used to compute the robust estimates 
of multivariate location and covariance matrix</p>
</dd>
<dt><code>singularity</code>:</dt><dd><p>a list with singularity 
information for the covariance matrix (or <code>NULL</code> 
of not singular)</p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code>"<a href="#topic+Outlier-class">Outlier</a>"</code>, directly.
</p>


<h3>Methods</h3>


<dl>
<dt>getCutoff</dt><dd><p>Return the cutoff value used to identify outliers </p>
</dd>
<dt>getDistance</dt><dd><p>Return a vector containing the computed distances </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> </p>


<h3>References</h3>

<p>Todorov V &amp; Filzmoser P (2009).
An Object Oriented Framework for Robust Multivariate Analysis.
<em>Journal of Statistical Software</em>, <b>32</b>(3), 1&ndash;47.
<a href="https://doi.org/10.18637/jss.v032.i03">doi:10.18637/jss.v032.i03</a>.   
</p>
<p>Filzmoser P &amp; Todorov V (2013).
Robust tools for the imperfect world,
<em>Information Sciences</em> <b>245</b>, 4&ndash;20.
<a href="https://doi.org/10.1016/j.ins.2012.10.017">doi:10.1016/j.ins.2012.10.017</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+OutlierMahdist">OutlierMahdist</a></code>, <code><a href="#topic+Outlier-class">Outlier-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("OutlierMahdist")
</code></pre>

<hr>
<h2 id='OutlierPCDist'>
Outlier identification in high dimensions using the PCDIST algorithm 
</h2><span id='topic+OutlierPCDist'></span><span id='topic+OutlierPCDist.formula'></span><span id='topic+OutlierPCDist.default'></span>

<h3>Description</h3>

<p>The function implements a simple, automatic outlier detection method suitable
for high dimensional data that treats each class independently and uses 
a statistically principled threshold for outliers. The algorithm can 
detect both mislabeled and abnormal samples without reference to other classes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    OutlierPCDist(x, ...)
    ## Default S3 method:
OutlierPCDist(x, grouping, control, k, explvar, trace=FALSE, ...)
    ## S3 method for class 'formula'
OutlierPCDist(formula, data, ..., subset, na.action)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="OutlierPCDist_+3A_formula">formula</code></td>
<td>
<p>a formula with no response variable, referring only to
numeric variables.</p>
</td></tr>
<tr><td><code id="OutlierPCDist_+3A_data">data</code></td>
<td>
<p>an optional data frame (or similar: see
<code><a href="stats.html#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.</p>
</td></tr>
<tr><td><code id="OutlierPCDist_+3A_subset">subset</code></td>
<td>
<p>an optional vector used to select rows (observations) of the
data matrix <code>x</code>.</p>
</td></tr>
<tr><td><code id="OutlierPCDist_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen
when the data contain <code>NA</code>s.  The default is set by
the <code>na.action</code> setting of <code><a href="base.html#topic+options">options</a></code>, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The default is <code><a href="stats.html#topic+na.omit">na.omit</a></code>.</p>
</td></tr>
<tr><td><code id="OutlierPCDist_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="OutlierPCDist_+3A_x">x</code></td>
<td>
<p>a matrix or data frame. </p>
</td></tr>
<tr><td><code id="OutlierPCDist_+3A_grouping">grouping</code></td>
<td>
<p>grouping variable:  a factor specifying the class for each observation.</p>
</td></tr>
<tr><td><code id="OutlierPCDist_+3A_control">control</code></td>
<td>
<p> a control object (S4) for one of the available control classes,
e.g. <code><a href="rrcov.html#topic+CovControlMcd-class">CovControlMcd-class</a></code>, <code><a href="rrcov.html#topic+CovControlOgk-class">CovControlOgk-class</a></code>,
<code><a href="rrcov.html#topic+CovControlSest-class">CovControlSest-class</a></code>, etc.,
containing estimation options. The class of this object defines
which estimator will be used. Alternatively a character string can be specified
which names the estimator - one of auto, sde, mcd, ogk, m, mve, sfast, surreal,
bisquare, rocke. If 'auto' is specified or the argument is missing, the
function will select the estimator (see below for details)</p>
</td></tr>
<tr><td><code id="OutlierPCDist_+3A_k">k</code></td>
<td>
<p>Number of components to select for PCA. If missing, the number 
of components will be calculated automatically</p>
</td></tr>
<tr><td><code id="OutlierPCDist_+3A_explvar">explvar</code></td>
<td>
<p>Minimal explained variance to be used for calculation of 
the number of components in PCA. If <code>explvar</code> is not provided, 
automatic dimensionality selection using profile likelihood, as 
proposed by Zhu and Ghodsi will be used.</p>
</td></tr>
<tr><td><code id="OutlierPCDist_+3A_trace">trace</code></td>
<td>
<p>whether to print intermediate results. Default is <code>trace = FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the data set consists of two or more classes 
(specified by the grouping variable <code>grouping</code>) the proposed method iterates
through the classes present in the data, separates each class from the rest and
identifies the outliers relative to this class, thus treating both types of outliers,
the mislabeled and the abnormal samples in a homogenous way.
</p>
<p>The first step of the algorithm is dimensionality reduction using (classical) PCA. 
The number of components to select can be provided by the user but if missing, 
the number of components will be calculated either using the provided minimal 
explained variance or by the automatic dimensionality selection using 
profile likelihood, as proposed by Zhu and Ghodsi.
</p>


<h3>Value</h3>

<p>An S4 object of class <code><a href="#topic+OutlierPCDist-class">OutlierPCDist</a></code> which 
is a subclass of the virtual class <code><a href="#topic+Outlier-class">Outlier</a></code>.
</p>


<h3>Author(s)</h3>

<p> Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>A.D. Shieh and Y.S. Hung  (2009).
Detecting Outlier Samples in Microarray Data,
<em>Statistical Applications in Genetics and Molecular Biology</em> <b>8</b>.
</p>
<p>M. Zhu, and A. Ghodsi (2006). Automatic dimensionality selection from the scree
plot via the use of profile likelihood. <em>Computational Statistics &amp; Data Analysis</em>,
<b>51</b>, pp. 918&ndash;930.
</p>
<p>Filzmoser P &amp; Todorov V (2013).
Robust tools for the imperfect world,
<em>Information Sciences</em> <b>245</b>, 4&ndash;20.
<a href="https://doi.org/10.1016/j.ins.2012.10.017">doi:10.1016/j.ins.2012.10.017</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+OutlierPCDist-class">OutlierPCDist</a></code>, <code><a href="#topic+Outlier-class">Outlier</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(hemophilia)
obj &lt;- OutlierPCDist(gr~.,data=hemophilia)
obj

getDistance(obj)            # returns an array of distances
getClassLabels(obj, 1)      # returns an array of indices for a given class
getCutoff(obj)              # returns an array of cutoff values (for each class, usually equal)
getFlag(obj)                #  returns an 0/1 array of flags
plot(obj, class=2)          # standard plot function
</code></pre>

<hr>
<h2 id='OutlierPCDist-class'>Class <code>"OutlierPCDist"</code> - Outlier identification in high dimensions using using the PCDIST algorithm</h2><span id='topic+OutlierPCDist-class'></span><span id='topic+getCutoff+2COutlierPCDist-method'></span><span id='topic+getDistance+2COutlierPCDist-method'></span>

<h3>Description</h3>

<p>The function implements a simple, automatic outlier detection method suitable
for high dimensional data that treats each class independently and uses 
a statistically principled threshold for outliers. The algorithm can 
detect both mislabeled and abnormal samples without reference to other classes.
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("OutlierPCDist", ...)</code> but the
usual way of creating <code>OutlierPCDist</code> objects is a call to the function
<code>OutlierPCDist()</code> which serves as a constructor.
</p>


<h3>Slots</h3>


<dl>
<dt><code>covobj</code>:</dt><dd><p>A list containing intermediate results of the PCDIST algorithm for each class</p>
</dd>
<dt><code>k</code>:</dt><dd><p>Number of selected PC</p>
</dd>
<dt><code>call</code>, <code>counts</code>, <code>grp</code>, <code>wt</code>,
<code>flag</code>, <code>method</code>, <code>singularity</code>:</dt><dd><p>from the <code>"<a href="#topic+Outlier-class">Outlier</a>"</code> class.</p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code>"<a href="#topic+Outlier-class">Outlier</a>"</code>, directly.
</p>


<h3>Methods</h3>


<dl>
<dt>getCutoff</dt><dd><p>Return the cutoff value used to identify outliers </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>A.D. Shieh and Y.S. Hung  (2009).
Detecting Outlier Samples in Microarray Data,
<em>Statistical Applications in Genetics and Molecular Biology</em> Vol. 8.
</p>
<p>Filzmoser P &amp; Todorov V (2013).
Robust tools for the imperfect world,
<em>Information Sciences</em> <b>245</b>, 4&ndash;20.
<a href="https://doi.org/10.1016/j.ins.2012.10.017">doi:10.1016/j.ins.2012.10.017</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+OutlierPCDist">OutlierPCDist</a></code>, <code><a href="#topic+Outlier-class">Outlier</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("OutlierPCDist")
</code></pre>

<hr>
<h2 id='OutlierPCOut'>
Outlier identification in high dimensions using the PCOUT algorithm 
</h2><span id='topic+OutlierPCOut'></span><span id='topic+OutlierPCOut.formula'></span><span id='topic+OutlierPCOut.default'></span>

<h3>Description</h3>

<p>The function implements a computationally fast procedure for identifying 
outliers that is particularly effective in high dimensions. This algorithm 
utilizes simple properties of principal components to identify outliers in 
the transformed space, leading to significant computational advantages 
for high-dimensional data. This approach requires considerably less 
computational time than existing methods for outlier detection, and is 
suitable for use on very large data sets. It is also capable of analyzing 
the data situation commonly found in certain biological applications in which 
the number of dimensions is several orders of magnitude larger than the number of observations. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    OutlierPCOut(x, ...)
    ## Default S3 method:
OutlierPCOut(x, grouping, explvar=0.99, trace=FALSE, ...)
    ## S3 method for class 'formula'
OutlierPCOut(formula, data, ..., subset, na.action)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="OutlierPCOut_+3A_formula">formula</code></td>
<td>
<p>a formula with no response variable, referring only to
numeric variables.</p>
</td></tr>
<tr><td><code id="OutlierPCOut_+3A_data">data</code></td>
<td>
<p>an optional data frame (or similar: see
<code><a href="stats.html#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.</p>
</td></tr>
<tr><td><code id="OutlierPCOut_+3A_subset">subset</code></td>
<td>
<p>an optional vector used to select rows (observations) of the
data matrix <code>x</code>.</p>
</td></tr>
<tr><td><code id="OutlierPCOut_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen
when the data contain <code>NA</code>s.  The default is set by
the <code>na.action</code> setting of <code><a href="base.html#topic+options">options</a></code>, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The default is <code><a href="stats.html#topic+na.omit">na.omit</a></code>.</p>
</td></tr>
<tr><td><code id="OutlierPCOut_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="OutlierPCOut_+3A_x">x</code></td>
<td>
<p>a matrix or data frame. </p>
</td></tr>
<tr><td><code id="OutlierPCOut_+3A_grouping">grouping</code></td>
<td>
<p>grouping variable:  a factor specifying the class for each observation.</p>
</td></tr>
<tr><td><code id="OutlierPCOut_+3A_explvar">explvar</code></td>
<td>
<p> a numeric value between 0 and 1 indicating how much variance
should be covered by the robust PCs (default to 0.99) </p>
</td></tr>    
<tr><td><code id="OutlierPCOut_+3A_trace">trace</code></td>
<td>
<p>whether to print intermediate results. Default is <code>trace = FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the data set consists of two or more classes 
(specified by the grouping variable <code>grouping</code>) the proposed method iterates
through the classes present in the data, separates each class from the rest and
identifies the outliers relative to this class, thus treating both types of outliers,
the mislabeled and the abnormal samples in a homogenous way.
</p>


<h3>Value</h3>

<p>An S4 object of class <code><a href="#topic+OutlierPCOut-class">OutlierPCOut</a></code> which 
is a subclass of the virtual class <code><a href="#topic+Outlier-class">Outlier</a></code>.
</p>


<h3>Author(s)</h3>

<p> Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>P. Filzmoser, R. Maronna and M. Werner (2008).
Outlier identification in high dimensions,
<em>Computational Statistics &amp; Data Analysis</em>, Vol. 52 1694&ndash;1711.
</p>
<p>Filzmoser P &amp; Todorov V (2013).
Robust tools for the imperfect world,
<em>Information Sciences</em> <b>245</b>, 4&ndash;20.
<a href="https://doi.org/10.1016/j.ins.2012.10.017">doi:10.1016/j.ins.2012.10.017</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+OutlierPCOut-class">OutlierPCOut</a></code>, <code><a href="#topic+Outlier-class">Outlier</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(hemophilia)
obj &lt;- OutlierPCOut(gr~.,data=hemophilia)
obj

getDistance(obj)            # returns an array of distances
getClassLabels(obj, 1)      # returns an array of indices for a given class
getCutoff(obj)              # returns an array of cutoff values (for each class, usually equal)
getFlag(obj)                #  returns an 0/1 array of flags
plot(obj, class=2)          # standard plot function
</code></pre>

<hr>
<h2 id='OutlierPCOut-class'>Class <code>"OutlierPCOut"</code> - Outlier identification in high dimensions using using the PCOUT algorithm</h2><span id='topic+OutlierPCOut-class'></span><span id='topic+getCutoff+2COutlierPCOut-method'></span><span id='topic+getDistance+2COutlierPCOut-method'></span><span id='topic+plot+2COutlierPCOut+2Cmissing-method'></span>

<h3>Description</h3>

<p>Holds the results of outlier identification using the PCOUT algorithm.   
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("OutlierPCOut", ...)</code> but the
usual way of creating <code>OutlierPCOut</code> objects is a call to the function
<code>OutlierPCOut()</code> which serves as a constructor.
</p>


<h3>Slots</h3>


<dl>
<dt><code>covobj</code>:</dt><dd><p>A list containing intermediate results of the PCOUT algorithm for each class</p>
</dd>
<dt><code>call</code>, <code>counts</code>, <code>grp</code>, <code>wt</code>,
<code>flag</code>, <code>method</code>, <code>singularity</code>:</dt><dd><p>from the <code>"<a href="#topic+Outlier-class">Outlier</a>"</code> class.</p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code>"<a href="#topic+Outlier-class">Outlier</a>"</code>, directly.
</p>


<h3>Methods</h3>


<dl>
<dt>getCutoff</dt><dd><p>Return the cutoff value used to identify outliers </p>
</dd>
<dt>getDistance</dt><dd><p>Return a vector containing the computed distances </p>
</dd>
<dt>plot</dt><dd><p>Plot the results of the outlier detection process</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> </p>


<h3>References</h3>

<p>P. Filzmoser, R. Maronna and M. Werner (2008).
Outlier identification in high dimensions,
<em>Computational Statistics &amp; Data Analysis</em>, Vol. 52 1694&ndash;1711.
</p>
<p>Filzmoser P &amp; Todorov V (2013).
Robust tools for the imperfect world,
<em>Information Sciences</em> <b>245</b>, 4&ndash;20.
<a href="https://doi.org/10.1016/j.ins.2012.10.017">doi:10.1016/j.ins.2012.10.017</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+OutlierPCOut">OutlierPCOut</a></code>, <code>"<a href="#topic+Outlier-class">Outlier</a>"</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("OutlierMahdist")
</code></pre>

<hr>
<h2 id='OutlierSign1'>
Outlier identification in high dimensions using the SIGN1 algorithm 
</h2><span id='topic+OutlierSign1'></span><span id='topic+OutlierSign1.formula'></span><span id='topic+OutlierSign1.default'></span>

<h3>Description</h3>

<p>Fast algorithm for identifying multivariate outliers in high-dimensional 
and/or large datasets, using spatial signs, see Filzmoser, Maronna, and Werner (CSDA, 2007). 
The computation of the distances is based on Mahalanobis distances. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    OutlierSign1(x, ...)
    ## Default S3 method:
OutlierSign1(x, grouping, qcrit = 0.975, trace=FALSE, ...)
    ## S3 method for class 'formula'
OutlierSign1(formula, data, ..., subset, na.action)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="OutlierSign1_+3A_formula">formula</code></td>
<td>
<p>a formula with no response variable, referring only to
numeric variables.</p>
</td></tr>
<tr><td><code id="OutlierSign1_+3A_data">data</code></td>
<td>
<p>an optional data frame (or similar: see
<code><a href="stats.html#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.</p>
</td></tr>
<tr><td><code id="OutlierSign1_+3A_subset">subset</code></td>
<td>
<p>an optional vector used to select rows (observations) of the
data matrix <code>x</code>.</p>
</td></tr>
<tr><td><code id="OutlierSign1_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen
when the data contain <code>NA</code>s.  The default is set by
the <code>na.action</code> setting of <code><a href="base.html#topic+options">options</a></code>, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The default is <code><a href="stats.html#topic+na.omit">na.omit</a></code>.</p>
</td></tr>
<tr><td><code id="OutlierSign1_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="OutlierSign1_+3A_x">x</code></td>
<td>
<p>a matrix or data frame. </p>
</td></tr>
<tr><td><code id="OutlierSign1_+3A_grouping">grouping</code></td>
<td>
<p>grouping variable:  a factor specifying the class for each observation.</p>
</td></tr>
<tr><td><code id="OutlierSign1_+3A_qcrit">qcrit</code></td>
<td>
<p>a numeric value between 0 and 1 indicating the quantile to be used as critical value for outlier detection (default to 0.975).</p>
</td></tr>
<tr><td><code id="OutlierSign1_+3A_trace">trace</code></td>
<td>
<p>whether to print intermediate results. Default is <code>trace = FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Based on the robustly sphered and normed data, robust principal components are 
computed. These are used for computing the covariance matrix which is the basis 
for Mahalanobis distances. A critical value from the chi-square distribution 
is then used as outlier cutoff. </p>


<h3>Value</h3>

<p>An S4 object of class <code><a href="#topic+OutlierSign1-class">OutlierSign1</a></code> which 
is a subclass of the virtual class <code><a href="#topic+Outlier-class">Outlier</a></code>.
</p>


<h3>Author(s)</h3>

<p> Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>P. Filzmoser, R. Maronna and M. Werner (2008).
Outlier identification in high dimensions,
<em>Computational Statistics &amp; Data Analysis</em>, Vol. 52 1694&ndash;1711.
</p>
<p>Filzmoser P &amp; Todorov V (2013).
Robust tools for the imperfect world,
<em>Information Sciences</em> <b>245</b>, 4&ndash;20.
<a href="https://doi.org/10.1016/j.ins.2012.10.017">doi:10.1016/j.ins.2012.10.017</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+OutlierSign1-class">OutlierSign1</a></code>, <code><a href="#topic+OutlierSign2-class">OutlierSign2</a></code>, <code><a href="#topic+Outlier-class">Outlier</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(hemophilia)
obj &lt;- OutlierSign1(gr~.,data=hemophilia)
obj

getDistance(obj)            # returns an array of distances
getClassLabels(obj, 1)      # returns an array of indices for a given class
getCutoff(obj)              # returns an array of cutoff values (for each class, usually equal)
getFlag(obj)                #  returns an 0/1 array of flags
plot(obj, class=2)          # standard plot function
</code></pre>

<hr>
<h2 id='OutlierSign1-class'>Class <code>"OutlierSign1"</code> - Outlier identification in high dimensions using the SIGN1 algorithm</h2><span id='topic+OutlierSign1-class'></span><span id='topic+getCutoff+2COutlierSign1-method'></span><span id='topic+getDistance+2COutlierSign1-method'></span>

<h3>Description</h3>

<p>Fast algorithm for identifying multivariate outliers in high-dimensional 
and/or large datasets, using spatial signs, see Filzmoser, Maronna, and Werner (CSDA, 2007). 
The computation of the distances is based on Mahalanobis distances. 
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("OutlierSign1", ...)</code> but the
usual way of creating <code>OutlierSign1</code> objects is a call to the function
<code>OutlierSign1()</code> which serves as a constructor.
</p>


<h3>Slots</h3>


<dl>
<dt><code>covobj</code>:</dt><dd><p>A list containing intermediate results of the SIGN1 algorithm for each class</p>
</dd>
<dt><code>call</code>, <code>counts</code>, <code>grp</code>, <code>wt</code>,
<code>flag</code>, <code>method</code>, <code>singularity</code>:</dt><dd><p>from the <code>"<a href="#topic+Outlier-class">Outlier</a>"</code> class.</p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code>"<a href="#topic+Outlier-class">Outlier</a>"</code>, directly.
</p>


<h3>Methods</h3>


<dl>
<dt>getCutoff</dt><dd><p>Return the cutoff value used to identify outliers </p>
</dd>
<dt>getDistance</dt><dd><p>Return a vector containing the computed distances </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>P. Filzmoser, R. Maronna and M. Werner (2008).
Outlier identification in high dimensions,
<em>Computational Statistics &amp; Data Analysis</em>, Vol. 52 1694&ndash;1711.
</p>
<p>Filzmoser P &amp; Todorov V (2013).
Robust tools for the imperfect world,
<em>Information Sciences</em> <b>245</b>, 4&ndash;20.
<a href="https://doi.org/10.1016/j.ins.2012.10.017">doi:10.1016/j.ins.2012.10.017</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+OutlierSign1">OutlierSign1</a></code>, <code><a href="#topic+OutlierSign2-class">OutlierSign2</a></code>, <code><a href="#topic+Outlier-class">Outlier</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("OutlierSign1")
</code></pre>

<hr>
<h2 id='OutlierSign2'>
Outlier identification in high dimensions using the SIGN2 algorithm 
</h2><span id='topic+OutlierSign2'></span><span id='topic+OutlierSign2.formula'></span><span id='topic+OutlierSign2.default'></span>

<h3>Description</h3>

<p>Fast algorithm for identifying multivariate outliers in high-dimensional 
and/or large datasets, using spatial signs, see Filzmoser, Maronna, and Werner (CSDA, 2007). 
The computation of the distances is based on principal components. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    OutlierSign2(x, ...)
    ## Default S3 method:
OutlierSign2(x, grouping, qcrit = 0.975, explvar=0.99, trace=FALSE, ...)
    ## S3 method for class 'formula'
OutlierSign2(formula, data, ..., subset, na.action)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="OutlierSign2_+3A_formula">formula</code></td>
<td>
<p>a formula with no response variable, referring only to
numeric variables.</p>
</td></tr>
<tr><td><code id="OutlierSign2_+3A_data">data</code></td>
<td>
<p>an optional data frame (or similar: see
<code><a href="stats.html#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.</p>
</td></tr>
<tr><td><code id="OutlierSign2_+3A_subset">subset</code></td>
<td>
<p>an optional vector used to select rows (observations) of the
data matrix <code>x</code>.</p>
</td></tr>
<tr><td><code id="OutlierSign2_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen
when the data contain <code>NA</code>s.  The default is set by
the <code>na.action</code> setting of <code><a href="base.html#topic+options">options</a></code>, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The default is <code><a href="stats.html#topic+na.omit">na.omit</a></code>.</p>
</td></tr>
<tr><td><code id="OutlierSign2_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="OutlierSign2_+3A_x">x</code></td>
<td>
<p>a matrix or data frame. </p>
</td></tr>
<tr><td><code id="OutlierSign2_+3A_grouping">grouping</code></td>
<td>
<p>grouping variable:  a factor specifying the class for each observation.</p>
</td></tr>
<tr><td><code id="OutlierSign2_+3A_explvar">explvar</code></td>
<td>
<p>a numeric value between 0 and 1 indicating how much variance should be covered by the robust PCs. Default is 0.99.</p>
</td></tr>
<tr><td><code id="OutlierSign2_+3A_qcrit">qcrit</code></td>
<td>
<p>a numeric value between 0 and 1 indicating the quantile to be used as critical value for outlier detection. Default is 0.975.</p>
</td></tr>
<tr><td><code id="OutlierSign2_+3A_trace">trace</code></td>
<td>
<p>whether to print intermediate results. Default is <code>trace = FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Based on the robustly sphered and normed data, robust principal components are 
computed which are needed for determining distances for each observation. 
The distances are transformed to approach chi-square distribution, and 
a critical value is then used as outlier cutoff. </p>


<h3>Value</h3>

<p>An S4 object of class <code><a href="#topic+OutlierSign2-class">OutlierSign2</a></code> which 
is a subclass of the virtual class <code><a href="#topic+Outlier-class">Outlier</a></code>.
</p>


<h3>Author(s)</h3>

<p> Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>P. Filzmoser, R. Maronna and M. Werner (2008).
Outlier identification in high dimensions,
<em>Computational Statistics &amp; Data Analysis</em>, Vol. 52 1694&ndash;1711.
</p>
<p>Filzmoser P &amp; Todorov V (2013).
Robust tools for the imperfect world,
<em>Information Sciences</em> <b>245</b>, 4&ndash;20.
<a href="https://doi.org/10.1016/j.ins.2012.10.017">doi:10.1016/j.ins.2012.10.017</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+OutlierSign2-class">OutlierSign2</a></code>, <code><a href="#topic+OutlierSign1-class">OutlierSign1</a></code>, <code><a href="#topic+Outlier-class">Outlier</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(hemophilia)
obj &lt;- OutlierSign2(gr~.,data=hemophilia)
obj

getDistance(obj)            # returns an array of distances
getClassLabels(obj, 1)      # returns an array of indices for a given class
getCutoff(obj)              # returns an array of cutoff values (for each class, usually equal)
getFlag(obj)                # returns an 0/1 array of flags
plot(obj, class=2)          # standard plot function
</code></pre>

<hr>
<h2 id='OutlierSign2-class'>Class <code>"OutlierSign2"</code> - Outlier identification in high dimensions using the SIGN2 algorithm</h2><span id='topic+OutlierSign2-class'></span><span id='topic+getCutoff+2COutlierSign2-method'></span><span id='topic+getDistance+2COutlierSign2-method'></span>

<h3>Description</h3>

<p>Fast algorithm for identifying multivariate outliers in high-dimensional 
and/or large datasets, using spatial signs, see Filzmoser, Maronna, and Werner (CSDA, 2007). 
The computation of the distances is based on principal components. 
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("OutlierSign2", ...)</code> but the
usual way of creating <code>OutlierSign2</code> objects is a call to the function
<code>OutlierSign2()</code> which serves as a constructor.
</p>


<h3>Slots</h3>


<dl>
<dt><code>covobj</code>:</dt><dd><p>A list containing intermediate results of the SIGN2 algorithm for each class</p>
</dd>
<dt><code>call</code>, <code>counts</code>, <code>grp</code>, <code>wt</code>,
<code>flag</code>, <code>method</code>, <code>singularity</code>:</dt><dd><p>from the <code>"<a href="#topic+Outlier-class">Outlier</a>"</code> class.</p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code>"<a href="#topic+Outlier-class">Outlier</a>"</code>, directly.
</p>


<h3>Methods</h3>


<dl>
<dt>getCutoff</dt><dd><p>Return the cutoff value used to identify outliers </p>
</dd>
<dt>getDistance</dt><dd><p>Return a vector containing the computed distances </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>P. Filzmoser, R. Maronna and M. Werner (2008).
Outlier identification in high dimensions,
<em>Computational Statistics &amp; Data Analysis</em>, Vol. 52 1694&ndash;1711.
</p>
<p>Filzmoser P &amp; Todorov V (2013).
Robust tools for the imperfect world,
<em>Information Sciences</em> <b>245</b>, 4&ndash;20.
<a href="https://doi.org/10.1016/j.ins.2012.10.017">doi:10.1016/j.ins.2012.10.017</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+OutlierSign2">OutlierSign2</a></code>, <code><a href="#topic+OutlierSign1-class">OutlierSign1</a></code>, <code><a href="#topic+Outlier-class">Outlier</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("OutlierSign2")
</code></pre>

<hr>
<h2 id='PredictSimca-class'>Class <code>"PredictSimca"</code> - prediction of <code>"Simca"</code> objects</h2><span id='topic+PredictSimca-class'></span><span id='topic+show+2CPredictSimca-method'></span>

<h3>Description</h3>

<p> The prediction of a &quot;Simca&quot; object </p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("PredictSimca", ...)</code>
but most often by invoking <code>predict()</code> on a <code>"Simca"</code> object. 
They contain values meant for printing by <code>show()</code></p>


<h3>Slots</h3>


<dl>
<dt><code>classification</code>:</dt><dd><p>Object of class <code>"factor"</code> ~~ </p>
</dd>
<dt><code>odsc</code>:</dt><dd><p>A <code>"matrix"</code> containing the standartized orthogonal distances for each group</p>
</dd>
<dt><code>sdsc</code>:</dt><dd><p>A <code>"matrix"</code> containing the standartized score distances for each group</p>
</dd>
<dt><code>ct</code>:</dt><dd><p>re-classification table of the training sample</p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>show</dt><dd><p><code>signature(object = "PredictSimca")</code>: Prints the results.. </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a></p>


<h3>References</h3>

<p>Todorov V &amp; Filzmoser P (2009),
An Object Oriented Framework for Robust Multivariate Analysis.
<em>Journal of Statistical Software</em>, <b>32</b>(3), 1&ndash;47.
<a href="https://doi.org/10.18637/jss.v032.i03">doi:10.18637/jss.v032.i03</a>.   
</p>
<p>Todorov V &amp; Filzmoser P (2014),
Software Tools for Robust Analysis of High-Dimensional Data.
<em>Austrian Journal of Statistics</em>, <b>43</b>(4),  255&ndash;266,
<a href="https://doi.org/10.17713/ajs.v43i4.44">doi:10.17713/ajs.v43i4.44</a>.     
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Simca-class">Simca-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("PredictSimca")
</code></pre>

<hr>
<h2 id='PredictSosDisc-class'>Class <code>"PredictSosDisc"</code> - prediction of <code>"SosDisc"</code> objects</h2><span id='topic+PredictSosDisc-class'></span><span id='topic+show+2CPredictSosDisc-method'></span>

<h3>Description</h3>

<p> The prediction of a &quot;SosDisc&quot; object </p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("PredictSosDisc", ...)</code>
but most often by invoking <code>predict()</code> on a <code>"SosDisc"</code> object. 
They contain values meant for printing by <code>show()</code></p>


<h3>Slots</h3>


<dl>
<dt><code>classification</code>:</dt><dd><p>Object of class <code>"factor"</code> representing the predicted classification </p>
</dd>
<dt><code>mahadist2</code>:</dt><dd><p>A <code>"matrix"</code> containing the squared robust Mahalanobis distances to each group center in the subspace (see Details).</p>
</dd>
<dt><code>w</code>:</dt><dd><p>A <code>"vector"</code> containing the weights derived from robust Mahalanobis distances to the closest group center (see Details).</p>
</dd>
</dl>



<h3>Details</h3>

<p>For the prediction of the class membership a two step approach is taken. 
First, the <code>newdata</code> are scaled and centered (by obj@scale and obj@center) 
and  multiplied by <code>obj@beta</code> for dimension reduction. Then the 
classification of the transformed data is obtained by prediction with 
the <code>Linda</code> object obj@fit. The Mahalanobis distances to the closest 
group center in this subspace is used to derive case weights <code>w</code>. 
Observations where the squared robust mahalanobis distance is larger 
than the 0.975 quantile of the chi-square distribution with Q degrees 
of freedom receive weight zero, all others weight one.
</p>


<h3>Methods</h3>


<dl>
<dt>show</dt><dd><p><code>signature(object = "PredictSosDisc")</code>: Prints the results.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Irene Ortner <a href="mailto:irene.ortner@applied-statistics.at">irene.ortner@applied-statistics.at</a> and Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>Clemmensen L, Hastie T, Witten D &amp; Ersboll B (2012),
Sparse discriminant analysis.
<em>Technometrics</em>, <b>53</b>(4), 406&ndash;413. 
</p>
<p>Ortner I, Filzmoser P &amp; Croux C (2020),
Robust and sparse multigroup classification by the optimal scoring approach.
Data Mining and Knowledge Discovery <b>34</b>, 723&ndash;741.
<a href="https://doi.org/10.1007/s10618-019-00666-8">doi:10.1007/s10618-019-00666-8</a>.     
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SosDisc-class">SosDisc-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    showClass("PredictSosDisc")
</code></pre>

<hr>
<h2 id='RSimca'>
Robust classification in high dimensions based on the SIMCA method
</h2><span id='topic+RSimca'></span><span id='topic+RSimca.default'></span><span id='topic+RSimca.formula'></span>

<h3>Description</h3>

<p>RSimca performs a robust version of the SIMCA method. This method classifies
a data matrix x with a known group structure. To reduce the dimension on 
each group a robust PCA analysis is performed. Afterwards a classification
rule is developped to determine the assignment of new observations. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RSimca(x, ...)
## Default S3 method:
RSimca(x, grouping, prior=proportions, k, kmax = ncol(x), 
    control="hubert", alpha, tol = 1.0e-4, trace=FALSE, ...)
## S3 method for class 'formula'
RSimca(formula, data = NULL, ..., subset, na.action)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RSimca_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>y~x</code>, it describes the response
and the predictors. The formula can be more complicated, such as
<code>y~log(x)+z</code> etc (see <code><a href="stats.html#topic+formula">formula</a></code> for more details).
The response should
be a factor representing the response variable, or any vector
that can be coerced to such (such as a logical variable).</p>
</td></tr>
<tr><td><code id="RSimca_+3A_data">data</code></td>
<td>
<p>an optional data frame (or similar: see
<code><a href="stats.html#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.</p>
</td></tr>
<tr><td><code id="RSimca_+3A_subset">subset</code></td>
<td>
<p>an optional vector used to select rows (observations) of the
data matrix <code>x</code>.</p>
</td></tr>
<tr><td><code id="RSimca_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen
when the data contain <code>NA</code>s.  The default is set by
the <code>na.action</code> setting of <code><a href="base.html#topic+options">options</a></code>, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The default is <code><a href="stats.html#topic+na.omit">na.omit</a></code>.</p>
</td></tr>
<tr><td><code id="RSimca_+3A_x">x</code></td>
<td>
<p>a matrix or data frame containing the explanatory variables (training set). </p>
</td></tr>
<tr><td><code id="RSimca_+3A_grouping">grouping</code></td>
<td>
<p>grouping variable:  a factor specifying the class for each observation.</p>
</td></tr>
<tr><td><code id="RSimca_+3A_prior">prior</code></td>
<td>
<p>prior probabilities, default to the class proportions for the training set.</p>
</td></tr>
<tr><td><code id="RSimca_+3A_tol">tol</code></td>
<td>
<p>tolerance</p>
</td></tr>
<tr><td><code id="RSimca_+3A_control">control</code></td>
<td>
<p> a control object (S4) for specifying one of the 
available PCA estimation methods and containing estimation options. 
The class of this object defines which estimator will be used. 
Alternatively a character string can be specified
which names the estimator - one of auto, hubert, locantore, grid, proj. 
If 'auto' is specified or the argument is missing, the
function will select the estimator (see below for details)</p>
</td></tr>
<tr><td><code id="RSimca_+3A_alpha">alpha</code></td>
<td>
<p>this parameter measures the fraction of outliers the algorithm should
resist. In MCD alpha controls the size of the subsets over which the
determinant is minimized, i.e. alpha*n observations are used for
computing the determinant. Allowed values are between 0.5 and 1
and the default is 0.5.</p>
</td></tr>
<tr><td><code id="RSimca_+3A_k">k</code></td>
<td>
<p>number of principal components to compute. If <code>k</code> is missing, 
or <code>k = 0</code>, the algorithm itself will determine the number of 
components by finding such <code>k</code> that <code class="reqn">l_k/l_1 &gt;= 10.E-3</code> and 
<code class="reqn">\Sigma_{j=1}^k l_j/\Sigma_{j=1}^r l_j &gt;= 0.8</code>. 
It is preferable to investigate the scree plot in order to choose the number 
of components and then run again. Default is <code>k=0</code>. </p>
</td></tr>
<tr><td><code id="RSimca_+3A_kmax">kmax</code></td>
<td>
<p>maximal number of principal components to compute.
Default is <code>kmax=10</code>. If <code>k</code> is provided, <code>kmax</code> 
does not need to be specified, unless <code>k</code> is larger than 10.</p>
</td></tr>
<tr><td><code id="RSimca_+3A_trace">trace</code></td>
<td>
<p>whether to print intermediate results. Default is <code>trace = FALSE</code></p>
</td></tr>
<tr><td><code id="RSimca_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>RSimca</code>, serving as a constructor for objects of class <code><a href="#topic+RSimca-class">RSimca-class</a></code> 
is a generic function with &quot;formula&quot; and &quot;default&quot; methods.
</p>
<p>SIMCA is a two phase procedure consisting of PCA performed on each group 
separately for dimension reduction followed by classification rules built 
in the lower dimensional space (note that the dimension in 
each group can be different). Instead of classical PCA robust alternatives will be used.
Any of the robust PCA methods available in package <code><a href="rrcov.html#topic+Pca-class">Pca-class</a></code> 
can be used through the argument <code>control</code>.
In original SIMCA new observations are 
classified by means of their deviations from the different PCA models.
Here the classification rules will be obtained using two popular distances arising from PCA - 
orthogonal distances (OD) and score distances (SD). For the definition of these distances,
the definition of the cutoff values and the standartization of the distances see 
Vanden Branden K, Hubert M (2005) and Todorov and Filzmoser (2009).
</p>


<h3>Value</h3>

<p>An S4 object of class <code><a href="#topic+RSimca-class">RSimca-class</a></code> which is a subclass of of the 
virtual class <code><a href="#topic+Simca-class">Simca-class</a></code>. 
</p>


<h3>Author(s)</h3>

<p> Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>Vanden Branden K, Hubert M (2005) Robust classification in high 
dimensions based on the SIMCA method. Chemometrics and 
Intellegent Laboratory Systems 79:10&ndash;21
</p>
<p>Todorov V &amp; Filzmoser P (2014),
Software Tools for Robust Analysis of High-Dimensional Data.
<em>Austrian Journal of Statistics</em>, <b>43</b>(4),  255&ndash;266,
<a href="https://doi.org/10.17713/ajs.v43i4.44">doi:10.17713/ajs.v43i4.44</a>.   
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(pottery)
dim(pottery)        # 27 observations in 2 classes, 6 variables
head(pottery)

## Build the SIMCA model. Use RSimca for a robust version
rs &lt;- RSimca(origin~., data=pottery)
rs
summary(rs)


## generate a sample from the pottery data set -
##  this will be the "new" data to be predicted
smpl &lt;- sample(1:nrow(pottery), 5)
test &lt;- pottery[smpl, -7]          # extract the test sample. Remove the last (grouping) variable
print(test)


## predict new data
pr &lt;- predict(rs, newdata=test)

pr@classification 
</code></pre>

<hr>
<h2 id='RSimca-class'>Class <code>"RSimca" - robust classification in high dimensions based on the SIMCA method</code></h2><span id='topic+RSimca-class'></span>

<h3>Description</h3>

<p>The class <code>RSimca</code> represents robust version of the  
SIMCA algorithm for classification in high dimensions. 
The objects of class <code>RSImca</code> contain the results
of the robust SIMCA method.
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("RSImca", ...)</code> but the 
usual way of creating <code>RSimca</code> objects is a call to the function
<code>RSimca()</code> which serves as a constructor.
</p>


<h3>Slots</h3>


<dl>
<dt><code>call</code>:</dt><dd><p>the (matched) function call.</p>
</dd>
<dt><code>prior</code>:</dt><dd><p>prior probabilities used, default to group proportions</p>
</dd>
<dt><code>counts</code>:</dt><dd><p>number of observations in each class</p>
</dd>
<dt><code>pcaobj</code>:</dt><dd><p>A list of Pca objects - one for each group</p>
</dd>
<dt><code>k</code>:</dt><dd><p>Object of class <code>"numeric"</code> number of (choosen) principal components </p>
</dd>
<dt><code>flag</code>:</dt><dd><p>Object of class <code>"Uvector"</code> The observations whose score distance is larger 
than cutoff.sd or whose orthogonal distance is larger than cutoff.od can be considered 
as outliers and receive a flag equal to zero.
The regular observations receive a flag 1 </p>
</dd>
<dt><code>X</code>:</dt><dd><p>the training data set (same as the input parameter x of the constructor function)</p>
</dd>
<dt><code>grp</code>:</dt><dd><p>grouping variable:  a factor specifying the class for each observation.</p>
</dd>   
</dl>



<h3>Extends</h3>

<p>Class <code>"<a href="#topic+Simca-class">Simca</a>"</code>, directly.
</p>


<h3>Methods</h3>

<p>No methods defined with class &quot;RSimca&quot; in the signature.
</p>


<h3>Author(s)</h3>

<p> Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>Vanden Branden K, Hubert M (2005) Robust classification in high 
dimensions based on the SIMCA method. Chemometrics and 
Intellegent Laboratory Systems 79:10&ndash;21
</p>
<p>Todorov V &amp; Filzmoser P (2009),
An Object Oriented Framework for Robust Multivariate Analysis.
<em>Journal of Statistical Software</em>, <b>32</b>(3), 1&ndash;47,
<a href="https://doi.org/10.18637/jss.v032.i03">doi:10.18637/jss.v032.i03</a>.   
</p>
<p>Todorov V &amp; Filzmoser P (2014),
Software Tools for Robust Analysis of High-Dimensional Data.
<em>Austrian Journal of Statistics</em>, <b>43</b>(4),  255&ndash;266,
<a href="https://doi.org/10.17713/ajs.v43i4.44">doi:10.17713/ajs.v43i4.44</a>.   
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("RSimca")
</code></pre>

<hr>
<h2 id='Simca-class'>Class <code>"Simca"</code> - virtual base class for all classic and robust SIMCA 
classes representing classification in high dimensions based on the SIMCA method</h2><span id='topic+Simca-class'></span><span id='topic+predict+2CSimca-method'></span><span id='topic+show+2CSimca-method'></span><span id='topic+summary+2CSimca-method'></span>

<h3>Description</h3>

<p>The class <code>Simca</code> searves as a base class for deriving all other 
classes representing the results of the classical and robust SIMCA methods</p>


<h3>Objects from the Class</h3>

<p>A virtual Class: No objects may be created from it.</p>


<h3>Slots</h3>


<dl>
<dt><code>call</code>:</dt><dd><p>the (matched) function call.</p>
</dd>
<dt><code>prior</code>:</dt><dd><p>prior probabilities used, default to group proportions</p>
</dd>
<dt><code>counts</code>:</dt><dd><p>number of observations in each class</p>
</dd>
<dt><code>pcaobj</code>:</dt><dd><p>A list of Pca objects - one for each group</p>
</dd>
<dt><code>k</code>:</dt><dd><p>Object of class <code>"numeric"</code> number of (choosen) principal components </p>
</dd>
<dt><code>flag</code>:</dt><dd><p>Object of class <code>"Uvector"</code> The observations whose score distance is larger 
than cutoff.sd or whose orthogonal distance is larger than cutoff.od can be considered 
as outliers and receive a flag equal to zero.
The regular observations receive a flag 1 </p>
</dd>
<dt><code>X</code>:</dt><dd><p>the training data set (same as the input parameter x of the constructor function)</p>
</dd>
<dt><code>grp</code>:</dt><dd><p>grouping variable:  a factor specifying the class for each observation.</p>
</dd>   
</dl>



<h3>Methods</h3>


<dl>
<dt>predict</dt><dd><p><code>signature(object = "Simca")</code>: calculates prediction using the results in 
<code>object</code>. An optional data frame or matrix in which to look for variables with which 
to predict. If omitted, the training data set is used. If the original fit used a formula or 
a data frame or a matrix with column names, newdata must contain columns with the 
same names. Otherwise it must contain the same number of columns, 
to be used in the same order. </p>
</dd>
<dt>show</dt><dd><p><code>signature(object = "Simca")</code>: prints the results </p>
</dd>
<dt>summary</dt><dd><p><code>signature(object = "Simca")</code>: prints summary information </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>Vanden Branden K, Hubert M (2005) Robust classification in high 
dimensions based on the SIMCA method. Chemometrics and 
Intellegent Laboratory Systems 79:10&ndash;21
</p>
<p>Todorov V &amp; Filzmoser P (2009),
An Object Oriented Framework for Robust Multivariate Analysis.
<em>Journal of Statistical Software</em>, <b>32</b>(3), 1&ndash;47,
<a href="https://doi.org/10.18637/jss.v032.i03">doi:10.18637/jss.v032.i03</a>.   
</p>
<p>Todorov V &amp; Filzmoser P (2014),
Software Tools for Robust Analysis of High-Dimensional Data.
<em>Austrian Journal of Statistics</em>, <b>43</b>(4),  255&ndash;266,
<a href="https://doi.org/10.17713/ajs.v43i4.44">doi:10.17713/ajs.v43i4.44</a>.   
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("Simca")
</code></pre>

<hr>
<h2 id='SosDisc-class'>
Class <code>"SosDisc"</code> - virtual base class for all classic and robust SosDisc 
classes representing the results of the robust and sparse multigroup classification 
by the optimal scoring approach
</h2><span id='topic+SosDisc-class'></span><span id='topic+predict+2C+20SosDisc-method'></span><span id='topic+show+2C+20SosDisc-method'></span><span id='topic+summary+2C+20SosDisc-method'></span>

<h3>Description</h3>

<p>Robust and sparse multigroup classification by the optimal scoring approach.
The class <code>SosDisc</code> searves as a base class for deriving all other 
classes representing the results of the robust and sparse multigroup classification 
by the optimal scoring approach.
</p>


<h3>Details</h3>

<p>The sparse optimal scoring problem (Clemmensen et al, 2011):
for <code class="reqn">h=1,....,Q</code>
</p>
<p style="text-align: center;"><code class="reqn">
\min_{\beta_h,\theta_h} \frac{1}{n} \|Y \theta_h - X \beta_h \|_2^2   + \lambda \|\beta_h\|_1
</code>
</p>

<p>subject to
</p>
<p style="text-align: center;"><code class="reqn">
\frac{1}{n} \theta_h^T Y^T Y\theta_h=1, \quad \theta_h^T Y^T Y \theta_l=0 \quad \forall l&lt;h.
</code>
</p>

<p>where <code class="reqn">X</code> deontes the robustly centered and scaled input matrix <code>x</code> (or alternativly the predictors from <code>formular</code>) and <code class="reqn">Y</code> is an dummy matrix coding die classmemberships from <code>grouping</code>.
</p>
<p>For each <code class="reqn">h</code> this problem can be solved interatively for <code class="reqn">\beta_h</code> and <code class="reqn">\theta_h</code>. In order to obtain robust estimates, <code class="reqn">\beta_h</code> is estimated with reweighted sparse least trimmed squares regression (Alfons et al, 2013) and <code class="reqn">\theta_h</code> with least absolut deviation regression in the first two iterations. To speed up the following repetitions an iterative down-weighting of observations with large residuals is combined with the iterative estimation of the optimal scoring coefficients with their classical estimates.
</p>
<p>The classification model is estimated on the low dimensional sparse subspace <code class="reqn">X[\beta_1,...,\beta_Q]</code> with robust LDA (<code><a href="rrcov.html#topic+Linda">Linda</a></code>).
</p>


<h3>Objects from the Class</h3>

<p>A virtual Class: No objects may be created from it.</p>


<h3>Slots</h3>


<dl>
<dt><code>call</code>:</dt><dd><p>The (matched) function call.</p>
</dd>
<dt><code>prior</code>:</dt><dd><p>Prior probabilities; same as input parameter.</p>
</dd>
<dt><code>counts</code>:</dt><dd><p>Number of observations in each class.</p>
</dd>
<dt><code>beta</code>:</dt><dd><p>Object of class <code>"matrix"</code>: Q coefficient vectors of the predictor matrix from optimal scoring (see Details); 
rows corespond to variables listed in <code>varnames</code>.</p>
</dd>
<dt><code>theta</code>:</dt><dd><p>Object of class <code>"matrix"</code>: Q coefficient vectors of the dummy matrix for class coding from optimal scoring (see Details).</p>
</dd>
<dt><code>lambda</code>:</dt><dd><p>Non-negative tuning paramer from L1 norm penaly; same as input parameter</p>
</dd>
<dt><code>varnames</code>:</dt><dd><p>Character vector: Names of included predictor variables 
(variables where at least one beta coefficient is non-zero).</p>
</dd>
<dt><code>center</code>:</dt><dd><p>Centering vector of the input predictors (coordinate wise median).</p>
</dd>
<dt><code>scale</code>:</dt><dd><p>Scaling vector of the input predictors (mad).</p>
</dd>
<dt><code>fit</code>:</dt><dd><p>Object of class <code>"Linda"</code>: Linda model (robust LDA model) estimated in the low dimensional subspace <code class="reqn">X[\beta_1,...,\beta_Q]</code> (see Details)</p>
</dd>
<dt><code>mahadist2</code>:</dt><dd><p>These will go later to Linda object: squared robust Mahalanobis distance 
(calculated with estimates from Linda, with common covariance structure of all groups) 
of each observation to its group center in the low dimensional subspace <code class="reqn">X[\beta_1,...,\beta_Q]</code> (see Details).</p>
</dd>
<dt><code>wlinda</code>:</dt><dd><p>These will go later to Linda object: 0-1 weights derived from <code>mahadist2</code>;
observations where the squred robust Mahalanobis distance is larger than the 0.975 quantile 
of the chi-square distribution with Q degrees of freedom resive weight zero.</p>
</dd>
<dt><code>X</code>:</dt><dd><p>The training data set (same as the input parameter <code>x</code> of the constructor function)</p>
</dd>
<dt><code>grp</code>:</dt><dd><p>Grouping variable: a factor specifying the class for each observation (same as the input parameter <code>grouping</code>)</p>
</dd>   
</dl>



<h3>Methods</h3>


<dl>
<dt>predict</dt><dd><p><code>signature(object = "SosDisc")</code>: calculates prediction using the results in 
<code>object</code>. An optional data frame or matrix in which to look for variables with which 
to predict. If omitted, the training data set is used. If the original fit used a formula or 
a data frame or a matrix with column names, newdata must contain columns with the 
same names. </p>
</dd>
<dt>show</dt><dd><p><code>signature(object = "SosDisc")</code>: prints the results </p>
</dd>
<dt>summary</dt><dd><p><code>signature(object = "SosDisc")</code>: prints summary information </p>
</dd>
</dl>



<h3>Author(s)</h3>

 
<p>Irene Ortner <a href="mailto:irene.ortner@applied-statistics.at">irene.ortner@applied-statistics.at</a> and Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>Clemmensen L, Hastie T, Witten D &amp; Ersboll B (2012),
Sparse discriminant analysis.
<em>Technometrics</em>, <b>53</b>(4), 406&ndash;413. 
</p>
<p>Ortner I, Filzmoser P &amp; Croux C (2020),
Robust and sparse multigroup classification by the optimal scoring approach.
Data Mining and Knowledge Discovery <b>34</b>, 723&ndash;741.
<a href="https://doi.org/10.1007/s10618-019-00666-8">doi:10.1007/s10618-019-00666-8</a>.     
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    showClass("SosDisc")
</code></pre>

<hr>
<h2 id='SosDiscClassic-class'>Class <code>SosDiscClassic</code> - sparse multigroup classification by the optimal scoring approach</h2><span id='topic+SosDiscClassic-class'></span>

<h3>Description</h3>

<p>Sparse multigroup classification by the optimal scoring approach.
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("SosDiscClassic", ...)</code> but the 
usual way of creating <code>SosDiscClassic</code> objects is a call to the function
<code>SosDiscRobust()</code> which serves as a constructor.
</p>


<h3>Slots</h3>


<dl>
<dt><code>call</code>:</dt><dd><p>The (matched) function call.</p>
</dd>
<dt><code>prior</code>:</dt><dd><p>Prior probabilities; same as input parameter.</p>
</dd>
<dt><code>counts</code>:</dt><dd><p>Number of observations in each class.</p>
</dd>
<dt><code>beta</code>:</dt><dd><p>Object of class <code>"matrix"</code>: Q coefficient vectors of the predictor matrix from optimal scoring (see Details); 
rows corespond to variables listed in <code>varnames</code>.</p>
</dd>
<dt><code>theta</code>:</dt><dd><p>Object of class <code>"matrix"</code>: Q coefficient vectors of the dummy matrix for class coding from optimal scoring (see Details).</p>
</dd>
<dt><code>lambda</code>:</dt><dd><p>Non-negative tuning paramer from L1 norm penaly; same as input parameter</p>
</dd>
<dt><code>varnames</code>:</dt><dd><p>Character vector: Names of included predictor variables 
(variables where at least one beta coefficient is non-zero).</p>
</dd>
<dt><code>center</code>:</dt><dd><p>Centering vector of the input predictors (coordinate wise median).</p>
</dd>
<dt><code>scale</code>:</dt><dd><p>Scaling vector of the input predictors (mad).</p>
</dd>
<dt><code>fit</code>:</dt><dd><p>Object of class <code>"Linda"</code>: Linda model (robust LDA model) estimated in the low dimensional subspace <code class="reqn">X[\beta_1,...,\beta_Q]</code> (see Details)</p>
</dd>
<dt><code>mahadist2</code>:</dt><dd><p>These will go later to Linda object: squared robust Mahalanobis distance 
(calculated with estimates from Linda, with common covariance structure of all groups) 
of each observation to its group center in the low dimensional subspace <code class="reqn">X[\beta_1,...,\beta_Q]</code> (see Details).</p>
</dd>
<dt><code>wlinda</code>:</dt><dd><p>These will go later to Linda object: 0-1 weights derived from <code>mahadist2</code>;
observations where the squred robust Mahalanobis distance is larger than the 0.975 quantile 
of the chi-square distribution with Q degrees of freedom resive weight zero.</p>
</dd>
<dt><code>X</code>:</dt><dd><p>The training data set (same as the input parameter <code>x</code> of the constructor function)</p>
</dd>
<dt><code>grp</code>:</dt><dd><p>Grouping variable: a factor specifying the class for each observation (same as the input parameter <code>grouping</code>)</p>
</dd>   
</dl>



<h3>Extends</h3>

<p>Class <code>"<a href="#topic+SosDisc-class">SosDisc</a>"</code>, directly.
</p>


<h3>Methods</h3>

<p>No methods defined with class &quot;SosDiscClassic&quot; in the signature.
</p>


<h3>Author(s)</h3>

 
<p>Irene Ortner <a href="mailto:irene.ortner@applied-statistics.at">irene.ortner@applied-statistics.at</a> and Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>Clemmensen L, Hastie T, Witten D &amp; Ersboll B (2012),
Sparse discriminant analysis.
<em>Technometrics</em>, <b>53</b>(4), 406&ndash;413. 
</p>
<p>Ortner I, Filzmoser P &amp; Croux C (2020),
Robust and sparse multigroup classification by the optimal scoring approach.
Data Mining and Knowledge Discovery <b>34</b>, 723&ndash;741.
<a href="https://doi.org/10.1007/s10618-019-00666-8">doi:10.1007/s10618-019-00666-8</a>.     
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    showClass("SosDiscClassic")
</code></pre>

<hr>
<h2 id='SosDiscRobust'>
Robust and sparse multigroup classification by the optimal scoring approach
</h2><span id='topic+SosDiscRobust'></span><span id='topic+SosDiscRobust.default'></span><span id='topic+SosDiscRobust.formula'></span>

<h3>Description</h3>

<p>Robust and sparse multigroup classification by the optimal scoring approach is robust against outliers, provides a low-dimensional and sparse representation of the predictors and is also applicable if the number of variables exeeds the number of observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SosDiscRobust(x, ...)
## Default S3 method:
SosDiscRobust(x, grouping, prior=proportions, 
    lambda, Q=length(unique(grouping))-1, alpha=0.5, maxit=100, 
    tol = 1.0e-4, trace=FALSE, ...)
## S3 method for class 'formula'
SosDiscRobust(formula, data = NULL, ..., subset, na.action)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SosDiscRobust_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>y~x</code>, it describes the response
and the predictors. The formula can be more complicated, such as
<code>y~log(x)+z</code> etc (see <code><a href="stats.html#topic+formula">formula</a></code> for more details).
The response should
be a factor representing the response variable, or any vector
that can be coerced to such (such as a logical variable).</p>
</td></tr>
<tr><td><code id="SosDiscRobust_+3A_data">data</code></td>
<td>
<p>An optional data frame (or similar: see
<code><a href="stats.html#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.</p>
</td></tr>
<tr><td><code id="SosDiscRobust_+3A_subset">subset</code></td>
<td>
<p>An optional vector used to select rows (observations) of the
data matrix <code>x</code>.</p>
</td></tr>
<tr><td><code id="SosDiscRobust_+3A_na.action">na.action</code></td>
<td>
<p>A function which indicates what should happen
when the data contain <code>NA</code>s.  The default is set by
the <code>na.action</code> setting of <code><a href="base.html#topic+options">options</a></code>, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The default is <code><a href="stats.html#topic+na.omit">na.omit</a></code>.</p>
</td></tr>
<tr><td><code id="SosDiscRobust_+3A_x">x</code></td>
<td>
<p>A matrix or data frame containing the explanatory variables (training set); 
colnames of x have to be provided. </p>
</td></tr>
<tr><td><code id="SosDiscRobust_+3A_grouping">grouping</code></td>
<td>
<p>Grouping variable:  a factor specifying the class for each observation.</p>
</td></tr>
<tr><td><code id="SosDiscRobust_+3A_prior">prior</code></td>
<td>
<p>Prior probabilities, a vector of positive numbers that sum up to 1;
default to the class proportions for the training set.</p>
</td></tr>
<tr><td><code id="SosDiscRobust_+3A_lambda">lambda</code></td>
<td>
<p>A non-negative tuning parameter for L1 norm penalty introducing sparsity on the 
optimal scoring coefficients <code class="reqn">\boldsymbol{\beta}_h</code> (see Details). 
If the number of variables exceeds the number of observations <code>lambda</code> has to be positive.</p>
</td></tr>
<tr><td><code id="SosDiscRobust_+3A_q">Q</code></td>
<td>
<p>Number of optimal scoring coefficient vectors; <code>Q</code> has to be smaller than the number of groups. 
Defaults to number of groups - 1.</p>
</td></tr>
<tr><td><code id="SosDiscRobust_+3A_alpha">alpha</code></td>
<td>
<p>Robustness parameter used in sparseLTS (for initial estimation, see Details). Default <code>alpha=0.5</code>.</p>
</td></tr>
<tr><td><code id="SosDiscRobust_+3A_maxit">maxit</code></td>
<td>
<p>Number of iterations for the estimation of optimal scoring coefficients and case weights. Default <code>maxit=100</code>.</p>
</td></tr>
<tr><td><code id="SosDiscRobust_+3A_tol">tol</code></td>
<td>
<p>Tolerance for convergence of the normed weighted change in the residual sum of squares
for the estiamtion of optimal scoring coefficeints. Default is <code>tol=1.0e-4</code>.</p>
</td></tr>
<tr><td><code id="SosDiscRobust_+3A_trace">trace</code></td>
<td>
<p>Whether to print intermediate results. Default is <code>trace = FALSE</code>.</p>
</td></tr>
<tr><td><code id="SosDiscRobust_+3A_...">...</code></td>
<td>
<p>Arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sparse optimal scoring problem (Clemmensen et al, 2011):
for <code class="reqn">h=1,....,Q</code>
</p>
<p style="text-align: center;"><code class="reqn">
\min_{\beta_h,\theta_h} \frac{1}{n} \|Y \theta_h - X \beta_h \|_2^2   + \lambda \|\beta_h\|_1
</code>
</p>

<p>subject to
</p>
<p style="text-align: center;"><code class="reqn">
\frac{1}{n} \theta_h^T Y^T Y\theta_h=1, \quad \theta_h^T Y^T Y \theta_l=0 \quad \forall l&lt;h,
</code>
</p>

<p>where <code class="reqn">X</code> deontes the robustly centered and scaled input matrix <code>x</code> (or alternativly the predictors from <code>formular</code>) and <code class="reqn">Y</code> is an dummy matrix coding die classmemberships from <code>grouping</code>.
</p>
<p>For each <code class="reqn">h</code> this problem can be solved interatively for <code class="reqn">\beta_h</code> and <code class="reqn">\theta_h</code>. In order to obtain robust estimates, <code class="reqn">\beta_h</code> is estimated with reweighted sparse least trimmed squares regression (Alfons et al, 2013) and <code class="reqn">\theta_h</code> with least absolut deviation regression in the first two iterations. To speed up the following repetitions an iterative down-weighting of observations with large residuals is combined with the iterative estimation of the optimal scoring coefficients with their classical estimates.
</p>
<p>The classification model is estimated on the low dimensional sparse subspace <code class="reqn">X[\beta_1,...,\beta_Q]</code> with robust LDA (<code><a href="rrcov.html#topic+Linda">Linda</a></code>).
</p>


<h3>Value</h3>

<p>An S4 object of class <code><a href="#topic+SosDiscRobust-class">SosDiscRobust-class</a></code> which is a subclass of of the 
virtual class <code><a href="#topic+SosDisc-class">SosDisc-class</a></code>. 
</p>


<h3>Author(s)</h3>

 
<p>Irene Ortner <a href="mailto:irene.ortner@applied-statistics.at">irene.ortner@applied-statistics.at</a> and Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>Clemmensen L, Hastie T, Witten D &amp; Ersboll B (2011),
Sparse discriminant analysis.
<em>Technometrics</em>, <b>53</b>(4), 406&ndash;413. 
</p>
<p>Alfons A, Croux C &amp; Gelper S (2013),
Sparse least trimmed squares regression for analysing high-dimensional large data sets.
<em>The Annals of Applied Statistics</em>, <b>7</b>(1), 226&ndash;248.
</p>
<p>Ortner I, Filzmoser P &amp; Croux C (2020),
Robust and sparse multigroup classification by the optimal scoring approach.
Data Mining and Knowledge Discovery <b>34</b>, 723&ndash;741.
<a href="https://doi.org/10.1007/s10618-019-00666-8">doi:10.1007/s10618-019-00666-8</a>.     
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## EXAMPLE 1 ######################################
data(olitos)
grind &lt;- which(colnames(olitos)=="grp")

set.seed(5008642)
mod &lt;- SosDiscRobust(grp~., data=olitos, lambda=0.3, maxIte=30, Q=3, tol=1e-2)

pred &lt;- predict(mod, newdata=olitos[,-grind])

summary(mod)
plot(mod, ind=c(1:3))


## EXAMPLE 2 ######################################
##

## Not run: 
library(sparseLDA)
data(penicilliumYES)

## for demonstration only:
set.seed(5008642)
X &lt;- penicilliumYES$X[, sample(1:ncol(penicilliumYES$X), 100)]

## takes a subsample of the variables
## to have quicker computation time

colnames(X) &lt;- paste0("V",1:ncol(X))
y &lt;- as.factor(c(rep(1,12), rep(2,12), rep(3,12)))

set.seed(5008642)
mod &lt;- SosDiscRobust(X, y, lambda=1, maxit=5, Q=2, tol=1e-2)

summary(mod)
plot(mod)

## End(Not run)

</code></pre>

<hr>
<h2 id='SosDiscRobust-class'>Class <code>SosDiscRobust</code> - robust and sparse multigroup classification by the optimal scoring approach</h2><span id='topic+SosDiscRobust-class'></span>

<h3>Description</h3>

<p>Robust and sparse multigroup classification by the optimal scoring approach.
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("SosDiscRobust", ...)</code> but the 
usual way of creating <code>SosDiscRobust</code> objects is a call to the function
<code>SosDiscRobust()</code> which serves as a constructor.
</p>


<h3>Slots</h3>


<dl>
<dt><code>call</code>:</dt><dd><p>The (matched) function call.</p>
</dd>
<dt><code>prior</code>:</dt><dd><p>Prior probabilities; same as input parameter.</p>
</dd>
<dt><code>counts</code>:</dt><dd><p>Number of observations in each class.</p>
</dd>
<dt><code>beta</code>:</dt><dd><p>Object of class <code>"matrix"</code>: Q coefficient vectors of the predictor matrix from optimal scoring (see Details); 
rows corespond to variables listed in <code>varnames</code>.</p>
</dd>
<dt><code>theta</code>:</dt><dd><p>Object of class <code>"matrix"</code>: Q coefficient vectors of the dummy matrix for class coding from optimal scoring (see Details).</p>
</dd>
<dt><code>lambda</code>:</dt><dd><p>Non-negative tuning paramer from L1 norm penaly; same as input parameter</p>
</dd>
<dt><code>varnames</code>:</dt><dd><p>Character vector: Names of included predictor variables 
(variables where at least one beta coefficient is non-zero).</p>
</dd>
<dt><code>center</code>:</dt><dd><p>Centering vector of the input predictors (coordinate wise median).</p>
</dd>
<dt><code>scale</code>:</dt><dd><p>Scaling vector of the input predictors (mad).</p>
</dd>
<dt><code>fit</code>:</dt><dd><p>Object of class <code>"Linda"</code>: Linda model (robust LDA model) estimated in the low dimensional subspace <code class="reqn">X[\beta_1,...,\beta_Q]</code> (see Details)</p>
</dd>
<dt><code>mahadist2</code>:</dt><dd><p>These will go later to Linda object: squared robust Mahalanobis distance 
(calculated with estimates from Linda, with common covariance structure of all groups) 
of each observation to its group center in the low dimensional subspace <code class="reqn">X[\beta_1,...,\beta_Q]</code> (see Details).</p>
</dd>
<dt><code>wlinda</code>:</dt><dd><p>These will go later to Linda object: 0-1 weights derived from <code>mahadist2</code>;
observations where the squred robust Mahalanobis distance is larger than the 0.975 quantile 
of the chi-square distribution with Q degrees of freedom resive weight zero.</p>
</dd>
<dt><code>X</code>:</dt><dd><p>The training data set (same as the input parameter <code>x</code> of the constructor function)</p>
</dd>
<dt><code>grp</code>:</dt><dd><p>Grouping variable: a factor specifying the class for each observation (same as the input parameter <code>grouping</code>)</p>
</dd>   
</dl>



<h3>Extends</h3>

<p>Class <code>"<a href="#topic+SosDisc-class">SosDisc</a>"</code>, directly.
</p>


<h3>Methods</h3>

<p>No methods defined with class &quot;SosDiscRobust&quot; in the signature.
</p>


<h3>Author(s)</h3>

 
<p>Irene Ortner <a href="mailto:irene.ortner@applied-statistics.at">irene.ortner@applied-statistics.at</a> and Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>Clemmensen L, Hastie T, Witten D &amp; Ersboll B (2012),
Sparse discriminant analysis.
<em>Technometrics</em>, <b>53</b>(4), 406&ndash;413. 
</p>
<p>Ortner I, Filzmoser P &amp; Croux C (2020),
Robust and sparse multigroup classification by the optimal scoring approach.
Data Mining and Knowledge Discovery <b>34</b>, 723&ndash;741.
<a href="https://doi.org/10.1007/s10618-019-00666-8">doi:10.1007/s10618-019-00666-8</a>.     
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    showClass("SosDiscRobust")
</code></pre>

<hr>
<h2 id='SPcaGrid'>Sparse Robust Principal Components based on Projection Pursuit (PP): GRID search Algorithm </h2><span id='topic+SPcaGrid'></span><span id='topic+SPcaGrid.formula'></span><span id='topic+SPcaGrid.default'></span>

<h3>Description</h3>

<p>Computes an approximation of the PP-estimators for sparse and robust PCA using the grid search algorithm in the plane.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    SPcaGrid(x, ...)
    ## Default S3 method:
SPcaGrid(x, k = 0, kmax = ncol(x), method = c ("mad", "sd", "qn", "Qn"), 
    lambda = 1, scale=FALSE, na.action = na.fail, trace=FALSE, ...)
    ## S3 method for class 'formula'
SPcaGrid(formula, data = NULL, subset, na.action, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SPcaGrid_+3A_formula">formula</code></td>
<td>
<p>a formula with no response variable, referring only to
numeric variables.</p>
</td></tr>
<tr><td><code id="SPcaGrid_+3A_data">data</code></td>
<td>
<p>an optional data frame (or similar: see
<code><a href="stats.html#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.</p>
</td></tr>
<tr><td><code id="SPcaGrid_+3A_subset">subset</code></td>
<td>
<p>an optional vector used to select rows (observations) of the
data matrix <code>x</code>.</p>
</td></tr>
<tr><td><code id="SPcaGrid_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen
when the data contain <code>NA</code>s.  The default is set by
the <code>na.action</code> setting of <code><a href="base.html#topic+options">options</a></code>, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The default is <code><a href="stats.html#topic+na.omit">na.omit</a></code>.</p>
</td></tr>
<tr><td><code id="SPcaGrid_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="SPcaGrid_+3A_x">x</code></td>
<td>
<p>a numeric matrix (or data frame) which provides
the data for the principal components analysis.</p>
</td></tr>
<tr><td><code id="SPcaGrid_+3A_k">k</code></td>
<td>
<p>number of principal components to compute. If <code>k</code> is missing, 
or <code>k = 0</code>, the algorithm itself will determine the number of 
components by finding such <code>k</code> that <code class="reqn">l_k/l_1 &gt;= 10.E-3</code> and 
<code class="reqn">\Sigma_{j=1}^k l_j/\Sigma_{j=1}^r l_j &gt;= 0.8</code>. 
It is preferable to investigate the scree plot in order to choose the number 
of components and then run again. Default is <code>k=0</code>. </p>
</td></tr>
<tr><td><code id="SPcaGrid_+3A_kmax">kmax</code></td>
<td>
<p>maximal number of principal components to compute.
Default is <code>kmax=10</code>. If <code>k</code> is provided, <code>kmax</code> 
does not need to be specified, unless <code>k</code> is larger than 10.</p>
</td></tr>
<tr><td><code id="SPcaGrid_+3A_method">method</code></td>
<td>
<p> the scale estimator used to detect the direction with the 
largest variance. Possible values are <code>"sd"</code>, <code>"mad"</code> and 
<code>"Qn"</code>. <code>"mad"</code> is the 
default value.</p>
</td></tr>
<tr><td><code id="SPcaGrid_+3A_lambda">lambda</code></td>
<td>
<p> the sparseness constraint's strength(<code>sPCAgrid</code> only).
A single value for all components, or a vector of length <code>k</code> with 
different values for each component can be specified.
See <code><a href="pcaPP.html#topic+opt.TPO">opt.TPO</a></code> for the choice of this argument. </p>
</td></tr>
<tr><td><code id="SPcaGrid_+3A_scale">scale</code></td>
<td>
<p>a value indicating whether and how the variables should be 
scaled. If <code>scale = FALSE</code> (default) or <code>scale = NULL</code> no scaling is 
performed (a vector of 1s is returned in the <code>scale</code> slot). 
If <code>scale = TRUE</code> the data are scaled to have unit variance. Alternatively it can 
be a function like <code>sd</code> or <code>mad</code> or a vector of length equal 
the number of columns of <code>x</code>. The value is passed to the underlying function
and the result returned is stored in the <code>scale</code> slot. 
Default is <code>scale = FALSE</code></p>
</td></tr>
<tr><td><code id="SPcaGrid_+3A_trace">trace</code></td>
<td>
<p>whether to print intermediate results. Default is <code>trace = FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>SPcaGrid</code>, serving as a constructor for objects of class <code><a href="#topic+SPcaGrid-class">SPcaGrid-class</a></code> 
is a generic function with &quot;formula&quot; and &quot;default&quot; methods. For details see 
<code><a href="pcaPP.html#topic+sPCAgrid">sPCAgrid</a></code> and the relevant references.
</p>


<h3>Value</h3>

<p>An S4 object of class <code><a href="#topic+SPcaGrid-class">SPcaGrid-class</a></code> which is a subclass of <code><a href="rrcov.html#topic+PcaGrid-class">PcaGrid-class</a></code> which in turn is a subclass of the 
virtual class <code><a href="rrcov.html#topic+PcaRobust-class">PcaRobust-class</a></code>. 
</p>


<h3>Author(s)</h3>

<p> Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>C. Croux, P. Filzmoser, M. Oliveira, (2007).
Algorithms for Projection-Pursuit Robust Principal Component Analysis,
<em>Chemometrics and Intelligent Laboratory Systems</em>, Vol. 87, pp. 218-225.
</p>
<p>C. Croux, P. Filzmoser, H. Fritz (2013).
Robust Sparse Principal Component Analysis,
<em>Technometrics</em> <b>55</b>(2), pp. 202&ndash;2014,
<a href="https://doi.org/10.1080/00401706.2012.727746">doi:10.1080/00401706.2012.727746</a>.   
</p>
<p>V. Todorov, P. Filzmoser (2013). Comparing classical and robust sparse PCA.
In R Kruse, M Berthold, C Moewes, M Gil, P Grzegorzewski, O Hryniewicz (eds.), 
<em>Synergies of Soft Computing and Statistics for Intelligent Data Analysis</em>, 
volume 190 of <em>Advances in Intelligent Systems and Computing</em>, pp. 283&ndash;291. 
Springer, Berlin; New York. ISBN 978-3-642-33041-4,
<a href="https://doi.org/10.1007/978-3-642-33042-1_31">doi:10.1007/978-3-642-33042-1_31</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(bus)
bus &lt;- as.matrix(bus)

## calculate MADN for each variable
xmad &lt;- apply(bus, 2, mad)
cat("\nMin, Max of MADN: ", min(xmad), max(xmad), "\n")

## calculate MADN for each variable
xqn &lt;- apply(bus, 2, Qn)
cat("\nMin, Max of Qn: ", min(xqn), max(xqn), "\n")


## MADN vary between 0 (for variable 9) and 34. Therefore exclude
##  variable 9 and divide the remaining variables by their MADNs.
bus1 &lt;- bus[, -c(9)]
p &lt;- ncol(bus1)

madbus &lt;- apply(bus1, 2, mad)
bus2 &lt;- sweep(bus1, 2, madbus, "/", check.margin = FALSE)

xsd &lt;- apply(bus1, 2, sd)
bus.sd &lt;- sweep(bus1, 2, xsd, "/", check.margin = FALSE)

xqn &lt;- apply(bus1, 2, Qn)
bus.qn &lt;- sweep(bus1, 2, xqn, "/", check.margin = FALSE)

## Not run: 
spc &lt;- SPcaGrid(bus2, lambda=0, method="sd", k=p, kmax=p)
rspc &lt;- SPcaGrid(bus2, lambda=0, method="Qn", k=p, kmax=p)
summary(spc)
summary(rspc)
screeplot(spc, type="line", main="Classical PCA", sub="PC", cex.main=2)
screeplot(rspc, type="line", main="Robust PCA", sub="PC", cex.main=2)

##  find lambda

K &lt;- 4
lambda.sd &lt;- 1.64
    to.sd &lt;- .tradeoff(bus2, k=K, lambda.max=2.5, lambda.n=100, method="sd")
    plot(to.sd, type="b", xlab="lambda", ylab="Explained Variance (percent)")
    abline(v=lambda.sd, lty="dotted")
 
spc.sd.p &lt;- SPcaGrid(bus2, lambda=lambda.sd, method="sd", k=p)
.CPEV(spc.sd.p, k=K)
spc.sd &lt;- SPcaGrid(bus2, lambda=lambda.sd, method="sd", k=K)
getLoadings(spc.sd)[,1:K]
plot(spc.sd)

lambda.qn &lt;- 2.06
    to.qn &lt;- .tradeoff(bus2, k=K, lambda.max=2.5, lambda.n=100, method="Qn")
    plot(to.qn, type="b", xlab="lambda", ylab="Explained Variance (percent)")
    abline(v=lambda.qn, lty="dotted")

spc.qn.p &lt;- SPcaGrid(bus2, lambda=lambda.qn, method="Qn", k=p)
.CPEV(spc.qn.p, k=K)
spc.qn &lt;- SPcaGrid(bus2, lambda=lambda.qn, method="Qn", k=K)
getLoadings(spc.qn)[,1:K]
plot(spc.qn)

## End(Not run)

## DD-plots
##
## Not run:
## Not run: 
usr &lt;- par(mfrow=c(2,2))
plot(SPcaGrid(bus2, lambda=0, method="sd", k=4), id.n.sd=0, main="Standard PCA")
plot(SPcaGrid(bus2, lambda=0, method="Qn", k=4), id.n.sd=0, ylim=c(0,20))

plot(SPcaGrid(bus2, lambda=1.64, method="sd", k=4), id.n.sd=0, main="Stdandard sparse PCA")
plot(SPcaGrid(bus2, lambda=3.07, method="Qn", k=4), id.n.sd=0, main="Robust sparse PCA")

par(usr)
## End (Not run)

## End(Not run)
</code></pre>

<hr>
<h2 id='SPcaGrid-class'>Class <code>SPcaGrid</code> - Sparse Robust PCA using PP - GRID search Algorithm</h2><span id='topic+SPcaGrid-class'></span><span id='topic+getQuan+2CSPcaGrid-method'></span>

<h3>Description</h3>

<p>Holds the results of an approximation of the PP-estimators for sparse and robust PCA using the grid search algorithm in the plane.
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("SPcaGrid", ...)</code> but the
usual way of creating <code>SPcaGrid</code> objects is a call to the function
<code>SPcaGrid()</code> which serves as a constructor.
</p>


<h3>Slots</h3>


<dl>
<dt><code>call</code>, <code>center</code>, <code>scale</code>, <code>loadings</code>,
<code>eigenvalues</code>, <code>scores</code>, <code>k</code>,
<code>sd</code>, <code>od</code>, <code>cutoff.sd</code>, <code>cutoff.od</code>,
<code>flag</code>, <code>n.obs</code>:</dt><dd>
<p>from the <code>"<a href="rrcov.html#topic+Pca-class">Pca-class</a>"</code> class.</p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code>"<a href="rrcov.html#topic+PcaGrid-class">PcaGrid</a>"</code>, directly.
Class <code>"<a href="rrcov.html#topic+PcaRobust-class">PcaRobust</a>"</code>, by class &quot;PcaGrid&quot;, distance 2.
Class <code>"<a href="rrcov.html#topic+Pca-class">Pca</a>"</code>, by class &quot;PcaGrid&quot;, distance 3.
</p>


<h3>Methods</h3>


<dl>
<dt>getQuan</dt><dd><p><code>signature(obj = "SPcaGrid")</code>: ... </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> </p>


<h3>References</h3>

<p>Todorov V &amp; Filzmoser P (2009), An Object Oriented Framework for Robust Multivariate Analysis. 
<em>Journal of Statistical Software</em>, <b>32</b>(3), 1&ndash;47.
<a href="https://doi.org/10.18637/jss.v032.i03">doi:10.18637/jss.v032.i03</a>.
</p>
<p>C. Croux, P. Filzmoser, H. Fritz (2013).
Robust Sparse Principal Component Analysis,
<em>Technometrics</em> <b>55</b>(2), pp. 202&ndash;2014,
<a href="https://doi.org/10.1080/00401706.2012.727746">doi:10.1080/00401706.2012.727746</a>.   
</p>
<p>V. Todorov, P. Filzmoser (2013). Comparing classical and robust sparse PCA.
In R Kruse, M Berthold, C Moewes, M Gil, P Grzegorzewski, O Hryniewicz (eds.), 
<em>Synergies of Soft Computing and Statistics for Intelligent Data Analysis</em>, 
volume 190 of <em>Advances in Intelligent Systems and Computing</em>, pp. 283&ndash;291. 
Springer, Berlin; New York. ISBN 978-3-642-33041-4,
<a href="https://doi.org/10.1007/978-3-642-33042-1_31">doi:10.1007/978-3-642-33042-1_31</a>.        
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SPcaGrid">SPcaGrid</a></code>, <code><a href="rrcov.html#topic+PcaGrid-class">PcaGrid-class</a></code>, <code><a href="rrcov.html#topic+PcaRobust-class">PcaRobust-class</a></code>, <code><a href="rrcov.html#topic+Pca-class">Pca-class</a></code>, <code><a href="rrcov.html#topic+PcaClassic">PcaClassic</a></code>, <code><a href="rrcov.html#topic+PcaClassic-class">PcaClassic-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("SPcaGrid")
</code></pre>

<hr>
<h2 id='SummarySimca-class'>Class <code>"SummarySimca"</code> - summary of <code>"Simca"</code> objects</h2><span id='topic+SummarySimca-class'></span><span id='topic+show+2CSummarySimca-method'></span>

<h3>Description</h3>

 
<p>Contains summary information about a <code>Simca</code> object - 
classification in high dimensions based on the SIMCA method 
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("SummarySimca", ...)</code>,
but most often by invoking <code>summary()</code> on an <code>"Simca"</code> object. 
They contain values meant for printing by <code>show()</code>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>simcaobj</code>:</dt><dd><p>Object of class <code>"Simca"</code></p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>show</dt><dd><p><code>signature(object = "SummarySimca")</code>: display the object </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a></p>


<h3>References</h3>

<p>Todorov V &amp; Filzmoser P (2009),
An Object Oriented Framework for Robust Multivariate Analysis.
<em>Journal of Statistical Software</em>, <b>32</b>(3), 1&ndash;47,
<a href="https://doi.org/10.18637/jss.v032.i03">doi:10.18637/jss.v032.i03</a>.   
</p>
<p>Todorov V &amp; Filzmoser P (2014),
Software Tools for Robust Analysis of High-Dimensional Data.
<em>Austrian Journal of Statistics</em>, <b>43</b>(4),  255&ndash;266,
<a href="https://doi.org/10.17713/ajs.v43i4.44">doi:10.17713/ajs.v43i4.44</a>.       
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Simca-class">Simca-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("SummarySimca")
</code></pre>

<hr>
<h2 id='SummarySosDisc-class'>Class <code>"SummarySosDisc"</code> - summary of <code>"SosDisc"</code> objects</h2><span id='topic+SummarySosDisc-class'></span><span id='topic+show+2CSummarySosDisc-method'></span>

<h3>Description</h3>

 
<p>Contains summary information about a <code>SosDisc</code> object representing 
the results of the robust and sparse multigroup classification 
by the optimal scoring approach.
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("SummarySosDisc", ...)</code>,
but most often by invoking <code>summary()</code> on an <code>"SosDisc"</code> object. 
They contain values meant for printing by <code>show()</code>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>obj</code>:</dt><dd><p>Object of class <code>"SosDisc"</code></p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>show</dt><dd><p><code>signature(object = "SummarySosDisc")</code>: display the object </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Irene Ortner <a href="mailto:irene.ortner@applied-statistics.at">irene.ortner@applied-statistics.at</a> and Valentin Todorov <a href="mailto:valentin.todorov@chello.at">valentin.todorov@chello.at</a> 
</p>


<h3>References</h3>

<p>Clemmensen L, Hastie T, Witten D &amp; Ersboll B (2012),
Sparse discriminant analysis.
<em>Technometrics</em>, <b>53</b>(4), 406&ndash;413. 
</p>
<p>Ortner I, Filzmoser P &amp; Croux C (2020),
Robust and sparse multigroup classification by the optimal scoring approach.
Data Mining and Knowledge Discovery <b>34</b>, 723&ndash;741.
<a href="https://doi.org/10.1007/s10618-019-00666-8">doi:10.1007/s10618-019-00666-8</a>.     
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SosDisc-class">SosDisc-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("SummarySosDisc")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
