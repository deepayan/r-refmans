<!DOCTYPE html><html><head><title>Help for package prclust</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {prclust}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#prclust-package'>
<p>Penalized Regression Based Cluster Method</p></a></li>
<li><a href='#clusterStat'>
<p>External Evaluation of Cluster Results</p></a></li>
<li><a href='#GCV'><p>Calculate the Generalized Cross-Validation Statistic (GCV)</p></a></li>
<li><a href='#PRclust'>
<p>Find the Solution of Penalized Regression-Based Clustering.</p></a></li>
<li><a href='#stability'><p>Calculate the stability based statistics</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Penalized Regression-Based Clustering Method</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2016-12-12</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.1)</td>
</tr>
<tr>
<td>Author:</td>
<td>Chong Wu, Wei Pan</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Chong Wu &lt;wuxx0845@umn.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Clustering is unsupervised and exploratory in nature. Yet, it can be performed through penalized regression with grouping pursuit. In this package, we provide two algorithms for fitting the penalized regression-based clustering (PRclust) with non-convex grouping penalties, such as group truncated lasso, MCP and SCAD. One algorithm is based on quadratic penalty and difference convex method. Another algorithm is based on difference convex and ADMM, called DC-ADD, which is more efficient. Generalized cross validation and stability based method were provided to select the tuning parameters. Rand index, adjusted Rand index and Jaccard index were provided to estimate the agreement between estimated cluster memberships and the truth.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.1), parallel</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2016-12-12 23:43:15 UTC; chong</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2016-12-13 07:57:15</td>
</tr>
</table>
<hr>
<h2 id='prclust-package'>
Penalized Regression Based Cluster Method
</h2><span id='topic+prclust-package'></span><span id='topic+prclust'></span>

<h3>Description</h3>

<p>Clustering analysis is widely used in many fields. Traditionally clustering is regarded as unsupervised learning for its lack of a class label or a quantitative response variable, which in contrast is present in supervised learning such as classification and regression. Here we formulate clustering as penalized regression with grouping pursuit. In addition to the novel use of a non-convex group penalty and its associated unique operating characteristics in the proposed clustering method, a main advantage of this formulation is its allowing borrowing some well established results in classification and regression, such as model selection criteria to select the number of clusters, a difficult problem in clustering analysis. In particular, we propose using the generalized cross-validation (GCV) based on generalized degrees of freedom (GDF) to select the number of clusters. we further develop this method by developing a more efficient algorithm for scalable computation as well as a new theory for PRclust. This algorithm, called DC-ADMM, combines difference of convex programming with the alternating direction method of multipliers (ADMM). This method is more efficient than the quadratic penalty algorithm used in Pan et al. (2013) due to the availability of closed-form updating formulas.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> prclust</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.3</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2016-12-12</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2 | GPL-3</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Chong Wu, Wei Pan<br />
Maintainer: Chong Wu &lt;wuxx0845@umn.edu&gt;
</p>


<h3>References</h3>

<p>Pan, W., Shen, X., &amp; Liu, B. (2013). Cluster analysis: unsupervised learning via supervised learning with a non-convex penalty. <em>Journal of Machine Learning Research</em>, 14(1), 1865-1889.
</p>
<p>Wu, C., Kwon, S., Shen, X., &amp; Pan, W. (2016). A New Algorithm and Theory for Penalized Regression-based Clustering. <em>Journal of Machine Learning Research</em>, 17(188), 1-25.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## In default, we use DC-ADMM, a faster algorithm to solve 
## the objective function and get the clustering result.
library("prclust")
## generate the data
data = matrix(NA,2,100)
data[1,1:50] = rnorm(50,0,0.33)
data[2,1:50] = rnorm(50,0,0.33)
data[1,51:100] = rnorm(50,1,0.33)
data[2,51:100] = rnorm(50,1,0.33)

# clustering via PRclsut 
a =PRclust(data,lambda1=0.4,lambda2=1,tau=0.5)
a$mu
a$group
</code></pre>

<hr>
<h2 id='clusterStat'>
External Evaluation of Cluster Results
</h2><span id='topic+clusterStat'></span>

<h3>Description</h3>

<p>Suppose we know the true cluster results beforehand. clusterStat provides Rand, adjusted Rand, Jaccard index to measure the quality of a cluster results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clusterStat(trueGroup, group)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clusterStat_+3A_truegroup">trueGroup</code></td>
<td>
<p>The true cluster results.</p>
</td></tr>
<tr><td><code id="clusterStat_+3A_group">group</code></td>
<td>
<p>The estimated cluster results, not neccessary calculating by PRclust.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The return value is a &quot;clusterStat&quot; class, providing the following information.
</p>
<table>
<tr><td><code>Rand</code></td>
<td>
<p>Rand Index</p>
</td></tr>
<tr><td><code>AdjustedRand</code></td>
<td>
<p>Adjusted Rand Index</p>
</td></tr>
<tr><td><code>Jaccard</code></td>
<td>
<p>Jaccard Index</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Chong Wu
</p>


<h3>Examples</h3>

<pre><code class='language-R'>	a &lt;- rep(1:3,3)
	a
	b &lt;- rep(c(4:6),3)
	b
	clusterStat(a,b)
</code></pre>

<hr>
<h2 id='GCV'>Calculate the Generalized Cross-Validation Statistic (GCV)</h2><span id='topic+GCV'></span>

<h3>Description</h3>

<p>Calculate the generalized cross-validation statistic with generalized degrees of freedom.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GCV(data,lambda1,lambda2,tau,sigma,B=100,
	loss.method = c("quadratic","lasso"),
	grouping.penalty = c("gtlp","L1","SCAD","MCP"), 
	algorithm = c("ADMM","Quadratic"), epsilon =0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GCV_+3A_data">data</code></td>
<td>

<p>Numeric data matrix .
</p>
</td></tr>
<tr><td><code id="GCV_+3A_lambda1">lambda1</code></td>
<td>

<p>Tuning parameter or step size: lambda1, typically set at 1 for quadratic penalty based algorithm; 0.4 for revised ADMM.
</p>
</td></tr>
<tr><td><code id="GCV_+3A_lambda2">lambda2</code></td>
<td>

<p>Tuning parameter: lambda2, the magnitude of grouping penalty.
</p>
</td></tr>
<tr><td><code id="GCV_+3A_tau">tau</code></td>
<td>

<p>Tuning parameter: tau, related to grouping penalty.
</p>
</td></tr>
<tr><td><code id="GCV_+3A_sigma">sigma</code></td>
<td>

<p>The perturbation size.
</p>
</td></tr>
<tr><td><code id="GCV_+3A_b">B</code></td>
<td>

<p>The Monte Carlo time. The defualt value is 100.
</p>
</td></tr>
<tr><td><code id="GCV_+3A_loss.method">loss.method</code></td>
<td>

<p>character may be abbreviated. &quot;lasso&quot; stands for <code class="reqn">L_1</code> loss function, while &quot;quadratic&quot; stands for the quadratic loss function.  
</p>
</td></tr>
<tr><td><code id="GCV_+3A_grouping.penalty">grouping.penalty</code></td>
<td>

<p>character: may be abbreviated. &quot;gtlp&quot; means generalized group lasso is used for grouping penalty. &quot;lasso&quot; means lasso is used for grouping penalty. &quot;SCAD&quot; and &quot;MCP&quot; are two other non-convex penalty.
</p>
</td></tr>
<tr><td><code id="GCV_+3A_algorithm">algorithm</code></td>
<td>

<p>character: may be abbreviated. The algorithm will use for finding the solution. The default algorithm is &quot;ADMM&quot;, which stands for the DC-ADMM.
</p>
</td></tr>
<tr><td><code id="GCV_+3A_epsilon">epsilon</code></td>
<td>
<p>The stopping critetion parameter. The default is 0.001.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A bonus with the regression approach to clustering is the potential application of many existing model selection methods for regression or supervised learning to clustering. We propose using generalized cross-validation (GCV). GCV can be regarded as an approximation to leave-one-out cross-validation (CV). Hence, GCV provides an approximately unbiased estimate of the prediction error.
</p>
<p>We use the generalized degrees of freedom (GDF) to consider the data-adaptive nature in estimating the centroids of the observations.
</p>
<p>The chosen tuning parameters are the one giving the smallest GCV error.
</p>


<h3>Value</h3>

<p>Return value: the Generalized cross-validation statistic (GCV)
</p>


<h3>Author(s)</h3>

<p>Chong Wu, Wei Pan
</p>


<h3>References</h3>

<p>Pan, W., Shen, X., &amp; Liu, B. (2013). Cluster analysis: unsupervised learning via supervised learning with a non-convex penalty. <em>Journal of Machine Learning Research</em>, 14(1), 1865-1889.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
library("prclust")
data = matrix(NA,2,50)
data[1,1:25] = rnorm(25,0,0.33)
data[2,1:25] = rnorm(25,0,0.33)
data[1,26:50] = rnorm(25,1,0.33)
data[2,26:50] = rnorm(25,1,0.33)

#case 1
gcv1 = GCV(data,lambda1=1,lambda2=1,tau=0.5,sigma=0.25,B =10)
gcv1

#case 2
gcv2 = GCV(data,lambda1=1,lambda2=0.7,tau=0.3,sigma=0.25,B = 10)
gcv2

# Note that the combination of tuning parameters in case 1 are better than 
# the combination of tuning parameters in case 2 since the value of GCV in case 1 is
# less than the value in case 2.
</code></pre>

<hr>
<h2 id='PRclust'>
Find the Solution of Penalized Regression-Based Clustering.
</h2><span id='topic+PRclust'></span>

<h3>Description</h3>

<p>Clustering is unsupervised and exploratory in nature. Yet, it can be performed through penalized regression with grouping pursuit. Prclust helps us peform penalized regression-based clustering with various loss functions and grouping penalities via two algorithm (DC-ADMM and quadratic penalty).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PRclust(data, lambda1, lambda2, tau, 
	loss.method = c("quadratic","lasso"), 
	grouping.penalty = c("gtlp","L1","SCAD","MCP"), 
	algorithm = c("ADMM","Quadratic"), epsilon=0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PRclust_+3A_data">data</code></td>
<td>

<p>input matrix, of dimension nvars x nobs; each column is an observation vector.
</p>
</td></tr>
<tr><td><code id="PRclust_+3A_lambda1">lambda1</code></td>
<td>

<p>Tuning parameter or step size: lambda1, typically set at 1 for quadratic penalty based algorithm; 0.4 for revised ADMM.
</p>
</td></tr>
<tr><td><code id="PRclust_+3A_lambda2">lambda2</code></td>
<td>

<p>Tuning parameter: lambda2, the magnitude of grouping penalty.
</p>
</td></tr>
<tr><td><code id="PRclust_+3A_tau">tau</code></td>
<td>

<p>Tuning parameter: tau, related to grouping penalty.
</p>
</td></tr>
<tr><td><code id="PRclust_+3A_loss.method">loss.method</code></td>
<td>

<p>The loss method. &quot;lasso&quot; stands for <code class="reqn">L_1</code> loss function, while &quot;quadratic&quot; stands for the quadratic loss function.  
</p>
</td></tr>
<tr><td><code id="PRclust_+3A_grouping.penalty">grouping.penalty</code></td>
<td>
<p>Grouping penalty. Character: may be abbreviated. &quot;gtlp&quot; means generalized group lasso is used for grouping penalty. &quot;lasso&quot; means lasso is used for grouping penalty. &quot;SCAD&quot; and &quot;MCP&quot; are two other non-convex penalty.
</p>
</td></tr>
<tr><td><code id="PRclust_+3A_algorithm">algorithm</code></td>
<td>

<p>character: may be abbreviated. The algorithm to use for finding the solution. The default algorithm is &quot;ADMM&quot;, which stands for the new algorithm we developed.
</p>
</td></tr>
<tr><td><code id="PRclust_+3A_epsilon">epsilon</code></td>
<td>
<p>The stopping critetion parameter. The default is 0.001.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Clustering analysis has been widely used in many fields. In the absence of a class label, clustering analysis is also called unsupervised learning. However, penalized regression-based clustering adopts a novel framework for clustering analysis by viewing it as a regression problem. In this method, a novel non-convex penalty for grouping pursuit was proposed which data-adaptively encourages the equality among some unknown subsets of parameter estimates. This new method can deal with some complex clustering situation, for example, in the presence of non-convex cluster, in which the K-means fails to work, PRclust might perform much better.
</p>


<h3>Value</h3>

<p>The return value is a list. In this list, it contains the following matrix.
</p>
<table>
<tr><td><code>mu</code></td>
<td>
<p>The centroid of the each observations.</p>
</td></tr>
<tr><td><code>theta</code></td>
<td>
<p>The theta value for the data set, not very useful.</p>
</td></tr>
<tr><td><code>group</code></td>
<td>
<p>The group for each points.</p>
</td></tr>
<tr><td><code>count</code></td>
<td>
<p>The iteration times.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Choosing tunning parameter is kind of time consuming job. It is always based on &quot;trials and errors&quot;.
</p>


<h3>Author(s)</h3>

<p>Chong Wu, Wei Pan
</p>


<h3>References</h3>

<p>Pan, W., Shen, X., &amp; Liu, B. (2013). Cluster analysis: unsupervised learning via supervised learning with a non-convex penalty. <em>Journal of Machine Learning Research</em>, 14(1), 1865-1889.
</p>
<p>Wu, C., Kwon, S., Shen, X., &amp; Pan, W. (2016). A New Algorithm and Theory for Penalized Regression-based Clustering. <em>Journal of Machine Learning Research</em>, 17(188), 1-25.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("prclust")
# To let you have a better understanding about the power and strength
# of PRclust method, 6 examples in original prclust paper were provided.
################################################
### case 1
################################################
## generate the data
data = matrix(NA,2,100)
data[1,1:50] = rnorm(50,0,0.33)
data[2,1:50] = rnorm(50,0,0.33)
data[1,51:100] = rnorm(50,1,0.33)
data[2,51:100] = rnorm(50,1,0.33)
## set the tunning parameter
lambda1 =1
lambda2 = 3
tau = 0.5
a =PRclust(data,lambda1,lambda2,tau)
a


</code></pre>

<hr>
<h2 id='stability'>Calculate the stability based statistics</h2><span id='topic+stability'></span>

<h3>Description</h3>

<p>Calculate the  the stability based statistics. We try with various tuning parameter values, obtaining their corresponding statbility based statistics average prediction strengths, then choose the set of the tuning parameters with the maximum average prediction stength.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stability(data,rho,lambda,tau,
    loss.function = c("quadratic","L1","MCP","SCAD"),
    grouping.penalty = c("gtlp","tlp"), 
    algorithm = c("DCADMM","Quadratic"),
    epsilon = 0.001,n.times = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stability_+3A_data">data</code></td>
<td>

<p>Input matrix. Each column is an observation vector.
</p>
</td></tr>
<tr><td><code id="stability_+3A_rho">rho</code></td>
<td>

<p>Tuning parameter or step size: rho, typically set at 1 for quadratic penalty based algorithm; 0.4 for DC-ADMM. (Note that rho is the lambda1 in quadratic penalty based algorithm.)
</p>
</td></tr>
<tr><td><code id="stability_+3A_lambda">lambda</code></td>
<td>

<p>Tuning parameter: lambda, the magnitude of grouping penalty.
</p>
</td></tr>
<tr><td><code id="stability_+3A_tau">tau</code></td>
<td>

<p>Tuning parameter: tau, a nonnegative tuning parameter controll ing the trade-off between the model fit and the number of clusters.
</p>
</td></tr>
<tr><td><code id="stability_+3A_loss.function">loss.function</code></td>
<td>

<p>The loss function. &quot;L1&quot; stands for <code class="reqn">L_1</code> loss function, while &quot;quadratic&quot; stands for the quadratic loss function.  
</p>
</td></tr>
<tr><td><code id="stability_+3A_grouping.penalty">grouping.penalty</code></td>
<td>
<p>Grouping penalty. Character: may be abbreviated. &quot;gtlp&quot; means generalized group lasso is used for grouping penalty. &quot;lasso&quot; means lasso is used for grouping penalty. &quot;SCAD&quot; and &quot;MCP&quot; are two other non-convex penalty.
</p>
</td></tr>
<tr><td><code id="stability_+3A_algorithm">algorithm</code></td>
<td>

<p>Two algorithms for PRclust. &quot;DC-ADMM&quot; and &quot;Quadratic&quot; stand for the DC-ADMM and quadratic penalty based criterion respectively. &quot;DC-ADMM&quot; is much faster than &quot;Quadratic&quot; and thus recommend it here. 
</p>
</td></tr>
<tr><td><code id="stability_+3A_epsilon">epsilon</code></td>
<td>
<p>The stopping critetion parameter corresponding to DC-ADMM. The default is 0.001.
</p>
</td></tr>
<tr><td><code id="stability_+3A_n.times">n.times</code></td>
<td>
<p>Repeat times. Based on our limited simulations, we find 10 is usually good enough.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A generalized degrees of freedom (GDF) together with generalized cross validation (GCV) was proposed for selection of tuning parameters for clustering (Pan et al., 2013). This method, while yielding good performance, requires extensive computation and specification of a hyper-parameter perturbation size. Here, we provide an alternative by modifying a stability-based criterion (Tibshirani and Walther, 2005; Liu et al., 2016) for determining the tuning parameters.
</p>
<p>The main idea of the method is based on cross-validation. That is, (1) randomly partition the entire data set into a training set and a test set with an almost equal size; (2) cluster the training and test sets separately via PRclust with the same tuning parameters; (3) measure how well the training set clusters predict the test clusters.
</p>
<p>We try with various tuning parameter values, obtaining their corresponding statbility based statistics average prediction strengths, then choose the set of the tuning parameters with the maximum average prediction stength.
</p>


<h3>Value</h3>

<p>Return value: the average prediction score.
</p>


<h3>Author(s)</h3>

<p>Chong Wu
</p>


<h3>References</h3>

<p>Wu, C., Kwon, S., Shen, X., &amp; Pan, W. (2016). A New Algorithm and Theory for Penalized Regression-based Clustering. <em>Journal of Machine Learning Research</em>, 17(188), 1-25.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
library("prclust")
data = matrix(NA,2,50)
data[1,1:25] = rnorm(25,0,0.33)
data[2,1:25] = rnorm(25,0,0.33)
data[1,26:50] = rnorm(25,1,0.33)
data[2,26:50] = rnorm(25,1,0.33)

#case 1
stab1 = stability(data,rho=1,lambda=1,tau=0.5,n.times = 2)
stab1

#case 2
stab2 = stability(data,rho=1,lambda=0.7,tau=0.3,n.times = 2)
stab2
# Note that the combination of tuning parameters in case 1 are better than 
# the combination of tuning parameters in case 2 since the value of GCV in case 1 is
# less than the value in case 2.
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
