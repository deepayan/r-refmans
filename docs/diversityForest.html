<!DOCTYPE html><html><head><title>Help for package diversityForest</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {diversityForest}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#diversityForest-package'><p>Diversity Forests</p></a></li>
<li><a href='#divfor'><p>Construct a basic diversity forest prediction rule that uses univariable, binary splitting.</p></a></li>
<li><a href='#importance.divfor'><p>Diversity Forest variable importance</p></a></li>
<li><a href='#interactionfor'><p>Construct an interaction forest prediction rule and calculate EIM values as described in Hornung &amp; Boulesteix (2022).</p></a></li>
<li><a href='#plot.interactionfor'><p>Plot method for <code>interactionfor</code> objects</p></a></li>
<li><a href='#plotEffects'><p>Interaction forest plots: exploring interaction forest results through visualisation</p></a></li>
<li><a href='#plotPair'><p>Plot of the (estimated) simultaneous influence of two variables</p></a></li>
<li><a href='#predict.divfor'><p>Diversity Forest prediction</p></a></li>
<li><a href='#predict.interactionfor'><p>Interaction Forest prediction</p></a></li>
<li><a href='#stock'><p>Data on stock prices of aerospace companies</p></a></li>
<li><a href='#tunedivfor'><p>Optimization of the values of the tuning parameters <code>nsplits</code> and <code>proptry</code></p></a></li>
<li><a href='#zoo'><p>Data on biological species</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Innovative Complex Split Procedures in Random Forests Through
Candidate Split Sampling</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-03-07</td>
</tr>
<tr>
<td>Author:</td>
<td>Roman Hornung [aut, cre], Marvin N. Wright [ctb, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Roman Hornung &lt;hornung@ibe.med.uni-muenchen.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements interaction forests [1], which are specific diversity forests and 
  the basic form of diversity forests that uses univariable, binary splitting [2].
  Interaction forests (IFs) are ensembles of decision trees that model quantitative and
  qualitative interaction effects using bivariable splitting. IFs come with the 
  Effect Importance Measure (EIM), which can be used to identify variable pairs that 
  feature quantitative and qualitative interaction effects with high predictive
  relevance. IFs and EIM focus on well interpretable forms of interactions.
  The package also offers plot functions for visualising the estimated forms of 
  interaction effects.
  Categorical, metric, and survival outcomes are supported.
  This is a fork of the R package 'ranger' (main author: Marvin N. Wright) that 
  implements random forests using an efficient C++  implementation.
  References:
  [1] Hornung, R. &amp; Boulesteix, A.-L. (2022) Interaction Forests: Identifying and 
      exploiting interpretable quantitative and qualitative interaction effects.
	  Computational Statistics &amp; Data Analysis 171:107460, &lt;<a href="https://doi.org/10.1016%2Fj.csda.2022.107460">doi:10.1016/j.csda.2022.107460</a>&gt;.
  [2] Hornung, R. (2022) Diversity forests: Using split sampling to enable 
      innovative complex split procedures in random forests.
	  SN Computer Science 3(2):1, &lt;<a href="https://doi.org/10.1007%2Fs42979-021-00920-1">doi:10.1007/s42979-021-00920-1</a>&gt;.</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++17</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.11.2), Matrix, ggplot2, ggpubr, scales, nnet,
sgeostat, rms, MapGAM, gam, rlang, grDevices, RColorBrewer,
RcppEigen, survival</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppEigen</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, BOLTSSIRR</td>
</tr>
<tr>
<td>Additional_repositories:</td>
<td><a href="https://romanhornung.github.io/drat">https://romanhornung.github.io/drat</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-03-07 19:11:54 UTC; hornung</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-03-08 08:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='diversityForest-package'>Diversity Forests</h2><span id='topic+diversityForest-package'></span><span id='topic+diversityForest'></span>

<h3>Description</h3>

<p>The diversity forest algorithm is not a specific algorithm, but an alternative candidate split sampling scheme 
that makes complex split procedures in random forests possible computationally by drastically reducing 
the numbers of candidate splits that need to be evaluated for each split. It also avoids the well-known variable 
selection bias in conventional random forests that has the effect that variables with many possible splits 
are selected too frequently for splitting (Strobl et al., 2007). For details, see Hornung (2022).
</p>


<h3>Details</h3>

<p>This package currently features two types of diversity forests:
</p>

<ul>
<li><p> the basic form of diversity forests that uses univariable, binary splitting, which is also used
in conventional random forests
</p>
</li>
<li><p> interaction forests (IFs) (Hornung &amp; Boulesteix, 2022), which use bivariable splitting to model quantitative and qualitative interaction effects.
IFs feature the Effect Importance Measure (EIM), which ranks the variable pairs with respect to the predictive importance
of their quantitative and qualitative interaction effects. The individual variables can be ranked as well
using EIM. For details, see Hornung &amp; Boulesteix (2022).
</p>
</li></ul>

<p>Diversity forests with univariable splitting can be constructed using the function <code><a href="#topic+divfor">divfor</a></code> and 
interaction forests using the function <code><a href="#topic+interactionfor">interactionfor</a></code>. Both functions support categorical,
metric, and survival outcomes.
</p>
<p>This package is a fork of the R package 'ranger' that implements random forests using an
efficient C++ implementation. The documentation is in large parts taken from
'ranger', where some parts of the documentation may not apply to (the current version of) the 'diversityForest' package.
</p>
<p>Details on further functionalities of the code that are not presented in the help pages of 'diversityForest' are found
in the help pages of 'ranger', version 0.11.0, because 'diversityForest' is based on the latter version of 'ranger'. 
The code in the example sections can be used as a template for all basic application scenarios with respect to classification, 
regression and survival prediction.
</p>


<h3>References</h3>


<ul>
<li><p> Hornung, R. (2022). Diversity forests: Using split sampling to enable innovative complex split procedures in random forests. SN Computer Science 3(2):1, &lt;<a href="https://doi.org/10.1007/s42979-021-00920-1">doi:10.1007/s42979-021-00920-1</a>&gt;.
</p>
</li>
<li><p> Hornung, R., Boulesteix, A.-L. (2022). Interaction forests: Identifying and exploiting interpretable quantitative and qualitative interaction effects. Computational Statistics &amp; Data Analysis 171:107460, &lt;<a href="https://doi.org/10.1016/j.csda.2022.107460">doi:10.1016/j.csda.2022.107460</a>&gt;.
</p>
</li>
<li><p> Strobl, C., Boulesteix, A.-L., Zeileis, A., Hothorn, T. (2007). Bias in random forest variable importance measures: Illustrations, sources and a solution. BMC Bioinformatics 8:25, &lt;<a href="https://doi.org/10.1186/1471-2105-8-25">doi:10.1186/1471-2105-8-25</a>&gt;.
</p>
</li>
<li><p> Wright, M. N., Ziegler, A. (2017). ranger: A fast Implementation of Random Forests for High Dimensional Data in C++ and R. Journal of Statistical Software 77:1-17, &lt;<a href="https://doi.org/10.18637/jss.v077.i01">doi:10.18637/jss.v077.i01</a>&gt;.
</p>
</li></ul>


<hr>
<h2 id='divfor'>Construct a basic diversity forest prediction rule that uses univariable, binary splitting.</h2><span id='topic+divfor'></span>

<h3>Description</h3>

<p>Implements the most basic form of diversity forests that uses univariable, binary splitting.
Currently, categorical, metric, and survival outcomes are supported.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>divfor(
  formula = NULL,
  data = NULL,
  num.trees = 500,
  mtry = NULL,
  importance = "none",
  write.forest = TRUE,
  probability = FALSE,
  min.node.size = NULL,
  max.depth = NULL,
  replace = TRUE,
  sample.fraction = ifelse(replace, 1, 0.632),
  case.weights = NULL,
  class.weights = NULL,
  splitrule = NULL,
  num.random.splits = 1,
  alpha = 0.5,
  minprop = 0.1,
  split.select.weights = NULL,
  always.split.variables = NULL,
  respect.unordered.factors = NULL,
  scale.permutation.importance = FALSE,
  keep.inbag = FALSE,
  inbag = NULL,
  holdout = FALSE,
  quantreg = FALSE,
  oob.error = TRUE,
  num.threads = NULL,
  save.memory = FALSE,
  verbose = TRUE,
  seed = NULL,
  dependent.variable.name = NULL,
  status.variable.name = NULL,
  classification = NULL,
  nsplits = 30,
  proptry = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="divfor_+3A_formula">formula</code></td>
<td>
<p>Object of class <code>formula</code> or <code>character</code> describing the model to fit. Interaction terms supported only for numerical variables.</p>
</td></tr>
<tr><td><code id="divfor_+3A_data">data</code></td>
<td>
<p>Training data of class <code>data.frame</code>, <code>matrix</code>, <code>dgCMatrix</code> (Matrix) or <code>gwaa.data</code> (GenABEL).</p>
</td></tr>
<tr><td><code id="divfor_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees. Default is 500.</p>
</td></tr>
<tr><td><code id="divfor_+3A_mtry">mtry</code></td>
<td>
<p>Artefact from 'ranger'. NOT needed for diversity forests.</p>
</td></tr>
<tr><td><code id="divfor_+3A_importance">importance</code></td>
<td>
<p>Variable importance mode, one of 'none', 'impurity', 'impurity_corrected', 'permutation'. The 'impurity' measure is the Gini index for classification, the variance of the responses for regression and the sum of test statistics (see <code>splitrule</code>) for survival. NOTE: Currently, only &quot;permutation&quot; (and &quot;none&quot;) work for diversity forests.</p>
</td></tr>
<tr><td><code id="divfor_+3A_write.forest">write.forest</code></td>
<td>
<p>Save <code>divfor.forest</code> object, required for prediction. Set to <code>FALSE</code> to reduce memory usage if no prediction intended.</p>
</td></tr>
<tr><td><code id="divfor_+3A_probability">probability</code></td>
<td>
<p>Grow a probability forest as in Malley et al. (2012). NOTE: Not yet implemented for diversity forests!</p>
</td></tr>
<tr><td><code id="divfor_+3A_min.node.size">min.node.size</code></td>
<td>
<p>Minimal node size. Default 1 for classification, 5 for regression, 3 for survival, and 5 for probability.</p>
</td></tr>
<tr><td><code id="divfor_+3A_max.depth">max.depth</code></td>
<td>
<p>Maximal tree depth. A value of NULL or 0 (the default) corresponds to unlimited depth, 1 to tree stumps (1 split per tree).</p>
</td></tr>
<tr><td><code id="divfor_+3A_replace">replace</code></td>
<td>
<p>Sample with replacement.</p>
</td></tr>
<tr><td><code id="divfor_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>Fraction of observations to sample. Default is 1 for sampling with replacement and 0.632 for sampling without replacement. For classification, this can be a vector of class-specific values.</p>
</td></tr>
<tr><td><code id="divfor_+3A_case.weights">case.weights</code></td>
<td>
<p>Weights for sampling of training observations. Observations with larger weights will be selected with higher probability in the bootstrap (or subsampled) samples for the trees.</p>
</td></tr>
<tr><td><code id="divfor_+3A_class.weights">class.weights</code></td>
<td>
<p>Weights for the outcome classes (in order of the factor levels) in the splitting rule (cost sensitive learning). Classification and probability prediction only. For classification the weights are also applied in the majority vote in terminal nodes.</p>
</td></tr>
<tr><td><code id="divfor_+3A_splitrule">splitrule</code></td>
<td>
<p>Splitting rule. For classification and probability estimation &quot;gini&quot; or &quot;extratrees&quot; with default &quot;gini&quot;. For regression &quot;variance&quot;, &quot;extratrees&quot; or &quot;maxstat&quot; with default &quot;variance&quot;. For survival &quot;logrank&quot;, &quot;extratrees&quot;, &quot;C&quot; or &quot;maxstat&quot; with default &quot;logrank&quot;. NOTE: For diversity forests currently only the default splitting rules are supported.</p>
</td></tr>
<tr><td><code id="divfor_+3A_num.random.splits">num.random.splits</code></td>
<td>
<p>Artefact from 'ranger'. NOT needed for diversity forests.</p>
</td></tr>
<tr><td><code id="divfor_+3A_alpha">alpha</code></td>
<td>
<p>For &quot;maxstat&quot; splitrule: Significance threshold to allow splitting. NOT needed for diversity forests.</p>
</td></tr>
<tr><td><code id="divfor_+3A_minprop">minprop</code></td>
<td>
<p>For &quot;maxstat&quot; splitrule: Lower quantile of covariate distribution to be considered for splitting. NOT needed for diversity forests.</p>
</td></tr>
<tr><td><code id="divfor_+3A_split.select.weights">split.select.weights</code></td>
<td>
<p>Numeric vector with weights between 0 and 1, representing the probability to select variables for splitting. Alternatively, a list of size num.trees, containing split select weight vectors for each tree can be used.</p>
</td></tr>
<tr><td><code id="divfor_+3A_always.split.variables">always.split.variables</code></td>
<td>
<p>Currently not useable. Character vector with variable names to be always selected.</p>
</td></tr>
<tr><td><code id="divfor_+3A_respect.unordered.factors">respect.unordered.factors</code></td>
<td>
<p>Handling of unordered factor covariates. One of 'ignore' and 'order' (the option 'partition' possible in 'ranger' is not (yet) possible with diversity forests). Default is 'ignore'. Alternatively TRUE (='order') or FALSE (='ignore') can be used.</p>
</td></tr>
<tr><td><code id="divfor_+3A_scale.permutation.importance">scale.permutation.importance</code></td>
<td>
<p>Scale permutation importance by standard error as in (Breiman 2001). Only applicable if permutation variable importance mode selected.</p>
</td></tr>
<tr><td><code id="divfor_+3A_keep.inbag">keep.inbag</code></td>
<td>
<p>Save how often observations are in-bag in each tree.</p>
</td></tr>
<tr><td><code id="divfor_+3A_inbag">inbag</code></td>
<td>
<p>Manually set observations per tree. List of size num.trees, containing inbag counts for each observation. Can be used for stratified sampling.</p>
</td></tr>
<tr><td><code id="divfor_+3A_holdout">holdout</code></td>
<td>
<p>Hold-out mode. Hold-out all samples with case weight 0 and use these for variable importance and prediction error.</p>
</td></tr>
<tr><td><code id="divfor_+3A_quantreg">quantreg</code></td>
<td>
<p>Prepare quantile prediction as in quantile regression forests (Meinshausen 2006). Regression only. Set <code>keep.inbag = TRUE</code> to prepare out-of-bag quantile prediction.</p>
</td></tr>
<tr><td><code id="divfor_+3A_oob.error">oob.error</code></td>
<td>
<p>Compute OOB prediction error. Set to <code>FALSE</code> to save computation time, e.g. for large survival forests.</p>
</td></tr>
<tr><td><code id="divfor_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads. Default is number of CPUs available.</p>
</td></tr>
<tr><td><code id="divfor_+3A_save.memory">save.memory</code></td>
<td>
<p>Use memory saving (but slower) splitting mode. No effect for survival and GWAS data. Warning: This option slows down the tree growing, use only if you encounter memory problems. NOT needed for diversity forests.</p>
</td></tr>
<tr><td><code id="divfor_+3A_verbose">verbose</code></td>
<td>
<p>Show computation status and estimated runtime.</p>
</td></tr>
<tr><td><code id="divfor_+3A_seed">seed</code></td>
<td>
<p>Random seed. Default is <code>NULL</code>, which generates the seed from <code>R</code>. Set to <code>0</code> to ignore the <code>R</code> seed.</p>
</td></tr>
<tr><td><code id="divfor_+3A_dependent.variable.name">dependent.variable.name</code></td>
<td>
<p>Name of outcome variable, needed if no formula given. For survival forests this is the time variable.</p>
</td></tr>
<tr><td><code id="divfor_+3A_status.variable.name">status.variable.name</code></td>
<td>
<p>Name of status variable, only applicable to survival data and needed if no formula given. Use 1 for event and 0 for censoring.</p>
</td></tr>
<tr><td><code id="divfor_+3A_classification">classification</code></td>
<td>
<p>Only needed if data is a matrix. Set to <code>TRUE</code> to grow a classification forest.</p>
</td></tr>
<tr><td><code id="divfor_+3A_nsplits">nsplits</code></td>
<td>
<p>Number of candidate splits to sample for each split. Default is 30.</p>
</td></tr>
<tr><td><code id="divfor_+3A_proptry">proptry</code></td>
<td>
<p>Parameter that restricts the number of candidate splits considered for small nodes. If <code>nsplits</code> is larger than <code>proptry</code> times the number of all possible splits, the number of candidate splits to draw is reduced to the largest integer smaller than <code>proptry</code> times the number of all possible splits. Default is 1, which corresponds to always using <code>nsplits</code> candidate splits.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>divfor</code> with elements
</p>
<table>
<tr><td><code>forest</code></td>
<td>
<p>Saved forest (If write.forest set to TRUE). Note that the variable IDs in the <code>split.varIDs</code> object do not necessarily represent the column number in R.</p>
</td></tr>
<tr><td><code>predictions</code></td>
<td>
<p>Predicted classes/values, based on out-of-bag samples (classification and regression only).</p>
</td></tr>
<tr><td><code>variable.importance</code></td>
<td>
<p>Variable importance for each independent variable.</p>
</td></tr>
<tr><td><code>prediction.error</code></td>
<td>
<p>Overall out-of-bag prediction error. For classification this is the fraction of missclassified samples, for probability estimation the Brier score, for regression the mean squared error and for survival one minus Harrell's C-index.</p>
</td></tr>
<tr><td><code>r.squared</code></td>
<td>
<p>R squared. Also called explained variance or coefficient of determination (regression only). Computed on out-of-bag data.</p>
</td></tr>
<tr><td><code>confusion.matrix</code></td>
<td>
<p>Contingency table for classes and predictions based on out-of-bag samples (classification only).</p>
</td></tr>
<tr><td><code>unique.death.times</code></td>
<td>
<p>Unique death times (survival only).</p>
</td></tr>
<tr><td><code>chf</code></td>
<td>
<p>Estimated cumulative hazard function for each sample (survival only).</p>
</td></tr>
<tr><td><code>survival</code></td>
<td>
<p>Estimated survival function for each sample (survival only).</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>Function call.</p>
</td></tr>
<tr><td><code>num.trees</code></td>
<td>
<p>Number of trees.</p>
</td></tr>
<tr><td><code>num.independent.variables</code></td>
<td>
<p>Number of independent variables.</p>
</td></tr>
<tr><td><code>min.node.size</code></td>
<td>
<p>Value of minimal node size used.</p>
</td></tr>
<tr><td><code>treetype</code></td>
<td>
<p>Type of forest/tree. classification, regression or survival.</p>
</td></tr>
<tr><td><code>importance.mode</code></td>
<td>
<p>Importance mode used.</p>
</td></tr>
<tr><td><code>num.samples</code></td>
<td>
<p>Number of samples.</p>
</td></tr>
<tr><td><code>splitrule</code></td>
<td>
<p>Splitting rule.</p>
</td></tr>
<tr><td><code>replace</code></td>
<td>
<p>Sample with replacement.</p>
</td></tr>
<tr><td><code>nsplits</code></td>
<td>
<p>Value of <code>nsplits</code> used.</p>
</td></tr>
<tr><td><code>proptry</code></td>
<td>
<p>Value of <code>proptry</code> used.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Roman Hornung, Marvin N. Wright
</p>


<h3>References</h3>


<ul>
<li><p> Hornung, R. (2022). Diversity forests: Using split sampling to enable innovative complex split procedures in random forests. SN Computer Science 3(2):1, &lt;<a href="https://doi.org/10.1007/s42979-021-00920-1">doi:10.1007/s42979-021-00920-1</a>&gt;.
</p>
</li>
<li><p> Wright, M. N., Ziegler, A. (2017). ranger: A fast implementation of random forests for high dimensional data in C++ and R. Journal of Statistical Software 77:1-17, &lt;<a href="https://doi.org/10.18637/jss.v077.i01">doi:10.18637/jss.v077.i01</a>&gt;.
</p>
</li>
<li><p> Breiman, L. (2001). Random forests. Machine Learning 45:5-32, &lt;<a href="https://doi.org/10.1023/A%3A1010933404324">doi:10.1023/A:1010933404324</a>&gt;.
</p>
</li>
<li><p> Malley, J. D., Kruppa, J., Dasgupta, A., Malley, K. G., &amp; Ziegler, A. (2012). Probability machines: consistent probability estimation using nonparametric learning machines. Methods of Information in Medicine 51:74-81, &lt;<a href="https://doi.org/10.3414/ME00-01-0052">doi:10.3414/ME00-01-0052</a>&gt;.
</p>
</li>
<li><p> Meinshausen (2006). Quantile Regression Forests. Journal of Machine Learning Research 7:983-999.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+predict.divfor">predict.divfor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

## Load package:
library("diversityForest")

## Set seed to obtain reproducible results:
set.seed(1234)

## Diversity forest with default settings (NOT recommended)
# Classification:
divfor(Species ~ ., data = iris, num.trees = 20)
# Regression:
iris2 &lt;- iris; iris2$Species &lt;- NULL; iris2$Y &lt;- rnorm(nrow(iris2))
divfor(Y ~ ., data = iris2, num.trees = 20)
# Survival:
library("survival")
divfor(Surv(time, status) ~ ., data = veteran, num.trees = 20, respect.unordered.factors = "order")
# NOTE: num.trees = 20 is specified too small for practical 
# purposes - the prediction performance of the resulting 
# forest will be suboptimal!!
# In practice, num.trees = 500 (default value) or a 
# larger number should be used.

## Diversity forest with specified values for nsplits and proptry (NOT recommended)
divfor(Species ~ ., data = iris, nsplits = 10, proptry = 0.4, num.trees = 20)
# NOTE again: num.trees = 20 is specified too small for practical purposes.

## Applying diversity forest after optimizing the values of nsplits and proptry (recommended)
tuneres &lt;- tunedivfor(formula = Species ~ ., data = iris, num.trees.pre = 20)
# NOTE: num.trees.pre = 20 is specified too small for practical 
# purposes - the out-of-bag error estimates of the forests 
# constructed during optimization will be much too variable!!
# In practice, num.trees.pre = 500 (default value) or a 
# larger number should be used.
divfor(Species ~ ., data = iris, nsplits = tuneres$nsplitsopt, 
  proptry = tuneres$proptryopt, num.trees = 20)
# NOTE again: num.trees = 20 is specified too small for practical purposes.

## Prediction
train.idx &lt;- sample(nrow(iris), 2/3 * nrow(iris))
iris.train &lt;- iris[train.idx, ]
iris.test &lt;- iris[-train.idx, ]
tuneres &lt;- tunedivfor(formula = Species ~ ., data = iris.train, num.trees.pre = 20)
# NOTE again: num.trees.pre = 20 is specified too small for practical purposes.
rg.iris &lt;- divfor(Species ~ ., data = iris.train, nsplits = tuneres$nsplitsopt, 
  proptry = tuneres$proptryopt, num.trees = 20)
# NOTE again: num.trees = 20 is specified too small for practical purposes.
pred.iris &lt;- predict(rg.iris, data = iris.test)
table(iris.test$Species, pred.iris$predictions)

## Variable importance
rg.iris &lt;- divfor(Species ~ ., data = iris, importance = "permutation", num.trees = 20)
# NOTE again: num.trees = 20 is specified too small for practical purposes.
rg.iris$variable.importance

## End(Not run)

</code></pre>

<hr>
<h2 id='importance.divfor'>Diversity Forest variable importance</h2><span id='topic+importance.divfor'></span><span id='topic+importance'></span>

<h3>Description</h3>

<p>Extract variable importance of <code>divfor</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'divfor'
importance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="importance.divfor_+3A_x">x</code></td>
<td>
<p><code>divfor</code> object.</p>
</td></tr>
<tr><td><code id="importance.divfor_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Variable importance measures.
</p>


<h3>Author(s)</h3>

<p>Marvin N. Wright
</p>


<h3>See Also</h3>

<p><code><a href="#topic+divfor">divfor</a></code>
</p>

<hr>
<h2 id='interactionfor'>Construct an interaction forest prediction rule and calculate EIM values as described in Hornung &amp; Boulesteix (2022).</h2><span id='topic+interactionfor'></span>

<h3>Description</h3>

<p>Implements interaction forests as described in Hornung &amp; Boulesteix (2022).
Currently, categorical, metric, and survival outcomes are supported. Interaction forests feature the effect importance measure (EIM),
which can be used to rank the covariate variable pairs with respect to the impact of their interaction effects on prediction.
This allows to identify relevant interaction effects. Interaction forests focus on well interpretable interaction effects.
See the 'Details' section below for more details. In addition, we strongly recommend to consult Section C of 
Supplementary Material 1 of Hornung &amp; Boulesteix (2022), which uses detailed examples of interaction forest analyses
with code to illustrate how interaction forests can be used in 
applications: <a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167947322000408-mmc1.pdf">Link</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interactionfor(
  formula = NULL,
  data = NULL,
  importance = "both",
  num.trees = NULL,
  simplify.large.n = TRUE,
  num.trees.eim.large.n = NULL,
  write.forest = TRUE,
  probability = FALSE,
  min.node.size = NULL,
  max.depth = NULL,
  replace = FALSE,
  sample.fraction = ifelse(replace, 1, 0.7),
  case.weights = NULL,
  class.weights = NULL,
  splitrule = NULL,
  always.split.variables = NULL,
  keep.inbag = FALSE,
  inbag = NULL,
  holdout = FALSE,
  quantreg = FALSE,
  oob.error = TRUE,
  num.threads = NULL,
  verbose = TRUE,
  seed = NULL,
  dependent.variable.name = NULL,
  status.variable.name = NULL,
  npairs = NULL,
  classification = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interactionfor_+3A_formula">formula</code></td>
<td>
<p>Object of class <code>formula</code> or <code>character</code> describing the model to fit.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_data">data</code></td>
<td>
<p>Training data of class <code>data.frame</code>, <code>matrix</code>, <code>dgCMatrix</code> (Matrix) or <code>gwaa.data</code> (GenABEL).</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_importance">importance</code></td>
<td>
<p>Effect importance mode. One of the following: &quot;both&quot; (the default), &quot;qualitative&quot;, &quot;quantitative&quot;, &quot;mainonly&quot;, &quot;none&quot;. 
See the 'Details' section below for explanation.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees. The default number is 20000, if EIM values should be computed 
and 2000 otherwise. Note that if <code>simplify.large.n = TRUE</code> (default), the number of observations 
is larger than 1000, and EIM values should be calculated two forests are constructed, one for calculating 
the EIM values and one for prediction (cf. 'Details' section). In such cases, the default number of 
trees used for the forest for EIM value calculation is 20000 and the default number of trees used 
for the forest for prediction is 2000.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_simplify.large.n">simplify.large.n</code></td>
<td>
<p>Should restricted tree depths be used, when calculating EIM values for large data sets? See the 'Details' section below for more information. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_num.trees.eim.large.n">num.trees.eim.large.n</code></td>
<td>
<p>Number of trees in the forest used for calculating the EIM values for large data sets. 
If <code>num.trees</code> is provided, but not <code>num.trees.eim.large.n</code>, the value given by <code>num.trees</code> 
will be used. The default number is 20000. Only used when <code>simplify.large.n = TRUE</code>.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_write.forest">write.forest</code></td>
<td>
<p>Save <code>interaction.forest</code> object, required for prediction. Set to <code>FALSE</code> to reduce 
memory usage if no prediction intended.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_probability">probability</code></td>
<td>
<p>Grow a probability forest as in Malley et al. (2012).</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_min.node.size">min.node.size</code></td>
<td>
<p>Minimal node size. Default 1 for classification, 5 for regression, 3 for survival, and 5 for probability.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_max.depth">max.depth</code></td>
<td>
<p>Maximal tree depth. A value of NULL or 0 (the default) corresponds to unlimited depth, 1 to tree stumps (1 split per tree).</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_replace">replace</code></td>
<td>
<p>Sample with replacement. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>Fraction of observations to sample. Default is 1 for sampling with replacement and 0.7 for sampling without replacement. For classification, this can be a vector of class-specific values.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_case.weights">case.weights</code></td>
<td>
<p>Weights for sampling of training observations. Observations with larger weights will be selected with higher probability in the bootstrap (or subsampled) samples for the trees.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_class.weights">class.weights</code></td>
<td>
<p>Weights for the outcome classes (in order of the factor levels) in the splitting rule (cost sensitive learning). Classification and probability prediction only. For classification the weights are also applied in the majority vote in terminal nodes.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_splitrule">splitrule</code></td>
<td>
<p>Splitting rule. For classification and probability estimation &quot;gini&quot; or &quot;extratrees&quot; with default &quot;gini&quot;. For regression &quot;variance&quot;, &quot;extratrees&quot; or &quot;maxstat&quot; with default &quot;variance&quot;. For survival &quot;logrank&quot;, &quot;extratrees&quot;, &quot;C&quot; or &quot;maxstat&quot; with default &quot;logrank&quot;. NOTE: For interaction forests currently only the default splitting rules are supported.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_always.split.variables">always.split.variables</code></td>
<td>
<p>Currently not useable. Character vector with variable names to be always selected.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_keep.inbag">keep.inbag</code></td>
<td>
<p>Save how often observations are in-bag in each tree.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_inbag">inbag</code></td>
<td>
<p>Manually set observations per tree. List of size num.trees, containing inbag counts for each observation. Can be used for stratified sampling.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_holdout">holdout</code></td>
<td>
<p>Hold-out mode. Hold-out all samples with case weight 0 and use these for variable importance and prediction error. NOTE: Currently, not useable for interaction forests.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_quantreg">quantreg</code></td>
<td>
<p>Prepare quantile prediction as in quantile regression forests (Meinshausen 2006). Regression only. Set <code>keep.inbag = TRUE</code> to prepare out-of-bag quantile prediction. NOTE: Currently, not useable for interaction forests.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_oob.error">oob.error</code></td>
<td>
<p>Compute OOB prediction error. Set to <code>FALSE</code> to save computation time, e.g. for large survival forests.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads. Default is number of CPUs available.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_verbose">verbose</code></td>
<td>
<p>Show computation status and estimated runtime.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_seed">seed</code></td>
<td>
<p>Random seed. Default is <code>NULL</code>, which generates the seed from <code>R</code>. Set to <code>0</code> to ignore the <code>R</code> seed.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_dependent.variable.name">dependent.variable.name</code></td>
<td>
<p>Name of outcome variable, needed if no formula given. For survival forests this is the time variable.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_status.variable.name">status.variable.name</code></td>
<td>
<p>Name of status variable, only applicable to survival data and needed if no formula given. Use 1 for event and 0 for censoring.</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_npairs">npairs</code></td>
<td>
<p>Number of variable pairs to sample for each split. Default is the square root of the number of independent variables divided by 2 (this number is rounded up).</p>
</td></tr>
<tr><td><code id="interactionfor_+3A_classification">classification</code></td>
<td>
<p>Only needed if data is a matrix. Set to <code>TRUE</code> to grow a classification forest.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The effect importance measure (EIM) of interaction forests distinguishes quantitative and qualitative interaction effects (Peto, 1982).
This is a common distinction as these two types of interaction effects are interpreted in different ways (see below). 
For both of these types, EIM values for each variable pair are obtained: the quantitative and qualitative EIM values.<br />
Interaction forests target easily interpretable types of interaction effects. These can be communicated clearly using statements 
of the following kind: &quot;The strength of the positive (negative) effect of variable A on the outcome depends on the level of variable B&quot;
for quantitative interactions, and &quot;for observations with small values of variable B, the effect of variable A is positive (negative), 
but for observations with large values of B, the effect of A is negative (positive)&quot; for qualitative interactions.<br />
In addition to calculating EIM values for variable pairs, importance values for the individual variables are calculated as well, the univariable
EIM values. These measure the variable importance as in the case of classical variable importance measures of random forests.<br />
The effect importance mode can be set via the <code>importance</code> argument: <code>"qualitative"</code>: Calculate only qualitative EIM values;
<code>"quantitative"</code>: Calculate only quantitative EIM values; <code>"both"</code> (the default): Calculate qualitative and quantitative EIM
values; <code>"mainonly"</code>: Calculate only univariable EIM values.<br />
The top variable pairs with largest quantitative and qualitative EIM values likely have quantitative and qualitative interactions,
respectively, which have a considerable impact on prediction. The top variables with largest univariable EIM values likely have a considerable
impact on prediction. Note that it is currently not possible to test the EIM values for 
statistical significance using the interaction forests algorithm itself. However, the p-values
shown in the plots obtained with <code><a href="#topic+plotEffects">plotEffects</a></code> (which are obtained using bivariable
models) can be adjusted for multiple testing using the Bonferroni procedure to obtain
practical p-values. See the end of the 'Details' section of <code><a href="#topic+plotEffects">plotEffects</a></code> for explanation and guidance.<br />
If the number of variables is larger than 100, not all possible variable pairs are considered, but, using a screening procedure, the
5000 variable pairs with the strongest indications of interaction effects are pre-selected.<br />
<strong>NOTE</strong>: To make interpretations, it is crucial to investigate (visually) the forms the interaction effects of variable pairs 
with large quantitative and qualitative EIM values take. This can be done using the plot function <code><a href="#topic+plot.interactionfor">plot.interactionfor</a></code> 
(first overview) and <code><a href="#topic+plotEffects">plotEffects</a></code>.<br />
NOTE ALSO: As described in Hornung &amp; Boulesteix (2022), in the case of data with larger numbers of variables (larger than 100, 
but more seriously for high-dimensional data), the univariable EIM values can be biased. Therefore, it is strongly recommended 
to interpret the univariable EIM values with caution, if the data are high-dimensional. If it is of interest to measure the univariable 
importance of the variables for high-dimensional data, an additional conventional random forest (e.g., using the <code>ranger</code> package)
should be constructed and the variable importance measure values of this random forest be used for ranking the univariable effects.<br />
For large data sets with many observations the calculation of the EIM values can become very costly - when using fully grown trees.
Therefore, when calculating EIM values for data sets with more than 1000 observations we use the following
maximum tree depths by default (argument: <code>simplify.large.n = TRUE</code>):
</p>

<ul>
<li><p> if <code class="reqn">n \le 1000</code>: Use fully grown trees.
</p>
</li>
<li><p> if <code class="reqn">1000 &lt; n \le 2000</code>: Use tree depth 10.
</p>
</li>
<li><p> if <code class="reqn">2000 &lt; n \le 5000</code>: Use tree depth 7.
</p>
</li>
<li><p> if <code class="reqn">n &gt; 5000</code>: Use tree depth 5.
</p>
</li></ul>

<p>Extensive analyses in Hornung &amp; Boulesteix (2022) suggest that by restricting the tree depth in this way,
the EIM values that would result when using fully grown trees are approximated well. However, the prediction
performance suffers, when using restricted trees. Therefore, we restrict the tree depth only when calculating
the EIM values (if <code class="reqn">n &gt; 1000</code>), but construct a second interaction forest with unrestricted tree depth,
which is then used for prediction purposes.
</p>


<h3>Value</h3>

<p>Object of class <code>interactionfor</code> with elements
</p>
<table>
<tr><td><code>predictions</code></td>
<td>
<p>Predicted classes/values, based on out-of-bag samples (classification and regression only).</p>
</td></tr>
<tr><td><code>num.trees</code></td>
<td>
<p>Number of trees.</p>
</td></tr> 
<tr><td><code>num.independent.variables</code></td>
<td>
<p>Number of independent variables.</p>
</td></tr> 
<tr><td><code>unique.death.times</code></td>
<td>
<p>Unique death times (survival only).</p>
</td></tr> 
<tr><td><code>min.node.size</code></td>
<td>
<p>Value of minimal node size used.</p>
</td></tr> 
<tr><td><code>npairs</code></td>
<td>
<p>Number of variable pairs sampled for each split.</p>
</td></tr> 
<tr><td><code>eim.univ.sorted</code></td>
<td>
<p>Univariable EIM values sorted in decreasing order.</p>
</td></tr> 
<tr><td><code>eim.univ</code></td>
<td>
<p>Univariable EIM values.</p>
</td></tr> 
<tr><td><code>eim.qual.sorted</code></td>
<td>
<p>Qualitative EIM values sorted in decreasing order.</p>
</td></tr> 
<tr><td><code>eim.qual</code></td>
<td>
<p>Qualitative EIM values.</p>
</td></tr> 
<tr><td><code>eim.quant.sorted</code></td>
<td>
<p>Quantitative EIM values sorted in decreasing order.<br />
The labeling of these values
provides the information on the type of quantitative interactions the respective variable
pairs feature. For example, consider a variable pair A and B and say the label reads &quot;A large AND B small&quot;.
This would mean that if the value of A is large and, at the same time, the value
of B is small, the expected value of the outcome variable is (considerably) different from all other cases. For this type of quantitative interaction,
the effect of B is weak for small values of A and strong for large values of A. See Hornung &amp; Boulesteix (2022) 
for more information on the types of quantitative interaction effects targeted by interaction forest.</p>
</td></tr>
<tr><td><code>eim.quant</code></td>
<td>
<p>Quantitative EIM values. These values are labeled analoguously as those in <code>eim.quant.sorted</code>.</p>
</td></tr> 
<tr><td><code>prediction.error</code></td>
<td>
<p>Overall out-of-bag prediction error. 
For classification this is the fraction of misclassified samples, for probability 
estimation the Brier score, for regression the mean squared error and for survival 
one minus Harrell's C-index. This is 'NA' for data sets with more than 100
covariate variables, because for such data sets we pre-select the 5000 variable
pairs with strongest indications of interaction effects. This pre-selection cannot be taken into
account in the out-of-bag error estimation, which is why the out-of-bag error estimates would
be (much) too optimistic for data sets with more than 100 covariate variables.</p>
</td></tr> 
<tr><td><code>forest</code></td>
<td>
<p>Saved forest (If write.forest set to TRUE). Note that the variable IDs in the <code>split.multvarIDs</code> object do not necessarily represent the column number in R.</p>
</td></tr> 
<tr><td><code>confusion.matrix</code></td>
<td>
<p>Contingency table for classes and predictions based on out-of-bag samples (classification only).</p>
</td></tr> 
<tr><td><code>chf</code></td>
<td>
<p>Estimated cumulative hazard function for each sample (survival only).</p>
</td></tr>
<tr><td><code>survival</code></td>
<td>
<p>Estimated survival function for each sample (survival only).</p>
</td></tr>
<tr><td><code>splitrule</code></td>
<td>
<p>Splitting rule.</p>
</td></tr>
<tr><td><code>treetype</code></td>
<td>
<p>Type of forest/tree. classification, regression or survival.</p>
</td></tr>
<tr><td><code>r.squared</code></td>
<td>
<p>R squared. Also called explained variance or coefficient of determination (regression only). Computed on out-of-bag data.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>Function call.</p>
</td></tr>
<tr><td><code>importance.mode</code></td>
<td>
<p>Importance mode used.</p>
</td></tr>
<tr><td><code>num.samples</code></td>
<td>
<p>Number of samples.</p>
</td></tr>
<tr><td><code>replace</code></td>
<td>
<p>Sample with replacement.</p>
</td></tr>
<tr><td><code>eim.quant.rawlists</code></td>
<td>
<p>List containing the four vectors of un-adjusted 'raw' quantitative EIM values 
and the four vectors of adjusted EIM values. These are usually not required by the user.<br />
For each of the four types of quantitative splits there exists a separate
vector of raw quantitative EIM values. For example, <code>eim.quant.large.small.raw</code> contains the raw 
quantitative EIM values of the quantitative split type associated with quantitative interaction effects 
for which the expected values of the outcome variable are different, if the value of variable A is large 
and, at the same time, the value of variable B is small.
The list entries of the un-adjusted 'raw' quantitative EIM values are labeled with the suffix <code>.raw</code>,
while the list entries of the adjusted quantitative EIM values miss this suffix. See Hornung &amp; Boulesteix (2022) for details
on the raw and adjusted EIM values.</p>
</td></tr>
<tr><td><code>promispairs</code></td>
<td>
<p>List giving the indices of the variables in the pre-selected variable pairs. If the number of variables is at most
100, all variable pairs are considered.</p>
</td></tr> 
<tr><td><code>plotres</code></td>
<td>
<p>List ob objects needed by the plot functions: <code>eim.univ.order</code> contains the sorting of the 
univariable EIM values in descending order, where the first element gives the index of the variable with largest EIM value, 
the second element the index of the variable with second-largest EIM value and so on; <code>eim.qual.order</code> / <code>eim.quant.order</code> 
contains the sorting in descending order of the qualitative / quantitative EIM values for the (pre-selected) variable pairs given 
by the object <code>promispairs</code> above. The first element gives the index of the (pre-selected) variable pair with largest 
qualitative / quantitative EIM value, the second element the index of the variable pair with second-largest 
qualitative / quantitative EIM value; <code>data</code> contains the data; <code>yvarname</code> is the name of the outcome variable 
(survival time for survival); <code>statusvarname</code> is the name of the status variable.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Roman Hornung, Marvin N. Wright
</p>


<h3>References</h3>


<ul>
<li><p> Hornung, R., Boulesteix, A.-L. (2022). Interaction forests: Identifying and exploiting interpretable quantitative and qualitative interaction effects. Computational Statistics &amp; Data Analysis 171:107460, &lt;<a href="https://doi.org/10.1016/j.csda.2022.107460">doi:10.1016/j.csda.2022.107460</a>&gt;.
</p>
</li>
<li><p> Hornung, R. (2022). Diversity forests: Using split sampling to enable innovative complex split procedures in random forests. SN Computer Science 3(2):1, &lt;<a href="https://doi.org/10.1007/s42979-021-00920-1">doi:10.1007/s42979-021-00920-1</a>&gt;.
</p>
</li>
<li><p> Peto, R., (1982) Statistical aspects of cancer trials. In: K.E. Halnam (Ed.), Treatment of Cancer. Chapman &amp; Hall: London.
</p>
</li>
<li><p> Wright, M. N., Ziegler, A. (2017). ranger: A fast Implementation of Random Forests for High Dimensional Data in C++ and R. Journal of Statistical Software 77:1-17, &lt;<a href="https://doi.org/10.18637/jss.v077.i01">doi:10.18637/jss.v077.i01</a>&gt;.
</p>
</li>
<li><p> Breiman, L. (2001). Random forests. Machine Learning 45:5-32, &lt;<a href="https://doi.org/10.1023/A%3A1010933404324">doi:10.1023/A:1010933404324</a>&gt;.
</p>
</li>
<li><p> Malley, J. D., Kruppa, J., Dasgupta, A., Malley, K. G., &amp; Ziegler, A. (2012). Probability machines: consistent probability estimation using nonparametric learning machines. Methods of Information in Medicine 51:74-81, &lt;<a href="https://doi.org/10.3414/ME00-01-0052">doi:10.3414/ME00-01-0052</a>&gt;.
</p>
</li>
<li><p> Meinshausen (2006). Quantile Regression Forests. Journal of Machine Learning Research 7:983-999.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+predict.divfor">predict.divfor</a></code>, <code><a href="#topic+plot.interactionfor">plot.interactionfor</a></code>, <code><a href="#topic+plotEffects">plotEffects</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

## Load package:

library("diversityForest")



## Set seed to make results reproducible:

set.seed(1234)



## Construct interaction forests and calculate EIM values:


# Binary outcome:
data(zoo)
modelcat &lt;- interactionfor(dependent.variable.name = "type", data = zoo, 
  num.trees = 20)


# Metric outcome:
data(stock)
modelcont &lt;- interactionfor(dependent.variable.name = "company10", data = stock, 
  num.trees = 20)  
  
  
# Survival outcome:
library("survival")
mgus2$id &lt;- NULL  # 'mgus2' data set is contained in the 'survival' package

# categorical variables need to be of factor format - important!!
mgus2$sex &lt;- factor(mgus2$sex)
mgus2$pstat &lt;- factor(mgus2$pstat)

# Remove the second time variable 'ptime':
mgus2$ptime &lt;- NULL

# Remove missing values:
mgus2 &lt;- mgus2[complete.cases(mgus2),]

# Take subset to make the calculations less computationally
# expensive for the example (in actual applications, we would of course
# use the whole data set):
mgus2sub &lt;- mgus2[sample(1:nrow(mgus2), size=500),]

# Apply 'interactionfor':
modelsurv &lt;- interactionfor(formula = Surv(futime, death) ~ ., data=mgus2sub, num.trees=20)

# NOTE: num.trees = 20 (in the above) would be much too small for practical 
# purposes. This small number of trees was simply used to keep the
# runtime of the example short.
# The default number of trees is num.trees = 20000 if EIM values are calculated
# and num.trees = 2000 otherwise.



## Inspect the rankings of the variables and variable pairs with respect to 
## the univariable, quantitative, and qualitative EIM values:

# Univariable EIM values: 
modelcat$eim.univ.sorted

# Pairs with top quantitative EIM values:
modelcat$eim.quant.sorted[1:5]

# Pairs with top qualitative EIM values:
modelcat$eim.qual.sorted[1:5]



## Investigate visually the forms of the interaction effects of the variable pairs with
## largest quantitative and qualitative EIM values:

plot(modelcat)
plotEffects(modelcat, type="quant") # type="quant" is default.
plotEffects(modelcat, type="qual")



## Prediction:

# Separate 'zoo' data set randomly in training
# and test data:

data(zoo)
train.idx &lt;- sample(nrow(zoo), 2/3 * nrow(zoo))
zoo.train &lt;- zoo[train.idx, ]
zoo.test &lt;- zoo[-train.idx, ]

# Construct interaction forest on training data:
# NOTE again: num.trees = 20 is specified too small for practical purposes.
modelcattrain &lt;- interactionfor(dependent.variable.name = "type", data = zoo, 
                                importance = "none", num.trees = 20)
# NOTE: Because we are only interested in prediction here, we do not
# calculate EIM values (by setting importance = "none"), because this
# speeds up calculations.

# Predict class values of the test data:
pred.zoo &lt;- predict(modelcattrain, data = zoo.test)

# Compare predicted and true class values of the test data:
table(zoo.test$type, pred.zoo$predictions)

## End(Not run)

</code></pre>

<hr>
<h2 id='plot.interactionfor'>Plot method for <code>interactionfor</code> objects</h2><span id='topic+plot.interactionfor'></span>

<h3>Description</h3>

<p>Plot function for <code>interactionfor</code> objects that allows to obtain a first overview of the result of the
interaction forest analysis. This function visualises the distributions of the EIM values and
the estimated forms of the bivariable influences of the variable pairs with largest quantitative and 
qualitative EIM values. Further visual exploration of the result of the interaction
forest analysis should be conducted using <code><a href="#topic+plotEffects">plotEffects</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'interactionfor'
plot(x, numpairsquant = 2, numpairsqual = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.interactionfor_+3A_x">x</code></td>
<td>
<p>Object of class <code>interactionfor</code>.</p>
</td></tr>
<tr><td><code id="plot.interactionfor_+3A_numpairsquant">numpairsquant</code></td>
<td>
<p>The number of pairs with largest quantitative EIM values to plot. Default is 2.</p>
</td></tr>
<tr><td><code id="plot.interactionfor_+3A_numpairsqual">numpairsqual</code></td>
<td>
<p>The number of pairs with largest qualitative EIM values to plot. Default is 2.</p>
</td></tr>
<tr><td><code id="plot.interactionfor_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For details on the plots of the estimated forms of the bivariable influences of the variable pairs see <code><a href="#topic+plotEffects">plotEffects</a></code>.
</p>
<p>NOTE: The p-values shown in the plots are generally much too optimistic and <strong>MUST NOT</strong> be reported 
as the result of a statistical test for significance of interaction. To obtain adjusted p-values that would correspond to
valid tests, it would be possible to multiply these p-values by the number of possible variable pairs, 
which would correspond to Bonferroni-adjusted p-values. See the 'Details' section of <code><a href="#topic+plotEffects">plotEffects</a></code> for further
explanation and guidance. Note, however, that these Bonferroni-adjusted p-values should be interpreted
with caution because, stemming from bivariable models, these p-values do not take the multivariable nature 
of the data into account.
</p>
<p>NOTE ALSO: As described in Hornung &amp; Boulesteix (2022), in the case of data with larger numbers of variables (larger than 100, but more seriously
for high-dimensional data), the univariable EIM values can be biased. Therefore, it is strongly recommended to interpret the univariable EIM values
with caution, if the data are high-dimensional. If it is of interest to measure the univariable importance of the variables for high-dimensional data,
an additional conventional random forest (e.g., using the <code>ranger</code> package) should be constructed and the variable importance measure values
of this random forest be used for ranking the univariable effects.
</p>


<h3>Value</h3>

<p>A ggplot2 plot.
</p>


<h3>Author(s)</h3>

<p>Roman Hornung
</p>


<h3>References</h3>


<ul>
<li><p> Hornung, R., Boulesteix, A.-L. (2022). Interaction forests: Identifying and exploiting interpretable quantitative and qualitative interaction effects. Computational Statistics &amp; Data Analysis 171:107460, &lt;<a href="https://doi.org/10.1016/j.csda.2022.107460">doi:10.1016/j.csda.2022.107460</a>&gt;.
</p>
</li>
<li><p> Hornung, R. (2022). Diversity forests: Using split sampling to enable innovative complex split procedures in random forests. SN Computer Science 3(2):1, &lt;<a href="https://doi.org/10.1007/s42979-021-00920-1">doi:10.1007/s42979-021-00920-1</a>&gt;.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+plotEffects">plotEffects</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

## Load package:

library("diversityForest")



## Set seed to make results reproducible:

set.seed(1234)



## Construct interaction forest and calculate EIM values:

data(stock)
model &lt;- interactionfor(dependent.variable.name = "company10", data = stock, 
                        num.trees = 20)

# NOTE: num.trees = 20 (in the above) would be much too small for practical 
# purposes. This small number of trees was simply used to keep the
# runtime of the example short.
# The default number of trees is num.trees = 20000 if EIM values are calculated
# and num.trees = 2000 otherwise.



## When using the plot() function without further specifications,
## by default the estimated bivariable influences of the two pairs with largest quantitative
## and qualitative EIM values are shown:

plot(model)

# It is, however, also possible to change the numbers of
# pairs with largest quantitative and qualitative EIM values
# to be shown:

plot(model, numpairsquant = 4, numpairsqual = 3)


## End(Not run)

</code></pre>

<hr>
<h2 id='plotEffects'>Interaction forest plots: exploring interaction forest results through visualisation</h2><span id='topic+plotEffects'></span>

<h3>Description</h3>

<p>This function allows to visualise the (estimated) bivariable influences of pairs of variables (with large quantitative and qualitative
EIM values) on the outcome. This step is crucial, because to interpret interaction effects
between variable pairs with large quantitative and qualitative EIM values, it is necessary to learn about the forms
these interaction effects take.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotEffects(
  intobj,
  type = "quant",
  numpairs = 5,
  indpairs = NULL,
  pairs = NULL,
  allwith = NULL,
  pvalues = TRUE,
  twoplots = TRUE,
  addtitles = TRUE,
  plotit = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotEffects_+3A_intobj">intobj</code></td>
<td>
<p>Object of class <code>interactionfor</code>.</p>
</td></tr>
<tr><td><code id="plotEffects_+3A_type">type</code></td>
<td>
<p>This can be either &quot;quant&quot; or &quot;qual&quot; and determines whether the plotted pairs
are sorted according to either the quantitative or qualitative EIM values in decreasing order. Default is &quot;quant&quot;.</p>
</td></tr>
<tr><td><code id="plotEffects_+3A_numpairs">numpairs</code></td>
<td>
<p>The number of pairs to plot (default: 5). This is overwritten by <code>indpairs</code>.</p>
</td></tr>
<tr><td><code id="plotEffects_+3A_indpairs">indpairs</code></td>
<td>
<p>Optional. The indices of the pairs in the sorted lists of quantitative (<code>type="quant"</code>) or qualitative EIM values to plot (<code>type="qual"</code>). This overwrites the <code>numpairs</code> argument.</p>
</td></tr>
<tr><td><code id="plotEffects_+3A_pairs">pairs</code></td>
<td>
<p>This can be used to specify the pairs to plot. It is an optional list of character string vectors, where each of these vectors has length two. Each list element corresponds to one
pair, where the first character string gives the name of the first member of the respective pair to plot and the second character string gives the name of the second member.
This argument overwrites <code>numpairs</code> and <code>indpairs</code>.</p>
</td></tr>
<tr><td><code id="plotEffects_+3A_allwith">allwith</code></td>
<td>
<p>This is an optional character string that can be set to the name of one of the variables. If provided, only variable pairs will
be considered that feature the variable specified by this argument <code>allwith</code>. These pairs are again sorted in decreasing order
according to the quantitative (<code>type="quant"</code>) or qualitative (<code>type="qual"</code>) EIM values and their number is restricted to the value given by <code>numpairs</code>.
This argument <code>allwith</code> can be used, if it is of interest to learn whether a specific variable (e.g., sex or age) interacts
with other variables in the data set and if so, which forms these interactions take.</p>
</td></tr>
<tr><td><code id="plotEffects_+3A_pvalues">pvalues</code></td>
<td>
<p>Set to <code>TRUE</code> (default) to add to the plots p-values from tests for interaction effect obtained using classical
parametric regression approaches. For categorical outcomes logistic regression is used, for metric outcomes linear
regression and for survival outcomes Cox regression. NOTE: These p-values are generally much too optimistic and <strong>MUST NOT</strong> be reported 
as the result of a statistical test for significance of interaction. See the 'Details' section below for further details.</p>
</td></tr>
<tr><td><code id="plotEffects_+3A_twoplots">twoplots</code></td>
<td>
<p>Set to <code>TRUE</code> / <code>FALSE</code> if for each plot page the results of two / one pair(s) of variables should be shown. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="plotEffects_+3A_addtitles">addtitles</code></td>
<td>
<p>Set to <code>TRUE</code> (default) to add headings providing the names of the variables in each pair. If <code>type="quant"</code>, these
headings also give information on the type of quantitative interaction effect. Setting <code>addtitles</code> to <code>FALSE</code> is, for example, useful,
when the produced plots are intended for use in a publication, where these headings might not be desirable.</p>
</td></tr>
<tr><td><code id="plotEffects_+3A_plotit">plotit</code></td>
<td>
<p>This states whether the plots are actually plotted or merely returned as <code>ggplot</code> objects. Default is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each considered pair the bivariable influence of both pair members on the outcome estimated 
using a two-dimensional flexible function is shown. Such visualisations make it possible to learn
about the forms of the interaction effects between variable pairs with large EIM values. Moreover,
these visualisations reveal (pathological) cases in which variable pairs do not show
indications of interaction effects despite featuring large EIM values.<br />
For binary outcomes the probabilities for the second class are estimated, for categorical outcomes with more than two classes
the probabilities for the largest class (i.e., the class with the most observations) are estimated (using the function <code><a href="#topic+plotPair">plotPair</a></code>, a different
class can be selected instead), for metric outcomes the means of the outcome are estimated, and for survival outcomes
the log hazards ratio values compared to the median effect are estimated.<br />
The kinds of estimates shown differ also according to whether both pair members are metric or only one of the two
members is metric and the other one categorical or both pair members are categorical:
</p>

<ul>
<li><p> If both pair members are metric and the outcome is categorical or metric we use two-dimensional LOESS regression, where 
in the case of categorical outcomes, to obtain probability estimates for
the first class (or largest class for multi-class outcomes), we use the value
'1' for the first class (largest class for multi-class outcomes) and the value '0' for the second class
(all other classes for multi-class outcomes).
</p>
</li>
<li><p> If both pair members are metric and the outcome is
survival we use a Cox proportional hazard additive model with a two-dimensional LOESS smooth (<code>gamcox</code> function
from the 'MapGAM' package (version 1.2-5)) and in the rare cases for which the latter fails, we use classical Cox regression
with an interaction term between the two covariates.
</p>
</li>
<li><p> If one pair member is metric and the other one
categorical and the outcome is categorical or metric, we use LOESS regression between
the outcome (coded as '0' and '1' in the case of categorical outcomes as described above)
and the values of the metric variable separately for each category of the categorical
variable. In the rare cases in which the LOESS regression fails we use classical linear regression.
</p>
</li>
<li><p> If one pair member is metric and the other one categorical and the outcome is survival,
we use Cox regression with a linear tail-restricted cubic spline with four knots (univariable
LOESS regression for survival outcomes does not seem to be available yet in R) separately for each
category of the categorical variable. In cases in which
the fitting of this spline regression fails we use classical Cox regression.
</p>
</li>
<li><p> If both pair members are categorical and the outcome is categorical or metric,
we simply calculate the mean of the outcome (coded as '0' and '1' in the case of categorical outcomes
as described above) for each possible combination of the categories of the two variables.
</p>
</li>
<li><p> If both pair members are categorical and the outcome is survival, we use classical 
Cox regression with an interaction term between the two variables (there is no need for
any flexible modelling in this setting, because the Cox model with two categorical variables
plus interaction term is saturated).
</p>
</li></ul>

<p>As described above (function argument: <code>pvalues</code>), there is an option to add p-values from tests for interaction effect
to the plots. If at least one of the variables in the considered variable pair is categorical and features more
than two categories, there are more than one interaction terms in the regression approaches used for testing,
because the categorical variables are dummy-coded. Therefore, in these cases we obtain a p-value for each interaction term.
to obtain a single p-value for the test for interaction we adjust these multiple p-values using the Holm-Bonferroni
procedure and take the minimum of the adjusted p-values.
</p>
<p><strong>NOTE</strong>: These p-values are generally much too optimistic, in particular for small data sets and 
large numbers of variables. The reason for this overoptimism is that these p-values are not adjusted 
for the fact that we already used the data to find the variable pairs with strongest indications of 
interaction effects. This is similar to a multiple testing problem.
Therefore, these p-values should only be seen as a rough guide to be interpreted very cautiously
and <strong>MUST NOT</strong> be reported as the results of a statistical test for significance
of interaction. To obtain adjusted p-values that would correspond to
valid tests, it would be possible to multiply these p-values by the number of possible pairs, 
which would correspond to Bonferroni-adjusted p-values. For example, assume that we have 30 
covariate variables. In that case the number of possible pairs would be 'choose(30, 2) = 435', 
which is why we would need to multiply each p-value by 435
to obtain an adjusted p-value (or keep the original p-values and divide the significance 
level 0.05 by 435). Note, however, that Bonferroni-adjusted p-values deliver quite conservative results,
that is, weaker effects might not be detected using these p-values, while, however, effects for
which these p-values are small (<code class="reqn">&lt; 0.05</code>) are most likely relevant.
Note further that these Bonferroni-adjusted p-values should be interpreted
with caution because, stemming from bivariable models, these p-values do not take the 
multivariable nature of the data into account.
</p>


<h3>Value</h3>

<p>A list of ggplot2 plots returned invisibly.
</p>


<h3>Author(s)</h3>

<p>Roman Hornung
</p>


<h3>References</h3>


<ul>
<li><p> Hornung, R., Boulesteix, A.-L. (2022). Interaction forests: Identifying and exploiting interpretable quantitative and qualitative interaction effects. Computational Statistics &amp; Data Analysis 171:107460, &lt;<a href="https://doi.org/10.1016/j.csda.2022.107460">doi:10.1016/j.csda.2022.107460</a>&gt;.
</p>
</li>
<li><p> Hornung, R. (2022). Diversity forests: Using split sampling to enable innovative complex split procedures in random forests. SN Computer Science 3(2):1, &lt;<a href="https://doi.org/10.1007/s42979-021-00920-1">doi:10.1007/s42979-021-00920-1</a>&gt;.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+plot.interactionfor">plot.interactionfor</a></code>, <code><a href="#topic+plotPair">plotPair</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

## Load package:

library("diversityForest")



## Set seed to make results reproducible:

set.seed(1234)



## Construct interaction forest and calculate EIM values:

data(stock)
model &lt;- interactionfor(dependent.variable.name = "company10", data = stock, 
                        num.trees = 20)

# NOTE: num.trees = 20 (in the above) would be much too small for practical 
# purposes. This small number of trees was simply used to keep the
# runtime of the example short.
# The default number of trees is num.trees = 20000 if EIM values are calculated
# and num.trees = 2000 otherwise.



## Obtain a first overview by applying the plot() function for
## interactionfor obects:

plot(model)



## Several possible application cases of the plotEffects() function:

# Visualise the estimated bivariable influences of the five variable pairs with the 
# largest quantitative EIM values:

plotEffects(model) # type="quant" is default.


# Visualise the estimated bivariable influences of the five pairs with the 
# largest qualitative EIM values:

plotEffects(model, type="qual")


# Visualise the estimated bivariable influences of all (eight) pairs that involve
# the variable "company7" sorted in decreasing order according to the
# qualitative EIM values:

plotEffects(model, allwith="company7", type="qual", numpairs=8)


# Visualise the estimated bivariable influences of the pairs with third and fifth
# largest qualitative EIM values:

plotEffects(model, type="qual", indpairs=c(3,5))


# Visualise the estimated bivariable influences of the pairs ("company3", "company5") and
# ("company1", "company9"):

plotEffects(model, pairs=list(c("company3", "company5"), c("company1", "company9")))



## Saving of plots generated with the plotEffects() function (e.g., for use in publications):

# Apply plotEffects() to obtain plots for the five variable pairs
# with the largest qualitative EIM values and store these plots in
# an object 'ps':

ps &lt;- plotEffects(model, type="qual", pvalues=FALSE, twoplots=FALSE, addtitles=FALSE, plotit=FALSE)

# pvalues = FALSE states that no p-values should be shown in the plots,
# because these might not be desired in plots meant for publication.
# twoplots = FALSE ensures that we get one plot for each page instead of two plots per page.
# addtitles = FALSE removes the automatically generated titles, because these are likely
# not desired in publications.
# plotit = FALSE ensures that the plots are not displayed, but only returned (invisibly) 
# by plotEffects().


# Save the plot with second largest qualitative EIM value:

p1 &lt;- ps[[2]]

# Add title:
library("ggpubr")
p1 &lt;- annotate_figure(p1, top = text_grob("My descriptive plot title 1", face = "bold", size = 14))
p1

# Save as PDF:
# library("ggplot2")
# ggsave(file="mypathtofolder/FigureXY1.pdf", width=14, height=6)


# Save the plot with fifth largest qualitative EIM value:

p2 &lt;- ps[[5]]

# Add title:
p2 &lt;- annotate_figure(p2, top = text_grob("My descriptive plot title 2", face = "bold", size = 14))
p2

# Save as PDF:
# ggsave(file="mypathtofolder/FigureXY1.pdf", width=14, height=6)


# Combine both of the above plots:
p &lt;- ggarrange(p1, p2, nrow = 2)
p

# Save the combined plot:
# ggsave(file="mypathtofolder/FigureXYcombined.pdf", width=14, height=11)

# NOTE: Using plotEffects() it is not possible to change the plots 
# themselves (by e.g., increasing the label sizes or changing the 
# axes ranges). However, the function plotPair() can be used to change 
# the plots themselves.


## End(Not run)

</code></pre>

<hr>
<h2 id='plotPair'>Plot of the (estimated) simultaneous influence of two variables</h2><span id='topic+plotPair'></span>

<h3>Description</h3>

<p>This function allows to visualise the (estimated) bivariable influence of a single specific pair of variables on the outcome. The estimation
and plotting is performed in the same way as in <code><a href="#topic+plotEffects">plotEffects</a></code>. However, <code>plotPair</code> does not require an <code>interactionfor</code> object
and can thus be used also without a constructed interaction forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotPair(
  pair,
  yvarname,
  statusvarname = NULL,
  data,
  levelsorder1 = NULL,
  levelsorder2 = NULL,
  categprob = NULL,
  pvalue = TRUE,
  returnseparate = FALSE,
  intobj = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotPair_+3A_pair">pair</code></td>
<td>
<p>Character string vector of length two, where the first character string gives the name of the first member of the respective pair to plot and the second character string gives the name of the second member.
Note that the order of the two pair members in <code>pair</code> determines how the results are 
visualised: The estimated influence of the second pair member is visualised conditionally
on different values of the first pair member.</p>
</td></tr>
<tr><td><code id="plotPair_+3A_yvarname">yvarname</code></td>
<td>
<p>Name of outcome variable.</p>
</td></tr>
<tr><td><code id="plotPair_+3A_statusvarname">statusvarname</code></td>
<td>
<p>Name of status variable, only applicable to survival data.</p>
</td></tr>
<tr><td><code id="plotPair_+3A_data">data</code></td>
<td>
<p>Data frame containing the variables.</p>
</td></tr>
<tr><td><code id="plotPair_+3A_levelsorder1">levelsorder1</code></td>
<td>
<p>Optional. Order the categories of the first variable should have in the plot (if it is categorical). Character string vector, where the
i-th entry contains the name of the category that should take the i-th place in the ordering of the categories of the first variable.</p>
</td></tr>
<tr><td><code id="plotPair_+3A_levelsorder2">levelsorder2</code></td>
<td>
<p>Optional. Order the categories of the second variable should have in the plot (if it is categorical). Character string vector specified in an analogous
way as <code>levelsorder1</code>.</p>
</td></tr>
<tr><td><code id="plotPair_+3A_categprob">categprob</code></td>
<td>
<p>Optional. Only relevant for categorical outcomes with more than two classes. 
Name of the class for which probabilities should be estimated. As described in <code><a href="#topic+plotEffects">plotEffects</a></code>, 
for categorical outcomes with more than two classes, by default the probabilities
for the largest class (i.e., the class with the most observations) are estimated when visualising the
bivariable influence of the variables. Using <code>categprob</code> a different class can be specified for the
class for which probabilities should be estimated.</p>
</td></tr>
<tr><td><code id="plotPair_+3A_pvalue">pvalue</code></td>
<td>
<p>Set to <code>TRUE</code> (default) to add to the plot a p-value from a test for interaction effect obtained using a classical
parametric regression approach. For categorical outcomes logistic regression is used, for metric outcomes linear
regression and for survival outcomes Cox regression. See the 'Details' section of <code><a href="#topic+plotEffects">plotEffects</a></code> for further details.</p>
</td></tr>
<tr><td><code id="plotPair_+3A_returnseparate">returnseparate</code></td>
<td>
<p>Set to <code>TRUE</code> to return invisibly the two generated ggplot plots separately in the form of a list. The
latter option is useful, because it allows to manipulate the resulting plots (label size etc.) and makes it possible to consider
only one of the two plots. The default is <code>FALSE</code>, which results in the two plots being returned together in the form of a 
<code>ggarrange</code> object.</p>
</td></tr>
<tr><td><code id="plotPair_+3A_intobj">intobj</code></td>
<td>
<p>Optional. Object of class <code>interactionfor</code>. If this is provided, the ordering of the categories
obtained when constructing the interaction forest will be used for categorical variables. See Hornung &amp; Boulesteix (2022) for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the 'Details' section of <code><a href="#topic+plotEffects">plotEffects</a></code>.
</p>


<h3>Value</h3>

<p>A ggplot2 plot.
</p>


<h3>Author(s)</h3>

<p>Roman Hornung
</p>


<h3>References</h3>


<ul>
<li><p> Hornung, R., Boulesteix, A.-L. (2022). Interaction forests: Identifying and exploiting interpretable quantitative and qualitative interaction effects. Computational Statistics &amp; Data Analysis 171:107460, &lt;<a href="https://doi.org/10.1016/j.csda.2022.107460">doi:10.1016/j.csda.2022.107460</a>&gt;.
</p>
</li>
<li><p> Hornung, R. (2022). Diversity forests: Using split sampling to enable innovative complex split procedures in random forests. SN Computer Science 3(2):1, &lt;<a href="https://doi.org/10.1007/s42979-021-00920-1">doi:10.1007/s42979-021-00920-1</a>&gt;.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+plotEffects">plotEffects</a></code>, <code><a href="#topic+plot.interactionfor">plot.interactionfor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

## Load package:

library("diversityForest")



## Visualise the estimated bivariable influence of 'toothed' and 'feathers' on
## the probability of type="mammal":

data(zoo)
plotPair(pair = c("toothed", "feathers"), yvarname="type", data = zoo)



## Visualise the estimated bivariable influence of 'creat' and 'hgb' on
## survival (more precisely, on the log hazards ratio compared to the
## median effect):

library("survival")
mgus2compl &lt;- mgus2[complete.cases(mgus2),]
plotPair(pair=c("creat", "hgb"), yvarname="futime", statusvarname = "death", data=mgus2compl)

# Problem: The outliers in the left plot make it difficult to see what is going
# on in the region with creat values smaller than about two even though the
# majority of values lie there.

# --&gt; Solution: We re-run the above line setting returnseparate = TRUE, because
# this allows to get the two ggplot plots separately, which can then be manipulated
# to change the x-axis range in order to remove the outliers:

ps &lt;- plotPair(pair=c("creat", "hgb"), yvarname="futime", statusvarname = "death", 
               data=mgus2compl, returnseparate = TRUE)

# Change the x-axis range:
library("ggplot2")
ps[[1]] + xlim(c(0.5,2))
# Save the plot:
# ggsave(file="mypathtofolder/FigureXY1.pdf", width=7, height=6)

# We can, for example, also change the label sizes of the second plot:
# With original label sizes:
ps[[2]]
# With larger label sizes:
ps[[2]] +  theme(axis.title=element_text(size=15))
# Save the plot:
# library("ggplot2")
# ggsave(file="mypathtofolder/FigureXY2.pdf", width=7, height=6)


## End(Not run)

</code></pre>

<hr>
<h2 id='predict.divfor'>Diversity Forest prediction</h2><span id='topic+predict.divfor'></span>

<h3>Description</h3>

<p>Prediction with new data and a saved forest from <code><a href="#topic+divfor">divfor</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'divfor'
predict(
  object,
  data = NULL,
  predict.all = FALSE,
  num.trees = object$num.trees,
  type = "response",
  se.method = "infjack",
  quantiles = c(0.1, 0.5, 0.9),
  seed = NULL,
  num.threads = NULL,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.divfor_+3A_object">object</code></td>
<td>
<p><code>divfor</code> object.</p>
</td></tr>
<tr><td><code id="predict.divfor_+3A_data">data</code></td>
<td>
<p>New test data of class <code>data.frame</code> or <code>gwaa.data</code> (GenABEL).</p>
</td></tr>
<tr><td><code id="predict.divfor_+3A_predict.all">predict.all</code></td>
<td>
<p>Return individual predictions for each tree instead of aggregated predictions for all trees. Return a matrix (sample x tree) for classification and regression, a 3d array for probability estimation (sample x class x tree) and survival (sample x time x tree).</p>
</td></tr>
<tr><td><code id="predict.divfor_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees used for prediction. The first <code>num.trees</code> in the forest are used.</p>
</td></tr>
<tr><td><code id="predict.divfor_+3A_type">type</code></td>
<td>
<p>Type of prediction. One of 'response', 'se', 'terminalNodes', 'quantiles' with default 'response'. See below for details.</p>
</td></tr>
<tr><td><code id="predict.divfor_+3A_se.method">se.method</code></td>
<td>
<p>Method to compute standard errors. One of 'jack', 'infjack' with default 'infjack'. Only applicable if type = 'se'. See below for details.</p>
</td></tr>
<tr><td><code id="predict.divfor_+3A_quantiles">quantiles</code></td>
<td>
<p>Vector of quantiles for quantile prediction. Set <code>type = 'quantiles'</code> to use.</p>
</td></tr>
<tr><td><code id="predict.divfor_+3A_seed">seed</code></td>
<td>
<p>Random seed. Default is <code>NULL</code>, which generates the seed from <code>R</code>. Set to <code>0</code> to ignore the <code>R</code> seed. The seed is used in case of ties in classification mode.</p>
</td></tr>
<tr><td><code id="predict.divfor_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads. Default is number of CPUs available.</p>
</td></tr>
<tr><td><code id="predict.divfor_+3A_verbose">verbose</code></td>
<td>
<p>Verbose output on or off.</p>
</td></tr>
<tr><td><code id="predict.divfor_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This package is a fork of the R package 'ranger' that implements random forests using an
efficient C++ implementation. More precisely, 'diversityForest' was written by modifying
the code of 'ranger', version 0.11.0. Therefore, details on further functionalities
of the code that are not presented in the help pages of 'diversityForest' are found
in the help pages of 'ranger' (version 0.11.0). The code in the example sections of <code><a href="#topic+divfor">divfor</a></code> and <code><a href="#topic+tunedivfor">tunedivfor</a></code> can be
used as a template for all common application scenarios with respect to classification,
regression and survival prediction using univariable, binary splitting. Some function 
arguments adopted from the 'ranger' package may not be useable with diversity forests
(for the current package version).
</p>


<h3>Value</h3>

<p>Object of class <code>divfor.prediction</code> with elements
</p>

<table>
<tr>
 <td style="text-align: left;">
      <code>predictions</code>    </td><td style="text-align: left;"> Predicted classes/values (only for classification and regression)  </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>unique.death.times</code> </td><td style="text-align: left;"> Unique death times (only for survival). </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>chf</code> </td><td style="text-align: left;"> Estimated cumulative hazard function for each sample (only for survival). </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>survival</code> </td><td style="text-align: left;"> Estimated survival function for each sample (only for survival). </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>num.trees</code>   </td><td style="text-align: left;"> Number of trees. </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>num.independent.variables</code> </td><td style="text-align: left;"> Number of independent variables. </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>treetype</code>    </td><td style="text-align: left;"> Type of forest/tree. Classification, regression or survival. </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>num.samples</code>     </td><td style="text-align: left;"> Number of samples.
  </td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Marvin N. Wright
</p>


<h3>References</h3>


<ul>
<li><p> Hornung, R. (2022). Diversity forests: Using split sampling to enable innovative complex split procedures in random forests. SN Computer Science 3(2):1, &lt;<a href="https://doi.org/10.1007/s42979-021-00920-1">doi:10.1007/s42979-021-00920-1</a>&gt;.
</p>
</li>
<li><p> Wright, M. N., Ziegler, A. (2017). ranger: A fast Implementation of Random Forests for High Dimensional Data in C++ and R. Journal of Statistical Software 77:1-17, &lt;<a href="https://doi.org/10.18637/jss.v077.i01">doi:10.18637/jss.v077.i01</a>&gt;.
</p>
</li>
<li><p> Wager, S., Hastie T., &amp; Efron, B. (2014). Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife. Journal of Machine Learning Research 15:1625-1651.
</p>
</li>
<li><p> Meinshausen (2006). Quantile Regression Forests. Journal of Machine Learning Research 7:983-999.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+divfor">divfor</a></code>
</p>

<hr>
<h2 id='predict.interactionfor'>Interaction Forest prediction</h2><span id='topic+predict.interactionfor'></span>

<h3>Description</h3>

<p>Prediction with new data and a saved interaction forest from <code><a href="#topic+interactionfor">interactionfor</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'interactionfor'
predict(
  object,
  data = NULL,
  predict.all = FALSE,
  num.trees = object$num.trees,
  type = "response",
  se.method = "infjack",
  quantiles = c(0.1, 0.5, 0.9),
  seed = NULL,
  num.threads = NULL,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.interactionfor_+3A_object">object</code></td>
<td>
<p><code>interactionfor</code> object.</p>
</td></tr>
<tr><td><code id="predict.interactionfor_+3A_data">data</code></td>
<td>
<p>New test data of class <code>data.frame</code> or <code>gwaa.data</code> (GenABEL).</p>
</td></tr>
<tr><td><code id="predict.interactionfor_+3A_predict.all">predict.all</code></td>
<td>
<p>Return individual predictions for each tree instead of aggregated predictions for all trees. Return a matrix (sample x tree) for classification and regression, a 3d array for probability estimation (sample x class x tree) and survival (sample x time x tree).</p>
</td></tr>
<tr><td><code id="predict.interactionfor_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees used for prediction. The first <code>num.trees</code> in the forest are used.</p>
</td></tr>
<tr><td><code id="predict.interactionfor_+3A_type">type</code></td>
<td>
<p>Type of prediction. One of 'response', 'se', 'terminalNodes', 'quantiles' with default 'response'. See below for details.</p>
</td></tr>
<tr><td><code id="predict.interactionfor_+3A_se.method">se.method</code></td>
<td>
<p>Method to compute standard errors. One of 'jack', 'infjack' with default 'infjack'. Only applicable if type = 'se'. See below for details.</p>
</td></tr>
<tr><td><code id="predict.interactionfor_+3A_quantiles">quantiles</code></td>
<td>
<p>Vector of quantiles for quantile prediction. Set <code>type = 'quantiles'</code> to use.</p>
</td></tr>
<tr><td><code id="predict.interactionfor_+3A_seed">seed</code></td>
<td>
<p>Random seed. Default is <code>NULL</code>, which generates the seed from <code>R</code>. Set to <code>0</code> to ignore the <code>R</code> seed. The seed is used in case of ties in classification mode.</p>
</td></tr>
<tr><td><code id="predict.interactionfor_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads. Default is number of CPUs available.</p>
</td></tr>
<tr><td><code id="predict.interactionfor_+3A_verbose">verbose</code></td>
<td>
<p>Verbose output on or off.</p>
</td></tr>
<tr><td><code id="predict.interactionfor_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that his package is a fork of the R package 'ranger' that implements random forests using an
efficient C++ implementation. The documentation is in large parts taken from
'ranger', where some parts of the documentation may not apply to (the current version of) the 'diversityForest' package.
Details on further functionalities of the code that are not presented in the help pages of 'diversityForest' are found
in the help pages of 'ranger' (version 0.11.0).
</p>


<h3>Value</h3>

<p>Object of class <code>interaction.prediction</code> with elements
</p>

<table>
<tr>
 <td style="text-align: left;">
      <code>predictions</code>    </td><td style="text-align: left;"> Predicted classes/values (only for classification and regression)  </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>unique.death.times</code> </td><td style="text-align: left;"> Unique death times (only for survival). </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>chf</code> </td><td style="text-align: left;"> Estimated cumulative hazard function for each sample (only for survival). </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>survival</code> </td><td style="text-align: left;"> Estimated survival function for each sample (only for survival). </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>num.trees</code>   </td><td style="text-align: left;"> Number of trees. </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>num.independent.variables</code> </td><td style="text-align: left;"> Number of independent variables. </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>treetype</code>    </td><td style="text-align: left;"> Type of forest/tree. Classification, regression or survival. </td>
</tr>
<tr>
 <td style="text-align: left;">
      <code>num.samples</code>     </td><td style="text-align: left;"> Number of samples.
  </td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Marvin N. Wright, Roman Hornung
</p>


<h3>References</h3>


<ul>
<li><p> Hornung, R., Boulesteix, A.-L. (2022). Interaction forests: Identifying and exploiting interpretable quantitative and qualitative interaction effects. Computational Statistics &amp; Data Analysis 171:107460, &lt;<a href="https://doi.org/10.1016/j.csda.2022.107460">doi:10.1016/j.csda.2022.107460</a>&gt;.
</p>
</li>
<li><p> Hornung, R. (2022). Diversity forests: Using split sampling to enable innovative complex split procedures in random forests. SN Computer Science 3(2):1, &lt;<a href="https://doi.org/10.1007/s42979-021-00920-1">doi:10.1007/s42979-021-00920-1</a>&gt;.
</p>
</li>
<li><p> Wright, M. N., Ziegler, A. (2017). ranger: A fast Implementation of Random Forests for High Dimensional Data in C++ and R. Journal of Statistical Software 77:1-17, &lt;<a href="https://doi.org/10.18637/jss.v077.i01">doi:10.18637/jss.v077.i01</a>&gt;.
</p>
</li>
<li><p> Wager, S., Hastie T., &amp; Efron, B. (2014). Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife. Journal of Machine Learning Research 15:1625-1651.
</p>
</li>
<li><p> Meinshausen (2006). Quantile Regression Forests. Journal of Machine Learning Research 7:983-999.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+interactionfor">interactionfor</a></code>
</p>

<hr>
<h2 id='stock'>Data on stock prices of aerospace companies</h2><span id='topic+stock'></span>

<h3>Description</h3>

<p>This data set contains 950 daily stock prices from January 1988 through October 1991, for ten aerospace companies. 
The names of the companies are anonymised and the stock prices for one of these companies (<code>company10</code>) were 
flagged as the outcome variable. Thus, for this data set, both the outcome and the covariates were metric.
</p>


<h3>Format</h3>

<p>A data frame with 950 observations, nine covariates and one metric outcome variable
</p>


<h3>Details</h3>

<p>The variables are as follows: covariates: <code>company1</code>, ..., <code>company9</code>, outcome variable: <code>company10</code>.
</p>


<h3>Source</h3>

<p>OpenML: data.name: stock, data.id: 223, link: <a href="https://www.openml.org/d/223/">https://www.openml.org/d/223/</a>
</p>


<h3>References</h3>


<ul>
<li><p> Vanschoren, J., van Rijn, J. N., Bischl, B., Torgo, L. (2013). OpenML: networked science in machine learning. SIGKDD Explorations 15(2):49-60, &lt;<a href="https://doi.org/10.1145/2641190.2641198">doi:10.1145/2641190.2641198</a>&gt;.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
## Load data:
data(stock)

## Dimension of data:
dim(stock)

## First rows of data:
head(stock) 

</code></pre>

<hr>
<h2 id='tunedivfor'>Optimization of the values of the tuning parameters <code>nsplits</code> and <code>proptry</code></h2><span id='topic+tunedivfor'></span>

<h3>Description</h3>

<p>First, both for <code>nsplits</code> and <code>proptry</code> a grid of possible values may be provided,
where default grids are used if no grids are provided. Second, for each pairwise combination of
values from these two grids a forest is constructed. Third, 
that pair of <code>nsplits</code> and <code>proptry</code> values is used as the optimized set of parameter
values that is associated with the smallest out-of-bag prediction error. If several pairs of
parameter values are associated with the same smallest out-of-bag prediction error, the
pair with the smallest (parameter) values is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tunedivfor(
  formula = NULL,
  data = NULL,
  nsplitsgrid = c(2, 5, 10, 30, 50, 100, 200),
  proptrygrid = c(0.05, 1),
  num.trees.pre = 500
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tunedivfor_+3A_formula">formula</code></td>
<td>
<p>Object of class <code>formula</code> or <code>character</code> describing the model to fit. Interaction terms supported only for numerical variables.</p>
</td></tr>
<tr><td><code id="tunedivfor_+3A_data">data</code></td>
<td>
<p>Training data of class <code>data.frame</code>, <code>matrix</code>, <code>dgCMatrix</code> (Matrix) or <code>gwaa.data</code> (GenABEL).</p>
</td></tr>
<tr><td><code id="tunedivfor_+3A_nsplitsgrid">nsplitsgrid</code></td>
<td>
<p>Grid of values to consider for <code>nsplits</code>. Default grid: 2, 5, 10, 30, 50, 100, 200.</p>
</td></tr>
<tr><td><code id="tunedivfor_+3A_proptrygrid">proptrygrid</code></td>
<td>
<p>Grid of values to consider for <code>proptry</code>. Default grid: 0.05, 1.</p>
</td></tr>
<tr><td><code id="tunedivfor_+3A_num.trees.pre">num.trees.pre</code></td>
<td>
<p>Number of trees used for each forest constructed during tuning parameter optimization. Default is 500.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with elements
</p>
<table>
<tr><td><code>nsplitsopt</code></td>
<td>
<p>Optimized value of <code>nsplits</code>.</p>
</td></tr>
<tr><td><code>proptryopt</code></td>
<td>
<p>Optimized value of <code>proptry</code>.</p>
</td></tr>
<tr><td><code>tunegrid</code></td>
<td>
<p>Two-dimensional <code>data.frame</code>, where each row contains one pair of values considered for <code>nsplits</code> (first entry) and <code>proptry</code> (second entry).</p>
</td></tr>
<tr><td><code>ooberrs</code></td>
<td>
<p>The out-of-bag prediction errors obtained for each pair of values considered for <code>nsplits</code> and <code>proptry</code>, where the ordering of pairs of values is the same as in <code>tunegrid</code> (see above).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Roman Hornung
</p>


<h3>References</h3>


<ul>
<li><p> Hornung, R. (2022). Diversity forests: Using split sampling to enable innovative complex split procedures in random forests. SN Computer Science 3(2):1, &lt;<a href="https://doi.org/10.1007/s42979-021-00920-1">doi:10.1007/s42979-021-00920-1</a>&gt;.
</p>
</li>
<li><p> Wright, M. N., Ziegler, A. (2017). ranger: A fast Implementation of Random Forests for High Dimensional Data in C++ and R. Journal of Statistical Software 77:1-17, &lt;<a href="https://doi.org/10.18637/jss.v077.i01">doi:10.18637/jss.v077.i01</a>&gt;.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+divfor">divfor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Load package:

library("diversityForest")


## Set seed to obtain reproducible results:

set.seed(1234)


## Tuning parameter optimization for the iris data set:

tuneres &lt;- tunedivfor(formula = Species ~ ., data = iris, num.trees.pre = 20)
# NOTE: num.trees.pre = 20 is specified too small for practical 
# purposes - the out-of-bag error estimates of the forests 
# constructed during optimization will be much too variable!!
# In practice, num.trees.pre = 500 (default value) or a 
# larger number should be used.

tuneres

tuneres$nsplitsopt
tuneres$proptryopt
tuneres$tunegrid
tuneres$ooberrs

</code></pre>

<hr>
<h2 id='zoo'>Data on biological species</h2><span id='topic+zoo'></span>

<h3>Description</h3>

<p>This data set describes 101 different biological species using 16 simple attributes,
where 15 of these are binary and one is metric (the number of legs). The outcome 
&quot;mammal vs. other&quot; (<code>type</code>) is binary.
</p>


<h3>Format</h3>

<p>A data frame with 101 observations, 16 covariates and one binary outcome variable
</p>


<h3>Details</h3>

<p>The variables are as follows:
</p>

<ul>
<li> <p><code>hair</code>. factor. Presence of hairs (true = yes; false = no)
</p>
</li>
<li> <p><code>feathers</code>. factor. Presence of feathers (true = yes; false = no)
</p>
</li>
<li> <p><code>eggs</code>. factor. Does the species lay eggs? (true = yes; false = no)
</p>
</li>
<li> <p><code>milk</code>. factor. Does the species give milk? (true = yes; false = no)  
</p>
</li>
<li> <p><code>airborne</code>. factor. Does the species fly? (true = yes; false = no)   
</p>
</li>
<li> <p><code>aquatic</code>. factor. Does the species live in the water? (true = yes; false = no) 
</p>
</li>
<li> <p><code>predator</code>. factor. Is the species a predator? (true = yes; false = no)
</p>
</li>
<li> <p><code>toothed</code>. factor. Presence of teeth (true = yes; false = no)  
</p>
</li>
<li> <p><code>backbone</code>. factor. Presence of backbone (true = yes; false = no)
</p>
</li>
<li> <p><code>breathes</code>. factor. Does the species breathe with lungs? (true = yes; false = no)
</p>
</li>
<li> <p><code>venomous</code>. factor. Is the species venomous? (true = yes; false = no)
</p>
</li>
<li> <p><code>fins</code>. factor. Presence of fins (true = yes; false = no)  
</p>
</li>
<li> <p><code>legs</code>. metric. Number of legs
</p>
</li>
<li> <p><code>tail</code>. factor. Presence of tail (true = yes; false = no)  
</p>
</li>
<li> <p><code>domestic</code>. factor. Is the species domestic? (true = yes; false = no)  
</p>
</li>
<li> <p><code>catsize</code>. factor. Is the species large? (true = yes; false = no)   
</p>
</li>
<li> <p><code>type</code>. factor. Binary outcome variable - type of species ('mammal' vs. 'other')
</p>
</li></ul>

<p><code style="white-space: pre;">&#8288; &#8288;</code><br />
The original openML dataset contains an additional variable <code>animal</code>, which is removed in this 
version of the data set. This variable provided the names of all species.
</p>


<h3>Source</h3>

<p>OpenML: data.name: zoo, data.id: 965, link: <a href="https://www.openml.org/d/965/">https://www.openml.org/d/965/</a>
</p>


<h3>References</h3>


<ul>
<li><p> Vanschoren, J., van Rijn, J. N., Bischl, B., Torgo, L. (2013). OpenML: networked science in machine learning. SIGKDD Explorations 15(2):49-60, &lt;<a href="https://doi.org/10.1145/2641190.2641198">doi:10.1145/2641190.2641198</a>&gt;.
</p>
</li>
<li><p> Dua, D., Graff, C. (2019) UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science. <a href="https://archive.ics.uci.edu/ml/">https://archive.ics.uci.edu/ml/</a>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
##' Load data:
data(zoo)

##' Numbers of observations in the two classes:
table(zoo$type)

##' Dimension of data:
dim(zoo)

##' First rows of data:
head(zoo) 

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
