<!DOCTYPE html><html><head><title>Help for package erboost</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {erboost}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#erboost'><p>ER-Boost Expectile Regression Modeling</p></a></li>
<li><a href='#erboost.object'><p>ER-Boost Expectile Regression Model Object</p></a></li>
<li><a href='#erboost.perf'><p>erboost performance</p></a></li>
<li><a href='#plot.erboost'><p> Marginal plots of fitted erboost objects</p></a></li>
<li><a href='#predict.erboost'><p> Predict method for erboost Model Fits</p></a></li>
<li><a href='#relative.influence'><p> Methods for estimating relative influence</p></a></li>
<li><a href='#summary.erboost'><p>Summary of a erboost object</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Nonparametric Multiple Expectile Regression via ER-Boost</td>
</tr>
<tr>
<td>Version:</td>
<td>1.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-01-17</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.12.0), lattice, splines</td>
</tr>
<tr>
<td>Description:</td>
<td>Expectile regression is a nice tool for estimating the conditional expectiles of a response variable given a set of covariates. This package implements a regression tree based gradient boosting estimator for nonparametric multiple expectile regression, proposed by Yang, Y., Qian, W. and Zou, H. (2018) &lt;<a href="https://doi.org/10.1080%2F00949655.2013.876024">doi:10.1080/00949655.2013.876024</a>&gt;. The code is based on the 'gbm' package originally developed by Greg Ridgeway.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-19 05:59:41 UTC; jiaozi</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-19 11:00:05 UTC</td>
</tr>
<tr>
<td>Author:</td>
<td>Yi Yang [aut, cre] (http://www.math.mcgill.ca/yyang/),
  Hui Zou [aut] (http://users.stat.umn.edu/~zouxx019/),
  Greg Ridgeway [ctb, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Yi Yang &lt;yi.yang6@mcgill.ca&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
</table>
<hr>
<h2 id='erboost'>ER-Boost Expectile Regression Modeling</h2><span id='topic+erboost'></span><span id='topic+erboost.more'></span><span id='topic+erboost.fit'></span>

<h3>Description</h3>

<p>Fits ER-Boost Expectile Regression models.</p>


<h3>Usage</h3>

<pre><code class='language-R'>erboost(formula = formula(data),
    distribution = list(name="expectile",alpha=0.5),
    data = list(),
    weights,
    var.monotone = NULL,
    n.trees = 3000,
    interaction.depth = 3,
    n.minobsinnode = 10,
    shrinkage = 0.001,
    bag.fraction = 0.5,
    train.fraction = 1.0,
    cv.folds=0,
    keep.data = TRUE,
    verbose = TRUE)

erboost.fit(x,y,
        offset = NULL,
        misc = NULL,
        distribution = list(name="expectile",alpha=0.5),
        w = NULL,
        var.monotone = NULL,
        n.trees = 3000,
        interaction.depth = 3,
        n.minobsinnode = 10,
        shrinkage = 0.001,
        bag.fraction = 0.5,
        train.fraction = 1.0,
        keep.data = TRUE,
        verbose = TRUE,
        var.names = NULL,
        response.name = NULL)

erboost.more(object,
         n.new.trees = 3000,
         data = NULL,
         weights = NULL,
         offset = NULL,
         verbose = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="erboost_+3A_formula">formula</code></td>
<td>
<p>a symbolic description of the model to be fit. The formula may 
include an offset term (e.g. y~offset(n)+x). If <code>keep.data=FALSE</code> in 
the initial call to <code>erboost</code> then it is the user's responsibility to 
resupply the offset to <code><a href="#topic+erboost.more">erboost.more</a></code>.</p>
</td></tr>
<tr><td><code id="erboost_+3A_distribution">distribution</code></td>
<td>
<p>a list with a component <code>name</code> specifying the distribution 
and any additional parameters needed. Expectile regression is available and <code>distribution</code> must a list of the form 
<code>list(name="expectile",alpha=0.25)</code> where <code>alpha</code> is the expectile 
to estimate. The current version's expectile regression methods do 
not handle non-constant weights and will stop.</p>
</td></tr>
<tr><td><code id="erboost_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the model. By
default the variables are taken from <code>environment(formula)</code>, typically 
the environment from which <code>erboost</code> is called. If <code>keep.data=TRUE</code> in 
the initial call to <code>erboost</code> then <code>erboost</code> stores a copy with the 
object. If <code>keep.data=FALSE</code> then subsequent calls to 
<code><a href="#topic+erboost.more">erboost.more</a></code> must resupply the same dataset. It becomes the user's 
responsibility to resupply the same data at this point.</p>
</td></tr>
<tr><td><code id="erboost_+3A_weights">weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting process. 
Must be positive but do not need to be normalized. If <code>keep.data=FALSE</code> 
in the initial call to <code>erboost</code> then it is the user's responsibility to 
resupply the weights to <code><a href="#topic+erboost.more">erboost.more</a></code>.</p>
</td></tr>
<tr><td><code id="erboost_+3A_var.monotone">var.monotone</code></td>
<td>
<p>an optional vector, the same length as the number of
predictors, indicating which variables have a monotone increasing (+1),
decreasing (-1), or arbitrary (0) relationship with the outcome.</p>
</td></tr>
<tr><td><code id="erboost_+3A_n.trees">n.trees</code></td>
<td>
<p>the total number of trees to fit. This is equivalent to the
number of iterations and the number of basis functions in the additive
expansion. The default number is 3000. <b>Users should not always use the default value, but choose 
the appropriate value of <code>n.trees</code> based on their data.</b> Please see &quot;details&quot; section below.</p>
</td></tr>
<tr><td><code id="erboost_+3A_cv.folds">cv.folds</code></td>
<td>
<p>Number of cross-validation folds to perform. If <code>cv.folds</code>&gt;1 then
<code>erboost</code>, in addition to the usual fit, will perform a cross-validation, calculate
an estimate of generalization error returned in <code>cv.error</code>.</p>
</td></tr>
<tr><td><code id="erboost_+3A_interaction.depth">interaction.depth</code></td>
<td>
<p>The maximum depth of variable interactions. 1 implies
an additive model, 2 implies a model with up to 2-way interactions, etc. 
The default value is 3. <b>Users should not always use the default value, but choose 
the appropriate value of <code>interaction.depth</code> based on their data.</b>
Please see &quot;details&quot; section below.</p>
</td></tr>
<tr><td><code id="erboost_+3A_n.minobsinnode">n.minobsinnode</code></td>
<td>
<p>minimum number of observations in the trees terminal
nodes. Note that this is the actual number of observations not the total
weight.</p>
</td></tr>
<tr><td><code id="erboost_+3A_shrinkage">shrinkage</code></td>
<td>
<p>a shrinkage parameter applied to each tree in the expansion.
Also known as the learning rate or step-size reduction.</p>
</td></tr>
<tr><td><code id="erboost_+3A_bag.fraction">bag.fraction</code></td>
<td>
<p>the fraction of the training set observations randomly
selected to propose the next tree in the expansion. This introduces randomnesses
into the model fit. If <code>bag.fraction</code>&lt;1 then running the same model twice
will result in similar but different fits. <code>erboost</code> uses the R random number
generator so <code>set.seed</code> can ensure that the model can be
reconstructed. Preferably, the user can save the returned
<code><a href="#topic+erboost.object">erboost.object</a></code> using <code><a href="base.html#topic+save">save</a></code>.</p>
</td></tr>
<tr><td><code id="erboost_+3A_train.fraction">train.fraction</code></td>
<td>
<p>The first <code>train.fraction * nrows(data)</code>
observations are used to fit the <code>erboost</code> and the remainder are used for
computing out-of-sample estimates of the loss function.</p>
</td></tr>
<tr><td><code id="erboost_+3A_keep.data">keep.data</code></td>
<td>
<p>a logical variable indicating whether to keep the data and
an index of the data stored with the object. Keeping the data and index makes
subsequent calls to <code><a href="#topic+erboost.more">erboost.more</a></code> faster at the cost of storing an
extra copy of the dataset.</p>
</td></tr>
<tr><td><code id="erboost_+3A_object">object</code></td>
<td>
<p>a <code>erboost</code> object created from an initial call to
<code><a href="#topic+erboost">erboost</a></code>.</p>
</td></tr>
<tr><td><code id="erboost_+3A_n.new.trees">n.new.trees</code></td>
<td>
<p>the number of additional trees to add to <code>object</code>. The default number is 3000.</p>
</td></tr>
<tr><td><code id="erboost_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, erboost will print out progress and performance indicators.
If this option is left unspecified for erboost.more then it uses <code>verbose</code> from
<code>object</code>.</p>
</td></tr>
<tr><td><code id="erboost_+3A_x">x</code>, <code id="erboost_+3A_y">y</code></td>
<td>
<p>For <code>erboost.fit</code>: <code>x</code> is a data frame or data matrix containing the
predictor variables and <code>y</code> is the vector of outcomes. The number of rows
in <code>x</code> must be the same as the length of <code>y</code>.</p>
</td></tr>
<tr><td><code id="erboost_+3A_offset">offset</code></td>
<td>
<p>a vector of values for the offset</p>
</td></tr>
<tr><td><code id="erboost_+3A_misc">misc</code></td>
<td>
<p>For <code>erboost.fit</code>: <code>misc</code> is an R object that is simply passed on to
the erboost engine.</p>
</td></tr>
<tr><td><code id="erboost_+3A_w">w</code></td>
<td>
<p>For <code>erboost.fit</code>: <code>w</code> is a vector of weights of the same
length as the <code>y</code>.</p>
</td></tr>
<tr><td><code id="erboost_+3A_var.names">var.names</code></td>
<td>
<p>For <code>erboost.fit</code>: A vector of strings of length equal to the
number of columns of <code>x</code> containing the names of the predictor variables.</p>
</td></tr>
<tr><td><code id="erboost_+3A_response.name">response.name</code></td>
<td>
<p>For <code>erboost.fit</code>: A character string label for the response
variable.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Expectile regression (Newey &amp; Powell 1987) is a nice tool for estimating the conditional expectiles of a response variable given a set of covariates. This package implements a regression tree based gradient boosting estimator for nonparametric multiple expectile regression. The code is a modified version of <code>gbm</code> library (<a href="https://cran.r-project.org/package=gbm">https://cran.r-project.org/package=gbm</a>) originally written by Greg Ridgeway.
</p>
<p>Boosting is the process of iteratively adding basis functions in a greedy
fashion so that each additional basis function further reduces the selected
loss function. This implementation closely follows Friedman's Gradient
Boosting Machine (Friedman, 2001).
</p>
<p>In addition to many of the features documented in the Gradient Boosting Machine,
<code>erboost</code> offers additional features including the out-of-bag estimator for
the optimal number of iterations, the ability to store and manipulate the
resulting <code>erboost</code> object.
</p>
<p>Concerning tuning parameters, <b><code>interaction.depth</code></b> and <b><code>n.trees</code></b> are two of the most important tuning parameters in <span class="pkg">erboost</span>. <b>Users should not always use the default values of those two parameters, instead they should choose the appropriate values of <code>interaction.depth</code> and <code>n.trees</code> according to their data.</b> For example, if <code>n.trees</code>, which is the maximal number of trees to fit, is set to be too small, then it is possible that the actual optimal number of trees (which is <code>best.iter</code> selected by the function <code>erboost.perf</code> in &quot;example&quot; section) for a particular data  exceeds this number, resulting a sub-optimal model. <b>Therefore, users should always fit the model with a large enough <code>n.trees</code> such that <code>n.trees</code> is greater than the potential optimal number of trees. The same principle also applies on <code>interaction.depth</code></b>.
</p>
<p><code>erboost.fit</code> provides the link between R and the C++ erboost engine. <code>erboost</code>
is a front-end to <code>erboost.fit</code> that uses the familiar R modeling formulas.
However, <code><a href="stats.html#topic+model.frame">model.frame</a></code> is very slow if there are many
predictor variables. For power-users with many variables use <code>erboost.fit</code>.
For general practice <code>erboost</code> is preferable.
</p>


<h3>Value</h3>

<p><code>erboost</code>, <code>erboost.fit</code>, and <code>erboost.more</code> return a
<code><a href="#topic+erboost.object">erboost.object</a></code>.
</p>


<h3>Author(s)</h3>

<p>Yi Yang <a href="mailto:yiyang@umn.edu">yiyang@umn.edu</a> and Hui Zou <a href="mailto:hzou@stat.umn.edu">hzou@stat.umn.edu</a>
</p>


<h3>References</h3>

<p>Yang, Y. and Zou, H. (2015), &ldquo;Nonparametric Multiple Expectile Regression via ER-Boost,&rdquo; <em>Journal of Statistical Computation and Simulation</em>, 84(1), 84-95.
</p>
<p>G. Ridgeway (1999). &ldquo;The state of boosting,&rdquo; <em>Computing Science and
Statistics</em> 31:172-181.
</p>
<p><a href="https://cran.r-project.org/package=gbm">https://cran.r-project.org/package=gbm</a><br />
</p>
<p>J.H. Friedman (2001). &ldquo;Greedy Function Approximation: A Gradient Boosting
Machine,&rdquo; <em>Annals of Statistics</em> 29(5):1189-1232.
</p>
<p>J.H. Friedman (2002). &ldquo;Stochastic Gradient Boosting,&rdquo; <em>Computational Statistics
and Data Analysis</em> 38(4):367-378.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+erboost.object">erboost.object</a></code>,
<code><a href="#topic+erboost.perf">erboost.perf</a></code>,
<code><a href="#topic+plot.erboost">plot.erboost</a></code>,
<code><a href="#topic+predict.erboost">predict.erboost</a></code>,
<code><a href="#topic+summary.erboost">summary.erboost</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
N &lt;- 200
X1 &lt;- runif(N)
X2 &lt;- 2*runif(N)
X3 &lt;- ordered(sample(letters[1:4],N,replace=TRUE),levels=letters[4:1])
X4 &lt;- factor(sample(letters[1:6],N,replace=TRUE))
X5 &lt;- factor(sample(letters[1:3],N,replace=TRUE))
X6 &lt;- 3*runif(N)
mu &lt;- c(-1,0,1,2)[as.numeric(X3)]

SNR &lt;- 10 # signal-to-noise ratio
Y &lt;- X1**1.5 + 2 * (X2**.5) + mu
sigma &lt;- sqrt(var(Y)/SNR)
Y &lt;- Y + rnorm(N,0,sigma)

# introduce some missing values
X1[sample(1:N,size=50)] &lt;- NA
X4[sample(1:N,size=30)] &lt;- NA

data &lt;- data.frame(Y=Y,X1=X1,X2=X2,X3=X3,X4=X4,X5=X5,X6=X6)

# fit initial model
erboost1 &lt;- erboost(Y~X1+X2+X3+X4+X5+X6,         # formula
    data=data,                   # dataset
    var.monotone=c(0,0,0,0,0,0), # -1: monotone decrease,
                                 # +1: monotone increase,
                                 #  0: no monotone restrictions
    distribution=list(name="expectile",alpha=0.5),
                                 # expectile
    n.trees=3000,                # number of trees
    shrinkage=0.005,             # shrinkage or learning rate,
                                 # 0.001 to 0.1 usually work
    interaction.depth=3,         # 1: additive model, 2: two-way interactions, etc.
    bag.fraction = 0.5,          # subsampling fraction, 0.5 is probably best
    train.fraction = 0.5,        # fraction of data for training,
                                 # first train.fraction*N used for training
    n.minobsinnode = 10,         # minimum total weight needed in each node
    cv.folds = 5,                # do 5-fold cross-validation
    keep.data=TRUE,              # keep a copy of the dataset with the object
    verbose=TRUE)                # print out progress


# check performance using a 50% heldout test set
best.iter &lt;- erboost.perf(erboost1,method="test")
print(best.iter)

# check performance using 5-fold cross-validation
best.iter &lt;- erboost.perf(erboost1,method="cv")
print(best.iter)

# plot the performance
# plot variable influence
summary(erboost1,n.trees=1)         # based on the first tree
summary(erboost1,n.trees=best.iter) # based on the estimated best number of trees

# make some new data
N &lt;- 20
X1 &lt;- runif(N)
X2 &lt;- 2*runif(N)
X3 &lt;- ordered(sample(letters[1:4],N,replace=TRUE))
X4 &lt;- factor(sample(letters[1:6],N,replace=TRUE))
X5 &lt;- factor(sample(letters[1:3],N,replace=TRUE))
X6 &lt;- 3*runif(N)
mu &lt;- c(-1,0,1,2)[as.numeric(X3)]

Y &lt;- X1**1.5 + 2 * (X2**.5) + mu + rnorm(N,0,sigma)

data2 &lt;- data.frame(Y=Y,X1=X1,X2=X2,X3=X3,X4=X4,X5=X5,X6=X6)

# predict on the new data using "best" number of trees
# f.predict generally will be on the canonical scale
f.predict &lt;- predict.erboost(erboost1,data2,best.iter)

# least squares error
print(sum((data2$Y-f.predict)^2))

# create marginal plots
# plot variable X1 after "best" iterations
plot.erboost(erboost1,1,best.iter)
# contour plot of variables 1 and 3 after "best" iterations
plot.erboost(erboost1,c(1,3),best.iter)

# do another 20 iterations
erboost2 &lt;- erboost.more(erboost1,20,
                 verbose=FALSE) # stop printing detailed progress
</code></pre>

<hr>
<h2 id='erboost.object'>ER-Boost Expectile Regression Model Object</h2><span id='topic+erboost.object'></span>

<h3>Description</h3>

<p>These are objects representing fitted <code>erboost</code>s.</p>


<h3>Value</h3>

<table>
<tr><td><code>initF</code></td>
<td>
<p>the &quot;intercept&quot; term, the initial predicted value to which trees
make adjustments</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>a vector containing the fitted values on the scale of regression
function</p>
</td></tr>
<tr><td><code>train.error</code></td>
<td>
<p>a vector of length equal to the number of fitted trees
containing the value of the loss function for each boosting iteration
evaluated on the training data</p>
</td></tr>
<tr><td><code>valid.error</code></td>
<td>
<p>a vector of length equal to the number of fitted trees
containing the value of the loss function for each boosting iteration
evaluated on the validation data</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>if <code>cv.folds</code>&lt;2 this component is NULL. Otherwise, this 
component is a vector of length equal to the number of fitted trees
containing a cross-validated estimate of the loss function for each boosting 
iteration</p>
</td></tr>
<tr><td><code>oobag.improve</code></td>
<td>
<p>a vector of length equal to the number of fitted trees
containing an out-of-bag estimate of the marginal reduction in the expected
value of the loss function. The out-of-bag estimate uses only the training
data and is useful for estimating the optimal number of boosting iterations.
See <code><a href="#topic+erboost.perf">erboost.perf</a></code></p>
</td></tr>
<tr><td><code>trees</code></td>
<td>
<p>a list containing the tree structures.</p>
</td></tr>
<tr><td><code>c.splits</code></td>
<td>
<p>a list of all the categorical splits in the collection of
trees. If the <code>trees[[i]]</code> component of a <code>erboost</code> object describes a
categorical split then the splitting value will refer to a component of
<code>c.splits</code>. That component of <code>c.splits</code> will be a vector of length
equal to the number of levels in the categorical split variable. -1 indicates
left, +1 indicates right, and 0 indicates that the level was not present in the
training data</p>
</td></tr>
</table>


<h3>Structure</h3>

<p>The following components must be included in a legitimate <code>erboost</code> object.</p>


<h3>Author(s)</h3>

<p>Yi Yang <a href="mailto:yiyang@umn.edu">yiyang@umn.edu</a> and Hui Zou <a href="mailto:hzou@stat.umn.edu">hzou@stat.umn.edu</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+erboost">erboost</a></code>
</p>

<hr>
<h2 id='erboost.perf'>erboost performance</h2><span id='topic+erboost.perf'></span>

<h3>Description</h3>

<p>Estimates the optimal number of boosting iterations for a <code>erboost</code> object and
optionally plots various performance measures
</p>


<h3>Usage</h3>

<pre><code class='language-R'>erboost.perf(object, 
         plot.it = TRUE, 
         oobag.curve = FALSE, 
         overlay = TRUE, 
         method)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="erboost.perf_+3A_object">object</code></td>
<td>
<p>a <code><a href="#topic+erboost.object">erboost.object</a></code> created from an initial call to 
<code><a href="#topic+erboost">erboost</a></code>.</p>
</td></tr>
<tr><td><code id="erboost.perf_+3A_plot.it">plot.it</code></td>
<td>
<p>an indicator of whether or not to plot the performance measures.
Setting <code>plot.it=TRUE</code> creates two plots. The first plot plots 
<code>object$train.error</code> (in black) and <code>object$valid.error</code> (in red) 
versus the iteration number. The scale of the error measurement, shown on the 
left vertical axis, depends on the <code>distribution</code> argument used in the 
initial call to <code><a href="#topic+erboost">erboost</a></code>.</p>
</td></tr>
<tr><td><code id="erboost.perf_+3A_oobag.curve">oobag.curve</code></td>
<td>
<p>indicates whether to plot the out-of-bag performance measures
in a second plot.</p>
</td></tr>
<tr><td><code id="erboost.perf_+3A_overlay">overlay</code></td>
<td>
<p>if TRUE and oobag.curve=TRUE then a right y-axis is added to the 
training and test error plot and the estimated cumulative improvement in the loss 
function is plotted versus the iteration number.</p>
</td></tr>
<tr><td><code id="erboost.perf_+3A_method">method</code></td>
<td>
<p>indicate the method used to estimate the optimal number
of boosting iterations. <code>method="OOB"</code> computes the out-of-bag
estimate and <code>method="test"</code> uses the test (or validation) dataset 
to compute an out-of-sample estimate. <code>method="cv"</code> extracts the 
optimal number of iterations using cross-validation if <code>erboost</code> was called
with <code>cv.folds</code>&gt;1</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>erboost.perf</code> returns the estimated optimal number of iterations. The method 
of computation depends on the <code>method</code> argument.</p>


<h3>Author(s)</h3>

<p>Yi Yang <a href="mailto:yiyang@umn.edu">yiyang@umn.edu</a> and Hui Zou <a href="mailto:hzou@stat.umn.edu">hzou@stat.umn.edu</a></p>


<h3>References</h3>

<p>Yang, Y. and Zou, H. (2015), &ldquo;Nonparametric Multiple Expectile Regression via ER-Boost,&rdquo; <em>Journal of Statistical Computation and Simulation</em>, 84(1), 84-95.
</p>
<p>G. Ridgeway (1999). &ldquo;The state of boosting,&rdquo; <em>Computing Science and
Statistics</em> 31:172-181.
</p>
<p><a href="https://cran.r-project.org/package=gbm">https://cran.r-project.org/package=gbm</a><br />
</p>


<h3>See Also</h3>

<p><code><a href="#topic+erboost">erboost</a></code>, <code><a href="#topic+erboost.object">erboost.object</a></code></p>

<hr>
<h2 id='plot.erboost'> Marginal plots of fitted erboost objects </h2><span id='topic+plot.erboost'></span>

<h3>Description</h3>

<p>Plots the marginal effect of the selected variables by &quot;integrating&quot; out the other variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'erboost'
plot(x,
     i.var = 1,
     n.trees = x$n.trees,
     continuous.resolution = 100,
     return.grid = FALSE,
     ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.erboost_+3A_x">x</code></td>
<td>
<p> a <code><a href="#topic+erboost.object">erboost.object</a></code> fitted using a call to <code><a href="#topic+erboost">erboost</a></code></p>
</td></tr>
<tr><td><code id="plot.erboost_+3A_i.var">i.var</code></td>
<td>
<p>a vector of indices or the names of the variables to plot. If
using indices, the variables are indexed in the same order that they appear
in the initial <code>erboost</code> formula.
If <code>length(i.var)</code> is between 1 and 3 then <code>plot.erboost</code> produces the plots. Otherwise,
<code>plot.erboost</code> returns only the grid of evaluation points and their average predictions</p>
</td></tr>
<tr><td><code id="plot.erboost_+3A_n.trees">n.trees</code></td>
<td>
<p> the number of trees used to generate the plot. Only the first
<code>n.trees</code> trees will be used</p>
</td></tr>
<tr><td><code id="plot.erboost_+3A_continuous.resolution">continuous.resolution</code></td>
<td>
<p> The number of equally space points at which to
evaluate continuous predictors </p>
</td></tr>
<tr><td><code id="plot.erboost_+3A_return.grid">return.grid</code></td>
<td>
<p> if <code>TRUE</code> then <code>plot.erboost</code> produces no graphics and only returns
the grid of evaluation points and their average predictions. This is useful for
customizing the graphics for special variable types or for dimensions greater
than 3 </p>
</td></tr>
<tr><td><code id="plot.erboost_+3A_...">...</code></td>
<td>
<p> other arguments passed to the plot function </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>plot.erboost</code> produces low dimensional projections of the
<code><a href="#topic+erboost.object">erboost.object</a></code> by integrating out the variables not included in the
<code>i.var</code> argument. The function selects a grid of points and uses the
weighted tree traversal method described in Friedman (2001) to do the
integration. Based on the variable types included in the projection,
<code>plot.erboost</code> selects an appropriate display choosing amongst line plots,
contour plots, and <code><a href="lattice.html#topic+lattice">lattice</a></code> plots. If the default graphics
are not sufficient the user may set <code>return.grid=TRUE</code>, store the result
of the function, and develop another graphic display more appropriate to the
particular example.
</p>


<h3>Value</h3>

<p>Nothing unless <code>return.grid</code> is true then <code>plot.erboost</code> produces no
graphics and only returns the grid of evaluation points and their average
predictions.
</p>


<h3>Author(s)</h3>

<p>Yi Yang <a href="mailto:yiyang@umn.edu">yiyang@umn.edu</a> and Hui Zou <a href="mailto:hzou@stat.umn.edu">hzou@stat.umn.edu</a></p>


<h3>References</h3>

<p>Yang, Y. and Zou, H. (2015), &ldquo;Nonparametric Multiple Expectile Regression via ER-Boost,&rdquo; <em>Journal of Statistical Computation and Simulation</em>, 84(1), 84-95.
</p>
<p>G. Ridgeway (1999). &ldquo;The state of boosting,&rdquo; <em>Computing Science and
Statistics</em> 31:172-181.
</p>
<p><a href="https://cran.r-project.org/package=gbm">https://cran.r-project.org/package=gbm</a><br />
</p>
<p>J.H. Friedman (2001). &quot;Greedy Function Approximation: A Gradient Boosting
Machine,&quot; Annals of Statistics 29(4).
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+erboost">erboost</a></code>, <code><a href="#topic+erboost.object">erboost.object</a></code>, <code><a href="graphics.html#topic+plot">plot</a></code> </p>

<hr>
<h2 id='predict.erboost'> Predict method for erboost Model Fits </h2><span id='topic+predict.erboost'></span>

<h3>Description</h3>

<p>Predicted values based on an ER-Boost Expectile regression model object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'erboost'
predict(object,
        newdata,
        n.trees,
        single.tree=FALSE,
        ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.erboost_+3A_object">object</code></td>
<td>
<p> Object of class inheriting from (<code><a href="#topic+erboost.object">erboost.object</a></code>) </p>
</td></tr>
<tr><td><code id="predict.erboost_+3A_newdata">newdata</code></td>
<td>
<p> Data frame of observations for which to make predictions </p>
</td></tr>
<tr><td><code id="predict.erboost_+3A_n.trees">n.trees</code></td>
<td>
<p> Number of trees used in the prediction. <code>n.trees</code> may
be a vector in which case predictions are returned for each
iteration specified</p>
</td></tr>
<tr><td><code id="predict.erboost_+3A_single.tree">single.tree</code></td>
<td>
<p>If <code>single.tree=TRUE</code> then <code>predict.erboost</code> returns
only the predictions from tree(s) <code>n.trees</code></p>
</td></tr>
<tr><td><code id="predict.erboost_+3A_...">...</code></td>
<td>
<p> further arguments passed to or from other methods </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>predict.erboost</code> produces predicted values for each observation in <code>newdata</code> using the the first <code>n.trees</code> iterations of the boosting sequence. If <code>n.trees</code> is a vector than the result is a matrix with each column representing the predictions from erboost models with <code>n.trees[1]</code> iterations, <code>n.trees[2]</code> iterations, and so on.
</p>
<p>The predictions from <code>erboost</code> do not include the offset term. The user may add the value of the offset to the predicted value if desired.
</p>
<p>If <code>object</code> was fit using <code><a href="#topic+erboost.fit">erboost.fit</a></code> there will be no
<code>Terms</code> component. Therefore, the user has greater responsibility to make
sure that <code>newdata</code> is of the same format (order and number of variables)
as the one originally used to fit the model.
</p>


<h3>Value</h3>

<p>Returns a vector of predictions. By default the predictions are on the scale of f(x).
</p>


<h3>Author(s)</h3>

<p>Yi Yang <a href="mailto:yiyang@umn.edu">yiyang@umn.edu</a> and Hui Zou <a href="mailto:hzou@stat.umn.edu">hzou@stat.umn.edu</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+erboost">erboost</a></code>, <code><a href="#topic+erboost.object">erboost.object</a></code>
</p>

<hr>
<h2 id='relative.influence'> Methods for estimating relative influence </h2><span id='topic+relative.influence'></span><span id='topic+permutation.test.erboost'></span><span id='topic+erboost.loss'></span>

<h3>Description</h3>

<p>Helper functions for computing the relative influence of each variable in the erboost object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relative.influence(object, n.trees)
permutation.test.erboost(object, n.trees)
erboost.loss(y,f,w,offset,dist,baseline)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relative.influence_+3A_object">object</code></td>
<td>
<p>a <code>erboost</code> object created from an initial call to <code><a href="#topic+erboost">erboost</a></code>.</p>
</td></tr>
<tr><td><code id="relative.influence_+3A_n.trees">n.trees</code></td>
<td>
<p> the number of trees to use for computations.</p>
</td></tr>
<tr><td><code id="relative.influence_+3A_y">y</code>, <code id="relative.influence_+3A_f">f</code>, <code id="relative.influence_+3A_w">w</code>, <code id="relative.influence_+3A_offset">offset</code>, <code id="relative.influence_+3A_dist">dist</code>, <code id="relative.influence_+3A_baseline">baseline</code></td>
<td>
<p>For <code>erboost.loss</code>: These components are the
outcome, predicted value, observation weight, offset, distribution, and comparison
loss function, respectively.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is not intended for end-user use. These functions offer the different
methods for computing the relative influence in <code><a href="#topic+summary.erboost">summary.erboost</a></code>.
<code>erboost.loss</code> is a helper function for <code>permutation.test.erboost</code>.
</p>


<h3>Value</h3>

<p>Returns an unprocessed vector of estimated relative influences.
</p>


<h3>Author(s)</h3>

<p>Yi Yang <a href="mailto:yiyang@umn.edu">yiyang@umn.edu</a> and Hui Zou <a href="mailto:hzou@stat.umn.edu">hzou@stat.umn.edu</a></p>


<h3>References</h3>

<p>Yang, Y. and Zou, H. (2015), &ldquo;Nonparametric Multiple Expectile Regression via ER-Boost,&rdquo; <em>Journal of Statistical Computation and Simulation</em>, 84(1), 84-95.
</p>
<p>G. Ridgeway (1999). &ldquo;The state of boosting,&rdquo; <em>Computing Science and
Statistics</em> 31:172-181.
</p>
<p><a href="https://cran.r-project.org/package=gbm">https://cran.r-project.org/package=gbm</a><br />
</p>
<p>J.H. Friedman (2001). &quot;Greedy Function Approximation: A Gradient Boosting
Machine,&quot; Annals of Statistics 29(5):1189-1232.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+summary.erboost">summary.erboost</a></code> </p>

<hr>
<h2 id='summary.erboost'>Summary of a erboost object </h2><span id='topic+summary.erboost'></span>

<h3>Description</h3>

<p>Computes the relative influence of each variable in the erboost object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'erboost'
summary(object,
        cBars=length(object$var.names),
        n.trees=object$n.trees,
        plotit=TRUE,
        order=TRUE,
        method=relative.influence,
        normalize=TRUE,
        ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.erboost_+3A_object">object</code></td>
<td>
<p>a <code>erboost</code> object created from an initial call to
<code><a href="#topic+erboost">erboost</a></code>.</p>
</td></tr>
<tr><td><code id="summary.erboost_+3A_cbars">cBars</code></td>
<td>
<p> the number of bars to plot. If <code>order=TRUE</code> the only the
variables with the <code>cBars</code> largest relative influence will appear in the
barplot. If <code>order=FALSE</code> then the first <code>cBars</code> variables will
appear in the plot. In either case, the function will return the relative
influence of all of the variables.</p>
</td></tr>
<tr><td><code id="summary.erboost_+3A_n.trees">n.trees</code></td>
<td>
<p> the number of trees used to generate the plot. Only the first
<code>n.trees</code> trees will be used.</p>
</td></tr>
<tr><td><code id="summary.erboost_+3A_plotit">plotit</code></td>
<td>
<p> an indicator as to whether the plot is generated. </p>
</td></tr>
<tr><td><code id="summary.erboost_+3A_order">order</code></td>
<td>
<p> an indicator as to whether the plotted and/or returned relative
influences are sorted. </p>
</td></tr>
<tr><td><code id="summary.erboost_+3A_method">method</code></td>
<td>
<p> The function used to compute the relative influence.
<code><a href="#topic+relative.influence">relative.influence</a></code> is the default and is the same as that
described in Friedman (2001). The other current (and experimental) choice is
<code><a href="#topic+permutation.test.erboost">permutation.test.erboost</a></code>. This method randomly permutes each predictor
variable at a time and computes the associated reduction in predictive
performance. This is similar to the variable importance measures Breiman uses
for random forests, but <code>erboost</code> currently computes using the entire training
dataset (not the out-of-bag observations.</p>
</td></tr>
<tr><td><code id="summary.erboost_+3A_normalize">normalize</code></td>
<td>
<p> if <code>FALSE</code> then <code>summary.erboost</code> returns the 
unnormalized influence. </p>
</td></tr>
<tr><td><code id="summary.erboost_+3A_...">...</code></td>
<td>
<p> other arguments passed to the plot function. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This returns the reduction attributeable to each varaible in sum of squared error in 
predicting the gradient on each iteration. It describes the relative influence 
of each variable in reducing the loss function. See the references below for 
exact details on the computation.
</p>


<h3>Value</h3>

<p>Returns a data frame where the first component is the variable name and the
second is the computed relative influence, normalized to sum to 100.
</p>


<h3>Author(s)</h3>

<p>Yi Yang <a href="mailto:yiyang@umn.edu">yiyang@umn.edu</a> and Hui Zou <a href="mailto:hzou@stat.umn.edu">hzou@stat.umn.edu</a></p>


<h3>References</h3>

<p>Yang, Y. and Zou, H. (2015), &ldquo;Nonparametric Multiple Expectile Regression via ER-Boost,&rdquo; <em>Journal of Statistical Computation and Simulation</em>, 84(1), 84-95.
</p>
<p>G. Ridgeway (1999). &ldquo;The state of boosting,&rdquo; <em>Computing Science and
Statistics</em> 31:172-181.
</p>
<p><a href="https://cran.r-project.org/package=gbm">https://cran.r-project.org/package=gbm</a><br />
</p>
<p>J.H. Friedman (2001). &quot;Greedy Function Approximation: A Gradient Boosting
Machine,&quot; Annals of Statistics 29(5):1189-1232.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+erboost">erboost</a></code> </p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
