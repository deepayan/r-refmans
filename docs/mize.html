<!DOCTYPE html><html lang="en"><head><title>Help for package mize</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mize}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#check_mize_convergence'><p>Check Optimization Convergence</p></a></li>
<li><a href='#make_mize'><p>Create an Optimizer</p></a></li>
<li><a href='#mize'><p>Numerical Optimization</p></a></li>
<li><a href='#mize_init'><p>Initialize the Optimizer.</p></a></li>
<li><a href='#mize_step'><p>One Step of Optimization</p></a></li>
<li><a href='#mize_step_summary'><p>Mize Step Summary</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Unconstrained Numerical Optimization Algorithms</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.4</td>
</tr>
<tr>
<td>Description:</td>
<td>Optimization algorithms implemented in R, including
    conjugate gradient (CG), Broyden-Fletcher-Goldfarb-Shanno (BFGS) and the
    limited memory BFGS (L-BFGS) methods. Most internal parameters can be set 
    through the call interface. The solvers hold up quite well for 
    higher-dimensional problems.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/BSD-2-Clause">BSD 2-clause License</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown, covr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/jlmelville/mize">https://github.com/jlmelville/mize</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/jlmelville/mize/issues">https://github.com/jlmelville/mize/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-08-29 16:17:36 UTC; jlmel</td>
</tr>
<tr>
<td>Author:</td>
<td>James Melville [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>James Melville &lt;jlmelville@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-08-30 05:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='check_mize_convergence'>Check Optimization Convergence</h2><span id='topic+check_mize_convergence'></span>

<h3>Description</h3>

<p>Updates the optimizer with information about convergence or termination,
signaling if the optimization process should stop.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_mize_convergence(mize_step_info)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="check_mize_convergence_+3A_mize_step_info">mize_step_info</code></td>
<td>
<p>Step info for this iteration, created by
<code><a href="#topic+mize_step_summary">mize_step_summary</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>On returning from this function, the updated value of <code>opt</code> will
contain: </p>

<ul>
<li><p> A boolean value <code>is_terminated</code> which is <code>TRUE</code> if
termination has been indicated, and <code>FALSE</code> otherwise.
</p>
</li>
<li><p> A list <code>terminate</code> if <code>is_terminated</code> is <code>TRUE</code>. This
contains two items: <code>what</code>, a short string describing what caused the
termination, and <code>val</code>, the value of the termination criterion that
caused termination. This list will not be present if <code>is_terminated</code> is
<code>FALSE</code>.</p>
</li></ul>

<p>Convergence criteria are only checked here. To set these criteria, use
<code><a href="#topic+make_mize">make_mize</a></code> or <code><a href="#topic+mize_init">mize_init</a></code>.
</p>


<h3>Value</h3>

<p><code>opt</code> updated with convergence and termination data. See
'Details'.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rb_fg &lt;- list(
  fn = function(x) {
    100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2
  },
  gr = function(x) {
    c(
      -400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),
      200 * (x[2] - x[1] * x[1])
    )
  }
)
rb0 &lt;- c(-1.2, 1)

opt &lt;- make_mize(method = "BFGS", par = rb0, fg = rb_fg, max_iter = 30)
mize_res &lt;- mize_step(opt = opt, par = rb0, fg = rb_fg)
step_info &lt;- mize_step_summary(mize_res$opt, mize_res$par, rb_fg, rb0)
# check convergence by looking at opt$is_terminated
opt &lt;- check_mize_convergence(step_info)
</code></pre>

<hr>
<h2 id='make_mize'>Create an Optimizer</h2><span id='topic+make_mize'></span>

<h3>Description</h3>

<p>Factory function for creating a (possibly uninitialized) optimizer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_mize(
  method = "L-BFGS",
  norm_direction = FALSE,
  scale_hess = TRUE,
  memory = 5,
  cg_update = "PR+",
  preconditioner = "",
  tn_init = 0,
  tn_exit = "curvature",
  nest_q = 0,
  nest_convex_approx = FALSE,
  nest_burn_in = 0,
  step_up = 1.1,
  step_up_fun = c("*", "+"),
  step_down = NULL,
  dbd_weight = 0.1,
  line_search = "More-Thuente",
  c1 = 1e-04,
  c2 = NULL,
  step0 = NULL,
  step_next_init = NULL,
  try_newton_step = NULL,
  ls_max_fn = 20,
  ls_max_gr = Inf,
  ls_max_fg = Inf,
  ls_max_alpha_mult = Inf,
  ls_max_alpha = Inf,
  ls_safe_cubic = FALSE,
  strong_curvature = NULL,
  approx_armijo = NULL,
  mom_type = NULL,
  mom_schedule = NULL,
  mom_init = NULL,
  mom_final = NULL,
  mom_switch_iter = NULL,
  mom_linear_weight = FALSE,
  use_init_mom = FALSE,
  restart = NULL,
  restart_wait = 10,
  par = NULL,
  fg = NULL,
  max_iter = 100,
  max_fn = Inf,
  max_gr = Inf,
  max_fg = Inf,
  abs_tol = NULL,
  rel_tol = abs_tol,
  grad_tol = NULL,
  ginf_tol = NULL,
  step_tol = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="make_mize_+3A_method">method</code></td>
<td>
<p>Optimization method. See 'Details' of <code><a href="#topic+mize">mize</a></code>.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_norm_direction">norm_direction</code></td>
<td>
<p>If <code>TRUE</code>, then the steepest descent direction is
normalized to unit length. Useful for adaptive step size methods where the
previous step size is used to initialize the next iteration.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_scale_hess">scale_hess</code></td>
<td>
<p>if <code>TRUE</code>, the approximation to the inverse Hessian is
scaled according to the method described by Nocedal and Wright
(approximating an eigenvalue). Applies only to the methods <code>BFGS</code>
(where the scaling is applied only during the first step) and <code>L-BFGS</code>
(where the scaling is applied during every iteration). Ignored otherwise.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_memory">memory</code></td>
<td>
<p>The number of updates to store if using the <code>L-BFGS</code>
method. Ignored otherwise. Must be a positive integer.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_cg_update">cg_update</code></td>
<td>
<p>Type of update to use for the <code>"CG"</code> method. For
details see the &quot;CG&quot; subsection of the &quot;Optimization Methods&quot; section.
Ignored if <code>method</code> is not <code>"CG"</code>.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_preconditioner">preconditioner</code></td>
<td>
<p>Type of preconditioner to use in Truncated Newton.
Leave blank or set to  <code>"L-BFGS"</code> to use a limited memory BFGS
preconditioner. Use the <code>"memory"</code> parameter to control the number of
updates to store. Applies only if <code>method = "TN"</code>, or <code>"CG"</code>,
ignored otherwise.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_tn_init">tn_init</code></td>
<td>
<p>Type of initialization to use in inner loop of Truncated
Newton. Use <code>0</code> to use the zero vector (the usual TN initialization),
or <code>"previous"</code> to use the final result from the previous iteration,
as suggested by Martens (2010). Applies only if <code>method = "TN"</code>,
ignored otherwise.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_tn_exit">tn_exit</code></td>
<td>
<p>Type of exit criterion to use when terminating the inner CG
loop of Truncated Newton method. Either <code>"curvature"</code> to use the
standard negative curvature test, or <code>"strong"</code> to use the modified
&quot;strong&quot; curvature test in TNPACK (Xie and Schlick, 1999). Applies only
if <code>method = "TN"</code>, ignored otherwise.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_nest_q">nest_q</code></td>
<td>
<p>Strong convexity parameter for the <code>"NAG"</code> method's
momentum term. Must take a value between 0 (strongly convex) and 1 (results
in steepest descent).Ignored unless the <code>method</code> is <code>"NAG"</code> and
<code>nest_convex_approx</code> is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_nest_convex_approx">nest_convex_approx</code></td>
<td>
<p>If <code>TRUE</code>, then use an approximation due to
Sutskever for calculating the momentum parameter in the NAG method. Only
applies if <code>method</code> is <code>"NAG"</code>.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_nest_burn_in">nest_burn_in</code></td>
<td>
<p>Number of iterations to wait before using a non-zero
momentum. Only applies if using the <code>"NAG"</code> method or setting the
<code>momentum_type</code> to &quot;Nesterov&quot;.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_step_up">step_up</code></td>
<td>
<p>Value by which to increase the step size for the <code>"bold"</code>
step size method or the <code>"DBD"</code> method.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_step_up_fun">step_up_fun</code></td>
<td>
<p>Operator to use when combining the current step size with
<code>step_up</code>. Can be one of <code>"*"</code> (to multiply the current step size
with <code>step_up</code>) or <code>"+"</code> (to add).</p>
</td></tr>
<tr><td><code id="make_mize_+3A_step_down">step_down</code></td>
<td>
<p>Multiplier to reduce the step size by if using the
<code>"DBD"</code> method or the <code>"bold"</code>. Can also be used with the
<code>"back"</code> line search method, but is optional. Should be a positive
value less than 1.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_dbd_weight">dbd_weight</code></td>
<td>
<p>Weighting parameter used by the <code>"DBD"</code> method only,
and only if no momentum scheme is provided. Must be an integer between 0
and 1.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_line_search">line_search</code></td>
<td>
<p>Type of line search to use. See 'Details' of
<code><a href="#topic+mize">mize</a></code>.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_c1">c1</code></td>
<td>
<p>Sufficient decrease parameter for Wolfe-type line searches. Should
be a value between 0 and 1.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_c2">c2</code></td>
<td>
<p>Sufficient curvature parameter for line search for Wolfe-type line
searches. Should be a value between <code>c1</code> and 1.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_step0">step0</code></td>
<td>
<p>Initial value for the line search on the first step. See
'Details' of <code><a href="#topic+mize">mize</a></code>.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_step_next_init">step_next_init</code></td>
<td>
<p>For Wolfe-type line searches only, how to initialize
the line search on iterations after the first. See 'Details' of
<code><a href="#topic+mize">mize</a></code>.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_try_newton_step">try_newton_step</code></td>
<td>
<p>For Wolfe-type line searches only, try the line step
value of 1 as the initial step size whenever <code>step_next_init</code> suggests
a step size &gt; 1. Defaults to <code>TRUE</code> for quasi-Newton methods such as
BFGS and L-BFGS, <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_ls_max_fn">ls_max_fn</code></td>
<td>
<p>Maximum number of function evaluations allowed during a line
search.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_ls_max_gr">ls_max_gr</code></td>
<td>
<p>Maximum number of gradient evaluations allowed during a line
search.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_ls_max_fg">ls_max_fg</code></td>
<td>
<p>Maximum number of function or gradient evaluations allowed
during a line search.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_ls_max_alpha_mult">ls_max_alpha_mult</code></td>
<td>
<p>The maximum value that can be attained by the ratio
of the initial guess for alpha for the current line search, to the final
value of alpha of the previous line search. Used to stop line searches
diverging due to very large initial guesses. Only applies for Wolfe-type
line searches.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_ls_max_alpha">ls_max_alpha</code></td>
<td>
<p>Maximum value of alpha allowed during line search. Only
applies for <code>line_search = "more-thuente"</code>.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_ls_safe_cubic">ls_safe_cubic</code></td>
<td>
<p>(Optional). If <code>TRUE</code>, check that cubic
interpolation in the Wolfe line search does not produce too small a value.
Only applies for <code>line_search = "more-thuente"</code>.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_strong_curvature">strong_curvature</code></td>
<td>
<p>(Optional). If <code>TRUE</code> use the strong
curvature condition in Wolfe line search. See the 'Line Search' section of
<code><a href="#topic+mize">mize</a></code> for details.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_approx_armijo">approx_armijo</code></td>
<td>
<p>(Optional). If <code>TRUE</code> use the approximate Armijo
condition in Wolfe line search. See the 'Line Search' section of
<code><a href="#topic+mize">mize</a></code> for details.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_mom_type">mom_type</code></td>
<td>
<p>Momentum type, either <code>"classical"</code> or
<code>"nesterov"</code>.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_mom_schedule">mom_schedule</code></td>
<td>
<p>Momentum schedule. See 'Details' of <code><a href="#topic+mize">mize</a></code>.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_mom_init">mom_init</code></td>
<td>
<p>Initial momentum value.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_mom_final">mom_final</code></td>
<td>
<p>Final momentum value.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_mom_switch_iter">mom_switch_iter</code></td>
<td>
<p>For <code>mom_schedule</code> <code>"switch"</code> only, the
iteration when <code>mom_init</code> is changed to <code>mom_final</code>.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_mom_linear_weight">mom_linear_weight</code></td>
<td>
<p>If <code>TRUE</code>, the gradient contribution to the
update is weighted using momentum contribution.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_use_init_mom">use_init_mom</code></td>
<td>
<p>If <code>TRUE</code>, then the momentum coefficient on the
first iteration is non-zero. Otherwise, it's zero. Only applies if using a
momentum schedule.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_restart">restart</code></td>
<td>
<p>Momentum restart type. Can be one of &quot;fn&quot; or &quot;gr&quot;. See
'Details' of <code><a href="#topic+mize">mize</a></code>.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_restart_wait">restart_wait</code></td>
<td>
<p>Number of iterations to wait between restarts. Ignored if
<code>restart</code> is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_par">par</code></td>
<td>
<p>(Optional) Initial values for the function to be optimized over.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_fg">fg</code></td>
<td>
<p>(Optional). Function and gradient list. See 'Details' of
<code><a href="#topic+mize">mize</a></code>.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_max_iter">max_iter</code></td>
<td>
<p>(Optional). Maximum number of iterations. See the
'Convergence' section of <code><a href="#topic+mize">mize</a></code> for details.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_max_fn">max_fn</code></td>
<td>
<p>(Optional). Maximum number of function evaluations. See the
'Convergence' section of <code><a href="#topic+mize">mize</a></code> for details.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_max_gr">max_gr</code></td>
<td>
<p>(Optional). Maximum number of gradient evaluations. See the
'Convergence' section of <code><a href="#topic+mize">mize</a></code> for details.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_max_fg">max_fg</code></td>
<td>
<p>(Optional). Maximum number of function or gradient evaluations.
See the 'Convergence' section of <code><a href="#topic+mize">mize</a></code> for details.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_abs_tol">abs_tol</code></td>
<td>
<p>(Optional). Absolute tolerance for comparing two function
evaluations. See the 'Convergence' section of <code><a href="#topic+mize">mize</a></code> for
details.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_rel_tol">rel_tol</code></td>
<td>
<p>(Optional). Relative tolerance for comparing two function
evaluations. See the 'Convergence' section of <code><a href="#topic+mize">mize</a></code> for
details.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_grad_tol">grad_tol</code></td>
<td>
<p>(Optional). Absolute tolerance for the length (l2-norm) of
the gradient vector. See the 'Convergence' section of <code><a href="#topic+mize">mize</a></code>
for details.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_ginf_tol">ginf_tol</code></td>
<td>
<p>(Optional). Absolute tolerance for the infinity norm (maximum
absolute component) of the gradient vector. See the 'Convergence' section
of <code><a href="#topic+mize">mize</a></code> for details.</p>
</td></tr>
<tr><td><code id="make_mize_+3A_step_tol">step_tol</code></td>
<td>
<p>(Optional). Absolute tolerance for the size of the parameter
update. See the 'Convergence' section of <code><a href="#topic+mize">mize</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the function to be optimized and starting point are not present at
creation time, then the optimizer should be initialized using
<code><a href="#topic+mize_init">mize_init</a></code> before being used with <code><a href="#topic+mize_step">mize_step</a></code>.
</p>
<p>See the documentation to <code><a href="#topic+mize">mize</a></code> for an explanation of all the
parameters.
</p>
<p>Details of the <code>fg</code> list containing the function to be optimized and its
gradient can be found in the 'Details' section of <code><a href="#topic+mize">mize</a></code>. It is
optional for this function, but if it is passed to this function, along with
the vector of initial values, <code>par</code>, the optimizer will be returned
already initialized for this function. Otherwise, <code><a href="#topic+mize_init">mize_init</a></code>
must be called before optimization begins.
</p>
<p>Additionally, optional convergence parameters may also be passed here, for
use with <code><a href="#topic+check_mize_convergence">check_mize_convergence</a></code>. They are optional here if you
plan to call <code><a href="#topic+mize_init">mize_init</a></code> later, or if you want to do your own
convergence checking.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Function to optimize and starting point
rosenbrock_fg &lt;- list(
  fn = function(x) {
    100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2
  },
  gr = function(x) {
    c(
      -400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),
      200 * (x[2] - x[1] * x[1])
    )
  }
)
rb0 &lt;- c(-1.2, 1)

# Create an optimizer and initialize it for use with the Rosenbrock function
opt &lt;- make_mize(method = "L-BFGS", par = rb0, fg = rosenbrock_fg)

# Create optimizer without initialization
opt &lt;- make_mize(method = "L-BFGS")

# Need to call mize_init separately:
opt &lt;- mize_init(opt, rb0, rosenbrock_fg)
</code></pre>

<hr>
<h2 id='mize'>Numerical Optimization</h2><span id='topic+mize'></span>

<h3>Description</h3>

<p>Numerical optimization including conjugate gradient,
Broyden-Fletcher-Goldfarb-Shanno (BFGS), and the limited memory BFGS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mize(
  par,
  fg,
  method = "L-BFGS",
  norm_direction = FALSE,
  memory = 5,
  scale_hess = TRUE,
  cg_update = "PR+",
  preconditioner = "",
  tn_init = 0,
  tn_exit = "curvature",
  nest_q = 0,
  nest_convex_approx = FALSE,
  nest_burn_in = 0,
  step_up = 1.1,
  step_up_fun = "*",
  step_down = NULL,
  dbd_weight = 0.1,
  line_search = "More-Thuente",
  c1 = 1e-04,
  c2 = NULL,
  step0 = NULL,
  step_next_init = NULL,
  try_newton_step = NULL,
  ls_max_fn = 20,
  ls_max_gr = Inf,
  ls_max_fg = Inf,
  ls_max_alpha_mult = Inf,
  ls_max_alpha = Inf,
  ls_safe_cubic = FALSE,
  strong_curvature = NULL,
  approx_armijo = NULL,
  mom_type = NULL,
  mom_schedule = NULL,
  mom_init = NULL,
  mom_final = NULL,
  mom_switch_iter = NULL,
  mom_linear_weight = FALSE,
  use_init_mom = FALSE,
  restart = NULL,
  restart_wait = 10,
  max_iter = 100,
  max_fn = Inf,
  max_gr = Inf,
  max_fg = Inf,
  abs_tol = sqrt(.Machine$double.eps),
  rel_tol = abs_tol,
  grad_tol = NULL,
  ginf_tol = NULL,
  step_tol = sqrt(.Machine$double.eps),
  check_conv_every = 1,
  log_every = check_conv_every,
  verbose = FALSE,
  store_progress = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mize_+3A_par">par</code></td>
<td>
<p>Initial values for the function to be optimized over.</p>
</td></tr>
<tr><td><code id="mize_+3A_fg">fg</code></td>
<td>
<p>Function and gradient list. See 'Details'.</p>
</td></tr>
<tr><td><code id="mize_+3A_method">method</code></td>
<td>
<p>Optimization method. See 'Details'.</p>
</td></tr>
<tr><td><code id="mize_+3A_norm_direction">norm_direction</code></td>
<td>
<p>If <code>TRUE</code>, then the steepest descent direction
is normalized to unit length. Useful for adaptive step size methods where
the previous step size is used to initialize the next iteration.</p>
</td></tr>
<tr><td><code id="mize_+3A_memory">memory</code></td>
<td>
<p>The number of updates to store if using the <code>L-BFGS</code>
method. Ignored otherwise. Must be a positive integer.</p>
</td></tr>
<tr><td><code id="mize_+3A_scale_hess">scale_hess</code></td>
<td>
<p>if <code>TRUE</code>, the approximation to the inverse Hessian
is scaled according to the method described by Nocedal and Wright
(approximating an eigenvalue). Applies only to the methods <code>BFGS</code>
(where the scaling is applied only during the first step) and <code>L-BFGS</code>
(where the scaling is applied during every iteration). Ignored otherwise.</p>
</td></tr>
<tr><td><code id="mize_+3A_cg_update">cg_update</code></td>
<td>
<p>Type of update to use for the <code>"CG"</code> method. For
details see the &quot;CG&quot; subsection of the &quot;Optimization Methods&quot; section.
Ignored if <code>method</code> is not <code>"CG"</code>.</p>
</td></tr>
<tr><td><code id="mize_+3A_preconditioner">preconditioner</code></td>
<td>
<p>Type of preconditioner to use in Truncated Newton.
Leave blank or set to <code>"L-BFGS"</code> to use a limited memory BFGS
preconditioner. Use the <code>"memory"</code> parameter to control the number of
updates to store. Applies only if <code>method = "TN"</code> or <code>"CG"</code>,
ignored otherwise.</p>
</td></tr>
<tr><td><code id="mize_+3A_tn_init">tn_init</code></td>
<td>
<p>Type of initialization to use in inner loop of Truncated
Newton. Use <code>0</code> to use the zero vector (the usual TN initialization),
or <code>"previous"</code> to use the final result from the previous iteration,
as suggested by Martens (2010). Applies only if <code>method = "TN"</code>,
ignored otherwise.</p>
</td></tr>
<tr><td><code id="mize_+3A_tn_exit">tn_exit</code></td>
<td>
<p>Type of exit criterion to use when terminating the inner CG
loop of Truncated Newton method. Either <code>"curvature"</code> to use the
standard negative curvature test, or <code>"strong"</code> to use the modified
&quot;strong&quot; curvature test in TNPACK (Xie and Schlick, 1999). Applies only
if <code>method = "TN"</code>, ignored otherwise.</p>
</td></tr>
<tr><td><code id="mize_+3A_nest_q">nest_q</code></td>
<td>
<p>Strong convexity parameter for the NAG
momentum term. Must take a value between 0 (strongly convex) and 1
(zero momentum). Only applies using the NAG method or a momentum method with
Nesterov momentum schedule. Also does nothing if <code>nest_convex_approx</code>
is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="mize_+3A_nest_convex_approx">nest_convex_approx</code></td>
<td>
<p>If <code>TRUE</code>, then use an approximation due to
Sutskever for calculating the momentum parameter in the NAG method. Only
applies using the NAG method or a momentum method with Nesterov momentum
schedule.</p>
</td></tr>
<tr><td><code id="mize_+3A_nest_burn_in">nest_burn_in</code></td>
<td>
<p>Number of iterations to wait before using a non-zero
momentum. Only applies using the NAG method or a momentum method with
Nesterov momentum schedule.</p>
</td></tr>
<tr><td><code id="mize_+3A_step_up">step_up</code></td>
<td>
<p>Value by which to increase the step size for the <code>"bold"</code>
step size method or the <code>"DBD"</code> method.</p>
</td></tr>
<tr><td><code id="mize_+3A_step_up_fun">step_up_fun</code></td>
<td>
<p>Operator to use when combining the current step size with
<code>step_up</code>. Can be one of <code>"*"</code> (to multiply the current step size
with <code>step_up</code>) or <code>"+"</code> (to add).</p>
</td></tr>
<tr><td><code id="mize_+3A_step_down">step_down</code></td>
<td>
<p>Multiplier to reduce the step size by if using the <code>"DBD"</code>
method or the <code>"bold"</code> line search method. Should be a positive value
less than 1. Also optional for use with the <code>"back"</code> line search method.</p>
</td></tr>
<tr><td><code id="mize_+3A_dbd_weight">dbd_weight</code></td>
<td>
<p>Weighting parameter used by the <code>"DBD"</code> method only, and
only if no momentum scheme is provided. Must be an integer between 0 and 1.</p>
</td></tr>
<tr><td><code id="mize_+3A_line_search">line_search</code></td>
<td>
<p>Type of line search to use. See 'Details'.</p>
</td></tr>
<tr><td><code id="mize_+3A_c1">c1</code></td>
<td>
<p>Sufficient decrease parameter for Wolfe-type line searches. Should
be a value between 0 and 1.</p>
</td></tr>
<tr><td><code id="mize_+3A_c2">c2</code></td>
<td>
<p>Sufficient curvature parameter for line search for Wolfe-type line
searches. Should be a value between <code>c1</code> and 1.</p>
</td></tr>
<tr><td><code id="mize_+3A_step0">step0</code></td>
<td>
<p>Initial value for the line search on the first step. See
'Details'.</p>
</td></tr>
<tr><td><code id="mize_+3A_step_next_init">step_next_init</code></td>
<td>
<p>For Wolfe-type line searches only, how to initialize
the line search on iterations after the first. See 'Details'.</p>
</td></tr>
<tr><td><code id="mize_+3A_try_newton_step">try_newton_step</code></td>
<td>
<p>For Wolfe-type line searches only, try the
line step value of 1 as the initial step size whenever <code>step_next_init</code>
suggests a step size &gt; 1. Defaults to <code>TRUE</code> for quasi-Newton methods
such as BFGS and L-BFGS, <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="mize_+3A_ls_max_fn">ls_max_fn</code></td>
<td>
<p>Maximum number of function evaluations allowed during a
line search.</p>
</td></tr>
<tr><td><code id="mize_+3A_ls_max_gr">ls_max_gr</code></td>
<td>
<p>Maximum number of gradient evaluations allowed during a
line search.</p>
</td></tr>
<tr><td><code id="mize_+3A_ls_max_fg">ls_max_fg</code></td>
<td>
<p>Maximum number of function or gradient evaluations allowed
during a line search.</p>
</td></tr>
<tr><td><code id="mize_+3A_ls_max_alpha_mult">ls_max_alpha_mult</code></td>
<td>
<p>The maximum value that can be attained by the ratio
of the initial guess for alpha for the current line search, to the final
value of alpha of the previous line search. Used to stop line searches
diverging due to very large initial guesses. Only applies for Wolfe-type
line searches.</p>
</td></tr>
<tr><td><code id="mize_+3A_ls_max_alpha">ls_max_alpha</code></td>
<td>
<p>Maximum value of alpha allowed during line search. Only
applies for <code>line_search = "more-thuente"</code>.</p>
</td></tr>
<tr><td><code id="mize_+3A_ls_safe_cubic">ls_safe_cubic</code></td>
<td>
<p>(Optional). If <code>TRUE</code>, check that cubic
interpolation in the Wolfe line search does not produce too small a value,
using method of Xie and Schlick (2002). Only applies for
<code>line_search = "more-thuente"</code>.</p>
</td></tr>
<tr><td><code id="mize_+3A_strong_curvature">strong_curvature</code></td>
<td>
<p>(Optional). If <code>TRUE</code> use the strong
curvature condition in Wolfe line search. See the 'Line Search' section
for details.</p>
</td></tr>
<tr><td><code id="mize_+3A_approx_armijo">approx_armijo</code></td>
<td>
<p>(Optional). If <code>TRUE</code> use the approximate Armijo
condition in Wolfe line search. See the 'Line Search' section for details.</p>
</td></tr>
<tr><td><code id="mize_+3A_mom_type">mom_type</code></td>
<td>
<p>Momentum type, either <code>"classical"</code> or
<code>"nesterov"</code>. See 'Details'.</p>
</td></tr>
<tr><td><code id="mize_+3A_mom_schedule">mom_schedule</code></td>
<td>
<p>Momentum schedule. See 'Details'.</p>
</td></tr>
<tr><td><code id="mize_+3A_mom_init">mom_init</code></td>
<td>
<p>Initial momentum value.</p>
</td></tr>
<tr><td><code id="mize_+3A_mom_final">mom_final</code></td>
<td>
<p>Final momentum value.</p>
</td></tr>
<tr><td><code id="mize_+3A_mom_switch_iter">mom_switch_iter</code></td>
<td>
<p>For <code>mom_schedule</code> <code>"switch"</code> only, the
iteration when <code>mom_init</code> is changed to <code>mom_final</code>.</p>
</td></tr>
<tr><td><code id="mize_+3A_mom_linear_weight">mom_linear_weight</code></td>
<td>
<p>If <code>TRUE</code>, the gradient contribution to the
update is weighted using momentum contribution.</p>
</td></tr>
<tr><td><code id="mize_+3A_use_init_mom">use_init_mom</code></td>
<td>
<p>If <code>TRUE</code>, then the momentum coefficient on
the first iteration is non-zero. Otherwise, it's zero. Only applies if
using a momentum schedule.</p>
</td></tr>
<tr><td><code id="mize_+3A_restart">restart</code></td>
<td>
<p>Momentum restart type. Can be one of &quot;fn&quot;, &quot;gr&quot; or &quot;speed&quot;.
See 'Details'. Ignored if no momentum scheme is being used.</p>
</td></tr>
<tr><td><code id="mize_+3A_restart_wait">restart_wait</code></td>
<td>
<p>Number of iterations to wait between restarts. Ignored
if <code>restart</code> is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="mize_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of iterations to optimize for. Defaults to
100. See the 'Convergence' section for details.</p>
</td></tr>
<tr><td><code id="mize_+3A_max_fn">max_fn</code></td>
<td>
<p>Maximum number of function evaluations. See the 'Convergence'
section for details.</p>
</td></tr>
<tr><td><code id="mize_+3A_max_gr">max_gr</code></td>
<td>
<p>Maximum number of gradient evaluations. See the 'Convergence'
section for details.</p>
</td></tr>
<tr><td><code id="mize_+3A_max_fg">max_fg</code></td>
<td>
<p>Maximum number of function or gradient evaluations. See the
'Convergence' section for details.</p>
</td></tr>
<tr><td><code id="mize_+3A_abs_tol">abs_tol</code></td>
<td>
<p>Absolute tolerance for comparing two function evaluations.
See the 'Convergence' section for details.</p>
</td></tr>
<tr><td><code id="mize_+3A_rel_tol">rel_tol</code></td>
<td>
<p>Relative tolerance for comparing two function evaluations.
See the 'Convergence' section for details.</p>
</td></tr>
<tr><td><code id="mize_+3A_grad_tol">grad_tol</code></td>
<td>
<p>Absolute tolerance for the length (l2-norm) of the gradient
vector. See the 'Convergence' section for details.</p>
</td></tr>
<tr><td><code id="mize_+3A_ginf_tol">ginf_tol</code></td>
<td>
<p>Absolute tolerance for the infinity norm (maximum absolute
component) of the gradient vector. See the 'Convergence' section for details.</p>
</td></tr>
<tr><td><code id="mize_+3A_step_tol">step_tol</code></td>
<td>
<p>Absolute tolerance for the size of the parameter update.
See the 'Convergence' section for details.</p>
</td></tr>
<tr><td><code id="mize_+3A_check_conv_every">check_conv_every</code></td>
<td>
<p>Positive integer indicating how often to check
convergence. Default is 1, i.e. every iteration. See the 'Convergence'
section for details.</p>
</td></tr>
<tr><td><code id="mize_+3A_log_every">log_every</code></td>
<td>
<p>Positive integer indicating how often to log convergence
results to the console. Ignored if <code>verbose</code> is <code>FALSE</code>.
If not an integer multiple of <code>check_conv_every</code>, it will be set to
<code>check_conv_every</code>.</p>
</td></tr>
<tr><td><code id="mize_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, log information about the progress of the
optimization to the console.</p>
</td></tr>
<tr><td><code id="mize_+3A_store_progress">store_progress</code></td>
<td>
<p>If <code>TRUE</code> store information about the progress
of the optimization in a data frame, and include it as part of the return
value.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function to be optimized should be passed as a list to the <code>fg</code>
parameter. This should consist of:
</p>

<ul>
<li><p><code>fn</code>. The function to be optimized. Takes a vector of parameters
and returns a scalar.
</p>
</li>
<li><p><code>gr</code>. The gradient of the function. Takes a vector of parameters
and returns a vector with the same length as the input parameter vector.
</p>
</li>
<li><p><code>fg</code>. (Optional) function which calculates the function and
gradient in the same routine. Takes a vector of parameters and returns a list
containing the function result as <code>fn</code> and the gradient result as
<code>gr</code>.
</p>
</li>
<li><p><code>hs</code>. (Optional) Hessian of the function. Takes a vector of
parameters and returns a square matrix with dimensions the same as the length
of the input vector, containing the second derivatives. Only required to be
present if using the <code>"NEWTON"</code> method. If present, it will be used
during initialization for the <code>"BFGS"</code> and <code>"SR1"</code> quasi-Newton
methods (otherwise, they will use the identity matrix). The quasi-Newton
methods are implemented using the inverse of the Hessian, and rather than
directly invert the provided Hessian matrix, will use the inverse of the
diagonal of the provided Hessian only.
</p>
</li></ul>

<p>The <code>fg</code> function is optional, but for some methods (e.g. line search
methods based on the Wolfe criteria), both the function and gradient values
are needed for the same parameter value. Calculating them in the same
function can save time if there is a lot of shared work.
</p>


<h3>Value</h3>

<p>A list with components:
</p>

<ul>
<li><p><code>par</code> Optimized parameters. Normally, this is the best set of
parameters seen during optimization, i.e. the set that produced the minimum
function value. This requires that convergence checking with is carried out,
including function evaluation where necessary. See the 'Convergence'
section for details.
</p>
</li>
<li><p><code>nf</code> Total number of function evaluations carried out. This
includes any extra evaluations required for convergence calculations. Also,
a function evaluation may be required to calculate the value of <code>f</code>
returned in this list (see below). Additionally, if the <code>verbose</code>
parameter is <code>TRUE</code>, then function and gradient information for the
initial value of <code>par</code> will be logged to the console. These values
are cached for subsequent use by the optimizer.
</p>
</li>
<li><p><code>ng</code> Total number of gradient evaluations carried out. This
includes any extra evaluations required for convergence calculations using
<code>grad_tol</code>. As with <code>nf</code>, additional gradient calculations beyond
what you're expecting may have been needed for logging, convergence and
calculating the value of <code>g2</code> or <code>ginf</code> (see below).
</p>
</li>
<li><p><code>f</code> Value of the function, evaluated at the returned
value of <code>par</code>.
</p>
</li>
<li><p><code>g2</code> Optional: the length (Euclidean or l2-norm) of the
gradient vector, evaluated at the returned value of <code>par</code>. Calculated
only if <code>grad_tol</code> is non-null.
</p>
</li>
<li><p><code>ginf</code> Optional: the infinity norm (maximum absolute component)
of the gradient vector, evaluated at the returned value of <code>par</code>.
Calculated only if <code>ginf_tol</code> is non-null.
</p>
</li>
<li><p><code>iter</code> The number of iterations the optimization was carried
out for.
</p>
</li>
<li><p><code>terminate</code> List containing items: <code>what</code>, indicating what
convergence criterion was met, and <code>val</code> specifying the value at
convergence. See the 'Convergence' section for more details.
</p>
</li>
<li><p><code>progress</code> Optional data frame containing information on the
value of the function, gradient, momentum, and step sizes evaluated at each
iteration where convergence is checked. Only present if
<code>store_progress</code> is set to <code>TRUE</code>. Could get quite large if the
optimization is long and the convergence is checked regularly.
</p>
</li></ul>



<h3>Optimization Methods</h3>

<p>The <code>method</code> specifies the optimization method:
</p>

<ul>
<li> <p><code>"SD"</code> is plain steepest descent. Not very effective on its own,
but can be combined with various momentum approaches.
</p>
</li>
<li> <p><code>"BFGS"</code> is the Broyden-Fletcher-Goldfarb-Shanno quasi-Newton
method. This stores an approximation to the inverse of the Hessian of the
function being minimized, which requires storage proportional to the
square of the length of <code>par</code>, so is unsuitable for large problems.
</p>
</li>
<li> <p><code>"SR1"</code> is the Symmetric Rank 1 quasi-Newton method, incorporating
the safeguard given by Nocedal and Wright. Even with the safeguard, the SR1
method is not guaranteed to produce a descent direction. If this happens, the
BFGS update is used for that iteration instead. Note that I have not done any
research into the theoretical justification for combining BFGS with SR1 like
this, but empirically (comparing to BFGS results with the datasets in the
funconstrain package <a href="https://github.com/jlmelville/funconstrain">https://github.com/jlmelville/funconstrain</a>), it
works competitively with BFGS, particularly with a loose line search.
</p>
</li>
<li> <p><code>"L-BFGS"</code> is the Limited memory Broyden-Fletcher-Goldfarb-Shanno
quasi-Newton method. This does not store the inverse Hessian approximation
directly and so can scale to larger-sized problems than <code>"BFGS"</code>. The
amount of memory used can be controlled with the <code>memory</code> parameter.
</p>
</li>
<li> <p><code>"CG"</code> is the conjugate gradient method. The <code>cg_update</code>
parameter allows for different methods for choosing the next direction:
</p>

<ul>
<li> <p><code>"FR"</code> The method of Fletcher and Reeves.
</p>
</li>
<li> <p><code>"PR"</code> The method of Polak and Ribiere.
</p>
</li>
<li> <p><code>"PR+"</code> The method of Polak and Ribiere with a restart to
steepest descent if conjugacy is lost. The default.
</p>
</li>
<li> <p><code>"HS"</code> The method of Hestenes and Stiefel.
</p>
</li>
<li> <p><code>"DY"</code> The method of Dai and Yuan.
</p>
</li>
<li> <p><code>"HZ"</code> The method of Hager and Zhang.
</p>
</li>
<li> <p><code>"HZ+"</code> The method of Hager and Zhang with restart, as used
in CG_DESCENT.
</p>
</li>
<li> <p><code>"PRFR"</code> The modified PR-FR method of Gilbert and Nocedal
(1992).
</p>
</li></ul>

<p>The <code>"PR+"</code> and <code>"HZ+"</code> are likely to be most robust in practice.
Other updates are available more for curiosity purposes.
</p>
</li>
<li> <p><code>"TN"</code> is the Truncated Newton method, which approximately solves
the Newton step without explicitly calculating the Hessian (at the expense
of extra gradient calculations).
</p>
</li>
<li> <p><code>"NAG"</code> is the Nesterov Accelerated Gradient method. The exact
form of the momentum update in this method can be controlled with the
following parameters:
</p>

<ul>
<li><p><code>nest_q</code> Strong convexity parameter. Must take a value
between 0 (strongly convex) and 1 (zero momentum). Ignored if
<code>nest_convex_approx</code> is <code>TRUE</code>.
</p>
</li>
<li><p><code>nest_convex_approx</code> If <code>TRUE</code>, then use an approximation
due to Sutskever for calculating the momentum parameter.
</p>
</li>
<li><p><code>nest_burn_in</code> Number of iterations to wait before using a
non-zero momentum.
</p>
</li></ul>

</li>
<li> <p><code>"DBD"</code> is the Delta-Bar-Delta method of Jacobs.
</p>
</li>
<li> <p><code>"Momentum"</code> is steepest descent with momentum. See below for
momentum options.
</p>
</li></ul>

<p>For more details on gradient-based optimization in general, and the BFGS,
L-BFGS and CG methods, see Nocedal and Wright.
</p>


<h3>Line Search</h3>

<p>The parameter <code>line_search</code> determines the line search to be carried
out:
</p>

<ul>
<li> <p><code>"Rasmussen"</code> carries out a line search using the strong Wolfe
conditions as implemented by Carl Edward Rasmussen's minimize.m routines.
</p>
</li>
<li> <p><code>"More-Thuente"</code> carries out a line search using the strong Wolfe
conditions and the method of More-Thuente. Can be abbreviated to
<code>"MT"</code>.
</p>
</li>
<li> <p><code>"Schmidt"</code> carries out a line search using the strong Wolfe
conditions as implemented in Mark Schmidt's minFunc routines.
</p>
</li>
<li> <p><code>"Backtracking"</code> carries out a back tracking line search using
the sufficient decrease (Armijo) condition. By default, cubic interpolation
using function and gradient values is used to find an acceptable step size.
A constant step size reduction can be used by specifying a value for 
<code>step_down</code> between 0 and 1 (e.g. step size will be halved if 
<code>step_down</code> is set to <code>0.5</code>). If a constant step size reduction
is used then only function evaluations are carried out and no extra 
gradient calculations are made.
</p>
</li>
<li> <p><code>"Bold Driver"</code> carries out a back tracking line search until a
reduction in the function value is found.
</p>
</li>
<li> <p><code>"Constant"</code> uses a constant line search, the value of which
should be provided with <code>step0</code>. Note that this value will be
multiplied by the magnitude of the direction vector used in the gradient
descent method. For method <code>"SD"</code> only, setting the
<code>norm_direction</code> parameter to <code>TRUE</code> will scale the direction
vector so it has unit length.
</p>
</li></ul>

<p>If using one of the methods: <code>"BFGS"</code>, <code>"L-BFGS"</code>, <code>"CG"</code> or
<code>"NAG"</code>, one of the Wolfe line searches: <code>"Rasmussen"</code> or
<code>"More-Thuente"</code>, <code>"Schmidt"</code> or <code>"Hager-Zhang"</code> should be
used, otherwise very poor performance is likely to be encountered. The
following parameters can be used to control the line search:
</p>

<ul>
<li><p><code>c1</code> The sufficient decrease condition. Normally left at its
default value of 1e-4.
</p>
</li>
<li><p><code>c2</code> The sufficient curvature condition. Defaults to 0.9 if
using the methods <code>"BFGS"</code> and <code>"L-BFGS"</code>, and to 0.1 for
every other method, more or less in line with the recommendations given
by Nocedal and Wright. The smaller the value of <code>c2</code>, the stricter
the line search, but it should not be set to smaller than <code>c1</code>.
</p>
</li>
<li><p><code>step0</code> Initial value for the line search on the first step.
If a positive numeric value is passed as an argument, that value is used
as-is. Otherwise, by passing a string as an argument, a guess is made
based on values of the gradient, function or parameters, at the starting
point:
</p>

<ul>
<li><p><code>"rasmussen"</code> As used by Rasmussen in <code>minimize.m</code>:
</p>
<p style="text-align: center;"><code class="reqn">\frac{1}{1+\left|g\right|^2}</code>
</p>

</li>
<li><p><code>"scipy"</code> As used in <code>optimize.py</code> in the python
library Scipy.
</p>
<p style="text-align: center;"><code class="reqn">\frac{1}{\left|g\right|}</code>
</p>

</li>
<li><p><code>"schmidt"</code> As used by Schmidt in <code>minFunc.m</code>
(the reciprocal of the l1 norm of g)
</p>
<p style="text-align: center;"><code class="reqn">\frac{1}{\left|g\right|_1}</code>
</p>

</li>
<li><p><code>"hz"</code> The method suggested by Hager and Zhang (2006) for
the CG_DESCENT software.
</p>
</li></ul>

<p>These arguments can be abbreviated.
</p>
</li>
<li><p><code>step_next_init</code> How to initialize alpha value of subsequent
line searches after the first, using results from the previous line search:
</p>

<ul>
<li><p><code>"slope ratio"</code> Slope ratio method.
</p>
</li>
<li><p><code>"quadratic"</code> Quadratic interpolation method.
</p>
</li>
<li><p><code>"hz"</code> The QuadStep method of Hager and Zhang (2006) for
the CG_DESCENT software.
</p>
</li>
<li><p>scalar numeric Set to a numeric value (e.g.
<code>step_next_init = 1</code>) to explicitly set alpha to this value
initially.
</p>
</li></ul>

<p>These arguments can be abbreviated. Details on the first two methods
are provided by Nocedal and Wright.
</p>
</li>
<li><p><code>try_newton_step</code> For quasi-Newton methods (e.g. <code>"TN"</code>,
<code>"BFGS"</code> and <code>"L-BFGS"</code>), setting this to <code>TRUE</code> will try
the &quot;natural&quot; step size of 1, whenever the <code>step_next_init</code> method
suggests an initial step size larger than that.
</p>
</li>
<li><p><code>strong_curvature</code> If <code>TRUE</code>, then the strong curvature
condition will be used to check termination in Wolfe line search methods.
If <code>FALSE</code>, then the standard curvature condition will be used. The
default is <code>NULL</code> which lets the different Wolfe line searches choose
whichever is their default behavior. This option is ignored if not using
a Wolfe line search method.
</p>
</li>
<li><p><code>approx_armijo</code> If <code>TRUE</code>, then the approximate Armijo
sufficient decrease condition (Hager and Zhang, 2005) will be used to
check termination in Wolfe line search methods. If <code>FALSE</code>, then the
exact curvature condition will be used. The default is <code>NULL</code> which
lets the different Wolfe line searches choose whichever is their default
behavior. This option is ignored if not using a Wolfe line search method.
</p>
</li></ul>

<p>For the Wolfe line searches, the methods of <code>"Rasmussen"</code>,
<code>"Schmidt"</code> and <code>"More-Thuente"</code> default to using the strong
curvature condition and the exact Armijo condition to terminate the line
search (i.e. Strong Wolfe conditions). The default step size initialization
methods use the Rasmussen method for the first iteration and quadratic
interpolation for subsequent iterations.
</p>
<p>The <code>"Hager-Zhang"</code> Wolfe line search method defaults to the standard
curvature condition and the approximate Armijo condition (i.e. approximate
Wolfe conditions). The default step size initialization methods are those
used by Hager and Zhang (2006) in the description of CG_DESCENT.
</p>
<p>If the <code>"DBD"</code> is used for the optimization <code>"method"</code>, then the
<code>line_search</code> parameter is ignored, because this method controls both
the direction of the search and the step size simultaneously. The following
parameters can be used to control the step size:
</p>

<ul>
<li><p><code>step_up</code> The amount by which to increase the step size in a
direction where the current step size is deemed to be too short. This
should be a positive scalar.
</p>
</li>
<li><p><code>step_down</code> The amount by which to decrease the step size in a
direction where the currents step size is deemed to be too long. This
should be a positive scalar smaller than 1. Default is 0.5.
</p>
</li>
<li><p><code>step_up_fun</code> How to increase the step size: either the method of
Jacobs (addition of <code>step_up</code>) or Janet and co-workers (multiplication
by <code>step_up</code>). Note that the step size decrease <code>step_down</code> is always
a multiplication.
</p>
</li></ul>

<p>The <code>"bold driver"</code> line search also uses the <code>step_up</code> and
<code>step_down</code> parameters with similar meanings to their use with the
<code>"DBD"</code> method: the backtracking portion reduces the step size by a
factor of <code>step_down</code>. Once a satisfactory step size has been found, the
line search for the next iteration is initialized by multiplying the
previously found step size by <code>step_up</code>.
</p>


<h3>Momentum</h3>

<p>For <code>method</code> <code>"Momentum"</code>, momentum schemes can be accessed
through the momentum arguments:
</p>

<ul>
<li><p><code>mom_type</code> Momentum type, either <code>"classical"</code> or
<code>"nesterov"</code> (case insensitive, can be abbreviated). Using
<code>"nesterov"</code> applies the momentum step before the
gradient descent as suggested by Sutskever, emulating the behavior of the
Nesterov Accelerated Gradient method.
</p>
</li>
<li><p><code>mom_schedule</code> How the momentum changes over the course of the
optimization:
</p>

<ul>
<li><p>If a numerical scalar is provided, a constant momentum will be
applied throughout.
</p>
</li>
<li><p><code>"nsconvex"</code> Use the momentum schedule from the Nesterov
Accelerated Gradient method suggested for non-strongly convex functions.
Parameters which control the NAG momentum
can also be used in combination with this option.
</p>
</li>
<li><p><code>"switch"</code> Switch from one momentum value (specified via
<code>mom_init</code>) to another (<code>mom_final</code>) at a
a specified iteration (<code>mom_switch_iter</code>).
</p>
</li>
<li><p><code>"ramp"</code> Linearly increase from one momentum value
(<code>mom_init</code>) to another (<code>mom_final</code>).
</p>
</li>
<li><p>If a function is provided, this will be invoked to provide a momentum
value. It must take one argument (the current iteration number) and return
a scalar.
</p>
</li></ul>

<p>String arguments are case insensitive and can be abbreviated.
</p>
</li></ul>

<p>The <code>restart</code> parameter provides a way to restart the momentum if the
optimization appears to be not be making progress, inspired by the method
of O'Donoghue and Candes (2013) and Su and co-workers (2014). There are three
strategies:
</p>

<ul>
<li><p><code>"fn"</code> A restart is applied if the function does not decrease
on consecutive iterations.
</p>
</li>
<li><p><code>"gr"</code> A restart is applied if the direction of the
optimization is not a descent direction.
</p>
</li>
<li><p><code>"speed"</code> A restart is applied if the update vector is not
longer (as measured by Euclidean 2-norm) in consecutive iterations.
</p>
</li></ul>

<p>The effect of the restart is to &quot;forget&quot; any previous momentum update vector,
and, for those momentum schemes that change with iteration number, to
effectively reset the iteration number back to zero. If the <code>mom_type</code>
is <code>"nesterov"</code>, the gradient-based restart is not available. The
<code>restart_wait</code> parameter controls how many iterations to wait after a
restart, before allowing another restart. Must be a positive integer. Default
is 10, as used by Su and co-workers (2014). Setting this too low could
cause premature convergence. These methods were developed specifically
for the NAG method, but can be employed with any momentum type and schedule.
</p>
<p>If <code>method</code> type <code>"momentum"</code> is specified with no other values,
the momentum scheme will default to a constant value of <code>0.9</code>.
</p>


<h3>Convergence</h3>

<p>There are several ways for the optimization to terminate. The type of
termination is communicated by a two-item list <code>terminate</code> in the return
value, consisting of <code>what</code>, a short string describing what caused the
termination, and <code>val</code>, the value of the termination criterion that
caused termination.
</p>
<p>The following parameters control various stopping criteria:
</p>

<ul>
<li><p><code>max_iter</code> Maximum number of iterations to calculate. Reaching
this limit is indicated by <code>terminate$what</code> being <code>"max_iter"</code>.
</p>
</li>
<li><p><code>max_fn</code> Maximum number of function evaluations allowed.
Indicated by <code>terminate$what</code> being <code>"max_fn"</code>.
</p>
</li>
<li><p><code>max_gr</code> Maximum number of gradient evaluations allowed.
Indicated by <code>terminate$what</code> being <code>"max_gr"</code>.
</p>
</li>
<li><p><code>max_fg</code> Maximum number of gradient evaluations allowed.
Indicated by <code>terminate$what</code> being <code>"max_fg"</code>.
</p>
</li>
<li><p><code>abs_tol</code> Absolute tolerance of the function value. If the
absolute value of the function falls below this threshold,
<code>terminate$what</code> will be <code>"abs_tol"</code>. Will only be triggered if
the objective function has a minimum value of zero.
</p>
</li>
<li><p><code>rel_tol</code> Relative tolerance of the function value, comparing
consecutive function evaluation results. Indicated by <code>terminate$what</code>
being <code>"rel_tol"</code>.
</p>
</li>
<li><p><code>grad_tol</code> Absolute tolerance of the l2 (Euclidean) norm of
the gradient. Indicated by <code>terminate$what</code> being <code>"grad_tol"</code>.
Note that the gradient norm is not a very reliable stopping criterion
(see Nocedal and co-workers 2002), but is quite commonly used, so this
might be useful for comparison with results from other optimization
software.
</p>
</li>
<li><p><code>ginf_tol</code> Absolute tolerance of the infinity norm (maximum
absolute component) of the gradient. Indicated by <code>terminate$what</code>
being <code>"ginf_tol"</code>.
</p>
</li>
<li><p><code>step_tol</code> Absolute tolerance of the step size, i.e. the
Euclidean distance between values of <code>par</code> fell below the specified
value. Indicated by <code>terminate$what</code> being <code>"step_tol"</code>.
For those optimization methods which allow for abandoning the result of an
iteration and restarting using the previous iteration's value of
<code>par</code> an iteration, <code>step_tol</code> will not be triggered.
</p>
</li></ul>

<p>Convergence is checked between specific iterations. How often is determined
by the <code>check_conv_every</code> parameter, which specifies the number of
iterations between each check. By default, this is set for every iteration.
</p>
<p>Be aware that if <code>abs_tol</code> or <code>rel_tol</code> are non-<code>NULL</code>, this
requires the function to have been evaluated at the current position at the
end of each iteration. If the function at that position has not been
calculated, it will be calculated and will contribute to the total reported
in the <code>counts</code> list in the return value. The calculated function value
is cached for use by the optimizer in the next iteration, so if the optimizer
would have needed to calculate the function anyway (e.g. use of the strong
Wolfe line search methods), there is no significant cost accrued by
calculating it earlier for convergence calculations. However, for methods
that don't use the function value at that location, this could represent a
lot of extra function evaluations. On the other hand, not checking
convergence could result in a lot of extra unnecessary iterations.
Similarly, if <code>grad_tol</code> or <code>ginf_tol</code> is non-<code>NULL</code>, then
the gradient will be calculated if needed.
</p>
<p>If extra function or gradient evaluations is an issue, set
<code>check_conv_every</code> to a higher value, but be aware that this can cause
convergence limits to be exceeded by a greater amount.
</p>
<p>Note also that if the <code>verbose</code> parameter is <code>TRUE</code>, then a summary
of the results so far will be logged to the console whenever a convergence
check is carried out. If the <code>store_progress</code> parameter is <code>TRUE</code>,
then the same information will be returned as a data frame in the return
value. For a long optimization this could be a lot of data, so by default it
is not stored.
</p>
<p>Other ways for the optimization to terminate is if an iteration generates a
non-finite (i.e. <code>Inf</code> or <code>NaN</code>) gradient or function value.
Some, but not all, line-searches will try to recover from the latter, by
reducing the step size, but a non-finite gradient calculation during the
gradient descent portion of optimization is considered catastrophic by mize,
and it will give up. Termination under non-finite gradient or function
conditions will result in <code>terminate$what</code> being <code>"gr_inf"</code> or
<code>"fn_inf"</code> respectively. Unlike the convergence criteria, the
optimization will detect these error conditions and terminate even if a
convergence check would not be carried out for this iteration.
</p>
<p>The value of <code>par</code> in the return value should be the parameters which
correspond to the lowest value of the function that has been calculated
during the optimization. As discussed above however, determining which set
of parameters requires a function evaluation at the end of each iteration,
which only happens if either the optimization method calculates it as part
of its own operation or if a convergence check is being carried out during
this iteration. Therefore, if your method does not carry out function
evaluations and <code>check_conv_every</code> is set to be so large that no
convergence calculation is carried out before <code>max_iter</code> is reached,
then the returned value of <code>par</code> is the last value encountered.
</p>


<h3>References</h3>

<p>Gilbert, J. C., &amp; Nocedal, J. (1992). Global convergence properties of conjugate gradient methods for optimization.
<em>SIAM Journal on optimization</em>, <em>2</em>(1), 21-42.
</p>
<p>Hager, W. W., &amp; Zhang, H. (2005).
A new conjugate gradient method with guaranteed descent and an efficient line search.
<em>SIAM Journal on Optimization</em>, <em>16</em>(1), 170-192.
</p>
<p>Hager, W. W., &amp; Zhang, H. (2006).
Algorithm 851: CG_DESCENT, a conjugate gradient method with guaranteed descent.
<em>ACM Transactions on Mathematical Software (TOMS)</em>, <em>32</em>(1), 113-137.
</p>
<p>Jacobs, R. A. (1988).
Increased rates of convergence through learning rate adaptation.
<em>Neural networks</em>, <em>1</em>(4), 295-307.
</p>
<p>Janet, J. A., Scoggins, S. M., Schultz, S. M., Snyder, W. E., White, M. W.,
&amp; Sutton, J. C. (1998, May).
Shocking: An approach to stabilize backprop training with greedy adaptive
learning rates.
In <em>1998 IEEE International Joint Conference on Neural Networks Proceedings.</em>
(Vol. 3, pp. 2218-2223). IEEE.
</p>
<p>Martens, J. (2010, June).
Deep learning via Hessian-free optimization.
In <em>Proceedings of the International Conference on Machine Learning.</em>
(Vol. 27, pp. 735-742).
</p>
<p>More', J. J., &amp; Thuente, D. J. (1994).
Line search algorithms with guaranteed sufficient decrease.
<em>ACM Transactions on Mathematical Software (TOMS)</em>, <em>20</em>(3), 286-307.
</p>
<p>Nocedal, J., Sartenaer, A., &amp; Zhu, C. (2002).
On the behavior of the gradient norm in the steepest descent method.
<em>Computational Optimization and Applications</em>, <em>22</em>(1), 5-35.
</p>
<p>Nocedal, J., &amp; Wright, S. (2006).
Numerical optimization.
Springer Science &amp; Business Media.
</p>
<p>O'Donoghue, B., &amp; Candes, E. (2013).
Adaptive restart for accelerated gradient schemes.
<em>Foundations of computational mathematics</em>, <em>15</em>(3), 715-732.
</p>
<p>Schmidt, M. (2005).
minFunc: unconstrained differentiable multivariate optimization in Matlab.
<a href="https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html">https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html</a>
</p>
<p>Su, W., Boyd, S., &amp; Candes, E. (2014).
A differential equation for modeling Nesterov's accelerated gradient method: theory and insights.
In <em>Advances in Neural Information Processing Systems</em> (pp. 2510-2518).
</p>
<p>Sutskever, I. (2013).
<em>Training recurrent neural networks</em>
(Doctoral dissertation, University of Toronto).
</p>
<p>Sutskever, I., Martens, J., Dahl, G., &amp; Hinton, G. (2013).
On the importance of initialization and momentum in deep learning.
In <em>Proceedings of the 30th international conference on machine learning (ICML-13)</em>
(pp. 1139-1147).
</p>
<p>Xie, D., &amp; Schlick, T. (1999).
Remark on Algorithm 702 - The updated truncated Newton minimization package.
<em>ACM Transactions on Mathematical Software (TOMS)</em>, <em>25</em>(1), 108-122.
</p>
<p>Xie, D., &amp; Schlick, T. (2002).
A more lenient stopping rule for line search algorithms.
<em>Optimization Methods and Software</em>, <em>17</em>(4), 683-700.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Function to optimize and starting point defined after creating optimizer
rosenbrock_fg &lt;- list(
  fn = function(x) {
    100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2
  },
  gr = function(x) {
    c(
      -400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),
      200 * (x[2] - x[1] * x[1])
    )
  }
)
rb0 &lt;- c(-1.2, 1)

# Minimize using L-BFGS
res &lt;- mize(rb0, rosenbrock_fg, method = "L-BFGS")

# Conjugate gradient with Fletcher-Reeves update, tight Wolfe line search
res &lt;- mize(rb0, rosenbrock_fg, method = "CG", cg_update = "FR", c2 = 0.1)

# Steepest decent with constant momentum = 0.9
res &lt;- mize(rb0, rosenbrock_fg, method = "MOM", mom_schedule = 0.9)

# Steepest descent with constant momentum in the Nesterov style as described
# in papers by Sutskever and Bengio
res &lt;- mize(rb0, rosenbrock_fg,
  method = "MOM", mom_type = "nesterov",
  mom_schedule = 0.9
)

# Nesterov momentum with adaptive restart comparing function values
res &lt;- mize(rb0, rosenbrock_fg,
  method = "MOM", mom_type = "nesterov",
  mom_schedule = 0.9, restart = "fn"
)
</code></pre>

<hr>
<h2 id='mize_init'>Initialize the Optimizer.</h2><span id='topic+mize_init'></span>

<h3>Description</h3>

<p>Prepares the optimizer for use with a specific function and starting point.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mize_init(
  opt,
  par,
  fg,
  max_iter = Inf,
  max_fn = Inf,
  max_gr = Inf,
  max_fg = Inf,
  abs_tol = NULL,
  rel_tol = abs_tol,
  grad_tol = NULL,
  ginf_tol = NULL,
  step_tol = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mize_init_+3A_opt">opt</code></td>
<td>
<p>Optimizer, created by <code><a href="#topic+make_mize">make_mize</a></code>.</p>
</td></tr>
<tr><td><code id="mize_init_+3A_par">par</code></td>
<td>
<p>Vector of initial values for the function to be optimized over.</p>
</td></tr>
<tr><td><code id="mize_init_+3A_fg">fg</code></td>
<td>
<p>Function and gradient list. See the documentation of
<code><a href="#topic+mize">mize</a></code>.</p>
</td></tr>
<tr><td><code id="mize_init_+3A_max_iter">max_iter</code></td>
<td>
<p>(Optional). Maximum number of iterations. See the
'Convergence' section of <code><a href="#topic+mize">mize</a></code> for details.</p>
</td></tr>
<tr><td><code id="mize_init_+3A_max_fn">max_fn</code></td>
<td>
<p>(Optional). Maximum number of function evaluations. See the
'Convergence' section of <code><a href="#topic+mize">mize</a></code> for details.</p>
</td></tr>
<tr><td><code id="mize_init_+3A_max_gr">max_gr</code></td>
<td>
<p>(Optional). Maximum number of gradient evaluations. See the
'Convergence' section of <code><a href="#topic+mize">mize</a></code> for details.</p>
</td></tr>
<tr><td><code id="mize_init_+3A_max_fg">max_fg</code></td>
<td>
<p>(Optional). Maximum number of function or gradient evaluations.
See the 'Convergence' section of <code><a href="#topic+mize">mize</a></code> for details.</p>
</td></tr>
<tr><td><code id="mize_init_+3A_abs_tol">abs_tol</code></td>
<td>
<p>(Optional). Absolute tolerance for comparing two function
evaluations. See the 'Convergence' section of <code><a href="#topic+mize">mize</a></code> for
details.</p>
</td></tr>
<tr><td><code id="mize_init_+3A_rel_tol">rel_tol</code></td>
<td>
<p>(Optional). Relative tolerance for comparing two function
evaluations. See the 'Convergence' section of <code><a href="#topic+mize">mize</a></code> for
details.</p>
</td></tr>
<tr><td><code id="mize_init_+3A_grad_tol">grad_tol</code></td>
<td>
<p>(Optional). Absolute tolerance for the length (l2-norm) of
the gradient vector. See the 'Convergence' section of <code><a href="#topic+mize">mize</a></code>
for details.</p>
</td></tr>
<tr><td><code id="mize_init_+3A_ginf_tol">ginf_tol</code></td>
<td>
<p>(Optional). Absolute tolerance for the infinity norm (maximum
absolute component) of the gradient vector. See the 'Convergence' section
of <code><a href="#topic+mize">mize</a></code> for details.</p>
</td></tr>
<tr><td><code id="mize_init_+3A_step_tol">step_tol</code></td>
<td>
<p>(Optional). Absolute tolerance for the size of the parameter
update. See the 'Convergence' section of <code><a href="#topic+mize">mize</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Should be called after creating an optimizer with <code><a href="#topic+make_mize">make_mize</a></code> and
before beginning any optimization with <code><a href="#topic+mize_step">mize_step</a></code>. Note that if
<code>fg</code> and <code>par</code> are available at the time <code><a href="#topic+mize_step">mize_step</a></code> is
called, they can be passed to that function and initialization will be
carried out automatically, avoiding the need to call <code>mize_init</code>.
</p>
<p>Optional convergence parameters may also be passed here, for use with
<code><a href="#topic+check_mize_convergence">check_mize_convergence</a></code>. They are optional if you do your own
convergence checking.
</p>
<p>Details of the <code>fg</code> list can be found in the 'Details' section of
<code><a href="#topic+mize">mize</a></code>.
</p>


<h3>Value</h3>

<p>Initialized optimizer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Create an optimizer
opt &lt;- make_mize(method = "L-BFGS")

# Function to optimize and starting point defined after creating optimizer
rosenbrock_fg &lt;- list(
  fn = function(x) {
    100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2
  },
  gr = function(x) {
    c(
      -400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),
      200 * (x[2] - x[1] * x[1])
    )
  }
)
rb0 &lt;- c(-1.2, 1)

# Initialize with function and starting point before commencing optimization
opt &lt;- mize_init(opt, rb0, rosebrock_fg)

# Finally, can commence the optimization loop
par &lt;- rb0
for (iter in 1:3) {
  res &lt;- mize_step(opt, par, rosenbrock_fg)
  par &lt;- res$par
  opt &lt;- res$opt
}
</code></pre>

<hr>
<h2 id='mize_step'>One Step of Optimization</h2><span id='topic+mize_step'></span>

<h3>Description</h3>

<p>Performs one iteration of optimization using a specified optimizer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mize_step(opt, par, fg)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mize_step_+3A_opt">opt</code></td>
<td>
<p>Optimizer, created by <code><a href="#topic+make_mize">make_mize</a></code>.</p>
</td></tr>
<tr><td><code id="mize_step_+3A_par">par</code></td>
<td>
<p>Vector of initial values for the function to be optimized over.</p>
</td></tr>
<tr><td><code id="mize_step_+3A_fg">fg</code></td>
<td>
<p>Function and gradient list. See the documentation of
<code><a href="#topic+mize">mize</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function returns both the (hopefully) optimized vector of parameters, and
an updated version of the optimizer itself. This is intended to be used when
you want more control over the optimization process compared to the more black
box approach of the <code><a href="#topic+mize">mize</a></code> function. In return for having to
manually call this function every time you want the next iteration of
optimization, you gain the ability to do your own checks for convergence,
logging and so on, as well as take other action between iterations, e.g.
visualization.
</p>
<p>Normally calling this function should return a more optimized vector of
parameters than the input, or at  least leave the parameters unchanged if no
improvement was found, although this is determined by how the optimizer was
configured by <code><a href="#topic+make_mize">make_mize</a></code>. It is very possible to create an
optimizer that can cause a solution to diverge. It is the responsibility of
the caller to check that the result of the optimization step has actually
reduced the value returned from function being optimized.
</p>
<p>Details of the <code>fg</code> list can be found in the 'Details' section of
<code><a href="#topic+mize">mize</a></code>.
</p>


<h3>Value</h3>

<p>Result of the current optimization step, a list with components:
</p>

<ul>
<li><p><code>opt</code>. Updated version of the optimizer passed to the <code>opt</code>
argument Should be passed as the <code>opt</code> argument in the next iteration.
</p>
</li>
<li><p><code>par</code>. Updated version of the parameters passed to the
<code>par</code> argument. Should be passed as the <code>par</code> argument in the next
iteration.
</p>
</li>
<li><p><code>nf</code>. Running total number of function evaluations carried out
since iteration 1.
</p>
</li>
<li><p><code>ng</code>. Running total number of gradient evaluations carried out
since iteration 1.
</p>
</li>
<li><p><code>f</code>. Optional. The new value of the function, evaluated at the
returned value of <code>par</code>. Only present if calculated as part of the
optimization step (e.g. during a line search calculation).
</p>
</li>
<li><p><code>g</code>. Optional. The gradient vector, evaluated at the returned
value of <code>par</code>. Only present if the gradient was calculated as part of
the optimization step (e.g. during a line search calculation.)</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+make_mize">make_mize</a></code> to create a value to pass to <code>opt</code>,
<code><a href="#topic+mize_init">mize_init</a></code> to initialize <code>opt</code> before passing it to this
function for the first time. <code><a href="#topic+mize">mize</a></code> creates an optimizer and
carries out a full optimization with it.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rosenbrock_fg &lt;- list(
  fn = function(x) {
    100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2
  },
  gr = function(x) {
    c(
      -400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),
      200 * (x[2] - x[1] * x[1])
    )
  }
)
rb0 &lt;- c(-1.2, 1)

opt &lt;- make_mize(
  method = "SD", line_search = "const", step0 = 0.0001,
  par = rb0, fg = rosenbrock_fg
)
par &lt;- rb0
for (iter in 1:3) {
  res &lt;- mize_step(opt, par, rosenbrock_fg)
  par &lt;- res$par
  opt &lt;- res$opt
}
</code></pre>

<hr>
<h2 id='mize_step_summary'>Mize Step Summary</h2><span id='topic+mize_step_summary'></span>

<h3>Description</h3>

<p>Produces a result summary for an optimization iteration. Information such as
function value, gradient norm and step size may be returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mize_step_summary(opt, par, fg, par_old = NULL, calc_fn = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mize_step_summary_+3A_opt">opt</code></td>
<td>
<p>Optimizer to generate summary for, from return value of
<code><a href="#topic+mize_step">mize_step</a></code>.</p>
</td></tr>
<tr><td><code id="mize_step_summary_+3A_par">par</code></td>
<td>
<p>Vector of parameters at the end of the iteration, from return value
of <code><a href="#topic+mize_step">mize_step</a></code>.</p>
</td></tr>
<tr><td><code id="mize_step_summary_+3A_fg">fg</code></td>
<td>
<p>Function and gradient list. See the documentation of
<code><a href="#topic+mize">mize</a></code>.</p>
</td></tr>
<tr><td><code id="mize_step_summary_+3A_par_old">par_old</code></td>
<td>
<p>(Optional). Vector of parameters at the end of the previous
iteration. Used to calculate step size.</p>
</td></tr>
<tr><td><code id="mize_step_summary_+3A_calc_fn">calc_fn</code></td>
<td>
<p>(Optional). If <code>TRUE</code>, force calculation of function if
not already cached in <code>opt</code>, even if it would not be needed for
convergence checking.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, convergence tolerance parameters will be used to determine what
function and gradient data is returned. The function value will be returned if
it was already calculated and cached in the optimization iteration. Otherwise,
it will be calculated only if a non-null absolute or relative tolerance value
was asked for. A gradient norm will be returned only if a non-null gradient
tolerance was specified, even if the gradient is available.
</p>
<p>Note that if a function tolerance was specified, but was not calculated for
the relevant value of <code>par</code>, they will be calculated here and the
calculation does contribute to the total function count (and will be cached
for potential use in the next iteration). The same applies for gradient
tolerances and gradient calculation. Function and gradient calculation can
also be forced here by setting the <code>calc_fn</code> and <code>calc_gr</code>
(respectively) parameters to <code>TRUE</code>.
</p>


<h3>Value</h3>

<p>A list with the following items: </p>

<ul>
<li> <p><code>opt</code> Optimizer with updated state (e.g. function and gradient
counts).
</p>
</li>
<li> <p><code>iter</code> Iteration number.
</p>
</li>
<li> <p><code>f</code> Function value at <code>par</code>.
</p>
</li>
<li> <p><code>g2n</code> 2-norm of the gradient at <code>par</code>.
</p>
</li>
<li> <p><code>ginfn</code> Infinity-norm of the gradient at <code>par</code>.
</p>
</li>
<li> <p><code>nf</code> Number of function evaluations so far.
</p>
</li>
<li> <p><code>ng</code> Number of gradient evaluations so far.
</p>
</li>
<li> <p><code>step</code> Size of the step between <code>par_old</code> and <code>par</code>,
if <code>par_old</code> is provided.
</p>
</li>
<li> <p><code>alpha</code> Step length of the gradient descent part of the step.
</p>
</li>
<li> <p><code>mu</code> Momentum coefficient for this iteration</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>rb_fg &lt;- list(
  fn = function(x) {
    100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2
  },
  gr = function(x) {
    c(
      -400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),
      200 * (x[2] - x[1] * x[1])
    )
  }
)
rb0 &lt;- c(-1.2, 1)

opt &lt;- make_mize(method = "BFGS", par = rb0, fg = rb_fg, max_iter = 30)
mize_res &lt;- mize_step(opt = opt, par = rb0, fg = rb_fg)
# Get info about first step, use rb0 to compare new par with initial value
step_info &lt;- mize_step_summary(mize_res$opt, mize_res$par, rb_fg, rb0)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
