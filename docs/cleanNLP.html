<!DOCTYPE html><html><head><title>Help for package cleanNLP</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {cleanNLP}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cleanNLP-package'><p>cleanNLP: A Tidy Data Model for Natural Language Processing</p></a></li>
<li><a href='#cnlp_annotate'><p>Run the annotation pipeline on a set of documents</p></a></li>
<li><a href='#cnlp_download_corenlp'><p>Download model files needed for coreNLP</p></a></li>
<li><a href='#cnlp_download_spacy'><p>Download model files needed for spacy</p></a></li>
<li><a href='#cnlp_init_corenlp'><p>Interface for initializing the coreNLP backend</p></a></li>
<li><a href='#cnlp_init_spacy'><p>Interface for initializing the spacy backend</p></a></li>
<li><a href='#cnlp_init_stringi'><p>Interface for initializing the standard R backend</p></a></li>
<li><a href='#cnlp_init_udpipe'><p>Interface for initializing the udpipe backend</p></a></li>
<li><a href='#cnlp_utils_pca'><p>Compute Principal Components and store as a Data Frame</p></a></li>
<li><a href='#cnlp_utils_tfidf'><p>Construct the TF-IDF Matrix from Annotation or Data Frame</p></a></li>
<li><a href='#un'><p>Universal Declaration of Human Rights</p></a></li>
<li><a href='#word_frequency'><p>Most frequent English words</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>A Tidy Data Model for Natural Language Processing</td>
</tr>
<tr>
<td>Version:</td>
<td>3.0.7</td>
</tr>
<tr>
<td>Author:</td>
<td>Taylor B. Arnold [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Taylor B. Arnold &lt;tarnold2@richmond.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides a set of fast tools for converting a textual corpus into
  a set of normalized tables. Users may make use of the 'udpipe' back end with
  no external dependencies, or two Python back ends with 'spaCy'
  <a href="https://spacy.io">https://spacy.io</a> or 'CoreNLP' <a href="https://stanfordnlp.github.io/CoreNLP/">https://stanfordnlp.github.io/CoreNLP/</a>.
  Exposed annotation tasks include tokenization, part of speech tagging, named
  entity recognition, and dependency parsing.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Matrix (&ge; 1.2), udpipe, reticulate, stringi, stats, methods</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr (&ge; 1.15), rmarkdown (&ge; 1.4), testthat (&ge; 1.0.1),
covr (&ge; 2.2.2)</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>Python (&gt;= 3.7.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-2">LGPL-2</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://statsmaths.github.io/cleanNLP/">https://statsmaths.github.io/cleanNLP/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/statsmaths/cleanNLP/issues">https://github.com/statsmaths/cleanNLP/issues</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-16 16:31:08 UTC; admin</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-16 17:03:58 UTC</td>
</tr>
</table>
<hr>
<h2 id='cleanNLP-package'>cleanNLP: A Tidy Data Model for Natural Language Processing</h2><span id='topic+cleanNLP'></span><span id='topic+cleanNLP-package'></span>

<h3>Description</h3>

<p>Provides a set of fast tools for converting a textual corpus into a set
of normalized tables. Multiple NLP backends can be used, with the output
standardized into a normalized format. Options include stringi (very fast,
but only provides tokenization), udpipe (fast, many languages, includes
part of speech tags and dependencies), coreNLP (using its Python backend),
and spacy (python backend; includes named entity recognition).
</p>


<h3>Details</h3>

<p>Once the package is set up, run one of <code><a href="#topic+cnlp_init_stringi">cnlp_init_stringi</a></code>,
<code><a href="#topic+cnlp_init_spacy">cnlp_init_spacy</a></code>, <code><a href="#topic+cnlp_init_corenlp">cnlp_init_corenlp</a></code>, or
<code><a href="#topic+cnlp_init_udpipe">cnlp_init_udpipe</a></code> to load the desired NLP backend.
After this function is done running, use <code><a href="#topic+cnlp_annotate">cnlp_annotate</a></code>
to run the annotation engine over a corpus of text. The package vignettes
provide more detailed set-up information.
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://statsmaths.github.io/cleanNLP/">https://statsmaths.github.io/cleanNLP/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/statsmaths/cleanNLP/issues">https://github.com/statsmaths/cleanNLP/issues</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
library(cleanNLP)

# load the annotation engine
cnlp_init_stringi()

# annotate your text
input &lt;- data.frame(
 text=c(
   "This is a sentence.",
   "Here is something else to parse!"
 ),
 stringsAsFactors=FALSE
)

## End(Not run)

</code></pre>

<hr>
<h2 id='cnlp_annotate'>Run the annotation pipeline on a set of documents</h2><span id='topic+cnlp_annotate'></span>

<h3>Description</h3>

<p>Runs the clean_nlp annotators over a given corpus of text
using the desired backend. The details for which annotators to run and
how to run them are specified by using one of:
<code><a href="#topic+cnlp_init_stringi">cnlp_init_stringi</a></code>, <code><a href="#topic+cnlp_init_spacy">cnlp_init_spacy</a></code>,
<code><a href="#topic+cnlp_init_udpipe">cnlp_init_udpipe</a></code>, or <code><a href="#topic+cnlp_init_corenlp">cnlp_init_corenlp</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cnlp_annotate(
  input,
  backend = NULL,
  verbose = 10,
  text_name = "text",
  doc_name = "doc_id"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cnlp_annotate_+3A_input">input</code></td>
<td>
<p>an object containing the data to parse. Either a
character vector with the texts (optional names can
be given to provide document ids) or a data frame. The
data frame should have a column named 'text' containing
the raw text to parse; if there is a column named
'doc_id', it is treated as a a document identifier.
The name of the text and document id columns can be
changed by setting <code>text_name</code> and <code>doc_name</code>
This conforms with corpus objects respecting the Text
Interchange Format (TIF), while allowing for some
variation.</p>
</td></tr>
<tr><td><code id="cnlp_annotate_+3A_backend">backend</code></td>
<td>
<p>name of the backend to use. Will default to the last
model to be initalized.</p>
</td></tr>
<tr><td><code id="cnlp_annotate_+3A_verbose">verbose</code></td>
<td>
<p>set to a positive integer n to display a progress
message to display every n'th record. The default is
10. Set to a non-positive integer to turn off messages.
Logical input is converted to an integer, so it also
possible to set to TRUE (1) to display a message for
every document and FALSE (0) to turn off messages.</p>
</td></tr>
<tr><td><code id="cnlp_annotate_+3A_text_name">text_name</code></td>
<td>
<p>column name containing the text input. The default
is 'text'. This parameter is ignored when <code>input</code>
is a character vector.</p>
</td></tr>
<tr><td><code id="cnlp_annotate_+3A_doc_name">doc_name</code></td>
<td>
<p>column name containing the document ids. The default
is 'doc_id'. This parameter is ignored when
<code>input</code> is a character vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The returned object is a named list where each element containing a data
frame. The document table contains one row for each document, along with
with all of the metadata that was passed as an input. The tokens table
has one row for each token detected in the input. The first three columns
are always &quot;doc_id&quot; (to index the input document), &quot;sid&quot; (an integer index
for the sentence number), and &quot;tid&quot; (an integer index to the specific
token). Together, these are a primary key for each row.
</p>
<p>Other columns provide extracted data about each token, which differ slightly
based on which backend, language, and options are supplied.
</p>

<ul>
<li> <p><b>token</b>: detected token, as given in the original input
</p>
</li>
<li> <p><b>token_with_ws</b>: detected token along with white space; in,
theory, collapsing this field through an entire document will yield
the original text
</p>
</li>
<li> <p><b>lemma</b>: lemmatised version of the token; the exact form
depends on the choosen language and backend
</p>
</li>
<li> <p><b>upos</b>: the universal part of speech code; see
<a href="https://universaldependencies.org/u/pos/all.html">https://universaldependencies.org/u/pos/all.html</a>
for more information
</p>
</li>
<li> <p><b>xpos</b>: language dependent part of speech code; the specific
categories and their meaning depend on the choosen backend, model
and language
</p>
</li>
<li> <p><b>feats</b>: other extracted linguistic features, typically given
as Universal Dependencies
(<a href="https://universaldependencies.org/u/feat/index.html">https://universaldependencies.org/u/feat/index.html</a>), but can
be model dependent; currently only provided by the udpipe backend
</p>
</li>
<li> <p><b>tid_source</b>: the token id (tid) of the head word for the
dependency relationship starting from this token; for the token
attached to the root, this will be given as zero
</p>
</li>
<li> <p><b>relation</b>: the dependency relation, usually provided using
Universal Dependencies (more information available here
<a href="https://universaldependencies.org/">https://universaldependencies.org/</a>
), but could be different for a specific model
</p>
</li></ul>



<h3>Value</h3>

<p>a named list with components &quot;token&quot;, &quot;document&quot; and (when
running spacy with NER) &quot;entity&quot;.
</p>


<h3>Author(s)</h3>

<p>Taylor B. Arnold, <a href="mailto:taylor.arnold@acm.org">taylor.arnold@acm.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cnlp_init_stringi()
cnlp_annotate(un)

</code></pre>

<hr>
<h2 id='cnlp_download_corenlp'>Download model files needed for coreNLP</h2><span id='topic+cnlp_download_corenlp'></span>

<h3>Description</h3>

<p>The cleanNLP package does not supply the model files required
for using the coreNLP backend. These files can be downloaded
with this function. If you need more control, download directly from
Python.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cnlp_download_corenlp(lang = "en")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cnlp_download_corenlp_+3A_lang">lang</code></td>
<td>
<p>string giving the languange code. Defaults to &quot;en&quot;.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
cnlp_download_corenlp(lang="en")
cnlp_download_corenlp(lang="de")

## End(Not run)
</code></pre>

<hr>
<h2 id='cnlp_download_spacy'>Download model files needed for spacy</h2><span id='topic+cnlp_download_spacy'></span>

<h3>Description</h3>

<p>The cleanNLP package does not supply the model files required
for using the spacy backend. These files can be downloaded
with this function. If you need more control, download directly from
Python.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cnlp_download_spacy(model_name = "en")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cnlp_download_spacy_+3A_model_name">model_name</code></td>
<td>
<p>string giving the model namel.
Defaults to &quot;en&quot;.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
cnlp_download_spacy(model_name="en")
cnlp_download_spacy(model_name="de")

## End(Not run)
</code></pre>

<hr>
<h2 id='cnlp_init_corenlp'>Interface for initializing the coreNLP backend</h2><span id='topic+cnlp_init_corenlp'></span>

<h3>Description</h3>

<p>This function must be run before annotating text with
the coreNLP backend. It sets the properties for the
spacy engine and loads the file using the R to Python
interface provided by reticulate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cnlp_init_corenlp(lang = NULL, models_dir = NULL, config = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cnlp_init_corenlp_+3A_lang">lang</code></td>
<td>
<p>string giving the language name for the corenlp backend.
Defaults to &quot;en&quot; (English) if set to NULL.</p>
</td></tr>
<tr><td><code id="cnlp_init_corenlp_+3A_models_dir">models_dir</code></td>
<td>
<p>directory where model files are located. Set to NULL to
use the default.</p>
</td></tr>
<tr><td><code id="cnlp_init_corenlp_+3A_config">config</code></td>
<td>
<p>An optional named list to be converted to a Python
dictionary.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Taylor B. Arnold, <a href="mailto:taylor.arnold@acm.org">taylor.arnold@acm.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
cnlp_init_corenlp()

## End(Not run)

</code></pre>

<hr>
<h2 id='cnlp_init_spacy'>Interface for initializing the spacy backend</h2><span id='topic+cnlp_init_spacy'></span>

<h3>Description</h3>

<p>This function must be run before annotating text with
the spacy backend. It sets the properties for the
spacy engine and loads the file using the R to Python
interface provided by reticulate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cnlp_init_spacy(model_name = NULL, disable = NULL, max_length = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cnlp_init_spacy_+3A_model_name">model_name</code></td>
<td>
<p>string giving the model name for the spacy backend.
Defaults to &quot;en&quot; (English) if set to NULL.</p>
</td></tr>
<tr><td><code id="cnlp_init_spacy_+3A_disable">disable</code></td>
<td>
<p>an optional vector of pipes to disable.</p>
</td></tr>
<tr><td><code id="cnlp_init_spacy_+3A_max_length">max_length</code></td>
<td>
<p>amount of temporary memory provided to Spacy, in
characters. The default of 1000000 should work for most
applications, but can be increased when working with
long documents.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Taylor B. Arnold, <a href="mailto:taylor.arnold@acm.org">taylor.arnold@acm.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
cnlp_init_spacy(model_name = "en")

## End(Not run)

</code></pre>

<hr>
<h2 id='cnlp_init_stringi'>Interface for initializing the standard R backend</h2><span id='topic+cnlp_init_stringi'></span>

<h3>Description</h3>

<p>This function must be run before annotating text with
the tokenizers backend.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cnlp_init_stringi(locale = NULL, include_spaces = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cnlp_init_stringi_+3A_locale">locale</code></td>
<td>
<p>string giving the locale name to
pass to the stringi functions. If
<code>NULL</code>, the default locale is
selected</p>
</td></tr>
<tr><td><code id="cnlp_init_stringi_+3A_include_spaces">include_spaces</code></td>
<td>
<p>logical. Should spaces be included as tokens in
the output. Defaults to FALSE</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Taylor B. Arnold, <a href="mailto:taylor.arnold@acm.org">taylor.arnold@acm.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
cnlp_init_stringi()

## End(Not run)

</code></pre>

<hr>
<h2 id='cnlp_init_udpipe'>Interface for initializing the udpipe backend</h2><span id='topic+cnlp_init_udpipe'></span>

<h3>Description</h3>

<p>This function must be run before annotating text with
the udpipe backend. It will parse in English by default,
but you can load other models as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cnlp_init_udpipe(
  model_name = NULL,
  model_path = NULL,
  tokenizer = "tokenizer",
  tagger = "default",
  parser = "default"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cnlp_init_udpipe_+3A_model_name">model_name</code></td>
<td>
<p>string giving the model namel.
Defaults to &quot;english&quot; if NULL.
Ignored if <code>model_path</code> is not NULL.</p>
</td></tr>
<tr><td><code id="cnlp_init_udpipe_+3A_model_path">model_path</code></td>
<td>
<p>provide a full path to a model file.</p>
</td></tr>
<tr><td><code id="cnlp_init_udpipe_+3A_tokenizer">tokenizer</code></td>
<td>
<p>a character string of length 1, which is either
'tokenizer' (default udpipe tokenisation) or a
character string with more
complex tokenisation options as specified in &lt;URL:
http://ufal.mff.cuni.cz/udpipe/users-manual&gt; in which
case tokenizer should be a character string where the
options are put after each other using the semicolon as
separation.</p>
</td></tr>
<tr><td><code id="cnlp_init_udpipe_+3A_tagger">tagger</code></td>
<td>
<p>a character string of length 1, which is either 'default'
(default udpipe POS tagging and lemmatisation) or 'none' (no
POS tagging and lemmatisation needed) or a character string
with more complex tagging options as specified in &lt;URL:
http://ufal.mff.cuni.cz/udpipe/users-manual&gt; in which case
tagger should be a character string where the options are
put after each other using the semicolon as separation.</p>
</td></tr>
<tr><td><code id="cnlp_init_udpipe_+3A_parser">parser</code></td>
<td>
<p>a character string of length 1, which is either 'default'
(default udpipe dependency parsing) or 'none' (no dependency
parsing needed) or a character string with more complex
parsing options as specified in &lt;URL:
http://ufal.mff.cuni.cz/udpipe/users-manual&gt; in which case
parser should be a character string where the options are
put after each other using the semicolon as separation.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Taylor B. Arnold, <a href="mailto:taylor.arnold@acm.org">taylor.arnold@acm.org</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
cnlp_init_udpipe(model_name = "english")

## End(Not run)

</code></pre>

<hr>
<h2 id='cnlp_utils_pca'>Compute Principal Components and store as a Data Frame</h2><span id='topic+cnlp_utils_pca'></span>

<h3>Description</h3>

<p>Takes a matrix and returns a data frame with the top principal components
extracted. This is a simple but powerful technique for visualizing a corpus
of documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cnlp_utils_pca(x, k = 2, center = TRUE, scale = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cnlp_utils_pca_+3A_x">x</code></td>
<td>
<p>a matrix object to pass to <code>prcomp</code></p>
</td></tr>
<tr><td><code id="cnlp_utils_pca_+3A_k">k</code></td>
<td>
<p>integer. The number of components to include in the output.</p>
</td></tr>
<tr><td><code id="cnlp_utils_pca_+3A_center">center</code></td>
<td>
<p>logical. Should the data be centered?</p>
</td></tr>
<tr><td><code id="cnlp_utils_pca_+3A_scale">scale</code></td>
<td>
<p>logical. Should the data be scaled? Note that this will
need to be set to false if any columns in <code>x</code> are
constant if <code>center</code> is also true.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data frame object containing the top <code>k</code> principal
components of the data in x.
</p>

<hr>
<h2 id='cnlp_utils_tfidf'>Construct the TF-IDF Matrix from Annotation or Data Frame</h2><span id='topic+cnlp_utils_tfidf'></span><span id='topic+cnlp_utils_tf'></span>

<h3>Description</h3>

<p>Given annotations, this function returns the term-frequency inverse
document frequency (tf-idf) matrix from the extracted lemmas.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cnlp_utils_tfidf(
  object,
  tf_weight = c("lognorm", "binary", "raw", "dnorm"),
  idf_weight = c("idf", "smooth", "prob", "uniform"),
  min_df = 0.1,
  max_df = 0.9,
  max_features = 10000,
  doc_var = "doc_id",
  token_var = "lemma",
  vocabulary = NULL,
  doc_set = NULL
)

cnlp_utils_tf(
  object,
  tf_weight = "raw",
  idf_weight = "uniform",
  min_df = 0,
  max_df = 1,
  max_features = 10000,
  doc_var = "doc_id",
  token_var = "lemma",
  vocabulary = NULL,
  doc_set = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cnlp_utils_tfidf_+3A_object">object</code></td>
<td>
<p>a data frame containing an identifier for the document
(set with <code>doc_var</code>) and token (set with
<code>token_var</code>)</p>
</td></tr>
<tr><td><code id="cnlp_utils_tfidf_+3A_tf_weight">tf_weight</code></td>
<td>
<p>the weighting scheme for the term frequency matrix.
The selection <code>lognorm</code> takes one plus
the log of the raw frequency (or zero if zero),
<code>binary</code> encodes a zero one matrix
indicating simply whether the token exists at all
in the document, <code>raw</code> returns raw counts,
and <code>dnorm</code> uses double normalization.</p>
</td></tr>
<tr><td><code id="cnlp_utils_tfidf_+3A_idf_weight">idf_weight</code></td>
<td>
<p>the weighting scheme for the inverse document
matrix. The selection <code>idf</code> gives the
logarithm of the simple inverse frequency,
<code>smooth</code> gives the logarithm of one plus
the simple inverse frequency, and <code>prob</code>
gives the log odds of the the token occurring
in a randomly selected document. Set to <code>uniform</code>
to return just the term frequencies.</p>
</td></tr>
<tr><td><code id="cnlp_utils_tfidf_+3A_min_df">min_df</code></td>
<td>
<p>the minimum proportion of documents a token
should be in to be included in the vocabulary</p>
</td></tr>
<tr><td><code id="cnlp_utils_tfidf_+3A_max_df">max_df</code></td>
<td>
<p>the maximum proportion of documents a token
should be in to be included in the vocabulary</p>
</td></tr>
<tr><td><code id="cnlp_utils_tfidf_+3A_max_features">max_features</code></td>
<td>
<p>the maximum number of tokens in the vocabulary</p>
</td></tr>
<tr><td><code id="cnlp_utils_tfidf_+3A_doc_var">doc_var</code></td>
<td>
<p>character vector. The name of the column in
<code>object</code> that contains the document ids. Defaults
to &quot;doc_id&quot;.</p>
</td></tr>
<tr><td><code id="cnlp_utils_tfidf_+3A_token_var">token_var</code></td>
<td>
<p>character vector. The name of the column in
<code>object</code> that contains the tokens. Defaults to
&quot;lemma&quot;.</p>
</td></tr>
<tr><td><code id="cnlp_utils_tfidf_+3A_vocabulary">vocabulary</code></td>
<td>
<p>character vector. The vocabulary set to use in
constructing the matrices. Will be computed
within the function if set to <code>NULL</code>. When
supplied, the options <code>min_df</code>, <code>max_df</code>,
and <code>max_features</code> are ignored.</p>
</td></tr>
<tr><td><code id="cnlp_utils_tfidf_+3A_doc_set">doc_set</code></td>
<td>
<p>optional character vector of document ids. Useful to
create empty rows in the output matrix for documents
without data in the input. Most users will want to keep
this equal to <code>NULL</code>, the default, to have the
function compute the document set automatically.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a sparse matrix with dimnames giving the documents and vocabular.
</p>

<hr>
<h2 id='un'>Universal Declaration of Human Rights</h2><span id='topic+un'></span>

<h3>Description</h3>

<p>Data frame containing the 30 Articles in the United Nations'
Universal Declaration of Human Rights, ratified on 10 December 1948
in Paris, France.
</p>


<h3>References</h3>

<p><a href="https://www.un.org/en/universal-declaration-human-rights/">https://www.un.org/en/universal-declaration-human-rights/</a>
</p>

<hr>
<h2 id='word_frequency'>Most frequent English words</h2><span id='topic+word_frequency'></span>

<h3>Description</h3>

<p>A dataset of the 150k most frequently used English words, extracted by
Peter Norvig from the Google Web Trillion Word Corpus. Frequencies are
multiplied by 100.
</p>


<h3>References</h3>

<p><a href="https://norvig.com/ngrams/">https://norvig.com/ngrams/</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
