<!DOCTYPE html><html><head><title>Help for package performanceEstimation</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {performanceEstimation}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bootEstimates'>
<p>Performance estimation using (e0 or .632) bootstrap</p></a></li>
<li><a href='#Bootstrap-class'><p>Class &quot;Bootstrap&quot;</p></a></li>
<li><a href='#CDdiagram.BD'>
<p>CD diagrams for the post-hoc Boferroni-Dunn test</p></a></li>
<li><a href='#CDdiagram.Nemenyi'>
<p>CD diagrams for the post-hoc Nemenyi test</p></a></li>
<li><a href='#classificationMetrics'>
<p>Calculate some standard classification evaluation metrics of predictive performance</p></a></li>
<li><a href='#ComparisonResults-class'><p>Class &quot;ComparisonResults&quot;</p></a></li>
<li><a href='#CV-class'><p>Class &quot;CV&quot;</p></a></li>
<li><a href='#cvEstimates'>
<p>Performance estimation using cross validation</p></a></li>
<li><a href='#EstCommon-class'><p>Class <code>"EstCommon"</code></p></a></li>
<li><a href='#EstimationMethod-class'><p>Class &quot;EstimationMethod&quot;</p></a></li>
<li><a href='#EstimationResults-class'><p>Class &quot;EstimationResults&quot;</p></a></li>
<li><a href='#estimationSummary'>
<p>Obtain a set of descriptive statistics of the scores of a workflow on</p>
a task</a></li>
<li><a href='#EstimationTask-class'><p>Class <code>"EstimationTask"</code></p></a></li>
<li><a href='#getIterationsInfo'>
<p>Obtaining the information returned by a workflow when applied to a task,</p>
on a particular iteration of the estimation process or on all iterations</a></li>
<li><a href='#getIterationsPreds'>
<p>Obtaining the predictions returned by a workflow when applied to a task,</p>
on a particular iteration of the estimation process, or on all iterations</a></li>
<li><a href='#getScores'>
<p>Obtaining the metric scores on the different iterations for a workflow /</p>
task combination</a></li>
<li><a href='#getWorkflow'>
<p>Obtain the workflow object corresponding to an ID</p></a></li>
<li><a href='#hldEstimates'>
<p>Performance estimation using holdout and random resampling</p></a></li>
<li><a href='#Holdout-class'><p>Class &quot;Holdout&quot;</p></a></li>
<li><a href='#is.classification'>
<p>Check if a certain predictive task is a classification problem</p></a></li>
<li><a href='#is.regression'>
<p>Check if a certain predictive task is a regression problem</p></a></li>
<li><a href='#knnImp'>
<p>Fill in NA values with the values of the nearest neighbours</p></a></li>
<li><a href='#LOOCV-class'><p>Class &quot;LOOCV&quot;</p></a></li>
<li><a href='#loocvEstimates'>
<p>Performance estimation using  Leave One Out Cross Validation</p></a></li>
<li><a href='#mcEstimates'>
<p>Performance estimation for time series prediction tasks using Monte Carlo</p>
</p></a></li>
<li><a href='#mergeEstimationRes'>
<p>Merging several <code>ComparisonResults</code> class objects</p></a></li>
<li><a href='#metricNames'>
<p>The evaluation metrics estimated in an experiment</p></a></li>
<li><a href='#metricsSummary'>
<p>Obtains a summary of the individual metric scores obtained by each</p>
workflow on a set of tasks.</a></li>
<li><a href='#MonteCarlo-class'><p>Class &quot;MonteCarlo&quot;</p></a></li>
<li><a href='#pairedComparisons'>
<p>Statistical hypothesis testing on the observed paired differences in</p>
estimated performance.</a></li>
<li><a href='#performanceEstimation'>
<p>Estimate the predictive performance of modeling alternatives on</p>
different predictive tasks</a></li>
<li><a href='#PredTask-class'><p>Class &quot;PredTask&quot;</p></a></li>
<li><a href='#rankWorkflows'>
<p>Provide a ranking of workflows involved in an estimation process.</p></a></li>
<li><a href='#regressionMetrics'>
<p>Calculate some standard regression evaluation metrics of predictive performance</p></a></li>
<li><a href='#responseValues'>
<p>Obtain the target variable values of a prediction task</p></a></li>
<li><a href='#results2table'>
<p>Obtains a dplyr data frame table object containing all the results of</p>
an experiment</a></li>
<li><a href='#runWorkflow'>
<p>Run a workflow on a predictive task</p></a></li>
<li><a href='#signifDiffs'>
<p>Obtains a list with the set of paired differences that are statistically</p>
significant according to a p-value threshold
</p></a></li>
<li><a href='#smote'>
<p>SMOTE algorithm for unbalanced classification problems</p></a></li>
<li><a href='#standardPOST'>
<p>A function for applying post-processing steps to the predictions of a model</p></a></li>
<li><a href='#standardPRE'>
<p>A function for applying data pre-processing steps</p></a></li>
<li><a href='#standardWF'>
<p>A function implementing a standard workflow for prediction tasks</p></a></li>
<li><a href='#subset-methods'><p>Methods for Function <code>subset</code> in Package <span class="pkg">performanceEstimation</span></p></a></li>
<li><a href='#taskNames'>
<p>The prediction tasks involved in an estimation experiment</p></a></li>
<li><a href='#timeseriesWF'>
<p>A function implementing sliding and growing window standard workflows for time series</p>
forecasting tasks</a></li>
<li><a href='#topPerformer'>
<p>Obtain the workflow that best performed in terms of a metric on a task</p></a></li>
<li><a href='#topPerformers'>
<p>Obtain the best scores from a performance estimation experiment</p></a></li>
<li><a href='#Workflow-class'><p>Class &quot;Workflow&quot;</p></a></li>
<li><a href='#workflowNames'>
<p>The IDs of the workflows involved in an estimation experiment</p></a></li>
<li><a href='#workflowVariants'>
<p>Generate (parameter) variants of a workflow</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>An Infra-Structure for Performance Estimation of Predictive
Models</td>
</tr>
<tr>
<td>Description:</td>
<td>An infra-structure for estimating the predictive performance of
    predictive models. In this context, it can also be used to compare and/or select
    among different alternative ways of solving one or more predictive tasks. The
    main goal of the package is to provide a generic infra-structure to estimate
    the values of different metrics of predictive performance using different
    estimation procedures. These estimation tasks can be applied to any solutions
    (workflows) to the predictive tasks. The package provides easy to use standard
    workflows that allow the usage of any available R modeling algorithm together
    with some pre-defined data pre-processing steps and also prediction post-
    processing methods. It also provides means for addressing issues related with
    the statistical significance of the observed differences.</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.0</td>
</tr>
<tr>
<td>Depends:</td>
<td>R(&ge; 3.0), methods</td>
</tr>
<tr>
<td>Imports:</td>
<td>ggplot2 (&ge; 0.9.3), parallelMap (&ge; 1.3), parallel, tidyr (&ge;
0.4.1), dplyr (&ge; 0.4.3)</td>
</tr>
<tr>
<td>Date:</td>
<td>2016-10-12</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/ltorgo/performanceEstimation">https://github.com/ltorgo/performanceEstimation</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ltorgo/performanceEstimation/issues">https://github.com/ltorgo/performanceEstimation/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, devtools, e1071, DMwR, randomForest,
quantmod, nnet, mlbench, MASS</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2016-10-12 23:07:23 UTC; ltorgo</td>
</tr>
<tr>
<td>Author:</td>
<td>Luis Torgo [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Luis Torgo &lt;ltorgo@dcc.fc.up.pt&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2016-10-13 20:37:05</td>
</tr>
</table>
<hr>
<h2 id='bootEstimates'>
Performance estimation using (e0 or .632) bootstrap 
</h2><span id='topic+bootEstimates'></span>

<h3>Description</h3>

<p>This function obtains boostrap  estimates of
performance  metrics for a given predictive task and method to solve
it (i.e. a  workflow). The function is general in the sense that the
workflow function that the user provides as the solution to the task,
can implement or call whatever modeling technique the user wants.
</p>
<p>The function implements both e0 boostrap estimates as well as .632
boostrap. The selection of the type of boostrap is done through the 
<code>estTask</code> argument (check the help page of
<code><a href="#topic+Bootstrap-class">Bootstrap</a></code>). 
</p>
<p>Please note that most of the times you will not call this function
directly, though there is nothing wrong in doing it, but instead you
will use the function <code>performanceEstimation</code>, that allows you to
carry out performance estimation for multiple workflows on multiple tasks,
using some estimation method like for instance boostrap. Still, when you
simply want to have the boostrap estimate for one workflow on one task,
you may use this function directly. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootEstimates(wf,task,estTask,cluster)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootEstimates_+3A_wf">wf</code></td>
<td>

<p>an object of the class <code>Workflow</code> representing the
modeling approach to be evaluated on a certain task.
</p>
</td></tr>
<tr><td><code id="bootEstimates_+3A_task">task</code></td>
<td>

<p>an object of the class <code>PredTask</code> representing the
prediction task to be used in the evaluation.
</p>
</td></tr>
<tr><td><code id="bootEstimates_+3A_esttask">estTask</code></td>
<td>

<p>an object of the class <code><a href="#topic+EstimationTask-class">EstimationTask</a></code> indicating the metrics to
be estimated and the boostrap settings to use.
</p>
</td></tr>
<tr><td><code id="bootEstimates_+3A_cluster">cluster</code></td>
<td>

<p>an optional parameter that can either be <code>TRUE</code> or a
<code><a href="survival.html#topic+cluster">cluster</a></code>. In case of <code>TRUE</code> the function will run in
parallel and will internally setup the parallel back-end (defaulting
to using half of the cores in your local machine). You may also setup
outside your parallel back-end (c.f. <code><a href="parallel.html#topic+makeCluster">makeCluster</a></code>) and
then pass the resulting <code>cluster</code> object to this function using
this parameter. In case no value is provided for this parameter the
function will run sequentially.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The idea of this function is to carry out a bootstrap
experiment with the goal of obtaining reliable estimates of the
predictive performance of a certain modeling approach (denoted here as
a <em>workflow</em>) on a given predictive task. Two 
types of bootstrap estimates are implemented: i) e0 bootstrap and ii)
.632 bootstrap.  Bootstrap
estimates are obtained by averaging over a set of k scores each
obtained in the following way: i) draw a random sample with replacement
with the same size as the original data set; ii) obtain a model with
this sample; iii) test it and obtain the estimates for this run on the
observations of the original data set that were not used in the sample
obtained in step i). This process is repeated k times and the average
scores are the bootstrap estimates. The main difference between e0 and
.632 bootstrap is the fact that the latter tries to integrate the e0
estimate with the resubstitution estimate, i.e. when the model is
learned and tested on the full available data sample.
</p>
<p>Parallel execution of the estimation experiment is only recommended
for minimally large data sets otherwise you may actually increase the
computation time due to communication costs between the processes.
</p>


<h3>Value</h3>

<p>The result of the function is an object of class <code><a href="#topic+EstimationResults-class">EstimationResults</a></code>.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Bootstrap-class">Bootstrap</a></code>,
<code><a href="#topic+Workflow-class">Workflow</a></code>,
<code><a href="#topic+standardWF">standardWF</a></code>,
<code><a href="#topic+PredTask-class">PredTask</a></code>,
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code>,
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>,
<code><a href="#topic+hldEstimates">hldEstimates</a></code>,
<code><a href="#topic+loocvEstimates">loocvEstimates</a></code>,  
<code><a href="#topic+cvEstimates">cvEstimates</a></code>,
<code><a href="#topic+mcEstimates">mcEstimates</a></code>,
<code><a href="#topic+EstimationResults-class">EstimationResults</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

## Estimating the MSE of a SVM variant on the 
##  swiss data, using 50 repetitions of .632 bootstrap
library(e1071)
data(swiss)

## running the estimation experiment
res &lt;- bootEstimates(
  Workflow(wfID="svmC10G01",
           learner="svm",learner.pars=list(cost=10,gamma=0.1)
          ),
  PredTask(Infant.Mortality ~ .,swiss),
  EstimationTask("mse",method=Bootstrap(type=".632",nReps=50))
  )

## Check a summary of the results
summary(res)


## End(Not run)
</code></pre>

<hr>
<h2 id='Bootstrap-class'>Class &quot;Bootstrap&quot; </h2><span id='topic+Bootstrap'></span><span id='topic+Bootstrap-class'></span><span id='topic+show+2CBootstrap-method'></span>

<h3>Description</h3>

<p>	 This class of objects contains the information
describing a bootstrap experiment, i.e. its settings.</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>Bootstrap(...)</code>
providing the values for the class slots.
The objects contain information on the type of boostrap, the number of
repetitions, the random number generator seed
and <em>optionally</em> the 
concrete data splits to use on each iteration of the boostrap
experiment. Note that most of the times you will not supply these data
splits as the boostrap routines in this infra-structure will take care of
building them. Still, this allows you to  replicate some experiment
carried out with specific train/test splits. 
</p>


<h3>Slots</h3>


<dl>
<dt><code>type</code>:</dt><dd><p>Object of class <code>character</code> indicating
the type of boostrap estimates to use: &quot;e0&quot; (default) or &quot;.632&quot;. </p>
</dd>
<dt><code>nReps</code>:</dt><dd><p>Object of class <code>numeric</code> indicating
the number of repetitions of the bootstrap experiment (defaulting
to 200). </p>
</dd>
<dt><code>seed</code>:</dt><dd><p>Object of class <code>numeric</code> with the
random number generator seed (defaulting to 1234). </p>
</dd>
<dt><code>dataSplits</code>:</dt><dd><p>Object of class <code>list</code> 
containing the data splits to use on each bootstrap
repetition. Each element should be a list with two components:
<code>test</code> and <code>train</code>, on this order. Each of these is a
vector with the row ids to use as test and train sets of each
repetition of the bootstrap experiment. </p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code><a href="#topic+EstCommon-class">EstCommon</a></code>, directly.
Class <code><a href="#topic+EstimationMethod-class">EstimationMethod</a></code>, directly.
</p>


<h3>Methods</h3>


<dl>
<dt>show</dt><dd><p><code>signature(object = "Bootstrap")</code>: method used to
show the contents of a <code>Bootstrap</code> object.  </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MonteCarlo-class">MonteCarlo</a></code>,
<code><a href="#topic+LOOCV-class">LOOCV</a></code>,
<code><a href="#topic+CV-class">CV</a></code>,
<code><a href="#topic+Holdout-class">Holdout</a></code>,
<code><a href="#topic+EstimationMethod-class">EstimationMethod</a></code>,
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("Bootstrap")

s &lt;- Bootstrap(type=".632",nReps=400)
s

## Small example illustrating the format of user supplied data splits
s2 &lt;- Bootstrap(dataSplits=list(list(test=sample(1:150,50),train=sample(1:150,50)),
                                list(test=sample(1:150,50),train=sample(1:150,50)),
                                list(test=sample(1:150,50),train=sample(1:150,50))
                               ))
s2
s2@dataSplits
</code></pre>

<hr>
<h2 id='CDdiagram.BD'>
CD diagrams for the post-hoc Boferroni-Dunn test
</h2><span id='topic+CDdiagram.BD'></span>

<h3>Description</h3>

<p>This function obtains a Critical Difference (CD) diagram for the
post-hoc Bonferroni-Dunn test along the lines defined by Demsar (2006). These
diagrams provide an interesting visualization of the statistical
significance of the observed paired differences between a set of
workflows and a selected baseline wrokflow. They allow us to compare a
set of alternative workflows against this baseline and answer the
question whether the differences are statisticall y significant.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CDdiagram.BD(r, metric = names(r)[1])
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CDdiagram.BD_+3A_r">r</code></td>
<td>

<p>A list resulting from a call to <code><a href="#topic+pairedComparisons">pairedComparisons</a></code>
</p>
</td></tr>
<tr><td><code id="CDdiagram.BD_+3A_metric">metric</code></td>
<td>

<p>The metric for which the CD diagram will be obtained (defaults to the
first metric of the comparison).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Critical Difference (CD) diagrams are interesting sucint
visualizations of the results of a Bonferroni-Dunn post-hoc test that is
designed to check the statistical significance between the differences
in average rank of a set of workflows against a baseline workflow, on
a set of predictive tasks.
</p>
<p>In the resulting graph each workflow is represented by a colored
line. The X axis where the lines end represents the average rank position
of the respective workflow across all tasks. The null hypothesis is that
the average rank of a baseline workflow does not differ with
statistical significance (at some confidence level defined in the call
to <code><a href="#topic+pairedComparisons">pairedComparisons</a></code> that creates the object used to
obtain these graphs) from the average ranks of a set of
alternative workflows. An horizontal line connects the baseline
workflow with the alternative workflows for which we cannot reject
this hypothesis. This means that only the alternative workflows that
are not connect with the baseline can be considered as having an
average rank that is different from the one of the baseline with
statistical significance. To help spotting these differences the name
of the baseline workflow is shown in bold, and the names of the
alternative workflows whose difference is significant are shown in
italics. 
</p>


<h3>Value</h3>

<p>Nothing, the graph is draw on the current device.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Demsar, J. (2006) <em>Statistical Comparisons of Classifiers over
Multiple Data Sets</em>. Journal of Machine Learning Research, 7, 1-30.
</p>
<p>Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CDdiagram.Nemenyi">CDdiagram.Nemenyi</a></code>,
<code><a href="#topic+CDdiagram.BD">CDdiagram.BD</a></code>,  
<code><a href="#topic+signifDiffs">signifDiffs</a></code>,
<code><a href="#topic+metricNames">metricNames</a></code>,
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>,
<code><a href="#topic+topPerformers">topPerformers</a></code>,
<code><a href="#topic+topPerformer">topPerformer</a></code>,
<code><a href="#topic+rankWorkflows">rankWorkflows</a></code>,
<code><a href="#topic+metricsSummary">metricsSummary</a></code>,
<code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Estimating MSE for 3 variants of both
## regression trees and SVMs, on  two data sets, using one repetition
## of 10-fold CV
library(e1071)
data(iris)
data(Satellite,package="mlbench")
data(LetterRecognition,package="mlbench")


## running the estimation experiment
res &lt;- performanceEstimation(
           c(PredTask(Species ~ .,iris),
             PredTask(classes ~ .,Satellite,"sat"),
             PredTask(lettr ~ .,LetterRecognition,"letter")),
           workflowVariants(learner="svm",
                 learner.pars=list(cost=1:4,gamma=c(0.1,0.01))),
           EstimationTask(metrics=c("err","acc"),method=CV()))


## checking the top performers
topPerformers(res)

## now let us assume that we will choose "svm.v2" as our baseline
## carry out the paired comparisons
pres &lt;- pairedComparisons(res,"svm.v2")

## obtaining a CD diagram comparing all workflows against
## the baseline (defined in the previous call to pairedComparisons)
CDdiagram.BD(pres,metric="err")


## End(Not run)
</code></pre>

<hr>
<h2 id='CDdiagram.Nemenyi'>
CD diagrams for the post-hoc Nemenyi test
</h2><span id='topic+CDdiagram.Nemenyi'></span>

<h3>Description</h3>

<p>This function obtains a Critical Difference (CD) diagram for the
post-hoc Nemenyi test in the lines defined by Demsar (2006). These
diagrams provide an interesting visualization of the statistical
significance of the observed paired differences between a set of
workflows on a set of predictive tasks. They allow us to compare all
workflows against each other on these set of tasks and check the results
of all these paired comparisons.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CDdiagram.Nemenyi(r, metric = names(r)[1])
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CDdiagram.Nemenyi_+3A_r">r</code></td>
<td>

<p>A list resulting from a call to <code><a href="#topic+pairedComparisons">pairedComparisons</a></code>
</p>
</td></tr>
<tr><td><code id="CDdiagram.Nemenyi_+3A_metric">metric</code></td>
<td>

<p>The metric for which the CD diagram will be obtained (defaults to the
first metric of the comparison).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Critical Difference (CD) diagrams are interesting sucint
visualizations of the results of a Nemenyi post-hoc test that is
designed to check the statistical significance between the differences
in average rank of a set of workflows on a set of predictive tasks.
</p>
<p>In the resulting graph each workflow is represented by a colored
line. The X axis where the lines end represents the average rank position
of the respective workflow across all tasks. The null hypothesis is that
the average ranks of each pair of workflows to not differ with
statistical significance (at some confidence level defined in the call
to <code><a href="#topic+pairedComparisons">pairedComparisons</a></code> that creates the object used to
obtain these graphs). Horizontal lines connect the lines of the
workflows for which we cannot exclude the hypothesis that their average
ranks is equal. Any pair of workflows whose lines are not connected with an
horizontal line can be seen as having an average rank that is different
with statistical significance. On top of the graph an horizontal line is
shown with the required difference between the average ranks (known as
the critical difference) for two pair of workflows to be considered
significantly different. 
</p>


<h3>Value</h3>

<p>Nothing, the graph is draw on the current device.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Demsar, J. (2006) <em>Statistical Comparisons of Classifiers over
Multiple Data Sets</em>. Journal of Machine Learning Research, 7, 1-30.
</p>
<p>Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CDdiagram.Nemenyi">CDdiagram.Nemenyi</a></code>,
<code><a href="#topic+CDdiagram.BD">CDdiagram.BD</a></code>,  
<code><a href="#topic+signifDiffs">signifDiffs</a></code>,
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>,
<code><a href="#topic+metricNames">metricNames</a></code>,
<code><a href="#topic+topPerformers">topPerformers</a></code>,
<code><a href="#topic+topPerformer">topPerformer</a></code>,
<code><a href="#topic+rankWorkflows">rankWorkflows</a></code>,
<code><a href="#topic+metricsSummary">metricsSummary</a></code>,
<code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Estimating MSE for 3 variants of both
## regression trees and SVMs, on  two data sets, using one repetition
## of 10-fold CV
library(e1071)
data(iris)
data(Satellite,package="mlbench")
data(LetterRecognition,package="mlbench")


## running the estimation experiment
res &lt;- performanceEstimation(
           c(PredTask(Species ~ .,iris),
             PredTask(classes ~ .,Satellite,"sat"),
             PredTask(lettr ~ .,LetterRecognition,"letter")),
           workflowVariants(learner="svm",
                 learner.pars=list(cost=1:4,gamma=c(0.1,0.01))),
           EstimationTask(metrics=c("err","acc"),method=CV()))


## checking the top performers
topPerformers(res)

## now let us assume that we will choose "svm.v2" as our baseline
## carry out the paired comparisons
pres &lt;- pairedComparisons(res,"svm.v2")

## obtaining a CD diagram comparing all workflows against
## each other
CDdiagram.Nemenyi(pres,metric="err")


## End(Not run)
</code></pre>

<hr>
<h2 id='classificationMetrics'>
Calculate some standard classification evaluation metrics of predictive performance
</h2><span id='topic+classificationMetrics'></span>

<h3>Description</h3>

<p>This function is able to calculate a series of classification evaluation
statistics given two vectors: one with the true target variable values,
and the other with the predicted target variable values. Some of the
metrics may require additional information to be given (see Details section).		
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classificationMetrics(trues,preds,
            metrics=NULL,
            benMtrx=NULL,
            allCls=unique(c(levels(as.factor(trues)),levels(as.factor(preds)))),
            posClass=allCls[1],
            beta=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="classificationMetrics_+3A_trues">trues</code></td>
<td>

<p>A vector or factor with the true values of the target variable.
</p>
</td></tr>
<tr><td><code id="classificationMetrics_+3A_preds">preds</code></td>
<td>

<p>A  vector or factor with the predicted values of the target variable.
</p>
</td></tr>
<tr><td><code id="classificationMetrics_+3A_metrics">metrics</code></td>
<td>

<p>A vector with the names of the evaluation statistics to
calculate (see Details section). If none is indicated (default) it will
calculate all available metrics.
</p>
</td></tr>
<tr><td><code id="classificationMetrics_+3A_benmtrx">benMtrx</code></td>
<td>

<p>An optional cost/benefit matrix with numeric values representing the benefits (positive
values) and costs (negative values) for all combinations of predicted
and true values of the nominal target variable of the task. In this
context, the matrix should have the dimensions C x C, where C is
the number of possible class values of the classification task. Benefits
(positive values) should be on the diagonal of the matrix (situations
where the true and predicted values are equal, i.e. the model
predicted the correct class and thus should be rewarded for that),
whilst costs (negative values) should be on all positions outside of
the diagonal of the matrix (situations where the predicted value is
different from the true class value and thus the model should incur on
a cost for this wrong prediction). The function assumes the rows of the matrix are 
the predicted values while the columns are the true class values.
</p>
</td></tr>
<tr><td><code id="classificationMetrics_+3A_allcls">allCls</code></td>
<td>

<p>An optional vector with the possible values of the nominal target variable,
i.e. a vector with the classes of the problem. The default of this
parameter is to infer these values from the given vector of true and
predicted values
values. However, if these are small vectors (e.g. you are evaluating
your model on a small test set), it may happen that not all possible class
values occur in this vector and this will potentially create problems
in the sub-sequent calculations. Moreover, even if the vector is not
small, for highly unbalanced classification tasks, this problem may
still occur. In these contexts, it is safer to specifically indicate
the possible class values through this parameter.
</p>
</td></tr>
<tr><td><code id="classificationMetrics_+3A_posclass">posClass</code></td>
<td>

<p>An optional string with the name of the class (a value of the target
nominal variable) that should be considered the &quot;positive&quot; class. This
is used typically on two class problems where one of the classes is
more relevant than the other (the positive class). It will default to
the first value of the vector of possible classes (<code>allCls</code>).
</p>
</td></tr>
<tr><td><code id="classificationMetrics_+3A_beta">beta</code></td>
<td>

<p>An optional number for the value of the Beta parameter in the
calulation of the F-measure (defaults to 1 that corresponds to giving
equal relevance to precision and recall in the calculation of the F score).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the
following description of the currently available metrics we denote the
vector of true target variable values as t, the vector of predictions
by p, while n denotes the size of these two vectors, i.e. the number
of test cases. Furthermore we will denote the number of classes
(different values of the target nominal variable) as C. For problems
with only two classes, where one is considered the &quot;positive&quot; class we
will use some extra notation, namely: TP = #p == + &amp; t == + ; FP =
#p == + &amp; t == - ; TN = #p == - &amp; t == -; FN = #p == - &amp; t == +
; P = #t == + ; N = #t == -; Finally, for some descriptions
we will use the concept of confusion matrix (CM). This is a C x C
square matrix where entry CM[i,j] contains the number of times (for a
certain test set) some model predicted class i for a true class value
of j, i.e. rows of this matrix represent predicted values and columns
true values. We will also refer to cost/benefit matrices (or utility
matrices) that have the same structure (squared C x C) where entry
CB[i,j] represents the cost/benefit of predicting a class i for a true
class of j.
</p>
<p>The currently available classification performance metrics
are:
</p>
<p>&quot;acc&quot;: sum(I(t_i == p_i))/n, where I() is an indicator function given 1 if its
argument is true and 0 otherwise. Note that &quot;acc&quot; is a value in the interval [0,1], 1
corresponding to  all predictions being correct.
</p>
<p>&quot;err&quot;: the error rate, calculated as 1 - &quot;acc&quot;
</p>
<p>&quot;totU&quot;: this is a metric that takes into consideration not only
the fact that the predictions are correct or not, but also the costs or
benefits of these predictions. As mentioned above it assumes that the
user provides a fully specified cost/benefit matrix though parameter <code>benMtrx</code>, with
benefits corresponding to correct predictions, i.e. where t_i ==
p_i, while costs correspond to erroneous predictions. These matrices are C x C square matrices, where C is the
number of possible values of the nominal target variable (i.e. the
number of classes). The entry benMtrx[x, y] represents the utility (a cost if x != y) of the model predicting x for a true value of y. The diagonal of these matrices corresponds to the
correct predictions (t_i == p_i) and should have positive values
(benefits). The positions outside of the diagonal correspond to
prediction errors and should have negative values (costs). The &quot;totU&quot;
measures the total Utility (sum of the costs and benefits) of the
predictions of a classification model. It is calculated as
sum(CB[p_i,t_j] * CM[p_i,t_j) where CB is a cost/benefit matrix and CM
is a confusion matrix. 
</p>
<p>&quot;fpr&quot;: false positives rate, is a metric applicable to two classes
tasks that measures the proportion of times the model
forecasted a positive class when it should not and it is given by FP/N
</p>
<p>&quot;fnr&quot;: false negatives rate, is a metric applicable to two classes
tasks that measures the proportion of times the model
forecasted a negative class when it should not, and it is given by FN/P
</p>
<p>&quot;tpr&quot;: true positives rate, is a metric applicable to two classes
tasks that measures the proportion of times the model
forecasted a positive class for the positive test cases, and it is given by TP/P
</p>
<p>&quot;tnr&quot;: true negatives rate, is a metric applicable to two classes
tasks that measures the proportion of times the model
forecasted a negative class for the negative test cases, and it is
given by TN/N
</p>
<p>&quot;rec&quot;: recall, it is equal to the true positive rate (&quot;tpr&quot;)
</p>
<p>&quot;sens&quot;: sensitivity, it is equal to the true positive rate (&quot;tpr&quot;)
</p>
<p>&quot;spec&quot;: specificity, it is equal to the true negative rate (&quot;tnr&quot;)
</p>
<p>&quot;prec&quot;: precision, it is a metric  applicable to two classes
tasks that measures the proportion of times the model
forecasted a positive class and it was correct, and it is
given by TP/(TP+FP)
</p>
<p>&quot;ppv&quot;: predicted positive value, it is equal to the precision (&quot;prec&quot;)
</p>
<p>&quot;fdr&quot;: false discovery rate, it is a metric  applicable to two classes
tasks that is given by FP/(TP+FP)
</p>
<p>&quot;npv&quot;: negative predicted value, it is a metric  applicable to two classes
tasks that is given by TN/(TN+FN)
</p>
<p>&quot;for&quot;: false omission rate, it is a metric  applicable to two classes
tasks that is given by FN/(TN+FN)
</p>
<p>&quot;plr&quot;: positive likelihood ratio, it is a metric  applicable to two classes
tasks that is given by &quot;tpr&quot;/&quot;fpr&quot;
</p>
<p>&quot;nlr&quot;: negative likelihood ratio, it is a metric  applicable to two classes
tasks that is given by &quot;fnr&quot;/&quot;tnr&quot;
</p>
<p>&quot;dor&quot;: diagnostic odds ratio, it is a metric  applicable to two classes
tasks that is given by &quot;plr&quot;/&quot;nlr&quot;
</p>
<p>&quot;rpp&quot;: rate of positive predictions, it is a metric  applicable to two classes
tasks that measures the proportion of times the model
forecasted a positive class, and it is
given by (TP+FP)/N
</p>
<p>&quot;lift&quot;: lift, it is a metric  applicable to two classes
tasks and it is given by TP/P/(TP+FP) or equivalently TP/(P*TP+P*FP)
</p>
<p>&quot;F&quot;: the F-nmeasure, it is a metric  applicable to two classes
tasks that considers both the values of precision and recall weighed
by a parameter Beta (defaults to 1 corresponding to equal weights to both), and it is
given by (1+Beta^2)*(&quot;prec&quot; * &quot;rec&quot;) / ( (Beta^2 * &quot;prec&quot;) + &quot;rec&quot;)
</p>
<p>&quot;microF&quot;: micro-averaged F-measure, it is equal to accuracy (&quot;acc&quot;)
</p>
<p>&quot;macroF&quot;: macro-averaged F-measure, it is the average of the F-measure
scores calculated by making the positive class each of the possible
class values in turn
</p>
<p>&quot;macroRec&quot;: macro-averaged recall, it is the average recall by making
the positive class each of the possible  class values in turn  
</p>
<p>&quot;macroPrec&quot;: macro-averaged precision, it is the average precision by making
the positive class each of the possible  class values in turn  
</p>


<h3>Value</h3>

<p>A named vector with the calculated statistics.
</p>


<h3>Note</h3>

<p>1. In case you require &quot;totU&quot; to be calculated you must supply a
cost/benefit matrix through parameter <code>benMtrx</code>.
</p>
<p>2. If not all possible class values are present in the vector of true
values  in parameter <code>trues</code>, you should provide a vector
with all the possible class values in parameter <code>allCls</code>.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+regressionMetrics">regressionMetrics</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(DMwR)
## Calculating several statistics of a classification tree on the Iris data
data(iris)
idx &lt;- sample(1:nrow(iris),100)
train &lt;- iris[idx,]
test &lt;- iris[-idx,]
tree &lt;- rpartXse(Species ~ .,train)
preds &lt;- predict(tree,test,type='class')
## Calculate the  error rate
classificationMetrics(test$Species,preds)
## Calculate the all possible error metrics
classificationMetrics(test$Species,preds)
## Now trying calculating the utility of the predictions
cbM &lt;- matrix(c(10,-20,-20,-20,20,-10,-20,-10,20),3,3)
classificationMetrics(test$Species,preds,"totU",cbM)

## End(Not run)
</code></pre>

<hr>
<h2 id='ComparisonResults-class'>Class &quot;ComparisonResults&quot; </h2><span id='topic+ComparisonResults'></span><span id='topic+ComparisonResults-class'></span><span id='topic+plot+2CComparisonResults-method'></span><span id='topic+show+2CComparisonResults-method'></span><span id='topic+summary+2CComparisonResults-method'></span>

<h3>Description</h3>

<p>	 This is the main class that holds the results of
performance estimation experiments involving several alternative
workflows being applied and compared to several predictive tasks. For
each workflow and task, a set of predictive performance metrics are
estimated using some methodology and the results of this process are
stored in these objets. </p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form
<code>ComparisonResults(...)</code>. These object are essentially
a list of lists of objects of class
<code><a href="#topic+EstimationResults-class">EstimationResults</a></code>. The top level is named list
with has as many components as there are tasks. For each task there
will be a named sub-list containing as many components as there are alternative workflows. Each
of these components will contain and object of class
<code><a href="#topic+EstimationResults-class">EstimationResults</a></code> with the estimation results for
the particular workflow / task combination.
</p>


<h3>Methods</h3>


<dl>
<dt>plot</dt><dd><p><code>signature(x = "ComparisonResults", y = "missing")</code>: plots
the results of the experiments. It can result in an over-cluttered
graph if too many workflows/tasks/evaluation metrics - use the
subset method (see below) to overcome this.</p>
</dd>
<dt>show</dt><dd><p><code>signature(object = "ComparisonResults")</code>: shows the contents of an object in a proper way</p>
</dd>
<dt>subset</dt><dd><p><code>signature(x = "ComparisonResults")</code>: can be used to obtain
a smaller ComparisonResults object containing only a subset of the information
of the provided object. This method also accepts the arguments &quot;tasks&quot;,
&quot;workflows&quot; and &quot;metrics&quot;. All are vectors of numbers or names
that can be used to subset the original object. They default to all values of each dimension. See &quot;methods?subset&quot; for further details.</p>
</dd>
<dt>summary</dt><dd><p><code>signature(object = "ComparisonResults")</code>: provides a
summary of the performance estimation experiment. </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+performanceEstimation">performanceEstimation</a></code>,
<code><a href="#topic+pairedComparisons">pairedComparisons</a></code>,
<code><a href="#topic+rankWorkflows">rankWorkflows</a></code>,
<code><a href="#topic+topPerformers">topPerformers</a></code>,
<code><a href="#topic+metricsSummary">metricsSummary</a></code>,
<code><a href="#topic+mergeEstimationRes">mergeEstimationRes</a></code>  
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("ComparisonResults")
## Not run: 
## Estimating MAE, MSE, RMSE and MAPE for 3 variants of both
## regression trees and SVMs, on  two data sets, using one repetition
## of 10-fold CV
library(e1071)
library(DMwR)
data(swiss)
data(mtcars)

## running the estimation experiment
res &lt;- performanceEstimation(
  c(PredTask(Infant.Mortality ~ .,swiss),PredTask(mpg ~ ., mtcars)),
  c(workflowVariants(learner="svm",
                     learner.pars=list(cost=c(1,10),gamma=c(0.01,0.5))),
    workflowVariants(learner="rpartXse",
                     learner.pars=list(se=c(0,0.5,1)))
  ),
  EstimationTask(metrics=c("mae","mse","rmse","mape"),method=CV())
  )

## Check a summary of the results
summary(res)

topPerformers(res)

summary(subset(res,metrics="mse"))
summary(subset(res,metrics="mse",partial=FALSE))
summary(subset(res,workflows="v1"))

## End(Not run)

</code></pre>

<hr>
<h2 id='CV-class'>Class &quot;CV&quot;</h2><span id='topic+CV'></span><span id='topic+CV-class'></span><span id='topic+show+2CCV-method'></span>

<h3>Description</h3>

<p>	This class of objects contains the information
describing a cross validation experiment, i.e. its settings.</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>CV(...)</code>
providing the values for the class slots. 
These objects include information on the number of repetitions of the
cross validation (CV) experiment, the number of folds, the random number generator seed,
whether the sampling should or not be stratified and <em>optionally</em>, the
concrete data splits to use on each repetition and iteration of the CV
experiment. Note that most of the times you will not supply these data
splits as the CV routines in this infra-structure will take care of
building them. Still, this allows you to  replicate some experiment
carried out with specific train/test splits.
</p>


<h3>Slots</h3>


<dl>
<dt><code>nReps</code>:</dt><dd><p>Object of class <code>numeric</code> indicating
the number of repetitions of the N folds CV experiment (defaulting
to 1). </p>
</dd>
<dt><code>nFolds</code>:</dt><dd><p>Object of class <code>numeric</code> with the
number of folds on each CV experiment (defaulting to 10). </p>
</dd>
<dt><code>strat</code>:</dt><dd><p>Object of class <code>logical</code> indicating
whether the sampling should or not be stratified (defaulting to FALSE). </p>
</dd>
<dt><code>seed</code>:</dt><dd><p>Object of class <code>numeric</code> with the
random number generator seed (defaulting to 1234). </p>
</dd>
<dt><code>dataSplits</code>:</dt><dd><p>Object of class <code>list</code> 
containing the data splits to use on each repetition of a
k-folds CV experiment (defaulting to <code>NULL</code>). This list
should contain <code>nReps x nFolds</code> elements. Each element should be a
vector with the row ids of the test set of the respective
iteration. For instance, on a 3 x 10-fold CV experiment the 10th
element should contain the ids of the test cases of the 10th fold
of the first repetition and the 11th element the ids of the test
cases on the 1st fold of the 2nd repetition. On all these
iterations the training set will be formed by the ids not
appearing in the test set. </p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code><a href="#topic+EstCommon-class">EstCommon</a></code>, directly.
Class <code><a href="#topic+EstimationMethod-class">EstimationMethod</a></code>, directly.
</p>


<h3>Methods</h3>


<dl>
<dt>show</dt><dd><p><code>signature(object = "CV")</code>: method used to
show the contents of a <code>CV</code> object. </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MonteCarlo-class">MonteCarlo</a></code>,
<code><a href="#topic+LOOCV-class">LOOCV</a></code>,
<code><a href="#topic+Bootstrap-class">Bootstrap</a></code>,
<code><a href="#topic+Holdout-class">Holdout</a></code>,
<code><a href="#topic+EstimationMethod-class">EstimationMethod</a></code>,
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("CV")

## the defaults (1 x 10-fold CV)
s &lt;- CV()

## stratified 2 x 5-fold CV
s1 &lt;- CV(nReps=2,nFolds=5,strat=TRUE)

## Small example illustrating the format of user supplied data splits.
## This could be a 3-fold CV process of a data set with 30 cases
s2 &lt;- CV(dataSplits=list(1:10,11:20,21:30))
s2

</code></pre>

<hr>
<h2 id='cvEstimates'>
Performance estimation using cross validation
</h2><span id='topic+cvEstimates'></span>

<h3>Description</h3>

<p>This function obtains cross validation estimates of performance
metrics for a given predictive task and method to solve it (i.e. a
workflow). The function is general in the sense that the workflow
function that the user provides as the solution to the task can
implement or call whatever modeling technique the user wants.
</p>
<p>The function implements N x K-fold cross validation (CV)
estimation. Different settings concerning this methodology are available
through the argument <code>estTask</code> (check the help page of
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code> and <code><a href="#topic+CV-class">CV</a></code>).
</p>
<p>Please note that most of the times you will not call this function
directly (though there is nothing wrong in doing it) but instead you
will use the function <code><a href="#topic+performanceEstimation">performanceEstimation</a></code>, that allows you to
carry out performance estimation for multiple workflows on multiple tasks,
using the estimation method you want (e.g. cross validation). Still, when you
simply want to have the CV estimate of one workflow on one task,
you may prefer to use this function directly. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvEstimates(wf,task,estTask,cluster)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvEstimates_+3A_wf">wf</code></td>
<td>

<p>an object of the class <code><a href="#topic+Workflow-class">Workflow</a></code> representing the
modeling approach to be evaluated on a certain task.
</p>
</td></tr>
<tr><td><code id="cvEstimates_+3A_task">task</code></td>
<td>

<p>an object of the class <code><a href="#topic+PredTask-class">PredTask</a></code> defining the
prediction task for which we want estimates.
</p>
</td></tr>
<tr><td><code id="cvEstimates_+3A_esttask">estTask</code></td>
<td>

<p>an object of the class <code><a href="#topic+EstimationTask-class">EstimationTask</a></code> indicating the metrics to
be estimated and the cross validation settings to use.
</p>
</td></tr>
<tr><td><code id="cvEstimates_+3A_cluster">cluster</code></td>
<td>

<p>an optional parameter that can either be <code>TRUE</code> or a
<code><a href="survival.html#topic+cluster">cluster</a></code>. In case of <code>TRUE</code> the function will run in
parallel and will internally setup the parallel back-end (defaulting
to using half of the cores in your local machine). You may also setup
outside your parallel back-end (c.f. <code><a href="parallel.html#topic+makeCluster">makeCluster</a></code>) and
then pass the resulting <code>cluster</code> object to this function using
this parameter. In case no value is provided for this parameter the
function will run sequentially.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The idea of this function is to carry out a cross validation
experiment with the goal of obtaining reliable estimates of the
predictive performance of a certain approach to a predictive
task. This approach (denoted here as a <em>workflow</em>) will be evaluated on
the given predictive task using some user-selected  metrics,
and this function will 
provide k-fold cross validation estimates of the true values of these
evaluation metrics.  k-Fold cross validation
estimates are obtained by randomly partitioning the given data set into k
equal size sub-sets. Then a learn+test process is repeated k times. At
each iteration one of the k partitions is left aside as test set and
the model is obtained with a training set formed by the remaining k-1
partitions. The process is repeated leaving each time one of the k
partitions aside as test set. In the end the average of the k scores
obtained on each iteration is the cross validation estimate.
</p>
<p>Parallel execution of the estimation experiment is only recommended
for minimally large data sets otherwise you may actually increase the
computation time due to communication costs between the processes.
</p>


<h3>Value</h3>

<p>The result of the function is an object of class <code><a href="#topic+EstimationResults-class">EstimationResults</a></code>.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CV-class">CV</a></code>,
<code><a href="#topic+Workflow-class">Workflow</a></code>,
<code><a href="#topic+standardWF">standardWF</a></code>,
<code><a href="#topic+PredTask-class">PredTask</a></code>,
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code>,
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>,
<code><a href="#topic+hldEstimates">hldEstimates</a></code>,
<code><a href="#topic+bootEstimates">bootEstimates</a></code>,  
<code><a href="#topic+loocvEstimates">loocvEstimates</a></code>,
<code><a href="#topic+mcEstimates">mcEstimates</a></code>,
<code><a href="#topic+EstimationResults-class">EstimationResults</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

## Estimating the mean squared error  of svm on the swiss data,
## using two repetitions of 10-fold CV
library(e1071)
data(swiss)

## Now the evaluation
eval.res &lt;- cvEstimates(
             Workflow(wf="standardWF", wfID="mySVMtrial",
                      learner="svm", learner.pars=list(cost=10,gamma=0.1)
                     ),
             PredTask(Infant.Mortality ~ ., swiss),
             EstimationTask(metrics="mse",method=CV(nReps=2,nFolds=10))
                       )

## Check a summary of the results
summary(eval.res)


## An example with a user-defined workflow function implementing a
## simple approach using linear regression models but also containing
## some data-preprocessing and well as results post-processing.
myLM &lt;- function(form,train,test,k=10,.outModel=FALSE) {
    require(DMwR)
    ## fill-in NAs on both the train and test sets
    ntr &lt;- knnImputation(train,k)
    nts &lt;- knnImputation(test,k,distData=train)
    ## obtain a linear regression model and simplify it
    md &lt;- lm(form,ntr)
    md &lt;- step(md)
    ## get the model predictions
    p &lt;- predict(md,nts)
    ## post-process the predictions (this is an example assuming the target
    ## variable is always positive so we change negative predictions into 0)
    p &lt;- ifelse(p &lt; 0,0,p)
    ## now get the final return object
    res &lt;- list(trues=responseValues(form,nts), preds=p)
    if (.outModel) res &lt;- c(res,list(model=m))
    res
}

## Now for the CV estimation 
data(algae,package="DMwR")
eval.res2 &lt;- cvEstimates(
             Workflow(wf="myLM",k=5),
             PredTask(a1 ~ ., algae[,1:12],"alga1"),
             EstimationTask("mse",method=CV()))

## Check a summary of the results
summary(eval.res2)

##
## Parallel execution example
##
## Comparing the time of sequential and parallel execution
## using half of the cores of the local machine
##
data(Satellite,package="mlbench")
library(e1071)
system.time({p &lt;- cvEstimates(Workflow(learner="svm"),
                              PredTask(classes ~ .,Satellite),
                              EstimationTask("err",Boot(nReps=10)),
                              cluster=TRUE)})[3]
system.time({np &lt;- cvEstimates(Workflow(learner="svm"),
                               PredTask(classes ~ .,Satellite),
                               EstimationTask("err",Boot(nReps=10)))})[3]


## End(Not run)
</code></pre>

<hr>
<h2 id='EstCommon-class'>Class <code>"EstCommon"</code></h2><span id='topic+EstCommon-class'></span>

<h3>Description</h3>

<p>An auxiliar class defining slots common to all experimental settings classes.
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("EstCommon", ...)</code>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>seed</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>dataSplits</code>:</dt><dd><p>Object of class <code>"OptMatrix"</code> </p>
</dd>
</dl>



<h3>Methods</h3>

<p>No methods defined with class &quot;EstCommon&quot; in the signature.
</p>


<h3>Author(s)</h3>

<p>Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a></p>


<h3>References</h3>

<p>Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CV-class">CV</a></code>,
<code><a href="#topic+MonteCarlo-class">MonteCarlo</a></code>,
<code><a href="#topic+LOOCV-class">LOOCV</a></code>,
<code><a href="#topic+Bootstrap-class">Bootstrap</a></code>,
<code><a href="#topic+Holdout-class">Holdout</a></code>,
<code><a href="#topic+EstimationMethod-class">EstimationMethod</a></code>,
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("EstCommon")
</code></pre>

<hr>
<h2 id='EstimationMethod-class'>Class &quot;EstimationMethod&quot;  </h2><span id='topic+EstimationMethod'></span><span id='topic+EstimationMethod-class'></span>

<h3>Description</h3>

<p>	 This is a class union formed by the classes CvSettings,
McSettings, HldSettings, LoocvSettings and BootSettings</p>


<h3>Objects from the Class</h3>

<p>A virtual Class: No objects may be created from it.</p>


<h3>Methods</h3>

<p>No methods defined with class &quot;EstimationMethod&quot; in the signature.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CV-class">CV</a></code>,
<code><a href="#topic+MonteCarlo-class">MonteCarlo</a></code>,
<code><a href="#topic+LOOCV-class">LOOCV</a></code>,
<code><a href="#topic+Holdout-class">Holdout</a></code>,
<code><a href="#topic+Bootstrap-class">Bootstrap</a></code>,
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("EstimationMethod")
</code></pre>

<hr>
<h2 id='EstimationResults-class'>Class &quot;EstimationResults&quot; </h2><span id='topic+EstimationResults'></span><span id='topic+EstimationResults-class'></span><span id='topic+plot+2CEstimationResults-method'></span><span id='topic+summary+2CEstimationResults-method'></span><span id='topic+show+2CEstimationResults-method'></span>

<h3>Description</h3>

<p>This is the class of the objects storing the results of
estimating the  performance of a workflow on a predictive task
using some estimation method.</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>EstimationResults(...)</code>
providing the values for the class slots. The objects contain
information on the predictive task, the workflow, the estimation
task, the metric scores and optionally also information on results
obtained at each iteration of the estimation process.
</p>


<h3>Slots</h3>


<dl>
<dt><code>task</code>:</dt><dd><p>Object of class <code>PredTask</code>  </p>
</dd>
<dt><code>workflow</code>:</dt><dd><p>Object of class <code>Workflow</code>  </p>
</dd>
<dt><code>estTask</code>:</dt><dd><p>Object belonging to class <code>EstimationTask</code> </p>
</dd>
<dt><code>iterationsScores</code>:</dt><dd><p>Object of class <code>matrix</code> </p>
</dd>
<dt><code>iterationsInfo</code>:</dt><dd><p>Object of class <code>list</code> </p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>plot</dt><dd><p><code>signature(x = "EstimationResults", y = "missing")</code>: method
used to visualize the results of the estimation process. </p>
</dd>
<dt>show</dt><dd><p><code>signature(object = "EstimationResults")</code>: shows the contents of an object in a proper way</p>
</dd>
<dt>summary</dt><dd><p><code>signature(object = "EstimationResults")</code>: method used to
obtain a summary of the results of the estimation process. </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+EstimationMethod-class">EstimationMethod</a></code>,
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code>,
<code><a href="#topic+PredTask-class">PredTask</a></code>,
<code><a href="#topic+Workflow-class">Workflow</a></code>,
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("EstimationResults")

## Not run: 
library(e1071)
data(swiss)

## Estimating the MAE and NMSE of an SVM on the swiss task
eval.res &lt;- cvEstimates(
                 Workflow(learner="svm",learner.pars=list(cost=10,gamma=0.1)),
                 PredTask(Infant.Mortality ~ .,swiss),
                 EstimationTask(metrics=c("mae","nmse"),method=CV(nReps=2))
                       )

## Check a summary of the results
summary(eval.res)


## End(Not run)
</code></pre>

<hr>
<h2 id='estimationSummary'>
Obtain a set of descriptive statistics of the scores of a workflow on
a task
</h2><span id='topic+estimationSummary'></span>

<h3>Description</h3>

<p>This function provides a set of descriptive statistics for each
evaluation metric that is estimated on a performance estimation
comparison. These statistics are obtained for a particular workflow,
and for one of the prediction tasks involved in the experiment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimationSummary(results,workflow,task)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimationSummary_+3A_results">results</code></td>
<td>

<p>This is a <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> object (type &quot;class?ComparisonResults&quot; for details)
that contains the results of a performance estimation  experiment obtained
through the <code>performanceEstimation()</code> function. 
</p>
</td></tr>
<tr><td><code id="estimationSummary_+3A_workflow">workflow</code></td>
<td>

<p>A string with the ID of a workflow (it can also be an integer).
</p>
</td></tr>
<tr><td><code id="estimationSummary_+3A_task">task</code></td>
<td>

<p>A string with the ID of a task (it can also be an integer).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a matrix with the rows representing summary
statistics of the scores obtained by the model on the different
iterations, and the columns representing the evaluation statistics
estimated in the experiment.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getScores">getScores</a></code>, <code><a href="#topic+performanceEstimation">performanceEstimation</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Estimating MSE for 3 variants of both
## regression trees and SVMs, on  two data sets, using one repetition
## of 10-fold CV
library(e1071)
data(swiss)

## running the estimation experiment
res &lt;- performanceEstimation(
  PredTask(Infant.Mortality ~ .,swiss),
  workflowVariants(learner="svm",
                   learner.pars=list(cost=c(1,10),gamma=c(0.01,0.5))),
  EstimationTask("mse",method=CV(nReps=2,nFolds=5))
  )

## Get the summary of the estimations of svm.v2 on swiss
estimationSummary(res,"svm.v2","swiss.Infant.Mortality")


## End(Not run)
</code></pre>

<hr>
<h2 id='EstimationTask-class'>Class <code>"EstimationTask"</code></h2><span id='topic+EstimationTask'></span><span id='topic+EstimationTask-class'></span><span id='topic+show+2CEstimationTask-method'></span>

<h3>Description</h3>

<p>This class of objects contains the information describing an
estimation task.
</p>


<h3>Details</h3>

<p>In case you are providing your own user-defined evaluator functions
(through parameters <code>evaluator</code> anbd <code>evaluator.pars</code>) you
need to follow some protocol in defining these functions so that the
package may correctly call them during the execution of the estimation
experiments. This protocol depends on the output of the workflows you 
plan to evaluate with your use-defined function. Standard workflows 
(<code><a href="#topic+standardWF">standardWF</a></code> or <code><a href="#topic+timeseriesWF">timeseriesWF</a></code>) will return 
at least a vector named <code>trues</code> with  the true values of the test 
cases and the vector named <code>preds</code> with the respective predicted 
values (in this order). This means that your evaluator function should 
assume that it will be called with these two vectors as the first two 
arguments. However, if you are not using the standard workflows, you 
have more flexibility. In effect, user-defined workflows return whatever 
the author wants them to return (unless they are going to be evaluated 
using either <code><a href="#topic+classificationMetrics">classificationMetrics</a></code> or 
<code><a href="#topic+regressionMetrics">regressionMetrics</a></code>, that require the vectors of true and 
predicted values). This means that in the most flexible case where you 
have your own user-defined workflow function and your user-defined evaluator 
function, you can use whatever parameters you want. The only thing you need 
to worry is to be aware that your user-defined evaluator function will 
be called with whatever your user-defined workflow has returned as result 
of its execution. Your user-defined evaluator function should calculate 
whatever metrics are indicated through the parameter <code>stats</code> that 
is a vector of strings. In case the slot <code>trainReq</code> is <code>TRUE</code> 
then the user-defined evaluator function should also have a parameter
named <code>train.y</code> that will &quot;receive&quot; the values of the
target variable on the training set. The remaining parameters of the
user-defined function can be freely defined by you and their values
will be specified through the contents of the <code>evaluator.pars</code>
list.
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form
<code>EstimationTask(...)</code>
providing the values for the class slots. These objects contain
information on the metrics to be estimated, as well as on the estimation
method to use to obtain the estimates. Moreover, in case you want to use
metrics not currently implemented by this package you can also provide
the name of a function (and its parameters) that will be called to
calculate these metrics.
</p>


<h3>Slots</h3>


<dl>
<dt><code>metrics</code>:</dt><dd><p>An optional vector of objects of class
<code>character</code> containing the names of the metrics to be
estimated. These can be any of the metrics provided by the
functions <code><a href="#topic+classificationMetrics">classificationMetrics</a></code> and
<code><a href="#topic+regressionMetrics">regressionMetrics</a></code> or &quot;trTime&quot;, &quot;tsTime&quot; or &quot;totTime&quot; for
training, testing and total time, respectively. You may also
provide the name of any other metrics, but in that case you need
to use the slots <code>evaluator</code> and <code>evaluator.pars</code> to
indicate the function to be use to calculate them. If you do not supply 
the name of any metric, all metrics of the used evaluator function will
be calculated.</p>
</dd>
<dt><code>method</code>:</dt><dd><p>Object of class <code><a href="#topic+EstimationMethod-class">EstimationMethod</a></code>
containing the estimation method and its settings to be used to
obtain the estimates of the metrics (defaulting to <code>CV()</code>). </p>
</dd>
<dt><code>evaluator</code>:</dt><dd><p>An optional object of class
<code>character</code> containing the name of a function to be used to
calculate the specified metrics. It will default to either
<code><a href="#topic+classificationMetrics">classificationMetrics</a></code> or
<code><a href="#topic+regressionMetrics">regressionMetrics</a></code> depending on the type of prediction
task.</p>
</dd>
<dt><code>evaluator.pars</code>:</dt><dd><p>An optional  list containing
the parameters to be passed to the function calculating the metrics.</p>
</dd>
<dt><code>trainReq</code>:</dt><dd><p>An optional  logical value indicating
whether the metrics to be calculated require that the training
data is supplied (defaulting to <code>FALSE</code>). Note that if the
user selects any of the metrics &quot;nmse&quot;, &quot;nmae&quot; or &quot;theil&quot; this
will be set to <code>TRUE</code>.</p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>show</dt><dd><p><code>signature(object = "EstimationTask")</code>: method used to
show the contents of a <code>EstimationTask</code> object. </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> 
</p>


<h3>References</h3>

<p>Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MonteCarlo-class">MonteCarlo</a></code>,
<code><a href="#topic+CV-class">CV</a></code>,
<code><a href="#topic+LOOCV-class">LOOCV</a></code>,
<code><a href="#topic+Bootstrap-class">Bootstrap</a></code>,
<code><a href="#topic+Holdout-class">Holdout</a></code>,
<code><a href="#topic+EstimationMethod-class">EstimationMethod</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("EstimationTask")

## Estimating Mean Squared Error using 10-fold cross validation
et &lt;- EstimationTask(metrics="mse")
et

## Estimating Accuracy and Total Time (train+test times) using 3
## repetitions of holdout with 20% of the cases used for testing. 
EstimationTask(metrics=c("acc","totTime"),method=Holdout(nReps=3,hldSz=0.2))

## An example with a user-defined metric: the average differences between true
## predicted values raised to a certain power.

## First we define the function that calculates this metric. It
## needs to have 'trues' and 'preds' as the first two arguments if we
## want it to be usable by standard workflows
powErr &lt;- function(trues,preds,metrics,pow=3) {
     if (metrics != "pow.err") stop("Unable to calculate that metric!")
     c(pow.err = mean((trues-preds)^pow))
}

## Now the estimation task (10-fold cv in this example)
EstimationTask(metrics="pow.err", method=CV(), 
               evaluator="powErr", evaluator.pars=list(pow=4))
</code></pre>

<hr>
<h2 id='getIterationsInfo'>
Obtaining the information returned by a workflow when applied to a task,
on a particular iteration of the estimation process or on all iterations
</h2><span id='topic+getIterationsInfo'></span>

<h3>Description</h3>

<p>In the estimation process workflows are applied many times to different
train+test samples of each task. We call these repetitions, the
iterations of the estimation process. On each of these executions of the
workflows, they typically return not only the predictions for the test
set but also some extra information, in the form of a list. This
function allows you to inspect this extra information 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getIterationsInfo(obj, workflow = 1, task = 1, rep, fold, it)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getIterationsInfo_+3A_obj">obj</code></td>
<td>

<p>A <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> object
</p>
</td></tr>
<tr><td><code id="getIterationsInfo_+3A_workflow">workflow</code></td>
<td>

<p>A string with the ID of a workflow (it can also be an integer). It
defaults to 1 (the first workflow of the estimation experiment)
</p>
</td></tr>
<tr><td><code id="getIterationsInfo_+3A_task">task</code></td>
<td>

<p>A string with the ID of a task (it can also be an integer). It
defaults to 1 (the first task of the estimation experiment)
</p>
</td></tr>
<tr><td><code id="getIterationsInfo_+3A_rep">rep</code></td>
<td>

<p>An integer representing the repetition, which allows you to identify the iteration you want to
inspect. You need to specify either this argument together with the
argument <code>fold</code>, or only the argument <code>it</code>
</p>
</td></tr>
<tr><td><code id="getIterationsInfo_+3A_fold">fold</code></td>
<td>

<p>An integer representing the fold, which allows you to identify the iteration you want to
inspect. You need to specify either this argument together with the
argument <code>rep</code>, or only the argument <code>it</code>
</p>
</td></tr>
<tr><td><code id="getIterationsInfo_+3A_it">it</code></td>
<td>

<p>An integer representing the iteration you want to
inspect. Alternatively, for cross validation experiments, you may
instead specify the repetition id and the fold id (arguments
<code>rep</code> and <code>fold</code>, respectivily)
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the information returned by the workflow on the selected
iteration or a list containing as many components as there are
iterations (i.e. a list of lists) in case you have opted for obtaining
all iterations results
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getIterationsPreds">getIterationsPreds</a></code>,
<code><a href="#topic+getScores">getScores</a></code>,
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Estimating MSE for 3 variants of both
## regression trees and SVMs, on  two data sets, using one repetition
## of 10-fold CV
library(e1071)
data(swiss)

## running the estimation experiment
res &lt;- performanceEstimation(
  PredTask(Infant.Mortality ~ .,swiss),
  workflowVariants(learner="svm",
                   learner.pars=list(cost=c(1,10),gamma=c(0.01,0.5))),
  EstimationTask("mse",method=CV(nReps=2,nFolds=5))
  )

## Get the iterations scores of svm.v2 on swiss
getIterationsInfo(res,"svm.v2","swiss.Infant.Mortality",rep=1,fold=2)
## this would get the same
getIterationsInfo(res,"svm.v2","swiss.Infant.Mortality",it=2)

getIterationsInfo(res,"svm.v2","swiss.Infant.Mortality",rep=2,fold=3)
## this would get the same
getIterationsInfo(res,"svm.v2","swiss.Infant.Mortality",it=8)

## Get the results of all iterations
getIterationsInfo(res,"svm.v1","swiss.Infant.Mortality")


## End(Not run)
</code></pre>

<hr>
<h2 id='getIterationsPreds'>
Obtaining the predictions returned by a workflow when applied to a task,
on a particular iteration of the estimation process, or on all iterations
</h2><span id='topic+getIterationsPreds'></span>

<h3>Description</h3>

<p>In the estimation process workflows are applied many times to different
train+test samples of each task. We call these repetitions, the
iterations of the estimation process. On each of these executions of the
workflows they must return the predictions for the test set. This
function allows you to obtain these predictions. The function also
allows you to obtain the predictions on all iterations, instead of a
single iteration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getIterationsPreds(obj, workflow = 1, task = 1, rep, fold, it, predComp="preds")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getIterationsPreds_+3A_obj">obj</code></td>
<td>

<p>A <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> object
</p>
</td></tr>
<tr><td><code id="getIterationsPreds_+3A_workflow">workflow</code></td>
<td>

<p>A string with the ID of a workflow (it can also be an integer). It
defaults to 1 (the first workflow of the estimation experiment)
</p>
</td></tr>
<tr><td><code id="getIterationsPreds_+3A_task">task</code></td>
<td>

<p>A string with the ID of a task (it can also be an integer). It
defaults to 1 (the first task of the estimation experiment)
</p>
</td></tr>
<tr><td><code id="getIterationsPreds_+3A_rep">rep</code></td>
<td>

<p>An integer representing the repetition, which allows you to identify the iteration you want to
inspect. You need to specify either this argument together with the
argument <code>fold</code>, or only the argument <code>it</code>
</p>
</td></tr>
<tr><td><code id="getIterationsPreds_+3A_fold">fold</code></td>
<td>

<p>An integer representing the fold, which allows you to identify the iteration you want to
inspect. You need to specify either this argument together with the
argument <code>rep</code>, or only the argument <code>it</code>
</p>
</td></tr>
<tr><td><code id="getIterationsPreds_+3A_it">it</code></td>
<td>

<p>An integer representing the iteration you want to
inspect. Alternatively, for cross validation experiments, you may
instead specify the repetition id and the fold id (arguments
<code>rep</code> and <code>fold</code>, respectivily)
</p>
</td></tr>
<tr><td><code id="getIterationsPreds_+3A_predcomp">predComp</code></td>
<td>

<p>A string with the name of the component of the list returned by the
workflow that contains the predictions (it defaults to &quot;preds&quot;)
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result is either a vector of the predictions of a particular
iteration or a matrix with the predictions on all iterations
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getScores">getScores</a></code>,
<code><a href="#topic+getIterationsInfo">getIterationsInfo</a></code>,
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Estimating MSE for 3 variants of both
## regression trees and SVMs, on  two data sets, using one repetition
## of 10-fold CV
library(e1071)
data(swiss)

## running the estimation experiment
res &lt;- performanceEstimation(
  PredTask(Infant.Mortality ~ .,swiss),
  workflowVariants(learner="svm",
                   learner.pars=list(cost=c(1,10),gamma=c(0.01,0.5))),
  EstimationTask("mse",method=CV(nReps=2,nFolds=5))
  )

## Get the iterations scores of svm.v2 on swiss
getIterationsPreds(res,"svm.v2","swiss.Infant.Mortality",rep=1,fold=2)
## this would get the same
getIterationsPreds(res,"svm.v2","swiss.Infant.Mortality",it=2)

getIterationsPreds(res,"svm.v2","swiss.Infant.Mortality",rep=2,fold=3)
## this would get the same
getIterationsPreds(res,"svm.v2","swiss.Infant.Mortality",it=8)

## Get the results of all iterations
getIterationsPreds(res,"svm.v1","swiss.Infant.Mortality")


## End(Not run)
</code></pre>

<hr>
<h2 id='getScores'>
Obtaining the metric scores on the different iterations for a workflow /
task combination
</h2><span id='topic+getScores'></span>

<h3>Description</h3>

<p>With this function we can obtain the different metric scores obtained
by a given workflow on a given task, in the different iterations of a
performance estimation experiment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getScores(results, workflow, task)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getScores_+3A_results">results</code></td>
<td>

<p>A <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> object
</p>
</td></tr>
<tr><td><code id="getScores_+3A_workflow">workflow</code></td>
<td>

<p>A string with the ID of a workflow
</p>
</td></tr>
<tr><td><code id="getScores_+3A_task">task</code></td>
<td>

<p>A string with the ID of a predictive task
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix with as many rows as there are iterations and as many columns
as there are metrics being estimated in the experiment
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+taskNames">taskNames</a></code>,<code><a href="#topic+workflowNames">workflowNames</a></code>,<code><a href="#topic+metricNames">metricNames</a></code>,<code><a href="#topic+estimationSummary">estimationSummary</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Estimating MSE for 3 variants of both
## regression trees and SVMs, on  two data sets, using one repetition
## of 10-fold CV
library(e1071)
library(DMwR)
data(swiss)
data(mtcars)

## running the estimation experiment
res &lt;- performanceEstimation(
  c(PredTask(Infant.Mortality ~ .,swiss),PredTask(mpg ~ ., mtcars)),
  c(workflowVariants(learner="svm",
                     learner.pars=list(cost=c(1,10),gamma=c(0.01,0.5))),
    workflowVariants(learner="rpartXse",
                     learner.pars=list(se=c(0,0.5,1)))
  ),
  EstimationTask("mse")
  )

## Get the iterations scores of svm.v2 on swiss
getScores(res,"svm.v2","swiss.Infant.Mortality")


## End(Not run)
</code></pre>

<hr>
<h2 id='getWorkflow'>
Obtain the workflow object corresponding to an ID
</h2><span id='topic+getWorkflow'></span>

<h3>Description</h3>

<p>This function can be used to obtain the <code><a href="#topic+Workflow-class">Workflow</a></code>
object corresponding to an ID used in a performance estimation
experiment. This allows you for instance to check the full details of
the workflow corresponding to that ID (e.g. the function implementing
the workflow, the parameters and their values, etc.)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getWorkflow(var, obj)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getWorkflow_+3A_var">var</code></td>
<td>

<p>The string with the workflow ID
</p>
</td></tr>
<tr><td><code id="getWorkflow_+3A_obj">obj</code></td>
<td>

<p>A <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> object with the data from a
performance estimation experiment
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+Workflow-class">Workflow</a></code> object
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Workflow-class">Workflow</a></code>,
<code><a href="#topic+runWorkflow">runWorkflow</a></code>,  
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Estimating MSE for 3 variants of both
## regression trees and SVMs, on  two data sets, using one repetition
## of 10-fold CV
library(e1071)
library(DMwR)
data(swiss)
data(mtcars)

## running the estimation experiment
res &lt;- performanceEstimation(
  c(PredTask(Infant.Mortality ~ .,swiss),PredTask(mpg ~ ., mtcars)),
  c(workflowVariants(learner="svm",
                     learner.pars=list(cost=c(1,10),gamma=c(0.01,0.5))),
    workflowVariants(learner="rpartXse",
                     learner.pars=list(se=c(0,0.5,1)))
  ),
  EstimationTask("mse",method=CV(nReps=2,nFolds=5))
  )

## Get the workflow corresponding to the ID svm.v2
getWorkflow("svm.v2",res)

## End(Not run)
</code></pre>

<hr>
<h2 id='hldEstimates'>
Performance estimation using holdout and random resampling
</h2><span id='topic+hldEstimates'></span>

<h3>Description</h3>

<p>This function obtains hold-out and random sub-sampling estimates of
performance  metrics for a given predictive task and method to solve
it (i.e. a  workflow). The function is general in the sense that the
workflow function that the user provides as the solution to the task,
can implement or call whatever modeling technique the user wants.
</p>
<p>The function implements hold-out and random sub-sampling (repeated
hold-out) estimation. Different settings concerning this methodology are
available through the argument <code>estTask</code> (check the help page of
<code><a href="#topic+Holdout-class">Holdout</a></code>).
</p>
<p>Please note that most of the times you will not call this function
directly (though there is nothing wrong in doing it) but instead you
will use the function <code><a href="#topic+performanceEstimation">performanceEstimation</a></code>, that allows you to
carry out performance estimation for multiple workflows on multiple tasks,
using some estimation method you want (e.g. hold-out). Still, when you
simply want to have the hold-out estimate of one workflow on one task,
you may prefer to use this function directly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hldEstimates(wf,task,estTask,cluster)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hldEstimates_+3A_wf">wf</code></td>
<td>

<p>an object of the class <code><a href="#topic+Workflow-class">Workflow</a></code> representing the
modeling approach to be evaluated on a certain task.
</p>
</td></tr>
<tr><td><code id="hldEstimates_+3A_task">task</code></td>
<td>

<p>an object of the class <code><a href="#topic+PredTask-class">PredTask</a></code> representing the
prediction task to be used in the evaluation.
</p>
</td></tr>
<tr><td><code id="hldEstimates_+3A_esttask">estTask</code></td>
<td>

<p>an object of the class <code><a href="#topic+EstimationTask-class">EstimationTask</a></code> representing the
hold-out settings to use.
</p>
</td></tr>
<tr><td><code id="hldEstimates_+3A_cluster">cluster</code></td>
<td>

<p>an optional parameter that can either be <code>TRUE</code> or a
<code><a href="survival.html#topic+cluster">cluster</a></code>. In case of <code>TRUE</code> the function will run in
parallel and will internally setup the parallel back-end (defaulting
to using half of the cores in your local machine). You may also setup
outside your parallel back-end (c.f. <code><a href="parallel.html#topic+makeCluster">makeCluster</a></code>) and
then pass the resulting <code>cluster</code> object to this function using
this parameter. In case no value is provided for this parameter the
function will run sequentially.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The idea of this function is to carry out a hold-out
experiment with the goal of obtaining reliable estimates of the
predictive performance of a certain approach to a predictive
task. This approach (denoted here as a <em>workflow</em>) will be evaluated on
the given predictive task using some user-selected  metrics,
and this function will 
provide hold-out or random sub-sampling estimates of the true value of these
evaluation metrics.  Hold-out estimates are obtained by randomly
partition the given data set into train and test sets. The training
set is used to obtain a model for the predictive task, which is then
tested by making predictions for the test set. This random split of
the given data can be repeated several times leading to what is
usually known as random sub-sampling estimates. In the end the average of
the scores over the several repetitions (if using <em>pure</em>
hold-out this is only one) are the hold-out estimates of the selected
metrics.   
</p>
<p>Parallel execution of the estimation experiment is only recommended
for minimally large data sets otherwise you may actually increase the
computation time due to communication costs between the processes.
</p>


<h3>Value</h3>

<p>The result of the function is an object of class <code><a href="#topic+EstimationResults-class">EstimationResults</a></code>.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Holdout-class">Holdout</a></code>,
<code><a href="#topic+Workflow-class">Workflow</a></code>,
<code><a href="#topic+standardWF">standardWF</a></code>,
<code><a href="#topic+PredTask-class">PredTask</a></code>,
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code>,
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>,
<code><a href="#topic+cvEstimates">cvEstimates</a></code>,
<code><a href="#topic+bootEstimates">bootEstimates</a></code>,  
<code><a href="#topic+loocvEstimates">loocvEstimates</a></code>,
<code><a href="#topic+mcEstimates">mcEstimates</a></code>,
<code><a href="#topic+EstimationResults-class">EstimationResults</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

## Estimating the mean absolute error and the normalized mean squared
## error of rpart on the swiss data, using 70%-30% hold-out
library(e1071)
data(swiss)

## Now the evaluation
eval.res &lt;- hldEstimates(
             Workflow(wf="standardWF",wfID="svmApproach",
                      learner="svm",learner.pars=list(cost=10,gamma=0.1)
                     ),
             PredTask(Infant.Mortality ~ ., swiss),
             EstimationTask(metrics=c("mae","nmse"),
                            method=Holdout(nReps=5,hldSz=0.3))
                        )

## Check a summary of the results
summary(eval.res)

## An example with a user-defined workflow function implementing a
## simple approach using linear regression models but also containing
## some data-preprocessing and well as results post-processing.
myLM &lt;- function(form,train,test,k=10,.outModel=FALSE) {
    require(DMwR)
    ## fill-in NAs on both the train and test sets
    ntr &lt;- knnImputation(train,k)
    nts &lt;- knnImputation(test,k,distData=train)
    ## obtain a linear regression model and simplify it
    md &lt;- lm(form,ntr)
    md &lt;- step(md)
    ## get the model predictions
    p &lt;- predict(md,nts)
    ## post-process the predictions (this is an example assuming the target
    ## variable is always positive so we change negative predictions into 0)
    p &lt;- ifelse(p &lt; 0,0,p)
    ## now get the final return object
    res &lt;- list(trues=responseValues(form,nts), preds=p)
    if (.outModel) res &lt;- c(res,list(model=m))
    res
}

## Now for the Holdout estimation 
data(algae,package="DMwR")
eval.res2 &lt;- hldEstimates(
             Workflow(wf="myLM",k=5),
             PredTask(a1 ~ ., algae[,1:12],"alga1"),
             EstimationTask("mse",method=Holdout(nReps=5)))

## Check a summary of the results
summary(eval.res2)

## End(Not run)
</code></pre>

<hr>
<h2 id='Holdout-class'>Class &quot;Holdout&quot; </h2><span id='topic+Holdout'></span><span id='topic+Holdout-class'></span><span id='topic+show+2CHoldout-method'></span>

<h3>Description</h3>

<p>	 This class of objects contains the information
describing a hold out experiment, i.e. its settings.</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>Holdout(...)</code>
providing the values for the class slots.
The objects contain information on the number of repetitions of the
hold out experiment, the percentage of the given data to set as hold
out test set, the random number generator seed, information on
whether stratified sampling should be used and <em>optionally</em> the
concrete data splits to use on each iteration of the holdout
experiment. Note that most of the times you will not supply these data
splits as the holdout routines in this infra-structure will take care of
building them. Still, this allows you to  replicate some experiment
carried out with specific train/test splits.
</p>


<h3>Slots</h3>


<dl>
<dt><code>nReps</code>:</dt><dd><p>Object of class <code>numeric</code> indicating
the number of repetitions of the N folds CV experiment (defaulting
to 1). </p>
</dd>
<dt><code>hldSz</code>:</dt><dd><p>Object of class <code>numeric</code> with the
percentage (a number between 0 and 1) of cases to use as hold out
(defaulting to 0.3).</p>
</dd>
<dt><code>strat</code>:</dt><dd><p>Object of class <code>logical</code> indicating
whether the sampling should or not be stratefied (defaulting to FALSE). </p>
</dd>
<dt><code>seed</code>:</dt><dd><p>Object of class <code>numeric</code> with the
random number generator seed (defaulting to 1234). </p>
</dd>
<dt><code>dataSplits</code>:</dt><dd><p>Object of class <code>list</code> 
containing the data splits to use on each repetition of a
<code>nReps</code> Holdout experiment (defaulting to <code>NULL</code>). This list
should contain <code>nReps</code> elements. Each element should be a
vector with the row ids of the test set of the respective
iteration.  On all these
iterations the training set will be formed by the ids not
appearing in the test set. </p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code><a href="#topic+EstCommon-class">EstCommon</a></code>, directly.
Class <code><a href="#topic+EstimationMethod-class">EstimationMethod</a></code>, directly.
</p>


<h3>Methods</h3>


<dl>
<dt>show</dt><dd><p><code>signature(object = "Holdout")</code>: method used to
show the contents of a <code>Holdout</code> object. </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MonteCarlo-class">MonteCarlo</a></code>,
<code><a href="#topic+LOOCV-class">LOOCV</a></code>,
<code><a href="#topic+Bootstrap-class">Bootstrap</a></code>,
<code><a href="#topic+CV-class">CV</a></code>,
<code><a href="#topic+EstimationMethod-class">EstimationMethod</a></code>,
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("Holdout")

## 10 repetitions of a holdout experiment leaving on each repetition
## 20% of the cases randomly chosen as test set (the holdout)
h1 &lt;- Holdout(nReps=10,hldSz=0.2,strat=TRUE)
h1

## Small example illustrating the format of user supplied data splits
## in this case for 3 repetitions of a Holdout process where each test
## set has 10 cases
h2 &lt;- Holdout(dataSplits=list(1:10,11:20,21:30))
h2

</code></pre>

<hr>
<h2 id='is.classification'>
Check if a certain predictive task is a classification problem
</h2><span id='topic+is.classification'></span>

<h3>Description</h3>

<p>This function tests if a task defined by a formula over a data set is
a classification task, which will be the case if the target variable is
nominal. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.classification(task)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.classification_+3A_task">task</code></td>
<td>

<p>An object of class <code><a href="#topic+PredTask-class">PredTask</a></code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A logical value
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+is.regression">is.regression</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
tsk &lt;- PredTask(Species ~ .,iris)
if (is.classification(tsk)) cat("This is a classification task.\n")
</code></pre>

<hr>
<h2 id='is.regression'>
Check if a certain predictive task is a regression problem
</h2><span id='topic+is.regression'></span>

<h3>Description</h3>

<p>This function tests if a task defined by a formula over a data set is
a regression task, which will be the case if the target variable is
numeric. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.regression(task)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.regression_+3A_task">task</code></td>
<td>

<p>An object of class <code><a href="#topic+PredTask-class">PredTask</a></code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A logical value
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+is.classification">is.classification</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)
tsk &lt;- PredTask(Species ~ .,iris)
if (!is.regression(tsk)) cat("This is not a regression task!\n")
</code></pre>

<hr>
<h2 id='knnImp'>
Fill in NA values with the values of the nearest neighbours
</h2><span id='topic+knnImp'></span>

<h3>Description</h3>

<p>Function that fills in all NA values using the k Nearest
Neighbours of each case with NA values. It uses the median/most
frequent value within the neighbours to fill in the NAs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>knnImp(data, k = 10, scale = TRUE, distData = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="knnImp_+3A_data">data</code></td>
<td>

<p>A data frame with the data set
</p>
</td></tr>
<tr><td><code id="knnImp_+3A_k">k</code></td>
<td>

<p>The number of nearest neighbours to use (defaults to 10)
</p>
</td></tr>
<tr><td><code id="knnImp_+3A_scale">scale</code></td>
<td>

<p>Boolean setting if the data should be scale before finding the
nearest neighbours (defaults to <code>TRUE</code>)
</p>
</td></tr>
<tr><td><code id="knnImp_+3A_distdata">distData</code></td>
<td>

<p>Optionally you may sepecify here a data frame containing the data set
that should be used to find the neighbours. This is usefull when
filling in NA values on a test set, where you should use only
information from the training set. This defaults to NULL, which means
that the neighbours will be searched in <code>data</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses the k-nearest neighbours to fill in the unknown (NA)
values in a data set. For each case with any NA value it will search for
its k most similar cases and use the values of these cases to fill in
the unknowns.
</p>
<p>The function will use either the median (in case of numeric variables)
or the most frequent value (in case of factors), of the neighbours to
fill in the NAs.  
</p>


<h3>Value</h3>

<p>A data frame without NA values
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+na.omit">na.omit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(algae,package="DMwR")
cleanAlgae &lt;- knnImp(algae)
summary(cleanAlgae)

## End(Not run)
</code></pre>

<hr>
<h2 id='LOOCV-class'>Class &quot;LOOCV&quot;  </h2><span id='topic+LOOCV'></span><span id='topic+LOOCV-class'></span><span id='topic+show+2CLOOCV-method'></span>

<h3>Description</h3>

<p>	This class of objects contains the information
describing a leave one out cross validation estimation experiment, i.e. its settings. </p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>LOOCV(...)</code>
providing the values for the class slots.
These objects contain infoprmation on the random number generator seed
and also whether the execution of the experiments should be verbose.
</p>


<h3>Slots</h3>


<dl>
<dt><code>seed</code>:</dt><dd><p>Object of class <code>numeric</code> with the
random number generator seed (defaulting to 1234). </p>
</dd>
<dt><code>dataSplits</code>:</dt><dd><p>Object of class <code>list</code> 
containing the data splits to use on each repetition of a
leave one out cross validation experiment (defaulting to <code>NULL</code>). This list
should contain as many elements as there are cases in the task
data set. Each element should be the row id of the test case of the respective
iteration.  On all these
iterations the training set will be formed by the remaining ids. </p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code><a href="#topic+EstCommon-class">EstCommon</a></code>, directly.
Class <code><a href="#topic+EstimationMethod-class">EstimationMethod</a></code>, directly.
</p>


<h3>Methods</h3>


<dl>
<dt>show</dt><dd><p><code>signature(object = "LOOCV")</code>: method used to
show the contents of a <code>LOOCV</code> object. </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MonteCarlo-class">MonteCarlo</a></code>,
<code><a href="#topic+CV-class">CV</a></code>,
<code><a href="#topic+Bootstrap-class">Bootstrap</a></code>,
<code><a href="#topic+Holdout-class">Holdout</a></code>,
<code><a href="#topic+EstimationMethod-class">EstimationMethod</a></code>,
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("LOOCV")

s &lt;- LOOCV()
s

</code></pre>

<hr>
<h2 id='loocvEstimates'>
Performance estimation using  Leave One Out Cross Validation 
</h2><span id='topic+loocvEstimates'></span>

<h3>Description</h3>

<p>This function obtains leave one out cross validation estimates of
performance  metrics for a given predictive task and method to solve
it (i.e. a  workflow). The function is general in the sense that the
workflow function that the user provides as the solution to the task,
can implement or call whatever modeling technique the user wants.
</p>
<p>The function implements leave one out cross validation
estimation. Different settings concering this methodology are 
available through the argument <code>estTask</code> (check the help page of
<code><a href="#topic+LOOCV-class">LOOCV</a></code>).
</p>
<p>Please note that most of the times you will not call this function
directly, though there is nothing wrong in doing it, but instead you
will use the function <code><a href="#topic+performanceEstimation">performanceEstimation</a></code>, that allows you to
carry out performance estimation of multiple workflows on multiple tasks,
using some estimation method like for instance cross validation. Still, when you
simply want to have the leave one out cross validation estimate of one
workflow on one task, you may use this function directly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loocvEstimates(wf,task,estTask,verbose=FALSE,cluster)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loocvEstimates_+3A_wf">wf</code></td>
<td>

<p>an object of the class <code><a href="#topic+Workflow-class">Workflow</a></code> representing the
modeling approach to be evaluated on a certain task.
</p>
</td></tr>
<tr><td><code id="loocvEstimates_+3A_task">task</code></td>
<td>

<p>an object of the class <code><a href="#topic+PredTask-class">PredTask</a></code> representing the
prediction task to be used in the evaluation.
</p>
</td></tr>
<tr><td><code id="loocvEstimates_+3A_esttask">estTask</code></td>
<td>

<p>an object of the class <code><a href="#topic+EstimationTask-class">EstimationTask</a></code> indicating the metrics to
be estimated and the leave one out cross validation settings to use.
</p>
</td></tr>
<tr><td><code id="loocvEstimates_+3A_verbose">verbose</code></td>
<td>

<p>A boolean value controlling the level of output of the function
execution, defaulting to <code>FALSE</code>
</p>
</td></tr>
<tr><td><code id="loocvEstimates_+3A_cluster">cluster</code></td>
<td>

<p>an optional parameter that can either be <code>TRUE</code> or a
<code><a href="survival.html#topic+cluster">cluster</a></code>. In case of <code>TRUE</code> the function will run in
parallel and will internally setup the parallel back-end (defaulting
to using half of the cores in your local machine). You may also setup
outside your parallel back-end (c.f. <code><a href="parallel.html#topic+makeCluster">makeCluster</a></code>) and
then pass the resulting <code>cluster</code> object to this function using
this parameter. In case no value is provided for this parameter the
function will run sequentially.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The idea of this function is to carry out a leave one out cross
validation  experiment with the goal of obtaining reliable estimates
of the predictive performance of a certain approach to a predictive
task. This approach (denoted here as a <em>workflow</em>) will be evaluated on
the given predictive task using some user-selected  metrics,
and this function will provide leave one out cross validation
estimates of the true value of these
evaluation metrics.  Leave one out cross validation estimates are
obtained as the average of <em>N</em> iterations, where <em>N</em> is the
size of the given data sample. On each of these iterations one of the
cases in the data sample is left out as <em>test set</em> and the
worflow is applied to the remaining <em>N-1</em> cases. The process is
repeated for all cases, i.e. <em>N</em> times. This estimation is
similar to k-fold cross validation where k equals to <em>N</em>. The
resulting estimates are obtained by averaging over the <em>N</em>
iteration scores. 
</p>
<p>Parallel execution of the estimation experiment is only recommended
for minimally large data sets otherwise you may actually increase the
computation time due to communication costs between the processes.
</p>


<h3>Value</h3>

<p>The result of the function is an object of class <code><a href="#topic+EstimationResults-class">EstimationResults</a></code>.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+LOOCV-class">LOOCV</a></code>,
<code><a href="#topic+Workflow-class">Workflow</a></code>,
<code><a href="#topic+standardWF">standardWF</a></code>,
<code><a href="#topic+PredTask-class">PredTask</a></code>,
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code>,
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>,
<code><a href="#topic+hldEstimates">hldEstimates</a></code>,
<code><a href="#topic+bootEstimates">bootEstimates</a></code>,  
<code><a href="#topic+cvEstimates">cvEstimates</a></code>,
<code><a href="#topic+mcEstimates">mcEstimates</a></code>,
<code><a href="#topic+EstimationResults-class">EstimationResults</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

## Estimating the error rate of an SVM on the iris data set using
## leave one out cross validation
library(e1071)
data(iris)

## Now the evaluation
eval.res &lt;- loocvEstimates(
             Workflow(wfID="svmTrial",
                      learner="svm",learner.pars=list(cost=10,gamma=0.1)
                     ),
             PredTask(Species ~ ., iris),
             EstimationTask("err",method=LOOCV()))

## Check a summary of the results
summary(eval.res)


## End(Not run)
</code></pre>

<hr>
<h2 id='mcEstimates'>
Performance estimation for time series prediction tasks using Monte Carlo
</h2><span id='topic+mcEstimates'></span>

<h3>Description</h3>

<p>This function performs a Monte Carlo experiment with the goal of
estimating the performance of a given approach (a workflow) on a
certain time  series prediction task. The function is general in the
sense that the workflow function that the user provides as the
solution to the task,  can implement or call whatever modeling
technique the user wants. 
</p>
<p>The function implements Monte Carlo estimation and different settings
concerning this methodology are available through the argument
<code>estTask</code> (check the help page of <code><a href="#topic+MonteCarlo-class">MonteCarlo</a></code>).
</p>
<p>Please note that most of the times you will not call this function
directly, though there is nothing wrong in doing it, but instead you
will use the function <code><a href="#topic+performanceEstimation">performanceEstimation</a></code>, that allows you to
carry out performance estimation for multiple workflows on multiple tasks,
using some estimation method. Still, when you
simply want to have the Monte Carlo estimates for one workflow on one task,
you may prefer to use this function directly.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcEstimates(wf, task, estTask, verbose = TRUE,cluster)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcEstimates_+3A_wf">wf</code></td>
<td>

<p>an object of the class <code>Workflow</code> representing the
modeling approach to be evaluated on a certain task.
</p>
</td></tr>
<tr><td><code id="mcEstimates_+3A_task">task</code></td>
<td>

<p>an object of the class <code>PredTask</code> representing the
prediction task to be used in the evaluation.
</p>
</td></tr>
<tr><td><code id="mcEstimates_+3A_esttask">estTask</code></td>
<td>

<p>an object of the class <code><a href="#topic+EstimationTask-class">EstimationTask</a></code> indicating the metrics to
be estimated and the Monte Carlo settings to use.
</p>
</td></tr>
<tr><td><code id="mcEstimates_+3A_verbose">verbose</code></td>
<td>

<p>A boolean value controlling the level of output of the function
execution, defaulting to <code>TRUE</code>
</p>
</td></tr>
<tr><td><code id="mcEstimates_+3A_cluster">cluster</code></td>
<td>

<p>an optional parameter that can either be <code>TRUE</code> or a
<code><a href="survival.html#topic+cluster">cluster</a></code>. In case of <code>TRUE</code> the function will run in
parallel and will internally setup the parallel back-end (defaulting
to using half of the cores in your local machine). You may also setup
outside your parallel back-end (c.f. <code><a href="parallel.html#topic+makeCluster">makeCluster</a></code>) and
then pass the resulting <code>cluster</code> object to this function using
this parameter. In case no value is provided for this parameter the
function will run sequentially.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function provides reliable estimates of a set of evaluation
statistics through a Monte Carlo experiment. The user supplies a worflow
function and a data set of a time series forecasting task, together
with the estimation task. This task should include both the metrics to
be estimated as well as the settings of the estimation methodology
(MOnte Carlo) that include,
among others, the size of the training (TR) and testing sets (TS) and
the number 
of repetitions (R) of the train+test cycle. The function randomly selects
a set of R numbers in the time interval [TR+1,NDS-TS+1], where NDS is the
size of the full data set. For each of these R numbers the previous TR
observations of the data set are used to learn a model and the
subsequent TS observations for testing it and obtaining the wanted
evaluation metrics. The resulting R estimates of the evaluation
metrics are averaged at the end of this process resulting in the
Monte Carlo estimates of these metrics.
</p>
<p>This function is targeted at obtaining estimates of
performance for time series prediction problems. The reason is that
the experimental repetitions ensure that the order of the rows in the
original data set are never swaped, as these rows are assumed to be
ordered by time. This is an important issue to
ensure that a prediction model is never tested on past observations of
the time series.
</p>
<p>For each train+test iteration the provided workflow function is called
and should return the predictions  of the workflow for the given test
period. To carry out this train+test iteration the user may use the
standard time series workflow that is provided (check the help page of
<code><a href="#topic+timeseriesWF">timeseriesWF</a></code>), or may provide hers/his own workflow that
should return a list as result. See the Examples section below for an
example of these functions. Further examples are given in the package
vignette.
</p>
<p>Parallel execution of the estimation experiment is only recommended
for minimally large data sets otherwise you may actually increase the
computation time due to communication costs between the processes.
</p>


<h3>Value</h3>

<p>The result of the function is an object of class <code>EstimationResults</code>.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MonteCarlo-class">MonteCarlo</a></code>,
<code><a href="#topic+Workflow-class">Workflow</a></code>,
<code><a href="#topic+timeseriesWF">timeseriesWF</a></code>,
<code><a href="#topic+PredTask-class">PredTask</a></code>,
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code>,
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>,
<code><a href="#topic+hldEstimates">hldEstimates</a></code>,
<code><a href="#topic+loocvEstimates">loocvEstimates</a></code>,  
<code><a href="#topic+cvEstimates">cvEstimates</a></code>,
<code><a href="#topic+bootEstimates">bootEstimates</a></code>,
<code><a href="#topic+EstimationResults-class">EstimationResults</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## The following is a small illustrative example using the quotes of the
## SP500 index. This example estimates the performance of a random
## forest on a illustrative example of trying to forecast the future
## variations of the adijusted close prices of the SP500 using a few
## predictors. The random forest is evaluated on 4 repetitions of a
## monte carlo experiment where 30% of the data is used for training
## the model that is then used to make predictions for the next 20%,
## using a sliding window approach with a relearn step of 10 periods
## (check the help page of the timeseriesWF() function to understand
## these and other settings)

## Not run: 
library(quantmod)
library(randomForest)

getSymbols('^GSPC',from='2008-01-01',to='2012-12-31')
data.model &lt;- specifyModel(Next(100*Delt(Ad(GSPC))) ~ Delt(Ad(GSPC),k=1:10)+Delt(Vo(GSPC),k=1:3))
data &lt;- as.data.frame(modelData(data.model))
colnames(data)[1] &lt;- 'PercVarClose'

spExp &lt;- mcEstimates(Workflow("timeseriesWF",wfID="rfTrial",
                              type="slide",relearn.step=10,
                              learner='randomForest'),
                    PredTask(PercVarClose ~ .,data,"sp500"),
                    EstimationTask(metrics=c("mse","theil"),
                                   method=MonteCarlo(nReps=4,szTrain=.3,szTest=.2)))

summary(spExp)

## End(Not run)
</code></pre>

<hr>
<h2 id='mergeEstimationRes'>
Merging several <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> class objects
</h2><span id='topic+mergeEstimationRes'></span>

<h3>Description</h3>

<p>This function can be used to join several objects of class <code>ComparisonResults</code>  
into a single object. The merge is carried out assuming that there is
something in common between the objects (e.g. all use the same
workflows on different tasks), and that the user specifies which
property should be used for the merging process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mergeEstimationRes(..., by = "tasks")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mergeEstimationRes_+3A_...">...</code></td>
<td>

<p>The <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> class object names separated by commas
</p>
</td></tr>
<tr><td><code id="mergeEstimationRes_+3A_by">by</code></td>
<td>

<p>The dimension of the <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> class objects that should be used
for the merge. All objects should have the same values on the
remaining dimensions of an estimation experiment. For instance, if you
merge by &quot;tasks&quot; (the default) it means that the objects being merged
should include estimation results on the same set of workflows on the
same set of metrics, using the same estimation method and
settings. The only thing that changes between the objects in this
example is the set of tasks. Possible values of this argument are:
&quot;tasks&quot;, &quot;workflows&quot; and &quot;metrics&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The objects of class <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> (type &quot;class?ComparisonResults&quot;
for details) contain several information on the results of an
estimation expriment for several workflows on several predictive
tasks. Sometimes, when you are trying too many workflows on too many
tasks, it is convinient to run these variants on different calls to
the function <code><a href="#topic+performanceEstimation">performanceEstimation</a></code>. After all calls are
completed we frequently want to have all results on a single
object. This is the objective of the current function: allow you to
merge these different <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> objects
into a single one. For being mergeable the objects need to have things
in common otherwise it makes no sense to merge them. For instance, we
could split our very large experiment by calling
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code> with different tasks, although the
rest (the workflows and the estimation task) stays the same. See the
Examples section for some illustrations.
</p>


<h3>Value</h3>

<p>The result of this function is a <code>ComparisonResults</code> object.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+performanceEstimation">performanceEstimation</a></code>, <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code>, <code><a href="#topic+subset+2CComparisonResults-method">subset</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Run some experiments with the swiss data and two different
## prediction models
data(swiss)

exp1 &lt;- performanceEstimation(
  PredTask(Infant.Mortality ~ .,swiss),
  workflowVariants(learner="svm",
                   learner.pars=list(cost=c(1,10),gamma=c(0.01,0.5))),
  EstimationTask("mse")
  )

exp2 &lt;- performanceEstimation(
  PredTask(Infant.Mortality ~ .,swiss),
  Workflow(learner="lm"),
  EstimationTask("mse")
  )


## joining the two experiments by workflows
all &lt;- mergeEstimationRes(exp1,exp2,by="workflows")
topPerformers(all) # check the best results

## now an example by adding new metrics
exp3 &lt;- performanceEstimation(
  PredTask(Infant.Mortality ~ .,swiss),
  Workflow(learner="lm"),
  EstimationTask(metrics=c("mae","totTime"))
  )

allLM &lt;- mergeEstimationRes(exp2,exp3,by="metrics")
topPerformers(allLM) 


## End(Not run)
</code></pre>

<hr>
<h2 id='metricNames'>
The evaluation metrics estimated in an experiment
</h2><span id='topic+metricNames'></span>

<h3>Description</h3>

<p>This function can be used to get a vector with the names of the
evaluation metrics that were used in a performance estimation experiment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>metricNames(o)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="metricNames_+3A_o">o</code></td>
<td>

<p>An object of class <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of strings (the names of the metrics)
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code>,
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>,
<code><a href="#topic+taskNames">taskNames</a></code>,
<code><a href="#topic+workflowNames">workflowNames</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Estimating MSE for 3 variants of both
## regression trees and SVMs, on  two data sets, using one repetition
## of 10-fold CV
library(e1071)
library(DMwR)
data(swiss)
data(mtcars)

## running the estimation experiment
res &lt;- performanceEstimation(
  c(PredTask(Infant.Mortality ~ .,swiss),PredTask(mpg ~ ., mtcars)),
  c(workflowVariants(learner="svm",
                     learner.pars=list(cost=c(1,10),gamma=c(0.01,0.5))),
    workflowVariants(learner="rpartXse",
                     learner.pars=list(se=c(0,0.5,1)))
  ),
  EstimationTask("mse")
  )

## the names of the metrics that were estimated in the above experiment
metricNames(res)

## End(Not run)
</code></pre>

<hr>
<h2 id='metricsSummary'>
Obtains a summary of the individual metric scores obtained by each
workflow on a set of tasks. 
</h2><span id='topic+metricsSummary'></span>

<h3>Description</h3>

<p>Given a <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> object this function provides a summary
statistic (defaulting to the mean) of the individual scores
obtained on a each evaluation metric over all repetitions carried
out in the estimation process. This is done for all workflows and
tasks of the performance estimation experiment. The function can be handy to
obtain things like for instance the maximum score obtained by each
workflow on a particular metric over all repetitions of the
experimental process. It is also usefull (using its defaults) as a way
to quickly getting the estimated values for each metric obtained by
each alternative workflow and task (see the Examples section).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>metricsSummary(compRes, summary = "mean", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="metricsSummary_+3A_compres">compRes</code></td>
<td>

<p>An object of class <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> with the results of a
performance estimation experiment.
</p>
</td></tr>
<tr><td><code id="metricsSummary_+3A_summary">summary</code></td>
<td>

<p>A string with the name of the function that you want to use to
obtain the summary (defaults to &quot;mean&quot;). This function will be applied
to the set of individual scores of each workflow on each task and for
all metrics.
</p>
</td></tr>
<tr><td><code id="metricsSummary_+3A_...">...</code></td>
<td>

<p>Further arguments passed to the selected summary function.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of this function is a named list with as many components as
there are predictive tasks. For each
task (component), we get a matrix with as many columns as there are
workflows and as many rows as there are evaluation metrics. The values
on this matrix are the results of applying the selected summary
function to the metric scores on each iteration of the estimation process.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+performanceEstimation">performanceEstimation</a></code>,
<code><a href="#topic+topPerformers">topPerformers</a></code>,
<code><a href="#topic+topPerformer">topPerformer</a></code>,
<code><a href="#topic+rankWorkflows">rankWorkflows</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Estimating several evaluation metrics on different variants of a
## regression tree and of a SVM, on  two data sets, using one repetition
## of  10-fold CV

data(swiss)
data(mtcars)
library(e1071)

## run the experimental comparison
results &lt;- performanceEstimation(
               c(PredTask(Infant.Mortality ~ ., swiss),
                 PredTask(mpg ~ ., mtcars)),
               c(workflowVariants(learner='svm',
                                  learner.pars=list(cost=c(1,5),gamma=c(0.1,0.01))
                                 )
               ),
               EstimationTask(metrics=c("mse","mae"),method=CV(nReps=2,nFolds=5))
                                 )

## Get the minium value of each metric on all iterations of the CV
## process. 
metricsSummary(results,summary="min")

## Get a summary table for each task with the estimated scores for each
## metric by each workflow
metricsSummary(results)

## End(Not run)
</code></pre>

<hr>
<h2 id='MonteCarlo-class'>Class &quot;MonteCarlo&quot;  </h2><span id='topic+MonteCarlo'></span><span id='topic+MonteCarlo-class'></span><span id='topic+show+2CMonteCarlo-method'></span>

<h3>Description</h3>

<p>	 This class of objects contains the information
describing a monte carlo experiment, i.e. its settings.</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>MonteCarlo(...)</code>
providing the values for the class slots.
These objects contain information on the number of repetitions of the
experiments, the data used for training the models on each repetition,
the  data used for testing these models, the random number
generator seed and <em>optionally</em> the 
concrete data splits to use on each iteration of the Monte Carlo
experiment. Note that most of the times you will not supply these data
splits as the Monte Carlo routines in this infra-structure will take care of
building them. Still, this allows you to  replicate some experiment
carried out with specific train/test splits.
</p>


<h3>Slots</h3>


<dl>
<dt><code>nReps</code>:</dt><dd><p>Object of class <code>numeric</code> indicating
the number of repetitions of the Monte Carlo experiment (defaulting
to 10). </p>
</dd>
<dt><code>szTrain</code>:</dt><dd><p>Object of class <code>numeric</code>. If it is a
value between 0 and 1 it is interpreted as a percentage of the
available data set, otherwise it is interpreted as the number of
cases to use. It defaults to 0.25. </p>
</dd>
<dt><code>szTest</code>:</dt><dd><p>Object of class <code>numeric</code> If it is a
value between 0 and 1 it is interpreted as a percentage of the
available data set, otherwise it is interpreted as the number of
cases to use. It defaults to 0.25. </p>
</dd>
<dt><code>seed</code>:</dt><dd><p>Object of class <code>numeric</code> with the
random number generator seed (defaulting to 1234). </p>
</dd>
<dt><code>dataSplits</code>:</dt><dd><p>Object of class <code>list</code> 
containing the data splits to use on each Monte Carlo
repetition. Each element should be a list with two components:
<code>test</code> and <code>train</code>, on this order. Each of these is a
vector with the row ids to use as test and train sets of each
repetition of the Monte Carlo experiment. </p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code><a href="#topic+EstCommon-class">EstCommon</a></code>, directly.
Class <code><a href="#topic+EstimationMethod-class">EstimationMethod</a></code>, directly.
</p>


<h3>Methods</h3>


<dl>
<dt>show</dt><dd><p><code>signature(object = "MonteCarlo")</code>: method used to
show the contents of a <code>MonteCarlo</code> object. </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+LOOCV-class">LOOCV</a></code>,
<code><a href="#topic+CV-class">CV</a></code>,
<code><a href="#topic+Bootstrap-class">Bootstrap</a></code>,
<code><a href="#topic+Holdout-class">Holdout</a></code>,
<code><a href="#topic+EstimationMethod-class">EstimationMethod</a></code>,
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("MonteCarlo")

m1 &lt;- MonteCarlo(nReps=10,szTrain=0.3,szTest=0.2)
m1

## Small example illustrating the format of user supplied data splits
## it assumes that the source data is formed by 10 cases and that each
## model is trainined with 3 cases and tested in the following case.
## This is obviously a unrealistic example in terms of size but
## illustrates the format of the data splits
m2 &lt;- MonteCarlo(dataSplits=list(list(test=sample(1:150,50),train=sample(1:150,50)),
                                   list(test=sample(1:150,50),train=sample(1:150,50)),
                                   list(test=sample(1:150,50),train=sample(1:150,50))
                                  ))
m2

</code></pre>

<hr>
<h2 id='pairedComparisons'>
Statistical hypothesis testing on the observed paired differences in
estimated performance. 
</h2><span id='topic+pairedComparisons'></span>

<h3>Description</h3>

<p>This function analyses the statistical significance of the paired
comparisons between the estimated performance scores of a set of
workflows.  When you run the <code>performanceEstimation()</code> function to
compare a set of workflows over a set of problems you obtain estimates
of their performances across these problems. This function implements
several statistical tests that can be used to test several hypothesis
concerning the observed differences in performance between the
workflows on the tasks. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pairedComparisons(obj,baseline,
                  maxs=rep(FALSE,length(metricNames(obj))),
                  p.value=0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pairedComparisons_+3A_obj">obj</code></td>
<td>

<p>An object of class <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> 
that contains the results of a performance estimation experiment. 
</p>
</td></tr>
<tr><td><code id="pairedComparisons_+3A_baseline">baseline</code></td>
<td>

<p>Several tests involve the hypothesis that a certain workflow is
significantly different from a set of other alternatives. This
argument allows you to specify the name of this baseline workflow. If
you omit this name the function will default to the 
name of the workflow that has the lower average rank position across
all tasks, for each estimation metric.
</p>
</td></tr>
<tr><td><code id="pairedComparisons_+3A_maxs">maxs</code></td>
<td>

<p>A vector of booleans with as many elements are there are metrics estimated in
the experiment. A <code>TRUE</code> value means the respective
metric is to be maximized, while a <code>FALSE</code> means
minimization. Defaults to all <code>FALSE</code> values, i.e. all metrics are to
be minimized.
</p>
</td></tr>
<tr><td><code id="pairedComparisons_+3A_p.value">p.value</code></td>
<td>

<p>A <em>p</em> value to be used in the calculations that involve using
values from statistical tables (defaults to 0.05).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="#topic+performanceEstimation">performanceEstimation</a></code> function allows you to obtain
estimates of the expected value of a series of performance metrics for
a set of alternative workflows and a set of predictive tasks. After
running this type of experiments we frequently want to check if there
is any statistical significance between the estimated performance of
the different workflows. The current function allows you to carry out
this type of checks.
</p>
<p>The function will only run on experiments containing more than one
workflow as paired comparisons do not make sense with one single
alternative workflow. Having more than one workflow we can distinguish
two situations: i) comparing the performance of two workflows; or
ii) comparisons among multiple workflows. The recommendations for checking  the statistical
significance of the  difference between the performance of the
alternative workflows varies within these two setups (see Demsar
(2006) for recommendations).
</p>
<p>The current function implements several statistical tests that can be
used for different hypothesis tests. Namely, it obtains 
the results of paired <em>t</em> tests and paired <em>Wilcoxon Signed
Rank</em> tests for situations where you are comparing the performance of
two workflows, with the latter being recommended given the typical
overlap among the training sets that does not ensure independence
among the scores of the different iterations. For the setup of
multiple workflows on multiple tasks the function also calculates the <em>Friedman</em> test
and the post-hoc <em>Nemenyi</em> and <em>Bonferroni-Dunn</em> tests,
according to the procedures described in Demsar (2006). The
combination <em>Friedman</em> test followed by the post-hoc
<em>Nemenyi</em> test is recommended when you want to carry out paired
comparisons between all alternative workflows on the set of tasks to
check for which differences are significant. The combination
<em>Friedman</em> test followed by the post-hoc <em>Bonferroni-Dunn</em>
test is recommended when you want to compare a set of alternative
workflows against a baseline workflow. For both of these two paths we
provide an implementation of the  diagrams (CD diagrams) described in
Demsar (2006) through the  functions <code><a href="#topic+CDdiagram.BD">CDdiagram.BD</a></code> and
<code><a href="#topic+CDdiagram.Nemenyi">CDdiagram.Nemenyi</a></code>. 
</p>
<p>The <code><a href="#topic+performanceEstimation">performanceEstimation</a></code> function ensures that all
compared workflows are run on exactly the same train+test partitions
on all repetitions and for all predictive tasks. In this context, we
can use pairwise statistical significance tests.
</p>


<h3>Value</h3>

<p>The result of this function is the information from performing all
these statistical tests. This information is returned as a list with
as many components as there are estimated metrics. For each metric a
list with several components describing the results of these tests is
provided.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Demsar, J. (2006) <em>Statistical Comparisons of Classifiers over
Multiple Data Sets</em>. Journal of Machine Learning Research, 7, 1-30.
</p>
<p>Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CDdiagram.Nemenyi">CDdiagram.Nemenyi</a></code>,
<code><a href="#topic+CDdiagram.BD">CDdiagram.BD</a></code>,  
<code><a href="#topic+signifDiffs">signifDiffs</a></code>,
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>,
<code><a href="#topic+topPerformers">topPerformers</a></code>,
<code><a href="#topic+topPerformer">topPerformer</a></code>,
<code><a href="#topic+rankWorkflows">rankWorkflows</a></code>,
<code><a href="#topic+metricsSummary">metricsSummary</a></code>,
<code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Estimating MSE for 3 variants of both
## regression trees and SVMs, on  two data sets, using one repetition
## of 10-fold CV
library(e1071)
data(iris)
data(Satellite,package="mlbench")
data(LetterRecognition,package="mlbench")


## running the estimation experiment
res &lt;- performanceEstimation(
           c(PredTask(Species ~ .,iris),
             PredTask(classes ~ .,Satellite,"sat"),
             PredTask(lettr ~ .,LetterRecognition,"letter")),
           workflowVariants(learner="svm",
                 learner.pars=list(cost=1:4,gamma=c(0.1,0.01))),
           EstimationTask(metrics=c("err","acc"),method=CV()))


## checking the top performers
topPerformers(res)

## now let us assume that we will choose "svm.v2" as our baseline
## carry out the paired comparisons
pres &lt;- pairedComparisons(res,"svm.v2")

## obtaining a CD diagram comparing all others against svm.v2 in terms
## of error rate
CDdiagram.BD(pres,metric="err")


## End(Not run)
</code></pre>

<hr>
<h2 id='performanceEstimation'>
Estimate the predictive performance of modeling alternatives on
different predictive tasks
</h2><span id='topic+performanceEstimation'></span>

<h3>Description</h3>

<p>This function can be used to estimate the predictive performance of
alternative approaches to a set of predictive tasks, using different
estimation methods. This is
a generic function that should work with any modeling approaches provided a
few assumptions are met. The function implements different estimation procedures, namely: cross validation, leave one out cross validation,
hold-out, monte carlo simulations and bootstrap.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>performanceEstimation(tasks,workflows,estTask,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="performanceEstimation_+3A_tasks">tasks</code></td>
<td>

<p>This is a vector of objects of class <code><a href="#topic+PredTask-class">PredTask</a></code>, containing the
predictive tasks that will be used in the estimation procedure.
</p>
</td></tr>
<tr><td><code id="performanceEstimation_+3A_workflows">workflows</code></td>
<td>

<p>This is a vector of objects of class <code><a href="#topic+Workflow-class">Workflow</a></code>, containing the
workflows representing different approaches to the predictive tasks,
and whose performance we want to estimate.
</p>
</td></tr>
<tr><td><code id="performanceEstimation_+3A_esttask">estTask</code></td>
<td>

<p>This is an object belonging to class
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code>.  It is used to specify the metrics
to be estimated and the method to use to obtain these estimates.  See
section Details for the possible values. 
</p>
</td></tr>
<tr><td><code id="performanceEstimation_+3A_...">...</code></td>
<td>

<p>Any further parameters that are to be passed to the lower-level
functions implementing each individual estimation methodology. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The goal of this function is to allow estimating the perfomance of a set
of alternative modelling approaches on a set of predictive tasks. The estimation
can be carried out using different methodologies. All alternative
approaches (which we will refer to as <em>workflows</em>) will be
applied using the same exact data partitions for each  task thus
ensuring the possibility of carrying out paired  comparisons using
adequate statistical tests for checking the significance of the
observed differences in performance.
</p>
<p>The first parameter of this function is a vector of
<code><a href="#topic+PredTask-class">PredTask</a></code> objects that define the tasks to use in
the estimation process.
</p>
<p>The second argument is a vector of <code><a href="#topic+Workflow-class">Workflow</a></code>
objects. These can be created in two different ways: either directly
by calling the constructor of this class; or by using the
<code><a href="#topic+workflowVariants">workflowVariants</a></code> function that can be used to
automatically generate different workflow objects as variants of some
base workflow. Either way there are two types of workflows:
user-defined workflows and what we call &quot;standard&quot; workflows. The
later are workflows that people typically follow to solve predictive
tasks and that are already implemented in this package to facilitate
the task of the user. These standard workflows are implemented in
functions <code><a href="#topic+standardWF">standardWF</a></code> and
<code><a href="#topic+timeseriesWF">timeseriesWF</a></code>. When specifying the vector of workflows if
you use (either in the constructor or in the function
<code><a href="#topic+workflowVariants">workflowVariants</a></code>) the parameter <code>wf</code> to indicate
which workflow you which to use. If you supply a name different from
the two provided standard workflows the function will assume that this
is a name of a function you have created to implement your own
workflow (see the Examples section for illustrations). In case you
omit the value of the <code>wf</code> parameter the function assumes you
want to use one of the standard workflows and will try to &quot;guess&quot;
which one. Namely, if you provide some value for the parameter
<code>type</code> (either &quot;slide&quot; or &quot;grow&quot;), it will assume that you are
addressing a time series task and thus will set <code>wf</code> to
<code><a href="#topic+timeseriesWF">timeseriesWF</a></code>. In all other cases will set it to
<code><a href="#topic+standardWF">standardWF</a></code>.  Summarizing, in terms of workflows you can
use: i) your own user-defined workflows; ii) the standard workflow
implemented by function <code><a href="#topic+standardWF">standardWF</a></code>; or iii) the standard
time series workflow implementd by <code><a href="#topic+timeseriesWF">timeseriesWF</a></code>.
</p>
<p>Currently, the function allows for 5 different types of estimation
methods to be used that are specified when you indicate the esitmation
task. These are different  methods for providing
reliable estimates of the true value of the selected evaluation
metrics. Both the metrics and the estimation method are defined
through the value provided in argument <code>estTask</code>. The 5
estimation methodologies are the following:
</p>
<p><em>Cross validation</em>: this type of estimates can be obtained by
providing in <code>estTask</code> argument an object of class
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code> with <code>method</code> set to an
object of class <code><a href="#topic+CV-class">CV</a></code> (this is the default). More
details on this type of method can be 
obtained in the help page of the class <code><a href="#topic+CV-class">CV</a></code>.
</p>
<p><em>Leave one out cross validation</em>: this type of estimates can be obtained by
providing in <code>estTask</code> argument an object of class
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code> with <code>method</code> set to an
object of class <code><a href="#topic+LOOCV-class">LOOCV</a></code>. More
details on this type of method can be 
obtained in the help page of the class <code><a href="#topic+LOOCV-class">LOOCV</a></code>.
</p>
<p><em>Hold out</em>: this type of estimates can be obtained by
providing in <code>estTask</code> argument an object of class
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code> with <code>method</code> set to an
object of class <code><a href="#topic+Holdout-class">Holdout</a></code>. More
details on this type of method can be 
obtained in the help page of the class <code><a href="#topic+Holdout-class">Holdout</a></code>.
</p>
<p><em>Monte Carlo</em>: this type of estimates can be obtained by
providing in <code>estTask</code> argument an object of class
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code> with <code>method</code> set to an
object of class <code><a href="#topic+MonteCarlo-class">MonteCarlo</a></code>. More
details on this type of method can be 
obtained in the help page of the class <code><a href="#topic+MonteCarlo-class">MonteCarlo</a></code>.
</p>
<p><em>Bootstrap</em>: this type of estimates can be obtained by
providing in <code>estTask</code> argument an object of class
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code> with <code>method</code> set to an
object of class <code><a href="#topic+Bootstrap-class">Bootstrap</a></code>. More
details on this type of method can be 
obtained in the help page of the class <code><a href="#topic+Bootstrap-class">Bootstrap</a></code>.
</p>


<h3>Value</h3>

<p>The result of the function is an object of class <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code>.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+workflowVariants">workflowVariants</a></code>,
<code><a href="#topic+topPerformers">topPerformers</a></code>,
<code><a href="#topic+rankWorkflows">rankWorkflows</a></code>,
<code><a href="#topic+pairedComparisons">pairedComparisons</a></code>,
<code><a href="#topic+CV">CV</a></code>,
<code><a href="#topic+LOOCV">LOOCV</a></code>,
<code><a href="#topic+Holdout">Holdout</a></code>,
<code><a href="#topic+MonteCarlo">MonteCarlo</a></code>,
<code><a href="#topic+Bootstrap">Bootstrap</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Estimating MSE for 3 variants of both
## regression trees and SVMs, on  two data sets, using one repetition
## of 10-fold CV
library(e1071)
library(DMwR)
data(swiss)
data(mtcars)

## Estimating MSE using 10-fold CV for 4 variants of a standard workflow
## using an SVM as base learner and 3 variants of a regression tree. 
res &lt;- performanceEstimation(
  c(PredTask(Infant.Mortality ~ .,swiss),PredTask(mpg ~ ., mtcars)),
  c(workflowVariants(learner="svm",
                     learner.pars=list(cost=c(1,10),gamma=c(0.01,0.5))),
    workflowVariants(learner="rpartXse",
                     learner.pars=list(se=c(0,0.5,1)))
  ),
  EstimationTask(metrics="mse")
  )

## Check a summary of the results
summary(res)

## best performers for each metric and task
topPerformers(res)


## Estimating the accuracy of a default SVM on IRIS using 10 repetitions
## of a 80%-20% Holdout
data(iris)
res1 &lt;- performanceEstimation(PredTask(Species  ~ .,iris),
             Workflow(learner="svm"),
             EstimationTask(metrics="acc",method=Holdout(nReps=10,hldSz=0.2)))
summary(res1)

## Now an example with a user-defined workflow
myWF &lt;- function(form,train,test,wL=0.5,...) {
    require(rpart,quietly=TRUE)
    ml &lt;- lm(form,train)
    mr &lt;- rpart(form,train)
    pl &lt;- predict(ml,test)
    pr &lt;- predict(mr,test)
    ps &lt;- wL*pl+(1-wL)*pr
    list(trues=responseValues(form,test),preds=ps)
}
resmywf &lt;- performanceEstimation(
             PredTask(mpg ~ ., mtcars),
             workflowVariants(wf="myWF",wL=seq(0,1,by=0.1)),
             EstimationTask(metrics="mae",method=Bootstrap(nReps=50))
           )
summary(resmywf)


## End(Not run)

</code></pre>

<hr>
<h2 id='PredTask-class'>Class &quot;PredTask&quot; </h2><span id='topic+PredTask'></span><span id='topic+PredTask-class'></span><span id='topic+show+2CPredTask-method'></span>

<h3>Description</h3>

<p>	 This is the class of objects that represent a predictive task  </p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls to the constructor
<code>PredTask(...)</code>. The constructor requires a formula and a data
frame on the first two arguments. You may also a name for the task
through the parameter <code>taskName</code> of the constructor. Optional
parameter <code>type</code> allows you to indicate the type of task (either
&quot;regr&quot;, &quot;class&quot; or &quot;ts&quot;, for regression, classification and time
series tasks, respectively). If not provided this will be inferred
from constructor. Setting the optional parameter <code>copy</code> to
<code>TRUE</code> (defaults to <code>FALSE</code>)  will force the constructor to make a copy
of the given data frame and store it in the <code>dataSource</code> slot of
the  <code>PredTask</code> object.
</p>


<h3>Slots</h3>


<dl>
<dt><code>formula</code>:</dt><dd><p>Object of class <code>formula</code> containing
the formula representing the predictive task. </p>
</dd>
<dt><code>dataSource</code>:</dt><dd><p>Object of class <code>data.frame</code>,
<code>call</code> or <code>name</code>. This will be used to fecth the task
data when necessary. The first of these options will only be used
if the user calls the constructor with <code>copy=TRUE</code> and will
result in the source data being copied into the <code>PredTask</code> object.</p>
</dd>
<dt><code>taskName</code>:</dt><dd><p>Optional object of class <code>character</code> containing
the ID of the predictive task </p>
</dd>
<dt><code>type</code>:</dt><dd><p>Optional object of class <code>character</code> containing
the type of the predictive task (it can be &quot;regr&quot;, &quot;class&quot; or &quot;ts&quot;). </p>
</dd>
<dt><code>target</code>:</dt><dd><p>Optional object of class <code>character</code> containing the
name of the target variable.</p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>show</dt><dd><p><code>signature(object = "PredTask")</code>: method used to
show the contents of a PredTask object. </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Workflow-class">Workflow</a></code>,
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code>,
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("PredTask")
data(iris)
PredTask(Species  ~ .,iris)
PredTask(Species  ~ .,iris[1:20,],"irisSubset")
## after the next example you can safely remove the iris data frame from
## your environment because the data was stored inside the "t" object.
t &lt;- PredTask(Species ~ ., iris,copy=TRUE) 
</code></pre>

<hr>
<h2 id='rankWorkflows'>
Provide a ranking of workflows involved in an estimation process.
</h2><span id='topic+rankWorkflows'></span>

<h3>Description</h3>

<p>Given a <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> object resulting from a
performance estimation experiment, this function provides a ranking
(by default the top 5) of  
the <em>best</em> workflows involved in the comparison. The rankings are provided by
task and for each evaluation metric. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rankWorkflows(compRes,top=min(5,length(workflowNames(compRes))),
              maxs=rep(FALSE,dim(compRes[[1]][[1]]@iterationsScores)[2]),stat="avg")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rankWorkflows_+3A_compres">compRes</code></td>
<td>

<p>An object of class <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> with the results of the
performance estimation experiment.
</p>
</td></tr>
<tr><td><code id="rankWorkflows_+3A_top">top</code></td>
<td>

<p>The number of workflows to include in the rankings (defaulting to 5
or the number of workflows in the experiment if less than 5)
</p>
</td></tr>
<tr><td><code id="rankWorkflows_+3A_maxs">maxs</code></td>
<td>

<p>A vector of booleans with as many elements are there are statistics measured in
the experimental comparison. A <code>TRUE</code> value means the respective
metric is to be maximized, while a <code>FALSE</code> means
minimization. Defaults to all <code>FALSE</code> values.
</p>
</td></tr>
<tr><td><code id="rankWorkflows_+3A_stat">stat</code></td>
<td>

<p>The statistic to be used to obtain the ranks. The options are the
statistics produced by the function <code>summary</code> applied to objects
of class  <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code>, i.e. &quot;avg&quot;, &quot;std&quot;,
&quot;med&quot;, &quot;iqr&quot;, &quot;min&quot;, &quot;max&quot; or &quot;invalid&quot; (defaults to &quot;avg&quot;).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a named list with as many components as there
are predictive tasks in the experiment. For each task you will get
another named list, with as many elements as there evaluation
metrics. For each of these components you have a data frame with <em>N</em>
lines, where <em>N</em> is the size of the requested rank. Each line includes
the name of the workflow in the respective rank position and the
estimated score it got on that particular task / evaluation metric.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+performanceEstimation">performanceEstimation</a></code>,
<code><a href="#topic+topPerformers">topPerformers</a></code>,
<code><a href="#topic+topPerformer">topPerformer</a></code>,
<code><a href="#topic+metricsSummary">metricsSummary</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Estimating several evaluation metrics on different variants of a
## regression tree and of a SVM, on  two data sets, using one repetition
## of  10-fold CV

data(swiss)
data(mtcars)
library(e1071)

## run the experimental comparison
results &lt;- performanceEstimation(
               c(PredTask(Infant.Mortality ~ ., swiss),
                 PredTask(mpg ~ ., mtcars)),
               c(workflowVariants(learner='svm',
                                  learner.pars=list(cost=c(1,5),gamma=c(0.1,0.01))
                                 )
               ),
               EstimationTask(metrics=c("mse","mae"),method=CV(nReps=2,nFolds=5))
                                 )
## get a ranking of the top workflows for each task and evaluation metric
rankWorkflows(results)
## get a ranking of the top workflows for each task and evaluation
## metric by the median score on all iterations instead of the mean score
rankWorkflows(results, stat="med")

## End(Not run)
</code></pre>

<hr>
<h2 id='regressionMetrics'>
Calculate some standard regression evaluation metrics of predictive performance
</h2><span id='topic+regressionMetrics'></span>

<h3>Description</h3>

<p>This function is able to calculate a series of regression evaluation
statistics given two vectors: one with the true target variable values,
and the other with the predicted target variable values. Some of the
metrics may require additional information to be given (see Details section).		
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regressionMetrics(trues, preds, metrics = NULL, train.y = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regressionMetrics_+3A_trues">trues</code></td>
<td>

<p>A numeric vector with the true values of the target variable.
</p>
</td></tr>
<tr><td><code id="regressionMetrics_+3A_preds">preds</code></td>
<td>

<p>A numeric vector with the predicted values of the target variable.
</p>
</td></tr>
<tr><td><code id="regressionMetrics_+3A_metrics">metrics</code></td>
<td>

<p>A vector with the names of the evaluation statistics to
calculate (see Details section). If none is indicated (default) it will
calculate all available metrics of this function. 
</p>
</td></tr>
<tr><td><code id="regressionMetrics_+3A_train.y">train.y</code></td>
<td>

<p>An optional numeric vector with the values
of the target variable on the set of data used to obtain the model
whose performance is being tested.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the
following description of the currently available metrics we denote the
vector of true target variable values as t, the vector of predictions
by p, while N denotes the size of these two vectors, i.e. the number
of test cases.
</p>
<p>The regression evaluation statistics calculated by this function belong
to two different groups of measures: absolute and relative. In terms of
absolute error metrics the function includes currently the following:
</p>
<p>&quot;mae&quot;: mean absolute error, which is calculated as sum(|t_i - p_i|)/N
</p>
<p>&quot;mse&quot;: mean squared error, which is calculated as sum( (t_i - p_i)^2
)/N
</p>
<p>&quot;rmse&quot;: root mean squared error that is calculated as sqrt(mse)
</p>
<p>The remaining measures (&quot;mape&quot;, &quot;nmse&quot;, &quot;nmae&quot; and &quot;theil&quot;) are relative
measures, the three later
comparing the performance of the model with a baseline. They are
unit-less measures with values always greater than 0. In the case of
&quot;nmse&quot;, &quot;nmae&quot; and &quot;theil&quot; the values are expected to be in the interval [0,1]
though occasionaly scores can overcome 1, which means that your model
is performing worse than the baseline model. The baseline used in both
&quot;nmse&quot; and &quot;nmae&quot; is a constant model that always predicts the average
target variable value, estimated using the values of this variable on
the training data (data used to obtain the model that generated the
predictions), which should be 
provided in the parameter <code>train.y</code>. The &quot;theil&quot; metric is
typically used in time series tasks and the used baseline is the last
observed value of the target variable. The relative error measure
&quot;mape&quot; does not require a baseline. It simply calculates the average
percentage difference between the true values and the
predictions.
</p>
<p>These measures are calculated as follows:
</p>
<p>&quot;mape&quot;: sum(|(t_i - p_i) / t_i|)/N
</p>
<p>&quot;nmse&quot;: sum( (t_i - p_i)^2 ) / sum( (t_i - AVG(Y))^2 ), where AVG(Y)
is the average of the values provided in vector <code>train.y</code>
</p>
<p>&quot;nmae&quot;: sum(|t_i - p_i|) / sum(|t_i - AVG(Y)|)
</p>
<p>&quot;theil&quot;: sum( (t_i - p_i)^2 ) / sum( (t_i - t_{i-1})^2 ), where
t_{i-1} is the last observed value of the target variable
</p>
<p>The user may also indicate the value &quot;all&quot; in the parameter
<code>metrics</code>. In this case all possible metrics will be
calculated. This will only include the &quot;nmse&quot;, &quot;nmae&quot; and &quot;theil&quot; metrics if
the value of the <code>train.y</code> parameter is set, otherwise only the
other metrics will be returned.
</p>


<h3>Value</h3>

<p>A named vector with the calculated evaluation scores.
</p>


<h3>Note</h3>

<p>In case you require either &quot;nmse&quot;, &quot;nmae&quot; or &quot;theil&quot; to be calculated you must
supply a vector of numeric values through the parameter
<code>train.y</code>, otherwise the function will return an error
message. These values are required to obtain a fair baseline against
which your model predictions will be compared to. 
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+classificationMetrics">classificationMetrics</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Calculating several statistics of a regression tree on the Swiss data
data(swiss)
idx &lt;- sample(1:nrow(swiss),as.integer(0.7*nrow(swiss)))
train &lt;- swiss[idx,]
test &lt;- swiss[-idx,]
library(rpart)
model &lt;- rpart(Infant.Mortality ~ .,train)
preds &lt;- predict(model,test)
## by default only mse is calculated
regressionMetrics(test[,'Infant.Mortality'],preds)
## calculate mae and rmse
regressionMetrics(test[,'Infant.Mortality'],preds,metrics=c('mae','rmse'))
## calculate all statistics
regressionMetrics(test[,'Infant.Mortality'],preds,train.y=train[,'Infant.Mortality'])

## End(Not run)
</code></pre>

<hr>
<h2 id='responseValues'>
Obtain the target variable values of a prediction task
</h2><span id='topic+responseValues'></span>

<h3>Description</h3>

<p>This function obtains the values in the column whose name is the
target variable of a prediction problem described by a formula.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>responseValues(formula, data, na=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="responseValues_+3A_formula">formula</code></td>
<td>

<p>A formula describing a prediction problem
</p>
</td></tr>
<tr><td><code id="responseValues_+3A_data">data</code></td>
<td>

<p>The data frame containing the data of the prediction problem
</p>
</td></tr>
<tr><td><code id="responseValues_+3A_na">na</code></td>
<td>

<p>A function to handle NAs in the data
</p>
</td></tr></table>


<h3>Value</h3>

<p>A vector of values
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
tgt &lt;- responseValues(Species ~ ., iris)
summary(tgt)
</code></pre>

<hr>
<h2 id='results2table'>
Obtains a dplyr data frame table object containing all the results of
an experiment
</h2><span id='topic+results2table'></span>

<h3>Description</h3>

<p>This function produces a dplyr data frame table object with the
information on all iterations of an experiment. This type of objects
may be easier to manipulate in terms of querying these results,
particular for larger experiments involving lots of tasks, workflows
and metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>results2table(res)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="results2table_+3A_res">res</code></td>
<td>

<p>This is a <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> object (type &quot;class?ComparisonResults&quot; for details)
that contains the results of a performance estimation  experiment obtained
through the <code>performanceEstimation()</code> function. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a dplyr data frame table object containing all
resutls of the experiment. The object has the columns: Task, Workflow,
nrIt, Metric and Score. Each row is one train+test cycle within the
experiment, i.e. contains the score of some metric obtained by some
workflow on one train+test iteration of a task.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getScores">getScores</a></code>, <code><a href="#topic+performanceEstimation">performanceEstimation</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Estimating MSE for 3 variants of both
## regression trees and SVMs, on  two data sets, using one repetition
## of 10-fold CV
library(e1071)
data(swiss)

## running the estimation experiment
res &lt;- performanceEstimation(
  PredTask(Infant.Mortality ~ .,swiss,"Swiss"),
  workflowVariants(learner="svm",
                   learner.pars=list(cost=c(1,10),gamma=c(0.01,0.5))),
  EstimationTask(metrics=c("mse","nmae"),method=CV(nReps=2,nFolds=5))
  )

## Obtaining a table with the results
library(dplyr)
tbl &lt;- results2table(res)

## Mean and standard deviation of each workflow per task (only one in
## this example) and metric
group_by(tbl,Task,Workflow,Metric) 
     summarize_each_(funs(mean,sd),"Score")

## Top 2 workflows in terms of MSE for this task
filter(tbl,Task=="Swiss",Metric=="mse") 
    group_by(Workflow) 
      summarize_each_(funs(mean),"Score") 
        arrange(Score) 
          slice(1:2)

## End(Not run)
</code></pre>

<hr>
<h2 id='runWorkflow'>
Run a workflow on a predictive task
</h2><span id='topic+runWorkflow'></span>

<h3>Description</h3>

<p>This function can be used to apply (run) a workflow.
</p>
<p>It receives in the first argument an object of class
<code><a href="#topic+Workflow-class">Workflow</a></code> and then any other arguments that are
required to run the workflow, which will typically involve at least a
formula, a training data set (a data frame), and a test data set
(another data frame).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>runWorkflow(l, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="runWorkflow_+3A_l">l</code></td>
<td>

<p>An object of class <code><a href="#topic+Workflow-class">Workflow</a></code>
</p>
</td></tr>
<tr><td><code id="runWorkflow_+3A_...">...</code></td>
<td>

<p>Further arguments that are required by the workflow function
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The execution of a workflow should produce a list as result. If you
plan to apply any of the functions provided in this package to
calculate standard performance metrics
(<code><a href="#topic+classificationMetrics">classificationMetrics</a></code> or
<code><a href="#topic+regressionMetrics">regressionMetrics</a></code>) then your list should contain at
least two components: one named <em>trues</em> with the true values of
the target variable in the test set, and the other named <em>preds</em>
with the respective predictions of the workflow.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Workflow-class">Workflow</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## A simple example
data(iris)
w1 &lt;- Workflow("mySolution",par1="gaussian",k=10)
runWorkflow(w1,Species ~ .,iris[1:100,],iris[101:150,])

## End(Not run)
</code></pre>

<hr>
<h2 id='signifDiffs'>
Obtains a list with the set of paired differences that are statistically
significant according to a p-value threshold
</h2><span id='topic+signifDiffs'></span>

<h3>Description</h3>

<p>This function receives as main argument the object resulting from a
call to the <code><a href="#topic+pairedComparisons">pairedComparisons</a></code> function and produces a
list with the subset of the paired comparisons using the <em>t</em> test and
the <em>Wilcoxon Signed Rank</em> test that are statistically
significant given a certain <em>p</em> value limit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>signifDiffs(ps, p.limit=0.05,
            metrics=names(ps),
            tasks=rownames(ps[[1]]$avgScores))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="signifDiffs_+3A_ps">ps</code></td>
<td>

<p>An object resulting from a call to the
<code><a href="#topic+pairedComparisons">pairedComparisons</a></code> function. 
</p>
</td></tr>
<tr><td><code id="signifDiffs_+3A_p.limit">p.limit</code></td>
<td>

<p>A number indicating the maximum value of the confidence level
(p.value) of the statistical hypothesis test for a paired comparison
to be considered statistically significant (defaults to 0.05). All
paired comparisons with a <em>p</em> value below this limit will
appear in the results of this function.
</p>
</td></tr>
<tr><td><code id="signifDiffs_+3A_metrics">metrics</code></td>
<td>

<p>A vector with the names of the metrics for which we want the results
(defaults to all metrics included in the paired comparison).
</p>
</td></tr>
<tr><td><code id="signifDiffs_+3A_tasks">tasks</code></td>
<td>

<p>A vector with the names of the prediction tasks for which we want the results
(defaults to all tasks included in the paired comparison).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function produces a list with as many components as the selected
metrics (defaulting to all metrics in the paired comparison). Each of
the components is another list with two components: i) one with the
results for the <em>t</em> tests; and ii) the other with the results for
the <em>Wilcoxon Signed Rank</em> test. Each of these two components is
an array with 3 dimensions, with the rows representing the workflows,
the columns a set of statistics and the thrid dimension being the
task. The first row of these arrays will contain the baseline workflow
against which all others are being compared (by either the <em>t</em>
test or through the <em>Wilcoxon Signed Rank</em> test). The remaining
rows will include the workflows whose comparison against this baseline
is statistically significant, i.e. whose <em>p</em> value of the paired
comparison is below the provided <em>p</em> limit.
</p>


<h3>Value</h3>

<p>The result of this function is a list (see the Details section).
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pairedComparisons">pairedComparisons</a></code>,
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Estimating MSE for 3 variants of both
## regression trees and SVMs, on  two data sets, using one repetition
## of 10-fold CV
library(e1071)
data(iris)
data(Satellite,package="mlbench")
data(LetterRecognition,package="mlbench")


## running the estimation experiment
res &lt;- performanceEstimation(
           c(PredTask(Species ~ .,iris),
             PredTask(classes ~ .,Satellite,"sat"),
             PredTask(lettr ~ .,LetterRecognition,"letter")),
           workflowVariants(learner="svm",
                 learner.pars=list(cost=1:4,gamma=c(0.1,0.01))),
           EstimationTask(metrics=c("err","acc"),method=CV()))

## now let us assume that we will choose "svm.v2" as our baseline
## carry out the paired comparisons
pres &lt;- pairedComparisons(res,"svm.v2")

## Obtaining the subset of differences that are significant
## with 99% confidence 
sds &lt;- signifDiffs(res,p.limit=0.01)


## End(Not run)
</code></pre>

<hr>
<h2 id='smote'>
SMOTE algorithm for unbalanced classification problems
</h2><span id='topic+smote'></span>

<h3>Description</h3>

<p>This function handles unbalanced classification problems using the SMOTE
method. Namely, it can generate a new &quot;SMOTEd&quot; data set that addresses
the class unbalance problem. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smote(form, data, perc.over = 2, k = 5, perc.under = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smote_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem
</p>
</td></tr>
<tr><td><code id="smote_+3A_data">data</code></td>
<td>

<p>A data frame containing the original (unbalanced) data set
</p>
</td></tr>
<tr><td><code id="smote_+3A_perc.over">perc.over</code></td>
<td>

<p>A number that drives the decision of how many extra cases from the
minority class are generated (known as over-sampling).
</p>
</td></tr>
<tr><td><code id="smote_+3A_k">k</code></td>
<td>

<p>A number indicating the number of nearest neighbours that are used to
generate the new examples of the minority class.
</p>
</td></tr>
<tr><td><code id="smote_+3A_perc.under">perc.under</code></td>
<td>

<p>A number that drives the decision of how many extra cases from the
majority classes are selected for each case generated from the
minority class (known as under-sampling)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Unbalanced classification problems cause problems to many learning
algorithms. These problems are characterized by  the uneven proportion
of cases that are available for each class of the problem.
</p>
<p>SMOTE (Chawla et. al. 2002) is a well-known algorithm to fight this
problem. The general idea of this method is to artificially generate
new examples of the minority class using the nearest neighbors of
these cases. Furthermore, the majority class examples are also
under-sampled, leading to a more balanced dataset. 
</p>
<p>The parameters <code>perc.over</code> and <code>perc.under</code> control the amount
of over-sampling of the minority class and under-sampling of the
majority classes, respectively. <code>perc.over</code> will tipically be a
number above 1. With this type of values, for each case in the orginal
data set belonging to the minority class, <code>perc.over</code> new
examples of that class will be created. If <code>perc.over</code> is a value
below 1 than a single case will be generated for a randomly selected
proportion (given by <code>perc.over</code>) of the cases belonging to the
minority class on the original data set. The parameter <code>perc.under</code>
controls the proportion of cases of the majority class that will be
randomly selected for the final &quot;balanced&quot; data set. This proportion is
calculated with respect to the number of newly generated minority class
cases. For instance, if 200 new examples were generated for the minority
class, a value of <code>perc.under</code> of 1 will randomly select exactly 200 cases
belonging to the majority classes from the original data set to belong
to the final data set. Values above 1 will select more examples from
the majority classes. 
</p>
<p>The parameter <code>k</code> controls the way the new examples are
created. For each currently existing minority class example X new
examples will be created (this is controlled by the parameter
<code>perc.over</code> as mentioned above). These examples will be generated
by using the information from the <code>k</code> nearest neighbours of each
example of the minority class. The parameter <code>k</code> controls how many
of these neighbours are used.
</p>


<h3>Value</h3>

<p>The function returns a data frame with
the new data set resulting from the application of the SMOTE
algorithm. 
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Chawla, N. V., Bowyer, K. W., Hall, L. O., and Kegelmeyer, W. P. (2002).
<em>Smote: Synthetic minority over-sampling technique</em>. Journal of Artificial
Intelligence Research, 16:321-357.
</p>
<p>Torgo, L. (2010) <em>Data Mining using R: learning with case studies</em>,
CRC Press (ISBN: 9781439810187).
<a href="http://www.dcc.fc.up.pt/~ltorgo/DataMiningWithR">http://www.dcc.fc.up.pt/~ltorgo/DataMiningWithR</a> 
</p>
<p>Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A small example with a data set created artificially from the IRIS
## data 
data(iris)
data &lt;- iris[, c(1, 2, 5)]
data$Species &lt;- factor(ifelse(data$Species == "setosa","rare","common")) 
## checking the class distribution of this artificial data set
table(data$Species)

## now using SMOTE to create a more "balanced problem"
newData &lt;- smote(Species ~ ., data, perc.over = 6,perc.under=1)
table(newData$Species)

## Checking visually the created data
## Not run: 
par(mfrow = c(1, 2))
plot(data[, 1], data[, 2], pch = 19 + as.integer(data[, 3]),
     main = "Original Data")
plot(newData[, 1], newData[, 2], pch = 19 + as.integer(newData[,3]),
     main = "SMOTE'd Data")

## End(Not run)

</code></pre>

<hr>
<h2 id='standardPOST'>
A function for applying post-processing steps to the predictions of a model
</h2><span id='topic+standardPOST'></span>

<h3>Description</h3>

<p>This function implements a series of simple post-processing steps
to be applied to the predictions of a model. It also allows the user
to supply hers/his own post-processing functions. The result of the
function is a new version of the predictions of the model (typically a
vector or a matrix in the case of models that predict class
probabilities, for instance).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>standardPOST(form, train, test, preds, steps, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="standardPOST_+3A_form">form</code></td>
<td>

<p>A formula specifying the predictive task.
</p>
</td></tr>
<tr><td><code id="standardPOST_+3A_train">train</code></td>
<td>

<p>A data frame containing the training set.
</p>
</td></tr>
<tr><td><code id="standardPOST_+3A_test">test</code></td>
<td>

<p>A data frame containing the test set.
</p>
</td></tr>
<tr><td><code id="standardPOST_+3A_preds">preds</code></td>
<td>

<p>The object resulting from the application of a model to the test set
to obtain its predictions (typically a vector or a matrix for
probabilistic classifiers) 
</p>
</td></tr>
<tr><td><code id="standardPOST_+3A_steps">steps</code></td>
<td>

<p>A vector with function names that are to be applied in the sequence
they appear in this vector to the predictions to obtain a new
version of these predictions.
</p>
</td></tr>
<tr><td><code id="standardPOST_+3A_...">...</code></td>
<td>

<p>Any further parameters that will be passed to all functions
specified in <code>steps</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is mainly used by both <code><a href="#topic+standardWF">standardWF</a></code> and
<code><a href="#topic+timeseriesWF">timeseriesWF</a></code> as a means to allow for users of these two
standard workflows to specify some post-processing steps for the
predictions of the models. These
are steps one wishes to apply to the predictions to somehow change the
outcome of the prediction stage.
</p>
<p>Nevertheless, the function can also be used outside of these standard
workflows for obtaining post-processed versions of the predictions.
</p>
<p>The function accepts as post-processing functions both some already
implemented functions as well as any function defined by the user
provided these follow some protocol. Namely, these user-defined
post-processing functions should be aware that they will be called with
a formula, a training data frame, a testing data frame and the
predictions in the first
four arguments. Moreover, any arguments used in the call to
<code>standardPOST</code> will also be forwarded to these user-defined
functions. Finally, these functions should return a new version of the
predictions. It is questionable the interest of supplying both the
training and test sets to these functions, on top of the formula and
the predictions. However, we have decided to pass them anyway not to
precule the usage of any special post-processing step that requires
this information. 
</p>
<p>The function already contains implementations of the following
post-processing steps that can be used in the <code>steps</code> parameter:
</p>
<p>&quot;na2central&quot; - this function fills in any <code>NA</code> predictions into
either the median (numeric targets) or mode (nominal targets) of the
target variable on the training set. Note that this is only applicable
to predictions that are vectors of values.
</p>
<p>&quot;onlyPos&quot; - in some numeric forecasting tasks the target variable
takes only positive values. Nevertheless, some models may insist in
forecasting negative values. This function casts these negative values
to zero. Note that this is only applicable
to predictions that are vectors of numeric values.
</p>
<p>&quot;cast2int&quot; - in some numeric forecasting tasks the target variable
takes only values within some interval. Nevertheless, some models may
insist in forecasting  values outside of this interval. This function
casts these values  into the nearest interval boundary. This function
requires that you supply the limits of this interval through
parameters <code>infLim</code> and <code>supLim</code>. Note that this is only
applicable to predictions that are vectors of numeric values.
</p>
<p>&quot;maxutil&quot; - maximize the utility of the predictions (Elkan, 2001) of a
classifier. This method is only applicable to classification tasks and
to algorithms that are able to produce as predictions a vector of
class probabilities for each test case, i.e. a matrix of probabilities
for a given test set. The method requires a cost-benefit matrix to be
provided through the parameter <code>cb.matrix</code>. For each test case,
and given the probabilities estimated by the classifier and the cost
benefit matrix, the method predicts the classifier that maximizes the
utility of the prediction. This approach (Elkan, 2001) is a slight
'evolution' of the original idea (Breiman et al., 1984) that only
considered the costs of errors and not the benefits of the correct
classifications as in the case of cost-benefit matrices we are using
here. The parameter <code>cb.matrix</code> must contain a (square) matrix of
dimension NClasses x NClasses where entry X_i,j corresponds to the
cost/benefit of predicting a test case as belonging to class j when it
is of class i. The diagonal of this matrix (correct predicitons)
should contain positive numbers (benefits), whilst numbers outside of
the matrix should contain negative numbers (costs of
misclassifications). See the Examples section for an illustration.
</p>


<h3>Value</h3>

<p>An object of the same class as the input parameter <code>preds</code>
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Breiman,L., Friedman,J., Olshen,R. and Stone,C. (1984),
<em>Classification and Regression Trees</em>, Wadsworth and Brooks.
</p>
<p>Elkan, C. (2001),  <em>The Foundations of Cost-Sensitive Learning</em>. Proceedings of
IJCAI'2001.
</p>
<p>Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+standardPRE">standardPRE</a></code>,
<code><a href="#topic+standardWF">standardWF</a></code>,
<code><a href="#topic+timeseriesWF">timeseriesWF</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

######
## Using in the context of an experiment


data(algae,package="DMwR")
library(e1071)

## This will issue several warnings because this implementation of SVMs
## will ignore test cases with NAs in some predictor. Our infra-structure
## issues a warning and fills in these with the prediction of an NA
res &lt;- performanceEstimation(
  PredTask(a1 ~ .,algae[,1:12],"alga1"),
  Workflow(learner="svm"),
  EstimationTask(metrics="mse")
  )
summary(getIterationPreds(res,1,1,it=1))

## one way of overcoming this would be to post-process the NA
## predictions into a statistic of centrality
resN &lt;- performanceEstimation(
  PredTask(a1 ~ .,algae[,1:12],"alga1"),
  Workflow(learner="svm",post="na2central"),
  EstimationTask(metrics="mse")
  )
summary(getIterationPreds(resN,1,1,it=1))

## because the SVM also predicts negative values which does not make
## sense in this application (the target are frequencies thus &gt;= 0) we
## could also include some further post-processing to take care of
## negative predictions
resN &lt;- performanceEstimation(
  PredTask(a1 ~ .,algae[,1:12],"alga1"),
  Workflow(learner="svm",post=c("na2central","onlyPos")),
  EstimationTask(metrics="mse")
  )
summary(getIterationPreds(resN,1,1,it=1))

######################
## An example with utility maximization learning for the
## BreastCancer data set on package mlbench
##
data(BreastCancer,package="mlbench")

## First lets create the cost-benefit matrix
cb &lt;- matrix(c(1,-10,-100,100),byrow=TRUE,ncol=2)
colnames(cb) &lt;- paste("p",levels(BreastCancer$Class),sep=".")
rownames(cb) &lt;- paste("t",levels(BreastCancer$Class),sep=".")

## This leads to the following cost-benefit matrix
##             p.benign p.malignant
## t.benign           1         -10
## t.malignant     -100         100

## Now the performance estimation. We are estimating error rate (wrong
## for cost sensitive tasks!) and total utility of the model predictions
## (the right thing to do here!)
library(rpart)

r &lt;- performanceEstimation(
        PredTask(Class ~ .,BreastCancer[,-1],"breastCancer"),
        c(Workflow(wfID="rpart.cost",
                   learner="rpart",
                   post="maxutil",
                   post.pars=list(cb.matrix=cb)
                  ),
          Workflow(wfID="rpart",
                   learner="rpart",
                   predictor.pars=list(type="class")
                  )
          ),
         EstimationTask(
              metrics=c("err","totU"),
              evaluator.pars=list(benMtrx=cb,posClass="malignant"),
              method=CV(strat=TRUE)))

## Analysing the results
rankWorkflows(r,maxs=c(FALSE,TRUE))

## Visualizing them
plot(r)


## End(Not run)

</code></pre>

<hr>
<h2 id='standardPRE'>
A function for applying data pre-processing steps
</h2><span id='topic+standardPRE'></span>

<h3>Description</h3>

<p>This function implements a series of simple data pre-processing steps
and also allows the user to supply hers/his own functions to be
applied to the data. The result of the function is a list containing
the new (pre-processed) versions of the given train and test sets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>standardPRE(form, train, test, steps, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="standardPRE_+3A_form">form</code></td>
<td>

<p>A formula specifying the predictive task.
</p>
</td></tr>
<tr><td><code id="standardPRE_+3A_train">train</code></td>
<td>

<p>A data frame containing the training set.
</p>
</td></tr>
<tr><td><code id="standardPRE_+3A_test">test</code></td>
<td>

<p>A data frame containing the test set.
</p>
</td></tr>
<tr><td><code id="standardPRE_+3A_steps">steps</code></td>
<td>

<p>A vector with function names that are to be applied in the sequence
they appear in this vector to both the training and testing sets, to
obtain new versions of these two data samples.
</p>
</td></tr>
<tr><td><code id="standardPRE_+3A_...">...</code></td>
<td>

<p>Any further parameters that will be passed to all functions
specified in <code>steps</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is mainly used by both <code><a href="#topic+standardWF">standardWF</a></code> and
<code><a href="#topic+timeseriesWF">timeseriesWF</a></code> as a means to allow for users of these two
standard workflows to specify some data pre-processing steps. These
are steps one wishes to apply to the different train and test samples
involved in an experimental comparison, before any model is learned or
any predictions are obtained.
</p>
<p>Nevertheless, the function can also be used outside of these standard
workflows for obtaining pre-processed versions of train and test
samples.
</p>
<p>The function accepts as pre-processing functions both some already
implemented functions as well as any function defined by the user
provided these follow some protocol. Namely, these user-defined
pre-processing functions should be aware that they will be called with
a formula, a training data frame and a testing data frame in the first
three arguments. Moreover, any arguments used in the call to
<code>standardPRE</code> will also be forwarded to these user-defined
functions. Finally, these functions should return a list with two
components: &quot;train&quot; and &quot;test&quot;, containing the pre-processed versions
of the supplied train and test data frames.
</p>
<p>The function already contains implementations of the following
pre-processing steps that can be used in the <code>steps</code> parameter:
</p>
<p>&quot;scale&quot; - that scales (subtracts the mean and divides by the standard
deviation) any knnumeric features on both the training and testing
sets. Note that the mean and standard deviation are calculated using
only the training sample.
</p>
<p>&quot;centralImp&quot; - that fills in any <code>NA</code> values in both sets using
the median value for numeric predictors and the mode for nominal
predictors. Once again these centrality statistics are calculated
using only the training set although they are applied to both train
and test sets.
</p>
<p>&quot;knnImp&quot; - that fills in any <code>NA</code> values in both sets using
the median value for numeric predictors and the mode for nominal
predictors, but using only the k-nearest neighbors to calculate these satistics.
</p>
<p>&quot;na.omit&quot; - that uses the R function <code><a href="stats.html#topic+na.omit">na.omit</a></code> to remove
any rows containing <code>NA</code>'s  from both the training and test sets.
</p>
<p>&quot;undersampl&quot; - this undersamples the training data cases that do not
belong to the minority class (this pre-processing step is only
available for classification tasks!). It takes the parameter
<code>perc.under</code> that controls the level of undersampling
(defaulting to 1, which means that there would be as many cases from
the minority as from the other(s) class(es)).
</p>
<p>&quot;smote&quot; - this operation uses the SMOTE (Chawla et. al. 2002)
resampling algorithm to  generate a new training sample with a more
&quot;balanced&quot; distributions of the target class (this pre-processing step
is only available for classification tasks!). It takes the parameters
<code>perc.under</code>,  <code>perc.over</code> and <code>k</code> to control the
algorithm. Read the  documentation of function <code><a href="#topic+smote">smote</a></code> to
know more details. 
</p>


<h3>Value</h3>

<p>A list with components &quot;train&quot; and &quot;test&quot; with both containing a data frame.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Chawla, N. V., Bowyer, K. W., Hall, L. O., and Kegelmeyer, W. P. (2002).
<em>Smote: Synthetic minority over-sampling technique</em>. Journal of Artificial
Intelligence Research, 16:321-357.
</p>
<p>Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+standardPOST">standardPOST</a></code>,
<code><a href="#topic+standardWF">standardWF</a></code>,
<code><a href="#topic+timeseriesWF">timeseriesWF</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

##  A small example with standard pre-preprocessing: clean NAs and scale
data(algae,package="DMwR")

idx &lt;- sample(1:nrow(algae),150)
tr &lt;- algae[idx,1:12]
ts &lt;- algae[-idx,1:12]
summary(tr)
summary(ts)

preData &lt;- standardPRE(a1 ~ ., tr, ts, steps=c("centralImp","scale"))
summary(preData$train)
summary(preData$test)

######
## Using in the context of an experiment
library(e1071)
res &lt;- performanceEstimation(
  PredTask(a1 ~ .,algae[,1:12],"alga1"),
  Workflow(learner="svm",pre=c("centralImp","scale")),
  EstimationTask(metrics="mse")
  )

summary(res)

######
## A user-defined pre-processing function
myScale &lt;- function(f,tr,ts,avg,std,...) {
    tgtVar &lt;- deparse(f[[2]])
    allPreds &lt;- setdiff(colnames(tr),tgtVar)
    numPreds &lt;- allPreds[sapply(allPreds,
                          function(p) is.numeric(tr[[p]]))]
    tr[,numPreds] &lt;- scale(tr[,numPreds],center=avg,scale=std)
    ts[,numPreds] &lt;- scale(ts[,numPreds],center=avg,scale=std)
    list(train=tr,test=ts)
}

## now using it with some random averages and stds for the 8 numeric
## predictors (just for illustration)
newData &lt;- standardPRE(a1 ~ .,tr,ts,steps="myScale",
                       avg=rnorm(8),std=rnorm(8))


## End(Not run)

</code></pre>

<hr>
<h2 id='standardWF'>
A function implementing a standard workflow for prediction tasks
</h2><span id='topic+standardWF'></span>

<h3>Description</h3>

<p>This function implements a standard workflow for both  classification
and regression tasks. The workflow consists of: (i) learning a
predictive model based on the given training set, (ii) using it to
make predictions for the provided test set,  and finally  (iii)
measuring some evaluation metrics of its performance. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>standardWF(form,train,test,
    learner,learner.pars=NULL,
    predictor='predict',predictor.pars=NULL,
    pre=NULL,pre.pars=NULL,
    post=NULL,post.pars=NULL,
    .fullOutput=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="standardWF_+3A_form">form</code></td>
<td>

<p>A formula specifying the predictive task.
</p>
</td></tr>
<tr><td><code id="standardWF_+3A_train">train</code></td>
<td>

<p>A data frame containing the data set to be used for obtaining the
predictive model (the training set).
</p>
</td></tr>
<tr><td><code id="standardWF_+3A_test">test</code></td>
<td>

<p>A data frame containing the data set to be used for testing the
obtained  model (the test set).
</p>
</td></tr>
<tr><td><code id="standardWF_+3A_learner">learner</code></td>
<td>

<p>A character string with the name of a function that is to be used to
obtain the prediction models.
</p>
</td></tr>
<tr><td><code id="standardWF_+3A_learner.pars">learner.pars</code></td>
<td>

<p>A list of parameter values to be passed to the learner (defaults to <code>NULL</code>).
</p>
</td></tr>
<tr><td><code id="standardWF_+3A_predictor">predictor</code></td>
<td>

<p>A character string with the name of a function that is to be used to
obtain the predictions for the test set using the obtained model
(defaults to 'predict').
</p>
</td></tr>
<tr><td><code id="standardWF_+3A_predictor.pars">predictor.pars</code></td>
<td>

<p>A list of parameter values to be passed to the predictor (defaults
to <code>NULL</code>).
</p>
</td></tr>
<tr><td><code id="standardWF_+3A_pre">pre</code></td>
<td>

<p>A vector of function names that will be applied in sequence to the train
and test data frames, generating new versions, i.e. a sequence of data
pre-processing functions.
</p>
</td></tr>
<tr><td><code id="standardWF_+3A_pre.pars">pre.pars</code></td>
<td>

<p>A named list of parameter values to be passed to the pre-processing functions.
</p>
</td></tr>
<tr><td><code id="standardWF_+3A_post">post</code></td>
<td>

<p>A vector of function names that will be applied in sequence to the
predictions of the model, generating a new version, i.e. a sequence of data
post-processing functions.
</p>
</td></tr>
<tr><td><code id="standardWF_+3A_post.pars">post.pars</code></td>
<td>

<p>A named list of parameter values to be passed to the post-processing functions.
</p>
</td></tr>
<tr><td><code id="standardWF_+3A_.fulloutput">.fullOutput</code></td>
<td>

<p>A boolean that if set to <code>TRUE</code> will make the function return
more information in the list that results from its execution like for
instance  the  models (defaults  to <code>FALSE</code>).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The main goal of this function is to facilitate the task of the users
of the experimental comparison infra-structure provided by function
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>. Namely, this function requires
the users to specify the workflows (solutions to predictive tasks)
whose performance she/he wants to estimate and compare. The user has
the flexibility of writing hers/his own workflow functions, however,
in most situations that is not really necessary. The reason is that
most of the times users just want to compare standard out of the box
learning algorithms on some tasks. In these contexts, the workflow
simply consists of applying some existing learning algorithm to the
training data, and then use it to obtain the predictions of the test
set.  This standard workflow may even include some standard
pre-processing tasks applied to the given data before the model is
learned, and eventually some post processing tasks applied to the
predictions before they are returned to the user. The goal of the
current function is to facilitate evaluating this sort of estimation
experiments. It implements this workflow thus avoiding the need of the
user to write these workflows. 
</p>
<p>Through parameter <code>learner</code> users may indicate the modeling
algorithm to use to obtain the predictive model. This can be any R function,
provided it can be called with a formula on the first argument and a
training set on a parameter named <code>data</code> (as most R modeling functions do). As
usual, these functions may include other arguments that are specific
to the modeling approach (i.e. are parameters of the model). The
values to be used for these parameters are specified as a list through
the parameter <code>learner.pars</code> of function <code>standardWF</code>. The
learning function can return any class of object that represents the
learned model. This object will be used to obtain the predictions in
this standard workflow.
</p>
<p>Equivalently, the user may specify a function for obtaining the
predictions for the test set using the previously learned model. Again
this can be any function, and it is indicated in parameter
<code>predictor</code> (defaulting to the usual <code><a href="stats.html#topic+predict">predict</a></code>
function). This function should be prepared to accept in the first
argument the learned model and in the second the test set, and should
return the predictions of the model for this set of data. It may also
have additional parameters whose values are specified as a list in
parameter <code>predictor.pars</code>.
</p>
<p>Additionally, the user may specify a set of data-preprocessing
functions to be applied to both the training and testing sets, through
parameter <code>pre</code> that accepts a vector of function names. These
functions will be applied to both the training and testing sets, in
the sequence provided in the vector of names, before the learning
algorithm is applied to the training set. Once again the user is free
to indicate as pre-processing functions any function, eventually
her/his own functions carrying our any sort of pre-processing
steps. These user-defined pre-processing functions will be applied by
function <code><a href="#topic+standardPRE">standardPRE</a></code>. Check the help page of this
function to know the protocol you need to follow to be able to use
your own pre-processing functions. Still, our infra-structure already
includes some common pre-processing functions so that you do not need
to implement them. The list of these functions is again described in
the help page of <code><a href="#topic+standardPRE">standardPRE</a></code>.
</p>
<p>The predictions obtained by the function specified in parameter
<code>predict</code> may also go through some post-processing steps before
they are return as a result of the <code>standardWF</code> function. Again
the user may specify a vector of post-processing functions to be
applied in sequence, through the parameter <code>post</code>. Parameters to
be passed to these functions can be specified through the parameter
<code>post.pars</code>. The goal of these functions is to obtain a new
version of the predictions of the models after going through some
post-processing steps. These functions will be applied to the
predictions by the function <code><a href="#topic+standardPOST">standardPOST</a></code>. Once again
this function already implements a few standard post-processing steps
but you are free to supply your own post-processing functions provided
they follow the protocol described in the help page of function
<code><a href="#topic+standardPOST">standardPOST</a></code>. 
</p>
<p>Finally, the parameter <code>.fullOutput</code> controls the ammount of
information that is returned by the <code>standardWF</code> function. By
default it is <code>FALSE</code> which means that the workflow will only
return (apart from the predictions) the train, test and total times of
the learning and prediction stages. This information is returned as a
component named &quot;times&quot; of the results list that can be obtained
for instance by using the
<code><a href="#topic+getIterationsInfo">getIterationsInfo</a></code> if the workflow is being used in the
context of an experimental comparison. If <code>.fullOutput</code> is set to
<code>TRUE</code> the workflow will also include information on the
pre-processing steps (in a component named &quot;preprocessing&quot;),
information on the model and predictions of the model (in a component
named &quot;modeling&quot;) and information on the post-processing steps (in a
component named &quot;postprocessing&quot;).
</p>


<h3>Value</h3>

<p>A list with several components containing the result of runing the
workflow. 
</p>


<h3>Note</h3>

<p>In order to use any of the available learning algorithms in R you must
have previously installed and loaded the respective packages, if necessary.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+performanceEstimation">performanceEstimation</a></code>,
<code><a href="#topic+timeseriesWF">timeseriesWF</a></code>,
<code><a href="#topic+getIterationsInfo">getIterationsInfo</a></code>,
<code><a href="#topic+getIterationsPreds">getIterationsPreds</a></code>,
<code><a href="#topic+standardPRE">standardPRE</a></code>,
<code><a href="#topic+standardPOST">standardPOST</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(iris)
library(e1071)

## a standard workflow using and SVM with default parameters
w.s &lt;- Workflow(wfID="std.svm",learner="svm")
w.s

irisExp &lt;- performanceEstimation(
  PredTask(Species ~ .,iris),
  w.s,
  EstimationTask("acc"))

getIterationsPreds(irisExp,1,1,it=4)
getIterationsInfo(irisExp,1,1,rep=1,fold=2)

## A more sophisticated standardWF
## - as pre-processing we imput NAs with either the median (numeric
## features) or the mode (nominal features); and we also scale
## (normalize) the numeric predictors
## - as learning algorithm we use and SVM with cost=10 and gamma=0.01
## - as post-processing we scale all predictions into the range [0..50]
w.s2 &lt;- Workflow(pre=c("centralImp","scale"), 
                 learner="svm",
                 learner.pars=list(cost=10,gamma=0.01),
                 post="cast2int",
                 post.pars=list(infLim=0,supLim=50),
                 .fullOutput=TRUE
                )

data(algae,package="DMwR")

a1.res &lt;- performanceEstimation(
            PredTask(a1 ~ ., algae[,1:12],"alga1"),
            w.s2,
            EstimationTask("mse")
            )

## Workflow variants of a standard workflow
ws &lt;- workflowVariants(
                 pre=c("centralImp","scale"), 
                 learner="svm",
                 learner.pars=list(cost=c(1,5,10),gamma=0.01),
                 post="cast2int",
                 post.pars=list(infLim=0,supLim=c(10,50,80)),
                 .fullOutput=TRUE,
                 as.is="pre"
                )
a1.res &lt;- performanceEstimation(
            PredTask(a1 ~ ., algae[,1:12],"alga1"),
            ws,
            EstimationTask("mse")
            )

## An example using GBM that is a bit different in terms of the
## prediction part as it requires to select the number of trees of the
## ensemble to use
data(Boston, package="MASS")
library(gbm)

## A user written predict function to allow for using the standard
## workflows
gbm.predict &lt;- function(model, test, method, ...) {
    best &lt;- gbm.perf(model, plot.it=FALSE, method=method)
    return(predict(model, test, n.trees=best, ...))
}

resG &lt;- performanceEstimation(
             PredTask(medv ~.,Boston),
             Workflow(learner="gbm",
                      learner.pars=list(n.trees=1000, cv.folds=10),
                      predictor="gbm.predict",
                      predictor.pars=list(method="cv")),
             EstimationTask(metrics="mse",method=CV())
        )


## End(Not run)
</code></pre>

<hr>
<h2 id='subset-methods'>Methods for Function <code>subset</code> in Package <span class="pkg">performanceEstimation</span></h2><span id='topic+subset-methods'></span><span id='topic+subset+2CComparisonResults-method'></span>

<h3>Description</h3>

<p>The method subset when applied to objects of class <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> can
be used to obtain another object of the same class with a subset of
the estimation results contained in the original object.
</p>


<h3>Methods</h3>


<dl>
<dt><code>signature(x = "ComparisonResults")</code></dt><dd>
<p>The method has as first argument the object of class <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code>
that you wish to subset. This method also includes 4 extra arguments that
you can use to supply the subsetting criteria.
</p>
<p>Namely, the parameter
<code>metrics</code> allows you to indicate a vector with the subset of
evaluation metrics in the orginal object. Alternatively, you can
provide a regular expression to be matched against the name
of the statistics measured in the experiment (see function
<code><a href="#topic+metricNames">metricNames</a></code>) to specify the subset of metrics 
you want to select.
</p>
<p>The parameter <code>workflows</code>
can be used to provide a vector with the subset of workflows
(approaches to the predictive tasks)
that are to be used in the sub-setting. Alternatively, you can also
provide a regular expression to be matched against the name
of the workflows (see function
<code><a href="#topic+workflowNames">workflowNames</a></code>)  evaluated in the experiment to specify the subset
you want to select.
</p>
<p>The parameter <code>tasks</code>
allows you to indicate a vector with the subset of predictive tasks to be
extracted. Alternatively, you can
provide a regular expression to be matched against the name
of the tasks used in the experiment (see function
<code><a href="#topic+taskNames">taskNames</a></code>) to specify the subset
you want to select.
</p>
<p>Finally, the parameter <code>partial</code> allows you to control how the
names of the other parameters (tasks, workflows and metrics) are
matched against the original names in the <code>ComparisonResults</code>
object. It defaults to <code>TRUE</code> which means that the function
<code><a href="base.html#topic+grep">grep</a></code> is used, whilst with <code>FALSE</code> the mathcin is
done using the function <code><a href="base.html#topic+match">match</a></code>. For instance, if you
have three metrics being estimated with names &quot;F&quot;, &quot;F1&quot; and &quot;F2&quot;,
and you call subset with <code>metrics="F"</code>, this would in effect
subset all three metrics, whilst with <code>partial=FALSE</code> only the
first of the three would be considered.
</p>
</dd>
</dl>

<hr>
<h2 id='taskNames'>
The prediction tasks involved in an estimation experiment
</h2><span id='topic+taskNames'></span>

<h3>Description</h3>

<p>This function can be used to get a vector with the IDs of the prediction
tasks that were used in a performance estimation experiment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>taskNames(o)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="taskNames_+3A_o">o</code></td>
<td>

<p>An object of class <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of strings (the names of the tasks)
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code>,
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>,
<code><a href="#topic+metricNames">metricNames</a></code>,
<code><a href="#topic+workflowNames">workflowNames</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Estimating MSE for 3 variants of both
## regression trees and SVMs, on  two data sets, using one repetition
## of 10-fold CV
library(e1071)
library(DMwR)
data(swiss)
data(mtcars)

## running the estimation experiment
res &lt;- performanceEstimation(
  c(PredTask(Infant.Mortality ~ .,swiss),PredTask(mpg ~ ., mtcars)),
  c(workflowVariants(learner="svm",
                     learner.pars=list(cost=c(1,10),gamma=c(0.01,0.5))),
    workflowVariants(learner="rpartXse",
                     learner.pars=list(se=c(0,0.5,1)))
  ),
  EstimationTask("mse")
  )

## the names of the metrics that were estimated in the above experiment
taskNames(res)

## End(Not run)
</code></pre>

<hr>
<h2 id='timeseriesWF'>
A function implementing sliding and growing window standard workflows for time series
forecasting tasks
</h2><span id='topic+timeseriesWF'></span>

<h3>Description</h3>

<p>This function implements sliding and growing window workflows for the
prediction  time series. The sliding window workflow consists of: (i) starting by
learning a prediction model based on the given training set, (ii) use
this model to obtain predictions for a pre-defined number of future
time steps of the test set; (iii) then  slide the training window forward this
pre-defined number of steps and obtain a new model with this new
training set; (iv) use this new model for obtaining another set of
predictions; and (v) keep repeting this sliding process until
predictions are obtained for all test set period.
</p>
<p>The growing window workflow is similar but instead of sliding the
training window, we grow this window, so each new set of predictions
is obtained with a model learned with all data since the beginning of
the training set till the current time step. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>timeseriesWF(form,train,test,
   learner,learner.pars=NULL,
   type='slide',relearn.step=1,
   predictor='predict',predictor.pars=NULL,
   pre=NULL,pre.pars=NULL,
   post=NULL,post.pars=NULL,
   .fullOutput=FALSE,verbose=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="timeseriesWF_+3A_form">form</code></td>
<td>

<p>A formula specifying the predictive task.
</p>
</td></tr>
<tr><td><code id="timeseriesWF_+3A_train">train</code></td>
<td>

<p>A data frame containing the data set to be used for obtaining the
first prediction model. In case we are using the sliding window
approach, the size of this training set will determine the size of all
future  training sets after each slide step.
</p>
</td></tr>
<tr><td><code id="timeseriesWF_+3A_test">test</code></td>
<td>

<p>A data frame containing the data set for which we want predictions.
</p>
</td></tr>
<tr><td><code id="timeseriesWF_+3A_learner">learner</code></td>
<td>

<p>A character string with the name of a function that is to be used to
obtain the prediction models.
</p>
</td></tr>
<tr><td><code id="timeseriesWF_+3A_learner.pars">learner.pars</code></td>
<td>

<p>A list of parameter values to be passed to the learner (defaults to <code>NULL</code>).
</p>
</td></tr>
<tr><td><code id="timeseriesWF_+3A_type">type</code></td>
<td>

<p>A character string specifying if we are using a sliding (value
'slide') or a growing (value 'grow') window workflow (defaults to 'slide'). 
</p>
</td></tr>
<tr><td><code id="timeseriesWF_+3A_relearn.step">relearn.step</code></td>
<td>

<p>The number of time steps (translated into number of rows in the test
set) after which a new model is re-learned (either by sliding or
growing the training window) (defaults to 1, i.e. each new row).
</p>
</td></tr>
<tr><td><code id="timeseriesWF_+3A_predictor">predictor</code></td>
<td>

<p>A character string with the name of a function that is to be used to
obtain the predictions for the test set using the obtained model
(defaults to 'predict').
</p>
</td></tr>
<tr><td><code id="timeseriesWF_+3A_predictor.pars">predictor.pars</code></td>
<td>

<p>A list of parameter values to be passed to the predictor (defaults
to <code>NULL</code>).
</p>
</td></tr>
<tr><td><code id="timeseriesWF_+3A_pre">pre</code></td>
<td>

<p>A vector of function names that will be applied in sequence to the train
and test data frames, generating new versions, i.e. a sequence of data
pre-processing functions.
</p>
</td></tr>
<tr><td><code id="timeseriesWF_+3A_pre.pars">pre.pars</code></td>
<td>

<p>A named list of parameter values to be passed to the pre-processing functions.
</p>
</td></tr>
<tr><td><code id="timeseriesWF_+3A_post">post</code></td>
<td>

<p>A vector of function names that will be applied in sequence to the
predictions of the model, generating a new version, i.e. a sequence of data
post-processing functions.
</p>
</td></tr>
<tr><td><code id="timeseriesWF_+3A_post.pars">post.pars</code></td>
<td>

<p>A named list of parameter values to be passed to the post-processing functions.
</p>
</td></tr>
<tr><td><code id="timeseriesWF_+3A_.fulloutput">.fullOutput</code></td>
<td>

<p>A boolean that if set to <code>TRUE</code> will make the function return
more information in the list that results from its execution like for
instance  the  models (defaults  to <code>FALSE</code>).
</p>
</td></tr>
<tr><td><code id="timeseriesWF_+3A_verbose">verbose</code></td>
<td>

<p>A Boolean indicating whether a &quot;*&quot; character should be printed every
time the window slides (defaults to <code>FALSE</code>).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The main goal of this function is to facilitate the task of the users
of the experimental comparison infra-structure provided by function
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code> for time series problems where
the target variable can be numeric or nominal.  Frequently, users
just want to compare existing algorithms or variants of these algorithms on a
set of forecasting tasks, using some standard  error
metrics.  The goal of the <code>timeseriesWF</code> function is to facilitate
this task by providing a standard workflow for time series tasks.
</p>
<p>The function works, and has almost the same parameters, as function
<code><a href="#topic+standardWF">standardWF</a></code>. The help page of this latter function
describes most of the parameters used in the current function and thus
we will not repeat the description here. The main difference to the
<code><a href="#topic+standardWF">standardWF</a></code> function is on the existance of two extra
parameters that control the sliding and growing window approaches to
time series forecasting. These are parameters <code>type</code> and
<code>relearn.step</code>.  We have considered two typical workflow approaches for time series
tasks where the user wants predictions for a certain future time
period. Both are based on the assumption that after &quot;some&quot; time
the model that we have obtained with the given training period data
may have become out-dated, and thus a new
model should be obtained with the most recent data. The idea is that
as we move in the testing period and get predictions for the successive
rows of the test set, it is like if a clock is advancing. Previous rows
for which we already made a prediction are &quot;past&quot; as we assume that the
successive rows in both the <code>train</code> and <code>test</code> data frames
are ordered by time (they are time series). In this context, as we
move forward in the test period we can regard the rows for which we
already made a prediction as past data, and thus potentially useful to be
added  to our initial training set with the goal of obtaining a fresh new
model with more recent data. This type of reasoning only makes sense
if we suspect that there is some kind of concept drift on our
data. For stationary data this makes no sense and we would be better
off using the workflow provided by function
<code><a href="#topic+standardWF">standardWF</a></code>. Still, the current function implements two
workflows following this model-updating reasoning: (i) sliding window;
and (ii) growing window. Both use the value of the parameter
(<code>relearn.step</code>) to decide the number of time periods after which we re-learn
the model using fresh new data. The difference between the two strategies lies on
how they treat the oldest data (the initial rows of the provided
training set). Sliding window, as the name suggests, after each
relearn step slides the training set forward thus forgetting the
oldest rows of the previous training set whilst incorporating the most
recent observations. With this approach all models are obtained with a
training set with the same amount of data (the number of rows of the
initially given training set). Growing window does not remove older
rows and thus the training sets keep growing in size after each
relearn step.
</p>


<h3>Value</h3>

<p>A list with several components containing the result of runing the
workflow.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+standardWF">standardWF</a></code>,
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>,
<code><a href="#topic+getIterationsInfo">getIterationsInfo</a></code>,
<code><a href="#topic+getIterationsPreds">getIterationsPreds</a></code>,
<code><a href="#topic+standardPRE">standardPRE</a></code>,
<code><a href="#topic+standardPOST">standardPOST</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## The following is a small illustrative example using the quotes of the
## SP500 index. This example compares two random forests with 500
## regression trees, one applyed in a standard way, and the other using
## a sliding window with a relearn step of every 10 days. The experiment
## uses 10 repetitions of a train+test cycle using 50% of the available
## data for training and 25% for testing.
## Not run: 
library(quantmod)
library(randomForest)
getSymbols('^GSPC',from='2008-01-01',to='2012-12-31')
data.model &lt;- specifyModel(
  Next(100*Delt(Ad(GSPC))) ~ Delt(Ad(GSPC),k=1:10)+Delt(Vo(GSPC),k=1:3))
data &lt;- as.data.frame(modelData(data.model))
colnames(data)[1] &lt;- 'PercVarClose'
spExp &lt;- performanceEstimation(
  PredTask(PercVarClose ~ .,data,'SP500_2012'),
  c(Workflow(wf='standardWF',wfID="standRF",
             learner='randomForest',
             learner.pars=list(ntree=500)),
    Workflow(wf='timeseriesWF',wfID="slideRF",
             learner='randomForest',
             learner.pars=list(ntree=500),
             type="slide",
             relearn.step=10)
   ),
  EstimationTask(
     metrics=c("mse","theil"),
     method=MonteCarlo(nReps=5,szTrain=0.5,szTest=0.25)
     )
)

## End(Not run)
</code></pre>

<hr>
<h2 id='topPerformer'>
Obtain the workflow that best performed in terms of a metric on a task
</h2><span id='topic+topPerformer'></span>

<h3>Description</h3>

<p>This function can be used to obtain the workflow (an object of class
<code><a href="#topic+Workflow-class">Workflow</a></code>) that performed better in terms of a
given metric on a certain task. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>topPerformer(compRes,metric,task,max=FALSE,stat="avg") 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="topPerformer_+3A_compres">compRes</code></td>
<td>

<p>A <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> object with the results of your experimental comparison.
</p>
</td></tr>
<tr><td><code id="topPerformer_+3A_metric">metric</code></td>
<td>

<p>A string with the name of a metric estimated in the comparison
</p>
</td></tr>
<tr><td><code id="topPerformer_+3A_task">task</code></td>
<td>

<p>A string with the name of a predictive task used in the comparison
</p>
</td></tr>
<tr><td><code id="topPerformer_+3A_max">max</code></td>
<td>

<p>A boolean (defaulting to <code>FALSE</code>) indicating the meaning of
best performance for the selected metric. If this is <code>FALSE</code> it
means that the goal is to minimize this metric, otherwise it means
that the metric is to be maximized.
</p>
</td></tr>
<tr><td><code id="topPerformer_+3A_stat">stat</code></td>
<td>

<p>The statistic to be used to obtain the ranks. The options are the
statistics produced by the function <code>summary</code> applied to objects
of class  <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code>, i.e. &quot;avg&quot;, &quot;std&quot;,
&quot;med&quot;, &quot;iqr&quot;, &quot;min&quot;, &quot;max&quot; or &quot;invalid&quot; (defaults to &quot;avg&quot;).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is an utility function that can be used to obtain the workflow
(an object of class <code><a href="#topic+Workflow-class">Workflow</a></code>) that achieved the
best performance on a given predictive task in terms of a certain
evaluation metric.  The notion of <em>best performance</em> depends on
the type of evaluation metric, thus the need for the <code>max</code>
argument. Some evaluation statistics are to be maximized
(e.g. accuracy), while others are to be minimized (e.g. mean squared
error). For the former you should use <code>max=TRUE</code>, while the
latter require <code>max=FALSE</code> (the default).
</p>


<h3>Value</h3>

<p>The function returns an object of class <code><a href="#topic+Workflow-class">Workflow</a></code> .
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+performanceEstimation">performanceEstimation</a></code>,
<code><a href="#topic+topPerformers">topPerformers</a></code>,
<code><a href="#topic+rankWorkflows">rankWorkflows</a></code>,
<code><a href="#topic+metricsSummary">metricsSummary</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Estimating several evaluation metrics on different variants of a
## regression tree and of a SVM, on  two data sets, using one repetition
## of  10-fold CV

data(swiss)
data(mtcars)
library(e1071)

## run the experimental comparison
results &lt;- performanceEstimation(
               c(PredTask(Infant.Mortality ~ ., swiss),
                 PredTask(mpg ~ ., mtcars)),
               c(workflowVariants(learner='svm',
                                  learner.pars=list(cost=c(1,5),gamma=c(0.1,0.01))
                                 )
               ),
               EstimationTask(metrics=c("mse","mae"),method=CV(nReps=2,nFolds=5))
                                 )

## get the top performer workflow for a metric and task
topPerformer(results,"mse","swiss.Infant.Mortality")

## End(Not run)
</code></pre>

<hr>
<h2 id='topPerformers'>
Obtain the best scores from a performance estimation experiment
</h2><span id='topic+topPerformers'></span>

<h3>Description</h3>

<p>This function can be used to obtain the names of the workflows that obtained
the best scores (the top performers) on an experimental
comparison. This information will  be shown for each of the evaluation
metrics involved in the  comparison and also for all predictive tasks
that were used. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>topPerformers(compRes,
           maxs=rep(FALSE,dim(compRes[[1]][[1]]@iterationsScores)[2]),
           stat="avg",digs=3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="topPerformers_+3A_compres">compRes</code></td>
<td>

<p>A <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code> object with the results of your experimental comparison.
</p>
</td></tr>
<tr><td><code id="topPerformers_+3A_maxs">maxs</code></td>
<td>

<p>A vector of booleans with as many elements are there are metrics estimated in
the experimental comparison. A <code>TRUE</code> value means the respective
statistic is to be maximized, while a <code>FALSE</code> means
minimization. Defaults to all <code>FALSE</code> values, i.e. all metrics are to
be minimized.
</p>
</td></tr>
<tr><td><code id="topPerformers_+3A_stat">stat</code></td>
<td>

<p>The statistic to be used to obtain the ranks. The options are the
statistics produced by the function <code>summary</code> applied to objects
of class  <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code>, i.e. &quot;avg&quot;, &quot;std&quot;,
&quot;med&quot;, &quot;iqr&quot;, &quot;min&quot;, &quot;max&quot; or &quot;invalid&quot; (defaults to &quot;avg&quot;).
</p>
</td></tr>
<tr><td><code id="topPerformers_+3A_digs">digs</code></td>
<td>

<p>The number of digits (defaults to 3) used in the scores column of the results.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is an utility function to check which were the top performers in a
comparative experiment for each data set and each evaluation
metric. The notion of <em>best performance</em> depends on the type of
evaluation metric, thus the need for the second argument. Some
evaluation statistics are to be maximized (e.g. accuracy), while
others are to be minimized (e.g. mean squared error). If you have a
mix of these types on your experiment then you can use the <code>maxs</code>
parameter to inform the function of which are to be maximized and
minimized. 
</p>


<h3>Value</h3>

<p>The function returns a list with named components. The components
correspond to the predictive tasks  used in the experimental comparison. For
each component you get a <code>data.frame</code>, where the rows represent the
statistics. For each statistic you get the name of the top performer
(1st column of the data frame) and the respective score on that
statistic (2nd column).
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+performanceEstimation">performanceEstimation</a></code>,
<code><a href="#topic+topPerformer">topPerformer</a></code>,
<code><a href="#topic+rankWorkflows">rankWorkflows</a></code>,
<code><a href="#topic+metricsSummary">metricsSummary</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Estimating several evaluation metrics on different variants of a
## regression tree and of a SVM, on  two data sets, using one repetition
## of  10-fold CV

data(swiss)
data(mtcars)
library(e1071)

## run the experimental comparison
results &lt;- performanceEstimation(
               c(PredTask(Infant.Mortality ~ ., swiss),
                 PredTask(mpg ~ ., mtcars)),
               c(workflowVariants(learner='svm',
                                  learner.pars=list(cost=c(1,5),gamma=c(0.1,0.01))
                                 )
               ),
               EstimationTask(metrics=c("mse","mae"),method=CV(nReps=2,nFolds=5))
                                 )
## get the top performers for each task and evaluation metric
topPerformers(results)

## End(Not run)
</code></pre>

<hr>
<h2 id='Workflow-class'>Class &quot;Workflow&quot;</h2><span id='topic+Workflow'></span><span id='topic+Workflow-class'></span><span id='topic+show+2CWorkflow-method'></span><span id='topic+summary+2CWorkflow-method'></span>

<h3>Description</h3>

<p>Objects of the class <code>Workflow</code> represent a
solution to a predictive task that typically involve learning a
prediction model using the given training data and then applying it to
the provided test set. Still, a workflow function may carry out many
more steps (e.g. special data pre-processing steps, or some form of
post-processing of the predictions, etc.)
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>Workflow(
  ...)</code>. This constructor function can generate 3 types of workflows: i)
standard workflows; ii) time series standard workflows; and iii)
user-defined workflows. The <code>wf</code> parameter of the constructor
controls which type of workflow is created. If the value of this
parameter is &quot;standardWF&quot; or absent and no parameter <code>type</code> is
included, then a workflow of type i) is created. If the value of
<code>wf</code> is &quot;timeseriesWF&quot; or absent but the parameter <code>type</code> is
set to either &quot;slide&quot; or &quot;grow&quot; then a workflow of type ii) is
created. In all other cases a user-defined workflow is created which
means that the function supplied in the parameter <code>wf</code> must exist
and follow the input/output protocol of user-defined workflows. This
protocol implies accepting a formula in the first argument, a training
data frame in the second and a testing data frame in the third, with
any other arguments being workflow-specific parameters. Moreover, the
user-defined workflow must return a <code>list</code>
object as result of its execution. See the Examples section for
illustrations. The constructor can also be given a workflow ID in
parameter  <code>wfID</code>. Finally, the constructor also accepts a
parameter <code>deps</code> with a valid value for the <code>deps</code> class
slot. 
</p>


<h3>Slots</h3>


<dl>
<dt><code>name</code>:</dt><dd><p>An optional string containing an
internal name of the workflow (a kind of ID)</p>
</dd>
<dt><code>func</code>:</dt><dd><p>A character string containing the name of the R
function that implements the workflow.</p>
</dd>
<dt><code>pars</code>:</dt><dd><p>A named list containing the parameters and
respective values to be used when calling the workflow (defaulting
to the empty list).</p>
</dd>
<dt><code>deps</code>:</dt><dd><p>An optional named list with components
&quot;packages&quot; and &quot;scripts&quot; each containing a vector of names of the
required packages and scripts, respectively, for the workflow to be
executable. </p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>show</dt><dd><p><code>signature(object = "Workflow")</code>: method used to
show the contents of a <code>Workflow</code> object. </p>
</dd>
<dt>summary</dt><dd><p><code>signature(object = "Workflow")</code>: method used to
obtain a summary of a <code>Workflow</code> object. </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PredTask-class">PredTask</a></code>,
<code><a href="#topic+EstimationTask-class">EstimationTask</a></code>,
<code><a href="#topic+runWorkflow">runWorkflow</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("Workflow")

## A simple standard workflow using a default svm
Workflow(learner="svm")

## Creating a standardWF 
Workflow(learner="randomForest",learner.pars=list(ntree=420),wfID="rf420")

## Another one
Workflow(learner="svm",
         pre=c("centralImp","scale"),
         post="onlyPos",
         deps=list(packages=c("e1071"),scripts=c()))

## Another one
Workflow(learner="rpart",.fullOutput=TRUE)

## A user-defined workflow
myWF &lt;- function(form,train,test,wL=0.5,...) {
    ml &lt;- lm(form,train)
    mr &lt;- rpart(form,train)
    pl &lt;- predict(ml,test)
    pr &lt;- predict(mr,test)
    ps &lt;- wL*pl+(1-wL)*pr
    list(trues=responseValues(form,test),preds=ps)
}

wu &lt;- Workflow(wf="myWF",wL=0.6)

</code></pre>

<hr>
<h2 id='workflowNames'>
The IDs of the workflows involved in an estimation experiment
</h2><span id='topic+workflowNames'></span>

<h3>Description</h3>

<p>This function can be used to get a vector with the IDs of the workflows
that were used in a performance estimation experiment. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>workflowNames(o)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="workflowNames_+3A_o">o</code></td>
<td>

<p>An object of class <code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of strings (the names of the workflows)
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ComparisonResults-class">ComparisonResults</a></code>,
<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>,
<code><a href="#topic+taskNames">taskNames</a></code>,
<code><a href="#topic+metricNames">metricNames</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Estimating MSE for 3 variants of both
## regression trees and SVMs, on  two data sets, using one repetition
## of 10-fold CV
library(e1071)
library(DMwR)
data(swiss)
data(mtcars)

## running the estimation experiment
res &lt;- performanceEstimation(
  c(PredTask(Infant.Mortality ~ .,swiss),PredTask(mpg ~ ., mtcars)),
  c(workflowVariants(learner="svm",
                     learner.pars=list(cost=c(1,10),gamma=c(0.01,0.5))),
    workflowVariants(learner="rpartXse",
                     learner.pars=list(se=c(0,0.5,1)))
  ),
  EstimationTask("mse")
  )

## the names of the metrics that were estimated in the above experiment
workflowNames(res)

## End(Not run)
</code></pre>

<hr>
<h2 id='workflowVariants'>
Generate (parameter) variants of a workflow
</h2><span id='topic+workflowVariants'></span>

<h3>Description</h3>

<p>The main goal of this function is to facilitate the generation of
different variants of a workflow. The idea is to be able to
supply several possible values for a set of parameters of the workflow,
and then have the function to return a set of <code><a href="#topic+Workflow-class">Workflow</a></code> objects,
each consisting of one of the different possible combinations of the
variants. This function finds its use in the context of performance
estimation experiments, where we may actually be interested
in comparing different parameter settings for a workflow.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>workflowVariants(wf,...,varsRootName,as.is=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="workflowVariants_+3A_wf">wf</code></td>
<td>

<p>This is the string representing the name of the function of the base
workflow from which variants should be generated. It can be ommited in
the case of standard workflows (see Details section).
</p>
</td></tr>
<tr><td><code id="workflowVariants_+3A_...">...</code></td>
<td>

<p>The function then accepts any number of named arguments, some of which
may have vectors of values. These named arguments  are supposed to be the names of
parameters of the base workflow, and the parameters containing sets of values are the
alternatives for the respective parameter that you want to consider in the variants generation (see
examples below).
</p>
</td></tr>
<tr><td><code id="workflowVariants_+3A_varsrootname">varsRootName</code></td>
<td>

<p>By default the names given to each variant will be formed by
concatenating the base name of the workflow with the terminations: &quot;.v1&quot;,
&quot;.v2&quot;, and so on. This parameter allows you to supply a different base name.
</p>
</td></tr>
<tr><td><code id="workflowVariants_+3A_as.is">as.is</code></td>
<td>

<p>This is a vector of parameter names (defaults to <code>NULL</code>) that are
not to be used as source for workflow variants. This is useful for workflows
that have parameters that accept as &quot;legal&quot; values sets (e.g. a vector)
and that we do not want the function <code>workflowVariants</code> to interprete as
source values for generating different workflow variants but instead
being use as they are given.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of this function is a list of <code><a href="#topic+Workflow-class">Workflow</a></code> objects. Each
of these objects represents one of the parameter variants of the
workflow you have supplied.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p> Torgo, L. (2014) <em>An Infra-Structure for Performance
Estimation and Experimental Comparison of Predictive Models in R</em>. arXiv:1412.0436 [cs.MS]
<a href="http://arxiv.org/abs/1412.0436">http://arxiv.org/abs/1412.0436</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Workflow-class">Workflow</a></code>,<code><a href="#topic+performanceEstimation">performanceEstimation</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Generating several variants of the "svm" learner using different
## values of the parameter "cost", under the "umbrella" of a standard
## workflow (i.e. it assumes wf="standardWF")
library(e1071)
workflowVariants(learner="svm",cost=c(1,2,5,10))

## variants of a user defined workflow (assumes that function "userWF"
## is defined and "understands" parameters par1 and par2)
workflowVariants(wf="userWF",par1=c(0.1,0.4),par2=c(-10,10))

## Variants of a standard time series workflows (it assumes that it is a
## time series workflow because of the use of the "type" parameter,
## otherwise you could make it explicit by adding wf="timeseriesWF").
workflowVariants(learner="svm",type=c("slide","grow"),gamma=c(0.1,0.4))
## or equivalently
workflowVariants(wf="timeseriesWF",learner="svm",type=c("slide","grow"),gamma=c(0.1,0.4))

## allowing that one parameter is not considered for variants generation
workflowVariants(wf="userWF",par1=c(0.1,0.4),par2=c(-10,10),as.is="par1")

## nesting is also allowed!
workflowVariants(wf="userWF",
                 xpto=list(par1=c(0.1,0.4),d="k",par3=c(1,3)),
                 par2=c(-10,10),
                 as.is="par1")

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
