<!DOCTYPE html><html lang="en"><head><title>Help for package asht</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {asht}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#asht-package'>
<p>Applied Statistical Hypothesis Tests</p></a></li>
<li><a href='#abcnonHtest'>
<p>Nonparametric ABC (Approximate Bootstrap Confidence) intervals.</p></a></li>
<li><a href='#ama1c1cpg'>
<p>Three arm phase 1 malaria vaccine trial</p></a></li>
<li><a href='#anovaOneWay'>
<p>One-Way ANOVA</p></a></li>
<li><a href='#bfControl'>
<p>Algorithm control arguments for Behrens-Fisher test</p></a></li>
<li><a href='#bfTest'><p>Behrens-Fisher Test</p></a></li>
<li><a href='#cvTest'>
<p>Coefficient of Variation Test</p></a></li>
<li><a href='#latentTransform'>
<p>Transform  Mann-Whiteny parameter to latent Mann-Whitney parameter</p></a></li>
<li><a href='#meldCD'>
<p>Meld Two Confidence Distributions</p></a></li>
<li><a href='#meldtTest'>
<p>Meld t Test</p></a></li>
<li><a href='#metaNorm'>
<p>Meta analysis of normally distributed parameters with assumed known variance</p></a></li>
<li><a href='#methodRuleWMW'>
<p>Function to pick the method for <code>wmwTest</code> given the data and <code>exact</code> argument.</p></a></li>
<li><a href='#pbf'>
<p>Behrens-Fisher distribution</p></a></li>
<li><a href='#prevSeSp'>
<p>Estimate prevalence with confidence interval accounting for sensitivity and specificity</p></a></li>
<li><a href='#quantileTest'>
<p>Tests and Confidence Intervals about a Quantile.</p></a></li>
<li><a href='#signTest'>
<p>Exact Sign Test with Confidence Intervals</p></a></li>
<li><a href='#simulateSS'>
<p>Simulate sample sizes</p></a></li>
<li><a href='#tukeyWelsch'>
<p>Tukey-Welsch Pairwise Tests</p></a></li>
<li><a href='#var1Test'>
<p>One Sample Test of Normal Variance</p></a></li>
<li><a href='#Vpo'>
<p>Variance for estimated Mann-Whitney parameter under proportional odds.</p></a></li>
<li><a href='#wmwControl'>
<p>Arguments passed to wmwTest.</p></a></li>
<li><a href='#wmwTest'>
<p>Wilcoxon-Mann-Whitney test with Confidence Interval on Mann-Whitney Parameter</p></a></li>
<li><a href='#WprevSeSp'><p>Weighted prevalence inferences adjusted for sensitivity and specificity</p></a></li>
<li><a href='#wspoissonTest'>
<p>Test and Confidence Intervals on Weighted Sum of Poissons</p></a></li>
<li><a href='#wsrTest'>
<p>Exact Wilcoxon Signed Rank Test</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Applied Statistical Hypothesis Tests</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-08-24</td>
</tr>
<tr>
<td>Author:</td>
<td>Michael P. Fay</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michael P. Fay &lt;mfay@niaid.nih.gov&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Gives some hypothesis test functions (sign test, median and other quantile tests, Wilcoxon signed rank test, coefficient of variation test, test of normal variance, test on weighted sums of Poisson [see Fay and Kim &lt;<a href="https://doi.org/10.1002%2Fbimj.201600111">doi:10.1002/bimj.201600111</a>&gt;], sample size for t-tests with different variances and non-equal n per arm, Behrens-Fisher test, nonparametric ABC intervals, Wilcoxon-Mann-Whitney test [with effect estimates and confidence intervals, see Fay and Malinovsky &lt;<a href="https://doi.org/10.1002%2Fsim.7890">doi:10.1002/sim.7890</a>&gt;], two-sample melding tests [see Fay, Proschan, and Brittain &lt;<a href="https://doi.org/10.1111%2Fbiom.12231">doi:10.1111/biom.12231</a>&gt;], one-way ANOVA allowing var.equal=FALSE [see Brown and Forsythe, 1974, Biometrics]), prevalence confidence intervals that adjust for sensitivity and specificity [see Lang and Reiczigel, 2014 &lt;<a href="https://doi.org/10.1016%2Fj.prevetmed.2013.09.015">doi:10.1016/j.prevetmed.2013.09.015</a>&gt;] or Bayer, Fay, and Graubard, 2023 &lt;<a href="https://doi.org/10.48550%2FarXiv.2205.13494">doi:10.48550/arXiv.2205.13494</a>&gt;). The focus is on hypothesis tests that have compatible confidence intervals, but some functions only have confidence intervals (e.g., prevSeSp).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>stats, exact2x2 (&ge; 1.6.4), exactci, bpcp, coin</td>
</tr>
<tr>
<td>Imports:</td>
<td>perm, ssanv</td>
</tr>
<tr>
<td>Suggests:</td>
<td>bootstrap</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-24 14:24:56 UTC; faym</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-24 14:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='asht-package'>
Applied Statistical Hypothesis Tests
</h2><span id='topic+asht-package'></span><span id='topic+asht'></span>

<h3>Description</h3>

<p>Test and confidence intervals for some applied statistical hypothesis tests.
</p>


<h3>Details</h3>

<p>A collection of statistical hypothesis tests, with a focus on non-asymptotic tests. Some tests are <code><a href="#topic+medianTest">medianTest</a></code> for exact tests and
confidence intervals about a median, <code><a href="#topic+quantileTest">quantileTest</a></code> which generalizes <code><a href="#topic+medianTest">medianTest</a></code> for other quantiles besides the median,  <code><a href="#topic+signTest">signTest</a></code> to run the exact sign test, <code><a href="#topic+bfTest">bfTest</a></code> to run the Behrens-Fisher test,
<code><a href="#topic+abcnonHtest">abcnonHtest</a></code> to calculate ABC intervals and tests,
<code><a href="#topic+wmwTest">wmwTest</a></code> to run the Wilcoxon-Mann-Whitney test (i.e., Wilcoxon rank sum test, or Mann-Whitney U test) and calculate confidence intervals on the Mann-Whitney parameter. In rare cases, the function only gives a confidence interval and and estimate and does not test a specific hypothesis (see <code><a href="#topic+prevSeSp">prevSeSp</a></code> which estimates prevalence accounting for sensitivity and specificity).
</p>


<h3>Author(s)</h3>

<p>Michael P. Fay
</p>
<p>Maintainer: Michael P. Fay &lt;mfay@niaid.nih.gov&gt;
</p>

<hr>
<h2 id='abcnonHtest'>
Nonparametric ABC (Approximate Bootstrap Confidence) intervals.
</h2><span id='topic+abcnonHtest'></span>

<h3>Description</h3>

<p>A hypothesis testing function using the nonparametric ABC intervals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>abcnonHtest(x, tt, nullValue = NULL, conf.level = 0.95, 
   alternative = c("two.sided", "less", "greater"), epsilon = 0.001, minp = 0.001)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="abcnonHtest_+3A_x">x</code></td>
<td>
<p>the data. Must be either a vector, or a matrix whose rows are
the observations</p>
</td></tr> 
<tr><td><code id="abcnonHtest_+3A_tt">tt</code></td>
<td>
<p>function defining the parameter in the resampling form
<code>tt(p,x)</code>, where <code>p</code> is the vector of proportions and <code>x</code>
is the data</p>
</td></tr> 
<tr><td><code id="abcnonHtest_+3A_nullvalue">nullValue</code></td>
<td>

<p>null value of the parameter for the two-sided hypothesis test, or boundary of null parameter space for one-sided ones
</p>
</td></tr>
<tr><td><code id="abcnonHtest_+3A_conf.level">conf.level</code></td>
<td>

<p>confidence level for interval
</p>
</td></tr>
<tr><td><code id="abcnonHtest_+3A_alternative">alternative</code></td>
<td>

<p>a character string specifying the alternative hypothesis, must be one of &quot;two.sided&quot; (default), &quot;greater&quot; or &quot;less&quot;. You can specify just the initial letter.
</p>
</td></tr>
<tr><td><code id="abcnonHtest_+3A_epsilon">epsilon</code></td>
<td>
<p>optional argument specifying step size for finite
difference calculations</p>
</td></tr> 
<tr><td><code id="abcnonHtest_+3A_minp">minp</code></td>
<td>

<p>minimum p-value (used in uniroot search to give a bound, toe two.sided alternatives actual minimum is 2*minp)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the nonparametric ABC confidence interval of DiCiccio and Efron (1992). See also Efron and Tibshirani (1993). 
</p>
<p>The p-values are calculated by solving for confidence limit that just touches the <code>nullValue</code>. If it is outside of the range (minp, 1-minp) for one-sided p-values, then it is set to minp.
If it is outside the range (2*minp, 1- 2*minp) for two-sided p-values, then it is set to 2*minp.
</p>


<h3>Value</h3>

<p>A value of class &quot;htest&quot; containing the following components:
</p>
<table role = "presentation">
<tr><td><code>p.value</code></td>
<td>
<p>p-value for test defined by alternative and nullValue</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>estimate of the parameter, calculated using <code>x</code> and the <code>tt</code> function </p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>confidence interval for the parameter associated with <code>tt</code> </p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p> the <code>nullValue</code> (or null boundary) for the hypothesis test</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p> a character string describing the alternative hypothesis </p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string describing the kind of test</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the name of the data
and the function</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>the function is modification of <code>abcnon</code> in the <code>bootstrap</code> R package, originally written by Rob Tibshirani, modifications by M.P. Fay
</p>


<h3>References</h3>

<p>DiCiccio, T and Efron, B (1992). More accurate confidence intervals in exponential families. Biometrika 79: 231-245.
</p>
<p>Efron, B and Tibshirani, RJ (1993). An introduction to the bootstrap. Chapman and Hall: New York.
</p>


<h3>See Also</h3>

<p>See also <code><a href="bootstrap.html#topic+abcnon">abcnon</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># compute abc intervals for the mean
x &lt;- c(2,4,12,4,6,3,5,7,6)
theta &lt;- function(p,x) {sum(p*x)/sum(p)}
## smallest p-value is 2*minp for two-sided alternatives
abcnonHtest(x, theta, nullValue=0)  
## test null at 95% confidence limit is like just barely
## rejecting at the two-sided 5% level, so p-value is 0.05
abcnonHtest(x, theta, nullValue=4.072772)  
# compute abc intervals for the correlation
set.seed(1)
x &lt;- matrix(rnorm(20),ncol=2)
theta &lt;- function(p, x)
{
    x1m &lt;- sum(p * x[, 1])/sum(p)
    x2m &lt;- sum(p * x[, 2])/sum(p)
    num &lt;- sum(p * (x[, 1] - x1m) * (x[, 2] - x2m))
    den &lt;- sqrt(sum(p * (x[, 1] - x1m)^2) *
              sum(p * (x[, 2] - x2m)^2))
    return(num/den)
}
abcnonHtest(x, theta) 
## compare with 
## Not run: 
library(bootstrap)
abcnon(x, theta, alpha=c(.025,.975))$limits[,"abc"]
## End(Not run)  
</code></pre>

<hr>
<h2 id='ama1c1cpg'>
Three arm phase 1 malaria vaccine trial
</h2><span id='topic+ama1c1cpg'></span>

<h3>Description</h3>

<p>Growth inhibition responses from a three arm vaccine trial (Mullen, et al, 2008).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("ama1c1cpg")</code></pre>


<h3>Format</h3>

<p>A data frame with 58 observations on the following 2 variables.
</p>

<dl>
<dt><code>vacc</code></dt><dd><p>a factor representing the three arms of the trial. The levels are: <code>20ug+CPG</code> <code>80ug</code> <code>80ug+CPG</code></p>
</dd>
<dt><code>resp</code></dt><dd><p>a numeric vector giving the response: day 70 sera percent in vitro growth inhibition of the 3D7 malaria parasite.</p>
</dd>
</dl>



<h3>References</h3>

<p>Mullen, GE, Ellis, RD, Miura, K, Malkin, E, Nolan, C, Han, M, Fay, MP, Saul, A, Zhu, D, Rausch, K, Moretz, S, Shou, H, Long, CA, Miller, LH, Treanor, J. 2008. Phase 1 trail of ama1-c1/alhydrogel plus cpg 7909: an asexual blood-stage vaccine for plasmodium falciparum malaria. PLoS ONE. 
3(8):32940.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ama1c1cpg)
## maybe str(ama1c1cpg) ; plot(ama1c1cpg) ...
</code></pre>

<hr>
<h2 id='anovaOneWay'>
One-Way ANOVA
</h2><span id='topic+anovaOneWay'></span>

<h3>Description</h3>

<p>Do one-way ANOVA with estimates and confidence intervals on 
parameters. The parameter is called tau.sq and is the 
weighted sum of the square of the difference between 
the true means and the weighted average of the true means. 
Allows var.equal=FALSE using the Brown-Forsythe method 
that generalizes Welch's t-test to the k-sample problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>anovaOneWay(y, g, var.equal = TRUE, nullValue = 0, 
    parm =c("ICC","varb"), conf.level = 0.9)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="anovaOneWay_+3A_y">y</code></td>
<td>

<p>numeric vector of responses
</p>
</td></tr>
<tr><td><code id="anovaOneWay_+3A_g">g</code></td>
<td>

<p>group membership vector (may be numeric, character, or factor) 
</p>
</td></tr>
<tr><td><code id="anovaOneWay_+3A_var.equal">var.equal</code></td>
<td>

<p>logical, are the variances for all groups be equal? TRUE gives usual anova, FALSE gives Brown-Forsythe method.
</p>
</td></tr>
<tr><td><code id="anovaOneWay_+3A_nullvalue">nullValue</code></td>
<td>

<p>null value of tau.square (between group variance) or tau.sq/sigma.sq (must be 0 now)
</p>
</td></tr>
<tr><td><code id="anovaOneWay_+3A_parm">parm</code></td>
<td>

<p>type of parameter, either 'ICC' (the parameter that R square estimates for this problem) or 
'varb' (the between group variance).
</p>
</td></tr>
<tr><td><code id="anovaOneWay_+3A_conf.level">conf.level</code></td>
<td>

<p>confidence level for the confidence interval. Default is 0.90 so that when the p-value&lt;0.05, the two-sided confidence interval will exclude 0.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The typical way to get the p-value for a one-way anova is 
<code>anova(lm(y~g))</code>. This function was written to add two new features. 
</p>
<p>First, using the method of Brown and Forsythe (1974a), the function allows for non-equal variances between the groups. This is one generalization of Welch's t-test to the one-way ANOVA case. Brown and Forsythe (1974b) give simulations showing 
that the type I error rate is close to the nominal (under the nomrality assumption with different variances).
</p>
<p>Second, the function gives confidence intervals on either 'ICC' or 'varb'.
The 'varb' (the between-group variance) is <code>sum((na/n)*(ua-u)^2)</code> where na is a vector of length k giving the sample size in each group, n is the total sample size, and ua is a vector of the k means in the groups, and u is the overall mean. Let varw be the within-group variance, then ICC=varb/(varb+varw).
ICC is the intraclass correlation coefficient, and in this situation it is the 
parameter that the R square is estimating.
</p>


<h3>Value</h3>

<p>A object of class 'htest'.
</p>


<h3>Note</h3>

<p>Note also that it is possible to get a 90 pct confidence interval for varb that is (0,0). This occurs when the group means are much closer to each other than they would be expected to be by chance, given the observed variability between observations within the groups. 
</p>


<h3>Author(s)</h3>

<p>Michael P. Fay
</p>


<h3>References</h3>

<p>Brown and Forsythe (1974a). Biometrics 30:719-724.
</p>
<p>Brown and Forsythe (1974b). Technometrics 16: 129-132.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(datasets)
library(asht)
ChickWeightTime20&lt;-ChickWeight[ChickWeight$Time==20,]

anovaOneWay(1:10,c(rep(1,4),rep(2,6)))
anova(lm(weight~Diet,data=ChickWeightTime20))
t.test(ChickWeightTime20$weight[ChickWeightTime20$Diet==1],
       ChickWeightTime20$weight[ChickWeightTime20$Diet==2],
       var.equal=FALSE)
anovaOneWay(ChickWeightTime20$weight, ChickWeightTime20$Diet,       
     var.equal=FALSE)
</code></pre>

<hr>
<h2 id='bfControl'>
Algorithm control arguments for Behrens-Fisher test
</h2><span id='topic+bfControl'></span>

<h3>Description</h3>

<p>Usually these arguments do not need to be changed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bfControl(calcmethod = c("int", "mc"), epsilon = 10^(-8), nmc = 10^5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bfControl_+3A_calcmethod">calcmethod</code></td>
<td>

<p>either 'int' for numeric integration (default), or 'mc' for Monte Carlo estimation</p>
</td></tr>
<tr><td><code id="bfControl_+3A_epsilon">epsilon</code></td>
<td>

<p>small value input into <code><a href="#topic+pbf">pbf</a></code> or <code><a href="#topic+qbf">qbf</a></code>
</p>
</td></tr>
<tr><td><code id="bfControl_+3A_nmc">nmc</code></td>
<td>

<p>number of Monte Carlo replications used when calcmethod='mc'
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When calcmethod='int' does numeric integration. This calls <code><a href="#topic+qbf">qbf</a></code> which uses the <code>epsilon</code> argument. 
</p>
<p>When calcmethod='mc' does Monte Carlo estimation of p-value and confidence interval. Uses <code>nmc</code> as the number of replicates.
</p>


<h3>Value</h3>

<p>A list with elements:
</p>
<table role = "presentation">
<tr><td><code>calcmethod</code></td>
<td>
<p>either 'int' or 'mc'</p>
</td></tr>
<tr><td><code>epsilon</code></td>
<td>
<p>small positive number</p>
</td></tr>
<tr><td><code>nmc</code></td>
<td>
<p>number of Monte Carlo replications used when calcmethod='mc'</p>
</td></tr>
</table>

<hr>
<h2 id='bfTest'>Behrens-Fisher Test</h2><span id='topic+bfTest'></span><span id='topic+bfTest.default'></span><span id='topic+bfTest.formula'></span>

<h3>Description</h3>

<p>Tests for a difference in means from two normally distributed variates with possibly different variances. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bfTest(x, ...)

## Default S3 method:
bfTest(x, y,
       alternative = c("two.sided", "less", "greater"),
       mu = 0, conf.level = 0.95, control=bfControl(), ...)

## S3 method for class 'formula'
bfTest(formula, data, subset, na.action, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bfTest_+3A_x">x</code></td>
<td>
<p>a (non-empty) numeric vector of data values.</p>
</td></tr>
<tr><td><code id="bfTest_+3A_y">y</code></td>
<td>
<p>an optional (non-empty) numeric vector of data values.</p>
</td></tr>
<tr><td><code id="bfTest_+3A_alternative">alternative</code></td>
<td>
<p>a character string specifying the alternative
hypothesis, must be one of <code>"two.sided"</code> (default),
<code>"greater"</code> or <code>"less"</code>.  You can specify just the initial
letter.</p>
</td></tr>
<tr><td><code id="bfTest_+3A_mu">mu</code></td>
<td>
<p>a number indicating the true value of the 
difference in means </p>
</td></tr>
<tr><td><code id="bfTest_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level of the interval.</p>
</td></tr>
<tr><td><code id="bfTest_+3A_control">control</code></td>
<td>
<p>a list of arguments used for determining the calculation algorithm, see <code><a href="#topic+bfControl">bfControl</a></code> </p>
</td></tr>
<tr><td><code id="bfTest_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>lhs ~ rhs</code> where <code>lhs</code>
is a numeric variable giving the data values and <code>rhs</code> a factor
with two levels giving the corresponding groups.</p>
</td></tr>
<tr><td><code id="bfTest_+3A_data">data</code></td>
<td>
<p>an optional matrix or data frame (or similar: see
<code><a href="stats.html#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="bfTest_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="bfTest_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.  Defaults to
<code>getOption("na.action")</code>.</p>
</td></tr>
<tr><td><code id="bfTest_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fisher (1935) developed a fiducial 
solution to the two-sample difference in means problem
with normally distributed data with different variances. That has become known as the Behrens-Fisher solution.  Robinson (1976) showed through extensive simulations, that the Behrens-Fisher solution is valid (i.e., the test gives  
type I error rate less than the significance level, and its confidence intervals on the difference in means have coverage at least as large as the nominal confidence level). 
</p>
<p>The following are the same as with the usual t-test in <code><a href="stats.html#topic+t.test">t.test</a></code>.
<code>alternative = "greater"</code> is the alternative that <code>x</code> has a
larger mean than <code>y</code>. Missing values are silently removed. If the input data are effectively constant an error is generated.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table role = "presentation">
<tr><td><code>statistic</code></td>
<td>
<p>the value of the t-statistic.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>R = <code>atan(SEMx/SEMy)</code> used in Behrens-Fisher distribution,
where SEMx=std error of the mean of x, see <code><a href="#topic+pbf">pbf</a></code>, but not used in calculation for this function
</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>a confidence interval for the difference in means 
(mean.x-mean.y) 
appropriate to the
specified alternative hypothesis.</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>the estimated means</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>the specified hypothesized value of the mean
difference</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative
hypothesis.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string describing the test.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the name(s) of the data.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Fisher, RA (1935). The fiducial argument in statistical inference.
Annals of Eugenics. 6, 391-398.
</p>
<p>Robinson, G (1976). Properties of Students t and of the Behrens-Fisher solution to the two means problem. The Annals of
Statistics 4, 963-971 (Corr: 1982, p. 321).
</p>


<h3>See Also</h3>

<p>The more common solution for this problem is Welch's t-test (the default in <code><a href="stats.html#topic+t.test">t.test</a></code>). Welch's t-test does not guarantee that the type I error rate is less than the significance level, but it appears to work well in most cases.  
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Classical example: Student's sleep data
## Traditional interface
with(sleep, bfTest(extra[group == 1], extra[group == 2]))
## Formula interface
bfTest(extra ~ group, data = sleep)
## Results are simular to Welch's t-test, 
## but a little more conservative
t.test(extra~group,data=sleep)
</code></pre>

<hr>
<h2 id='cvTest'>
Coefficient of Variation Test
</h2><span id='topic+cvTest'></span>

<h3>Description</h3>

<p>One-sample coefficient of variation tests and confidence intervals based on either normal or lognormal assumptions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvTest(x, nullCV = 1, 
alternative = c("two.sided", "less", "greater"), 
conf.level = 0.95, distn = c("normal", "lognormal"), 
CVmax = 10^6)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cvTest_+3A_x">x</code></td>
<td>

<p>numeric vector
</p>
</td></tr>
<tr><td><code id="cvTest_+3A_nullcv">nullCV</code></td>
<td>

<p>null coefficient of variation, or CV on boundary between null and alternative hypotheses
</p>
</td></tr>
<tr><td><code id="cvTest_+3A_alternative">alternative</code></td>
<td>

<p>a character string specifying the alternative hypothesis, must be one of &quot;two.sided&quot; (default), &quot;greater&quot; or &quot;less&quot;. You can specify just the initial letter.
</p>
</td></tr>
<tr><td><code id="cvTest_+3A_conf.level">conf.level</code></td>
<td>

<p>confidence level of the interval
</p>
</td></tr>
<tr><td><code id="cvTest_+3A_distn">distn</code></td>
<td>

<p>assumed distribution
</p>
</td></tr>
<tr><td><code id="cvTest_+3A_cvmax">CVmax</code></td>
<td>

<p>maximum coefficient of variation used in uniroot CI searches when distn='normal'
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of class 'htest'
</p>
<table role = "presentation">
<tr><td><code>statistic</code></td>
<td>
<p>mean</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>stadard deviation</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>estimate of coefficient of variation: sd(x)/mean(x) for distn='normal', and sqrt(exp(var(log(x)))-1) for distn='lognormal'</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>p.value associated with alternative</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>confidence interval</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>null CV</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>alternative</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>description of method</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael P. Fay
</p>


<h3>References</h3>

<p>Koopmans, Owen, Rosenblatt (1964) &quot;Confidence intervals for the coefficient of variation for the normal and log normal distributions&quot; Biometrika 25-32.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cvTest(rnorm(25,mean=3,sd=.2),distn="normal")
</code></pre>

<hr>
<h2 id='latentTransform'>
Transform  Mann-Whiteny parameter to latent Mann-Whitney parameter
</h2><span id='topic+latentTransform'></span>

<h3>Description</h3>

<p>Take data and Mann-Whitney parameter values and transform them to latent continuous Mann-Whitney parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>latentTransform(x, y, phiValues, output = c("mw", "po"), epsilon = 10^(-6))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="latentTransform_+3A_x">x</code></td>
<td>

<p>numeric vector of responses from group 1
</p>
</td></tr>
<tr><td><code id="latentTransform_+3A_y">y</code></td>
<td>

<p>numeric vector of responses from group 2
</p>
</td></tr>
<tr><td><code id="latentTransform_+3A_phivalues">phiValues</code></td>
<td>

<p>vector of Mann-Whitney parameters to be transformed
</p>
</td></tr>
<tr><td><code id="latentTransform_+3A_output">output</code></td>
<td>

<p>either 'mw'  (to outpout latnet continuous Mann-Whitney parameter) or 'po' (to output proportional odds parameter)
</p>
</td></tr>
<tr><td><code id="latentTransform_+3A_epsilon">epsilon</code></td>
<td>

<p>small value for limits of proportional odds parameter, smallest is epsilon and largest is 1/epsilon
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the empirical distribution from both groups and uses that and the sample sizes to transform the <code>phiValues</code> to their associated latent continuous Mann-Whitney parameter. Extreme values may not be transformed by this method and are replaced by 0 or 1, where those extreme values are data dependent (and depend on <code>epsilon</code>). See the code and its comments for details.
</p>


<h3>Value</h3>

<p>a vector of latent continuous Mann-Whitney parameters (when output='mw') or proportional odds parameters (when output='po')
</p>

<hr>
<h2 id='meldCD'>
Meld Two Confidence Distributions
</h2><span id='topic+meldCD'></span>

<h3>Description</h3>

<p>Melding is a very general way of combining two independent confidence interval proceedures to create a confidence interval on a function of the two associated parameters (e.g., difference or ratio). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>meldCD(H1, H2, nullparm = NULL, parmtype = c("difference", "ratio", "oddsratio"), 
    conf.level = 0.95, alternative = c("two.sided", "less", "greater"), 
    estimate = c("median", "mean"), lim = c(-Inf, Inf), parmGrid = NULL,
    nmc = 1e5, ngrid = 1e4, calcmethod = "int", epsilon=1e-8, utol=1e-8)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="meldCD_+3A_h1">H1</code></td>
<td>

<p>a function representing the confidence distribution for parameter 1 (see details)
</p>
</td></tr>
<tr><td><code id="meldCD_+3A_h2">H2</code></td>
<td>

<p>a function representing the confidence distribution for parameter 2
</p>
</td></tr>
<tr><td><code id="meldCD_+3A_nullparm">nullparm</code></td>
<td>

<p>null parameter value for the parameter defined by parmtype
</p>
</td></tr>
<tr><td><code id="meldCD_+3A_parmtype">parmtype</code></td>
<td>

<p>parameter type, 'difference' gives parm2-parm1, 'ratio' gives parm2/parm1 (for 'oddsratio' see details).
</p>
</td></tr>
<tr><td><code id="meldCD_+3A_alternative">alternative</code></td>
<td>
<p>a character string specifying the alternative
hypothesis, must be one of <code>"two.sided"</code> (default),
<code>"greater"</code> or <code>"less"</code>.  You can specify just the initial
letter.</p>
</td></tr>
<tr><td><code id="meldCD_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level of the interval.</p>
</td></tr>
<tr><td><code id="meldCD_+3A_estimate">estimate</code></td>
<td>

<p>type of estimate derived from each confidence distribution, either 'median' or 'mean' 
</p>
</td></tr>
<tr><td><code id="meldCD_+3A_lim">lim</code></td>
<td>

<p>a vector with limits on the parameters (both parameters should have the same limits)
</p>
</td></tr>
<tr><td><code id="meldCD_+3A_parmgrid">parmGrid</code></td>
<td>

<p>a vector of a grid of possible values of the parameter, if NULL one is produced based on the lim argument
</p>
</td></tr>
<tr><td><code id="meldCD_+3A_nmc">nmc</code></td>
<td>

<p>number of Monte Carlo replications, used if calcmethod='mc'
</p>
</td></tr>
<tr><td><code id="meldCD_+3A_ngrid">ngrid</code></td>
<td>

<p>minimum number of elements in the parameter grid, used if parmGrid=NULL 
</p>
</td></tr>
<tr><td><code id="meldCD_+3A_calcmethod">calcmethod</code></td>
<td>

<p>calculation method, either 'int' (numeric integration) or 'mc' (Monte Carlo)
</p>
</td></tr>
<tr><td><code id="meldCD_+3A_epsilon">epsilon</code></td>
<td>
<p>small value for warning check, we want the minimum of the CD over the parameter grid to be greater than epsilon, and the maximum to be less than 1-epsilon</p>
</td></tr>
<tr><td><code id="meldCD_+3A_utol">utol</code></td>
<td>
<p>small value for passing to tol option in uniroot for confidence interval calculations</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For continuous responses, a confidence distribution (CD) is like a frequentist posterior distribution. 
We represent the CDs as cumulative distribution functions in the parameter space. 
The CD gets its name because it is created from the confidence interval process.
If (L,U) is the 1-alpha confidence interval for group 1, then H1(L) = alpha/2 
and H1(U)=1-alpha/2. Typically, the the CDs can be formulated as one-sided 
(alternative='greater') p-value functions, 
or 1-p for alternative='less', where the main function argument is  the boundary value on the parameter space between the null and alternative. See binomial example below.
</p>
<p>The median of the CD can be used as an estimate of the parameter. 
</p>
<p>We want inferences on a function of the parameters, say g(parm1, parm2), where when 
</p>

<ul>
<li><p> parmtype=&quot;difference&quot; then g(parm1,parm2)=parm2-parm1
</p>
</li>
<li><p> parmtype=&quot;ratio&quot; then g(parm1,parm2)=parm2/parm1
</p>
</li>
<li><p> parmtype=&quot;oddsratio&quot; then g(parm1,parm2)=(parm2*(1-parm1))/(parm1*(1-parm2)).
</p>
</li></ul>
   
<p>The function g(parm1, parm2) must be increasing in parm2 and decreasing in parm1, 
so for example normal CDs (or any with a range -Inf to Inf) are not allowed for parmtype='ratio'.
The <code>lim</code> argument checks to see if the parmtype is allowed.
</p>
<p>Let T1 and T2 be simulated independent random variables associated with the CDs H1 and H2. 
Then to get a two-sided 1-alpha confidence interval on 
g(parm1,parm2) we can use <code>quantile(g(T1,T2),probs=c(alpha/2,1-alpha/2))</code>.
This is basically how it works when <code>calcmethod='mc'</code>. When <code>calcmethod='int'</code>
then numeric integration is used. 
</p>
<p>For discrete responses, to ensure validity of the resulting confidence intervals, 
each group uses either a lower or upper CD, depending on the one-sided alternative. 
Thus, confidence intervals for two-sided alternatives cannot be calculated in one call 
to the <code>meldCD</code> for discrete data. See Fay, Proschan, and Brittain (2015) and the example. 
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table role = "presentation">
<tr><td><code>p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>a confidence interval for the mean appropriate to the
specified alternative hypothesis.</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>vector of parameter estimates for each group and using the parmtype,
uses the median of the CDs for estimates</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>the specified hypothesized value of the
difference in parameters</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative
hypothesis.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string describing the test.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the name(s) of the data.</p>
</td></tr>
</table>


<h3>Warning </h3>

<p>The function has not been tested for discrete confidence distributions.
Note most confidence distributions for discrete data are not discrete CDs because the parameters are 
continuous.</p>


<h3>Author(s)</h3>

<p>Michael P. Fay
</p>


<h3>References</h3>

<p>Fay, MP, Proschan, MA, Brittain, E (2015). Combining One-sample confidence procedures for inference in the two-sample case. Biometrics. 71: 146-156.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+meldtTest">meldtTest</a></code> and <code><a href="exact2x2.html#topic+binomMeld.test">binomMeld.test</a></code> for special cases.</p>


<h3>Examples</h3>

<pre><code class='language-R'>x1&lt;-4
n1&lt;-11
x2&lt;- 13
n2&lt;-24


#  we use the upper and lower CDs
# this is needed for discrete data to ensure valid intervals
H1L&lt;-function(theta){ pbeta(theta,x1,n1-x1+1)}
# Note, this is just a p-value function that inputs the null boundary value: 
binom.test(x1,n1,p=.4,alternative="greater")$p.value
H1L(.4)
H1U&lt;-function(theta){ pbeta(theta,x1+1,n1-x1)}
# Note, but this is just a function for 1-p that inputs the null boundary value: 
1-binom.test(x1,n1,p=.4,alternative="less")$p.value
H1U(.4)
H2L&lt;-function(theta){ pbeta(theta,x2,n2-x2+1)}
H2U&lt;-function(theta){ pbeta(theta,x2+1,n2-x2)}

meldCD(H1U,H2L, lim=c(0,1),conf.level=0.975,alternative="greater")
meldCD(H1L,H2U, lim=c(0,1),conf.level=0.975,alternative="less")
# notice that the estimates are different than the usual 
# difference in sample proportions
require(exact2x2)
binomMeld.test(x1,n1,x2,n2, conf.level=0.975, alternative="greater")
# compare to two-.sided from 
binomMeld.test(x1,n1,x2,n2, conf.level=0.95, alternative="two.sided")

</code></pre>

<hr>
<h2 id='meldtTest'>
Meld t Test
</h2><span id='topic+meldtTest'></span>

<h3>Description</h3>

<p>Tests for a difference in parameters, when the parameter estimates are independent and both have t distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>meldtTest(x, y, alternative = c("two.sided", "less", "greater"), delta = 0, 
    conf.level = 0.95, control = bfControl(), ...)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="meldtTest_+3A_x">x</code></td>
<td>
<p>a list from the first group with objects: estimate (estimate of parameter),
stderr (standard error of the estimate), and df 
(degrees of freedom associated with t distribution)</p>
</td></tr>
<tr><td><code id="meldtTest_+3A_y">y</code></td>
<td>
<p>a list from the second group with objects: estimate, stderr, and df</p>
</td></tr>
<tr><td><code id="meldtTest_+3A_alternative">alternative</code></td>
<td>
<p>a character string specifying the alternative
hypothesis, must be one of <code>"two.sided"</code> (default),
<code>"greater"</code> or <code>"less"</code>.  You can specify just the initial
letter.</p>
</td></tr>
<tr><td><code id="meldtTest_+3A_delta">delta</code></td>
<td>
<p>a number indicating the null hypothesis value of the 
difference in parameters when  <code>alternative="two.sided"</code>. See details for one-sided hypotheses</p>
</td></tr>
<tr><td><code id="meldtTest_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level of the interval.</p>
</td></tr>
<tr><td><code id="meldtTest_+3A_control">control</code></td>
<td>
<p>a list of arguments used for determining the calculation algorithm, see <code><a href="#topic+bfControl">bfControl</a></code> </p>
</td></tr>
<tr><td><code id="meldtTest_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods (currently not used)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suppose x$estimate and y$estimate estimate the parameters xParm and yParm. Let Delta=yParm-xParm. This function tests hypotheses of the form, 
</p>

<ul>
<li><p> alternative=&quot;two.sided&quot; tests H0: Delta=delta versus H1: Delta != delta
</p>
</li>
<li><p> alternative=&quot;less&quot; tests H0: Delta &gt;= delta versus H1: Delta&lt; delta
</p>
</li>
<li><p> alternative=&quot;greater&quot; tests H0: Delta &lt;= delta versus H1: Delta&gt; delta
</p>
</li></ul>
   
<p>The test uses the theory of melding (Fay, Proschan and Brittain, 2015). The idea is to use confidence distribution random variables (CD-RVs). It is easiest to understand the melding confidence intervals by looking at the Monte Carlo implementation. Let nmc be the number of Monte Carlo replicates, then the simulated CD-RV associated with x are
Bx = x$estimate + x$stderr * rt(nmc,df=x$df). Similarly define By. Then the 95 percent melded confidence interval for Delta=yParm-xParm is estimated by
quantile(By-Bx, probs=c(0.025,0.975)). 
</p>
<p>When the estimates are means from normal distributions, then the meldtTest reduces to the Behrens-Fisher solution (see <code><a href="#topic+bfTest">bfTest</a></code>).  
</p>
<p>Only one of <code>x$stderr</code> or <code>y$stderr</code> may be zero.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table role = "presentation">
<tr><td><code>statistic</code></td>
<td>
<p>the value of the t-statistic.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>R = <code>atan(x$stderr/y$stderr)</code> used in Behrens-Fisher distribution, see <code><a href="#topic+pbf">pbf</a></code>
</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>a confidence interval for the difference in means appropriate to the
specified alternative hypothesis.</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>means and difference in means estimates</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>the specified hypothesized value of the
difference in parameters</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative
hypothesis.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string describing the test.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the name(s) of the data.</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>If the two estimates are not independent, this function may give invalid p-values and confidence intervals!
</p>


<h3>Author(s)</h3>

<p>Michael P. Fay
</p>


<h3>References</h3>

<p>Fay, MP, Proschan, MA, Brittain, E (2015). Combining One-sample confidence procedures for inference in the two-sample case. Biometrics. 71: 146-156.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bfTest">bfTest</a></code> and <code><a href="#topic+pbf">pbf</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Classical example: Student's sleep data
## Compare to bfTest
xValues&lt;- sleep$extra[sleep$group==1]
yValues&lt;- sleep$extra[sleep$group==2]


x&lt;-list(estimate=mean(xValues),
    stderr=sd(xValues)/sqrt(length(xValues)),
    df=length(xValues)-1)
y&lt;-list(estimate=mean(yValues),
    stderr=sd(yValues)/sqrt(length(yValues)),
    df=length(yValues)-1)
bfTest(xValues,yValues)
# by convention the meldtTest does mean(y)-mean(x)
meldtTest(x,y)
meldtTest(y,x)
</code></pre>

<hr>
<h2 id='metaNorm'>
Meta analysis of normally distributed parameters with assumed known variance
</h2><span id='topic+metaNorm'></span>

<h3>Description</h3>

<p>Performs either a random effects meta analysis (Paule-Mandel method or Dersimonian-Laird method) or a fixed effects meta analysis. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>metaNorm(y, s2, method = c("PM", "DL", "fixed"), df = NULL, nullparm = 0,
    conf.level = 0.95, alternative = c("two.sided", "less", "greater"),  
    niter = 100, epsilon = 1e-10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="metaNorm_+3A_y">y</code></td>
<td>

<p>vector of parameter estimates
</p>
</td></tr>
<tr><td><code id="metaNorm_+3A_s2">s2</code></td>
<td>

<p>vector of variances of parameter estimates
</p>
</td></tr>
<tr><td><code id="metaNorm_+3A_method">method</code></td>
<td>

<p>either &quot;PM&quot; (Paule-Mandel random effects method), &quot;DL&quot; (Dersimonian-Laird random effects method) or &quot;fixed&quot; (fixed effects method)
</p>
</td></tr>
<tr><td><code id="metaNorm_+3A_df">df</code></td>
<td>

<p>degrees of freedom, NULL gives either df=k-1 (method=&quot;PM&quot;), df=Inf (method=&quot;DL&quot; or &quot;fixed&quot;)
</p>
</td></tr>
<tr><td><code id="metaNorm_+3A_conf.level">conf.level</code></td>
<td>

<p>confidence level 
</p>
</td></tr>
<tr><td><code id="metaNorm_+3A_alternative">alternative</code></td>
<td>

<p>type of alternative hypothesis
</p>
</td></tr>
<tr><td><code id="metaNorm_+3A_nullparm">nullparm</code></td>
<td>

<p>null value of the parameter for calculating the p-value
</p>
</td></tr>
<tr><td><code id="metaNorm_+3A_niter">niter</code></td>
<td>

<p>maximum number of iterations for method=&quot;PM&quot; 
</p>
</td></tr>
<tr><td><code id="metaNorm_+3A_epsilon">epsilon</code></td>
<td>

<p>small number for determining convergence of Paule-Mandel method.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume you have a vector of treatment effect estimates from K studies (y), together with variance estimates (s2). Assume that 
y[i] is distributed normal with mean theta[i] and variance s2[i], and assume the theta[i] (the latent treatment effect for the ith study) is normally distributed with mean theta and variance tau2 (tau^2). Assume independence between studies. 
</p>
<p>We are interested in estimating the weighted average of the theta[i]. If tau2 is known, then an efficient estimator weighs each study proportional to the inverse of its variance, w[i] = 1/(tau2 + s2[i]). We can either assume tau2=0, and we have a fixed effects model (in other words, the treatment effect is constant across all the studies), or estimate tau2. The method for estimating tau2 either uses a simple method of moments estimator of Dersimonian and Laird (1986), or an iterative method of moments estimator of Paule and Mandel (1982). Dersimonian and Kacker (2007) give the details. 
</p>
<p>For the Paule-Mandel estimator, to account for the fact that we are estimating tau2, we default to using a t-distribution with K-1 degrees of freedom (for motivation see Brittain, Fay and Follmann, 2012, Supplement, Section 3).
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table role = "presentation">
<tr><td><code>statistic</code></td>
<td>
<p>a vector of [1] the estimator of tau2 and [2] the t-statistic (or Z-statistic)</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>degrees of freedom of the t-distribution (df=Inf gives a normal distribution)</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>a confidence interval</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>a vector of [1] the estimated weighted means and [2] the estimated standard error of the weighted means</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>the specified hypothesized value of the weighted means</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative
hypothesis.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string describing the test.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the name(s) of the data.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael P. Fay
</p>


<h3>References</h3>

<p>Brittain, Fay, and Follmann (2012) A valid formulation of the analysis of noninferiority trials under random effects meta-analysis. Biostatistics 13(4): 637-649.
</p>
<p>Dersimonian, R and Kacker, R (2007) Random-effects model for meta-analysis of clinical trials: an update. Contemporary Clinical Trials 28:105-144. 
</p>
<p>Dersimonian, R and Laird, N. (1986). Meta-analysis in clinical trials. Controled Clinical Trials. 7:177-187.
</p>
<p>Paule, RC and Mandel, J (1982). Consensus values and weighting factors. J Res Natl Bur Stand 87: 377-385.
</p>


<h3>See Also</h3>

<p><a href="https://CRAN.R-project.org/package=meta"><span class="pkg">meta</span></a>   package on CRAN
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Data from Table III of Teo et al, BMJ 303:1499-1503
# Effects of intravenous magnesium in suspected acute myocardial
# infarction: overview of randomised trials
# xt/nt = deaths/total in treatment group (magnesium)
# xc/nc = deaths/total in control group
xt&lt;-c(1,9,2,1,10,1,1)
nt&lt;-c(40,135,200,48,150,59,25)
xc&lt;-c(2,23,7,1,8,9,3)
nc&lt;-c(36,135,200,46,148,56,23)

rt&lt;- xt/nt
rc&lt;- xc/nc
logOR&lt;- log(rt*(1-rc)/(rc*(1-rt)))
varLogOR&lt;- 1/(nt*rt*(1-rt)) + 1/(nc*rc*(1-rc))

# Compare weighted mean and std err to Table 4 of Dersimonian and Kacker, 2007
metaNorm(logOR,varLogOR,method="PM")
metaNorm(logOR,varLogOR,method="DL")
metaNorm(logOR,varLogOR,method="fixed")
# Compare tau values to Table 3 of Dersimonian and Kacker, 2007
sqrt( metaNorm(logOR,varLogOR,method="PM")$statistic["tau squared"] )
sqrt( metaNorm(logOR,varLogOR,method="DL")$statistic["tau squared"] )

</code></pre>

<hr>
<h2 id='methodRuleWMW'>
Function to pick the method for <code>wmwTest</code> given the data and <code>exact</code> argument.
</h2><span id='topic+methodRuleWMW'></span>

<h3>Description</h3>

<p>Inputs x,y,exact, and chooseLimit, and outputs the method, one of 'asymptotic', 'exact.ce', or 'exact.mc'
</p>


<h3>Usage</h3>

<pre><code class='language-R'>methodRuleWMW(x, y, exact, chooseLimit = 5000)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="methodRuleWMW_+3A_x">x</code></td>
<td>

<p>vector of numeric responses from group 1
</p>
</td></tr>
<tr><td><code id="methodRuleWMW_+3A_y">y</code></td>
<td>

<p>vector of numeric responses from group 2
</p>
</td></tr>
<tr><td><code id="methodRuleWMW_+3A_exact">exact</code></td>
<td>

<p>logical, should exact methods be used?
</p>
</td></tr>
<tr><td><code id="methodRuleWMW_+3A_chooselimit">chooseLimit</code></td>
<td>

<p>boundary for choosing between methods
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let J=choose(m+n,n) where length(x)=m and length(y)=n. If exact=NULL then return 'exact.ce' if J is less than or equal to chooseLimit, otherwise return 'asymptotic'. If exact=FALSE return 'asymptotic'. If exact=TRUE then 
return 'exact.ce' if J is less than or equal to chooseLimit, otherwise return 'exact.mc'.
</p>


<h3>Value</h3>

<p> a character vector with one element, either:
</p>
<table role = "presentation">
<tr><td><code>asymptotic</code></td>
<td>
<p>for using normal approximations</p>
</td></tr>
<tr><td><code>exact.ce</code></td>
<td>
<p>for using exact methods with complete enumeration</p>
</td></tr>
<tr><td><code>exact.mc</code></td>
<td>
<p>for a Monte Carlo implementation of the exact version of the test</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>methodRuleWMW(rnorm(7),rnorm(7),exact=NULL)
methodRuleWMW(rnorm(7),rnorm(8),exact=NULL)
</code></pre>

<hr>
<h2 id='pbf'>
Behrens-Fisher distribution
</h2><span id='topic+pbf'></span><span id='topic+qbf'></span>

<h3>Description</h3>

<p>Used in <code><a href="#topic+bfTest">bfTest</a></code>. Distribution of the t-statistic used in 
Welch's t-test. The distribution depends on the sample sizes of the two groups, and the ratio of the two standard errors of the means.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pbf(q, n1, n2, R = NULL, s1 = NULL, s2 = NULL, epsilon = 10^(-8))
qbf(p,n1,n2,R=NULL,s1=NULL,s2=NULL,epsilon=10^(-8))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pbf_+3A_q">q</code></td>
<td>

<p>vector of quantiles
</p>
</td></tr>
<tr><td><code id="pbf_+3A_p">p</code></td>
<td>

<p>vector of probabilities
</p>
</td></tr>
<tr><td><code id="pbf_+3A_n1">n1</code></td>
<td>

<p>sample size in group 1</p>
</td></tr>
<tr><td><code id="pbf_+3A_n2">n2</code></td>
<td>

<p>sample size in group 2</p>
</td></tr>
<tr><td><code id="pbf_+3A_r">R</code></td>
<td>

<p>a function of the ratio of the two standard errors of the mean. Specifically, 
<code>atan((s1/sqrt(n1))/(s2/sqrt(n2)))</code>
</p>
</td></tr>
<tr><td><code id="pbf_+3A_s1">s1</code></td>
<td>

<p>sample standard deviation in group 1
</p>
</td></tr>
<tr><td><code id="pbf_+3A_s2">s2</code></td>
<td>

<p>sample standard deviation in group 2
</p>
</td></tr>
<tr><td><code id="pbf_+3A_epsilon">epsilon</code></td>
<td>

<p>a small positive number used to avoid computer errors</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The user must supply either <code>s1</code> and <code>s2</code> or supply <code>R</code>.
Suppose m1 and m2 are the means of the two groups and D is the true difference in means. Then the Behrens-Fisher test statistic is 
<code>T=(m1-m2-D)/sqrt(s1/sqrt(n1)+ s2/sqrt(n2))</code>. The value T can be equivalently written as <code>T=T1*sin(R)+T2*cos(R)</code>, where T1 and T2 are t random variables with n1-1 and n2-1 degrees of freedom. The cumulative distribution of T is found by numeric integration.
</p>
<p>We rewrite <code class="reqn">Pr[T&lt;=q]</code> as <code class="reqn"> \int_{-Inf}^{Inf} Pr[T2&lt;= (q-u*sin(R))/cos(R) | T1=u] Pr[T1=u] du</code>.
</p>


<h3>Value</h3>

<p>pbf gives the distribution function, qbf givds the quantile function.
</p>


<h3>References</h3>

<p>Kim, S-H, and Cohen, AS (1996). Table of percentage points of the Behrens-Fisher distribution. Journal of Statistical Computation and Simulation. 55(3) 181-187.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See Table 1 from Kim and Cohen, 1996
# at v1=n1-1=8 and v2=n2-1=12 with 45 degrees = 45*pi/180 radians
# for 0.95th percentile
# Table gives: 1.83496
qbf(0.95,9,13,45*pi/180)
# check Inf degrees of freedom, should give qnorm value
qbf(.95,Inf,Inf,45*pi/180)
qnorm(.95)
</code></pre>

<hr>
<h2 id='prevSeSp'>
Estimate prevalence with confidence interval accounting for sensitivity and specificity</h2><span id='topic+prevSeSp'></span>

<h3>Description</h3>

<p>Using the method of Lang and Reiczigel (2014), estimate prevalence and get a confidence interval 
adjusting for the sensitivity and specificity (including accounting for the variability of the sensitivity and specificity estimates).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prevSeSp(AP, nP, Se, nSe, Sp, nSp, conf.level = 0.95, neg.to.zero=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prevSeSp_+3A_ap">AP</code></td>
<td>

<p>apparent prevalence (proportion positive by test)
</p>
</td></tr>
<tr><td><code id="prevSeSp_+3A_np">nP</code></td>
<td>

<p>number tested for AP
</p>
</td></tr>
<tr><td><code id="prevSeSp_+3A_se">Se</code></td>
<td>

<p>estimated sensitivity (true positive rate)
</p>
</td></tr>
<tr><td><code id="prevSeSp_+3A_nse">nSe</code></td>
<td>

<p>number of positive controls used to estimate sensitivity
</p>
</td></tr>
<tr><td><code id="prevSeSp_+3A_sp">Sp</code></td>
<td>

<p>estimated specificity (1- false positive rate)
</p>
</td></tr>
<tr><td><code id="prevSeSp_+3A_nsp">nSp</code></td>
<td>

<p>number of negative controls used to estimate specificity
</p>
</td></tr>
<tr><td><code id="prevSeSp_+3A_conf.level">conf.level</code></td>
<td>

<p>confidence level
</p>
</td></tr>
<tr><td><code id="prevSeSp_+3A_neg.to.zero">neg.to.zero</code></td>
<td>

<p>logical, should negative prevalence estimates and lower confidence limits be set to zero?
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When measuring the prevalence of some disease in a population, it is useful to adjust for the fact that the test for the disease may not be perfect. 
We adjust the apparent prevalence (the proportion of people tested positive) for the sensitivity (true positive rate: proportion of the population that
has the disease that tests positive) and the specificity (1-false positive rate: proportion of the  population that do not have the disease that tests negative).
So if the true prevalence is <code class="reqn">\theta</code> and the true sensitivity and specificity are <code class="reqn">Se</code> and <code class="reqn">Sp</code>, then the expected value of the apparent prevalence is 
the sum of the expected proportion of true positive results and the expected proportion of false positive results:  
</p>
<p style="text-align: center;"><code class="reqn">AP = \theta Se + (1-Sp) (1-\theta).</code>
</p>
<p> Plugging in the estimates (and using the same notation for the estimates as the true values) 
and solving for <code class="reqn">\theta</code> we get the estimate of prevalence of 
</p>
<p style="text-align: center;"><code class="reqn">\theta = \frac{AP - (1-Sp)}{Se -(1-Sp)}.</code>
</p>

<p>Lang and Reiczigel (2014) developed an approximate confidence interval for the prevalence that not only adjusts for the sensitivity and specificity, but also 
adjusts for the fact that the sensitivity is estimated from a sample of true positive individuals (<code>nSe</code>) and
the specificity is estimate from a sample of true negative individuals (<code>nSp</code>).
</p>
<p>If the estimated false positive rate (1-specificity) is larger than the apparent prevalence, the prevalence estimate will be negative. This occurs because we observe
a smaller proportion of positive results than we would expect from a population known not to have the disease. The lower confidence limit can also be negative because of the variability in the specificity estimate. The default with <code>neg.to.zero=TRUE</code> sets those negative estimates and lower confidence limits to zero. 
</p>
<p>The Lang-Reiczigel method uses an idea discussed in Agresti and Coull (1998) to get approximate confidence intervals. 
For 95% confidence intervals, the idea is similar to adding 2 positive and 2 negative individuals to the apparent prevalence results, 
and adding 1 positive and 1 negative individual to the sensitivity and specificity test results, then using asymptotic normality. Simulations 
in Lang and Reiczigel (2014) show the method works well for true sensitivity and specificity each in ranges from 70% to over 90%.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:

</p>
<table role = "presentation">
<tr><td><code>estimate</code></td>
<td>
<p>the adjusted prevalence estimate, adjusted for sensitivity and specificity</p>
</td></tr>
<tr><td><code>statistic</code></td>
<td>
<p>the estimated sensitivity given by <code>Se</code></p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the estimated specificity given by <code>Sp</code></p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>a confidence interval for the prevalence.</p>
</td></tr>


<tr><td><code>method</code></td>
<td>
<p>the character string describing the output.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the unadjusted prevalence value and the sample size used to estimate it (nP).</p>
</td></tr>
</table>


<h3>Note</h3>

<p>There is a typo in equation 4 of Lang and Reiczigel (2014), the <code class="reqn">(1+\hat{P})^2</code> should be <code class="reqn">(1-\hat{P})^2</code>.</p>


<h3>Author(s)</h3>

<p>Michael P. Fay
</p>


<h3>References</h3>

<p>Agresti, A., Coull, B.A., 1998. Approximate is better than 'exact'for interval estimation of binomial proportions. Am. Stat. 52,119-126.
</p>
<p>Lang, Z. and Reiczigel, J., 2014. Confidence limits for prevalence of disease adjusted for estimated sensitivity and specificity. Preventive veterinary medicine, 113(1), pp.13-22.
</p>


<h3>See Also</h3>

<p><code>truePrev</code> in package <a href="https://CRAN.R-project.org/package=prevalence"><span class="pkg">prevalence</span></a> for Bayesian methods for this problem (but this requires JAGS (Just Another Gibbs Sampler), a separate software that can be called from R if it is installed on the user's system.)
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1 of Lang and Reiczigel, 2014
# 95% CI should be 0.349, 0.372
prevSeSp(AP=4060/11284,nP=11284,Se=178/179,nSe=179,Sp=358/359, nSp=359)

# Example 2 of Lang and Reiczigel, 2014
# 95% CI should be  0, 0.053
prevSeSp(AP=51/2971,nP=2971,Se=32/33,nSe=33,Sp=20/20, nSp=20)

# Example 3 of Lang and Reiczigel, 2014
# 95% CI should be 0 and 0.147
prevSeSp(AP=0.06,nP=11862,Se=0.80,nSe=10,Sp=1, nSp=12)

# Example 4 of Lang and Reiczigel, 2014
# 95% CI should be 0.58 to 0.87
prevSeSp(AP=259/509,nP=509,Se=84/127,nSe=127,Sp=96/109, nSp=109)
# 95% CI should be 0.037 to 0.195
prevSeSp(AP=51/509,nP=509,Se=23/41,nSe=41,Sp=187/195, nSp=195)
</code></pre>

<hr>
<h2 id='quantileTest'>
Tests and Confidence Intervals about a Quantile.
</h2><span id='topic+quantileTest'></span><span id='topic+quantileTest.default'></span><span id='topic+quantileTest.ordered'></span><span id='topic+medianTest'></span>

<h3>Description</h3>

<p>The ath quantile of a distribution is the value, q, such that 
F(q-) &lt;= a &lt;= F(q), where F(x)=Pr[X &lt;= x]. 
These are exact tests and confidence intervals on independent observations that do not any assumptions on the distribution, F. For example, the tests are exact when data are discrete or continuous, and when the distribution is non-symmetric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ordered'
quantileTest(x,...)

## Default S3 method:
quantileTest(x, q = 0, prob = 0.5, 
   alternative = c("two.sided", "less", "greater"), 
   conf.level = 0.95, ...)

medianTest(x, m=0, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="quantileTest_+3A_x">x</code></td>
<td>

<p>a vector of numeric, integer or ordered factor values
</p>
</td></tr>
<tr><td><code id="quantileTest_+3A_q">q</code></td>
<td>

<p>null quantile for test
</p>
</td></tr>
<tr><td><code id="quantileTest_+3A_m">m</code></td>
<td>
<p> null median for test </p>
</td></tr>
<tr><td><code id="quantileTest_+3A_prob">prob</code></td>
<td>
<p> quantile
</p>
</td></tr>
<tr><td><code id="quantileTest_+3A_alternative">alternative</code></td>
<td>

<p>a character string specifying the alternative hypothesis, must be one of &quot;two.sided&quot; (default), &quot;greater&quot; or &quot;less&quot;. You can specify just the initial letter.
</p>
</td></tr>
<tr><td><code id="quantileTest_+3A_conf.level">conf.level</code></td>
<td>

<p>confidence level of the interval
</p>
</td></tr>
<tr><td><code id="quantileTest_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A test on the quantile. The <code>medianTest</code> is just a wrapper function to call <code>quantileTest</code> with <code>prob</code>=0.5. 
</p>
<p>Ordinal factors may be used. The calculations just use as.numeric(x) for the factors, then return the character associated with that value. Estimates that are between two ordered factors, say &quot;C&quot; and &quot;D&quot;, return the character &quot;C/D&quot;. 
</p>


<h3>Value</h3>

<p>A list of class 'htest'. 
</p>


<h3>Author(s)</h3>

<p>Michael P. Fay
</p>


<h3>See Also</h3>

<p><code><a href="#topic+signTest">signTest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## For Poisson(mean=2.5) the median is 2
x&lt;-rpois(20,2.5)
medianTest(x)
x&lt;-ordered(c(rep("A",10),rep("B",60),rep("C",30)),levels=c("A","B","C"))
xnum&lt;-as.numeric(x)
quantileTest(xnum,q=2,prob=0.705)
quantileTest(x,q=2,prob=0.705)
</code></pre>

<hr>
<h2 id='signTest'>
Exact Sign Test with Confidence Intervals
</h2><span id='topic+signTest'></span>

<h3>Description</h3>

<p>Uses <code>link{binom.exact}</code> or <code><a href="exact2x2.html#topic+mcnemarExactDP">mcnemarExactDP</a></code> to create sign test with confidence intervals on different parameters.
Mid-p versions are available for some parameterizations (see details).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>signTest(x, stat=c("cd","cpp","ud"), nullparm=NULL, 
   alternative=c("two.sided","less","greater"), conf.level=0.95,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="signTest_+3A_x">x</code></td>
<td>

<p>numeric vector
</p>
</td></tr>
<tr><td><code id="signTest_+3A_stat">stat</code></td>
<td>

<p>statistic for estimates and confidence intervals,
&quot;cd&quot;= conditional difference: proportion positive - proportion negative, 
&quot;cpp&quot;= conditional proportion positive, and 
&quot;ud&quot;= unconditional difference: proportion positive-proportion negative
(conditional proportions are out of non-zero values, unconditional are out of all values)
</p>
</td></tr>
<tr><td><code id="signTest_+3A_nullparm">nullparm</code></td>
<td>

<p>null parameter value associated with <code>stat</code>, NULL value defaults to the exact  sign test (i.e., <code>stat="cd"</code> and 
codestat=&quot;ud&quot; gives 0, and <code>stat="cpp"</code> gives 0.5).
</p>
</td></tr>
<tr><td><code id="signTest_+3A_alternative">alternative</code></td>
<td>

<p>a character string specifying the alternative hypothesis, must be one of &quot;two.sided&quot; (default), &quot;greater&quot; or &quot;less&quot;. You can specify just the initial letter.
</p>
</td></tr>
<tr><td><code id="signTest_+3A_conf.level">conf.level</code></td>
<td>

<p>confidence level of the interval
</p>
</td></tr>
<tr><td><code id="signTest_+3A_...">...</code></td>
<td>

<p>arguments passed to <code><a href="exactci.html#topic+binom.exact">binom.exact</a></code>
or <code><a href="exact2x2.html#topic+mcnemarExactDP">mcnemarExactDP</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sign test is a conditional test, conditioning on the total number of non-zero observations testing that the proportion positive is different (or less, or greater) than the proportion negative.  When the responses are differences in paired binary observations this 
is the same as a McNemar test. 
</p>
<p>This function gives estimates and confidence intervals compatible with the exact sign test for three different parameterizations.
Let n.pos, n.neg,
and 
n.nonzero
be the number of positive, negative, and non-zero observations respectively out of 
n=<code>length(x)</code>.
The conditional proportion positive are n.pos/n.nonzero, and the unconditional proportion positive are n.pos/n. Similarly, 
the conditional proportion negative are n.neg/n.nonzero and the unconditional proportion negative are n.neg/n. 
When <code>stat='cd'</code> the parameterization is the conditional difference in proportions (pos-neg),
and when <code>stat='ud'</code> the parameterization is the uncondtional difference in proportions (pos-neg).
The third parameterization is <code>stat='cpp'</code> the conditional proportion positive. 
The argument <code>nullparm</code> gives the null value of the test when <code>alternative='two.sided'</code>. When <code>nullparm=NULL</code>, this gives 
the traditional sign test, where <code>nullparm=0</code> for <code>stat='cd'</code> and <code>stat='ud'</code> and 
<code>nullparm=0.5</code> for <code>stat='cpp'</code>.  
</p>
<p>Conditioning on m=n.nonzero, Y is binomial with parameters m and beta. So when <code>stat='cpp'</code> the parameter we are estimating is beta,
and when  <code>stat='cd'</code> the parameter we are estimating is beta - (1-beta) = 2*beta-1. 
We use <code><a href="exactci.html#topic+binom.exact">binom.exact</a></code> to do the p-value and confidence interval calculations. Thus, <code>midp</code> versions and different two-sided methods (given by <code>tsmethod</code>) can be calculated. 
</p>
<p>Unconditionally, we treat M (the number non-zero) as a random variable, and assume M is binomial with parameters n and theta.  
When  <code>stat='ud'</code> the parameter we are estimating is delta = theta*(2*beta-1), which is the unconditional difference: (proportion positive out of the total) - (proprtion negative out of the total). We use <code><a href="exact2x2.html#topic+mcnemarExactDP">mcnemarExactDP</a></code> to do the the p-value and confidence interval calculations. The methods associated with that function are described in Fay and Lumbard (2020). As of now, when <code>stat='ud'</code> a <code>midp</code> version is not available, and the only two-sided method available is a 'central' one, meaning the error for the 95% confidence interval is bounded by 2.5% on each side. 
</p>


<h3>Value</h3>

<p>A list of class 'htest' (use <code>str</code> to see elements)
</p>
<table role = "presentation">
<tr><td><code>statistic</code></td>
<td>
<p>vector of number of positive, negative, zero, and non-zero</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>vector of estimates related to <code>stat</code> argument</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>p.value associated with alternative</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>confidence interval</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>null parameter value</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>alternative</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>description of method</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>name of x argument</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The sign test can be interpreted as a test that the median is zero assuming continuous data. If you want to test on the median without making continuity assumptions use <code><a href="#topic+medianTest">medianTest</a></code>.
</p>
<p>Previous versions of <code>signTest</code> had <code>stat='pos-neg'</code> and <code>stat='prop pos'</code>, which are 
now referred to as <code>stat='cd'</code> and <code>stat='cpp'</code>, respectively. The old names give a warning, but may be removed in future versions.  
</p>


<h3>Author(s)</h3>

<p>Michael P. Fay
</p>


<h3>References</h3>

<p>Fay MP, and Lumbard, K (2020). Confidence Intervals for Difference in Proportions for Matched Pairs Compatible with Exact McNemar's or Sign Tests. (unpublished manuscript).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x&lt;-c(rep(-1,10),rep(0,60),rep(1,30))
signTest(x, stat='cd')
signTest(x, stat='cpp')
signTest(x, stat='ud')
# sample median is zero, 
# and not surprisingly the median test 
# properly gives a large p-value
medianTest(x)
</code></pre>

<hr>
<h2 id='simulateSS'>
Simulate sample sizes
</h2><span id='topic+simulateSS'></span>

<h3>Description</h3>

<p>A function that simulates sample sizes in an efficient manner. Inputs two functions: (1) a decision function that returns 1=reject, or 0=fail to reject, and (2) a data generating function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulateSS(decFunc, dataGenFunc, nstart = 100, numBatches = 100, repsPerBatch = 100, 
   power = 0.3, alpha = 0.025, nrepeatSwitch = 3, printSteps = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="simulateSS_+3A_decfunc">decFunc</code></td>
<td>

<p>decision function, inputs data from dataGenFunc and outputs 1 (reject) or 0 (fail to reject).
</p>
</td></tr>
<tr><td><code id="simulateSS_+3A_datagenfunc">dataGenFunc</code></td>
<td>

<p>data generating function, inputs a sample size and outputs simulated data object. Class of the data must match input for decFunc. 
</p>
</td></tr>
<tr><td><code id="simulateSS_+3A_nstart">nstart</code></td>
<td>

<p>starting sample size value
</p>
</td></tr>
<tr><td><code id="simulateSS_+3A_numbatches">numBatches</code></td>
<td>

<p>number of batches (must be at least 5), default=100
</p>
</td></tr>
<tr><td><code id="simulateSS_+3A_repsperbatch">repsPerBatch</code></td>
<td>

<p>number of replications per batch (must be at least 10), default=100
</p>
</td></tr>
<tr><td><code id="simulateSS_+3A_power">power</code></td>
<td>

<p>power desired
</p>
</td></tr>
<tr><td><code id="simulateSS_+3A_alpha">alpha</code></td>
<td>

<p>one-sided alpha level, used for estimating power from batches by normal approximation
</p>
</td></tr>
<tr><td><code id="simulateSS_+3A_nrepeatswitch">nrepeatSwitch</code></td>
<td>

<p>one of 2,3,4 or 5. default=3. If nrepeatSwitch batch estimates of sample size are the same in a row, then switch to an up-and-down method (adding or subtracting 1 to sample size).
</p>
</td></tr>
<tr><td><code id="simulateSS_+3A_printsteps">printSteps</code></td>
<td>

<p>logical, print intermediate steps of algorithm?
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is an algorithm proposed in Fay and Brittain (2022, Chapter 20). Here are the details of the algorithm. For step 1, we pick a starting sample size, say $N_1$, and the number of replications within a batch, $m$,
and the total number of batches, $b_tot$.
We simulate $m$ data sets with sample size $N_1$, and get the proportion of rejections, say $P_1$.
Then we use a normal approximation to estimate the target sample size, say $N_norm$. In step 2, we replicate $m$ data sets with sample size $N_2 = N_norm$
to get the associated proportion of rejections, say $P_2$. We repeat 2 more batches with $N_3=N_norm/2$ and $N_4=2 N_norm$,
to get proportions $P_3$, and $P_4$. Then in step 3, we use isotonic regression (which forces monotonicity, power to be non-decreasing with sample size) on the 4 observed pairs ($(N_1,P_1),...,(N_4,P_4)$), and linear interpolation to get our best estimate of
the sample size at the target power, $N_target$. We use that estimate of $N_target$ for our sample size for the next
batch of simulations. This idea is of using the best estimate of the target for the next iteration is studied in
Wu (1985, see Section 3).   Step 4 is iterative, for the $i$th batch we repeat the isotonic regression, except now with $N_i$ estimated from the  first $(i-1)$ observation pairs.  We repeat step 4 until either the number of batches is $b_tot$,
or the current sample size estimate is the same as the last nrepeatSwitch-1 estimates, in which case we switch to an up-and-down-like method.
For each iteration of the up-and-down-like method, if the current proportion of rejections from the last batch of $m$ replicates is greater than the target power, then subtract 1 from the
current sample size estimate, otherwise add 1. Continue with that up-and-down-like method until we reach the number of batches equal to $b_tot$. The up-and-down-like method was added because sometimes the algorithm would get stuck in too large of a sample size estimate.
</p>


<h3>Value</h3>

<p>A list with elements:
</p>
<table role = "presentation">
<tr><td><code>N</code></td>
<td>
<p>vector of estimated sample sizes at the end of each batch</p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p>vector of power estimates at the end of each batch</p>
</td></tr>
<tr><td><code>Nstar</code></td>
<td>
<p>estimate of sample size, not necessarily an integer</p>
</td></tr>
<tr><td><code>Nestimate</code></td>
<td>
<p>integer estimate of sample size equal to ceiling(Nstar)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael P. Fay
</p>


<h3>References</h3>

<p>Fay, M.P. and Brittain, E.H. (2022). Statistical Hypothesis Testing in Context. Cambridge University Press. New York. 
</p>
<p>Wu, CJ (1985). Efficient sequential designs with binary data. Journal of the American Statistical Association. 19: 1085-1098.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simple example to show method
# simulate 2-sample t-test power
# for this simple case, better to just use power.t.test
power.t.test(delta=.5,sig.level=0.025,power=.8,
   type="two.sample",alternative="one.sided")
decFunc&lt;-function(d){
  ifelse(t.test(d$y1,d$y2,alternative="less")$p.value&lt;=0.025,1,0)
}
dataGenFunc&lt;-function(n){
  list(y1=rnorm(n,0),y2=rnorm(n,.5))
}
# for example use on 20 batches with 20 per batch
set.seed(1)
simulateSS(decFunc,dataGenFunc,nstart=100,numBatches=100,repsPerBatch=100,
   power=0.80, alpha=0.025,printSteps=FALSE)
</code></pre>

<hr>
<h2 id='tukeyWelsch'>
Tukey-Welsch Pairwise Tests
</h2><span id='topic+tukeyWelsch'></span>

<h3>Description</h3>

<p>Calculate pairwise comparisons between groups levels using step down correction for multiple testing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tukeyWelsch(y, g, method = c("aov", "kw", "sr", "user"), 
    pvalfunc = NULL, padjfunc = padjTW, maxnTest=10^4,nTestMessage=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tukeyWelsch_+3A_y">y</code></td>
<td>

<p>response vector
</p>
</td></tr>
<tr><td><code id="tukeyWelsch_+3A_g">g</code></td>
<td>

<p>grouping vector or factor
</p>
</td></tr>
<tr><td><code id="tukeyWelsch_+3A_method">method</code></td>
<td>

<p>type of method for tests, one of 'aov' (ANOVA which is a t-test for the pairwise comparisons) 'kw' (Kruskal-Wallis test, which is a Wilcoxon-Mann-Whitney test for the pairwise comparisons), 
'sr' (studentized range test), or 'user' (user supplied function, see details).
</p>
</td></tr>
<tr><td><code id="tukeyWelsch_+3A_pvalfunc">pvalfunc</code></td>
<td>

<p>function to test for effects and return a p-value. Used if method='user'. See details. 
</p>
</td></tr>
<tr><td><code id="tukeyWelsch_+3A_padjfunc">padjfunc</code></td>
<td>

<p>function that takes the unadjusted p-value vector from a stage, and returns the adjusted p-value vector for that stage (see details)
</p>
</td></tr>
<tr><td><code id="tukeyWelsch_+3A_maxntest">maxnTest</code></td>
<td>
<p>maximum number of tests, if the number of tests is larger than <code>maxnTest</code> then
gives an error</p>
</td></tr>
<tr><td><code id="tukeyWelsch_+3A_ntestmessage">nTestMessage</code></td>
<td>

<p>logical, print a message at the start of calculations telling how many tests will be calculated
</p>
</td></tr>
<tr><td><code id="tukeyWelsch_+3A_...">...</code></td>
<td>

<p>additional arguments to pass to XXX (if method='aov'), YYY (if method='kw')
or pvalfunc (if method='user')
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function does a k-sample test (either one-way ANOVA [method='aov'] or Kruskal-Wallis test [method='kw']) on the responses when the g vector has k levels. Then it does all the pairwise comparisons (either t.tests [method='aov'] or Wilcoxon-Mann-Whitney tests [method='kw']) giving multiple 
comparison adjusted p-values. The adjustment uses a step-down method, that is different from (and potentially more powerful than) the single step procedures in <code><a href="stats.html#topic+pairwise.t.test">pairwise.t.test</a></code> and <code><a href="stats.html#topic+pairwise.wilcox.test">pairwise.wilcox.test</a></code>. The method is described in Einot and Gabriel (1975) [for the anova case] and Campbell and Skillings (1985) for the Kruskal-Wallis case. See also Hochberg and Tamhane (1987, p. 111 for 'aov' case, and p. 247-248 for the 'kw' case). 
</p>
<p>Here are the details. First, the k-sample test is done, where the type of test is determined  by the <code>method</code>. The function repeats that type of test k-1 times, leaving out a different level of the group each time. These are k-1 tests, each having k-1 levels. This process repeats itself (i.e., do choose(k,k-2) tests each having k-2 levels, then do choose(k,k-3) tests each having k-3 levels, etc) until we get to the choose(k,2) pairwise tests. Reject at level aj = 1- (1-alpha)^(j/k), for all tests where there are j groups, for j=2,..,k-2 and at level aj=alpha for j=k-1 and k. These adjusted significance levels are known as the Tukey-Welch (see Hochberg and Tamhane, p. 111) or Ryan (see Einot and Gabriel, 1975) levels. Then we only reject each pairwise comparison, if we reject at all null hypotheses that contain that pair.
</p>
<p>We convert this procedure into adjusted p-values, by finding the lowest alpha level such that each pairwise comparison would be rejected, that is its adjusted p-value. The <code>padjfunc</code> is a function that takes the unadjusted p-values 
and gives the adjustment for each level by itself. For example, the default uses the Tukey-Welch adjusted significance levels, and the function solves for alpha as a function of aj (i.e.,  inputs unadjP and returns either  1-(1-unadjP^(k/j) for j=2,3,...k-2 or unadjP for j=k-1 or k)). Then taking the individual level adjusted p-values, we define the step-down adjusted p-value for each pairwise comparison as the maximum of all the individual level adjusted p-values for each hypothesis that contains the pair as part of its groups tested. 
</p>
<p>When k=3, this method gives an adjusted p-value for each pairwise comparison that is the maximum of the k-sample test p-value and the unadjusted p-value for the two-sample test using that pair of levels.
</p>
<p>When method='user' the function uses the <code>pvalfunc</code> function to test for p-values. The function must input y and g and output the p-value for the j-sample test, where j is the number of levels present in g. So the function must be defined when j=2,3,...,k. 
</p>


<h3>Value</h3>

<p>An object of class 'tukeyWelsch', a list with elements:
</p>
<table role = "presentation">
<tr><td><code>fullResults</code></td>
<td>
<p>a list of all the intermediate p-values (unadjusted and adjusted). Not printed by default</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>description of method</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>description of input data</p>
</td></tr>
<tr><td><code>ksample.pvalue</code></td>
<td>
<p>p-value for k-sample test</p>
</td></tr>
<tr><td><code>pairwise.pvalues</code></td>
<td>
<p>vector of adjusted p-values for pairwise comparisons</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael P. Fay
</p>


<h3>References</h3>

<p>Campbell and Skillings (1985) JASA 998-1003.
</p>
<p>Einot and Gabriel (1975) JASA 574-583.
</p>
<p>Hochberg, Y and Tamhane, AC (1987) Multiple Comparison Procedures. Wiley: New York.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+pairwise.wilcox.test">pairwise.wilcox.test</a></code> and   <code><a href="stats.html#topic+pairwise.t.test">pairwise.t.test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##
createData&lt;-function(n,props,shifts,ry=rnorm){
  k&lt;-length(props)
  if (round(sum(props),8)!=1) stop("sum of props must be 1")
  props&lt;- props/sum(props)
  if (length(shifts)!=k) stop("length of shifts must equal length of props")
  g&lt;-rep(1:k,as.vector(rmultinom(1,n,props)))
  y&lt;-ry(n)
  for (i in 1:k){
    y[g==i]&lt;-y[g==i]+shifts[i]
    }
  list(y=y,g=g)
}
set.seed(1)
d&lt;-createData(100,c(.2,.3,.2,.3),c(0,0,0,1))
tukeyWelsch(d$y,factor(d$g),method="kw")
tukeyWelsch(d$y,factor(d$g),method="aov")
tukeyWelsch(d$y,factor(d$g),method="sr")
TukeyHSD(aov(d$y~factor(d$g)))[[1]][,"p adj"]

</code></pre>

<hr>
<h2 id='var1Test'>
One Sample Test of Normal Variance
</h2><span id='topic+var1Test'></span>

<h3>Description</h3>

<p>Give tests and confidence intervals on the variance of a sample from a normal distribution. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var1Test(x, nullVar = 1, 
   alternative = c("two.sided", "less", "greater"), 
   conf.level = 0.95)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="var1Test_+3A_x">x</code></td>
<td>

<p>numeric vector
</p>
</td></tr>
<tr><td><code id="var1Test_+3A_nullvar">nullVar</code></td>
<td>

<p>null variance, or variance on the boundary between the null and alternative hypotheses
</p>
</td></tr>
<tr><td><code id="var1Test_+3A_alternative">alternative</code></td>
<td>

<p>a character string specifying the alternative hypothesis, must be one of &quot;two.sided&quot; (default), &quot;greater&quot; or &quot;less&quot;. You can specify just the initial letter.
</p>
</td></tr>
<tr><td><code id="var1Test_+3A_conf.level">conf.level</code></td>
<td>

<p>confidence level of the interval
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Tests derived from normality assumption.
</p>


<h3>Value</h3>

<p>A list of class 'htest' (use str to see elements)
</p>


<h3>Author(s)</h3>

<p>Michael P. Fay
</p>


<h3>References</h3>

<p>Dudewicz, EJ and Mishra, SN (1988) Modern Mathematical Statistics. Wiley. (Section 9.6).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>var1Test(rnorm(25))
</code></pre>

<hr>
<h2 id='Vpo'>
Variance for estimated Mann-Whitney parameter under proportional odds.
</h2><span id='topic+Vpo'></span>

<h3>Description</h3>

<p>A function to calculate the variance of the estimated Mann-Whitney parameter under the proportional odds model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Vpo(PHI, tf, ny, nx)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Vpo_+3A_phi">PHI</code></td>
<td>

<p>Mann-Whitney parameter
</p>
</td></tr>
<tr><td><code id="Vpo_+3A_tf">tf</code></td>
<td>

<p>tie factor
</p>
</td></tr>
<tr><td><code id="Vpo_+3A_ny">ny</code></td>
<td>

<p>sample size for the y variable
</p>
</td></tr>
<tr><td><code id="Vpo_+3A_nx">nx</code></td>
<td>

<p>sample size for the x variable  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses uniroot to find the variance numerically. The approximation using the LAPH method (Lehmann alternative model combined with the proportional hazards model) is much faster and is the default.  
</p>


<h3>Value</h3>

<p>the variance of the estimated Mann-Whitney parameter
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Vpo(.7,.8,5,7)
## Compare to LAPH method
VLAPH&lt;- function(PHI,tf=tiefactor,ny=n.y,nx=n.x){
        tf*(PHI*(1-PHI)/(ny*nx))*
          (1+((ny+nx-2)/2)*
             ((1-PHI)/(2-PHI)+PHI/(1+PHI)))
      }   
VLAPH(.7,.8,5,7)      
</code></pre>

<hr>
<h2 id='wmwControl'>
Arguments passed to wmwTest.
</h2><span id='topic+wmwControl'></span>

<h3>Description</h3>

<p>Creates a list of arguments that are used for algorithm 
control and output control.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wmwControl(nMC = 10^4, epsilon = 10^(-8), 
   digits = 10, latentOutput = c("mw", "po"),
   removeTieAdjustment = FALSE, ncheckgrid=100,
   rcheckgrid=0.1, Vmethod="LAPH")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="wmwControl_+3A_nmc">nMC</code></td>
<td>

<p>number of Monte Carlo replications
</p>
</td></tr>
<tr><td><code id="wmwControl_+3A_epsilon">epsilon</code></td>
<td>

<p>small number, used in uniroot for limits. Rarely needs to be changed.
</p>
</td></tr>
<tr><td><code id="wmwControl_+3A_digits">digits</code></td>
<td>

<p>number of digits for rounding. This is needed so that true ties are treated as ties by the computer, otherwise there can be non-trival errors in the p.values 
</p>
</td></tr>
<tr><td><code id="wmwControl_+3A_latentoutput">latentOutput</code></td>
<td>
<p>either 'mw' (Mann-Whitney parameter) or 'po' (proportional odds parameter).
Describe output of parameter, estimate, and confidence intervals when <code>latentContinuous=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="wmwControl_+3A_removetieadjustment">removeTieAdjustment</code></td>
<td>
<p>logical, should tie adjustment be removed? If there are no ties TRUE and FALSE give the same answer. <code>removeTieAdjustment=TRUE</code> is not recommended, only use TRUE when used with <code>correct=FALSE</code> to reproduce method 5 of Newcombe (2006), see example.
</p>
</td></tr>
<tr><td><code id="wmwControl_+3A_ncheckgrid">ncheckgrid</code></td>
<td>
<p>number of elements in the grid for checking 
after the uniroot call when <code>tsmethod='abs'</code>
</p>
</td></tr>
<tr><td><code id="wmwControl_+3A_rcheckgrid">rcheckgrid</code></td>
<td>
<p>range of how far back or forward to look in the checking after the uniroot call when <code>tsmethod='abs'</code>
</p>
</td></tr>
<tr><td><code id="wmwControl_+3A_vmethod">Vmethod</code></td>
<td>
<p>character to determine the variance function for the asymptotic method. 'LAPH' gives the combination Lehmann alternative and proportional hazards method, and 'PO' gives the proportional odds method <code><a href="#topic+Vpo">Vpo</a></code>. The methods are nearly identical, but the 'PO' method takes longer, so the default is 'LAPH'. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with values of the same name as the arguments of the function.
</p>

<hr>
<h2 id='wmwTest'>
Wilcoxon-Mann-Whitney test with Confidence Interval on Mann-Whitney Parameter
</h2><span id='topic+wmwTest'></span><span id='topic+wmwTest.default'></span><span id='topic+wmwTest.formula'></span><span id='topic+wmwTest.matrix'></span>

<h3>Description</h3>

<p>The <code>wmwTest</code> function calculates the 
Wilcoxon-Mann-Whitney test (normal approximation, exact complete enumeration, and exact Mante Carlo implementation) together with confidence intervals on the Mann-Whitney parameter, Pr[ X&lt;Y] + 0.5 Pr[X=Y].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wmwTest(x, ...)

## Default S3 method:
wmwTest(x, y, alternative = c("two.sided", "less", "greater"), 
   phiNull = 0.5, exact = NULL, correct = TRUE, conf.int = TRUE, conf.level = 0.95, 
   latentContinuous = FALSE, method = NULL, methodRule = methodRuleWMW, 
   tsmethod = c("central", "abs"), control = wmwControl(),...)

## S3 method for class 'formula'
wmwTest(formula, data, subset, na.action, ...)

## S3 method for class 'matrix'
wmwTest(x,...)


</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="wmwTest_+3A_x">x</code></td>
<td>
<p>a (non-empty) numeric vector of data values from group 1, or a contingency table matrix with 2 rows with the top row representing group 1 and the bottom row group 2</p>
</td></tr>
<tr><td><code id="wmwTest_+3A_y">y</code></td>
<td>
<p>an optional (non-empty) numeric vector of data values from group 2</p>
</td></tr>
<tr><td><code id="wmwTest_+3A_alternative">alternative</code></td>
<td>
<p>a character string specifying the alternative
hypothesis, must be one of <code>"two.sided"</code> (default),
<code>"greater"</code> or <code>"less"</code>.  You can specify just the initial
letter.</p>
</td></tr>
<tr><td><code id="wmwTest_+3A_phinull">phiNull</code></td>
<td>

<p>null hypothesis value for the Mann-Whitney parameter, Pr[X&lt;Y]+0.5*Pr[X=Y].
Defaults to 0.5.
</p>
</td></tr>
<tr><td><code id="wmwTest_+3A_exact">exact</code></td>
<td>
<p> logical, should exact test be calculated? (see method) </p>
</td></tr>
<tr><td><code id="wmwTest_+3A_correct">correct</code></td>
<td>
<p>a logical indicating whether to apply continuity correction in the normal approximation for the p-value and confidence interval (when method='asymptotic')
</p>
</td></tr>
<tr><td><code id="wmwTest_+3A_conf.int">conf.int</code></td>
<td>
<p>logical, should confidence intervals be calculated?</p>
</td></tr>
<tr><td><code id="wmwTest_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level of the interval.</p>
</td></tr>
<tr><td><code id="wmwTest_+3A_latentcontinuous">latentContinuous</code></td>
<td>
<p>logical, should estimates and confidence intervals be presented as 
latent continuous parameters? (see details)</p>
</td></tr>
<tr><td><code id="wmwTest_+3A_method">method</code></td>
<td>
<p> character defining method, one of 'asymptotic', 'exact.ce' (exact by complete enumeration), 'exact.mc' (exact by Monte Carlo approximation). NULL value defaults to result of methodRule</p>
</td></tr>
<tr><td><code id="wmwTest_+3A_methodrule">methodRule</code></td>
<td>
<p> function that inputs x,y, and exact and outputs a method, see <code><a href="#topic+methodRuleWMW">methodRuleWMW</a></code></p>
</td></tr>
<tr><td><code id="wmwTest_+3A_tsmethod">tsmethod</code></td>
<td>
<p>two-sided method, either  'central' (double the one-sided p-values) or 'abs' (test statistic uses absolute value of difference in phi estimate and phiNull)</p>
</td></tr> 
<tr><td><code id="wmwTest_+3A_control">control</code></td>
<td>
<p>a list of arguments for control of algorithms and output, see <code><a href="#topic+wmwControl">wmwControl</a></code></p>
</td></tr>
<tr><td><code id="wmwTest_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>lhs ~ rhs</code> where <code>lhs</code>
is a numeric variable giving the data values and <code>rhs</code> a factor
with two levels giving the corresponding groups.</p>
</td></tr>
<tr><td><code id="wmwTest_+3A_data">data</code></td>
<td>
<p>an optional matrix or data frame (or similar: see
<code><a href="stats.html#topic+model.frame">model.frame</a></code>) containing the variables in the
formula <code>formula</code>.  By default the variables are taken from
<code>environment(formula)</code>.</p>
</td></tr>
<tr><td><code id="wmwTest_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="wmwTest_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.  Defaults to
<code>getOption("na.action")</code>.</p>
</td></tr>
<tr><td><code id="wmwTest_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>wmwTest</code> evaluates the Wilcoxon-Mann-Whitney test (also called the Mann-Whitney U test or the Wilcoxon rank sum test). The WMW test is a permutation two-sample rank test, and the test may be evaluated under many different sets of assumptions (Fay and Proschan, 2010). The least restrictive set of assumptions tests the null hypothesis that the two distributions of the two samples are equal versus the alternative that they are different. Unfortunately, with only those assumptions, we cannot get confidence intervals on the Mann-Whitney parameter, phi= Pr[ X&lt;Y] + 0.5 Pr[X=Y]. In order to get confidence intervals on phi, we need additional assumptions, and for this function we use the proportional odds assumption. This assumption can be interpreted as saying that there exists some unknown monotonic transformation of the responses that leads to a location shift in a logistic distribution. This can work for discrete data (i.e., with ties allowed) if we interpret discrete responses as a grouping of some underlying latent continuous response. The proportional odds assumption is less restrictive that the assumption used in <code>wilcox.test</code>, which assumes a location shift on the unknown continuous distribution of the untransformed data. 
</p>
<p>In summary, the two-sided p-value can be interpreted as testing the null that the two 
distributions are equal, and the confidence intervals on the Mann-Whitney parameter 
are intrepreted under the proportional odds assumption.
In general the confidence intervals are compatible with the associated p-values,
for details see Fay and Malinovsky (2018).
</p>
<p>There is a choice of three methods. When <code>method='asymptotic'</code>, the test is implemented using a normal approximation, with (<code>correct=TRUE</code>) or without 
(<code>correct=FALSE</code>) a continuity correction. The resulting p-values should 
match <code><a href="stats.html#topic+wilcox.test">wilcox.test</a></code> (when 
<code>paired=FALSE</code>  and <code>exact=FALSE</code>). When <code>method='exact.ce'</code>, the test is implemented using complete enumeration of all permutations, and hence is only tractible for very small sample sizes (less than 10 in each group). When <code>method='exact.mc'</code>, the test is implemented using Monte Carlo with B=10^4 replications (change B with <code>control=controlWMW(nMC=B)</code>). As B gets larger the p-value approaches the exact one. (See 'note' section, sometimes the <code>method='exact.mc'</code> will not work.)    
</p>
<p>The  <code>tsmethod='central'</code> gives two-sided p-value that is equal to 
<code>min(1,min(2*pless,2*pgreater))</code>. 
Alternatively,  <code>tsmethod='abs'</code> gives the two-sided method, 
which is based on the test statistic  |phi - phiNull|.
Under the proportional odds assumption,  <code>tsmethod='central'</code> allows us to interpret p.value/2 as one-sided p-values (this is not allowed using <code>tsmethod='abs'</code>). 
With continuous data, the p-values will be the same, but with ties they can be different.
</p>
<p>From the two groups x (or top row of contringency table, or first factor in rhs of formula) and y (or bottom row of contingency table, or second factor in rhs of formula) the Mann-Whitney parameter represents Pr[X&lt;Y]+0.5Pr[X=Y]. It is also the area under the curve of an ROC curve (see Hanley and McNeil, 1982). The confidence interval when <code>method='asymptotic'</code> generalizes the Method 5 of Newcombe (2006), which was a 
score-type modification of the interval of Hanley and McNeil (1982). The generalization is that the confidence interval 
adjusts for ties and allows a continuity correction (see examples below).
</p>
<p>The <code>methodRule</code> function allows automatic choice of the method of calculation based on the data and the <code>exact</code> argument. 
</p>
<p>When the data are discrete, we can treat the data as if they are a grouping of some underlying continuous responses. Using the proportional odds assumption, we can then translate the Mann-Whitney parameter on the observed discrete data into the Mann-Whitney parameter on the latent continuous data (when <code>latentContinuous=TRUE</code> and using the default 
<code>control=controlWMW(latentOutput='mw')</code>). You can also translate the results into the proportional odds parameter on the latent continuous responses (when <code>latentContinuous=TRUE</code> and using  
<code>control=controlWMW(latentOutput='po')</code>). Translation is done with <code><a href="#topic+latentTransform">latentTransform</a></code>.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table role = "presentation">
<tr><td><code>statistic</code></td>
<td>
<p>U statistic estimate of the Mann-Whitney parameter.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>tie factor</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>a confidence interval for the Mann-Whitney parameter 
appropriate to the
specified alternative hypothesis.</p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p>the estimated difference in means</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>the specified hypothesized value of the mean
difference</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>a character string describing the alternative
hypothesis.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string describing the test.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the name(s) of the data.</p>
</td></tr>
</table>


<h3>Warning </h3>

<p>The algorithm for calculating the confidence interval when <code>tsmethod='abs'</code> is not guaranteed to give the correct value. It is possible to skip over a value. For more accurate results 
increase <code>control=wmwControl(rcheckgrid)</code>
and <code>control=wmwControl(ncheckgrid)</code>
</p>


<h3>Note</h3>

<p>The <code>method='exact.mc'</code> can sometimes fail. The issue is that for some Monte Carlo simulations the one-sided p-value function is not monotonic, even in for data sets where the one-sided p-value would be monotonic if we could do complete enumeration. In this case, the confidence limit will be set to <code>NA</code> and a warning will suggest trying <code>method='asymptotic'</code>
or <code>method='exact.ce'</code> if feasible. Here is an example where that occurs: 
<code>set.seed(1);</code> <code>g&lt;- c(rep(0,6),1,rep(0,4),1,rep(0,3),1,1,0,1,1,0,rep(1,5));</code>
<code>y&lt;-1:26;</code> <code>wmwTest(y~g,exact=TRUE)</code>.
</p>


<h3>References</h3>

<p>Fay, MP and Malinovsky, Y (2018). Confidence Intervals of the Mann-Whitney Parameter that are Compatible with the Wilcoxon-Mann-Whitney Test. Statistics in Medicine:
DOI: 10.1002/sim.7890.
</p>
<p>Fay, MP and Proschan MA (2010). Wilcoxon-Mann-Whitney of t-test? On assumptions for hypothesis tests and multiple interpretations of decision rules. Statistics Surveys 4:1-39.
</p>
<p>Hanley, JA, and McNeil, BJ (1982). The Meaning and Use of the Area under a Receiver Operating Characteristic (ROC) Curve. Radiology 143: 29-36.
</p>
<p>Newcombe, Robert G. (2006). Confidence intervals for an effect size measure based on the Mann-Whitney statistic. Part 2: asymptotic methods and evaluation. Statistics in medicine 25(4): 559-573.
</p>


<h3>See Also</h3>

<p>See <code><a href="stats.html#topic+wilcox.test">wilcox.test</a></code> for either exact p-value or the same asymptotic p-value and confidence interval on location shift under the shift assumption. 
</p>
<p>See <code><a href="coin.html#topic+wilcox_test">wilcox_test</a></code> for exact p-value and exact confidence interval on location shift.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># data from Table 1 of Hanley and McNeil (also given in Table 1 of Newcombe, 2006)
HMdata&lt;-matrix(c(33,3,6,2,6,2,11,11,2,33),nrow=2,dimnames=
   list(c("Normal","Abnormal"),
   c("Definitely Normal",
   "Probably Normal",
   "Questionable",
   "Probably Abnormal",
   "Definitely Abnormal")))
HMdata
# to match Newcombe (2006, Table 1, Method 5) exactly 
# use correct=FALSE and RemoveTeAdjustment=TRUE
wmwTest(HMdata, correct=FALSE, RemoveTieAdjustment=TRUE)
# generally smaller intervals with closer to nominal coverage with 
# tie adjustment and continuity correction
wmwTest(HMdata)
</code></pre>

<hr>
<h2 id='WprevSeSp'>Weighted prevalence inferences adjusted for sensitivity and specificity</h2><span id='topic+WprevSeSp'></span>

<h3>Description</h3>

<p>Prevalence inferences from weighted survey data adjusted for
sensitivity and specificity estimates as well as their variability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>WprevSeSp(
  x,
  n,
  w,
  cn,
  mn,
  cp,
  mp,
  method = c("binomial", "poisson"),
  conf.level = 0.95,
  nmc = 1e+05,
  seed = 49201
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="WprevSeSp_+3A_x">x</code></td>
<td>
<p>integer  vector of apparent positive tests for each group</p>
</td></tr>
<tr><td><code id="WprevSeSp_+3A_n">n</code></td>
<td>
<p>integer vector of number of tests for each group</p>
</td></tr>
<tr><td><code id="WprevSeSp_+3A_w">w</code></td>
<td>
<p>numeric vector of weights for each group (must sum to 1)</p>
</td></tr>
<tr><td><code id="WprevSeSp_+3A_cn">cn</code></td>
<td>
<p>number of positive tests for negative controls</p>
</td></tr>
<tr><td><code id="WprevSeSp_+3A_mn">mn</code></td>
<td>
<p>number of negative controls tested</p>
</td></tr>
<tr><td><code id="WprevSeSp_+3A_cp">cp</code></td>
<td>
<p>number of positive tests for positive controls</p>
</td></tr>
<tr><td><code id="WprevSeSp_+3A_mp">mp</code></td>
<td>
<p>number of positive controls tested</p>
</td></tr>
<tr><td><code id="WprevSeSp_+3A_method">method</code></td>
<td>
<p>either &quot;binomial&quot; or &quot;poisson&quot;</p>
</td></tr>
<tr><td><code id="WprevSeSp_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level of the interval</p>
</td></tr>
<tr><td><code id="WprevSeSp_+3A_nmc">nmc</code></td>
<td>
<p>number of Monte Carlo replications</p>
</td></tr>
<tr><td><code id="WprevSeSp_+3A_seed">seed</code></td>
<td>
<p>seed for random number generation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is for estimating prevalence from a population using a complex 
survey with an imperfect assay. The survey data are partitioned into K strata 
(or groups), and the ith stratum is weighted by w[i] and we  observe a 
proportion x[i]/n[i] that 
test positive on the assay. The weights are required to sum to 1, so that
the apparent prevalence (i.e., the prevalence 
assuming the assay is perfect) is a weighted average of the proportions of the 
strata (i.e., <code>sum(w*(x/n))</code>). The assay's sensitivity and specificity is  
measured by two populations, a positive control population gives a sensitivity 
estimate (<code>cp/mp</code>), and a negative control population gives a 
specificity estimate (<code>1 - cn/mn</code>). The adjusted prevalence estimate (adjusted 
for sensitivity and specificity) is the standard one described by Rogan and 
Gladen (1978). The <code>WprevSeSP</code> gives confidence intervals on the prevalence
that account for the sampling variability in the survey 
(by a multinomial approximation), as well as for the varibility in the 
sensitivity and specificity estimates, but it does not account for the variability 
of the weights. 
</p>
<p>There are two methods, the 'binomial' method is based on the method of Korn and 
Graubard (1998) which is developed using a modification of exact binomial 
intervals, and the 'poisson' method which is based on the gamma method of Fay and 
Feuer (1997) which is developed assuming a weighted sum of Poisson variates. 
Simulations show that the 'poisson' method had greater than nominal coverage in all
simulated scenarios at the cost of conservativeness, while the 'binomial' method
was less conservative but had less than nominal coverage in some scenarios.
</p>
<p>For details see Bayer, Fay and Graubard (2023). 
</p>


<h3>Value</h3>

<p>A list with class &quot;htest&quot; containing the following components:
</p>
<table role = "presentation">
<tr><td><code>estimate</code></td>
<td>
<p>the adjusted prevalence estimate, adjusted for sensitivity and specificity</p>
</td></tr>
<tr><td><code>statistic</code></td>
<td>
<p>the estimated sensitivity given by cp / mp</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the estimated specificity given by 1 - cn/ mn</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>a confidence interval for the prevalence</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the unadjusted prevalence value</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the character string describing the output</p>
</td></tr>
</table>


<h3>References</h3>

<p>Bayer, D.M., Fay, M.P., and Graubard, B.I. (2023). &rdquo;Confidence intervals for prevalence estimates from complex surveys with imperfect assays&rdquo; arXiv:2205.13494.
</p>
<p>Fay MP, Feuer EJ. Confidence intervals for directly standardized rates: a method based on the gamma distribution. Statistics in Medicine 1997; 16(7): 791-801.
</p>
<p>Korn EL, Graubard BI. Confidence intervals for proportions with small expected number of positive counts estimated from
survey data. Survey Methodology 1998; 24: 193-201.
</p>
<p>Rogan WJ, Gladen B. Estimating prevalence from the results of a screening test. Am J Epidemiol 1978; 107(1): 71-76.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>example_data_WprevSeSp &lt;- list(
  x = c(
    53, 47, 63, 50, 54, 54, 57, 51, 66, 51, 52, 48, 37, 44, 59,
    55, 50, 58, 52, 54, 41, 45, 49, 54, 37, 53, 57, 58, 55, 55, 56,
    42, 58, 47, 49, 63, 54, 54, 54, 41, 43, 56, 44, 49, 47, 45, 62,
    53, 54, 47
  ),
  n = c(
    200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,
    200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,
    200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,
    200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200
  ),
  w = c(
    0.0205962892844504, 0.0204062236737538, 0.0203843096373626,
    0.0202785701233134, 0.0202617051778543, 0.0202138087214499, 0.0201972974884707,
    0.0201818190015587, 0.0201631543739836, 0.0201560795402158, 0.0201555234250465,
    0.0201461978246263, 0.0201342022821394, 0.0201264004067009, 0.0201167314250592,
    0.0201015081093692, 0.0201003484427457, 0.0201002680000886, 0.0200817537259523,
    0.0200573433887284, 0.0200443907258367, 0.0200358187073312, 0.0200349749335002,
    0.0200264994605187, 0.0200112846914561, 0.020006219121804, 0.0199975642569458,
    0.0199649774153205, 0.0199614929059539, 0.0199426355876479, 0.0199334287088002,
    0.0199298633246975, 0.0199150015155486, 0.0199063452368827, 0.0198920051366782,
    0.0198877425787182, 0.0198679831412633, 0.0198500844815989, 0.0198381388412286,
    0.0198348595904904, 0.0198348180141822, 0.0198174510243331, 0.0197922036364436,
    0.0197821574067888, 0.0197204417557631, 0.0197004976818864, 0.019682896458092,
    0.019649677766428, 0.0196158425485035, 0.019563169292488
  ),
  cn = 77,
  cp = 58,
  mn = 300,
  mp = 60
)


WprevSeSp(
  method = "binomial",
  x = example_data_WprevSeSp$x,
  n = example_data_WprevSeSp$n,
  w = example_data_WprevSeSp$w,
  cn = example_data_WprevSeSp$cn,
  mn = example_data_WprevSeSp$mn,
  cp = example_data_WprevSeSp$cp,
  mp = example_data_WprevSeSp$mp
)

WprevSeSp(
  method = "poisson",
  x = example_data_WprevSeSp$x,
  n = example_data_WprevSeSp$n,
  w = example_data_WprevSeSp$w,
  cn = example_data_WprevSeSp$cn,
  mn = example_data_WprevSeSp$mn,
  cp = example_data_WprevSeSp$cp,
  mp = example_data_WprevSeSp$mp
)

</code></pre>

<hr>
<h2 id='wspoissonTest'>
Test and Confidence Intervals on Weighted Sum of Poissons
</h2><span id='topic+wspoissonTest'></span>

<h3>Description</h3>

<p>The test is not as important as the confidence intervals, which are often used for directly standardized rates. The default uses the gamma method by fay and Feuer (1997), which by all simulations appears to retain nominal coverage for any set of parameters or weights. There is a mid-p-like version that is less conservative.    
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wspoissonTest(x, w, nullValue = NULL, 
   alternative = c("two.sided", "less", "greater"), 
   conf.level = 0.95, midp = FALSE, nmc = 0,  
   wmtype = c("max", "mean", "minmaxavg", "tcz"), 
   mult = 1, unirootTolFactor=10^(-6))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="wspoissonTest_+3A_x">x</code></td>
<td>

<p>a vector of counts (each assumed Poisson with a different parameter)
</p>
</td></tr>
<tr><td><code id="wspoissonTest_+3A_w">w</code></td>
<td>

<p>a vector of weights.
</p>
</td></tr>
<tr><td><code id="wspoissonTest_+3A_nullvalue">nullValue</code></td>
<td>

<p>a null hypothesis value of the weighted sum of the Poisson means, if NULL no test is done.
</p>
</td></tr>
<tr><td><code id="wspoissonTest_+3A_alternative">alternative</code></td>
<td>

<p>type of alternative hypothesis 
</p>
</td></tr>
<tr><td><code id="wspoissonTest_+3A_conf.level">conf.level</code></td>
<td>

<p>confidence level
</p>
</td></tr>
<tr><td><code id="wspoissonTest_+3A_midp">midp</code></td>
<td>

<p>logical, should the mid-p confidence distribution method be used</p>
</td></tr>
<tr><td><code id="wspoissonTest_+3A_nmc">nmc</code></td>
<td>

<p>Caculation method when midp=TRUE. If nmc=0 (default) does calculations that are very accurate using uniroot. If nmc&gt;0 does Monte Carlo simulations. The Monte Carlo simulations are not needed for general use.
</p>
</td></tr>
<tr><td><code id="wspoissonTest_+3A_wmtype">wmtype</code></td>
<td>

<p>type of modification for the gamma confidence interval, 'max' is the original gamma method that adds <code>max(w)</code> to <code>sum(x*w)</code> for the upper interval, 'mean' adds <code>mean(w)</code>, 'minmaxavg' adds <code>mean(c(min(w),max(w))</code>, 'tcz' does a modification of Tiwari, Clegg, and Zou (2006). 
</p>
</td></tr>
<tr><td><code id="wspoissonTest_+3A_mult">mult</code></td>
<td>

<p>a factor to multiply the estimate and confidence intervals by, to give rates per <code>mult</code>
</p>
</td></tr>
<tr><td><code id="wspoissonTest_+3A_uniroottolfactor">unirootTolFactor</code></td>
<td>

<p>tol factor used in uniroot for calculating when midp=TRUE and nmc=0. Value multiplies by a value close to the quantile of interest in confidence interval, so that if the standardized rates are very small (e.g., 0.00001 before using mult) then the uniroot tol will be unirootTolFactor times that.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fay and Feuer (1997) developed the gamma method (wmtype='max') for calculating confidence intervals on directly standardized rates. The assumptions is that the k by 1 vector of counts, x, are Poisson with an unknown k by 1 vector of means, theta. There are standardizing weights, w. We are interested in <code>sum(theta*w)</code>.
</p>
<p>For age-standardization, <code>x</code> is the vector of counts of the event for each of the k age groups. The weights are <code>n.standard/(n.x *sum(n.standard)</code>, where <code>n.x[i]</code> is the  person-years associated <code>x[i]</code> and <code>n.standard[i]</code> is  person-years fro the standard population associated with the ith  age group.
</p>
<p>Since the gamma method is conservative, Tiwari, Clegg, and Zou (2006) proposed a modification (wmtype='tcz') and also explored (wmtype='mean'). 
</p>
<p>Ng, Filardo, and Zheng (2008) studied these and other methods (for example, wmtype='minmaxavg') through extensive simulations. They showed that the gamma method (wmtype='max') was the only method that maintained at least nominal  coverage in all the simulations. But that method is conservative. 
</p>
<p>Fay and Kim (2017) proposed the mid-p gamma method. It appears less conservative, while appearing to retain the nominal coverage in almost all simulations. It is calculated by numeric calculations using uniroot. 
</p>


<h3>Value</h3>

<p>a list of class <code>htest</code>, containing:
</p>
<table role = "presentation">
<tr><td><code>statistic</code></td>
<td>
<p>k=length(x)</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>a vector with sample variance of the calibrated weights (so sum(w)=k), and mult (only if mult !=1)</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>p-value, set to NA if null.value=NULL</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>confidence interval on true directly standardized rate, sum(theta*w) </p>
</td></tr>
<tr><td><code>estimate</code></td>
<td>
<p> directly standardized rate, sum(x*w) </p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>null hypothesis value for true DSR</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>alternative hypothesis</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>description of method</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>desciption of data</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael P. Fay</p>


<h3>References</h3>

<p>Fay and Feuer (1997). &quot;Confidence intervals for directly standardized rates: a method based on the gamma distribution.&quot;
Statistics in Medicine. 16: 791-801.
</p>
<p>Fay and Kim (2017). &quot;Confidence intervals for directly standardized rates using mid-p gamma intervals.&quot; 
Biometrical Journal. 59(2): 377-387.
</p>
<p>Ng, Filardo, and Zheng (2008). &quot;Confidenc interval estimating procedures for standardized incidence rates.&quot; Computational Statistics and Data Analysis 52: 3501-3516.
</p>
<p>Tiwari, Clegg, and Zou (2006). &quot;Efficient interval estimation for age-adjusted cancer rates.&quot; Statistical Methods in Medical Research. 15: 547-569.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## birth data on Down's syndrome from Michigan, 1950-1964
## see Table II  of Fay and Feuer (1997)
##xfive= counts for mothers who have had 5 or more children
## nfive and ntotal are number of live births 
xfive&lt;-c(0,8,63,112,262,295)
nfive&lt;-c(327,30666,123419,149919,104088,34392)
ntotal&lt;-c(319933,931318,786511,488235,237863,61313)
## use mult =10^5 to give rates per 100,000
## gamma method of Fay and Feuer (1997) is default
wspoissonTest(xfive,ntotal/(nfive*sum(ntotal)),mult=10^5)
 
</code></pre>

<hr>
<h2 id='wsrTest'>
Exact Wilcoxon Signed Rank Test
</h2><span id='topic+wsrTest'></span>

<h3>Description</h3>

<p>Calculates the exact Wilcoxon signed rank test (using Pratt's method if there are zero values). Gives exact matching confidence intervals based on repeated calls to <code><a href="coin.html#topic+wilcoxsign_test">wilcoxsign_test</a></code>, and gives associated Hodges-Lehmann estimator of center of the symmetric distribution of the difference.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wsrTest(x, y = NULL, conf.int = TRUE, conf.level = 0.95, 
   mu = 0, alternative = c("two.sided", "less", "greater"),
   digits = NULL, tieDigits=8)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="wsrTest_+3A_x">x</code></td>
<td>

<p>numeric vector, either the difference (if y=NULL) or the first of the paired responses (so difference is x-y).
</p>
</td></tr>
<tr><td><code id="wsrTest_+3A_y">y</code></td>
<td>

<p>second of paired differences. If NULL assumes x is the vector of paired differences.
</p>
</td></tr>
<tr><td><code id="wsrTest_+3A_conf.int">conf.int</code></td>
<td>

<p>logica, calculate confidence interval on median of differences
</p>
</td></tr>
<tr><td><code id="wsrTest_+3A_conf.level">conf.level</code></td>
<td>

<p>confidence level
</p>
</td></tr>
<tr><td><code id="wsrTest_+3A_mu">mu</code></td>
<td>

<p>null median difference
</p>
</td></tr>
<tr><td><code id="wsrTest_+3A_alternative">alternative</code></td>
<td>

<p>alternative hypothesis
</p>
</td></tr>
<tr><td><code id="wsrTest_+3A_digits">digits</code></td>
<td>

<p>number of digits for accuracy of confidence intervals, results are accurate to round(ci,cidigits). If digits=NULL picks about 4 digits if the range of the differences is 0 to 1, with similar accuracy as the range changes (see details). 
</p>
</td></tr>
<tr><td><code id="wsrTest_+3A_tiedigits">tieDigits</code></td>
<td>

<p>number of digits to round x and y, values closer than that number of digits are treated as tied. This is to avoid rankings based on computer error.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Wilcoxon signed rank test tests the null hypothesis of 
whether a set of values (x values, if y=NULL)
or differences (x-y, if y!=NULL) are symmetric about <code>mu</code>. 
</p>
<p>This function calculates the exact Wilcoxon signed rank test using the Pratt method if there are zeros. In other words, rank the differences equal to zero together with the absolute value of the differences, but then permute the signs of only the non-zero ranks. The p-values are calculated using <code><a href="coin.html#topic+wilcoxsign_test">wilcoxsign_test</a></code>, this function is just a wrapper to get confidence intervals.
</p>
<p>When <code>conf.int=TRUE</code>, we get an estimator of the center of the symmetric distribution of the differences
based on the shift value where the one-sided p-values are equal (or the middle of the range if there are many values where they are equal). This type of estimator is called a Hodges-Lehmann estimator (see for example, Hodges and Lehmann, 1983).
The upper confidence limit when alternative='less' is  the
smallest shift value that gives a one-sided (alternative='less') p-value that is  
less than alpha=1-conf.level. Analogously, the lower confidence limit when alternative='greater' is the largest shift value that gives a one-sided (alternative='greater') p-value that is less than alpha. When alternative='two.sided' 
the confidence interval is the union of the two one-sided intervals 
each with level 1-alpha/2 (where alpha=1-conf.level).
Under the symmetry assumption, the center of a symmetric distribution is its median, pseudo-median, and mean.
</p>


<h3>Value</h3>

<p>An object of class 'htest', list with elements:
</p>
<table role = "presentation">
<tr><td><code>estimate</code></td>
<td>
<p>estimator of median difference</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>p.value associated with alternative</p>
</td></tr>
<tr><td><code>conf.int</code></td>
<td>
<p>confidence interval</p>
</td></tr>
<tr><td><code>null.value</code></td>
<td>
<p>null median difference</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>alternative</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>description of method</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The estimator and confidence interval here are different than the ones used in <code><a href="stats.html#topic+wilcox.test">wilcox.test</a></code>
(with paired=TRUE and exact=TRUE).
</p>


<h3>Author(s)</h3>

<p>Michael P. Fay
</p>


<h3>References</h3>

<p>Pratt, JW (1959). Remarks on zeros and ties in the Wilcoxon signed rank procedures. JASA 54(287) 655-667.
</p>
<p>Hodges, JL, and Lehmann, EL (1983). Hodges-Lehmann Estimators.
In Encyclopedian of Statistics, Volume 3. Editors S. Kotz and NL Johnson. Wiley: New York.
</p>


<h3>See Also</h3>

<p><code><a href="coin.html#topic+wilcoxsign_test">wilcoxsign_test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>wsrTest((-3:8))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
