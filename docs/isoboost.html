<!DOCTYPE html><html><head><title>Help for package isoboost</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {isoboost}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#isoboost-package'>
<p>Isotonic Boosting Classification Rules</p></a></li>
<li><a href='#amilb'><p>(Adjacent-categories) Multiple Isotonic LogitBoost</p></a></li>
<li><a href='#asilb'><p>(Adjacent-categories) Simple Isotonic LogitBoost</p></a></li>
<li><a href='#cmilb'><p>Cumulative probabilities Multiple Isotonic LogitBoost</p></a></li>
<li><a href='#csilb'><p>Cumulative probabilities Simple Isotonic LogitBoost</p></a></li>
<li><a href='#isoboost-internal'><p>Internal isoboost functions</p></a></li>
<li><a href='#motors'><p>Diagnostic of electrical induction motors</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Isotonic Boosting Classification Rules</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-05-01</td>
</tr>
<tr>
<td>Description:</td>
<td>In classification problems a monotone relation between some
  predictors and the classes may be assumed. In this package 'isoboost' 
  we propose new boosting algorithms, based on LogitBoost, that 
  incorporate this isotonicity information, yielding more accurate 
  and easily interpretable rules.</td>
</tr>
<tr>
<td>Imports:</td>
<td>Iso, isotone, rpart</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-05-01 13:27:57 UTC; davidconde</td>
</tr>
<tr>
<td>Author:</td>
<td>David Conde [aut, cre],
  Miguel A. Fernandez [aut],
  Cristina Rueda [aut],
  Bonifacio Salvador [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>David Conde &lt;dconde@eio.uva.es&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-05-01 15:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='isoboost-package'>
Isotonic Boosting Classification Rules
</h2><span id='topic+isoboost-package'></span><span id='topic+isoboost'></span>

<h3>Description</h3>

<p>In this package we present new boosting classification rules based on LogitBoost when it can be assumed that higher (or lower) values of some predictors are related to higher levels of the response.
</p>


<h3>Details</h3>

<p>Package: isoboost
</p>
<p>Type: Package
</p>
<p>Version: 1.0.1
</p>
<p>Date: 2021-05-01
</p>
<p>License: GPL-2 | GPL-3
</p>
<p>For a complete list of functions with individual help pages, use <code>library(help = "isoboost")</code>.
</p>


<h3>Author(s)</h3>

<p>David Conde, Miguel A. Fernandez, Cristina Rueda, Bonifacio Salvador
</p>
<p>Maintainer: David Conde &lt;dconde@eio.uva.es&gt;
</p>

<hr>
<h2 id='amilb'>(Adjacent-categories) Multiple Isotonic LogitBoost</h2><span id='topic+amilb'></span><span id='topic+amilb.formula'></span><span id='topic+amilb.default'></span><span id='topic+amilb.data.frame'></span><span id='topic+amilb.matrix'></span>

<h3>Description</h3>

<p>Train and predict logitboost-based classification algorithm using multivariate isotonic regression (linear regression for no monotone features) as weak learners, based on the adjacent-categories logistic model (see Agresti (2010)). For full details on this algorithm, see Conde et al. (2020).</p>


<h3>Usage</h3>

<pre><code class='language-R'>amilb(xlearn, ...)

## S3 method for class 'formula'
amilb(formula, data, ...)

## Default S3 method:
amilb(xlearn, ylearn, xtest = xlearn, mfinal = 100, 
monotone_constraints = rep(0, dim(xlearn)[2]), prior = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="amilb_+3A_formula">formula</code></td>
<td>
<p> A formula of the form <code>groups ~ x1 + x2 + ...</code>. That is, the response is the class variable and the right hand side specifies the explanatory variables. </p>
</td></tr>
<tr><td><code id="amilb_+3A_data">data</code></td>
<td>
<p> Data frame from which variables specified in <code>formula</code> are to be taken. </p>
</td></tr>
<tr><td><code id="amilb_+3A_xlearn">xlearn</code></td>
<td>
<p> (Required if no formula is given as the principal argument.) A data frame or matrix containing the explanatory variables. </p>
</td></tr>
<tr><td><code id="amilb_+3A_ylearn">ylearn</code></td>
<td>
<p> (Required if no formula is given as the principal argument.) A numeric vector or factor with numeric levels specifying the class for each observation. </p>
</td></tr>
<tr><td><code id="amilb_+3A_xtest">xtest</code></td>
<td>
<p> A data frame or matrix of cases to be classified, containing the features used in <code>formula</code> or <code>xlearn</code>. </p>
</td></tr>
<tr><td><code id="amilb_+3A_mfinal">mfinal</code></td>
<td>
<p> Maximum number of iterations of the algorithm. </p>
</td></tr>
<tr><td><code id="amilb_+3A_monotone_constraints">monotone_constraints</code></td>
<td>
<p> Numerical vector consisting of 1, 0 and -1, its length equals the number of features in <code>xlearn</code>. 1 is increasing, -1 is decreasing and 0 is no constraint. </p>
</td></tr>
<tr><td><code id="amilb_+3A_prior">prior</code></td>
<td>
<p> The prior probabilities of class membership. If unspecified, equal prior probabilities are used. If present, the probabilities must be specified in the order of the factor levels. </p>
</td></tr>
<tr><td><code id="amilb_+3A_...">...</code></td>
<td>
<p> Arguments passed to or from other methods. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following components:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p> The (matched) function call. </p>
</td></tr>
<tr><td><code>trainset</code></td>
<td>
<p> Matrix with the training set used (first columns) and the class for each observation (last column). </p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p> Prior probabilities of class membership used. </p>
</td></tr>
<tr><td><code>apparent</code></td>
<td>
<p> Apparent error rate. </p>
</td></tr>
<tr><td><code>mfinal</code></td>
<td>
<p> Number of iterations of the algorithm. </p>
</td></tr>
<tr><td><code>loglikelihood</code></td>
<td>
<p> Log-likelihood. </p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p> Posterior probabilities of class membership for <code>xtest</code> set. </p>
</td></tr>
<tr><td><code>class</code></td>
<td>
<p> Labels of the class with maximal probability for <code>xtest</code> set. </p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function may be called using either a formula and data frame, or a data frame and grouping variable, or a matrix and grouping variable as the first two arguments. All other arguments are optional.
</p>
<p>Classes must be identified, either in a column of <code>data</code> or in the <code>ylearn</code> vector, by natural numbers varying from 1 to the number of classes. The number of classes must be greater than 1.
</p>
<p>If there are missing values in either <code>data</code>, <code>xlearn</code> or <code>ylearn</code>, corresponding observations will be deleted.
</p>


<h3>Author(s)</h3>

<p>David Conde
</p>


<h3>References</h3>

<p>Agresti, A. (2010). Analysis of Ordinal Categorical Data, 2nd edition. John Wiley and Sons. New Jersey.
</p>
<p>Conde, D., Fernandez, M. A., Rueda, C., and Salvador, B. (2020). Isotonic boosting classification rules. <em>Advances in Data Analysis and Classification</em>, 1-25.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+asilb">asilb</a></code>, <code><a href="#topic+csilb">csilb</a></code>, <code><a href="#topic+cmilb">cmilb</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(motors)
table(motors$condition)
##  1  2  3  4 
## 83 67 70 60 

## Let us consider the first three variables as predictors
data &lt;- motors[, 1:3]
grouping = motors$condition
## 
## Lower values of the amplitudes are expected to be 
## related to higher levels of damage severity, so 
## we can consider the following monotone constraints
monotone_constraints = rep(-1, 3)

set.seed(7964)
values &lt;- runif(dim(data)[1])
trainsubset &lt;- values &lt; 0.2
obj &lt;- amilb(data[trainsubset, ], grouping[trainsubset], 
               data[-trainsubset, ], 100, monotone_constraints)

## Apparent error
obj$apparent
## 4.761905

## Error rate
100*mean(obj$class != grouping[-trainsubset])
## 15.41219

</code></pre>

<hr>
<h2 id='asilb'>(Adjacent-categories) Simple Isotonic LogitBoost</h2><span id='topic+asilb'></span><span id='topic+asilb.formula'></span><span id='topic+asilb.default'></span><span id='topic+asilb.data.frame'></span><span id='topic+asilb.matrix'></span>

<h3>Description</h3>

<p>Train and predict logitboost-based classification algorithm using isotonic regression (decision stumps for no monotone features) as weak learners, based on the adjacent-categories logistic model (see Agresti (2010)). For full details on this algorithm, see Conde et al. (2020).</p>


<h3>Usage</h3>

<pre><code class='language-R'>asilb(xlearn, ...)

## S3 method for class 'formula'
asilb(formula, data, ...)

## Default S3 method:
asilb(xlearn, ylearn, xtest = xlearn, mfinal = 100, 
monotone_constraints = rep(0, dim(xlearn)[2]), prior = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="asilb_+3A_formula">formula</code></td>
<td>
<p> A formula of the form <code>groups ~ x1 + x2 + ...</code>. That is, the response is the class variable and the right hand side specifies the explanatory variables. </p>
</td></tr>
<tr><td><code id="asilb_+3A_data">data</code></td>
<td>
<p> Data frame from which variables specified in <code>formula</code> are to be taken. </p>
</td></tr>
<tr><td><code id="asilb_+3A_xlearn">xlearn</code></td>
<td>
<p> (Required if no formula is given as the principal argument.) A data frame or matrix containing the explanatory variables. </p>
</td></tr>
<tr><td><code id="asilb_+3A_ylearn">ylearn</code></td>
<td>
<p> (Required if no formula is given as the principal argument.) A numeric vector or factor with numeric levels specifying the class for each observation. </p>
</td></tr>
<tr><td><code id="asilb_+3A_xtest">xtest</code></td>
<td>
<p> A data frame or matrix of cases to be classified, containing the features used in <code>formula</code> or <code>xlearn</code>. </p>
</td></tr>
<tr><td><code id="asilb_+3A_mfinal">mfinal</code></td>
<td>
<p> Number of iterations of the algorithm. </p>
</td></tr>
<tr><td><code id="asilb_+3A_monotone_constraints">monotone_constraints</code></td>
<td>
<p> Numerical vector consisting of 1, 0 and -1, its length equals the number of features in <code>xlearn</code>. 1 is increasing, -1 is decreasing and 0 is no constraint. </p>
</td></tr>
<tr><td><code id="asilb_+3A_prior">prior</code></td>
<td>
<p> The prior probabilities of class membership. If unspecified, equal prior probabilities are used. If present, the probabilities must be specified in the order of the factor levels. </p>
</td></tr>
<tr><td><code id="asilb_+3A_...">...</code></td>
<td>
<p> Arguments passed to or from other methods. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following components:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p> The (matched) function call. </p>
</td></tr>
<tr><td><code>trainset</code></td>
<td>
<p> Matrix with the training set used (first columns) and the class for each observation (last column). </p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p> Prior probabilities of class membership used. </p>
</td></tr>
<tr><td><code>apparent</code></td>
<td>
<p> Apparent error rate. </p>
</td></tr>
<tr><td><code>mfinal</code></td>
<td>
<p> Number of iterations of the algorithm. </p>
</td></tr>
<tr><td><code>loglikelihood</code></td>
<td>
<p> Log-likelihood. </p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p> Posterior probabilities of class membership for <code>xtest</code> set. </p>
</td></tr>
<tr><td><code>class</code></td>
<td>
<p> Labels of the class with maximal probability for <code>xtest</code> set. </p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function may be called using either a formula and data frame, or a data frame and grouping variable, or a matrix and grouping variable as the first two arguments. All other arguments are optional.
</p>
<p>Classes must be identified, either in a column of <code>data</code> or in the <code>ylearn</code> vector, by natural numbers varying from 1 to the number of classes. The number of classes must be greater than 1.
</p>
<p>If there are missing values in either <code>data</code>, <code>xlearn</code> or <code>ylearn</code>, corresponding observations will be deleted.
</p>


<h3>Author(s)</h3>

<p>David Conde
</p>


<h3>References</h3>

<p>Agresti, A. (2010). Analysis of Ordinal Categorical Data, 2nd edition. John Wiley and Sons. New Jersey.
</p>
<p>Conde, D., Fernandez, M. A., Rueda, C., and Salvador, B. (2020). Isotonic boosting classification rules. <em>Advances in Data Analysis and Classification</em>, 1-25.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+amilb">amilb</a></code>, <code><a href="#topic+csilb">csilb</a></code>, <code><a href="#topic+cmilb">cmilb</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(motors)
table(motors$condition)
##  1  2  3  4 
## 83 67 70 60 

## Let us consider the first three variables as predictors
data &lt;- motors[, 1:3]
grouping = motors$condition
## 
## Lower values of the amplitudes are expected to be 
## related to higher levels of damage severity, so 
## we can consider the following monotone constraints
monotone_constraints = rep(-1, 3)

set.seed(7964)
values &lt;- runif(dim(data)[1])
trainsubset &lt;- values &lt; 0.2
obj &lt;- asilb(data[trainsubset, ], grouping[trainsubset], 
               data[-trainsubset, ], 50, monotone_constraints)

## Apparent error
obj$apparent
## 4.761905

## Error rate
100*mean(obj$class != grouping[-trainsubset])
## 14.69534
</code></pre>

<hr>
<h2 id='cmilb'>Cumulative probabilities Multiple Isotonic LogitBoost</h2><span id='topic+cmilb'></span><span id='topic+cmilb.formula'></span><span id='topic+cmilb.default'></span><span id='topic+cmilb.data.frame'></span><span id='topic+cmilb.matrix'></span>

<h3>Description</h3>

<p>Train and predict logitboost-based classification algorithm using multivariate isotonic regression (linear regression for no monotone features) as weak learners, based on the cumulative probabilities logistic model (see Agresti (2010)). For full details on this algorithm, see Conde et al. (2020).</p>


<h3>Usage</h3>

<pre><code class='language-R'>cmilb(xlearn, ...)

## S3 method for class 'formula'
cmilb(formula, data, ...)

## Default S3 method:
cmilb(xlearn, ylearn, xtest = xlearn, mfinal = 100, 
monotone_constraints = rep(0, dim(xlearn)[2]), prior = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cmilb_+3A_formula">formula</code></td>
<td>
<p> A formula of the form <code>groups ~ x1 + x2 + ...</code>. That is, the response is the class variable and the right hand side specifies the explanatory variables. </p>
</td></tr>
<tr><td><code id="cmilb_+3A_data">data</code></td>
<td>
<p> Data frame from which variables specified in <code>formula</code> are to be taken. </p>
</td></tr>
<tr><td><code id="cmilb_+3A_xlearn">xlearn</code></td>
<td>
<p> (Required if no formula is given as the principal argument.) A data frame or matrix containing the explanatory variables. </p>
</td></tr>
<tr><td><code id="cmilb_+3A_ylearn">ylearn</code></td>
<td>
<p> (Required if no formula is given as the principal argument.) A numeric vector or factor with numeric levels specifying the class for each observation. </p>
</td></tr>
<tr><td><code id="cmilb_+3A_xtest">xtest</code></td>
<td>
<p> A data frame or matrix of cases to be classified, containing the features used in <code>formula</code> or <code>xlearn</code>. </p>
</td></tr>
<tr><td><code id="cmilb_+3A_mfinal">mfinal</code></td>
<td>
<p> Maximum number of iterations of the algorithm. </p>
</td></tr>
<tr><td><code id="cmilb_+3A_monotone_constraints">monotone_constraints</code></td>
<td>
<p> Numerical vector consisting of 1, 0 and -1, its length equals the number of features in <code>xlearn</code>. 1 is increasing, -1 is decreasing and 0 is no constraint. </p>
</td></tr>
<tr><td><code id="cmilb_+3A_prior">prior</code></td>
<td>
<p> The prior probabilities of class membership. If unspecified, equal prior probabilities are used. If present, the probabilities must be specified in the order of the factor levels. </p>
</td></tr>
<tr><td><code id="cmilb_+3A_...">...</code></td>
<td>
<p> Arguments passed to or from other methods. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following components:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p> The (matched) function call. </p>
</td></tr>
<tr><td><code>trainset</code></td>
<td>
<p> Matrix with the training set used (first columns) and the class for each observation (last column). </p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p> Prior probabilities of class membership used. </p>
</td></tr>
<tr><td><code>apparent</code></td>
<td>
<p> Apparent error rate. </p>
</td></tr>
<tr><td><code>mfinal</code></td>
<td>
<p> Number of iterations of the algorithm. </p>
</td></tr>
<tr><td><code>loglikelihood</code></td>
<td>
<p> Log-likelihood. </p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p> Posterior probabilities of class membership for <code>xtest</code> set. </p>
</td></tr>
<tr><td><code>class</code></td>
<td>
<p> Labels of the class with maximal probability for <code>xtest</code> set. </p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function may be called using either a formula and data frame, or a data frame and grouping variable, or a matrix and grouping variable as the first two arguments. All other arguments are optional.
</p>
<p>Classes must be identified, either in a column of <code>data</code> or in the <code>ylearn</code> vector, by natural numbers varying from 1 to the number of classes. The number of classes must be greater than 1.
</p>
<p>If there are missing values in either <code>data</code>, <code>xlearn</code> or <code>ylearn</code>, corresponding observations will be deleted.
</p>


<h3>Author(s)</h3>

<p>David Conde
</p>


<h3>References</h3>

<p>Agresti, A. (2010). Analysis of Ordinal Categorical Data, 2nd edition. John Wiley and Sons. New Jersey.
</p>
<p>Conde, D., Fernandez, M. A., Rueda, C., and Salvador, B. (2020). Isotonic boosting classification rules. <em>Advances in Data Analysis and Classification</em>, 1-25.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+asilb">asilb</a></code>, <code><a href="#topic+amilb">amilb</a></code>, <code><a href="#topic+csilb">csilb</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(motors)
table(motors$condition)
##  1  2  3  4 
## 83 67 70 60 

## Let us consider the first three variables as predictors
data &lt;- motors[, 1:3]
grouping = motors$condition
## 
## Lower values of the amplitudes are expected to be 
## related to higher levels of damage severity, so 
## we can consider the following monotone constraints
monotone_constraints = rep(-1, 3)

set.seed(7964)
values &lt;- runif(dim(data)[1])
trainsubset &lt;- values &lt; 0.2
obj &lt;- cmilb(data[trainsubset, ], grouping[trainsubset], 
               data[-trainsubset, ], 20, monotone_constraints)

## Apparent error
obj$apparent
## 4.761905

## Error rate
100*mean(obj$class != grouping[-trainsubset])
## 15.77061
</code></pre>

<hr>
<h2 id='csilb'>Cumulative probabilities Simple Isotonic LogitBoost</h2><span id='topic+csilb'></span><span id='topic+csilb.formula'></span><span id='topic+csilb.default'></span><span id='topic+csilb.data.frame'></span><span id='topic+csilb.matrix'></span>

<h3>Description</h3>

<p>Train and predict logitboost-based classification algorithm using isotonic regression (decision stumps for no monotone features) as weak learners, based on the cumulative probabilities logistic model (see Agresti (2010)). For full details on this algorithm, see Conde et al. (2020).</p>


<h3>Usage</h3>

<pre><code class='language-R'>csilb(xlearn, ...)

## S3 method for class 'formula'
csilb(formula, data, ...)

## Default S3 method:
csilb(xlearn, ylearn, xtest = xlearn, mfinal = 100, 
monotone_constraints = rep(0, dim(xlearn)[2]), prior = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="csilb_+3A_formula">formula</code></td>
<td>
<p> A formula of the form <code>groups ~ x1 + x2 + ...</code>. That is, the response is the class variable and the right hand side specifies the explanatory variables. </p>
</td></tr>
<tr><td><code id="csilb_+3A_data">data</code></td>
<td>
<p> Data frame from which variables specified in <code>formula</code> are to be taken. </p>
</td></tr>
<tr><td><code id="csilb_+3A_xlearn">xlearn</code></td>
<td>
<p> (Required if no formula is given as the principal argument.) A data frame or matrix containing the explanatory variables. </p>
</td></tr>
<tr><td><code id="csilb_+3A_ylearn">ylearn</code></td>
<td>
<p> (Required if no formula is given as the principal argument.) A numeric vector or factor with numeric levels specifying the class for each observation. </p>
</td></tr>
<tr><td><code id="csilb_+3A_xtest">xtest</code></td>
<td>
<p> A data frame or matrix of cases to be classified, containing the features used in <code>formula</code> or <code>xlearn</code>. </p>
</td></tr>
<tr><td><code id="csilb_+3A_mfinal">mfinal</code></td>
<td>
<p> Number of iterations of the algorithm. </p>
</td></tr>
<tr><td><code id="csilb_+3A_monotone_constraints">monotone_constraints</code></td>
<td>
<p> Numerical vector consisting of 1, 0 and -1, its length equals the number of features in <code>xlearn</code>. 1 is increasing, -1 is decreasing and 0 is no constraint. </p>
</td></tr>
<tr><td><code id="csilb_+3A_prior">prior</code></td>
<td>
<p> The prior probabilities of class membership. If unspecified, equal prior probabilities are used. If present, the probabilities must be specified in the order of the factor levels. </p>
</td></tr>
<tr><td><code id="csilb_+3A_...">...</code></td>
<td>
<p> Arguments passed to or from other methods. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following components:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p> The (matched) function call. </p>
</td></tr>
<tr><td><code>trainset</code></td>
<td>
<p> Matrix with the training set used (first columns) and the class for each observation (last column). </p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p> Prior probabilities of class membership used. </p>
</td></tr>
<tr><td><code>apparent</code></td>
<td>
<p> Apparent error rate. </p>
</td></tr>
<tr><td><code>mfinal</code></td>
<td>
<p> Number of iterations of the algorithm. </p>
</td></tr>
<tr><td><code>loglikelihood</code></td>
<td>
<p> Log-likelihood. </p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p> Posterior probabilities of class membership for <code>xtest</code> set. </p>
</td></tr>
<tr><td><code>class</code></td>
<td>
<p> Labels of the class with maximal probability for <code>xtest</code> set. </p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function may be called using either a formula and data frame, or a data frame and grouping variable, or a matrix and grouping variable as the first two arguments. All other arguments are optional.
</p>
<p>Classes must be identified, either in a column of <code>data</code> or in the <code>ylearn</code> vector, by natural numbers varying from 1 to the number of classes. The number of classes must be greater than 1.
</p>
<p>If there are missing values in either <code>data</code>, <code>xlearn</code> or <code>ylearn</code>, corresponding observations will be deleted.
</p>


<h3>Author(s)</h3>

<p>David Conde
</p>


<h3>References</h3>

<p>Agresti, A. (2010). Analysis of Ordinal Categorical Data, 2nd edition. John Wiley and Sons. New Jersey.
</p>
<p>Conde, D., Fernandez, M. A., Rueda, C., and Salvador, B. (2020). Isotonic boosting classification rules. <em>Advances in Data Analysis and Classification</em>, 1-25.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+asilb">asilb</a></code>, <code><a href="#topic+amilb">amilb</a></code>, <code><a href="#topic+cmilb">cmilb</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(motors)
table(motors$condition)
##  1  2  3  4 
## 83 67 70 60 

## Let us consider the first three variables as predictors
data &lt;- motors[, 1:3]
grouping = motors$condition
## 
## Lower values of the amplitudes are expected to be 
## related to higher levels of damage severity, so 
## we can consider the following monotone constraints
monotone_constraints = rep(-1, 3)

set.seed(7964)
values &lt;- runif(dim(data)[1])
trainsubset &lt;- values &lt; 0.2
obj &lt;- csilb(data[trainsubset, ], grouping[trainsubset], 
               data[-trainsubset, ], 100, monotone_constraints)

## Apparent error
obj$apparent
## 4.761905

## Error rate
100*mean(obj$class != grouping[-trainsubset])
## 17.92115
</code></pre>

<hr>
<h2 id='isoboost-internal'>Internal isoboost functions</h2><span id='topic+checks'></span><span id='topic+estimate'></span><span id='topic+pavap'></span>

<h3>Description</h3>

<p>Internal isoboost functions
</p>


<h3>Details</h3>

<p>These are not to be called by the user.
</p>

<hr>
<h2 id='motors'>Diagnostic of electrical induction motors</h2><span id='topic+motors'></span>

<h3>Description</h3>

<p>Electrical induction motors are widely used in industry. In the industrual context, the early detection of possible damage in the motor is very important since failures can result in financial losses. Motor Current Signature Analysis is the most widespread technique to diagnose a faulty motor, see Choudhary et al. (2019). This technique is based on the spectral analysis of the stator current: motor faults cause an asymmetry that reflects as additional harmonics in the current spectrum, so side bands around the main frequency are considered and amplitudes of these side bands around odd harmonics are measured. 
</p>
<p>The data were generated by Oscar Duque and Daniel Morinigo at the Electrical Engineering laboratory of the Universidad de Valladolid.
</p>
<p>Four condition states of damage severity are considered: 1 - undamaged, 2 - incipient fault, 3 - moderate damage, 4 - severe damage.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(motors)</code></pre>


<h3>Format</h3>

<p>A data frame with 280 observations on 7 variables, six are numerical and one nominal defining the condition state of the motors.
</p>

<table>
<tr>
 <td style="text-align: right;"> 
  [,1] </td><td style="text-align: left;"> amplitude_l.1 </td><td style="text-align: left;"> Amplitude of the first lower side band around harmonic 1 </td>
</tr>
<tr>
 <td style="text-align: right;">
  [,2] </td><td style="text-align: left;"> amplitude_u.1 </td><td style="text-align: left;"> Amplitude of the first upper side band around harmonic 1 </td>
</tr>
<tr>
 <td style="text-align: right;">
  [,3] </td><td style="text-align: left;"> amplitude_l.5 </td><td style="text-align: left;"> Amplitude of the first lower side band around harmonic 5 </td>
</tr>
<tr>
 <td style="text-align: right;">
  [,4] </td><td style="text-align: left;"> amplitude_u.5 </td><td style="text-align: left;"> Amplitude of the first upper side band around harmonic 5 </td>
</tr>
<tr>
 <td style="text-align: right;">
  [,5] </td><td style="text-align: left;"> amplitude_l.7 </td><td style="text-align: left;"> Amplitude of the first lower side band around harmonic 7 </td>
</tr>
<tr>
 <td style="text-align: right;">
  [,6] </td><td style="text-align: left;"> amplitude_u.7 </td><td style="text-align: left;"> Amplitude of the first upper side band around harmonic 7 </td>
</tr>
<tr>
 <td style="text-align: right;">
  [,7] </td><td style="text-align: left;"> condition     </td><td style="text-align: left;"> Condition state </td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>



<h3>Source</h3>


<ul>
<li><p> Creator: Oscar Duque and Daniel Morinigo, Electrical Engineering Department laboratory, Universidad de Valladolid, Valladolid, Spain.
</p>
</li></ul>



<h3>References</h3>

<p>Choudhary, A. &amp; Goyal, D. &amp; Shimi, S. L. &amp; Akula, A. (2019). Condition monitoring and fault diagnosis of induction motors: A review. Archives of Computational Methods in Engineering. In press. doi:10.1007/s11831-018-9286-z.
</p>
<p>Garcia-Escudero, L. A., Duque-Perez, O., Fernandez-Temprano, M., Morinigo-Sotelo, D. (2016). Robust Detection of Incipient Faults in VSI-Fed Induction Motors Using Quality Control Charts. IEEE Transactions on Industry Applications, 53(3), 3076-3085.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(motors)
summary(motors)</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
