<!DOCTYPE html><html lang="en"><head><title>Help for package doc2vec</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {doc2vec}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#as.matrix.paragraph2vec'><p>Get the document or word vectors of a paragraph2vec model</p></a></li>
<li><a href='#be_parliament_2020'><p>Corpus with Questions asked in the Belgium Federal Parliament in 2020</p></a></li>
<li><a href='#paragraph2vec'><p>Train a paragraph2vec also known as doc2vec model on text</p></a></li>
<li><a href='#paragraph2vec_similarity'><p>Similarity between document / word vectors as used in paragraph2vec</p></a></li>
<li><a href='#predict.paragraph2vec'><p>Predict functionalities for a paragraph2vec model</p></a></li>
<li><a href='#read.paragraph2vec'><p>Read a binary paragraph2vec model from disk</p></a></li>
<li><a href='#summary.top2vec'><p>Get summary information of a top2vec model</p></a></li>
<li><a href='#top2vec'><p>Distributed Representations of Topics</p></a></li>
<li><a href='#txt_count_words'><p>Count the number of spaces occurring in text</p></a></li>
<li><a href='#update.top2vec'><p>Update a Top2vec model</p></a></li>
<li><a href='#write.paragraph2vec'><p>Save a paragraph2vec model to disk</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Distributed Representations of Sentences, Documents and Topics</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jan Wijffels &lt;jwijffels@bnosac.be&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Learn vector representations of sentences, paragraphs or documents by using the 'Paragraph Vector' algorithms,
    namely the distributed bag of words ('PV-DBOW') and the distributed memory ('PV-DM') model. 
    The techniques in the package are detailed in the paper "Distributed Representations of Sentences and Documents" by Mikolov et al. (2014), available at &lt;<a href="https://doi.org/10.48550/arXiv.1405.4053">doi:10.48550/arXiv.1405.4053</a>&gt;.
    The package also provides an implementation to cluster documents based on these embedding using a technique called top2vec. 
    Top2vec finds clusters in text documents by combining techniques to embed documents and words and density-based clustering.
    It does this by embedding documents in the semantic space as defined by the 'doc2vec' algorithm. Next it maps
    these document embeddings to a lower-dimensional space using the 'Uniform Manifold Approximation and Projection' (UMAP) clustering algorithm 
    and finds dense areas in that space using a 'Hierarchical Density-Based Clustering' technique (HDBSCAN). These dense
    areas are the topic clusters which can be represented by the corresponding topic vector which is an aggregate of the 
    document embeddings of the documents which are part of that topic cluster. In the same semantic space similar words can 
    be found which are representative of the topic.
    More details can be found in the paper 'Top2Vec: Distributed Representations of Topics' by D. Angelov available at &lt;<a href="https://doi.org/10.48550/arXiv.2008.09470">doi:10.48550/arXiv.2008.09470</a>&gt;. </td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/bnosac/doc2vec">https://github.com/bnosac/doc2vec</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.11.5), stats, utils</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>tokenizers.bpe, word2vec (&ge; 0.3.3), uwot, dbscan, udpipe (&ge;
0.8)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-03-25 17:04:41 UTC; Jan</td>
</tr>
<tr>
<td>Author:</td>
<td>Jan Wijffels [aut, cre, cph] (R wrapper),
  BNOSAC [cph] (R wrapper),
  hiyijian [ctb, cph] (Code in src/doc2vec)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-03-28 00:00:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='as.matrix.paragraph2vec'>Get the document or word vectors of a paragraph2vec model</h2><span id='topic+as.matrix.paragraph2vec'></span>

<h3>Description</h3>

<p>Get the document or word vectors of a paragraph2vec model as a dense matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'paragraph2vec'
as.matrix(
  x,
  which = c("docs", "words"),
  normalize = TRUE,
  encoding = "UTF-8",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as.matrix.paragraph2vec_+3A_x">x</code></td>
<td>
<p>a paragraph2vec model as returned by <code><a href="#topic+paragraph2vec">paragraph2vec</a></code> or <code><a href="#topic+read.paragraph2vec">read.paragraph2vec</a></code></p>
</td></tr>
<tr><td><code id="as.matrix.paragraph2vec_+3A_which">which</code></td>
<td>
<p>either one of 'docs' or 'words'</p>
</td></tr>
<tr><td><code id="as.matrix.paragraph2vec_+3A_normalize">normalize</code></td>
<td>
<p>logical indicating to normalize the embeddings. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="as.matrix.paragraph2vec_+3A_encoding">encoding</code></td>
<td>
<p>set the encoding of the row names to the specified encoding. Defaults to 'UTF-8'.</p>
</td></tr>
<tr><td><code id="as.matrix.paragraph2vec_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix with the document or word vectors where the rownames are the documents or words upon which the model was trained
</p>


<h3>See Also</h3>

<p><code><a href="#topic+paragraph2vec">paragraph2vec</a></code>, <code><a href="#topic+read.paragraph2vec">read.paragraph2vec</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(tokenizers.bpe)
data(belgium_parliament, package = "tokenizers.bpe")
x &lt;- subset(belgium_parliament, language %in% "french")
x &lt;- subset(x, nchar(text) &gt; 0 &amp; txt_count_words(text) &lt; 1000)

model &lt;- paragraph2vec(x = x, type = "PV-DM",   dim = 15,  iter = 5)

model &lt;- paragraph2vec(x = x, type = "PV-DBOW", dim = 100, iter = 20)


embedding &lt;- as.matrix(model, which = "docs")
embedding &lt;- as.matrix(model, which = "words")
embedding &lt;- as.matrix(model, which = "docs", normalize = FALSE)
embedding &lt;- as.matrix(model, which = "words", normalize = FALSE)

</code></pre>

<hr>
<h2 id='be_parliament_2020'>Corpus with Questions asked in the Belgium Federal Parliament in 2020</h2><span id='topic+be_parliament_2020'></span>

<h3>Description</h3>

<p>The dataset was extracted from http://data.dekamer.be and contains questions asked by members in the Belgium Federal parliament in 2020. <br />
The Questions are in Dutch and French and contains 6059 text fragments. <br />
</p>
<p>The dataset contains the following information:
</p>

<ul>
<li><p> doc_id: an identifier
</p>
</li>
<li><p> text_nl: the question itself in Dutch
</p>
</li>
<li><p> text_fr: the question itself in French
</p>
</li></ul>



<h3>Source</h3>

<p>data is provided by http://www.dekamer.be in the public domain (CC0).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(be_parliament_2020)
str(be_parliament_2020)
</code></pre>

<hr>
<h2 id='paragraph2vec'>Train a paragraph2vec also known as doc2vec model on text</h2><span id='topic+paragraph2vec'></span>

<h3>Description</h3>

<p>Construct a paragraph2vec model on text. 
The algorithm is explained at <a href="https://arxiv.org/pdf/1405.4053.pdf">https://arxiv.org/pdf/1405.4053.pdf</a>.
People also refer to this model as doc2vec.<br />
The model is an extension to the word2vec algorithm, 
where an additional vector for every paragraph is added directly in the training.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>paragraph2vec(
  x,
  type = c("PV-DBOW", "PV-DM"),
  dim = 50,
  window = ifelse(type == "PV-DM", 5L, 10L),
  iter = 5L,
  lr = 0.05,
  hs = FALSE,
  negative = 5L,
  sample = 0.001,
  min_count = 5L,
  threads = 1L,
  encoding = "UTF-8",
  embeddings = matrix(nrow = 0, ncol = dim),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="paragraph2vec_+3A_x">x</code></td>
<td>
<p>a data.frame with columns doc_id and text or the path to the file on disk containing training data.<br />
Note that the text column should be of type character, should contain less than 1000 words where space or tab is 
used as a word separator and that the text should not contain newline characters as these are considered document delimiters.</p>
</td></tr>
<tr><td><code id="paragraph2vec_+3A_type">type</code></td>
<td>
<p>character string with the type of algorithm to use, either one of
</p>

<ul>
<li><p>'PV-DM': Distributed Memory paragraph vectors
</p>
</li>
<li><p>'PV-DBOW': Distributed Bag Of Words paragraph vectors
</p>
</li></ul>

<p>Defaults to 'PV-DBOW'.</p>
</td></tr>
<tr><td><code id="paragraph2vec_+3A_dim">dim</code></td>
<td>
<p>dimension of the word and paragraph vectors. Defaults to 50.</p>
</td></tr>
<tr><td><code id="paragraph2vec_+3A_window">window</code></td>
<td>
<p>skip length between words. Defaults to 10 for PV-DM and 5 for PV-DBOW</p>
</td></tr>
<tr><td><code id="paragraph2vec_+3A_iter">iter</code></td>
<td>
<p>number of training iterations. Defaults to 20.</p>
</td></tr>
<tr><td><code id="paragraph2vec_+3A_lr">lr</code></td>
<td>
<p>initial learning rate also known as alpha. Defaults to 0.05</p>
</td></tr>
<tr><td><code id="paragraph2vec_+3A_hs">hs</code></td>
<td>
<p>logical indicating to use hierarchical softmax instead of negative sampling. Defaults to FALSE indicating to do negative sampling.</p>
</td></tr>
<tr><td><code id="paragraph2vec_+3A_negative">negative</code></td>
<td>
<p>integer with the number of negative samples. Only used in case hs is set to FALSE</p>
</td></tr>
<tr><td><code id="paragraph2vec_+3A_sample">sample</code></td>
<td>
<p>threshold for occurrence of words. Defaults to 0.001</p>
</td></tr>
<tr><td><code id="paragraph2vec_+3A_min_count">min_count</code></td>
<td>
<p>integer indicating the number of time a word should occur to be considered as part of the training vocabulary. Defaults to 5.</p>
</td></tr>
<tr><td><code id="paragraph2vec_+3A_threads">threads</code></td>
<td>
<p>number of CPU threads to use. Defaults to 1.</p>
</td></tr>
<tr><td><code id="paragraph2vec_+3A_encoding">encoding</code></td>
<td>
<p>the encoding of <code>x</code> and <code>stopwords</code>. Defaults to 'UTF-8'. 
Calculating the model always starts from files allowing to build a model on large corpora. The encoding argument 
is passed on to <code>file</code> when writing <code>x</code> to hard disk in case you provided it as a data.frame.</p>
</td></tr>
<tr><td><code id="paragraph2vec_+3A_embeddings">embeddings</code></td>
<td>
<p>optionally a matrix with pretrained word embeddings which will be used to initialise the word embedding space with (transfer learning). 
The rownames of this matrix should consist of words. Only words overlapping with the vocabulary extracted from <code>x</code> will be used.</p>
</td></tr>
<tr><td><code id="paragraph2vec_+3A_...">...</code></td>
<td>
<p>further arguments passed on to the C++ function <code>paragraph2vec_train</code> - for expert use only</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class <code>paragraph2vec_trained</code> which is a list with elements 
</p>

<ul>
<li><p>model: a Rcpp pointer to the model
</p>
</li>
<li><p>data: a list with elements file: the training data used, n (the number of words in the training data), n_vocabulary (number of words in the vocabulary) and n_docs (number of documents)
</p>
</li>
<li><p>control: a list of the training arguments used, namely min_count, dim, window, iter, lr, skipgram, hs, negative, sample
</p>
</li></ul>



<h3>References</h3>

<p><a href="https://arxiv.org/pdf/1405.4053.pdf">https://arxiv.org/pdf/1405.4053.pdf</a>, <a href="https://groups.google.com/g/word2vec-toolkit/c/Q49FIrNOQRo/m/J6KG8mUj45sJ">https://groups.google.com/g/word2vec-toolkit/c/Q49FIrNOQRo/m/J6KG8mUj45sJ</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.paragraph2vec">predict.paragraph2vec</a></code>, <code><a href="#topic+as.matrix.paragraph2vec">as.matrix.paragraph2vec</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(tokenizers.bpe)
## Take data and standardise it a bit
data(belgium_parliament, package = "tokenizers.bpe")
str(belgium_parliament)
x &lt;- subset(belgium_parliament, language %in% "french")
x$text   &lt;- tolower(x$text)
x$text   &lt;- gsub("[^[:alpha:]]", " ", x$text)
x$text   &lt;- gsub("[[:space:]]+", " ", x$text)
x$text   &lt;- trimws(x$text)
x$nwords &lt;- txt_count_words(x$text)
x &lt;- subset(x, nwords &lt; 1000 &amp; nchar(text) &gt; 0)

## Build the model
model &lt;- paragraph2vec(x = x, type = "PV-DM",   dim = 15,  iter = 5)

model &lt;- paragraph2vec(x = x, type = "PV-DBOW", dim = 100, iter = 20)

str(model)
embedding &lt;- as.matrix(model, which = "words")
embedding &lt;- as.matrix(model, which = "docs")
head(embedding)

## Get vocabulary
vocab &lt;- summary(model, type = "vocabulary",  which = "docs")
vocab &lt;- summary(model, type = "vocabulary",  which = "words")


## Transfer learning using existing word embeddings
library(word2vec)
w2v   &lt;- word2vec(x$text, dim = 50, type = "cbow", iter = 20, min_count = 5)
emb   &lt;- as.matrix(w2v)
model &lt;- paragraph2vec(x = x, dim = 50, type = "PV-DM", iter = 20, min_count = 5, 
                       embeddings = emb)


## Transfer learning - proof of concept without learning (iter=0, set to higher to learn)
emb       &lt;- matrix(rnorm(30), nrow = 2, dimnames = list(c("en", "met")))
model     &lt;- paragraph2vec(x = x, type = "PV-DM",   dim = 15,  iter = 0, embeddings = emb)
embedding &lt;- as.matrix(model, which = "words", normalize = FALSE)
embedding[c("en", "met"), ]
emb

</code></pre>

<hr>
<h2 id='paragraph2vec_similarity'>Similarity between document / word vectors as used in paragraph2vec</h2><span id='topic+paragraph2vec_similarity'></span>

<h3>Description</h3>

<p>The similarity between document / word vectors is defined as the inner product of the vector elements
</p>


<h3>Usage</h3>

<pre><code class='language-R'>paragraph2vec_similarity(x, y, top_n = +Inf)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="paragraph2vec_similarity_+3A_x">x</code></td>
<td>
<p>a matrix with embeddings where the rownames of the matrix provide the label of the term</p>
</td></tr>
<tr><td><code id="paragraph2vec_similarity_+3A_y">y</code></td>
<td>
<p>a matrix with embeddings where the rownames of the matrix provide the label of the term</p>
</td></tr>
<tr><td><code id="paragraph2vec_similarity_+3A_top_n">top_n</code></td>
<td>
<p>integer indicating to return only the top n most similar terms from y for each row of x. 
If <code>top_n</code> is supplied, a data.frame will be returned with only the highest similarities between x and y 
instead of all pairwise similarities</p>
</td></tr>
</table>


<h3>Value</h3>

<p>By default, the function returns a similarity matrix between the rows of <code>x</code> and the rows of <code>y</code>. 
The similarity between row i of <code>x</code> and row j of <code>y</code> is found in cell <code>[i, j]</code> of the returned similarity matrix.<br />
If <code>top_n</code> is provided, the return value is a data.frame with columns term1, term2, similarity and rank 
indicating the similarity between the provided terms in <code>x</code> and <code>y</code> 
ordered from high to low similarity and keeping only the top_n most similar records.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+paragraph2vec">paragraph2vec</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(rnorm(6), nrow = 2, ncol = 3)
rownames(x) &lt;- c("word1", "word2")
y &lt;- matrix(rnorm(15), nrow = 5, ncol = 3)
rownames(y) &lt;- c("doc1", "doc2", "doc3", "doc4", "doc5")

paragraph2vec_similarity(x, y)
paragraph2vec_similarity(x, y, top_n = 1)
paragraph2vec_similarity(x, y, top_n = 2)
paragraph2vec_similarity(x, y, top_n = +Inf)
paragraph2vec_similarity(y, y)
paragraph2vec_similarity(y, y, top_n = 1)
paragraph2vec_similarity(y, y, top_n = 2)
paragraph2vec_similarity(y, y, top_n = +Inf)
</code></pre>

<hr>
<h2 id='predict.paragraph2vec'>Predict functionalities for a paragraph2vec model</h2><span id='topic+predict.paragraph2vec'></span>

<h3>Description</h3>

<p>Use the paragraph2vec model to 
</p>

<ul>
<li><p>get the embedding of documents, sentences or words
</p>
</li>
<li><p>find the nearest documents/words which are similar to either a set of documents, words or a set of sentences containing words
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'paragraph2vec'
predict(
  object,
  newdata,
  type = c("embedding", "nearest"),
  which = c("docs", "words", "doc2doc", "word2doc", "word2word", "sent2doc"),
  top_n = 10L,
  encoding = "UTF-8",
  normalize = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.paragraph2vec_+3A_object">object</code></td>
<td>
<p>a paragraph2vec model as returned by <code><a href="#topic+paragraph2vec">paragraph2vec</a></code> or <code><a href="#topic+read.paragraph2vec">read.paragraph2vec</a></code></p>
</td></tr>
<tr><td><code id="predict.paragraph2vec_+3A_newdata">newdata</code></td>
<td>
<p>either a character vector of words, a character vector of doc_id's or a list of sentences
where the list elements are words part of the model dictionary. What needs to be provided depends on the argument you provide in <code>which</code>. 
See the examples.</p>
</td></tr>
<tr><td><code id="predict.paragraph2vec_+3A_type">type</code></td>
<td>
<p>either 'embedding' or 'nearest' to get the embeddings or to find the closest text items. 
Defaults to 'nearest'.</p>
</td></tr>
<tr><td><code id="predict.paragraph2vec_+3A_which">which</code></td>
<td>
<p>either one of 'docs', 'words', 'doc2doc', 'word2doc', 'word2word' or 'sent2doc' where
</p>

<ul>
<li><p>'docs' or 'words' can be chosen if <code>type</code> is set to 'embedding' to indicate that <code>newdata</code> contains either doc_id's or words
</p>
</li>
<li><p>'doc2doc', 'word2doc', 'word2word', 'sent2doc' can be chosen if <code>type</code> is set to 'nearest' indicating to extract respectively
the closest document to a document (doc2doc), the closest document to a word (word2doc), the closest word to a word (word2word) or the closest document to sentences (sent2doc).
</p>
</li></ul>
</td></tr>
<tr><td><code id="predict.paragraph2vec_+3A_top_n">top_n</code></td>
<td>
<p>show only the top n nearest neighbours. Defaults to 10, with a maximum value of 100. Only used for <code>type</code> 'nearest'.</p>
</td></tr>
<tr><td><code id="predict.paragraph2vec_+3A_encoding">encoding</code></td>
<td>
<p>set the encoding of the text elements to the specified encoding. Defaults to 'UTF-8'.</p>
</td></tr>
<tr><td><code id="predict.paragraph2vec_+3A_normalize">normalize</code></td>
<td>
<p>logical indicating to normalize the embeddings. Defaults to <code>TRUE</code>. Only used for <code>type</code> 'embedding'.</p>
</td></tr>
<tr><td><code id="predict.paragraph2vec_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>depending on the type, you get a different output:
</p>

<ul>
<li><p>for type nearest: returns a list of data.frames with columns term1, term2, similarity and rank indicating the elements which are closest to the provided <code>newdata</code>
</p>
</li>
<li><p>for type embedding: a matrix of embeddings of the words/documents or sentences provided in <code>newdata</code>, 
rownames are either taken from the words/documents or list names of the sentences. The matrix has always the
same number of rows as the length of <code>newdata</code>, possibly with NA values if the word/doc_id is not part of the dictionary
</p>
</li></ul>

<p>See the examples.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+paragraph2vec">paragraph2vec</a></code>, <code><a href="#topic+read.paragraph2vec">read.paragraph2vec</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(tokenizers.bpe)
data(belgium_parliament, package = "tokenizers.bpe")
x &lt;- belgium_parliament
x &lt;- subset(x, language %in% "dutch")
x &lt;- subset(x, nchar(text) &gt; 0 &amp; txt_count_words(text) &lt; 1000)
x$doc_id &lt;- sprintf("doc_%s", 1:nrow(x))
x$text   &lt;- tolower(x$text)
x$text   &lt;- gsub("[^[:alpha:]]", " ", x$text)
x$text   &lt;- gsub("[[:space:]]+", " ", x$text)
x$text   &lt;- trimws(x$text)

## Build model
model &lt;- paragraph2vec(x = x, type = "PV-DM",   dim = 15,  iter = 5)

model &lt;- paragraph2vec(x = x, type = "PV-DBOW", dim = 100, iter = 20)


sentences &lt;- list(
  example = c("geld", "diabetes"),
  hi = c("geld", "diabetes", "koning"),
  test = c("geld"),
  nothing = character(), 
  repr = c("geld", "diabetes", "koning"))
  
## Get embeddings (type =  'embedding')
predict(model, newdata = c("geld", "koning", "unknownword", NA, "&lt;/s&gt;", ""), 
               type = "embedding", which = "words")
predict(model, newdata = c("doc_1", "doc_10", "unknowndoc", NA, "&lt;/s&gt;"), 
               type = "embedding", which = "docs")
predict(model, sentences, type = "embedding")

## Get most similar items (type =  'nearest')
predict(model, newdata = c("doc_1", "doc_10"), type = "nearest", which = "doc2doc")
predict(model, newdata = c("geld", "koning"), type = "nearest", which = "word2doc")
predict(model, newdata = c("geld", "koning"), type = "nearest", which = "word2word")
predict(model, newdata = sentences, type = "nearest", which = "sent2doc", top_n = 7)

## Similar way on extracting similarities
emb &lt;- predict(model, sentences, type = "embedding")
emb_docs &lt;- as.matrix(model, type = "docs")
paragraph2vec_similarity(emb, emb_docs, top_n = 3)

</code></pre>

<hr>
<h2 id='read.paragraph2vec'>Read a binary paragraph2vec model from disk</h2><span id='topic+read.paragraph2vec'></span>

<h3>Description</h3>

<p>Read a binary paragraph2vec model from disk
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read.paragraph2vec(file)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="read.paragraph2vec_+3A_file">file</code></td>
<td>
<p>the path to the model file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class paragraph2vec which is a list with elements
</p>

<ul>
<li><p>model: a Rcpp pointer to the model
</p>
</li>
<li><p>model_path: the path to the model on disk
</p>
</li>
<li><p>dim: the dimension of the embedding matrix
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
library(tokenizers.bpe)
data(belgium_parliament, package = "tokenizers.bpe")
x &lt;- subset(belgium_parliament, language %in% "french")
x &lt;- subset(x, nchar(text) &gt; 0 &amp; txt_count_words(text) &lt; 1000)


model &lt;- paragraph2vec(x = x, type = "PV-DM",   dim = 100, iter = 20)
model &lt;- paragraph2vec(x = x, type = "PV-DBOW", dim = 100, iter = 20)


path &lt;- "mymodel.bin"

write.paragraph2vec(model, file = path)
model &lt;- read.paragraph2vec(file = path)

vocab &lt;- summary(model, type = "vocabulary", which = "docs")
vocab &lt;- summary(model, type = "vocabulary", which = "words")
embedding &lt;- as.matrix(model, which = "docs")
embedding &lt;- as.matrix(model, which = "words")


</code></pre>

<hr>
<h2 id='summary.top2vec'>Get summary information of a top2vec model</h2><span id='topic+summary.top2vec'></span>

<h3>Description</h3>

<p>Get summary information of a top2vec model. Namely the topic centers and the most similar words
to a certain topic
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'top2vec'
summary(
  object,
  type = c("similarity", "c-tfidf"),
  top_n = 10,
  data = object$data,
  embedding_words = object$embedding$words,
  embedding_docs = object$embedding$docs,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.top2vec_+3A_object">object</code></td>
<td>
<p>an object of class <code>top2vec</code> as returned by <code><a href="#topic+top2vec">top2vec</a></code></p>
</td></tr>
<tr><td><code id="summary.top2vec_+3A_type">type</code></td>
<td>
<p>a character string with the type of summary information to extract for the topwords. Either 'similarity' or 'c-tfidf'.
The first extracts most similar words to the topic based on semantic similarity, the second by extracting
the words with the highest tf-idf score for each topic</p>
</td></tr>
<tr><td><code id="summary.top2vec_+3A_top_n">top_n</code></td>
<td>
<p>integer indicating to find the <code>top_n</code> most similar words to a topic</p>
</td></tr>
<tr><td><code id="summary.top2vec_+3A_data">data</code></td>
<td>
<p>a data.frame with columns 'doc_id' and 'text' representing documents. 
For each topic, the function extracts the most similar documents. 
And in case <code>type</code> is <code>'c-tfidf'</code> it get the words with the highest tf-idf scores for each topic.</p>
</td></tr>
<tr><td><code id="summary.top2vec_+3A_embedding_words">embedding_words</code></td>
<td>
<p>a matrix of word embeddings to limit the most similar words to. Defaults to 
the embedding of words from the <code>object</code></p>
</td></tr>
<tr><td><code id="summary.top2vec_+3A_embedding_docs">embedding_docs</code></td>
<td>
<p>a matrix of document embeddings to limit the most similar documents to. Defaults to 
the embedding of words from the <code>object</code></p>
</td></tr>
<tr><td><code id="summary.top2vec_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># For an example, look at the documentation of ?top2vec
</code></pre>

<hr>
<h2 id='top2vec'>Distributed Representations of Topics</h2><span id='topic+top2vec'></span>

<h3>Description</h3>

<p>Perform text clustering by using semantic embeddings of documents and words
to find topics of text documents which are semantically similar.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>top2vec(
  x,
  data = data.frame(doc_id = character(), text = character(), stringsAsFactors = FALSE),
  control.umap = list(n_neighbors = 15L, n_components = 5L, metric = "cosine"),
  control.dbscan = list(minPts = 100L),
  control.doc2vec = list(),
  umap = uwot::umap,
  trace = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="top2vec_+3A_x">x</code></td>
<td>
<p>either an object returned by <code><a href="#topic+paragraph2vec">paragraph2vec</a></code> or a data.frame 
with columns 'doc_id' and 'text' storing document ids and texts as character vectors or a matrix with document embeddings to cluster
or a list with elements docs and words containing document embeddings to cluster and word embeddings for deriving topic summaries</p>
</td></tr>
<tr><td><code id="top2vec_+3A_data">data</code></td>
<td>
<p>optionally, a data.frame with columns 'doc_id' and 'text' representing documents. 
This dataset is just stored, in order to extract the text of the most similar documents to a topic.
If it also contains a field 'text_doc2vec', this will be used to indicate the most relevant topic words
by class-based tfidf</p>
</td></tr>
<tr><td><code id="top2vec_+3A_control.umap">control.umap</code></td>
<td>
<p>a list of arguments to pass on to <code><a href="uwot.html#topic+umap">umap</a></code> for reducing the dimensionality of the embedding space</p>
</td></tr>
<tr><td><code id="top2vec_+3A_control.dbscan">control.dbscan</code></td>
<td>
<p>a list of arguments to pass on to <code><a href="dbscan.html#topic+hdbscan">hdbscan</a></code> for clustering the reduced embedding space</p>
</td></tr>
<tr><td><code id="top2vec_+3A_control.doc2vec">control.doc2vec</code></td>
<td>
<p>optionally, a list of arguments to pass on to <code><a href="#topic+paragraph2vec">paragraph2vec</a></code> in case <code>x</code> is a data.frame
instead of a doc2vec model trained by <code><a href="#topic+paragraph2vec">paragraph2vec</a></code></p>
</td></tr>
<tr><td><code id="top2vec_+3A_umap">umap</code></td>
<td>
<p>function to apply UMAP. Defaults to <code><a href="uwot.html#topic+umap">umap</a></code>, can as well be <code><a href="uwot.html#topic+tumap">tumap</a></code></p>
</td></tr>
<tr><td><code id="top2vec_+3A_trace">trace</code></td>
<td>
<p>logical indicating to print evolution of the algorithm</p>
</td></tr>
<tr><td><code id="top2vec_+3A_...">...</code></td>
<td>
<p>further arguments not used yet</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class <code>top2vec</code> which is a list with elements
</p>

<ul>
<li><p>embedding: a list of matrices with word and document embeddings
</p>
</li>
<li><p>doc2vec: a doc2vec model
</p>
</li>
<li><p>umap: a matrix of representations of the documents of <code>x</code>
</p>
</li>
<li><p>dbscan: the result of the hdbscan clustering
</p>
</li>
<li><p>data: a data.frame with columns doc_id and text
</p>
</li>
<li><p>size: a vector of frequency statistics of topic occurrence
</p>
</li>
<li><p>k: the number of clusters
</p>
</li>
<li><p>control: a list of control arguments to doc2vec / umap / dbscan
</p>
</li></ul>



<h3>Note</h3>

<p>The topic '0' is the noise topic
</p>


<h3>References</h3>

<p><a href="https://arxiv.org/abs/2008.09470">https://arxiv.org/abs/2008.09470</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+paragraph2vec">paragraph2vec</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

library(word2vec)
library(uwot)
library(dbscan)
data(be_parliament_2020, package = "doc2vec")
x      &lt;- data.frame(doc_id = be_parliament_2020$doc_id,
                     text   = be_parliament_2020$text_nl,
                     stringsAsFactors = FALSE)
x$text &lt;- txt_clean_word2vec(x$text)
x      &lt;- subset(x, txt_count_words(text) &lt; 1000)
d2v    &lt;- paragraph2vec(x, type = "PV-DBOW", dim = 50, 
                        lr = 0.05, iter = 10,
                        window = 15, hs = TRUE, negative = 0,
                        sample = 0.00001, min_count = 5, 
                        threads = 1)
# write.paragraph2vec(d2v, "d2v.bin")
# d2v    &lt;- read.paragraph2vec("d2v.bin")
model  &lt;- top2vec(d2v, data = x,
                  control.dbscan = list(minPts = 50), 
                  control.umap = list(n_neighbors = 15L, n_components = 4), trace = TRUE)
model  &lt;- top2vec(d2v, data = x,
                  control.dbscan = list(minPts = 50), 
                  control.umap = list(n_neighbors = 15L, n_components = 3), umap = tumap, 
                  trace = TRUE)
                                  
info   &lt;- summary(model, top_n = 7)
info$topwords
info$topdocs
library(udpipe)
info   &lt;- summary(model, top_n = 7, type = "c-tfidf")
info$topwords

## Change the model: reduce doc2vec model to 2D
model  &lt;- update(model, type = "umap", 
                 n_neighbors = 100, n_components = 2, metric = "cosine", umap = tumap, 
                 trace = TRUE)
info   &lt;- summary(model, top_n = 7)
data = x
info$topwords
info$topdocs

## Change the model: have minimum 200 points for the core elements in the hdbscan density
model  &lt;- update(model, type = "hdbscan", minPts = 200, trace = TRUE)
info   &lt;- summary(model, top_n = 7)
data = x
info$topwords
info$topdocs



##
## Example on a small sample 
##  with unrealistic hyperparameter settings especially regarding dim / iter / n_epochs
##  in order to have a basic example finishing &lt; 5 secs
##

library(uwot)
library(dbscan)
library(word2vec)
data(be_parliament_2020, package = "doc2vec")
x        &lt;- data.frame(doc_id = be_parliament_2020$doc_id,
                       text   = be_parliament_2020$text_nl,
                       stringsAsFactors = FALSE)
x        &lt;- head(x, 1000)
x$text   &lt;- txt_clean_word2vec(x$text)
x        &lt;- subset(x, txt_count_words(text) &lt; 1000)
d2v      &lt;- paragraph2vec(x, type = "PV-DBOW", dim = 10, 
                          lr = 0.05, iter = 0,
                          window = 5, hs = TRUE, negative = 0,
                          sample = 0.00001, min_count = 5)
emb      &lt;- list(docs  = as.matrix(d2v, which = "docs"),
                 words = as.matrix(d2v, which = "words"))
model    &lt;- top2vec(emb, 
                    data = x,
                    control.dbscan = list(minPts = 50), 
                    control.umap = list(n_neighbors = 15, n_components = 2, 
                                        init = "spectral"), 
                    umap = tumap, trace = TRUE)
info     &lt;- summary(model, top_n = 7)
print(info, top_n = c(5, 2))

</code></pre>

<hr>
<h2 id='txt_count_words'>Count the number of spaces occurring in text</h2><span id='topic+txt_count_words'></span>

<h3>Description</h3>

<p>The C++ doc2vec functionalities in this package assume words are either separated 
by a space or tab symbol and that each document contains less than 1000 words.<br /> 
This function calculates how many words there are in each element of a character vector by counting
the number of occurrences of the space or tab symbol.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_count_words(x, pattern = "[ \t]", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_count_words_+3A_x">x</code></td>
<td>
<p>a character vector with text</p>
</td></tr>
<tr><td><code id="txt_count_words_+3A_pattern">pattern</code></td>
<td>
<p>a text pattern to count which might be contained in <code>x</code>. Defaults to either space or tab.</p>
</td></tr>
<tr><td><code id="txt_count_words_+3A_...">...</code></td>
<td>
<p>other arguments, passed on to <code><a href="base.html#topic+gregexpr">gregexpr</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>an integer vector of the same length as <code>x</code> indicating how many times the pattern is occurring in <code>x</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c("Count me in.007", "this is a set  of words",
       "more\texamples tabs-and-spaces.only", NA)
txt_count_words(x)
</code></pre>

<hr>
<h2 id='update.top2vec'>Update a Top2vec model</h2><span id='topic+update.top2vec'></span>

<h3>Description</h3>

<p>Update a Top2vec model by updating the UMAP dimension reduction together with the HDBSCAN clustering
or update only the HDBSCAN clustering
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'top2vec'
update(
  object,
  type = c("umap", "hdbscan"),
  umap = object$umap_FUN,
  trace = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="update.top2vec_+3A_object">object</code></td>
<td>
<p>an object of class <code>top2vec</code> as returned by <code><a href="#topic+top2vec">top2vec</a></code></p>
</td></tr>
<tr><td><code id="update.top2vec_+3A_type">type</code></td>
<td>
<p>a character string indicating what to udpate. Either 'umap' or 'hdbscan' where the former (type = 'umap') indicates to 
update the umap as well as the hdbscan procedure and the latter (type = 'hdbscan') indicates to update only the hdbscan step.</p>
</td></tr>
<tr><td><code id="update.top2vec_+3A_umap">umap</code></td>
<td>
<p>see <code>umap</code> argument in <code><a href="#topic+top2vec">top2vec</a></code></p>
</td></tr>
<tr><td><code id="update.top2vec_+3A_trace">trace</code></td>
<td>
<p>logical indicating to print evolution of the algorithm</p>
</td></tr>
<tr><td><code id="update.top2vec_+3A_...">...</code></td>
<td>
<p>further arguments either passed on to <code><a href="dbscan.html#topic+hdbscan">hdbscan</a></code> in case type is 'hdbscan' or to <code><a href="uwot.html#topic+umap">umap</a></code>
in case type is 'umap'</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an updated top2vec object
</p>


<h3>Examples</h3>

<pre><code class='language-R'># For an example, look at the documentation of ?top2vec
</code></pre>

<hr>
<h2 id='write.paragraph2vec'>Save a paragraph2vec model to disk</h2><span id='topic+write.paragraph2vec'></span>

<h3>Description</h3>

<p>Save a paragraph2vec model as a binary file to disk
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write.paragraph2vec(x, file)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="write.paragraph2vec_+3A_x">x</code></td>
<td>
<p>an object of class <code>paragraph2vec</code> or <code>paragraph2vec_trained</code> as returned by <code><a href="#topic+paragraph2vec">paragraph2vec</a></code></p>
</td></tr>
<tr><td><code id="write.paragraph2vec_+3A_file">file</code></td>
<td>
<p>the path to the file where to store the model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>invisibly a logical if the resulting file exists and has been written on your hard disk
</p>


<h3>See Also</h3>

<p><code><a href="#topic+paragraph2vec">paragraph2vec</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(tokenizers.bpe)
data(belgium_parliament, package = "tokenizers.bpe")
x &lt;- subset(belgium_parliament, language %in% "french")
x &lt;- subset(x, nchar(text) &gt; 0 &amp; txt_count_words(text) &lt; 1000)


model &lt;- paragraph2vec(x = x, type = "PV-DM",   dim = 100, iter = 20)
model &lt;- paragraph2vec(x = x, type = "PV-DBOW", dim = 100, iter = 20)


path &lt;- "mymodel.bin"

write.paragraph2vec(model, file = path)
model &lt;- read.paragraph2vec(file = path)

vocab &lt;- summary(model, type = "vocabulary", which = "docs")
vocab &lt;- summary(model, type = "vocabulary", which = "words")
embedding &lt;- as.matrix(model, which = "docs")
embedding &lt;- as.matrix(model, which = "words")


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
