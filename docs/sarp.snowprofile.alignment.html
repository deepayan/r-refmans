<!DOCTYPE html><html><head><title>Help for package sarp.snowprofile.alignment</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sarp.snowprofile.alignment}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#averageSP'><p>Average a group of snow profiles</p></a></li>
<li><a href='#averageSPalongSeason'><p>Compute a seasonal timeseries of an average snowprofile</p></a></li>
<li><a href='#backtrackLayers'><p>Backtrack layers from average or summary profile</p></a></li>
<li><a href='#chooseICavg'><p>Get index of appropriate initial condition average profile</p></a></li>
<li><a href='#concat_avgSP_timeseries'><p>Concatenate time series of average profiles</p></a></li>
<li><a href='#ddateDistance'><p>Deposition Date Distance</p></a></li>
<li><a href='#densityDistance'><p>Difference in layer density</p></a></li>
<li><a href='#distanceSP'><p>Wrapper for dtwSP and simSP</p></a></li>
<li><a href='#distMatSP'><p>Calculate a multidimensional distance matrix between two profiles</p></a></li>
<li><a href='#dtwSP'><p>Calculate DTW alignment of two snow profiles</p></a></li>
<li><a href='#extractFromScoringMatrix'><p>Extract from Scoring matrix</p></a></li>
<li><a href='#flipLayers'><p>Flip snow profile layers top down</p></a></li>
<li><a href='#grainSimilarity_align'><p>Grain Type similarity matrix for DTW alignments</p></a></li>
<li><a href='#grainSimilarity_evaluate'><p>Grain type similarity matrix for evaluation purposes</p></a></li>
<li><a href='#hardnessDistance'><p>Difference in Hand Hardness</p></a></li>
<li><a href='#interactiveAlignment'><p>Run interactive alignment app</p></a></li>
<li><a href='#layerWeightingMat'><p>Weighting scheme for preferential layer matching</p></a></li>
<li><a href='#match_with_tolerance'><p>Match with numeric tolerance</p></a></li>
<li><a href='#medoidSP'><p>Find the medoid snow profile among a group of profiles</p></a></li>
<li><a href='#mergeIdentLayers'><p>Merge layers with identical properties</p></a></li>
<li><a href='#ogsDistance'><p>Difference in layer ogs</p></a></li>
<li><a href='#plotCostDensitySP'><p>Plot alignment cost density and warping path</p></a></li>
<li><a href='#plotSPalignment'><p>Align and plot two snow profiles using DTW</p></a></li>
<li><a href='#resampleSP'><p>Resample snowprofile</p></a></li>
<li><a href='#resampleSPpairs'><p>Resample a pair of profiles</p></a></li>
<li><a href='#reScaleSampleSPx'><p>Rescale and resample a snow profile list</p></a></li>
<li><a href='#return_conceptually_similar_gtypes'><p>Return conceptually similar grain types</p></a></li>
<li><a href='#rmZeroThicknessLayers'><p>Remove layers with a thickness of 'zero cm'</p></a></li>
<li><a href='#sarp.snowprofile.alignment-package'><p>sarp.snowprofile.alignment: Snow Profile Alignment, Aggregation, and Clustering</p></a></li>
<li><a href='#scaleSnowHeight'><p>Scale total height of a snow profile</p></a></li>
<li><a href='#sim2dist'><p>Convert 'similarity' matrix to 'distance' matrix</p></a></li>
<li><a href='#simSP'><p>Similarity measure between snow profile pairs</p></a></li>
<li><a href='#SPgroup2'><p>Additional example set of snow profiles</p></a></li>
<li><a href='#SPspacetime'><p>Additional example set of snow profiles</p></a></li>
<li><a href='#swissSimilarityMatrix'><p>Similarity Matrix of Snow Grain Types</p></a></li>
<li><a href='#warpSP'><p>Warp one snow profile onto another one</p></a></li>
<li><a href='#warpWindowSP'><p>Restrict the DTW warping window for snow profiles alignment</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Snow Profile Alignment, Aggregation, and Clustering</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-02-07</td>
</tr>
<tr>
<td>Description:</td>
<td>Snow profiles describe the vertical (1D) stratigraphy of layered 
    snow with different layer characteristics, such as grain type, hardness, 
    deposition date, and many more. Hence, they represent a data format similar 
    to multivariate time series containing categorical, ordinal, and numerical 
    data types. Use this package to align snow profiles by matching their 
    individual layers based on Dynamic Time Warping (DTW). The aligned profiles 
    can then be assessed with an independent, global similarity measure that is 
    geared towards avalanche hazard assessment. Finally, through exploiting data
    aggregation and clustering methods, the similarity measure provides the
    foundation for grouping and summarizing snow profiles according to similar
    hazard conditions. In particular, this package allows for averaging large
    numbers of snow profiles with DTW Barycenter Averaging and thereby 
    facilitates the computation of individual layer distributions and summary 
    statistics that are relevant for avalanche forecasting purposes. 
    For more background information refer to Herla, Horton, Mair,
    and Haegeli (2021) &lt;<a href="https://doi.org/10.5194%2Fgmd-14-239-2021">doi:10.5194/gmd-14-239-2021</a>&gt;, and Herla, Mair, and Haegeli 
    (2022) &lt;<a href="https://doi.org/10.5194%2Ftc-16-3149-2022">doi:10.5194/tc-16-3149-2022</a>&gt;.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://avalancheresearch.ca/">https://avalancheresearch.ca/</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.2</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>Imports:</td>
<td>dtw, grid, data.table</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10), sarp.snowprofile (&ge; 1.2.1)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, shiny, dendextend, smacof, testthat,
progress</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-08 10:03:33 UTC; flo</td>
</tr>
<tr>
<td>Author:</td>
<td>Florian Herla [aut, cre],
  Pascal Haegeli [aut],
  Simon Horton [aut],
  Paul Billecocq [aut],
  SFU Avalanche Research Program [fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Florian Herla &lt;fherla@sfu.ca&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-08 10:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='averageSP'>Average a group of snow profiles</h2><span id='topic+averageSP'></span><span id='topic+dbaSP'></span>

<h3>Description</h3>

<p>The functions <a href="#topic+dbaSP">dbaSP</a> and <a href="#topic+averageSP">averageSP</a> implement Dynamic Time Warping Barycenter Averaging of snow profiles.
The convenient wrapper <a href="#topic+averageSP">averageSP</a> takes care of choosing several appropriate initial conditions and picking the optimal end result (by minimizing the mean squared error
between the average profile and the profile set). To pay appropriate attention to (thin) weak layers, weak layers need to be labeled in the profiles.
You can either do that manually before calling this routine to suit your personal needs, or you can provide specific properties (in <code>classifyPWLs</code>)
so that weak layers be labeled according to these properties by <a href="sarp.snowprofile.html#topic+findPWL">sarp.snowprofile::labelPWL</a>.
For more details, refer to the reference paper.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>averageSP(
  SPx,
  n = 5,
  sm = summary(SPx),
  progressbar = requireNamespace("progress", quietly = TRUE),
  progressbar_pretext = NULL,
  classifyPWLs = list(pwl_gtype = c("SH", "DH")),
  classifyCRs = list(pwl_gtype = c("MFcr", "IF", "IFsc", "IFrc")),
  proportionPWL = 0.5,
  breakAtSim = 0.9,
  breakAfter = 2,
  verbose = FALSE,
  tz = "auto",
  ...
)

dbaSP(
  SPx,
  Avg,
  sm = summary(SPx),
  resamplingRate = 0.5,
  proportionPWL = 0.3,
  maxiter = 10,
  breakAtSim = 0.99,
  breakAfter = 1,
  plotChanges = FALSE,
  verbose = TRUE,
  tz = "auto",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="averageSP_+3A_spx">SPx</code></td>
<td>
<p>SPx a snowprofileSet object. Note that the profile layers need to contain a column
called <code style="white-space: pre;">&#8288;$layerOfInterest&#8288;</code> which classifies weak layers. While <a href="#topic+averageSP">averageSP</a> will label weak layers automatically if not done by the user beforehand, <a href="#topic+dbaSP">dbaSP</a> won't do that but fail instead!;
consider thinking about how you want to label weak layers, see Description, <code>classifyPWLs</code> below, and the references.
Also note, that if you wish to average the <em>rescaled</em> profile set, do so manually before calling this function (see examples).</p>
</td></tr>
<tr><td><code id="averageSP_+3A_n">n</code></td>
<td>
<p>the number of initial conditions that will be used to run <a href="#topic+dbaSP">dbaSP</a>; see also <a href="#topic+chooseICavg">chooseICavg</a>.</p>
</td></tr>
<tr><td><code id="averageSP_+3A_sm">sm</code></td>
<td>
<p>a <a href="base.html#topic+summary">summary</a> of <code>SPx</code> metadata</p>
</td></tr>
<tr><td><code id="averageSP_+3A_progressbar">progressbar</code></td>
<td>
<p>should a progressbar be displayed (the larger n, the more meaningful the progressbar)</p>
</td></tr>
<tr><td><code id="averageSP_+3A_progressbar_pretext">progressbar_pretext</code></td>
<td>
<p>a character string to be prepended to the progressbar (mainly used by higher level cluster function)</p>
</td></tr>
<tr><td><code id="averageSP_+3A_classifypwls">classifyPWLs</code></td>
<td>
<p>an argument list for a function call to <a href="sarp.snowprofile.html#topic+findPWL">sarp.snowprofile::findPWL</a> which returns relevant PWLs for identifying initial conditions. <strong>Importantly</strong>, these arguments will also be used
to label weak layers in the profiles, if these labels do not yet exist in the layers objects as column <code style="white-space: pre;">&#8288;$layerOfInterest&#8288;</code>.
Check out the documentation of <code>findPWL</code> to familiarize yourself with your manifold options!</p>
</td></tr>
<tr><td><code id="averageSP_+3A_classifycrs">classifyCRs</code></td>
<td>
<p>an argument list for a function call to <a href="sarp.snowprofile.html#topic+findPWL">sarp.snowprofile::findPWL</a> which returns relevant crusts for identifying initial conditions.</p>
</td></tr>
<tr><td><code id="averageSP_+3A_proportionpwl">proportionPWL</code></td>
<td>
<p>decimal number that specifies the proportion required to average an ensemble of grain types as weak layer type.
A value of 0.3, for example, means that layers will get averaged to a PWL type if 30% of the layers are of PWL type.
Meaningful range is between <code style="white-space: pre;">&#8288;[0.1, 0.5]&#8288;</code>. Values larger than 0.5 get set to 0.5.</p>
</td></tr>
<tr><td><code id="averageSP_+3A_breakatsim">breakAtSim</code></td>
<td>
<p>stop iterations when <a href="#topic+simSP">simSP</a> between the last average profiles is beyond that value. Can range between <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>. Default values differ between <a href="#topic+dbaSP">dbaSP</a> and <a href="#topic+averageSP">averageSP</a>.</p>
</td></tr>
<tr><td><code id="averageSP_+3A_breakafter">breakAfter</code></td>
<td>
<p>integer specifying how many values of simSP need to be above <code>breakAtSim</code> to stop iterating. Default values differ between <a href="#topic+dbaSP">dbaSP</a> and <a href="#topic+averageSP">averageSP</a>.</p>
</td></tr>
<tr><td><code id="averageSP_+3A_verbose">verbose</code></td>
<td>
<p>print similarities between old and new average in between iterations?</p>
</td></tr>
<tr><td><code id="averageSP_+3A_tz">tz</code></td>
<td>
<p>timezone of profiles; necessary for assigning the correct timezone to the average profile's ddate/bdate. Either <code>'auto'</code> or a timezone known to <code style="white-space: pre;">&#8288;[as.POSIXct]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="averageSP_+3A_...">...</code></td>
<td>
<p>alignment configurations which are passed on to <a href="#topic+dbaSP">dbaSP</a> and then further to <a href="#topic+dtwSP">dtwSP</a>. Note, that you can't provide <code>rescale2refHS</code>, which is always set to FALSE. If you wish to rescale
the profiles, read the description of the <code>SPx</code> parameter and the examples.</p>
</td></tr>
<tr><td><code id="averageSP_+3A_avg">Avg</code></td>
<td>
<p>the initial average snow profile: either a snowprofile object or an index to an initial average profile in SPx</p>
</td></tr>
<tr><td><code id="averageSP_+3A_resamplingrate">resamplingRate</code></td>
<td>
<p>Resampling rate for a regular depth grid among the profiles</p>
</td></tr>
<tr><td><code id="averageSP_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations</p>
</td></tr>
<tr><td><code id="averageSP_+3A_plotchanges">plotChanges</code></td>
<td>
<p>specify whether and how you want to plot the dba process: either <code>FALSE</code>, 'TRUE<code>==</code>'iterations'<code style="white-space: pre;">&#8288;, or &#8288;</code>'averages+last''</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Technical note: Since the layer characteristics of the average profile represent the median characteristics of the individual profiles, it can happen that ddates of the
averaged layers are not in a monotonical order. That is, of course unphysical, but we specifically decided not to override these values to highlight these slight inconsistencies
to users, so that they can decide how to deal with them. As a consequence, the function <a href="sarp.snowprofile.html#topic+deriveDatetag">sarp.snowprofile::deriveDatetag</a> does not work for these average profiles with ddate
inconsistencies, but throws an error. The suggested workaround for this issue is to apply that function to all individual profiles <em>before</em> computing the average profile. This
ensures that bdates or datetags are also included in the average profile.
</p>
<p>For developers: Including new variables into the averaging/dba routines can be done easily by following commit #9f9e6f9
</p>


<h3>Value</h3>

<p>A list of class <code>avgSP</code> that contains the fields
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$avg&#8288;</code>: the resulting average profile
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$set&#8288;</code>: the corresponding resampled profiles of the group
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$call&#8288;</code>: (only with <code>averageSP</code>) the function call
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$prelabeledPWLs&#8288;</code>: (only with <code>averageSP</code>) boolean scalar whether PWLs (or any other layers of interest) were prelabeled before this routine (<code>TRUE</code>) or
labeled by this routine with the defaults specified in <code>classifyPWLs</code> (<code>FALSE</code>)
</p>
</li></ul>

<p>The profile layers of the average profile refer to the median properties of the predominant layers. For example, if you labeled all SH/DH layers as your 'layersOfInterest',
and you find a SH or DH layer in the average profile, then it means that the predominant grain type is SH/DH (i.e., more profiles than specified in <code>proportionPWL</code> have that layer)
and layer properties like hardness, p_unstable, etc refer to the median properties of these SH/DH layers. If you find a RG layer in your average profile, it means that most
profiles have that RG layer and the layer properties refer to the median properties of all these RG layers. There are two exceptions to this rule, one for <code>height</code>/<code>depth</code>, and one
for layer properties with the ending <code style="white-space: pre;">&#8288;_all&#8288;</code>, such as <code>ppu_all</code>:
</p>

<ul>
<li> <p><code>height</code> and <code>depth</code> provide the vertical grid of the average profile, and for algorithmic reasons, this grid is not always equal to the actual median height or depth
of the predominant layers. To account for that, two layer columns exist called <code>medianPredominantHeight</code> and <code>medianPredominantDepth</code>.
</p>
</li>
<li><p> Properties ending with <code style="white-space: pre;">&#8288;_all&#8288;</code>: For example, while <code>ppu</code> refers to the proportion of profiles, whose <em>predominant</em> layers are unstable (i.e., <code>p_unstable</code> &gt;= 0.77),
<code>ppu_all</code> refers to the the proportion of profiles, whose layers are unstable while taking into account <em>all</em> individual layers matched to this average layer
(i.e., despite grain type, etc).
</p>
</li>
<li><p> Other layer properties specific to the average profile: <code>distribution</code> ranges between <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code> and specifies the proportion of profiles that contain the predominant layer described in the other properties.
</p>
</li></ul>



<h3>Functions</h3>


<ul>
<li> <p><code>averageSP()</code>: convenient wrapper function
</p>
</li>
<li> <p><code>dbaSP()</code>: DTW barycenter averaging of snow profiles (low level worker function)
</p>
</li></ul>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>References</h3>

<p>Herla, F., Haegeli, P., and Mair, P. (2022). A data exploration tool for averaging and accessing large data sets of snow stratigraphy profiles useful for avalanche forecasting,
The Cryosphere, 16(8), 3149–3162, https://doi.org/10.5194/tc-16-3149-2022
</p>


<h3>See Also</h3>

<p><a href="#topic+averageSPalongSeason">averageSPalongSeason</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## EXAMPLES OF averageSP
this_example_runs_about_10s &lt;- TRUE
if (!this_example_runs_about_10s) {  # exclude from cran checks

## compute the average profile of the demo object 'SPgroup'
## * by labeling SH/DH layers as weak layers,
##   - choosing 3 initial conditions with an above average number of weak layers
##   - in as many depth ranges as possible
## * and neglecting crusts for initial conditions

  avgList &lt;- averageSP(SPgroup, n = 3,
                       classifyPWLs = list(pwl_gtype = c("SH", "DH")),
                       classifyCRs = NULL)

  opar &lt;- par(mfrow = c(1, 2))
  plot(avgList$avg, ymax = max(summary(avgList$set)$hs))
  plot(avgList$set, SortMethod = "unsorted", xticklabels = "originalIndices")
  par(opar)


## compute the average profile of the demo object 'SPgroup'
## * by labeling SH/DH/FC/FCxr layers with an RTA threshold of 0.65 as weak layers,
## * otherwise as above

  SPx &lt;- computeRTA(SPgroup)
  avgList &lt;- averageSP(SPx, n = 3,
                       classifyPWLs = list(pwl_gtype = c("SH", "DH", "FC", "FCxr"),
                                           threshold_RTA = 0.65),
                       classifyCRs = NULL)

  opar &lt;- par(mfrow = c(1, 2))
  plot(avgList$avg, ymax = max(summary(avgList$set)$hs))
  plot(avgList$set, SortMethod = "unsorted", xticklabels = "originalIndices")
  par(opar)

## compute the average profile of the other demo object 'SPgroup2', which
## contains more stability indices, such as SK38 or p_unstable
## * by labeling SH/DH/FC/FCxr layers that either
##   - have an SK38 below 0.95, *or*
##   - have a p_unstable above 0.77

  SPx &lt;- snowprofileSet(SPgroup2)
  avgList &lt;- averageSP(SPx,
                       classifyPWLs = list(pwl_gtype = c("SH", "DH", "FC", "FCxr"),
                                           threshold_SK38 = 0.95, threshold_PU = 0.77))

  opar &lt;- par(mfrow = c(1, 2))
  plot(avgList$avg, ymax = max(summary(avgList$set)$hs))
  plot(avgList$set, SortMethod = "unsorted", xticklabels = "originalIndices")
  par(opar)

}



## EXAMPLES OF dbaSP
## either rescale profiles beforehand...
if (FALSE) {  # don't run in package check to save time
  SPx &lt;- reScaleSampleSPx(SPgroup)$set          # rescale profiles
  SPx &lt;- snowprofileSet(lapply(SPx, labelPWL))  # label PWLs
  DBA &lt;- dbaSP(SPx, 5, plotChanges = TRUE)      # average profiles
}

## or use unscaled snow heights:
if (FALSE) {  # don't run in package check to save time
  SPx &lt;- snowprofileSet(lapply(SPgroup, labelPWL))  # label PWLs
  DBA &lt;- dbaSP(SPx, 5, plotChanges = TRUE)          # average profiles
}
</code></pre>

<hr>
<h2 id='averageSPalongSeason'>Compute a seasonal timeseries of an average snowprofile</h2><span id='topic+averageSPalongSeason'></span>

<h3>Description</h3>

<p>This routine computes the seasonal timeseries of the average snow profile for a given region/set of profiles. The total snow height of the
seasonal average profile closely follows the <em>median snow height</em> represented by the group of profiles each day. Also the
new snow amounts represent the <em>median new snow amounts</em> within the group (i.e., PP and DF grains). The routine maintains
temporal consistency by using the previous day average profile as initial condition to derive the next day's. This creates the need for re-scaling
the layer thicknesses each day to account for snow settlement and melting. Two different re-scaling approaches have been implemented,
which both aim to re-scale the old snow part of the column (i.e., the snow which was on the ground already at the previous day).
See parameter description for more details. Also note, that the routine can be started at any day of the season by providing
an average profile from the previous day. The routine modifies several parameters, which are passed on to <a href="#topic+dtwSP">dtwSP</a>. These
parameters differ from the defaults specified in <a href="#topic+dtwSP">dtwSP</a>, which are held very generic, whereas the application in this function
is much more specific to certain requirements and algorithm behavior. For more details, refer to the reference paper.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>averageSPalongSeason(
  SPx,
  sm = summary(SPx),
  AvgDayBefore = NULL,
  DateEnd = max(sm$date),
  keep.profiles = TRUE,
  progressbar = requireNamespace("progress", quietly = TRUE),
  dailyRescaling = c("settleTopOldSnow", "settleEntireOldSnow")[1],
  proportionPWL = 0.3,
  breakAtSim = 0.9,
  breakAfter = 2,
  verbose = FALSE,
  resamplingRate = 0.5,
  top.down = FALSE,
  checkGlobalAlignment = FALSE,
  prefLayerWeights = NA,
  dims = c("gtype", "hardness", "ddate"),
  weights = c(0.375, 0.125, 0.5),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="averageSPalongSeason_+3A_spx">SPx</code></td>
<td>
<p>a snowprofileSet that contains all profiles from the region to be averaged at all days of the season for which you want to compute the average profile.
Identically to <a href="#topic+dbaSP">dbaSP</a>, weak layers need to be labeled prior to this function call, see <a href="#topic+dbaSP">dbaSP</a> and <a href="sarp.snowprofile.html#topic+findPWL">sarp.snowprofile::labelPWL</a>. Note that only daily sampling is
allowed at this point (i.e., one profile per grid point per day).</p>
</td></tr>
<tr><td><code id="averageSPalongSeason_+3A_sm">sm</code></td>
<td>
<p>a summary of <code>SPx</code> containing meta-data</p>
</td></tr>
<tr><td><code id="averageSPalongSeason_+3A_avgdaybefore">AvgDayBefore</code></td>
<td>
<p>an average snowprofile from the previous day. This is only necessary if you want to resume the computation
mid season.</p>
</td></tr>
<tr><td><code id="averageSPalongSeason_+3A_dateend">DateEnd</code></td>
<td>
<p>an end date character string (<code>"YYYY-MM-DD"</code>) if you only want to compute the timeseries up to a certain point
in time. Defaults to the future-most date contained in the meta-data object <code>sm</code>.</p>
</td></tr>
<tr><td><code id="averageSPalongSeason_+3A_keep.profiles">keep.profiles</code></td>
<td>
<p>Do you want to keep the (resampled) individual snow profiles from <code>SPx</code> in your return object? <strong>Note</strong>
that this must be <code>TRUE</code> if you plan to <a href="#topic+backtrackLayers">backtrackLayers</a> to derive any kind of summary statistics for the averaged layers.
See Notes below, and examples of how to conveniently <a href="#topic+backtrackLayers">backtrackLayers</a>.</p>
</td></tr>
<tr><td><code id="averageSPalongSeason_+3A_progressbar">progressbar</code></td>
<td>
<p>display a progress bar during computation?</p>
</td></tr>
<tr><td><code id="averageSPalongSeason_+3A_dailyrescaling">dailyRescaling</code></td>
<td>
<p>choose between two settlement rescaling approaches. <code>settleEntireOldSnow</code> re-scales the entire old snow
column so that the average snow height represents the median snow height from the profile set. <code>settleTopOldSnow</code> (the default)
re-scales the upper part of the old snow column to achieve the same goal. While the former mostly leads to buried layers being
settled to too deep snow depths, the default approach aims to leave those buried layers unchanged, which are located at depths
that represent the median depths of their aligned layers.</p>
</td></tr>
<tr><td><code id="averageSPalongSeason_+3A_proportionpwl">proportionPWL</code></td>
<td>
<p>decimal number that specifies the proportion required to average an ensemble of grain types as weak layer type.
A value of 0.3, for example, means that layers will get averaged to a PWL type if 30% of the layers are of PWL type.
Meaningful range is between <code style="white-space: pre;">&#8288;[0.1, 0.5]&#8288;</code>. Values larger than 0.5 get set to 0.5.</p>
</td></tr>
<tr><td><code id="averageSPalongSeason_+3A_breakatsim">breakAtSim</code></td>
<td>
<p>stop iterations when <a href="#topic+simSP">simSP</a> between the last average profiles is beyond that value. Can range between <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>. Default values differ between <a href="#topic+dbaSP">dbaSP</a> and <a href="#topic+averageSP">averageSP</a>.</p>
</td></tr>
<tr><td><code id="averageSPalongSeason_+3A_breakafter">breakAfter</code></td>
<td>
<p>integer specifying how many values of simSP need to be above <code>breakAtSim</code> to stop iterating. Default values differ between <a href="#topic+dbaSP">dbaSP</a> and <a href="#topic+averageSP">averageSP</a>.</p>
</td></tr>
<tr><td><code id="averageSPalongSeason_+3A_verbose">verbose</code></td>
<td>
<p>print similarities between old and new average in between iterations?</p>
</td></tr>
<tr><td><code id="averageSPalongSeason_+3A_resamplingrate">resamplingRate</code></td>
<td>
<p>Resampling rate for a regular depth grid among the profiles</p>
</td></tr>
<tr><td><code id="averageSPalongSeason_+3A_top.down">top.down</code></td>
<td>
<p>a <a href="#topic+dtwSP">dtwSP</a> parameter, which needs to be set to <code>FALSE</code> to ensure correct growing of the snowpack during snowfall.</p>
</td></tr>
<tr><td><code id="averageSPalongSeason_+3A_checkglobalalignment">checkGlobalAlignment</code></td>
<td>
<p>a <a href="#topic+dtwSP">dtwSP</a> parameter, which needs to be set to <code>FALSE</code> analogous to <code>top.down</code></p>
</td></tr>
<tr><td><code id="averageSPalongSeason_+3A_preflayerweights">prefLayerWeights</code></td>
<td>
<p>a <a href="#topic+dtwSP">dtwSP</a> parameter. Might be best to set this to <code>NA</code>, but can potentially be set to
<code>layerWeightingMat(FALSE)</code> <em>in case of</em> averaging a very large geographic region with temporal lags between weather events.</p>
</td></tr>
<tr><td><code id="averageSPalongSeason_+3A_dims">dims</code></td>
<td>
<p>a <a href="#topic+dtwSP">dtwSP</a> parameter, which is modified to include deposition date alignments per default</p>
</td></tr>
<tr><td><code id="averageSPalongSeason_+3A_weights">weights</code></td>
<td>
<p>a <a href="#topic+dtwSP">dtwSP</a> parameter that sets the according weights to the <code>dims</code> specified above.</p>
</td></tr>
<tr><td><code id="averageSPalongSeason_+3A_...">...</code></td>
<td>
<p>any other parameters passed on to <a href="#topic+dbaSP">dbaSP</a> and then <a href="#topic+dtwSP">dtwSP</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computing the seasonal average profile for an entire season and about 100 grid points (with a max of 150 cm snow depth) takes roughly 60 mins.
</p>


<h3>Value</h3>

<p>A list of class <code>avgSP_timeseries</code> containing the fields <code style="white-space: pre;">&#8288;$avgs&#8288;</code> with a snowprofileSet of the average profiles at each day.
If <code>keep.profiles == TRUE</code> a field <code style="white-space: pre;">&#8288;$sets&#8288;</code> with the according profiles informing the average profile at each day (which can be
used to <a href="#topic+backtrackLayers">backtrackLayers</a> to compute summary statistics of the averaged layers). And two fields <code style="white-space: pre;">&#8288;$call&#8288;</code> and <code style="white-space: pre;">&#8288;$meta&#8288;</code>. The
latter contains several useful meta-information such as <code>...$date</code>, <code>...$hs</code>, <code>...$hs_median</code>, <code>...$thicknessPPDF_median</code>, or <code>...$rmse</code>, which gauges
the representativity of the average profile (the closer to <code>0</code>, the better; the closer to <code>1</code>, the worse).
</p>


<h3>Note</h3>


<ul>
<li><p> If you don't provide an AvgDayBefore, it will be computed with <a href="#topic+averageSP">averageSP</a> and <em>default</em> parameters
(dots won't be passed to initializing the first average profile)!
</p>
</li>
<li><p> Even though <a href="#topic+backtrackLayers">backtrackLayers</a> allows for backtracking layers based on height, it is not recommended to try and
backtrack layers if <code>keep.profiles = FALSE</code>, since profiles that can't be aligned to the average profile (<code style="white-space: pre;">&#8288;$avgs[[i]]&#8288;</code>)
are being discarded from the profile set at that day (<code style="white-space: pre;">&#8288;$sets[[i]]&#8288;</code>), which changes <code>queryID</code>s in the backtrackingTable.
Conclusion: If you want to backtrack layers from the seasonal average profile, you <em>must</em> <code>keep.profiles = TRUE</code>. See examples!
</p>
</li></ul>



<h3>Author(s)</h3>

<p>fherla
</p>


<h3>References</h3>

<p>Herla, F., Haegeli, P., and Mair, P. (2022). A data exploration tool for averaging and accessing large data sets of snow stratigraphy profiles useful for avalanche forecasting,
The Cryosphere, 16(8), 3149–3162, https://doi.org/10.5194/tc-16-3149-2022
</p>


<h3>See Also</h3>

<p><a href="#topic+dbaSP">dbaSP</a>, <a href="#topic+averageSP">averageSP</a>, <a href="sarp.snowprofile.html#topic+findPWL">sarp.snowprofile::labelPWL</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
run_the_examples &lt;- FALSE  # exclude long-running examples
if (run_the_examples) {

## compute average timeseries for simplistic example data set 'SPspacetime'
## first: label weak layers (you can choose your own rules and thresholds!)
SPspacetime &lt;- snowprofileSet(lapply(SPspacetime, function(sp) {
 labelPWL(sp, pwl_gtype = c("SH", "DH", "FC", "FCxr"), threshold_RTA = 0.8)
}))  # label weak layers in each profile of the profile set 'SPspacetime'

## second: average along several days
avgTS &lt;- averageSPalongSeason(SPspacetime)

## explore resulting object
names(avgTS)

# timeseries figure
plot(avgTS$avgs, main = "average time series")
# add line representing median snow height
lines(avgTS$meta$date, avgTS$meta$hs_median)
# add line representing median new snow amounts
lines(avgTS$meta$date, avgTS$meta$hs - avgTS$meta$thicknessPPDF_median, lty = 'dashed')

# individual profile sets from one day
plot(avgTS$sets[[1]], SortMethod = "hs", main = "individual profiles from first day")


## backtrack individual layers of the average profile...
individualLayers &lt;- backtrackLayers(avgProfile = avgTS$avgs[[1]],
                      profileSet = avgTS$sets[[1]],
                      layer = findPWL(avgTS$avgs[[1]], pwl_gtype = c("SH", "DH"),
                                      pwl_date = "2018-10-17", threshold_RTA = 0.8))
## ... to retrieve summary statistics or distributions, e.g. stability distribution
hist(individualLayers[[1]]$rta)
hist(individualLayers[[1]]$depth)

## see the Vignette about averaging profiles for more examples!


}

</code></pre>

<hr>
<h2 id='backtrackLayers'>Backtrack layers from average or summary profile</h2><span id='topic+backtrackLayers'></span>

<h3>Description</h3>

<p>An average profile as computed by <a href="#topic+dbaSP">dbaSP</a> summarizes the prevalent layer properties of the entire profile set. To better
understand the distribution of layer properties within the set, use this function to retrieve layers of interest from the individual profiles of the original profile set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>backtrackLayers(
  avgProfile,
  layer = NA,
  profileSet = NULL,
  layer_units = "row#",
  condition = NULL,
  computationByHeight = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="backtrackLayers_+3A_avgprofile">avgProfile</code></td>
<td>
<p>an average profile as per <a href="#topic+dbaSP">dbaSP</a></p>
</td></tr>
<tr><td><code id="backtrackLayers_+3A_layer">layer</code></td>
<td>
<p>the height or row number of the layer to retrieve the distribution for (given as height or row number of the average profile). If layer
is <code>NA</code>, all layers from the avgProfile are considered.</p>
</td></tr>
<tr><td><code id="backtrackLayers_+3A_profileset">profileSet</code></td>
<td>
<p>the profile set that is averaged by <code>avgProfile</code>. Optimally, it is the resampled profile set as returned by <a href="#topic+dbaSP">dbaSP</a> or <a href="#topic+averageSP">averageSP</a>, see parameter <code>computationByHeight</code>
if that resampled profile set is not available anymore.</p>
</td></tr>
<tr><td><code id="backtrackLayers_+3A_layer_units">layer_units</code></td>
<td>
<p>either <code>"row#"</code> or <code>"cm"</code></p>
</td></tr>
<tr><td><code id="backtrackLayers_+3A_condition">condition</code></td>
<td>
<p>a condition that subsets which layers are returned. E.g., only layers with a specific grain type, etc.. Note that the condition needs to be substituted in the
function call, e.g. <code>condition = substitute(gtype == "SH")</code>. In most cases, it's best to subset the data.frame manually after this function has been called. A <em>secret</em> and <em>dangerous</em> trick is
to use <code>condition = substitute(gtype %in% return_conceptually_similar_gtypes(as.character(avgProfile$layers$gtype[lidx])))</code> to get the very same layers that have been used to compute the median
layer properties which are included in the avgProfile$layers.</p>
</td></tr>
<tr><td><code id="backtrackLayers_+3A_computationbyheight">computationByHeight</code></td>
<td>
<p>There are two ways of how to backtrack layers that were aligned to <code>avgProfile$layers</code>. The first and safest approach is by index, which requires the
resampled <code>profileSet</code> as returned by <a href="#topic+dbaSP">dbaSP</a> or <a href="#topic+averageSP">averageSP</a>. The second approach is by layer height, which should yield the same results (beta phase: still bugs possible, check yourself!)
and allows to backtrack the layers even if the resampled profileSet is not available anymore, but only the original unmodified set which was used to create the average profile.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns a list of data.frames with the backtracked layers. Each (named) list item corresponds to a specific layer height (cm).
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See Vignette for examples.

</code></pre>

<hr>
<h2 id='chooseICavg'>Get index of appropriate initial condition average profile</h2><span id='topic+chooseICavg'></span>

<h3>Description</h3>

<p>To average a set of snow profiles, <a href="#topic+dbaSP">dbaSP</a> requires a snow profile as initial condition (IC) to start the algorithm. To prevent persistent weak layers (PWLs) and crusts
from being averaged-out during the call to <code>dbaSP</code>, it is advised to start the algorithm with a best-guess IC. This best guess IC contains a large number of PWLs and crusts to ensure
that the most prevalent ones actually make their way into the final average profile. This function helps to choose meaningful IC profiles. See Details or (better) the source code
for how this function picks the profiles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chooseICavg(
  set,
  n,
  classifyPWLs,
  classifyCRs,
  nPWL = round((2 * n/3) + 0.001),
  sm = summary(set)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chooseICavg_+3A_set">set</code></td>
<td>
<p>a snowprofileSet</p>
</td></tr>
<tr><td><code id="chooseICavg_+3A_n">n</code></td>
<td>
<p>number of profile indices to be picked (i.e., returned)</p>
</td></tr>
<tr><td><code id="chooseICavg_+3A_classifypwls">classifyPWLs</code></td>
<td>
<p>an argument list for a function call to <a href="sarp.snowprofile.html#topic+findPWL">sarp.snowprofile::findPWL</a> which returns relevant PWLs for identifying initial conditions</p>
</td></tr>
<tr><td><code id="chooseICavg_+3A_classifycrs">classifyCRs</code></td>
<td>
<p>an argument list for a function call to <a href="sarp.snowprofile.html#topic+findPWL">sarp.snowprofile::findPWL</a> which returns relevant CR(ust)s for identifying initial conditions</p>
</td></tr>
<tr><td><code id="chooseICavg_+3A_npwl">nPWL</code></td>
<td>
<p>number of profile indices to be picked from profiles that have many PWLs in many different vertical levels; an analogous <code>nCR</code> will be the difference <code>n - nPWL</code>.</p>
</td></tr>
<tr><td><code id="chooseICavg_+3A_sm">sm</code></td>
<td>
<p>a (precomputed) summary of the <code>set</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function first computes how many PWLs and how many crusts are in the profiles that have a close to median total snow height HS.
Each of these profile is then divided into several vertical levels (by numberOfPWLsPerVerticalLevel).
nPWL and nCR profiles are then randomly picked from the profiles that have PWLs or CR in most vertical levels and additionally have a rather large number of PWLs/CR overall.
The larger <code>n</code>, the more profiles with decreasing number of PWLs/CR in different levels are also returned. Note that this function is best applied
to large profile sets to obtain semi-random results. For small sets, the indices returned can actually be deterministic since the pool of relevant profiles is too small.
</p>


<h3>Value</h3>

<p><code>n</code> number of indices that correspond to profiles in the <code>set</code>
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>See Also</h3>

<p><a href="sarp.snowprofile.html#topic+findPWL">sarp.snowprofile::findPWL</a>, <a href="#topic+averageSP">averageSP</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>plot(SPgroup, SortMethod = "unsorted", TopDown = TRUE,
     xticklabels = "originalIndices", main = "entire profile set")
IC_ids_pwl &lt;- chooseICavg(SPgroup, n = 4, nPWL = 4,
                          classifyPWLs = list(pwl_gtype = c("SH", "DH")),
                          classifyCRs = NULL)
plot(SPgroup[IC_ids_pwl], SortMethod = "unsorted", hardnessResidual = 0, TopDown = TRUE,
     xticklabels = IC_ids_pwl, main = "sample of profiles with rather many and distributed PWLs")

</code></pre>

<hr>
<h2 id='concat_avgSP_timeseries'>Concatenate time series of average profiles</h2><span id='topic+concat_avgSP_timeseries'></span>

<h3>Description</h3>

<p>This is useful in operations to update a time series that was computed in the past
with a newly computed average time series. The routine merges all entries with duplicated
entries (read dates) being taken from <code>avgSP2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>concat_avgSP_timeseries(avgSP1, avgSP2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="concat_avgSP_timeseries_+3A_avgsp1">avgSP1</code></td>
<td>
<p>old time series of average profiles as returned by <a href="#topic+averageSPalongSeason">averageSPalongSeason</a></p>
</td></tr>
<tr><td><code id="concat_avgSP_timeseries_+3A_avgsp2">avgSP2</code></td>
<td>
<p>new time series of average profiles as returned by <a href="#topic+averageSPalongSeason">averageSPalongSeason</a></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>See Also</h3>

<p><a href="#topic+averageSPalongSeason">averageSPalongSeason</a>
</p>

<hr>
<h2 id='ddateDistance'>Deposition Date Distance</h2><span id='topic+ddateDistance'></span>

<h3>Description</h3>

<p>Calculate the distance (i.e. dissimilarity) between two deposition dates
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ddateDistance(
  ddate1,
  ddate2,
  normalizeBy = 5,
  clipWindow = FALSE,
  na.dist = 0.5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ddateDistance_+3A_ddate1">ddate1</code></td>
<td>
<p>1D array of POSIX dates</p>
</td></tr>
<tr><td><code id="ddateDistance_+3A_ddate2">ddate2</code></td>
<td>
<p>same format and length as ddate1</p>
</td></tr>
<tr><td><code id="ddateDistance_+3A_normalizeby">normalizeBy</code></td>
<td>
<p>Numeric scalar to be used for normalization, i.e. the number of days, that defines the distance value of 1</p>
</td></tr>
<tr><td><code id="ddateDistance_+3A_clipwindow">clipWindow</code></td>
<td>
<p>Should differences larger than 'normalizeBy' number of days be set to distance 'Infinity'? default FALSE.</p>
</td></tr>
<tr><td><code id="ddateDistance_+3A_na.dist">na.dist</code></td>
<td>
<p>replace NA values with that distance</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An array of length(ddate1) containing the distances according to the configurations.
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## create ddate arrays..
ddate &lt;- as.POSIXct("2019/04/20 12:00", tz = "UTC")
ddate1 &lt;- rep(ddate, 5)
ddate2 &lt;- as.POSIXct(c("2019/04/12 08:00", "2019/04/16 10:00", "2019/04/20 12:00",
                       "2019/04/21 16:00", "2019/04/22 20:00"), tz = "UTC")

## .. and calculate distance:
ddateDistance(ddate1, ddate2, normalizeBy = 5)
</code></pre>

<hr>
<h2 id='densityDistance'>Difference in layer density</h2><span id='topic+densityDistance'></span>

<h3>Description</h3>

<p>Calculate the difference (i.e. distance) in layer density
</p>


<h3>Usage</h3>

<pre><code class='language-R'>densityDistance(density1, density2, normalize = FALSE, absDist = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="densityDistance_+3A_density1">density1</code></td>
<td>
<p>numeric density values (1D array)</p>
</td></tr>
<tr><td><code id="densityDistance_+3A_density2">density2</code></td>
<td>
<p>numeric density values (1D array)</p>
</td></tr>
<tr><td><code id="densityDistance_+3A_normalize">normalize</code></td>
<td>
<p>Should result be normalized? boolean, default False.</p>
</td></tr>
<tr><td><code id="densityDistance_+3A_absdist">absDist</code></td>
<td>
<p>Interested in absolute distance? default True.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric density distance
</p>


<h3>Author(s)</h3>

<p>pbillecocq
</p>

<hr>
<h2 id='distanceSP'>Wrapper for dtwSP and simSP</h2><span id='topic+distanceSP'></span>

<h3>Description</h3>

<p>Calculate the distance between two snowprofile objects by
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distanceSP(query, ref, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distanceSP_+3A_query">query</code></td>
<td>
<p>The query snowprofile object (will be warped onto <code>ref</code>)</p>
</td></tr>
<tr><td><code id="distanceSP_+3A_ref">ref</code></td>
<td>
<p>The reference snowprofile object (will <em>not</em> be warped)</p>
</td></tr>
<tr><td><code id="distanceSP_+3A_...">...</code></td>
<td>
<p>passed on to <a href="#topic+dtwSP">dtwSP</a></p>
</td></tr>
</table>


<h3>Details</h3>


<ol>
<li><p> Matching their layers and aligning them (i.e., warp one profile onto the other one)
</p>
</li>
<li><p> Assessing the similarity of the aligned profiles based on avalanche hazard relevant characteristics
</p>
</li>
<li><p> Convert the similarity score into a distance value between <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>
</p>
</li></ol>

<p>This procedure is useful for clustering and aggregating tasks, given a set of multiple profiles.
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>See Also</h3>

<p><a href="#topic+dtwSP">dtwSP</a>, <a href="#topic+simSP">simSP</a>, <a href="#topic+medoidSP">medoidSP</a>
</p>

<hr>
<h2 id='distMatSP'>Calculate a multidimensional distance matrix between two profiles</h2><span id='topic+distMatSP'></span>

<h3>Description</h3>

<p>This routine calculates a distance matrix for two given profiles (<code>query</code> and <code>ref</code>). Analogously to other DTW
routines, the query is arranged along the matrix rows, the ref along the columns. Every cell of the matrix represents
the distance between the corresponding profile layers. The distance is calculated based on the specified layer properties
(e.g., <code>hardness</code>, <code>gtype</code>, <code>ddate</code>). The routine calls subroutines to calculate the distance for each property and
combines the normalized distances by weighted averaging.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distMatSP(
  query,
  ref,
  dims = c("hardness", "gtype"),
  weights = c(0.2, 0.8),
  gtype_distMat = sim2dist(grainSimilarity_align(FALSE)),
  prefLayerWeights = layerWeightingMat(FALSE),
  ddateNorm = 5,
  windowFunction = warpWindowSP,
  top.down.mirroring = FALSE,
  warn.if.na.in.distance.calc = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distMatSP_+3A_query">query</code></td>
<td>
<p>The query snowprofile object</p>
</td></tr>
<tr><td><code id="distMatSP_+3A_ref">ref</code></td>
<td>
<p>The ref snowprofile object</p>
</td></tr>
<tr><td><code id="distMatSP_+3A_dims">dims</code></td>
<td>
<p>Character vector containing the layer properties to calculate the distance over. Currently implemented
are the properties <code>hardness</code>, <code>gtype</code>, <code>ddate</code>, <code>density</code>, <code>ogs</code>.</p>
</td></tr>
<tr><td><code id="distMatSP_+3A_weights">weights</code></td>
<td>
<p>Numeric vector of the same length as <code>dims</code> specifying the averaging weights to each element of dims.</p>
</td></tr>
<tr><td><code id="distMatSP_+3A_gtype_distmat">gtype_distMat</code></td>
<td>
<p>A symmetric <strong>distance</strong> scoring matrix provided as data.frame that stores information about
the distances between the encountered grain types of the provided profiles. Default is the corresponding distance
matrix of <a href="#topic+grainSimilarity_align">grainSimilarity_align</a>, cf. <a href="#topic+sim2dist">sim2dist</a>.</p>
</td></tr>
<tr><td><code id="distMatSP_+3A_preflayerweights">prefLayerWeights</code></td>
<td>
<p>A matrix similar to <code>gtype_distMat</code>, but storing weights for preferential layer matching,
e.g. defaults to <a href="#topic+layerWeightingMat">layerWeightingMat</a>; the higher the values for a given grain type pair, the more the algorithm will try to
match those layers above others. To turn weighting scheme off, set <code>prefLayerWeights = NA</code></p>
</td></tr>
<tr><td><code id="distMatSP_+3A_ddatenorm">ddateNorm</code></td>
<td>
<p>Normalize the deposition date distance by <code>ddateNorm</code> number of days. Numeric, default 5.</p>
</td></tr>
<tr><td><code id="distMatSP_+3A_windowfunction">windowFunction</code></td>
<td>
<p>a window function analogous to <a href="#topic+warpWindowSP">warpWindowSP</a> (Other compatible window functions can be
found in <a href="dtw.html#topic+dtwWindowingFunctions">dtw::dtwWindowingFunctions</a>.)</p>
</td></tr>
<tr><td><code id="distMatSP_+3A_top.down.mirroring">top.down.mirroring</code></td>
<td>
<p>Will the resulting distance matrix be used for top down alignments? i.e., do you want to mirror the
matrix about its anti-diagonal (top-left/bottom-right diagonal)?</p>
</td></tr>
<tr><td><code id="distMatSP_+3A_warn.if.na.in.distance.calc">warn.if.na.in.distance.calc</code></td>
<td>
<p>most dependent functions in this package should be able to deal with NA values encountered in distance
calculations. Set this argument to <code>TRUE</code> if you want to be warned anyways.</p>
</td></tr>
<tr><td><code id="distMatSP_+3A_...">...</code></td>
<td>
<p>arguments to the window function, e.g. <code>window.size</code>, <code>window.size.abs</code>, <code>ddate.window.size</code>, ...</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A distance matrix of dimension (n x m), where n, m are the number of layers in the query and ref, respectively.
</p>


<h3>Note</h3>

<p>For package developers: dot inputs to the function (i.e., <code>...</code>) also necessary to keep <a href="#topic+dtwSP">dtwSP</a> highly flexible
and customizable. Dot inputs may contain arguments that remain unused in this function.
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>See Also</h3>

<p><a href="#topic+resampleSPpairs">resampleSPpairs</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## call function with two snow profiles of unequal lengths, without using a window function:
dMat_noWindow &lt;- distMatSP(SPpairs$A_modeled, SPpairs$A_manual, windowFunction = NA)
graphics::image(dMat_noWindow, main = "Default distance matrix without a warping window")


## compute distance based on grain type alone,
## and additionally disable preferential layer matching:
dMat &lt;- distMatSP(SPpairs$A_modeled, SPpairs$A_manual, windowFunction = NA,
                  dims = "gtype", weights = 1, prefLayerWeights = NA)
graphics::image(dMat,
                main = "Only based on grain type, and without preferential layer matching")

## enable preferential layer matching:
dMat &lt;- distMatSP(SPpairs$A_modeled, SPpairs$A_manual, windowFunction = NA)
graphics::image(dMat,
                main = "... with preferential layer matching")


## using a warping window:
dMat &lt;- distMatSP(SPpairs$A_modeled, SPpairs$A_manual, window.size.abs = 50)
graphics::image(dMat, main = "... and superimposing an absolute warping window of 50 cm")

</code></pre>

<hr>
<h2 id='dtwSP'>Calculate DTW alignment of two snow profiles</h2><span id='topic+dtwSP'></span>

<h3>Description</h3>

<p>This is the core function of the package and allows to match layers between pairs of snow profiles to align them. It
provides a variety of options, where the default values represent a good starting point to the alignment of most generic
profiles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtwSP(
  query,
  ref,
  open.end = TRUE,
  checkGlobalAlignment = "auto",
  keep.internals = TRUE,
  step.pattern = symmetricP1,
  resamplingRate = 0.5,
  rescale2refHS = FALSE,
  bottom.up = TRUE,
  top.down = TRUE,
  nonMatchedSim = 0,
  nonMatchedThickness = 10,
  simType = "HerlaEtAl2021",
  apply_scalingFactor = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dtwSP_+3A_query">query</code></td>
<td>
<p>The query snow profile to be warped</p>
</td></tr>
<tr><td><code id="dtwSP_+3A_ref">ref</code></td>
<td>
<p>The reference snow profile to be warped against</p>
</td></tr>
<tr><td><code id="dtwSP_+3A_open.end">open.end</code></td>
<td>
<p>Is an open end alignment desired? Recommended if profiles will not be rescaled.</p>
</td></tr>
<tr><td><code id="dtwSP_+3A_checkglobalalignment">checkGlobalAlignment</code></td>
<td>
<p>Do you want to check whether a global alignment performs better (i.e.,
<code>open.end = FALSE</code>), and use the optimal one of the computed alignments? <code>'auto'</code> sets to <code>TRUE</code> if <code>simType == "HerlaEtAl2021"</code> and to <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="dtwSP_+3A_keep.internals">keep.internals</code></td>
<td>
<p>Append resampled and aligned snow profiles as well as internal parameters to the output object?</p>
</td></tr>
<tr><td><code id="dtwSP_+3A_step.pattern">step.pattern</code></td>
<td>
<p>The local slope constraint of the warping path, defaults to Sakoe-Chiba's symmetric pattern
described by a slope factor of P = 1, see <a href="dtw.html#topic+stepPattern">dtw::stepPattern</a></p>
</td></tr>
<tr><td><code id="dtwSP_+3A_resamplingrate">resamplingRate</code></td>
<td>
<p>Scalar, numeric resampling rate for a regular depth grid. If the profiles have been rescaled prior to calling this routine, set to <code>NA</code>.
To resample onto the smallest possible mutual (original, likely irregular) depth grid (see Details, bullet point 2.2), set to <code>'irregularInterfaces'</code>.</p>
</td></tr>
<tr><td><code id="dtwSP_+3A_rescale2refhs">rescale2refHS</code></td>
<td>
<p>Rescale the query snow height to match the ref snow height?</p>
</td></tr>
<tr><td><code id="dtwSP_+3A_bottom.up">bottom.up</code></td>
<td>
<p>Compute an open.end alignment from the ground upwards?</p>
</td></tr>
<tr><td><code id="dtwSP_+3A_top.down">top.down</code></td>
<td>
<p>Compute an open.end alignment from the snow surface downwards?</p>
</td></tr>
<tr><td><code id="dtwSP_+3A_nonmatchedsim">nonMatchedSim</code></td>
<td>
<p>Similarity value <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code> for non-matched layers, see <a href="#topic+simSP">simSP</a>. indifference = 0.5, penalty &lt; 0.5</p>
</td></tr>
<tr><td><code id="dtwSP_+3A_nonmatchedthickness">nonMatchedThickness</code></td>
<td>
<p>How strongly should the thicknesses of non-matched layers influence the resulting similarity of
the profiles? The smaller this (positive!) value, the more influence; and vice versa. See <a href="#topic+simSP">simSP</a> for more details.</p>
</td></tr>
<tr><td><code id="dtwSP_+3A_simtype">simType</code></td>
<td>
<p>the similarity between two profiles can be computed with different approaches, see <a href="#topic+simSP">simSP</a></p>
</td></tr>
<tr><td><code id="dtwSP_+3A_apply_scalingfactor">apply_scalingFactor</code></td>
<td>
<p>Setting for <a href="#topic+simSP">simSP</a> in case <code style="white-space: pre;">&#8288;simType == "layerwise&#8288;</code>.</p>
</td></tr>
<tr><td><code id="dtwSP_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code><a href="#topic+distMatSP">distMatSP</a></code>, and <code>dtw</code> e.g.
</p>

<ul>
<li> <p><code>dims</code>, <code>weights</code> (defaults specified in <code><a href="#topic+distMatSP">distMatSP</a></code>)
</p>
</li>
<li> <p><code>ddateNorm</code>, numeric, normalize deposition date (default specified in <code><a href="#topic+distMatSP">distMatSP</a></code>)
</p>
</li>
<li> <p><code>windowFunction</code>, default <code><a href="#topic+warpWindowSP">warpWindowSP</a></code>
</p>
</li>
<li> <p><code>window.size</code>, <code>window.size.abs</code>, <code>ddate.window.size</code> (defaults specified in <code><a href="#topic+warpWindowSP">warpWindowSP</a></code>)
</p>
</li>
<li> <p><code>gtype_distMat</code>, (default specified in <code><a href="#topic+distMatSP">distMatSP</a></code>), cf. e.g. <a href="#topic+grainSimilarity_align">grainSimilarity_align</a>
</p>
</li>
<li> <p><code>prefLayerWeights</code>, weighting matrix for preferential layer matching, e.g. <a href="#topic+layerWeightingMat">layerWeightingMat</a>
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>The individual steps of aligning snow profiles (which can all be managed from this function):
</p>

<ol>
<li><p> (optional) <strong>Rescale</strong> the profiles to the same height (cf., <a href="#topic+scaleSnowHeight">scaleSnowHeight</a>)
</p>
</li>
<li> <p><strong>Resample</strong> the profiles onto the same depth grid. 2 different approaches:
</p>

<ul>
<li><p> regular grid with a sampling rate that is provided by the user (recommended, cf., <a href="#topic+resampleSP">resampleSP</a>).
</p>
</li>
<li><p> irregular grid that includes all layer interfaces within the two profiles (i.e., set <code>resamplingRate = 'irregularInterfaces'</code>) (cf., <a href="#topic+resampleSPpairs">resampleSPpairs</a>)
</p>
</li></ul>

</li>
<li><p> Compute a weighted <strong>local cost matrix</strong> from multiple layer characteristics (cf., <a href="#topic+distMatSP">distMatSP</a>)
</p>
</li>
<li> <p><strong>Match the layers</strong> of the profiles with a call to dtw (eponymous R package)
</p>
</li>
<li><p> Align the profiles by <strong>warping</strong> the query profile onto the reference profile (cf., <a href="#topic+warpSP">warpSP</a>)
</p>
</li>
<li><p> (optional) If the function has been called with multiple different boundary conditions (global, top-down, or bottom-up alignments),
the optimal alignment as determined by <a href="#topic+simSP">simSP</a> or by the DTW distance will be returned.
</p>
</li>
<li><p> (optional) Compute a <strong>similarity score</strong> for the two profiles with <a href="#topic+simSP">simSP</a>
</p>
</li></ol>



<h3>Value</h3>

<p>An alignment object of class 'dtwSP' is returned. This is essentially a list with various information about the alignment.
If <code>keep.internals = TRUE</code>, the resampled snow profiles 'query', 'reference' and 'queryWarped', as well as the
'costMatrix' and 'directionMatrix' are elements of the returned object.
</p>


<h3>Note</h3>

<p>Furthermore, the alignment based on grain type information is currently only possible for specific grain types. These grain types
require a pre-defined distance or similarity, such as given by <a href="#topic+grainSimilarity_align">grainSimilarity_align</a>. If your profile contains other grain types,
you are required to define your custom <code>grainSimilarity</code> matrix.
</p>
<p>The package used to require re-scaling of the profiles to identical snow heights. This requirement has been removed in v1.1.0.
Profiles therefore can be resampled onto a regular grid, whilst keeping their original total snow heights. The alignment can
then be carried out bottom.up or top.down with a relative or absolute window size. If the profiles have different snow heights and a relative
window size is provided, the window size is computed using the larger snow height of the two profiles (e.g., Profile A HS 100 cm,
Profile B HS 80 cm; window.size = 0.3 translates to an effective window size of +/- 33 cm).
See examples for alignments without prior re-scaling.
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>References</h3>

<p>Herla, F., Horton, S., Mair, P., &amp; Haegeli, P. (2021). Snow profile alignment and similarity assessment for aggregating,
clustering, and evaluating of snowpack model output for avalanche forecasting. Geoscientific Model Development, 14(1), 239–258.
https://doi.org/10.5194/gmd-14-239-2021
</p>


<h3>See Also</h3>

<p><a href="#topic+plotSPalignment">plotSPalignment</a>, <a href="#topic+simSP">simSP</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Align a modeled and a manual snow profile, primarily based on default settings:
dtwAlignment &lt;- dtwSP(SPpairs$A_modeled, SPpairs$A_manual, open.end = FALSE)

## check out the resulting dtwSP alignment object:
summary(dtwAlignment)
plotSPalignment(dtwAlignment = dtwAlignment)
plotCostDensitySP(dtwAlignment)


## Align profiles from subsequent days without re-scaling them:
dtwAlignment &lt;- dtwSP(SPpairs$C_day3, SPpairs$C_day1, resamplingRate = 0.5, rescale2refHS = FALSE,
                      window.size.abs = 30)
## Note, per default both bottom.up and top.down alignments have been considered,
#  let's check out which one was suited better:
dtwAlignment$direction  # i.e., bottom up
## Check it out visually:
plotSPalignment(dtwAlignment = dtwAlignment,
                mainQu = "3 Days after...", mainRef = "...the reference profile.")
plotCostDensitySP(dtwAlignment, labelHeight = TRUE)

</code></pre>

<hr>
<h2 id='extractFromScoringMatrix'>Extract from Scoring matrix</h2><span id='topic+extractFromScoringMatrix'></span>

<h3>Description</h3>

<p>Vectorized function to efficiently extract elements from scoring matrix of type data.frame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extractFromScoringMatrix(
  ScoringFrame,
  grainType1,
  grainType2,
  profile_handle = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extractFromScoringMatrix_+3A_scoringframe">ScoringFrame</code></td>
<td>
<p>Scoring matrix of type data.frame (needs to be of symmetric, matrix like format)</p>
</td></tr>
<tr><td><code id="extractFromScoringMatrix_+3A_graintype1">grainType1</code></td>
<td>
<p>character vector (yes, vector!) of grain type contained in ScoringFrame</p>
</td></tr>
<tr><td><code id="extractFromScoringMatrix_+3A_graintype2">grainType2</code></td>
<td>
<p>same as <code>grainType1</code></p>
</td></tr>
<tr><td><code id="extractFromScoringMatrix_+3A_profile_handle">profile_handle</code></td>
<td>
<p>character or numeric handle that links a potential warning message to the set of grain types,
if an unknown grain type is encountered (must be of length = 1)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector of length <code>grainType1</code> with the elements of <code>ScoringFrame</code>
that are defined by <code>grainType1</code> and <code>grainType2</code>
</p>


<h3>Author(s)</h3>

<p>fherla
</p>

<hr>
<h2 id='flipLayers'>Flip snow profile layers top down</h2><span id='topic+flipLayers'></span>

<h3>Description</h3>

<p>Flip snow profile layers top down
</p>


<h3>Usage</h3>

<pre><code class='language-R'>flipLayers(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="flipLayers_+3A_x">x</code></td>
<td>
<p>snowprofile or snowprofileLayers object with layers to be flipped</p>
</td></tr>
</table>


<h3>Value</h3>

<p>same object with layers dataframe flipped upside down
</p>


<h3>Note</h3>

<p>only do that with a specific reason (better, don&quot;t do it!), as all functions with snowprofile objects are
designed to have the layers increase in height.
</p>

<hr>
<h2 id='grainSimilarity_align'>Grain Type similarity matrix for DTW alignments</h2><span id='topic+grainSimilarity_align'></span>

<h3>Description</h3>

<p>Get the relative similarity matrix of grain types as used for snow profile alignments. This similarity matrix
considers the formation and metamorphosis of grain types, as well as quirks of the SNOWPACK model. <br />
<a href="#topic+grainSimilarity_evaluate">grainSimilarity_evaluate</a> is an analogous matrix designed for assessing the similarity between two profiles, which
requires considering the resulting avalanche hazard implications of grain types. <br />
The domain is <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code> &mdash; <code>1</code> representing identical grain types. The column 'NA' can be used for unknown grain
types.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>grainSimilarity_align(triag = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grainSimilarity_align_+3A_triag">triag</code></td>
<td>
<p>Return a triangular matrix (TRUE, default) or a symmetric matrix (FALSE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame, either triangular or symmetric
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>See Also</h3>

<p><a href="#topic+grainSimilarity_evaluate">grainSimilarity_evaluate</a>, <a href="#topic+layerWeightingMat">layerWeightingMat</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## "similarity" matrix:
simMat &lt;- grainSimilarity_align()
print(simMat)

## equivalent "distance" matrix:
distMat &lt;- sim2dist(grainSimilarity_align())
print(distMat)

</code></pre>

<hr>
<h2 id='grainSimilarity_evaluate'>Grain type similarity matrix for evaluation purposes</h2><span id='topic+grainSimilarity_evaluate'></span>

<h3>Description</h3>

<p>Similar to <a href="#topic+grainSimilarity_align">grainSimilarity_align</a>, but designed for assessing the similarity between snow profiles based
on avalanche hazard relevant characteristics. To be used in combination with <a href="#topic+simSP">simSP</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>grainSimilarity_evaluate(triag = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grainSimilarity_evaluate_+3A_triag">triag</code></td>
<td>
<p>Return a triangular matrix (TRUE, default) or a symmetric matrix (FALSE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame, either triangular or symmetric
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
simMat &lt;- grainSimilarity_evaluate()
print(simMat)

</code></pre>

<hr>
<h2 id='hardnessDistance'>Difference in Hand Hardness</h2><span id='topic+hardnessDistance'></span>

<h3>Description</h3>

<p>Calculate the difference (i.e. distance) in hand hardness
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hardnessDistance(hardness1, hardness2, normalize = FALSE, absDist = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hardnessDistance_+3A_hardness1">hardness1</code></td>
<td>
<p>character or numeric hand hardness value (1D array)</p>
</td></tr>
<tr><td><code id="hardnessDistance_+3A_hardness2">hardness2</code></td>
<td>
<p>character or numeric hand hardness value (1D array)</p>
</td></tr>
<tr><td><code id="hardnessDistance_+3A_normalize">normalize</code></td>
<td>
<p>Should result be normalized? boolean, default False.</p>
</td></tr>
<tr><td><code id="hardnessDistance_+3A_absdist">absDist</code></td>
<td>
<p>Interested in absolute distance? default True.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric Hand Hardness Distance
</p>


<h3>Author(s)</h3>

<p>fherla
</p>

<hr>
<h2 id='interactiveAlignment'>Run interactive alignment app</h2><span id='topic+interactiveAlignment'></span>

<h3>Description</h3>

<p>This app allows to interactively explore the alignment of two snowprofiles, which are either
given as input to this function, or are uploaded to the app interactively as caaml files.
Example profiles are also provided in the app.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interactiveAlignment(query = NaN, ref = NaN)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interactiveAlignment_+3A_query">query</code></td>
<td>
<p>an optional query snowprofile</p>
</td></tr>
<tr><td><code id="interactiveAlignment_+3A_ref">ref</code></td>
<td>
<p>an optional reference snowprofile</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An interactive session will be started
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (FALSE){  # this example won't be started in package tests.

## start app and choose profiles from within the app:
interactiveAlignment()

## start app with package internal profile data (from `sarp.snowprofile`):
interactiveAlignment(query = SPpairs$A_modeled, ref = SPpairs$A_manual)

}

</code></pre>

<hr>
<h2 id='layerWeightingMat'>Weighting scheme for preferential layer matching</h2><span id='topic+layerWeightingMat'></span>

<h3>Description</h3>

<p>A matrix, of the same form as <a href="#topic+grainSimilarity_align">grainSimilarity_align</a>, but containing weighting coefficients for preferential layer
matching based on the grain types of the layers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layerWeightingMat(triag = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layerWeightingMat_+3A_triag">triag</code></td>
<td>
<p>Return a triangular matrix (TRUE, default) or a symmetric matrix (FALSE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame, either triangular or symmetric
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
weightsMat &lt;- layerWeightingMat()
print(weightsMat)

</code></pre>

<hr>
<h2 id='match_with_tolerance'>Match with numeric tolerance</h2><span id='topic+match_with_tolerance'></span>

<h3>Description</h3>

<p>Match with numeric tolerance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>match_with_tolerance(x, y, d = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="match_with_tolerance_+3A_x">x</code></td>
<td>
<p>numeric vector</p>
</td></tr>
<tr><td><code id="match_with_tolerance_+3A_y">y</code></td>
<td>
<p>numeric vector</p>
</td></tr>
<tr><td><code id="match_with_tolerance_+3A_d">d</code></td>
<td>
<p>numeric tolerance in form of digits</p>
</td></tr>
</table>


<h3>Value</h3>

<p>boolean vector equivalently to <a href="base.html#topic+match">match</a>
</p>

<hr>
<h2 id='medoidSP'>Find the medoid snow profile among a group of profiles</h2><span id='topic+medoidSP'></span>

<h3>Description</h3>

<p>Find the medoid snowprofile among a group of profiles, based on their pairwise dissimilarity. Either provide a list
of <code>snowprofile</code> objects, or a precomputed distance matrix. <br />
If you provide a list of profiles the profiles can optionally be rescaled and resampled before the distance matrix
for the medoid calculation is computed. When computing the distance matrix this routine calls <a href="#topic+distanceSP">distanceSP</a> for
<em>every possible pair</em> of profiles among the group. During that call the profile pair is aligned by <a href="#topic+dtwSP">dtwSP</a>
and the aligned pair is evaluated by <a href="#topic+simSP">simSP</a>.
Note that the number of possible profile pairs grows exponentially with the number of profiles in the group (i.e.,
O(n^2) calls, where n is the number of profiles in the group).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>medoidSP(
  profileList = NULL,
  rescale_resample = TRUE,
  retDistmat = FALSE,
  distmat = NULL,
  verbose = FALSE,
  resamplingRate = 0.5,
  progressbar = requireNamespace("progress", quietly = TRUE),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="medoidSP_+3A_profilelist">profileList</code></td>
<td>
<p>List of snowprofile objects</p>
</td></tr>
<tr><td><code id="medoidSP_+3A_rescale_resample">rescale_resample</code></td>
<td>
<p>Do you want to uniformly rescale and resample the set of profiles prior to calculating the distance matrix?</p>
</td></tr>
<tr><td><code id="medoidSP_+3A_retdistmat">retDistmat</code></td>
<td>
<p>Do you want to <em>return</em> the pairwise distance matrix?</p>
</td></tr>
<tr><td><code id="medoidSP_+3A_distmat">distmat</code></td>
<td>
<p>If you have a precalculated distance matrix, provide it here to compute the medoid on it.</p>
</td></tr>
<tr><td><code id="medoidSP_+3A_verbose">verbose</code></td>
<td>
<p>print pairwise distance matrix? default FALSE</p>
</td></tr>
<tr><td><code id="medoidSP_+3A_resamplingrate">resamplingRate</code></td>
<td>
<p>The resampling rate that is used for the whole set if <code>rescale_resample = TRUE</code></p>
</td></tr>
<tr><td><code id="medoidSP_+3A_progressbar">progressbar</code></td>
<td>
<p>Do you want to print a progress bar with recommended package &quot;progress&quot;?</p>
</td></tr>
<tr><td><code id="medoidSP_+3A_...">...</code></td>
<td>
<p>arguments passed to <a href="#topic+distanceSP">distanceSP</a> and then further to <a href="#topic+dtwSP">dtwSP</a></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the pairwise distance matrix is modified within the function call to represent a symmetric distance matrix.
That is,, however, not originally the case, since <code>dtwSP(A, B) != dtwSP(B, A)</code>. The matrix is therefore made symmetric by
setting the similarity between the profiles A and B to <code style="white-space: pre;">&#8288;max({dtwSP(A, B), dtwSP(B, A)})&#8288;</code>.
</p>


<h3>Value</h3>

<p>If <code>retDistmat = FALSE</code> return the (named) index of the medoid snow profile, otherwise return a list with the elements
<code>iMedoid</code> and <code>distmat</code>.
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>See Also</h3>

<p><a href="#topic+reScaleSampleSPx">reScaleSampleSPx</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>this_example_runs_about_5s &lt;- TRUE
if (!this_example_runs_about_5s) {  # exclude from cran checks

  ## take a list of profiles
  grouplist &lt;- SPgroup[1:4]
  plot(grouplist, SortMethod = 'unsorted', xticklabels = "originalIndices")

  ## calulate medoid profile
  idxMedoid &lt;- medoidSP(grouplist)
  representativeProfile &lt;- grouplist[[idxMedoid]]
  plot(representativeProfile, main = paste0("medoid (i.e., profile ", idxMedoid, ")"))

}
</code></pre>

<hr>
<h2 id='mergeIdentLayers'>Merge layers with identical properties</h2><span id='topic+mergeIdentLayers'></span>

<h3>Description</h3>

<p>Merge adjacent layers that have identical properties, such as grain type, hardness etc..
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mergeIdentLayers(x, properties = c("hardness", "gtype"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mergeIdentLayers_+3A_x">x</code></td>
<td>
<p>a snowprofile or snowprofileLayers object with <em>height</em> grid information</p>
</td></tr>
<tr><td><code id="mergeIdentLayers_+3A_properties">properties</code></td>
<td>
<p>a character array of layer properties that are considered when searching for identical layers
(e.g., <code>hardness</code>, <code>gtype</code>, ...)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new <code>snowprofileLayers</code> object will be returned with the dimensions <code>height</code>, <code>hardness</code>, <code>gtype</code> and any other
properties given in 'properties'. Depth and thickness information will be auto-calculated. For snowprofile objects, the
field 'changes' will be initialized or extended.
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Merge identical layers based on hardness and grain type:
fewerLayers &lt;- mergeIdentLayers(x = SPpairs$A_modeled, properties = c("hardness", "gtype"))
summary(SPpairs$A_modeled)[, c("hs", "nLayers")]
summary(fewerLayers)[, c("hs", "nLayers")]

## compare profile plots before and after merging (i.e., appear identical!)
opar &lt;- par(no.readonly =TRUE)
par(mfrow = c(1, 2))
plot(SPpairs$A_modeled, main = "original", ylab = "Snow height")
plot(fewerLayers, main = "merged layers", ylab = "Snow height")
par(opar)

</code></pre>

<hr>
<h2 id='ogsDistance'>Difference in layer ogs</h2><span id='topic+ogsDistance'></span>

<h3>Description</h3>

<p>Calculate the difference (i.e. distance) in layer ogs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ogsDistance(ogs1, ogs2, normalize = FALSE, absDist = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ogsDistance_+3A_ogs1">ogs1</code></td>
<td>
<p>numeric ogs values (1D array)</p>
</td></tr>
<tr><td><code id="ogsDistance_+3A_ogs2">ogs2</code></td>
<td>
<p>numeric ogs values (1D array)</p>
</td></tr>
<tr><td><code id="ogsDistance_+3A_normalize">normalize</code></td>
<td>
<p>Should result be normalized? boolean, default False.</p>
</td></tr>
<tr><td><code id="ogsDistance_+3A_absdist">absDist</code></td>
<td>
<p>Interested in absolute distance? default True.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric ogs distance
</p>


<h3>Author(s)</h3>

<p>pbillecocq
</p>

<hr>
<h2 id='plotCostDensitySP'>Plot alignment cost density and warping path</h2><span id='topic+plotCostDensitySP'></span>

<h3>Description</h3>

<p>Plot alignment cost density and warping path, optionally with the two snow profiles plotted in the margins
along the axes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotCostDensitySP(
  alignment,
  localCost = TRUE,
  labelHeight = FALSE,
  marginalPros = TRUE,
  pathCol = "black",
  target = FALSE,
  movingTarget = FALSE,
  tlty = "dotted",
  tlwd = 1.5,
  tcol = "black",
  tcex = 1.5,
  cex.lab = 1,
  xlab = NULL,
  ylab = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotCostDensitySP_+3A_alignment">alignment</code></td>
<td>
<p>object from <a href="#topic+dtwSP">dtwSP</a></p>
</td></tr>
<tr><td><code id="plotCostDensitySP_+3A_localcost">localCost</code></td>
<td>
<p>plot <em>local</em> cost matrix, otherwise plot accumulated global cost.</p>
</td></tr>
<tr><td><code id="plotCostDensitySP_+3A_labelheight">labelHeight</code></td>
<td>
<p>plot axes in units of height (cm) or in unitless (i.e., layer index).</p>
</td></tr>
<tr><td><code id="plotCostDensitySP_+3A_marginalpros">marginalPros</code></td>
<td>
<p>plot profiles in margins along the axes. default TRUE</p>
</td></tr>
<tr><td><code id="plotCostDensitySP_+3A_pathcol">pathCol</code></td>
<td>
<p>color of warping path</p>
</td></tr>
<tr><td><code id="plotCostDensitySP_+3A_target">target</code></td>
<td>
<p>draw horizontal &amp; vertical lines from matrix cells to corresponding layers in the (marginal) profiles.
Provide either a vector of length 1 (i.e., index of warping path) or length 2 (i.e., x, y coordinates in terms of
layer indices), or a matrix with 2 columns, specifying (x, y) if you desire multiple 'targets'</p>
</td></tr>
<tr><td><code id="plotCostDensitySP_+3A_movingtarget">movingTarget</code></td>
<td>
<p>Do you want to draw the warping path only partially, from the origin to the target cross?
Only possible if target cross is given as a scalar! default = FALSE (Useful to create GIF animations of a moving path)</p>
</td></tr>
<tr><td><code id="plotCostDensitySP_+3A_tlty">tlty</code></td>
<td>
<p>target lty</p>
</td></tr>
<tr><td><code id="plotCostDensitySP_+3A_tlwd">tlwd</code></td>
<td>
<p>target lwd</p>
</td></tr>
<tr><td><code id="plotCostDensitySP_+3A_tcol">tcol</code></td>
<td>
<p>target col</p>
</td></tr>
<tr><td><code id="plotCostDensitySP_+3A_tcex">tcex</code></td>
<td>
<p>target cex</p>
</td></tr>
<tr><td><code id="plotCostDensitySP_+3A_cex.lab">cex.lab</code></td>
<td>
<p>cex of axis labels (cf. <a href="graphics.html#topic+par">par</a>)</p>
</td></tr>
<tr><td><code id="plotCostDensitySP_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label to change default labeling</p>
</td></tr>
<tr><td><code id="plotCostDensitySP_+3A_ylab">ylab</code></td>
<td>
<p>y-axis label to change default labeling</p>
</td></tr>
<tr><td><code id="plotCostDensitySP_+3A_...">...</code></td>
<td>
<p>forwarded to <a href="graphics.html#topic+par">par</a></p>
</td></tr>
</table>


<h3>Note</h3>

<p>If you can't see the axis labels, try e.g., <code>par(oma = c(3, 3, 0, 0))</code> before calling the function. Note, there
seems to be a problem (only sometimes) with the left-hand labels that are for some reason not plotted parallel
to the axis. Also, the routine is not bulletproof with respect to drawing 'targets'. Apologies for any inconveniences!
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## first align profiles:
dtwAlignment &lt;- dtwSP(SPpairs$A_modeled, SPpairs$A_manual, open.end = FALSE)

## then plot cost density:
plotCostDensitySP(dtwAlignment)

## label height instead of layer index, and don't show warping path:
plotCostDensitySP(dtwAlignment, labelHeight = TRUE, pathCol = "transparent")

## draw lines to the cell that corresponds to the DH and SH layers
plotCostDensitySP(dtwAlignment, target = c(191, 208))

## "moving target", i.e., draw warping path only from origin to target:
plotCostDensitySP(dtwAlignment, target = 200, movingTarget = TRUE)
plotCostDensitySP(dtwAlignment, target = 266, movingTarget = TRUE)


## A cool GIF can be created from frames like those
create_GIF &lt;- FALSE
if (create_GIF){
  nPath &lt;- length(dtwAlignment$index1)
  resolution &lt;- 100  # i.e. super low, make value smaller for smoother GIF
  for (k in seq(1, nPath, by = resolution)) {
    plotCostDensitySP(dtwAlignment, target = k, movingTarget = TRUE)
  }
}

</code></pre>

<hr>
<h2 id='plotSPalignment'>Align and plot two snow profiles using DTW</h2><span id='topic+plotSPalignment'></span>

<h3>Description</h3>

<p>This is a plotting routine for the DTW alignment of two snow profiles. Either provide two snow profiles or
a <code>dtwSP</code> alignment object. Don't resize the figure, otherwise the plotted alignment segments will not be
in correct place anymore! If you need a specific figure size, use <code>grDevices::png</code> with a width/height aspect
ratio of about 5/3.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotSPalignment(
  query,
  ref,
  dtwAlignment = NULL,
  keep.alignment = FALSE,
  plot.costDensity = FALSE,
  plot.warpedQuery = TRUE,
  label.ddate = FALSE,
  segCol = "gray70",
  segLty = "dotted",
  segLwd = 1,
  segTidy = FALSE,
  segInd = TRUE,
  segEmph = NA,
  cex = 1,
  mainQu = "query",
  mainRef = "reference",
  mainQwarped = "warped query",
  emphasizeLayers_qu = FALSE,
  emphasizeLayers_ref = FALSE,
  failureLayers_qu = FALSE,
  failureLayers_qu_col = "red",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotSPalignment_+3A_query">query</code></td>
<td>
<p>The query snowprofile to be warped</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_ref">ref</code></td>
<td>
<p>The reference snowprofile to be warped against</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_dtwalignment">dtwAlignment</code></td>
<td>
<p><code>dtwSP</code> object (optional)</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_keep.alignment">keep.alignment</code></td>
<td>
<p>Return <code>dtwSP</code> object with resampled query, ref and warped query? boolean</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_plot.costdensity">plot.costDensity</code></td>
<td>
<p>First graph, <a href="#topic+plotCostDensitySP">plotCostDensitySP</a> with warping path? boolean, default = FALSE</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_plot.warpedquery">plot.warpedQuery</code></td>
<td>
<p>plot warped query additionally to query, ref and alignment segments? (i.e. three pane plot) boolean, default = TRUE</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_label.ddate">label.ddate</code></td>
<td>
<p>Label deposition date in profiles? (Only possible if <code>ddate</code> is given in 'dims', cf <a href="#topic+distMatSP">distMatSP</a>)</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_segcol">segCol</code></td>
<td>
<p>Color of alignment segments. Passed to <a href="grid.html#topic+gpar">gpar</a>, default = &quot;gray70&quot;</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_seglty">segLty</code></td>
<td>
<p>Linestyle of alignment segments. Passed to <a href="grid.html#topic+gpar">gpar</a>, default = &quot;dotted&quot;</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_seglwd">segLwd</code></td>
<td>
<p>Linewidth of alignment segments, default = 1</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_segtidy">segTidy</code></td>
<td>
<p>Tidy up alignment segments, if profiles have not been resampled? boolean,
default FALSE i.e. one segment line per (synthetic) layer interface -&gt; supports visual understanding of alignment, but is also often confusing
(segTidy currently only implemented for tidying up to gtype and hardness interfaces)</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_segind">segInd</code></td>
<td>
<p>Index vector of query layers that will get alignment segments drawn. Note, that the profiles might get resampled, so pre-calculate your correct indices!</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_segemph">segEmph</code></td>
<td>
<p>Index vector of query layers, the alignment segments of which will be emphasized (thick and red). Note, that the profiles might get resampled, so pre-calculate your correct indices!</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_cex">cex</code></td>
<td>
<p>font size, cf. <code>par</code></p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_mainqu">mainQu</code></td>
<td>
<p>subtitle for query subfigure</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_mainref">mainRef</code></td>
<td>
<p>subtitle for reference subfigure</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_mainqwarped">mainQwarped</code></td>
<td>
<p>subtitle for warped query subfigure</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_emphasizelayers_qu">emphasizeLayers_qu</code></td>
<td>
<p>emphasize Layers in query, see plot.snowprofile</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_emphasizelayers_ref">emphasizeLayers_ref</code></td>
<td>
<p>emphasize Layers in reference, see plot.snowprofile</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_failurelayers_qu">failureLayers_qu</code></td>
<td>
<p>draw arrow to failure layers (see plot.snowprofile)? provide height vector.</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_failurelayers_qu_col">failureLayers_qu_col</code></td>
<td>
<p>color of arrow(s) (individual color string or vector, see plot.snowprofile)</p>
</td></tr>
<tr><td><code id="plotSPalignment_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code><a href="#topic+distMatSP">distMatSP</a></code> and <code><a href="#topic+dtwSP">dtwSP</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>dtw object with the resampled '$query' and '$reference', as well as the warped query '$queryWarped'
(only if keep.alignment is TRUE)
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
plotSPalignment(SPpairs$B_modeled1, SPpairs$B_modeled2)

plotSPalignment(SPpairs$B_modeled1, SPpairs$B_modeled2, dims = c("gtype"), weights = c(1))

## alternatively keep alignment:
alignment &lt;- plotSPalignment(SPpairs$B_modeled1, SPpairs$B_modeled2, keep.alignment = TRUE)
print(paste("Similarity between profiles:", alignment$sim))

## alternatively, with precomputed alignment and emphasized layer matches:
dtwAlignment &lt;- dtwSP(SPpairs$A_modeled, SPpairs$A_manual, open.end = FALSE)
plotSPalignment(dtwAlignment = dtwAlignment, segEmph = c(190, 192))

## directly after plotting, add text to figure:
grid::grid.text("Profiles SPpairs$A (modeled/manual)", x = 0.5, y = 0.8,
                gp = grid::gpar(fontsize=12, col="grey"))


</code></pre>

<hr>
<h2 id='resampleSP'>Resample snowprofile</h2><span id='topic+resampleSP'></span>

<h3>Description</h3>

<p>Resample an individual snow profile onto a new depth-grid (i.e., height-grid).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>resampleSP(x, h = 0.5, n = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="resampleSP_+3A_x">x</code></td>
<td>
<p>snowprofile (or snowprofileLayers) object</p>
</td></tr>
<tr><td><code id="resampleSP_+3A_h">h</code></td>
<td>
<p>Sampling rate (i.e. constant depth increment) in centimeters, if given as scalar (default is 0.5 cm).
Layers smaller than the scalar h will not be resolved in the resampled profile.
Can also be a vector specifying the desired <em>height</em> grid of the resampled profile (useful for non-constant increments).
But, be WARNED, that a meaningless grid will produce colorful but senseless output!</p>
</td></tr>
<tr><td><code id="resampleSP_+3A_n">n</code></td>
<td>
<p>Number of layers in resampled profile (optional). <em>A given n will overrule a conflicting h!</em></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This routine alters how the layer information of snow profiles is <em>stored</em> without changing how the profiles appear.
Note, however, that only layer properties that are constant within the individual layers will be resampled:
i.e., <code>height</code>, <code>hardness</code>, <code>gtype</code>, <code>ddate</code> will be resampled. However, <code>temperature</code>, for example,
will not be resampled, because it is not constant within layers.
</p>


<h3>Value</h3>

<p>resampled snowprofile with the same metadata as x, but resampled &quot;layers&quot;. <strong>Note</strong> that only
the following layer properties will be resampled: <code>height</code>, <code>hardness</code>, <code>gtype</code>, <code>ddate</code>.
If input was a snowprofileLayers object, the output will be, too.
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>See Also</h3>

<p><a href="#topic+resampleSPpairs">resampleSPpairs</a>, <a href="#topic+mergeIdentLayers">mergeIdentLayers</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## (1) constant sampling rate of 1 cm:
profileResampled &lt;- resampleSP(SPpairs$A_modeled, h = 1.0)

## compare profile summary before and after resampling:
summary(SPpairs$A_modeled)[, c("hs", "nLayers")]
summary(profileResampled)[, c("hs", "nLayers", "changes")]
head(profileResampled$layers)

## compare profile plots before and after resampling (i.e., appear identical!)
opar &lt;- par(no.readonly=TRUE)
par(mfrow = c(1, 2))
plot(SPpairs$A_modeled, main = "original", ylab = "Snow height")
plot(profileResampled, main = "resampled", ylab = "Snow height")
par(opar)

## (2) resample to 150 layers:
profileResampled &lt;- resampleSP(SPpairs$A_manual, n = 150)
summary(profileResampled)[, c("hs", "nLayers", "changes")]
head(profileResampled$layers)


## (3) resample onto arbitrarily specified grid
## (issues a warning when the new-grid HS deviates too much from the original HS)
irregularGrid &lt;- c(2 + cumsum(c(0, c(10, 15, 5, 1, 3, 30, 50))), 120)
profileResampled &lt;- resampleSP(SPpairs$A_manual, h = irregularGrid)

</code></pre>

<hr>
<h2 id='resampleSPpairs'>Resample a pair of profiles</h2><span id='topic+resampleSPpairs'></span>

<h3>Description</h3>

<p>Resample a pair of (irregularly layered) profiles onto the smallest common height grid. To reduce data storage
this routine can be used to merge layers based on specified layer properties, if the input profiles
have been resampled earlier, or if due to other reasons existing layers in the individual profiles can be merged.
In summary, this routine alters how the layer information of snow profiles is <em>stored</em> without changing how the
profiles appear.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>resampleSPpairs(
  query,
  ref,
  mergeBeforeResampling = FALSE,
  dims = c("gtype", "hardness")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="resampleSPpairs_+3A_query">query</code></td>
<td>
<p>query snowprofile or snowprofileLayers object</p>
</td></tr>
<tr><td><code id="resampleSPpairs_+3A_ref">ref</code></td>
<td>
<p>reference snowprofile or snowprofileLayers object</p>
</td></tr>
<tr><td><code id="resampleSPpairs_+3A_mergebeforeresampling">mergeBeforeResampling</code></td>
<td>
<p>shall adjacent layers with identical layer properties be merged? (boolean)</p>
</td></tr>
<tr><td><code id="resampleSPpairs_+3A_dims">dims</code></td>
<td>
<p>layer properties to consider for a potential merging</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The smallest common height grid is found by
</p>

<ol>
<li><p> extract all unique layer interfaces in both profiles
</p>
</li>
<li><p> resample each profile with the above height grid, <br />
(!) but set all height values that exceed each's max snow height to that max snow height!
</p>
</li></ol>



<h3>Value</h3>

<p>a list with the resampled input objects under the entries <code>query</code> and <code>ref</code>.
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>See Also</h3>

<p><a href="#topic+resampleSP">resampleSP</a>, <a href="#topic+mergeIdentLayers">mergeIdentLayers</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## initial situation before mutual resampling:
## two profiles with different snow heights and different numbers of layers
summary(SPpairs$A_manual)[, c("hs", "nLayers")]
summary(SPpairs$A_modeled)[, c("hs", "nLayers")]
opar &lt;- par(no.readonly=TRUE)
par(mfrow = c(1, 2))
plot(SPpairs$A_manual, main = "Initial profiles before resampling",
     ylab = "Snow height", ymax = 272)
plot(SPpairs$A_modeled, ylab = "Snow height", ymax = 272)

## resampling:
resampledSPlist &lt;- resampleSPpairs(SPpairs$A_manual, SPpairs$A_modeled,
                                   mergeBeforeResampling = TRUE)

## two profiles with different snow heights and IDENTICAL numbers of layers
summary(resampledSPlist$query)[, c("hs", "nLayers")]
summary(resampledSPlist$ref)[, c("hs", "nLayers")]
plot(resampledSPlist$query, main = "Profiles after resampling",
     ylab = "Snow height", ymax = 272)
plot(resampledSPlist$ref, ylab = "Snow height", ymax = 272)
par(opar)

</code></pre>

<hr>
<h2 id='reScaleSampleSPx'>Rescale and resample a snow profile list</h2><span id='topic+reScaleSampleSPx'></span>

<h3>Description</h3>

<p>Rescale and resample all snow profiles provided in a list to an identical snow height and resampling rate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reScaleSampleSPx(SPx, resamplingRate = 0.5, scHeight = median, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reScaleSampleSPx_+3A_spx">SPx</code></td>
<td>
<p>list of <code>snowprofile</code> objects</p>
</td></tr>
<tr><td><code id="reScaleSampleSPx_+3A_resamplingrate">resamplingRate</code></td>
<td>
<p>resampling rate, units in centimeters</p>
</td></tr>
<tr><td><code id="reScaleSampleSPx_+3A_scheight">scHeight</code></td>
<td>
<p>a function that calculates the resulting height from the profiles, default <code>median</code></p>
</td></tr>
<tr><td><code id="reScaleSampleSPx_+3A_...">...</code></td>
<td>
<p>arguments passed on to the function provided in <code>scHeight</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the first entry <code style="white-space: pre;">&#8288;$set&#8288;</code> storing the rescaled and resampled profile list, the second entry
<code style="white-space: pre;">&#8288;$maxHS&#8288;</code> stores the maximum snow height found among the profiles
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## let's take the 'SPgroup' object as profile list
SPrr &lt;- reScaleSampleSPx(SPgroup)
print(paste0("max height before rescaling: ", SPrr$maxHS, " cm"))
print(paste0("rescaled height: ", SPrr$set[[1]]$hs, " cm"))
plot(SPrr$set, SortMethod = 'unsorted')

</code></pre>

<hr>
<h2 id='return_conceptually_similar_gtypes'>Return conceptually similar grain types</h2><span id='topic+return_conceptually_similar_gtypes'></span>

<h3>Description</h3>

<p>Note, use this function with care. It's a brief helper function for specific usage, not generally applicable!
It is, however, sometimes useful for backtracking layers, see <a href="#topic+backtrackLayers">backtrackLayers</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>return_conceptually_similar_gtypes(gt)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="return_conceptually_similar_gtypes_+3A_gt">gt</code></td>
<td>
<p>a single gtype</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector of similar gtypes
</p>


<h3>Examples</h3>

<pre><code class='language-R'>return_conceptually_similar_gtypes("SH")
return_conceptually_similar_gtypes("MFcr")
return_conceptually_similar_gtypes("RG")
</code></pre>

<hr>
<h2 id='rmZeroThicknessLayers'>Remove layers with a thickness of 'zero cm'</h2><span id='topic+rmZeroThicknessLayers'></span>

<h3>Description</h3>

<p>Find layers in a snow profile that are zero cm thick (i.e. height vector stays constant for some layers, even
though grain types or hardness may change). Then, either remove those layers, or reset them with the layer
characteristics of the lower adjacent (non-zero-thickness) layer. In the latter case (i.e., reset), the number of
layers won't change, but those non-zero thickness layers will be made ineffective.
This procedure is particularly necessary for warping snow profiles (cf., <a href="#topic+dtwSP">dtwSP</a>, <a href="#topic+warpSP">warpSP</a>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmZeroThicknessLayers(x, rm.zero.thickness = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmZeroThicknessLayers_+3A_x">x</code></td>
<td>
<p>A <code>snowprofile</code> or <code>snowprofileLayers</code> object</p>
</td></tr>
<tr><td><code id="rmZeroThicknessLayers_+3A_rm.zero.thickness">rm.zero.thickness</code></td>
<td>
<p>Want to remove zero-thickness layers from profile? boolean, default TRUE. If FALSE, those
zero-thickness layers will be reset to the lower adjacent (non-zero-thickness) layer; thus, the number of layers
won't be changed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A modified copy of the input object. For snowprofile objects, the field <code style="white-space: pre;">&#8288;$changes&#8288;</code> will be initialized or
extended.
</p>


<h3>Author(s)</h3>

<p>fherla
</p>

<hr>
<h2 id='sarp.snowprofile.alignment-package'>sarp.snowprofile.alignment: Snow Profile Alignment, Aggregation, and Clustering</h2><span id='topic+sarp.snowprofile.alignment'></span><span id='topic+sarp.snowprofile.alignment-package'></span>

<h3>Description</h3>

<p>Snow profiles describe the vertical (1D) stratigraphy of layered snow with different layer characteristics, such as grain type, hardness, deposition date, and many more. Hence, they represent a data format similar to multivariate time series containing categorical, ordinal, and numerical data types. Use this package to align snow profiles by matching their individual layers based on Dynamic Time Warping (DTW). The aligned profiles can then be assessed with an independent, global similarity measure that is geared towards avalanche hazard assessment. Finally, through exploiting data aggregation and clustering methods, the similarity measure provides the foundation for grouping and summarizing snow profiles according to similar hazard conditions. In particular, this package allows for averaging large numbers of snow profiles with DTW Barycenter Averaging and thereby facilitates the computation of individual layer distributions and summary statistics that are relevant for avalanche forecasting purposes. For more background information refer to Herla, Horton, Mair, and Haegeli (2021) <a href="https://doi.org/10.5194/gmd-14-239-2021">doi:10.5194/gmd-14-239-2021</a>, and Herla, Mair, and Haegeli (2022) <a href="https://doi.org/10.5194/tc-16-3149-2022">doi:10.5194/tc-16-3149-2022</a>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Florian Herla <a href="mailto:fherla@sfu.ca">fherla@sfu.ca</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Pascal Haegeli <a href="mailto:pascal_haegeli@sfu.ca">pascal_haegeli@sfu.ca</a>
</p>
</li>
<li><p> Simon Horton <a href="mailto:shorton@sfu.ca">shorton@sfu.ca</a>
</p>
</li>
<li><p> Paul Billecocq <a href="mailto:paul.billecocq@usherbrooke.ca">paul.billecocq@usherbrooke.ca</a>
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> SFU Avalanche Research Program [funder]
</p>
</li></ul>



<h3>See Also</h3>

<p>SPpairs, <a href="#topic+dtwSP">dtwSP</a>, <a href="#topic+averageSP">averageSP</a>
</p>

<hr>
<h2 id='scaleSnowHeight'>Scale total height of a snow profile</h2><span id='topic+scaleSnowHeight'></span>

<h3>Description</h3>

<p>Scale the snow height of a snow profile either (1) based on another profile, or (2) based on a provided (predetermined) snow height.
This function can therefore be used to scale two snow profiles to an identical snow height by scaling the height vector of the (query) profile
against the height vector of the (reference) profile.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scaleSnowHeight(query, ref = NA, height = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scaleSnowHeight_+3A_query">query</code></td>
<td>
<p>the query snow profile (whose height vector will be scaled)</p>
</td></tr>
<tr><td><code id="scaleSnowHeight_+3A_ref">ref</code></td>
<td>
<p>the reference snow profile (whose total snow height will be used as the reference height for the scaling)</p>
</td></tr>
<tr><td><code id="scaleSnowHeight_+3A_height">height</code></td>
<td>
<p>an optional reference height that can be given instead of the query profile</p>
</td></tr>
</table>


<h3>Value</h3>

<p>query profile with scaled height vector
</p>


<h3>Author(s)</h3>

<p>fherla
</p>

<hr>
<h2 id='sim2dist'>Convert 'similarity' matrix to 'distance' matrix</h2><span id='topic+sim2dist'></span>

<h3>Description</h3>

<p>Convert a 'similarity' matrix to 'distance' matrix. <em>Note</em> that the similarity must be normalized (i.e. within [0, 1])
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim2dist(SimMat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim2dist_+3A_simmat">SimMat</code></td>
<td>
<p>similarity matrix of type data.frame with ranges [0, 1]</p>
</td></tr>
</table>


<h3>Value</h3>

<p>copy of input data.frame with similarities inverted to distances (i.e. dist = 1 - sim)
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## the 'swissSimilarityMatrix' as similarity and as distance
graphics::image(as.matrix(swissSimilarityMatrix))
graphics::image(as.matrix(sim2dist(swissSimilarityMatrix)))

</code></pre>

<hr>
<h2 id='simSP'>Similarity measure between snow profile pairs</h2><span id='topic+simSP'></span>

<h3>Description</h3>

<p>This function calculates a similarity measure for two snow profiles
that have been aligned onto the same height grid (either through DTW or resampling).
If one profile contains more layers than the other one, the layers with a non-matched height
represent missing layers and will be treated accordingly.
The similarity measure is compatible with top-down alignments and is symmetric with respect to its inputs, i.e.
<code>simSP(P1, P2) == simSP(P2, P1)</code>. <strong>Several different approaches of computing the measure have been implemented by now,
see Details below.</strong>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simSP(
  ref,
  qw,
  gtype_distMat = sim2dist(grainSimilarity_evaluate(triag = FALSE)),
  type = "HerlaEtAl2021",
  nonMatchedSim = 0,
  nonMatchedThickness = 10,
  verbose = FALSE,
  returnDF = FALSE,
  apply_scalingFactor = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simSP_+3A_ref">ref</code></td>
<td>
<p>snowprofile object 1</p>
</td></tr>
<tr><td><code id="simSP_+3A_qw">qw</code></td>
<td>
<p>snowprofile object 2 (matched layers need to be on the same height grid of ref)</p>
</td></tr>
<tr><td><code id="simSP_+3A_gtype_distmat">gtype_distMat</code></td>
<td>
<p>a distance matrix that stores <strong>distance</strong> information of grain types (<em>Be careful</em> to convert
similarities, as in <a href="#topic+grainSimilarity_evaluate">grainSimilarity_evaluate</a>, into dissimilarities with <a href="#topic+sim2dist">sim2dist</a>.)</p>
</td></tr>
<tr><td><code id="simSP_+3A_type">type</code></td>
<td>
<p>the similarity measure can be computed in several different ways (of sophistication). See Details section.
Possible choices
</p>

<ul>
<li> <p><code>simple</code>
</p>
</li>
<li> <p><code>HerlaEtAl2021</code> (= <code>simple2</code>)
</p>
</li>
<li> <p><code>tsa_WLdetection</code> &amp; <code>rta_WLdetection</code>
</p>
</li>
<li> <p><code>layerwise</code> &amp; <code>rta_scaling</code>
</p>
</li>
<li> <p><code>remotesensing</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="simSP_+3A_nonmatchedsim">nonMatchedSim</code></td>
<td>
<p>sets the similarity value of non-matched layers <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>. &quot;indifference&quot; = 0.5, penalty &lt; 0.5.
Note that <a href="#topic+dtwSP">dtwSP</a> sets the same value and overrides the default value in this function!</p>
</td></tr>
<tr><td><code id="simSP_+3A_nonmatchedthickness">nonMatchedThickness</code></td>
<td>
<p>If <code>NA</code>, every unique non-matched layer (i.e., contiguous resampled layers with identical properties)
contributes to the overall similarity by 1 x <code>nonMatchedSim</code>. In that case, 5cm of non-matched new snow has the same effect on
the overall similarity as 50cm of non-matched new snow. To make the effect of non-matched layers dependent on the layer thickness,
provide a positive number to <code>nonMatchedThickness</code>. For <code>nonMatchedThickness = 10</code>, every 10cm of a unique non-matched layer
contribute to the overall similarity by 1 x <code>nonMatchedSim</code>. So, 50cm of non-matched new snow would contribute 5 times stronger
than 5cm of non-matched new snow.
Note that <a href="#topic+dtwSP">dtwSP</a> sets the same value and overrides the default value in this function!</p>
</td></tr>
<tr><td><code id="simSP_+3A_verbose">verbose</code></td>
<td>
<p>print similarities of different grain classes to console? default FALSE</p>
</td></tr>
<tr><td><code id="simSP_+3A_returndf">returnDF</code></td>
<td>
<p>additionally return the similarities of the grain classes as data.frame (analogously to verbose);
the return object then has the fields <code style="white-space: pre;">&#8288;$sim&#8288;</code> and <code style="white-space: pre;">&#8288;$simDF&#8288;</code></p>
</td></tr>
<tr><td><code id="simSP_+3A_apply_scalingfactor">apply_scalingFactor</code></td>
<td>
<p>Only applicable to <code>type = layerwise</code>: <code>TRUE</code> or <code>FALSE</code>, see Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The first several implementation types (<strong>simple</strong>, <strong>HerlaEtAl2021</strong>, <strong>tsa_WLdetection</strong>, <strong>rta_WLdetection</strong>) represent different flavors of the approach detailed in
Herla et al (2021). In essence, they are a simple approach to incorporate avalanche hazard relevant characteristics into the score by
computing the score as arithmetic mean of 4 different grain type classes:
</p>

<ul>
<li><p> weak layers (wl): SH and DH
</p>
</li>
<li><p> new snow (pp): PP and DF
</p>
</li>
<li><p> crusts (cr): MFcr and IF
</p>
</li>
<li><p> bulk: the rest (i.e., predominantly RG, FC, FCxr &mdash; MF falls also in here, will maybe be adjusted in future.)
</p>
</li></ul>

<p>Additionally, for classes wl and cr, vertical windows are computed to weigh layers more heavily that have no other wl or cr
grain types in their neighborhood.
</p>
<p>Type <strong>simple</strong> deviates from <em>simple2</em> (= <em>HerlaEtAl2021</em>) by computing the aforementioned vertical windows based on heuristic
depth ranges (i.e., Surface&ndash;30cm depth&ndash;80cm depth&ndash;150cm depth&ndash;Ground). It is otherwise identical to the <strong>simple2</strong>
type, which computes as many numbers of equidistant vertical windows as number of wl or cr are present in the profile.
</p>
<p>Type <strong>tsa_WLdetection</strong> employs a similar approach as <em>simple</em>, but it identifies weak layers (wl) based on the Threshold Sum Approach
(&gt;= 5 TSA, lemons, German 'Nieten'). Therefore, the original profiles need to contain grain size information, which allows you to pre-compute the lemons
for all layers (additionally to the otherwise
necessary gain type and hardness information). It is thus more targeted to simulated profiles or detailed manual profiles of very high quality.
While the former two types neglect hardness information of wl and cr classes, this type does not.
Type <strong>rta_WLdetection</strong> works analogous, but uses RTA instead of TSA and a threshold of &gt;= 0.8.
</p>
<p>Unlike the former types, <strong>layerwise</strong> applies no weighting at all if used as per default. That means that the similarity of each individual layer
contributes equally to the overall similarity measure. It is, however, very flexible in that any custom scaling factor can be applied to each layer. The resulting similarity score is then computed by
</p>

<ul>
<li><p> simSP = sum(sim * scalingFactor) / sum(scalingFactor),
</p>
</li></ul>

<p>where the denominator ensures that the resulting score will be within <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>. If you want to explore your own scaling approach,
both input snow profiles need to contain a column called <code style="white-space: pre;">&#8288;$layers$scalingFactor&#8288;</code> that store the desired factor.
Type <strong>rta_scaling</strong> is a special case of <code>layerwise</code>, where the scaling is determined by the relative lemons of each layer (RTA, see Monti et al 2013).
Type <strong>remotesensing</strong> makes use of the layerwise algorithm, but triggers an alternative similarity computation beforehand. Similarity is first computed from density and Optical Grain Size (ogs),
and then the layerwise similarity is called upon to compute the global sim score.
</p>
<p><strong>NOTE</strong> that for all types that include TSA/RTA values, these values need to be computed <em>prior to aligning</em> the profiles
(and therefore need to be present in the profiles provided to this function!)
</p>


<h3>Value</h3>

<p>Either a scalar similarity between <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code> with 1 referring to the two profiles being identical, or
(if <code>returnDF</code> is TRUE) a list with the elements <code style="white-space: pre;">&#8288;$sim&#8288;</code> and <code style="white-space: pre;">&#8288;$simDF&#8288;</code>.
</p>


<h3>References</h3>

<p>Herla, F., Horton, S., Mair, P., &amp; Haegeli, P. (2021). Snow profile alignment and similarity assessment for aggregating, clustering,
and evaluating of snowpack model output for avalanche forecasting. Geoscientific Model Development, 14(1), 239–258. https://doi.org/10.5194/gmd-14-239-2021
</p>
<p>Monti, F., &amp; Schweizer, J. (2013). A relative difference approach to detect potential weak layers within a snow profile.
Proceedings of the 2013 International Snow Science Workshop, Grenoble, France, 339–343. Retrieved from https://arc.lib.montana.edu/snow-science/item.php?id=1861
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## first align two profiles, then assess the similarity of the aligned profiles
alignment &lt;- dtwSP(SPpairs$A_modeled, SPpairs$A_manual)
SIM &lt;- simSP(alignment$queryWarped, alignment$reference, verbose = TRUE)

## similarity of identical profiles
SIM &lt;- simSP(alignment$queryWarped, alignment$queryWarped, verbose = TRUE)

## non-matched layers become apparent here:
alignment &lt;- plotSPalignment(SPpairs$C_day1, SPpairs$C_day2, keep.alignment = TRUE,
                             rescale2refHS = FALSE, checkGlobalAlignment = FALSE)
simSP(alignment$queryWarped, alignment$reference, nonMatchedSim = 0.5)
## smaller similarity score due to 'penalty' of non-matched layers:
simSP(alignment$queryWarped, alignment$reference, nonMatchedSim = 0)
## even smaller similarity score due to higher impact of non-matched layer thickness:
simSP(alignment$queryWarped, alignment$reference, nonMatchedSim = 0, nonMatchedThickness = 1)

## detect WL based on lemons (instead of grain type alone):
P1 &lt;- computeTSA(SPpairs$D_generalAlignment1)
P2 &lt;- computeTSA(SPpairs$D_generalAlignment2)
alignment &lt;- dtwSP(P1, P2, simType = "tsa_wldetection")
# sim based on WL-detection with TSA:
simSP(alignment$queryWarped, alignment$reference, type = "tsa_wldetection", verbose = TRUE)
# sim solely based on grain type, neglecting TSA information
simSP(alignment$queryWarped, alignment$reference, type = "simple", verbose = TRUE)

## RTA scaling type
P1 &lt;- computeRTA(P1)
P2 &lt;- computeRTA(P2)
alignment &lt;- dtwSP(P1, P2, simType = "rta_scaling")
# sim based on scaling with RTA
simSP(alignment$queryWarped, alignment$reference, type = "rta_scaling")
# sim based on WL-detection with RTA
simSP(alignment$queryWarped, alignment$reference, type = "rta_wldetection")
# sim based on WL-detection with TSA
simSP(alignment$queryWarped, alignment$reference, type = "tsa_wldetection")

## layerwise similarity (i) unscaled...
simSP(alignment$queryWarped, alignment$reference, type = "layerwise", verbose = TRUE)

##... or (ii) with custom scaling factor (example only illustrative)
alignment$queryWarped$layers$scalingFactor &lt;- 0.1
alignment$queryWarped$layers$scalingFactor[findPWL(alignment$queryWarped)] &lt;- 1
alignment$reference$layers$scalingFactor &lt;- 0.1
alignment$reference$layers$scalingFactor[findPWL(alignment$reference)] &lt;- 1
simSP(alignment$queryWarped, alignment$reference, type = "layerwise",
      apply_scalingFactor = TRUE, verbose = TRUE)

</code></pre>

<hr>
<h2 id='SPgroup2'>Additional example set of snow profiles</h2><span id='topic+SPgroup2'></span>

<h3>Description</h3>

<p>Additional example set of snow profiles. The main difference to the example data set SPgroup is that <code>SPgroup2</code> contains various different stability indices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SPgroup2
</code></pre>


<h3>Format</h3>

<p>A snowprofileSet
</p>


<h3>See Also</h3>

<p>SPgroup
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
plot(SPgroup2, SortMethod = "unsorted")

</code></pre>

<hr>
<h2 id='SPspacetime'>Additional example set of snow profiles</h2><span id='topic+SPspacetime'></span>

<h3>Description</h3>

<p>Additional example set of 4 spatially distributed snow profiles for 5 consecutive days, also containing different stability indices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SPspacetime
</code></pre>


<h3>Format</h3>

<p>A snowprofileSet
</p>


<h3>See Also</h3>

<p><a href="#topic+SPgroup2">SPgroup2</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
plot(SPspacetime, SortMethod = "elev")

</code></pre>

<hr>
<h2 id='swissSimilarityMatrix'>Similarity Matrix of Snow Grain Types</h2><span id='topic+swissSimilarityMatrix'></span>

<h3>Description</h3>

<p>as defined by Lehning et al (2001). A similarity of 1 represents identity, 0 represents total dissimilarity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>swissSimilarityMatrix
</code></pre>


<h3>Format</h3>

<p>A data.frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
print(swissSimilarityMatrix)

</code></pre>

<hr>
<h2 id='warpSP'>Warp one snow profile onto another one</h2><span id='topic+warpSP'></span>

<h3>Description</h3>

<p>After the DTW alignment of two profiles, the maps between the two profiles can be used to
warp one profile onto the other profile. In other words, the layer thicknesses of the warped profile
are adjusted to optimally align with the corresponding layers of the other profile.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>warpSP(alignment, whom = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="warpSP_+3A_alignment">alignment</code></td>
<td>
<p>DTW alignment object from <a href="#topic+dtwSP">dtwSP</a> containing the two profiles (i.e., called <code>dtwSP(..., keep.internals = TRUE)</code>)</p>
</td></tr>
<tr><td><code id="warpSP_+3A_whom">whom</code></td>
<td>
<p>whom to warp? &quot;query&quot; (= &quot;jmin&quot;), &quot;imin&quot;, &quot;queryTopDown&quot; (= &quot;jminTopDown&quot;), &quot;iminTopDown&quot;, &quot;ref&quot;;
if 'NA' the routine determines that itself from the structure of the alignment object. (see Details)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>After this procedure, the thickness of some layers can be zero, which leads to the layers disappearing.
</p>
<p>This function is automatically called in <code>dtwSP(..., keep.internals = TRUE)</code> to warp the query profile
onto the reference profile.
</p>
<p><em>Whom</em> to warp: There exist 8 different options, 4 for warping the query onto the ref and 4 for vice versa.
The 4 options for warping the query onto the ref are:
</p>

<ul>
<li><p> global alignment / partial alignment where entire query is matched to subsequence of ref (&quot;jmin&quot;)
</p>
</li>
<li><p> partial alignment where entire ref is matched to subsequence of query (&quot;imin&quot;)
</p>
</li>
<li><p> partial top down alignment where entire query is matched to subsequence of ref (&quot;jminTopDown&quot;)
</p>
</li>
<li><p> partial top down alignment where entire ref is matched to subsequence of query (&quot;iminTopDown&quot;)
</p>
</li></ul>

<p>For the other case, warping the ref onto the query, only the equivalent of the first option is implemented.
</p>
<p>For developers: Including new variables in the output of warped profiles can easily be done by inserting a respective command
at the end of this function. There are many example variables added already.
</p>


<h3>Value</h3>

<p>Returns the input alignment object including the element alignment$queryWarped (or $referenceWarped),
which are the warped snow profiles. The class of the alignment object is altered to &quot;dtwSP&quot;, but still inherits &quot;dtw&quot;.
</p>


<h3>Author(s)</h3>

<p>fherla
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## first align profiles
alignment &lt;- dtwSP(SPpairs$A_modeled, SPpairs$A_manual, open.end = FALSE)

## warp reference profile onto query profile:
refWarped &lt;- warpSP(alignment, whom = "ref")$referenceWarped
opar &lt;- par(no.readonly =TRUE)
par(mfrow = c(1, 2))
plot(alignment$query, main = "query")
plot(refWarped, main = "warped reference")
par(opar)

</code></pre>

<hr>
<h2 id='warpWindowSP'>Restrict the DTW warping window for snow profiles alignment</h2><span id='topic+warpWindowSP'></span>

<h3>Description</h3>

<p>Given a matrix, this function sets all elements of the matrix that are outside the so-called warping
window to <code>NA</code>. The warping window is a slanted band of constant width around the main diagonal
(i.e., <em>Sakoe-Chiba</em>-band), and it's size can be controlled with function arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>warpWindowSP(
  iw,
  jw,
  iheight,
  jheight,
  iddate,
  jddate,
  profile.size,
  profile.height,
  window.size = 0.3,
  window.size.abs = NA,
  ddate.window.size = Inf,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="warpWindowSP_+3A_iw">iw</code></td>
<td>
<p>matrix of integers indicating their row number (cf., <code>?row</code>)</p>
</td></tr>
<tr><td><code id="warpWindowSP_+3A_jw">jw</code></td>
<td>
<p>matrix of integers indicating their column number (cf., <code>?col</code>)</p>
</td></tr>
<tr><td><code id="warpWindowSP_+3A_iheight">iheight</code></td>
<td>
<p>matrix of query height filled into the columns of the matrix</p>
</td></tr>
<tr><td><code id="warpWindowSP_+3A_jheight">jheight</code></td>
<td>
<p>matrix of ref height filled into the rows of the matrix</p>
</td></tr>
<tr><td><code id="warpWindowSP_+3A_iddate">iddate</code></td>
<td>
<p>same as iheight, but containing deposition date information (i.e., POSIXct data converted to numeric through matrix call!)</p>
</td></tr>
<tr><td><code id="warpWindowSP_+3A_jddate">jddate</code></td>
<td>
<p>same as jheight, but containing deposition date information (i.e., POSIXct data converted to numeric through matrix call!)</p>
</td></tr>
<tr><td><code id="warpWindowSP_+3A_profile.size">profile.size</code></td>
<td>
<p>number of layers in the longer one of the two profiles (scalar)</p>
</td></tr>
<tr><td><code id="warpWindowSP_+3A_profile.height">profile.height</code></td>
<td>
<p>snow height of the deeper one of the two profiles (scalar)</p>
</td></tr>
<tr><td><code id="warpWindowSP_+3A_window.size">window.size</code></td>
<td>
<p>percentage of profile.size or profile.height defining the size of the warping window
(i.e., the most restrictive of the two will be applied)</p>
</td></tr>
<tr><td><code id="warpWindowSP_+3A_window.size.abs">window.size.abs</code></td>
<td>
<p>Instead of a <code>window.size</code> percentage, an absolute value (in <em>cm</em>!) can be provided</p>
</td></tr>
<tr><td><code id="warpWindowSP_+3A_ddate.window.size">ddate.window.size</code></td>
<td>
<p>number of days that exclude layers from the warping window if their deposition dates
differ by more than these days</p>
</td></tr>
<tr><td><code id="warpWindowSP_+3A_...">...</code></td>
<td>
<p>unused&mdash;but important to be able to provide other warping functions to <a href="#topic+distMatSP">distMatSP</a></p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="dtw.html#topic+dtwWindowingFunctions">dtw::dtwWindowingFunctions</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
