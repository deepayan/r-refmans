<!DOCTYPE html><html lang="en"><head><title>Help for package flevr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {flevr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#biomarkers'><p>Example biomarker data</p></a></li>
<li><a href='#extract_importance_glm'><p>Extract the learner-specific importance from a glm object</p></a></li>
<li><a href='#extract_importance_glmnet'><p>Extract the learner-specific importance from a glmnet object</p></a></li>
<li><a href='#extract_importance_mean'><p>Extract the learner-specific importance from a mean object</p></a></li>
<li><a href='#extract_importance_polymars'><p>Extract the learner-specific importance from a polymars object</p></a></li>
<li><a href='#extract_importance_ranger'><p>Extract the learner-specific importance from a ranger object</p></a></li>
<li><a href='#extract_importance_SL'><p>Extract extrinsic importance from a Super Learner object</p></a></li>
<li><a href='#extract_importance_SL_learner'><p>Extract the learner-specific importance from a fitted SuperLearner algorithm</p></a></li>
<li><a href='#extract_importance_svm'><p>Extract the learner-specific importance from an svm object</p></a></li>
<li><a href='#extract_importance_xgboost'><p>Extract the learner-specific importance from an xgboost object</p></a></li>
<li><a href='#extrinsic_selection'><p>Perform extrinsic, ensemble-based variable selection</p></a></li>
<li><a href='#flevr'><p>flevr: Flexible, Ensemble-Based Variable Selection with Potentially Missing Data</p></a></li>
<li><a href='#get_augmented_set'><p>Get an augmented set based on the next-most significant variables</p></a></li>
<li><a href='#get_base_set'><p>Get an initial selected set based on intrinsic importance and a base method</p></a></li>
<li><a href='#intrinsic_control'><p>Control parameters for intrinsic variable selection</p></a></li>
<li><a href='#intrinsic_selection'><p>Perform intrinsic, ensemble-based variable selection</p></a></li>
<li><a href='#pool_selected_sets'><p>Pool selected sets from multiply-imputed data</p></a></li>
<li><a href='#pool_spvims'><p>Pool SPVIM Estimates Using Rubin's Rules</p></a></li>
<li><a href='#SL_stabs_fitfun'><p>Wrapper for using Super Learner-based extrinsic selection within stability selection</p></a></li>
<li><a href='#SL.ranger.imp'><p>Super Learner wrapper for a ranger object with variable importance</p></a></li>
<li><a href='#spvim_vcov'><p>Extract a Variance-Covariance Matrix for SPVIM Estimates</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Flexible, Ensemble-Based Variable Selection with Potentially
Missing Data</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.4</td>
</tr>
<tr>
<td>Description:</td>
<td>Perform variable selection in settings with possibly missing data
    based on extrinsic (algorithm-specific) and intrinsic (population-level)
    variable importance. Uses a Super Learner ensemble to estimate the
    underlying prediction functions that give rise to estimates of variable importance. For more information about the methods, please see Williamson and Huang (2023+) &lt;<a href="https://doi.org/10.48550/arXiv.2202.12989">doi:10.48550/arXiv.2202.12989</a>&gt;.</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>SuperLearner, dplyr, magrittr, tibble, caret, mvtnorm,
kernlab, rlang, ranger</td>
</tr>
<tr>
<td>Suggests:</td>
<td>vimp, stabs, testthat, knitr, rmarkdown, mice, xgboost,
glmnet, polspline</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/bdwilliamson/flevr">https://github.com/bdwilliamson/flevr</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/bdwilliamson/flevr/issues">https://github.com/bdwilliamson/flevr/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-29 19:23:19 UTC; L107067</td>
</tr>
<tr>
<td>Author:</td>
<td>Brian D. Williamson
    <a href="https://orcid.org/0000-0002-7024-548X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Brian D. Williamson &lt;brian.d.williamson@kp.org&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-30 10:20:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='biomarkers'>Example biomarker data</h2><span id='topic+biomarkers'></span>

<h3>Description</h3>

<p>A dataset inspired by data collected by the Early Detection Research
Network (EDRN). Biomarkers developed at six &quot;labs&quot; are validated at
at least one of four &quot;validation sites&quot; on 306 cysts. The data also include two binary outcome variables: whether or not the cyst was classified as mucinous,
and whether or not the cyst was determined to have high malignant potential.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>biomarkers
</code></pre>


<h3>Format</h3>

<p><code>biomarkers</code>: a tibble with 306 rows and 24 columns, where the first
column is the validation site, the next two columns are the possible outcomes,
and the remaining columns are the biomarkers:
</p>

<dl>
<dt>institution</dt><dd><p>the validation site</p>
</dd>
<dt>mucinous</dt><dd><p>a binary indicator of whether the cyst was classified as mucinous</p>
</dd>
<dt>high_malignancy</dt><dd><p>a binary indicator of whether the cyst was classified as having high malignant potential</p>
</dd>
<dt>lab1_actb</dt><dd><p>a biomarker</p>
</dd>
<dt>lab1_molecules_score</dt><dd><p>a biomarker</p>
</dd>
<dt>lab1_telomerase_score</dt><dd><p>a biomarker</p>
</dd>
<dt>lab2_fluorescence_score</dt><dd><p>a biomarker</p>
</dd>
<dt>lab3_muc3ac_score</dt><dd><p>a biomarker</p>
</dd>
<dt>lab3_muc5ac_score</dt><dd><p>a biomarker</p>
</dd>
<dt>lab4_areg_score</dt><dd><p>a biomarker</p>
</dd>
<dt>lab4_glucose_score</dt><dd><p>a biomarker</p>
</dd>
<dt>lab5_mucinous_call</dt><dd><p>a biomarker (binary)</p>
</dd>
<dt>lab5_neoplasia_v1_call</dt><dd><p>a biomarker (binary)</p>
</dd>
<dt>lab5_neoplasia_v2_call</dt><dd><p>a biomarker (binary)</p>
</dd>
<dt>lab6_ab_score</dt><dd><p>a biomarker</p>
</dd>
<dt>cea</dt><dd><p>a biomarker</p>
</dd>
<dt>lab1_molecules_neoplasia_call</dt><dd><p>binary indicator of whether <code>lab1_molecules_score</code> &gt; 25</p>
</dd>
<dt>lab1_telomerase_neoplasia_call</dt><dd><p>binary indicator of whether <code>lab1_telomerase_score</code> &gt; 730</p>
</dd>
<dt>lab2_fluorescence_mucinous_call</dt><dd><p>binary indicator of whether <code>lab2_fluorescence_score</code> &gt; 1.23</p>
</dd>
<dt>lab4_areg_mucinous_call</dt><dd><p>binary indicator of whether <code>lab4_areg_score</code> &gt; 112</p>
</dd>
<dt>lab4_glucose_mucinous_call</dt><dd><p>binary indicator of whether <code>lab4_glucose_score</code> &lt; 50</p>
</dd>
<dt>lab4_combined_mucinous_call</dt><dd><p>binary indicator of whether <code>lab4_areg_score</code> &gt; 112 and <code>lab4_glucose_score</code> &lt; 50</p>
</dd>
<dt>lab6_ab_neoplasia_call</dt><dd><p>binary indicator of whether <code>lab6_ab_score</code> &gt; 0.104</p>
</dd>
<dt>cea_call</dt><dd><p>binary indicator of whether <code>cea</code> &gt; 192</p>
</dd>
</dl>



<h3>Source</h3>

<p>Inspired by data collected by the EDRN <a href="https://edrn.nci.nih.gov/">https://edrn.nci.nih.gov/</a>.
</p>

<hr>
<h2 id='extract_importance_glm'>Extract the learner-specific importance from a glm object</h2><span id='topic+extract_importance_glm'></span>

<h3>Description</h3>

<p>Extract the individual-algorithm extrinsic importance from a glm object,
along with the importance rank.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_importance_glm(fit = NULL, feature_names = "", coef = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extract_importance_glm_+3A_fit">fit</code></td>
<td>
<p>the <code>glm</code> object.</p>
</td></tr>
<tr><td><code id="extract_importance_glm_+3A_feature_names">feature_names</code></td>
<td>
<p>the feature names</p>
</td></tr>
<tr><td><code id="extract_importance_glm_+3A_coef">coef</code></td>
<td>
<p>the Super Learner coefficient associated with the learner.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble, with columns <code>algorithm</code> (the fitted algorithm),
<code>feature</code> (the feature), <code>importance</code> (the algorithm-specific
extrinsic importance of the feature), <code>rank</code> (the feature importance
rank, with 1 indicating the most important feature), and <code>weight</code>
(the algorithm's weight in the Super Learner)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("biomarkers")
# subset to complete cases for illustration
cc &lt;- complete.cases(biomarkers)
dat_cc &lt;- biomarkers[cc, ]
# use only the mucinous outcome, not the high-malignancy outcome
y &lt;- dat_cc$mucinous
x &lt;- dat_cc[, !(names(dat_cc) %in% c("mucinous", "high_malignancy"))]
feature_nms &lt;- names(x)
# get the fit
fit &lt;- stats::glm(y ~ ., family = "binomial", data = data.frame(y = y, x))
# extract importance
importance &lt;- extract_importance_glm(fit = fit, feature_names = feature_nms)
importance

</code></pre>

<hr>
<h2 id='extract_importance_glmnet'>Extract the learner-specific importance from a glmnet object</h2><span id='topic+extract_importance_glmnet'></span>

<h3>Description</h3>

<p>Extract the individual-algorithm extrinsic importance from a glmnet object,
along with the importance rank.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_importance_glmnet(fit = NULL, feature_names = "", coef = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extract_importance_glmnet_+3A_fit">fit</code></td>
<td>
<p>the <code>glmnet</code> or <code>cv.glmnet</code> object</p>
</td></tr>
<tr><td><code id="extract_importance_glmnet_+3A_feature_names">feature_names</code></td>
<td>
<p>the feature names</p>
</td></tr>
<tr><td><code id="extract_importance_glmnet_+3A_coef">coef</code></td>
<td>
<p>the Super Learner coefficient associated with the learner.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble, with columns <code>algorithm</code> (the fitted algorithm),
<code>feature</code> (the feature), <code>importance</code> (the algorithm-specific
extrinsic importance of the feature), <code>rank</code> (the feature importance
rank, with 1 indicating the most important feature), and <code>weight</code>
(the algorithm's weight in the Super Learner)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("biomarkers")
# subset to complete cases for illustration
cc &lt;- complete.cases(biomarkers)
dat_cc &lt;- biomarkers[cc, ]
# use only the mucinous outcome, not the high-malignancy outcome
y &lt;- dat_cc$mucinous
x &lt;- dat_cc[, !(names(dat_cc) %in% c("mucinous", "high_malignancy"))]
feature_nms &lt;- names(x)
# get the fit (using only 3 CV folds for illustration only)
set.seed(20231129)
fit &lt;- glmnet::cv.glmnet(x = as.matrix(x), y = y, 
                         family = "binomial", nfolds = 3)
# extract importance
importance &lt;- extract_importance_glmnet(fit = fit, feature_names = feature_nms)
importance

</code></pre>

<hr>
<h2 id='extract_importance_mean'>Extract the learner-specific importance from a mean object</h2><span id='topic+extract_importance_mean'></span>

<h3>Description</h3>

<p>Extract the individual-algorithm extrinsic importance from a mean object,
along with the importance rank.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_importance_mean(fit = NULL, feature_names = "", coef = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extract_importance_mean_+3A_fit">fit</code></td>
<td>
<p>the <code>mean</code> object.</p>
</td></tr>
<tr><td><code id="extract_importance_mean_+3A_feature_names">feature_names</code></td>
<td>
<p>the feature names</p>
</td></tr>
<tr><td><code id="extract_importance_mean_+3A_coef">coef</code></td>
<td>
<p>the Super Learner coefficient associated with the learner.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble, with columns <code>algorithm</code> (the fitted algorithm),
<code>feature</code> (the feature), <code>importance</code> (the algorithm-specific
extrinsic importance of the feature), <code>rank</code> (the feature importance
rank, with 1 indicating the most important feature), and <code>weight</code>
(the algorithm's weight in the Super Learner)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("biomarkers")
# subset to complete cases for illustration
cc &lt;- complete.cases(biomarkers)
dat_cc &lt;- biomarkers[cc, ]
# use only the mucinous outcome, not the high-malignancy outcome
y &lt;- dat_cc$mucinous
x &lt;- dat_cc[, !(names(dat_cc) %in% c("mucinous", "high_malignancy"))]
feature_nms &lt;- names(x)
# get the mean outcome
fit &lt;- mean(y)
# extract importance
importance &lt;- extract_importance_mean(fit = fit, feature_names = feature_nms)
importance

</code></pre>

<hr>
<h2 id='extract_importance_polymars'>Extract the learner-specific importance from a polymars object</h2><span id='topic+extract_importance_polymars'></span>

<h3>Description</h3>

<p>Extract the individual-algorithm extrinsic importance from a polymars object,
along with the importance rank.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_importance_polymars(fit = NULL, feature_names = "", coef = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extract_importance_polymars_+3A_fit">fit</code></td>
<td>
<p>the <code>polymars</code> object.</p>
</td></tr>
<tr><td><code id="extract_importance_polymars_+3A_feature_names">feature_names</code></td>
<td>
<p>the feature names</p>
</td></tr>
<tr><td><code id="extract_importance_polymars_+3A_coef">coef</code></td>
<td>
<p>the Super Learner coefficient associated with the learner.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble, with columns <code>algorithm</code> (the fitted algorithm),
<code>feature</code> (the feature), <code>importance</code> (the algorithm-specific
extrinsic importance of the feature), <code>rank</code> (the feature importance
rank, with 1 indicating the most important feature), and <code>weight</code>
(the algorithm's weight in the Super Learner)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("biomarkers")
# subset to complete cases for illustration
cc &lt;- complete.cases(biomarkers)
dat_cc &lt;- biomarkers[cc, ]
# use only the mucinous outcome, not the high-malignancy outcome
y &lt;- dat_cc$mucinous
x &lt;- dat_cc[, !(names(dat_cc) %in% c("mucinous", "high_malignancy"))]
feature_nms &lt;- names(x)
x_mat &lt;- as.matrix(x)
# get the fit
set.seed(20231129)
fit &lt;- polspline::polyclass(y, x_mat)
# extract importance
importance &lt;- extract_importance_polymars(fit = fit, feature_names = feature_nms)
importance

</code></pre>

<hr>
<h2 id='extract_importance_ranger'>Extract the learner-specific importance from a ranger object</h2><span id='topic+extract_importance_ranger'></span>

<h3>Description</h3>

<p>Extract the individual-algorithm extrinsic importance from a ranger object,
along with the importance rank.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_importance_ranger(fit = NULL, feature_names = "", coef = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extract_importance_ranger_+3A_fit">fit</code></td>
<td>
<p>the <code>ranger</code> object.</p>
</td></tr>
<tr><td><code id="extract_importance_ranger_+3A_feature_names">feature_names</code></td>
<td>
<p>the feature names</p>
</td></tr>
<tr><td><code id="extract_importance_ranger_+3A_coef">coef</code></td>
<td>
<p>the Super Learner coefficient associated with the learner.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble, with columns <code>algorithm</code> (the fitted algorithm),
<code>feature</code> (the feature), <code>importance</code> (the algorithm-specific
extrinsic importance of the feature), <code>rank</code> (the feature importance
rank, with 1 indicating the most important feature), and <code>weight</code>
(the algorithm's weight in the Super Learner)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("biomarkers")
# subset to complete cases for illustration
cc &lt;- complete.cases(biomarkers)
dat_cc &lt;- biomarkers[cc, ]
# use only the mucinous outcome, not the high-malignancy outcome
y &lt;- dat_cc$mucinous
x &lt;- dat_cc[, !(names(dat_cc) %in% c("mucinous", "high_malignancy"))]
feature_nms &lt;- names(x)
# get the fit
set.seed(20231129)
fit &lt;- ranger::ranger(y ~ ., data = data.frame(y = y, x), importance = "impurity")
# extract importance
importance &lt;- extract_importance_ranger(fit = fit, feature_names = feature_nms)
importance

</code></pre>

<hr>
<h2 id='extract_importance_SL'>Extract extrinsic importance from a Super Learner object</h2><span id='topic+extract_importance_SL'></span>

<h3>Description</h3>

<p>Extract the individual-algorithm extrinsic importance from each fitted
algorithm within the Super Learner; compute the average weighted rank of the
importance scores, with weights specified by each algorithm's weight in the
Super Learner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_importance_SL(fit, feature_names, import_type = "all", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extract_importance_SL_+3A_fit">fit</code></td>
<td>
<p>the fitted Super Learner ensemble</p>
</td></tr>
<tr><td><code id="extract_importance_SL_+3A_feature_names">feature_names</code></td>
<td>
<p>the names of the features</p>
</td></tr>
<tr><td><code id="extract_importance_SL_+3A_import_type">import_type</code></td>
<td>
<p>the level of granularity for importance: <code>"all"</code> is the
importance based on the weighted average of ranks across algorithmrithms
(weights are SL coefs); <code>"best"</code> is the importance based on the algorithmrithm
with highest weight. Defaults to <code>"all"</code>.</p>
</td></tr>
<tr><td><code id="extract_importance_SL_+3A_...">...</code></td>
<td>
<p>other arguments to pass to individual-algorithm extractors.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble, with columns <code>feature</code> (the feature) and
<code>rank</code> (the weighted feature importance rank, with 1 indicating the
most important feature).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("biomarkers")
# subset to complete cases for illustration
cc &lt;- complete.cases(biomarkers)
dat_cc &lt;- biomarkers[cc, ]
# use only the mucinous outcome, not the high-malignancy outcome
y &lt;- dat_cc$mucinous
x &lt;- dat_cc[, !(names(dat_cc) %in% c("mucinous", "high_malignancy"))]
feature_nms &lt;- names(x)
# get the fit (using a simple library and 2 folds for illustration only)
set.seed(20231129)
library("SuperLearner")
fit &lt;- SuperLearner::SuperLearner(Y = y, X = x, SL.library = c("SL.glm", "SL.mean"), 
                                  cvControl = list(V = 2))
# extract importance using all learners
importance &lt;- extract_importance_SL(fit = fit, feature_names = feature_nms)
importance
# extract importance of best learner
best_importance &lt;- extract_importance_SL(fit = fit, feature_names = feature_nms, 
                                         import_type = "best")
best_importance

</code></pre>

<hr>
<h2 id='extract_importance_SL_learner'>Extract the learner-specific importance from a fitted SuperLearner algorithm</h2><span id='topic+extract_importance_SL_learner'></span>

<h3>Description</h3>

<p>Extract the individual-algorithm extrinsic importance from one fitted
algorithm within the Super Learner, along with the importance rank.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_importance_SL_learner(fit = NULL, coef = 0, feature_names = "", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extract_importance_SL_learner_+3A_fit">fit</code></td>
<td>
<p>the specific learner (e.g., from the Super Learner's
<code>fitLibrary</code> list).</p>
</td></tr>
<tr><td><code id="extract_importance_SL_learner_+3A_coef">coef</code></td>
<td>
<p>the Super Learner coefficient associated with the learner.</p>
</td></tr>
<tr><td><code id="extract_importance_SL_learner_+3A_feature_names">feature_names</code></td>
<td>
<p>the feature names</p>
</td></tr>
<tr><td><code id="extract_importance_SL_learner_+3A_...">...</code></td>
<td>
<p>other arguments to pass to algorithm-specific importance extractors.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble, with columns <code>algorithm</code> (the fitted algorithm),
<code>feature</code> (the feature), <code>importance</code> (the algorithm-specific
extrinsic importance of the feature), <code>rank</code> (the feature importance
rank, with 1 indicating the most important feature), and <code>weight</code>
(the algorithm's weight in the Super Learner)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("biomarkers")
# subset to complete cases for illustration
cc &lt;- complete.cases(biomarkers)
dat_cc &lt;- biomarkers[cc, ]
# use only the mucinous outcome, not the high-malignancy outcome
y &lt;- dat_cc$mucinous
x &lt;- dat_cc[, !(names(dat_cc) %in% c("mucinous", "high_malignancy"))]
feature_nms &lt;- names(x)
# get the fit (using a simple library and 2 folds for illustration only)
library("SuperLearner")
set.seed(20231129)
fit &lt;- SuperLearner::SuperLearner(Y = y, X = x, SL.library = c("SL.glm", "SL.mean"), 
                                  cvControl = list(V = 2))
# extract importance
importance &lt;- extract_importance_SL_learner(fit = fit$fitLibrary[[1]]$object, 
                                            feature_names = feature_nms, coef = fit$coef[1])
importance

</code></pre>

<hr>
<h2 id='extract_importance_svm'>Extract the learner-specific importance from an svm object</h2><span id='topic+extract_importance_svm'></span>

<h3>Description</h3>

<p>Extract the individual-algorithm extrinsic importance from a glm object,
along with the importance rank.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_importance_svm(
  fit = NULL,
  feature_names = "",
  coef = 0,
  x = NULL,
  y = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extract_importance_svm_+3A_fit">fit</code></td>
<td>
<p>the <code>svm</code> object.</p>
</td></tr>
<tr><td><code id="extract_importance_svm_+3A_feature_names">feature_names</code></td>
<td>
<p>the feature names</p>
</td></tr>
<tr><td><code id="extract_importance_svm_+3A_coef">coef</code></td>
<td>
<p>the Super Learner coefficient associated with the learner.</p>
</td></tr>
<tr><td><code id="extract_importance_svm_+3A_x">x</code></td>
<td>
<p>the features</p>
</td></tr>
<tr><td><code id="extract_importance_svm_+3A_y">y</code></td>
<td>
<p>the outcome</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble, with columns <code>algorithm</code> (the fitted algorithm),
<code>feature</code> (the feature), <code>importance</code> (the algorithm-specific
extrinsic importance of the feature), <code>rank</code> (the feature importance
rank, with 1 indicating the most important feature), and <code>weight</code>
(the algorithm's weight in the Super Learner)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("biomarkers")
# subset to complete cases for illustration
cc &lt;- complete.cases(biomarkers)
dat_cc &lt;- biomarkers[cc, ]
# use only the mucinous outcome, not the high-malignancy outcome
y &lt;- dat_cc$mucinous
x &lt;- as.data.frame(dat_cc[, !(names(dat_cc) %in% c("mucinous", "high_malignancy"))])
x_mat &lt;- as.matrix(x)
feature_nms &lt;- names(x)
# get the fit 
set.seed(20231129)
fit &lt;- kernlab::ksvm(x_mat, y)
# extract importance
importance &lt;- extract_importance_svm(fit = fit, feature_names = feature_nms, x = x, y = y)
importance

</code></pre>

<hr>
<h2 id='extract_importance_xgboost'>Extract the learner-specific importance from an xgboost object</h2><span id='topic+extract_importance_xgboost'></span>

<h3>Description</h3>

<p>Extract the individual-algorithm extrinsic importance from an xgboost object,
along with the importance rank.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_importance_xgboost(fit = NULL, feature_names = "", coef = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extract_importance_xgboost_+3A_fit">fit</code></td>
<td>
<p>the <code>xgboost</code> object.</p>
</td></tr>
<tr><td><code id="extract_importance_xgboost_+3A_feature_names">feature_names</code></td>
<td>
<p>the feature names</p>
</td></tr>
<tr><td><code id="extract_importance_xgboost_+3A_coef">coef</code></td>
<td>
<p>the Super Learner coefficient associated with the learner.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble, with columns <code>algorithm</code> (the fitted algorithm),
<code>feature</code> (the feature), <code>importance</code> (the algorithm-specific
extrinsic importance of the feature), <code>rank</code> (the feature importance
rank, with 1 indicating the most important feature), and <code>weight</code>
(the algorithm's weight in the Super Learner)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("biomarkers")
# subset to complete cases for illustration
cc &lt;- complete.cases(biomarkers)
dat_cc &lt;- biomarkers[cc, ]
# use only the mucinous outcome, not the high-malignancy outcome
y &lt;- dat_cc$mucinous
x &lt;- as.matrix(dat_cc[, !(names(dat_cc) %in% c("mucinous", "high_malignancy"))])
feature_nms &lt;- names(x)
set.seed(20231129)
xgbmat &lt;- xgboost::xgb.DMatrix(data = x, label = y)
# get the fit, using a small number of rounds for illustration only 
fit &lt;- xgboost::xgboost(data = xgbmat, objective = "binary:logistic", nthread = 1, nrounds = 10)
# extract importance
importance &lt;- extract_importance_xgboost(fit = fit, feature_names = feature_nms)
importance


</code></pre>

<hr>
<h2 id='extrinsic_selection'>Perform extrinsic, ensemble-based variable selection</h2><span id='topic+extrinsic_selection'></span>

<h3>Description</h3>

<p>Based on a fitted Super Learner ensemble, extract extrinsic
variable importance estimates, rank them, and do variable
selection using the specified rank threshold.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extrinsic_selection(
  fit = NULL,
  feature_names = "",
  threshold = 20,
  import_type = "all",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extrinsic_selection_+3A_fit">fit</code></td>
<td>
<p>the fitted Super Learner ensemble.</p>
</td></tr>
<tr><td><code id="extrinsic_selection_+3A_feature_names">feature_names</code></td>
<td>
<p>the names of the features (a character vector of
length <code>p</code> (the total number of features)); only used if the
fitted Super Learner ensemble was fit on a <code>matrix</code> rather than on a
<code>data.frame</code>, <code>tibble</code>, etc.</p>
</td></tr>
<tr><td><code id="extrinsic_selection_+3A_threshold">threshold</code></td>
<td>
<p>the threshold for selection based on ranked
variable importance; rank 1 is the most important. Defaults
to 20 (though this is arbitrary, and really should be
specified for the task at hand).</p>
</td></tr>
<tr><td><code id="extrinsic_selection_+3A_import_type">import_type</code></td>
<td>
<p>the type of extrinsic importance (either <code>"all"</code>,
the default, for a weighted combination of the individual-algorithm importance;
or <code>"best"</code>, for the importance from the algorithm with the highest
weight in the Super Learner).</p>
</td></tr>
<tr><td><code id="extrinsic_selection_+3A_...">...</code></td>
<td>
<p>other arguments to pass to algorithm-specific importance extractors.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble with the estimated extrinsic variable importance,
the corresponding variable importance ranks, and the selected
variables.
</p>


<h3>See Also</h3>

<p><code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner</a></code> for specific usage of
the <code>SuperLearner</code> function and package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("biomarkers")
# subset to complete cases for illustration
cc &lt;- complete.cases(biomarkers)
dat_cc &lt;- biomarkers[cc, ]
# use only the mucinous outcome, not the high-malignancy outcome
y &lt;- dat_cc$mucinous
x &lt;- dat_cc[, !(names(dat_cc) %in% c("mucinous", "high_malignancy"))]
feature_nms &lt;- names(x)
# get the fit (using a simple library and 2 folds for illustration only)
library("SuperLearner")
set.seed(20231129)
fit &lt;- SuperLearner::SuperLearner(Y = y, X = x, SL.library = c("SL.glm", "SL.mean"), 
                                  cvControl = list(V = 2))
# extract importance
importance &lt;- extrinsic_selection(fit = fit, feature_names = feature_nms, threshold = 1.5, 
                                  import_type = "all")
importance

</code></pre>

<hr>
<h2 id='flevr'>flevr: Flexible, Ensemble-Based Variable Selection with Potentially Missing Data</h2><span id='topic+flevr'></span>

<h3>Description</h3>

<p>A framework for flexible, ensemble-based variable selection using either
extrinsic or intrinsic variable importance. You provide
the data and a library of candidate algorithms for estimating the
conditional mean outcome given covariates; <code>flevr</code> handles the rest.
</p>


<h3>Author(s)</h3>

<p><b>Maintainer</b>: Brian Williamson <a href="https://bdwilliamson.github.io/">https://bdwilliamson.github.io/</a>
</p>
<p>Methodology authors:
</p>

<ul>
<li><p>Brian D. Williamson
</p>
</li>
<li><p>Ying Huang
</p>
</li></ul>



<h3>See Also</h3>

<p>Papers:
</p>

<ul>
<li><p><a href="https://arxiv.org/abs/2202.12989">https://arxiv.org/abs/2202.12989</a>
</p>
</li></ul>

<p>Other useful links:
</p>

<ul>
<li><p><a href="https://bdwilliamson.github.io/flevr/">https://bdwilliamson.github.io/flevr/</a>
</p>
</li>
<li><p><a href="https://github.com/bdwilliamson/flevr">https://github.com/bdwilliamson/flevr</a>
</p>
</li>
<li><p>Report bugs at <a href="https://github.com/bdwilliamson/flevr/issues">https://github.com/bdwilliamson/flevr/issues</a>
</p>
</li></ul>



<h3>Imports</h3>

<p>The packages that we import either make the internal code nice
(dplyr, magrittr, tibble) or are directly relevant for estimating
variable importance (SuperLearner, caret).
</p>
<p>We suggest several other packages: xgboost, ranger, glmnet, kernlab, polspline
and quadprog allow a flexible library of candidate learners in the Super
Learner; stabs allows importance to be embedded within stability selection;
testthat and covr help with unit tests; and
knitr, rmarkdown,and RCurl help with the vignettes and examples.
</p>

<hr>
<h2 id='get_augmented_set'>Get an augmented set based on the next-most significant variables</h2><span id='topic+get_augmented_set'></span>

<h3>Description</h3>

<p>Based on the adjusted p-values from a FWER-controlling procedure and a
more general error rate for which control is desired (e.g., generalized
FWER, proportion of false positives, or FDR), augment the set based on FWER
control with the next-most significant variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_augmented_set(
  p_values = NULL,
  num_rejected = 0,
  alpha = 0.05,
  quantity = "gFWER",
  q = 0.05,
  k = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_augmented_set_+3A_p_values">p_values</code></td>
<td>
<p>the adjusted p-values.</p>
</td></tr>
<tr><td><code id="get_augmented_set_+3A_num_rejected">num_rejected</code></td>
<td>
<p>the number of rejected null hypotheses from the base
FWER-controlling procedure.</p>
</td></tr>
<tr><td><code id="get_augmented_set_+3A_alpha">alpha</code></td>
<td>
<p>the significance level.</p>
</td></tr>
<tr><td><code id="get_augmented_set_+3A_quantity">quantity</code></td>
<td>
<p>the quantity to control (i.e., <code>"gFWER"</code>, <code>"PFP"</code>,
or <code>"FDR"</code>).</p>
</td></tr>
<tr><td><code id="get_augmented_set_+3A_q">q</code></td>
<td>
<p>the proportion for FDR or PFP control.</p>
</td></tr>
<tr><td><code id="get_augmented_set_+3A_k">k</code></td>
<td>
<p>the number of false positives for gFWER control.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of the variables selected into the augmentation set. Contains the following values:
</p>

<ul>
<li> <p><code>set</code>, a numeric vector where 1 denotes that the variable was selected and 0 otherwise
</p>
</li>
<li> <p><code>k</code>, the value of k used
</p>
</li>
<li> <p><code>q_star</code>, the value of q-star used
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
data("biomarkers")
# subset to complete cases for illustration
cc &lt;- complete.cases(biomarkers)
dat_cc &lt;- biomarkers[cc, ]
# use only the mucinous outcome, not the high-malignancy outcome
y &lt;- dat_cc$mucinous
x &lt;- dat_cc[, !(names(dat_cc) %in% c("mucinous", "high_malignancy"))]
feature_nms &lt;- names(x)
# estimate SPVIMs (using simple library and V = 2 for illustration only)
set.seed(20231129)
library("SuperLearner")
est &lt;- vimp::sp_vim(Y = y, X = x, V = 2, type = "auc", SL.library = "SL.glm", 
                    cvControl = list(V = 2))
# get base set
base_set &lt;- get_base_set(test_statistics = est$test_statistic, p_values = est$p_value, 
                         alpha = 0.2, method = "Holm")
# get augmented set
augmented_set &lt;- get_augmented_set(p_values = base_set$p_values, 
                                   num_rejected = sum(base_set$decision), alpha = 0.2, 
                                   quantity = "gFWER", k = 1)
augmented_set$set

</code></pre>

<hr>
<h2 id='get_base_set'>Get an initial selected set based on intrinsic importance and a base method</h2><span id='topic+get_base_set'></span>

<h3>Description</h3>

<p>Using the estimated intrinsic importance and a base method
designed to control the family-wise error rate (e.g., Holm),
obtain an initial selected set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_base_set(
  test_statistics = NULL,
  p_values = NULL,
  alpha = 0.05,
  method = "maxT",
  B = 10000,
  Sigma = diag(1, nrow = length(test_statistics)),
  q = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_base_set_+3A_test_statistics">test_statistics</code></td>
<td>
<p>the test statistics (used with &quot;maxT&quot;)</p>
</td></tr>
<tr><td><code id="get_base_set_+3A_p_values">p_values</code></td>
<td>
<p>(used with &quot;minP&quot; or &quot;Holm&quot;)</p>
</td></tr>
<tr><td><code id="get_base_set_+3A_alpha">alpha</code></td>
<td>
<p>the alpha level</p>
</td></tr>
<tr><td><code id="get_base_set_+3A_method">method</code></td>
<td>
<p>the method (one of &quot;maxT&quot;, &quot;minP&quot;, or &quot;Holm&quot;)</p>
</td></tr>
<tr><td><code id="get_base_set_+3A_b">B</code></td>
<td>
<p>the number of resamples (for minP or maxT)</p>
</td></tr>
<tr><td><code id="get_base_set_+3A_sigma">Sigma</code></td>
<td>
<p>the estimated covariance matrix for the test statistics</p>
</td></tr>
<tr><td><code id="get_base_set_+3A_q">q</code></td>
<td>
<p>the false discovery rate (for method = &quot;BY&quot;)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the initial selected set, a list of the following:
</p>

<ul>
<li> <p><code>decision</code>, a numeric vector with 1 indicating that the variable was selected and 0 otherwise
</p>
</li>
<li> <p><code>p_values</code>, the p-values used to make the decision
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
data("biomarkers")
# subset to complete cases for illustration
cc &lt;- complete.cases(biomarkers)
dat_cc &lt;- biomarkers[cc, ]
# use only the mucinous outcome, not the high-malignancy outcome
y &lt;- dat_cc$mucinous
x &lt;- dat_cc[, !(names(dat_cc) %in% c("mucinous", "high_malignancy"))]
feature_nms &lt;- names(x)
# estimate SPVIMs (using simple library and V = 2 for illustration only)
set.seed(20231129)
library("SuperLearner")
est &lt;- vimp::sp_vim(Y = y, X = x, V = 2, type = "auc", SL.library = "SL.glm", 
                    cvControl = list(V = 2))
# get base set
base_set &lt;- get_base_set(test_statistics = est$test_statistic, p_values = est$p_value, 
                         alpha = 0.2, method = "Holm")
base_set$decision

</code></pre>

<hr>
<h2 id='intrinsic_control'>Control parameters for intrinsic variable selection</h2><span id='topic+intrinsic_control'></span>

<h3>Description</h3>

<p>Control parameters for SPVIM-based intrinsic variable selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>intrinsic_control(
  quantity = "gFWER",
  base_method = "Holm",
  fdr_method = "Holm",
  q = 0.2,
  k = 5
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="intrinsic_control_+3A_quantity">quantity</code></td>
<td>
<p>the desired quantity for error-rate control: possible values
are <code>"gFWER"</code> (the generalized family-wise error rate),
<code>"PFP"</code> (the proportion of false positives), and <code>"FDR"</code> (the
false discovery rate).</p>
</td></tr>
<tr><td><code id="intrinsic_control_+3A_base_method">base_method</code></td>
<td>
<p>the family-wise error rate controlling method to use for
obtaining the initial set of selected variables. Possible values are
<code>"maxT"</code> and <code>"minP"</code> (for step-down procedures based on the
test statistics ranked from largest to smallest or the p-values ranked from
smallest to largest, respectively) or <code>"Holm"</code> for a procedure based
on Holm-adjusted p-values.</p>
</td></tr>
<tr><td><code id="intrinsic_control_+3A_fdr_method">fdr_method</code></td>
<td>
<p>the method for controlling the FDR (if
<code>quantity = "FDR"</code>); possible values are <code>"BY"</code> (for
Benjamini-Yekutieli) or one of the <code>base_method</code>s.</p>
</td></tr>
<tr><td><code id="intrinsic_control_+3A_q">q</code></td>
<td>
<p>the desired proportion of false positives (only used if
<code>quantity = "PFP"</code> or <code>"FDR"</code>; a fraction between 0 and 1).</p>
</td></tr>
<tr><td><code id="intrinsic_control_+3A_k">k</code></td>
<td>
<p>the desired number of family-wise errors (an integer, greater than
or equal to zero.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with the control parameters.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>control &lt;- intrinsic_control(quantity = "gFWER", base_method = "Holm", fdr_method = "Holm", 
                             k = 1)
control
</code></pre>

<hr>
<h2 id='intrinsic_selection'>Perform intrinsic, ensemble-based variable selection</h2><span id='topic+intrinsic_selection'></span>

<h3>Description</h3>

<p>Based on estimated SPVIM values, do variable selection using the
specified error-controlling method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>intrinsic_selection(
  spvim_ests = NULL,
  sample_size = NULL,
  feature_names = "",
  alpha = 0.05,
  control = list(quantity = "gFWER", base_method = "Holm", fdr_method = NULL, q = NULL, k
    = NULL)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="intrinsic_selection_+3A_spvim_ests">spvim_ests</code></td>
<td>
<p>the estimated SPVIM values (an object of class <code>vim</code>,
resulting from a call to <code>vimp::sp_vim</code>). Can also be a list of
estimated SPVIMs, if multiple imputation was used to handle missing data; in
this case, Rubin's rules will be used to combine the estimated SPVIMs, and
then selection will be based on the combined SPVIMs.</p>
</td></tr>
<tr><td><code id="intrinsic_selection_+3A_sample_size">sample_size</code></td>
<td>
<p>the number of independent observations used to estimate
the SPVIM values.</p>
</td></tr>
<tr><td><code id="intrinsic_selection_+3A_feature_names">feature_names</code></td>
<td>
<p>the names of the features (a character vector of
length <code>p</code> (the total number of features)); only used if the
fitted Super Learner ensemble was fit on a <code>matrix</code> rather than on a
<code>data.frame</code>, <code>tibble</code>, etc.</p>
</td></tr>
<tr><td><code id="intrinsic_selection_+3A_alpha">alpha</code></td>
<td>
<p>the nominal generalized family-wise error rate, proportion of
false positives, or false discovery rate level to control at (e.g., 0.05).</p>
</td></tr>
<tr><td><code id="intrinsic_selection_+3A_control">control</code></td>
<td>
<p>a list of parameters to control the variable selection process.
Parameters include <code>quantity</code>, <code>base_method</code>, <code>q</code>, and
<code>k</code>. See <code><a href="#topic+intrinsic_control">intrinsic_control</a></code> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tibble with the estimated intrinsic variable importance,
the corresponding variable importance ranks, and the selected
variables.
</p>


<h3>See Also</h3>

<p><code><a href="vimp.html#topic+sp_vim">sp_vim</a></code> for specific usage of
the <code>sp_vim</code> function and the <code>vimp</code> package for estimating
intrinsic variable importance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("biomarkers")
# subset to complete cases for illustration
cc &lt;- complete.cases(biomarkers)
dat_cc &lt;- biomarkers[cc, ]
# use only the mucinous outcome, not the high-malignancy outcome
y &lt;- dat_cc$mucinous
x &lt;- dat_cc[, !(names(dat_cc) %in% c("mucinous", "high_malignancy"))]
feature_nms &lt;- names(x)
# estimate SPVIMs (using simple library and V = 2 for illustration only)
set.seed(20231129)
library("SuperLearner")
est &lt;- vimp::sp_vim(Y = y, X = x, V = 2, type = "auc", SL.library = "SL.glm", 
                    cvControl = list(V = 2))
# do intrinsic selection
intrinsic_set &lt;- intrinsic_selection(spvim_ests = est, sample_size = nrow(dat_cc), alpha = 0.2, 
                                     feature_names = feature_nms, 
                                     control = list(quantity = "gFWER", base_method = "Holm", 
                                                    k = 1))
intrinsic_set

</code></pre>

<hr>
<h2 id='pool_selected_sets'>Pool selected sets from multiply-imputed data</h2><span id='topic+pool_selected_sets'></span>

<h3>Description</h3>

<p>Pool the selected sets from multiply-imputed or bootstrap + imputed data. Uses
the &quot;stability&quot; of the variables over the multiple selected sets to select
variables that are stable across the sets, where stability is determined by
presence in a certain fraction of the selected sets (and the fraction must be
above the specified threshold to be &quot;stable&quot;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pool_selected_sets(sets = list(), threshold = 0.8)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pool_selected_sets_+3A_sets">sets</code></td>
<td>
<p>a list of sets of selected variables from the multiply-imputed datasets.
Expects each set of selected variables to be a binary vector, where 1 denotes
that the variable was selected.</p>
</td></tr>
<tr><td><code id="pool_selected_sets_+3A_threshold">threshold</code></td>
<td>
<p>a numeric threshold between 0 and 1 detemining the &quot;stability&quot;
of a feature; only features with stability above the threshold after pooling
will be in the final selected set of variables.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector denoting the final set of selected variables (1 denotes
selected, 0 denotes not selected)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("biomarkers")
x &lt;- biomarkers[, !(names(biomarkers) %in% c("mucinous", "high_malignancy"))]
feature_nms &lt;- names(x)
library("dplyr")
library("SuperLearner")
# do multiple imputation (with a small number for illustration only)
library("mice")
n_imp &lt;- 2
set.seed(20231129)
mi_biomarkers &lt;- mice::mice(data = biomarkers, m = n_imp, printFlag = FALSE)
imputed_biomarkers &lt;- mice::complete(mi_biomarkers, action = "long") %&gt;%
  rename(imp = .imp, id = .id)
# set up a list to collect selected sets
all_selected_vars &lt;- vector("list", length = 5)
for (i in 1:n_imp) {
  # fit a Super Learner using simple library for illustration only
  these_data &lt;- imputed_biomarkers %&gt;%
    filter(imp == i)
  this_y &lt;- these_data$mucinous
  this_x &lt;- these_data %&gt;%
    select(starts_with("lab"), starts_with("cea"))
  this_x_df &lt;- as.data.frame(this_x)
  fit &lt;- SuperLearner::SuperLearner(Y = this_y, X = this_x_df,
                                  SL.library = "SL.glm",
                                  cvControl = list(V = 2),
                                  family = "binomial")
  # do extrinsic selection
  all_selected_vars[[i]] &lt;- extrinsic_selection(
    fit = fit, feature_names = feature_nms, threshold = 5, import_type = "all"
  )$selected
}
# perform extrinsic variable selection
selected_vars &lt;- pool_selected_sets(sets = all_selected_vars, threshold = 1 / n_imp)
feature_nms[selected_vars]

</code></pre>

<hr>
<h2 id='pool_spvims'>Pool SPVIM Estimates Using Rubin's Rules</h2><span id='topic+pool_spvims'></span>

<h3>Description</h3>

<p>If multiple imputation was used due to the presence of missing data,
pool SPVIM estimates from individual imputed datasets using Rubin's rules.
Results in point estimates averaged over the imputations, along with
within-imputation variance estimates and across-imputation variance estimates;
and test statistics and p-values for hypothesis testing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pool_spvims(spvim_ests = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pool_spvims_+3A_spvim_ests">spvim_ests</code></td>
<td>
<p>a list of estimated SPVIMs (of class <code>vim</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of results containing the following:
</p>

<ul>
<li> <p><code>est</code>, the average SPVIM estimate over the multiply-imputed datasets
</p>
</li>
<li> <p><code>se</code>, the average of the within-imputation SPVIM variance estimates
</p>
</li>
<li> <p><code>test_statistics</code>, the test statistics for hypothesis tests of zero importance, using the Rubin's rules standard error estimator and average SPVIM estimate
</p>
</li>
<li> <p><code>p_values</code>, p-values computed using the above test statistics
</p>
</li>
<li> <p><code>tau_n</code>, the across-imputation variance estimates
</p>
</li>
<li> <p><code>vcov</code>, the overall variance-covariance matrix
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
data("biomarkers")
library("dplyr")
# do multiple imputation (with a small number for illustration only)
library("mice")
n_imp &lt;- 2
set.seed(20231129)
mi_biomarkers &lt;- mice::mice(data = biomarkers, m = n_imp, printFlag = FALSE)
imputed_biomarkers &lt;- mice::complete(mi_biomarkers, action = "long") %&gt;%
  rename(imp = .imp, id = .id)
# estimate SPVIMs for each imputed dataset, using simple library for illustration only
library("SuperLearner")
est_lst &lt;- lapply(as.list(1:n_imp), function(l) {
  this_x &lt;- imputed_biomarkers %&gt;%
    filter(imp == l) %&gt;%
    select(starts_with("lab"), starts_with("cea"))
  this_y &lt;- biomarkers$mucinous
  suppressWarnings(
    vimp::sp_vim(Y = this_y, X = this_x, V = 2, type = "auc", 
                 SL.library = "SL.glm", gamma = 0.1, alpha = 0.05, delta = 0,
                 cvControl = list(V = 2), env = environment())
  )
})
# pool the SPVIMs using Rubin's rules
pooled_spvims &lt;- pool_spvims(spvim_ests = est_lst)
pooled_spvims

</code></pre>

<hr>
<h2 id='SL_stabs_fitfun'>Wrapper for using Super Learner-based extrinsic selection within stability selection</h2><span id='topic+SL_stabs_fitfun'></span>

<h3>Description</h3>

<p>A wrapper function for Super Learner-based extrinsic variable selection within
stability selection, using the <code>stabs</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SL_stabs_fitfun(x, y, q, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SL_stabs_fitfun_+3A_x">x</code></td>
<td>
<p>the features.</p>
</td></tr>
<tr><td><code id="SL_stabs_fitfun_+3A_y">y</code></td>
<td>
<p>the outcome of interest.</p>
</td></tr>
<tr><td><code id="SL_stabs_fitfun_+3A_q">q</code></td>
<td>
<p>the number of features to select on average.</p>
</td></tr>
<tr><td><code id="SL_stabs_fitfun_+3A_...">...</code></td>
<td>
<p>other arguments to pass to <code>SuperLearner</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list, with elements: <code>selected</code> (a logical vector
indicating whether or not each variable was selected); and <code>path</code> (
a logical matrix indicating which variable was selected at each step).
</p>


<h3>See Also</h3>

<p><code><a href="stabs.html#topic+stabsel">stabsel</a></code> for general usage of stability selection.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("biomarkers")
# subset to complete cases for illustration
cc &lt;- complete.cases(biomarkers)
dat_cc &lt;- biomarkers[cc, ]
# use only the mucinous outcome, not the high-malignancy outcome
y &lt;- dat_cc$mucinous
x &lt;- dat_cc[, !(names(dat_cc) %in% c("mucinous", "high_malignancy"))]
feature_nms &lt;- names(x)
# use stability selection with SL (using small number of folds for CV, 
# small SL library and small number of bootstrap replicates for illustration only)
set.seed(20231129)
library("SuperLearner")
sl_stabs &lt;- stabs::stabsel(x = x, y = y,
                           fitfun = SL_stabs_fitfun,
                           args.fitfun = list(SL.library = "SL.glm", cvControl = list(V = 2)),
                           q = 2, B = 5, PFER = 5)
sl_stabs

</code></pre>

<hr>
<h2 id='SL.ranger.imp'>Super Learner wrapper for a ranger object with variable importance</h2><span id='topic+SL.ranger.imp'></span>

<h3>Description</h3>

<p>Super Learner wrapper for a ranger object with variable importance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SL.ranger.imp(
  Y,
  X,
  newX,
  family,
  obsWeights = rep(1, length(Y)),
  num.trees = 500,
  mtry = floor(sqrt(ncol(X))),
  write.forest = TRUE,
  probability = family$family == "binomial",
  min.node.size = ifelse(family$family == "gaussian", 5, 1),
  replace = TRUE,
  sample.fraction = ifelse(replace, 1, 0.632),
  num.threads = 1,
  verbose = FALSE,
  importance = "impurity",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SL.ranger.imp_+3A_y">Y</code></td>
<td>
<p>Outcome variable</p>
</td></tr>
<tr><td><code id="SL.ranger.imp_+3A_x">X</code></td>
<td>
<p>Training dataframe</p>
</td></tr>
<tr><td><code id="SL.ranger.imp_+3A_newx">newX</code></td>
<td>
<p>Test dataframe</p>
</td></tr>
<tr><td><code id="SL.ranger.imp_+3A_family">family</code></td>
<td>
<p>Gaussian or binomial</p>
</td></tr>
<tr><td><code id="SL.ranger.imp_+3A_obsweights">obsWeights</code></td>
<td>
<p>Observation-level weights</p>
</td></tr>
<tr><td><code id="SL.ranger.imp_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees.</p>
</td></tr>
<tr><td><code id="SL.ranger.imp_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables to possibly split at in each node. Default is
the (rounded down) square root of the number variables.</p>
</td></tr>
<tr><td><code id="SL.ranger.imp_+3A_write.forest">write.forest</code></td>
<td>
<p>Save ranger.forest object, required for prediction. Set
to FALSE to reduce memory usage if no prediction intended.</p>
</td></tr>
<tr><td><code id="SL.ranger.imp_+3A_probability">probability</code></td>
<td>
<p>Grow a probability forest as in Malley et al. (2012).</p>
</td></tr>
<tr><td><code id="SL.ranger.imp_+3A_min.node.size">min.node.size</code></td>
<td>
<p>Minimal node size. Default 1 for classification, 5 for
regression, 3 for survival, and 10 for probability.</p>
</td></tr>
<tr><td><code id="SL.ranger.imp_+3A_replace">replace</code></td>
<td>
<p>Sample with replacement.</p>
</td></tr>
<tr><td><code id="SL.ranger.imp_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>Fraction of observations to sample. Default is 1 for
sampling with replacement and 0.632 for sampling without replacement.</p>
</td></tr>
<tr><td><code id="SL.ranger.imp_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="SL.ranger.imp_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, display additional output during execution.</p>
</td></tr>
<tr><td><code id="SL.ranger.imp_+3A_importance">importance</code></td>
<td>
<p>Variable importance mode, one of 'none', 'impurity', 'impurity_corrected', 'permutation'. The 'impurity' measure is the Gini index for classification, the variance of the responses for regression and the sum of test statistics (see <code>splitrule</code>) for survival.</p>
</td></tr>
<tr><td><code id="SL.ranger.imp_+3A_...">...</code></td>
<td>
<p>Any additional arguments, not currently used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list with elements <code>pred</code> (predictions on <code>newX</code>) and <code>fit</code> (the fitted <code>ranger</code> object).
</p>


<h3>References</h3>

<p>Breiman, L. (2001). Random forests. Machine learning 45:5-32.
</p>
<p>Wright, M. N. &amp; Ziegler, A. (2016). ranger: A Fast Implementation of Random
Forests for High Dimensional Data in C++ and R. Journal of Statistical
Software, in press. http://arxiv.org/abs/1508.04409.
</p>


<h3>See Also</h3>

<p><code><a href="SuperLearner.html#topic+SL.ranger">SL.ranger</a></code> <code><a href="ranger.html#topic+ranger">ranger</a></code>
<code><a href="ranger.html#topic+predict.ranger">predict.ranger</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("biomarkers")
# subset to complete cases for illustration
cc &lt;- complete.cases(biomarkers)
dat_cc &lt;- biomarkers[cc, ]
# use only the mucinous outcome, not the high-malignancy outcome
y &lt;- dat_cc$mucinous
x &lt;- dat_cc[, !(names(dat_cc) %in% c("mucinous", "high_malignancy"))]
feature_nms &lt;- names(x)
# get the fit
set.seed(20231129)
fit &lt;- SL.ranger.imp(Y = y, X = x, newX = x, family = binomial())
fit
</code></pre>

<hr>
<h2 id='spvim_vcov'>Extract a Variance-Covariance Matrix for SPVIM Estimates</h2><span id='topic+spvim_vcov'></span>

<h3>Description</h3>

<p>Extract a variance-covariance matrix based on the efficient influence function
for each of the estimated SPVIMs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spvim_vcov(spvim_ests = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spvim_vcov_+3A_spvim_ests">spvim_ests</code></td>
<td>
<p>estimated SPVIMs</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a variance-covariance matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("biomarkers")
# subset to complete cases for illustration
cc &lt;- complete.cases(biomarkers)
dat_cc &lt;- biomarkers[cc, ]
# use only the mucinous outcome, not the high-malignancy outcome
y &lt;- dat_cc$mucinous
x &lt;- dat_cc[, !(names(dat_cc) %in% c("mucinous", "high_malignancy"))]
feature_nms &lt;- names(x)
# estimate SPVIMs (using simple library and V = 2 for illustration only)
set.seed(20231129)
library("SuperLearner")
est &lt;- vimp::sp_vim(Y = y, X = x, V = 2, type = "auc", SL.library = "SL.glm", 
                    cvControl = list(V = 2))
# get variance-covariance matrix
vcov &lt;- spvim_vcov(spvim_ests = est)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
