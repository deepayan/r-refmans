<!DOCTYPE html><html><head><title>Help for package ngramr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ngramr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#chunk'><p>Chunk a vector or list</p></a></li>
<li><a href='#corpuses'><p>Google n-gram corpus information</p></a></li>
<li><a href='#ggram'><p>Plot n-gram frequencies</p></a></li>
<li><a href='#hacker'><p>Sample n-gram data</p></a></li>
<li><a href='#ngram'><p>Get n-gram frequencies</p></a></li>
<li><a href='#ngrami'><p>Get n-gram frequencies (case insensitive version)</p></a></li>
<li><a href='#ngramr'><p>ngramr: Dig into the Google Ngram Viewer using R</p></a></li>
<li><a href='#ngramw'><p>Get n-gram frequencies (&quot;wide&quot; format)</p></a></li>
<li><a href='#print.ngram'><p>Print n-gram contents</p></a></li>
<li><a href='#theme_google'><p>Google Ngram theme for ggplot2</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Retrieve and Plot Google n-Gram Data</td>
</tr>
<tr>
<td>Version:</td>
<td>1.9.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-01-16</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Sean Carmody &lt;seancarmody@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Retrieve and plot word frequencies through time from the "Google
    Ngram Viewer" <a href="https://books.google.com/ngrams">https://books.google.com/ngrams</a>.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>httr, rlang, curl, dplyr (&ge; 1.0.3), cli, tibble, tidyr,
rjson, stringr, ggplot2, scales, xml2, textutils</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/seancarmody/ngramr">https://github.com/seancarmody/ngramr</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/seancarmody/ngramr/issues">https://github.com/seancarmody/ngramr/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.2</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>Language:</td>
<td>en-AU</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-01-16 10:39:17 UTC; sean</td>
</tr>
<tr>
<td>Author:</td>
<td>Sean Carmody [aut, cre, cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-01-16 11:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='chunk'>Chunk a vector or list</h2><span id='topic+chunk'></span>

<h3>Description</h3>

<p><code>chunk</code> takes a vector (or list) and returns a list of chunks
which all have lengths (approximately) equal to a specified value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chunk(x, len = NULL, n = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chunk_+3A_x">x</code></td>
<td>
<p>vector of list</p>
</td></tr>
<tr><td><code id="chunk_+3A_len">len</code></td>
<td>
<p>target length of chunks</p>
</td></tr>
<tr><td><code id="chunk_+3A_n">n</code></td>
<td>
<p>number of chunks</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>n</code> is specified, <code>len</code> is ignored and <code>chunk</code> returns
a list of length <code>n</code> of &quot;chunks&quot; of <code>x</code>. Otherwise
<code>n</code> is calculated to break the vector into chunks which are
each approximately of length <code>len</code>. If both <code>len</code> and
<code>n</code> are unspecified, <code>chunk</code> simply returns <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>chunk(letters, 10)
chunk(LETTERS, n = 3)

</code></pre>

<hr>
<h2 id='corpuses'>Google n-gram corpus information</h2><span id='topic+corpuses'></span>

<h3>Description</h3>

<p>Details of the various corpuses available through the Google n-gram tool
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corpuses
</code></pre>


<h3>Format</h3>

<p>a 33 x 6 ngram data frame
</p>

<hr>
<h2 id='ggram'>Plot n-gram frequencies</h2><span id='topic+ggram'></span>

<h3>Description</h3>

<p><code>ggram</code> downloads data from the Google Ngram Viewer website and
plots it in <code>ggplot2</code> style.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ggram(
  phrases,
  ignore_case = FALSE,
  code_corpus = FALSE,
  geom = "line",
  geom_options = list(),
  lab = NA,
  google_theme = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggram_+3A_phrases">phrases</code></td>
<td>
<p>vector of phrases. Alternatively, phrases can be an ngram
object returned by <code><a href="#topic+ngram">ngram</a></code> or <code><a href="#topic+ngrami">ngrami</a></code>.</p>
</td></tr>
<tr><td><code id="ggram_+3A_ignore_case">ignore_case</code></td>
<td>
<p>logical, indicating whether the frequencies are case
insensitive.
Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="ggram_+3A_code_corpus">code_corpus</code></td>
<td>
<p>logical, indicating whether to use abbreviated corpus
'codes or longer form descriptions. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="ggram_+3A_geom">geom</code></td>
<td>
<p>the ggplot2 geom used to plot the data; defaults to &quot;line&quot;</p>
</td></tr>
<tr><td><code id="ggram_+3A_geom_options">geom_options</code></td>
<td>
<p>list of additional parameters passed to the ggplot2 geom.</p>
</td></tr>
<tr><td><code id="ggram_+3A_lab">lab</code></td>
<td>
<p>y-axis label. Defaults to &quot;Frequency&quot;.</p>
</td></tr>
<tr><td><code id="ggram_+3A_google_theme">google_theme</code></td>
<td>
<p>use a Google Ngram-style plot theme.</p>
</td></tr>
<tr><td><code id="ggram_+3A_...">...</code></td>
<td>
<p>additional parameters passed to <code>ngram</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Google generated two datasets drawn from digitised books in the Google
books collection. One was generated in July 2009, the second in July 2012.
Google will update these datasets as book scanning continues.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
ggram(c("hacker", "programmer"), year_start = 1950)

# Changing the geom.
ggram(c("cancer", "fumer", "cigarette"),
      year_start = 1900,
      corpus = "fr-2012",
      smoothing = 0,
      geom = "step")

# Passing more options.
ggram(c("cancer", "smoking", "tobacco"),
      year_start = 1900,
      corpus = "en-fiction-2012",
      geom = "point",
      smoothing = 0,
      geom_options = list(alpha = .5)) +
  stat_smooth(method="loess", se = FALSE, formula = y  ~ x)

# Setting the layers manually.
ggram(c("cancer", "smoking", "tobacco"),
      year_start = 1900,
      corpus = "en-fiction-2012",
      smoothing = 0,
      geom = NULL) +
  stat_smooth(method="loess", se=FALSE, span = 0.3, formula = y ~ x)

# Setting the legend placement on a long query and using the Google theme.
# Example taken from a post by Ben Zimmer at Language Log.
p &lt;- c("((The United States is + The United States has) / The United States)",
      "((The United States are + The United States have) / The United States)")
ggram(p, year_start = 1800, google_theme = TRUE) +
      theme(legend.direction="vertical")

# Pass ngram data rather than phrases
ggram(hacker) + facet_wrap(~ Corpus)

</code></pre>

<hr>
<h2 id='hacker'>Sample n-gram data</h2><span id='topic+hacker'></span>

<h3>Description</h3>

<p>Frequency data for the phrases &quot;hacker&quot;, &quot;programmer&quot;, from 1950 to 2008.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hacker
</code></pre>


<h3>Format</h3>

<p>a 236 x 4 ngram data frame
</p>

<hr>
<h2 id='ngram'>Get n-gram frequencies</h2><span id='topic+ngram'></span>

<h3>Description</h3>

<p><code>ngram</code> downloads data from the Google Ngram Viewer website and
returns it in a tibble.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ngram(
  phrases,
  corpus = "en-2019",
  year_start = 1800,
  year_end = 2020,
  smoothing = 3,
  case_ins = FALSE,
  aggregate = FALSE,
  count = FALSE,
  drop_corpus = FALSE,
  drop_parent = FALSE,
  drop_all = FALSE,
  type = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ngram_+3A_phrases">phrases</code></td>
<td>
<p>vector of phrases, with a maximum of 12 items</p>
</td></tr>
<tr><td><code id="ngram_+3A_corpus">corpus</code></td>
<td>
<p>Google corpus to search (see Details for possible values)</p>
</td></tr>
<tr><td><code id="ngram_+3A_year_start">year_start</code></td>
<td>
<p>start year, default is 1800. Data available back to 1500.</p>
</td></tr>
<tr><td><code id="ngram_+3A_year_end">year_end</code></td>
<td>
<p>end year, default is 2008</p>
</td></tr>
<tr><td><code id="ngram_+3A_smoothing">smoothing</code></td>
<td>
<p>smoothing parameter, default is 3</p>
</td></tr>
<tr><td><code id="ngram_+3A_case_ins">case_ins</code></td>
<td>
<p>Logical indicating whether to force a case insensitive search.
Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="ngram_+3A_aggregate">aggregate</code></td>
<td>
<p>Sum up the frequencies for ngrams associated with wildcard
or case insensitive searches. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="ngram_+3A_count">count</code></td>
<td>
<p>Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="ngram_+3A_drop_corpus">drop_corpus</code></td>
<td>
<p>When a corpus is specified directly with the ngram
(e.g <code>dog:eng_fiction_2012</code>) specifies whether the corpus be used retained in
the phrase column of the results. Note that that this method requires that
the old corpus codes (eng_fiction_2012 not en-fiction-2012) are used. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="ngram_+3A_drop_parent">drop_parent</code></td>
<td>
<p>Drop the parent phrase associated with a wildcard
or case-insensitive search. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="ngram_+3A_drop_all">drop_all</code></td>
<td>
<p>Delete the suffix &quot;(All)&quot; from aggregated case-insensitive
searches. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="ngram_+3A_type">type</code></td>
<td>
<p>Include the Google return type (e.g. NGRAM, NGRAM_COLLECTION,
EXPANSION) from result set. Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Google generated two datasets drawn from digitised books in the Google
Books collection. One was generated in July 2009, the second in July 2012
and the third in 2019. Google is expected to update these datasets as book
scanning continues.
</p>
<p>This function provides the annual frequency of words or phrases, known
as n-grams, in a sub-collection or &quot;corpus&quot; taken from the Google Books
collection.The search across the corpus is case-sensitive.
</p>
<p>If the function is unable to retrieve data from the Google Ngram Viewer
site (either because of access issues or if the format of Google's site
has changed) a NULL result is returned and messages are printed to the
console but no errors or warnings are raised (this is to align with
CRAN package policies).
</p>
<p>Below is a list of available corpora. Note that the data for the 2012
corpuses only extends to 2009.
</p>

<table>
<tr>
 <td style="text-align: left;">
<b>Corpus</b> </td><td style="text-align: left;"> <b>Corpus Name</b></td>
</tr>
<tr>
 <td style="text-align: left;">
en-US-2019</td><td style="text-align: left;"> American English 2019</td>
</tr>
<tr>
 <td style="text-align: left;">
en-US-2012</td><td style="text-align: left;"> American English 2012</td>
</tr>
<tr>
 <td style="text-align: left;">
en-US-2009</td><td style="text-align: left;"> American English 2009</td>
</tr>
<tr>
 <td style="text-align: left;">
en-GB-2019</td><td style="text-align: left;"> British English 2019</td>
</tr>
<tr>
 <td style="text-align: left;">
en-GB-2012</td><td style="text-align: left;"> British English 2012</td>
</tr>
<tr>
 <td style="text-align: left;">
en-GB-2009</td><td style="text-align: left;"> British English 2009</td>
</tr>
<tr>
 <td style="text-align: left;">
zh-Hans-2019</td><td style="text-align: left;"> Chinese 2019</td>
</tr>
<tr>
 <td style="text-align: left;">
zh-Hans-2012</td><td style="text-align: left;"> Chinese 2012</td>
</tr>
<tr>
 <td style="text-align: left;">
zh-Hans-2009</td><td style="text-align: left;"> Chinese 2009</td>
</tr>
<tr>
 <td style="text-align: left;">
en-2019</td><td style="text-align: left;"> English 2019</td>
</tr>
<tr>
 <td style="text-align: left;">
en-2012</td><td style="text-align: left;"> English 2012</td>
</tr>
<tr>
 <td style="text-align: left;">
en-2009</td><td style="text-align: left;"> English 2009</td>
</tr>
<tr>
 <td style="text-align: left;">
en-fiction-2019</td><td style="text-align: left;"> English Fiction 2019</td>
</tr>
<tr>
 <td style="text-align: left;">
en-fiction-2012</td><td style="text-align: left;"> English Fiction 2012</td>
</tr>
<tr>
 <td style="text-align: left;">
en-fiction-2009</td><td style="text-align: left;"> English Fiction 2009</td>
</tr>
<tr>
 <td style="text-align: left;">
en-1M-2009</td><td style="text-align: left;"> English One Million</td>
</tr>
<tr>
 <td style="text-align: left;">
fr-2019</td><td style="text-align: left;"> French 2019</td>
</tr>
<tr>
 <td style="text-align: left;">
fr-2012</td><td style="text-align: left;"> French 2012</td>
</tr>
<tr>
 <td style="text-align: left;">
fr-2009</td><td style="text-align: left;"> French 2009</td>
</tr>
<tr>
 <td style="text-align: left;">
de-2019</td><td style="text-align: left;"> German 2019</td>
</tr>
<tr>
 <td style="text-align: left;">
de-2012</td><td style="text-align: left;"> German 2012</td>
</tr>
<tr>
 <td style="text-align: left;">
de-2009</td><td style="text-align: left;"> German 2009</td>
</tr>
<tr>
 <td style="text-align: left;">
iw-2019</td><td style="text-align: left;"> Hebrew 2019</td>
</tr>
<tr>
 <td style="text-align: left;">
iw-2012</td><td style="text-align: left;"> Hebrew 2012</td>
</tr>
<tr>
 <td style="text-align: left;">
iw-2009</td><td style="text-align: left;"> Hebrew 2009</td>
</tr>
<tr>
 <td style="text-align: left;">
es-2019</td><td style="text-align: left;"> Spanish 2019</td>
</tr>
<tr>
 <td style="text-align: left;">
es-2012</td><td style="text-align: left;"> Spanish 2012</td>
</tr>
<tr>
 <td style="text-align: left;">
es-2009</td><td style="text-align: left;"> Spanish 2009</td>
</tr>
<tr>
 <td style="text-align: left;">
ru-2019</td><td style="text-align: left;"> Russian 2019</td>
</tr>
<tr>
 <td style="text-align: left;">
ru-2012</td><td style="text-align: left;"> Russian 2012</td>
</tr>
<tr>
 <td style="text-align: left;">
ru-2009</td><td style="text-align: left;"> Russian 2009</td>
</tr>
<tr>
 <td style="text-align: left;">
it-2019</td><td style="text-align: left;"> Italian 2019</td>
</tr>
<tr>
 <td style="text-align: left;">
it-2012</td><td style="text-align: left;"> Italian 2012</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>The Google Million is a sub-collection of Google Books. All are in
English with dates ranging from 1500 to 2008.
No more than about 6,000 books were chosen from any one year, which
means that all of the scanned books from early years are present,
and books from later years are randomly sampled. The random samplings
reflect the subject distributions for the year (so there are more
computer books in 2000 than 1980).
</p>
<p>See <a href="http://books.google.com/ngrams/info">http://books.google.com/ngrams/info</a> for the full Ngram syntax.
</p>


<h3>Value</h3>

<p><code>ngram</code> returns an object of class &quot;<code>ngram</code>&quot;,
which is a tidyverse <code>tibble</code> enriched with attributes reflecting
some of the parameters used in the Ngram Viewer query.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ngram(c("mouse", "rat"), year_start = 1950)
ngram(c("blue_ADJ", "red_ADJ"))
ngram(c("_START_ President Roosevelt", "_START_ President Truman"), year_start = 1920)

</code></pre>

<hr>
<h2 id='ngrami'>Get n-gram frequencies (case insensitive version)</h2><span id='topic+ngrami'></span>

<h3>Description</h3>

<p>This function is a simple wrapper of <code>ngram</code> for case insensitive searches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ngrami(phrases, aggregate = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ngrami_+3A_phrases">phrases</code></td>
<td>
<p>vector of phrases</p>
</td></tr>
<tr><td><code id="ngrami_+3A_aggregate">aggregate</code></td>
<td>
<p>sum up each of the terms</p>
</td></tr>
<tr><td><code id="ngrami_+3A_...">...</code></td>
<td>
<p>remaining parameters passed to ngram</p>
</td></tr>
</table>

<hr>
<h2 id='ngramr'>ngramr: Dig into the Google Ngram Viewer using R</h2><span id='topic+ngramr'></span><span id='topic+_PACKAGE'></span><span id='topic+ngramr-package'></span>

<h3>Description</h3>

<p>The <a href="http://books.google.com/ngrams">Google Books Ngram Viewer</a>
allows you to enter a list of phrases and then displays a graph showing
how often the phrases have occurred in a corpus of books
(e.g., &quot;British English&quot;, &quot;English Fiction&quot;, &quot;French&quot;) over time.
The underlying data is hidden in web page, embedded in some Javascript.
</p>
<p>This package extracts the data an provides it in the form of an R dataframe.
</p>
<p>The key function is <code>ngram</code> which, given a collection of
phrases, returns a dataframe containing the frequencies by year.
</p>
<p>The code is based on the <code>getNgrams.py</code> Python script available on
<a href="http://www.culturomics.org/Resources/get-ngrams">Culturomics Code</a>
written by Jean-Baptiste Michel. The
<a href="http://www.culturomics.org/home">Culturomics</a>
website itself is worth exploring.
</p>
<p>Note that compared to the 2009 versions, the 2012 and 2019 versions have
larger numbers of books, improved OCR, improved library and publisher
metadata. The 2012 and 2019 corpuses also don't form ngrams that cross
sentence boundaries, and do form ngrams across page boundaries and
support part of speech tagging, unlike the 2009 versions.
</p>
<p>Like the Google Ngram Viewer website itself, this package is aimed at for
quick inquiries into the usage of small sets of phrases.
</p>
<p>Please respect the terms of service of the Google Books Ngram Viewer while
using this code. This code is meant to help viewers retrieve data behind
a few queries, not bang at Google's  servers with dozens of queries.
The complete dataset can be
<a href="https://storage.googleapis.com/books/ngrams/books/datasetsv3.html">downloaded here</a>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Sean Carmody <a href="mailto:seancarmody@gmail.com">seancarmody@gmail.com</a> [copyright holder]
</p>


<h3>References</h3>

<p>Michel, Jean-Baptiste, et al. &quot;Quantitative analysis of culture using
millions of digitized books.&quot; <em>Science</em> 331, No. 6014 (2011): 176&ndash;182.
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/seancarmody/ngramr">https://github.com/seancarmody/ngramr</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/seancarmody/ngramr/issues">https://github.com/seancarmody/ngramr/issues</a>
</p>
</li></ul>


<hr>
<h2 id='ngramw'>Get n-gram frequencies (&quot;wide&quot; format)</h2><span id='topic+ngramw'></span>

<h3>Description</h3>

<p>Get n-gram frequencies (&quot;wide&quot; format)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ngramw(phrases, ignore_case = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ngramw_+3A_phrases">phrases</code></td>
<td>
<p>vector of phrases</p>
</td></tr>
<tr><td><code id="ngramw_+3A_ignore_case">ignore_case</code></td>
<td>
<p>ignore case of phrases (i.e. call <code>ngrami</code>
rather than <code>ngram</code>). Default value is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="ngramw_+3A_...">...</code></td>
<td>
<p>remaining parameters passed to <code>ngram</code></p>
</td></tr>
</table>

<hr>
<h2 id='print.ngram'>Print n-gram contents</h2><span id='topic+print.ngram'></span>

<h3>Description</h3>

<p>Print n-gram contents
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ngram'
print(x, rows = 6, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.ngram_+3A_x">x</code></td>
<td>
<p>ngram object as returned by <code>link{ngram}</code></p>
</td></tr>
<tr><td><code id="print.ngram_+3A_rows">rows</code></td>
<td>
<p>number of rows to print. Default is 6.</p>
</td></tr>
<tr><td><code id="print.ngram_+3A_...">...</code></td>
<td>
<p>additional parameters passed to default print method.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- ngram(c("hacker", "programmer"), year_start = 1950)
print(x)

</code></pre>

<hr>
<h2 id='theme_google'>Google Ngram theme for ggplot2</h2><span id='topic+theme_google'></span>

<h3>Description</h3>

<p>Google Ngram theme for ggplot2
</p>


<h3>Usage</h3>

<pre><code class='language-R'>theme_google(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="theme_google_+3A_...">...</code></td>
<td>
<p>additional parameters to pass to <code>theme</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Use a Google Ngram-style plot theme.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
