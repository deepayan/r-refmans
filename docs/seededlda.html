<!DOCTYPE html><html><head><title>Help for package seededlda</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {seededlda}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#data_corpus_moviereviews'><p>Movie reviews from Pang and Lee (2004)</p></a></li>
<li><a href='#divergence'><p>Optimize the number of topics</p></a></li>
<li><a href='#print.textmodel_lda'><p>Print method for a LDA model</p></a></li>
<li><a href='#sizes'><p>Compute the sizes of topics</p></a></li>
<li><a href='#terms'><p>Extract most likely terms</p></a></li>
<li><a href='#textmodel_lda'><p>Unsupervised Latent Dirichlet allocation</p></a></li>
<li><a href='#textmodel_seededlda'><p>Semisupervised Latent Dirichlet allocation</p></a></li>
<li><a href='#textmodel_seqlda'><p>Sequential Latent Dirichlet allocation</p></a></li>
<li><a href='#topics'><p>Extract most likely topics</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Seeded Sequential LDA for Topic Modeling</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Seeded Sequential LDA can classify sentences of texts into pre-define topics with a small number of seed words (Watanabe &amp; Baturo, 2023) &lt;<a href="https://doi.org/10.1177%2F08944393231178605">doi:10.1177/08944393231178605</a>&gt;.
    Implements Seeded LDA (Lu et al., 2010) &lt;<a href="https://doi.org/10.1109%2FICDMW.2011.125">doi:10.1109/ICDMW.2011.125</a>&gt; and Sequential LDA (Du et al., 2012) &lt;<a href="https://doi.org/10.1007%2Fs10115-011-0425-1">doi:10.1007/s10115-011-0425-1</a>&gt; with the distributed LDA algorithm (Newman, et al., 2009) for parallel computing.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/koheiw/seededlda">https://github.com/koheiw/seededlda</a>,
<a href="https://koheiw.github.io/seededlda/">https://koheiw.github.io/seededlda/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/koheiw/seededlda/issues">https://github.com/koheiw/seededlda/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), quanteda (&ge; 4.0.0), methods, proxyC (&ge; 0.3.1)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Matrix</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo (&ge; 0.7.600.1.0), quanteda, testthat</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, topicmodels, keyATM</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-04-08 01:35:56 UTC; watan</td>
</tr>
<tr>
<td>Author:</td>
<td>Kohei Watanabe [aut, cre, cph],
  Phan Xuan-Hieu [aut, cph] (GibbsLDA++)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kohei Watanabe &lt;watanabe.kohei@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-04-10 06:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='data_corpus_moviereviews'>Movie reviews from Pang and Lee (2004)</h2><span id='topic+data_corpus_moviereviews'></span>

<h3>Description</h3>

<p>A corpus object containing 2,000 movie reviews.
</p>


<h3>Source</h3>

<p><a href="https://www.cs.cornell.edu/people/pabo/movie-review-data/">https://www.cs.cornell.edu/people/pabo/movie-review-data/</a>
</p>


<h3>References</h3>

<p>Pang, B., Lee, L.  (2004)
&quot;<a href="https://www.cs.cornell.edu/home/llee/papers/cutsent.pdf">A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts.</a>&quot;, Proceedings of the ACL.
</p>

<hr>
<h2 id='divergence'>Optimize the number of topics</h2><span id='topic+divergence'></span>

<h3>Description</h3>

<p><code>divergence()</code> computes the regularized topic divergence to find the optimal
number of topics for LDA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>divergence(x, min_size = 0.01, select = NULL, regularize = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="divergence_+3A_x">x</code></td>
<td>
<p>a LDA model fitted by <code><a href="#topic+textmodel_seededlda">textmodel_seededlda()</a></code> or <code><a href="#topic+textmodel_lda">textmodel_lda()</a></code>.</p>
</td></tr>
<tr><td><code id="divergence_+3A_min_size">min_size</code></td>
<td>
<p>the minimum size of topics for regularized topic divergence.
Ignored when <code>regularize = FALSE</code>.</p>
</td></tr>
<tr><td><code id="divergence_+3A_select">select</code></td>
<td>
<p>names of topics for which the divergence is computed.</p>
</td></tr>
<tr><td><code id="divergence_+3A_regularize">regularize</code></td>
<td>
<p>if <code>TRUE</code>, returns the regularized divergence.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>divergence()</code> computes the average Jensen-Shannon divergence
between all the pairs of topic vectors in <code>x$phi</code>. The divergence score
maximizes when the chosen number of topic <code>k</code> is optimal (Deveaud et al.,
2014). The regularized divergence penalizes topics smaller than <code>min_size</code>
to avoid fragmentation (Watanabe &amp; Baturo, forthcoming).
</p>


<h3>References</h3>

<p>Deveaud, Romain et al. (2014). &quot;Accurate and Effective Latent
Concept Modeling for Ad Hoc Information Retrieval&quot;.
doi:10.3166/DN.17.1.61-84. <em>Document Num√©rique</em>.
</p>
<p>Watanabe, Kohei &amp; Baturo, Alexander. (2023). &quot;Seeded Sequential LDA:
A Semi-supervised Algorithm for Topic-specific Analysis of Sentences&quot;.
doi:10.1177/08944393231178605. <em>Social Science Computer Review</em>.
</p>


<h3>See Also</h3>

<p><a href="#topic+sizes">sizes</a>
</p>

<hr>
<h2 id='print.textmodel_lda'>Print method for a LDA model</h2><span id='topic+print.textmodel_lda'></span>

<h3>Description</h3>

<p>Print method for a LDA model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'textmodel_lda'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.textmodel_lda_+3A_x">x</code></td>
<td>
<p>for print method, the object to be printed</p>
</td></tr>
<tr><td><code id="print.textmodel_lda_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>

<hr>
<h2 id='sizes'>Compute the sizes of topics</h2><span id='topic+sizes'></span>

<h3>Description</h3>

<p>Compute the sizes of topics as the proportions of topic words in the corpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sizes(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sizes_+3A_x">x</code></td>
<td>
<p>a LDA model fitted by <code><a href="#topic+textmodel_seededlda">textmodel_seededlda()</a></code> or <code><a href="#topic+textmodel_lda">textmodel_lda()</a></code></p>
</td></tr>
</table>

<hr>
<h2 id='terms'>Extract most likely terms</h2><span id='topic+terms'></span>

<h3>Description</h3>

<p><code>terms()</code> returns the most likely terms, or words, for topics based on the
<code>phi</code> parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>terms(x, n = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="terms_+3A_x">x</code></td>
<td>
<p>a LDA model fitted by <code><a href="#topic+textmodel_seededlda">textmodel_seededlda()</a></code> or <code><a href="#topic+textmodel_lda">textmodel_lda()</a></code></p>
</td></tr>
<tr><td><code id="terms_+3A_n">n</code></td>
<td>
<p>number of terms to be extracted</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Users can access the original matrix <code>x$phi</code> for likelihood scores.
</p>

<hr>
<h2 id='textmodel_lda'>Unsupervised Latent Dirichlet allocation</h2><span id='topic+textmodel_lda'></span>

<h3>Description</h3>

<p>Implements unsupervised Latent Dirichlet allocation (LDA). Users can run
Seeded LDA by setting <code>gamma &gt; 0</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textmodel_lda(
  x,
  k = 10,
  max_iter = 2000,
  auto_iter = FALSE,
  alpha = 0.5,
  beta = 0.1,
  gamma = 0,
  model = NULL,
  batch_size = 1,
  verbose = quanteda_options("verbose")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textmodel_lda_+3A_x">x</code></td>
<td>
<p>the dfm on which the model will be fit.</p>
</td></tr>
<tr><td><code id="textmodel_lda_+3A_k">k</code></td>
<td>
<p>the number of topics.</p>
</td></tr>
<tr><td><code id="textmodel_lda_+3A_max_iter">max_iter</code></td>
<td>
<p>the maximum number of iteration in Gibbs sampling.</p>
</td></tr>
<tr><td><code id="textmodel_lda_+3A_auto_iter">auto_iter</code></td>
<td>
<p>if <code>TRUE</code>, stops Gibbs sampling on convergence before
reaching <code>max_iter</code>. See details.</p>
</td></tr>
<tr><td><code id="textmodel_lda_+3A_alpha">alpha</code></td>
<td>
<p>the values to smooth topic-document distribution.</p>
</td></tr>
<tr><td><code id="textmodel_lda_+3A_beta">beta</code></td>
<td>
<p>the values to smooth topic-word distribution.</p>
</td></tr>
<tr><td><code id="textmodel_lda_+3A_gamma">gamma</code></td>
<td>
<p>a parameter to determine change of topics between sentences or
paragraphs. When <code>gamma &gt; 0</code>, Gibbs sampling of topics for the current
document is affected by the previous document's topics.</p>
</td></tr>
<tr><td><code id="textmodel_lda_+3A_model">model</code></td>
<td>
<p>a fitted LDA model; if provided, <code>textmodel_lda()</code> inherits
parameters from an existing model. See details.</p>
</td></tr>
<tr><td><code id="textmodel_lda_+3A_batch_size">batch_size</code></td>
<td>
<p>split the corpus into the smaller batches (specified in
proportion) for distributed computing; it is disabled when a batch include
all the documents <code>batch_size = 1.0</code>. See details.</p>
</td></tr>
<tr><td><code id="textmodel_lda_+3A_verbose">verbose</code></td>
<td>
<p>logical; if <code>TRUE</code> print diagnostic information during
fitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>auto_iter = TRUE</code>, the iteration stops even before <code>max_iter</code>
when <code>delta &lt;= 0</code>. <code>delta</code> is computed to measure the changes in the number
of words whose topics are updated by the Gibbs sampler in every 100
iteration as shown in the verbose message.
</p>
<p>If <code>batch_size &lt; 1.0</code>, the corpus is partitioned into sub-corpora of
<code>ndoc(x) * batch_size</code> documents for Gibbs sampling in sub-processes with
synchronization of parameters in every 10 iteration. Parallel processing is
more efficient when <code>batch_size</code> is small (e.g. 0.01). The algorithm is the
Approximate Distributed LDA proposed by Newman et al. (2009). User can
changed the number of sub-processes used for the parallel computing via
<code>options(seededlda_threads)</code>.
</p>
<p><code>set.seed()</code> should be called immediately before <code>textmodel_lda()</code> or
<code>textmodel_seededlda()</code> to control random topic assignment. If the random
number seed is the same, the serial algorithm produces identical results;
the parallel algorithm produces non-identical results because it
classifies documents in different orders using multiple processors.
</p>
<p>To predict topics of new documents (i.e. out-of-sample), first, create a
new LDA model from a existing LDA model passed to <code>model</code> in
<code>textmodel_lda()</code>; second, apply <code><a href="#topic+topics">topics()</a></code> to the new model. The <code>model</code>
argument takes objects created either by <code>textmodel_lda()</code> or
<code>textmodel_seededlda()</code>.
</p>


<h3>Value</h3>

<p><code>textmodel_seededlda()</code> and <code>textmodel_lda()</code> returns a list of model
parameters. <code>theta</code> is the distribution of topics over documents; <code>phi</code> is
the distribution of words over topics. <code>alpha</code> and <code>beta</code> are the small
constant added to the frequency of words to estimate <code>theta</code> and <code>phi</code>,
respectively, in Gibbs sampling. Other elements in the list subject to
change.
</p>


<h3>References</h3>

<p>Newman, D., Asuncion, A., Smyth, P., &amp; Welling, M. (2009). Distributed
Algorithms for Topic Models. The Journal of Machine Learning Research, 10,
1801‚Äì1828.
</p>


<h3>See Also</h3>

<p><a href="topicmodels.html#topic+lda">LDA</a> <a href="keyATM.html#topic+weightedLDA">weightedLDA</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require(seededlda)
require(quanteda)

corp &lt;- head(data_corpus_moviereviews, 500)
toks &lt;- tokens(corp, remove_punct = TRUE, remove_symbols = TRUE, remove_number = TRUE)
dfmt &lt;- dfm(toks) %&gt;%
    dfm_remove(stopwords("en"), min_nchar = 2) %&gt;%
    dfm_trim(max_docfreq = 0.1, docfreq_type = "prop")

lda &lt;- textmodel_lda(dfmt, k = 6, max_iter = 500) # 6 topics
terms(lda)
topics(lda)

</code></pre>

<hr>
<h2 id='textmodel_seededlda'>Semisupervised Latent Dirichlet allocation</h2><span id='topic+textmodel_seededlda'></span>

<h3>Description</h3>

<p>Implements semisupervised Latent Dirichlet allocation
(Seeded LDA). <code>textmodel_seededlda()</code> allows users to specify
topics using a seed word dictionary. Users can run Seeded Sequential LDA by
setting <code>gamma &gt; 0</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textmodel_seededlda(
  x,
  dictionary,
  levels = 1,
  valuetype = c("glob", "regex", "fixed"),
  case_insensitive = TRUE,
  residual = 0,
  weight = 0.01,
  max_iter = 2000,
  auto_iter = FALSE,
  alpha = 0.5,
  beta = 0.1,
  gamma = 0,
  batch_size = 1,
  ...,
  verbose = quanteda_options("verbose")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textmodel_seededlda_+3A_x">x</code></td>
<td>
<p>the dfm on which the model will be fit.</p>
</td></tr>
<tr><td><code id="textmodel_seededlda_+3A_dictionary">dictionary</code></td>
<td>
<p>a <code><a href="quanteda.html#topic+dictionary">quanteda::dictionary()</a></code> with seed words that define
topics.</p>
</td></tr>
<tr><td><code id="textmodel_seededlda_+3A_levels">levels</code></td>
<td>
<p>levels of entities in a hierarchical dictionary to be used as
seed words. See also <a href="quanteda.html#topic+flatten_dictionary">quanteda::flatten_dictionary</a>.</p>
</td></tr>
<tr><td><code id="textmodel_seededlda_+3A_valuetype">valuetype</code></td>
<td>
<p>see <a href="quanteda.html#topic+valuetype">quanteda::valuetype</a></p>
</td></tr>
<tr><td><code id="textmodel_seededlda_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>see <a href="quanteda.html#topic+valuetype">quanteda::valuetype</a></p>
</td></tr>
<tr><td><code id="textmodel_seededlda_+3A_residual">residual</code></td>
<td>
<p>the number of undefined topics. They are named &quot;other&quot; by
default, but it can be changed via <code>base::options(seededlda_residual_name)</code>.</p>
</td></tr>
<tr><td><code id="textmodel_seededlda_+3A_weight">weight</code></td>
<td>
<p>determines the size of pseudo counts given to matched seed words.</p>
</td></tr>
<tr><td><code id="textmodel_seededlda_+3A_max_iter">max_iter</code></td>
<td>
<p>the maximum number of iteration in Gibbs sampling.</p>
</td></tr>
<tr><td><code id="textmodel_seededlda_+3A_auto_iter">auto_iter</code></td>
<td>
<p>if <code>TRUE</code>, stops Gibbs sampling on convergence before
reaching <code>max_iter</code>. See details.</p>
</td></tr>
<tr><td><code id="textmodel_seededlda_+3A_alpha">alpha</code></td>
<td>
<p>the values to smooth topic-document distribution.</p>
</td></tr>
<tr><td><code id="textmodel_seededlda_+3A_beta">beta</code></td>
<td>
<p>the values to smooth topic-word distribution.</p>
</td></tr>
<tr><td><code id="textmodel_seededlda_+3A_gamma">gamma</code></td>
<td>
<p>a parameter to determine change of topics between sentences or
paragraphs. When <code>gamma &gt; 0</code>, Gibbs sampling of topics for the current
document is affected by the previous document's topics.</p>
</td></tr>
<tr><td><code id="textmodel_seededlda_+3A_batch_size">batch_size</code></td>
<td>
<p>split the corpus into the smaller batches (specified in
proportion) for distributed computing; it is disabled when a batch include
all the documents <code>batch_size = 1.0</code>. See details.</p>
</td></tr>
<tr><td><code id="textmodel_seededlda_+3A_...">...</code></td>
<td>
<p>passed to <a href="quanteda.html#topic+dfm_trim">quanteda::dfm_trim</a> to restrict seed words based on
their term or document frequency. This is useful when glob patterns in the
dictionary match too many words.</p>
</td></tr>
<tr><td><code id="textmodel_seededlda_+3A_verbose">verbose</code></td>
<td>
<p>logical; if <code>TRUE</code> print diagnostic information during
fitting.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Lu, Bin et al. (2011). &quot;Multi-aspect Sentiment Analysis with
Topic Models&quot;. doi:10.5555/2117693.2119585. <em>Proceedings of the 2011 IEEE
11th International Conference on Data Mining Workshops</em>.
</p>
<p>Watanabe, Kohei &amp; Zhou, Yuan (2020). &quot;Theory-Driven Analysis of Large
Corpora: Semisupervised Topic Classification of the UN Speeches&quot;.
doi:10.1177/0894439320907027. <em>Social Science Computer Review</em>.
</p>
<p>Watanabe, Kohei &amp; Baturo, Alexander. (2023). &quot;Seeded Sequential LDA:
A Semi-supervised Algorithm for Topic-specific Analysis of Sentences&quot;.
doi:10.1177/08944393231178605. <em>Social Science Computer Review</em>.
</p>


<h3>See Also</h3>

<p><a href="keyATM.html#topic+keyATM">keyATM</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require(seededlda)
require(quanteda)

corp &lt;- head(data_corpus_moviereviews, 500)
toks &lt;- tokens(corp, remove_punct = TRUE, remove_symbols = TRUE, remove_number = TRUE)
dfmt &lt;- dfm(toks) %&gt;%
    dfm_remove(stopwords("en"), min_nchar = 2) %&gt;%
    dfm_trim(max_docfreq = 0.1, docfreq_type = "prop")

dict &lt;- dictionary(list(people = c("family", "couple", "kids"),
                        space = c("alien", "planet", "space"),
                        moster = c("monster*", "ghost*", "zombie*"),
                        war = c("war", "soldier*", "tanks"),
                        crime = c("crime*", "murder", "killer")))
lda_seed &lt;- textmodel_seededlda(dfmt, dict, residual = TRUE, min_termfreq = 10,
                                max_iter = 500)
terms(lda_seed)
topics(lda_seed)

</code></pre>

<hr>
<h2 id='textmodel_seqlda'>Sequential Latent Dirichlet allocation</h2><span id='topic+textmodel_seqlda'></span>

<h3>Description</h3>

<p>Implements Sequential Latent Dirichlet allocation (Sequential LDA).
<code>textmodel_seqlda()</code> allows the users to classify sentences of texts. It
considers the topics of previous document in inferring the topics of currency
document. <code>textmodel_seqlda()</code> is a shortcut equivalent to
<code>textmodel_lda(gamma = 0.5)</code>. Seeded Sequential LDA is
<code>textmodel_seededlda(gamma = 0.5)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textmodel_seqlda(
  x,
  k = 10,
  max_iter = 2000,
  auto_iter = FALSE,
  alpha = 0.5,
  beta = 0.1,
  batch_size = 1,
  model = NULL,
  verbose = quanteda_options("verbose")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textmodel_seqlda_+3A_x">x</code></td>
<td>
<p>the dfm on which the model will be fit.</p>
</td></tr>
<tr><td><code id="textmodel_seqlda_+3A_k">k</code></td>
<td>
<p>the number of topics.</p>
</td></tr>
<tr><td><code id="textmodel_seqlda_+3A_max_iter">max_iter</code></td>
<td>
<p>the maximum number of iteration in Gibbs sampling.</p>
</td></tr>
<tr><td><code id="textmodel_seqlda_+3A_auto_iter">auto_iter</code></td>
<td>
<p>if <code>TRUE</code>, stops Gibbs sampling on convergence before
reaching <code>max_iter</code>. See details.</p>
</td></tr>
<tr><td><code id="textmodel_seqlda_+3A_alpha">alpha</code></td>
<td>
<p>the values to smooth topic-document distribution.</p>
</td></tr>
<tr><td><code id="textmodel_seqlda_+3A_beta">beta</code></td>
<td>
<p>the values to smooth topic-word distribution.</p>
</td></tr>
<tr><td><code id="textmodel_seqlda_+3A_batch_size">batch_size</code></td>
<td>
<p>split the corpus into the smaller batches (specified in
proportion) for distributed computing; it is disabled when a batch include
all the documents <code>batch_size = 1.0</code>. See details.</p>
</td></tr>
<tr><td><code id="textmodel_seqlda_+3A_model">model</code></td>
<td>
<p>a fitted LDA model; if provided, <code>textmodel_lda()</code> inherits
parameters from an existing model. See details.</p>
</td></tr>
<tr><td><code id="textmodel_seqlda_+3A_verbose">verbose</code></td>
<td>
<p>logical; if <code>TRUE</code> print diagnostic information during
fitting.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Du, Lan et al. (2012). &quot;Sequential Latent Dirichlet Allocation&quot;.
doi.org/10.1007/s10115-011-0425-1. <em>Knowledge and Information Systems</em>.
</p>
<p>Watanabe, Kohei &amp; Baturo, Alexander. (2023). &quot;Seeded Sequential LDA:
A Semi-supervised Algorithm for Topic-specific Analysis of Sentences&quot;.
doi:10.1177/08944393231178605. <em>Social Science Computer Review</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require(seededlda)
require(quanteda)

corp &lt;- head(data_corpus_moviereviews, 500) %&gt;%
    corpus_reshape()
toks &lt;- tokens(corp, remove_punct = TRUE, remove_symbols = TRUE, remove_number = TRUE)
dfmt &lt;- dfm(toks) %&gt;%
    dfm_remove(stopwords("en"), min_nchar = 2) %&gt;%
    dfm_trim(max_docfreq = 0.01, docfreq_type = "prop")

lda_seq &lt;- textmodel_seqlda(dfmt, k = 6, max_iter = 500) # 6 topics
terms(lda_seq)
topics(lda_seq)

</code></pre>

<hr>
<h2 id='topics'>Extract most likely topics</h2><span id='topic+topics'></span>

<h3>Description</h3>

<p><code>topics()</code> returns the most likely topics for documents based on the <code>theta</code>
parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>topics(x, min_prob = 0, select = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="topics_+3A_x">x</code></td>
<td>
<p>a LDA model fitted by <code><a href="#topic+textmodel_seededlda">textmodel_seededlda()</a></code> or <code><a href="#topic+textmodel_lda">textmodel_lda()</a></code></p>
</td></tr>
<tr><td><code id="topics_+3A_min_prob">min_prob</code></td>
<td>
<p>ignores topics if their probability is lower than this value.</p>
</td></tr>
<tr><td><code id="topics_+3A_select">select</code></td>
<td>
<p>returns the selected topic with the
highest probability; specify by the names of columns in <code>x$theta</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Users can access the original matrix <code>x$theta</code> for likelihood
scores; run <code>max.col(x$theta)</code> to obtain the same result as <code>topics(x)</code>.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
