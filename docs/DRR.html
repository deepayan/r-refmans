<!DOCTYPE html><html><head><title>Help for package DRR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {DRR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#constructFastKRRLearner'><p>Fast implementation for Kernel Ridge Regression.</p></a></li>
<li><a href='#drr'><p>Dimensionality Reduction via Regression</p></a></li>
<li><a href='#DRR-package'><p>Dimensionality Reduction via Regression.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Dimensionality Reduction via Regression</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.4</td>
</tr>
<tr>
<td>Description:</td>
<td>An Implementation of Dimensionality Reduction
    via Regression using Kernel Ridge Regression.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> | file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/gdkrmr/DRR">https://github.com/gdkrmr/DRR</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/gdkrmr/DRR/issues">https://github.com/gdkrmr/DRR/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, methods</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>kernlab, CVST, Matrix</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.0.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-02-12 12:31:48 UTC; gkraemer</td>
</tr>
<tr>
<td>Author:</td>
<td>Guido Kraemer [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Guido Kraemer &lt;gkraemer@bgc-jena.mpg.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-02-12 13:10:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='constructFastKRRLearner'>Fast implementation for Kernel Ridge Regression.</h2><span id='topic+constructFastKRRLearner'></span>

<h3>Description</h3>

<p>Constructs a learner for the divide and conquer version of KRR.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>constructFastKRRLearner()
</code></pre>


<h3>Details</h3>

<p>This function is to be used with the CVST package as a drop in
replacement for <code><a href="CVST.html#topic+constructKRRLearner">constructKRRLearner</a></code>. The
implementation approximates the inversion of the kernel Matrix
using the divide an conquer scheme, lowering computational and
memory complexity from <code class="reqn">O(n^3)</code> and <code class="reqn">O(n^2)</code> to
<code class="reqn">O(n^3/m^2)</code> and <code class="reqn">O(n^2/m^2)</code> respectively, where m are the
number of blocks to be used (parameter nblocks). Theoretically safe
values for <code class="reqn">m</code> are <code class="reqn">&lt; n^{1/3}</code>, but practically <code class="reqn">m</code> may
be a little bit larger. The function will issue a warning, if the
value for <code class="reqn">m</code> is too large.
</p>


<h3>Value</h3>

<p>Returns a learner similar to <code><a href="CVST.html#topic+constructKRRLearner">constructKRRLearner</a></code>
suitable for the use with <code><a href="CVST.html#topic+CV">CV</a></code> and
<code><a href="CVST.html#topic+fastCV">fastCV</a></code>.
</p>


<h3>References</h3>

<p>Zhang, Y., Duchi, J.C., Wainwright, M.J., 2013. Divide and Conquer
Kernel Ridge Regression: A Distributed Algorithm with Minimax
Optimal Rates. arXiv:1305.5029 [cs, math, stat].
</p>


<h3>See Also</h3>

<p><code><a href="CVST.html#topic+constructLearner">constructLearner</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ns &lt;- noisySinc(1000)
nsTest &lt;- noisySinc(1000)

fast.krr &lt;- constructFastKRRLearner()
fast.p &lt;- list(kernel="rbfdot", sigma=100, lambda=.1/getN(ns), nblocks = 4)
system.time(fast.m &lt;- fast.krr$learn(ns, fast.p))
fast.pred &lt;- fast.krr$predict(fast.m, nsTest)
sum((fast.pred - nsTest$y)^2) / getN(nsTest)

## Not run: 
krr &lt;- CVST::constructKRRLearner()
p &lt;- list(kernel="rbfdot", sigma=100, lambda=.1/getN(ns))
system.time(m &lt;- krr$learn(ns, p))
pred &lt;- krr$predict(m, nsTest)
sum((pred - nsTest$y)^2) / getN(nsTest)

plot(ns, col = '#00000030', pch = 19)
lines(sort(nsTest$x), fast.pred[order(nsTest$x)], col = '#00C000', lty = 2)
lines(sort(nsTest$x), pred[order(nsTest$x)], col = '#0000C0', lty = 2)
legend('topleft', legend = c('fast KRR', 'KRR'),
       col = c('#00C000', '#0000C0'), lty = 2)

## End(Not run)

</code></pre>

<hr>
<h2 id='drr'>Dimensionality Reduction via Regression</h2><span id='topic+drr'></span>

<h3>Description</h3>

<p><code>drr</code> Implements Dimensionality Reduction via Regression using
Kernel Ridge Regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>drr(
  X,
  ndim = ncol(X),
  lambda = c(0, 10^(-3:2)),
  kernel = "rbfdot",
  kernel.pars = list(sigma = 10^(-3:4)),
  pca = TRUE,
  pca.center = TRUE,
  pca.scale = FALSE,
  fastcv = FALSE,
  cv.folds = 5,
  fastcv.test = NULL,
  fastkrr.nblocks = 4,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="drr_+3A_x">X</code></td>
<td>
<p>input data, a matrix.</p>
</td></tr>
<tr><td><code id="drr_+3A_ndim">ndim</code></td>
<td>
<p>the number of output dimensions and regression
functions to be estimated, see details for inversion.</p>
</td></tr>
<tr><td><code id="drr_+3A_lambda">lambda</code></td>
<td>
<p>the penalty term for the Kernel Ridge Regression.</p>
</td></tr>
<tr><td><code id="drr_+3A_kernel">kernel</code></td>
<td>
<p>a kernel function or string, see
<code><a href="kernlab.html#topic+kernel-class">kernel-class</a></code> for details.</p>
</td></tr>
<tr><td><code id="drr_+3A_kernel.pars">kernel.pars</code></td>
<td>
<p>a list with parameters for the kernel. each
parameter can be a vector, crossvalidation will choose the best
combination.</p>
</td></tr>
<tr><td><code id="drr_+3A_pca">pca</code></td>
<td>
<p>logical, do a preprocessing using pca.</p>
</td></tr>
<tr><td><code id="drr_+3A_pca.center">pca.center</code></td>
<td>
<p>logical, center data before applying pca.</p>
</td></tr>
<tr><td><code id="drr_+3A_pca.scale">pca.scale</code></td>
<td>
<p>logical, scale data before applying pca.</p>
</td></tr>
<tr><td><code id="drr_+3A_fastcv">fastcv</code></td>
<td>
<p>if <code>TRUE</code> uses <code><a href="CVST.html#topic+fastCV">fastCV</a></code>, if
<code>FALSE</code> uses <code><a href="CVST.html#topic+CV">CV</a></code> for crossvalidation.</p>
</td></tr>
<tr><td><code id="drr_+3A_cv.folds">cv.folds</code></td>
<td>
<p>if using normal crossvalidation, the number of
folds to be used.</p>
</td></tr>
<tr><td><code id="drr_+3A_fastcv.test">fastcv.test</code></td>
<td>
<p>an optional separate test data set to be used
for <code><a href="CVST.html#topic+fastCV">fastCV</a></code>, handed over as option
<code>test</code> to <code><a href="CVST.html#topic+fastCV">fastCV</a></code>.</p>
</td></tr>
<tr><td><code id="drr_+3A_fastkrr.nblocks">fastkrr.nblocks</code></td>
<td>
<p>the number of blocks used for fast KRR,
higher numbers are faster to compute but may introduce
numerical inaccurracies, see
<code><a href="#topic+constructFastKRRLearner">constructFastKRRLearner</a></code> for details.</p>
</td></tr>
<tr><td><code id="drr_+3A_verbose">verbose</code></td>
<td>
<p>logical, should the crossvalidation report back.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Parameter combination will be formed and cross-validation used to
select the best combination. Cross-validation uses
<code><a href="CVST.html#topic+CV">CV</a></code> or <code><a href="CVST.html#topic+fastCV">fastCV</a></code>.
</p>
<p>Pre-treatment of the data using a PCA and scaling is made
<code class="reqn">\alpha = Vx</code>.  the representation in reduced dimensions is
</p>
<p style="text-align: center;"><code class="reqn">y_i = \alpha - f_i(\alpha_1, \ldots, \alpha_{i-1})</code>
</p>

<p>then the final DRR representation is:
</p>
<p style="text-align: center;"><code class="reqn">r = (\alpha_1, y_2, y_3, \ldots,y_d)</code>
</p>

<p>DRR is invertible by
</p>
<p style="text-align: center;"><code class="reqn">\alpha_i = y_i + f_i(\alpha_1,\alpha_2, \ldots, alpha_{i-1})</code>
</p>

<p>If less dimensions are estimated, there will be less inverse
functions and calculating the inverse will be inaccurate.
</p>


<h3>Value</h3>

<p>A list the following items:
</p>

<ul>
<li> <p>&quot;fitted.data&quot; The data in reduced dimensions.
</p>
</li>
<li> <p>&quot;pca.means&quot; The means used to center the original data.
</p>
</li>
<li> <p>&quot;pca.scale&quot; The standard deviations used to scale the original data.
</p>
</li>
<li> <p>&quot;pca.rotation&quot; The rotation matrix of the PCA.
</p>
</li>
<li> <p>&quot;models&quot; A list of models used to estimate each dimension.
</p>
</li>
<li> <p>&quot;apply&quot; A function to fit new data to the estimated model.
</p>
</li>
<li> <p>&quot;inverse&quot; A function to untransform data.
</p>
</li></ul>



<h3>References</h3>

<p>Laparra, V., Malo, J., Camps-Valls, G., 2015. Dimensionality
Reduction via Regression in Hyperspectral Imagery. IEEE Journal
of Selected Topics in Signal Processing 9,
1026-1036. doi:10.1109/JSTSP.2015.2417833
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tt &lt;- seq(0,4*pi, length.out = 200)
helix &lt;- cbind(
  x = 3 * cos(tt) + rnorm(length(tt), sd = seq(0.1, 1.4, length.out = length(tt))),
  y = 3 * sin(tt) + rnorm(length(tt), sd = seq(0.1, 1.4, length.out = length(tt))),
  z = 2 * tt      + rnorm(length(tt), sd = seq(0.1, 1.4, length.out = length(tt)))
)
helix &lt;- helix[sample(nrow(helix)),] # shuffling data is important!!
system.time(
drr.fit  &lt;- drr(helix, ndim = 3, cv.folds = 4,
                lambda = 10^(-2:1),
                kernel.pars = list(sigma = 10^(0:3)),
                fastkrr.nblocks = 2, verbose = TRUE,
                fastcv = FALSE)
)

## Not run: 
library(rgl)
plot3d(helix)
points3d(drr.fit$inverse(drr.fit$fitted.data[,1,drop = FALSE]), col = 'blue')
points3d(drr.fit$inverse(drr.fit$fitted.data[,1:2]),             col = 'red')

plot3d(drr.fit$fitted.data)
pad &lt;- -3
fd &lt;- drr.fit$fitted.data
xx &lt;- seq(min(fd[,1]),       max(fd[,1]),       length.out = 25)
yy &lt;- seq(min(fd[,2]) - pad, max(fd[,2]) + pad, length.out = 5)
zz &lt;- seq(min(fd[,3]) - pad, max(fd[,3]) + pad, length.out = 5)

dd &lt;- as.matrix(expand.grid(xx, yy, zz))
plot3d(helix)
for(y in yy) for(x in xx)
  rgl.linestrips(drr.fit$inverse(cbind(x, y, zz)), col = 'blue')
for(y in yy) for(z in zz)
  rgl.linestrips(drr.fit$inverse(cbind(xx, y, z)), col = 'blue')
for(x in xx) for(z in zz)
  rgl.linestrips(drr.fit$inverse(cbind(x, yy, z)), col = 'blue')

## End(Not run)

</code></pre>

<hr>
<h2 id='DRR-package'>Dimensionality Reduction via Regression.</h2><span id='topic+DRR'></span><span id='topic+DRR-package'></span>

<h3>Description</h3>

<p>DRR implements the Dimensionality Reduction via Regression using
Kernel Ridge Regression. It also adds a faster implementation of
Kernel Ridge regression that can be used with the CVST package.
</p>


<h3>Details</h3>

<p>Funding provided by the Department for Biogeochemical Integration,
Empirical Inference of the Earth System Group, at the Max Plack
Institute for Biogeochemistry, Jena.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Guido Kraemer <a href="mailto:gkraemer@bgc-jena.mpg.de">gkraemer@bgc-jena.mpg.de</a>
</p>


<h3>References</h3>

<p>Laparra, V., Malo, J., Camps-Valls, G., 2015. Dimensionality
Reduction via Regression in Hyperspectral Imagery. IEEE Journal
of Selected Topics in Signal Processing 9,
1026-1036. doi:10.1109/JSTSP.2015.2417833
Zhang, Y., Duchi, J.C., Wainwright, M.J., 2013. Divide and Conquer
Kernel Ridge Regression: A Distributed Algorithm with Minimax
Optimal Rates. arXiv:1305.5029 [cs, math, stat].
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/gdkrmr/DRR">https://github.com/gdkrmr/DRR</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/gdkrmr/DRR/issues">https://github.com/gdkrmr/DRR/issues</a>
</p>
</li></ul>


</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
