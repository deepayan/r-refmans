<!DOCTYPE html><html lang="en-US"><head><title>Help for package qtkit</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {qtkit}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#add_pkg_to_bib'><p>Add Package Citations to BibTeX File</p></a></li>
<li><a href='#calc_assoc_metrics'><p>Calculate Association Metrics for Bigrams</p></a></li>
<li><a href='#calc_df'><p>Calculate Document Frequency</p></a></li>
<li><a href='#calc_dp'><p>Calculate Gries' Deviation of Proportions</p></a></li>
<li><a href='#calc_idf'><p>Calculate Inverse Document Frequency</p></a></li>
<li><a href='#calc_normalized_entropy'><p>Calculate Normalized Entropy for Categorical Variables</p></a></li>
<li><a href='#calc_orf'><p>Calculate Observed Relative Frequency</p></a></li>
<li><a href='#calc_rf'><p>Internal Functions for Calculating Dispersion and Frequency Metrics</p></a></li>
<li><a href='#calc_type_metrics'><p>Calculate Frequency and Dispersion Metrics for Text Types</p></a></li>
<li><a href='#calculate_bigram_probabilities'><p>Calculate Probabilities for Bigrams</p></a></li>
<li><a href='#calculate_metrics'><p>Calculate Association Metrics</p></a></li>
<li><a href='#clean_filenames'><p>Clean Downloaded File Names</p></a></li>
<li><a href='#confirm_if_needed'><p>Check if Permission Confirmation is Needed</p></a></li>
<li><a href='#create_data_dictionary'><p>Create Data Dictionary</p></a></li>
<li><a href='#create_data_origin'><p>Create Data Origin Documentation</p></a></li>
<li><a href='#curate_enntt_data'><p>Curate ENNTT Data</p></a></li>
<li><a href='#curate_enntt_file'><p>Curate Single ENNTT File Pair</p></a></li>
<li><a href='#curate_swda_data'><p>Curate SWDA data</p></a></li>
<li><a href='#curate_swda_file'><p>Process a single SWDA utterance file</p></a></li>
<li><a href='#download_and_decompress'><p>Download and Decompress Archive File</p></a></li>
<li><a href='#extract_dat_attrs'><p>Extract Attributes from XML Line Node</p></a></li>
<li><a href='#extract_speaker_info'><p>Extract speaker information from document lines</p></a></li>
<li><a href='#extract_utterances'><p>Extract and process utterances from document lines</p></a></li>
<li><a href='#find_enntt_files'><p>Find ENNTT Files</p></a></li>
<li><a href='#find_outliers'><p>Detect Statistical Outliers Using IQR Method</p></a></li>
<li><a href='#get_archive_data'><p>Download and Extract Archive Files</p></a></li>
<li><a href='#get_gutenberg_data'><p>Get Works from Project Gutenberg</p></a></li>
<li><a href='#process_speaker_info'><p>Process speaker turn information</p></a></li>
<li><a href='#validate_dir_path'><p>Validate Directory Path</p></a></li>
<li><a href='#validate_file_extension'><p>Validate Archive File Extension</p></a></li>
<li><a href='#validate_inputs_cam'><p>Validate Inputs for Association Metrics Calculation</p></a></li>
<li><a href='#validate_inputs_ctm'><p>Validate Inputs for Type Metrics Calculation</p></a></li>
<li><a href='#write_gg'><p>Save ggplot Objects to Files</p></a></li>
<li><a href='#write_kbl'><p>Write a kable object to a file</p></a></li>
<li><a href='#write_obj'><p>Write an R object as a file</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Quantitative Text Kit</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Support package for the textbook "An Introduction to
    Quantitative Text Analysis for Linguists: Reproducible Research Using
    R" (Francom, 2024) &lt;<a href="https://doi.org/10.4324%2F9781003393764">doi:10.4324/9781003393764</a>&gt;. Includes functions to
    acquire, clean, and analyze text data as well as functions to document
    and share the results of text analysis. The package is designed to be
    used in conjunction with the book, but can also be used as a standalone
    package for text analysis.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://cran.r-project.org/package=qtkit">https://cran.r-project.org/package=qtkit</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/qtalr/qtkit/issues">https://github.com/qtalr/qtkit/issues</a></td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>Chromium-based browser (e.g., Chrome, Chromium, or
Brave)</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1)</td>
</tr>
<tr>
<td>Imports:</td>
<td>chromote, dplyr, ggplot2, gutenbergr, kableExtra, knitr,
Matrix, openai, rlang, xml2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>httptest, rmarkdown, testthat (&ge; 3.0.0), webshot2, fs,
tibble, glue, readr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Author:</td>
<td>Jerid Francom <a href="https://orcid.org/0000-0001-5972-6330"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jerid Francom &lt;francojc@wfu.edu&gt;</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-14 04:29:08 UTC; francojc</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-14 06:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='add_pkg_to_bib'>Add Package Citations to BibTeX File</h2><span id='topic+add_pkg_to_bib'></span>

<h3>Description</h3>

<p>Adds citation information for R packages to a BibTeX file. Uses the
<code>knitr::write_bib</code> function to generate and append package citations
in BibTeX format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_pkg_to_bib(pkg_name, bib_file = "packages.bib")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="add_pkg_to_bib_+3A_pkg_name">pkg_name</code></td>
<td>
<p>Character string. The name of the R package to add to
the BibTeX file.</p>
</td></tr>
<tr><td><code id="add_pkg_to_bib_+3A_bib_file">bib_file</code></td>
<td>
<p>Character string. The path and name of the BibTeX file
to write to. Default is &quot;packages.bib&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function will create the BibTeX file if it doesn't exist,
or append to it if it does. It includes citations for both the
specified package and all currently loaded packages.
</p>


<h3>Value</h3>

<p>Invisible NULL. The function is called for its side effect of
writing to the BibTeX file.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create a temporary BibTeX file
my_bib_file &lt;- tempfile(fileext = ".bib")

# Add citations for dplyr package
add_pkg_to_bib("dplyr", my_bib_file)

# View the contents of the BibTeX file
readLines(my_bib_file) |&gt; cat(sep = "\n")

</code></pre>

<hr>
<h2 id='calc_assoc_metrics'>Calculate Association Metrics for Bigrams</h2><span id='topic+calc_assoc_metrics'></span>

<h3>Description</h3>

<p>This function calculates various association metrics
(PMI, Dice's Coefficient, G-score) for bigrams in a given corpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_assoc_metrics(
  data,
  doc_index,
  token_index,
  type,
  association = "all",
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calc_assoc_metrics_+3A_data">data</code></td>
<td>
<p>A data frame containing the corpus.</p>
</td></tr>
<tr><td><code id="calc_assoc_metrics_+3A_doc_index">doc_index</code></td>
<td>
<p>Column in 'data' which represents the document index.</p>
</td></tr>
<tr><td><code id="calc_assoc_metrics_+3A_token_index">token_index</code></td>
<td>
<p>Column in 'data' which represents the token index.</p>
</td></tr>
<tr><td><code id="calc_assoc_metrics_+3A_type">type</code></td>
<td>
<p>Column in 'data' which represents the tokens or terms.</p>
</td></tr>
<tr><td><code id="calc_assoc_metrics_+3A_association">association</code></td>
<td>
<p>A character vector specifying which metrics to calculate.
Can be any combination of 'pmi', 'dice_coeff', 'g_score', or 'all'.
Default is 'all'.</p>
</td></tr>
<tr><td><code id="calc_assoc_metrics_+3A_verbose">verbose</code></td>
<td>
<p>A logical value indicating whether to keep the intermediate
probability columns. Default is FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with one row per bigram and columns for each
calculated metric.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data_path &lt;- system.file("extdata", "bigrams_data.rds", package = "qtkit")
data &lt;- readRDS(data_path)

calc_assoc_metrics(data, doc_index, token_index, type)

</code></pre>

<hr>
<h2 id='calc_df'>Calculate Document Frequency</h2><span id='topic+calc_df'></span>

<h3>Description</h3>

<p>Computes the document frequency (DF) for each term in a term-document matrix.
DF is the number of documents in which each term appears at least once.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_df(tdm)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calc_df_+3A_tdm">tdm</code></td>
<td>
<p>A term-document matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of document frequencies for each term
</p>

<hr>
<h2 id='calc_dp'>Calculate Gries' Deviation of Proportions</h2><span id='topic+calc_dp'></span>

<h3>Description</h3>

<p>Computes the Deviation of Proportions (DP) measure developed by Stefan Th. Gries.
DP measures how evenly distributed a term is across all parts of the corpus.
The normalized version (DP_norm) is returned, which ranges from 0 (evenly distributed)
to 1 (extremely clumped distribution).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_dp(tdm)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calc_dp_+3A_tdm">tdm</code></td>
<td>
<p>A term-document matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of normalized DP values for each term
</p>


<h3>References</h3>

<p>Gries, S. T. (2008). Dispersions and adjusted frequencies in corpora.
International Journal of Corpus Linguistics, 13(4), 403-437.
</p>

<hr>
<h2 id='calc_idf'>Calculate Inverse Document Frequency</h2><span id='topic+calc_idf'></span>

<h3>Description</h3>

<p>Computes the inverse document frequency (IDF) for each term in a term-document matrix.
IDF is calculated as log(N/df) where N is the total number of documents and df is
the document frequency of the term.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_idf(tdm)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calc_idf_+3A_tdm">tdm</code></td>
<td>
<p>A term-document matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of inverse document frequencies for each term
</p>

<hr>
<h2 id='calc_normalized_entropy'>Calculate Normalized Entropy for Categorical Variables</h2><span id='topic+calc_normalized_entropy'></span>

<h3>Description</h3>

<p>Computes the normalized entropy (uncertainty measure) for categorical variables,
providing a standardized measure of dispersion or randomness in the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_normalized_entropy(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calc_normalized_entropy_+3A_x">x</code></td>
<td>
<p>A character vector or factor containing categorical data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function:
</p>

<ul>
<li><p> Handles both character vectors and factors as input
</p>
</li>
<li><p> Treats NA values as a separate category
</p>
</li>
<li><p> Normalizes entropy to range (0,1) where:
</p>

<ul>
<li><p> 0 indicates complete certainty (one category dominates)
</p>
</li>
<li><p> 1 indicates maximum uncertainty (equal distribution)
</p>
</li></ul>

</li></ul>

<p>The calculation process:
</p>

<ol>
<li><p> Computes category proportions
</p>
</li>
<li><p> Calculates raw entropy using Shannon's formula
</p>
</li>
<li><p> Normalizes by dividing by maximum possible entropy
</p>
</li></ol>



<h3>Value</h3>

<p>A numeric value between 0 and 1 representing the normalized entropy:
</p>

<ul>
<li><p> Values closer to 0 indicate less diversity/uncertainty
</p>
</li>
<li><p> Values closer to 1 indicate more diversity/uncertainty
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Calculate entropy for a simple categorical vector
x &lt;- c("A", "B", "B", "C", "C", "C", "D", "D", "D", "D")
calc_normalized_entropy(x)

# Handle missing values
y &lt;- c("A", "B", NA, "C", "C", NA, "D", "D")
calc_normalized_entropy(y)

# Works with factors too
z &lt;- factor(c("Low", "Med", "Med", "High", "High", "High"))
calc_normalized_entropy(z)

</code></pre>

<hr>
<h2 id='calc_orf'>Calculate Observed Relative Frequency</h2><span id='topic+calc_orf'></span>

<h3>Description</h3>

<p>Computes the observed relative frequency (ORF) for each term in a term-document matrix.
ORF is the relative frequency expressed as a percentage (RF * 100).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_orf(tdm)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calc_orf_+3A_tdm">tdm</code></td>
<td>
<p>A term-document matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of observed relative frequencies (as percentages) for each term
</p>

<hr>
<h2 id='calc_rf'>Internal Functions for Calculating Dispersion and Frequency Metrics</h2><span id='topic+calc_rf'></span>

<h3>Description</h3>

<p>A collection of internal helper functions that calculate various dispersion
and frequency metrics from term-document matrices. These functions support
the main <code>calc_type_metrics</code> function by providing specialized calculations
for different statistical measures.
</p>
<p>Computes the relative frequency (RF) for each term in a term-document matrix,
representing how often each term occurs relative to the total corpus size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_rf(tdm)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calc_rf_+3A_tdm">tdm</code></td>
<td>
<p>A sparse term-document matrix (Matrix package format)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The package implements these metrics:
</p>
<p>Dispersion measures:
</p>

<ul>
<li><p> Document Frequency (DF): Count of documents containing each term
</p>
</li>
<li><p> Inverse Document Frequency (IDF): Log-scaled inverse of DF, emphasizing rare terms
</p>
</li>
<li><p> Deviation of Proportions (DP): Gries' measure of distributional evenness
ranging from 0 (perfectly even) to 1 (completely clumped)
</p>
</li></ul>

<p>Frequency measures:
</p>

<ul>
<li><p> Relative Frequency (RF): Term frequency normalized by total corpus size
</p>
</li>
<li><p> Observed Relative Frequency (ORF): RF expressed as percentage (RF * 100)
</p>
</li></ul>

<p>Implementation notes:
</p>

<ul>
<li><p> All functions expect a sparse term-document matrix input
</p>
</li>
<li><p> Matrix operations are optimized using the Matrix package
</p>
</li>
<li><p> NA values are handled appropriately for each metric
</p>
</li>
<li><p> Results are returned as numeric vectors
</p>
</li></ul>

<p>The calculation process:
</p>

<ol>
<li><p> Sums occurrences of each term across all documents
</p>
</li>
<li><p> Divides by total corpus size (sum of all terms)
</p>
</li>
<li><p> Returns proportions between 0 and 1
</p>
</li></ol>



<h3>Value</h3>

<p>A numeric vector where each element represents a term's relative
frequency in the corpus (range: 0-1)
</p>


<h3>References</h3>

<p>Gries, S. T. (2008). Dispersions and adjusted frequencies in corpora.
International Journal of Corpus Linguistics, 13(4), 403-437.
</p>

<hr>
<h2 id='calc_type_metrics'>Calculate Frequency and Dispersion Metrics for Text Types</h2><span id='topic+calc_type_metrics'></span>

<h3>Description</h3>

<p>Calculates various frequency and dispersion metrics for types (terms/tokens)
in tokenized text data. Provides a comprehensive analysis of how types are
distributed across documents in a corpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_type_metrics(data, type, document, frequency = NULL, dispersion = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calc_type_metrics_+3A_data">data</code></td>
<td>
<p>Data frame. Contains the tokenized text data with document IDs
and types/terms.</p>
</td></tr>
<tr><td><code id="calc_type_metrics_+3A_type">type</code></td>
<td>
<p>Symbol. Column in <code>data</code> containing the types to analyze
(e.g., terms, lemmas).</p>
</td></tr>
<tr><td><code id="calc_type_metrics_+3A_document">document</code></td>
<td>
<p>Symbol. Column in <code>data</code> containing the document identifiers.</p>
</td></tr>
<tr><td><code id="calc_type_metrics_+3A_frequency">frequency</code></td>
<td>
<p>Character vector. Frequency metrics to calculate:
- NULL (default): Returns only type counts
- 'all': All available metrics
- 'rf': Relative frequency
- 'orf': Observed relative frequency (per 100)</p>
</td></tr>
<tr><td><code id="calc_type_metrics_+3A_dispersion">dispersion</code></td>
<td>
<p>Character vector. Dispersion metrics to calculate:
- NULL (default): Returns only type counts
- 'all': All available metrics
- 'df': Document frequency
- 'idf': Inverse document frequency
- 'dp': Gries' deviation of proportions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function creates a term-document matrix internally and calculates the
requested metrics. Frequency metrics show how often types occur, while
dispersion metrics show how evenly they are distributed across documents.
</p>
<p>The 'dp' metric (Gries' Deviation of Proportions) ranges from 0 (perfectly
even distribution) to 1 (completely clumped distribution).
</p>


<h3>Value</h3>

<p>Data frame containing requested metrics:
</p>

<ul>
<li><p> type: Unique types from input data
</p>
</li>
<li><p> n: Raw frequency count
</p>
</li>
<li><p> rf: Relative frequency (if requested)
</p>
</li>
<li><p> orf: Observed relative frequency per 100 (if requested)
</p>
</li>
<li><p> df: Document frequency (if requested)
</p>
</li>
<li><p> idf: Inverse document frequency (if requested)
</p>
</li>
<li><p> dp: Deviation of proportions (if requested)
</p>
</li></ul>



<h3>References</h3>

<p>Gries, Stefan Th. (2023). Statistical Methods in Corpus Linguistics.
In Readings in Corpus Linguistics: A Teaching and Research Guide
for Scholars in Nigeria and Beyond, pp. 78-114.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data_path &lt;- system.file("extdata", "types_data.rds", package = "qtkit")
df &lt;- readRDS(data_path)
calc_type_metrics(
  data = df,
  type = letter,
  document = doc_id,
  frequency = c("rf", "orf"),
  dispersion = "dp"
)
</code></pre>

<hr>
<h2 id='calculate_bigram_probabilities'>Calculate Probabilities for Bigrams</h2><span id='topic+calculate_bigram_probabilities'></span>

<h3>Description</h3>

<p>Helper function that calculates joint and marginal probabilities for
bigrams in the input data using dplyr. It processes the data to create
bigrams and computes their probabilities along with individual token
probabilities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calculate_bigram_probabilities(data, doc_index, token_index, type)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calculate_bigram_probabilities_+3A_data">data</code></td>
<td>
<p>A data frame containing the corpus</p>
</td></tr>
<tr><td><code id="calculate_bigram_probabilities_+3A_doc_index">doc_index</code></td>
<td>
<p>Column name for document index</p>
</td></tr>
<tr><td><code id="calculate_bigram_probabilities_+3A_token_index">token_index</code></td>
<td>
<p>Column name for token position</p>
</td></tr>
<tr><td><code id="calculate_bigram_probabilities_+3A_type">type</code></td>
<td>
<p>Column name for the actual tokens/terms</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame containing:
</p>

<ul>
<li><p> x: First token in bigram
</p>
</li>
<li><p> y: Second token in bigram
</p>
</li>
<li><p> p_xy: Joint probability of the bigram
</p>
</li>
<li><p> p_x: Marginal probability of first token
</p>
</li>
<li><p> p_y: Marginal probability of second token
</p>
</li></ul>


<hr>
<h2 id='calculate_metrics'>Calculate Association Metrics</h2><span id='topic+calculate_metrics'></span>

<h3>Description</h3>

<p>Helper function that computes various association metrics for bigrams
based on their probability distributions. Supports PMI (Pointwise Mutual
Information), Dice's Coefficient, and G-score calculations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calculate_metrics(bigram_probs, association)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calculate_metrics_+3A_bigram_probs">bigram_probs</code></td>
<td>
<p>A data frame containing bigram probability data with
columns:
</p>

<ul>
<li> <p>p_xy Joint probability of bigram
</p>
</li>
<li> <p>p_x Marginal probability of first token
</p>
</li>
<li> <p>p_y Marginal probability of second token
</p>
</li></ul>
</td></tr>
<tr><td><code id="calculate_metrics_+3A_association">association</code></td>
<td>
<p>Character vector specifying which metrics to calculate</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame containing the original probability columns plus
requested association metrics:
</p>

<ul>
<li><p> pmi: Pointwise Mutual Information
</p>
</li>
<li><p> dice_coeff: Dice's Coefficient
</p>
</li>
<li><p> g_score: G-score
</p>
</li></ul>


<hr>
<h2 id='clean_filenames'>Clean Downloaded File Names</h2><span id='topic+clean_filenames'></span>

<h3>Description</h3>

<p>Helper function that removes spaces from filenames in the target directory,
replacing them with underscores.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clean_filenames(target_dir)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="clean_filenames_+3A_target_dir">target_dir</code></td>
<td>
<p>Character string of the target directory path</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisible NULL, called for side effects
</p>

<hr>
<h2 id='confirm_if_needed'>Check if Permission Confirmation is Needed</h2><span id='topic+confirm_if_needed'></span>

<h3>Description</h3>

<p>Helper function that determines whether to prompt the user for permission
confirmation based on the confirmed parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confirm_if_needed(confirmed)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="confirm_if_needed_+3A_confirmed">confirmed</code></td>
<td>
<p>Logical indicating if permission is pre-confirmed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Logical indicating if permission is granted
</p>

<hr>
<h2 id='create_data_dictionary'>Create Data Dictionary</h2><span id='topic+create_data_dictionary'></span>

<h3>Description</h3>

<p>This function takes a data frame and creates a data dictionary. The data
dictionary includes the variable name, a human-readable name, the variable
type, and a description. If a model is specified, the function uses OpenAI's
API to generate the information based on the characteristics of the data
frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_data_dictionary(
  data,
  file_path,
  model = NULL,
  sample_n = 5,
  grouping = NULL,
  force = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_data_dictionary_+3A_data">data</code></td>
<td>
<p>A data frame to create a data dictionary for.</p>
</td></tr>
<tr><td><code id="create_data_dictionary_+3A_file_path">file_path</code></td>
<td>
<p>The file path to save the data dictionary to.</p>
</td></tr>
<tr><td><code id="create_data_dictionary_+3A_model">model</code></td>
<td>
<p>The ID of the OpenAI chat completion models to use for
generating descriptions (see <code>openai::list_models()</code>). If NULL (default), a
scaffolding for the data dictionary is created.</p>
</td></tr>
<tr><td><code id="create_data_dictionary_+3A_sample_n">sample_n</code></td>
<td>
<p>The number of rows to sample from the data frame to use as
input for the model. Default NULL.</p>
</td></tr>
<tr><td><code id="create_data_dictionary_+3A_grouping">grouping</code></td>
<td>
<p>A character vector of column names to group by when sampling
rows from the data frame for the model. Default NULL.</p>
</td></tr>
<tr><td><code id="create_data_dictionary_+3A_force">force</code></td>
<td>
<p>If TRUE, overwrite the file at <code>file_path</code> if it already exists.
Default FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame containing the variable name, human-readable name,
variable type, and description for each variable in the input data frame.
</p>

<hr>
<h2 id='create_data_origin'>Create Data Origin Documentation</h2><span id='topic+create_data_origin'></span>

<h3>Description</h3>

<p>Creates a standardized data origin documentation file in CSV format,
containing essential metadata about a dataset's source, format, and usage
rights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_data_origin(file_path, return = FALSE, force = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_data_origin_+3A_file_path">file_path</code></td>
<td>
<p>Character string. Path where the CSV file should be saved.</p>
</td></tr>
<tr><td><code id="create_data_origin_+3A_return">return</code></td>
<td>
<p>Logical. If TRUE, returns the data frame in addition to saving.
Default is FALSE.</p>
</td></tr>
<tr><td><code id="create_data_origin_+3A_force">force</code></td>
<td>
<p>Logical. If TRUE, overwrites existing file at path.
Default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generates a template with the following metadata fields:
</p>

<ul>
<li><p> Resource name
</p>
</li>
<li><p> Data source (URL/DOI)
</p>
</li>
<li><p> Sampling frame (language, modality, genre)
</p>
</li>
<li><p> Collection dates
</p>
</li>
<li><p> Data format
</p>
</li>
<li><p> Schema description
</p>
</li>
<li><p> License information
</p>
</li>
<li><p> Attribution requirements
</p>
</li></ul>



<h3>Value</h3>

<p>If return=TRUE, returns a data frame containing the data origin
template. Otherwise returns invisible(NULL).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tmp_file &lt;- tempfile(fileext = ".csv")
create_data_origin(tmp_file)
read.csv(tmp_file)
</code></pre>

<hr>
<h2 id='curate_enntt_data'>Curate ENNTT Data</h2><span id='topic+curate_enntt_data'></span>

<h3>Description</h3>

<p>This function processes and curates ENNTT (European Parliament) data from a
specified directory.
It handles both .dat files (containing XML metadata) and .tok files
'(containing text content).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>curate_enntt_data(dir_path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="curate_enntt_data_+3A_dir_path">dir_path</code></td>
<td>
<p>A string. The path to the directory containing the ENNTT
data files. Must be an existing directory.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function expects a directory containing paired .dat and .tok files with
matching names, as found in the raw ENNTT data
<a href="https://github.com/senisioi/enntt-release">https://github.com/senisioi/enntt-release</a>.
The .dat files should contain XML-formatted metadata with attributes:
</p>

<ul>
<li><p> session_id: Unique identifier for the parliamentary session
</p>
</li>
<li><p> mepid: Member of European Parliament ID
</p>
</li>
<li><p> state: Country or state representation
</p>
</li>
<li><p> seq_speaker_id: Sequential ID within the session
</p>
</li></ul>

<p>The .tok files should contain the corresponding text content, one entry per
line.
</p>


<h3>Value</h3>

<p>A tibble containing the curated ENNTT data with columns:
</p>

<ul>
<li><p> session_id: Parliamentary session identifier
</p>
</li>
<li><p> speaker_id: Speaker's MEP ID
</p>
</li>
<li><p> state: Representative's state/country
</p>
</li>
<li><p> session_seq: Sequential position in session
</p>
</li>
<li><p> text: Speech content
</p>
</li>
<li><p> type: Corpus type identifier
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Example using simulated data bundled with the package
example_data &lt;- system.file("extdata", "simul_enntt", package = "qtkit")
curated_data &lt;- curate_enntt_data(example_data)

str(curated_data)

</code></pre>

<hr>
<h2 id='curate_enntt_file'>Curate Single ENNTT File Pair</h2><span id='topic+curate_enntt_file'></span>

<h3>Description</h3>

<p>Curate Single ENNTT File Pair
</p>


<h3>Usage</h3>

<pre><code class='language-R'>curate_enntt_file(dir_path, corpus_type)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="curate_enntt_file_+3A_dir_path">dir_path</code></td>
<td>
<p>Directory containing the files</p>
</td></tr>
<tr><td><code id="curate_enntt_file_+3A_corpus_type">corpus_type</code></td>
<td>
<p>Type identifier for the corpus</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame of curated data
</p>

<hr>
<h2 id='curate_swda_data'>Curate SWDA data</h2><span id='topic+curate_swda_data'></span>

<h3>Description</h3>

<p>Process and curate Switchboard Dialog Act (SWDA) data by reading all .utt
files from a specified directory and converting them into a structured
format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>curate_swda_data(dir_path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="curate_swda_data_+3A_dir_path">dir_path</code></td>
<td>
<p>Character string. Path to the directory containing .utt
files. Must be an existing directory.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function expects a directory containing .utt files or subdirectories
with .utt files, as found in the raw SWDA data
(Linguistic Data Consortium. LDC97S62: Switchboard Dialog Act Corpus.)
</p>


<h3>Value</h3>

<p>A data frame containing the curated SWDA data with columns:
</p>

<ul>
<li><p> doc_id: Document identifier
</p>
</li>
<li><p> damsl_tag: Dialog act annotation
</p>
</li>
<li><p> speaker_id: Unique speaker identifier
</p>
</li>
<li><p> speaker: Speaker designation (A or B)
</p>
</li>
<li><p> turn_num: Turn number in conversation
</p>
</li>
<li><p> utterance_num: Utterance number
</p>
</li>
<li><p> utterance_text: Actual spoken text
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Example using simulated data bundled with the package
example_data &lt;- system.file("extdata", "simul_swda", package = "qtkit")
swda_data &lt;- curate_swda_data(example_data)

str(swda_data)

</code></pre>

<hr>
<h2 id='curate_swda_file'>Process a single SWDA utterance file</h2><span id='topic+curate_swda_file'></span>

<h3>Description</h3>

<p>Process a single SWDA utterance file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>curate_swda_file(file_path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="curate_swda_file_+3A_file_path">file_path</code></td>
<td>
<p>Character string. Path to the .utt file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame containing processed data from the file
</p>

<hr>
<h2 id='download_and_decompress'>Download and Decompress Archive File</h2><span id='topic+download_and_decompress'></span>

<h3>Description</h3>

<p>Helper function that downloads an archive file to a temporary location
and decompresses it to the target directory.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>download_and_decompress(url, target_dir, ext)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="download_and_decompress_+3A_url">url</code></td>
<td>
<p>Character string of the archive file URL</p>
</td></tr>
<tr><td><code id="download_and_decompress_+3A_target_dir">target_dir</code></td>
<td>
<p>Character string of the target directory path</p>
</td></tr>
<tr><td><code id="download_and_decompress_+3A_ext">ext</code></td>
<td>
<p>Character string of the file extension</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, called for side effects
</p>

<hr>
<h2 id='extract_dat_attrs'>Extract Attributes from XML Line Node</h2><span id='topic+extract_dat_attrs'></span>

<h3>Description</h3>

<p>Extract Attributes from XML Line Node
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_dat_attrs(line_node)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extract_dat_attrs_+3A_line_node">line_node</code></td>
<td>
<p>XML node containing line attributes</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame of extracted attributes
</p>

<hr>
<h2 id='extract_speaker_info'>Extract speaker information from document lines</h2><span id='topic+extract_speaker_info'></span>

<h3>Description</h3>

<p>Extract speaker information from document lines
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_speaker_info(doc_lines)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extract_speaker_info_+3A_doc_lines">doc_lines</code></td>
<td>
<p>Character vector of file lines</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named list of speaker information
</p>

<hr>
<h2 id='extract_utterances'>Extract and process utterances from document lines</h2><span id='topic+extract_utterances'></span>

<h3>Description</h3>

<p>Extract and process utterances from document lines
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_utterances(doc_lines, speaker_info)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extract_utterances_+3A_doc_lines">doc_lines</code></td>
<td>
<p>Character vector of file lines</p>
</td></tr>
<tr><td><code id="extract_utterances_+3A_speaker_info">speaker_info</code></td>
<td>
<p>List of speaker information</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame of processed utterances
</p>

<hr>
<h2 id='find_enntt_files'>Find ENNTT Files</h2><span id='topic+find_enntt_files'></span>

<h3>Description</h3>

<p>Find ENNTT Files
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_enntt_files(dir_path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="find_enntt_files_+3A_dir_path">dir_path</code></td>
<td>
<p>Directory to search for ENNTT files</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of unique corpus types
</p>

<hr>
<h2 id='find_outliers'>Detect Statistical Outliers Using IQR Method</h2><span id='topic+find_outliers'></span>

<h3>Description</h3>

<p>Identifies statistical outliers in a numeric variable using the Interquartile
Range (IQR) method. Provides detailed diagnostics about the outlier detection
process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_outliers(data, variable_name, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="find_outliers_+3A_data">data</code></td>
<td>
<p>Data frame containing the variable to analyze.</p>
</td></tr>
<tr><td><code id="find_outliers_+3A_variable_name">variable_name</code></td>
<td>
<p>Unquoted name of the numeric variable to check for
outliers.</p>
</td></tr>
<tr><td><code id="find_outliers_+3A_verbose">verbose</code></td>
<td>
<p>Logical. If TRUE, prints diagnostic information about
quartiles, fences, and number of outliers found. Default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function uses the standard IQR method for outlier detection:
</p>

<ul>
<li><p> Calculates Q1 (25th percentile) and Q3 (75th percentile)
</p>
</li>
<li><p> Computes IQR = Q3 - Q1
</p>
</li>
<li><p> Defines outliers as values outside (Q1 - 1.5<em>IQR, Q3 + 1.5</em>IQR)
</p>
</li></ul>



<h3>Value</h3>

<p>If outliers are found:
</p>

<ul>
<li><p> Data frame containing rows with outlier values
</p>
</li>
<li><p> Prints diagnostic information about quartiles and fences
</p>
</li></ul>

<p>If no outliers:
</p>

<ul>
<li><p> Returns NULL
</p>
</li>
<li><p> Prints confirmation message
</p>
</li></ul>



<h3>Diagnostic Output</h3>


<ul>
<li><p> Variable name
</p>
</li>
<li><p> Q1 and Q3 values
</p>
</li>
<li><p> IQR value
</p>
</li>
<li><p> Upper and lower fence values
</p>
</li>
<li><p> Number of outliers found
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data(mtcars)
find_outliers(mtcars, mpg)
find_outliers(mtcars, wt, verbose = FALSE)

</code></pre>

<hr>
<h2 id='get_archive_data'>Download and Extract Archive Files</h2><span id='topic+get_archive_data'></span>

<h3>Description</h3>

<p>Downloads compressed archive files from a URL and extracts their contents to a
specified directory. Supports multiple archive formats and handles permission
confirmation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_archive_data(url, target_dir, force = FALSE, confirmed = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_archive_data_+3A_url">url</code></td>
<td>
<p>Character string. Full URL to the compressed archive file.</p>
</td></tr>
<tr><td><code id="get_archive_data_+3A_target_dir">target_dir</code></td>
<td>
<p>Character string. Directory where the archive contents
should be extracted.</p>
</td></tr>
<tr><td><code id="get_archive_data_+3A_force">force</code></td>
<td>
<p>Logical. If TRUE, overwrites existing data in target directory.
Default is FALSE.</p>
</td></tr>
<tr><td><code id="get_archive_data_+3A_confirmed">confirmed</code></td>
<td>
<p>Logical. If TRUE, skips permission confirmation prompt.
Useful for reproducible workflows. Default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Supported archive formats:
</p>

<ul>
<li><p> ZIP (.zip)
</p>
</li>
<li><p> Gzip (.gz)
</p>
</li>
<li><p> Tar (.tar)
</p>
</li>
<li><p> Compressed tar (.tgz)
</p>
</li></ul>

<p>The function includes safety features:
</p>

<ul>
<li><p> Permission confirmation for data usage
</p>
</li>
<li><p> Directory existence checks
</p>
</li>
<li><p> Archive format validation
</p>
</li>
<li><p> Automatic file cleanup
</p>
</li></ul>



<h3>Value</h3>

<p>Invisible NULL. Called for side effects:
</p>

<ul>
<li><p> Downloads archive file
</p>
</li>
<li><p> Creates target directory if needed
</p>
</li>
<li><p> Extracts archive contents
</p>
</li>
<li><p> Cleans up temporary files
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data_dir &lt;- file.path(tempdir(), "data")
url &lt;-
  "https://raw.githubusercontent.com/qtalr/qtkit/main/inst/extdata/test_data.zip"
get_archive_data(
  url = url,
  target_dir = data_dir,
  confirmed = TRUE
)

## End(Not run)
</code></pre>

<hr>
<h2 id='get_gutenberg_data'>Get Works from Project Gutenberg</h2><span id='topic+get_gutenberg_data'></span>

<h3>Description</h3>

<p>Retrieves works from Project Gutenberg based on specified criteria
and saves the data to a CSV file. This function is a wrapper for the
gutenbergr package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_gutenberg_data(
  target_dir,
  lcc_subject,
  birth_year = NULL,
  death_year = NULL,
  n_works = 100,
  force = FALSE,
  confirmed = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_gutenberg_data_+3A_target_dir">target_dir</code></td>
<td>
<p>The directory where the CSV file will be saved.</p>
</td></tr>
<tr><td><code id="get_gutenberg_data_+3A_lcc_subject">lcc_subject</code></td>
<td>
<p>A character vector specifying the Library of Congress
Classification (LCC) subjects to filter the works.</p>
</td></tr>
<tr><td><code id="get_gutenberg_data_+3A_birth_year">birth_year</code></td>
<td>
<p>An optional integer specifying the minimum birth year of
authors to include.</p>
</td></tr>
<tr><td><code id="get_gutenberg_data_+3A_death_year">death_year</code></td>
<td>
<p>An optional integer specifying the maximum death year of
authors to include.</p>
</td></tr>
<tr><td><code id="get_gutenberg_data_+3A_n_works">n_works</code></td>
<td>
<p>An integer specifying the number of works to retrieve.
Default is 100.</p>
</td></tr>
<tr><td><code id="get_gutenberg_data_+3A_force">force</code></td>
<td>
<p>A logical value indicating whether to overwrite existing data
if it already exists.</p>
</td></tr>
<tr><td><code id="get_gutenberg_data_+3A_confirmed">confirmed</code></td>
<td>
<p>If <code>TRUE</code>, the user has confirmed that they have
permission to use the data.
If <code>FALSE</code>, the function will prompt the user to confirm permission.
Setting this to <code>TRUE</code> is useful for reproducible workflows.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function retrieves Gutenberg works based on the specified LCC
subjects and optional author birth and death years.
It checks if the data already exists in the target directory and provides
an option to overwrite it.
The function also creates the target directory if it doesn't exist.
If the number of works is greater than 1000 and the 'confirmed' parameter
is not set to TRUE, it prompts the user for confirmation.
The retrieved works are filtered based on public domain rights in the USA
and availability of text.
The resulting works are downloaded and saved as a CSV file in the target
directory.
</p>
<p>For more information on Library of Congress Classification (LCC) subjects,
refer to the <a href="https://www.loc.gov/catdir/cpso/lcco/">https://www.loc.gov/catdir/cpso/lcco/</a> Library of
Congress Classification Guide.
</p>


<h3>Value</h3>

<p>A message indicating whether the data was acquired or already
existed on disk, writes the data files to disk in the specified target
directory.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data_dir &lt;- file.path(tempdir(), "data")

get_gutenberg_data(
  target_dir = data_dir,
  lcc_subject = "JC",
  n_works = 5,
  confirmed = TRUE
)

## End(Not run)

</code></pre>

<hr>
<h2 id='process_speaker_info'>Process speaker turn information</h2><span id='topic+process_speaker_info'></span>

<h3>Description</h3>

<p>Process speaker turn information
</p>


<h3>Usage</h3>

<pre><code class='language-R'>process_speaker_info(speaker_turn, speaker_a_id, speaker_b_id)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="process_speaker_info_+3A_speaker_turn">speaker_turn</code></td>
<td>
<p>Vector of speaker turns</p>
</td></tr>
<tr><td><code id="process_speaker_info_+3A_speaker_a_id">speaker_a_id</code></td>
<td>
<p>ID for speaker A</p>
</td></tr>
<tr><td><code id="process_speaker_info_+3A_speaker_b_id">speaker_b_id</code></td>
<td>
<p>ID for speaker B</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with processed speaker information
</p>

<hr>
<h2 id='validate_dir_path'>Validate Directory Path</h2><span id='topic+validate_dir_path'></span>

<h3>Description</h3>

<p>Validate Directory Path
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate_dir_path(dir_path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="validate_dir_path_+3A_dir_path">dir_path</code></td>
<td>
<p>Directory path to validate</p>
</td></tr>
</table>

<hr>
<h2 id='validate_file_extension'>Validate Archive File Extension</h2><span id='topic+validate_file_extension'></span>

<h3>Description</h3>

<p>Helper function that checks if the file extension is supported
(zip, gz, tar, or tgz).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate_file_extension(ext)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="validate_file_extension_+3A_ext">ext</code></td>
<td>
<p>Character string of the file extension</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Stops execution if extension is not supported
</p>


<h3>Value</h3>

<p>No return value, called for side effects
</p>

<hr>
<h2 id='validate_inputs_cam'>Validate Inputs for Association Metrics Calculation</h2><span id='topic+validate_inputs_cam'></span>

<h3>Description</h3>

<p>Helper function that validates the input parameters for
the calc_assoc_metrics function. Checks data frame structure, column
existence, and association metric specifications.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate_inputs_cam(data, doc_index, token_index, type, association)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="validate_inputs_cam_+3A_data">data</code></td>
<td>
<p>A data frame to validate</p>
</td></tr>
<tr><td><code id="validate_inputs_cam_+3A_doc_index">doc_index</code></td>
<td>
<p>Column name for document index</p>
</td></tr>
<tr><td><code id="validate_inputs_cam_+3A_token_index">token_index</code></td>
<td>
<p>Column name for token position</p>
</td></tr>
<tr><td><code id="validate_inputs_cam_+3A_type">type</code></td>
<td>
<p>Column name for the tokens/terms</p>
</td></tr>
<tr><td><code id="validate_inputs_cam_+3A_association">association</code></td>
<td>
<p>Character vector of requested association metrics</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Stops execution with error message if:
</p>

<ul>
<li><p> data is not a data frame
</p>
</li>
<li><p> required columns are missing
</p>
</li>
<li><p> association contains invalid metric names
</p>
</li></ul>



<h3>Value</h3>

<p>No return value, called for side effects
</p>

<hr>
<h2 id='validate_inputs_ctm'>Validate Inputs for Type Metrics Calculation</h2><span id='topic+validate_inputs_ctm'></span>

<h3>Description</h3>

<p>Helper function that validates the input parameters for the calc_type_metrics
function. Checks data frame structure, column existence, and metric
specifications.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate_inputs_ctm(data, type, document, frequency, dispersion)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="validate_inputs_ctm_+3A_data">data</code></td>
<td>
<p>A data frame to validate</p>
</td></tr>
<tr><td><code id="validate_inputs_ctm_+3A_type">type</code></td>
<td>
<p>Column name for the type/term variable</p>
</td></tr>
<tr><td><code id="validate_inputs_ctm_+3A_document">document</code></td>
<td>
<p>Column name for the document ID variable</p>
</td></tr>
<tr><td><code id="validate_inputs_ctm_+3A_frequency">frequency</code></td>
<td>
<p>Character vector of requested frequency metrics</p>
</td></tr>
<tr><td><code id="validate_inputs_ctm_+3A_dispersion">dispersion</code></td>
<td>
<p>Character vector of requested dispersion metrics</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Stops execution with error message if:
</p>

<ul>
<li><p> data is not a data frame
</p>
</li>
<li><p> required columns are missing
</p>
</li>
<li><p> frequency contains invalid metric names
</p>
</li>
<li><p> dispersion contains invalid metric names
</p>
</li></ul>



<h3>Value</h3>

<p>No return value, called for side effects
</p>

<hr>
<h2 id='write_gg'>Save ggplot Objects to Files</h2><span id='topic+write_gg'></span>

<h3>Description</h3>

<p>A wrapper around <code>ggsave</code> that facilitates saving ggplot objects within knitr
documents. Automatically handles file naming and directory creation, with
support for multiple output formats.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_gg(
  gg_obj = NULL,
  file = NULL,
  target_dir = NULL,
  device = "pdf",
  theme = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="write_gg_+3A_gg_obj">gg_obj</code></td>
<td>
<p>The ggplot to be written. If not specified, the last
ggplot created will be written.</p>
</td></tr>
<tr><td><code id="write_gg_+3A_file">file</code></td>
<td>
<p>The name of the file to be written. If not specified, the
label of the code block will be used.</p>
</td></tr>
<tr><td><code id="write_gg_+3A_target_dir">target_dir</code></td>
<td>
<p>The directory where the file will be written. If not
specified, the current working directory will be used.</p>
</td></tr>
<tr><td><code id="write_gg_+3A_device">device</code></td>
<td>
<p>The device to be used for saving the ggplot. Options
include &quot;pdf&quot; (default), &quot;png&quot;, &quot;jpeg&quot;, &quot;tiff&quot;, and &quot;svg&quot;.</p>
</td></tr>
<tr><td><code id="write_gg_+3A_theme">theme</code></td>
<td>
<p>The ggplot2 theme to be applied to the ggplot. Default is
the theme specified in the ggplot2 options.</p>
</td></tr>
<tr><td><code id="write_gg_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the <code>ggsave</code>
function from the <code>ggplot2</code> package.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function extends <code>ggplot2::ggsave</code> by:
</p>

<ul>
<li><p> Using knitr code block labels for automatic file naming
</p>
</li>
<li><p> Creating target directories if they don't exist
</p>
</li>
<li><p> Supporting multiple output formats (PDF, PNG, JPEG, TIFF, SVG)
</p>
</li>
<li><p> Applying custom themes to plots before saving
</p>
</li></ul>



<h3>Value</h3>

<p>The path of the written file.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(ggplot2)

plot_dir &lt;- file.path(tempdir(), "plot")

# Write a ggplot object as a PDF file
p &lt;- ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point()

write_gg(
  gg_obj = p,
  file = "plot_file",
  target_dir = plot_dir,
  device = "pdf"
)

unlink(plot_dir)

## End(Not run)
</code></pre>

<hr>
<h2 id='write_kbl'>Write a kable object to a file</h2><span id='topic+write_kbl'></span>

<h3>Description</h3>

<p>A wrapper around <code>kableExtra::save_kable</code> that facilitates saving kable objects
within knitr documents. Automatically handles file naming, directory creation,
and supports multiple output formats with Bootstrap theming options.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_kbl(
  kbl_obj,
  file = NULL,
  target_dir = NULL,
  device = "pdf",
  bs_theme = "bootstrap",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="write_kbl_+3A_kbl_obj">kbl_obj</code></td>
<td>
<p>The knitr_kable object to be written.</p>
</td></tr>
<tr><td><code id="write_kbl_+3A_file">file</code></td>
<td>
<p>The name of the file to be written. If not specified,
the name will be based on the current knitr code block label.</p>
</td></tr>
<tr><td><code id="write_kbl_+3A_target_dir">target_dir</code></td>
<td>
<p>The directory where the file will be written. If not
specified, the current working directory will be used.</p>
</td></tr>
<tr><td><code id="write_kbl_+3A_device">device</code></td>
<td>
<p>The device to be used for saving the file. Options
include &quot;pdf&quot; (default), &quot;html&quot;, &quot;latex&quot;, &quot;png&quot;, and &quot;jpeg&quot;. Note that
a Chromium-based browser (e.g., Google Chrome, Chromium, Microsoft Edge or
Brave) is required on your system for all options except &quot;latex'. If a
suitable browser is not available, the function will stop and return an
error message.</p>
</td></tr>
<tr><td><code id="write_kbl_+3A_bs_theme">bs_theme</code></td>
<td>
<p>The Bootstrap theme to be applied to the kable object
(only applicable for HTML output). Default is &quot;bootstrap&quot;.</p>
</td></tr>
<tr><td><code id="write_kbl_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to the <code>save_kable</code>
function from the <code>kableExtra</code> package.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function extends <code>save_kable</code> functionality by:
</p>

<ul>
<li><p> Using knitr code block labels for automatic file naming
</p>
</li>
<li><p> Creating target directories if they don't exist
</p>
</li>
<li><p> Supporting multiple output formats (PDF, HTML, LaTeX, PNG, JPEG)
</p>
</li>
<li><p> Applying Bootstrap themes for HTML output
</p>
</li>
<li><p> Preserving table styling and formatting
</p>
</li></ul>

<p>For HTML output, the function supports all Bootstrap themes available in
kableExtra. The default theme is &quot;bootstrap&quot;.
</p>


<h3>Value</h3>

<p>The path of the written file.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(knitr)

table_dir &lt;- file.path(tempdir(), "table")

mtcars_kbl &lt;- kable(
  x = mtcars[1:5, ],
  format = "html"
)

# Write a kable object as a PDF file
write_kbl(
  kbl_obj = mtcars_kbl,
  file = "kable_pdf",
  target_dir = table_dir,
  device = "pdf"
)

# Write a kable as an HTML file with a custom Bootstrap theme
write_kbl(
  kbl_obj = mtcars_kbl,
  file = "kable_html",
  target_dir = table_dir,
  device = "html",
  bs_theme = "flatly"
)

unlink(table_dir)

## End(Not run)
</code></pre>

<hr>
<h2 id='write_obj'>Write an R object as a file</h2><span id='topic+write_obj'></span>

<h3>Description</h3>

<p>A wrapper around <code>dput</code> that facilitates saving R objects within knitr documents.
Automatically handles file naming and directory creation, with support for
preserving object structure and attributes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_obj(obj, file = NULL, target_dir = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="write_obj_+3A_obj">obj</code></td>
<td>
<p>The R object to be written.</p>
</td></tr>
<tr><td><code id="write_obj_+3A_file">file</code></td>
<td>
<p>The name of the file to be written. If not specified, the
label of the code block will be used.</p>
</td></tr>
<tr><td><code id="write_obj_+3A_target_dir">target_dir</code></td>
<td>
<p>The directory where the file will be written. If not
specified, the current working directory will be used.</p>
</td></tr>
<tr><td><code id="write_obj_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to <code>dput</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function extends <code>dput</code> functionality by:
</p>

<ul>
<li><p> Using knitr code block labels for automatic file naming
</p>
</li>
<li><p> Creating target directories if they don't exist
</p>
</li>
<li><p> Preserving complex object structures and attributes
</p>
</li>
<li><p> Supporting all R object types (vectors, lists, data frames, etc.)
</p>
</li></ul>

<p>Objects saved with this function can be read back using the standard
<code>dget</code> function.
</p>


<h3>Value</h3>

<p>The path of the written file.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
obj_dir &lt;- file.path(tempdir(), "obj")

# Write a data frame as a file
write_obj(
  obj = mtcars,
  file = "mtcars_data",
  target_dir = obj_dir
)

# Read the file back into an R session
my_mtcars &lt;- dget(file.path(obj_dir, "mtcars_data"))

unlink(obj_dir)

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
