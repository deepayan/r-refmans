<!DOCTYPE html><html><head><title>Help for package arf</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {arf}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#adversarial_rf'><p>Adversarial Random Forests</p></a></li>
<li><a href='#col_rename'><p>Adaptive column renaming</p></a></li>
<li><a href='#expct'><p>Expected Value</p></a></li>
<li><a href='#forde'><p>Forests for Density Estimation</p></a></li>
<li><a href='#forge'><p>Forests for Generative Modeling</p></a></li>
<li><a href='#leaf_posterior'><p>Compute leaf posterior</p></a></li>
<li><a href='#lik'><p>Likelihood Estimation</p></a></li>
<li><a href='#post_x'><p>Post-process data</p></a></li>
<li><a href='#prep_evi'><p>Preprocess evidence</p></a></li>
<li><a href='#prep_x'><p>Preprocess input data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Adversarial Random Forests</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-01-24</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Marvin N. Wright &lt;cran@wrig.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Adversarial random forests (ARFs) recursively partition data into 
    fully factorized leaves, where features are jointly independent. The 
    procedure is iterative, with alternating rounds of generation and 
    discrimination. Data becomes increasingly realistic at each round, until 
    original and synthetic samples can no longer be reliably distinguished. 
    This is useful for several unsupervised learning tasks, such as density
    estimation and data synthesis. Methods for both are implemented in this
    package. ARFs naturally handle unstructured data with mixed continuous and 
    categorical covariates. They inherit many of the benefits of random forests, 
    including speed, flexibility, and solid performance with default parameters. 
    For details, see Watson et al. (2022) &lt;<a href="https://arxiv.org/abs/2205.09435">arXiv:2205.09435</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/bips-hb/arf">https://github.com/bips-hb/arf</a>, <a href="https://bips-hb.github.io/arf/">https://bips-hb.github.io/arf/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/bips-hb/arf/issues">https://github.com/bips-hb/arf/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table, ranger, foreach, truncnorm</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.0</td>
</tr>
<tr>
<td>Suggests:</td>
<td>ggplot2, doParallel, mlbench, knitr, rmarkdown, tibble,
testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-24 13:15:33 UTC; wright</td>
</tr>
<tr>
<td>Author:</td>
<td>Marvin N. Wright <a href="https://orcid.org/0000-0002-8542-6291"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  David S. Watson <a href="https://orcid.org/0000-0001-9632-2159"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Kristin Blesch <a href="https://orcid.org/0000-0001-6241-3079"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Jan Kapar <a href="https://orcid.org/0009-0000-6408-2840"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-24 14:53:15 UTC</td>
</tr>
</table>
<hr>
<h2 id='adversarial_rf'>Adversarial Random Forests</h2><span id='topic+adversarial_rf'></span>

<h3>Description</h3>

<p>Implements an adversarial random forest to learn independence-inducing splits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adversarial_rf(
  x,
  num_trees = 10L,
  min_node_size = 2L,
  delta = 0,
  max_iters = 10L,
  early_stop = TRUE,
  prune = TRUE,
  verbose = TRUE,
  parallel = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adversarial_rf_+3A_x">x</code></td>
<td>
<p>Input data. Integer variables are recoded as ordered factors with
a warning. See Details.</p>
</td></tr>
<tr><td><code id="adversarial_rf_+3A_num_trees">num_trees</code></td>
<td>
<p>Number of trees to grow in each forest. The default works
well for most generative modeling tasks, but should be increased for
likelihood estimation. See Details.</p>
</td></tr>
<tr><td><code id="adversarial_rf_+3A_min_node_size">min_node_size</code></td>
<td>
<p>Minimal number of real data samples in leaf nodes.</p>
</td></tr>
<tr><td><code id="adversarial_rf_+3A_delta">delta</code></td>
<td>
<p>Tolerance parameter. Algorithm converges when OOB accuracy is
&lt; 0.5 + <code>delta</code>.</p>
</td></tr>
<tr><td><code id="adversarial_rf_+3A_max_iters">max_iters</code></td>
<td>
<p>Maximum iterations for the adversarial loop.</p>
</td></tr>
<tr><td><code id="adversarial_rf_+3A_early_stop">early_stop</code></td>
<td>
<p>Terminate loop if performance fails to improve from one
round to the next?</p>
</td></tr>
<tr><td><code id="adversarial_rf_+3A_prune">prune</code></td>
<td>
<p>Impose <code>min_node_size</code> by pruning?</p>
</td></tr>
<tr><td><code id="adversarial_rf_+3A_verbose">verbose</code></td>
<td>
<p>Print discriminator accuracy after each round?</p>
</td></tr>
<tr><td><code id="adversarial_rf_+3A_parallel">parallel</code></td>
<td>
<p>Compute in parallel? Must register backend beforehand, e.g.
via <code>doParallel</code>.</p>
</td></tr>
<tr><td><code id="adversarial_rf_+3A_...">...</code></td>
<td>
<p>Extra parameters to be passed to <code>ranger</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The adversarial random forest (ARF) algorithm partitions data into fully
factorized leaves where features are jointly independent. ARFs are trained
iteratively, with alternating rounds of generation and discrimination. In
the first instance, synthetic data is generated via independent bootstraps of
each feature, and a RF classifier is trained to distinguish between real and
fake samples. In subsequent rounds, synthetic data is generated separately in
each leaf, using splits from the previous forest. This creates increasingly
realistic data that satisfies local independence by construction. The
algorithm converges when a RF cannot reliably distinguish between the two
classes, i.e. when OOB accuracy falls below 0.5 + <code>delta</code>.
</p>
<p>ARFs are useful for several unsupervised learning tasks, such as density
estimation (see <code><a href="#topic+forde">forde</a></code>) and data synthesis (see
<code><a href="#topic+forge">forge</a></code>). For the former, we recommend increasing the number of
trees for improved performance (typically on the order of 100-1000 depending
on sample size).
</p>
<p>Integer variables are recoded with a warning. Default behavior is to convert
those with six or more unique values to numeric, while those with up to five
unique values are treated as ordered factors. To override this behavior,
explicitly recode integer variables to the target type prior to training.
</p>
<p>Note: convergence is not guaranteed in finite samples. The <code>max_iters</code>
argument sets an upper bound on the number of training rounds. Similar
results may be attained by increasing <code>delta</code>. Even a single round can
often give good performance, but data with strong or complex dependencies may
require more iterations. With the default <code>early_stop = TRUE</code>, the
adversarial loop terminates if performance does not improve from one round
to the next, in which case further training may be pointless.
</p>


<h3>Value</h3>

<p>A random forest object of class <code>ranger</code>.
</p>


<h3>References</h3>

<p>Watson, D., Blesch, K., Kapar, J., &amp; Wright, M. (2023). Adversarial random
forests for density estimation and generative modeling. In <em>Proceedings
of the 26th International Conference on Artificial Intelligence and
Statistics</em>, pp. 5357-5375.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+forde">forde</a></code>, <code><a href="#topic+forge">forge</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>arf &lt;- adversarial_rf(iris)


</code></pre>

<hr>
<h2 id='col_rename'>Adaptive column renaming</h2><span id='topic+col_rename'></span>

<h3>Description</h3>

<p>This function renames columns in case the input data.frame includes any
colnames required by internal functions (e.g., <code>"y"</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>col_rename(df, old_name)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="col_rename_+3A_df">df</code></td>
<td>
<p>Input data.frame.</p>
</td></tr>
<tr><td><code id="col_rename_+3A_old_name">old_name</code></td>
<td>
<p>Name of column to be renamed.</p>
</td></tr>
</table>

<hr>
<h2 id='expct'>Expected Value</h2><span id='topic+expct'></span>

<h3>Description</h3>

<p>Compute the expectation of some query variable(s), optionally conditioned
on some event(s).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>expct(params, query = NULL, evidence = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="expct_+3A_params">params</code></td>
<td>
<p>Circuit parameters learned via <code><a href="#topic+forde">forde</a></code>.</p>
</td></tr>
<tr><td><code id="expct_+3A_query">query</code></td>
<td>
<p>Optional character vector of variable names. Estimates will be
computed for each. If <code>NULL</code>, all variables other than those in
<code>evidence</code> will be estimated.</p>
</td></tr>
<tr><td><code id="expct_+3A_evidence">evidence</code></td>
<td>
<p>Optional set of conditioning events. This can take one of
three forms: (1) a partial sample, i.e. a single row of data with some but
not all columns; (2) a data frame of conditioning events, which allows for
inequalities; or (3) a posterior distribution over leaves. See Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes expected values for any subset of features, optionally
conditioned on some event(s).
</p>


<h3>Value</h3>

<p>A one row data frame with values for all query variables.
</p>


<h3>References</h3>

<p>Watson, D., Blesch, K., Kapar, J., &amp; Wright, M. (2023). Adversarial random
forests for density estimation and generative modeling. In <em>Proceedings
of the 26th International Conference on Artificial Intelligence and
Statistics</em>, pp. 5357-5375.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adversarial_rf">adversarial_rf</a></code>, <code><a href="#topic+forde">forde</a></code>, <code><a href="#topic+lik">lik</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Train ARF and corresponding circuit
arf &lt;- adversarial_rf(iris)
psi &lt;- forde(arf, iris)

# What is the expected value Sepal.Length?
expct(psi, query = "Sepal.Length")

# What if we condition on Species = "setosa"?
evi &lt;- data.frame(Species = "setosa")
expct(psi, query = "Sepal.Length", evidence = evi)

# Compute expectations for all features other than Species
expct(psi, evidence = evi)


</code></pre>

<hr>
<h2 id='forde'>Forests for Density Estimation</h2><span id='topic+forde'></span>

<h3>Description</h3>

<p>Uses a pre-trained ARF model to estimate leaf and distribution parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>forde(
  arf,
  x,
  oob = FALSE,
  family = "truncnorm",
  finite_bounds = FALSE,
  alpha = 0,
  epsilon = 0,
  parallel = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="forde_+3A_arf">arf</code></td>
<td>
<p>Pre-trained <code><a href="#topic+adversarial_rf">adversarial_rf</a></code>. Alternatively, any
object of class <code>ranger</code>.</p>
</td></tr>
<tr><td><code id="forde_+3A_x">x</code></td>
<td>
<p>Training data for estimating parameters.</p>
</td></tr>
<tr><td><code id="forde_+3A_oob">oob</code></td>
<td>
<p>Only use out-of-bag samples for parameter estimation? If
<code>TRUE</code>, <code>x</code> must be the same dataset used to train <code>arf</code>.</p>
</td></tr>
<tr><td><code id="forde_+3A_family">family</code></td>
<td>
<p>Distribution to use for density estimation of continuous
features. Current options include truncated normal (the default
<code>family = "truncnorm"</code>) and uniform (<code>family = "unif"</code>). See
Details.</p>
</td></tr>
<tr><td><code id="forde_+3A_finite_bounds">finite_bounds</code></td>
<td>
<p>Impose finite bounds on all continuous variables?</p>
</td></tr>
<tr><td><code id="forde_+3A_alpha">alpha</code></td>
<td>
<p>Optional pseudocount for Laplace smoothing of categorical
features. This avoids zero-mass points when test data fall outside the
support of training data. Effectively parametrizes a flat Dirichlet prior
on multinomial likelihoods.</p>
</td></tr>
<tr><td><code id="forde_+3A_epsilon">epsilon</code></td>
<td>
<p>Optional slack parameter on empirical bounds when
<code>family = "unif"</code> or <code>finite_bounds = TRUE</code>. This avoids
zero-density points when test data fall outside the support of training
data. The gap between lower and upper bounds is expanded by a factor of
<code>1 + epsilon</code>.</p>
</td></tr>
<tr><td><code id="forde_+3A_parallel">parallel</code></td>
<td>
<p>Compute in parallel? Must register backend beforehand, e.g.
via <code>doParallel</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>forde</code> extracts leaf parameters from a pretrained forest and learns
distribution parameters for data within each leaf. The former includes
coverage (proportion of data falling into the leaf) and split criteria. The
latter includes proportions for categorical features and mean/variance for
continuous features. The result is a probabilistic circuit, stored as a
<code>data.table</code>, which can be used for various downstream inference tasks.
</p>
<p>Currently, <code>forde</code> only provides support for a limited number of
distributional families: truncated normal or uniform for continuous data,
and multinomial for discrete data. Future releases will accommodate a larger
set of options.
</p>
<p>Though <code>forde</code> was designed to take an adversarial random forest as
input, the function's first argument can in principle be any object of class
<code>ranger</code>. This allows users to test performance with alternative
pipelines (e.g., with supervised forest input). There is also no requirement
that <code>x</code> be the data used to fit <code>arf</code>, unless <code>oob = TRUE</code>.
In fact, using another dataset here may protect against overfitting. This
connects with Wager &amp; Athey's (2018) notion of &quot;honest trees&quot;.
</p>


<h3>Value</h3>

<p>A <code>list</code> with 5 elements: (1) parameters for continuous data; (2)
parameters for discrete data; (3) leaf indices and coverage; (4) metadata on
variables; and (5) the data input class. This list is used for estimating
likelihoods with <code><a href="#topic+lik">lik</a></code> and generating data with <code><a href="#topic+forge">forge</a></code>.
</p>


<h3>References</h3>

<p>Watson, D., Blesch, K., Kapar, J., &amp; Wright, M. (2023). Adversarial random
forests for density estimation and generative modeling. In <em>Proceedings
of the 26th International Conference on Artificial Intelligence and
Statistics</em>, pp. 5357-5375.
</p>
<p>Wager, S. &amp; Athey, S. (2018). Estimation and inference of heterogeneous
treatment effects using random forests. <em>J. Am. Stat. Assoc.</em>,
<em>113</em>(523): 1228-1242.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adversarial_rf">adversarial_rf</a></code>, <code><a href="#topic+forge">forge</a></code>, <code><a href="#topic+lik">lik</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>arf &lt;- adversarial_rf(iris)
psi &lt;- forde(arf, iris)
head(psi)


</code></pre>

<hr>
<h2 id='forge'>Forests for Generative Modeling</h2><span id='topic+forge'></span>

<h3>Description</h3>

<p>Uses pre-trained FORDE model to simulate synthetic data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>forge(params, n_synth, evidence = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="forge_+3A_params">params</code></td>
<td>
<p>Circuit parameters learned via <code><a href="#topic+forde">forde</a></code>.</p>
</td></tr>
<tr><td><code id="forge_+3A_n_synth">n_synth</code></td>
<td>
<p>Number of synthetic samples to generate.</p>
</td></tr>
<tr><td><code id="forge_+3A_evidence">evidence</code></td>
<td>
<p>Optional set of conditioning events. This can take one of
three forms: (1) a partial sample, i.e. a single row of data with
some but not all columns; (2) a data frame of conditioning events,
which allows for inequalities; or (3) a posterior distribution over leaves.
See Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>forge</code> simulates a synthetic dataset of <code>n_synth</code> samples. First,
leaves are sampled in proportion to either their coverage (if
<code>evidence = NULL</code>) or their posterior probability. Then, each feature is
sampled independently within each leaf according to the probability mass or
density function learned by <code><a href="#topic+forde">forde</a></code>. This will create realistic
data so long as the adversarial RF used in the previous step satisfies the
local independence criterion. See Watson et al. (2023).
</p>
<p>There are three methods for (optionally) encoding conditioning events via the
<code>evidence</code> argument. The first is to provide a partial sample, where
some but not all columns from the training data are present. The second is to
provide a data frame with three columns: <code>variable</code>, <code>relation</code>,
and <code>value</code>. This supports inequalities via <code>relation</code>.
Alternatively, users may directly input a pre-calculated posterior
distribution over leaves, with columns <code>f_idx</code> and <code>wt</code>. This may
be preferable for complex constraints. See Examples.
</p>


<h3>Value</h3>

<p>A dataset of <code>n_synth</code> synthetic samples.
</p>


<h3>References</h3>

<p>Watson, D., Blesch, K., Kapar, J., &amp; Wright, M. (2023). Adversarial random
forests for density estimation and generative modeling. In <em>Proceedings
of the 26th International Conference on Artificial Intelligence and
Statistics</em>, pp. 5357-5375.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adversarial_rf">adversarial_rf</a></code>, <code><a href="#topic+forde">forde</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>arf &lt;- adversarial_rf(iris)
psi &lt;- forde(arf, iris)
x_synth &lt;- forge(psi, n_synth = 100)

# Condition on Species = "setosa"
evi &lt;- data.frame(Species = "setosa")
x_synth &lt;- forge(psi, n_synth = 100, evidence = evi)

# Condition in Species = "setosa" and Sepal.Length &gt; 6
evi &lt;- data.frame(variable = c("Species", "Sepal.Length"),
                  relation = c("==", "&gt;"), 
                  value = c("setosa", 6))
x_synth &lt;- forge(psi, n_synth = 100, evidence = evi)

# Or just input some distribution on leaves
# (Weights that do not sum to unity are automatically scaled)
n_leaves &lt;- nrow(psi$forest)
evi &lt;- data.frame(f_idx = psi$forest$f_idx, wt = rexp(n_leaves))
x_synth &lt;- forge(psi, n_synth = 100, evidence = evi)


</code></pre>

<hr>
<h2 id='leaf_posterior'>Compute leaf posterior</h2><span id='topic+leaf_posterior'></span>

<h3>Description</h3>

<p>This function returns a posterior distribution on leaves, conditional on some
evidence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>leaf_posterior(params, evidence)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="leaf_posterior_+3A_params">params</code></td>
<td>
<p>Circuit parameters learned via <code><a href="#topic+forde">forde</a></code>.</p>
</td></tr>
<tr><td><code id="leaf_posterior_+3A_evidence">evidence</code></td>
<td>
<p>Data frame of conditioning event(s).</p>
</td></tr>
</table>

<hr>
<h2 id='lik'>Likelihood Estimation</h2><span id='topic+lik'></span>

<h3>Description</h3>

<p>Compute the likelihood of input data, optionally conditioned on some event(s).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lik(
  params,
  query,
  evidence = NULL,
  arf = NULL,
  oob = FALSE,
  log = TRUE,
  batch = NULL,
  parallel = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lik_+3A_params">params</code></td>
<td>
<p>Circuit parameters learned via <code><a href="#topic+forde">forde</a></code>.</p>
</td></tr>
<tr><td><code id="lik_+3A_query">query</code></td>
<td>
<p>Data frame of samples, optionally comprising just a subset of
training features. Likelihoods will be computed for each sample. Missing
features will be marginalized out. See Details.</p>
</td></tr>
<tr><td><code id="lik_+3A_evidence">evidence</code></td>
<td>
<p>Optional set of conditioning events. This can take one of
three forms: (1) a partial sample, i.e. a single row of data with some but
not all columns; (2) a data frame of conditioning events, which allows for
inequalities; or (3) a posterior distribution over leaves. See Details.</p>
</td></tr>
<tr><td><code id="lik_+3A_arf">arf</code></td>
<td>
<p>Pre-trained <code><a href="#topic+adversarial_rf">adversarial_rf</a></code> or other object of class
<code>ranger</code>. This is not required but speeds up computation considerably
for total evidence queries. (Ignored for partial evidence queries.)</p>
</td></tr>
<tr><td><code id="lik_+3A_oob">oob</code></td>
<td>
<p>Only use out-of-bag leaves for likelihood estimation? If
<code>TRUE</code>, <code>x</code> must be the same dataset used to train <code>arf</code>.
Only applicable for total evidence queries.</p>
</td></tr>
<tr><td><code id="lik_+3A_log">log</code></td>
<td>
<p>Return likelihoods on log scale? Recommended to prevent underflow.</p>
</td></tr>
<tr><td><code id="lik_+3A_batch">batch</code></td>
<td>
<p>Batch size. The default is to compute densities for all of
queries in one round, which is always the fastest option if memory allows.
However, with large samples or many trees, it can be more memory efficient
to split the data into batches. This has no impact on results.</p>
</td></tr>
<tr><td><code id="lik_+3A_parallel">parallel</code></td>
<td>
<p>Compute in parallel? Must register backend beforehand, e.g.
via <code>doParallel</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the likelihood of input data, optionally conditioned
on some event(s). Queries may be partial, i.e. covering some but not all
features, in which case excluded variables will be marginalized out.
</p>
<p>There are three methods for (optionally) encoding conditioning events via the
<code>evidence</code> argument. The first is to provide a partial sample, where
some but not all columns from the training data are present. The second is to
provide a data frame with three columns: <code>variable</code>, <code>relation</code>,
and <code>value</code>. This supports inequalities via <code>relation</code>.
Alternatively, users may directly input a pre-calculated posterior
distribution over leaves, with columns <code>f_idx</code> and <code>wt</code>. This may
be preferable for complex constraints. See Examples.
</p>


<h3>Value</h3>

<p>A vector of likelihoods, optionally on the log scale.
</p>


<h3>References</h3>

<p>Watson, D., Blesch, K., Kapar, J., &amp; Wright, M. (2023). Adversarial random
forests for density estimation and generative modeling. In <em>Proceedings
of the 26th International Conference on Artificial Intelligence and
Statistics</em>, pp. 5357-5375.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adversarial_rf">adversarial_rf</a></code>, <code><a href="#topic+forge">forge</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Estimate average log-likelihood
arf &lt;- adversarial_rf(iris)
psi &lt;- forde(arf, iris)
ll &lt;- lik(psi, iris, arf = arf, log = TRUE)
mean(ll)

# Identical but slower
ll &lt;- lik(psi, iris, log = TRUE)
mean(ll)

# Partial evidence query
lik(psi, query = iris[1, 1:3])

# Condition on Species = "setosa"
evi &lt;- data.frame(Species = "setosa")
lik(psi, query = iris[1, 1:3], evidence = evi)

# Condition on Species = "setosa" and Petal.Width &gt; 0.3
evi &lt;- data.frame(variable = c("Species", "Petal.Width"),
                  relation = c("==", "&gt;"), 
                  value = c("setosa", 0.3))
lik(psi, query = iris[1, 1:3], evidence = evi)


</code></pre>

<hr>
<h2 id='post_x'>Post-process data</h2><span id='topic+post_x'></span>

<h3>Description</h3>

<p>This function prepares output data for forge.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>post_x(x, params)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="post_x_+3A_x">x</code></td>
<td>
<p>Input data.frame.</p>
</td></tr>
<tr><td><code id="post_x_+3A_params">params</code></td>
<td>
<p>Circuit parameters learned via <code><a href="#topic+forde">forde</a></code>.</p>
</td></tr>
</table>

<hr>
<h2 id='prep_evi'>Preprocess evidence</h2><span id='topic+prep_evi'></span>

<h3>Description</h3>

<p>This function prepares the evidence for computing leaf posteriors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prep_evi(params, evidence)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prep_evi_+3A_params">params</code></td>
<td>
<p>Circuit parameters learned via <code><a href="#topic+forde">forde</a></code>.</p>
</td></tr>
<tr><td><code id="prep_evi_+3A_evidence">evidence</code></td>
<td>
<p>Optional set of conditioning events.</p>
</td></tr>
</table>

<hr>
<h2 id='prep_x'>Preprocess input data</h2><span id='topic+prep_x'></span>

<h3>Description</h3>

<p>This function prepares input data for ARFs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prep_x(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prep_x_+3A_x">x</code></td>
<td>
<p>Input data.frame.</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
