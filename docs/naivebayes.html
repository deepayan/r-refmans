<!DOCTYPE html><html><head><title>Help for package naivebayes</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {naivebayes}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#naivebayes'><p>naivebayes</p></a></li>
<li><a href='#bernoulli_naive_bayes'><p>Bernoulli Naive Bayes Classifier</p></a></li>
<li><a href='#coef'><p>Extract Model Coefficients</p></a></li>
<li><a href='#gaussian_naive_bayes'><p>Gaussian Naive Bayes Classifier</p></a></li>
<li><a href='#get_cond_dist'><p>Obtain names of class conditional distribution assigned to features</p></a></li>
<li><a href='#Infix+20operators'><p>Predict Method for Family of Naive Bayes Objects</p></a></li>
<li><a href='#multinomial_naive_bayes'><p>Multinomial Naive Bayes Classifier</p></a></li>
<li><a href='#naive_bayes'><p>Naive Bayes Classifier</p></a></li>
<li><a href='#nonparametric_naive_bayes'><p>Non-Parametric Naive Bayes Classifier</p></a></li>
<li><a href='#plot.bernoulli_naive_bayes'><p>Plot Method for bernoulli_naive_bayes Objects</p></a></li>
<li><a href='#plot.gaussian_naive_bayes'><p>Plot Method for gaussian_naive_bayes Objects</p></a></li>
<li><a href='#plot.naive_bayes'><p>Plot Method for naive_bayes Objects</p></a></li>
<li><a href='#plot.nonparametric_naive_bayes'><p>Plot Method for nonparametric_naive_bayes Objects</p></a></li>
<li><a href='#plot.poisson_naive_bayes'><p>Plot Method for poisson_naive_bayes Objects</p></a></li>
<li><a href='#poisson_naive_bayes'><p>Poisson Naive Bayes Classifier</p></a></li>
<li><a href='#predict.bernoulli_naive_bayes'><p>Predict Method for bernoulli_naive_bayes Objects</p></a></li>
<li><a href='#predict.gaussian_naive_bayes'><p>Predict Method for gaussian_naive_bayes Objects</p></a></li>
<li><a href='#predict.multinomial_naive_bayes'><p>Predict Method for multinomial_naive_bayes Objects</p></a></li>
<li><a href='#predict.naive_bayes'><p>Predict Method for naive_bayes Objects</p></a></li>
<li><a href='#predict.nonparametric_naive_bayes'><p>Predict Method for nonparametric_naive_bayes Objects</p></a></li>
<li><a href='#predict.poisson_naive_bayes'><p>Predict Method for poisson_naive_bayes Objects</p></a></li>
<li><a href='#tables'><p>Browse Tables of Naive Bayes Classifier</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>High Performance Implementation of the Naive Bayes Algorithm</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Description:</td>
<td>In this implementation of the Naive Bayes classifier
    following class conditional distributions are available: 'Bernoulli',
    'Categorical', 'Gaussian', 'Poisson', 'Multinomial' and non-parametric
    representation of the class conditional density estimated via Kernel
    Density Estimation. Implemented classifiers handle missing data and
    can take advantage of sparse data.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/majkamichal/naivebayes">https://github.com/majkamichal/naivebayes</a>,
<a href="https://majkamichal.github.io/naivebayes/">https://majkamichal.github.io/naivebayes/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/majkamichal/naivebayes/issues">https://github.com/majkamichal/naivebayes/issues</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, Matrix</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-16 12:22:56 UTC; michal</td>
</tr>
<tr>
<td>Author:</td>
<td>Michal Majka <a href="https://orcid.org/0000-0002-7524-8014"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michal Majka &lt;michalmajka@hotmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-16 12:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='naivebayes'>naivebayes</h2><span id='topic+naivebayes'></span><span id='topic+naivebayes-package'></span>

<h3>Description</h3>

<p>The <b>naivebayes</b> package presents an efficient implementation of the widely-used Naive Bayes classifier. It upholds three core principles: efficiency, user-friendliness, and reliance solely on Base R. By adhering to the latter principle, the package ensures stability and reliability without introducing external dependencies. This design choice maintains efficiency by leveraging the optimized routines inherent in Base R, many of which are programmed in high-performance languages like C/C++ or FORTRAN. By following these principles, the naivebayes package provides a reliable and efficient tool for Naive Bayes classification tasks, ensuring that users can perform their analyses effectively and with ease, even in the presence of missing data.
</p>


<h3>Details</h3>

<p>The general <b><code>naive_bayes()</code></b> function is designed to determine the class of each feature in a dataset, and depending on user specifications, it can assume various distributions for each feature. It currently supports the following class conditional distributions:
</p>

<ul>
<li><p> Categorical distribution for discrete features (with Bernoulli distribution as a special case for binary outcomes)
</p>
</li>
<li><p> Poisson distribution for non-negative integer features
</p>
</li>
<li><p> Gaussian distribution for continuous features
</p>
</li>
<li><p> non-parametrically estimated densities via Kernel Density Estimation for continuous features
</p>
</li></ul>

<p>In addition to the general Naive Bayes function, the package provides specialized functions for various types of Naive Bayes classifiers. The specialized functions are carefully optimized for efficiency, utilizing linear algebra operations to excel when handling dense matrices. Additionally, they can also exploit sparsity of matrices for enhanced performance:
</p>

<ul>
<li><p> Bernoulli Naiive Bayes via <b><code>bernoulli_naive_bayes()</code></b>
</p>
</li>
<li><p> Multinomial Naive Bayes via <b><code>multinomial_naive_bayes()</code></b>
</p>
</li>
<li><p> Poisson Naive Bayes via <b><code>poisson_naive_bayes()</code></b>
</p>
</li>
<li><p> Gaussian Naive Bayes via <b><code>gaussian_naive_bayes()</code></b>
</p>
</li>
<li><p> Non-Parametric Naive Bayes via <b><code>nonparametric_naive_bayes()</code></b>
</p>
</li></ul>

<p>These specialized classifiers are tailored to different assumptions about the underlying data distributions, offering users versatile tools for classification tasks. Moreover, the package incorporates various helper functions aimed at enhancing the user experience. Notably, the model fitting functions provided by the package can effectively handle missing data, ensuring that users can utilize the classifiers even in the presence of incomplete information.
</p>
<p><b>Extended documentation can be found on the website:</b>
</p>

<ul>
<li> <p><a href="https://majkamichal.github.io/naivebayes/">https://majkamichal.github.io/naivebayes/</a>
</p>
</li></ul>

<p><b>Bug reports:</b>
</p>

<ul>
<li> <p><a href="https://github.com/majkamichal/naivebayes/issues">https://github.com/majkamichal/naivebayes/issues</a>
</p>
</li></ul>

<p><b>Contact:</b>
</p>

<ul>
<li> <p><a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>
</li></ul>


<hr>
<h2 id='bernoulli_naive_bayes'>Bernoulli Naive Bayes Classifier</h2><span id='topic+bernoulli_naive_bayes'></span>

<h3>Description</h3>

<p><code>bernoulli_naive_bayes</code> is used to fit the Bernoulli Naive Bayes model in which all class conditional distributions are assumed to be Bernoulli and be independent.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bernoulli_naive_bayes(x, y, prior = NULL, laplace = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bernoulli_naive_bayes_+3A_x">x</code></td>
<td>
<p>matrix with numeric 0-1 predictors (matrix or dgCMatrix from Matrix package).</p>
</td></tr>
<tr><td><code id="bernoulli_naive_bayes_+3A_y">y</code></td>
<td>
<p>class vector (character/factor/logical).</p>
</td></tr>
<tr><td><code id="bernoulli_naive_bayes_+3A_prior">prior</code></td>
<td>
<p>vector with prior probabilities of the classes. If unspecified, the class proportions for the training set are used. If present, the probabilities should be specified in the order of the factor levels.</p>
</td></tr>
<tr><td><code id="bernoulli_naive_bayes_+3A_laplace">laplace</code></td>
<td>
<p>value used for Laplace smoothing (additive smoothing). Defaults to 0 (no Laplace smoothing).</p>
</td></tr>
<tr><td><code id="bernoulli_naive_bayes_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a specialized version of the Naive Bayes classifier, in which all features take on numeric 0-1 values and class conditional probabilities are modelled with the Bernoulli distribution.
</p>
<p>The Bernoulli Naive Bayes is available in both, <code>naive_bayes</code> and <code>bernoulli_naive_bayes</code>. The latter provides more efficient performance though. Faster calculation times come from restricting the data to a numeric 0-1 matrix and taking advantage of linear algebra operations. Sparse matrices of class &quot;dgCMatrix&quot; (Matrix package) are supported in order to furthermore speed up calculation times.
</p>
<p>The <code>bernoulli_naive_bayes</code> and <code>naive_bayes()</code> are equivalent when the latter uses &quot;0&quot;-&quot;1&quot; character matrix.
</p>
<p>The missing values (NAs) are omited while constructing probability tables. Also, the corresponding predict function excludes all NAs from the calculation of posterior probabilities  (an informative warning is always given).
</p>


<h3>Value</h3>

<p><code>bernoulli_naive_bayes</code> returns an object of class &quot;bernoulli_naive_bayes&quot; which is a list with following components:
</p>
<table>
<tr><td><code>data</code></td>
<td>
<p>list with two components: <code>x</code> (matrix with predictors) and <code>y</code> (class variable).</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>character vector with values of the class variable.</p>
</td></tr>
<tr><td><code>laplace</code></td>
<td>
<p>amount of Laplace smoothing (additive smoothing).</p>
</td></tr>
<tr><td><code>prob1</code></td>
<td>
<p>matrix with class conditional probabilities for the value 1. Based on this matrix full probability tables can be constructed. Please, see <code><a href="#topic+tables">tables</a></code> and <code><a href="#topic+coef">coef</a></code>.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>numeric vector with prior probabilities.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call that produced this object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+naive_bayes">naive_bayes</a></code>, <code><a href="#topic+predict.bernoulli_naive_bayes">predict.bernoulli_naive_bayes</a></code>, <code><a href="#topic+plot.bernoulli_naive_bayes">plot.bernoulli_naive_bayes</a></code>, <code><a href="#topic+tables">tables</a></code>, <code><a href="#topic+get_cond_dist">get_cond_dist</a></code>, <code><a href="#topic++25class+25">%class%</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># library(naivebayes)

### Simulate the data:
set.seed(1)
cols &lt;- 10 ; rows &lt;- 100 ; probs &lt;- c("0" = 0.9, "1" = 0.1)
M &lt;- matrix(sample(0:1, rows * cols,  TRUE, probs), nrow = rows, ncol = cols)
y &lt;- factor(sample(paste0("class", LETTERS[1:2]), rows, TRUE, prob = c(0.3,0.7)))
colnames(M) &lt;- paste0("V", seq_len(ncol(M)))
laplace &lt;- 0

### Train the Bernoulli Naive Bayes
bnb &lt;- bernoulli_naive_bayes(x = M, y = y, laplace = laplace)
summary(bnb)

# Classification
head(predict(bnb, newdata = M, type = "class")) # head(bnb %class% M)

# Posterior probabilities
head(predict(bnb, newdata = M, type = "prob")) # head(bnb %prob% M)

# Parameter estimates
coef(bnb)


### Sparse data: train the Bernoulli Naive Bayes
library(Matrix)
M_sparse &lt;- Matrix(M, sparse = TRUE)
class(M_sparse) # dgCMatrix

# Fit the model with sparse data
bnb_sparse &lt;- bernoulli_naive_bayes(M_sparse, y, laplace = laplace)

# Classification
head(predict(bnb_sparse, newdata = M_sparse, type = "class"))

# Posterior probabilities
head(predict(bnb_sparse, newdata = M_sparse, type = "prob"))

# Parameter estimates
coef(bnb_sparse)


### Equivalent calculation with general naive_bayes function.
### (no sparse data support by naive_bayes)

# Make sure that the columns are factors with the 0-1 levels
df &lt;- as.data.frame(lapply(as.data.frame(M), factor, levels = c(0,1)))
# sapply(df, class)

nb &lt;- naive_bayes(df, y, laplace = laplace)
summary(nb)
head(predict(nb, type = "prob"))

# Obtain probability tables
tables(nb, which = "V1")
tables(bnb, which = "V1")

# Visualise class conditional Bernoulli distributions
plot(nb, "V1", prob = "conditional")
plot(bnb, which = "V1", prob = "conditional")

# Check the equivalence of the class conditional distributions
all(get_cond_dist(nb) == get_cond_dist(bnb))
</code></pre>

<hr>
<h2 id='coef'>Extract Model Coefficients</h2><span id='topic+coef.bernoulli_naive_bayes'></span><span id='topic+coef.multinomial_naive_bayes'></span><span id='topic+coef.poisson_naive_bayes'></span><span id='topic+coef.gaussian_naive_bayes'></span>

<h3>Description</h3>

<p><code>coef</code> is a generic function which extracts model coefficients from specialized Naive Bayes objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bernoulli_naive_bayes'
coef(object, ...)

## S3 method for class 'multinomial_naive_bayes'
coef(object, ...)

## S3 method for class 'poisson_naive_bayes'
coef(object, ...)

## S3 method for class 'gaussian_naive_bayes'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef_+3A_object">object</code></td>
<td>
<p>object of class inheriting from <code>"*_naive_bayes"</code>.</p>
</td></tr>
<tr><td><code id="coef_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Coefficients extracted from the specialised Naive Bayes objects in form of a data frame.
</p>


<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bernoulli_naive_bayes">bernoulli_naive_bayes</a></code>, <code><a href="#topic+multinomial_naive_bayes">multinomial_naive_bayes</a></code>, <code><a href="#topic+poisson_naive_bayes">poisson_naive_bayes</a></code>, <code><a href="#topic+gaussian_naive_bayes">gaussian_naive_bayes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
y &lt;- iris[[5]]
M &lt;- as.matrix(iris[-5])

### Train the Gaussian Naive Bayes
gnb &lt;- gaussian_naive_bayes(x = M, y = y)

### Extract coefficients
coef(gnb)

coef(gnb)[c(TRUE,FALSE)] # only means

coef(gnb)[c(FALSE,TRUE)] # only standard deviations
</code></pre>

<hr>
<h2 id='gaussian_naive_bayes'>Gaussian Naive Bayes Classifier</h2><span id='topic+gaussian_naive_bayes'></span>

<h3>Description</h3>

<p><code>gaussian_naive_bayes</code> is used to fit the Gaussian Naive Bayes model in which all class conditional distributions are assumed to be Gaussian and be independent.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gaussian_naive_bayes(x, y, prior = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gaussian_naive_bayes_+3A_x">x</code></td>
<td>
<p>numeric matrix with metric predictors (matrix or dgCMatrix from Matrix package).</p>
</td></tr>
<tr><td><code id="gaussian_naive_bayes_+3A_y">y</code></td>
<td>
<p>class vector (character/factor/logical).</p>
</td></tr>
<tr><td><code id="gaussian_naive_bayes_+3A_prior">prior</code></td>
<td>
<p>vector with prior probabilities of the classes. If unspecified, the class proportions for the training set are used. If present, the probabilities should be specified in the order of the factor levels.</p>
</td></tr>
<tr><td><code id="gaussian_naive_bayes_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a specialized version of the Naive Bayes classifier, in which all features take on real values (numeric/integer) and class conditional probabilities are modelled with the Gaussian distribution.
</p>
<p>The Gaussian Naive Bayes is available in both, <code>naive_bayes</code> and <code>gaussian_naive_bayes</code>.The latter provides more efficient performance though. Faster calculation times come from restricting the data to a matrix with numeric columns and taking advantage of linear algebra operations. Sparse matrices of class &quot;dgCMatrix&quot; (Matrix package) are supported in order to furthermore speed up calculation times.
</p>
<p>The <code>gaussian_naive_bayes</code> and <code>naive_bayes()</code> are equivalent when the latter is used with <code>usepoisson = FALSE</code> and <code>usekernel = FALSE</code>; and a matrix/data.frame contains numeric columns.
</p>
<p>The missing values (NAs) are omited during the estimation process. Also, the corresponding predict function excludes all NAs from the calculation of posterior probabilities  (an informative warning is always given).
</p>


<h3>Value</h3>

<p><code>gaussian_naive_bayes</code> returns an object of class <code>"gaussian_naive_bayes"</code> which is a list with following components:
</p>
<table>
<tr><td><code>data</code></td>
<td>
<p>list with two components: <code>x</code> (matrix with predictors) and <code>y</code> (class variable).</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>character vector with values of the class variable.</p>
</td></tr>
<tr><td><code>params</code></td>
<td>
<p>list with two matrices, first containing the class conditional means and the second containing the class conditional standard deviations.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>numeric vector with prior probabilities.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call that produced this object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+naive_bayes">naive_bayes</a></code>, <code><a href="#topic+predict.gaussian_naive_bayes">predict.gaussian_naive_bayes</a></code>, <code><a href="#topic+plot.gaussian_naive_bayes">plot.gaussian_naive_bayes</a></code>, <code><a href="#topic+tables">tables</a></code>, <code><a href="#topic+get_cond_dist">get_cond_dist</a></code>, <code><a href="#topic++25class+25">%class%</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># library(naivebayes)
set.seed(1)
cols &lt;- 10 ; rows &lt;- 100
M &lt;- matrix(rnorm(rows * cols, 100, 15), nrow = rows, ncol = cols)
y &lt;- factor(sample(paste0("class", LETTERS[1:2]), rows, TRUE, prob = c(0.3,0.7)))
colnames(M) &lt;- paste0("V", seq_len(ncol(M)))


### Train the Gaussian Naive Bayes
gnb &lt;- gaussian_naive_bayes(x = M, y = y)
summary(gnb)

# Classification
head(predict(gnb, newdata = M, type = "class")) # head(gnb %class% M)

# Posterior probabilities
head(predict(gnb, newdata = M, type = "prob")) # head(gnb %prob% M)

# Parameter estimates
coef(gnb)


### Sparse data: train the Gaussian Naive Bayes
library(Matrix)
M_sparse &lt;- Matrix(M, sparse = TRUE)
class(M_sparse) # dgCMatrix

# Fit the model with sparse data
gnb_sparse &lt;- gaussian_naive_bayes(M_sparse, y)

# Classification
head(predict(gnb_sparse, newdata = M_sparse, type = "class"))

# Posterior probabilities
head(predict(gnb_sparse, newdata = M_sparse, type = "prob"))

# Parameter estimates
coef(gnb_sparse)


### Equivalent calculation with general naive_bayes function.
### (no sparse data support by naive_bayes)

nb &lt;- naive_bayes(M, y)
summary(nb)
head(predict(nb, type = "prob"))

# Obtain probability tables
tables(nb, which = "V1")
tables(gnb, which = "V1")

# Visualise class conditional Gaussian distributions
plot(nb, "V1", prob = "conditional")
plot(gnb, which = "V1", prob = "conditional")

# Check the equivalence of the class conditional distributions
all(get_cond_dist(nb) == get_cond_dist(gnb))
</code></pre>

<hr>
<h2 id='get_cond_dist'>Obtain names of class conditional distribution assigned to features</h2><span id='topic+get_cond_dist'></span>

<h3>Description</h3>

<p>Auxiliary function for <code>"naive_bayes"</code>, <code>"*_naive_bayes"</code> and <code>"naive_bayes_tables"</code>  objects for obtaining names of class conditional distributions assigned to the features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_cond_dist(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_cond_dist_+3A_object">object</code></td>
<td>
<p>object of class inheriting from <code>"naive_bayes"</code> or <code>"*_naive_bayes"</code> or <code>"naive_bayes_tables"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with names of class conditional distributions assigned to the features.
</p>


<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+naive_bayes">naive_bayes</a></code>, <code><a href="#topic+bernoulli_naive_bayes">bernoulli_naive_bayes</a></code>, <code><a href="#topic+multinomial_naive_bayes">multinomial_naive_bayes</a></code>, <code><a href="#topic+poisson_naive_bayes">poisson_naive_bayes</a></code>, <code><a href="#topic+gaussian_naive_bayes">gaussian_naive_bayes</a></code>, <code><a href="#topic+tables">tables</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
nb &lt;- naive_bayes(Species ~ ., data = iris)
get_cond_dist(nb) # &lt;=&gt; attr(nb$tables, "cond_dist")
get_cond_dist(tables(nb))
</code></pre>

<hr>
<h2 id='Infix+20operators'>Predict Method for Family of Naive Bayes Objects</h2><span id='topic++25class+25'></span><span id='topic++25prob+25'></span>

<h3>Description</h3>

<p>The infix operators <code>%class%</code> and <code>%prob%</code> are shorthands for performing classification and obtaining posterior probabilities, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %class% rhs
lhs %prob% rhs
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Infix+2B20operators_+3A_lhs">lhs</code></td>
<td>
<p>object of class inheriting from <code>"naive_bayes"</code> and <code>"*_naive_bayes"</code> family.</p>
</td></tr>
<tr><td><code id="Infix+2B20operators_+3A_rhs">rhs</code></td>
<td>
<p>dataframe or matrix for &quot;naive_bayes&quot; objects OR matrix for all &quot;*_naive_bayes&quot; objects.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>lhs</code> is of class inheriting from the family of the Naive Bayes objects and <code>rhs</code> is either dataframe or matrix then the infix operators <code>%class%</code> and <code>%prob%</code> are equivalent to:
</p>

<ul>
<li> <p><code>lhs %class% rhs</code> &lt;=&gt; <code>predict(lhs, newdata = rhs, type = "class", threshold = 0.001, eps = 0)</code>
</p>
</li>
<li> <p><code>lhs %prob% rhs</code> &lt;=&gt; <code>predict(lhs, newdata = rhs, type == "prob", threshold = 0.001, eps = 0)</code>
</p>
</li></ul>

<p>Compared to <code>predict()</code>, both operators do not allow changing values of fine tuning parameters <code>threshold</code> and <code>eps</code>.
</p>


<h3>Value</h3>


<ul>
<li> <p><code>%class%</code> returns factor with class labels corresponding to the maximal conditional posterior probabilities.
</p>
</li>
<li> <p><code>%prob%</code> returns a matrix with class label specific conditional posterior probabilities.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.naive_bayes">predict.naive_bayes</a></code>, <code><a href="#topic+predict.bernoulli_naive_bayes">predict.bernoulli_naive_bayes</a></code>, <code><a href="#topic+predict.multinomial_naive_bayes">predict.multinomial_naive_bayes</a></code>, <code><a href="#topic+predict.poisson_naive_bayes">predict.poisson_naive_bayes</a></code>, <code><a href="#topic+predict.gaussian_naive_bayes">predict.gaussian_naive_bayes</a></code>, <code><a href="#topic+predict.nonparametric_naive_bayes">predict.nonparametric_naive_bayes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Fit the model
nb &lt;- naive_bayes(Species ~ ., iris)

newdata &lt;- iris[1:5,-5] # Let's pretend

### Classification
nb %class% newdata
predict(nb, newdata, type = "class")

### Posterior probabilities
nb %prob% newdata
predict(nb, newdata, type = "prob")


</code></pre>

<hr>
<h2 id='multinomial_naive_bayes'>Multinomial Naive Bayes Classifier</h2><span id='topic+multinomial_naive_bayes'></span>

<h3>Description</h3>

<p><code>multinomial_naive_bayes</code> is used to fit the Multinomial Naive Bayes model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multinomial_naive_bayes(x, y, prior = NULL, laplace = 0.5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multinomial_naive_bayes_+3A_x">x</code></td>
<td>
<p>numeric matrix with integer predictors (matrix or dgCMatrix from Matrix package).</p>
</td></tr>
<tr><td><code id="multinomial_naive_bayes_+3A_y">y</code></td>
<td>
<p>class vector (character/factor/logical).</p>
</td></tr>
<tr><td><code id="multinomial_naive_bayes_+3A_prior">prior</code></td>
<td>
<p>vector with prior probabilities of the classes. If unspecified, the class proportions for the training set are used. If present, the probabilities should be specified in the order of the factor levels.</p>
</td></tr>
<tr><td><code id="multinomial_naive_bayes_+3A_laplace">laplace</code></td>
<td>
<p>value used for Laplace smoothing (additive smoothing). Defaults to 0.5.</p>
</td></tr>
<tr><td><code id="multinomial_naive_bayes_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a specialized version of the Naive Bayes classifier, where the features represent frequencies generated by a multinomial distribution.
</p>
<p>Sparse matrices of class &quot;dgCMatrix&quot; (Matrix package) are supported in order to speed up calculation times.
</p>
<p>Please note that the Multinomial Naive Bayes is not available through the <code><a href="#topic+naive_bayes">naive_bayes</a></code> function.
</p>


<h3>Value</h3>

<p><code>multinomial_naive_bayes</code> returns an object of class &quot;multinomial_naive_bayes&quot; which is a list with following components:
</p>
<table>
<tr><td><code>data</code></td>
<td>
<p>list with two components: <code>x</code> (matrix with predictors) and <code>y</code> (class variable).</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>character vector with values of the class variable.</p>
</td></tr>
<tr><td><code>laplace</code></td>
<td>
<p>amount of Laplace smoothing (additive smoothing).</p>
</td></tr>
<tr><td><code>params</code></td>
<td>
<p>matrix with class conditional parameter estimates.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>numeric vector with prior probabilities.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call that produced this object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>References</h3>

<p>Manning, C.D., Raghavan, P., &amp; Schütze, H. (2008). An
Introduction to Information Retrieval. Cambridge: Cambridge University
Press (Chapter 13). Available at <a href="https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf">https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.multinomial_naive_bayes">predict.multinomial_naive_bayes</a></code>, <code><a href="#topic+tables">tables</a></code>, <code><a href="#topic+get_cond_dist">get_cond_dist</a></code>, <code><a href="#topic++25class+25">%class%</a></code>, <code><a href="#topic+coef.multinomial_naive_bayes">coef.multinomial_naive_bayes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># library(naivebayes)

### Simulate the data:
set.seed(1)
cols &lt;- 3 # words
rows &lt;- 10000 # all documents
rows_spam &lt;- 100 # spam documents

prob_word_non_spam &lt;- prop.table(runif(cols))
prob_word_spam &lt;- prop.table(runif(cols))

M1 &lt;- t(rmultinom(rows_spam, size = cols, prob = prob_word_spam))
M2 &lt;- t(rmultinom(rows - rows_spam, size = cols, prob = prob_word_non_spam))
M &lt;- rbind(M1, M2)
colnames(M) &lt;- paste0("word", 1:cols) ; rownames(M) &lt;- paste0("doc", 1:rows)
head(M)
y &lt;- c(rep("spam", rows_spam), rep("non-spam", rows - rows_spam))

### Train the Multinomial Naive Bayes
laplace &lt;- 1
mnb &lt;- multinomial_naive_bayes(x = M, y = y, laplace = laplace)
summary(mnb)

# Classification
head(predict(mnb, newdata = M, type = "class")) # head(mnb %class% M)

# Posterior probabilities
head(predict(mnb, newdata = M, type = "prob")) # head(mnb %prob% M)

# Parameter estimates
coef(mnb)

# Compare
round(cbind(non_spam = prob_word_non_spam, spam = prob_word_spam), 3)



### Sparse data: train the Multinomial Naive Bayes
library(Matrix)
M_sparse &lt;- Matrix(M, sparse = TRUE)
class(M_sparse) # dgCMatrix

# Fit the model with sparse data
mnb_sparse &lt;- multinomial_naive_bayes(M_sparse, y, laplace = laplace)

# Classification
head(predict(mnb_sparse, newdata = M_sparse, type = "class"))

# Posterior probabilities
head(predict(mnb_sparse, newdata = M_sparse, type = "prob"))

# Parameter estimates
coef(mnb_sparse)
</code></pre>

<hr>
<h2 id='naive_bayes'>Naive Bayes Classifier</h2><span id='topic+naive_bayes'></span><span id='topic+naive_bayes.default'></span><span id='topic+naive_bayes.formula'></span>

<h3>Description</h3>

<p><code>naive_bayes</code> is used to fit Naive Bayes model in which predictors are assumed to be independent within each class label.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
naive_bayes(x, y, prior = NULL, laplace = 0,
  usekernel = FALSE, usepoisson = FALSE, ...)

## S3 method for class 'formula'
naive_bayes(formula, data, prior = NULL, laplace = 0,
  usekernel = FALSE, usepoisson = FALSE,
  subset, na.action = stats::na.pass, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="naive_bayes_+3A_x">x</code></td>
<td>
<p>matrix or dataframe with categorical (character/factor/logical) or metric (numeric) predictors.</p>
</td></tr>
<tr><td><code id="naive_bayes_+3A_y">y</code></td>
<td>
<p>class vector (character/factor/logical).</p>
</td></tr>
<tr><td><code id="naive_bayes_+3A_formula">formula</code></td>
<td>
<p>an object of class <code>"formula"</code> (or one that can be coerced to &quot;formula&quot;) of the form: <code>class ~ predictors</code> (class has to be a factor/character/logical).</p>
</td></tr>
<tr><td><code id="naive_bayes_+3A_data">data</code></td>
<td>
<p>matrix or dataframe with categorical (character/factor/logical) or metric (numeric) predictors.</p>
</td></tr>
<tr><td><code id="naive_bayes_+3A_prior">prior</code></td>
<td>
<p>vector with prior probabilities of the classes. If unspecified, the class proportions for the training set are used. If present, the probabilities should be specified in the order of the factor levels.</p>
</td></tr>
<tr><td><code id="naive_bayes_+3A_laplace">laplace</code></td>
<td>
<p>value used for Laplace smoothing (additive smoothing). Defaults to 0 (no Laplace smoothing).</p>
</td></tr>
<tr><td><code id="naive_bayes_+3A_usekernel">usekernel</code></td>
<td>
<p>logical; if <code>TRUE</code>, <code><a href="stats.html#topic+density">density</a></code> is used to estimate the class conditional densities of metric predictors. This applies to vectors with class &quot;numeric&quot;. For further details on interaction between <code>usekernel</code> and <code>usepoisson</code> parameters please see Note below.</p>
</td></tr>
<tr><td><code id="naive_bayes_+3A_usepoisson">usepoisson</code></td>
<td>
<p>logical; if <code>TRUE</code>, Poisson distribution is used to estimate the class conditional PMFs of integer predictors (vectors with class &quot;integer&quot;).</p>
</td></tr>
<tr><td><code id="naive_bayes_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be used in the fitting process.</p>
</td></tr>
<tr><td><code id="naive_bayes_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when the data contain <code>NAs</code>. By default (<code><a href="stats.html#topic+na.pass">na.pass</a></code>), missing values are not removed from the data and are then omited while constructing tables. Alternatively, <code><a href="stats.html#topic+na.omit">na.omit</a></code> can be used to exclude rows with at least one missing value before constructing tables.</p>
</td></tr>
<tr><td><code id="naive_bayes_+3A_...">...</code></td>
<td>
<p>other parameters to <code><a href="stats.html#topic+density">density</a></code> when <code>usekernel = TRUE</code> (<code>na.rm</code> defaults to <code>TRUE</code>) (for instance <code>adjust</code>, <code>kernel</code> or <code>bw</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Numeric (metric) predictors are handled by assuming that they follow Gaussian distribution, given the class label. Alternatively, kernel density estimation can be used (<code>usekernel = TRUE</code>) to estimate their class-conditional distributions. Also, non-negative integer predictors (variables representing counts) can be modelled with Poisson distribution (<code>usepoisson = TRUE</code>); for further details please see <code>Note</code> below. Missing values are not included into constructing tables. Logical variables are treated as categorical (binary) variables.
</p>


<h3>Value</h3>

<p><code>naive_bayes</code> returns an object of class <code>"naive_bayes"</code> which is a list with following components:
</p>
<table>
<tr><td><code>data</code></td>
<td>
<p>list with two components: <code>x</code> (dataframe with predictors) and <code>y</code> (class variable).</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>character vector with values of the class variable.</p>
</td></tr>
<tr><td><code>laplace</code></td>
<td>
<p>amount of Laplace smoothing (additive smoothing).</p>
</td></tr>
<tr><td><code>tables</code></td>
<td>
<p>list of tables. For each categorical predictor a table with class-conditional probabilities, for each integer predictor a table with Poisson mean (if <code>usepoisson = TRUE</code>) and for each metric predictor a table with a mean and standard deviation or <code><a href="stats.html#topic+density">density</a></code> objects for each class. The object <code>tables</code> contains also an additional attribute &quot;cond_dist&quot; - a character vector with the names of conditional distributions assigned to each feature.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>numeric vector with prior probabilities.</p>
</td></tr>
<tr><td><code>usekernel</code></td>
<td>
<p>logical; <code>TRUE</code>, if the kernel density estimation was used for estimating class conditional densities of numeric variables.</p>
</td></tr>
<tr><td><code>usepoisson</code></td>
<td>
<p>logical; <code>TRUE</code>, if the Poisson distribution was used for estimating class conditional PMFs of non-negative integer variables.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call that produced this object.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The class &quot;numeric&quot; contains &quot;double&quot; (double precision floating point numbers) and &quot;integer&quot;. Depending on the parameters <code>usekernel</code> and <code>usepoisson</code> different class conditional distributions are applied to columns in the dataset with the class &quot;numeric&quot;:
</p>

<ul>
<li><p> If <code>usekernel=FALSE</code> and <code>poisson=FALSE</code> then Gaussian distribution is applied to each &quot;numeric&quot; variable (&quot;numeric&quot;&amp;&quot;integer&quot; or &quot;numeric&quot;&amp;&quot;double&quot;)
</p>
</li>
<li><p> If <code>usekernel=TRUE</code> and <code>poisson=FALSE</code> then kernel density estimation (KDE) is applied to each &quot;numeric&quot; variable (&quot;numeric&quot;&amp;&quot;integer&quot; or &quot;numeric&quot;&amp;&quot;double&quot;)
</p>
</li>
<li><p> If <code>usekernel=FALSE</code> and <code>poisson=TRUE</code> then Gaussian distribution is applied to each &quot;double&quot; vector and Poisson to each &quot;integer&quot; vector. (Gaussian: &quot;numeric&quot; &amp; &quot;double&quot;; Poisson: &quot;numeric&quot; &amp; &quot;integer&quot;)
</p>
</li>
<li><p> If <code>usekernel=TRUE</code> and <code>poisson=TRUE</code> then kernel density estimation (KDE) is applied to each &quot;double&quot; vector and Poisson to each &quot;integer&quot; vector. (KDE: &quot;numeric&quot; &amp; &quot;double&quot;; Poisson: &quot;numeric&quot; &amp; &quot;integer&quot;)
</p>
</li></ul>

<p>By default <code>usekernel=FALSE</code> and <code>poisson=FALSE</code>, thus Gaussian is applied to each numeric variable.
</p>
<p>On the other hand, &quot;character&quot;, &quot;factor&quot; and &quot;logical&quot; variables are assigned to the Categorical distribution with Bernoulli being its special case.
</p>
<p>Prior the model fitting the classes of columns in the data.frame &quot;data&quot; can be easily checked via:
</p>

<ul>
<li> <p><code>sapply(data, class)</code>
</p>
</li>
<li> <p><code>sapply(data, is.numeric)</code>
</p>
</li>
<li> <p><code>sapply(data, is.double)</code>
</p>
</li>
<li> <p><code>sapply(data, is.integer)</code>
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.naive_bayes">predict.naive_bayes</a></code>, <code><a href="#topic+plot.naive_bayes">plot.naive_bayes</a></code>, <code><a href="#topic+tables">tables</a></code>, <code><a href="#topic+get_cond_dist">get_cond_dist</a></code>, <code><a href="#topic++25class+25">%class%</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Simulate example data
n &lt;- 100
set.seed(1)
data &lt;- data.frame(class = sample(c("classA", "classB"), n, TRUE),
                   bern = sample(LETTERS[1:2], n, TRUE),
                   cat  = sample(letters[1:3], n, TRUE),
                   logical = sample(c(TRUE,FALSE), n, TRUE),
                   norm = rnorm(n),
                   count = rpois(n, lambda = c(5,15)))
train &lt;- data[1:95, ]
test &lt;- data[96:100, -1]


### 1) General usage via formula interface
nb &lt;- naive_bayes(class ~ ., train)
summary(nb)

# Classification
predict(nb, test, type = "class")
nb %class% test

# Posterior probabilities
predict(nb, test, type = "prob")
nb %prob% test

# Helper functions
tables(nb, 1)
get_cond_dist(nb)

# Note: all "numeric" (integer, double) variables are modelled
#       with Gaussian distribution by default.


### 2) General usage via matrix/data.frame and class vector
X &lt;- train[-1]
class &lt;- train$class
nb2 &lt;- naive_bayes(x = X, y = class)
nb2 %prob% test


### 3) Model continuous variables non-parametrically
###    via kernel density estimation (KDE)
nb_kde &lt;- naive_bayes(class ~ ., train, usekernel = TRUE)
summary(nb_kde)
get_cond_dist(nb_kde)

nb_kde %prob% test

# Visualize class conditional densities
plot(nb_kde, "norm", arg.num = list(legend.cex = 0.9), prob = "conditional")
plot(nb_kde, "count", arg.num = list(legend.cex = 0.9), prob = "conditional")

### ?density and ?bw.nrd for further documentation

# 3.1) Change Gaussian kernel to biweight kernel
nb_kde_biweight &lt;- naive_bayes(class ~ ., train, usekernel = TRUE,
                               kernel = "biweight")
nb_kde_biweight %prob% test
plot(nb_kde_biweight, c("norm", "count"),
     arg.num = list(legend.cex = 0.9), prob = "conditional")


# 3.2) Change "nrd0" (Silverman's rule of thumb) bandwidth selector
nb_kde_SJ &lt;- naive_bayes(class ~ ., train, usekernel = TRUE,
                               bw = "SJ")
nb_kde_SJ %prob% test
plot(nb_kde_SJ, c("norm", "count"),
     arg.num = list(legend.cex = 0.9), prob = "conditional")


# 3.3) Adjust bandwidth
nb_kde_adjust &lt;- naive_bayes(class ~ ., train, usekernel = TRUE,
                         adjust = 1.5)
nb_kde_adjust %prob% test
plot(nb_kde_adjust, c("norm", "count"),
     arg.num = list(legend.cex = 0.9), prob = "conditional")


### 4) Model non-negative integers with Poisson distribution
nb_pois &lt;- naive_bayes(class ~ ., train, usekernel = TRUE, usepoisson = TRUE)
summary(nb_pois)
get_cond_dist(nb_pois)

# Posterior probabilities
nb_pois %prob% test

# Class conditional distributions
plot(nb_pois, "count", prob = "conditional")

# Marginal distributions
plot(nb_pois, "count", prob = "marginal")


## Not run: 
vars &lt;- 10
rows &lt;- 1000000
y &lt;- sample(c("a", "b"), rows, TRUE)

# Only categorical variables
X1 &lt;- as.data.frame(matrix(sample(letters[5:9], vars * rows, TRUE),
                           ncol = vars))
nb_cat &lt;- naive_bayes(x = X1, y = y)
nb_cat
system.time(pred2 &lt;- predict(nb_cat, X1))

## End(Not run)
</code></pre>

<hr>
<h2 id='nonparametric_naive_bayes'>Non-Parametric Naive Bayes Classifier</h2><span id='topic+nonparametric_naive_bayes'></span>

<h3>Description</h3>

<p><code>nonparametric_naive_bayes</code> is used to fit the Non-Parametric Naive Bayes model in which all class conditional distributions are non-parametrically estimated using kernel density estimator and are assumed to be independent.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nonparametric_naive_bayes(x, y, prior = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nonparametric_naive_bayes_+3A_x">x</code></td>
<td>
<p>matrix with metric predictors (only numeric matrix accepted).</p>
</td></tr>
<tr><td><code id="nonparametric_naive_bayes_+3A_y">y</code></td>
<td>
<p>class vector (character/factor/logical).</p>
</td></tr>
<tr><td><code id="nonparametric_naive_bayes_+3A_prior">prior</code></td>
<td>
<p>vector with prior probabilities of the classes. If unspecified, the class proportions for the training set are used. If present, the probabilities should be specified in the order of the factor levels.</p>
</td></tr>
<tr><td><code id="nonparametric_naive_bayes_+3A_...">...</code></td>
<td>
<p>other parameters to <code><a href="stats.html#topic+density">density</a></code> (for instance <code>adjust</code>, <code>kernel</code> or <code>bw</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a specialized version of the Naive Bayes classifier, in which all features take on real values (numeric/integer) and class conditional probabilities are estimated in a non-parametric way with the kernel density estimator (KDE). By default Gaussian kernel is used and the smoothing bandwidth is selected according to the Silverman's 'rule of thumb'. For more details, please see the references and the documentation of <code><a href="stats.html#topic+density">density</a></code> and <code><a href="stats.html#topic+bw.nrd0">bw.nrd0</a></code>.
</p>
<p>The Non-Parametric Naive Bayes is available in both, <code>naive_bayes()</code> and <code>nonparametric_naive_bayes()</code>. The latter does not provide a substantial speed up over the general <code>naive_bayes()</code> function but it is meant to be more transparent and user friendly.
</p>
<p>The <code>nonparametric_naive_bayes</code> and <code>naive_bayes()</code> are equivalent when the latter is used with <code>usekernel = TRUE</code> and <code>usepoisson = FALSE</code>; and a matrix/data.frame contains only numeric variables.
</p>
<p>The missing values (NAs) are omitted during the estimation process. Also, the corresponding predict function excludes all NAs from the calculation of posterior probabilities  (an informative warning is always given).
</p>


<h3>Value</h3>

<p><code>nonparametric_naive_bayes</code> returns an object of class <code>"nonparametric_naive_bayes"</code> which is a list with following components:
</p>
<table>
<tr><td><code>data</code></td>
<td>
<p>list with two components: <code>x</code> (matrix with predictors) and <code>y</code> (class variable).</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>character vector with values of the class variable.</p>
</td></tr>
<tr><td><code>dens</code></td>
<td>
<p>nested list containing <code><a href="stats.html#topic+density">density</a></code> objects for each feature and class.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>numeric vector with prior probabilities.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call that produced this object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>References</h3>

<p>Silverman, B. W. (1986). Density Estimation for Statistics and Data Analysis. Chapman &amp; Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.nonparametric_naive_bayes">predict.nonparametric_naive_bayes</a></code>, <code><a href="#topic+plot.nonparametric_naive_bayes">plot.nonparametric_naive_bayes</a></code>, <code><a href="#topic+tables">tables</a></code>, <code><a href="#topic+get_cond_dist">get_cond_dist</a></code>, <code><a href="#topic++25class+25">%class%</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># library(naivebayes)
data(iris)
y &lt;- iris[[5]]
M &lt;- as.matrix(iris[-5])

### Train the Non-Parametric Naive Bayes
nnb &lt;- nonparametric_naive_bayes(x = M, y = y)
summary(nnb)
head(predict(nnb, newdata = M, type = "prob"))

###  Equivalent calculation with general naive_bayes function:
nb &lt;- naive_bayes(M, y, usekernel = TRUE)
summary(nb)
head(predict(nb, type = "prob"))

### Change kernel
nnb_kernel &lt;- nonparametric_naive_bayes(x = M, y = y, kernel = "biweight")
plot(nnb_kernel, 1, prob = "conditional")

### Adjust bandwidth
nnb_adjust &lt;- nonparametric_naive_bayes(M, y, adjust = 1.5)
plot(nnb_adjust, 1, prob = "conditional")

### Change bandwidth selector
nnb_bw &lt;- nonparametric_naive_bayes(M, y, bw = "SJ")
plot(nnb_bw, 1, prob = "conditional")

### Obtain tables with conditional densities
# tables(nnb, which = 1)
</code></pre>

<hr>
<h2 id='plot.bernoulli_naive_bayes'>Plot Method for bernoulli_naive_bayes Objects</h2><span id='topic+plot.bernoulli_naive_bayes'></span>

<h3>Description</h3>

<p>Plot method for objects of class <code>"bernoulli_naive_bayes"</code> designed for a quick look at the class marginal distributions or class conditional distributions of 0-1 valued predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bernoulli_naive_bayes'
plot(x, which = NULL, ask = FALSE, arg.cat = list(),
     prob = c("marginal", "conditional"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.bernoulli_naive_bayes_+3A_x">x</code></td>
<td>
<p>object of class inheriting from <code>"bernoulli_naive_bayes"</code>.</p>
</td></tr>
<tr><td><code id="plot.bernoulli_naive_bayes_+3A_which">which</code></td>
<td>
<p>variables to be plotted (all by default). This can be any valid indexing vector or vector containing names of variables.</p>
</td></tr>
<tr><td><code id="plot.bernoulli_naive_bayes_+3A_ask">ask</code></td>
<td>
<p>logical; if <code>TRUE</code>, the user is asked before each plot, see <code><a href="graphics.html#topic+par">par</a>(ask=.)</code>.</p>
</td></tr>
<tr><td><code id="plot.bernoulli_naive_bayes_+3A_arg.cat">arg.cat</code></td>
<td>
<p>other parameters to be passed as a named list to <code><a href="graphics.html#topic+mosaicplot">mosaicplot</a></code>.</p>
</td></tr>
<tr><td><code id="plot.bernoulli_naive_bayes_+3A_prob">prob</code></td>
<td>
<p>character; if &quot;marginal&quot; then marginal distributions of predictor variables for each class are visualised and if &quot;conditional&quot; then the class conditional distributions of predictor variables are depicted. By default, prob=&quot;marginal&quot;.</p>
</td></tr>
<tr><td><code id="plot.bernoulli_naive_bayes_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Class conditional or class conditional distributions are visualised by <code><a href="graphics.html#topic+mosaicplot">mosaicplot</a></code>.
</p>
<p>The parameter <code>prob</code> controls the kind of probabilities to be visualized for each individual predictor <code class="reqn">Xi</code>. It can take on two values:
</p>

<ul>
<li><p> &quot;marginal&quot;: <code class="reqn">P(Xi|class) * P(class)</code>
</p>
</li>
<li><p> &quot;conditional&quot;: <code class="reqn">P(Xi|class)</code>
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+naive_bayes">naive_bayes</a></code>,<code><a href="#topic+bernoulli_naive_bayes">bernoulli_naive_bayes</a></code> <code><a href="#topic+predict.bernoulli_naive_bayes">predict.bernoulli_naive_bayes</a></code>, <code><a href="#topic+tables">tables</a></code>, <code><a href="#topic+get_cond_dist">get_cond_dist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulate data
cols &lt;- 10 ; rows &lt;- 100 ; probs &lt;- c("0" = 0.4, "1" = 0.1)
M &lt;- matrix(sample(0:1, rows * cols,  TRUE, probs), nrow = rows, ncol = cols)
y &lt;- factor(sample(paste0("class", LETTERS[1:2]), rows, TRUE, prob = c(0.3,0.7)))
colnames(M) &lt;- paste0("V", seq_len(ncol(M)))
laplace &lt;- 0.5

# Train the Bernoulli Naive Bayes model
bnb &lt;- bernoulli_naive_bayes(x = M, y = y, laplace = laplace)

# Visualize class marginal probabilities corresponding to the first feature
plot(bnb, which = 1)

# Visualize class conditional probabilities corresponding to the first feature
plot(bnb, which = 1, prob = "conditional")

</code></pre>

<hr>
<h2 id='plot.gaussian_naive_bayes'>Plot Method for gaussian_naive_bayes Objects</h2><span id='topic+plot.gaussian_naive_bayes'></span>

<h3>Description</h3>

<p>Plot method for objects of class <code>"gaussian_naive_bayes"</code> designed for a quick look at the class marginal or conditional Gaussian distributions of metric predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gaussian_naive_bayes'
plot(x, which = NULL, ask = FALSE, legend = TRUE,
  legend.box = FALSE, arg.num = list(),
  prob = c("marginal", "conditional"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.gaussian_naive_bayes_+3A_x">x</code></td>
<td>
<p>object of class inheriting from <code>"gaussian_naive_bayes"</code>.</p>
</td></tr>
<tr><td><code id="plot.gaussian_naive_bayes_+3A_which">which</code></td>
<td>
<p>variables to be plotted (all by default). This can be any valid indexing vector or vector containing names of variables.</p>
</td></tr>
<tr><td><code id="plot.gaussian_naive_bayes_+3A_ask">ask</code></td>
<td>
<p>logical; if <code>TRUE</code>, the user is asked before each plot, see <code><a href="graphics.html#topic+par">par</a>(ask=.)</code>.</p>
</td></tr>
<tr><td><code id="plot.gaussian_naive_bayes_+3A_legend">legend</code></td>
<td>
<p>logical; if <code>TRUE</code> a <code><a href="graphics.html#topic+legend">legend</a></code> will be be plotted.</p>
</td></tr>
<tr><td><code id="plot.gaussian_naive_bayes_+3A_legend.box">legend.box</code></td>
<td>
<p>logical; if <code>TRUE</code> a box will be drawn around the legend.</p>
</td></tr>
<tr><td><code id="plot.gaussian_naive_bayes_+3A_arg.num">arg.num</code></td>
<td>
<p>other parameters to be passed as a named list to <code><a href="graphics.html#topic+matplot">matplot</a></code>.</p>
</td></tr>
<tr><td><code id="plot.gaussian_naive_bayes_+3A_prob">prob</code></td>
<td>
<p>character; if &quot;marginal&quot; then marginal distributions of predictor variables for each class are visualised and if &quot;conditional&quot; then the class conditional distributions of predictor variables are depicted. By default, prob=&quot;marginal&quot;.</p>
</td></tr>
<tr><td><code id="plot.gaussian_naive_bayes_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Class marginal and class conditional Gaussian distributions are visualised by <code><a href="graphics.html#topic+matplot">matplot</a></code>.
</p>
<p>The parameter <code>prob</code> controls the kind of probabilities to be visualized for each individual predictor <code class="reqn">Xi</code>. It can take on two values:
</p>

<ul>
<li><p> &quot;marginal&quot;: <code class="reqn">P(Xi|class) * P(class)</code>
</p>
</li>
<li><p> &quot;conditional&quot;: <code class="reqn">P(Xi|class)</code>
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+naive_bayes">naive_bayes</a></code>, <code><a href="#topic+gaussian_naive_bayes">gaussian_naive_bayes</a></code>, <code><a href="#topic+predict.gaussian_naive_bayes">predict.gaussian_naive_bayes</a></code>, <code><a href="#topic+tables">tables</a></code>, <code><a href="#topic+get_cond_dist">get_cond_dist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
y &lt;- iris[[5]]
M &lt;- as.matrix(iris[-5])

### Train the Gaussian Naive Bayes with custom prior
gnb &lt;- gaussian_naive_bayes(x = M, y = y, prior = c(0.1,0.3,0.6))

# Visualize class marginal Gaussian distributions corresponding
# to the first feature
plot(gnb, which = 1)

# Visualize class conditional Gaussian distributions corresponding
# to the first feature
plot(gnb, which = 1, prob = "conditional")
</code></pre>

<hr>
<h2 id='plot.naive_bayes'>Plot Method for naive_bayes Objects</h2><span id='topic+plot.naive_bayes'></span>

<h3>Description</h3>

<p>Plot method for objects of class <code>"naive_bayes"</code> designed for a quick look at the class marginal distributions or class conditional distributions of predictor variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'naive_bayes'
plot(x, which = NULL, ask = FALSE, legend = TRUE,
  legend.box = FALSE, arg.num = list(), arg.cat = list(),
  prob = c("marginal", "conditional"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.naive_bayes_+3A_x">x</code></td>
<td>
<p>object of class inheriting from <code>"naive_bayes"</code>.</p>
</td></tr>
<tr><td><code id="plot.naive_bayes_+3A_which">which</code></td>
<td>
<p>variables to be plotted (all by default). This can be any valid indexing vector or vector containing names of variables.</p>
</td></tr>
<tr><td><code id="plot.naive_bayes_+3A_ask">ask</code></td>
<td>
<p>logical; if <code>TRUE</code>, the user is asked before each plot, see <code><a href="graphics.html#topic+par">par</a>(ask=.)</code>.</p>
</td></tr>
<tr><td><code id="plot.naive_bayes_+3A_legend">legend</code></td>
<td>
<p>logical; if <code>TRUE</code> a <code><a href="graphics.html#topic+legend">legend</a></code> will be be plotted.</p>
</td></tr>
<tr><td><code id="plot.naive_bayes_+3A_legend.box">legend.box</code></td>
<td>
<p>logical; if <code>TRUE</code> a box will be drawn around the legend.</p>
</td></tr>
<tr><td><code id="plot.naive_bayes_+3A_arg.num">arg.num</code></td>
<td>
<p>other parameters to be passed as a named list to <code><a href="graphics.html#topic+matplot">matplot</a></code>.</p>
</td></tr>
<tr><td><code id="plot.naive_bayes_+3A_arg.cat">arg.cat</code></td>
<td>
<p>other parameters to be passed as a named list to <code><a href="graphics.html#topic+mosaicplot">mosaicplot</a></code>.</p>
</td></tr>
<tr><td><code id="plot.naive_bayes_+3A_prob">prob</code></td>
<td>
<p>character; if &quot;marginal&quot; then marginal distributions of predictor variables for each class are visualised and if &quot;conditional&quot; then the class conditional distributions of predictor variables are depicted. By default, prob=&quot;marginal&quot;.</p>
</td></tr>
<tr><td><code id="plot.naive_bayes_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Probabilities are visualised by <code><a href="graphics.html#topic+matplot">matplot</a></code> (for numeric (metric) predictors) and  <code><a href="graphics.html#topic+mosaicplot">mosaicplot</a></code> (for categorical predictors). In case of non parametric estimation of densities, the bandwidths are reported for each class. Nothing is returned. For numeric (metric) predictors position of the legend can be adjusted changed via <code>arg.num(..., legend.position = "topright")</code>. <code>legend.position</code> can be one of <code>"topright" "topleft", "bottomright", "bottomleft"</code>. In order to adjust the legend size following argument can be used: <code>arg.num(..., legend.cex = 0.9)</code>.
</p>
<p>The parameter <code>prob</code> controls the kind of probabilities to be visualized for each individual predictor <code class="reqn">Xi</code>. It can take on two values:
</p>

<ul>
<li><p> &quot;marginal&quot;: <code class="reqn">P(Xi|class) * P(class)</code>
</p>
</li>
<li><p> &quot;conditional&quot;: <code class="reqn">P(Xi|class)</code>
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+naive_bayes">naive_bayes</a></code>, <code><a href="#topic+predict.naive_bayes">predict.naive_bayes</a></code>, <code><a href="#topic++25class+25">%class%</a></code>, <code><a href="#topic+tables">tables</a></code>, <code><a href="#topic+get_cond_dist">get_cond_dist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
iris2 &lt;- cbind(iris, New = sample(letters[1:3], 150, TRUE))

# Fit the model with custom prior probabilities
nb &lt;- naive_bayes(Species ~ ., data = iris2, prior = c(0.1, 0.3, 0.6))

# Visualize marginal distributions of two predictors
plot(nb, which = c("Sepal.Width", "Sepal.Length"), ask = TRUE)

# Visualize class conditional distributions corresponding to the first predictor
# with customized settings
plot(nb, which = 1, ask = FALSE, prob = "conditional",
     arg.num = list(col = 1:3, lty = 1,
     main = "Naive Bayes Plot", legend.position = "topright",
     legend.cex = 0.55))

# Visualize class marginal distributions corresponding to the first predictor
# with customized settings
plot(nb, which = 1, ask = FALSE, prob = "marginal",
     arg.num = list(col = 1:3, lty = 1,
     main = "Naive Bayes Plot", legend.position = "topright",
     legend.cex = 0.55))

# Visualize class marginal distribution corresponding to the predictor "new"
# with custom colours
plot(nb, which = "New", arg.cat = list(color = gray.colors(3)))
</code></pre>

<hr>
<h2 id='plot.nonparametric_naive_bayes'>Plot Method for nonparametric_naive_bayes Objects</h2><span id='topic+plot.nonparametric_naive_bayes'></span>

<h3>Description</h3>

<p>Plot method for objects of class <code>"nonparametric_naive_bayes"</code> designed for a quick look at the estimated class marginal or class conditional densities of metric predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nonparametric_naive_bayes'
plot(x, which = NULL, ask = FALSE, legend = TRUE,
  legend.box = FALSE, arg.num = list(),
  prob = c("marginal", "conditional"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.nonparametric_naive_bayes_+3A_x">x</code></td>
<td>
<p>object of class inheriting from <code>"nonparametric_naive_bayes"</code>.</p>
</td></tr>
<tr><td><code id="plot.nonparametric_naive_bayes_+3A_which">which</code></td>
<td>
<p>variables to be plotted (all by default). This can be any valid indexing vector or vector containing names of variables.</p>
</td></tr>
<tr><td><code id="plot.nonparametric_naive_bayes_+3A_ask">ask</code></td>
<td>
<p>logical; if <code>TRUE</code>, the user is asked before each plot, see <code><a href="graphics.html#topic+par">par</a>(ask=.)</code>.</p>
</td></tr>
<tr><td><code id="plot.nonparametric_naive_bayes_+3A_legend">legend</code></td>
<td>
<p>logical; if <code>TRUE</code> a <code><a href="graphics.html#topic+legend">legend</a></code> will be be plotted.</p>
</td></tr>
<tr><td><code id="plot.nonparametric_naive_bayes_+3A_legend.box">legend.box</code></td>
<td>
<p>logical; if <code>TRUE</code> a box will be drawn around the legend.</p>
</td></tr>
<tr><td><code id="plot.nonparametric_naive_bayes_+3A_arg.num">arg.num</code></td>
<td>
<p>other parameters to be passed as a named list to <code><a href="graphics.html#topic+matplot">matplot</a></code>.</p>
</td></tr>
<tr><td><code id="plot.nonparametric_naive_bayes_+3A_prob">prob</code></td>
<td>
<p>character; if &quot;marginal&quot; then marginal distributions of predictor variables for each class are visualised and if &quot;conditional&quot; then the class conditional distributions of predictor variables are depicted. By default, prob=&quot;marginal&quot;.</p>
</td></tr>
<tr><td><code id="plot.nonparametric_naive_bayes_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimated class marginal or class conditional densities are visualised by <code><a href="graphics.html#topic+matplot">matplot</a></code>.
</p>
<p>The parameter <code>prob</code> controls the kind of probabilities to be visualized for each individual predictor <code class="reqn">Xi</code>. It can take on two values:
</p>

<ul>
<li><p> &quot;marginal&quot;: <code class="reqn">P(Xi|class) * P(class)</code>
</p>
</li>
<li><p> &quot;conditional&quot;: <code class="reqn">P(Xi|class)</code>
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+naive_bayes">naive_bayes</a></code>, <code><a href="#topic+nonparametric_naive_bayes">nonparametric_naive_bayes</a></code> <code><a href="#topic+predict.nonparametric_naive_bayes">predict.nonparametric_naive_bayes</a></code>, <code><a href="#topic+tables">tables</a></code>, <code><a href="#topic+get_cond_dist">get_cond_dist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
y &lt;- iris[[5]]
M &lt;- as.matrix(iris[-5])

### Train the Non-Parametric Naive Bayes with custom prior
prior &lt;- c(0.1,0.3,0.6)
nnb &lt;- nonparametric_naive_bayes(x = M, y = y, prior = prior)
nnb2 &lt;- nonparametric_naive_bayes(x = M, y = y, prior = prior, adjust = 1.5)
nnb3 &lt;- nonparametric_naive_bayes(x = M, y = y, prior = prior, bw = "ucv")

# Visualize estimated class conditional densities corresponding
# to the first feature
plot(nnb, which = 1, prob = "conditional")
plot(nnb2, which = 1, prob = "cond")
plot(nnb3, which = 1, prob = "c")

# Visualize estimated class marginal densities corresponding
# to the first feature
plot(nnb, which = 1)
plot(nnb2, which = 1)
plot(nnb3, which = 1)
</code></pre>

<hr>
<h2 id='plot.poisson_naive_bayes'>Plot Method for poisson_naive_bayes Objects</h2><span id='topic+plot.poisson_naive_bayes'></span>

<h3>Description</h3>

<p>Plot method for objects of class <code>"poisson_naive_bayes"</code> designed for a quick look at the class marginal or class conditional Poisson distributions of non-negative integer predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'poisson_naive_bayes'
plot(x, which = NULL, ask = FALSE, legend = TRUE,
  legend.box = FALSE, arg.num = list(),
  prob = c("marginal", "conditional"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.poisson_naive_bayes_+3A_x">x</code></td>
<td>
<p>object of class inheriting from <code>"poisson_naive_bayes"</code>.</p>
</td></tr>
<tr><td><code id="plot.poisson_naive_bayes_+3A_which">which</code></td>
<td>
<p>variables to be plotted (all by default). This can be any valid indexing vector or vector containing names of variables.</p>
</td></tr>
<tr><td><code id="plot.poisson_naive_bayes_+3A_ask">ask</code></td>
<td>
<p>logical; if <code>TRUE</code>, the user is asked before each plot, see <code><a href="graphics.html#topic+par">par</a>(ask=.)</code>.</p>
</td></tr>
<tr><td><code id="plot.poisson_naive_bayes_+3A_legend">legend</code></td>
<td>
<p>logical; if <code>TRUE</code> a <code><a href="graphics.html#topic+legend">legend</a></code> will be be plotted.</p>
</td></tr>
<tr><td><code id="plot.poisson_naive_bayes_+3A_legend.box">legend.box</code></td>
<td>
<p>logical; if <code>TRUE</code> a box will be drawn around the legend.</p>
</td></tr>
<tr><td><code id="plot.poisson_naive_bayes_+3A_arg.num">arg.num</code></td>
<td>
<p>other parameters to be passed as a named list to <code><a href="graphics.html#topic+matplot">matplot</a></code>.</p>
</td></tr>
<tr><td><code id="plot.poisson_naive_bayes_+3A_prob">prob</code></td>
<td>
<p>character; if &quot;marginal&quot; then marginal distributions of predictor variables for each class are visualised and if &quot;conditional&quot; then the class conditional distributions of predictor variables are depicted. By default, prob=&quot;marginal&quot;.</p>
</td></tr>
<tr><td><code id="plot.poisson_naive_bayes_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Class marginal or class conditional Poisson distributions are visualised by <code><a href="graphics.html#topic+matplot">matplot</a></code>.
</p>
<p>The parameter <code>prob</code> controls the kind of probabilities to be visualized for each individual predictor <code class="reqn">Xi</code>. It can take on two values:
</p>

<ul>
<li><p> &quot;marginal&quot;: <code class="reqn">P(Xi|class) * P(class)</code>
</p>
</li>
<li><p> &quot;conditional&quot;: <code class="reqn">P(Xi|class)</code>
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+naive_bayes">naive_bayes</a></code>, <code><a href="#topic+poisson_naive_bayes">poisson_naive_bayes</a></code>, <code><a href="#topic+predict.poisson_naive_bayes">predict.poisson_naive_bayes</a></code>, <code><a href="#topic+tables">tables</a></code>, <code><a href="#topic+get_cond_dist">get_cond_dist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cols &lt;- 10 ; rows &lt;- 100
M &lt;- matrix(rpois(rows * cols, lambda = 3), nrow = rows, ncol = cols)
# is.integer(M) # [1] TRUE
y &lt;- factor(sample(paste0("class", LETTERS[1:2]), rows, TRUE))
colnames(M) &lt;- paste0("V", seq_len(ncol(M)))
laplace &lt;- 0

### Train the Poisson Naive Bayes
pnb &lt;- poisson_naive_bayes(x = M, y = y, laplace = laplace)

# Visualize class conditional Poisson distributions corresponding
# to the first feature
plot(pnb, which = 1, prob = "conditional")

# Visualize class marginal Poisson distributions corresponding
# to the first feature
plot(pnb, which = 1)
</code></pre>

<hr>
<h2 id='poisson_naive_bayes'>Poisson Naive Bayes Classifier</h2><span id='topic+poisson_naive_bayes'></span>

<h3>Description</h3>

<p><code>poisson_naive_bayes</code> is used to fit the Poisson Naive Bayes model in which all class conditional distributions are assumed to be Poisson and be independent.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>poisson_naive_bayes(x, y, prior = NULL, laplace = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="poisson_naive_bayes_+3A_x">x</code></td>
<td>
<p>numeric matrix with integer predictors (matrix or dgCMatrix from Matrix package).</p>
</td></tr>
<tr><td><code id="poisson_naive_bayes_+3A_y">y</code></td>
<td>
<p>class vector (character/factor/logical).</p>
</td></tr>
<tr><td><code id="poisson_naive_bayes_+3A_prior">prior</code></td>
<td>
<p>vector with prior probabilities of the classes. If unspecified, the class proportions for the training set are used. If present, the probabilities should be specified in the order of the factor levels.</p>
</td></tr>
<tr><td><code id="poisson_naive_bayes_+3A_laplace">laplace</code></td>
<td>
<p>value used for Laplace smoothing (additive smoothing). Defaults to 0 (no Laplace smoothing).</p>
</td></tr>
<tr><td><code id="poisson_naive_bayes_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a specialized version of the Naive Bayes classifier, in which all features take on non-negative integers (numeric/integer) and class conditional probabilities are modelled with the Poisson distribution.
</p>
<p>The Poisson Naive Bayes is available in both, <code>naive_bayes</code> and <code>poisson_naive_bayes</code>. The latter provides more efficient performance though. Faster calculation times come from restricting the data to an integer-valued matrix and taking advantage of linear algebra operations. Sparse matrices of class &quot;dgCMatrix&quot; (Matrix package) are supported in order to furthermore speed up calculation times.
</p>
<p>The <code>poisson_naive_bayes</code> and <code>naive_bayes()</code> are equivalent when the latter is used with <code>usepoisson = TRUE</code> and <code>usekernel = FALSE</code>; and a matrix/data.frame contains only integer-valued columns.
</p>
<p>The missing values (NAs) are omited during the estimation process. Also, the corresponding predict function excludes all NAs from the calculation of posterior probabilities  (an informative warning is always given).
</p>


<h3>Value</h3>

<p><code>poisson_naive_bayes</code> returns an object of class <code>"poisson_naive_bayes"</code> which is a list with following components:
</p>
<table>
<tr><td><code>data</code></td>
<td>
<p>list with two components: <code>x</code> (matrix with predictors) and <code>y</code> (class variable).</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>character vector with values of the class variable.</p>
</td></tr>
<tr><td><code>laplace</code></td>
<td>
<p>amount of Laplace smoothing (additive smoothing).</p>
</td></tr>
<tr><td><code>params</code></td>
<td>
<p>matrix containing class conditional means.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>numeric vector with prior probabilities.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call that produced this object.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>When the parameter <code>laplace</code> is set to positive constant <code>c</code> then this amount is added to all counts. This leads to the (&quot;global&quot;) Bayesian estimation with an improper prior. In each case, the estimate is the expected value of the posterior which is given by the gamma distribution with parameters: <code>cell count + c</code> and number of observations in the cell.
</p>
<p>If in one cell there is a zero count and <code>laplace = 0</code> then one pseudo-count is automatically to each such cell. This corresponds to the &quot;local&quot; Bayesian estimation with uniform prior.
</p>


<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.poisson_naive_bayes">predict.poisson_naive_bayes</a></code>, <code><a href="#topic+plot.poisson_naive_bayes">plot.poisson_naive_bayes</a></code>, <code><a href="#topic+tables">tables</a></code>, <code><a href="#topic+get_cond_dist">get_cond_dist</a></code>, <code><a href="#topic++25class+25">%class%</a></code>, <code><a href="#topic+coef.poisson_naive_bayes">coef.poisson_naive_bayes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(naivebayes)

### Simulate the data:
set.seed(1)
cols &lt;- 10 ; rows &lt;- 100
M &lt;- matrix(rpois(rows * cols, lambda = 3), nrow = rows, ncol = cols)
y &lt;- factor(sample(paste0("class", LETTERS[1:2]), rows, TRUE, prob = c(0.3,0.7)))
colnames(M) &lt;- paste0("V", seq_len(ncol(M)))
laplace &lt;- 0.5


### Train the Poisson Naive Bayes
pnb &lt;- poisson_naive_bayes(x = M, y = y, laplace = laplace)
summary(pnb)

# Classification
head(predict(pnb, newdata = M, type = "class")) # head(pnb %class% M)

# Posterior probabilities
head(predict(pnb, newdata = M, type = "prob")) # head(pnb %prob% M)

# Parameter estimates
coef(pnb)


### Sparse data: train the Poisson Naive Bayes
library(Matrix)
M_sparse &lt;- Matrix(M, sparse = TRUE)
class(M_sparse) # dgCMatrix

# Fit the model with sparse data
pnb_sparse &lt;- poisson_naive_bayes(M_sparse, y, laplace = laplace)

# Classification
head(predict(pnb_sparse, newdata = M_sparse, type = "class"))

# Posterior probabilities
head(predict(pnb_sparse, newdata = M_sparse, type = "prob"))

# Parameter estimates
coef(pnb_sparse)


### Equivalent calculation with general naive_bayes function.
### (no sparse data support by naive_bayes)

nb &lt;- naive_bayes(M, y, laplace = laplace, usepoisson = TRUE)
summary(nb)
head(predict(nb, type = "prob"))

# Obtain probability tables
tables(nb, which = "V1")
tables(pnb, which = "V1")

# Visualise class conditional Poisson distributions
plot(nb, "V1", prob = "conditional")
plot(pnb, which = "V1", prob = "conditional")

# Check the equivalence of the class conditional distributions
all(get_cond_dist(nb) == get_cond_dist(pnb))
</code></pre>

<hr>
<h2 id='predict.bernoulli_naive_bayes'>Predict Method for bernoulli_naive_bayes Objects</h2><span id='topic+predict.bernoulli_naive_bayes'></span>

<h3>Description</h3>

<p>Classification based on the Bernoulli Naive Bayes model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bernoulli_naive_bayes'
predict(object, newdata = NULL, type = c("class","prob"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.bernoulli_naive_bayes_+3A_object">object</code></td>
<td>
<p>object of class inheriting from <code>"bernoulli_naive_bayes"</code>.</p>
</td></tr>
<tr><td><code id="predict.bernoulli_naive_bayes_+3A_newdata">newdata</code></td>
<td>
<p>matrix with numeric 0-1 predictors.</p>
</td></tr>
<tr><td><code id="predict.bernoulli_naive_bayes_+3A_type">type</code></td>
<td>
<p>if &quot;class&quot;, new data points are classified according to the highest posterior probabilities. If &quot;prob&quot;, the posterior probabilities for each class are returned.</p>
</td></tr>
<tr><td><code id="predict.bernoulli_naive_bayes_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a specialized version of the Naive Bayes classifier, in which all features take on numeric 0-1 values and class conditional probabilities are modelled with the Bernoulli distribution.
</p>
<p>Class posterior probabilities are calculated using the Bayes' rule under the assumption of independence of predictors. If no <code>newdata</code> is provided, the data from the object is used.
</p>
<p>The Bernoulli Naive Bayes is available in both, <code>naive_bayes</code> and <code>bernoulli_naive_bayes</code>. The implementation of the specialized Naive Bayes provides more efficient performance though. The speedup comes from the restricting the data input to a numeric 0-1 matrix and performing the linear algebra as well as vectorized operations on it. In other words, the efficiency comes at cost of the flexibility.
</p>
<p>The NAs in the newdata are not included into the calculation of posterior probabilities; and if present an informative warning is given.
</p>
<p>The <code>bernoulli_naive_bayes</code> function is equivalent to the <code>naive_bayes</code> function with the numeric 0-1 matrix being coerced, for instance,  to the &quot;0&quot;-&quot;1&quot; character matrix.
</p>


<h3>Value</h3>

<p><code>predict.bernoulli_naive_bayes</code> returns either a factor with class labels corresponding to the maximal conditional posterior probabilities or a matrix with class label specific conditional posterior probabilities.
</p>


<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+naive_bayes">naive_bayes</a></code>, <code><a href="#topic+bernoulli_naive_bayes">bernoulli_naive_bayes</a></code>, <code><a href="#topic+plot.bernoulli_naive_bayes">plot.bernoulli_naive_bayes</a></code>, <code><a href="#topic+tables">tables</a></code>, <code><a href="#topic+get_cond_dist">get_cond_dist</a></code>, <code><a href="#topic++25class+25">%class%</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cols &lt;- 10 ; rows &lt;- 100 ; probs &lt;- c("0" = 0.4, "1" = 0.1)
M &lt;- matrix(sample(0:1, rows * cols,  TRUE, probs), nrow = rows, ncol = cols)
y &lt;- factor(sample(paste0("class", LETTERS[1:2]), rows, TRUE, prob = c(0.3,0.7)))
colnames(M) &lt;- paste0("V", seq_len(ncol(M)))
laplace &lt;- 0.5

### Train the Bernoulli Naive Bayes
bnb &lt;- bernoulli_naive_bayes(x = M, y = y, laplace = laplace)

### Classification
head(predict(bnb, newdata = M, type = "class"))
head(bnb %class% M)

### Posterior probabilities
head(predict(bnb, newdata = M, type = "prob"))
head(bnb %prob% M)
</code></pre>

<hr>
<h2 id='predict.gaussian_naive_bayes'>Predict Method for gaussian_naive_bayes Objects</h2><span id='topic+predict.gaussian_naive_bayes'></span>

<h3>Description</h3>

<p>Classification based on the Gaussian Naive Bayes model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gaussian_naive_bayes'
predict(object, newdata = NULL, type = c("class","prob"),
  threshold = 0.001, eps = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.gaussian_naive_bayes_+3A_object">object</code></td>
<td>
<p>object of class inheriting from <code>"gaussian_naive_bayes"</code>.</p>
</td></tr>
<tr><td><code id="predict.gaussian_naive_bayes_+3A_newdata">newdata</code></td>
<td>
<p>matrix with metric predictors (only numeric matrix accepted).</p>
</td></tr>
<tr><td><code id="predict.gaussian_naive_bayes_+3A_type">type</code></td>
<td>
<p>if &quot;class&quot;, new data points are classified according to the highest posterior probabilities. If &quot;prob&quot;, the posterior probabilities for each class are returned.</p>
</td></tr>
<tr><td><code id="predict.gaussian_naive_bayes_+3A_threshold">threshold</code></td>
<td>
<p>value by which zero probabilities or probabilities within the epsilon-range corresponding to metric variables are replaced (zero probabilities corresponding to categorical variables can be handled with Laplace (additive) smoothing).</p>
</td></tr>
<tr><td><code id="predict.gaussian_naive_bayes_+3A_eps">eps</code></td>
<td>
<p>value that specifies an epsilon-range to replace zero or close to zero probabilities by <code>threshold</code>. It applies to metric variables.</p>
</td></tr>
<tr><td><code id="predict.gaussian_naive_bayes_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a specialized version of the Naive Bayes classifier, in which all features take on real values and class conditional probabilities are modelled with the Gaussian distribution.
</p>
<p>Class posterior probabilities are calculated using the Bayes' rule under the assumption of independence of predictors. If no <code>newdata</code> is provided, the data from the object is used.
</p>
<p>The Gaussian Naive Bayes is available in both, <code>naive_bayes</code> and <code>gaussian_naive_bayes</code>. The implementation of the specialized Naive Bayes provides more efficient performance though. The speedup comes from the restricting the data input to a numeric matrix and performing the linear algebra as well vectorized operations on it. In other words, the efficiency comes at cost of the flexibility.
</p>
<p>The NAs in the newdata are not included into the calculation of posterior probabilities; and if present an informative warning is given.
</p>
<p>The <code>gaussian_naive_bayes</code> function is equivalent to the <code>naive_bayes</code> function with the numeric matrix or a data.frame containing only numeric variables.
</p>


<h3>Value</h3>

<p><code>predict.gaussian_naive_bayes</code> returns either a factor with class labels corresponding to the maximal conditional posterior probabilities or a matrix with class label specific conditional posterior probabilities.
</p>


<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+naive_bayes">naive_bayes</a></code>, <code><a href="#topic+gaussian_naive_bayes">gaussian_naive_bayes</a></code>,  <code><a href="#topic+plot.gaussian_naive_bayes">plot.gaussian_naive_bayes</a></code>, <code><a href="#topic+tables">tables</a></code>, <code><a href="#topic+get_cond_dist">get_cond_dist</a></code>, <code><a href="#topic++25class+25">%class%</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
y &lt;- iris[[5]]
M &lt;- as.matrix(iris[-5])

### Train the Gaussian Naive Bayes
gnb &lt;- gaussian_naive_bayes(x = M, y = y)

### Classification
head(predict(gnb, newdata = M, type = "class"))
head(gnb %class% M)

### Posterior probabilities
head(predict(gnb, newdata = M, type = "prob"))
head(gnb %prob% M)
</code></pre>

<hr>
<h2 id='predict.multinomial_naive_bayes'>Predict Method for multinomial_naive_bayes Objects</h2><span id='topic+predict.multinomial_naive_bayes'></span>

<h3>Description</h3>

<p>Classification based on the Multinomial Naive Bayes model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'multinomial_naive_bayes'
predict(object, newdata = NULL, type = c("class","prob"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.multinomial_naive_bayes_+3A_object">object</code></td>
<td>
<p>object of class inheriting from <code>"multinomial_naive_bayes"</code>.</p>
</td></tr>
<tr><td><code id="predict.multinomial_naive_bayes_+3A_newdata">newdata</code></td>
<td>
<p>matrix with non-negative integer predictors (only numeric matrix is accepted).</p>
</td></tr>
<tr><td><code id="predict.multinomial_naive_bayes_+3A_type">type</code></td>
<td>
<p>if &quot;class&quot;, new data points are classified according to the highest posterior probabilities. If &quot;prob&quot;, the posterior probabilities for each class are returned.</p>
</td></tr>
<tr><td><code id="predict.multinomial_naive_bayes_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a specialized version of the Naive Bayes classifier, where the features represent the frequencies with which events have been generated by a multinomial distribution.
</p>
<p>The Multinomial Naive Bayes is not available through the <code><a href="#topic+naive_bayes">naive_bayes</a></code> function.
</p>
<p>The NAs in the newdata are not included into the calculation of posterior probabilities; and if present an informative warning is given.
</p>


<h3>Value</h3>

<p><code>predict.multinomial_naive_bayes</code> returns either a factor with class labels corresponding to the maximal conditional posterior probabilities or a matrix with class label specific conditional posterior probabilities.
</p>


<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>References</h3>

<p>McCallum, Andrew; Nigam, Kamal (1998). A comparison of event models for Naive Bayes text classification (PDF). AAAI-98 workshop on learning for text categorization. 752. <code>http://www.cs.cmu.edu/~knigam/papers/multinomial-aaaiws98.pdf</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+multinomial_naive_bayes">multinomial_naive_bayes</a></code>, <code><a href="#topic+tables">tables</a></code>, <code><a href="#topic+get_cond_dist">get_cond_dist</a></code>, <code><a href="#topic++25class+25">%class%</a></code>, <code><a href="#topic+coef.multinomial_naive_bayes">coef.multinomial_naive_bayes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Simulate the data:
cols &lt;- 10 ; rows &lt;- 100
M &lt;- matrix(sample(0:5, rows * cols,  TRUE), nrow = rows, ncol = cols)
y &lt;- factor(sample(paste0("class", LETTERS[1:2]), rows, TRUE, prob = c(0.3,0.7)))
colnames(M) &lt;- paste0("V", seq_len(ncol(M)))
laplace &lt;- 1

### Train the Multinomial Naive Bayes
mnb &lt;- multinomial_naive_bayes(x = M, y = y, laplace = laplace)

# Classification
head(predict(mnb, newdata = M, type = "class"))
head(mnb %class% M)

# Posterior probabilities
head(predict(mnb, newdata = M, type = "prob"))
head(mnb %prob% M)
</code></pre>

<hr>
<h2 id='predict.naive_bayes'>Predict Method for naive_bayes Objects</h2><span id='topic+predict.naive_bayes'></span>

<h3>Description</h3>

<p>Classification based on Naive Bayes models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'naive_bayes'
predict(object, newdata = NULL, type = c("class","prob"),
  threshold = 0.001, eps = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.naive_bayes_+3A_object">object</code></td>
<td>
<p>object of class inheriting from <code>"naive_bayes"</code>.</p>
</td></tr>
<tr><td><code id="predict.naive_bayes_+3A_newdata">newdata</code></td>
<td>
<p>matrix or dataframe with categorical (character/factor/logical) or metric (numeric) predictors.</p>
</td></tr>
<tr><td><code id="predict.naive_bayes_+3A_type">type</code></td>
<td>
<p>if &quot;class&quot;, new data points are classified according to the highest posterior probabilities. If &quot;prob&quot;, the posterior probabilities for each class are returned.</p>
</td></tr>
<tr><td><code id="predict.naive_bayes_+3A_threshold">threshold</code></td>
<td>
<p>value by which zero probabilities or probabilities within the epsilon-range corresponding to metric variables are replaced (zero probabilities corresponding to categorical variables can be handled with Laplace (additive) smoothing).</p>
</td></tr>
<tr><td><code id="predict.naive_bayes_+3A_eps">eps</code></td>
<td>
<p>value that specifies an epsilon-range to replace zero or close to zero probabilities by <code>threshold</code>. It applies to metric variables.</p>
</td></tr>
<tr><td><code id="predict.naive_bayes_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes conditional posterior probabilities for each class label using the Bayes' rule under the assumption of independence of predictors. If no new data is provided, the data from the object is used. Logical variables are treated as categorical (binary) variables. Predictors with missing values are not included into the computation of posterior probabilities.
</p>


<h3>Value</h3>

<p><code>predict.naive_bayes</code> returns either a factor with class labels corresponding to the maximal conditional posterior probabilities or a matrix with class label specific conditional posterior probabilities.
</p>


<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+naive_bayes">naive_bayes</a></code>, <code><a href="#topic+plot.naive_bayes">plot.naive_bayes</a></code>, <code><a href="#topic+tables">tables</a></code>, <code><a href="#topic+get_cond_dist">get_cond_dist</a></code>, <code><a href="#topic++25class+25">%class%</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Simulate example data
n &lt;- 100
set.seed(1)
data &lt;- data.frame(class = sample(c("classA", "classB"), n, TRUE),
                   bern = sample(LETTERS[1:2], n, TRUE),
                   cat  = sample(letters[1:3], n, TRUE),
                   logical = sample(c(TRUE,FALSE), n, TRUE),
                   norm = rnorm(n),
                   count = rpois(n, lambda = c(5,15)))
train &lt;- data[1:95, ]
test &lt;- data[96:100, -1]

### Fit the model with default settings
nb &lt;- naive_bayes(class ~ ., train)

# Classification
predict(nb, test, type = "class")
nb %class% test

# Posterior probabilities
predict(nb, test, type = "prob")
nb %prob% test


## Not run: 
vars &lt;- 10
rows &lt;- 1000000
y &lt;- sample(c("a", "b"), rows, TRUE)

# Only categorical variables
X1 &lt;- as.data.frame(matrix(sample(letters[5:9], vars * rows, TRUE),
                           ncol = vars))
nb_cat &lt;- naive_bayes(x = X1, y = y)
nb_cat
system.time(pred2 &lt;- predict(nb_cat, X1))

## End(Not run)
</code></pre>

<hr>
<h2 id='predict.nonparametric_naive_bayes'>Predict Method for nonparametric_naive_bayes Objects</h2><span id='topic+predict.nonparametric_naive_bayes'></span>

<h3>Description</h3>

<p>Classification based on the Non-Parametric Naive Bayes model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nonparametric_naive_bayes'
predict(object, newdata = NULL, type = c("class","prob"),
  threshold = 0.001, eps = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.nonparametric_naive_bayes_+3A_object">object</code></td>
<td>
<p>object of class inheriting from <code>"nonparametric_naive_bayes"</code>.</p>
</td></tr>
<tr><td><code id="predict.nonparametric_naive_bayes_+3A_newdata">newdata</code></td>
<td>
<p>matrix with metric predictors (only numeric matrix accepted).</p>
</td></tr>
<tr><td><code id="predict.nonparametric_naive_bayes_+3A_type">type</code></td>
<td>
<p>if &quot;class&quot;, new data points are classified according to the highest posterior probabilities. If &quot;prob&quot;, the posterior probabilities for each class are returned.</p>
</td></tr>
<tr><td><code id="predict.nonparametric_naive_bayes_+3A_threshold">threshold</code></td>
<td>
<p>value by which zero probabilities or probabilities within the epsilon-range corresponding to metric variables are replaced (zero probabilities corresponding to categorical variables can be handled with Laplace (additive) smoothing).</p>
</td></tr>
<tr><td><code id="predict.nonparametric_naive_bayes_+3A_eps">eps</code></td>
<td>
<p>value that specifies an epsilon-range to replace zero or close to zero probabilities by <code>threshold</code>. It applies to metric variables.</p>
</td></tr>
<tr><td><code id="predict.nonparametric_naive_bayes_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a specialized version of the Naive Bayes classifier, in which all features take on real values (numeric/integer) and class conditional probabilities are non-parametrically estimated with kernel density estimator. By default Gaussian kernel is used and the smoothing bandwidth is selected according to the Silverman's 'rule of thumb'. For more details, please see the references and the documentation of <code><a href="stats.html#topic+density">density</a></code> and <code><a href="stats.html#topic+bw.nrd0">bw.nrd0</a></code>.
</p>
<p>The Non-Parametric Naive Bayes is available in both, <code>naive_bayes()</code> and <code>nonparametric_naive_bayes()</code>. This specialized implementation of the Naive Bayes does not provide a substantial speed-up over the general <code>naive_bayes()</code> function but it should be more transparent and user friendly.
</p>
<p>The <code>nonparametric_naive_bayes</code> function is equivalent to <code>naive_bayes()</code> when the numeric matrix or a data.frame contains only numeric variables and <code>usekernel = TRUE</code>.
</p>
<p>The missing values (NAs) are omitted during the parameter estimation. The NAs in the newdata in <code>predict.nonparametric_naive_bayes()</code> are not included into the calculation of posterior probabilities; and if present an informative warning is given.
</p>


<h3>Value</h3>

<p><code>predict.nonparametric_naive_bayes</code> returns either a factor with class labels corresponding to the maximal conditional posterior probabilities or a matrix with class label specific conditional posterior probabilities.
</p>


<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>References</h3>

<p>Silverman, B. W. (1986). Density Estimation for Statistics and Data Analysis. Chapman &amp; Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+naive_bayes">naive_bayes</a></code>, <code><a href="#topic+nonparametric_naive_bayes">nonparametric_naive_bayes</a></code>, <code><a href="#topic+plot.nonparametric_naive_bayes">plot.nonparametric_naive_bayes</a></code>, <code><a href="#topic+tables">tables</a></code>, <code><a href="#topic+get_cond_dist">get_cond_dist</a></code>, <code><a href="#topic+naive_bayes">naive_bayes</a></code>, <code><a href="#topic++25class+25">%class%</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
y &lt;- iris[[5]]
M &lt;- as.matrix(iris[-5])

### Train the Non-Parametric Naive Bayes
nnb &lt;- nonparametric_naive_bayes(x = M, y = y, bw = "SJ")

### Classification
head(predict(nnb, newdata = M, type = "class"))
head(nnb %class% M)

### Posterior probabilities
head(predict(nnb, newdata = M, type = "prob"))
head(nnb %prob% M)
</code></pre>

<hr>
<h2 id='predict.poisson_naive_bayes'>Predict Method for poisson_naive_bayes Objects</h2><span id='topic+predict.poisson_naive_bayes'></span>

<h3>Description</h3>

<p>Classification based on the Poisson Naive Bayes model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'poisson_naive_bayes'
predict(object, newdata = NULL, type = c("class","prob"),
  threshold = 0.001, eps = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.poisson_naive_bayes_+3A_object">object</code></td>
<td>
<p>object of class inheriting from <code>"poisson_naive_bayes"</code>.</p>
</td></tr>
<tr><td><code id="predict.poisson_naive_bayes_+3A_newdata">newdata</code></td>
<td>
<p>matrix with non-negative integer predictors (only numeric matrix is accepted).</p>
</td></tr>
<tr><td><code id="predict.poisson_naive_bayes_+3A_type">type</code></td>
<td>
<p>if &quot;class&quot;, new data points are classified according to the highest posterior probabilities. If &quot;prob&quot;, the posterior probabilities for each class are returned.</p>
</td></tr>
<tr><td><code id="predict.poisson_naive_bayes_+3A_threshold">threshold</code></td>
<td>
<p>value by which zero probabilities or probabilities within the epsilon-range corresponding to metric variables are replaced (zero probabilities corresponding to categorical variables can be handled with Laplace (additive) smoothing).</p>
</td></tr>
<tr><td><code id="predict.poisson_naive_bayes_+3A_eps">eps</code></td>
<td>
<p>value that specifies an epsilon-range to replace zero or close to zero probabilities by <code>threshold</code>.</p>
</td></tr>
<tr><td><code id="predict.poisson_naive_bayes_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a specialized version of the Naive Bayes classifier, in which all features are non-negative integers and class conditional probabilities are modelled with the Poisson distribution.
</p>
<p>Class posterior probabilities are calculated using the Bayes' rule under the assumption of independence of predictors. If no <code>newdata</code> is provided, the data from the object is used.
</p>
<p>The Poisson Naive Bayes is available in both, <code>naive_bayes</code> and <code>poisson_naive_bayes</code>. The implementation of the specialized Naive Bayes provides more efficient performance though. The speedup comes from the restricting the data input to a numeric matrix and performing the linear algebra as well vectorized operations on it.
</p>
<p>The NAs in the newdata are not included into the calculation of posterior probabilities; and if present an informative warning is given.
</p>
<p>The <code>poisson_naive_bayes</code> function is equivalent to the <code>naive_bayes</code> function with <code>usepoisson=TRUE</code> and a numeric matrix or a data.frame containing only non-negative integer valued features (each variable has class &quot;integer&quot;).
</p>


<h3>Value</h3>

<p><code>predict.poisson_naive_bayes</code> returns either a factor with class labels corresponding to the maximal conditional posterior probabilities or a matrix with class label specific conditional posterior probabilities.
</p>


<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+poisson_naive_bayes">poisson_naive_bayes</a></code>, <code><a href="#topic+plot.poisson_naive_bayes">plot.poisson_naive_bayes</a></code>, <code><a href="#topic+tables">tables</a></code>, <code><a href="#topic+get_cond_dist">get_cond_dist</a></code>, <code><a href="#topic++25class+25">%class%</a></code>, <code><a href="#topic+coef.poisson_naive_bayes">coef.poisson_naive_bayes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cols &lt;- 10 ; rows &lt;- 100
M &lt;- matrix(rpois(rows * cols, lambda = 3), nrow = rows, ncol = cols)
# is.integer(M) # [1] TRUE
y &lt;- factor(sample(paste0("class", LETTERS[1:2]), rows, TRUE))
colnames(M) &lt;- paste0("V", seq_len(ncol(M)))
laplace &lt;- 0

### Train the Poisson Naive Bayes
pnb &lt;- poisson_naive_bayes(x = M, y = y, laplace = laplace)

### Classification
head(predict(pnb, newdata = M, type = "class"))
head(pnb %class% M)

### Posterior probabilities
head(predict(pnb, newdata = M, type = "prob"))
head(pnb %prob% M)
</code></pre>

<hr>
<h2 id='tables'>Browse Tables of Naive Bayes Classifier</h2><span id='topic+tables'></span>

<h3>Description</h3>

<p>Auxiliary function for <code>"naive_bayes"</code> and <code>"*_naive_bayes"</code> objects for easy browsing tables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tables(object, which = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tables_+3A_object">object</code></td>
<td>
<p>object of class inheriting from: <code>"naive_bayes"</code> and <code>"*_naive_bayes"</code>.</p>
</td></tr>
<tr><td><code id="tables_+3A_which">which</code></td>
<td>
<p>tables to be showed (all by default). This can be any valid indexing vector or vector containing names of variables.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Default print method for <code>"naive_bayes"</code> and <code>"*_naive_bayes"</code> objects shows at most five first tables. This auxiliary function <code>tables</code> returns by default all tables and allows easy subsetting via indexing variables.
</p>


<h3>Value</h3>

<p>list with tables.
</p>


<h3>Author(s)</h3>

<p>Michal Majka, <a href="mailto:michalmajka@hotmail.com">michalmajka@hotmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+naive_bayes">naive_bayes</a></code>, <code><a href="#topic+bernoulli_naive_bayes">bernoulli_naive_bayes</a></code>, <code><a href="#topic+multinomial_naive_bayes">multinomial_naive_bayes</a></code>, <code><a href="#topic+poisson_naive_bayes">poisson_naive_bayes</a></code>, <code><a href="#topic+gaussian_naive_bayes">gaussian_naive_bayes</a></code>, <code><a href="#topic+tables">tables</a></code>, <code><a href="#topic+get_cond_dist">get_cond_dist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
nb &lt;- naive_bayes(Species ~ ., data = iris)
tables(nb, "Sepal.Length")
tables(nb, c("Sepal.Length", "Sepal.Width"))
tabs &lt;- tables(nb, 1:2)
tabs
tabs[1]
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
