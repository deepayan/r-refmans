<!DOCTYPE html><html><head><title>Help for package lmridge</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {lmridge}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bias.plot'><p>Bias Variance and MSE Trade-off Plot</p></a></li>
<li><a href='#cv.plot'><p>Ridge CV and GCV Plot</p></a></li>
<li><a href='#Hald'><p>Portland Cement benchmark of Hald(1952)</p></a></li>
<li><a href='#hatr.lmridge'><p>Ridge Regression: Hat Matrix</p></a></li>
<li><a href='#info.plot'><p>Model Selection Criteria Plots</p></a></li>
<li><a href='#infocr.lmridge'><p>Model Selection Criteria for Ridge Regression</p></a></li>
<li><a href='#isrm.plot'><p>ISRM and m-scale Plot</p></a></li>
<li><a href='#kest.lmridge'><p>Computation of Ridge Biasing Parameter <code class="reqn">K</code></p></a></li>
<li><a href='#lmridge'><p>Linear Ridge Regression</p></a></li>
<li><a href='#lmridge-package'><p>Linear Ridge Regression</p></a></li>
<li><a href='#plot.lmridge'><p>VIF and Ridge Trace Plot</p></a></li>
<li><a href='#predict.lmridge'><p>Predict method for Linear Ridge Model Fits</p></a></li>
<li><a href='#press.lmridge'><p>Predicted Residual Sum of Squares</p></a></li>
<li><a href='#residuals.lmridge'><p>Ridge Regression Residuals</p></a></li>
<li><a href='#rplots.plot'><p>Miscellaneous Ridge Plots</p></a></li>
<li><a href='#rstats1.lmridge'><p>Ordinary Ridge Regression Statistics 1</p></a></li>
<li><a href='#rstats2.lmridge'><p>Ordinary Ridge Regression Statistics 2</p></a></li>
<li><a href='#summary.lmridge'><p>Summarizing Linear Ridge Regression Fits</p></a></li>
<li><a href='#vcov.lmridge'><p>Variance-Covariance Matrix for Fitted Ridge Model</p></a></li>
<li><a href='#vif.lmridge'><p>Variance Inflation Fator for Linear Ridge Regression</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Linear Ridge Regression with Ridge Penalty and Ridge Statistics</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.2</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Imdad Ullah Muhammad &lt;mimdadasad@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Linear ridge regression coefficient's estimation and testing with different ridge related measures such as MSE, R-squared etc.
  REFERENCES
  i.   Hoerl and Kennard (1970) &lt;<a href="https://doi.org/10.1080%2F00401706.1970.10488634">doi:10.1080/00401706.1970.10488634</a>&gt;,
  ii.  Halawa and El-Bassiouni (2000) &lt;<a href="https://doi.org/10.1080%2F00949650008812006">doi:10.1080/00949650008812006</a>&gt;,
  iii. Imdadullah, Aslam, and Saima (2017),
  iv.  Marquardt (1970) &lt;<a href="https://doi.org/10.2307%2F1267205">doi:10.2307/1267205</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2.0)]</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.4), stats</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://rfaqs.com/contact/">https://rfaqs.com/contact/</a></td>
</tr>
<tr>
<td>Note:</td>
<td>Department of Statistics, Bahauddin Zakariya University, Multan,
Pakistan</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-01-15 09:17:59 UTC; Dr. Imdad</td>
</tr>
<tr>
<td>Author:</td>
<td>Imdad Ullah Muhammad
    <a href="https://orcid.org/0000-0002-1315-491X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre],
  Aslam Muhammad [aut, ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-01-15 09:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bias.plot'>Bias Variance and MSE Trade-off Plot</h2><span id='topic+bias.plot'></span>

<h3>Description</h3>

<p>Trade-off between bias, variance and MSE of the linear ridge regression against vector or scalar value of biasing parameter <code class="reqn">K</code> (see Kalivas and Palmer, 2014 &lt;<a href="https://doi.org/10.1002/cem.2555">doi:10.1002/cem.2555</a>&gt;).</p>


<h3>Usage</h3>

<pre><code class='language-R'>bias.plot(x, abline = TRUE, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bias.plot_+3A_x">x</code></td>
<td>
<p>An object of class &quot;lmridge&quot;.</p>
</td></tr>
<tr><td><code id="bias.plot_+3A_abline">abline</code></td>
<td>
<p>Horizontal and vertical lines show the minimum value of the ridge MSE at certain value of biasing parameter <code class="reqn">K</code>.</p>
</td></tr>
<tr><td><code id="bias.plot_+3A_...">...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The effect of multicollinearity on the coefficient estimates can be identified using different graphical display. One of them is plot of bias, variance and MSE. A little addition of bias lead to a substantial decrease in variance, and MSE. Therefore, a trade-off is made between bias and variance to have acceptable MSE. The <code>bias.plot</code> can be helpful for selection of optimal value of biasing parameter <code class="reqn">K</code>.</p>


<h3>Value</h3>

<p>Nothing returned</p>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>
<p>Kalivas, J. H., and Palmer, J. (2014). Characterizing multivariate calibration tradeoffs (bias, variance, selectivity, and sensitivity) to select model tuning parameters. <em>Journal of Chemometrics</em>, <strong>28</strong>(5), 347&ndash;357. <a href="https://doi.org/10.1002/cem.2555">doi:10.1002/cem.2555</a>.
</p>


<h3>See Also</h3>

<p>The ridge model fitting <code><a href="#topic+lmridge">lmridge</a></code>, ridge CV and GCV plots <code><a href="#topic+cv.plot">cv.plot</a></code>, ridge AIC and BIC plots <code><a href="#topic+info.plot">info.plot</a></code>, m-scale and isrm plots <code><a href="#topic+isrm.plot">isrm.plot</a></code>, ridge and VIF trace <code><a href="#topic+plot.lmridge">plot.lmridge</a></code>, miscellaneous ridge plots <code><a href="#topic+rplots.plot">rplots.plot</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>mod &lt;- lmridge(y~., as.data.frame(Hald), K = seq(0, 0.3, 0.002))
## for indication vertical line (biasing parameter k) and
## horizontal line (minimum minimum ridge MSE values corresponding to vertical line)
bias.plot(mod)

## without Horizontal and vertical line as set \code{abline = FALSE}
bias.plot(mod, abline=FALSE)
</code></pre>

<hr>
<h2 id='cv.plot'>Ridge CV and GCV Plot</h2><span id='topic+cv.plot'></span>

<h3>Description</h3>

<p>Plot of ridge CV and GCV against scalar or vector values of biasing parameter <code class="reqn">K</code> (see Golub et al., 1979 &lt;<a href="https://doi.org/10.1080/00401706.1979.10489751">doi:10.1080/00401706.1979.10489751</a>&gt;).</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.plot(x, abline = TRUE, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.plot_+3A_x">x</code></td>
<td>
<p>An object of class &quot;lmridge&quot;.</p>
</td></tr>
<tr><td><code id="cv.plot_+3A_abline">abline</code></td>
<td>
<p>Horizontal and vertical lines to show minimum value of ridge GCV and CV at certain value of biasing parameter <code class="reqn">K</code>.</p>
</td></tr>
<tr><td><code id="cv.plot_+3A_...">...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function <code>cv.plot</code> can be used to plot the values of ridge CV and GCV against scalar or vector value of biasing parameter <code class="reqn">K</code>. The <code>cv.plot</code> can be helpful for selection of optimal value of ridge biasing parameter <code class="reqn">K</code>. If no argument is used then horizontal line will indicate minimum GCV and Cv at certain value of biasing parameter <code class="reqn">K</code>.
</p>


<h3>Value</h3>

<p>Nothing returned</p>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Delaney, N. J. and Chatterjee, S. (1986). Use of the Bootstrap and Cross-Validation in Ridge Regression. <em>Journal of Business &amp; Economic Statistics</em>. <strong>4</strong>(2), 255&ndash;262.
</p>
<p>Golub, G., Wahba, G. and Heat, C. (1979). Generalized Cross Validation as a Method for Choosing a Good Ridge Parameter. <em>Technometrics.</em> <strong>21</strong>, 215&ndash;223. <a href="https://doi.org/10.2307/1268518">doi:10.2307/1268518</a>.
</p>
<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>


<h3>See Also</h3>

<p>The ridge model fitting <code><a href="#topic+lmridge">lmridge</a></code>, bias variance trade-off plot <code><a href="#topic+bias.plot">bias.plot</a></code>, ridge AIC and BIC plots <code><a href="#topic+info.plot">info.plot</a></code>, m-scale and isrm plots <code><a href="#topic+isrm.plot">isrm.plot</a></code>, ridge and VIF trace <code><a href="#topic+plot.lmridge">plot.lmridge</a></code>, miscellaneous ridge plots <code><a href="#topic+rplots.plot">rplots.plot</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>mod &lt;- lmridge(y~., as.data.frame(Hald), K = seq(0, 0.2, 0.002))
## for indication vertical line (biasing parameter k) and
## horizontal line (minimum respective CV and GCV values corresponding to vertical line)
cv.plot(mod)

## without Horizontal and vertical line set \code{abline = FALSE}
cv.plot(mod, abline = FALSE)
</code></pre>

<hr>
<h2 id='Hald'>Portland Cement benchmark of Hald(1952)</h2><span id='topic+Hald'></span>

<h3>Description</h3>

<p>Heat evolved during setting of 13 cement mixtures of four basic ingredients.  Each
ingredient percentage appears to be rounded down to a full integer.  The sum of the four mixture percentages varies from a maximum of 99% to a minimum of 95%.  If all four regressor
X-variables always summed to 100%, the centered X-matrix would then be of rank only 3.  Thus, the regression of heat on four X-percentages is ill-conditioned, with an approximate rank deficiency of MCAL = 1. </p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Hald)</code></pre>


<h3>Format</h3>

<p>A data frame with 13 observations on the following 5 variables.
</p>

<dl>
<dt><code>X1</code></dt><dd><p>p3ca: Integer percentage of 3CaO.Al2O3 in the mixture.</p>
</dd>
<dt><code>X2</code></dt><dd><p>p3cs: Integer percentage of 3CaO.SiO2 in the mixture.</p>
</dd>
<dt><code>X3</code></dt><dd><p>p4caf: Integer percentage of 4CaO.Al2O3.Fe2O3 in the mixture.</p>
</dd>
<dt><code>X4</code></dt><dd><p>p2cs: Integer percentage of 2CaO.SiO2 in the mixture.</p>
</dd>
<dt><code>y</code></dt><dd><p>hear: Heat (cals/gm) evolved in setting, recorded to nearest tenth.</p>
</dd>
</dl>



<h3>Details</h3>

<p>The (lmridge) Hald data are identical to the (MASS) cement data except for
variable names.</p>


<h3>Source</h3>

<p>Woods, H., Steinour, H.H. and Starke, H.R. (1932). Effect of Composition of Portland Cement on Heat Evolved During Hardening. <em>Industrial Engineering and Chemistry</em> <strong>24</strong>: 1207&ndash;1214.
</p>


<h3>References</h3>

<p>Hald, A. (1952). <em>Statistical Theory with Engineering Applications</em>.(page 647.) New York; Wiley.
</p>

<hr>
<h2 id='hatr.lmridge'>Ridge Regression: Hat Matrix</h2><span id='topic+hatr'></span><span id='topic+hatr.lmridge'></span>

<h3>Description</h3>

<p>The <code>hatr</code> function computes hat matrix (see Hastie and Tibshirani, 1990).</p>


<h3>Usage</h3>

<pre><code class='language-R'>hatr(x, ...)
## S3 method for class 'lmridge'
hatr(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hatr.lmridge_+3A_x">x</code></td>
<td>
<p>An object of class &quot;lmridge&quot;.</p>
</td></tr>
<tr><td><code id="hatr.lmridge_+3A_...">...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Hat matrix for scalar or vector values of biasing parameter provided as argument to <code>lmridge</code>. It is used to compute degrees of freedom for given <code class="reqn">K</code>, and error degree of freedom etc. The hat matrix can be computed using formula <code class="reqn">X(X'X+kI)^{-1}X'</code> equivalently <code class="reqn">\sum{\frac{\lambda_j}{(\lambda_j+k)}}</code>. </p>


<h3>Value</h3>

<p>returns a list of matrix for each biasing parameter <code class="reqn">K</code>:
</p>
<table>
<tr><td><code>hatr</code></td>
<td>
<p>A list of hat matrix for each biasing parameter <code class="reqn">K</code></p>
</td></tr></table>
<p>. </p>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Cule, E. and De lorio, M. (2012). A semi-Automatic method to guide the choice of ridge parameter in ridge regression.  <em>arXiv:abs/1205.0686v1 [stat.AP]</em>.
</p>
<p>Hastie, T. and Tibshirani, R. (1990). <em>Generalized Additive Models</em>. Chapman and Hall.
</p>
<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>


<h3>See Also</h3>

<p>The ridge model fitting <code><a href="#topic+lmridge">lmridge</a></code>, ridge Var-Cov matrix <code><a href="#topic+vcov.lmridge">vcov.lmridge</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>mod &lt;- lmridge(y~., as.data.frame(Hald), K = c(0, 0.1, 0.2, 0.3))
## Hat matrix for each biasing parameter
hatr(mod)

## Hat matrix for first biasing parameter i.e. K = 0.1
hatr(mod)[[2]]
</code></pre>

<hr>
<h2 id='info.plot'>Model Selection Criteria Plots</h2><span id='topic+info.plot'></span>

<h3>Description</h3>

<p>Plot of ridge AIC and BIC model selection criteria against ridge degrees of freedom (see Akaike, 1974 &lt;<a href="https://doi.org/10.1109/TAC.1974.1100705">doi:10.1109/TAC.1974.1100705</a>&gt;; Imdad, 2017 and Schwarz, 1978 &lt;<a href="https://doi.org/10.1214/aos/1176344136">doi:10.1214/aos/1176344136</a>&gt;).</p>


<h3>Usage</h3>

<pre><code class='language-R'>info.plot(x, abline = TRUE, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="info.plot_+3A_x">x</code></td>
<td>
<p>An object of class &quot;lmridge&quot;.</p>
</td></tr>
<tr><td><code id="info.plot_+3A_abline">abline</code></td>
<td>
<p>Vertical line to show minimum value of ridge MSE at certain value of ridge degrees of freedom.</p>
</td></tr>
<tr><td><code id="info.plot_+3A_...">...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Plot of ridge AIC and BIC against ridge degress of freedom <code class="reqn">\sum_{j=1}^p \frac{\lambda_j}{\lambda_j+k}</code>. A vertical line represents the minimum ridge MSE at certain value of ridge df.</p>


<h3>Value</h3>

<p>Nothing returned</p>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Akaike, H. (1974). A new look at the Statistical Model Identification. <em>IEEE Transaction on Automatic Control</em>, <strong>9</strong>(6), 716&ndash;723. <a href="https://doi.org/10.1109/TAC.1974.1100705">doi:10.1109/TAC.1974.1100705</a>.
</p>
<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>
<p>Schwarz, G. (1978). Estimating the Dimension of a Model. <em>Annals of Statistics</em>, <strong>6</strong>(2), 461&ndash;464. <a href="https://doi.org/10.1214/aos/1176344136">doi:10.1214/aos/1176344136</a>.
</p>


<h3>See Also</h3>

<p>The ridge model fitting <code><a href="#topic+lmridge">lmridge</a></code>, ridge CV and GCV plot<code><a href="#topic+cv.plot">cv.plot</a></code>, variance biase trade-off plot <code><a href="#topic+bias.plot">bias.plot</a></code>, m-scale and isrm plots <code><a href="#topic+isrm.plot">isrm.plot</a></code>, ridge and VIF trace <code><a href="#topic+plot.lmridge">plot.lmridge</a></code>, miscellaneous ridge plots <code><a href="#topic+rplots.plot">rplots.plot</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>mod &lt;- lmridge(y~., as.data.frame(Hald), K = seq(0, 0.15, 0.002))
## for indication vertical line (df ridge)
info.plot(mod)

## without vertical line set \code{abline = FALSE}
info.plot(mod, abline = FALSE)
</code></pre>

<hr>
<h2 id='infocr.lmridge'>Model Selection Criteria for Ridge Regression</h2><span id='topic+infocr'></span><span id='topic+infocr.lmridge'></span>

<h3>Description</h3>

<p>The <code>infocr.lmridge</code> function computes model information selection criteria (AIC and BIC), see Akaike, 1974 &lt;<a href="https://doi.org/10.1109/TAC.1974.1100705">doi:10.1109/TAC.1974.1100705</a>&gt;; Imdad, 2017 and Schwarz, 1978 &lt;<a href="https://doi.org/10.1214/aos/1176344136">doi:10.1214/aos/1176344136</a>&gt;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>infocr(object, ...)
## S3 method for class 'lmridge'
infocr(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="infocr.lmridge_+3A_object">object</code></td>
<td>
<p>An object of class &quot;lmridge&quot;.</p>
</td></tr>
<tr><td><code id="infocr.lmridge_+3A_...">...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Model information selection criteria are common way of selecting among model while balancing the competing goals of fit and parsimony. The model selection criteria AIC and BIC are computed by quantifying <code>df</code> in the ridge regression model, using formula (<code class="reqn">df=trace[X(X'X+kI)^{-1}X']</code>). It can be helpful for selecting optimal value of biasing parameter <code class="reqn">K</code>.</p>


<h3>Value</h3>

<p>It returns a matrix of information criteria, AIC and BIC for each biasing parameter <code class="reqn">K</code>. Column of matrix indicates model selection criteria AIC and BIC, respectively, while rows indicate value of biasing parameter <code class="reqn">K</code> for which model selection criteria are computed. </p>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Akaike, H. (1974). A new look at the Statistical Model Identification. <em>IEEE Transaction on Automatic Control</em>, <strong>9</strong>(6), 716-723. <a href="https://doi.org/10.1109/TAC.1974.1100705">doi:10.1109/TAC.1974.1100705</a>.
</p>
<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>
<p>Schwarz, G. (1978). Estimating the Dimension of a Model. <em>Annals of Statistics</em>, <strong>6</strong>(2), 461&ndash;464. <a href="https://doi.org/10.1214/aos/1176344136">doi:10.1214/aos/1176344136</a>.
</p>


<h3>See Also</h3>

<p>the ridge model fitting <code><a href="#topic+lmridge">lmridge</a></code>, ridge AIC and BIC plot <code><a href="#topic+info.plot">info.plot</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>mod &lt;- lmridge(y~., as.data.frame(Hald), K = seq(0, .2, 0.001))
infocr(mod)

## Vector of AIC values
infocr(mod)[,1]

## vector of BIC values
infocr(mod)[,2]
</code></pre>

<hr>
<h2 id='isrm.plot'>ISRM and m-scale Plot</h2><span id='topic+isrm.plot'></span>

<h3>Description</h3>

<p>Plot of m-scale and ISRM against scalar or vector values of biasing parameter <code class="reqn">K</code> (Vinod, 1976 &lt;<a href="https://doi.org/10.1080/01621459.1976.10480955">doi:10.1080/01621459.1976.10480955</a>&gt;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isrm.plot(x, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="isrm.plot_+3A_x">x</code></td>
<td>
<p>An object of class &quot;lmridge&quot;.</p>
</td></tr>
<tr><td><code id="isrm.plot_+3A_...">...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td></tr></table>


<h3>Details</h3>

<p>The <code>isrm.plot</code> function can be used to plot the values of m-scale and ISRM against given list (scalar or vector values) of biasing parameter  <code class="reqn">K</code> as argument to <code>lmridge</code>. It can be helpful for the optimal selection of the biasing parameter  <code class="reqn">K</code>.
</p>


<h3>Value</h3>

<p>Nothing returned</p>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>
<p>Vinod, H. (1976). Application of New Ridge Regression Methods to a Study of Bell System Scale Economics. <em>Journal of the American Statistical Association</em>, <strong>71</strong>, 835&ndash;841. <a href="https://doi.org/10.2307/2286847">doi:10.2307/2286847</a>.
</p>


<h3>See Also</h3>

<p>The ridge model fitting <code><a href="#topic+lmridge">lmridge</a></code>, ridge CV and GCV plots <code><a href="#topic+cv.plot">cv.plot</a></code>, ridge AIC and BIC plots <code><a href="#topic+info.plot">info.plot</a></code>, variance bias trade-off plot <code><a href="#topic+bias.plot">bias.plot</a></code>, ridge and VIF trace <code><a href="#topic+plot.lmridge">plot.lmridge</a></code>, miscellaneous ridge plots<code><a href="#topic+rplots.plot">rplots.plot</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>mod &lt;- lmridge(y~., as.data.frame(Hald), K = seq(0, 0.2, 0.002))

isrm.plot(mod)
isrm.plot(mod, abline=FALSE)
</code></pre>

<hr>
<h2 id='kest.lmridge'>Computation of Ridge Biasing Parameter <code class="reqn">K</code></h2><span id='topic+kest'></span><span id='topic+kest.lmridge'></span><span id='topic+print.klmridge'></span>

<h3>Description</h3>

<p>The <code>kest</code> function computes different biasing parameters available in the literature proposed by different researchers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kest(object, ...)
## S3 method for class 'lmridge'
kest(object, ...)
## S3 method for class 'klmridge'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kest.lmridge_+3A_object">object</code></td>
<td>
<p>An object of class &quot;lmridge&quot; for the <code>kest</code>.</p>
</td></tr>
<tr><td><code id="kest.lmridge_+3A_x">x</code></td>
<td>
<p>An object of class &quot;klmridge&quot; for the <code>print.kest.klmridge</code>.</p>
</td></tr>
<tr><td><code id="kest.lmridge_+3A_...">...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>kest</code> function computes different biasing parameter for the ordinary linear ridge regression. All these methods are already available in the literature and proposed by various authors. See reference section.</p>


<h3>Value</h3>

<p>The function returns the list of following biasing parameter methods proposed by various researchers.
</p>
<table>
<tr><td><code>mHKB</code></td>
<td>
<p>By Thisted (1976), <code class="reqn">\frac{((p-2)*\hat{\sigma}^2)}{\sum(\beta^2)}</code> </p>
</td></tr>
<tr><td><code>LW</code></td>
<td>
<p>As in <code>lm.ridge</code> of <code>MASS</code> <code class="reqn">\frac{((p-2)*\hat{\sigma}^2*n)}{\sum(\hat{y}^2)}</code></p>
</td></tr>
<tr><td><code>LW76</code></td>
<td>
<p>By Lawless and Wang (1976), <code class="reqn">\frac{p*\hat{\sigma}^22}{\sum(\lambda_j*\hat{\alpha}_j^2)}</code></p>
</td></tr>
<tr><td><code>CV</code></td>
<td>
<p>Value of Cross Validation (CV) for each biasing parameter <code class="reqn">K</code>, <code class="reqn">CV_k=\frac{1}{n}\sum_{j=1}^n (y_i-X_j \hat{\beta}_{j_K})^2 </code>.</p>
</td></tr>
<tr><td><code>kCV</code></td>
<td>
<p>Value of biasing parameter at which CV is small.</p>
</td></tr>
<tr><td><code>HKB</code></td>
<td>
<p>By Hoerl and Kennard (1970), <code class="reqn">\frac{p*\hat{\sigma}^2}{\hat{\beta}'\hat{\beta}}</code></p>
</td></tr>
<tr><td><code>kibAM</code></td>
<td>
<p>By Kibria (2003), <code class="reqn">\frac{1}{p}*\sum(\frac{\hat{\sigma}^2}{\hat{\beta}_j^2)}</code> </p>
</td></tr>
<tr><td><code>GCV</code></td>
<td>
<p>Value of Generalized Cross Validation (GCV) for each biasing parameter <code class="reqn">K</code>, <code class="reqn">\frac{(y_i-X_j\hat{\beta}_{J_K})^2}{[n-(1+Trace(H_{R,k}))]^2}</code>.</p>
</td></tr>
<tr><td><code>kcGCV</code></td>
<td>
<p>Value of biasing parameter at which GCV is small.</p>
</td></tr>
<tr><td><code>DSK</code></td>
<td>
<p>By Dwividi and Shrivastava, (1978), <code class="reqn">\frac{\hat{\sigma}^2}{\hat{\beta}'\hat{\beta}}</code></p>
</td></tr>
<tr><td><code>kibGM</code></td>
<td>
<p>By Kibria (2003), <code class="reqn">\frac{\hat{\sigma}^2}{(\prod(\hat{\alpha}_j^2))^(1/p)}</code> </p>
</td></tr>
<tr><td><code>kibMEd</code></td>
<td>
<p>By Kibria (2003), <code class="reqn">median(\frac{\hat{\sigma}^2}{\hat{\alpha}_j^2})</code> </p>
</td></tr>
<tr><td><code>KM2</code></td>
<td>
<p>By Muniz and Kibria (2009), <code class="reqn">max[\frac{1}{\sqrt{\frac{\hat{\sigma}^2}{\hat{\alpha}^2_j}}}]</code> </p>
</td></tr>
<tr><td><code>KM3</code></td>
<td>
<p>By Muniz and Kibria (2009), <code class="reqn">max[\sqrt{\frac{\hat{\sigma}^2}{\hat{\alpha}_j^2}}]</code></p>
</td></tr>
<tr><td><code>KM4</code></td>
<td>
<p>By Muniz and Kibria (2009), <code class="reqn">[\prod\frac{1}{\sqrt{\frac{\hat{\sigma}^2}{\hat{\alpha}_j^2}}}]^\frac{1}{p}</code></p>
</td></tr>
<tr><td><code>KM5</code></td>
<td>
<p>By Muniz and Kibria (2009), <code class="reqn">[\prod \sqrt{\frac{\hat{\sigma}^2}{\hat{\alpha}_j^2}}]^{\frac{1}{p}}</code></p>
</td></tr>
<tr><td><code>KM6</code></td>
<td>
<p>By Muniz and Kibria (2009), <code class="reqn">Median[\frac{1}{\sqrt{\frac{\hat{\sigma}^2}{\hat{\alpha}^2_j}}}]</code></p>
</td></tr>
<tr><td><code>KM8</code></td>
<td>
<p>By Muniz <em>et al.</em> (2012), <code class="reqn">max(\frac{1}{\sqrt{\frac{\lambda_{max} \hat{\sigma}^2} {(n-p)\hat{\sigma}^2+\lambda_{max}\hat{\alpha}^2_j}}})</code></p>
</td></tr>
<tr><td><code>KM9</code></td>
<td>
<p>By Muniz <em>et al.</em> (2012), <code class="reqn">max[\sqrt{\frac{\lambda_{max}\hat{\sigma}^2}{(n-p)\hat{\sigma}^2}+\lambda_{max}\hat{\alpha}^2_j}]</code></p>
</td></tr>
<tr><td><code>KM10</code></td>
<td>
<p>By Muniz <em>et al.</em> (2012), <code class="reqn">[\prod(\frac{1}{\sqrt{\frac{\lambda_{max}\hat{\sigma}^2}{(n-p)\hat{\sigma}^2+\lambda_{max}\hat{\alpha}^2_j}}})]^{\frac{1}{p}}</code></p>
</td></tr>
<tr><td><code>KM11</code></td>
<td>
<p>By Muniz <em>et al.</em> (2012), <code class="reqn">[\prod(\sqrt{\frac{\lambda_{max}\hat{\sigma}^2}{(n-p) \hat{\sigma}^2+\lambda_{max}\hat{\alpha}^2_j}})^{\frac{1}{p}}</code></p>
</td></tr>
<tr><td><code>KM12</code></td>
<td>
<p>By Muniz <em>et al.</em>, <code class="reqn">Median[\frac{1}{\sqrt{\frac{\lambda_{max}\hat{\sigma}^2}{(n-p)\hat{\sigma}^2+\lambda_{max}\hat{\alpha}^2_j}}}]</code></p>
</td></tr>
<tr><td><code>KD</code></td>
<td>
<p>By Dorugade and Kashid (2012), <code class="reqn">0, \frac{p\hat{\sigma}^2}{\hat{\alpha}'\hat{\alpha}}-\frac{1}{n(VIF_j)_{max}}</code></p>
</td></tr>
<tr><td><code>KAD4</code></td>
<td>
<p>By Dorugade and Kashid (2012), <code class="reqn">HM[\frac{2p}{\lambda_{max}} \sum(\frac{\hat{\sigma}^2}{\hat{\alpha}^2_j})]</code></p>
</td></tr>
<tr><td><code>alphahat</code></td>
<td>
<p>The OLS estimator in canonical form, i.e., <code class="reqn">\hat{\alpha}=(P'X'XP)^{-1}X'^*y</code>, where <code class="reqn">X^*=XP</code> <code class="reqn">P</code> is eigenvector of <code class="reqn">X'X</code>.</p>
</td></tr>



</table>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Dorugade, A. and Kashid, D. (2010). Alternative Method for Choosing Ridge Parameter for Regression. <em>Applied Mathematical Sciences</em>, <strong>4</strong>(9), 447-456.
</p>
<p>Dorugade, A. (2014). New Ridge Parameters for Ridge Regression. <em>Journal of the Association of Arab Universities for Basic and Applied Sciences</em>, <strong>15</strong>, 94-99. <a href="https://doi.org/10.1016/j.jaubas.2013.03.005">doi:10.1016/j.jaubas.2013.03.005</a>.
</p>
<p>Hoerl, A. E., Kennard, R. W., and Baldwin, K. F. (1975). Ridge Regression: Some Simulation. <em>Communication in Statistics</em>, <strong>4</strong>, 105-123. <a href="https://doi.org/10.1080/03610927508827232">doi:10.1080/03610927508827232</a>.
</p>
<p>Hoerl, A. E. and Kennard, R. W., (1970). Ridge Regression: Biased Estimation of Nonorthogonal Problems. <em>Technometrics</em>, <strong>12</strong>, 55-67. <a href="https://doi.org/10.1080/00401706.1970.10488634">doi:10.1080/00401706.1970.10488634</a>.
</p>
<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>
<p>Kibria, B. (2003). Performance of Some New Ridge Regression Estimators. <em>Communications in Statistics-Simulation and Computation</em>, <strong>32</strong>(2), 491-435. <a href="https://doi.org/10.1081/SAC-120017499">doi:10.1081/SAC-120017499</a>.
</p>
<p>Lawless, J., and Wang, P. (1976). A Simulation Study of Ridge and Other Regression Estimators. <em>Communications in Statistics-Theory and Methods</em>, <strong>5</strong>(4), 307-323. <a href="https://doi.org/10.1080/03610927608827353">doi:10.1080/03610927608827353</a>.
</p>
<p>Muniz, G., and Kibria, B. (2009). On Some Ridge Regression Estimators: An Empirical Comparisons. <em>Communications in Statistics-Simulation and Computation</em>, <strong>38</strong>(3), 621-630. <a href="https://doi.org/10.1080/03610910802592838">doi:10.1080/03610910802592838</a>.
</p>
<p>Muniz, G., Kibria, B., Mansson, K., and Shukur, G. (2012). On developing Ridge Regression Parameters: A Graphical Investigation. <em>SORT-Statistics and Operations Research Transactions</em>, <strong>36</strong>(2), 115&ndash;138.
</p>
<p>Thisted, R. A. (1976). Ridge Regression, Minimax Estimation and Empirical Bayes Methods. <em>Technical Report 28, Division of Biostatistics</em>, Stanford University, California.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002). <em>Modern Applied Statistics with S</em>. Springer New York, 4th edition, ISBN 0-387-95457-0.
</p>


<h3>See Also</h3>

<p>The ridge model fitting <code><a href="#topic+lmridge">lmridge</a></code>, Ridge Var-Cov matrix <code><a href="#topic+vcov">vcov</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>mod &lt;- lmridge(y~., as.data.frame(Hald), K = seq(0, 0.2, 0.001))
kest(mod)

## GCV values
kest(mod)$GCV

## minimum GCV value at certain k
kest(mod)$kGCV

## CV Values
kest(mod)$CV

## minimum CV value at certain k
kest(mod)$kCV

## Hoerl and Kennard (1970)
kest(mod)$HKB


</code></pre>

<hr>
<h2 id='lmridge'>Linear Ridge Regression</h2><span id='topic+lmridge'></span><span id='topic+lmridge.default'></span><span id='topic+coef.lmridge'></span><span id='topic+print.lmridge'></span><span id='topic+fitted.lmridge'></span><span id='topic+lmridgeEst'></span>

<h3>Description</h3>

<p>Fits a linear ridge regression model after scaling regressors and returns an object of class &quot;lmridge&quot; (by calling <code>lmridgeEst</code> function) designed to be used in plotting method, testing of ridge coefficients and for computation of different ridge related statistics. The ridge biasing parameter <code class="reqn">K</code> can be a scalar or a vector. See Hoerl et al., 1975 &lt;<a href="https://doi.org/10.1080/03610927508827232">doi:10.1080/03610927508827232</a>&gt;,  Horel and Kennard, 1970 &lt;<a href="https://doi.org/10.1080/00401706.1970.10488634">doi:10.1080/00401706.1970.10488634</a>&gt;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lmridge(formula, data, K = 0, scaling=c("sc", "scaled", "non", "centered"), ...)
lmridgeEst(formula, data, K=0, scaling=c("sc", "scaled", "non", "centered"), ...)
## Default S3 method:
lmridge(formula, data, K = 0, scaling=c("sc", "scaled", "non", "centered"), ...)
## S3 method for class 'lmridge'
coef(object, ...)
## S3 method for class 'lmridge'
print(x, digits = max(5,getOption("digits") - 5), ...)
## S3 method for class 'lmridge'
fitted(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lmridge_+3A_formula">formula</code></td>
<td>
<p>Standard R formula expression, that is, a symbolic representation of the model to be fitted and has form <code>response~predictors</code>. For further details, see <code><a href="stats.html#topic+formula">formula</a></code>. </p>
</td></tr>
<tr><td><code id="lmridge_+3A_data">data</code></td>
<td>
<p>An optional data frame containing the variables in the model. If not found in data, the variables are taken from <code>environment(formula)</code>, typically the environment from which <code>lmridge</code> or <code>lmridgeEst</code> is called.</p>
</td></tr>
<tr><td><code id="lmridge_+3A_k">K</code></td>
<td>
<p>Ridge biasing parameter (may be a vector).</p>
</td></tr>
<tr><td><code id="lmridge_+3A_scaling">scaling</code></td>
<td>
<p>The method to be used to scale the predictors. The scaling option <code>"sc"</code> scales the predictors to correlation form, such that the correlation matrix has unit diagonal elements. <code>"scaled"</code> option standardizes the predictors to have zero mean and unit variance. <code>"non"</code> no scaling or centering is done to predictors. <code>"centered"</code> option centers the predictors.</p>
</td></tr>
<tr><td><code id="lmridge_+3A_object">object</code></td>
<td>
<p>A lmridge object, typically generated by a call to <code>lmridge</code> for <code>fitted.lmridge</code>, <code>predict.lmridge</code>, <code>vcov.lmridge</code>, <code>residuals.lmridge</code>, <code>infocr.lmridge</code>, <code>coef.lmridge</code>, <code>summary.lmridge</code> and <code>press.lmridge</code> functions.</p>
</td></tr>
<tr><td><code id="lmridge_+3A_x">x</code></td>
<td>
<p>An object of class <code>lmridge</code> (for the <code>hatr.lmridge</code>, <code>rstats1.lmridge</code>, <code>rstats2.lmridge</code>, <code>vif.lmridge</code>, <code>kest.lmridge</code>, <code>summary.lmride</code>, <code>print.lmridge</code>, <code>print.summary.lmridge</code>, <code>print.klmridge</code>, <code>print.rstats1</code>, <code>print.rstats2</code>,  and <code>plot.lmridge</code>, <code>bias.plot</code>, <code>cv.plot</code>, <code>info.plot</code>, <code>isrm.plot</code>, and <code>rplots.plot</code> functions).</p>
</td></tr>
<tr><td><code id="lmridge_+3A_digits">digits</code></td>
<td>
<p>Minimum number of significant digits to be used.</p>
</td></tr>
<tr><td><code id="lmridge_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>lmridge</code> or <code>lmridgeEst</code> function fits in linear ridge regression after scaling the regressors and centering the response. The <code>lmridge</code> is default a function that calls <code>lmridgeEst</code> for computation of ridge coefficients and returns an object of class &quot;lmridge&quot; designed to be used in plotting method, testing of ridge coefficients and for computation of different ridge related statistics. If intercept is present in the model, its coefficient is not penalized. However, intercept is estimated from the relation <code class="reqn">y=\overline{y}-\beta \overline{X}</code>. <code>print.lmridge</code> tries to be smart about formatting of ridge coefficients.</p>


<h3>Value</h3>

<p><code>lmridge</code> function returns an object of class &quot;lmridge&quot; after calling list of named objects from <code>lmridgeEst</code> function:
</p>
<table>
<tr><td><code>coef</code></td>
<td>
<p>A named vector of fitted coefficients.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>Inter</code></td>
<td>
<p>Was an intercept included?</p>
</td></tr>
<tr><td><code>scaling</code></td>
<td>
<p>The scaling method used.</p>
</td></tr>
<tr><td><code>mf</code></td>
<td>
<p>Actual data used.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>The response variable.</p>
</td></tr>
<tr><td><code>xs</code></td>
<td>
<p>The scaled matrix of predictors.</p>
</td></tr>
<tr><td><code>xm</code></td>
<td>
<p>The vector of means of the predictors.</p>
</td></tr>
<tr><td><code>terms</code></td>
<td>
<p>The <code><a href="stats.html#topic+terms">terms</a></code> object used.</p>
</td></tr>
<tr><td><code>xscale</code></td>
<td>
<p>Square root of sum of squared deviation from mean regarding the scaling option used in <code>lmridge</code> or <code>lmridgeEst</code> function as argument.</p>
</td></tr>
<tr><td><code>rfit</code></td>
<td>
<p>The fitted value of ridge regression for given biasing parameter <code class="reqn">K</code>.</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>The ridge regression biasing parameter <code class="reqn">K</code> which can be scalar or a vector.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>A vector of singular values of scaled <code>X</code> matrix.</p>
</td></tr>
<tr><td><code>div</code></td>
<td>
<p>Eigenvalues of scaled regressors.</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>A list of matrix <code class="reqn">(X'X+KI)^{-1}X'</code> for further computations.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The function at the current form cannot handle missing values. The user has to take prior action with missing values before using this function.</p>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Hoerl, A. E., Kennard, R. W., and Baldwin, K. F. (1975). Ridge Regression: Some Simulation. <em>Communication in Statistics</em>, <strong>4</strong>, 105-123. <a href="https://doi.org/10.1080/03610927508827232">doi:10.1080/03610927508827232</a>.
</p>
<p>Hoerl, A. E. and Kennard, R. W., (1970). Ridge Regression: Biased Estimation of Nonorthogonal Problems. <em>Technometrics</em>, <strong>12</strong>, 55-67. <a href="https://doi.org/10.2307/1267351">doi:10.2307/1267351</a>.
</p>
<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>


<h3>See Also</h3>

<p>Testing of ridge coefficient <code><a href="#topic+summary.lmridge">summary.lmridge</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Hald)
mod &lt;- lmridge(y~., data = as.data.frame(Hald), K = seq(0, 0.1, 0.01), scaling = "sc")
## Scaled Coefficients
mod$coef

## Re-Scaled Coefficients
coef(mod)

## ridge predicted values
predict(mod)

## ridge residuals
residuals(mod)

##ridge and VIF trace
plot(mod)

## ridge VIF values
vif(mod)

## ridge Var-Cov matrix
vcov(mod)

## ridge biasing parameter by researchers
kest(mod)

## ridge fitted values
fitted(mod)

## ridge statistics 1
rstats1(mod)

## ridge statistics 2
rstats2(mod)

## list of objects from lmridgeEst function
lmridgeEst(y~., data = as.data.frame(Hald), K = seq(0, 0.1, 0.01), scaling = "sc")

lmridgeEst(y~., data = as.data.frame(Hald), K = seq(0, 0.1, 0.01), scaling = "non")
</code></pre>

<hr>
<h2 id='lmridge-package'>Linear Ridge Regression</h2><span id='topic+lmridge-package'></span>

<h3>Description</h3>

<p>R package for fitting linear ridge regression models.</p>


<h3>Details</h3>

<p>This package contains functions for fitting linear ridge regression models, including functions for computation of different ridge related statistics (such as MSE, Var-Cov matrix, effective degrees of freedom and condition numbers), estimation of biasing parameter from different researchers, testing of ridge coefficients, model selection criteria, residuals, predicted values and fitted values. The package also includes function for plotting of ridge coefficients and different ridge statistics for selection of optimal value of biasing parameters <code class="reqn">K</code>.
</p>
<p>For a complete list of functions, use <code>library(help="lmridge")</code>.
</p>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>

<hr>
<h2 id='plot.lmridge'>VIF and Ridge Trace Plot</h2><span id='topic+plot.lmridge'></span><span id='topic+plot'></span>

<h3>Description</h3>

<p>Plot of VIF values (VIF trace) and ridge coefficients (ridge trace) for scalar or vector values of biasing parameter <code class="reqn">K</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lmridge'
plot(x, type = c("ridge", "vif"), abline = TRUE, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.lmridge_+3A_x">x</code></td>
<td>
<p>An object of class &quot;lmridge&quot;. </p>
</td></tr>
<tr><td><code id="plot.lmridge_+3A_type">type</code></td>
<td>
<p>Either VIF trace or ridge trace.</p>
</td></tr>
<tr><td><code id="plot.lmridge_+3A_abline">abline</code></td>
<td>
<p>Horizontal and vertical line to show minimum value of MSE and GCV value at certain value of biasing parameter <code class="reqn">K</code> on ridge and VIF trace respectively.</p>
</td></tr>
<tr><td><code id="plot.lmridge_+3A_...">...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Graphical way of selecting optimal value of biasing parameter <code class="reqn">K</code>. The biasing parameter is selected when coefficients becomes stable in case of ridge trace. In cae of VIF trace <code class="reqn">K</code> (ridge biasing parameter) can be selected for which VIF of each regressor near to one or value of <code class="reqn">K</code> at which GCV is minimum. If no argument is used then all traces of ridge coefficients will be displayed. A vertical and horizontal line will also be displayed on <em>ridge trace</em> graph to indicate minimum ridge MSE (among the all computed ridge MSE based on provided vector of <code class="reqn">K</code>) along with the value of respective biasing parameter <code class="reqn">K</code>. For VIF trace, vetical line shows minmum GCV value at certain value of biasing parameter <code class="reqn">K</code>.</p>


<h3>Value</h3>

<p>Nothing</p>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Hoerl, A. E., Kennard, R. W., and Baldwin, K. F. (1975). Ridge Regression: Some Simulation. <em>Communication in Statistics</em>, <strong>4</strong>, 105-123. <a href="https://doi.org/10.1080/03610927508827232">doi:10.1080/03610927508827232</a>.
</p>
<p>Hoerl, A. E. and Kennard, R. W., (1970). Ridge Regression: Biased Estimation of Nonorthogonal Problems. <em>Technometrics</em>, <strong>12</strong>, 55-67. <a href="https://doi.org/10.1080/00401706.1970.10488634">doi:10.1080/00401706.1970.10488634</a>.
</p>
<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>


<h3>See Also</h3>

<p>The ridge model fitting <code><a href="#topic+lmridge">lmridge</a></code>, ridge CV and GCV plots <code><a href="#topic+cv.plot">cv.plot</a></code>, variance bias trade-off plot <code><a href="#topic+bias.plot">bias.plot</a></code>, m-scale and isrm plots <code><a href="#topic+isrm.plot">isrm.plot</a></code>, ridge AIC and BIC plots <code><a href="#topic+info.plot">info.plot</a></code>, miscellaneous ridge plots <code><a href="#topic+rplots.plot">rplots.plot</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>mod &lt;- lmridge(y~., as.data.frame(Hald), K = seq(0, 0.15, 0.002))
## Ridge trace
plot(mod)
plot(mod, type = "ridge")

## VIF trace
plot(mod, type = "vif")
## Ridge trace without abline
plot(mod, type = "ridge", abline = FALSE)
</code></pre>

<hr>
<h2 id='predict.lmridge'>Predict method for Linear Ridge Model Fits</h2><span id='topic+predict'></span><span id='topic+predict.lmridge'></span>

<h3>Description</h3>

<p>Predicted values based on linear ridge regression model for scalar or vector values of biasing parameter <code class="reqn">K</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lmridge'
predict(object, newdata, na.action=na.pass, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.lmridge_+3A_object">object</code></td>
<td>
<p>An object of class &quot;lmridge&quot;.</p>
</td></tr>
<tr><td><code id="predict.lmridge_+3A_newdata">newdata</code></td>
<td>
<p>An optional data frame in which to look for variables with which to predict.</p>
</td></tr>
<tr><td><code id="predict.lmridge_+3A_na.action">na.action</code></td>
<td>
<p>Function determine what should be done with missing values in <code>newdata</code>.  The default is to predict <code>NA</code>.</p>
</td></tr>
<tr><td><code id="predict.lmridge_+3A_...">...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>predict.lmridge</code> function produces predicted values, obtained by evaluating the regression function in the frame <code>newdata</code> which defaults to model.frame (<code>object</code>). If <code>newdata</code> is omitted the predictions are based on the data used for the fit. In that case how cases with missing values in the original fit are handled is determined by the <code>na.action</code> argument of that fit. If <code>na.action = na.omit</code> omitted cases will not appear in the predictions, whereas if <code>na.action = na.exclude</code> they will appear (in predictions), with value NA.</p>


<h3>Value</h3>

<p><code>predict.lmridge</code> produces a vector of predictions or a matrix of predictions for scalar or vector values of biasing parameter.
</p>


<h3>Note</h3>

<p>Variables are first looked for in <code>newdata</code> and then are searched for in the usual way (which will include the environment of the formula used in the fit). A warning will be given if the variables found are not of the same length as those in the <code>newdata</code> if it was supplied.
</p>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Cule, E. and De lorio, M. (2012). A semi-Automatic method to guide the choice of ridge parameter in ridge regression.  <em>arXiv:1205.0686v1 [stat.AP]</em>.
</p>
<p>Hoerl, A. E., Kennard, R. W., and Baldwin, K. F. (1975). Ridge Regression: Some Simulation. <em>Communication in Statistics</em>, <strong>4</strong>, 105-123. <a href="https://doi.org/10.1080/03610927508827232">doi:10.1080/03610927508827232</a>.
</p>
<p>Hoerl, A. E. and Kennard, R. W., (1970). Ridge Regression: Biased Estimation of Nonorthogonal Problems. <em>Technometrics</em>, <strong>12</strong>, 55-67. <a href="https://doi.org/10.1080/00401706.1970.10488634">doi:10.1080/00401706.1970.10488634</a>.
</p>
<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>


<h3>See Also</h3>

<p>The ridge model fitting <code><a href="#topic+lmridge">lmridge</a></code>, ridge residuals <code><a href="#topic+residuals">residuals</a></code>, ridge PRESS <code><a href="#topic+press.lmridge">press.lmridge</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>mod &lt;- lmridge(y~., as.data.frame(Hald), K = seq(0, 0.2, 0.05))
predict(mod)
predict(mod, newdata = as.data.frame(Hald[1:5, -1]))
</code></pre>

<hr>
<h2 id='press.lmridge'>Predicted Residual Sum of Squares</h2><span id='topic+press.lmridge'></span><span id='topic+press'></span>

<h3>Description</h3>

<p>The <code>press.lmridge</code> function computes predicted residual sum of squares (PRESS) (see Allen, 1971).</p>


<h3>Usage</h3>

<pre><code class='language-R'>press(object, ...)
## S3 method for class 'lmridge'
press(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="press.lmridge_+3A_object">object</code></td>
<td>
<p>An object of class &quot;lmridge&quot;.</p>
</td></tr>
<tr><td><code id="press.lmridge_+3A_...">...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>All of the <code>n</code> leave-one-out predicted residual sum of squares is calculated by fitting full regression model by using, <code class="reqn">\sum\frac{\hat{e}_{i,k}}{1-\frac{1}{n}-H_{ii_{R,k}}}</code>, where <code class="reqn">H_{ii_{R,k}}</code> is hat matrix from ridge model fit, <code class="reqn">\hat{e_{i,k}}</code> is the ith residual at specific value of <code class="reqn">K</code>.
</p>


<h3>Value</h3>

<p>The <code>press.lmridge</code> produces a vector of PRESS or a matrix of PRESS for scalar or vector values of biasing parameter.
</p>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Allen, D. M. (1971). Mean Square Error of Prediction as a Criterion for Selecting Variables. <em>Technometrics</em>, <strong>13</strong>, 469-475. <a href="https://doi.org/10.1080/00401706.1971.10488811">doi:10.1080/00401706.1971.10488811</a>.
</p>
<p>Allen, D. M. (1974). The Relationship between Variable Selection and Data Augmentation and Method for Prediction. <em>Technometrics</em>, <strong>16</strong>, 125-127. <a href="https://doi.org/10.1080/00401706.1974.10489157">doi:10.1080/00401706.1974.10489157</a>.
</p>
<p>Hoerl, A. E., Kennard, R. W., and Baldwin, K. F. (1975). Ridge Regression: Some Simulation. <em>Communication in Statistics</em>, <strong>4</strong>, 105-123. <a href="https://doi.org/10.1080/03610927508827232">doi:10.1080/03610927508827232</a>.
</p>
<p>Hoerl, A. E. and Kennard, R. W., (1970). Ridge Regression: Biased Estimation of Nonorthogonal Problems. <em>Technometrics</em>, <strong>12</strong>, 55-67. <a href="https://doi.org/10.1080/00401706.1970.10488634">doi:10.1080/00401706.1970.10488634</a>.
</p>
<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>


<h3>See Also</h3>

<p>The ridge model fitting <code><a href="#topic+lmridge">lmridge</a></code>, ridge residual <code><a href="#topic+residuals">residuals</a></code>, ridge predicted value <code><a href="#topic+predict">predict</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mod &lt;- lmridge(y~., as.data.frame(Hald), K = seq(0, 0.5, 0.04))
press(mod)
</code></pre>

<hr>
<h2 id='residuals.lmridge'>Ridge Regression Residuals</h2><span id='topic+residuals.lmridge'></span><span id='topic+residuals'></span>

<h3>Description</h3>

<p>The <code>residuals</code> function computes the ridge residuals for scalar or vector value of biasing parameter <code class="reqn">K</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lmridge'
residuals(object, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals.lmridge_+3A_object">object</code></td>
<td>
<p>An object of class &quot;lmridge&quot;.</p>
</td></tr>
<tr><td><code id="residuals.lmridge_+3A_...">...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The generic functions <code>residuals</code> can be used to compute residuals object of linear ridge regression from <code>lmridge</code> function.
</p>


<h3>Value</h3>

<p>Returns a vector or a matrix of ridge residuals for scalar or vector value biasing parameter <code class="reqn">K</code> provided as argument to <code>lmridge</code> function.
</p>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Berk, R. (2008). <em>Statistical Learning from a Regression Perspective.</em> Springer.
</p>
<p>Hoerl, A. E., Kennard, R. W., and Baldwin, K. F. (1975). Ridge Regression: Some Simulation. <em>Communication in Statistics</em>, <strong>4</strong>, 105-123. <a href="https://doi.org/10.1080/03610927508827232">doi:10.1080/03610927508827232</a>.
</p>
<p>Hoerl, A. E. and Kennard, R. W., (1970). Ridge Regression: Biased Estimation of Nonorthogonal Problems. <em>Technometrics</em>, <strong>12</strong>, 55-67. <a href="https://doi.org/10.1080/00401706.1970.10488634">doi:10.1080/00401706.1970.10488634</a>.
</p>
<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>
<p>Lee, W. F. (1979). Model Estimation Using Ridge Regression with the Variance Normalization Criterion. <em>Master thesis, Department of Educational Foundation, Memorial University of Newfoundland.</em>
</p>


<h3>See Also</h3>

<p>The ridge mode fitting <code><a href="#topic+lmridge">lmridge</a></code>, ridge prediction <code><a href="#topic+predict">predict</a></code>, ridge PRESS values <code><a href="#topic+press">press</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>mod &lt;- lmridge(y~., as.data.frame(Hald), K = seq(0, 1, 0.2))
residuals(mod)
</code></pre>

<hr>
<h2 id='rplots.plot'>Miscellaneous Ridge Plots</h2><span id='topic+rplots.plot'></span>

<h3>Description</h3>

<p>Panel of three ridge related plots, df trace vs <code class="reqn">K</code>, RSS vs <code class="reqn">K</code> and PRESS vs <code class="reqn">K</code> for graphical judgement of optimal value of <code class="reqn">K</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>rplots.plot(x, abline = TRUE, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rplots.plot_+3A_x">x</code></td>
<td>
<p>An object of class &quot;lmridge&quot;</p>
</td></tr>
<tr><td><code id="rplots.plot_+3A_abline">abline</code></td>
<td>
<p>Vertical line to show minimum value of ridge PRESS at cartain value of biasing parameter <code class="reqn">K</code> on PRESS vs <code class="reqn">K</code> plot.</p>
</td></tr>
<tr><td><code id="rplots.plot_+3A_...">...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function <code>rplots.plot</code> can be used to plot the values of df vs <code class="reqn">K</code>, RSS vs <code class="reqn">K</code> and PRESS vs <code class="reqn">K</code> for scalar or vector values of biasing parameter <code class="reqn">K</code>. If no argument is used then a vertical line will be drawn on ridge PRESS plot to show the minimum value of PRESS at certain <code class="reqn">K</code>. The panel of these three plots can be helful in selecting the optimal value of biasing parameter <code class="reqn">K</code>.
</p>


<h3>Value</h3>

<p>nothing</p>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Allen, D. M. (1971). Mean Square Error of Prediction as a Criterion for Selecting Variables. <em>Technometrics</em>, <strong>13</strong>, 469-475. <a href="https://doi.org/10.1080/00401706.1971.10488811">doi:10.1080/00401706.1971.10488811</a>.
</p>
<p>Allen, D. M. (1974). The Relationship between Variable Selection and Data Augmentation and Method for Prediction. <em>Technometrics</em>, <strong>16</strong>, 125-127. <a href="https://doi.org/10.1080/00401706.1974.10489157">doi:10.1080/00401706.1974.10489157</a>.
</p>
<p>Berk, R. (2008). <em>Statistical Learning from a Regression Perspective.</em> Springer.
</p>
<p>Hoerl, A. E., Kennard, R. W., and Baldwin, K. F. (1975). Ridge Regression: Some Simulation. <em>Communication in Statistics</em>, <strong>4</strong>, 105-123. <a href="https://doi.org/10.1080/03610927508827232">doi:10.1080/03610927508827232</a>.
</p>
<p>Hoerl, A. E. and Kennard, R. W., (1970). Ridge Regression: Biased Estimation of Nonorthogonal Problems. <em>Technometrics</em>, <strong>12</strong>, 55-67. <a href="https://doi.org/10.1080/00401706.1970.10488634">doi:10.1080/00401706.1970.10488634</a>.
</p>
<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>


<h3>See Also</h3>

<p>The ridge model fitting <code><a href="#topic+lmridge">lmridge</a></code>, ridge CV and GCV plots <code><a href="#topic+cv.plot">cv.plot</a></code>, variance bias trade-off plot <code><a href="#topic+bias.plot">bias.plot</a></code>, m-scale and isrm plots <code><a href="#topic+isrm.plot">isrm.plot</a></code>, ridge AIC and BIC plots <code><a href="#topic+info.plot">info.plot</a></code>, ridge and VIF trace <code><a href="#topic+plot.lmridge">plot.lmridge</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>mod &lt;- lmridge(y~., as.data.frame(Hald), K = seq(0, 0.2, 0.005))
rplots.plot(mod)
rplots.plot(mod, abline = FALSE)
</code></pre>

<hr>
<h2 id='rstats1.lmridge'>Ordinary Ridge Regression Statistics 1</h2><span id='topic+rstats1'></span><span id='topic+rstats1.lmridge'></span><span id='topic+print.rstats1'></span>

<h3>Description</h3>

<p>The <code>rstats1</code> function computes the ordinary ridge related statistics such as variance, squared bias, MSE, R-squared and condition number (CN), etc. (see Lee, 1979; Kalivas and Palmer, 2014 &lt;<a href="https://doi.org/10.1002/cem.2555">doi:10.1002/cem.2555</a>&gt;)</p>


<h3>Usage</h3>

<pre><code class='language-R'>   rstats1(x, ...)
   ## S3 method for class 'lmridge'
rstats1(x, ...)
   ## S3 method for class 'rstats1'
print(x, digits = max(5,getOption("digits") - 5), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rstats1.lmridge_+3A_x">x</code></td>
<td>
<p>An object of class &quot;lmridge&quot; (for the <code>rstats1</code> or <code>print.rstats1.lmridge)</code></p>
</td></tr>
<tr><td><code id="rstats1.lmridge_+3A_digits">digits</code></td>
<td>
<p>Minimum number of significant digits to be used for most numbers.</p>
</td></tr>
<tr><td><code id="rstats1.lmridge_+3A_...">...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>rstats1</code> function computes the ordinary ridge regression related statistics which may help in selecting optimal value of biasing parameter <code class="reqn">K</code>. If value of <code class="reqn">K</code> is zero then these statistics are equivalent to the relevant OLS statistics.
</p>


<h3>Value</h3>

<p>Following are the ridge related statistics computed for given scalar or vector value of biasing parameter <code class="reqn">K</code> provided as argument to <code>lmridge</code> or <code>lmridgeEst</code> function.
</p>
<table>
<tr><td><code>var</code></td>
<td>
<p>Variance of ridge regression for given biasing parameter <code class="reqn">K</code>.</p>
</td></tr>
<tr><td><code>bias2</code></td>
<td>
<p>Squared bias of ridge regression for given biasing parameter <code class="reqn">K</code>.</p>
</td></tr>
<tr><td><code>mse</code></td>
<td>
<p>Total MSE value for given biasing parameter <code class="reqn">K</code>.</p>
</td></tr>
<tr><td><code>Fv</code></td>
<td>
<p>F-statistics value for testing of the significance of the ordinary ridge regression estimator computed for given biasing parameter <code class="reqn">K</code>.</p>
</td></tr>
<tr><td><code>rfact</code></td>
<td>
<p>Shrinkage factor <code class="reqn">\frac{\lambda_j}{\lambda_j+K}</code> for given biasing parameter <code class="reqn">K</code>.</p>
</td></tr>
<tr><td><code>R2</code></td>
<td>
<p>R-squared for given biasing parameter <code class="reqn">K</code>.</p>
</td></tr>
<tr><td><code>adjR2</code></td>
<td>
<p>Adjusted R-squared for given biasing parameter <code class="reqn">K</code>.</p>
</td></tr>
<tr><td><code>eigval</code></td>
<td>
<p>Eigenvalue of <code class="reqn">X'X</code> matrix for <code class="reqn">K=0</code>.</p>
</td></tr>
<tr><td><code>CN</code></td>
<td>
<p>Condition number after addition of biasing parameter in <code class="reqn">X'X</code> matrix.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Cule, E. and De lorio, M. (2012). A semi-Automatic method to guide the choice of ridge parameter in ridge regression.  <em>arXiv:1205.0686v1 [stat.AP]</em>.
</p>
<p>Hoerl, A. E., Kennard, R. W., and Baldwin, K. F. (1975). Ridge Regression: Some Simulation. <em>Communication in Statistics</em>, <strong>4</strong>, 105-123. <a href="https://doi.org/10.1080/03610927508827232">doi:10.1080/03610927508827232</a>.
</p>
<p>Hoerl, A. E. and Kennard, R. W., (1970). Ridge Regression: Biased Estimation of Nonorthogonal Problems. <em>Technometrics</em>, <strong>12</strong>, 55-67. <a href="https://doi.org/10.1080/00401706.1970.10488634">doi:10.1080/00401706.1970.10488634</a>.
</p>
<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>
<p>Kalivas, J. H., and Palmer, J. (2014). Characterizing multivariate calibration tradeoffs (bias, variance, selectivity, and sensitivity) to select model tuning parameters. <em>Journal of Chemometrics</em>, <strong>28</strong>(5), 347&ndash;357. <a href="https://doi.org/10.1002/cem.2555">doi:10.1002/cem.2555</a>.
</p>


<h3>See Also</h3>

<p>Ridge related statistics <code><a href="#topic+rstats2">rstats2</a></code>, the ridge model fitting <code><a href="#topic+lmridge">lmridge</a></code>, ridge var-cov matrix <code><a href="#topic+vcov">vcov</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Hald)
mod &lt;- lmridge(y~., data = as.data.frame(Hald), K = seq(0,0.2, 0.005) )

rstats1(mod)

## Getting only Ridge MSE
rstats1(mod)[3]

rstats1(mod)$mse
</code></pre>

<hr>
<h2 id='rstats2.lmridge'>Ordinary Ridge Regression Statistics 2</h2><span id='topic+rstats2'></span><span id='topic+rstats2.lmridge'></span><span id='topic+print.rstats2'></span>

<h3>Description</h3>

<p>The <code>rstats2</code> function computes the ordinary ridge related statistics such as <code class="reqn">Ck</code>, <code class="reqn">\sigma^2</code>, ridge degrees of freedom, effective degrees of freedom (EDF), and prediction residual error sum of squares PRESS statistics for scalar or vector value of biasing parameter <code class="reqn">K</code> (See Allen, 1974 &lt;<a href="https://doi.org/10.2307/1267500">doi:10.2307/1267500</a>&gt;; Lee, 1979; Hoerl and Kennard, 1970 &lt;<a href="https://doi.org/10.2307/1267351">doi:10.2307/1267351</a>&gt;).</p>


<h3>Usage</h3>

<pre><code class='language-R'>   rstats2(x, ...)
   ## S3 method for class 'lmridge'
rstats2(x, ...)
   ## S3 method for class 'rstats2'
print(x, digits = max(5,getOption("digits") - 5), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rstats2.lmridge_+3A_x">x</code></td>
<td>
<p>For the <code>rstats2</code> method, an object of class &quot;lmridge&quot;, i.e., a fitted model.</p>
</td></tr>
<tr><td><code id="rstats2.lmridge_+3A_digits">digits</code></td>
<td>
<p>Minimum number of significant digits to be used.</p>
</td></tr>
<tr><td><code id="rstats2.lmridge_+3A_...">...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>rstats2</code> function computes the ridge regression related different statistics which may help in selecting the optimal value of biasing parameter <code class="reqn">K</code>. If value of <code class="reqn">K</code> is zero then these statistics are equivalent to the relevant OLS statistics.
</p>


<h3>Value</h3>

<p>Following are ridge related statistics computed for given scalar or vector value of biasing parameter <code class="reqn">K</code> provided as argument to <code>lmridge</code> or <code>lmridgeEst</code> function.
</p>
<table>
<tr><td><code>CK</code></td>
<td>
<p><code class="reqn">Ck</code> similar to Mallows <code class="reqn">Cp</code> statistics for given biasing parameter <code class="reqn">K</code>.</p>
</td></tr>
<tr><td><code>dfridge</code></td>
<td>
<p>DF of ridge for given biasing parameter <code class="reqn">K</code>, i.e., <code class="reqn">Trace[Hat_{R,k}]</code>.</p>
</td></tr>
<tr><td><code>EP</code></td>
<td>
<p>Effective number of Parameters for given biasing parameter <code class="reqn">K</code>, i.e., <code class="reqn">Trace[2Hat_{R,k}-Hat_{R,k}t(Hat_{R,k})]</code>. </p>
</td></tr>
<tr><td><code>redf</code></td>
<td>
<p>Residual effective degrees of freedom for given biasing parameter <code class="reqn">K</code> from Hastie and Tibshirani, (1990), i.e., <code class="reqn">n-Trace[2Hat_{R,k}-Hat_{R,k}t(Hat_{R,k})]</code>.</p>
</td></tr>
<tr><td><code>EF</code></td>
<td>
<p>Effectiveness index for given biasing parameter <code class="reqn">K</code>, also called the ratio of reduction in total variance in the total squared bias by the ridge regression, i.e., <code class="reqn">EF=\frac{\sigma^2 trace(X'X)^{-1}-\sigma^2 trace(VIF_R)}{Bias^2(\hat{\beta}_R)}</code>.</p>
</td></tr>
<tr><td><code>ISRM</code></td>
<td>
<p>Quantification of concept of stable region proposed by Vinod and Ullah, 1981, i.e., <code class="reqn">ISRM_k=\sum_{j=1}^p (\frac{p(\frac{\lambda_j}{\lambda_j+k})^2}{\sum_{j=1}^p \frac{\lambda_j}{(\lambda_j+k)^2} \lambda_j}-1)^2</code>.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>m-scale for given value of biasing parameter proposed by Vinod (1976) alternative to plotting of the ridge coefficients, i.e., <code class="reqn">p-\sum_{j-1}^p \frac{\lambda_j}{\lambda_j+k}</code>.</p>
</td></tr>
<tr><td><code>PRESS</code></td>
<td>
<p>PRESS statistics for ridge regression introduced by Allen, 1971, 1974, i.e., <code class="reqn">PRESS_k=\sum_{i=1}^n e^2_{i,-i} </code> for scalar or vector value of biasing parameter <code class="reqn">K</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Allen, D. M. (1971). Mean Square Error of Prediction as a Criterion for Selecting Variables. <em>Technometrics</em>, <strong>13</strong>, 469-475. <a href="https://doi.org/10.1080/00401706.1971.10488811">doi:10.1080/00401706.1971.10488811</a>.
</p>
<p>Allen, D. M. (1974). The Relationship between Variable Selection and Data Augmentation and Method for Prediction. <em>Technometrics</em>, <strong>16</strong>, 125-127. <a href="https://doi.org/10.1080/00401706.1974.10489157">doi:10.1080/00401706.1974.10489157</a>.
</p>
<p>Cule, E. and De lorio, M. (2012). A semi-Automatic method to guide the choice of ridge parameter in ridge regression.  <em>arXiv:1205.0686v1 [stat.AP]</em>.
</p>
<p>Hastie, T. and Tibshirani, R. (1990). <em>Generalized Additive Models</em>. Chapman &amp; Hall.
</p>
<p>Hoerl, A. E., Kennard, R. W., and Baldwin, K. F. (1975). Ridge Regression: Some Simulation. <em>Communication in Statistics</em>, <strong>4</strong>, 105-123. <a href="https://doi.org/10.1080/03610927508827232">doi:10.1080/03610927508827232</a>.
</p>
<p>Hoerl, A. E. and Kennard, R. W., (1970). Ridge Regression: Biased Estimation of Nonorthogonal Problems. <em>Technometrics</em>, <strong>12</strong>, 55-67. <a href="https://doi.org/10.1080/00401706.1970.10488634">doi:10.1080/00401706.1970.10488634</a>.
</p>
<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>
<p>Kalivas, J. H., and Palmer, J. (2014). Characterizing Multivariate Calibration Tradeoffs (Bias, Variance, Selectivity, and Sensitivity) to Select Model Tuning Parameters. <em>Journal of Chemometrics</em>, <strong>28</strong>(5), 347&ndash;357. <a href="https://doi.org/10.1002/cem.2555">doi:10.1002/cem.2555</a>.
</p>
<p>Lee, W. F. (1979). Model Estimation Using Ridge Regression with the Variane Normalization Criterion. <em>Master thesis, Department of Educational Foundation Memorial University of Newfoundland.</em>
</p>


<h3>See Also</h3>

<p>Ridge related statistics <code><a href="#topic+rstats1">rstats1</a></code>, ridge model fitting <code><a href="#topic+lmridge">lmridge</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Hald)
mod &lt;- lmridge(y~., data=as.data.frame(Hald), K = seq(0,0.2, 0.001) )

rstats2(mod)

</code></pre>

<hr>
<h2 id='summary.lmridge'>Summarizing Linear Ridge Regression Fits</h2><span id='topic+summary.lmridge'></span><span id='topic+print.summary.lmridge'></span>

<h3>Description</h3>

<p>The <code>summary</code> method for class &quot;lmridge&quot; for scalar or vector biasing parameter <code class="reqn">K</code> (Cule and De lorio, 2012).</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lmridge'
summary(object, ...)
## S3 method for class 'summary.lmridge'
print(x, digits = max(3, getOption("digits") - 3),
           signif.stars = getOption("show.signif.stars"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.lmridge_+3A_object">object</code></td>
<td>
<p>An &quot;lmridge&quot; object, typically generated by a call to <code>lmridge</code>.</p>
</td></tr>
<tr><td><code id="summary.lmridge_+3A_x">x</code></td>
<td>
<p>An object of class <code>summary.lmridge</code> for the <code>print.summary.lmridge</code>.</p>
</td></tr>
<tr><td><code id="summary.lmridge_+3A_signif.stars">signif.stars</code></td>
<td>
<p>logical: if <code>TRUE</code>, <em>p</em>-values are additionally encoded visually as  <code>significance starts</code> in order to help scanning of long coefficient tables. It default to the <code>show.signif.stars</code> slot of <code>options</code>.</p>
</td></tr>
<tr><td><code id="summary.lmridge_+3A_digits">digits</code></td>
<td>
<p>The number of significant digits to use when printing.</p>
</td></tr>
<tr><td><code id="summary.lmridge_+3A_...">...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>print.summary.lmridge</code> tries to be smart about formatting the coefficients, standard errors etc. and additionally gives 'significance stars' if <code>signif.stars</code> is <code>TRUE</code>.</p>


<h3>Value</h3>

<p>The function <code>summary</code> computes and returns a list of summary statistics of the fitted linear ridge regression model for scalar or vector value biasing parameter <code class="reqn">K</code> given as argument in <code>lmridge</code> function. All summary information can be called using list object <code>summaries</code>.
</p>
<table>
<tr><td><code>coefficients</code></td>
<td>
<p>A <code class="reqn">p \times 5</code> matrix with columns for the scaled estimated, descaled estimated coefficients, scaled standard error, scaled <em>t</em>-statistics, and corresponding <em>p</em>-value (two-tailed). The Intercept term is computed by the relation <code class="reqn">\hat{\beta}_{R_{0K}}=\overline{y}-\sum_{j=1}^{p}\overline{X}_j \hat{\beta}_{R_{0K}}</code>. The standard error of intercept term is computed as, <code class="reqn">SE(\hat{\beta}_{R_{0K}})=\sqrt{Var(\overline{y}) +\overline{X}_j^2 diag[Cov(\hat{\beta}_{R_{0K}})]}</code>.</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>
<p>Ridge related statistics of <em>R</em>-squared, adjusted <em>R</em>-squared, <em>F</em>-statistics for testing of coefficients, AIC and BIC values for given biasing parameter <code class="reqn">K</code>.</p>
</td></tr>
<tr><td><code>rmse1</code></td>
<td>
<p>Minimum MSE value for given biasing parameter <code class="reqn">K</code>.</p>
</td></tr>
<tr><td><code>rmse2</code></td>
<td>
<p>Value of <code class="reqn">K</code> at which MSE is minimum.</p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p>Value of given biasing parameter.</p>
</td></tr>
<tr><td><code>df1</code></td>
<td>
<p>Numerator degrees of freedom for p-value of F-statistics.</p>
</td></tr>
<tr><td><code>df2</code></td>
<td>
<p>Denominator degrees of freedom for p-value of F-statistics.</p>
</td></tr>
<tr><td><code>fpvalue</code></td>
<td>
<p>p-value for each F-statistics.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Cule, E. and De lorio, M. (2012). A semi-Automatic method to guide the choice of ridge parameter in ridge regression.  <em>arXiv:1205.0686v1 [stat.AP]</em>.
</p>
<p>Hoerl, A. E., Kennard, R. W., and Baldwin, K. F. (1975). Ridge Regression: Some Simulation. <em>Communication in Statistics</em>, <strong>4</strong>, 105-123. <a href="https://doi.org/10.1080/03610927508827232">doi:10.1080/03610927508827232</a>.
</p>
<p>Hoerl, A. E. and Kennard, R. W., (1970). Ridge Regression: Biased Estimation of Nonorthogonal Problems. <em>Technometrics</em>, <strong>12</strong>, 55-67. <a href="https://doi.org/10.1080/00401706.1970.10488634">doi:10.1080/00401706.1970.10488634</a>.
</p>
<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>


<h3>See Also</h3>

<p>The ridge model fitting <code><a href="#topic+lmridge">lmridge</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>mod &lt;- lmridge(y~., as.data.frame(Hald), K = c(0, 0.0132, 0.1))
summary(mod)

## coefficients for first biasing parameter
summary(mod)$summaries[[1]]$coefficients
summary(mod)$summaries[[1]][[1]]

## ridge related statistics from summary function
summary(mod)$summaries[[1]]$stats

## Ridge F-test's p-value
summary(mod)$summaries[[1]]$fpvalue
</code></pre>

<hr>
<h2 id='vcov.lmridge'>Variance-Covariance Matrix for Fitted Ridge Model</h2><span id='topic+vcov.lmridge'></span><span id='topic+vcov'></span>

<h3>Description</h3>

<p>The <code>vcov</code> function computes the variance-covariance matrix for the estimates of linear ridge regression model.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lmridge'
vcov(object, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcov.lmridge_+3A_object">object</code></td>
<td>
<p>For <code>VCOV</code> method, an object of class &quot;lmridge&quot;, i.e., a fitted model.</p>
</td></tr>
<tr><td><code id="vcov.lmridge_+3A_...">...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>vcov</code> function computes variance-covariance matrix for scalar or vector value of biasing parameter <code class="reqn">K</code> provided as argument to <code>lmridge</code> function.</p>


<h3>Value</h3>

<p>A list of matrix of estimated covariances in the linear ridge regression model for scalar or vector biasing parameter <code>K</code>K is produced. Each list element has row and column names corresponding to the parameter names given by the <code>coef(mod)</code>. List items are named correspond to values of biasing parameter <code class="reqn">K</code>.</p>


<h3>Note</h3>

<p>Covariance will be without intercept term, as intercept term is not penalized in ridge regression.</p>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Brown, G.W. and Beattie, B.R. (1975). Improving Estimates of Economic Parameters by use of Ridge Regression with Production Function Applications. <em>American Journal of Agricultural Economics</em>, 57(1), 21-32. <a href="https://doi.org/10.2307/1238836">doi:10.2307/1238836</a>.
</p>


<h3>See Also</h3>

<p>The ridge model fitting <code><a href="#topic+lmridge">lmridge</a></code>, ridge VIF values <code><a href="#topic+vif">vif</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Hald)
mod&lt;- lmridge(y~., data=as.data.frame(Hald), scaling="sc", K=seq(0,1,.2) )

vcov.lmridge(mod)
vcov(mod)
</code></pre>

<hr>
<h2 id='vif.lmridge'>Variance Inflation Fator for Linear Ridge Regression</h2><span id='topic+vif'></span><span id='topic+vif.lmridge'></span>

<h3>Description</h3>

<p>Computes VIF values for each scalar or vector value of biasing parameter <code class="reqn">K</code> (Marquardt, 1970).</p>


<h3>Usage</h3>

<pre><code class='language-R'>vif(x, ...)
## S3 method for class 'lmridge'
vif(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vif.lmridge_+3A_x">x</code></td>
<td>
<p>For VIF method, an object of class &quot;lmridge&quot;, i.e., a fitted model.</p>
</td></tr>
<tr><td><code id="vif.lmridge_+3A_...">...</code></td>
<td>
<p>Not presently used in this implementation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>vif.lmridge</code> function computes VIF value for each regressor in data set after addition of biasing parameter as argument to <code>lmridge</code> function. The VIF is computed using <code class="reqn">(X'X+kI)^{-1}X'X(X'X+kI)^{-1}</code>, given by Marquardt, (1970).
</p>


<h3>Value</h3>

<p>The <code>vif</code> function returns a matrix of VIF values for each regressor after adding scalar or vector biasing parameter <code class="reqn">K</code> to <code class="reqn">X'X</code> matrix. The column of returned matrix indicates regressors name and row indicates value of each biasing parameter <code class="reqn">K</code> provided as argument to <code>lmridge</code> function.
</p>


<h3>Author(s)</h3>

<p>Muhammad Imdad Ullah, Muhammad Aslam</p>


<h3>References</h3>

<p>Fox, J. and Monette, G. (1992). Generalized Collinearity Diagnostics. <em>JASA</em>, <strong>87</strong>, 178&ndash;183.
</p>
<p>Imdad, M. U. <em>Addressing Linear Regression Models with Correlated Regressors: Some Package Development in R</em> (Doctoral Thesis, Department of Statistics, Bahauddin Zakariya University, Multan, Pakistan), 2017.
</p>
<p>Marquardt, D. (1970). Generalized Inverses, Ridge Regression, Biased Linear Estimation, and Nonlinear Estimation. <em>Technometrics</em>, <strong>12</strong>(3), 591&ndash;612.
</p>


<h3>See Also</h3>

<p>The ridge model fitting <code><a href="#topic+lmridge">lmridge</a></code>, ridge Var-Cov matrix <code><a href="#topic+vcov">vcov</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Hald)
mod &lt;- lmridge(y~., data = as.data.frame(Hald), scaling = "sc", K = seq(0,1,.2) )
vif(mod)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
