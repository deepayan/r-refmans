<!DOCTYPE html><html><head><title>Help for package TeachNet</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {TeachNet}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#TeachNet-package'>
<p>Fit neural networks with up to 2 hidden layers and one output neuron</p></a></li>
<li><a href='#accuracy.me'>
<p>Computes accuracy</p></a></li>
<li><a href='#computeGrad1'>
<p>Computes a gradient</p></a></li>
<li><a href='#computeGrad2'>
<p>Computes a gradient</p></a></li>
<li><a href='#computeOutput1'>
<p>Computes output</p></a></li>
<li><a href='#computeOutput2'>
<p>Computes output</p></a></li>
<li><a href='#confusion'>
<p>Computes confusion matrix</p></a></li>
<li><a href='#createWeights1'>
<p>Creates random weights</p></a></li>
<li><a href='#createWeights2'>
<p>Creates random weights</p></a></li>
<li><a href='#crossEntropy'>
<p>Cross entropy</p></a></li>
<li><a href='#find.Threshold'>
<p>Finds best threshold</p></a></li>
<li><a href='#fitTeachNet1'>
<p>One step in backpropagation</p></a></li>
<li><a href='#fitTeachNet2'>
<p>One step in backpropagation</p></a></li>
<li><a href='#is.acct'>
<p>Checks for correct input</p></a></li>
<li><a href='#is.data'>
<p>Checks for correct input</p></a></li>
<li><a href='#is.decay'>
<p>Checks for correct input</p></a></li>
<li><a href='#is.err'>
<p>Checks for correct input</p></a></li>
<li><a href='#is.learn'>
<p>Checks for correct input</p></a></li>
<li><a href='#is.numberOfNeurons'>
<p>Checks for correct input</p></a></li>
<li><a href='#is.sample'>
<p>Checks for correct input</p></a></li>
<li><a href='#is.sampleLeng'>
<p>Checks for correct input</p></a></li>
<li><a href='#is.stepMax'>
<p>Checks for correct input</p></a></li>
<li><a href='#is.thres.error'>
<p>Checks for correct input</p></a></li>
<li><a href='#logistic'>
<p>Logistic function</p></a></li>
<li><a href='#logistic.differential'>
<p>Differential of logistic function</p></a></li>
<li><a href='#predict.Weights'>
<p>Computes prediction</p></a></li>
<li><a href='#predict.Weights2'>
<p>Computes prediction</p></a></li>
<li><a href='#squaredError'>
<p>Computes squared error</p></a></li>
<li><a href='#sumCrossEntropy'>
<p>Sums up cross entropy</p></a></li>
<li><a href='#sumSquaredError'>
<p>Sums up squared error</p></a></li>
<li><a href='#TeachNet'>
<p>Fits the neural network</p></a></li>
<li><a href='#transformPrediction'>
<p>Transforms prediction</p></a></li>
<li><a href='#Weights-class'><p>Weights objects</p></a></li>
<li><a href='#Weights2-class'><p>Weights2 objects</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Fits Neural Networks to Learn About Backpropagation</td>
</tr>
<tr>
<td>Version:</td>
<td>0.7.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-11-20</td>
</tr>
<tr>
<td>Author:</td>
<td>Georg Steinbuss</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Georg Steinbuss &lt;gspam@steinbuss.de&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>methods</td>
</tr>
<tr>
<td>Description:</td>
<td>Can fit neural networks with up to two hidden layer and two different error functions. Also able to handle a weight decay. But just able to compute one output neuron and very slow. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-11-27 14:47:27 UTC; georg</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-11-27 16:30:11 UTC</td>
</tr>
</table>
<hr>
<h2 id='TeachNet-package'>
Fit neural networks with up to 2 hidden layers and one output neuron
</h2><span id='topic+TeachNet-package'></span>

<h3>Description</h3>

<p>Can fit neural networks with up to two hidden layers and two different error functions. But just able to compute one output neuron. Also able to handle a weight decay.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> TeachNet</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.7</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2013-11-20</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 2)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>The function TeachNet trains the neural network and also does some testing at the end. It's also possible to get the final weights returned. In the beginning the weights are initialized with a standard normal distribution. But this package is due to its very slow code just to understand the backpropagation algorithm. A good package for real training of neural networks is for example 'nnet'. 
</p>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>
<p>Maintainer: Who to complain to &lt;gspam@steinbuss.de&gt;
</p>


<h3>References</h3>

<p>Predicting credit default using neural networks, Georg Steinbuss 2013
</p>

<hr>
<h2 id='accuracy.me'>
Computes accuracy
</h2><span id='topic+accuracy.me'></span>

<h3>Description</h3>

<p>For a given observation and prediction this function computes the accuracy of the prediction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>accuracy.me(obs, predict, thres = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="accuracy.me_+3A_obs">obs</code></td>
<td>

<p>The observations
</p>
</td></tr>
<tr><td><code id="accuracy.me_+3A_predict">predict</code></td>
<td>

<p>The predictions for the observations
</p>
</td></tr>
<tr><td><code id="accuracy.me_+3A_thres">thres</code></td>
<td>

<p>A threshold up to which a prediction is class 0 or 1. A value from 0 to 1. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a 1 x 3 matrix with the percentage of observations with class zero, with class one and last the accuracy of the prediction.
</p>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>


<h3>See Also</h3>

<p><a href="#topic+confusion">confusion</a>
</p>

<hr>
<h2 id='computeGrad1'>
Computes a gradient
</h2><span id='topic+computeGrad1'></span>

<h3>Description</h3>

<p>This function computes the gradient for a one hidden layer network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>computeGrad1(x, y, I, H, weights, f, f_d, m_f)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="computeGrad1_+3A_x">x</code></td>
<td>

<p>properties of observation
</p>
</td></tr>
<tr><td><code id="computeGrad1_+3A_y">y</code></td>
<td>

<p>characteristic of observation (zero or one)
</p>
</td></tr>
<tr><td><code id="computeGrad1_+3A_i">I</code></td>
<td>

<p>numbers of input neurons
</p>
</td></tr>
<tr><td><code id="computeGrad1_+3A_h">H</code></td>
<td>

<p>numbers of hidden neurons
</p>
</td></tr>
<tr><td><code id="computeGrad1_+3A_weights">weights</code></td>
<td>

<p>the weights with that the gradient should be computed
</p>
</td></tr>
<tr><td><code id="computeGrad1_+3A_f">f</code></td>
<td>

<p>the activation function of the neural network
</p>
</td></tr>
<tr><td><code id="computeGrad1_+3A_f_d">f_d</code></td>
<td>

<p>the derivative of the activation function 
</p>
</td></tr>
<tr><td><code id="computeGrad1_+3A_m_f">m_f</code></td>
<td>

<p>the function for the interim value m. It is two times the output of the network minus the observed characteristic.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A Weights class with the gradient parts
</p>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>


<h3>See Also</h3>

<p><a href="#topic+Weights-class">Weights-class</a>
<a href="#topic+computeGrad2">computeGrad2</a>
</p>

<hr>
<h2 id='computeGrad2'>
Computes a gradient
</h2><span id='topic+computeGrad2'></span>

<h3>Description</h3>

<p>This function computes the gradient for a two hidden layer network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>computeGrad2(x, y, I, M, H, weights, f, f_d, m_f)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="computeGrad2_+3A_x">x</code></td>
<td>

<p>properties of observation
</p>
</td></tr>
<tr><td><code id="computeGrad2_+3A_y">y</code></td>
<td>

<p>characteristic of observation (zero or one)
</p>
</td></tr>
<tr><td><code id="computeGrad2_+3A_i">I</code></td>
<td>

<p>numbers of input neurons
</p>
</td></tr>
<tr><td><code id="computeGrad2_+3A_m">M</code></td>
<td>

<p>number of neurons in first hidden layer
</p>
</td></tr>
<tr><td><code id="computeGrad2_+3A_h">H</code></td>
<td>

<p>number of neurons in second hidden layer
</p>
</td></tr>
<tr><td><code id="computeGrad2_+3A_weights">weights</code></td>
<td>

<p>the weights with that the gradient should be computed
</p>
</td></tr>
<tr><td><code id="computeGrad2_+3A_f">f</code></td>
<td>

<p>the activation function of the neural network
</p>
</td></tr>
<tr><td><code id="computeGrad2_+3A_f_d">f_d</code></td>
<td>

<p>the derivative of the activation function 
</p>
</td></tr>
<tr><td><code id="computeGrad2_+3A_m_f">m_f</code></td>
<td>

<p>the function for the interim value m. It is two times the output of the network minus the observed characteristic.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A Weights2 class with the gradient parts
</p>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>


<h3>See Also</h3>

<p><a href="#topic+Weights-class">Weights-class</a>
<a href="#topic+computeGrad2">computeGrad2</a>
</p>

<hr>
<h2 id='computeOutput1'>
Computes output
</h2><span id='topic+computeOutput1'></span>

<h3>Description</h3>

<p>Computes output (prediction) for a one hidden layer network for one observation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>computeOutput1(x, weights)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="computeOutput1_+3A_x">x</code></td>
<td>

<p>properties of observation
</p>
</td></tr>
<tr><td><code id="computeOutput1_+3A_weights">weights</code></td>
<td>

<p>weights of the neural network
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a single numeric value.
</p>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='computeOutput2'>
Computes output
</h2><span id='topic+computeOutput2'></span>

<h3>Description</h3>

<p>Computes output (prediction) for a two hidden layers network for one observation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>computeOutput2(x, weights)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="computeOutput2_+3A_x">x</code></td>
<td>

<p>properties of observation
</p>
</td></tr>
<tr><td><code id="computeOutput2_+3A_weights">weights</code></td>
<td>

<p>weights of the neural network
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a single numeric value.
</p>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='confusion'>
Computes confusion matrix
</h2><span id='topic+confusion'></span>

<h3>Description</h3>

<p>Computes confusion matrix for a specific threshold
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confusion(pred, obs, threshold = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confusion_+3A_pred">pred</code></td>
<td>

<p>the prediction
</p>
</td></tr>
<tr><td><code id="confusion_+3A_obs">obs</code></td>
<td>

<p>the observation
</p>
</td></tr>
<tr><td><code id="confusion_+3A_threshold">threshold</code></td>
<td>

<p>A threshold up to which a prediction is class 0 or 1. A value from 0 to 1
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a confusion matrix
</p>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='createWeights1'>
Creates random weights
</h2><span id='topic+createWeights1'></span>

<h3>Description</h3>

<p>Creates random weights for a single hidden layer network
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createWeights1(I, H)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="createWeights1_+3A_i">I</code></td>
<td>

<p>number of input neurons
</p>
</td></tr>
<tr><td><code id="createWeights1_+3A_h">H</code></td>
<td>

<p>number of hidden neurons
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a S4 class object Weights
</p>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>


<h3>See Also</h3>

<p><a href="#topic+Weights-class">Weights-class</a>
</p>

<hr>
<h2 id='createWeights2'>
Creates random weights
</h2><span id='topic+createWeights2'></span>

<h3>Description</h3>

<p>Creates random weights for a two hidden layers network
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createWeights2(I, H)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="createWeights2_+3A_i">I</code></td>
<td>

<p>number of input neurons
</p>
</td></tr>
<tr><td><code id="createWeights2_+3A_h">H</code></td>
<td>

<p>vector with first element the number of hidden neurons in the first hidden layer second element for the second hidden layer
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a S4 class object Weights2
</p>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>


<h3>See Also</h3>

<p><a href="#topic+Weights2-class">Weights2-class</a>
</p>

<hr>
<h2 id='crossEntropy'>
Cross entropy
</h2><span id='topic+crossEntropy'></span>

<h3>Description</h3>

<p>The error function cross entropy
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crossEntropy(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crossEntropy_+3A_x">x</code></td>
<td>

<p>properties of observation
</p>
</td></tr>
<tr><td><code id="crossEntropy_+3A_y">y</code></td>
<td>

<p>characteristic of observation (zero or one)
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a single numeric value
</p>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>


<h3>See Also</h3>

<p>linksquaredError
</p>

<hr>
<h2 id='find.Threshold'>
Finds best threshold
</h2><span id='topic+find.Threshold'></span>

<h3>Description</h3>

<p>Finds the best threshold to transform probabilities in classes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find.Threshold(obs, stepsize = 0.1, predict)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find.Threshold_+3A_obs">obs</code></td>
<td>

<p>class of observation
</p>
</td></tr>
<tr><td><code id="find.Threshold_+3A_stepsize">stepsize</code></td>
<td>

<p>in which step size the threshold is raised
</p>
</td></tr>
<tr><td><code id="find.Threshold_+3A_predict">predict</code></td>
<td>

<p>prediction of network
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='fitTeachNet1'>
One step in backpropagation
</h2><span id='topic+fitTeachNet1'></span>

<h3>Description</h3>

<p>One step in the backpropagation algorithm for a one hidden layer network
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fitTeachNet1(data, weights, hidden.structure, learning.rate, f, f_d, decay, m_f, er)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitTeachNet1_+3A_data">data</code></td>
<td>

<p>the data set
</p>
</td></tr>
<tr><td><code id="fitTeachNet1_+3A_weights">weights</code></td>
<td>

<p>current weights
</p>
</td></tr>
<tr><td><code id="fitTeachNet1_+3A_hidden.structure">hidden.structure</code></td>
<td>

<p>the number of neurons in the hidden layer
</p>
</td></tr>
<tr><td><code id="fitTeachNet1_+3A_learning.rate">learning.rate</code></td>
<td>

<p>rate by which factor for backpropagation gets smaller
</p>
</td></tr>
<tr><td><code id="fitTeachNet1_+3A_f">f</code></td>
<td>

<p>activation function
</p>
</td></tr>
<tr><td><code id="fitTeachNet1_+3A_f_d">f_d</code></td>
<td>

<p>derivative of activation function
</p>
</td></tr>
<tr><td><code id="fitTeachNet1_+3A_decay">decay</code></td>
<td>

<p>value of weight decay
</p>
</td></tr>
<tr><td><code id="fitTeachNet1_+3A_m_f">m_f</code></td>
<td>

<p>interim value m
</p>
</td></tr>
<tr><td><code id="fitTeachNet1_+3A_er">er</code></td>
<td>

<p>error function
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns new the weight after gradient update 
</p>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='fitTeachNet2'>
One step in backpropagation
</h2><span id='topic+fitTeachNet2'></span>

<h3>Description</h3>

<p>One step in the backpropagation algorithm for a two hidden layers network
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fitTeachNet2(data, weights, hidden.structure, learning.rate, f, f_d, decay, m_f, er)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitTeachNet2_+3A_data">data</code></td>
<td>

<p>the data set
</p>
</td></tr>
<tr><td><code id="fitTeachNet2_+3A_weights">weights</code></td>
<td>

<p>current weights
</p>
</td></tr>
<tr><td><code id="fitTeachNet2_+3A_hidden.structure">hidden.structure</code></td>
<td>

<p>vector with first element the number of hidden neurons in the first hidden layer second element for the second hidden layer
</p>
</td></tr>
<tr><td><code id="fitTeachNet2_+3A_learning.rate">learning.rate</code></td>
<td>

<p>rate by which factor for backpropagation gets smaller
</p>
</td></tr>
<tr><td><code id="fitTeachNet2_+3A_f">f</code></td>
<td>

<p>activation function
</p>
</td></tr>
<tr><td><code id="fitTeachNet2_+3A_f_d">f_d</code></td>
<td>

<p>derivative of activation function
</p>
</td></tr>
<tr><td><code id="fitTeachNet2_+3A_decay">decay</code></td>
<td>

<p>value of weight decay
</p>
</td></tr>
<tr><td><code id="fitTeachNet2_+3A_m_f">m_f</code></td>
<td>

<p>interim value m
</p>
</td></tr>
<tr><td><code id="fitTeachNet2_+3A_er">er</code></td>
<td>

<p>error function
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns the new weight after gradient update 
</p>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='is.acct'>
Checks for correct input
</h2><span id='topic+is.acct'></span>

<h3>Description</h3>

<p>Checks for correct input
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.acct(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.acct_+3A_x">x</code></td>
<td>

<p>activation function
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='is.data'>
Checks for correct input
</h2><span id='topic+is.data'></span>

<h3>Description</h3>

<p>Checks for correct input
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.data(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.data_+3A_data">data</code></td>
<td>

<p>data frame
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='is.decay'>
Checks for correct input
</h2><span id='topic+is.decay'></span>

<h3>Description</h3>

<p>Checks for correct input
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.decay(x, tol = .Machine$double.eps^0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.decay_+3A_x">x</code></td>
<td>

<p>decay
</p>
</td></tr>
<tr><td><code id="is.decay_+3A_tol">tol</code></td>
<td>

<p>tolerance
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='is.err'>
Checks for correct input
</h2><span id='topic+is.err'></span>

<h3>Description</h3>

<p>Checks for correct input
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.err(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.err_+3A_x">x</code></td>
<td>

<p>error function
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='is.learn'>
Checks for correct input
</h2><span id='topic+is.learn'></span>

<h3>Description</h3>

<p>Checks for correct input
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.learn(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.learn_+3A_x">x</code></td>
<td>

<p>learning rate
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='is.numberOfNeurons'>
Checks for correct input
</h2><span id='topic+is.numberOfNeurons'></span>

<h3>Description</h3>

<p>Checks for correct input
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.numberOfNeurons(x, tol = .Machine$double.eps^0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.numberOfNeurons_+3A_x">x</code></td>
<td>

<p>Number of neurons
</p>
</td></tr>
<tr><td><code id="is.numberOfNeurons_+3A_tol">tol</code></td>
<td>

<p>tolerance
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='is.sample'>
Checks for correct input
</h2><span id='topic+is.sample'></span>

<h3>Description</h3>

<p>Checks for correct input
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.sample(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.sample_+3A_x">x</code></td>
<td>

<p>Boolean value for sameSample
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='is.sampleLeng'>
Checks for correct input
</h2><span id='topic+is.sampleLeng'></span>

<h3>Description</h3>

<p>Checks for correct input
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.sampleLeng(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.sampleLeng_+3A_x">x</code></td>
<td>

<p>sample length
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='is.stepMax'>
Checks for correct input
</h2><span id='topic+is.stepMax'></span>

<h3>Description</h3>

<p>Checks for correct input
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.stepMax(x, tol = .Machine$double.eps^0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.stepMax_+3A_x">x</code></td>
<td>

<p>maximal number of steps
</p>
</td></tr>
<tr><td><code id="is.stepMax_+3A_tol">tol</code></td>
<td>

<p>tolerance
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='is.thres.error'>
Checks for correct input
</h2><span id='topic+is.thres.error'></span>

<h3>Description</h3>

<p>Checks for correct input
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.thres.error(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.thres.error_+3A_x">x</code></td>
<td>

<p>threshold.TotalError
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='logistic'>
Logistic function
</h2><span id='topic+logistic'></span>

<h3>Description</h3>

<p>Computes the value of the logistic function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logistic(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logistic_+3A_x">x</code></td>
<td>

<p>Input for logistic function
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='logistic.differential'>
Differential of logistic function
</h2><span id='topic+logistic.differential'></span>

<h3>Description</h3>

<p>Computes value for the differential of the logistic function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logistic.differential(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logistic.differential_+3A_x">x</code></td>
<td>

<p>Input for differential of logistic function
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='predict.Weights'>
Computes prediction
</h2><span id='topic+predict.Weights'></span>

<h3>Description</h3>

<p>This function computes for a given data set and weights of a one hidden layer network, a prediction from a TeachNet neural network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Weights'
predict(object, newdata, delete.firstColumn=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.Weights_+3A_object">object</code></td>
<td>

<p>The Weights object TeachNet returned after training.
</p>
</td></tr>
<tr><td><code id="predict.Weights_+3A_newdata">newdata</code></td>
<td>

<p>The data set you which to predict. Has to have the same variables as the used training data set (except for the class variable) and has to be scaled (Z-Scores)!
</p>
</td></tr>
<tr><td><code id="predict.Weights_+3A_delete.firstcolumn">delete.firstColumn</code></td>
<td>

<p>When class variable is first column, set to TRUE 
</p>
</td></tr>
<tr><td><code id="predict.Weights_+3A_...">...</code></td>
<td>

<p>additional arguments affecting the predictions produced
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a vector with the predictions of TeachNet
</p>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>


<h3>See Also</h3>

<p><a href="#topic+predict.Weights2">predict.Weights2</a>,
<a href="#topic+Weights-class">Weights-class</a>,
<a href="#topic+Weights2-class">Weights2-class</a>
</p>

<hr>
<h2 id='predict.Weights2'>
Computes prediction
</h2><span id='topic+predict.Weights2'></span>

<h3>Description</h3>

<p>This function computes for a given data set and weights of a two hidden layer network, a prediction from a TeachNet neural network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Weights2'
predict(object, newdata, delete.firstColumn=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.Weights2_+3A_object">object</code></td>
<td>

<p>The Weights2 object TeachNet returned after training.
</p>
</td></tr>
<tr><td><code id="predict.Weights2_+3A_newdata">newdata</code></td>
<td>

<p>The data set you which to predict. Has to have the same variables as the used training data set (except for the class variable) and has to be scaled (Z-Scores)!
</p>
</td></tr>
<tr><td><code id="predict.Weights2_+3A_delete.firstcolumn">delete.firstColumn</code></td>
<td>

<p>When class variable is first column, set to TRUE 
</p>
</td></tr>
<tr><td><code id="predict.Weights2_+3A_...">...</code></td>
<td>

<p>additional arguments affecting the predictions produced
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a vector with the predictions of TeachNet
</p>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>


<h3>See Also</h3>

<p><a href="#topic+predict.Weights">predict.Weights</a>,
<a href="#topic+Weights-class">Weights-class</a>,
<a href="#topic+Weights2-class">Weights2-class</a>
</p>

<hr>
<h2 id='squaredError'>
Computes squared error
</h2><span id='topic+squaredError'></span>

<h3>Description</h3>

<p>Computes squared difference between two values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>squaredError(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="squaredError_+3A_x">x</code></td>
<td>

<p>value 1
</p>
</td></tr>
<tr><td><code id="squaredError_+3A_y">y</code></td>
<td>

<p>value 2
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='sumCrossEntropy'>
Sums up cross entropy
</h2><span id='topic+sumCrossEntropy'></span>

<h3>Description</h3>

<p>Computes the full value of the cross entropy for TeachNet
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sumCrossEntropy(weights, data, h2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sumCrossEntropy_+3A_weights">weights</code></td>
<td>

<p>current weights
</p>
</td></tr>
<tr><td><code id="sumCrossEntropy_+3A_data">data</code></td>
<td>

<p>data frame
</p>
</td></tr>
<tr><td><code id="sumCrossEntropy_+3A_h2">h2</code></td>
<td>

<p>number of neurons in second hidden layer
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>


<h3>See Also</h3>

<p><a href="#topic+squaredError">squaredError</a>
</p>

<hr>
<h2 id='sumSquaredError'>
Sums up squared error
</h2><span id='topic+sumSquaredError'></span>

<h3>Description</h3>

<p>Computes the full value of the squared error for TeachNet
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sumSquaredError(weights, data, h2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sumSquaredError_+3A_weights">weights</code></td>
<td>

<p>current weights
</p>
</td></tr>
<tr><td><code id="sumSquaredError_+3A_data">data</code></td>
<td>

<p>data frame
</p>
</td></tr>
<tr><td><code id="sumSquaredError_+3A_h2">h2</code></td>
<td>

<p>number of neurons in second hidden layer
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>


<h3>See Also</h3>

<p><a href="#topic+crossEntropy">crossEntropy</a>
</p>

<hr>
<h2 id='TeachNet'>
Fits the neural network
</h2><span id='topic+TeachNet'></span>

<h3>Description</h3>

<p>The function TeachNet trains the neural network for a two class classification and also does some testing at the end. The class attribute os assumed to be the first column and coded as 1. Data gets scaled with Z-scores before training. It's also possible to get the final weights returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TeachNet(data, hidden.structure = -1, threshold.TotalError = 0.1, stepMax = 100,
learning.rate = 0.9, acc.fct = "logistic", err.fct = "sse", startWeights = NULL, 
decay = 0, sameSample = FALSE, sampleLength = 0.7, all = FALSE, eval = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TeachNet_+3A_data">data</code></td>
<td>

<p>A data frame. The first column must be the class (0,1), the others the input variables (just numerical).
</p>
</td></tr>
<tr><td><code id="TeachNet_+3A_hidden.structure">hidden.structure</code></td>
<td>

<p>The number of hidden neurons. A vector for two hidden layers. Default, -1 means that the automatic rule is applied (number of hidden neurons = number of variables divided by two, one hidden layer).
</p>
</td></tr>
<tr><td><code id="TeachNet_+3A_threshold.totalerror">threshold.TotalError</code></td>
<td>

<p>Algorithm stops if total error falls below this threshold
</p>
</td></tr>
<tr><td><code id="TeachNet_+3A_stepmax">stepMax</code></td>
<td>

<p>The maximum steps the algorithm does. One step is equal to one update of the weights (one cycle through the training set) for that he has to calculate the gradient of the total error. 
</p>
</td></tr>
<tr><td><code id="TeachNet_+3A_learning.rate">learning.rate</code></td>
<td>

<p>Multiplicative factor by which the actual learning rate is iteratively reduced until the new error is smaller than the old one
</p>
</td></tr>
<tr><td><code id="TeachNet_+3A_acc.fct">acc.fct</code></td>
<td>

<p>The activation function that is used. In this version only &quot;logistic&quot; possible.
</p>
</td></tr>
<tr><td><code id="TeachNet_+3A_err.fct">err.fct</code></td>
<td>

<p>The error function that is used. You can choose &quot;sse&quot; for sum squared error, or &quot;ce&quot; for cross entropy.
</p>
</td></tr>
<tr><td><code id="TeachNet_+3A_startweights">startWeights</code></td>
<td>

<p>This is where you can give TeachNet weights to start with. 
</p>
</td></tr>
<tr><td><code id="TeachNet_+3A_decay">decay</code></td>
<td>

<p>The factor for the weight decay.
</p>
</td></tr>
<tr><td><code id="TeachNet_+3A_samesample">sameSample</code></td>
<td>

<p>If TRUE the training and test data will be a data set with nearly same number of class 0 and 1. Randomly chosen out of the data.
</p>
</td></tr>
<tr><td><code id="TeachNet_+3A_samplelength">sampleLength</code></td>
<td>

<p>Ratio that implies rows of the training data set depending on the full data set. Should be a number greater than 0 and less than 1. Test data set has size (1 - sampleLenght). 
</p>
</td></tr>
<tr><td><code id="TeachNet_+3A_all">all</code></td>
<td>

<p>If TRUE training data is the whole dataset (test data is training data). 
</p>
</td></tr>
<tr><td><code id="TeachNet_+3A_eval">eval</code></td>
<td>

<p>If TRUE evaluation is computed.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the beginning the weights are initialized with a standard normal distribution. But this package is due to its very slow code just to understand the backpropagation algorithm. A good package for real training of neural networks is for example 'nnet'.
</p>


<h3>Value</h3>

<p>TeachNet returns a S4 class object Weights for one hidden layer or Weights2  for two hidden layer. In addition if 'all' is FALSE, it prints an Evaluation. First part is the best found Threshold and V (V=True Positive - False Positive) for the prediction on the test Dataset. Then a confusion matrix and the accuracy of the model compared to the percentage of observation with class zero and one. 
</p>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>


<h3>See Also</h3>

<p><a href="#topic+Weights-class">Weights-class</a>,
<a href="#topic+Weights2-class">Weights2-class</a>,
<a href="#topic+predict.Weights">predict.Weights</a>
<a href="#topic+predict.Weights2">predict.Weights2</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>df &lt;- sample(c(rep(1,20),rep(0,20))) 
income &lt;- c(rnorm(40,mean=1000,sd=10)) 
debt &lt;- rnorm(40,mean=0.5,sd=0.1)
data &lt;- data.frame(df, income, debt)

weights &lt;- TeachNet(data,sameSample=TRUE,sampleLength=0.9,stepMax=2)

</code></pre>

<hr>
<h2 id='transformPrediction'>
Transforms prediction
</h2><span id='topic+transformPrediction'></span>

<h3>Description</h3>

<p>Transforms prediction from prediction to class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transformPrediction(pred, threshold)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="transformPrediction_+3A_pred">pred</code></td>
<td>

<p>Prediction
</p>
</td></tr>
<tr><td><code id="transformPrediction_+3A_threshold">threshold</code></td>
<td>

<p>A threshold up to which a prediction is class 0 or 1. A value from 0 to 1
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss
</p>

<hr>
<h2 id='Weights-class'>Weights objects</h2><span id='topic+Weights-class'></span><span id='topic+-+2CWeights+2CWeights-method'></span><span id='topic++2A+2Cnumeric+2CWeights-method'></span><span id='topic++2B+2CWeights+2CWeights-method'></span>

<h3>Description</h3>

<p>Contains the weights for a one hidden layer neural network in TeachNet the here cold &quot;Arguments&quot; are the slots in the S4 class Weights
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="Weights-class_+3A_alpha">alpha</code></td>
<td>

<p>Intercept from output layer
</p>
</td></tr>
<tr><td><code id="Weights-class_+3A_alpha_h">alpha_h</code></td>
<td>

<p>Intercept from hidden layer
</p>
</td></tr>
<tr><td><code id="Weights-class_+3A_w_h">w_h</code></td>
<td>

<p>Weights from hidden layer to output layer
</p>
</td></tr>
<tr><td><code id="Weights-class_+3A_w_ih">w_ih</code></td>
<td>

<p>Weights from input layer to hidden layer 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss</p>


<h3>See Also</h3>

<p><code><a href="#topic+Weights2-class">Weights2-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
H &lt;- 3 # number of neurons in hidden layer
I &lt;- 6 # number of inputs

random_weights &lt;- new("Weights", alpha = rnorm(1), alpha_h = rnorm(H), w_h = rnorm(H), 
                      w_ih = matrix(nrow=I,ncol=H, data=rnorm(I*H)))

</code></pre>

<hr>
<h2 id='Weights2-class'>Weights2 objects</h2><span id='topic+Weights2-class'></span><span id='topic+-+2CWeights2+2CWeights2-method'></span><span id='topic++2A+2Cnumeric+2CWeights2-method'></span><span id='topic++2B+2CWeights2+2CWeights2-method'></span>

<h3>Description</h3>

<p>Contains the weights for a two hidden layer neural network in TeachNet the here cold &quot;Arguments&quot; are the slots in the S4 class Weights2
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="Weights2-class_+3A_alpha">alpha</code></td>
<td>

<p>Intercept from output layer
</p>
</td></tr>
<tr><td><code id="Weights2-class_+3A_alpha_1m">alpha_1m</code></td>
<td>

<p>Intercept from hidden layer
</p>
</td></tr>
<tr><td><code id="Weights2-class_+3A_alpha_2h">alpha_2h</code></td>
<td>

<p>Intercept from second hidden layer
</p>
</td></tr>
<tr><td><code id="Weights2-class_+3A_w_h">w_h</code></td>
<td>

<p>Weights from second hidden layer to output layer
</p>
</td></tr>
<tr><td><code id="Weights2-class_+3A_q_mh">q_mh</code></td>
<td>

<p>Weights from first hidden layer to second hidden layer
</p>
</td></tr>
<tr><td><code id="Weights2-class_+3A_w_im">w_im</code></td>
<td>

<p>Weights from input layer to first hidden layer 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Georg Steinbuss</p>


<h3>See Also</h3>

<p><code><a href="#topic+Weights-class">Weights-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
M &lt;- 3 # number of neurons in first hidden layer
H &lt;- 3 # number of neurons in second hidden layer
I &lt;- 6 # number of inputs

random_weights &lt;- new("Weights2", alpha = rnorm(1), alpha_1m = rnorm(M), alpha_2h = rnorm(H), 
                      w_h = rnorm(H), q_mh = matrix(nrow=M,ncol=H, data=rnorm(M*H)),
                      w_im = matrix(nrow=I,ncol=M, data=rnorm(I*M)))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
