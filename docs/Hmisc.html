<!DOCTYPE html><html><head><title>Help for package Hmisc</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Hmisc}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#+25nin+25'>
<p>Find Matching (or Non-Matching) Elements</p></a></li>
<li><a href='#abs.error.pred'>
<p>Indexes of Absolute Prediction Error for Linear Models</p></a></li>
<li><a href='#addggLayers'><p>addggLayers</p></a></li>
<li><a href='#addMarginal'><p>Add Marginal Observations</p></a></li>
<li><a href='#all.is.numeric'><p>Check if All Elements in Character Vector are Numeric</p></a></li>
<li><a href='#approxExtrap'><p>Linear Extrapolation</p></a></li>
<li><a href='#areg'><p>Additive Regression with Optimal Transformations on Both Sides using</p>
Canonical Variates</a></li>
<li><a href='#aregImpute'>
<p>Multiple Imputation using Additive Regression, Bootstrapping, and</p>
Predictive Mean Matching</a></li>
<li><a href='#binconf'>
<p>Confidence Intervals for Binomial Probabilities</p></a></li>
<li><a href='#biVar'><p>Bivariate Summaries Computed Separately by a Series of Predictors</p></a></li>
<li><a href='#bootkm'>
<p>Bootstrap Kaplan-Meier Estimates</p></a></li>
<li><a href='#bpower'>
<p>Power and Sample Size for Two-Sample Binomial Test</p></a></li>
<li><a href='#bpplot'>
<p>Box-percentile plots</p></a></li>
<li><a href='#bystats'>
<p>Statistics by Categories</p></a></li>
<li><a href='#capitalize'><p> capitalize the first letter of a string</p></a></li>
<li><a href='#ciapower'>
<p>Power of Interaction Test for Exponential Survival</p></a></li>
<li><a href='#cnvrt.coords'><p>Convert between the 5 different coordinate sytems on a graphical device</p></a></li>
<li><a href='#colorFacet'><p>Miscellaneous ggplot2 and grid Helper Functions</p></a></li>
<li><a href='#combine.levels'><p>combine.levels</p></a></li>
<li><a href='#combplotp'><p>Combination Plot</p></a></li>
<li><a href='#completer'><p>completer</p></a></li>
<li><a href='#consolidate'><p> Element Merging</p></a></li>
<li><a href='#contents'><p>Metadata for a Data Frame</p></a></li>
<li><a href='#cpower'>
<p>Power of Cox/log-rank Two-Sample Test</p></a></li>
<li><a href='#Cs'>
<p>Character strings from unquoted names</p></a></li>
<li><a href='#csv.get'><p>Read Comma-Separated Text Data Files</p></a></li>
<li><a href='#curveRep'><p>Representative Curves</p></a></li>
<li><a href='#cut2'><p>Cut a Numeric Variable into Intervals</p></a></li>
<li><a href='#data.frame.create.modify.check'>
<p>Tips for Creating, Modifying, and Checking Data Frames</p></a></li>
<li><a href='#dataRep'>
<p>Representativeness of Observations in a Data Set</p></a></li>
<li><a href='#deff'>
<p>Design Effect and Intra-cluster Correlation</p></a></li>
<li><a href='#describe'><p>Concise Statistical Description of a Vector, Matrix, Data Frame,</p>
or Formula</a></li>
<li><a href='#discrete'><p> Discrete Vector tools</p></a></li>
<li><a href='#dotchart2'>
<p>Enhanced Dot Chart</p></a></li>
<li><a href='#dotchart3'><p>Enhanced Version of dotchart Function</p></a></li>
<li><a href='#dotchartpl'><p>Enhanced Version of dotchart Function for plotly</p></a></li>
<li><a href='#ebpcomp'><p>ebpcomp</p></a></li>
<li><a href='#Ecdf'><p>Empirical Cumulative Distribution Plot</p></a></li>
<li><a href='#ecdfSteps'><p>ecdfSteps</p></a></li>
<li><a href='#equalBins'><p>Multicolumn Formating</p></a></li>
<li><a href='#errbar'><p>Plot Error Bars</p></a></li>
<li><a href='#escapeRegex'><p> Escapes any characters that would have special meaning in a reqular expression.</p></a></li>
<li><a href='#estSeqMarkovOrd'><p>estSeqMarkovOrd</p></a></li>
<li><a href='#estSeqSim'><p>estSeqSim</p></a></li>
<li><a href='#event.chart'>
<p>Flexible Event Chart for Time-to-Event Data</p></a></li>
<li><a href='#event.convert'>
<p>Event Conversion for Time-to-Event Data</p></a></li>
<li><a href='#event.history'><p>Produces event.history graph for survival data</p></a></li>
<li><a href='#extractlabs'><p>extractlabs</p></a></li>
<li><a href='#fImport'><p>fImport</p></a></li>
<li><a href='#find.matches'>
<p>Find Close Matches</p></a></li>
<li><a href='#first.word'><p>First Word in a String or Expression</p></a></li>
<li><a href='#format.df'>
<p>Format a Data Frame or Matrix for LaTeX or HTML</p></a></li>
<li><a href='#format.pval'><p>Format P Values</p></a></li>
<li><a href='#gbayes'>
<p>Gaussian Bayesian Posterior and Predictive Distributions</p></a></li>
<li><a href='#gbayesSeqSim'><p>gbayesSeqSim</p></a></li>
<li><a href='#getabd'><p>getabd</p></a></li>
<li><a href='#getHdata'>
<p>Download and Install Datasets for <span class="pkg">Hmisc</span>, <span class="pkg">rms</span>, and Statistical</p>
Modeling</a></li>
<li><a href='#getRs'><p>Interact with github rscripts Project</p></a></li>
<li><a href='#getZip'><p>Open a Zip File From a URL</p></a></li>
<li><a href='#ggfreqScatter'><p>Frequency Scatterplot</p></a></li>
<li><a href='#ggplotlyr'><p>ggplotlyr</p></a></li>
<li><a href='#GiniMd'><p>Gini's Mean Difference</p></a></li>
<li><a href='#hashCheck'><p>hashCheck</p></a></li>
<li><a href='#hdquantile'><p>Harrell-Davis Distribution-Free Quantile Estimator</p></a></li>
<li><a href='#hidingTOC'><p>Moving and Hiding Table of Contents</p></a></li>
<li><a href='#hist.data.frame'><p>Histograms for Variables in a Data Frame</p></a></li>
<li><a href='#histbackback'>
<p>Back to Back Histograms</p></a></li>
<li><a href='#histboxp'><p>Use plotly to Draw Stratified Spike Histogram and Box Plot Statistics</p></a></li>
<li><a href='#hlab'><p>hlab</p></a></li>
<li><a href='#hlabs'><p>hlabs</p></a></li>
<li><a href='#Hmisc-internal'><p>Internal Hmisc functions</p></a></li>
<li><a href='#HmiscOverview'>
<p>Overview of Hmisc Library</p></a></li>
<li><a href='#hoeffd'>
<p>Matrix of Hoeffding's D Statistics</p></a></li>
<li><a href='#html'><p>Convert an S object to HTML</p></a></li>
<li><a href='#htmltabv'><p>htmltabc</p></a></li>
<li><a href='#impute'>
<p>Generic Functions and Methods for Imputation</p></a></li>
<li><a href='#intMarkovOrd'><p>intMarkovOrd</p></a></li>
<li><a href='#knitrSet'><p>knitr Setup and plotly Service Function</p></a></li>
<li><a href='#labcurve'><p>Label Curves, Make Keys, and Interactively Draw Points and Curves</p></a></li>
<li><a href='#label'>
<p>Label Attribute of an Object</p></a></li>
<li><a href='#Lag'><p>Lag a Numeric, Character, or Factor Vector</p></a></li>
<li><a href='#latestFile'><p>latestFile</p></a></li>
<li><a href='#latex'>
<p>Convert an S object to LaTeX, and Related Utilities</p></a></li>
<li><a href='#latexCheckOptions'><p>Check whether the options for latex functions have been specified.</p></a></li>
<li><a href='#latexDotchart'><p>Enhanced Dot Chart for LaTeX Picture Environment with epic</p></a></li>
<li><a href='#latexTabular'><p>Convert a Data Frame or Matrix to a LaTeX Tabular</p></a></li>
<li><a href='#latexTherm'><p>Create LaTeX Thermometers and Colored Needles</p></a></li>
<li><a href='#legendfunctions'><p>Legend Creation Functions</p></a></li>
<li><a href='#list.tree'>
<p>Pretty-print the Structure of a Data Object</p></a></li>
<li><a href='#makeNstr'><p> creates a string that is a repeat of a substring</p></a></li>
<li><a href='#mApply'><p>Apply a Function to Rows of a Matrix or Vector</p></a></li>
<li><a href='#mChoice'><p>Methods for Storing and Analyzing Multiple Choice Variables</p></a></li>
<li><a href='#mdb.get'><p>Read Tables in a Microsoft Access Database</p></a></li>
<li><a href='#meltData'><p>meltData</p></a></li>
<li><a href='#Merge'><p>Merge Multiple Data Frames or Data Tables</p></a></li>
<li><a href='#mgp.axis'><p>Draw Axes With Side-Specific mgp Parameters</p></a></li>
<li><a href='#mhgr'><p>Miscellaneous Functions for Epidemiology</p></a></li>
<li><a href='#minor.tick'><p>Minor Tick Marks</p></a></li>
<li><a href='#Misc'><p>Miscellaneous Functions</p></a></li>
<li><a href='#movStats'><p>movStats</p></a></li>
<li><a href='#mtitle'>
<p>Margin Titles</p></a></li>
<li><a href='#multLines'><p>Plot Multiple Lines</p></a></li>
<li><a href='#na.delete'>
<p>Row-wise Deletion na.action</p></a></li>
<li><a href='#na.detail.response'>
<p>Detailed Response Variable Information</p></a></li>
<li><a href='#na.keep'>
<p>Do-nothing na.action</p></a></li>
<li><a href='#nCoincident'><p>nCoincident</p></a></li>
<li><a href='#nobsY'><p>Compute Number of Observations for Left Hand Side of Formula</p></a></li>
<li><a href='#nstr'><p> Creates a string of arbitry length</p></a></li>
<li><a href='#num.intercepts'><p>Extract number of intercepts</p></a></li>
<li><a href='#pairUpDiff'><p>pairUpDiff</p></a></li>
<li><a href='#panel.bpplot'>
<p>Box-Percentile Panel Function for Trellis</p></a></li>
<li><a href='#partition'><p>Patitions an object into different sets</p></a></li>
<li><a href='#pc1'><p>First Principal Component</p></a></li>
<li><a href='#plot.princmp'><p>plot.princmp</p></a></li>
<li><a href='#plotCorrM'><p>plotCorrM</p></a></li>
<li><a href='#plotCorrPrecision'><p>Plot Precision of Estimate of Pearson Correlation Coefficient</p></a></li>
<li><a href='#plotlyM'><p>plotly Multiple</p></a></li>
<li><a href='#plsmo'>
<p>Plot smoothed estimates</p></a></li>
<li><a href='#popower'><p>Power and Sample Size for Ordinal Response</p></a></li>
<li><a href='#princmp'><p>princmp</p></a></li>
<li><a href='#print.char.list'><p> prints a list of lists in a visually readable format.</p></a></li>
<li><a href='#print.char.matrix'><p> Function to print a matrix with stacked cells</p></a></li>
<li><a href='#print.princmp'><p>print.princmp</p></a></li>
<li><a href='#printL'><p>printL</p></a></li>
<li><a href='#prnz'>
<p>Print and Object with its Name</p></a></li>
<li><a href='#prselect'><p>Selectively Print Lines of a Text Vector</p></a></li>
<li><a href='#pstamp'><p>Date/Time/Directory Stamp the Current Plot</p></a></li>
<li><a href='#qcrypt'><p>qcrypt</p></a></li>
<li><a href='#r2describe'><p>r2describe</p></a></li>
<li><a href='#R2Measures'><p>R2Measures</p></a></li>
<li><a href='#rcorr'><p>Matrix of Correlations and P-values</p></a></li>
<li><a href='#rcorr.cens'>
<p>Rank Correlation for Censored Data</p></a></li>
<li><a href='#rcorrp.cens'>
<p>Rank Correlation for Paired Predictors with a Possibly Censored</p>
Response, and Integrated Discrimination Index</a></li>
<li><a href='#rcspline.eval'>
<p>Restricted Cubic Spline Design Matrix</p></a></li>
<li><a href='#rcspline.plot'>
<p>Plot Restricted Cubic Spline Function</p></a></li>
<li><a href='#rcspline.restate'>
<p>Re-state Restricted Cubic Spline Function</p></a></li>
<li><a href='#redun'><p>Redundancy Analysis</p></a></li>
<li><a href='#reShape'><p>Reshape Matrices and Serial Data</p></a></li>
<li><a href='#rlegend'><p>Special Version of legend for R</p></a></li>
<li><a href='#rm.boot'>
<p>Bootstrap Repeated Measurements Model</p></a></li>
<li><a href='#rMultinom'><p>Generate Multinomial Random Variables with Varying Probabilities</p></a></li>
<li><a href='#runifChanged'><p>runifChanged</p></a></li>
<li><a href='#runParallel'><p>runParallel</p></a></li>
<li><a href='#samplesize.bin'>
<p>Sample Size for 2-sample Binomial</p></a></li>
<li><a href='#sas.get'><p>Convert a SAS Dataset to an S Data Frame</p></a></li>
<li><a href='#sasxport.get'><p>Enhanced Importing of SAS Transport Files using read.xport</p></a></li>
<li><a href='#Save'><p>Faciliate Use of save and load to Remote Directories</p></a></li>
<li><a href='#scat1d'><p>One-Dimensional Scatter Diagram, Spike Histogram, or Density</p></a></li>
<li><a href='#score.binary'>
<p>Score a Series of Binary Variables</p></a></li>
<li><a href='#sedit'>
<p>Character String Editing and Miscellaneous Character Handling Functions</p></a></li>
<li><a href='#seqFreq'><p>seqFreq</p></a></li>
<li><a href='#show.pch'><p>Display Colors, Plotting Symbols, and Symbol Numeric Equivalents</p></a></li>
<li><a href='#showPsfrag'>
<p>Display image from psfrag LaTeX strings</p></a></li>
<li><a href='#simMarkovOrd'><p>simMarkovOrd</p></a></li>
<li><a href='#simplifyDims'><p>List Simplification</p></a></li>
<li><a href='#simRegOrd'><p>Simulate Power for Adjusted Ordinal Regression Two-Sample Test</p></a></li>
<li><a href='#smean.sd'>
<p>Compute Summary Statistics on a Vector</p></a></li>
<li><a href='#solvet'>
<p>solve Function with tol argument</p></a></li>
<li><a href='#somers2'>
<p>Somers' Dxy Rank Correlation</p></a></li>
<li><a href='#soprobMarkovOrd'><p>soprobMarkovOrd</p></a></li>
<li><a href='#soprobMarkovOrdm'><p>soprobMarkovOrdm</p></a></li>
<li><a href='#spikecomp'><p>spikecomp</p></a></li>
<li><a href='#spower'>
<p>Simulate Power of 2-Sample Test for Survival under Complex Conditions</p></a></li>
<li><a href='#spss.get'><p>Enhanced Importing of SPSS Files</p></a></li>
<li><a href='#src'><p>Source a File from the Current Working Directory</p></a></li>
<li><a href='#stat_plsmo'><p>Add a lowess smoother without counfidence bands.</p></a></li>
<li><a href='#stata.get'><p>Enhanced Importing of STATA Files</p></a></li>
<li><a href='#string.bounding.box'><p>Determine Dimensions of Strings</p></a></li>
<li><a href='#string.break.line'><p>Break a String into Many Lines at Newlines</p></a></li>
<li><a href='#stringDims'><p>String Dimentions</p></a></li>
<li><a href='#subplot'><p>Embed a new plot within an existing plot</p></a></li>
<li><a href='#summarize'><p>Summarize Scalars or Matrices by Cross-Classification</p></a></li>
<li><a href='#summary.formula'><p>Summarize Data for Making Tables and Plots</p></a></li>
<li><a href='#summaryM'><p>Summarize Mixed Data Types vs. Groups</p></a></li>
<li><a href='#summaryP'><p>Multi-way Summary of Proportions</p></a></li>
<li><a href='#summaryRc'><p>Graphical Summarization of Continuous Variables Against a Response</p></a></li>
<li><a href='#summaryS'><p>Summarize Multiple Response Variables and Make Multipanel Scatter</p>
or Dot Plot</a></li>
<li><a href='#symbol.freq'><p>Graphic Representation of a Frequency Table</p></a></li>
<li><a href='#sys'>
<p>Run Unix or Dos Depending on System</p></a></li>
<li><a href='#t.test.cluster'><p>t-test for Clustered Data</p></a></li>
<li><a href='#tabulr'><p>Interface to Tabular Function</p></a></li>
<li><a href='#testCharDateTime'><p>testCharDateTime</p></a></li>
<li><a href='#tex'>
<p>function for use in graphs that are used with the psfrag package in LaTeX</p></a></li>
<li><a href='#transace'>
<p>Additive Regression and Transformations using ace or avas</p></a></li>
<li><a href='#transcan'>
<p>Transformations/Imputations using Canonical Variates</p></a></li>
<li><a href='#translate'>
<p>Translate Vector or Matrix of Text Strings</p></a></li>
<li><a href='#trunc.POSIXt'><p>Return the floor, ceiling, or rounded value of date or time to</p>
specified unit.</a></li>
<li><a href='#units'>
<p>Units Attribute of a Vector</p></a></li>
<li><a href='#upData'>
<p>Update a Data Frame or Cleanup a Data Frame after Importing</p></a></li>
<li><a href='#upFirst'><p>Change First Letters to Upper Case</p></a></li>
<li><a href='#valueTags'><p>Store Descriptive Information About an Object</p></a></li>
<li><a href='#varclus'>
<p>Variable Clustering</p></a></li>
<li><a href='#vlab'><p>vlab</p></a></li>
<li><a href='#wtd.stats'>
<p>Weighted Statistical Estimates</p></a></li>
<li><a href='#xtfrm.labelled'>
<p>Auxiliary Function Method for Sorting and Ranking</p></a></li>
<li><a href='#xy.group'>
<p>Mean x vs. function of y in groups of x</p></a></li>
<li><a href='#xYplot'><p>xyplot and dotplot with Matrix Variables to Plot Error Bars and Bands</p></a></li>
<li><a href='#yearDays'><p> Get Number of Days in Year or Month</p></a></li>
<li><a href='#ynbind'><p>Combine Variables in a Matrix</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>5.1-3</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-05-18</td>
</tr>
<tr>
<td>Title:</td>
<td>Harrell Miscellaneous</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Frank E Harrell Jr &lt;fh@fharrell.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, ggplot2, cluster, rpart, nnet, foreign, gtable, grid,
gridExtra, data.table, htmlTable (&ge; 1.11.0), viridis,
htmltools, base64enc, colorspace, rmarkdown, knitr, Formula</td>
</tr>
<tr>
<td>Suggests:</td>
<td>survival, qreport, acepack, chron, rms, mice, rstudioapi,
tables, plotly (&ge; 4.5.6), rlang, plyr, VGAM, leaps, pcaPP,
digest, parallel, polspline, abind, kableExtra, rio, lattice,
latticeExtra, gt, sparkline, jsonlite, htmlwidgets, qs,
getPass, keyring, safer, htm2txt</td>
</tr>
<tr>
<td>Description:</td>
<td>Contains many functions useful for data
	analysis, high-level graphics, utility operations, functions for
	computing sample size and power, simulation, importing and annotating datasets,
	imputing missing values, advanced table making, variable clustering,
	character string manipulation, conversion of R objects to LaTeX and html code,
	recoding variables, caching, simplified parallel computing, encrypting and decrypting data using a safe workflow, general moving window statistical estimation, and assistance in interpreting principal component analysis.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>Yes</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://hbiostat.org/R/Hmisc/">https://hbiostat.org/R/Hmisc/</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-05-18 13:26:32 UTC; harrelfe</td>
</tr>
<tr>
<td>Author:</td>
<td>Frank E Harrell Jr
    <a href="https://orcid.org/0000-0002-8271-5493"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre],
  Charles Dupont [ctb] (contributed several functions and maintains latex
    functions)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-05-28 07:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25nin+25'>
Find Matching (or Non-Matching) Elements
</h2><span id='topic++25nin+25'></span>

<h3>Description</h3>

<p><code>%nin%</code> is a binary operator, which returns a logical vector indicating
if there is a match or not for its left operand. A true vector element
indicates no match in left operand, false indicates a match.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>x %nin% table
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B25nin+2B25_+3A_x">x</code></td>
<td>

<p>a vector (numeric, character, factor)
</p>
</td></tr>
<tr><td><code id="+2B25nin+2B25_+3A_table">table</code></td>
<td>

<p>a vector (numeric, character, factor), matching the mode of <code>x</code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of logical values with length equal to length of <code>x</code>.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+match">match</a></code> <code><a href="base.html#topic++25in+25">%in%</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>c('a','b','c') %nin% c('a','b')
</code></pre>

<hr>
<h2 id='abs.error.pred'>
Indexes of Absolute Prediction Error for Linear Models
</h2><span id='topic+abs.error.pred'></span><span id='topic+print.abs.error.pred'></span>

<h3>Description</h3>

<p>Computes the mean and median of various absolute errors related to
ordinary multiple regression models.  The mean and median absolute
errors correspond to the mean square due to regression, error, and
total. The absolute errors computed are derived from <code class="reqn">\hat{Y} -
    \mbox{median($\hat{Y}$)}</code>,
<code class="reqn">\hat{Y} - Y</code>, and <code class="reqn">Y -
    \mbox{median($Y$)}</code>.  The function also
computes ratios that correspond to <code class="reqn">R^2</code> and <code class="reqn">1 - R^2</code> (but
these ratios do not add to 1.0); the <code class="reqn">R^2</code> measure is the ratio of
mean or median absolute <code class="reqn">\hat{Y} - \mbox{median($\hat{Y}$)}</code> to the mean or median absolute <code class="reqn">Y -
    \mbox{median($Y$)}</code>. The <code class="reqn">1 - R^2</code> or SSE/SST
measure is the mean or median absolute <code class="reqn">\hat{Y} - Y</code>
divided by the mean or median absolute <code class="reqn">\hat{Y} -
   \mbox{median($Y$)}</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>abs.error.pred(fit, lp=NULL, y=NULL)

## S3 method for class 'abs.error.pred'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="abs.error.pred_+3A_fit">fit</code></td>
<td>

<p>a fit object typically from <code><a href="stats.html#topic+lm">lm</a></code> or <code><a href="rms.html#topic+ols">ols</a></code>
that contains a y vector (i.e., you should have specified
<code>y=TRUE</code> to the fitting function) unless the <code>y</code> argument
is given to <code>abs.error.pred</code>.  If you do not specify the
<code>lp</code> argument, <code>fit</code> must contain <code>fitted.values</code> or
<code>linear.predictors</code>.  You must specify <code>fit</code> or both of
<code>lp</code> and <code>y</code>.
</p>
</td></tr>
<tr><td><code id="abs.error.pred_+3A_lp">lp</code></td>
<td>

<p>a vector of predicted values (Y hat above) if <code>fit</code> is not given
</p>
</td></tr>
<tr><td><code id="abs.error.pred_+3A_y">y</code></td>
<td>

<p>a vector of response variable values if <code>fit</code> (with
<code>y=TRUE</code> in effect) is not given
</p>
</td></tr>
<tr><td><code id="abs.error.pred_+3A_x">x</code></td>
<td>
<p>an object created by <code>abs.error.pred</code></p>
</td></tr>
<tr><td><code id="abs.error.pred_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of class <code>abs.error.pred</code> (used by
<code>print.abs.error.pred</code>) containing two matrices:
<code>differences</code> and <code>ratios</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University School of Medicine<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Schemper M (2003): Stat in Med 22:2299-2308.
</p>
<p>Tian L, Cai T, Goetghebeur E, Wei LJ (2007): Biometrika 94:297-311.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+lm">lm</a></code>, <code><a href="rms.html#topic+ols">ols</a></code>, <code><a href="stats.html#topic+cor">cor</a></code>,
<code><a href="rms.html#topic+validate.ols">validate.ols</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)         # so can regenerate results
x1 &lt;- rnorm(100)
x2 &lt;- rnorm(100)
y  &lt;- exp(x1+x2+rnorm(100))
f &lt;- lm(log(y) ~ x1 + poly(x2,3), y=TRUE)
abs.error.pred(lp=exp(fitted(f)), y=y)
rm(x1,x2,y,f)
</code></pre>

<hr>
<h2 id='addggLayers'>addggLayers</h2><span id='topic+addggLayers'></span>

<h3>Description</h3>

<p>Add Spike Histograms and Extended Box Plots to <code>ggplot</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addggLayers(
  g,
  data,
  type = c("ebp", "spike"),
  ylim = layer_scales(g)$y$get_limits(),
  by = "variable",
  value = "value",
  frac = 0.065,
  mult = 1,
  facet = NULL,
  pos = c("bottom", "top"),
  showN = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addggLayers_+3A_g">g</code></td>
<td>
<p>a <code>ggplot</code> object</p>
</td></tr>
<tr><td><code id="addggLayers_+3A_data">data</code></td>
<td>
<p>data frame/table containing raw data</p>
</td></tr>
<tr><td><code id="addggLayers_+3A_type">type</code></td>
<td>
<p>specifies either extended box plot or spike histogram.  Both are horizontal so are showing the distribution of the x-axis variable.</p>
</td></tr>
<tr><td><code id="addggLayers_+3A_ylim">ylim</code></td>
<td>
<p>y-axis limits to use for scaling the height of the added plots, if you don't want to use the limits that <code>ggplot</code> has stored</p>
</td></tr>
<tr><td><code id="addggLayers_+3A_by">by</code></td>
<td>
<p>the name of a variable in <code>data</code> used to stratify raw data</p>
</td></tr>
<tr><td><code id="addggLayers_+3A_value">value</code></td>
<td>
<p>name of x-variable</p>
</td></tr>
<tr><td><code id="addggLayers_+3A_frac">frac</code></td>
<td>
<p>fraction of y-axis range to devote to vertical aspect of the added plot</p>
</td></tr>
<tr><td><code id="addggLayers_+3A_mult">mult</code></td>
<td>
<p>fudge factor for scaling y aspect</p>
</td></tr>
<tr><td><code id="addggLayers_+3A_facet">facet</code></td>
<td>
<p>optional faceting variable</p>
</td></tr>
<tr><td><code id="addggLayers_+3A_pos">pos</code></td>
<td>
<p>position for added plot</p>
</td></tr>
<tr><td><code id="addggLayers_+3A_shown">showN</code></td>
<td>
<p>sete to <code>FALSE</code> to not show sample sizes</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For an example see <a href="https://hbiostat.org/rflow/analysis.html#fig-table1">this</a>.  Note that it was not possible to just create the layers needed to be added, as creating these particular layers in isolation resulted in a <code>ggplot</code> error.
</p>


<h3>Value</h3>

<p>the original <code>ggplot</code> object with more layers added
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><code>spikecomp()</code>
</p>

<hr>
<h2 id='addMarginal'>Add Marginal Observations</h2><span id='topic+addMarginal'></span>

<h3>Description</h3>

<p>Given a data frame and the names of variable, doubles the
data frame for each variable with a new category
<code>"All"</code> by default, or by the value of <code>label</code>.
A new variable <code>.marginal.</code> is added to the resulting data frame,
with value <code>""</code> if the observation is an original one, and with
value equal to the names of the variable being marginalized (separated
by commas) otherwise.  If there is another stratification variable
besides the one in ..., and that variable is nested inside the
variable in ..., specify <code>nested=variable name</code> to have the value
of that variable set fo <code>label</code> whenever marginal observations are
created for ....  See the state-city example below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addMarginal(data, ..., label = "All", margloc=c('last', 'first'), nested)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addMarginal_+3A_data">data</code></td>
<td>
<p>a data frame</p>
</td></tr>
<tr><td><code id="addMarginal_+3A_...">...</code></td>
<td>
<p>a list of names of variables to marginalize</p>
</td></tr>
<tr><td><code id="addMarginal_+3A_label">label</code></td>
<td>
<p>category name for added marginal observations</p>
</td></tr>
<tr><td><code id="addMarginal_+3A_margloc">margloc</code></td>
<td>
<p>location for marginal category within factor variable
specifying categories.  Set to <code>"first"</code> to override the
default - to put a category with value <code>label</code> as the first
category.</p>
</td></tr>
<tr><td><code id="addMarginal_+3A_nested">nested</code></td>
<td>
<p>a single unquoted variable name if used</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>d &lt;- expand.grid(sex=c('female', 'male'), country=c('US', 'Romania'),
                 reps=1:2)
addMarginal(d, sex, country)

# Example of nested variables
d &lt;- data.frame(state=c('AL', 'AL', 'GA', 'GA', 'GA'),
                city=c('Mobile', 'Montgomery', 'Valdosto',
                       'Augusta', 'Atlanta'),
                x=1:5, stringsAsFactors=TRUE)
addMarginal(d, state, nested=city) # cite set to 'All' when state is
</code></pre>

<hr>
<h2 id='all.is.numeric'>Check if All Elements in Character Vector are Numeric</h2><span id='topic+all.is.numeric'></span>

<h3>Description</h3>

<p>Tests, without issuing warnings, whether all elements of a character
vector are legal numeric values, or optionally converts the vector to a
numeric vector.  Leading and trailing blanks in <code>x</code> are ignored.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>all.is.numeric(x, what = c("test", "vector", "nonnum"), extras=c('.','NA'))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="all.is.numeric_+3A_x">x</code></td>
<td>
<p>a character vector</p>
</td></tr>
<tr><td><code id="all.is.numeric_+3A_what">what</code></td>
<td>
<p>specify <code>what="vector"</code> to return a numeric vector if
it passes the test, or the original character vector otherwise, the
default <code>"test"</code> to return <code>FALSE</code> if there are no
non-missing non-<code>extra</code> values of <code>x</code> or there is at least
one non-numeric value of <code>x</code>, or <code>"nonnum"</code> to return the
vector of non-<code>extra</code>, non-NA, non-numeric values of <code>x</code>.</p>
</td></tr>
<tr><td><code id="all.is.numeric_+3A_extras">extras</code></td>
<td>
<p>a vector of character strings to count as numeric
values, other than <code>""</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a logical value if <code>what="test"</code> or a vector otherwise</p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+as.numeric">as.numeric</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>all.is.numeric(c('1','1.2','3'))
all.is.numeric(c('1','1.2','3a'))
all.is.numeric(c('1','1.2','3'),'vector')
all.is.numeric(c('1','1.2','3a'),'vector')
all.is.numeric(c('1','',' .'),'vector')
all.is.numeric(c('1', '1.2', '3a'), 'nonnum')
</code></pre>

<hr>
<h2 id='approxExtrap'>Linear Extrapolation</h2><span id='topic+approxExtrap'></span>

<h3>Description</h3>

<p>Works in conjunction with the <code><a href="stats.html#topic+approx">approx</a></code> function to do linear
extrapolation.  <code><a href="stats.html#topic+approx">approx</a></code> in R does not support extrapolation at
all, and it is buggy in S-Plus 6. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>approxExtrap(x, y, xout, method = "linear", n = 50, rule = 2, f = 0,
             ties = "ordered", na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="approxExtrap_+3A_x">x</code>, <code id="approxExtrap_+3A_y">y</code>, <code id="approxExtrap_+3A_xout">xout</code>, <code id="approxExtrap_+3A_method">method</code>, <code id="approxExtrap_+3A_n">n</code>, <code id="approxExtrap_+3A_rule">rule</code>, <code id="approxExtrap_+3A_f">f</code></td>
<td>

<p>see <code><a href="stats.html#topic+approx">approx</a></code>
</p>
</td></tr>
<tr><td><code id="approxExtrap_+3A_ties">ties</code></td>
<td>

<p>applies only to R.  See <code><a href="stats.html#topic+approx">approx</a></code>
</p>
</td></tr>
<tr><td><code id="approxExtrap_+3A_na.rm">na.rm</code></td>
<td>

<p>set to <code>TRUE</code> to remove <code>NA</code>s in <code>x</code> and
<code>y</code> before proceeding
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Duplicates in <code>x</code> (and corresponding <code>y</code> elements) are removed
before using <code>approx</code>.
</p>


<h3>Value</h3>

<p>a vector the same length as <code>xout</code>
</p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+approx">approx</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>approxExtrap(1:3,1:3,xout=c(0,4))
</code></pre>

<hr>
<h2 id='areg'>Additive Regression with Optimal Transformations on Both Sides using
Canonical Variates</h2><span id='topic+areg'></span><span id='topic+print.areg'></span><span id='topic+predict.areg'></span><span id='topic+plot.areg'></span>

<h3>Description</h3>

<p>Expands continuous variables into restricted cubic spline bases and
categorical variables into dummy variables and fits a multivariate
equation using canonical variates.  This finds optimum transformations
that maximize <code class="reqn">R^2</code>.  Optionally, the bootstrap is used to estimate
the covariance matrix of both left- and right-hand-side transformation
parameters, and to estimate the bias in the <code class="reqn">R^2</code> due to overfitting
and compute the bootstrap optimism-corrected <code class="reqn">R^2</code>.
Cross-validation can also be used to get an unbiased estimate of
<code class="reqn">R^2</code> but this is not as precise as the bootstrap estimate.  The
bootstrap and cross-validation may also used to get estimates of mean
and median absolute error in predicted values on the original <code>y</code>
scale.  These two estimates are perhaps the best ones for gauging the
accuracy of a flexible model, because it is difficult to compare
<code class="reqn">R^2</code> under different y-transformations, and because <code class="reqn">R^2</code>
allows for an out-of-sample recalibration (i.e., it only measures
relative errors).
</p>
<p>Note that uncertainty about the proper transformation of <code>y</code> causes
an enormous amount of model uncertainty.  When the transformation for
<code>y</code> is estimated from the data a high variance in predicted values
on the original <code>y</code> scale may result, especially if the true
transformation is linear.  Comparing bootstrap or cross-validated mean
absolute errors with and without restricted the <code>y</code> transform to be
linear (<code>ytype='l'</code>) may help the analyst choose the proper model
complexity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>areg(x, y, xtype = NULL, ytype = NULL, nk = 4,
     B = 0, na.rm = TRUE, tolerance = NULL, crossval = NULL)

## S3 method for class 'areg'
print(x, digits=4, ...)

## S3 method for class 'areg'
plot(x, whichx = 1:ncol(x$x), ...)

## S3 method for class 'areg'
predict(object, x, type=c('lp','fitted','x'),
                       what=c('all','sample'), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="areg_+3A_x">x</code></td>
<td>

<p>A single predictor or a matrix of predictors.  Categorical
predictors are required to be coded as integers (as <code>factor</code>
does internally).
For <code>predict</code>, <code>x</code> is a data matrix with the same integer
codes that were originally used for categorical variables.
</p>
</td></tr>
<tr><td><code id="areg_+3A_y">y</code></td>
<td>
<p>a <code>factor</code>, categorical, character, or numeric response
variable</p>
</td></tr>
<tr><td><code id="areg_+3A_xtype">xtype</code></td>
<td>

<p>a vector of one-letter character codes specifying how each predictor
is to be modeled, in order of columns of <code>x</code>.  The codes are
<code>"s"</code> for smooth function (using restricted cubic splines),
<code>"l"</code> for no transformation (linear), or <code>"c"</code> for
categorical (to cause expansion into dummy variables).  Default is
<code>"s"</code> if <code>nk &gt; 0</code> and <code>"l"</code> if <code>nk=0</code>.
</p>
</td></tr>
<tr><td><code id="areg_+3A_ytype">ytype</code></td>
<td>
<p>same coding as for <code>xtype</code>.  Default is <code>"s"</code>
for a numeric variable with more than two unique values, <code>"l"</code>
for a binary numeric variable, and <code>"c"</code> for a factor,
categorical, or character variable.</p>
</td></tr>
<tr><td><code id="areg_+3A_nk">nk</code></td>
<td>
<p>number of knots, 0 for linear, or 3 or more.  Default is 4
which will fit 3 parameters to continuous variables (one linear term
and two nonlinear terms)</p>
</td></tr>
<tr><td><code id="areg_+3A_b">B</code></td>
<td>
<p>number of bootstrap resamples used to estimate covariance
matrices of transformation parameters.  Default is no bootstrapping.</p>
</td></tr>
<tr><td><code id="areg_+3A_na.rm">na.rm</code></td>
<td>
<p>set to <code>FALSE</code> if you are sure that observations
with <code>NA</code>s have already been removed</p>
</td></tr>
<tr><td><code id="areg_+3A_tolerance">tolerance</code></td>
<td>
<p>singularity tolerance.  List source code for
<code>lm.fit.qr.bare</code> for details.</p>
</td></tr>
<tr><td><code id="areg_+3A_crossval">crossval</code></td>
<td>
<p>set to a positive integer k to compute k-fold
cross-validated R-squared (square of first canonical correlation)
and mean and median absolute error of predictions on the original scale</p>
</td></tr>
<tr><td><code id="areg_+3A_digits">digits</code></td>
<td>
<p>number of digits to use in formatting for printing</p>
</td></tr>
<tr><td><code id="areg_+3A_object">object</code></td>
<td>
<p>an object created by <code>areg</code></p>
</td></tr>
<tr><td><code id="areg_+3A_whichx">whichx</code></td>
<td>
<p>integer or character vector specifying which predictors
are to have their transformations plotted (default is all).  The
<code>y</code> transformation is always plotted.</p>
</td></tr>
<tr><td><code id="areg_+3A_type">type</code></td>
<td>
<p>tells <code>predict</code> whether to obtain predicted
untransformed <code>y</code> (<code>type='lp'</code>, the default) or predicted
<code>y</code> on the original scale (<code>type='fitted'</code>), or the design
matrix for the right-hand side (<code>type='x'</code>).</p>
</td></tr>
<tr><td><code id="areg_+3A_what">what</code></td>
<td>
<p>When the <code>y</code>-transform is non-monotonic you may
specify <code>what='sample'</code> to <code>predict</code> to obtain a random
sample of <code>y</code> values on the original scale instead of a matrix
of all <code>y</code>-inverses.  See <code><a href="#topic+inverseFunction">inverseFunction</a></code>.</p>
</td></tr>
<tr><td><code id="areg_+3A_...">...</code></td>
<td>
<p>arguments passed to the plot function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>areg</code> is a competitor of <code>ace</code> in the <code>acepack</code>
package.  Transformations from <code>ace</code> are seldom smooth enough and
are often overfitted.  With <code>areg</code> the complexity can be controlled
with the <code>nk</code> parameter, and predicted values are easy to obtain
because parametric functions are fitted.
</p>
<p>If one side of the equation has a categorical variable with more than
two categories and the other side has a continuous variable not assumed
to act linearly, larger sample sizes are needed to reliably estimate
transformations, as it is difficult to optimally score categorical
variables to maximize <code class="reqn">R^2</code> against a simultaneously optimally
transformed continuous variable.
</p>


<h3>Value</h3>

<p>a list of class <code>"areg"</code> containing many objects
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Breiman and Friedman, Journal of the American Statistical
Association (September, 1985).</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+cancor">cancor</a></code>,<code><a href="acepack.html#topic+ace">ace</a></code>, <code><a href="#topic+transcan">transcan</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)

ns &lt;- c(30,300,3000)
for(n in ns) {
  y &lt;- sample(1:5, n, TRUE)
  x &lt;- abs(y-3) + runif(n)
  par(mfrow=c(3,4))
  for(k in c(0,3:5)) {
    z &lt;- areg(x, y, ytype='c', nk=k)
    plot(x, z$tx)
	title(paste('R2=',format(z$rsquared)))
    tapply(z$ty, y, range)
    a &lt;- tapply(x,y,mean)
    b &lt;- tapply(z$ty,y,mean)
    plot(a,b)
	abline(lsfit(a,b))
    # Should get same result to within linear transformation if reverse x and y
    w &lt;- areg(y, x, xtype='c', nk=k)
    plot(z$ty, w$tx)
    title(paste('R2=',format(w$rsquared)))
    abline(lsfit(z$ty, w$tx))
 }
}

par(mfrow=c(2,2))
# Example where one category in y differs from others but only in variance of x
n &lt;- 50
y &lt;- sample(1:5,n,TRUE)
x &lt;- rnorm(n)
x[y==1] &lt;- rnorm(sum(y==1), 0, 5)
z &lt;- areg(x,y,xtype='l',ytype='c')
z
plot(z)
z &lt;- areg(x,y,ytype='c')
z
plot(z)

## Not run: 		
# Examine overfitting when true transformations are linear
par(mfrow=c(4,3))
for(n in c(200,2000)) {
  x &lt;- rnorm(n); y &lt;- rnorm(n) + x
    for(nk in c(0,3,5)) {
    z &lt;- areg(x, y, nk=nk, crossval=10, B=100)
    print(z)
    plot(z)
    title(paste('n=',n))
  }
}
par(mfrow=c(1,1))

# Underfitting when true transformation is quadratic but overfitting
# when y is allowed to be transformed
set.seed(49)
n &lt;- 200
x &lt;- rnorm(n); y &lt;- rnorm(n) + .5*x^2
#areg(x, y, nk=0, crossval=10, B=100)
#areg(x, y, nk=4, ytype='l', crossval=10, B=100)
z &lt;- areg(x, y, nk=4) #, crossval=10, B=100)
z
# Plot x vs. predicted value on original scale.  Since y-transform is
# not monotonic, there are multiple y-inverses
xx &lt;- seq(-3.5,3.5,length=1000)
yhat &lt;- predict(z, xx, type='fitted')
plot(x, y, xlim=c(-3.5,3.5))
for(j in 1:ncol(yhat)) lines(xx, yhat[,j], col=j)
# Plot a random sample of possible y inverses
yhats &lt;- predict(z, xx, type='fitted', what='sample')
points(xx, yhats, pch=2)

## End(Not run)

# True transformation of x1 is quadratic, y is linear
n &lt;- 200
x1 &lt;- rnorm(n); x2 &lt;- rnorm(n); y &lt;- rnorm(n) + x1^2
z &lt;- areg(cbind(x1,x2),y,xtype=c('s','l'),nk=3)
par(mfrow=c(2,2))
plot(z)

# y transformation is inverse quadratic but areg gets the same answer by
# making x1 quadratic
n &lt;- 5000
x1 &lt;- rnorm(n); x2 &lt;- rnorm(n); y &lt;- (x1 + rnorm(n))^2
z &lt;- areg(cbind(x1,x2),y,nk=5)
par(mfrow=c(2,2))
plot(z)

# Overfit 20 predictors when no true relationships exist
n &lt;- 1000
x &lt;- matrix(runif(n*20),n,20)
y &lt;- rnorm(n)
z &lt;- areg(x, y, nk=5)  # add crossval=4 to expose the problem

# Test predict function
n &lt;- 50
x &lt;- rnorm(n)
y &lt;- rnorm(n) + x
g &lt;- sample(1:3, n, TRUE)
z &lt;- areg(cbind(x,g),y,xtype=c('s','c'))
range(predict(z, cbind(x,g)) - z$linear.predictors)
</code></pre>

<hr>
<h2 id='aregImpute'>
Multiple Imputation using Additive Regression, Bootstrapping, and
Predictive Mean Matching
</h2><span id='topic+aregImpute'></span><span id='topic+print.aregImpute'></span><span id='topic+plot.aregImpute'></span><span id='topic+reformM'></span>

<h3>Description</h3>

<p>The <code>transcan</code> function creates flexible additive imputation models
but provides only an approximation to true multiple imputation as the
imputation models are fixed before all multiple imputations are
drawn.  This ignores variability caused by having to fit the
imputation models.  <code>aregImpute</code> takes all aspects of uncertainty in
the imputations into account by using the bootstrap to approximate the
process of drawing predicted values from a full Bayesian predictive
distribution.  Different bootstrap resamples are used for each of the
multiple imputations, i.e., for the <code>i</code>th imputation of a sometimes
missing variable, <code>i=1,2,... n.impute</code>, a flexible additive
model is fitted on a sample with replacement from the original data and
this model is used to predict all of the original missing and
non-missing values for the target variable.
</p>
<p><code>areg</code> is used to fit the imputation models.  By default, linearity
is assumed for target variables (variables being imputed) and
<code>nk=3</code> knots are assumed for continuous predictors transformed
using restricted cubic splines.  If <code>nk</code> is three or greater and
<code>tlinear</code> is set to <code>FALSE</code>, <code>areg</code>
simultaneously finds transformations of the target variable and of all of
the predictors, to get a good fit assuming additivity, maximizing
<code class="reqn">R^2</code>, using the same canonical correlation method as
<code>transcan</code>.  Flexible transformations may be overridden for
specific variables by specifying the identity transformation for them.
When a categorical variable is being predicted, the flexible
transformation is Fisher's optimum scoring method.  Nonlinear transformations for continuous variables may be nonmonotonic.  If
<code>nk</code> is a vector, <code>areg</code>'s bootstrap and <code>crossval=10</code>
options will be used to help find the optimum validating value of
<code>nk</code> over values of that vector, at the last imputation iteration.
For the imputations, the minimum value of <code>nk</code> is used.
</p>
<p>Instead of defaulting to taking random draws from fitted imputation
models using random residuals as is done by <code>transcan</code>,
<code>aregImpute</code> by default uses predictive mean matching with optional
weighted probability sampling of donors rather than using only the
closest match.  Predictive mean matching works for binary, categorical,
and continuous variables without the need for iterative maximum
likelihood fitting for binary and categorical variables, and without the
need for computing residuals or for curtailing imputed values to be in
the range of actual data.  Predictive mean matching is especially
attractive when the variable being imputed is also being transformed
automatically.  Constraints may be placed on variables being imputed
with predictive mean matching, e.g., a missing hospital discharge date
may be required to be imputed from a donor observation whose discharge
date is before the recipient subject's first post-discharge visit date.
See Details below for more information about the
algorithm.  A <code>"regression"</code> method is also available that is
similar to that used in <code>transcan</code>.  This option should be used
when mechanistic missingness requires the use of extrapolation during
imputation.
</p>
<p>A <code>print</code> method summarizes the results, and a <code>plot</code> method plots
distributions of imputed values.  Typically, <code>fit.mult.impute</code> will
be called after <code>aregImpute</code>.
</p>
<p>If a target variable is transformed nonlinearly (i.e., if <code>nk</code> is
greater than zero and <code>tlinear</code> is set to <code>FALSE</code>) and the
estimated target variable transformation is non-monotonic, imputed
values are not unique.  When <code>type='regression'</code>, a random choice
of possible inverse values is made.
</p>
<p>The <code>reformM</code> function provides two ways of recreating a formula to
give to <code>aregImpute</code> by reordering the variables in the formula.
This is a modified version of a function written by Yong Hao Pua.  One
can specify <code>nperm</code> to obtain a list of <code>nperm</code> randomly
permuted variables.  The list is converted to a single ordinary formula
if <code>nperm=1</code>.  If <code>nperm</code> is omitted, variables are sorted in
descending order of the number of <code>NA</code>s.  <code>reformM</code> also
prints a recommended number of multiple imputations to use, which is a
minimum of 5 and the percent of incomplete observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aregImpute(formula, data, subset, n.impute=5, group=NULL,
           nk=3, tlinear=TRUE, type=c('pmm','regression','normpmm'),
           pmmtype=1, match=c('weighted','closest','kclosest'),
           kclosest=3, fweighted=0.2,
           curtail=TRUE, constraint=NULL,
           boot.method=c('simple', 'approximate bayesian'),
           burnin=3, x=FALSE, pr=TRUE, plotTrans=FALSE, tolerance=NULL, B=75)
## S3 method for class 'aregImpute'
print(x, digits=3, ...)
## S3 method for class 'aregImpute'
plot(x, nclass=NULL, type=c('ecdf','hist'),
     datadensity=c("hist", "none", "rug", "density"),
     diagnostics=FALSE, maxn=10, ...)
reformM(formula, data, nperm)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aregImpute_+3A_formula">formula</code></td>
<td>

<p>an S model formula.  You can specify restrictions for transformations
of variables.  The function automatically determines which variables
are categorical (i.e., <code>factor</code>, <code>category</code>, or character vectors).
Binary variables are automatically restricted to be linear.  Force
linear transformations of continuous variables by enclosing variables
by the identify function (<code>I()</code>).  It is recommended that
<code>factor()</code> or <code>as.factor()</code> do not appear in the formula but
instead variables be converted to factors as needed and stored in the
data frame.  That way imputations for factor variables (done using
<code><a href="#topic+impute.transcan">impute.transcan</a></code> for example) will be correct.  Currently
<code>reformM</code> does not handle variables that are enclosed in functions
such as <code>I()</code>.
</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_x">x</code></td>
<td>

<p>an object created by <code>aregImpute</code>.  For <code>aregImpute</code>, set
<code>x</code> to <code>TRUE</code> to save the data matrix containing the final (number
<code>n.impute</code>) imputations in the result.  This
is needed if you want to later do out-of-sample imputation.
Categorical variables are coded as integers in this matrix.
</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_data">data</code></td>
<td>
<p>input raw data</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_subset">subset</code></td>
<td>

<p>These may be also be specified.  You may not specify <code>na.action</code> as
<code>na.retain</code> is always used.
</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_n.impute">n.impute</code></td>
<td>

<p>number of multiple imputations.  <code>n.impute=5</code> is frequently
recommended but 10 or more doesn't hurt.
</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_group">group</code></td>
<td>
<p>a character or factor variable the same length as the
number of observations in <code>data</code> and containing no <code>NA</code>s.
When <code>group</code> is present, causes a bootstrap sample of the
observations corresponding to non-<code>NA</code>s of a target variable to
have the same frequency distribution of <code>group</code> as the
that in the non-<code>NA</code>s of the original sample.  This can handle
k-sample problems as well as lower the chance that a bootstrap sample
will have a missing cell when the original cell frequency was low.
</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_nk">nk</code></td>
<td>
<p>number of knots to use for continuous variables.  When both
the target variable and the predictors are having optimum
transformations estimated, there is more instability than with normal
regression so the complexity of the model should decrease more sharply
as the sample size decreases.  Hence set <code>nk</code> to 0 (to force
linearity for non-categorical variables) or 3 (minimum number of knots
possible with a linear tail-restricted cubic spline) for small sample
sizes.  Simulated problems as in the examples section can assist in
choosing <code>nk</code>.  Set <code>nk</code> to a vector to get bootstrap-validated
and 10-fold cross-validated <code class="reqn">R^2</code> and mean and median absolute
prediction errors for imputing each sometimes-missing variable, with
<code>nk</code> ranging over the given vector.  The errors are on the
original untransformed scale.  The mean absolute error is the
recommended basis for choosing the number of knots (or linearity).
</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_tlinear">tlinear</code></td>
<td>
<p>set to <code>FALSE</code> to allow a target variable (variable
being imputed) to have a nonlinear left-hand-side transformation when
<code>nk</code> is 3 or greater</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_type">type</code></td>
<td>

<p>The default is <code>"pmm"</code> for predictive mean matching,
which is a more nonparametric approach that will work for categorical
as well as continuous predictors.  Alternatively, use
<code>"regression"</code> when all variables that are sometimes missing are
continuous and the missingness mechanism is such that entire intervals
of population values are unobserved.  See the Details section for more
information.  Another method, <code>type="normpmm"</code>, only works
when variables containing <code>NA</code>s are continuous and <code>tlinear</code>
is <code>TRUE</code> (the default), meaning that the variable being imputed
is not transformed when it is on the left hand model side.
<code>normpmm</code> assumes that the imputation regression parameter
estimates are multivariately normally distributed and that the
residual variance has a scaled chi-squared distribution.  For each
imputation a random draw of the estimates is taken and a random draw
from sigma is combined with those to get a random draw from the
posterior predicted value distribution.  Predictive mean matching is
then done matching these predicted values from incomplete observations
with predicted values from complete potential donor observations,
where the latter predictions are based on the imputation model least
squares parameter estimates and not on random draws from the posterior.
For the <code>plot</code> method, specify <code>type="hist"</code>
to draw histograms of imputed values with rug plots at the top, or
<code>type="ecdf"</code> (the default) to draw empirical CDFs with spike
histograms at the bottom.
</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_pmmtype">pmmtype</code></td>
<td>
<p>type of matching to be used for predictive mean
matching when <code>type="pmm"</code>.  <code>pmmtype=2</code> means that predicted 
values for both target incomplete and complete observations come from
a fit from the same bootstrap sample.  <code>pmmtype=1</code>, the default,
means that predicted values for complete observations are based
on additive regression fits on original complete observations (using last
imputations for non-target variables as with the other methds), and using
fits on a bootstrap sample to get predicted values for missing target variables.
See van Buuren (2012) section 3.4.2 where <code>pmmtype=1</code> is said to
work much better when the number of variables is small.
<code>pmmtype=3</code> means that complete observation predicted values come
from a bootstrap sample fit whereas target incomplete observation
predicted values come from a sample with replacement from the bootstrap
fit (approximate Bayesian bootstrap).</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_match">match</code></td>
<td>

<p>Defaults to <code>match="weighted"</code> to do weighted multinomial
probability sampling using the tricube function (similar to lowess)
as the weights.  The argument of the tricube function is the absolute
difference in transformed predicted values of all the donors and of
the target predicted value, divided by a scaling factor.
The scaling factor in the tricube function is <code>fweighted</code> times
the mean absolute difference between the target predicted value and
all the possible donor predicted values.  Set <code>match="closest"</code>
to find as the donor the observation having the closest predicted
transformed value, even if that same donor is found repeatedly.  Set
<code>match="kclosest"</code> to use a slower implementation that finds,
after jittering the complete case predicted values, the
<code>kclosest</code> complete cases on the target variable being imputed,
then takes a random sample of one of these <code>kclosest</code> cases.</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_kclosest">kclosest</code></td>
<td>
<p>see <code>match</code></p>
</td></tr>
<tr><td><code id="aregImpute_+3A_fweighted">fweighted</code></td>
<td>

<p>Smoothing parameter (multiple of mean absolute difference) used when
<code>match="weighted"</code>, with a default value of 0.2.  Set
<code>fweighted</code> to a number between 0.02 and 0.2 to force the donor
to have a predicted value closer to the target, and set
<code>fweighted</code> to larger values (but seldom larger than 1.0) to allow
donor values to be less tightly matched.  See the examples below to
learn how to study the relationship between <code>fweighted</code> and the
standard deviation of multiple imputations within individuals.</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_curtail">curtail</code></td>
<td>
<p>applies if <code>type='regression'</code>, causing imputed
values to be curtailed at the observed range of the target variable.
Set to <code>FALSE</code> to allow extrapolation outside the data range.</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_constraint">constraint</code></td>
<td>
<p>for predictive mean matching <code>constraint</code> is a
named list specifying R <code>expression()</code>s encoding constaints on
which donor observations are allowed to be used, based on variables
that are not missing, i.e., based on donor observations and/or
recipient observations as long as the target variable being imputed is
not used for the recipients.  The expressions must evaluate to a
logical vector with no <code>NA</code>s and whose length is the number of
rows in the donor observations.  The expressions refer to donor
observations by prefixing variable names by <code>d$</code>, and to a single
recipient observation by prefixing variables names by <code>r$</code>.</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_boot.method">boot.method</code></td>
<td>
<p>By default, simple boostrapping is used in which the
target variable is predicted using a sample with replacement from the
observations with non-missing target variable.  Specify
<code>boot.method='approximate bayesian'</code> to build the imputation
models from a sample with replacement from a sample with replacement
of the observations with non-missing targets.  Preliminary simulations
have shown this results in good confidence coverage of the final model
parameters when <code>type='regression'</code> is used.  Not implemented
when <code>group</code> is used.</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_burnin">burnin</code></td>
<td>

<p><code>aregImpute</code> does <code>burnin + n.impute</code> iterations of the
entire modeling process.  The first <code>burnin</code> imputations are
discarded.  More burn-in iteractions may be requied when multiple
variables are missing on the same observations.  When only one
variable is missing, no burn-ins are needed and <code>burnin</code> is set
to zero if unspecified.</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_pr">pr</code></td>
<td>

<p>set to <code>FALSE</code> to suppress printing of iteration messages
</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_plottrans">plotTrans</code></td>
<td>

<p>set to <code>TRUE</code> to plot <code>ace</code> or <code>avas</code> transformations
for each variable for each of the multiple imputations.  This is
useful for determining whether transformations are reasonable.  If
transformations are too noisy or have long flat sections (resulting in
&quot;lumps&quot; in the distribution of imputed values), it may be advisable to
place restrictions on the transformations (monotonicity or linearity).
</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_tolerance">tolerance</code></td>
<td>
<p>singularity criterion; list the source code in the
<code>lm.fit.qr.bare</code> function for details</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_b">B</code></td>
<td>
<p>number of bootstrap resamples to use if <code>nk</code> is a vector</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_digits">digits</code></td>
<td>
<p>number of digits for printing</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_nclass">nclass</code></td>
<td>
<p>number of bins to use in drawing histogram</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_datadensity">datadensity</code></td>
<td>
<p>see <code><a href="#topic+Ecdf">Ecdf</a></code></p>
</td></tr>
<tr><td><code id="aregImpute_+3A_diagnostics">diagnostics</code></td>
<td>

<p>Specify <code>diagnostics=TRUE</code> to draw plots of imputed values against
sequential imputation numbers, separately for each missing
observations and variable. 
</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_maxn">maxn</code></td>
<td>

<p>Maximum number of observations shown for diagnostics.  Default is
<code>maxn=10</code>, which limits the number of observations plotted to at most
the first 10.
</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_nperm">nperm</code></td>
<td>
<p>number of random formula permutations for <code>reformM</code>;
omit to sort variables by descending missing count.</p>
</td></tr>
<tr><td><code id="aregImpute_+3A_...">...</code></td>
<td>
<p>other arguments that are ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sequence of steps used by the <code>aregImpute</code> algorithm is the
following.
<br />
(1) For each variable containing m <code>NA</code>s where m &gt; 0, initialize the
<code>NA</code>s to values from a random sample (without replacement if
a sufficient number of non-missing values exist) of size m from the
non-missing values.
<br />
(2) For <code>burnin+n.impute</code> iterations do the following steps.  The
first <code>burnin</code> iterations provide a burn-in, and imputations are
saved only from the last <code>n.impute</code> iterations.
<br />
(3) For each variable containing any <code>NA</code>s, draw a sample with
replacement from the observations in the entire dataset in which the
current variable being imputed is non-missing.  Fit a flexible
additive model to predict this target variable while finding the
optimum transformation of it (unless the identity
transformation is forced).  Use this fitted flexible model to
predict the target variable in all of the original observations.
Impute each missing value of the target variable with the observed
value whose predicted transformed value is closest to the predicted
transformed value of the missing value (if <code>match="closest"</code> and
<code>type="pmm"</code>), 
or use a draw from a multinomial distribution with probabilities derived
from distance weights, if <code>match="weighted"</code> (the default).
<br />
(4) After these imputations are computed, use these random draw
imputations the next time the curent target variable is used as a
predictor of other sometimes-missing variables.
</p>
<p>When <code>match="closest"</code>, predictive mean matching does not work well
when fewer than 3 variables are used to predict the target variable,
because many of the multiple imputations for an observation will be
identical.  In the extreme case of one right-hand-side variable and
assuming that only monotonic transformations of left and right-side
variables are allowed, every bootstrap resample will give predicted
values of the target variable that are monotonically related to
predicted values from every other bootstrap resample.  The same is true
for Bayesian predicted values.  This causes predictive mean matching to
always match on the same donor observation.
</p>
<p>When the missingness mechanism for a variable is so systematic that the
distribution of observed values is truncated, predictive mean matching
does not work.  It will only yield imputed values that are near observed
values, so intervals in which no values are observed will not be
populated by imputed values.  For this case, the only hope is to make
regression assumptions and use extrapolation.  With
<code>type="regression"</code>, <code>aregImpute</code> will use linear
extrapolation to obtain a (hopefully) reasonable distribution of imputed
values.  The <code>"regression"</code> option causes <code>aregImpute</code> to
impute missing values by adding a random sample of residuals (with
replacement if there are more <code>NA</code>s than measured values) on the
transformed scale of the target variable.  After random residuals are
added, predicted random draws are obtained on the original untransformed
scale using reverse linear interpolation on the table of original and
transformed target values (linear extrapolation when a random residual
is large enough to put the random draw prediction outside the range of
observed values).  The bootstrap is used as with <code>type="pmm"</code> to
factor in the uncertainty of the imputation model.
</p>
<p>As model uncertainty is high when the transformation of a target
variable is unknown, <code>tlinear</code> defaults to <code>TRUE</code> to limit the
variance in predicted values when <code>nk</code> is positive.
</p>


<h3>Value</h3>

<p>a list of class <code>"aregImpute"</code> containing the following elements:
</p>
<table>
<tr><td><code>call</code></td>
<td>

<p>the function call expression
</p>
</td></tr>
<tr><td><code>formula</code></td>
<td>

<p>the formula specified to <code>aregImpute</code>
</p>
</td></tr>
<tr><td><code>match</code></td>
<td>

<p>the <code>match</code> argument
</p>
</td></tr>
<tr><td><code>fweighted</code></td>
<td>

<p>the <code>fweighted</code> argument
</p>
</td></tr>
<tr><td><code>n</code></td>
<td>

<p>total number of observations in input dataset
</p>
</td></tr>
<tr><td><code>p</code></td>
<td>

<p>number of variables
</p>
</td></tr>
<tr><td><code>na</code></td>
<td>

<p>list of subscripts of observations for which values were originally missing
</p>
</td></tr>
<tr><td><code>nna</code></td>
<td>

<p>named vector containing the numbers of missing values in the data
</p>
</td></tr>
<tr><td><code>type</code></td>
<td>

<p>vector of types of transformations used for each variable
(<code>"s","l","c"</code> for smooth spline, linear, or categorical with dummy
variables)
</p>
</td></tr>
<tr><td><code>tlinear</code></td>
<td>
<p>value of <code>tlinear</code> parameter</p>
</td></tr>
<tr><td><code>nk</code></td>
<td>
<p>number of knots used for smooth transformations</p>
</td></tr>
<tr><td><code>cat.levels</code></td>
<td>

<p>list containing character vectors specifying the <code>levels</code> of
categorical variables
</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>degrees of freedom (number of parameters estimated) for each
variable</p>
</td></tr>
<tr><td><code>n.impute</code></td>
<td>

<p>number of multiple imputations per missing value
</p>
</td></tr>
<tr><td><code>imputed</code></td>
<td>

<p>a list containing matrices of imputed values in the same format as
those created by <code>transcan</code>.  Categorical variables are coded using
their integer codes.  Variables having no missing values will have
<code>NULL</code> matrices in the list.
</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>if <code>x</code> is <code>TRUE</code>, the original data matrix with
integer codes for categorical variables</p>
</td></tr>
<tr><td><code>rsq</code></td>
<td>

<p>for the last round of imputations, a vector containing the R-squares
with which each sometimes-missing variable could be predicted from the
others by <code>ace</code> or <code>avas</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>van Buuren, Stef.  Flexible Imputation of Missing Data.  Chapman &amp;
Hall/CRC, Boca Raton FL, 2012.
</p>
<p>Little R, An H.  Robust likelihood-based analysis of multivariate data
with missing values. Statistica Sinica 14:949-968, 2004.
</p>
<p>van Buuren S, Brand JPL, Groothuis-Oudshoorn CGM, Rubin DB.  Fully
conditional specifications in multivariate imputation.  J Stat Comp
Sim 72:1049-1064, 2006.
</p>
<p>de Groot JAH, Janssen KJM, Zwinderman AH, Moons KGM, Reitsma JB.
Multiple imputation to correct for partial verification bias
revisited.  Stat Med 27:5880-5889, 2008.
</p>
<p>Siddique J. Multiple imputation using an iterative hot-deck with
distance-based donor selection.  Stat Med 27:83-102, 2008.
</p>
<p>White IR, Royston P, Wood AM.  Multiple imputation using chained
equations: Issues and guidance for practice.  Stat Med 30:377-399,
2011.
</p>
<p>Curnow E, Carpenter JR, Heron JE, et al: Multiple imputation of
missing data under missing at random: compatible imputation models are
not sufficient to avoid bias if they are mis-specified. J Clin Epi
June 9, 2023.  DOI:10.1016/j.jclinepi.2023.06.011. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.mult.impute">fit.mult.impute</a></code>, <code><a href="#topic+transcan">transcan</a></code>, <code><a href="#topic+areg">areg</a></code>, <code><a href="#topic+naclus">naclus</a></code>, <code><a href="#topic+naplot">naplot</a></code>, <code><a href="mice.html#topic+mice">mice</a></code>,
<code><a href="#topic+dotchart3">dotchart3</a></code>, <code><a href="#topic+Ecdf">Ecdf</a></code>, <code><a href="#topic+completer">completer</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Check that aregImpute can almost exactly estimate missing values when
# there is a perfect nonlinear relationship between two variables
# Fit restricted cubic splines with 4 knots for x1 and x2, linear for x3
set.seed(3)
x1 &lt;- rnorm(200)
x2 &lt;- x1^2
x3 &lt;- runif(200)
m &lt;- 30
x2[1:m] &lt;- NA
a &lt;- aregImpute(~x1+x2+I(x3), n.impute=5, nk=4, match='closest')
a
matplot(x1[1:m]^2, a$imputed$x2)
abline(a=0, b=1, lty=2)

x1[1:m]^2
a$imputed$x2


# Multiple imputation and estimation of variances and covariances of
# regression coefficient estimates accounting for imputation
# Example 1: large sample size, much missing data, no overlap in
# NAs across variables
x1 &lt;- factor(sample(c('a','b','c'),1000,TRUE))
x2 &lt;- (x1=='b') + 3*(x1=='c') + rnorm(1000,0,2)
x3 &lt;- rnorm(1000)
y  &lt;- x2 + 1*(x1=='c') + .2*x3 + rnorm(1000,0,2)
orig.x1 &lt;- x1[1:250]
orig.x2 &lt;- x2[251:350]
x1[1:250] &lt;- NA
x2[251:350] &lt;- NA
d &lt;- data.frame(x1,x2,x3,y, stringsAsFactors=TRUE)
# Find value of nk that yields best validating imputation models
# tlinear=FALSE means to not force the target variable to be linear
f &lt;- aregImpute(~y + x1 + x2 + x3, nk=c(0,3:5), tlinear=FALSE,
                data=d, B=10) # normally B=75
f
# Try forcing target variable (x1, then x2) to be linear while allowing
# predictors to be nonlinear (could also say tlinear=TRUE)
f &lt;- aregImpute(~y + x1 + x2 + x3, nk=c(0,3:5), data=d, B=10)
f

## Not run: 
# Use 100 imputations to better check against individual true values
f &lt;- aregImpute(~y + x1 + x2 + x3, n.impute=100, data=d)
f
par(mfrow=c(2,1))
plot(f)
modecat &lt;- function(u) {
 tab &lt;- table(u)
 as.numeric(names(tab)[tab==max(tab)][1])
}
table(orig.x1,apply(f$imputed$x1, 1, modecat))
par(mfrow=c(1,1))
plot(orig.x2, apply(f$imputed$x2, 1, mean))
fmi &lt;- fit.mult.impute(y ~ x1 + x2 + x3, lm, f, 
                       data=d)
sqrt(diag(vcov(fmi)))
fcc &lt;- lm(y ~ x1 + x2 + x3)
summary(fcc)   # SEs are larger than from mult. imputation

## End(Not run)
## Not run: 
# Example 2: Very discriminating imputation models,
# x1 and x2 have some NAs on the same rows, smaller n
set.seed(5)
x1 &lt;- factor(sample(c('a','b','c'),100,TRUE))
x2 &lt;- (x1=='b') + 3*(x1=='c') + rnorm(100,0,.4)
x3 &lt;- rnorm(100)
y  &lt;- x2 + 1*(x1=='c') + .2*x3 + rnorm(100,0,.4)
orig.x1 &lt;- x1[1:20]
orig.x2 &lt;- x2[18:23]
x1[1:20] &lt;- NA
x2[18:23] &lt;- NA
#x2[21:25] &lt;- NA
d &lt;- data.frame(x1,x2,x3,y, stringsAsFactors=TRUE)
n &lt;- naclus(d)
plot(n); naplot(n)  # Show patterns of NAs
# 100 imputations to study them; normally use 5 or 10
f  &lt;- aregImpute(~y + x1 + x2 + x3, n.impute=100, nk=0, data=d)
par(mfrow=c(2,3))
plot(f, diagnostics=TRUE, maxn=2)
# Note: diagnostics=TRUE makes graphs similar to those made by:
# r &lt;- range(f$imputed$x2, orig.x2)
# for(i in 1:6) {  # use 1:2 to mimic maxn=2
#   plot(1:100, f$imputed$x2[i,], ylim=r,
#        ylab=paste("Imputations for Obs.",i))
#   abline(h=orig.x2[i],lty=2)
# }

table(orig.x1,apply(f$imputed$x1, 1, modecat))
par(mfrow=c(1,1))
plot(orig.x2, apply(f$imputed$x2, 1, mean))


fmi &lt;- fit.mult.impute(y ~ x1 + x2, lm, f, 
                       data=d)
sqrt(diag(vcov(fmi)))
fcc &lt;- lm(y ~ x1 + x2)
summary(fcc)   # SEs are larger than from mult. imputation

## End(Not run)

## Not run: 
# Study relationship between smoothing parameter for weighting function
# (multiplier of mean absolute distance of transformed predicted
# values, used in tricube weighting function) and standard deviation
# of multiple imputations.  SDs are computed from average variances
# across subjects.  match="closest" same as match="weighted" with
# small value of fweighted.
# This example also shows problems with predicted mean
# matching almost always giving the same imputed values when there is
# only one predictor (regression coefficients change over multiple
# imputations but predicted values are virtually 1-1 functions of each
# other)

set.seed(23)
x &lt;- runif(200)
y &lt;- x + runif(200, -.05, .05)
r &lt;- resid(lsfit(x,y))
rmse &lt;- sqrt(sum(r^2)/(200-2))   # sqrt of residual MSE

y[1:20] &lt;- NA
d &lt;- data.frame(x,y)
f &lt;- aregImpute(~ x + y, n.impute=10, match='closest', data=d)
# As an aside here is how to create a completed dataset for imputation
# number 3 as fit.mult.impute would do automatically.  In this degenerate
# case changing 3 to 1-2,4-10 will not alter the results.
imputed &lt;- impute.transcan(f, imputation=3, data=d, list.out=TRUE,
                           pr=FALSE, check=FALSE)
sd &lt;- sqrt(mean(apply(f$imputed$y, 1, var)))

ss &lt;- c(0, .01, .02, seq(.05, 1, length=20))
sds &lt;- ss; sds[1] &lt;- sd

for(i in 2:length(ss)) {
  f &lt;- aregImpute(~ x + y, n.impute=10, fweighted=ss[i])
  sds[i] &lt;- sqrt(mean(apply(f$imputed$y, 1, var)))
}

plot(ss, sds, xlab='Smoothing Parameter', ylab='SD of Imputed Values',
     type='b')
abline(v=.2,  lty=2)  # default value of fweighted
abline(h=rmse, lty=2)  # root MSE of residuals from linear regression

## End(Not run)

## Not run: 
# Do a similar experiment for the Titanic dataset
getHdata(titanic3)
h &lt;- lm(age ~ sex + pclass + survived, data=titanic3)
rmse &lt;- summary(h)$sigma
set.seed(21)
f &lt;- aregImpute(~ age + sex + pclass + survived, n.impute=10,
                data=titanic3, match='closest')
sd &lt;- sqrt(mean(apply(f$imputed$age, 1, var)))

ss &lt;- c(0, .01, .02, seq(.05, 1, length=20))
sds &lt;- ss; sds[1] &lt;- sd

for(i in 2:length(ss)) {
  f &lt;- aregImpute(~ age + sex + pclass + survived, data=titanic3,
                  n.impute=10, fweighted=ss[i])
  sds[i] &lt;- sqrt(mean(apply(f$imputed$age, 1, var)))
}

plot(ss, sds, xlab='Smoothing Parameter', ylab='SD of Imputed Values',
     type='b')
abline(v=.2,   lty=2)  # default value of fweighted
abline(h=rmse, lty=2)  # root MSE of residuals from linear regression

## End(Not run)


set.seed(2)
d &lt;- data.frame(x1=runif(50), x2=c(rep(NA, 10), runif(40)),
                x3=c(runif(4), rep(NA, 11), runif(35)))
reformM(~ x1 + x2 + x3, data=d)
reformM(~ x1 + x2 + x3, data=d, nperm=2)
# Give result or one of the results as the first argument to aregImpute

# Constrain imputed values for two variables
# Require imputed values for x2 to be above 0.2
# Assume x1 is never missing and require imputed values for
# x3 to be less than the recipient's value of x1
a &lt;- aregImpute(~ x1 + x2 + x3, data=d,
                constraint=list(x2 = expression(d$x2 &gt; 0.2),
                                x3 = expression(d$x3 &lt; r$x1)))
a
</code></pre>

<hr>
<h2 id='binconf'>
Confidence Intervals for Binomial Probabilities
</h2><span id='topic+binconf'></span>

<h3>Description</h3>

<p>Produces 1-alpha confidence intervals for binomial probabilities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>binconf(x, n, alpha=0.05,
        method=c("wilson","exact","asymptotic","all"),
        include.x=FALSE, include.n=FALSE, return.df=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binconf_+3A_x">x</code></td>
<td>

<p>vector containing the number of &quot;successes&quot; for binomial variates
</p>
</td></tr>
<tr><td><code id="binconf_+3A_n">n</code></td>
<td>

<p>vector containing the numbers of corresponding observations 
</p>
</td></tr>
<tr><td><code id="binconf_+3A_alpha">alpha</code></td>
<td>

<p>probability of a type I error, so confidence coefficient = 1-alpha
</p>
</td></tr>
<tr><td><code id="binconf_+3A_method">method</code></td>
<td>

<p>character string specifing which method to use.  The &quot;all&quot; method only
works when 
x and n are length 1.  The &quot;exact&quot; method uses the F distribution
to compute exact (based on the binomial cdf) intervals; the
&quot;wilson&quot; interval is score-test-based; and the &quot;asymptotic&quot; is the
text-book, asymptotic normal interval.  Following Agresti and
Coull, the Wilson interval is to be preferred and so is the
default.
</p>
</td></tr>
<tr><td><code id="binconf_+3A_include.x">include.x</code></td>
<td>

<p>logical flag to indicate whether <code>x</code> should be included in the
returned matrix or data frame 
</p>
</td></tr>
<tr><td><code id="binconf_+3A_include.n">include.n</code></td>
<td>

<p>logical flag to indicate whether <code>n</code> should be included in the
returned matrix or data frame 
</p>
</td></tr>
<tr><td><code id="binconf_+3A_return.df">return.df</code></td>
<td>

<p>logical flag to indicate that a data frame rather than a matrix be
returned
</p>
</td></tr></table>


<h3>Value</h3>

<p>a matrix or data.frame containing the computed intervals and,
optionally, <code>x</code> and <code>n</code>.  
</p>


<h3>Author(s)</h3>

<p>Rollin Brant, Modified by Frank Harrell and
<br />
Brad Biggerstaff
<br />
Centers for Disease Control and Prevention
<br />
National Center for Infectious Diseases
<br />
Division of Vector-Borne Infectious Diseases
<br />
P.O. Box 2087, Fort Collins, CO, 80522-2087, USA
<br />
<a href="mailto:bkb5@cdc.gov">bkb5@cdc.gov</a>
</p>


<h3>References</h3>

<p>A. Agresti and B.A. Coull, Approximate is better than &quot;exact&quot; for
interval estimation of binomial proportions,  
<em>American Statistician,</em>
<b>52</b>:119&ndash;126, 1998.
</p>
<p>R.G. Newcombe, Logit confidence intervals and the inverse sinh
transformation,
<em>American Statistician,</em>
<b>55</b>:200&ndash;202, 2001.
</p>
<p>L.D. Brown, T.T. Cai and A. DasGupta, Interval estimation for
a binomial proportion (with discussion),
<em>Statistical Science,</em>
<b>16</b>:101&ndash;133, 2001.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>binconf(0:10,10,include.x=TRUE,include.n=TRUE)
binconf(46,50,method="all")
</code></pre>

<hr>
<h2 id='biVar'>Bivariate Summaries Computed Separately by a Series of Predictors</h2><span id='topic+biVar'></span><span id='topic+print.biVar'></span><span id='topic+plot.biVar'></span><span id='topic+spearman2'></span><span id='topic+spearman2.default'></span><span id='topic+spearman2.formula'></span><span id='topic+spearman'></span><span id='topic+spearman.test'></span><span id='topic+chiSquare'></span>

<h3>Description</h3>

<p><code>biVar</code> is a generic function that accepts a formula and usual
<code>data</code>, <code>subset</code>, and <code>na.action</code> parameters plus a
list <code>statinfo</code> that specifies a function of two variables to
compute along with information about labeling results for printing and
plotting.  The function is called separately with each right hand side
variable and the same left hand variable.  The result is a matrix of
bivariate statistics and the <code>statinfo</code> list that drives printing
and plotting.  The plot method draws a dot plot with x-axis values by
default sorted in order of one of the statistics computed by the function.
</p>
<p><code>spearman2</code> computes the square of Spearman's rho rank correlation
and a generalization of it in which <code>x</code> can relate
non-monotonically to <code>y</code>.  This is done by computing the Spearman
multiple rho-squared between <code>(rank(x), rank(x)^2)</code> and <code>y</code>.
When <code>x</code> is categorical, a different kind of Spearman correlation
used in the Kruskal-Wallis test is computed (and <code>spearman2</code> can do
the Kruskal-Wallis test).  This is done by computing the ordinary
multiple <code>R^2</code> between <code>k-1</code> dummy variables and
<code>rank(y)</code>, where <code>x</code> has <code>k</code> categories.  <code>x</code> can
also be a formula, in which case each predictor is correlated separately
with <code>y</code>, using non-missing observations for that predictor.
<code>biVar</code> is used to do the looping and bookkeeping.  By default the
plot shows the adjusted <code>rho^2</code>, using the same formula used for
the ordinary adjusted <code>R^2</code>.  The <code>F</code> test uses the unadjusted
R2.
</p>
<p><code>spearman</code> computes Spearman's rho on non-missing values of two
variables.  <code>spearman.test</code> is a simple version of
<code>spearman2.default</code>.
</p>
<p><code>chiSquare</code> is set up like <code>spearman2</code> except it is intended
for a categorical response variable.  Separate Pearson chi-square tests
are done for each predictor, with optional collapsing of infrequent
categories.  Numeric predictors having more than <code>g</code> levels are
categorized into <code>g</code> quantile groups.  <code>chiSquare</code> uses
<code>biVar</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>biVar(formula, statinfo, data=NULL, subset=NULL,
      na.action=na.retain, exclude.imputed=TRUE, ...)

## S3 method for class 'biVar'
print(x, ...)

## S3 method for class 'biVar'
plot(x, what=info$defaultwhat,
                       sort.=TRUE, main, xlab,
                       vnames=c('names','labels'), ...)

spearman2(x, ...)

## Default S3 method:
spearman2(x, y, p=1, minlev=0, na.rm=TRUE, exclude.imputed=na.rm, ...)

## S3 method for class 'formula'
spearman2(formula, data=NULL,
          subset, na.action=na.retain, exclude.imputed=TRUE, ...)

spearman(x, y)

spearman.test(x, y, p=1)

chiSquare(formula, data=NULL, subset=NULL, na.action=na.retain,
          exclude.imputed=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="biVar_+3A_formula">formula</code></td>
<td>
<p>a formula with a single left side variable</p>
</td></tr>
<tr><td><code id="biVar_+3A_statinfo">statinfo</code></td>
<td>
<p>see <code>spearman2.formula</code> or <code>chiSquare</code> code</p>
</td></tr>
<tr><td><code id="biVar_+3A_data">data</code>, <code id="biVar_+3A_subset">subset</code>, <code id="biVar_+3A_na.action">na.action</code></td>
<td>

<p>the usual options for models.  Default for <code>na.action</code> is to retain
all values, NA or not, so that NAs can be deleted in only a pairwise
fashion.
</p>
</td></tr>
<tr><td><code id="biVar_+3A_exclude.imputed">exclude.imputed</code></td>
<td>

<p>set to <code>FALSE</code> to include imputed values (created by
<code>impute</code>) in the calculations.
</p>
</td></tr>
<tr><td><code id="biVar_+3A_...">...</code></td>
<td>
<p>other arguments that are passed to the function used to
compute the bivariate statistics or to <code>dotchart3</code> for
<code>plot</code>.
</p>
</td></tr>
<tr><td><code id="biVar_+3A_na.rm">na.rm</code></td>
<td>
<p>logical; delete NA values?</p>
</td></tr>
<tr><td><code id="biVar_+3A_x">x</code></td>
<td>

<p>a numeric matrix with at least 5 rows and at least 2 columns (if
<code>y</code> is absent).  For <code>spearman2</code>, the first argument may
be a vector of any type, including character or factor.  The first
argument may also be a formula, in which case all predictors are
correlated individually with 
the response variable.  <code>x</code> may be a formula for <code>spearman2</code>
in which case <code>spearman2.formula</code> is invoked.  Each
predictor in the right hand side of the formula is separately correlated
with the response variable.  For <code>print</code> or <code>plot</code>, <code>x</code>
is an object produced by <code>biVar</code>.  For <code>spearman</code> and
<code>spearman.test</code> <code>x</code> is a numeric vector, as is <code>y</code>.  For
<code>chiSquare</code>, <code>x</code> is a formula.
</p>
</td></tr>
<tr><td><code id="biVar_+3A_y">y</code></td>
<td>

<p>a numeric vector
</p>
</td></tr>
<tr><td><code id="biVar_+3A_p">p</code></td>
<td>

<p>for numeric variables, specifies the order of the Spearman <code>rho^2</code> to
use.  The default is <code>p=1</code> to compute the ordinary
<code>rho^2</code>.  Use <code>p=2</code> to compute the quadratic rank
generalization to allow non-monotonicity.  <code>p</code> is ignored for
categorical predictors.
</p>
</td></tr>
<tr><td><code id="biVar_+3A_minlev">minlev</code></td>
<td>

<p>minimum relative frequency that a level of a categorical predictor
should have before it is pooled with other categories (see
<code>combine.levels</code>) in <code>spearman2</code> and <code>chiSquare</code> (in
which case it also applies to the response).  The default,
<code>minlev=0</code> causes no pooling.
</p>
</td></tr>
<tr><td><code id="biVar_+3A_what">what</code></td>
<td>

<p>specifies which statistic to plot.  Possibilities include the
column names that appear with the print method is used.
</p>
</td></tr>
<tr><td><code id="biVar_+3A_sort.">sort.</code></td>
<td>

<p>set <code>sort.=FALSE</code> to suppress sorting variables by the
statistic being plotted
</p>
</td></tr>
<tr><td><code id="biVar_+3A_main">main</code></td>
<td>

<p>main title for plot.  Default title shows the name of the response
variable.
</p>
</td></tr>
<tr><td><code id="biVar_+3A_xlab">xlab</code></td>
<td>

<p>x-axis label.  Default constructed from <code>what</code>.
</p>
</td></tr>
<tr><td><code id="biVar_+3A_vnames">vnames</code></td>
<td>

<p>set to <code>"labels"</code> to use variable labels in place of names for
plotting.  If a variable does not have a label the name is always
used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses midranks in case of ties, as described by Hollander and Wolfe.
P-values for Spearman, Wilcoxon, or Kruskal-Wallis tests are
approximated by using the <code>t</code> or <code>F</code> distributions.
</p>


<h3>Value</h3>

<p><code>spearman2.default</code> (the
function that is called for a single <code>x</code>, i.e., when there is no
formula) returns a vector of statistics for the variable.
<code>biVar</code>, <code>spearman2.formula</code>, and <code>chiSquare</code> return a
matrix with rows corresponding to predictors.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Hollander M. and Wolfe D.A. (1973).  Nonparametric Statistical Methods.
New York: Wiley.
</p>
<p>Press WH, Flannery BP, Teukolsky SA, Vetterling, WT (1988): Numerical
Recipes in C.  Cambridge: Cambridge University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+combine.levels">combine.levels</a></code>,
<code><a href="#topic+varclus">varclus</a></code>, <code><a href="#topic+dotchart3">dotchart3</a></code>, <code><a href="#topic+impute">impute</a></code>,
<code><a href="stats.html#topic+chisq.test">chisq.test</a></code>, <code><a href="#topic+cut2">cut2</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(-2, -1, 0, 1, 2)
y &lt;- c(4,   1, 0, 1, 4)
z &lt;- c(1,   2, 3, 4, NA)
v &lt;- c(1,   2, 3, 4, 5)

spearman2(x, y)
plot(spearman2(z ~ x + y + v, p=2))

f &lt;- chiSquare(z ~ x + y + v)
f
</code></pre>

<hr>
<h2 id='bootkm'>
Bootstrap Kaplan-Meier Estimates
</h2><span id='topic+bootkm'></span>

<h3>Description</h3>

<p>Bootstraps Kaplan-Meier estimate of the probability of survival to at
least a fixed time (<code>times</code> variable) or the estimate of the <code>q</code>
quantile of the survival distribution (e.g., median survival time, the
default).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootkm(S, q=0.5, B=500, times, pr=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootkm_+3A_s">S</code></td>
<td>

<p>a <code>Surv</code> object for possibly right-censored survival time
</p>
</td></tr>
<tr><td><code id="bootkm_+3A_q">q</code></td>
<td>

<p>quantile of survival time, default is 0.5 for median
</p>
</td></tr>
<tr><td><code id="bootkm_+3A_b">B</code></td>
<td>

<p>number of bootstrap repetitions (default=500)
</p>
</td></tr>
<tr><td><code id="bootkm_+3A_times">times</code></td>
<td>

<p>time vector (currently only a scalar is allowed) at which to compute
survival estimates.  You may specify only one of <code>q</code> and
<code>times</code>, and if <code>times</code> is specified <code>q</code> is ignored. 
</p>
</td></tr>
<tr><td><code id="bootkm_+3A_pr">pr</code></td>
<td>

<p>set to <code>FALSE</code> to suppress printing the iteration number every
10 iterations
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>bootkm</code> uses Therneau's <code>survfitKM</code> function to efficiently
compute Kaplan-Meier estimates.
</p>


<h3>Value</h3>

<p>a vector containing <code>B</code> bootstrap estimates
</p>


<h3>Side Effects</h3>

<p>updates <code>.Random.seed</code>, and, if <code>pr=TRUE</code>, prints progress
of simulations
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University School of Medicine
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Akritas MG (1986): Bootstrapping the Kaplan-Meier estimator.  JASA
81:1032&ndash;1038.
</p>


<h3>See Also</h3>

<p><code><a href="survival.html#topic+survfit">survfit</a></code>, <code><a href="survival.html#topic+Surv">Surv</a></code>,
<code><a href="rms.html#topic+Survival.cph">Survival.cph</a></code>, <code><a href="rms.html#topic+Quantile.cph">Quantile.cph</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compute 0.95 nonparametric confidence interval for the difference in
# median survival time between females and males (two-sample problem)
set.seed(1)
library(survival)
S &lt;- Surv(runif(200))      # no censoring
sex &lt;- c(rep('female',100),rep('male',100))
med.female &lt;- bootkm(S[sex=='female',], B=100) # normally B=500
med.male   &lt;- bootkm(S[sex=='male',],   B=100)
describe(med.female-med.male)
quantile(med.female-med.male, c(.025,.975), na.rm=TRUE)
# na.rm needed because some bootstrap estimates of median survival
# time may be missing when a bootstrap sample did not include the
# longer survival times
</code></pre>

<hr>
<h2 id='bpower'>
Power and Sample Size for Two-Sample Binomial Test
</h2><span id='topic+bpower'></span><span id='topic+bsamsize'></span><span id='topic+ballocation'></span><span id='topic+bpower.sim'></span>

<h3>Description</h3>

<p>Uses method of Fleiss, Tytun, and Ury (but without the continuity
correction) to estimate the power (or the sample size to achieve a given
power) of a two-sided test for the difference in two proportions.  The two
sample sizes are allowed to be unequal, but for <code>bsamsize</code> you must specify
the fraction of observations in group 1.  For power calculations, one
probability (<code>p1</code>) must be given, and either the other probability (<code>p2</code>),
an <code>odds.ratio</code>, or a <code>percent.reduction</code> must be given.  For <code>bpower</code> or
<code>bsamsize</code>, any or all of the arguments may be vectors, in which case they
return a vector of powers or sample sizes.  All vector arguments must have
the same length.
</p>
<p>Given <code>p1, p2</code>, <code>ballocation</code> uses the method of Brittain and Schlesselman
to compute the optimal fraction of observations to be placed in group 1
that either (1) minimize the variance of the difference in two proportions,
(2) minimize the variance of the ratio of the two proportions, 
(3) minimize the variance of the log odds ratio, or
(4) maximize the power of the 2-tailed test for differences.  For (4)
the total sample size must be given, or the fraction optimizing
the power is not returned.  The fraction for (3) is one minus the fraction
for (1).
</p>
<p><code>bpower.sim</code> estimates power by simulations, in minimal time.  By using
<code>bpower.sim</code> you can see that the formulas without any continuity correction
are quite accurate, and that the power of a continuity-corrected test
is significantly lower.  That's why no continuity corrections are implemented
here.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bpower(p1, p2, odds.ratio, percent.reduction, 
       n, n1, n2, alpha=0.05)


bsamsize(p1, p2, fraction=.5, alpha=.05, power=.8)


ballocation(p1, p2, n, alpha=.05)


bpower.sim(p1, p2, odds.ratio, percent.reduction, 
           n, n1, n2, 
           alpha=0.05, nsim=10000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bpower_+3A_p1">p1</code></td>
<td>

<p>population probability in the group 1
</p>
</td></tr>
<tr><td><code id="bpower_+3A_p2">p2</code></td>
<td>

<p>probability for group 2
</p>
</td></tr>
<tr><td><code id="bpower_+3A_odds.ratio">odds.ratio</code></td>
<td>
<p>odds ratio to detect</p>
</td></tr>
<tr><td><code id="bpower_+3A_percent.reduction">percent.reduction</code></td>
<td>
<p>percent reduction in risk to detect</p>
</td></tr>
<tr><td><code id="bpower_+3A_n">n</code></td>
<td>

<p>total sample size over the two groups.  If you omit this for
<code>ballocation</code>, the <code>fraction</code> which optimizes power will not be
returned.
</p>
</td></tr>
<tr><td><code id="bpower_+3A_n1">n1</code></td>
<td>
<p>sample size in group 1</p>
</td></tr>
<tr><td><code id="bpower_+3A_n2">n2</code></td>
<td>
<p>sample size in group 2.  <code>bpower</code>, if <code>n</code> is given,
<code>n1</code> and <code>n2</code> are set to <code>n/2</code>.</p>
</td></tr>
<tr><td><code id="bpower_+3A_alpha">alpha</code></td>
<td>
<p>type I assertion probability</p>
</td></tr>
<tr><td><code id="bpower_+3A_fraction">fraction</code></td>
<td>
<p>fraction of observations in group 1</p>
</td></tr>
<tr><td><code id="bpower_+3A_power">power</code></td>
<td>
<p>the desired probability of detecting a difference</p>
</td></tr>
<tr><td><code id="bpower_+3A_nsim">nsim</code></td>
<td>
<p>number of simulations of binomial responses</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>bpower.sim</code>, all arguments must be of length one.
</p>


<h3>Value</h3>

<p>for <code>bpower</code>, the power estimate; for <code>bsamsize</code>, a vector containing
the sample sizes in the two groups; for <code>ballocation</code>, a vector with
4 fractions of observations allocated to group 1, optimizing the four
criteria mentioned above.  For <code>bpower.sim</code>, a vector with three
elements is returned, corresponding to the simulated power and its
lower and upper 0.95 confidence limits.
</p>


<h3>AUTHOR</h3>

<p>Frank Harrell
</p>
<p>Department of Biostatistics
</p>
<p>Vanderbilt University
</p>
<p><a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Fleiss JL, Tytun A, Ury HK (1980): A simple approximation for calculating
sample sizes for comparing independent proportions.  Biometrics 36:343&ndash;6.
</p>
<p>Brittain E, Schlesselman JJ (1982): Optimal allocation for the comparison
of proportions.  Biometrics 38:1003&ndash;9.
</p>
<p>Gordon I, Watson R (1996): The myth of continuity-corrected sample size
formulae.  Biometrics 52:71&ndash;6.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+samplesize.bin">samplesize.bin</a></code>, <code><a href="stats.html#topic+chisq.test">chisq.test</a></code>, <code><a href="#topic+binconf">binconf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>bpower(.1, odds.ratio=.9, n=1000, alpha=c(.01,.05))
bpower.sim(.1, odds.ratio=.9, n=1000)
bsamsize(.1, .05, power=.95)
ballocation(.1, .5, n=100)


# Plot power vs. n for various odds ratios  (base prob.=.1)
n  &lt;- seq(10, 1000, by=10)
OR &lt;- seq(.2,.9,by=.1)
plot(0, 0, xlim=range(n), ylim=c(0,1), xlab="n", ylab="Power", type="n")
for(or in OR) {
  lines(n, bpower(.1, odds.ratio=or, n=n))
  text(350, bpower(.1, odds.ratio=or, n=350)-.02, format(or))
}


# Another way to plot the same curves, but letting labcurve do the
# work, including labeling each curve at points of maximum separation
pow &lt;- lapply(OR, function(or,n)list(x=n,y=bpower(p1=.1,odds.ratio=or,n=n)),
              n=n)
names(pow) &lt;- format(OR)
labcurve(pow, pl=TRUE, xlab='n', ylab='Power')


# Contour graph for various probabilities of outcome in the control
# group, fixing the odds ratio at .8 ([p2/(1-p2) / p1/(1-p1)] = .8)
# n is varied also
p1 &lt;- seq(.01,.99,by=.01)
n  &lt;- seq(100,5000,by=250)
pow &lt;- outer(p1, n, function(p1,n) bpower(p1, n=n, odds.ratio=.8))
# This forms a length(p1)*length(n) matrix of power estimates
contour(p1, n, pow)
</code></pre>

<hr>
<h2 id='bpplot'>
Box-percentile plots
</h2><span id='topic+bpplot'></span>

<h3>Description</h3>

<p>Producess side-by-side box-percentile plots from several vectors or a
list of vectors.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bpplot(..., name=TRUE, main="Box-Percentile Plot", 
       xlab="", ylab="", srtx=0, plotopts=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bpplot_+3A_...">...</code></td>
<td>

<p>vectors or lists containing 
numeric components (e.g., the output of <code>split</code>).
</p>
</td></tr>
<tr><td><code id="bpplot_+3A_name">name</code></td>
<td>

<p>character vector of names for the groups.  
Default is <code>TRUE</code> to put names on the x-axis.  Such names are taken from the 
data vectors or the <code>names</code> attribute of the first argument if it is a list.
Set <code>name</code> to <code>FALSE</code> to suppress names.
If a character vector is supplied the names in the vector are
used to label the groups.
</p>
</td></tr>
<tr><td><code id="bpplot_+3A_main">main</code></td>
<td>

<p>main title for the plot.
</p>
</td></tr>
<tr><td><code id="bpplot_+3A_xlab">xlab</code></td>
<td>

<p>x axis label.
</p>
</td></tr>
<tr><td><code id="bpplot_+3A_ylab">ylab</code></td>
<td>

<p>y axis label.
</p>
</td></tr>
<tr><td><code id="bpplot_+3A_srtx">srtx</code></td>
<td>
<p>rotation angle for x-axis labels.  Default is zero.</p>
</td></tr>
<tr><td><code id="bpplot_+3A_plotopts">plotopts</code></td>
<td>
<p>a list of other parameters to send to <code>plot</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>There are no returned values
</p>


<h3>Side Effects</h3>

<p>A plot is created on the current graphics device.
</p>


<h3>BACKGROUND</h3>

<p>Box-percentile plots are similiar to boxplots, except box-percentile plots
supply more information about the univariate distributions.  At any height
the width of the irregular &quot;box&quot; is proportional to the percentile of that
height, up to the 50th percentile, and above the 50th percentile the width
is proportional to 100 minus the percentile.  Thus, the width at any given
height is proportional to the percent of observations that are more 
extreme in that direction.  As in boxplots, the median, 25th and 75th 
percentiles are marked with line segments across the box.
</p>


<h3>Author(s)</h3>

<p>Jeffrey Banfield
<br />
<a href="mailto:umsfjban@bill.oscs.montana.edu">umsfjban@bill.oscs.montana.edu</a>
<br />
Modified by F. Harrell 30Jun97
</p>


<h3>References</h3>

<p>Esty WW, Banfield J: The box-percentile plot.  J Statistical
Software 8 No. 17, 2003.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+panel.bpplot">panel.bpplot</a></code>, <code><a href="graphics.html#topic+boxplot">boxplot</a></code>, <code><a href="#topic+Ecdf">Ecdf</a></code>,
<code><a href="lattice.html#topic+xyplot">bwplot</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x1 &lt;- rnorm(500)
x2 &lt;- runif(500, -2, 2)
x3 &lt;- abs(rnorm(500))-2
bpplot(x1, x2, x3)
g &lt;- sample(1:2, 500, replace=TRUE)
bpplot(split(x2, g), name=c('Group 1','Group 2'))
rm(x1,x2,x3,g)
</code></pre>

<hr>
<h2 id='bystats'>
Statistics by Categories
</h2><span id='topic+bystats'></span><span id='topic+print.bystats'></span><span id='topic+latex.bystats'></span><span id='topic+bystats2'></span><span id='topic+print.bystats2'></span><span id='topic+latex.bystats2'></span>

<h3>Description</h3>

<p>For any number of cross-classification variables, <code>bystats</code>
returns a matrix with the sample size, number missing <code>y</code>, and
<code>fun(non-missing y)</code>, with the cross-classifications designated
by rows. Uses Harrell's modification of the <code>interaction</code>
function to produce cross-classifications.  The default <code>fun</code> is
<code>mean</code>, and if <code>y</code> is binary, the mean is labeled as
<code>Fraction</code>.  There is a <code>print</code> method as well as a
<code>latex</code> method for objects created by <code>bystats</code>.
<code>bystats2</code> handles the special case in which there are 2
classifcation variables, and places the first one in rows and the
second in columns.  The <code>print</code> method for <code>bystats2</code> uses
the <code>print.char.matrix</code> function to organize statistics
for cells into boxes. </p>


<h3>Usage</h3>

<pre><code class='language-R'>bystats(y, ..., fun, nmiss, subset)
## S3 method for class 'bystats'
print(x, ...)
## S3 method for class 'bystats'
latex(object, title, caption, rowlabel, ...)
bystats2(y, v, h, fun, nmiss, subset)
## S3 method for class 'bystats2'
print(x, abbreviate.dimnames=FALSE,
   prefix.width=max(nchar(dimnames(x)[[1]])), ...)
## S3 method for class 'bystats2'
latex(object, title, caption, rowlabel, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bystats_+3A_y">y</code></td>
<td>

<p>a binary, logical, or continuous variable or a matrix or data frame of
such variables.  If <code>y</code> is a data frame it is converted to a matrix.
If <code>y</code> is a data frame or matrix, computations are done on subsets of
the rows of <code>y</code>, and you should specify <code>fun</code> so as to be able to operate
on the matrix.  For matrix <code>y</code>, any column with a missing value causes
the entire row to be considered missing, and the row is not passed to
<code>fun</code>.
</p>
</td></tr>
<tr><td><code id="bystats_+3A_...">...</code></td>
<td>

<p>For <code>bystats</code>, one or more classifcation variables separated by commas.
For <code>print.bystats</code>, options passed to <code>print.default</code> such as <code>digits</code>.
For <code>latex.bystats</code>, and <code>latex.bystats2</code>,
options passed to <code>latex.default</code> such as <code>digits</code>.
If you pass <code>cdec</code> to <code>latex.default</code>, keep in mind that the first one or
two positions (depending on <code>nmiss</code>) should have zeros since these
correspond with frequency counts. 
</p>
</td></tr>
<tr><td><code id="bystats_+3A_v">v</code></td>
<td>

<p>vertical variable for <code>bystats2</code>.  Will be converted to <code>factor</code>.
</p>
</td></tr>
<tr><td><code id="bystats_+3A_h">h</code></td>
<td>

<p>horizontal variable for <code>bystats2</code>.  Will be converted to <code>factor</code>.
</p>
</td></tr>
<tr><td><code id="bystats_+3A_fun">fun</code></td>
<td>

<p>a function to compute on the non-missing <code>y</code> for a given subset.
You must specify <code>fun=</code> in front of the function name or definition.
<code>fun</code> may return a single number or a vector or matrix of any length.
Matrix results are rolled out into a vector, with names preserved.
When <code>y</code> is a matrix, a common <code>fun</code> is <code>function(y) apply(y, 2, ff)</code>
where <code>ff</code> is the name of a function which operates on one column of
<code>y</code>.
</p>
</td></tr>
<tr><td><code id="bystats_+3A_nmiss">nmiss</code></td>
<td>

<p>A column containing a count of missing values is included if <code>nmiss=TRUE</code>
or if there is at least one missing value.
</p>
</td></tr>
<tr><td><code id="bystats_+3A_subset">subset</code></td>
<td>

<p>a vector of subscripts or logical values indicating the subset of
data to analyze
</p>
</td></tr>
<tr><td><code id="bystats_+3A_abbreviate.dimnames">abbreviate.dimnames</code></td>
<td>
<p>set to <code>TRUE</code> to abbreviate
<code>dimnames</code> in output</p>
</td></tr>
<tr><td><code id="bystats_+3A_prefix.width">prefix.width</code></td>
<td>
<p>see <code><a href="#topic+print.char.matrix">print.char.matrix</a></code></p>
</td></tr>
<tr><td><code id="bystats_+3A_title">title</code></td>
<td>

<p><code>title</code> to pass to <code>latex.default</code>.  Default is the first word of
the character string version of the first calling argument.
</p>
</td></tr>
<tr><td><code id="bystats_+3A_caption">caption</code></td>
<td>

<p>caption to pass to <code>latex.default</code>.  Default is the <code>heading</code>
attribute from the object produced by <code>bystats</code>.
</p>
</td></tr>
<tr><td><code id="bystats_+3A_rowlabel">rowlabel</code></td>
<td>

<p><code>rowlabel</code> to pass to <code>latex.default</code>.  Default is the <code>byvarnames</code>
attribute from the object produced by <code>bystats</code>.  For <code>bystats2</code> the
default is <code>""</code>.
</p>
</td></tr>
<tr><td><code id="bystats_+3A_x">x</code></td>
<td>
<p>an object created by <code>bystats</code> or <code>bystats2</code></p>
</td></tr>
<tr><td><code id="bystats_+3A_object">object</code></td>
<td>
<p>an object created by <code>bystats</code> or <code>bystats2</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>for <code>bystats</code>, a matrix with row names equal to the classification labels and column
names <code>N, Missing, funlab</code>, where <code>funlab</code> is determined from <code>fun</code>.
A row is added to the end with the summary statistics computed 
on all observations combined.  The class of this matrix is <code>bystats</code>.
For <code>bystats</code>, returns a 3-dimensional array with the last dimension
corresponding to statistics being computed.  The class of the array is
<code>bystats2</code>.
</p>


<h3>Side Effects</h3>

<p><code>latex</code> produces a <code>.tex</code> file.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+interaction">interaction</a></code>, <code><a href="base.html#topic+cut">cut</a></code>, <code><a href="#topic+cut2">cut2</a></code>, <code><a href="#topic+latex">latex</a></code>, <code><a href="#topic+print.char.matrix">print.char.matrix</a></code>,
<code><a href="#topic+translate">translate</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
bystats(sex==2, county, city)
bystats(death, race)
bystats(death, cut2(age,g=5), race)
bystats(cholesterol, cut2(age,g=4), sex, fun=median)
bystats(cholesterol, sex, fun=quantile)
bystats(cholesterol, sex, fun=function(x)c(Mean=mean(x),Median=median(x)))
latex(bystats(death,race,nmiss=FALSE,subset=sex=="female"), digits=2)
f &lt;- function(y) c(Hazard=sum(y[,2])/sum(y[,1]))
# f() gets the hazard estimate for right-censored data from exponential dist.
bystats(cbind(d.time, death), race, sex, fun=f)
bystats(cbind(pressure, cholesterol), age.decile, 
        fun=function(y) c(Median.pressure   =median(y[,1]),
                          Median.cholesterol=median(y[,2])))
y &lt;- cbind(pressure, cholesterol)
bystats(y, age.decile, 
        fun=function(y) apply(y, 2, median))   # same result as last one
bystats(y, age.decile, fun=function(y) apply(y, 2, quantile, c(.25,.75)))
# The last one computes separately the 0.25 and 0.75 quantiles of 2 vars.
latex(bystats2(death, race, sex, fun=table))

## End(Not run)
</code></pre>

<hr>
<h2 id='capitalize'> capitalize the first letter of a string</h2><span id='topic+capitalize'></span>

<h3>Description</h3>

<p>Capitalizes the first letter of each element of the string vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>capitalize(string)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="capitalize_+3A_string">string</code></td>
<td>
<p> String to be capitalized </p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a vector of charaters with the first letter capitalized
</p>


<h3>Author(s)</h3>

<p> Charles Dupont </p>


<h3>Examples</h3>

<pre><code class='language-R'>capitalize(c("Hello", "bob", "daN"))
</code></pre>

<hr>
<h2 id='ciapower'>
Power of Interaction Test for Exponential Survival
</h2><span id='topic+ciapower'></span>

<h3>Description</h3>

<p>Uses the method of Peterson and George to compute the power of an
interaction test in a 2 x 2 setup in which all 4 distributions are
exponential.  This will be the same as the power of the Cox model
test if assumptions hold.  The test is 2-tailed.  
The duration of accrual is specified
(constant accrual is assumed), as is the minimum follow-up time.
The maximum follow-up time is then <code>accrual + tmin</code>.  Treatment
allocation is assumed to be 1:1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ciapower(tref, n1, n2, m1c, m2c, r1, r2, accrual, tmin, 
         alpha=0.05, pr=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ciapower_+3A_tref">tref</code></td>
<td>

<p>time at which mortalities estimated
</p>
</td></tr>
<tr><td><code id="ciapower_+3A_n1">n1</code></td>
<td>

<p>total sample size, stratum 1
</p>
</td></tr>
<tr><td><code id="ciapower_+3A_n2">n2</code></td>
<td>

<p>total sample size, stratum 2
</p>
</td></tr>
<tr><td><code id="ciapower_+3A_m1c">m1c</code></td>
<td>

<p>tref-year mortality, stratum 1 control
</p>
</td></tr>
<tr><td><code id="ciapower_+3A_m2c">m2c</code></td>
<td>

<p>tref-year mortality, stratum 2 control
</p>
</td></tr>
<tr><td><code id="ciapower_+3A_r1">r1</code></td>
<td>

<p>% reduction in <code>m1c</code> by intervention, stratum 1
</p>
</td></tr>
<tr><td><code id="ciapower_+3A_r2">r2</code></td>
<td>

<p>% reduction in <code>m2c</code> by intervention, stratum 2
</p>
</td></tr>
<tr><td><code id="ciapower_+3A_accrual">accrual</code></td>
<td>

<p>duration of accrual period
</p>
</td></tr>
<tr><td><code id="ciapower_+3A_tmin">tmin</code></td>
<td>

<p>minimum follow-up time
</p>
</td></tr>
<tr><td><code id="ciapower_+3A_alpha">alpha</code></td>
<td>

<p>type I error probability
</p>
</td></tr>
<tr><td><code id="ciapower_+3A_pr">pr</code></td>
<td>

<p>set to <code>FALSE</code> to suppress printing of details
</p>
</td></tr></table>


<h3>Value</h3>

<p>power
</p>


<h3>Side Effects</h3>

<p>prints
</p>


<h3>AUTHOR</h3>

<p>Frank Harrell
</p>
<p>Department of Biostatistics
</p>
<p>Vanderbilt University
</p>
<p><a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Peterson B, George SL: Controlled Clinical Trials 14:511&ndash;522; 1993.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cpower">cpower</a></code>, <code><a href="#topic+spower">spower</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Find the power of a race x treatment test.  25% of patients will
# be non-white and the total sample size is 14000.  
# Accrual is for 1.5 years and minimum follow-up is 5y.
# Reduction in 5-year mortality is 15% for whites, 0% or -5% for
# non-whites.  5-year mortality for control subjects if assumed to
# be 0.18 for whites, 0.23 for non-whites.
n &lt;- 14000
for(nonwhite.reduction in c(0,-5)) {
  cat("\n\n\n% Reduction in 5-year mortality for non-whites:",
      nonwhite.reduction, "\n\n")
  pow &lt;- ciapower(5,  .75*n, .25*n,  .18, .23,  15, nonwhite.reduction,  
                  1.5, 5)
  cat("\n\nPower:",format(pow),"\n")
}
</code></pre>

<hr>
<h2 id='cnvrt.coords'>Convert between the 5 different coordinate sytems on a graphical device</h2><span id='topic+cnvrt.coords'></span>

<h3>Description</h3>

<p>Takes a set of coordinates in any of the 5 coordinate systems (usr,
plt, fig, dev, or tdev) and returns the same points in all 5
coordinate systems.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cnvrt.coords(x, y = NULL, input = c("usr", "plt", "fig", "dev","tdev"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cnvrt.coords_+3A_x">x</code></td>
<td>
<p>Vector, Matrix, or list of x coordinates (or x and y
coordinates), NA's allowed. </p>
</td></tr>
<tr><td><code id="cnvrt.coords_+3A_y">y</code></td>
<td>
<p>y coordinates (if <code>x</code> is a vector), NA's allowed. </p>
</td></tr>
<tr><td><code id="cnvrt.coords_+3A_input">input</code></td>
<td>
<p>Character scalar indicating the coordinate system of the
input points. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Every plot has 5 coordinate systems:
</p>
<p>usr (User): the coordinate system of the data, this is shown by the
tick marks and axis labels.
</p>
<p>plt (Plot): Plot area, coordinates range from 0 to 1 with 0
corresponding to the x and y axes and 1 corresponding to the top and
right of the plot area.  Margins of the plot correspond to plot
coordinates less than 0 or greater than 1.
</p>
<p>fig (Figure): Figure area, coordinates range from 0 to 1 with 0
corresponding to the bottom and left edges of the figure (including
margins, label areas) and 1 corresponds to the top and right edges.
fig and dev coordinates will be identical if there is only 1 figure
area on the device (layout, mfrow, or mfcol has not been used).
</p>
<p>dev (Device): Device area, coordinates range from 0 to 1 with 0
corresponding to the bottom and left of the device region within the
outer margins and 1 is the top and right of the region withing the
outer margins.  If the outer margins are all set to 0 then tdev and
dev should be identical.
</p>
<p>tdev (Total Device): Total Device area, coordinates range from 0 to 1 with 0
corresponding to the bottom and left edges of the device (piece of
paper, window on screen) and 1 corresponds to the top and right edges.
</p>


<h3>Value</h3>

<p>A list with 5 components, each component is a list with vectors named
x and y.  The 5 sublists are:
</p>
<table>
<tr><td><code>usr</code></td>
<td>
<p>The coordinates of the input points in usr (User) coordinates.</p>
</td></tr>
<tr><td><code>plt</code></td>
<td>
<p>The coordinates of the input points in plt (Plot)
coordinates.</p>
</td></tr>
<tr><td><code>fig</code></td>
<td>
<p>The coordinates of the input points in fig (Figure)
coordinates.</p>
</td></tr>
<tr><td><code>dev</code></td>
<td>
<p>The coordinates of the input points in dev (Device)
coordinates.</p>
</td></tr>
<tr><td><code>tdev</code></td>
<td>
<p>The coordinates of the input points in tdev (Total Device)
coordinates.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p> You must provide both x and y, but one of them may be <code>NA</code>.
</p>
<p>This function is becoming depricated with the new functions
<code>grconvertX</code> and <code>grconvertY</code> in R version 2.7.0 and beyond.
These new functions use the correct coordinate system names and have
more coordinate systems available, you should start using them instead.
</p>


<h3>Author(s)</h3>

<p>Greg Snow <a href="mailto:greg.snow@imail.org">greg.snow@imail.org</a></p>


<h3>See Also</h3>

 <p><code><a href="graphics.html#topic+par">par</a></code> specifically 'usr','plt', and 'fig'.  Also
'xpd' for plotting outside of the plotting region and 'mfrow' and
'mfcol' for multi figure plotting. <code><a href="#topic+subplot">subplot</a></code>,
<code>grconvertX</code> and <code>grconvertY</code> in R2.7.0 and later</p>


<h3>Examples</h3>

<pre><code class='language-R'>
old.par &lt;- par(no.readonly=TRUE)

par(mfrow=c(2,2),xpd=NA)

# generate some sample data
tmp.x &lt;- rnorm(25, 10, 2)
tmp.y &lt;- rnorm(25, 50, 10)
tmp.z &lt;- rnorm(25, 0, 1)

plot( tmp.x, tmp.y)

# draw a diagonal line across the plot area
tmp1 &lt;- cnvrt.coords( c(0,1), c(0,1), input='plt' )
lines(tmp1$usr, col='blue')

# draw a diagonal line accross figure region
tmp2 &lt;- cnvrt.coords( c(0,1), c(1,0), input='fig')
lines(tmp2$usr, col='red')

# save coordinate of point 1 and y value near top of plot for future plots
tmp.point1 &lt;- cnvrt.coords(tmp.x[1], tmp.y[1])
tmp.range1 &lt;- cnvrt.coords(NA, 0.98, input='plt')

# make a second plot and draw a line linking point 1 in each plot
plot(tmp.y, tmp.z)

tmp.point2 &lt;- cnvrt.coords( tmp.point1$dev, input='dev' )
arrows( tmp.y[1], tmp.z[1], tmp.point2$usr$x, tmp.point2$usr$y,
 col='green')

# draw another plot and add rectangle showing same range in 2 plots

plot(tmp.x, tmp.z)
tmp.range2 &lt;- cnvrt.coords(NA, 0.02, input='plt')
tmp.range3 &lt;- cnvrt.coords(NA, tmp.range1$dev$y, input='dev')
rect( 9, tmp.range2$usr$y, 11, tmp.range3$usr$y, border='yellow')

# put a label just to the right of the plot and
#  near the top of the figure region.
text( cnvrt.coords(1.05, NA, input='plt')$usr$x,
	cnvrt.coords(NA, 0.75, input='fig')$usr$y,
	"Label", adj=0)

par(mfrow=c(1,1))

## create a subplot within another plot (see also subplot)

plot(1:10, 1:10)

tmp &lt;- cnvrt.coords( c( 1, 4, 6, 9), c(6, 9, 1, 4) )

par(plt = c(tmp$dev$x[1:2], tmp$dev$y[1:2]), new=TRUE)
hist(rnorm(100))

par(fig = c(tmp$dev$x[3:4], tmp$dev$y[3:4]), new=TRUE)
hist(rnorm(100))

par(old.par)

</code></pre>

<hr>
<h2 id='colorFacet'>Miscellaneous ggplot2 and grid Helper Functions</h2><span id='topic+colorFacet'></span><span id='topic+arrGrob'></span><span id='topic+print.arrGrob'></span>

<h3>Description</h3>

<p>These functions are used on <code>ggplot2</code> objects or as layers when
building a <code>ggplot2</code> object, and to facilitate use of
<code>gridExtra</code>.  <code>colorFacet</code> colors the thin 
rectangles used to separate panels created by <code>facet_grid</code> (and
probably by <code>facet_wrap</code>). 
A better approach may be found at <a href="https://stackoverflow.com/questions/28652284/">https://stackoverflow.com/questions/28652284/</a>.
<code>arrGrob</code> is a front-end to <code>gridExtra::arrangeGrob</code> that
allows for proper printing.  See
<a href="https://stackoverflow.com/questions/29062766/store-output-from-gridextragrid-arrange-into-an-object/">https://stackoverflow.com/questions/29062766/store-output-from-gridextragrid-arrange-into-an-object/</a>.  The <code>arrGrob</code> <code>print</code> method calls <code>grid::grid.draw</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>colorFacet(g, col = adjustcolor("blue", alpha.f = 0.3))

arrGrob(...)

## S3 method for class 'arrGrob'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="colorFacet_+3A_g">g</code></td>
<td>
<p>a <code>ggplot2</code> object that used faceting</p>
</td></tr>
<tr><td><code id="colorFacet_+3A_col">col</code></td>
<td>
<p>color for facet separator rectanges</p>
</td></tr>
<tr><td><code id="colorFacet_+3A_...">...</code></td>
<td>
<p>passed to <code>arrangeGrob</code></p>
</td></tr>
<tr><td><code id="colorFacet_+3A_x">x</code></td>
<td>
<p>an object created by <code>arrGrob</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sandy Muspratt</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
require(ggplot2)
s &lt;- summaryP(age + sex ~ region + treatment)
colorFacet(ggplot(s))   # prints directly
# arrGrob is called by rms::ggplot.Predict and others

## End(Not run)
</code></pre>

<hr>
<h2 id='combine.levels'>combine.levels</h2><span id='topic+combine.levels'></span>

<h3>Description</h3>

<p>Combine Infrequent Levels of a Categorical Variable
</p>


<h3>Usage</h3>

<pre><code class='language-R'>combine.levels(
  x,
  minlev = 0.05,
  m,
  ord = is.ordered(x),
  plevels = FALSE,
  sep = ","
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="combine.levels_+3A_x">x</code></td>
<td>
<p>a factor, 'ordered' factor, or numeric or character variable that will be turned into a 'factor'</p>
</td></tr>
<tr><td><code id="combine.levels_+3A_minlev">minlev</code></td>
<td>
<p>the minimum proportion of observations in a cell before that cell is combined with one or more cells.  If more than one cell has fewer than minlev*n observations, all such cells are combined into a new cell labeled '&quot;OTHER&quot;'.  Otherwise, the lowest frequency cell is combined with the next lowest frequency cell, and the level name is the combination of the two old level levels. When 'ord=TRUE' combinations happen only for consecutive levels.</p>
</td></tr>
<tr><td><code id="combine.levels_+3A_m">m</code></td>
<td>
<p>alternative to 'minlev', is the minimum number of observations in a cell before it will be combined with others</p>
</td></tr>
<tr><td><code id="combine.levels_+3A_ord">ord</code></td>
<td>
<p>set to 'TRUE' to treat 'x' as if it were an ordered factor, which allows only consecutive levels to be combined</p>
</td></tr>
<tr><td><code id="combine.levels_+3A_plevels">plevels</code></td>
<td>
<p>by default 'combine.levels' pools low-frequency levels into a category named 'OTHER' when 'x' is not ordered and 'ord=FALSE'.  To instead name this category the concatenation of all the pooled level names, separated by a comma, set 'plevels=TRUE'.</p>
</td></tr>
<tr><td><code id="combine.levels_+3A_sep">sep</code></td>
<td>
<p>the separator for concatenating levels when 'plevels=TRUE'</p>
</td></tr>
</table>


<h3>Details</h3>

<p>After turning 'x' into a 'factor' if it is not one already, combines
levels of 'x' whose frequency falls below a specified relative frequency 'minlev' or absolute count 'm'.  When 'x' is not treated as ordered, all of the
small frequency levels are combined into '&quot;OTHER&quot;', unless 'plevels=TRUE'.
When 'ord=TRUE' or 'x' is an ordered factor, only consecutive levels
are combined.  New levels are constructed by concatenating the levels with
'sep' as a separator.  This is useful when comparing ordinal regression
with polytomous (multinomial) regression and there are too many
categories for polytomous regression.  'combine.levels' is also useful
when assumptions of ordinal models are being checked empirically by
computing exceedance probabilities for various cutoffs of the
dependent variable.
</p>


<h3>Value</h3>

<p>a factor variable, or if 'ord=TRUE' an ordered factor variable
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(rep('A', 1), rep('B', 3), rep('C', 4), rep('D',1), rep('E',1))
combine.levels(x, m=3)
combine.levels(x, m=3, plevels=TRUE)
combine.levels(x, ord=TRUE, m=3)
x &lt;- c(rep('A', 1), rep('B', 3), rep('C', 4), rep('D',1), rep('E',1),
       rep('F',1))
combine.levels(x, ord=TRUE, m=3)
</code></pre>

<hr>
<h2 id='combplotp'>Combination Plot</h2><span id='topic+combplotp'></span>

<h3>Description</h3>

<p>Generates a plotly attribute plot given a series of possibly overlapping binary variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>combplotp(
  formula,
  data = NULL,
  subset,
  na.action = na.retain,
  vnames = c("labels", "names"),
  includenone = FALSE,
  showno = FALSE,
  maxcomb = NULL,
  minfreq = NULL,
  N = NULL,
  pos = function(x) 1 * (tolower(x) %in% c("true", "yes", "y", "positive", "+",
    "present", "1")),
  obsname = "subjects",
  ptsize = 35,
  width = NULL,
  height = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="combplotp_+3A_formula">formula</code></td>
<td>
<p>a formula containing all the variables to be cross-tabulated, on the formula's right hand side.  There is no left hand side variable.  If <code>formula</code> is omitted, then all variables from <code>data</code> are analyzed.</p>
</td></tr>
<tr><td><code id="combplotp_+3A_data">data</code></td>
<td>
<p>input data frame.  If none is specified the data are assumed to come from the parent frame.</p>
</td></tr>
<tr><td><code id="combplotp_+3A_subset">subset</code></td>
<td>
<p>an optional subsetting expression applied to <code>data</code></p>
</td></tr>
<tr><td><code id="combplotp_+3A_na.action">na.action</code></td>
<td>
<p>see <code>lm</code> etc.</p>
</td></tr>
<tr><td><code id="combplotp_+3A_vnames">vnames</code></td>
<td>
<p>set to <code>"names"</code> to use variable names to label axes instead of variable labels.  When using the default <code>labels</code>, any variable not having a label will have its name used instead.</p>
</td></tr>
<tr><td><code id="combplotp_+3A_includenone">includenone</code></td>
<td>
<p>set to <code>TRUE</code> to include the combination where all conditions are absent</p>
</td></tr>
<tr><td><code id="combplotp_+3A_showno">showno</code></td>
<td>
<p>set to <code>TRUE</code> to show a light dot for conditions that are not part of the currently tabulated combination</p>
</td></tr>
<tr><td><code id="combplotp_+3A_maxcomb">maxcomb</code></td>
<td>
<p>maximum number of combinations to display</p>
</td></tr>
<tr><td><code id="combplotp_+3A_minfreq">minfreq</code></td>
<td>
<p>if specified, any combination having a frequency less than this will be omitted from the display</p>
</td></tr>
<tr><td><code id="combplotp_+3A_n">N</code></td>
<td>
<p>set to an integer to override the global denominator, instead of using the number of rows in the data</p>
</td></tr>
<tr><td><code id="combplotp_+3A_pos">pos</code></td>
<td>
<p>a function of vector returning a logical vector with <code>TRUE</code> values indicating positive</p>
</td></tr>
<tr><td><code id="combplotp_+3A_obsname">obsname</code></td>
<td>
<p>character string noun describing observations, default is <code>"subjects"</code></p>
</td></tr>
<tr><td><code id="combplotp_+3A_ptsize">ptsize</code></td>
<td>
<p>point size, defaults to 35</p>
</td></tr>
<tr><td><code id="combplotp_+3A_width">width</code></td>
<td>
<p>width of <code>plotly</code> plot</p>
</td></tr>
<tr><td><code id="combplotp_+3A_height">height</code></td>
<td>
<p>height of <code>plotly</code> plot</p>
</td></tr>
<tr><td><code id="combplotp_+3A_...">...</code></td>
<td>
<p>optional arguments to pass to <code>table</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Similar to the <code>UpSetR</code> package, draws a special dot chart sometimes called an attribute plot that depicts all possible combination of the binary variables.  By default a positive value, indicating that a certain condition pertains for a subject, is any of logical <code>TRUE</code>, numeric 1, <code>"yes"</code>, <code>"y"</code>, <code>"positive"</code>, <code>"+"</code> or <code>"present"</code> value, and all others are considered negative.  The user can override this determination by specifying her own <code>pos</code> function.  Case is ignored in the variable values.
</p>
<p>The plot uses solid dots arranged in a vertical line to indicate which combination of conditions is being considered.  Frequencies of all possible combinations are shown above the dot chart.  Marginal frequencies of positive values for the input variables are shown to the left of the dot chart.  More information for all three of these component symbols is provided in hover text.
</p>
<p>Variables are sorted in descending order of marginal frqeuencies and likewise for combinations of variables.
</p>


<h3>Value</h3>

<p><code>plotly</code> object
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (requireNamespace("plotly")) {
  g &lt;- function() sample(0:1, n, prob=c(1 - p, p), replace=TRUE)
  set.seed(2); n &lt;- 100; p &lt;- 0.5
  x1 &lt;- g(); label(x1) &lt;- 'A long label for x1 that describes it'
  x2 &lt;- g()
  x3 &lt;- g(); label(x3) &lt;- 'This is&lt;br&gt;a label for x3'
  x4 &lt;- g()
  combplotp(~ x1 + x2 + x3 + x4, showno=TRUE, includenone=TRUE)

  n &lt;- 1500; p &lt;- 0.05
  pain       &lt;- g()
  anxiety    &lt;- g()
  depression &lt;- g()
  soreness   &lt;- g()
  numbness   &lt;- g()
  tiredness  &lt;- g()
  sleepiness &lt;- g()
  combplotp(~ pain + anxiety + depression + soreness + numbness +
            tiredness + sleepiness, showno=TRUE)
}
</code></pre>

<hr>
<h2 id='completer'>completer</h2><span id='topic+completer'></span>

<h3>Description</h3>

<p>Create imputed dataset(s) using <code>transcan</code> and <code>aregImpute</code> objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>completer(a, nimpute, oneimpute = FALSE, mydata)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="completer_+3A_a">a</code></td>
<td>
<p>An object of class <code>transcan</code> or <code>aregImpute</code></p>
</td></tr>
<tr><td><code id="completer_+3A_nimpute">nimpute</code></td>
<td>
<p>A numeric vector between 1 and <code>a$n.impute</code>. For <code>transcan</code> object, this is set to 1. For <code>aregImpute</code> object, returns a list of <code>nimpute</code> datasets when <code>oneimpute</code> is set to <code>FALSE</code> (default).</p>
</td></tr>
<tr><td><code id="completer_+3A_oneimpute">oneimpute</code></td>
<td>
<p>A logical vector. When set to <code>TRUE</code>, returns a single completed dataset for the imputation number specified by <code>nimpute</code></p>
</td></tr>
<tr><td><code id="completer_+3A_mydata">mydata</code></td>
<td>
<p>A data frame in which its missing values will be imputed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Similar in function to <code>mice::complete</code>, this function uses <code>transcan</code> and <code>aregImpute</code> objects to impute missing data
and returns the completed dataset(s) as a dataframe or a list.
It assumes that <code>transcan</code> is used for single regression imputation.
</p>


<h3>Value</h3>

<div class="sourceCode"><pre>      A single or a list of completed dataset(s).
</pre></div>


<h3>Author(s)</h3>

<div class="sourceCode"><pre>      Yong-Hao Pua, Singapore General Hospital
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
mtcars$hp[1:5]    &lt;- NA
mtcars$wt[1:10]   &lt;- NA
myrform &lt;- ~ wt + hp + I(carb)
mytranscan  &lt;- transcan( myrform,  data = mtcars, imputed = TRUE,
  pl = FALSE, pr = FALSE, trantab = TRUE, long = TRUE)
myareg      &lt;- aregImpute(myrform, data = mtcars, x=TRUE, n.impute = 5)
completer(mytranscan)                    # single completed dataset
completer(myareg, 3, oneimpute = TRUE)
# single completed dataset based on the `n.impute`th set of multiple imputation
completer(myareg, 3)
# list of completed datasets based on first `nimpute` sets of multiple imputation
completer(myareg)
# list of completed datasets based on all available sets of multiple imputation
# To get a stacked data frame of all completed datasets use
# do.call(rbind, completer(myareg, data=mydata))
# or use rbindlist in data.table

## End(Not run)
</code></pre>

<hr>
<h2 id='consolidate'> Element Merging </h2><span id='topic+consolidate'></span><span id='topic+consolidate+3C-'></span><span id='topic+consolidate.default'></span>

<h3>Description</h3>

<p>Merges an object by the names of its elements.  Inserting elements in
<code>value</code> into <code>x</code> that do not exists in <code>x</code> and
replacing elements in <code>x</code> that exists in <code>value</code> with
<code>value</code> elements if <code>protect</code> is false.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>consolidate(x, value, protect, ...)
## Default S3 method:
consolidate(x, value, protect=FALSE, ...)

consolidate(x, protect, ...) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="consolidate_+3A_x">x</code></td>
<td>
<p> named list or vector </p>
</td></tr>
<tr><td><code id="consolidate_+3A_value">value</code></td>
<td>
<p> named list or vector </p>
</td></tr>
<tr><td><code id="consolidate_+3A_protect">protect</code></td>
<td>

<p>logical; should elements in <code>x</code> be kept instead
of elements in <code>value</code>?
</p>
</td></tr>
<tr><td><code id="consolidate_+3A_...">...</code></td>
<td>
<p> currently does nothing; included if ever want to make generic. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Charles Dupont </p>


<h3>See Also</h3>

 <p><code><a href="base.html#topic+names">names</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 1:5
names(x) &lt;- LETTERS[x]

y &lt;- 6:10
names(y) &lt;- LETTERS[y-2]

x                  # c(A=1,B=2,C=3,D=4,E=5)
y                  # c(D=6,E=7,F=8,G=9,H=10)

consolidate(x, y)      # c(A=1,B=2,C=3,D=6,E=7,F=8,G=9,H=10)
consolidate(x, y, protect=TRUE)      # c(A=1,B=2,C=3,D=4,E=5,F=8,G=9,H=10)

</code></pre>

<hr>
<h2 id='contents'>Metadata for a Data Frame</h2><span id='topic+contents'></span><span id='topic+contents.data.frame'></span><span id='topic+print.contents.data.frame'></span><span id='topic+html.contents.data.frame'></span><span id='topic+contents.list'></span><span id='topic+print.contents.list'></span>

<h3>Description</h3>

<p><code>contents</code> is a generic method for which <code>contents.data.frame</code>
is currently the only method.  <code>contents.data.frame</code> creates an
object containing the following attributes of the variables 
from a data frame: names, labels (if any), units (if any), number of
factor levels (if any), factor levels,
class, storage mode, and number of NAs.  <code>print.contents.data.frame</code>
will print the results, with options for sorting the variables.
<code>html.contents.data.frame</code> creates HTML code for displaying the
results.  This code has hyperlinks so that if the user clicks on the
number of levels the browser jumps to the correct part of a table of
factor levels for all the <code>factor</code> variables.  If long labels are
present (<code>"longlabel"</code> attributes on variables), these are printed
at the bottom and the <code>html</code> method links to them through the
regular labels.  Variables having the same <code>levels</code> in the same
order have the levels factored out for brevity.
</p>
<p><code>contents.list</code> prints a directory of datasets when
<code><a href="#topic+sasxport.get">sasxport.get</a></code> imported more than one SAS dataset.
</p>
<p>If <code>options(prType='html')</code> is in effect, calling <code>print</code> on
an object that is the contents of a data frame will result in
rendering the HTML version.  If run from the console a browser window
will open.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>contents(object, ...)
## S3 method for class 'data.frame'
contents(object, sortlevels=FALSE, id=NULL,
  range=NULL, values=NULL, ...)
## S3 method for class 'contents.data.frame'
print(x,
    sort=c('none','names','labels','NAs'), prlevels=TRUE, maxlevels=Inf,
    number=FALSE, ...) 
## S3 method for class 'contents.data.frame'
html(object,
           sort=c('none','names','labels','NAs'), prlevels=TRUE, maxlevels=Inf,
           levelType=c('list','table'),
           number=FALSE, nshow=TRUE, ...)
## S3 method for class 'list'
contents(object, dslabels, ...)
## S3 method for class 'contents.list'
print(x,
    sort=c('none','names','labels','NAs','vars'), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="contents_+3A_object">object</code></td>
<td>

<p>a data frame.  For <code>html</code> is an object created by
<code>contents</code>.  For <code>contents.list</code> is a list of data frames.
</p>
</td></tr>
<tr><td><code id="contents_+3A_sortlevels">sortlevels</code></td>
<td>
<p>set to <code>TRUE</code> to sort levels of all factor
variables into alphabetic order.  This is especially useful when two
variables use the same levels but in different orders.  They will
still be recognized by the <code>html</code> method as having identical
levels if sorted.</p>
</td></tr>
<tr><td><code id="contents_+3A_id">id</code></td>
<td>
<p>an optional subject ID variable name that if present in
<code>object</code> will cause the number of unique IDs to be printed in
the contents header</p>
</td></tr>
<tr><td><code id="contents_+3A_range">range</code></td>
<td>
<p>an optional variable name that if present in <code>object</code>
will cause its range to be printed in the contents header</p>
</td></tr>
<tr><td><code id="contents_+3A_values">values</code></td>
<td>
<p>an optional variable name that if present in
<code>object</code> will cause its unique values to be printed in the
contents header</p>
</td></tr>
<tr><td><code id="contents_+3A_x">x</code></td>
<td>

<p>an object created by <code>contents</code>
</p>
</td></tr>
<tr><td><code id="contents_+3A_sort">sort</code></td>
<td>

<p>Default is to print the variables in their original order in the
data frame.  Specify one of 
<code>"names"</code>, <code>"labels"</code>, or <code>"NAs"</code> to sort the variables by,
respectively, alphabetically by names, alphabetically by labels, or by
increaseing order of number of missing values.  For
<code>contents.list</code>, <code>sort</code> may also be the value
<code>"vars"</code> to cause sorting by the number of variables in the dataset.
</p>
</td></tr>
<tr><td><code id="contents_+3A_prlevels">prlevels</code></td>
<td>

<p>set to <code>FALSE</code> to not print all levels of <code>factor</code> variables
</p>
</td></tr>
<tr><td><code id="contents_+3A_maxlevels">maxlevels</code></td>
<td>
<p>maximum number of levels to print for a <code>factor</code> variable</p>
</td></tr>
<tr><td><code id="contents_+3A_number">number</code></td>
<td>

<p>set to <code>TRUE</code> to have the <code>print</code> and <code>latex</code> methods
number the variables by their order in the data frame
</p>
</td></tr>
<tr><td><code id="contents_+3A_nshow">nshow</code></td>
<td>
<p>set to <code>FALSE</code> to suppress outputting number of
observations and number of <code>NA</code>s; useful when these counts
would unblind information to blinded reviewers</p>
</td></tr>
<tr><td><code id="contents_+3A_leveltype">levelType</code></td>
<td>

<p>By default, bullet lists of category levels are
constructed in html.  Set <code>levelType='table'</code> to put levels in
html table format.
</p>
</td></tr>
<tr><td><code id="contents_+3A_...">...</code></td>
<td>

<p>arguments passed from <code>html</code> to <code>format.df</code>,
unused otherwise
</p>
</td></tr>
<tr><td><code id="contents_+3A_dslabels">dslabels</code></td>
<td>

<p>named vector of SAS dataset labels, created for
example by <code><a href="#topic+sasdsLabels">sasdsLabels</a></code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class <code>"contents.data.frame"</code> or
<code>"contents.list"</code>.  For the <code>html</code> method is an <code>html</code>
character vector object.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+describe">describe</a></code>, <code><a href="#topic+html">html</a></code>, <code><a href="#topic+upData">upData</a></code>,
<code><a href="#topic+extractlabs">extractlabs</a></code>, <code><a href="#topic+hlab">hlab</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
dfr &lt;- data.frame(x=rnorm(400),y=sample(c('male','female'),400,TRUE),
                  stringsAsFactors=TRUE)
contents(dfr)
dfr &lt;- upData(dfr, labels=c(x='Label for x', y='Label for y'))
attr(dfr$x, 'longlabel') &lt;-
 'A very long label for x that can continue onto multiple long lines of text'

k &lt;- contents(dfr)
print(k, sort='names', prlevels=FALSE)
## Not run: 
html(k)
html(contents(dfr))            # same result
latex(k$contents)              # latex.default just the main information

## End(Not run)
</code></pre>

<hr>
<h2 id='cpower'>
Power of Cox/log-rank Two-Sample Test
</h2><span id='topic+cpower'></span>

<h3>Description</h3>

<p>Assumes exponential distributions for both treatment groups.
Uses the George-Desu method along with
formulas of Schoenfeld that allow estimation of the expected number of
events in the two groups.  
To allow for drop-ins (noncompliance to control therapy, crossover to
intervention) and noncompliance of the intervention, the method of
Lachin and Foulkes is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cpower(tref, n, mc, r, accrual, tmin, noncomp.c=0, noncomp.i=0, 
       alpha=0.05, nc, ni, pr=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cpower_+3A_tref">tref</code></td>
<td>

<p>time at which mortalities estimated
</p>
</td></tr>
<tr><td><code id="cpower_+3A_n">n</code></td>
<td>

<p>total sample size (both groups combined).  If allocation is unequal
so that there are not <code>n/2</code> observations in each group, you may specify
the sample sizes in <code>nc</code> and <code>ni</code>.
</p>
</td></tr>
<tr><td><code id="cpower_+3A_mc">mc</code></td>
<td>

<p>tref-year mortality, control
</p>
</td></tr>
<tr><td><code id="cpower_+3A_r">r</code></td>
<td>

<p>% reduction in <code>mc</code> by intervention
</p>
</td></tr>
<tr><td><code id="cpower_+3A_accrual">accrual</code></td>
<td>

<p>duration of accrual period
</p>
</td></tr>
<tr><td><code id="cpower_+3A_tmin">tmin</code></td>
<td>

<p>minimum follow-up time
</p>
</td></tr>
<tr><td><code id="cpower_+3A_noncomp.c">noncomp.c</code></td>
<td>

<p>% non-compliant in control group (drop-ins)
</p>
</td></tr>
<tr><td><code id="cpower_+3A_noncomp.i">noncomp.i</code></td>
<td>

<p>% non-compliant in intervention group (non-adherers)
</p>
</td></tr>
<tr><td><code id="cpower_+3A_alpha">alpha</code></td>
<td>

<p>type I error probability.  A 2-tailed test is assumed.
</p>
</td></tr>
<tr><td><code id="cpower_+3A_nc">nc</code></td>
<td>

<p>number of subjects in control group
</p>
</td></tr>
<tr><td><code id="cpower_+3A_ni">ni</code></td>
<td>

<p>number of subjects in intervention group.  <code>nc</code> and <code>ni</code> are specified
exclusive of <code>n</code>.
</p>
</td></tr>
<tr><td><code id="cpower_+3A_pr">pr</code></td>
<td>

<p>set to <code>FALSE</code> to suppress printing of details
</p>
</td></tr></table>


<h3>Details</h3>

<p>For handling noncompliance, uses a modification of formula (5.4) of
Lachin and Foulkes.  Their method is based on a test for the difference
in two hazard rates, whereas <code>cpower</code> is based on testing the difference
in two log hazards.  It is assumed here that the same correction factor
can be approximately applied to the log hazard ratio as Lachin and Foulkes applied to
the hazard difference.
</p>
<p>Note that Schoenfeld approximates the variance
of the log hazard ratio by <code>4/m</code>, where <code>m</code> is the total number of events,
whereas the George-Desu method uses the slightly better <code>1/m1 + 1/m2</code>.
Power from this function will thus differ slightly from that obtained with
the SAS <code>samsizc</code> program.
</p>


<h3>Value</h3>

<p>power
</p>


<h3>Side Effects</h3>

<p>prints
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Peterson B, George SL: Controlled Clinical Trials 14:511&ndash;522; 1993.
</p>
<p>Lachin JM, Foulkes MA: Biometrics 42:507&ndash;519; 1986.
</p>
<p>Schoenfeld D: Biometrics 39:499&ndash;503; 1983.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spower">spower</a></code>, <code><a href="#topic+ciapower">ciapower</a></code>, <code><a href="#topic+bpower">bpower</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#In this example, 4 plots are drawn on one page, one plot for each
#combination of noncompliance percentage.  Within a plot, the
#5-year mortality % in the control group is on the x-axis, and
#separate curves are drawn for several % reductions in mortality
#with the intervention.  The accrual period is 1.5y, with all
#patients followed at least 5y and some 6.5y.


par(mfrow=c(2,2),oma=c(3,0,3,0))


morts &lt;- seq(10,25,length=50)
red &lt;- c(10,15,20,25)


for(noncomp in c(0,10,15,-1)) {
  if(noncomp&gt;=0) nc.i &lt;- nc.c &lt;- noncomp else {nc.i &lt;- 25; nc.c &lt;- 15}
  z &lt;- paste("Drop-in ",nc.c,"%, Non-adherence ",nc.i,"%",sep="")
  plot(0,0,xlim=range(morts),ylim=c(0,1),
           xlab="5-year Mortality in Control Patients (%)",
           ylab="Power",type="n")
  title(z)
  cat(z,"\n")
  lty &lt;- 0
  for(r in red) {
        lty &lt;- lty+1
        power &lt;- morts
        i &lt;- 0
        for(m in morts) {
          i &lt;- i+1
          power[i] &lt;- cpower(5, 14000, m/100, r, 1.5, 5, nc.c, nc.i, pr=FALSE)
        }
        lines(morts, power, lty=lty)
  }
  if(noncomp==0)legend(18,.55,rev(paste(red,"% reduction",sep="")),
           lty=4:1,bty="n")
}
mtitle("Power vs Non-Adherence for Main Comparison",
           ll="alpha=.05, 2-tailed, Total N=14000",cex.l=.8)
#
# Point sample size requirement vs. mortality reduction
# Root finder (uniroot()) assumes needed sample size is between
# 1000 and 40000
#
nc.i &lt;- 25; nc.c &lt;- 15; mort &lt;- .18
red &lt;- seq(10,25,by=.25)
samsiz &lt;- red


i &lt;- 0
for(r in red) {
  i &lt;- i+1
  samsiz[i] &lt;- uniroot(function(x) cpower(5, x, mort, r, 1.5, 5,
                                          nc.c, nc.i, pr=FALSE) - .8,
                       c(1000,40000))$root
}


samsiz &lt;- samsiz/1000
par(mfrow=c(1,1))
plot(red, samsiz, xlab='% Reduction in 5-Year Mortality',
	 ylab='Total Sample Size (Thousands)', type='n')
lines(red, samsiz, lwd=2)
title('Sample Size for Power=0.80\nDrop-in 15%, Non-adherence 25%')
title(sub='alpha=0.05, 2-tailed', adj=0)
</code></pre>

<hr>
<h2 id='Cs'>
Character strings from unquoted names
</h2><span id='topic+Cs'></span><span id='topic+.q'></span>

<h3>Description</h3>

<p><code>Cs</code> makes a vector of character strings from a list of valid R
names.  <code>.q</code> is similar but also makes uses of names of arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Cs(...)
.q(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cs_+3A_...">...</code></td>
<td>

<p>any number of names separated by commas.  For <code>.q</code> any names of
arguments will be used.
</p>
</td></tr></table>


<h3>Value</h3>

<p>character string vector.  For <code>.q</code> there will be a <code>names</code>
attribute to the vector if any names appeared in ....
</p>


<h3>See Also</h3>

<p>sys.frame, deparse
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Cs(a,cat,dog)
# subset.data.frame &lt;- dataframe[,Cs(age,sex,race,bloodpressure,height)]
.q(a, b, c, 'this and that')
.q(dog=a, giraffe=b, cat=c)
</code></pre>

<hr>
<h2 id='csv.get'>Read Comma-Separated Text Data Files</h2><span id='topic+csv.get'></span>

<h3>Description</h3>

<p>Read comma-separated text data files, allowing optional translation
to lower case for variable names after making them valid S names.
There is a facility for reading long variable labels as one of the
rows.  If labels are not specified and a final variable name is not
the same as that in the header, the original variable name is saved as
a variable label.  Uses <code>read.csv</code> if the <code>data.table</code>
package is not in effect, otherwise calls <code>fread</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>csv.get(file, lowernames=FALSE, datevars=NULL, datetimevars=NULL,
        dateformat='%F',
        fixdates=c('none','year'), comment.char="", autodate=TRUE,
        allow=NULL, charfactor=FALSE,
        sep=',', skip=0, vnames=NULL, labels=NULL, text=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="csv.get_+3A_file">file</code></td>
<td>
<p>the file name for import.</p>
</td></tr>
<tr><td><code id="csv.get_+3A_lowernames">lowernames</code></td>
<td>
<p>set this to <code>TRUE</code> to change variable names to
lower case.</p>
</td></tr>
<tr><td><code id="csv.get_+3A_datevars">datevars</code></td>
<td>
<p>character vector of names (after <code>lowernames</code> is
applied) of variables to consider as a factor or character vector
containing dates in a format matching <code>dateformat</code>.  The
default is <code>"%F"</code> which uses the yyyy-mm-dd format.</p>
</td></tr>
<tr><td><code id="csv.get_+3A_datetimevars">datetimevars</code></td>
<td>
<p>character vector of names (after <code>lowernames</code>
is applied) of variables to consider to be date-time variables, with
date formats as described under <code>datevars</code> followed by a space
followed by time in hh:mm:ss format.  <code>chron</code> is used to store
such variables.  If all times in the variable
are 00:00:00 the variable will be converted to an ordinary date variable.</p>
</td></tr>
<tr><td><code id="csv.get_+3A_dateformat">dateformat</code></td>
<td>
<p>for <code>cleanup.import</code> is the input format (see
<code><a href="base.html#topic+strptime">strptime</a></code>)</p>
</td></tr>
<tr><td><code id="csv.get_+3A_fixdates">fixdates</code></td>
<td>
<p>for any of the variables listed in <code>datevars</code>
that have a <code>dateformat</code> that <code>cleanup.import</code> understands,
specifying <code>fixdates</code> allows corrections of certain formatting
inconsistencies before the fields are attempted to be converted to
dates (the default is to assume that the <code>dateformat</code> is followed
for all observation for <code>datevars</code>).  Currently
<code>fixdates='year'</code> is implemented, which will cause 2-digit or
4-digit years to be shifted to the alternate number of digits when
<code>dateform</code> is the default <code>"%F"</code> or is <code>"%y-%m-%d"</code>,
<code>"%m/%d/%y"</code>, or <code>"%m/%d/%Y"</code>.  Two-digits years are
padded with <code>20</code> on the left.  Set <code>dateformat</code> to the
desired format, not the exceptional format.</p>
</td></tr>
<tr><td><code id="csv.get_+3A_comment.char">comment.char</code></td>
<td>
<p>a character vector of length one containing a
single character or an empty string.  Use '&quot;&quot;' to turn off the
interpretation of comments altogether.</p>
</td></tr>
<tr><td><code id="csv.get_+3A_autodate">autodate</code></td>
<td>
<p>Set to true to allow function to guess at which
variables are dates</p>
</td></tr>
<tr><td><code id="csv.get_+3A_allow">allow</code></td>
<td>
<p>a vector of characters allowed by <span class="rlang"><b>R</b></span> that should not be
converted to periods in variable names.  By default, underscores in
variable names are converted to periods as with <span class="rlang"><b>R</b></span> before version
1.9.</p>
</td></tr>
<tr><td><code id="csv.get_+3A_charfactor">charfactor</code></td>
<td>
<p>set to <code>TRUE</code> to change character variables to
factors if they have fewer than n/2 unique values.  Blanks and null
strings are converted to <code>NA</code>s.</p>
</td></tr>
<tr><td><code id="csv.get_+3A_sep">sep</code></td>
<td>
<p>field separator, defaults to comma</p>
</td></tr>
<tr><td><code id="csv.get_+3A_skip">skip</code></td>
<td>
<p>number of records to skip before data start.  Required if
<code>vnames</code> or <code>labels</code> is given.</p>
</td></tr>
<tr><td><code id="csv.get_+3A_vnames">vnames</code></td>
<td>
<p>number of row containing variable names, default is one</p>
</td></tr>
<tr><td><code id="csv.get_+3A_labels">labels</code></td>
<td>
<p>number of row containing variable labels, default is no
labels</p>
</td></tr>
<tr><td><code id="csv.get_+3A_text">text</code></td>
<td>
<p>a character string containing the <code>.csv</code> file to use
instead of <code>file=</code>.  Passed to <code>read.csv</code> as the
<code>text=</code> argument.</p>
</td></tr>
<tr><td><code id="csv.get_+3A_...">...</code></td>
<td>
<p>arguments to pass to <code>read.csv</code> other than
<code>skip</code> and <code>sep</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>csv.get</code> reads comma-separated text data files, allowing optional
translation to lower case for variable names after making them valid S
names.  Original possibly non-legal names are taken to be variable
labels if <code>labels</code> is not specified.  Character or factor
variables containing dates can be converted to date variables.
<code>cleanup.import</code> is invoked to finish the job.
</p>


<h3>Value</h3>

<p>a new data frame.</p>


<h3>Author(s)</h3>

<p>Frank Harrell, Vanderbilt University</p>


<h3>See Also</h3>

<p><code><a href="#topic+sas.get">sas.get</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code>,
<code><a href="#topic+cleanup.import">cleanup.import</a></code>, <code><a href="utils.html#topic+read.csv">read.csv</a></code>,
<code><a href="base.html#topic+strptime">strptime</a></code>, <code><a href="base.html#topic+POSIXct">POSIXct</a></code>, <code><a href="base.html#topic+Date">Date</a></code>,
<code><a href="data.table.html#topic+fread">fread</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dat &lt;- csv.get('myfile.csv')

# Read a csv file with junk in the first row, variable names in the
# second, long variable labels in the third, and junk in the 4th row
dat &lt;- csv.get('myfile.csv', vnames=2, labels=3, skip=4)

## End(Not run)
</code></pre>

<hr>
<h2 id='curveRep'>Representative Curves</h2><span id='topic+curveRep'></span><span id='topic+print.curveRep'></span><span id='topic+plot.curveRep'></span><span id='topic+curveSmooth'></span>

<h3>Description</h3>

<p><code>curveRep</code> finds representative curves from a
relatively large collection of curves.  The curves usually represent
time-response profiles as in serial (longitudinal or repeated) data
with possibly unequal time points and greatly varying sample sizes per
subject.  After excluding records containing missing <code>x</code> or
<code>y</code>, records are first stratified into <code>kn</code> groups having similar
sample sizes per curve (subject).  Within these strata, curves are
next stratified according to the distribution of <code>x</code> points per
curve (typically measurement times per subject).  The
<code><a href="cluster.html#topic+clara">clara</a></code> clustering/partitioning function is used
to do this, clustering on one, two, or three <code>x</code> characteristics
depending on the minimum sample size in the current interval of sample
size.  If the interval has a minimum number of unique <code>values</code> of
one, clustering is done on the single <code>x</code> values.  If the minimum
number of unique <code>x</code> values is two, clustering is done to create
groups that are similar on both <code>min(x)</code> and <code>max(x)</code>.  For
groups containing no fewer than three unique <code>x</code> values,
clustering is done on the trio of values <code>min(x)</code>, <code>max(x)</code>,
and the longest gap between any successive <code>x</code>.  Then within
sample size and <code>x</code> distribution strata, clustering of
time-response profiles is based on <code>p</code> values of <code>y</code> all
evaluated at the same <code>p</code> equally-spaced <code>x</code>'s within the
stratum.  An option allows per-curve data to be smoothed with
<code><a href="stats.html#topic+lowess">lowess</a></code> before proceeding.  Outer <code>x</code> values are
taken as extremes of <code>x</code> across all curves within the stratum.
Linear interpolation within curves is used to estimate <code>y</code> at the
grid of <code>x</code>'s.  For curves within the stratum that do not extend
to the most extreme <code>x</code> values in that stratum, extrapolation
uses flat lines from the observed extremes in the curve unless
<code>extrap=TRUE</code>. The <code>p</code> <code>y</code> values are clustered using
<code><a href="cluster.html#topic+clara">clara</a></code>.
</p>
<p><code>print</code> and <code>plot</code> methods show results.  By specifying an
auxiliary <code>idcol</code> variable to <code>plot</code>, other variables such
as treatment may be depicted to allow the analyst to determine for
example whether subjects on different treatments are assigned to
different time-response profiles.  To write the frequencies of a
variable such as treatment in the upper left corner of each panel
(instead of the grand total number of clusters in that panel), specify
<code>freq</code>.
</p>
<p><code>curveSmooth</code> takes a set of curves and smooths them using
<code><a href="stats.html#topic+lowess">lowess</a></code>.  If the number of unique <code>x</code> points in a curve is
less than <code>p</code>, the smooth is evaluated at the unique <code>x</code>
values.  Otherwise it is evaluated at an equally spaced set of
<code>x</code> points over the observed range.  If fewer than 3 unique
<code>x</code> values are in a curve, those points are used and smoothing is not done.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>curveRep(x, y, id, kn = 5, kxdist = 5, k = 5, p = 5,
         force1 = TRUE, metric = c("euclidean", "manhattan"),
         smooth=FALSE, extrap=FALSE, pr=FALSE)

## S3 method for class 'curveRep'
print(x, ...)

## S3 method for class 'curveRep'
plot(x, which=1:length(res),
                        method=c('all','lattice','data'),
                        m=NULL, probs=c(.5, .25, .75), nx=NULL, fill=TRUE,
                        idcol=NULL, freq=NULL, plotfreq=FALSE,
                        xlim=range(x), ylim=range(y),
                        xlab='x', ylab='y', colorfreq=FALSE, ...)
curveSmooth(x, y, id, p=NULL, pr=TRUE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="curveRep_+3A_x">x</code></td>
<td>
<p>a numeric vector, typically measurement times.
For <code>plot.curveRep</code> is an object created by <code>curveRep</code>.</p>
</td></tr>
<tr><td><code id="curveRep_+3A_y">y</code></td>
<td>
<p>a numeric vector of response values</p>
</td></tr>
<tr><td><code id="curveRep_+3A_id">id</code></td>
<td>
<p>a vector of curve (subject) identifiers, the same length as
<code>x</code> and <code>y</code></p>
</td></tr>
<tr><td><code id="curveRep_+3A_kn">kn</code></td>
<td>
<p>number of curve sample size groups to construct.
<code>curveRep</code> tries to divide the data into equal numbers of
curves across sample size intervals.</p>
</td></tr>
<tr><td><code id="curveRep_+3A_kxdist">kxdist</code></td>
<td>
<p>maximum number of x-distribution clusters to derive
using <code>clara</code></p>
</td></tr>
<tr><td><code id="curveRep_+3A_k">k</code></td>
<td>
<p>maximum number of x-y profile clusters to derive using <code>clara</code></p>
</td></tr>
<tr><td><code id="curveRep_+3A_p">p</code></td>
<td>
<p>number of <code>x</code> points at which to interpolate <code>y</code>
for profile clustering.  For <code>curveSmooth</code> is the number of
equally spaced points at which to evaluate the lowess smooth, and if
<code>p</code> is omitted the smooth is evaluated at the original <code>x</code>
values (which will allow <code>curveRep</code> to still know the <code>x</code>
distribution</p>
</td></tr>
<tr><td><code id="curveRep_+3A_force1">force1</code></td>
<td>
<p>By default if any curves have only one point, all curves
consisting of one point will be placed in a separate stratum.  To
prevent this separation, set <code>force1 = FALSE</code>.</p>
</td></tr>
<tr><td><code id="curveRep_+3A_metric">metric</code></td>
<td>
<p>see <code><a href="cluster.html#topic+clara">clara</a></code></p>
</td></tr>
<tr><td><code id="curveRep_+3A_smooth">smooth</code></td>
<td>
<p>By default, linear interpolation is used on raw data to
obtain <code>y</code> values to cluster to determine x-y profiles.
Specify <code>smooth = TRUE</code> to replace observed points with
<code><a href="stats.html#topic+lowess">lowess</a></code> before computing <code>y</code> points on the grid.
Also, when <code>smooth</code> is used, it may be desirable to use
<code>extrap=TRUE</code>.</p>
</td></tr>
<tr><td><code id="curveRep_+3A_extrap">extrap</code></td>
<td>
<p>set to <code>TRUE</code> to use linear extrapolation to
evaluate <code>y</code> points for x-y clustering.  Not recommended unless
smoothing has been or is being done.</p>
</td></tr>
<tr><td><code id="curveRep_+3A_pr">pr</code></td>
<td>
<p>set to <code>TRUE</code> to print progress notes</p>
</td></tr>
<tr><td><code id="curveRep_+3A_which">which</code></td>
<td>
<p>an integer vector specifying which sample size intervals
to plot.  Must be specified if <code>method='lattice'</code> and must be a
single number in that case.</p>
</td></tr>
<tr><td><code id="curveRep_+3A_method">method</code></td>
<td>
<p>The default makes individual plots of possibly all
x-distribution by sample size by cluster combinations.  Fewer may be
plotted by specifying <code>which</code>.  Specify <code>method='lattice'</code>
to show a lattice <code>xyplot</code> of a single sample size interval,
with x distributions going across and clusters going down.  To not
plot but instead return a data frame for a single sample size
interval, specify <code>method='data'</code></p>
</td></tr>
<tr><td><code id="curveRep_+3A_m">m</code></td>
<td>
<p>the number of curves in a cluster to randomly sample if there
are more than <code>m</code> in a cluster.  Default is to draw all curves
in a cluster.  For <code>method = "lattice"</code> you can specify
<code>m = "quantiles"</code> to use the <code>xYplot</code> function to show
quantiles of <code>y</code> as a function of <code>x</code>, with the quantiles
specified by the <code>probs</code> argument.  This cannot be used to draw
a group containing <code>n = 1</code>.</p>
</td></tr>
<tr><td><code id="curveRep_+3A_nx">nx</code></td>
<td>
<p>applies if <code>m = "quantiles"</code>.  See <code><a href="#topic+xYplot">xYplot</a></code>.</p>
</td></tr>
<tr><td><code id="curveRep_+3A_probs">probs</code></td>
<td>
<p>3-vector of probabilities with the central quantile
first.  Default uses quartiles.</p>
</td></tr>
<tr><td><code id="curveRep_+3A_fill">fill</code></td>
<td>
<p>for <code>method = "all"</code>, by default if a sample size
x-distribution stratum did not have enough curves to stratify into
<code>k</code> x-y profiles, empty graphs are drawn so that a matrix of
graphs will have the next row starting with a different sample size
range or x-distribution.  See the example below.</p>
</td></tr>
<tr><td><code id="curveRep_+3A_idcol">idcol</code></td>
<td>
<p>a named vector to be used as a table lookup for color
assignments (does not apply when <code>m = "quantile"</code>).  The names of
this vector are curve <code>id</code>s and the values are color names or
numbers.</p>
</td></tr>
<tr><td><code id="curveRep_+3A_freq">freq</code></td>
<td>
<p>a named vector to be used as a table lookup for a grouping
variable such as treatment.  The names are curve <code>id</code>s and
values are any values useful for grouping in a frequency tabulation.</p>
</td></tr>
<tr><td><code id="curveRep_+3A_plotfreq">plotfreq</code></td>
<td>
<p>set to <code>TRUE</code> to plot the frequencies from the
<code>freq</code> variable as horizontal bars instead of printing them.
Applies only to <code>method = "lattice"</code>.  By default the largest bar
is 0.1 times the length of a panel's x-axis.  Specify
<code>plotfreq = 0.5</code> for example to make the longest bar half this long.</p>
</td></tr>
<tr><td><code id="curveRep_+3A_colorfreq">colorfreq</code></td>
<td>
<p>set to <code>TRUE</code> to color the frequencies printed by 
<code>plotfreq</code> using the colors provided by <code>idcol</code>.</p>
</td></tr>
<tr><td><code id="curveRep_+3A_xlim">xlim</code>, <code id="curveRep_+3A_ylim">ylim</code>, <code id="curveRep_+3A_xlab">xlab</code>, <code id="curveRep_+3A_ylab">ylab</code></td>
<td>
<p>plotting parameters.  Default ranges are
the ranges in the entire set of raw data given to <code>curveRep</code>.</p>
</td></tr>
<tr><td><code id="curveRep_+3A_...">...</code></td>
<td>
<p>arguments passed to other functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the graph titles for the default graphic output, <code>n</code> refers to the
minimum sample size, <code>x</code> refers to the sequential x-distribution
cluster, and <code>c</code> refers to the sequential x-y profile cluster.  Graphs
from <code>method = "lattice"</code> are produced by
<code><a href="lattice.html#topic+xyplot">xyplot</a></code> and in the panel titles
<code>distribution</code> refers to the x-distribution stratum and
<code>cluster</code> refers to the x-y profile cluster.
</p>


<h3>Value</h3>

<p>a list of class <code>"curveRep"</code> with the following elements
</p>
<table>
<tr><td><code>res</code></td>
<td>
<p>a hierarchical list first split by sample size intervals,
then by x distribution clusters, then containing a vector of cluster
numbers with <code>id</code> values as a names attribute</p>
</td></tr>
<tr><td><code>ns</code></td>
<td>
<p>a table of frequencies of sample sizes per curve after
removing <code>NA</code>s</p>
</td></tr>
<tr><td><code>nomit</code></td>
<td>
<p>total number of records excluded due to <code>NA</code>s</p>
</td></tr>
<tr><td><code>missfreq</code></td>
<td>
<p>a table of frequencies of number of <code>NA</code>s
excluded per curve</p>
</td></tr>
<tr><td><code>ncuts</code></td>
<td>
<p>cut points for sample size intervals</p>
</td></tr>
<tr><td><code>kn</code></td>
<td>
<p>number of sample size intervals</p>
</td></tr>
<tr><td><code>kxdist</code></td>
<td>
<p>number of clusters on x distribution</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>number of clusters of curves within sample size and
distribution groups</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>number of points at which to evaluate each curve for clustering</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
</td></tr>
<tr><td><code>y</code></td>
<td>
</td></tr>
<tr><td><code>id</code></td>
<td>
<p>input data after removing <code>NA</code>s</p>
</td></tr>
</table>
<p><code>curveSmooth</code> returns a list with elements <code>x,y,id</code>.
</p>


<h3>Note</h3>

<p>The references describe other methods for deriving
representative curves, but those methods were not used here.  The last
reference which used a cluster analysis on principal components
motivated <code>curveRep</code> however.  The <code>kml</code> package does k-means clustering of longitudinal data with imputation.</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Segal M. (1994): Representative curves for longitudinal data via
regression trees.  J Comp Graph Stat 3:214-233.
</p>
<p>Jones MC, Rice JA (1992): Displaying the important features of large
collections of similar curves.  Am Statistician 46:140-145.
</p>
<p>Zheng X, Simpson JA, et al (2005): Data from a study of effectiveness
suggested potential prognostic factors related to the patterns of
shoulder pain.  J Clin Epi 58:823-830.
</p>


<h3>See Also</h3>

<p><code><a href="cluster.html#topic+clara">clara</a></code>,<code><a href="#topic+dataRep">dataRep</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Simulate 200 curves with per-curve sample sizes ranging from 1 to 10
# Make curves with odd-numbered IDs have an x-distribution that is random
# uniform [0,1] and those with even-numbered IDs have an x-dist. that is
# half as wide but still centered at 0.5.  Shift y values higher with
# increasing IDs
set.seed(1)
N &lt;- 200
nc &lt;- sample(1:10, N, TRUE)
id &lt;- rep(1:N, nc)
x &lt;- y &lt;- id
for(i in 1:N) {
  x[id==i] &lt;- if(i %% 2) runif(nc[i]) else runif(nc[i], c(.25, .75))
  y[id==i] &lt;- i + 10*(x[id==i] - .5) + runif(nc[i], -10, 10)
}

w &lt;- curveRep(x, y, id, kxdist=2, p=10)
w
par(ask=TRUE, mfrow=c(4,5))
plot(w)                # show everything, profiles going across
par(mfrow=c(2,5))
plot(w,1)              # show n=1 results
# Use a color assignment table, assigning low curves to green and
# high to red.  Unique curve (subject) IDs are the names of the vector.
cols &lt;- c(rep('green', N/2), rep('red', N/2))
names(cols) &lt;- as.character(1:N)
plot(w, 3, idcol=cols)
par(ask=FALSE, mfrow=c(1,1))

plot(w, 1, 'lattice')  # show n=1 results
plot(w, 3, 'lattice')  # show n=4-5 results
plot(w, 3, 'lattice', idcol=cols)  # same but different color mapping
plot(w, 3, 'lattice', m=1)  # show a single "representative" curve
# Show median, 10th, and 90th percentiles of supposedly representative curves
plot(w, 3, 'lattice', m='quantiles', probs=c(.5,.1,.9))
# Same plot but with much less grouping of x variable
plot(w, 3, 'lattice', m='quantiles', probs=c(.5,.1,.9), nx=2)

# Use ggplot2 for one sample size interval
z &lt;- plot(w, 2, 'data')
require(ggplot2)
ggplot(z, aes(x, y, color=curve)) + geom_line() +
       facet_grid(distribution ~ cluster) +
       theme(legend.position='none') +
       labs(caption=z$ninterval[1])


# Smooth data before profiling.  This allows later plotting to plot
# smoothed representative curves rather than raw curves (which
# specifying smooth=TRUE to curveRep would do, if curveSmooth was not used)
d &lt;- curveSmooth(x, y, id)
w &lt;- with(d, curveRep(x, y, id))

# Example to show that curveRep can cluster profiles correctly when
# there is no noise.  In the data there are four profiles - flat, flat
# at a higher mean y, linearly increasing then flat, and flat at the
# first height except for a sharp triangular peak

set.seed(1)
x &lt;- 0:100
m &lt;- length(x)
profile &lt;- matrix(NA, nrow=m, ncol=4)
profile[,1] &lt;- rep(0, m)
profile[,2] &lt;- rep(3, m)
profile[,3] &lt;- c(0:3, rep(3, m-4))
profile[,4] &lt;- c(0,1,3,1,rep(0,m-4))
col &lt;- c('black','blue','green','red')
matplot(x, profile, type='l', col=col)
xeval &lt;- seq(0, 100, length.out=5)
s &lt;- x 
matplot(x[s], profile[s,], type='l', col=col)

id &lt;- rep(1:100, each=m)
X &lt;- Y &lt;- id
cols &lt;- character(100)
names(cols) &lt;- as.character(1:100)
for(i in 1:100) {
  s &lt;- id==i
  X[s] &lt;- x
  j &lt;- sample(1:4,1)
  Y[s] &lt;- profile[,j]
  cols[i] &lt;- col[j]
}
table(cols)
yl &lt;- c(-1,4)
w &lt;- curveRep(X, Y, id, kn=1, kxdist=1, k=4)
plot(w, 1, 'lattice', idcol=cols, ylim=yl)
# Found 4 clusters but two have same profile
w &lt;- curveRep(X, Y, id, kn=1, kxdist=1, k=3)
plot(w, 1, 'lattice', idcol=cols, freq=cols, plotfreq=TRUE, ylim=yl)
# Incorrectly combined black and red because default value p=5 did
# not result in different profiles at x=xeval
w &lt;- curveRep(X, Y, id, kn=1, kxdist=1, k=4, p=40)
plot(w, 1, 'lattice', idcol=cols, ylim=yl)
# Found correct clusters because evaluated curves at 40 equally
# spaced points and could find the sharp triangular peak in profile 4

## End(Not run)
</code></pre>

<hr>
<h2 id='cut2'>Cut a Numeric Variable into Intervals</h2><span id='topic+cut2'></span>

<h3>Description</h3>

<p>Function like cut but left endpoints are inclusive and labels are of
the form <code>[lower, upper)</code>, except that last interval is <code>[lower,upper]</code>.  
If cuts are given, will by default make sure that cuts include entire
range of <code>x</code>.
Also, if cuts are not given, will cut <code>x</code> into quantile groups 
(<code>g</code> given) or groups
with a given minimum number of observations (<code>m</code>).  Whereas cut creates a
category object, <code>cut2</code> creates a factor object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cut2(x, cuts, m=150, g, levels.mean=FALSE, digits, minmax=TRUE,
oneval=TRUE, onlycuts=FALSE, formatfun=format, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cut2_+3A_x">x</code></td>
<td>

<p>numeric vector to classify into intervals
</p>
</td></tr>
<tr><td><code id="cut2_+3A_cuts">cuts</code></td>
<td>

<p>cut points
</p>
</td></tr>
<tr><td><code id="cut2_+3A_m">m</code></td>
<td>

<p>desired minimum number of observations in a group.  The algorithm does
not guarantee that all groups will have at least <code>m</code> observations.
</p>
</td></tr>
<tr><td><code id="cut2_+3A_g">g</code></td>
<td>

<p>number of quantile groups
</p>
</td></tr>
<tr><td><code id="cut2_+3A_levels.mean">levels.mean</code></td>
<td>

<p>set to <code>TRUE</code> to make the new categorical vector have levels attribute that is
the group means of <code>x</code> instead of interval endpoint labels
</p>
</td></tr>
<tr><td><code id="cut2_+3A_digits">digits</code></td>
<td>

<p>number of significant digits to use in constructing levels.  Default is 3
(5 if <code>levels.mean=TRUE</code>)
</p>
</td></tr>
<tr><td><code id="cut2_+3A_minmax">minmax</code></td>
<td>

<p>if cuts is specified but <code>min(x)&lt;min(cuts)</code> or <code>max(x)&gt;max(cuts)</code>, augments
cuts to include min and max <code>x</code>
</p>
</td></tr>
<tr><td><code id="cut2_+3A_oneval">oneval</code></td>
<td>

<p>if an interval contains only one unique value, the interval will be
labeled with the formatted version of that value instead of the
interval endpoints, unless <code>oneval=FALSE</code>
</p>
</td></tr>
<tr><td><code id="cut2_+3A_onlycuts">onlycuts</code></td>
<td>

<p>set to <code>TRUE</code> to only return the vector of computed cuts.  This
consists of the interior values plus outer ranges.
</p>
</td></tr>
<tr><td><code id="cut2_+3A_formatfun">formatfun</code></td>
<td>

<p>formatting function, supports formula notation (if <code>rlang</code> is installed)
</p>
</td></tr>
<tr><td><code id="cut2_+3A_...">...</code></td>
<td>

<p>additional arguments passed to <code>formatfun</code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a factor variable with levels of the form <code>[a,b)</code> or formatted means
(character strings) unless <code>onlycuts</code> is <code>TRUE</code> in which case
a numeric vector is returned
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+cut">cut</a></code>, <code><a href="stats.html#topic+quantile">quantile</a></code>, <code><a href="#topic+combine.levels">combine.levels</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x &lt;- runif(1000, 0, 100)
z &lt;- cut2(x, c(10,20,30))
table(z)
table(cut2(x, g=10))      # quantile groups
table(cut2(x, m=50))      # group x into intevals with at least 50 obs.
</code></pre>

<hr>
<h2 id='data.frame.create.modify.check'>
Tips for Creating, Modifying, and Checking Data Frames
</h2><span id='topic+data.frame.create.modify.check'></span>

<h3>Description</h3>

<p>This help file contains a template for importing data to create an R
data frame, correcting some problems resulting from the import and
making the data frame be stored more efficiently, modifying the data
frame (including better annotating it and changing the names of some
of its variables), and checking and inspecting the data frame for
reasonableness of the values of its variables and to describe patterns
of missing data.  Various built-in functions and functions in the
Hmisc library are used.  At the end some methods for creating data
frames &ldquo;from scratch&rdquo; within <span class="rlang"><b>R</b></span> are presented.
</p>
<p>The examples below attempt to clarify the separation of operations
that are done on a data frame as a whole, operations that are done on
a small subset of its variables without attaching the whole data
frame, and operations that are done on many variables after attaching
the data frame in search position one.  It also tries to clarify that
for analyzing several separate variables using <span class="rlang"><b>R</b></span> commands that do not
support a <code>data</code> argument, it is helpful to attach the data frame
in a search position later than position one.
</p>
<p>It is often useful to create, modify, and process datasets in the
following order.
</p>

<ol>
<li>
<p>Import external data into a data frame (if the raw data do not
contain column names, provide these during the import if possible)

</p>
</li>
<li>
<p>Make global changes to a data frame (e.g., changing variable
names)

</p>
</li>
<li>
<p>Change attributes or values of variables within a data frame

</p>
</li>
<li>
<p>Do analyses involving the whole data frame (without attaching it)<br />
(Data frame still in .Data)

</p>
</li>
<li>
<p>Do analyses of individual variables (after attaching the data
frame in search position two or later)

</p>
</li></ol>



<h3>Details</h3>

<p>The examples below use the <code>FEV</code> dataset from
<cite>Rosner 1995</cite>. Almost any dataset would do.  The jcetable data
are taken from <cite>Galobardes, etal.</cite>
</p>
<p>Presently, giving a variable the <code>"units"</code> attribute (using the
<span class="pkg">Hmisc</span> <code><a href="#topic+units">units</a></code> function) only benefits the
<span class="pkg">Hmisc</span> <code><a href="#topic+describe">describe</a></code> function and the <span class="pkg">rms</span>
library's version of the <code>link[rms]{Surv}</code> function.  Variables
labels defined with the Hmisc <code><a href="#topic+label">label</a></code> function are used by
<code><a href="#topic+describe">describe</a></code>, <code><a href="#topic+summary.formula">summary.formula</a></code>,  and many of
the plotting functions in <span class="pkg">Hmisc</span> and <span class="pkg">rms</span>.
</p>


<h3>References</h3>

<p>Alzola CF, Harrell FE (2006):
<em>An Introduction to S and the Hmisc and Design Libraries.</em>
Chapters 3 and 4,
<a href="https://hbiostat.org/R/doc/sintro.pdf">https://hbiostat.org/R/doc/sintro.pdf</a>.
</p>
<p>Galobardes, et al. (1998), <em>J Clin Epi</em> 51:875-881.
</p>
<p>Rosner B (1995): <em>Fundamentals of Biostatistics, 4th Edition.  </em>
New York: Duxbury Press.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+scan">scan</a></code>, <code><a href="utils.html#topic+read.table">read.table</a></code>,
<code><a href="#topic+cleanup.import">cleanup.import</a></code>, <code><a href="#topic+sas.get">sas.get</a></code>,
<code><a href="base.html#topic+data.frame">data.frame</a></code>, <code><a href="base.html#topic+attach">attach</a></code>, <code><a href="base.html#topic+detach">detach</a></code>,
<code><a href="#topic+describe">describe</a></code>, <code><a href="#topic+datadensity">datadensity</a></code>,
<code><a href="graphics.html#topic+plot.data.frame">plot.data.frame</a></code>, <code><a href="#topic+hist.data.frame">hist.data.frame</a></code>,
<code><a href="#topic+naclus">naclus</a></code>, <code><a href="base.html#topic+factor">factor</a></code>, <code><a href="#topic+label">label</a></code>,
<code><a href="#topic+units">units</a></code>, <code><a href="base.html#topic+names">names</a></code>, <code><a href="base.html#topic+expand.grid">expand.grid</a></code>,
<code><a href="#topic+summary.formula">summary.formula</a></code>, <code><a href="base.html#topic+summary.data.frame">summary.data.frame</a></code>,
<code><a href="base.html#topic+casefold">casefold</a></code>, <code><a href="utils.html#topic+edit">edit</a></code>, <code><a href="utils.html#topic+page">page</a></code>,
<code><a href="graphics.html#topic+plot.data.frame">plot.data.frame</a></code>, <code><a href="#topic+Cs">Cs</a></code>,
<code><a href="#topic+combine.levels">combine.levels</a></code>,<code><a href="#topic+upData">upData</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# First, we do steps that create or manipulate the data
# frame in its entirety.  For S-Plus, these are done with
# .Data in search position one (the default at the
# start of the session).
#
# -----------------------------------------------------------------------
# Step 1: Create initial draft of data frame
# 
# We usually begin by importing a dataset from
# # another application.  ASCII files may be imported
# using the scan and read.table functions.  SAS
# datasets may be imported using the Hmisc sas.get
# function (which will carry more attributes from
# SAS than using File \dots  Import) from the GUI
# menus.  But for most applications (especially
# Excel), File \dots Import will suffice.  If using
# the GUI, it is often best to provide variable
# names during the import process, using the Options
# tab, rather than renaming all fields later Of
# course, if the data to be imported already have
# field names (e.g., in Excel), let S use those
# automatically.  If using S-Plus, you can use a
# command to execute File \dots  Import, e.g.:


import.data(FileName = "/windows/temp/fev.asc",
            FileType = "ASCII", DataFrame = "FEV")


# Here we name the new data frame FEV rather than
# fev, because we wanted to distinguish a variable
# in the data frame named fev from the data frame
# name.  For S-Plus the command will look
# instead like the following:


FEV &lt;- importData("/tmp/fev.asc")




# -----------------------------------------------------------------------
# Step 2: Clean up data frame / make it be more
# efficiently stored
# 
# Unless using sas.get to import your dataset
# (sas.get already stores data efficiently), it is
# usually a good idea to run the data frame through
# the Hmisc cleanup.import function to change
# numeric variables that are always whole numbers to
# be stored as integers, the remaining numerics to
# single precision, strange values from Excel to
# NAs, and character variables that always contain
# legal numeric values to numeric variables.
# cleanup.import typically halves the size of the
# data frame.  If you do not specify any parameters
# to cleanup.import, the function assumes that no
# numeric variable needs more than 7 significant
# digits of precision, so all non-integer-valued
# variables will be converted to single precision.


FEV &lt;- cleanup.import(FEV)




# -----------------------------------------------------------------------
# Step 3: Make global changes to the data frame
# 
# A data frame has attributes that are "external" to
# its variables.  There are the vector of its
# variable names ("names" attribute), the
# observation identifiers ("row.names"), and the
# "class" (whose value is "data.frame").  The
# "names" attribute is the one most commonly in need
# of modification.  If we had wanted to change all
# the variable names to lower case, we could have
# specified lowernames=TRUE to the cleanup.import
# invocation above, or type


names(FEV) &lt;- casefold(names(FEV))


# The upData function can also be used to change
# variable names in two ways (see below).
# To change names in a non-systematic way we use
# other options.  Under Windows/NT the most
# straigtforward approach is to change the names
# interactively.  Click on the data frame in the
# left panel of the Object Browser, then in the
# right pane click twice (slowly) on a variable.
# Use the left arrow and other keys to edit the
# name.  Click outside that name field to commit the
# change.  You can also rename columns while in a
# Data Sheet.  To instead use programming commands
# to change names, use something like:


names(FEV)[6] &lt;- 'smoke'   # assumes you know the positions!  
names(FEV)[names(FEV)=='smoking'] &lt;- 'smoke' 
names(FEV) &lt;- edit(names(FEV))


# The last example is useful if you are changing
# many names.  But none of the interactive
# approaches such as edit() are handy if you will be
# re-importing the dataset after it is updated in
# its original application.  This problem can be
# addressed by saving the new names in a permanent
# vector in .Data:


new.names &lt;- names(FEV)


# Then if the data are re-imported, you can type


names(FEV) &lt;- new.names


# to rename the variables.




# -----------------------------------------------------------------------
# Step 4: Delete unneeded variables
# 
# To delete some of the variables, you can
# right-click on variable names in the Object
# Browser's right pane, then select Delete.  You can
# also set variables to have NULL values, which
# causes the system to delete them.  We don't need
# to delete any variables from FEV but suppose we
# did need to delete some from mydframe.


mydframe$x1 &lt;- NULL 
mydframe$x2 &lt;- NULL
mydframe[c('age','sex')] &lt;- NULL   # delete 2 variables 
mydframe[Cs(age,sex)]    &lt;- NULL   # same thing


# The last example uses the Hmisc short-cut quoting
# function Cs.  See also the drop parameter to upData.




# -----------------------------------------------------------------------
# Step 5: Make changes to individual variables
#         within the data frame
# 
# After importing data, the resulting variables are
# seldom self - documenting, so we commonly need to
# change or enhance attributes of individual
# variables within the data frame.
# 
# If you are only changing a few variables, it is
# efficient to change them directly without
# attaching the entire data frame.


FEV$sex   &lt;- factor(FEV$sex,   0:1, c('female','male')) 
FEV$smoke &lt;- factor(FEV$smoke, 0:1, 
                    c('non-current smoker','current smoker')) 
units(FEV$age)    &lt;- 'years'
units(FEV$fev)    &lt;- 'L' 
label(FEV$fev)    &lt;- 'Forced Expiratory Volume' 
units(FEV$height) &lt;- 'inches'


# When changing more than one or two variables it is
# more convenient change the data frame using the
# Hmisc upData function.


FEV2 &lt;- upData(FEV,
  rename=c(smoking='smoke'), 
  # omit if renamed above
  drop=c('var1','var2'),
  levels=list(sex  =list(female=0,male=1),
              smoke=list('non-current smoker'=0,
                         'current smoker'=1)),
  units=list(age='years', fev='L', height='inches'),
  labels=list(fev='Forced Expiratory Volume'))


# An alternative to levels=list(\dots) is for example
# upData(FEV, sex=factor(sex,0:1,c('female','male'))).
# 
# Note that we saved the changed data frame into a
# new data frame FEV2.  If we were confident of the
# correctness of our changes we could have stored
# the new data frame on top of the old one, under
# the original name FEV.


# -----------------------------------------------------------------------
# Step 6:  Check the data frame
# 
# The Hmisc describe function is perhaps the first
# function that should be used on the new data
# frame.  It provides documentation of all the
# variables and the frequency tabulation, counts of
# NAs,  and 5 largest and smallest values are
# helpful in detecting data errors.  Typing
# describe(FEV) will write the results to the
# current output window.  To put the results in a
# new window that can persist, even upon exiting
# S, we use the page function.  The describe
# output can be minimized to an icon but kept ready
# for guiding later steps of the analysis.


page(describe(FEV2), multi=TRUE) 
# multi=TRUE allows that window to persist while
# control is returned to other windows


# The new data frame is OK.  Store it on top of the
# old FEV and then use the graphical user interface
# to delete FEV2 (click on it and hit the Delete
# key) or type rm(FEV2) after the next statement.


FEV &lt;- FEV2


# Next, we can use a variety of other functions to
# check and describe all of the variables.  As we
# are analyzing all or almost all of the variables,
# this is best done without attaching the data
# frame.  Note that plot.data.frame plots inverted
# CDFs for continuous variables and dot plots
# showing frequency distributions of categorical
# ones.


summary(FEV)
# basic summary function (summary.data.frame) 


plot(FEV)                # plot.data.frame 
datadensity(FEV)         
# rug plots and freq. bar charts for all var.


hist.data.frame(FEV)     
# for variables having &gt; 2 values 


by(FEV, FEV$smoke, summary)  
# use basic summary function with stratification




# -----------------------------------------------------------------------
# Step 7:  Do detailed analyses involving individual
#          variables
# 
# Analyses based on the formula language can use
# data= so attaching the data frame may not be
# required.  This saves memory.  Here we use the
# Hmisc summary.formula function to compute 5
# statistics on height, stratified separately by age
# quartile and by sex.


options(width=80) 
summary(height ~ age + sex, data=FEV,
        fun=function(y)c(smean.sd(y),
                         smedian.hilow(y,conf.int=.5)))
# This computes mean height, S.D., median, outer quartiles


fit &lt;- lm(height ~ age*sex, data=FEV) 
summary(fit)


# For this analysis we could also have attached the
# data frame in search position 2.  For other
# analyses, it is mandatory to attach the data frame
# unless FEV$ prefixes each variable name.
# Important: DO NOT USE attach(FEV, 1) or
# attach(FEV, pos=1, \dots) if you are only analyzing
# and not changing the variables, unless you really
# need to avoid conflicts with variables in search
# position 1 that have the same names as the
# variables in FEV.  Attaching into search position
# 1 will cause S-Plus to be more of a memory hog.


attach(FEV)
# Use e.g. attach(FEV[,Cs(age,sex)]) if you only
# want to analyze a small subset of the variables
# Use e.g. attach(FEV[FEV$sex=='male',]) to
# analyze a subset of the observations


summary(height ~ age + sex,
        fun=function(y)c(smean.sd(y),
          smedian.hilow(y,conf.int=.5)))
fit &lt;- lm(height ~ age*sex)


# Run generic summary function on height and fev, 
# stratified by sex
by(data.frame(height,fev), sex, summary)


# Cross-classify into 4 sex x smoke groups
by(FEV, list(sex,smoke), summary)


# Plot 5 quantiles
s &lt;- summary(fev ~ age + sex + height,
              fun=function(y)quantile(y,c(.1,.25,.5,.75,.9)))


plot(s, which=1:5, pch=c(1,2,15,2,1), #pch=c('=','[','o',']','='), 
     main='A Discovery', xlab='FEV')


# Use the nonparametric bootstrap to compute a 
# 0.95 confidence interval for the population mean fev
smean.cl.boot(fev)    # in Hmisc


# Use the Statistics \dots Compare Samples \dots One Sample 
# keys to get a normal-theory-based C.I.  Then do it 
# more manually.  The following method assumes that 
# there are no NAs in fev


sd &lt;- sqrt(var(fev))
xbar &lt;- mean(fev)
xbar
sd
n &lt;- length(fev)
qt(.975,n-1)     
# prints 0.975 critical value of t dist. with n-1 d.f.


xbar + c(-1,1)*sd/sqrt(n)*qt(.975,n-1)   
# prints confidence limits


# Fit a linear model
# fit &lt;- lm(fev ~ other variables \dots)


detach()


# The last command is only needed if you want to
# start operating on another data frame and you want
# to get FEV out of the way.




# -----------------------------------------------------------------------
# Creating data frames from scratch
# 
# Data frames can be created from within S.  To
# create a small data frame containing ordinary
# data, you can use something like


dframe &lt;- data.frame(age=c(10,20,30), 
                     sex=c('male','female','male'),
                     stringsAsFactors=TRUE)


# You can also create a data frame using the Data
# Sheet.  Create an empty data frame with the
# correct variable names and types, then edit in the
# data.


dd &lt;- data.frame(age=numeric(0),sex=character(0),
                 stringsAsFactors=TRUE)


# The sex variable will be stored as a factor, and
# levels will be automatically added to it as you
# define new values for sex in the Data Sheet's sex
# column.
# 
# When the data frame you need to create is defined
# by systematically varying variables (e.g., all
# possible combinations of values of each variable),
# the expand.grid function is useful for quickly
# creating the data.  Then you can add
# non-systematically-varying variables to the object
# created by expand.grid, using programming
# statements or editing the Data Sheet.  This
# process is useful for creating a data frame
# representing all the values in a printed table.
# In what follows we create a data frame
# representing the combinations of values from an 8
# x 2 x 2 x 2 (event x method x sex x what) table,
# and add a non-systematic variable percent to the
# data.


jcetable &lt;- expand.grid(
 event=c('Wheezing at any time',
         'Wheezing and breathless',
         'Wheezing without a cold',
         'Waking with tightness in the chest',
         'Waking with shortness of breath',
         'Waking with an attack of cough',
         'Attack of asthma',
         'Use of medication'),
 method=c('Mail','Telephone'), 
 sex=c('Male','Female'),
 what=c('Sensitivity','Specificity'))


jcetable$percent &lt;- 
c(756,618,706,422,356,578,289,333,
  576,421,789,273,273,212,212,212,
  613,763,713,403,377,541,290,226,
  613,684,632,290,387,613,258,129,
  656,597,438,780,732,679,938,919,
  714,600,494,877,850,703,963,987,
  755,420,480,794,779,647,956,941,
  766,423,500,833,833,604,955,986) / 10


# In jcetable, event varies most rapidly, then
# method, then sex, and what.

## End(Not run)
</code></pre>

<hr>
<h2 id='dataRep'>
Representativeness of Observations in a Data Set
</h2><span id='topic+dataRep'></span><span id='topic+print.dataRep'></span><span id='topic+predict.dataRep'></span><span id='topic+print.predict.dataRep'></span><span id='topic+roundN'></span><span id='topic++5B.roundN'></span>

<h3>Description</h3>

<p>These functions are intended to be used to describe how well a given
set of new observations (e.g., new subjects) were represented in a
dataset used to develop a predictive model.
The <code>dataRep</code> function forms a data frame that contains all the unique
combinations of variable values that existed in a given set of
variable values.  Cross&ndash;classifications of values are created using
exact values of variables, so for continuous numeric variables it is
often necessary to round them to the nearest <code>v</code> and to possibly
curtail the values to some lower and upper limit before rounding.
Here <code>v</code> denotes a numeric constant specifying the matching tolerance
that will be used.  <code>dataRep</code> also stores marginal distribution
summaries for all the variables.  For numeric variables, all 101
percentiles are stored, and for all variables, the frequency
distributions are also stored (frequencies are computed after any
rounding and curtailment of numeric variables).  For the purposes of
rounding and curtailing, the <code>roundN</code> function is provided.  A <code>print</code>
method will summarize the calculations made by <code>dataRep</code>, and if
<code>long=TRUE</code> all unique combinations of values and their frequencies in
the original dataset are printed.
</p>
<p>The <code>predict</code> method for <code>dataRep</code> takes a new data frame having
variables named the same as the original ones (but whose factor levels
are not necessarily in the same order) and examines the collapsed
cross-classifications created by <code>dataRep</code> to find how many
observations were similar to each of the new observations after any
rounding or curtailment of limits is done.  <code>predict</code> also does some
calculations to describe how the variable values of the new
observations &quot;stack up&quot; against the marginal distributions of the
original data.  For categorical variables, the percent of observations
having a given variable with the value of the new observation (after
rounding for variables that were through <code>roundN</code> in the formula given
to <code>dataRep</code>) is computed.  For numeric variables, the percentile of
the original distribution in which the current value falls will be
computed.  For this purpose, the data are not rounded because the 101
original percentiles were retained; linear interpolation is used to
estimate percentiles for values between two tabulated percentiles.
The lowest marginal frequency of matching values across all variables
is also computed.  For example, if an age, sex combination matches 10
subjects in the original dataset but the age value matches 100 ages
(after rounding) and the sex value matches the sex code of 300
observations, the lowest marginal frequency is 100, which is a &quot;best
case&quot; upper limit for multivariable matching.  I.e., matching on all
variables has to result on a lower frequency than this amount.
A <code>print</code> method for the output of <code>predict.dataRep</code> prints all
calculations done by <code>predict</code> by default.  Calculations can be
selectively suppressed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataRep(formula, data, subset, na.action)

roundN(x, tol=1, clip=NULL)

## S3 method for class 'dataRep'
print(x, long=FALSE, ...)

## S3 method for class 'dataRep'
predict(object, newdata, ...)

## S3 method for class 'predict.dataRep'
print(x, prdata=TRUE, prpct=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataRep_+3A_formula">formula</code></td>
<td>

<p>a formula with no left-hand-side.  Continuous numeric variables in
need of rounding should appear in the formula as e.g. <code>roundN(x,5)</code> to
have a tolerance of e.g. +/- 2.5 in matching.  Factor or character
variables as well as numeric ones not passed through <code>roundN</code> are
matched on exactly.
</p>
</td></tr>
<tr><td><code id="dataRep_+3A_x">x</code></td>
<td>

<p>a numeric vector or an object created by <code>dataRep</code>
</p>
</td></tr>
<tr><td><code id="dataRep_+3A_object">object</code></td>
<td>

<p>the object created by <code>dataRep</code> or <code>predict.dataRep</code>
</p>
</td></tr>
<tr><td><code id="dataRep_+3A_data">data</code>, <code id="dataRep_+3A_subset">subset</code>, <code id="dataRep_+3A_na.action">na.action</code></td>
<td>

<p>standard modeling arguments.  Default <code>na.action</code> is <code>na.delete</code>,
i.e., observations in the original dataset having any variables
missing are deleted up front.
</p>
</td></tr>
<tr><td><code id="dataRep_+3A_tol">tol</code></td>
<td>

<p>rounding constant (tolerance is actually <code>tol/2</code> as values are rounded
to the nearest <code>tol</code>)
</p>
</td></tr>
<tr><td><code id="dataRep_+3A_clip">clip</code></td>
<td>

<p>a 2-vector specifying a lower and upper limit to curtail values of <code>x</code>
before rounding
</p>
</td></tr>
<tr><td><code id="dataRep_+3A_long">long</code></td>
<td>

<p>set to <code>TRUE</code> to see all unique combinations and frequency count
</p>
</td></tr>
<tr><td><code id="dataRep_+3A_newdata">newdata</code></td>
<td>

<p>a data frame containing all the variables given to <code>dataRep</code> but not
necessarily in the same order or having factor levels in the same order
</p>
</td></tr>
<tr><td><code id="dataRep_+3A_prdata">prdata</code></td>
<td>

<p>set to <code>FALSE</code> to suppress printing <code>newdata</code> and the count of matching
observations (plus the worst-case marginal frequency). 
</p>
</td></tr>
<tr><td><code id="dataRep_+3A_prpct">prpct</code></td>
<td>
<p>set to <code>FALSE</code> to not print percentiles and percents</p>
</td></tr>
<tr><td><code id="dataRep_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>dataRep</code> returns a list of class <code>"dataRep"</code> containing the collapsed
data frame and frequency counts along with marginal distribution
information.  <code>predict</code> returns an object of class <code>"predict.dataRep"</code>
containing information determined by matching observations in
<code>newdata</code> with the original (collapsed) data.
</p>


<h3>Side Effects</h3>

<p><code>print.dataRep</code> prints.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University School of Medicine
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+round">round</a></code>, <code><a href="base.html#topic+table">table</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(13)
num.symptoms &lt;- sample(1:4, 1000,TRUE)
sex &lt;- factor(sample(c('female','male'), 1000,TRUE))
x    &lt;- runif(1000)
x[1] &lt;- NA
table(num.symptoms, sex, .25*round(x/.25))


d &lt;- dataRep(~ num.symptoms + sex + roundN(x,.25))
print(d, long=TRUE)


predict(d, data.frame(num.symptoms=1:3, sex=c('male','male','female'),
                      x=c(.03,.5,1.5)))
</code></pre>

<hr>
<h2 id='deff'>
Design Effect and Intra-cluster Correlation
</h2><span id='topic+deff'></span>

<h3>Description</h3>

<p>Computes the Kish design effect and corresponding intra-cluster correlation
for a single cluster-sampled variable
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deff(y, cluster)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deff_+3A_y">y</code></td>
<td>

<p>variable to analyze
</p>
</td></tr>
<tr><td><code id="deff_+3A_cluster">cluster</code></td>
<td>

<p>a variable whose unique values indicate cluster membership.  Any
type of variable is allowed.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector with named elements <code>n</code> (total number of non-missing
observations), <code>clusters</code> (number of clusters after deleting
missing data), <code>rho</code>(intra-cluster correlation), and <code>deff</code>
(design effect).
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="rms.html#topic+bootcov">bootcov</a></code>, <code><a href="rms.html#topic+robcov">robcov</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
blood.pressure &lt;- rnorm(1000, 120, 15)
clinic &lt;- sample(letters, 1000, replace=TRUE)
deff(blood.pressure, clinic)
</code></pre>

<hr>
<h2 id='describe'>Concise Statistical Description of a Vector, Matrix, Data Frame,
or Formula</h2><span id='topic+describe'></span><span id='topic+describe.default'></span><span id='topic+describe.vector'></span><span id='topic+describe.matrix'></span><span id='topic+describe.formula'></span><span id='topic+describe.data.frame'></span><span id='topic+plot.describe'></span><span id='topic+print.describe'></span><span id='topic+print.describe.single'></span><span id='topic++5B.describe'></span><span id='topic+latex.describe'></span><span id='topic+latex.describe.single'></span><span id='topic+html.describe'></span><span id='topic+html.describe.single'></span><span id='topic+formatdescribeSingle'></span>

<h3>Description</h3>

<p><code>describe</code> is a generic method that invokes <code>describe.data.frame</code>,
<code>describe.matrix</code>, <code>describe.vector</code>, or
<code>describe.formula</code>. <code>describe.vector</code> is the basic 
function for handling a single variable.
This function determines whether the variable is character, factor,
category, binary, discrete numeric, and continuous numeric, and prints
a concise statistical summary according to each. A numeric variable is
deemed discrete if it has &lt;= 10 distinct values. In this case,
quantiles are not printed. A frequency table is printed 
for any non-binary variable if it has no more than 20 distinct
values.  For any variable for which the frequency table is not printed,
the 5 lowest and highest values are printed.  This behavior can be
overriden for long character variables with many levels using the
<code>listunique</code> parameter, to get a complete tabulation.
</p>
<p><code>describe</code> is especially useful for
describing data frames created by <code>*.get</code>, as labels, formats,
value labels, and (in the case of <code>sas.get</code>) frequencies of special
missing values are printed.
</p>
<p>For a binary variable, the sum (number of 1's) and mean (proportion of
1's) are printed. If the first argument is a formula, a model frame
is created and passed to describe.data.frame.  If a variable
is of class <code>"impute"</code>, a count of the number of imputed values is
printed.  If a date variable has an attribute <code>partial.date</code>
(this is set up by <code>sas.get</code>), counts of how many partial dates are
actually present (missing month, missing day, missing both) are also presented.
If a variable was created by the special-purpose function <code>substi</code> (which
substitutes values of a second variable if the first variable is NA),
the frequency table of substitutions is also printed.
</p>
<p>For numeric variables, <code>describe</code> adds an item called <code>Info</code>
which is a relative information measure using the relative efficiency of
a proportional odds/Wilcoxon test on the variable relative to the same
test on a variable that has no ties.  <code>Info</code> is related to how
continuous the variable is, and ties are less harmful the more untied
values there are.  The formula for <code>Info</code> is one minus the sum of
the cubes of relative frequencies of values divided by one minus the
square of the reciprocal of the sample size.  The lowest information
comes from a variable having only one distinct value following by a
highly skewed binary variable.  <code>Info</code> is reported to
two decimal places.
</p>
<p>A latex method exists for converting the <code>describe</code> object to a
LaTeX file.  For numeric variables having more than 20 distinct values,
<code>describe</code> saves in its returned object the frequencies of 100
evenly spaced bins running from minimum observed value to the maximum.
When there are less than or equal to 20 distinct values, the original
values are maintained.
<code>latex</code> and <code>html</code> insert a spike histogram displaying these
frequency counts in the tabular material using the LaTeX picture
environment.  For example output see
<a href="https://hbiostat.org/doc/rms/book/chapter7edition1.pdf">https://hbiostat.org/doc/rms/book/chapter7edition1.pdf</a>.
Note that the latex method assumes you have the following styles
installed in your latex installation: setspace and relsize.
</p>
<p>The <code>html</code> method mimics the LaTeX output.  This is useful in the
context of Quarto/Rmarkdown html and html notebook output.
If <code>options(prType='html')</code> is in effect, calling <code>print</code> on
an object that is the result of running <code>describe</code> on a data frame
will result in rendering the HTML version.  If run from the console a
browser window will open.  When <code>which</code> is specified to
<code>print</code>, whether or not <code>prType='html'</code> is in effect, a
<code>gt</code> package html table will be produced containing only 
the types of variables requested.  When <code>which='both'</code> a list with
element names <code>Continuous</code> and <code>Categorical</code> is produced,
making it convenient for the user to print as desired, or to pass the
list directed to the <code>qreport</code> <code>maketabs</code> function when using Quarto.
</p>
<p>The <code>plot</code> method is for <code>describe</code> objects run on data
frames.  It produces spike histograms for a graphic of
continuous variables and a dot chart for categorical variables, showing
category proportions.  The graphic format is <code>ggplot2</code> if the user
has not set <code>options(grType='plotly')</code> or has set the <code>grType</code>
option to something other than <code>'plotly'</code>.  Otherwise <code>plotly</code>
graphics that are interactive are produced, and these can be placed into
an Rmarkdown html notebook.  The user must install the <code>plotly</code>
package for this to work.  When the use hovers the mouse over a bin for
a raw data value, the actual value will pop-up (formatted using
<code>digits</code>).  When the user hovers over the minimum data value, most
of the information calculated by <code>describe</code> will pop up.  For each
variable, the number of missing values is used to assign the color to
the histogram or dot chart, and a legend is drawn.  Color is not used if
there are no missing values in any variable. For categorical variables,
hovering over the leftmost point for a variable displays details, and
for all points proportions, numerators, and denominators are displayed
in the popup.  If both continuous and categorical variables are present
and <code>which='both'</code> is specified, the <code>plot</code> method returns an
unclassed <code>list</code> containing two objects, named <code>'Categorical'</code>
and <code>'Continuous'</code>, in that order.
</p>
<p>Sample weights may be specified to any of the functions, resulting
in weighted means, quantiles, and frequency tables.
</p>
<p>Note: As discussed in Cox and Longton (2008), Stata Technical Bulletin 8(4)
pp. 557, the term &quot;unique&quot; has been replaced with &quot;distinct&quot; in the
output (but not in parameter names).
</p>
<p>When <code>weights</code> are not used, Gini's mean difference is computed for
numeric variables.  This is a robust measure of dispersion that is the
mean absolute difference between any pairs of observations.  In simple
output Gini's difference is labeled <code>Gmd</code>.
</p>
<p><code>formatdescribeSingle</code> is a service function for <code>latex</code>,
<code>html</code>, and <code>print</code> methods for single variables that is not
intended to be called by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'vector'
describe(x, descript, exclude.missing=TRUE, digits=4,
         listunique=0, listnchar=12,
         weights=NULL, normwt=FALSE, minlength=NULL, shortmChoice=TRUE,
         rmhtml=FALSE, trans=NULL, lumptails=0.01, ...)
## S3 method for class 'matrix'
describe(x, descript, exclude.missing=TRUE, digits=4, ...)
## S3 method for class 'data.frame'
describe(x, descript, exclude.missing=TRUE,
    digits=4, trans=NULL, ...)
## S3 method for class 'formula'
describe(x, descript, data, subset, na.action,
    digits=4, weights, ...)
## S3 method for class 'describe'
print(x, which = c('both', 'categorical', 'continuous'), ...)
## S3 method for class 'describe'
latex(object, title=NULL,
      file=paste('describe',first.word(expr=attr(object,'descript')),'tex',sep='.'),
      append=FALSE, size='small', tabular=TRUE, greek=TRUE,
      spacing=0.7, lspace=c(0,0), ...)
## S3 method for class 'describe.single'
latex(object, title=NULL, vname,
      file, append=FALSE, size='small', tabular=TRUE, greek=TRUE,
      lspace=c(0,0), ...)
## S3 method for class 'describe'
html(object, size=85, tabular=TRUE,
      greek=TRUE, scroll=FALSE, rows=25, cols=100, ...)
## S3 method for class 'describe.single'
html(object, size=85,
      tabular=TRUE, greek=TRUE, ...)
formatdescribeSingle(x, condense=c('extremes', 'frequencies', 'both', 'none'),
           lang=c('plain', 'latex', 'html'), verb=0, lspace=c(0, 0),
           size=85, ...)
## S3 method for class 'describe'
plot(x, which=c('both', 'continuous', 'categorical'),
                          what=NULL,
                          sort=c('ascending', 'descending', 'none'),
                          n.unique=10, digits=5, bvspace=2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="describe_+3A_x">x</code></td>
<td>

<p>a data frame, matrix, vector, or formula.  For a data frame, the 
<code>describe.data.frame</code>
function is automatically invoked.  For a matrix, <code>describe.matrix</code> is
called.  For a formula, describe.data.frame(model.frame(x))
is invoked. The formula may or may not have a response variable.  For
<code>print</code>, <code>latex</code>, <code>html</code>, or
<code>formatdescribeSingle</code>, <code>x</code> is an object created by
<code>describe</code>.
</p>
</td></tr>
<tr><td><code id="describe_+3A_descript">descript</code></td>
<td>

<p>optional title to print for x. The default is the name of the argument
or the &quot;label&quot; attributes of individual variables. When the first argument
is a formula, <code>descript</code> defaults to a character representation of
the formula.
</p>
</td></tr>
<tr><td><code id="describe_+3A_exclude.missing">exclude.missing</code></td>
<td>

<p>set toTRUE to print the names of variables that contain only missing values.
This list appears at the bottom of the printout, and no space is taken
up for such variables in the main listing.
</p>
</td></tr>
<tr><td><code id="describe_+3A_digits">digits</code></td>
<td>

<p>number of significant digits to print.  For <code>plot.describe</code> is
the number of significant digits to put in hover text for
<code>plotly</code> when showing raw variable values.</p>
</td></tr> 
<tr><td><code id="describe_+3A_listunique">listunique</code></td>
<td>

<p>For a character variable that is not an <code>mChoice</code> variable, that
has its longest string length greater than <code>listnchar</code>, and that
has no more than <code>listunique</code> distinct values, all values are
listed in alphabetic order.  Any value having more than one occurrence
has the frequency of occurrence included.  Specify
<code>listunique</code> equal to some value at least as large as the number
of observations to ensure that all character variables will have all
their values listed.  For purposes of tabulating character strings,
multiple white spaces of any kind are translated to a single space,
leading and trailing white space are ignored, and case is ignored.
</p>
</td></tr>
<tr><td><code id="describe_+3A_listnchar">listnchar</code></td>
<td>
<p>see <code>listunique</code></p>
</td></tr>
<tr><td><code id="describe_+3A_weights">weights</code></td>
<td>

<p>a numeric vector of frequencies or sample weights.  Each observation
will be treated as if it were sampled <code>weights</code> times.
</p>
</td></tr>
<tr><td><code id="describe_+3A_minlength">minlength</code></td>
<td>
<p>value passed to summary.mChoice</p>
</td></tr>
<tr><td><code id="describe_+3A_shortmchoice">shortmChoice</code></td>
<td>
<p>set to <code>FALSE</code> to have summary of
<code>mChoice</code> variables use actual levels everywhere, instead of
abbreviating to integers and printing of all original labels at the
top</p>
</td></tr>
<tr><td><code id="describe_+3A_rmhtml">rmhtml</code></td>
<td>
<p>set to <code>TRUE</code> to strip html from variable labels</p>
</td></tr>
<tr><td><code id="describe_+3A_trans">trans</code></td>
<td>
<p>for <code>describe.vector</code> is a list specifying how to
transform <code>x</code> for constructing the frequency distribution used in
spike histograms.  The first element of the list is a character string
describing the transformation, the second is the transformation
function, and the third argument is the inverse of this function that
is used in labeling points on the original scale,
e.g. <code>trans=list('log', log, exp)</code>.  For
<code>describe.data.frame</code> <code>trans</code> is a list of such lists, with
the name of each list being name of the variable to which the
transformation applies.  See
<a href="https://hbiostat.org/rmsc/impred.html#data">https://hbiostat.org/rmsc/impred.html#data</a> for an example.</p>
</td></tr>
<tr><td><code id="describe_+3A_lumptails">lumptails</code></td>
<td>
<p>specifies the quantile to use (its complement is also
used) for grouping observations in the tails so that outliers have
less chance of distorting the variable's range for sparkline spike
histograms.  The default is 0.01, i.e., observations below the 0.01
quantile are grouped together in the leftmost bin, and observations
above the 0.99 quantile are grouped to form the last bin.</p>
</td></tr>
<tr><td><code id="describe_+3A_normwt">normwt</code></td>
<td>

<p>The default, <code>normwt=FALSE</code> results in the use of <code>weights</code> as
weights in computing various statistics.  In this case the sample size
is assumed to be equal to the sum of <code>weights</code>.  Specify
<code>normwt=TRUE</code> to divide 
<code>weights</code> by a constant so that <code>weights</code> sum to the number of
observations (length of vectors specified to <code>describe</code>).  In this
case the number of observations is taken to be the actual number of
records given to <code>describe</code>.
</p>
</td></tr>
<tr><td><code id="describe_+3A_object">object</code></td>
<td>
<p>a result of <code>describe</code></p>
</td></tr>
<tr><td><code id="describe_+3A_title">title</code></td>
<td>
<p>unused</p>
</td></tr>
<tr><td><code id="describe_+3A_data">data</code></td>
<td>
<p>a data frame, data table, or list</p>
</td></tr>
<tr><td><code id="describe_+3A_subset">subset</code></td>
<td>
<p>a subsetting expression</p>
</td></tr>
<tr><td><code id="describe_+3A_na.action">na.action</code></td>
<td>

<p>These are used if a formula is specified.  <code>na.action</code> defaults to
<code>na.retain</code> which does not delete any <code>NA</code>s from the data frame.
Use <code>na.action=na.omit</code> or <code>na.delete</code> to drop any observation with
any <code>NA</code> before processing.
</p>
</td></tr>
<tr><td><code id="describe_+3A_...">...</code></td>
<td>

<p>arguments passed to <code>describe.default</code> which are passed to calls
to <code>format</code> for numeric variables.  For example if using R
<code>POSIXct</code> or <code>Date</code> date/time formats, specifying
<code>describe(d,format='%d%b%y')</code> will print date/time variables as
<code>"01Jan2000"</code>.  This is useful for omitting the time
component.  See the help file for <code>format.POSIXct</code> or
<code>format.Date</code> for more
information.  For <code>plot</code> methods, ... is ignored.
For <code>html</code> and <code>latex</code> methods, ... is used to pass
optional arguments to <code>formatdescribeSingle</code>, especially the
<code>condense</code> argument.  For the <code>print</code> method when
<code>which=</code> is given, possible
arguments to use for tabulating continuous variable output are
<code>sparkwidth</code> (the width of the spike histogram sparkline in pixels,
defaulting to 200), <code>qcondense</code> (set to <code>FALSE</code> to devote
separate columns to all quantiles), <code>extremes</code> (set to
<code>TRUE</code> to print the 5 lowest and highest values in the table of
continuous variables).  For categorical variable output, the argument
<code>freq</code> can be used to specify how frequency tables are rendered:
<code>'chart'</code> (the default; an interactive sparkline frequency bar chart) or
<code>freq='table'</code> for small tables.  <code>sort</code> is another argument
passed to <code>html_describe_cat</code>.  For sparkline frequency charts
the default is to sort non-numeric categories in descending order of
frequency. 	Set <code>code=FALSE</code> to use the original data order.  The
<code>w</code> argument also applies to categorical variable output.
</p>
</td></tr>
<tr><td><code id="describe_+3A_file">file</code></td>
<td>

<p>name of output file (should have a suffix of .tex).  Default name is
formed from the first word of the <code>descript</code> element of the
<code>describe</code> object, prefixed by <code>"describe"</code>.  Set
<code>file=""</code> to send LaTeX code to standard output instead of a file.
</p>
</td></tr>
<tr><td><code id="describe_+3A_append">append</code></td>
<td>

<p>set to <code>TRUE</code> to have <code>latex</code> append text to an existing file
named <code>file</code>
</p>
</td></tr>
<tr><td><code id="describe_+3A_size">size</code></td>
<td>

<p>LaTeX text size (<code>"small"</code>, the default, or <code>"normalsize"</code>,
<code>"tiny"</code>, <code>"scriptsize"</code>, etc.) for the <code>describe</code> output
in LaTeX. For html is the percent of the prevailing font size to use for
the output.
</p>
</td></tr>
<tr><td><code id="describe_+3A_tabular">tabular</code></td>
<td>

<p>set to <code>FALSE</code> to use verbatim rather than tabular (or html
table) environment for the summary statistics output.  By default,
tabular is used if the output is not too wide.</p>
</td></tr>
<tr><td><code id="describe_+3A_greek">greek</code></td>
<td>
<p>By default, the <code>latex</code> and <code>html</code> methods
will change names of greek letters that appear in variable
labels to appropriate LaTeX symbols in math mode, or html symbols,  unless
<code>greek=FALSE</code>.</p>
</td></tr>
<tr><td><code id="describe_+3A_spacing">spacing</code></td>
<td>
<p>By default, the <code>latex</code> method for <code>describe</code> run
on a matrix or data frame uses the <code>setspace</code> LaTeX package with a
line spacing of 0.7 so as to no waste space.  Specify <code>spacing=0</code>
to suppress the use of the <code>setspace</code>'s <code>spacing</code> environment,
or specify another positive value to use this environment with a
different spacing.</p>
</td></tr>
<tr><td><code id="describe_+3A_lspace">lspace</code></td>
<td>
<p>extra vertical scape, in character size units (i.e., &quot;ex&quot;
as appended to the space).  When using certain font sizes, there is
too much space left around LaTeX verbatim environments.  This
two-vector specifies space to remove (i.e., the values are negated in
forming the <code>vspace</code> command) before (first element) and after
(second element of <code>lspace</code>) verbatims</p>
</td></tr>
<tr><td><code id="describe_+3A_scroll">scroll</code></td>
<td>
<p>set to <code>TRUE</code> to create an html scrollable box for
the html output</p>
</td></tr>
<tr><td><code id="describe_+3A_rows">rows</code>, <code id="describe_+3A_cols">cols</code></td>
<td>
<p>the number of rows or columns to allocate for the
scrollable box</p>
</td></tr>
<tr><td><code id="describe_+3A_vname">vname</code></td>
<td>
<p>unused argument in <code>latex.describe.single</code></p>
</td></tr>
<tr><td><code id="describe_+3A_which">which</code></td>
<td>
<p>specifies whether to plot numeric continuous or
binary/categorical variables, or both.  When <code>"both"</code> a list with
two elements is created.  Each element is a <code>ggplot2</code> or
<code>plotly</code> object.  If there are no variables of a given type, a
single <code>ggplot2</code> or <code>plotly</code> object is returned, ready to
print.  For <code>print.describe</code> may be <code>"categorical"</code> or
<code>"continuous"</code>, causing a <code>gt</code> table to be created with the
categorical or continuous variable <code>describe</code> results.</p>
</td></tr>  
<tr><td><code id="describe_+3A_what">what</code></td>
<td>
<p>character or numeric vector specifying which variables to
plot; default is to plot all</p>
</td></tr>
<tr><td><code id="describe_+3A_sort">sort</code></td>
<td>
<p>specifies how and whether variables are sorted in order of
the proportion of positives when <code>which="categorical"</code>.  Specify
<code>sort="none"</code> to leave variables in the order they appear in the
original data.</p>
</td></tr>
<tr><td><code id="describe_+3A_n.unique">n.unique</code></td>
<td>
<p>the minimum number of distinct values a numeric variable
must have before <code>plot.describe</code> uses it in a continuous variable
plot</p>
</td></tr>
<tr><td><code id="describe_+3A_bvspace">bvspace</code></td>
<td>
<p>the between-variable spacing for categorical variables.
Defaults to 2, meaning twice the amount of vertical space as what is
used for between-category spacing within a variable</p>
</td></tr>
<tr><td><code id="describe_+3A_condense">condense</code></td>
<td>
<p>specifies whether to condense the output with regard to
the 5 lowest and highest values (<code>"extremes"</code>) and the frequency table
</p>
</td></tr>
<tr><td><code id="describe_+3A_lang">lang</code></td>
<td>
<p>specifies the markup language</p>
</td></tr>
<tr><td><code id="describe_+3A_verb">verb</code></td>
<td>
<p>set to 1 if a verbatim environment is already in effect for LaTeX</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>options(na.detail.response=TRUE)</code>
has been set and <code>na.action</code> is <code>"na.delete"</code> or
<code>"na.keep"</code>, summary  statistics on
the response variable are printed separately for missing and non-missing
values of each predictor.  The default summary function returns
the number of non-missing response values and the mean of the last
column of the response values, with a <code>names</code> attribute of
<code>c("N","Mean")</code>. 
When the response is a <code>Surv</code> object and the mean is used, this will
result in the crude proportion of events being used to summarize
the response.  The actual summary function can be designated through
<code>options(na.fun.response = "function name")</code>.
</p>
<p>If you are modifying LaTex <code>parskip</code> or certain other parameters,
you may need to shrink the area around <code>tabular</code> and
<code>verbatim</code> environments produced by <code>latex.describe</code>.  You can
do this using for example
<code>\usepackage{etoolbox}\makeatletter\preto{\@verbatim}{\topsep=-1.4pt
	\partopsep=0pt}\preto{\@tabular}{\parskip=2pt
	\parsep=0pt}\makeatother</code> in the LaTeX preamble.
</p>


<h3>Value</h3>

<p>a list containing elements <code>descript</code>, <code>counts</code>,
<code>values</code>.  The list  is of class <code>describe</code>.  If the input
object was a matrix or a data 
frame, the list is a list of lists, one list for each variable
analyzed. <code>latex</code> returns a standard <code>latex</code> object.  For numeric
variables having at least 20 distinct values, an additional component
<code>intervalFreq</code>.  This component is a list with two elements, <code>range</code>
(containing two values) and <code>count</code>, a vector of 100 integer frequency
counts.  <code>print</code> with <code>which=</code> returns a 'gt' table object.
The user can modify the table by piping formatting changes, column
removals, and other operations, before final rendering.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spikecomp">spikecomp</a></code>, <code><a href="#topic+sas.get">sas.get</a></code>, <code><a href="stats.html#topic+quantile">quantile</a></code>,
<code><a href="#topic+GiniMd">GiniMd</a></code>,
<code><a href="base.html#topic+table">table</a></code>, <code><a href="base.html#topic+summary">summary</a></code>,
<code><a href="stats.html#topic+model.frame.default">model.frame.default</a></code>, 
<code><a href="stats.html#topic+naprint">naprint</a></code>, <code><a href="base.html#topic+lapply">lapply</a></code>, <code><a href="base.html#topic+tapply">tapply</a></code>,
<code><a href="survival.html#topic+Surv">Surv</a></code>, <code><a href="#topic+na.delete">na.delete</a></code>,
<code><a href="#topic+na.keep">na.keep</a></code>, 
<code><a href="#topic+na.detail.response">na.detail.response</a></code>, <code><a href="#topic+latex">latex</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
describe(runif(200),dig=2)    #single variable, continuous
                              #get quantiles .05,.10,\dots

dfr &lt;- data.frame(x=rnorm(400),y=sample(c('male','female'),400,TRUE))
describe(dfr)

## Not run: 
options(grType='plotly')
d &lt;- describe(mydata)
p &lt;- plot(d)   # create plots for both types of variables
p[[1]]; p[[2]] # or p$Categorical; p$Continuous
plotly::subplot(p[[1]], p[[2]], nrows=2)  # plot both in one
plot(d, which='categorical')    # categorical ones

d &lt;- sas.get(".","mydata",special.miss=TRUE,recode=TRUE)
describe(d)      #describe entire data frame
attach(d, 1)
describe(relig)  #Has special missing values .D .F .M .R .T
                 #attr(relig,"label") is "Religious preference"

#relig : Religious preference  Format:relig
#    n missing  D  F M R T distinct 
# 4038     263 45 33 7 2 1        8
#
#0:none (251, 6%), 1:Jewish (372, 9%), 2:Catholic (1230, 30%) 
#3:Jehovah's Witnes (25, 1%), 4:Christ Scientist (7, 0%) 
#5:Seventh Day Adv (17, 0%), 6:Protestant (2025, 50%), 7:other (111, 3%) 


# Method for describing part of a data frame:
 describe(death.time ~ age*sex + rcs(blood.pressure))
 describe(~ age+sex)
 describe(~ age+sex, weights=freqs)  # weighted analysis

 fit &lt;- lrm(y ~ age*sex + log(height))
 describe(formula(fit))
 describe(y ~ age*sex, na.action=na.delete)   
# report on number deleted for each variable
 options(na.detail.response=TRUE)  
# keep missings separately for each x, report on dist of y by x=NA
 describe(y ~ age*sex)
 options(na.fun.response="quantile")
 describe(y ~ age*sex)   # same but use quantiles of y by x=NA

 d &lt;- describe(my.data.frame)
 d$age                   # print description for just age
 d[c('age','sex')]       # print description for two variables
 d[sort(names(d))]       # print in alphabetic order by var. names
 d2 &lt;- d[20:30]          # keep variables 20-30
 page(d2)                # pop-up window for these variables

# Test date/time formats and suppression of times when they don't vary
 library(chron)
 d &lt;- data.frame(a=chron((1:20)+.1),
                 b=chron((1:20)+(1:20)/100),
                 d=ISOdatetime(year=rep(2003,20),month=rep(4,20),day=1:20,
                               hour=rep(11,20),min=rep(17,20),sec=rep(11,20)),
                 f=ISOdatetime(year=rep(2003,20),month=rep(4,20),day=1:20,
                               hour=1:20,min=1:20,sec=1:20),
                 g=ISOdate(year=2001:2020,month=rep(3,20),day=1:20))
 describe(d)

# Make a function to run describe, latex.describe, and use the kdvi
# previewer in Linux to view the result and easily make a pdf file

 ldesc &lt;- function(data) {
  options(xdvicmd='kdvi')
  d &lt;- describe(data, desc=deparse(substitute(data)))
  dvi(latex(d, file='/tmp/z.tex'), nomargins=FALSE, width=8.5, height=11)
 }

 ldesc(d)

## End(Not run)
</code></pre>

<hr>
<h2 id='discrete'> Discrete Vector tools </h2><span id='topic+as.discrete'></span><span id='topic+as.discrete.default'></span><span id='topic+discrete'></span><span id='topic++5B+3C-.discrete'></span><span id='topic++5B.discrete'></span><span id='topic++5B+5B.discrete'></span><span id='topic+is.discrete'></span><span id='topic+is.na+3C-.discrete'></span><span id='topic+length+3C-.discrete'></span>

<h3>Description</h3>

<p><code>discrete</code> creates a discrete vector which is distinct from a
continuous vector, or a factor/ordered vector.
The other function are tools for manipulating descrete vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.discrete(x, ...)
## Default S3 method:
as.discrete(x, ...)
discrete(x, levels = sort(unique.default(x), na.last = TRUE), exclude = NA)
## S3 replacement method for class 'discrete'
x[...] &lt;- value
## S3 method for class 'discrete'
x[..., drop = FALSE]
## S3 method for class 'discrete'
x[[i]]
is.discrete(x)
## S3 replacement method for class 'discrete'
is.na(x) &lt;- value
## S3 replacement method for class 'discrete'
length(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="discrete_+3A_x">x</code></td>
<td>
<p> a vector </p>
</td></tr>
<tr><td><code id="discrete_+3A_drop">drop</code></td>
<td>
<p> Should unused levels be dropped. </p>
</td></tr>
<tr><td><code id="discrete_+3A_exclude">exclude</code></td>
<td>
<p>logical: should <code>NA</code> be excluded. </p>
</td></tr>
<tr><td><code id="discrete_+3A_i">i</code></td>
<td>
<p> indexing vector </p>
</td></tr>
<tr><td><code id="discrete_+3A_levels">levels</code></td>
<td>
<p> charater: list of individual level values </p>
</td></tr>
<tr><td><code id="discrete_+3A_value">value</code></td>
<td>
<p> index of elements to set to <code>NA</code> </p>
</td></tr>
<tr><td><code id="discrete_+3A_...">...</code></td>
<td>
<p> arguments to be passed to other functions </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>as.discrete</code> converts a vector into a discrete vector.
</p>
<p><code>discrete</code> creates a discrete vector from provided values.
</p>
<p><code>is.discrete</code> tests to see if the vector is a discrete vector.
</p>


<h3>Value</h3>

<p><code>as.discrete</code>, <code>discrete</code> returns a vector of
<code>discrete</code> type.
</p>
<p><code>is.discrete</code> returan logical <code>TRUE</code> if the vector is of
class discrete other wise it returns <code>FALSE</code>.
</p>


<h3>Author(s)</h3>

<p> Charles Dupont</p>


<h3>See Also</h3>

 <p><code><a href="base.html#topic++5B+5B">[[</a></code>, <code><a href="base.html#topic++5B">[</a></code>, <code><a href="base.html#topic+factor">factor</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- discrete(1:25)
a

is.discrete(a)

b &lt;- as.discrete(2:4)
b
</code></pre>

<hr>
<h2 id='dotchart2'>
Enhanced Dot Chart
</h2><span id='topic+dotchart2'></span>

<h3>Description</h3>

<p><code>dotchart2</code> is an enhanced version of the <code>dotchart</code> function 
with several new options.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dotchart2(data, labels, groups=NULL, gdata=NA, horizontal=TRUE, pch=16,
          xlab='', ylab='', xlim=NULL, auxdata, auxgdata=NULL, auxtitle,
          lty=1, lines=TRUE, dotsize = .8,
          cex = par("cex"), cex.labels = cex,
          cex.group.labels = cex.labels*1.25, sort.=TRUE, 
	      add=FALSE, dotfont=par('font'), groupfont=2, 
	      reset.par=add, xaxis=TRUE, width.factor=1.1,
          lcolor='gray', leavepar=FALSE,
          axisat=NULL, axislabels=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dotchart2_+3A_data">data</code></td>
<td>
<p>a numeric vector whose values are shown on the x-axis</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_labels">labels</code></td>
<td>
<p>a vector of labels for each point, corresponding to
<code>x</code>.  If omitted, <code>names(data)</code> are used, and if there are
no <code>names</code>, integers prefixed by <code>"#"</code> are used.</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_groups">groups</code></td>
<td>
<p>an optional categorical variable indicating how
<code>data</code> values are grouped</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_gdata">gdata</code></td>
<td>
<p>data values for groups, typically summaries such as group
medians</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_horizontal">horizontal</code></td>
<td>
<p>set to <code>FALSE</code> to make the chart vertical
instead of the default</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_pch">pch</code></td>
<td>

<p>default character number or value for plotting dots in dot charts.
The default is 16.</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_xlab">xlab</code></td>
<td>
<p>x-axis title</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_ylab">ylab</code></td>
<td>
<p>y-axis title</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_xlim">xlim</code></td>
<td>
<p>x-axis limits.  Applies only to <code>horizontal=TRUE</code>.</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_auxdata">auxdata</code></td>
<td>

<p>a vector of auxiliary data given to <code>dotchart2</code>, of the same length
as the first (<code>data</code>) argument.  If present, this
vector of values will be printed outside the right margin of the dot
chart.  Usually <code>auxdata</code> represents cell sizes.
</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_auxgdata">auxgdata</code></td>
<td>

<p>similar to <code>auxdata</code> but corresponding to the <code>gdata</code>
argument.  These usually represent overall sample sizes for each
group of lines.</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_auxtitle">auxtitle</code></td>
<td>

<p>if <code>auxdata</code> is given, <code>auxtitle</code> specifies a column
heading for the extra printed data in the chart, e.g., <code>"N"</code></p>
</td></tr>
<tr><td><code id="dotchart2_+3A_lty">lty</code></td>
<td>
<p>line type for horizontal lines.  Default is 1 for R, 2 for S-Plus</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_lines">lines</code></td>
<td>
<p>set to <code>FALSE</code> to suppress drawing of reference
lines</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_dotsize">dotsize</code></td>
<td>

<p><code>cex</code> value for drawing dots.  Default is 0.8.  Note that the original
<code>dotchart</code> function used a default of 1.2.</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_cex">cex</code></td>
<td>
<p>see <code><a href="graphics.html#topic+par">par</a></code></p>
</td></tr>
<tr><td><code id="dotchart2_+3A_cex.labels">cex.labels</code></td>
<td>

<p><code>cex</code> parameter that applies only to the line labels for the
dot chart <code>cex</code> parameter for major grouping labels for
<code>dotchart2</code>.  Defaults to <code>cex</code>.</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_cex.group.labels">cex.group.labels</code></td>
<td>
<p>value of <code>cex</code> corresponding to <code>gdata</code></p>
</td></tr>
<tr><td><code id="dotchart2_+3A_sort.">sort.</code></td>
<td>

<p>set to <code>FALSE</code> to keep <code>dotchart2</code> from sorting the input
data, i.e., it will assume that the data are already properly
arranged.  This is especially useful when you are using <code>gdata</code>
and <code>groups</code> and you want to control the
order that groups appear on the chart (from top to bottom).</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_add">add</code></td>
<td>
<p>set to <code>TRUE</code> to add to an existing plot</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_dotfont">dotfont</code></td>
<td>

<p>font number of plotting dots.  Default is one.  Use <code>-1</code> to
use &quot;outline&quot; fonts.  For example, <code>pch=183, dotfont=-1</code>
plots an open circle for UNIX on postscript.  <code>pch=1</code> makes
an open octagon under Windows.</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_groupfont">groupfont</code></td>
<td>

<p>font number to use in drawing <code>group</code> labels for <code>dotchart2</code>.
Default is <code>2</code> for boldface.
</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_reset.par">reset.par</code></td>
<td>

<p>set to <code>FALSE</code> to cause <code>dotchart2</code> to not reset the <code>par</code>
parameters when finished.  This is useful when <code>add=TRUE</code> is about to
be used in another call.  The default is to reset the <code>par</code>
parameters if <code>add=TRUE</code> and not if <code>add=FALSE</code>, i.e., the
program assumes that only one set of points will be added to an
existing set.  If you fail to use <code>reset.par=TRUE</code> for the 
first of a series of plots, the next call to <code>plot</code> with
<code>add=TRUE</code> will result in distorted x-axis scaling.</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_xaxis">xaxis</code></td>
<td>
<p>set to <code>FALSE</code> to suppress drawing x-axis</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_width.factor">width.factor</code></td>
<td>

<p>When the calculated left margin turns out to be faulty, specify a
factor by which to multiple the left margin as <code>width.factor</code> to get
the appropriate space for labels on horizonal charts.</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_lcolor">lcolor</code></td>
<td>

<p>color for horizontal reference lines.  Default is <code>"gray"</code> for R,
<code>par("col")</code> for S-Plus.</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_leavepar">leavepar</code></td>
<td>
<p>set to <code>TRUE</code> to leave <code>par()</code> unchanged.
This assumes the user has allocated sufficient left and right
margins for a horizontal dot chart.</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_axisat">axisat</code></td>
<td>
<p>a vector of tick mark locations to pass to <code>axis</code>.
Useful if transforming the data axis</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_axislabels">axislabels</code></td>
<td>
<p>a vector of strings specifying axis tick mark
labels.  Useful if transforming the data axis</p>
</td></tr>
<tr><td><code id="dotchart2_+3A_...">...</code></td>
<td>
<p>arguments passed to <code>plot.default</code></p>
</td></tr>
</table>


<h3>Side Effects</h3>

<p><code>dotchart</code> will leave <code>par</code> altered if <code>reset.par=FALSE</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+dotchart">dotchart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(135)
maj &lt;- factor(c(rep('North',13),rep('South',13)))
g &lt;- paste('Category',rep(letters[1:13],2))
n &lt;- sample(1:15000, 26, replace=TRUE)
y1 &lt;- runif(26)
y2 &lt;- pmax(0, y1 - runif(26, 0, .1))
dotchart2(y1, g, groups=maj, auxdata=n, auxtitle='n', xlab='Y')
dotchart2(y2, g, groups=maj, pch=17, add=TRUE)
## Compare with dotchart function (no superpositioning or auxdata allowed):
## dotchart(y1, g, groups=maj, xlab='Y')

## To plot using a transformed scale add for example
## axisat=sqrt(pretty(y)), axislabels=pretty(y)
</code></pre>

<hr>
<h2 id='dotchart3'>Enhanced Version of dotchart Function</h2><span id='topic+dotchart3'></span><span id='topic+dotchartp'></span><span id='topic+summaryD'></span><span id='topic+summaryDp'></span>

<h3>Description</h3>

<p>These are adaptations of the R dotchart function that sorts categories
top to bottom, adds <code>auxdata</code> and <code>auxtitle</code> arguments to put
extra information in the right margin, and for <code>dotchart3</code> adds
arguments <code>cex.labels</code>, <code>cex.group.labels</code>, and
<code>groupfont</code>.  By default, group headings are in a larger, bold
font.  <code>dotchart3</code> also cuts a bit of white space from the top and
bottom of the chart.  The most significant change, however, is in how
<code>x</code> is interpreted.  Columns of <code>x</code> no longer provide an
alternate way to define groups.  Instead, they define superpositioned
values.  This is useful for showing three quartiles, for example.  Going
along with this change, for <code>dotchart3</code> <code>pch</code> can now be a
vector specifying symbols to use going across columns of <code>x</code>.
<code>x</code> was changed in this way because to put multiple points on a
line (e.g., quartiles) and keeping track of <code>par()</code> parameters when
<code>dotchart2</code> was called with <code>add=TRUE</code> was cumbersome.
<code>dotchart3</code> changes the margins to account for horizontal labels.
</p>
<p><code>dotchartp</code> is a version of <code>dotchart3</code> for making the chart
with the <code>plotly</code> package.
</p>
<p><code>summaryD</code> creates aggregate data using <code><a href="#topic+summarize">summarize</a></code> and
calls <code>dotchart3</code> with suitable arguments to summarize data by
major and minor categories.  If <code>options(grType='plotly')</code> is in
effect and the <code>plotly</code> package is installed, <code>summaryD</code> uses
<code>dotchartp</code> instead of <code>dotchart3</code>.
</p>
<p><code>summaryDp</code> is a streamlined <code>summaryD</code>-like function that
uses the <code>dotchartpl</code> function to render a <code>plotly</code> graphic.
It is used to compute summary statistics stratified separately by a
series of variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dotchart3(x, labels = NULL, groups = NULL, gdata = NULL,
          cex = par("cex"), pch = 21, gpch = pch, bg = par("bg"),
          color = par("fg"), gcolor = par("fg"), lcolor = "gray",
          xlim = range(c(x, gdata), na.rm=TRUE), main = NULL, xlab = NULL,
          ylab = NULL, auxdata = NULL, auxtitle = NULL, auxgdata=NULL,
          axisat=NULL, axislabels=NULL,
          cex.labels = cex, cex.group.labels = cex.labels * 1.25,
          cex.auxdata=cex, groupfont = 2,
          auxwhere=NULL, height=NULL, width=NULL, ...)

dotchartp(x, labels = NULL, groups = NULL, gdata = NULL,
            xlim = range(c(x, gdata), na.rm=TRUE), main=NULL,
            xlab = NULL, ylab = '', auxdata=NULL, auxtitle=NULL,
            auxgdata=NULL, auxwhere=c('right', 'hover'),
            symbol='circle', col=colorspace::rainbow_hcl,
            legendgroup=NULL,
            axisat=NULL, axislabels=NULL, sort=TRUE, digits=4, dec=NULL,
            height=NULL, width=700, layoutattr=FALSE, showlegend=TRUE, ...) 

summaryD(formula, data=NULL, fun=mean, funm=fun,
         groupsummary=TRUE, auxvar=NULL, auxtitle='',
         auxwhere=c('hover', 'right'),
         vals=length(auxvar) &gt; 0, fmtvals=format,
         symbol=if(use.plotly) 'circle' else 21,
         col=if(use.plotly) colorspace::rainbow_hcl else 1:10,
         legendgroup=NULL,
         cex.auxdata=.7, xlab=v[1], ylab=NULL,
         gridevery=NULL, gridcol=gray(.95), sort=TRUE, ...)

summaryDp(formula,
          fun=function(x) c(Mean=mean(x, na.rm=TRUE),
                            N=sum(! is.na(x))),
          overall=TRUE, xlim=NULL, xlab=NULL,
          data=NULL, subset=NULL, na.action=na.retain,
          ncharsmax=c(50, 30),
          digits=4, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dotchart3_+3A_x">x</code></td>
<td>
<p>a numeric vector or matrix</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_labels">labels</code></td>
<td>
<p>labels for categories corresponding to rows of
<code>x</code>.  If not specified these are taken from row names of <code>x</code>.</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_groups">groups</code>, <code id="dotchart3_+3A_gdata">gdata</code>, <code id="dotchart3_+3A_cex">cex</code>, <code id="dotchart3_+3A_pch">pch</code>, <code id="dotchart3_+3A_gpch">gpch</code>, <code id="dotchart3_+3A_bg">bg</code>, <code id="dotchart3_+3A_color">color</code>, <code id="dotchart3_+3A_gcolor">gcolor</code>, <code id="dotchart3_+3A_lcolor">lcolor</code>, <code id="dotchart3_+3A_xlim">xlim</code>, <code id="dotchart3_+3A_main">main</code>, <code id="dotchart3_+3A_xlab">xlab</code>, <code id="dotchart3_+3A_ylab">ylab</code></td>
<td>
<p>see <code><a href="graphics.html#topic+dotchart">dotchart</a></code></p>
</td></tr>
<tr><td><code id="dotchart3_+3A_auxdata">auxdata</code></td>
<td>
<p>a vector of information to be put in the right margin,
in the same order as <code>x</code>.  May be numeric, character, or a
vector of expressions containing <code><a href="grDevices.html#topic+plotmath">plotmath</a></code> markup.  For
<code>dotchartp</code>, <code>auxdata</code> may be a matrix to go along
with the numeric x-axis variable, to result in point-specific
hover text.</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_auxtitle">auxtitle</code></td>
<td>
<p>a column heading for <code>auxdata</code></p>
</td></tr>
<tr><td><code id="dotchart3_+3A_auxgdata">auxgdata</code></td>
<td>
<p>similar to <code>auxdata</code> but corresponding to the
<code>gdata</code> argument.  These usually represent overall sample sizes
for each group of lines.</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_axisat">axisat</code></td>
<td>
<p>a vector of tick mark locations to pass to <code>axis</code>.
Useful if transforming the data axis</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_axislabels">axislabels</code></td>
<td>
<p>a vector of strings specifying axis tick mark
labels.  Useful if transforming the data axis</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_digits">digits</code></td>
<td>
<p>number of significant digits for formatting numeric data
in hover text for <code>dotchartp</code> and <code>summaryDp</code></p>
</td></tr>
<tr><td><code id="dotchart3_+3A_dec">dec</code></td>
<td>
<p>for <code>dotchartp</code> only, overrides <code>digits</code> to
specify the argument to <code>round()</code> for rounding values for
hover labels</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_cex.labels">cex.labels</code></td>
<td>
<p><code>cex</code> for labels</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_cex.group.labels">cex.group.labels</code></td>
<td>
<p><code>cex</code> for group labels</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_cex.auxdata">cex.auxdata</code></td>
<td>
<p><code>cex</code> for <code>auxdata</code></p>
</td></tr>
<tr><td><code id="dotchart3_+3A_groupfont">groupfont</code></td>
<td>
<p>font number for group headings</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_auxwhere">auxwhere</code></td>
<td>
<p>for <code>summaryD</code> and <code>dotchartp</code> specifies
whether <code>auxdata</code> 
and <code>auxgdata</code> are to be placed on the far right of the
chart, or should appear as pop-up tooltips when hovering the
mouse over the ordinary <code>x</code> data points on the chart.
Ignored for <code>dotchart3</code>.</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_...">...</code></td>
<td>
<p>other arguments passed to some of the graphics functions,
or to <code>dotchart3</code> or <code>dotchartp</code> from <code>summaryD</code>.
The <code>auxwhere='hover'</code> option is a useful argument to pass
from <code>summaryD</code> to <code>dotchartp</code>.  Also used to pass
other arguments to <code>dotchartpl</code> from <code>summaryDp</code>.</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_layoutattr">layoutattr</code></td>
<td>
<p>set to <code>TRUE</code> to put <code>plotly::layout</code>
information in a list as an attribute <code>layout</code> of the
returned <code>plotly</code> object instead of running the
<code>plotly</code> object through the <code>layout</code> function.  This
is useful if running <code>dotchartp</code> multiple times to later
put together using <code>plotly::subplot</code> and only then running
the result through <code>plotly::layout</code>.</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_showlegend">showlegend</code></td>
<td>
<p>set to <code>FALSE</code> to suppress the <code>plotly</code>
legend with <code>dotchartp</code></p>
</td></tr>
<tr><td><code id="dotchart3_+3A_formula">formula</code></td>
<td>
<p>a formula with one variable on the left hand side (the
variable to compute summary statistics on), and one or two
variables on the right hand side.  If there are two variables,
the first is taken as the major grouping variable.  If the left
hand side variable is a matrix it has to be a legal R variable
name, not an expression, and <code>fun</code> needs to be able to
process a matrix.  For <code>summaryDp</code> there may be more than
two right-hand-side variables.</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_data">data</code></td>
<td>
<p>a data frame or list used to find the variables in
<code>formula</code>.  If omitted, the parent environment is used.</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_fun">fun</code></td>
<td>
<p>a summarization function creating a single number from a
vector.  Default is the mean.  For <code>summaryDp</code>, <code>fun</code>
produces a named vector of summary statistics, with the default
computing the <code>Mean</code> and <code>N</code> (number of non-missing values).</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_funm">funm</code></td>
<td>
<p>applies if there are two right hand variables and
<code>groupsummary=TRUE</code> and the marginal summaries over just
the first <code>x</code> variable need to be computed differently
than the summaries that are cross-classified by both
variables.  <code>funm</code> defaults to <code>fun</code> and should
have the same structure as <code>fun</code>.</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_groupsummary">groupsummary</code></td>
<td>
<p>By default, when there are two right-hand
variables, <code>summarize(..., fun)</code> is called a second time
without the use of the second variable, to obtain marginal
summaries for the major grouping variable and display the
results as a dot (and optionally in the right margin).  Set
<code>groupsummary=FALSE</code> to suppress this information.</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_auxvar">auxvar</code></td>
<td>
<p>when <code>fun</code> returns more than one statistic and the
user names the elements in the returned vector, you can specify
<code>auxvar</code> as a single character string naming one of them.
This will cause the named element to be written in the right
margin, and that element to be deleted when plotting the statistics.</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_vals">vals</code></td>
<td>
<p>set to <code>TRUE</code> to show data values (dot
locations) in the right margin.  Defaults to <code>TRUE</code> if
<code>auxvar</code> is specified.</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_fmtvals">fmtvals</code></td>
<td>
<p>an optional function to format values before putting
them in the right margin.  Default is the <code>format</code>
function.</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_symbol">symbol</code></td>
<td>
<p>a scalar or vector of <code>pch</code> values for ordinary
graphics or a character vector or scalar of <code>plotly</code>
symbols.   These correspond to columns of <code>x</code> or elements
produced by <code>fun</code>.</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_col">col</code></td>
<td>
<p>a function or vector of colors to assign to multiple points
plotted in one line. If a function it will be evaluated with an
argument equal to the number of groups/columns.</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_legendgroup">legendgroup</code></td>
<td>
<p>see <code>plotly</code> documentation; corresponds to
column names/<code>fun</code> output for <code>plotly</code> graphs only</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_gridevery">gridevery</code></td>
<td>
<p>specify a positive number to draw very faint vertical
grid lines every <code>gridevery</code> <code>x</code>-axis units; for
non-<code>plotly</code> charts</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_gridcol">gridcol</code></td>
<td>
<p>color for grid lines; default is very faint gray scale</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_sort">sort</code></td>
<td>
<p>specify <code>sort=FALSE</code> to plot data in the original
order, from top to bottom on the dot chart.  For
<code>dotchartp</code>, set <code>sort</code> to <code>'descending'</code> to
sort in descending order of the first column of <code>x</code>, or
<code>'ascending'</code> to do the reverse.  These do not make sense
if <code>groups</code> is present.</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_height">height</code>, <code id="dotchart3_+3A_width">width</code></td>
<td>
<p>height and width in pixels for <code>dotchartp</code> if
not using <code>plotly</code> defaults.  Ignored for
<code>dotchart3</code>.  If set to <code>"auto"</code> the height is
computed using <code>Hmisc::plotlyHeightDotchart</code>.</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_overall">overall</code></td>
<td>
<p>set to <code>FALSE</code> to suppress plotting of
unstratified estimates</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_subset">subset</code></td>
<td>
<p>an observation subsetting expression</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_na.action">na.action</code></td>
<td>
<p>an <code>NA</code> action function</p>
</td></tr>
<tr><td><code id="dotchart3_+3A_ncharsmax">ncharsmax</code></td>
<td>
<p>a 2-vector specifying the number of characters after
which an html new line character should be placed, respectively for
the x-axis label and the stratification variable levels</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the function returns invisibly</p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+dotchart">dotchart</a></code>,<code><a href="#topic+dotchart2">dotchart2</a></code>,<code><a href="#topic+summarize">summarize</a></code>,
<code><a href="#topic+rlegend">rlegend</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(135)
maj &lt;- factor(c(rep('North',13),rep('South',13)))
g &lt;- paste('Category',rep(letters[1:13],2))
n &lt;- sample(1:15000, 26, replace=TRUE)
y1 &lt;- runif(26)
y2 &lt;- pmax(0, y1 - runif(26, 0, .1))
dotchart3(cbind(y1,y2), g, groups=maj, auxdata=n, auxtitle='n',
          xlab='Y', pch=c(1,17))
## Compare with dotchart function (no superpositioning or auxdata allowed):
## dotchart(y1, g, groups=maj, xlab='Y')

## Not run: 
dotchartp(cbind(y1, y2), g, groups=maj, auxdata=n, auxtitle='n',
          xlab='Y', gdata=cbind(c(0,.1), c(.23,.44)), auxgdata=c(-1,-2),
          symbol=c('circle', 'line-ns-open'))

summaryDp(sbp ~ region + sex + race + cut2(age, g=5), data=mydata)

## End(Not run)

## Put options(grType='plotly') to have the following use dotchartp
## (rlegend will not apply)
## Add argument auxwhere='hover' to summaryD or dotchartp to put
## aux info in hover text instead of right margin
summaryD(y1 ~ maj + g, xlab='Mean')
summaryD(y1 ~ maj + g, groupsummary=FALSE)
summaryD(y1 ~ g, fmtvals=function(x) sprintf('%4.2f', x))
Y &lt;- cbind(y1, y2)   # summaryD cannot handle cbind(...) ~ ...
summaryD(Y  ~ maj + g, fun=function(y) y[1,], symbol=c(1,17))
rlegend(.1, 26, c('y1','y2'), pch=c(1,17))

summaryD(y1 ~ maj, fun=function(y) c(Mean=mean(y), n=length(y)),
         auxvar='n', auxtitle='N')
</code></pre>

<hr>
<h2 id='dotchartpl'>Enhanced Version of dotchart Function for plotly</h2><span id='topic+dotchartpl'></span>

<h3>Description</h3>

<p>This function produces a <code>plotly</code> interactive graphic and accepts
a different format of data input than the other <code>dotchart</code>
functions.  It was written to handle a hierarchical data structure
including strata that further subdivide the main classes.  Strata,
indicated by the <code>mult</code> variable, are shown  on the same
horizontal line, and if the variable <code>big</code> is <code>FALSE</code> will
appear slightly below the main line, using smaller symbols, and having
some transparency.  This is intended to handle output such as that
from the <code>summaryP</code> function when there is a superpositioning
variable <code>group</code> and a stratification variable <code>mult</code>,
especially when the data have been run through the <code>addMarginal</code>
function to create <code>mult</code> categories labelled <code>"All"</code> for
which the user will specify <code>big=TRUE</code> to indicate non-stratified
estimates (stratified only on <code>group</code>) to emphasize.
</p>
<p>When viewing graphics that used <code>mult</code> and <code>big</code>, the user
can click on the legends for the small points for <code>group</code>s to
vanish the finely stratified estimates.
</p>
<p>When <code>group</code> is used by <code>mult</code> and <code>big</code> are not, and
when the <code>group</code> variable has exactly two distinct values, you
can specify <code>refgroup</code> to get the difference between two
proportions in addition to the individual proportions.  The individual
proportions are plotted, but confidence intervals for the difference
are shown in hover text and half-width confidence intervals for the
difference, centered at the midpoint of the proportions, are shown.
These have the property of intersecting the two proportions if and
only if there is no significant difference at the <code>1 - conf.int</code>
level.
</p>
<p>Specify <code>fun=exp</code> and <code>ifun=log</code> if estimates and confidence
limits are on the log scale.  Make sure that zeros were prevented in
the original calculations.  For exponential hazard rates this can be
accomplished by replacing event counts of 0 with 0.5.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dotchartpl(x, major=NULL, minor=NULL, group=NULL, mult=NULL,
           big=NULL, htext=NULL, num=NULL, denom=NULL,
           numlabel='', denomlabel='',
           fun=function(x) x, ifun=function(x) x, op='-',
           lower=NULL, upper=NULL,
           refgroup=NULL, sortdiff=TRUE, conf.int=0.95,
           minkeep=NULL, xlim=NULL, xlab='Proportion',
           tracename=NULL, limitstracename='Limits',
           nonbigtracename='Stratified Estimates',
           dec=3, width=800, height=NULL,
           col=colorspace::rainbow_hcl)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dotchartpl_+3A_x">x</code></td>
<td>
<p>a numeric vector used for values on the <code>x</code>-axis</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_major">major</code></td>
<td>
<p>major vertical category, e.g., variable labels</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_minor">minor</code></td>
<td>
<p>minor vertical category, e.g. category levels within
variables</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_group">group</code></td>
<td>
<p>superpositioning variable such as treatment</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_mult">mult</code></td>
<td>
<p>strata names for further subdivisions without
<code>group</code>s</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_big">big</code></td>
<td>
<p>omit if all levels of <code>mult</code> are equally important or
if <code>mult</code> is omitted.  Otherwise denotes major (larger points,
right on horizontal lines) vs. minor (smaller, transparent points
slightly below the line).</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_htext">htext</code></td>
<td>
<p>additional hover text per point</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_num">num</code></td>
<td>
<p>if <code>x</code> represents proportions, optionally specifies
numerators to be used in fractions added to hover text.  When
<code>num</code> is given, <code>x</code> is automatically added to hover text,
rounded to 3 digits after the decimal point.</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_denom">denom</code></td>
<td>
<p>like <code>num</code> but for denominators</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_numlabel">numlabel</code></td>
<td>
<p>character string to put to the right of the numerator
in hover text</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_denomlabel">denomlabel</code></td>
<td>
<p>character string to put to the right of the
denominator in hover text</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_fun">fun</code></td>
<td>
<p>a transformation to make when printing estimates.  For
example, one may specify <code>fun=exp</code> to anti-log estimates and
confidence limites that were computed on a log basis</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_ifun">ifun</code></td>
<td>
<p>inverse transformation of <code>fun</code></p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_op">op</code></td>
<td>
<p>set to for example <code>'/'</code> when <code>fun=exp</code> and
effects are computed as ratios instead of differences.  This is used
in hover text.</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_lower">lower</code></td>
<td>
<p>lower limits for optional error bars</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_upper">upper</code></td>
<td>
<p>upper limits for optional error bars</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_refgroup">refgroup</code></td>
<td>
<p>if <code>group</code> is specified and there are exactly two
groups, specify the character string for the reference group in
computing difference in proportions.  For example if
<code>refgroup='A'</code> and the <code>group</code> levels are <code>'A','B'</code>,
you will get B - A.</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_sortdiff">sortdiff</code></td>
<td>
<p><code>minor</code> categories are sorted by descending
values of the difference in proportions when <code>refgroup</code> is used,
unless you specify <code>sortdiff=FALSE</code></p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_conf.int">conf.int</code></td>
<td>
<p>confidence level for computing confidence intervals
for the difference in two proportions.  Specify <code>conf.int=FALSE</code>
to suppress confidence intervals.</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_minkeep">minkeep</code></td>
<td>
<p>if <code>refgroup</code> and <code>minkeep</code> are both given,
observations that are at or above <code>minkeep</code> for at least one of
the groups are retained.  The defaults to to keep all observations.</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_xlim">xlim</code></td>
<td>
<p><code>x</code>-axis limits</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_xlab">xlab</code></td>
<td>
<p><code>x</code>-axis label</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_tracename">tracename</code></td>
<td>
<p><code>plotly</code> trace name if <code>group</code> is not used</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_limitstracename">limitstracename</code></td>
<td>
<p><code>plotly</code> trace name for <code>lower</code> and
<code>upper</code> if <code>group</code> is not used</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_nonbigtracename">nonbigtracename</code></td>
<td>
<p><code>plotly</code> trace name used for non-big
elements, which usually represent stratified versions of the &quot;big&quot;
observations</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_col">col</code></td>
<td>
<p>a function or vector of colors to assign to <code>group</code>.
If a function it will be evaluated with an argument equal to the
number of distinct groups.</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_dec">dec</code></td>
<td>
<p>number of places to the right of the decimal place for
formatting numeric quantities in hover text</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_width">width</code></td>
<td>
<p>width of plot in pixels</p>
</td></tr>
<tr><td><code id="dotchartpl_+3A_height">height</code></td>
<td>
<p>height of plot in pixels; computed from number of strata
by default</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>plotly</code> object.  An attribute <code>levelsRemoved</code> is
added if <code>minkeep</code> is used and any categories were omitted from
the plot as a result.  This is a character vector with categories
removed.  If <code>major</code> is present, the strings are of the form
<code>major:minor</code></p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="#topic+dotchartp">dotchartp</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(1)
d &lt;- expand.grid(major=c('Alabama', 'Alaska', 'Arkansas'),
                 minor=c('East', 'West'),
                 group=c('Female', 'Male'),
                 city=0:2)
n &lt;- nrow(d)
d$num   &lt;- round(100*runif(n))
d$denom &lt;- d$num + round(100*runif(n))
d$x     &lt;- d$num / d$denom
d$lower &lt;- d$x - runif(n)
d$upper &lt;- d$x + runif(n)

with(d,
 dotchartpl(x, major, minor, group, city, lower=lower, upper=upper,
            big=city==0, num=num, denom=denom, xlab='x'))

# Show half-width confidence intervals for Female - Male differences
# after subsetting the data to have only one record per
# state/region/group
d &lt;- subset(d, city == 0)
with(d,
 dotchartpl(x, major, minor, group, num=num, denom=denom,
            lower=lower, upper=upper, refgroup='Male')
)

n &lt;- 500
set.seed(1)
d &lt;- data.frame(
  race         = sample(c('Asian', 'Black/AA', 'White'), n, TRUE),
  sex          = sample(c('Female', 'Male'), n, TRUE),
  treat        = sample(c('A', 'B'), n, TRUE),
  smoking      = sample(c('Smoker', 'Non-smoker'), n, TRUE),
  hypertension = sample(c('Hypertensive', 'Non-Hypertensive'), n, TRUE),
  region       = sample(c('North America','Europe','South America',
                          'Europe', 'Asia', 'Central America'), n, TRUE))

d &lt;- upData(d, labels=c(race='Race', sex='Sex'))

dm &lt;- addMarginal(d, region)
s &lt;- summaryP(race + sex + smoking + hypertension ~
                region + treat,  data=dm)

s$region &lt;- ifelse(s$region == 'All', 'All Regions', as.character(s$region))

with(s, 
 dotchartpl(freq / denom, major=var, minor=val, group=treat, mult=region,
            big=region == 'All Regions', num=freq, denom=denom)
)

s2 &lt;- s[- attr(s, 'rows.to.exclude1'), ]
with(s2, 
     dotchartpl(freq / denom, major=var, minor=val, group=treat, mult=region,
                big=region == 'All Regions', num=freq, denom=denom)
)
# Note these plots can be created by plot.summaryP when options(grType='plotly')

# Plot hazard rates and ratios with confidence limits, on log scale
d &lt;- data.frame(tx=c('a', 'a', 'b', 'b'),
                event=c('MI', 'stroke', 'MI', 'stroke'),
                count=c(10, 5, 5, 2),
                exposure=c(1000, 1000, 900, 900))
# There were no zero event counts in this dataset.  In general we
# want to handle that, hence the 0.5 below
d &lt;- upData(d, hazard = pmax(0.5, count) / exposure,
               selog  = sqrt(1. / pmax(0.5, count)),
               lower  = log(hazard) - 1.96 * selog,
               upper  = log(hazard) + 1.96 * selog)
with(d,
     dotchartpl(log(hazard), minor=event, group=tx, num=count, denom=exposure,
                lower=lower, upper=upper,
                fun=exp, ifun=log, op='/',
                numlabel='events', denomlabel='years',
                refgroup='a', xlab='Events Per Person-Year')
)

## End(Not run)
</code></pre>

<hr>
<h2 id='ebpcomp'>ebpcomp</h2><span id='topic+ebpcomp'></span>

<h3>Description</h3>

<p>Computation of Coordinates of Extended Box Plots Elements
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ebpcomp(x, qref = c(0.5, 0.25, 0.75), probs = c(0.05, 0.125, 0.25, 0.375))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ebpcomp_+3A_x">x</code></td>
<td>
<p>a numeric variable</p>
</td></tr>
<tr><td><code id="ebpcomp_+3A_qref">qref</code></td>
<td>
<p>quantiles for major corners</p>
</td></tr>
<tr><td><code id="ebpcomp_+3A_probs">probs</code></td>
<td>
<p>quantiles for minor corners</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For an extended box plots computes all the elements needed for plotting it.  This is typically used when adding to a <code>ggplot2</code> plot.
</p>


<h3>Value</h3>

<p>list with elements <code>segments</code>, <code>lines</code>, <code>points</code>, <code>points2</code>
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ebpcomp(1:1000)
</code></pre>

<hr>
<h2 id='Ecdf'>Empirical Cumulative Distribution Plot</h2><span id='topic+Ecdf'></span><span id='topic+Ecdf.default'></span><span id='topic+Ecdf.data.frame'></span><span id='topic+Ecdf.formula'></span><span id='topic+panel.Ecdf'></span><span id='topic+prepanel.Ecdf'></span>

<h3>Description</h3>

<p>Computes coordinates of cumulative distribution function of x, and by defaults
plots it as a step function.  A grouping variable may be specified so that
stratified estimates are computed and (by default) plotted.  If there is
more than one group, the <code>labcurve</code> function is used (by default) to label
the multiple step functions or to draw a legend defining line types, colors,
or symbols by linking them with group labels.  A <code>weights</code> vector may
be specified to get weighted estimates.  Specify <code>normwt</code> to make
<code>weights</code> sum to the length of <code>x</code> (after removing NAs).  Other wise
the total sample size is taken to be the sum of the weights.
</p>
<p><code>Ecdf</code> is actually a method, and <code>Ecdf.default</code> is what's
called for a vector argument.  <code>Ecdf.data.frame</code> is called when the
first argument is a data frame.  This function can automatically set up
a matrix of ECDFs and wait for a mouse click if the matrix requires more
than one page.  Categorical variables, character variables, and
variables having fewer than a set number of unique values are ignored.
If <code>par(mfrow=..)</code> is not set up before <code>Ecdf.data.frame</code> is
called, the function will try to figure the best layout depending on the
number of variables in the data frame.  Upon return the original
<code>mfrow</code> is left intact.
</p>
<p>When the first argument to <code>Ecdf</code> is a formula, a Trellis/Lattice function
<code>Ecdf.formula</code> is called.  This allows for multi-panel
conditioning, superposition using a <code>groups</code> variable, and other
Trellis features, along with the ability to easily plot transformed
ECDFs using the <code>fun</code> argument.  For example, if <code>fun=qnorm</code>,
the inverse normal transformation will be used for the y-axis.  If the
transformed curves are linear this indicates normality.  Like the
<code>xYplot</code> function, <code>Ecdf</code> will create a function <code>Key</code> if
the <code>groups</code> variable is used.  This function can be invoked by the
user to define the keys for the groups.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Ecdf(x, ...)

## Default S3 method:
Ecdf(x, what=c('F','1-F','f','1-f'),
     weights=rep(1, length(x)), normwt=FALSE,
     xlab, ylab, q, pl=TRUE, add=FALSE, lty=1, 
     col=1, group=rep(1,length(x)), label.curves=TRUE, xlim, 
     subtitles=TRUE, datadensity=c('none','rug','hist','density'),
     side=1, 
     frac=switch(datadensity,none=NA,rug=.03,hist=.1,density=.1),
     dens.opts=NULL, lwd=1, log='', ...)


## S3 method for class 'data.frame'
Ecdf(x, group=rep(1,nrows),
     weights=rep(1, nrows), normwt=FALSE,
     label.curves=TRUE, n.unique=10, na.big=FALSE, subtitles=TRUE, 
     vnames=c('labels','names'),...)

## S3 method for class 'formula'
Ecdf(x, data=sys.frame(sys.parent()), groups=NULL,
     prepanel=prepanel.Ecdf, panel=panel.Ecdf, ..., xlab,
     ylab, fun=function(x)x, what=c('F','1-F','f','1-f'), subset=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Ecdf_+3A_x">x</code></td>
<td>
<p>a numeric vector, data frame, or Trellis/Lattice formula</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_what">what</code></td>
<td>

<p>The default is <code>"F"</code> which results in plotting the fraction of values
&lt;= x.  Set to <code>"1-F"</code> to plot the fraction &gt; x or <code>"f"</code> to plot the
cumulative frequency of values &lt;= x.  Use <code>"1-f"</code> to plot the
cumulative frequency of values &gt;= x.
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_weights">weights</code></td>
<td>

<p>numeric vector of weights.  Omit or specify a zero-length vector or
NULL to get unweighted estimates.
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_normwt">normwt</code></td>
<td>
<p>see above</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_xlab">xlab</code></td>
<td>

<p>x-axis label.  Default is label(x) or name of calling argument.  For
<code>Ecdf.formula</code>, <code>xlab</code> defaults to the <code>label</code> attribute
of the x-axis variable.
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_ylab">ylab</code></td>
<td>

<p>y-axis label.  Default is <code>"Proportion &lt;= x"</code>, <code>"Proportion &gt; x"</code>, 
or &quot;Frequency &lt;= x&quot; depending on value of <code>what</code>.
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_q">q</code></td>
<td>

<p>a vector for quantiles for which to draw reference lines on the plot.
Default is not to draw any.
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_pl">pl</code></td>
<td>
<p>set to F to omit the plot, to just return estimates</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_add">add</code></td>
<td>

<p>set to TRUE to add the cdf to an existing plot.  Does not apply if using
lattice graphics (i.e., if a formula is given as the first argument).
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_lty">lty</code></td>
<td>

<p>integer line type for plot.  If <code>group</code> is specified, this can be a vector.
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_lwd">lwd</code></td>
<td>

<p>line width for plot.  Can be a vector corresponding to <code>group</code>s.
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_log">log</code></td>
<td>

<p>see <code><a href="base.html#topic+plot">plot</a></code>.  Set <code>log='x'</code> to use log scale for
<code>x</code>-axis.
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_col">col</code></td>
<td>

<p>color for step function.  Can be a vector.
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_group">group</code></td>
<td>

<p>a numeric, character, or <code>factor</code> categorical variable used for stratifying
estimates.  If <code>group</code> is present, as many ECDFs are drawn as there are
non&ndash;missing group levels.
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_label.curves">label.curves</code></td>
<td>

<p>applies if more than one <code>group</code> exists.
Default is <code>TRUE</code> to use <code>labcurve</code> to label curves where they are farthest
apart.  Set <code>label.curves</code> to a <code>list</code> to specify options to
<code>labcurve</code>, e.g., <code>label.curves=list(method="arrow", cex=.8)</code>.
These option names may be abbreviated in the usual way arguments
are abbreviated.  Use for example <code>label.curves=list(keys=1:5)</code>
to draw symbols periodically (as in <code>pch=1:5</code> - see <code>points</code>)
on the curves and automatically position a legend
in the most empty part of the plot.  Set <code>label.curves=FALSE</code> to
suppress drawing curve labels.  The <code>col</code>, <code>lty</code>, and <code>type</code>
parameters are automatically passed to <code>labcurve</code>, although you
can override them here.  You can set <code>label.curves=list(keys="lines")</code> to
have different line types defined in an automatically positioned key.
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_xlim">xlim</code></td>
<td>

<p>x-axis limits.  Default is entire range of <code>x</code>.
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_subtitles">subtitles</code></td>
<td>

<p>set to <code>FALSE</code> to suppress putting a subtitle at the bottom left of each
plot.  The subtitle indicates the numbers of
non-missing and missing observations, which are labeled <code>n</code>, <code>m</code>.
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_datadensity">datadensity</code></td>
<td>

<p>If <code>datadensity</code> is not <code>"none"</code>, either <code>scat1d</code> or <code>histSpike</code> is called to
add a rug plot (<code>datadensity="rug"</code>), spike histogram
(<code>datadensity="hist"</code>), or smooth density estimate (<code>"density"</code>) to
the bottom or top of the ECDF.
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_side">side</code></td>
<td>

<p>If <code>datadensity</code> is not <code>"none"</code>, the default is to place the additional
information on top of the x-axis (<code>side=1</code>).  Use <code>side=3</code> to place at
the top of the graph.
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_frac">frac</code></td>
<td>

<p>passed to <code>histSpike</code>
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_dens.opts">dens.opts</code></td>
<td>

<p>a list of optional arguments for <code>histSpike</code>
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_...">...</code></td>
<td>

<p>other parameters passed to plot if add=F.  For data frames, other
parameters to pass to <code>Ecdf.default</code>.
For <code>Ecdf.formula</code>, if <code>groups</code> is not used, you can also add
data density information to each panel's ECDF by specifying the
<code>datadensity</code> and optional <code>frac</code>, <code>side</code>,
<code>dens.opts</code> arguments. 
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_n.unique">n.unique</code></td>
<td>

<p>minimum number of unique values before an ECDF is drawn for a variable
in a data frame.  Default is 10.
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_na.big">na.big</code></td>
<td>

<p>set to <code>TRUE</code> to draw the number of NAs in larger letters in the middle of
the plot for <code>Ecdf.data.frame</code>
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_vnames">vnames</code></td>
<td>

<p>By default, variable labels are used to label x-axes.  Set <code>vnames="names"</code>
to instead use variable names.
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_method">method</code></td>
<td>

<p>method for computing the empirical cumulative distribution.  See
<code>wtd.Ecdf</code>.  The default is to use the standard <code>"i/n"</code> method as is
used by the non-Trellis versions of <code>Ecdf</code>.
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_fun">fun</code></td>
<td>

<p>a function to transform the cumulative proportions, for the
Trellis-type usage of <code>Ecdf</code>
</p>
</td></tr>
<tr><td><code id="Ecdf_+3A_data">data</code>, <code id="Ecdf_+3A_groups">groups</code>, <code id="Ecdf_+3A_subset">subset</code>, <code id="Ecdf_+3A_prepanel">prepanel</code>, <code id="Ecdf_+3A_panel">panel</code></td>
<td>
<p>the usual Trellis/Lattice parameters, with <code>groups</code>
causing <code>Ecdf.formula</code> to overlay multiple ECDFs on one panel.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>for <code>Ecdf.default</code> an invisible list with elements x and y giving the
coordinates of the cdf.  If there is more than one <code>group</code>, a list of
such lists is returned.  An attribute, <code>N</code>, is in the returned
object.  It contains the elements <code>n</code> and <code>m</code>, the number of
non-missing and missing observations, respectively.
</p>


<h3>Side Effects</h3>

<p>plots
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics, Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+wtd.Ecdf">wtd.Ecdf</a></code>, <code><a href="#topic+label">label</a></code>, <code><a href="base.html#topic+table">table</a></code>, <code><a href="base.html#topic+cumsum">cumsum</a></code>, <code><a href="#topic+labcurve">labcurve</a></code>, <code><a href="#topic+xYplot">xYplot</a></code>, <code><a href="#topic+histSpike">histSpike</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
ch &lt;- rnorm(1000, 200, 40)
Ecdf(ch, xlab="Serum Cholesterol")
scat1d(ch)                       # add rug plot
histSpike(ch, add=TRUE, frac=.15)   # add spike histogram
# Better: add a data density display automatically:
Ecdf(ch, datadensity='density')


label(ch) &lt;- "Serum Cholesterol"
Ecdf(ch)
other.ch &lt;- rnorm(500, 220, 20)
Ecdf(other.ch,add=TRUE,lty=2)


sex &lt;- factor(sample(c('female','male'), 1000, TRUE))
Ecdf(ch, q=c(.25,.5,.75))  # show quartiles
Ecdf(ch, group=sex,
     label.curves=list(method='arrow'))


# Example showing how to draw multiple ECDFs from paired data
pre.test &lt;- rnorm(100,50,10)
post.test &lt;- rnorm(100,55,10)
x &lt;- c(pre.test, post.test)
g &lt;- c(rep('Pre',length(pre.test)),rep('Post',length(post.test)))
Ecdf(x, group=g, xlab='Test Results', label.curves=list(keys=1:2))
# keys=1:2 causes symbols to be drawn periodically on top of curves


# Draw a matrix of ECDFs for a data frame
m &lt;- data.frame(pre.test, post.test, 
                sex=sample(c('male','female'),100,TRUE))
Ecdf(m, group=m$sex, datadensity='rug')


freqs &lt;- sample(1:10, 1000, TRUE)
Ecdf(ch, weights=freqs)  # weighted estimates


# Trellis/Lattice examples:


region &lt;- factor(sample(c('Europe','USA','Australia'),100,TRUE))
year &lt;- factor(sample(2001:2002,1000,TRUE))
Ecdf(~ch | region*year, groups=sex)
Key()           # draw a key for sex at the default location
# Key(locator(1)) # user-specified positioning of key
age &lt;- rnorm(1000, 50, 10)
Ecdf(~ch | lattice::equal.count(age), groups=sex)  # use overlapping shingles
Ecdf(~ch | sex, datadensity='hist', side=3)  # add spike histogram at top
</code></pre>

<hr>
<h2 id='ecdfSteps'>ecdfSteps</h2><span id='topic+ecdfSteps'></span>

<h3>Description</h3>

<p>Compute Coordinates of an Empirical Distribution Function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ecdfSteps(x, extend)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ecdfSteps_+3A_x">x</code></td>
<td>
<p>numeric vector, possibly with <code>NA</code>s that are ignored</p>
</td></tr>
<tr><td><code id="ecdfSteps_+3A_extend">extend</code></td>
<td>
<p>a 2-vector do extend the range of x (low, high).  Set <code>extend=FALSE</code> to not extend <code>x</code>, or leave it missing to extend it 1/20th of the observed range on other side.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a numeric vector uses the R built-in <code>ecdf</code> function to compute
coordinates of the ECDF, with extension slightly below and above the
range of <code>x</code> by default.  This is useful for <code>ggplot2</code> where the ECDF may need to be transformed.  The returned object is suitable for creating stratified statistics using <code>data.table</code> and other methods.
</p>


<h3>Value</h3>

<p>a list with components <code>x</code> and <code>y</code>
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+ecdf">stats::ecdf()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ecdfSteps(0:10)
## Not run: 
# Use data.table for obtaining ECDFs by country and region
w &lt;- d[, ecdfSteps(z, extend=c(1,11)), by=.(country, region)]  # d is a DT
# Use ggplot2 to make one graph with multiple regions' ECDFs
# and use faceting for countries
ggplot(w, aes(x, y, color=region)) + geom_step() +
       facet_wrap(~ country)

## End(Not run)
</code></pre>

<hr>
<h2 id='equalBins'>Multicolumn Formating</h2><span id='topic+equalBins'></span>

<h3>Description</h3>

<p>Expands the width either supercolumns or the subcolumns so that the
the sum of the supercolumn widths is the same as the sum of the
subcolumn widths.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>equalBins(widths, subwidths)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="equalBins_+3A_widths">widths</code></td>
<td>
<p>widths of the supercolumns.</p>
</td></tr>
<tr><td><code id="equalBins_+3A_subwidths">subwidths</code></td>
<td>
<p>list of widths of the subcolumns for each supercolumn.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This determins the correct subwidths of each of various columns in a table
for printing.  The correct width of the multicolumns is deterimed by
summing the widths of it subcolumns.
</p>


<h3>Value</h3>

<p>widths of the the columns for a table.
</p>


<h3>Author(s)</h3>

<p>Charles Dupont</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+nchar">nchar</a></code>, <code><a href="#topic+stringDims">stringDims</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>mcols &lt;- c("Group 1", "Group 2")
mwidth &lt;- nchar(mcols, type="width")
spancols &lt;- c(3,3)
ccols &lt;- c("a", "deer", "ad", "cat", "help", "bob")
cwidth &lt;- nchar(ccols, type="width")

subwidths &lt;- partition.vector(cwidth, spancols)

equalBins(mwidth, subwidths)
</code></pre>

<hr>
<h2 id='errbar'>Plot Error Bars</h2><span id='topic+errbar'></span>

<h3>Description</h3>

<p>Add vertical error bars to an existing plot or makes a new
plot with error bars.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>errbar(x, y, yplus, yminus, cap=0.015, main = NULL,
       sub=NULL, xlab=as.character(substitute(x)),
       ylab=if(is.factor(x) || is.character(x)) ""
           else as.character(substitute(y)),
       add=FALSE, lty=1, type='p', ylim=NULL,
       lwd=1, pch=16, errbar.col, Type=rep(1, length(y)), 
       ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="errbar_+3A_x">x</code></td>
<td>

<p>vector of numeric x-axis values (for vertical error bars) or a factor or
character variable (for horizontal error bars, <code>x</code> representing the
group labels)
</p>
</td></tr>
<tr><td><code id="errbar_+3A_y">y</code></td>
<td>

<p>vector of y-axis values.
</p>
</td></tr>
<tr><td><code id="errbar_+3A_yplus">yplus</code></td>
<td>

<p>vector of y-axis values: the tops of the error bars.
</p>
</td></tr>
<tr><td><code id="errbar_+3A_yminus">yminus</code></td>
<td>

<p>vector of y-axis values: the bottoms of the error bars.
</p>
</td></tr>
<tr><td><code id="errbar_+3A_cap">cap</code></td>
<td>

<p>the width of the little lines at the tops and bottoms of the error bars
in units of the width of the plot.  Defaults to <code>0.015</code>.
</p>
</td></tr>
<tr><td><code id="errbar_+3A_main">main</code></td>
<td>

<p>a main title for the plot, passed to <code>plot</code>, see also <code><a href="graphics.html#topic+title">title</a></code>.
</p>
</td></tr>
<tr><td><code id="errbar_+3A_sub">sub</code></td>
<td>

<p>a sub title for the plot, passed to <code>plot</code>
</p>
</td></tr>
<tr><td><code id="errbar_+3A_xlab">xlab</code></td>
<td>

<p>optional x-axis labels if <code>add=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="errbar_+3A_ylab">ylab</code></td>
<td>

<p>optional y-axis labels if <code>add=FALSE</code>.  Defaults to blank for horizontal charts.
</p>
</td></tr>
<tr><td><code id="errbar_+3A_add">add</code></td>
<td>

<p>set to <code>TRUE</code> to add bars to an existing plot (available only for vertical
error bars)
</p>
</td></tr>
<tr><td><code id="errbar_+3A_lty">lty</code></td>
<td>

<p>type of line for error bars
</p>
</td></tr>
<tr><td><code id="errbar_+3A_type">type</code></td>
<td>

<p>type of point.  Use <code>type="b"</code> to connect dots.
</p>
</td></tr>
<tr><td><code id="errbar_+3A_ylim">ylim</code></td>
<td>

<p>y-axis limits.  Default is to use range of <code>y</code>, <code>yminus</code>, and <code>yplus</code>.  For
horizonal charts, <code>ylim</code> is really the <code>x</code>-axis range, excluding
differences.
</p>
</td></tr>
<tr><td><code id="errbar_+3A_lwd">lwd</code></td>
<td>

<p>line width for line segments (not main line)
</p>
</td></tr>
<tr><td><code id="errbar_+3A_pch">pch</code></td>
<td>

<p>character to use as the point.
</p>
</td></tr>
<tr><td><code id="errbar_+3A_errbar.col">errbar.col</code></td>
<td>

<p>color to use for drawing error bars.
</p>
</td></tr>
<tr><td><code id="errbar_+3A_type">Type</code></td>
<td>

<p>used for horizontal bars only.  Is an integer vector with values <code>1</code>
if corresponding values represent simple estimates, <code>2</code> if they
represent differences.
</p>
</td></tr>
<tr><td><code id="errbar_+3A_...">...</code></td>
<td>

<p>other parameters passed to all graphics functions.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>errbar</code> adds vertical error bars to an existing plot or makes a new
plot with error bars.  It can also make a horizontal error bar plot
that shows error bars for group differences as well as bars for
groups.  For the latter type of plot, the lower x-axis scale
corresponds to group estimates and the upper scale corresponds to
differences.  The spacings of the two scales are identical but the
scale for differences has its origin shifted so that zero may be
included.  If at least one of the confidence intervals includes zero,
a vertical dotted reference line at zero is drawn.
</p>


<h3>Author(s)</h3>

<p>Charles Geyer, University of Chicago.  Modified by Frank Harrell,
Vanderbilt University, to handle missing data, to add the parameters
<code>add</code> and <code>lty</code>, and to implement horizontal charts with differences.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x &lt;- 1:10
y &lt;- x + rnorm(10)
delta &lt;- runif(10)
errbar( x, y, y + delta, y - delta )


# Show bootstrap nonparametric CLs for 3 group means and for
# pairwise differences on same graph
group &lt;- sample(c('a','b','d'), 200, TRUE)
y     &lt;- runif(200) + .25*(group=='b') + .5*(group=='d')
cla &lt;- smean.cl.boot(y[group=='a'],B=100,reps=TRUE)  # usually B=1000
a   &lt;- attr(cla,'reps')
clb &lt;- smean.cl.boot(y[group=='b'],B=100,reps=TRUE)
b   &lt;- attr(clb,'reps')
cld &lt;- smean.cl.boot(y[group=='d'],B=100,reps=TRUE)
d   &lt;- attr(cld,'reps')
a.b &lt;- quantile(a-b,c(.025,.975))
a.d &lt;- quantile(a-d,c(.025,.975))
b.d &lt;- quantile(b-d,c(.025,.975))
errbar(c('a','b','d','a - b','a - d','b - d'),
       c(cla[1],clb[1],cld[1],cla[1]-clb[1],cla[1]-cld[1],clb[1]-cld[1]),
       c(cla[3],clb[3],cld[3],a.b[2],a.d[2],b.d[2]),
       c(cla[2],clb[2],cld[2],a.b[1],a.d[1],b.d[1]),
       Type=c(1,1,1,2,2,2), xlab='', ylab='')
       
</code></pre>

<hr>
<h2 id='escapeRegex'> Escapes any characters that would have special meaning in a reqular expression. </h2><span id='topic+escapeRegex'></span><span id='topic+escapeBS'></span>

<h3>Description</h3>

<p>Escapes any characters that would have special meaning in a reqular expression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>escapeRegex(string)
escapeBS(string)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="escapeRegex_+3A_string">string</code></td>
<td>
<p> string being operated on. </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>escapeRegex</code> will escape any characters that would have
special meaning in a reqular expression. For any string
<code>grep(regexpEscape(string), string)</code> will always be true.
</p>
<p><code>escapeBS</code> will escape any backslash &lsquo;<span class="samp">&#8288;\&#8288;</span>&rsquo; in a string.
</p>


<h3>Value</h3>

<p>The value of the string with any characters that would have
special meaning in a reqular expression escaped.
</p>


<h3>Author(s)</h3>

<p>Charles Dupont<br />
Department of Biostatistics<br />
Vanderbilt University
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+grep">grep</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>string &lt;- "this\\(system) {is} [full]."
escapeRegex(string)

escapeBS(string)


</code></pre>

<hr>
<h2 id='estSeqMarkovOrd'>estSeqMarkovOrd</h2><span id='topic+estSeqMarkovOrd'></span>

<h3>Description</h3>

<p>Simulate Comparisons For Use in Sequential Markov Longitudinal Clinical Trial Simulations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estSeqMarkovOrd(
  y,
  times,
  initial,
  absorb = NULL,
  intercepts,
  parameter,
  looks,
  g,
  formula,
  ppo = NULL,
  yprevfactor = TRUE,
  groupContrast = NULL,
  cscov = FALSE,
  timecriterion = NULL,
  coxzph = FALSE,
  sstat = NULL,
  rdsample = NULL,
  maxest = NULL,
  maxvest = NULL,
  nsim = 1,
  progress = FALSE,
  pfile = ""
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estSeqMarkovOrd_+3A_y">y</code></td>
<td>
<p>vector of possible y values in order (numeric, character, factor)</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_times">times</code></td>
<td>
<p>vector of measurement times</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_initial">initial</code></td>
<td>
<p>a vector of probabilities summing to 1.0 that specifies the frequency distribution of initial values to be sampled from.  The vector must have names that correspond to values of <code>y</code> representing non-absorbing states.</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_absorb">absorb</code></td>
<td>
<p>vector of absorbing states, a subset of <code>y</code>.  The default is no absorbing states.  Observations are truncated when an absorbing state is simulated.  May be numeric, character, or factor.</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_intercepts">intercepts</code></td>
<td>
<p>vector of intercepts in the proportional odds model.  There must be one fewer of these than the length of <code>y</code>.</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_parameter">parameter</code></td>
<td>
<p>vector of true parameter (effects; group differences) values.  These are group 2:1 log odds ratios in the transition model, conditioning on the previous <code>y</code>.</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_looks">looks</code></td>
<td>
<p>integer vector of ID numbers at which maximum likelihood estimates and their estimated variances are computed.  For a single look specify a scalar value for <code>loops</code> equal to the number of subjects in the sample.</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_g">g</code></td>
<td>
<p>a user-specified function of three or more arguments which in order are <code>yprev</code> - the value of <code>y</code> at the previous time, the current time <code>t</code>, the <code>gap</code> between the previous time and the current time, an optional (usually named) covariate vector <code>X</code>, and optional arguments such as a regression coefficient value to simulate from.  The function needs to allow <code>yprev</code> to be a vector and <code>yprev</code> must not include any absorbing states.  The <code>g</code> function returns the linear predictor for the proportional odds model aside from <code>intercepts</code>.  The returned value must be a matrix with row names taken from <code>yprev</code>.  If the model is a proportional odds model, the returned value must be one column.  If it is a partial proportional odds model, the value must have one column for each distinct value of the response variable Y after the first one, with the levels of Y used as optional column names.  So columns correspond to <code>intercepts</code>. The different columns are used for <code>y</code>-specific contributions to the linear predictor (aside from <code>intercepts</code>) for a partial or constrained partial proportional odds model.  Parameters for partial proportional odds effects may be included in the ... arguments.</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_formula">formula</code></td>
<td>
<p>a formula object given to the <code>lrm()</code> function using variables with these name: <code>y</code>, <code>time</code>, <code>yprev</code>, and <code>group</code> (factor variable having values '1' and '2').  The <code>yprev</code> variable is converted to a factor before fitting the model unless <code>yprevfactor=FALSE</code>.</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_ppo">ppo</code></td>
<td>
<p>a formula specifying the part of <code>formula</code> for which proportional odds is not to be assumed, i.e., that specifies a partial proportional odds model.  Specifying <code>ppo</code> triggers the use of <code>VGAM::vglm()</code> instead of <code>rms::lrm</code> and will make the simulations run slower.</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_yprevfactor">yprevfactor</code></td>
<td>
<p>see <code>formula</code></p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_groupcontrast">groupContrast</code></td>
<td>
<p>omit this argument if <code>group</code> has only one regression coefficient in <code>formula</code>.  Otherwise if <code>ppo</code> is omitted, provide <code>groupContrast</code> as a list of two lists that are passed to <code>rms::contrast.rms()</code> to compute the contrast of interest and its standard error.  The first list corresponds to group 1, the second to group 2, to get a 2:1 contrast.  If <code>ppo</code> is given and the group effect is not just a simple regression coefficient, specify as <code>groupContrast</code> a function of a <code>vglm</code> fit that computes the contrast of interest and its standard error and returns a list with elements named <code>Contrast</code> and <code>SE</code>.  For the latter type you can optionally have formal arguments <code>n1</code>, <code>n2</code>, and <code>parameter</code> that are passed to <code>groupContrast</code> to compute the standard error of the group contrast, where <code>n1</code> and <code>n2</code> respectively are the sample sizes for the two groups and <code>parameter</code> is the true group effect parameter value.</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_cscov">cscov</code></td>
<td>
<p>applies if <code>ppo</code> is not used.  Set to <code>TRUE</code> to use the cluster sandwich covariance estimator of the variance of the group comparison.</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_timecriterion">timecriterion</code></td>
<td>
<p>a function of a time-ordered vector of simulated ordinal responses <code>y</code> that returns a vector <code>FALSE</code> or <code>TRUE</code> values denoting whether the current <code>y</code> level met the condition of interest.  For example <code>estSeqMarkovOrd</code> will compute the first time at which <code>y &gt;= 5</code> if you specify <code>timecriterion=function(y) y &gt;= 5</code>.  This function is only called at the last data look for each simulated study.  To have more control, instead of <code>timecriterion</code> returning a logical vector have it return a numeric 2-vector containing, in order, the event/censoring time and the 1/0 event/censoring indicator.</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_coxzph">coxzph</code></td>
<td>
<p>set to <code>TRUE</code> if <code>timecriterion</code> is specified and you want to compute a statistic for testing proportional hazards at the last look of each simulated data</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_sstat">sstat</code></td>
<td>
<p>set to a function of the time vector and the corresponding vector of ordinal responses for a single group if you want to compute a Wilcoxon test on a derived quantity such as the number of days in a given state.</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_rdsample">rdsample</code></td>
<td>
<p>an optional function to do response-dependent sampling.  It is a function of these arguments, which are vectors that stop at any absorbing state: <code>times</code> (ascending measurement times for one subject), <code>y</code> (vector of ordinal outcomes at these times for one subject.  The function returns <code>NULL</code> if no observations are to be dropped, returns the vector of new times to sample.</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_maxest">maxest</code></td>
<td>
<p>maximum acceptable absolute value of the contrast estimate, ignored if <code>NULL</code>.  Any values exceeding <code>maxest</code> will result in the estimate being set to <code>NA</code>.</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_maxvest">maxvest</code></td>
<td>
<p>like <code>maxest</code> but for the estimated variance of the contrast estimate</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_nsim">nsim</code></td>
<td>
<p>number of simulations (default is 1)</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_progress">progress</code></td>
<td>
<p>set to <code>TRUE</code> to send current iteration number to <code>pfile</code> every 10 iterations.  Each iteration will really involve multiple simulations, if <code>parameter</code> has length greater than 1.</p>
</td></tr>
<tr><td><code id="estSeqMarkovOrd_+3A_pfile">pfile</code></td>
<td>
<p>file to which to write progress information.  Defaults to <code>''</code> which is the console.  Ignored if <code>progress=FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Simulates sequential clinical trials of longitudinal ordinal outcomes using a first-order Markov model.  Looks are done sequentially after subject ID numbers given in the vector <code>looks</code> with the earliest possible look being after subject 2.  At each look, a subject's repeated records are either all used or all ignored depending on the sequent ID number.  For each true effect parameter value, simulation, and at each look, runs a function to compute the estimate of the parameter of interest along with its variance.  For each simulation, data are first simulated for the last look, and these data are sequentially revealed for earlier looks.  The user provides a function <code>g</code> that has extra arguments specifying the true effect of <code>parameter</code> the treatment <code>group</code> expecting treatments to be coded 1 and 2.  <code>parameter</code> is usually on the scale of a regression coefficient, e.g., a log odds ratio.  Fitting is done using the <code>rms::lrm()</code> function, unless non-proportional odds is allowed in which case <code>VGAM::vglm()</code> is used.  If <code>timecriterion</code> is specified, the function also, for the last data look only, computes the first time at which the criterion is satisfied for the subject or use the event time and event/censoring indicator computed by <code>timecriterion</code>.  The Cox/logrank chi-square statistic for comparing groups on the derived time variable is saved.  If <code>coxzph=TRUE</code>, the <code>survival</code> package correlation coefficient <code>rho</code> from the scaled partial residuals is also saved so that the user can later determine to what extent the Markov model resulted in the proportional hazards assumption being violated when analyzing on the time scale.  <code>vglm</code> is accelerated by saving the first successful fit for the largest sample size and using its coefficients as starting value for further <code>vglm</code> fits for any sample size for the same setting of <code>parameter</code>.
</p>


<h3>Value</h3>

<p>a data frame with number of rows equal to the product of <code>nsim</code>, the length of <code>looks</code>, and the length of <code>parameter</code>, with variables <code>sim</code>, <code>parameter</code>, <code>look</code>, <code>est</code> (log odds ratio for group), and <code>vest</code> (the variance of the latter).  If <code>timecriterion</code> is specified the data frame also contains <code>loghr</code> (Cox log hazard ratio for group), <code>lrchisq</code> (chi-square from Cox test for group), and if <code>coxph=TRUE</code>, <code>phchisq</code>, the chi-square for testing proportional hazards.  The attribute <code>etimefreq</code> is also present if <code>timecriterion</code> is present, and it probvides the frequency distribution of derived event times by group and censoring/event indicator.  If <code>sstat</code> is given, the attribute <code>sstat</code> is also present, and it contains an array with dimensions corresponding to simulations, parameter values within simulations, <code>id</code>, and a two-column subarray with columns <code>group</code> and <code>y</code>, the latter being the summary measure computed by the <code>sstat</code> function.  The returned data frame also has attribute <code>lrmcoef</code> which are the last-look logistic regression coefficient estimates over the <code>nsim</code> simulations and the parameter settings, and an attribute <code>failures</code> which is a data frame containing the variables <code>reason</code> and <code>frequency</code> cataloging the reasons for unsuccessful model fits.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><code>gbayesSeqSim()</code>, <code>simMarkovOrd()</code>, <a href="https://hbiostat.org/R/Hmisc/markov/">https://hbiostat.org/R/Hmisc/markov/</a>
</p>

<hr>
<h2 id='estSeqSim'>estSeqSim</h2><span id='topic+estSeqSim'></span>

<h3>Description</h3>

<p>Simulate Comparisons For Use in Sequential Clinical Trial Simulations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estSeqSim(parameter, looks, gendat, fitter, nsim = 1, progress = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estSeqSim_+3A_parameter">parameter</code></td>
<td>
<p>vector of true parameter (effects; group differences) values</p>
</td></tr>
<tr><td><code id="estSeqSim_+3A_looks">looks</code></td>
<td>
<p>integer vector of observation numbers at which posterior probabilities are computed</p>
</td></tr>
<tr><td><code id="estSeqSim_+3A_gendat">gendat</code></td>
<td>
<p>a function of three arguments: true parameter value (scalar), sample size for first group, sample size for second group</p>
</td></tr>
<tr><td><code id="estSeqSim_+3A_fitter">fitter</code></td>
<td>
<p>a function of two arguments: 0/1 group indicator vector and the dependent variable vector</p>
</td></tr>
<tr><td><code id="estSeqSim_+3A_nsim">nsim</code></td>
<td>
<p>number of simulations (default is 1)</p>
</td></tr>
<tr><td><code id="estSeqSim_+3A_progress">progress</code></td>
<td>
<p>set to <code>TRUE</code> to send current iteration number to the console</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Simulates sequential clinical trials.  Looks are done sequentially at observation numbers given in the vector <code>looks</code> with the earliest possible look being at observation 2.  For each true effect parameter value, simulation, and at each look, runs a function to compute the estimate of the parameter of interest along with its variance.  For each simulation, data are first simulated for the last look, and these data are sequentially revealed for earlier looks.  The user provides a function <code>gendat</code> that given a true effect of <code>parameter</code> and the two sample sizes (for treatment groups 1 and 2) returns a list with vectors <code>y1</code> and <code>y2</code> containing simulated data.  The user also provides a function <code>fitter</code> with arguments <code>x</code> (group indicator 0/1) and <code>y</code> (response variable) that returns a 2-vector containing the effect estimate and its variance.  <code>parameter</code> is usually on the scale of a regression coefficient, e.g., a log odds ratio.
</p>


<h3>Value</h3>

<p>a data frame with number of rows equal to the product of <code>nsim</code>, the length of <code>looks</code>, and the length of <code>parameter</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><code>gbayesSeqSim()</code>, <code>simMarkovOrd()</code>, <code>estSeqMarkovOrd()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (requireNamespace("rms", quietly = TRUE)) {
  # Run 100 simulations, 5 looks, 2 true parameter values
  # Total simulation time: 2s
  lfit &lt;- function(x, y) {
  f &lt;- rms::lrm.fit(x, y)
    k &lt;- length(coef(f))
    c(coef(f)[k], vcov(f)[k, k])
  }
  gdat &lt;- function(beta, n1, n2) {
    # Cell probabilities for a 7-category ordinal outcome for the control group
    p &lt;- c(2, 1, 2, 7, 8, 38, 42) / 100

    # Compute cell probabilities for the treated group
    p2 &lt;- pomodm(p=p, odds.ratio=exp(beta))
    y1 &lt;- sample(1 : 7, n1, p,  replace=TRUE)
    y2 &lt;- sample(1 : 7, n2, p2, replace=TRUE)
    list(y1=y1, y2=y2)
  }

  set.seed(1)
  est &lt;- estSeqSim(c(0, log(0.7)), looks=c(50, 75, 95, 100, 200),
                    gendat=gdat,
                    fitter=lfit, nsim=100)
  head(est)
}
</code></pre>

<hr>
<h2 id='event.chart'>
Flexible Event Chart for Time-to-Event Data
</h2><span id='topic+event.chart'></span>

<h3>Description</h3>

<p>Creates an event chart on the current graphics device.  Also, allows user
to plot legend on plot area or on separate page.
Contains features useful for plotting data with time-to-event outcomes
Which arise in a variety of studies
including randomized clinical trials and non-randomized cohort studies.
This function can use as input a matrix or a data frame, although greater
utility and ease of use will be seen with a data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>event.chart(data, subset.r = 1:dim(data)[1], subset.c = 1:dim(data)[2],

           sort.by = NA, sort.ascending = TRUE,
           sort.na.last = TRUE, sort.after.subset = TRUE,
           y.var = NA, y.var.type = "n",
           y.jitter = FALSE, y.jitter.factor = 1,
           y.renum = FALSE, NA.rm = FALSE, x.reference = NA,
           now = max(data[, subset.c], na.rm = TRUE),
           now.line = FALSE, now.line.lty = 2,
           now.line.lwd = 1, now.line.col = 1, pty = "m",
           date.orig = c(1, 1, 1960), titl = "Event Chart",

           y.idlabels = NA, y.axis = "auto",
           y.axis.custom.at = NA, y.axis.custom.labels = NA,
           y.julian = FALSE, y.lim.extend = c(0, 0),
           y.lab = ifelse(is.na(y.idlabels), "", as.character(y.idlabels)),

           x.axis.all = TRUE, x.axis = "auto",
           x.axis.custom.at = NA, x.axis.custom.labels = NA,
           x.julian = FALSE, x.lim.extend = c(0, 0), x.scale = 1,
           x.lab = ifelse(x.julian, "Follow-up Time", "Study Date"),

           line.by = NA, line.lty = 1, line.lwd = 1, line.col = 1,
           line.add = NA, line.add.lty = NA,
           line.add.lwd = NA, line.add.col = NA,
           point.pch = 1:length(subset.c),
           point.cex = rep(0.6, length(subset.c)),
           point.col = rep(1, length(subset.c)),

           point.cex.mult = 1., point.cex.mult.var = NA,
           extra.points.no.mult = rep(NA, length(subset.c)),

           legend.plot = FALSE, legend.location = "o", legend.titl = titl,
           legend.titl.cex = 3, legend.titl.line = 1,
           legend.point.at = list(x = c(5, 95), y = c(95, 30)),
           legend.point.pch = point.pch,
           legend.point.text = ifelse(rep(is.data.frame(data), length(subset.c)),
                                      names(data[, subset.c]),
                                      subset.c),
           legend.cex = 2.5, legend.bty = "n",
           legend.line.at = list(x = c(5, 95), y = c(20, 5)),
           legend.line.text = names(table(as.character(data[, line.by]),
                                          exclude = c("", "NA"))),
           legend.line.lwd = line.lwd, legend.loc.num = 1,

           ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="event.chart_+3A_data">data</code></td>
<td>

<p>a matrix or data frame with rows corresponding to subjects and
columns corresponding to variables.  Note that for a data frame or
matrix containing multiple time-to-event
data (e.g., time to recurrence, time to death, and time to
last follow-up), one column is required for each specific event.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_subset.r">subset.r</code></td>
<td>

<p>subset of rows of original matrix or data frame to place in event chart.
Logical arguments may be used here (e.g., <code>treatment.arm == 'a'</code>, if
the data frame, data, has been attached to the search directory;
otherwise, <code>data$treatment.arm == "a"</code>).
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_subset.c">subset.c</code></td>
<td>

<p>subset of columns of original matrix or data frame to place in event chart;
if working with a data frame, a vector of data frame variable names may be
used for subsetting purposes (e.g., <code>c('randdate', 'event1')</code>.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_sort.by">sort.by</code></td>
<td>

<p>column(s) or data frame variable name(s) with which to sort the chart's output.
The default is <code>NA</code>, thereby resulting in a chart sorted by original row number.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_sort.ascending">sort.ascending</code></td>
<td>

<p>logical flag (which takes effect only if the argument <code>sort.by</code> is utilized).
If <code>TRUE</code> (default), sorting is done in ascending order; if <code>FALSE</code>, descending order.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_sort.na.last">sort.na.last</code></td>
<td>

<p>logical flag (which takes effect only if the argument <code>sort.by</code> is utilized).
If <code>TRUE</code> (default), <code>NA</code> values are considered as last values in ordering.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_sort.after.subset">sort.after.subset</code></td>
<td>

<p>logical flag (which takes effect only if the argument sort.by is utilized).
If <code>FALSE</code>, sorting data (via <code>sort.by</code> specified variables
or columns) will be performed prior to row subsetting (via <code>subset.r</code>);
if <code>TRUE</code> (default), row subsetting of original data will be done before sorting.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_y.var">y.var</code></td>
<td>

<p>variable name or column number of original matrix or data frame with
which to scale y-axis.  
Default is <code>NA</code>, which will result in equally spaced lines on y-axis
(based on original data or sorted data if requested by sort.by).
Otherwise, location of lines on y-axis will be dictated by specified variable
or column.  Examples of specified variables may be date of an event
or a physiological covariate.  Any observation which has
a missing value for the y.var variable will not appear on the graph.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_y.var.type">y.var.type</code></td>
<td>

<p>type of variable specified in <code>y.var</code> (which will only take effect if
argument <code>y.var</code> is utilized). If <code>"d"</code>, specifed variable is a date (either
numeric julian date or an S-Plus dates object);  if <code>"n"</code>, specifed variable
is numeric (e.g., systolic blood pressure level) although not a julian date.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_y.jitter">y.jitter</code></td>
<td>

<p>logical flag (which takes effect only if the argument <code>y.var</code> is utilized).
Due to potential ties in <code>y.var</code> variable, <code>y.jitter</code> (when <code>TRUE</code>) will jitter
the data to allow discrimination between observations at the possible cost
of producing slightly inaccurate dates or covariate values;  if <code>FALSE</code> (the
default), no jittering will be performed.  The <code>y.jitter</code> algorithm
assumes a uniform distribution of observations across the range of <code>y.var</code>.
The algorithm is as follows:
</p>
<p><code>
    size.jitter &lt;-
    ( diff(range(y.var)) /  (2 * (length(y.var) - 1)) ) * y.jitter.factor
    </code>
</p>
<p>The default of <code>y.jitter.factor</code> is 1.  The entire product is then used as an
argument into <code>runif</code>:  <code>y.var &lt;-
    y.var + runif(length(y.var), -size.jitter, size.jitter)</code>
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_y.jitter.factor">y.jitter.factor</code></td>
<td>

<p>an argument used with the <code>y.jitter</code> function to scale the range of added noise.
Default is 1.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_y.renum">y.renum</code></td>
<td>

<p>logical flag.  If <code>TRUE</code>, subset observations are listed on y-axis from
1 to <code>length(subset.r)</code>; if <code>FALSE</code> (default), subset observations are listed
on y-axis in original form.  As an example, if <code>subset.r = 301:340</code> and
<code>y.renum ==TRUE</code>, y-axis will be shown as 1 through 40.  However, if
<code>y.renum ==FALSE</code>, y-axis will be shown as 301 through 340.  The above examples
assume the following argument, <code>NA.rm</code>, is set to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_na.rm">NA.rm</code></td>
<td>

<p>logical flag.  If <code>TRUE</code>, subset observations which have
<code>NA</code> for each variable specified in subset.c will not have an
entry on the y-axis.  Also, if the following argument,
<code>x.reference</code>, is specified, observations with missing
<code>x.reference</code> values will also not have an entry on the y-axis.
If <code>FALSE</code> (default), user can identify those observations
which do have <code>NA</code> for every variable specified in
<code>subset.c</code> (or, if <code>x.reference</code> is specified, also
those observations which are missing only the <code>x.reference</code> value); this can
easily be done by examining the resulting y-axis and
recognizing the observations without any plotting symbols.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_x.reference">x.reference</code></td>
<td>

<p>column of original matrix or data frame with which to reference the x-axis.
That is, if specified, all columns specified in <code>subset.c</code> will be substracted
by <code>x.reference</code>.  An example may be to see the timing of events before and
after treatment or to see time-to-event after entry into study.
The event times will be aligned using the <code>x.reference</code> argument
as the reference point.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_now">now</code></td>
<td>

<p>the &ldquo;now&rdquo; date which will be used for top of y-axis
when creating the Goldman eventchart (see reference below).
Default is <code>max(data[, subset.c], na.rm =TRUE)</code>.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_now.line">now.line</code></td>
<td>

<p>logical flag.   A feature utilized by the Goldman Eventchart.
When <code>x.reference</code> is specified as the start of follow-up and
<code>y.var = x.reference</code>, then the Goldman chart can be created.
This argument, if <code>TRUE</code>, will cause the plot region to be square, and will
draw a line with a slope of -1 from the top of the y-axis to the right
end of the x-axis.  Essentially, it denotes end of current follow-up period
for looking at the time-to-event data.  Default is <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_now.line.lty">now.line.lty</code></td>
<td>

<p>line type of <code>now.line</code>.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_now.line.lwd">now.line.lwd</code></td>
<td>

<p>line width of <code>now.line</code>.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_now.line.col">now.line.col</code></td>
<td>

<p>color of <code>now.line</code>.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_pty">pty</code></td>
<td>

<p>graph option, <code>pty='m'</code> is the default; use <code>pty='s'</code> for the square looking
Goldman's event chart.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_date.orig">date.orig</code></td>
<td>

<p>date of origin to consider if dates are in julian, SAS , or S-Plus dates
object format;  default is January 1, 1960 (which is the default origin
used by both  S-Plus and SAS).  Utilized when either
<code>y.julian = FALSE</code> or <code>x.julian = FALSE</code>.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_titl">titl</code></td>
<td>

<p>title for event chart.  Default is 'Event Chart'.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_y.idlabels">y.idlabels</code></td>
<td>

<p>column or data frame variable name used for y-axis labels.  For example,
if <code>c('pt.no')</code> is specified, patient ID (stored in <code>pt.no</code>)
will be seen on y-axis labels
instead of sequence specified by <code>subset.r</code>.  This argument takes precedence
over both <code>y.axis = 'auto'</code> and <code>y.axis = 'custom'</code> (see below).
NOTE:  Program will issue warning if this argument is
specified and if <code>is.na(y.var) == FALSE</code>;  <code>y.idlabels</code> will not be
used in this situation.  Also, attempting to plot too many patients
on a single event chart will cause undesirable plotting of <code>y.idlabels</code>.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_y.axis">y.axis</code></td>
<td>

<p>character string specifying whether program will control labelling
of y-axis (with argument <code>"auto"</code>), or if user will control labelling
(with argument <code>"custom"</code>).  If <code>"custom"</code> is chosen, user must specify
location and text of labels using <code>y.axis.custom.at</code> and
<code>y.axis.custom.labels</code> arguments, respectively, listed below.
This argument will not be utilized if <code>y.idlabels</code> is specified.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_y.axis.custom.at">y.axis.custom.at</code></td>
<td>

<p>user-specified vector of y-axis label locations.
Must be used when <code>y.axis = "custom"</code>; will not be used otherwise.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_y.axis.custom.labels">y.axis.custom.labels</code></td>
<td>

<p>user-specified vector of y-axis labels.
Must be used when <code>y.axis = "custom"</code>; will not be used otherwise.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_y.julian">y.julian</code></td>
<td>

<p>logical flag (which will only be considered if <code>y.axis == "auto"</code> and
<code>(!is.na(y.var) &amp; y.var.type== "d")</code>.  If <code>FALSE</code> (default), will convert julian
numeric dates or S-Plus dates objects into &ldquo;mm/dd/yy&rdquo; format
for the y-axis labels.  If <code>TRUE</code>, dates will be printed in
julian (numeric) format.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_y.lim.extend">y.lim.extend</code></td>
<td>

<p>two-dimensional vector representing the number of units that the user
wants to increase <code>ylim</code> on bottom and top of y-axis, respectively.
Default <code>c(0,0)</code>.  This argument will not take effect if the Goldman chart
is utilized.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_y.lab">y.lab</code></td>
<td>

<p>single label to be used for entire y-axis.  Default will be the variable name
or column number of <code>y.idlabels</code> (if non-missing) and blank otherwise.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_x.axis.all">x.axis.all</code></td>
<td>

<p>logical flag. If <code>TRUE</code> (default), lower and upper limits of x-axis will be
based on all observations (rows) in matrix or data frame.  If <code>FALSE</code>, lower and
upper limits will be based only on those observations specified by <code>subset.r</code>
(either before or after sorting depending on specification of <code>sort.by</code> and
value of <code>sort.after.subset</code>).
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_x.axis">x.axis</code></td>
<td>

<p>character string specifying whether program will control labelling
of x-axis (with argument <code>"auto"</code>), or if user will control labelling
(with argument <code>"custom"</code>).  If <code>"custom"</code> is chosen, user must specify
location and text of labels using <code>x.axis.custom.at</code> and
<code>x.axis.custom.labels</code> arguments, respectively, listed below.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_x.axis.custom.at">x.axis.custom.at</code></td>
<td>

<p>user-specified vector of x-axis label locations.
Must be used when <code>x.axis == "custom"</code>; will not be used otherwise.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_x.axis.custom.labels">x.axis.custom.labels</code></td>
<td>

<p>user-specified vector of x-axis labels.
Must be used when <code>x.axis == "custom"</code>; will not be used otherwise.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_x.julian">x.julian</code></td>
<td>

<p>logical flag (which will only be considered if <code>x.axis == "auto"</code>).
If <code>FALSE</code> (default), will convert julian dates or S-plus dates objects
into &ldquo;mm/dd/yy&rdquo; format for the x-axis labels.  If <code>TRUE</code>, dates will be
printed in julian (numeric) format.  NOTE:  This argument should remain <code>TRUE</code> if
<code>x.reference</code> is specified.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_x.lim.extend">x.lim.extend</code></td>
<td>

<p>two-dimensional vector representing the number of time units (usually in days)
that the user wants to increase <code>xlim</code> on left-hand side and right-hand
side of x-axis, respectively.  Default is <code>c(0,0)</code>.  This argument will not
take effect if the Goldman chart is utilized.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_x.scale">x.scale</code></td>
<td>

<p>a factor whose reciprocal is multiplied to original units of the
x-axis.  For example, if the original data frame is in units of days,
<code>x.scale = 365</code> will result in units of years (notwithstanding leap years).
Default is 1.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_x.lab">x.lab</code></td>
<td>

<p>single label to be used for entire x-axis.  Default will be &ldquo;On Study Date&rdquo;
if <code>x.julian = FALSE</code> and &ldquo;Time on Study&rdquo; if <code>x.julian = TRUE</code>.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_line.by">line.by</code></td>
<td>

<p>column or data frame variable name for plotting unique lines by unique
values of vector (e.g., specify <code>c('arm')</code> to plot unique lines by
treatment arm).  Can take at most one column or variable name.
Default is <code>NA</code> which produces identical lines for each patient.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_line.lty">line.lty</code></td>
<td>

<p>vector of line types corresponding to ascending order of <code>line.by</code> values.
If <code>line.by</code> is specified, the vector should be the length of
the number of unique values of <code>line.by</code>.
If <code>line.by</code> is <code>NA</code>, only <code>line.lty[1]</code> will be used.
The default is 1.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_line.lwd">line.lwd</code></td>
<td>

<p>vector of line widths corresponding to ascending order of <code>line.by</code> values.
If <code>line.by</code> is specified, the vector should be the length of
the number of unique values of <code>line.by</code>.
If <code>line.by</code> is <code>NA</code>, only <code>line.lwd[1]</code> will be used.
The default is 1.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_line.col">line.col</code></td>
<td>

<p>vector of line colors corresponding to ascending order of <code>line.by</code> values.
If <code>line.by</code> is specified, the vector should be the length of
the number of unique values of <code>line.by</code>.
If <code>line.by</code> is <code>NA</code>, only <code>line.col[1]</code> will be used.
The default is 1.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_line.add">line.add</code></td>
<td>

<p>a 2xk matrix with k=number of pairs of additional line segments to add.
For example, if it is of interest to draw additional line segments
connecting events one and two, two and three, and four and five,
(possibly with different colors), an appropriate <code>line.add</code> argument would be
<code>matrix(c('first.event','second.event','second.event','third.event',
    'fourth.event','fifth.event'), 2, 3)</code>.  One line segment
would be drawn between <code>first.event</code> and <code>second.event</code>,
a second line segment would be drawn between <code>second.event</code> and <code>third.event</code>,
and a third line segment would be drawn between <code>fourth.event</code> and <code>fifth.event</code>.
Different line types, widths and colors can be specified (in arguments
listed just below).
</p>
<p>The convention use of <code>subset.c</code> and <code>line.add</code> must match (i.e., column name
must be used for both or column number must be used for both).
</p>
<p>If <code>line.add != NA</code>, length of <code>line.add.lty</code>, <code>line.add.lwd</code>, and <code>line.add.col</code>
must be the same as number of pairs of additional line segments to add.
</p>
<p>NOTE:  The drawing of the original default line
may be suppressed (with <code>line.col = 0</code>),
and <code>line.add</code> can be used to do all the line plotting for the event chart.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_line.add.lty">line.add.lty</code></td>
<td>

<p>a kx1 vector corresponding to the columns of <code>line.add</code>; specifies the line
types for the k line segments.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_line.add.lwd">line.add.lwd</code></td>
<td>

<p>a kx1 vector corresponding to the columns of <code>line.add</code>; specifies the line
widths for the k line segments.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_line.add.col">line.add.col</code></td>
<td>

<p>a kx1 vector corresponding to the columns of <code>line.add</code>; specifies the line
colors for the k line segments.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_point.pch">point.pch</code></td>
<td>

<p>vector of <code>pch</code> values for points representing each event.  If similar
events are listed in multiple columns (e.g., regular visits or
a recurrent event), repeated <code>pch</code> values may be listed in the
vector (e.g., <code>c(2,4,rep(183,3))</code>).
If <code>length(point.pch) &lt; length(subset.c)</code>, <code>point.pch</code> will be repeated until
lengths are equal; a warning message will verify this condition.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_point.cex">point.cex</code></td>
<td>

<p>vector of size of points representing each event.
If <code>length(point.cex) &lt; length(subset.c)</code>, <code>point.cex</code> will be repeated until
lengths are equal; a warning message will verify this condition.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_point.col">point.col</code></td>
<td>

<p>vector of colors of points representing each event.
If <code>length(point.col) &lt; length(subset.c)</code>, <code>point.col</code> will be repeated until
lengths are equal; a warning message will verify this condition.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_point.cex.mult">point.cex.mult</code></td>
<td>

<p>a single number (may be non-integer), which is the base multiplier for the value of
the <code>cex</code> of the plotted points, when interest lies in 
a variable size allowed for certain points, as a function of
the quantity of the variable(s) in the dataset specified in the <code>point.cex.mult.var</code> argument;
multiplied by original <code>point.cex</code> value and then the value of interest (for an individual)
from the <code>point.cex.mult.var argument</code>; 
used only when non-<code>NA</code> arguments are provided to <code>point.cex.mult.var</code>;
default is 1. .
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_point.cex.mult.var">point.cex.mult.var</code></td>
<td>

<p>vector of variables to be used in determining what point.cex.mult is multiplied by
for determining size of plotted points from (possibly a subset of) 
<code>subset.c</code> variables, when interest lies in 
a variable size allowed for certain points, as a function of
the level of some variable(s) in the dataset;
default is <code>NA</code>.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_extra.points.no.mult">extra.points.no.mult</code></td>
<td>

<p>vector of variables in the dataset to ignore for purposes of using 
<code>point.cex.mult</code>; for example, for some variables there may be interest in
allowing a variable size allowed for the plotting of the points, whereas
other variables (e.g., dropout time), there may be no interest in such manipulation;
the vector should be the same size as the number of variables specified in <code>subset.c</code>,
with <code>NA</code> entries where variable point size is of interest 
and the variable name (or location in <code>subset.c</code>) specified when the variable
point size is not of interest; in this latter case, 
the associated argument in <code>point.cex</code> is instead used as the point <code>cex</code>;
used only when non-<code>NA</code> arguments are provided to <code>point.cex.mult.var</code>;
default is <code>NA</code>
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_legend.plot">legend.plot</code></td>
<td>

<p>logical flag;  if <code>TRUE</code>, a legend will be plotted.  Location of legend will
be based on specification of legend.location along with values of other
arguments listed below.  Default is <code>FALSE</code> (i.e., no legend plotting).
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_legend.location">legend.location</code></td>
<td>

<p>will be used only if <code>legend.plot = TRUE</code>.
If <code>"o"</code> (default), a one-page legend will precede the output of the chart.
The user will need to hit <kbd>enter</kbd> in order for the event chart to be displayed.
This feature is possible due to the <b><code>dev.ask</code></b> option.
If <code>"i"</code>, an internal legend will be placed in the plot region
based on <code>legend.point.at</code>.  If <code>"l"</code>, a legend will be placed in the plot region
using the locator option.  Legend will map points to events (via column
names, by default) and, if <code>line.by</code> is specified, lines to groups (based on
levels of <code>line.by</code>).
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_legend.titl">legend.titl</code></td>
<td>

<p>title for the legend; default is title to be used for main plot.
Only used when <code>legend.location = "o"</code>.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_legend.titl.cex">legend.titl.cex</code></td>
<td>

<p>size of text for legend title.  Only used when <code>legend.location = "o"</code>.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_legend.titl.line">legend.titl.line</code></td>
<td>

<p>line location of legend title dictated by <code>mtext</code> function with
<code>outer = FALSE</code> option;
default is 1.0.  Only used when <code>legend.location = "o"</code>.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_legend.point.at">legend.point.at</code></td>
<td>

<p>location of upper left and lower right corners of legend area to
be utilized for describing events via points and text.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_legend.point.pch">legend.point.pch</code></td>
<td>

<p>vector of <code>pch</code> values for points representing each event in the legend.
Default is <code>point.pch</code>.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_legend.point.text">legend.point.text</code></td>
<td>

<p>text to be used for describing events;  the default is setup for a data frame,
as it will print the names of the columns specified by <code>subset.c</code>.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_legend.cex">legend.cex</code></td>
<td>

<p>size of text for points and event descriptions.  Default is 2.5 which is setup
for <code>legend.location = "o"</code>.  A much smaller <code>cex</code> is recommended (possibly 0.75)
for use with <code>legend.location = "i"</code> or <code>legend.location = "l"</code>.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_legend.bty">legend.bty</code></td>
<td>

<p>option to put a box around the legend(s); default is to have no box
(<code>legend.bty = "n"</code>).  Option <code>legend.bty = "o"</code> will produce a legend box.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_legend.line.at">legend.line.at</code></td>
<td>

<p>if <code>line.by</code> was specified (with <code>legend.location = "o"</code> or <code>legend.location = "i"</code>),
this argument will dictate the location of the upper left and lower right
corners of legend area to be utilized for describing the different
<code>line.by</code> values (e.g., <code>treatment.arm</code>).  The default is setup for
<code>legend.location = "o"</code>.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_legend.line.text">legend.line.text</code></td>
<td>

<p>text to be used for describing <code>line.by</code> values;  the default are the names
of the unique non-missing <code>line.by</code> values as produced from the table function.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_legend.line.lwd">legend.line.lwd</code></td>
<td>

<p>vector of line widths corresponding to <code>line.by</code> values.
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_legend.loc.num">legend.loc.num</code></td>
<td>

<p>number used for locator argument when <code>legend.locator = "l"</code>.  If 1 (default),
user is to locate only the top left corner of the legend box.  If 2, user
is to locate both the top left corner and the lower right corner.  This will
be done twice when <code>line.by</code> is specified (once for points and once for lines).
</p>
</td></tr>
<tr><td><code id="event.chart_+3A_...">...</code></td>
<td>

<p>additional par arguments for use in main plot.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>if you want to put, say, two eventcharts side-by-side, in a plot
region, you should not set up <code>par(mfrow=c(1,2))</code> before running the
first plot.  Instead, you should add the argument <code>mfg=c(1,1,1,2)</code>
to the first plot call followed by the argument <code>mfg=c(1,2,1,2)</code>
to the second plot call.
</p>
<p>if dates in original data frame are in a specialized form
(eg., mm/dd/yy) of mode CHARACTER, the user must convert those columns to
become class dates or julian numeric mode (see <code><a href="base.html#topic+Date">Date</a></code> for more information).
For example, in a data frame called <code>testdata</code>, with specialized
dates in columns 4 thru 10, the following code could be used:
<code>as.numeric(dates(testdata[,4:10]))</code>.  This will convert the columns
to numeric julian dates based on the function's default origin
of January 1, 1960.  If original dates are in class dates or julian form,
no extra work is necessary.
</p>
<p>In the survival analysis, the data typically come  in  two
columns: one column containing survival time and the other
containing  censoring  indicator  or   event   code.   The
<code>event.convert</code>  function  converts  this  type of data into
multiple columns of event times, one column of each  event
type, suitable for the <code>event.chart</code> function.
</p>


<h3>Side Effects</h3>

<p>an event chart is created on the current graphics device.
If legend.plot =TRUE and legend.location = 'o',
a one-page legend will precede the event chart.  Please note that par
parameters on completion of function will be reset to par parameters
existing prior to start of function.
</p>


<h3>Author(s)</h3>

<p>J. Jack Lee and Kenneth R. Hess
<br />
Department of Biostatistics
<br />
University of Texas
<br />
M.D. Anderson Cancer Center
<br />
Houston, TX 77030
<br />
<a href="mailto:jjlee@mdanderson.org">jjlee@mdanderson.org</a>, <a href="mailto:khess@mdanderson.org">khess@mdanderson.org</a>
</p>
<p>Joel A. Dubin
<br />
Department of Statistics
<br />
University of Waterloo
<br />
<a href="mailto:jdubin@uwaterloo.ca">jdubin@uwaterloo.ca</a>
</p>


<h3>References</h3>

<p>Lee J.J., Hess, K.R., Dubin, J.A. (2000).  Extensions and applications
of event charts.
<em>The American Statistician,</em>
<b>54:1</b>, 63&ndash;70.
</p>
<p>Dubin, J.A., Lee, J.J., Hess, K.R. (1997).
The Utility of Event Charts.
<em>Proceedings of the Biometrics Section, American</em>
Statistical Association.
</p>
<p>Dubin, J.A., Muller H-G, Wang J-L (2001).
Event history graphs for censored survival data.
<em>Statistics in Medicine,</em>
<b>20:</b> 2951&ndash;2964.
</p>
<p>Goldman, A.I. (1992).
EVENTCHARTS:  Visualizing Survival and Other Timed-Events Data.
<em>The American Statistician,</em>
<b>46:1</b>, 13&ndash;18.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+event.history">event.history</a></code>, <code><a href="base.html#topic+Date">Date</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The sample data set is an augmented CDC AIDS dataset (ASCII)
# which is used in the examples in the help file.  This dataset is 
# described in Kalbfleisch and Lawless (JASA, 1989).
# Here, we have included only children 4 years old and younger.
# We have also added a new field, dethdate, which
# represents a fictitious death date for each patient.  There was
# no recording of death date on the original dataset.  In addition, we have
# added a fictitious viral load reading (copies/ml) for each patient at time of AIDS diagnosis,
# noting viral load was also not part of the original dataset.
#   
# All dates are julian with julian=0 being 
# January 1, 1960, and julian=14000 being 14000 days beyond
# January 1, 1960 (i.e., May 1, 1998).


cdcaids &lt;- data.frame(
age=c(4,2,1,1,2,2,2,4,2,1,1,3,2,1,3,2,1,2,4,2,2,1,4,2,4,1,4,2,1,1,3,3,1,3),
infedate=c(
7274,7727,7949,8037,7765,8096,8186,7520,8522,8609,8524,8213,8455,8739,
8034,8646,8886,8549,8068,8682,8612,9007,8461,8888,8096,9192,9107,9001,
9344,9155,8800,8519,9282,8673),
diagdate=c(
8100,8158,8251,8343,8463,8489,8554,8644,8713,8733,8854,8855,8863,8983,
9035,9037,9132,9164,9186,9221,9224,9252,9274,9404,9405,9433,9434,9470,
9470,9472,9489,9500,9585,9649),
diffdate=c(
826,431,302,306,698,393,368,1124,191,124,330,642,408,244,1001,391,246,
615,1118,539,612,245,813,516,1309,241,327,469,126,317,689,981,303,976),
dethdate=c(
8434,8304,NA,8414,8715,NA,8667,9142,8731,8750,8963,9120,9005,9028,9445,
9180,9189,9406,9711,9453,9465,9289,9640,9608,10010,9488,9523,9633,9667,
9547,9755,NA,9686,10084),
censdate=c(
NA,NA,8321,NA,NA,8519,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,
NA,NA,NA,NA,NA,NA,NA,NA,NA,10095,NA,NA),
viralload=c(
13000,36000,70000,90000,21000,110000,75000,12000,125000,110000,13000,39000,79000,135000,14000,
42000,123000,20000,12000,18000,16000,140000,16000,58000,11000,120000,85000,31000,24000,115000,
17000,13100,72000,13500)
)

cdcaids &lt;- upData(cdcaids,
 labels=c(age     ='Age, y', infedate='Date of blood transfusion',
          diagdate='Date of AIDS diagnosis',
          diffdate='Incubation period (days from HIV to AIDS)',
          dethdate='Fictitious date of death',
          censdate='Fictitious censoring date',
	  viralload='Fictitious viral load'))


# Note that the style options listed with these
# examples are best suited for output to a postscript file (i.e., using
# the postscript function with horizontal=TRUE) as opposed to a graphical
# window (e.g., motif).


# To produce simple calendar event chart (with internal legend):
# postscript('example1.ps', horizontal=TRUE)
 event.chart(cdcaids,
  subset.c=c('infedate','diagdate','dethdate','censdate'),
  x.lab = 'observation dates',
  y.lab='patients (sorted by AIDS diagnosis date)',
  titl='AIDS data calendar event chart 1',
  point.pch=c(1,2,15,0), point.cex=c(1,1,0.8,0.8),
  legend.plot=TRUE, legend.location='i', legend.cex=1.0,
  legend.point.text=c('transfusion','AIDS diagnosis','death','censored'),
  legend.point.at = list(c(7210, 8100), c(35, 27)), legend.bty='o')


# To produce simple interval event chart (with internal legend):
# postscript('example2.ps', horizontal=TRUE)
 event.chart(cdcaids,
  subset.c=c('infedate','diagdate','dethdate','censdate'),
  x.lab = 'time since transfusion (in days)',
  y.lab='patients (sorted by AIDS diagnosis date)',
  titl='AIDS data interval event chart 1',
  point.pch=c(1,2,15,0), point.cex=c(1,1,0.8,0.8),
  legend.plot=TRUE, legend.location='i', legend.cex=1.0,
  legend.point.text=c('transfusion','AIDS diagnosis','death','censored'),
  x.reference='infedate', x.julian=TRUE,
  legend.bty='o', legend.point.at = list(c(1400, 1950), c(7, -1)))


# To produce simple interval event chart (with internal legend),
# but now with flexible diagdate symbol size based on viral load variable:
# postscript('example2a.ps', horizontal=TRUE)
 event.chart(cdcaids,
  subset.c=c('infedate','diagdate','dethdate','censdate'),
  x.lab = 'time since transfusion (in days)',
  y.lab='patients (sorted by AIDS diagnosis date)',
  titl='AIDS data interval event chart 1a, with viral load at diagdate represented',
  point.pch=c(1,2,15,0), point.cex=c(1,1,0.8,0.8),
  point.cex.mult = 0.00002, point.cex.mult.var = 'viralload', extra.points.no.mult = c(1,NA,1,1), 
  legend.plot=TRUE, legend.location='i', legend.cex=1.0,
  legend.point.text=c('transfusion','AIDS diagnosis','death','censored'),
  x.reference='infedate', x.julian=TRUE,
  legend.bty='o', legend.point.at = list(c(1400, 1950), c(7, -1)))


# To produce more complicated interval chart which is
# referenced by infection date, and sorted by age and incubation period:
# postscript('example3.ps', horizontal=TRUE)
 event.chart(cdcaids,
  subset.c=c('infedate','diagdate','dethdate','censdate'),
  x.lab = 'time since diagnosis of AIDS (in days)',
  y.lab='patients (sorted by age and incubation length)',
  titl='AIDS data interval event chart 2 (sorted by age, incubation)',
  point.pch=c(1,2,15,0), point.cex=c(1,1,0.8,0.8),
  legend.plot=TRUE, legend.location='i',legend.cex=1.0,
  legend.point.text=c('transfusion','AIDS diagnosis','death','censored'),
  x.reference='diagdate', x.julian=TRUE, sort.by=c('age','diffdate'),
  line.by='age', line.lty=c(1,3,2,4), line.lwd=rep(1,4), line.col=rep(1,4),
  legend.bty='o', legend.point.at = list(c(-1350, -800), c(7, -1)),
  legend.line.at = list(c(-1350, -800), c(16, 8)),
  legend.line.text=c('age = 1', '       = 2', '       = 3', '       = 4'))


# To produce the Goldman chart:
# postscript('example4.ps', horizontal=TRUE)
 event.chart(cdcaids,
  subset.c=c('infedate','diagdate','dethdate','censdate'),
  x.lab = 'time since transfusion (in days)', y.lab='dates of observation',
  titl='AIDS data Goldman event chart 1',
  y.var = c('infedate'), y.var.type='d', now.line=TRUE, y.jitter=FALSE,
  point.pch=c(1,2,15,0), point.cex=c(1,1,0.8,0.8), mgp = c(3.1,1.6,0),
  legend.plot=TRUE, legend.location='i',legend.cex=1.0,
  legend.point.text=c('transfusion','AIDS diagnosis','death','censored'),
  x.reference='infedate', x.julian=TRUE,
  legend.bty='o', legend.point.at = list(c(1500, 2800), c(9300, 10000)))


# To convert coded time-to-event data, then, draw an event chart:
surv.time &lt;- c(5,6,3,1,2)
cens.ind   &lt;- c(1,0,1,1,0)
surv.data  &lt;- cbind(surv.time,cens.ind)
event.data &lt;- event.convert(surv.data)
event.chart(cbind(rep(0,5),event.data),x.julian=TRUE,x.reference=1)
</code></pre>

<hr>
<h2 id='event.convert'>
Event Conversion for Time-to-Event Data
</h2><span id='topic+event.convert'></span>

<h3>Description</h3>

<p>Convert a two-column data matrix with event time and event code into
multiple column event time with one event in each column
</p>


<h3>Usage</h3>

<pre><code class='language-R'>event.convert(data2, event.time = 1, event.code = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="event.convert_+3A_data2">data2</code></td>
<td>

<p>a matrix or dataframe with at least 2 columns; by default, the first
column contains the event time and the second column contains the k
event codes (e.g. 1=dead, 0=censord)
</p>
</td></tr>
<tr><td><code id="event.convert_+3A_event.time">event.time</code></td>
<td>

<p>the column number in data contains the event time
</p>
</td></tr>
<tr><td><code id="event.convert_+3A_event.code">event.code</code></td>
<td>

<p>the column number in data contains the event code
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the survival analysis, the data typically come  in  two
columns: one column containing survival time and the other
containing  censoring  indicator  or   event   code.   The
<code>event.convert</code>  function  converts  this  type of data into
multiple columns of event times, one column of each  event
type, suitable for the <code>event.chart</code> function.
</p>


<h3>Author(s)</h3>

<p>J. Jack Lee and Kenneth R. Hess
<br />
Department of Biostatistics
<br />
University of Texas
<br />
M.D. Anderson Cancer Center
<br />
Houston, TX 77030
<br />
<a href="mailto:jjlee@mdanderson.org">jjlee@mdanderson.org</a>, <a href="mailto:khess@mdanderson.org">khess@mdanderson.org</a>
</p>
<p>Joel A. Dubin
<br />
Department of Statistics
<br />
University of Waterloo
<br />
<a href="mailto:jdubin@uwaterloo.ca">jdubin@uwaterloo.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+event.history">event.history</a></code>, <code><a href="base.html#topic+Date">Date</a></code>, <code><a href="#topic+event.chart">event.chart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># To convert coded time-to-event data, then, draw an event chart:
surv.time &lt;- c(5,6,3,1,2)
cens.ind   &lt;- c(1,0,1,1,0)
surv.data  &lt;- cbind(surv.time,cens.ind)
event.data &lt;- event.convert(surv.data)
event.chart(cbind(rep(0,5),event.data),x.julian=TRUE,x.reference=1)
</code></pre>

<hr>
<h2 id='event.history'>Produces event.history graph for survival data</h2><span id='topic+event.history'></span>

<h3>Description</h3>

<p>Produces an event history graph for right-censored survival data,
including time-dependent covariate status, as described in
Dubin, Muller, and Wang (2001).  Effectively,
a Kaplan-Meier curve is produced with supplementary information
regarding individual survival information, censoring information, and
status over time of an individual time-dependent covariate or 
time-dependent covariate function for both uncensored and censored 
individuals.   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>event.history(data, survtime.col, surv.col,
              surv.ind = c(1, 0), subset.rows = NULL,
              covtime.cols = NULL, cov.cols = NULL,
              num.colors = 1, cut.cov = NULL, colors = 1,
              cens.density = 10, mult.end.cens = 1.05,
              cens.mark.right =FALSE, cens.mark = "-",
              cens.mark.ahead = 0.5, cens.mark.cutoff = -1e-08,
              cens.mark.cex = 1,
              x.lab = "time under observation",
              y.lab = "estimated survival probability",
              title = "event history graph", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="event.history_+3A_data">data</code></td>
<td>

<p>A matrix or data frame with rows corresponding to units
(often individuals) and columns corresponding to survival time,
event/censoring indicator.  Also, multiple columns may be devoted to
time-dependent covariate level and time change.
</p>
</td></tr>
<tr><td><code id="event.history_+3A_survtime.col">survtime.col</code></td>
<td>

<p>Column (in data) representing minimum of time-to-event or 
right-censoring time for individual.
</p>
</td></tr>
<tr><td><code id="event.history_+3A_surv.col">surv.col</code></td>
<td>

<p>Column (in data) representing event indicator for an individual.
Though, traditionally, such an indicator will be 1 for an event and
0 for a censored observation, this indicator can be represented 
by any two numbers, made explicit by the surv.ind argument.
</p>
</td></tr>
<tr><td><code id="event.history_+3A_surv.ind">surv.ind</code></td>
<td>

<p>Two-element vector representing, respectively, the 
number for an event, as listed in <code>surv.col</code>, 
followed by the number for a censored
observation.  Default is traditional survival data 
represention, i.e., <code>c(1,0)</code>.
</p>
</td></tr>
<tr><td><code id="event.history_+3A_subset.rows">subset.rows</code></td>
<td>

<p>Subset of rows of original matrix or data frame (data) to 
place in event history graph.
Logical arguments may be used here (e.g., <code>treatment.arm == "a"</code>, if
the data frame, data, has been attached to the search directory; 
</p>
</td></tr>
<tr><td><code id="event.history_+3A_covtime.cols">covtime.cols</code></td>
<td>

<p>Column(s) (in data) representing the time when change of time-dependent 
covariate (or time-dependent covariate function) occurs.  
There should be a unique non-<code>NA</code> entry in the column for each such change 
(along with corresponding <code>cov.cols</code> column entry representing 
the value of the covariate or function at that change time).  
Default is <code>NULL</code>, meaning no time-dependent covariate information 
will be presented in the graph.  
</p>
</td></tr>
<tr><td><code id="event.history_+3A_cov.cols">cov.cols</code></td>
<td>

<p>Column(s) (in data) representing the level of the time-dependent 
covariate (or time-dependent covariate function).  There should be 
a unique non-<code>NA</code> column entry representing each change in the level 
(along with a corresponding covtime.cols column entry representing 
the time of the change).  Default is <code>NULL</code>, meaning
no time-dependent covariate information will be presented in
the graph. 
</p>
</td></tr>
<tr><td><code id="event.history_+3A_num.colors">num.colors</code></td>
<td>

<p>Colors are utilized for the time-dependent covariate level for an
individual.  This argument provides the number of unique covariate
levels which will be displayed by mapping the number of colors 
(via <code>num.colors</code>) to the number of desired covariate levels.  
This will divide the covariate span into roughly equally-sized 
intervals, via the S-Plus cut function.
Default is one color, meaning no time-dependent information
will be presented in the graph.  Note that this argument will
be ignored/superceded if a non-NULL argument is provided for the
<code>cut.cov</code> parameter.
</p>
</td></tr>
<tr><td><code id="event.history_+3A_cut.cov">cut.cov</code></td>
<td>

<p>This argument allows the user to explicitly state how to 
define the intervals for the time-dependent covariate, such that
different colors will be allocated to the user-defined covariate levels.
For example, for plotting five colors, six ordered points within the 
span of the data's covariate levels should be provided.
Default is <code>NULL</code>, meaning that the <code>num.colors</code> argument value
will dictate the number of breakpoints, with the covariate span
defined into roughly equally-sized intervals via the S-Plus cut
function.  However, if <code>is.null(cut.cov) == FALSE</code>, 
then this argument supercedes any entry for the <code>num.colors</code> argument.
</p>
</td></tr>
<tr><td><code id="event.history_+3A_colors">colors</code></td>
<td>

<p>This is a vector argument defining the actual colors used 
for the time-dependent covariate levels in the plot, with the
index of this vector corresponding to the ordered levels
of the covariate.  The number of colors (i.e., the length
of the colors vector) should correspond to the 
value provided to the <code>num.colors</code> argument or the number 
of ordered points - 1 as defined in the <code>cut.cov</code> argument
(with <code>cut.cov</code> superceding <code>num.colors</code> if
<code>is.null(cut.cov) == FALSE</code>).  
The function, as currently written, allows for as much as 
twenty distinct colors.  This argument effectively feeds
into the col argument for the S-Plus polygon function.  
Default is <code>colors = 1</code>.  See the col argument for the both the 
S-Plus par function and polygon function for more information.
</p>
</td></tr>
<tr><td><code id="event.history_+3A_cens.density">cens.density</code></td>
<td>

<p>This will provide the shading density at the end of the 
individual bars for those who are censored.  For more information
on shading density, see the density argument in the S-Plus
polygon function.  Default is <code>cens.density=10</code>.
</p>
</td></tr>
<tr><td><code id="event.history_+3A_mult.end.cens">mult.end.cens</code></td>
<td>

<p>This is a multiplier that extends the length of 
the longest surviving individual bar (or bars, if a tie exists) 
if right-censored, presuming that no event times eventually follow this
final censored time.  Default extends the length 5 percent beyond 
the length of the observed right-censored survival time.
</p>
</td></tr>
<tr><td><code id="event.history_+3A_cens.mark.right">cens.mark.right</code></td>
<td>

<p>A logical argument that states whether an explicit mark 
should be placed to the right of the individual right-censored 
survival bars.  This argument is most useful for
large sample sizes, where it may be hard to detect the special 
shading via cens.density, particularly for the short-term survivors.
</p>
</td></tr>
<tr><td><code id="event.history_+3A_cens.mark">cens.mark</code></td>
<td>

<p>Character argument which describes the censored mark that should be
used if <code>cens.mark.right = TRUE</code>.  Default is <code>"-"</code>.  
</p>
</td></tr>
<tr><td><code id="event.history_+3A_cens.mark.ahead">cens.mark.ahead</code></td>
<td>

<p>A numeric argument, which specifies the absolute distance
to be placed between the individual right-censored
survival bars and the mark as defined in the above cens.mark
argument.  Default is 0.5 (that is, a half of day, if
survival time is measured in days), but may very well need
adjusting depending on the maximum survival time
observed in the dataset.
</p>
</td></tr>
<tr><td><code id="event.history_+3A_cens.mark.cutoff">cens.mark.cutoff</code></td>
<td>

<p>A negative number very close to 0 
(by default <code>cens.mark.cutoff = -1e-8</code>) to ensure that 
the censoring marks get plotted correctly.  See <code>event.history</code>
code in order to see its usage.  This argument typically will not
need adjustment.
</p>
</td></tr>
<tr><td><code id="event.history_+3A_cens.mark.cex">cens.mark.cex</code></td>
<td>

<p>Numeric argument defining the size of the mark defined in 
the <code>cens.mark</code> argument above.  See more information 
by viewing the <code>cex</code> argument for the S-Plus <code><a href="graphics.html#topic+par">par</a></code> function.
Default is <code>cens.mark.cex = 1.0</code>.
</p>
</td></tr>
<tr><td><code id="event.history_+3A_x.lab">x.lab</code></td>
<td>
<p>Single label to be used for entire x-axis.  
Default is <code>"time under observation"</code>. 
</p>
</td></tr>
<tr><td><code id="event.history_+3A_y.lab">y.lab</code></td>
<td>
<p>Single label to be used for entire y-axis.  
Default is <code>"estimated survival probability"</code>. 
</p>
</td></tr>
<tr><td><code id="event.history_+3A_title">title</code></td>
<td>
<p>Title for the event history graph.  
Default is <code>"event history graph"</code>.
</p>
</td></tr>
<tr><td><code id="event.history_+3A_...">...</code></td>
<td>

<p>This allows arguments to the plot function call within 
the <code>event.history</code> function.  
So, for example, the axes representations can be manipulated
with appropriate arguments, or particular areas of the <code>event.history</code> 
graph can be &ldquo;zoomed&rdquo;.  See the details section for more 
comments about zooming.  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In order to focus on a particular area of the event history graph,
zooming can be performed.  This is best done by 
specifying appropriate <code>xlim</code> and <code>ylim</code> 
arguments at the end of the <code>event.history</code> function call, 
taking advantage of the <code>...</code> argument link to the plot function.
An example of zooming can be seen
in Plate 4 of the paper referenced below.
</p>
<p>Please read the reference below to understand how the
individual covariate and survival information is provided in the plot,
how ties are handled, how right-censoring is handled, etc.
</p>


<h3>WARNING</h3>

<p>This function has been tested thoroughly, but only within 
a restricted version and environment, 
i.e., only within S-Plus 2000, Version 3, and within S-Plus 6.0,
version 2, both on a Windows 2000 machine.  
Hence, we cannot currently vouch
for the function's effectiveness 
in other versions of S-Plus (e.g., S-Plus 3.4) 
nor in other operating environments (e.g., Windows 95, Linux or Unix).
The function has also been verified to work on R under Linux.
</p>


<h3>Note</h3>

<p>The authors have found better control of the use of color by 
producing the graphs via the postscript plotting device
in S-Plus.  In fact, the provided examples utilize 
the postscript function.
However, your past experiences may be different, 
and you may prefer to control color directly (to the graphsheet
in Windows environment, for example).  The event.history
function will work with either approach.
</p>


<h3>Author(s)</h3>

<p>Joel Dubin<br />
<a href="mailto:jdubin@uwaterloo.ca">jdubin@uwaterloo.ca</a>
</p>


<h3>References</h3>

<p>Dubin, J.A., Muller, H.-G., and Wang, J.-L. (2001).
Event history graphs for censored survival data.
<em>Statistics in Medicine</em>, <b>20</b>, 2951-2964.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+plot">plot</a></code>,<code><a href="graphics.html#topic+polygon">polygon</a></code>,
<code><a href="#topic+event.chart">event.chart</a></code>, <code><a href="graphics.html#topic+par">par</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Code to produce event history graphs for SIM paper
#
# before generating plots, some pre-processing needs to be performed,
#  in order to get dataset in proper form for event.history function;
#  need to create one line per subject and sort by time under observation, 
#  with those experiencing event coming before those tied with censoring time;
require('survival')
data(heart)

# creation of event.history version of heart dataset (call heart.one):

heart.one &lt;- matrix(nrow=length(unique(heart$id)), ncol=8)
for(i in 1:length(unique(heart$id)))
 {
  if(length(heart$id[heart$id==i]) == 1)
   heart.one[i,] &lt;- as.numeric(unlist(heart[heart$id==i, ]))
  else if(length(heart$id[heart$id==i]) == 2)
   heart.one[i,] &lt;- as.numeric(unlist(heart[heart$id==i,][2,]))
 }

heart.one[,3][heart.one[,3] == 0] &lt;- 2 	## converting censored events to 2, from 0
if(is.factor(heart$transplant))
 heart.one[,7] &lt;- heart.one[,7] - 1
 ## getting back to correct transplantation coding
heart.one &lt;- as.data.frame(heart.one[order(unlist(heart.one[,2]), unlist(heart.one[,3])),])
names(heart.one) &lt;- names(heart)
# back to usual censoring indicator:
heart.one[,3][heart.one[,3] == 2] &lt;- 0 
# note: transplant says 0 (for no transplants) or 1 (for one transplant)
#        and event = 1 is death, while event = 0 is censored

# plot single Kaplan-Meier curve from heart data, first creating survival object
heart.surv &lt;- survfit(Surv(stop, event) ~ 1, data=heart.one, conf.int = FALSE)

# figure 3: traditional Kaplan-Meier curve
# postscript('ehgfig3.ps', horiz=TRUE)
# omi &lt;- par(omi=c(0,1.25,0.5,1.25))
 plot(heart.surv, ylab='estimated survival probability',
      xlab='observation time (in days)')
 title('Figure 3: Kaplan-Meier curve for Stanford data', cex=0.8)
# dev.off()

## now, draw event history graph for Stanford heart data; use as Figure 4

# postscript('ehgfig4.ps', horiz=TRUE, colors = seq(0, 1, len=20))
# par(omi=c(0,1.25,0.5,1.25))
 event.history(heart.one, 
		survtime.col=heart.one[,2], surv.col=heart.one[,3],
		covtime.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,1]),
		cov.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,7]),
		num.colors=2, colors=c(6,10),
		x.lab = 'time under observation (in days)',
		title='Figure 4: Event history graph for\nStanford data',
		cens.mark.right =TRUE, cens.mark = '-', 
		cens.mark.ahead = 30.0, cens.mark.cex = 0.85)
# dev.off()



# now, draw age-stratified event history graph for Stanford heart data; 
#  use as Figure 5

# two plots, stratified by age status
# postscript('c:\temp\ehgfig5.ps', horiz=TRUE, colors = seq(0, 1, len=20))
# par(omi=c(0,1.25,0.5,1.25))
 par(mfrow=c(1,2))

 event.history(data=heart.one, subset.rows = (heart.one[,4] &lt; 0),
		survtime.col=heart.one[,2], surv.col=heart.one[,3],
		covtime.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,1]),
		cov.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,7]),
		num.colors=2, colors=c(6,10),  
		x.lab = 'time under observation\n(in days)',
		title = 'Figure 5a:\nStanford data\n(age &lt; 48)',
		cens.mark.right =TRUE, cens.mark = '-', 
		cens.mark.ahead = 40.0, cens.mark.cex = 0.85,
		xlim=c(0,1900))

 event.history(data=heart.one, subset.rows = (heart.one[,4] &gt;= 0),
		survtime.col=heart.one[,2], surv.col=heart.one[,3],
		covtime.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,1]),
		cov.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,7]),
		num.colors=2, colors=c(6,10),
		x.lab = 'time under observation\n(in days)',
		title = 'Figure 5b:\nStanford data\n(age &gt;= 48)',
		cens.mark.right =TRUE, cens.mark = '-', 
		cens.mark.ahead = 40.0, cens.mark.cex = 0.85,
		xlim=c(0,1900))
# dev.off()
# par(omi=omi)

# we will not show liver cirrhosis data manipulation, as it was 
#  a bit detailed; however, here is the 
#  event.history code to produce Figure 7 / Plate 1

# Figure 7 / Plate 1 : prothrombin ehg with color
## Not run: 
second.arg &lt;- 1				### second.arg is for shading
third.arg &lt;- c(rep(1,18),0,1)		### third.arg is for intensity

# postscript('c:\temp\ehgfig7.ps', horiz=TRUE, 
# colors = cbind(seq(0, 1, len = 20), second.arg, third.arg)) 
# par(omi=c(0,1.25,0.5,1.25), col=19)
 event.history(cirrhos2.eh, subset.rows = NULL,
               survtime.col=cirrhos2.eh$time, surv.col=cirrhos2.eh$event,
		covtime.cols = as.matrix(cirrhos2.eh[, ((2:18)*2)]),
		cov.cols = as.matrix(cirrhos2.eh[, ((2:18)*2) + 1]),
		cut.cov =  as.numeric(quantile(as.matrix(cirrhos2.eh[, ((2:18)*2) + 1]),
				c(0,.2,.4,.6,.8,1), na.rm=TRUE) + c(-1,0,0,0,0,1)),	
 		colors=c(20,4,8,11,14),
		x.lab = 'time under observation (in days)',
		title='Figure 7: Event history graph for liver cirrhosis data (color)',
		cens.mark.right =TRUE, cens.mark = '-', 
		cens.mark.ahead = 100.0, cens.mark.cex = 0.85)
# dev.off()

## End(Not run)
</code></pre>

<hr>
<h2 id='extractlabs'>extractlabs</h2><span id='topic+extractlabs'></span>

<h3>Description</h3>

<p>Extract Labels and Units From Multiple Datasets
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extractlabs(..., print = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extractlabs_+3A_...">...</code></td>
<td>
<p>one ore more data frames or data tables</p>
</td></tr>
<tr><td><code id="extractlabs_+3A_print">print</code></td>
<td>
<p>set to <code>FALSE</code> to not print details about variables with conflicting attributes</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For one or more data frames/tables extracts all labels and units and comb ines them over dataset, dropping any variables not having either labels or units defined.  The resulting data table is returned and is used by the <code>hlab</code> function if the user stores the result in an objectnamed <code>LabelsUnits</code>.  The result is <code>NULL</code> if no variable in any dataset has a non-blank <code>label</code> or <code>units</code>.  Variables found in more than one dataset with duplicate <code>label</code> and <code>units</code> are consolidated.  A warning message is issued when duplicate variables have conflicting labels or units, and by default, details are printed.  No attempt is made to resolve these conflicts.
</p>


<h3>Value</h3>

<p>a data table
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><code><a href="#topic+label">label()</a></code>, <code><a href="#topic+contents">contents()</a></code>, <code><a href="#topic+units">units()</a></code>, <code><a href="#topic+hlab">hlab()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d &lt;- data.frame(x=1:10, y=(1:10)/10)
d &lt;- upData(d, labels=c(x='X', y='Y'), units=c(x='mmHg'), print=FALSE)
d2 &lt;- d
units(d2$x) &lt;- 'cm'
LabelsUnits &lt;- extractlabs(d, d2)
LabelsUnits
</code></pre>

<hr>
<h2 id='fImport'>fImport</h2><span id='topic+fImport'></span>

<h3>Description</h3>

<p>General File Import Using <code>rio</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fImport(
  file,
  format,
  lowernames = c("not mixed", "no", "yes"),
  und. = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fImport_+3A_file">file</code></td>
<td>
<p>name of file to import, or full URL.  <code>rio</code> determines the file type from the file suffix unless you override this with <code>format</code></p>
</td></tr>
<tr><td><code id="fImport_+3A_format">format</code></td>
<td>
<p>format of file to import, usually not needed.  See <code>rio::import()</code> for details</p>
</td></tr>
<tr><td><code id="fImport_+3A_lowernames">lowernames</code></td>
<td>
<p>defaults to changing variable names to all lower case unless the name as mixed upper and lower case, which results in keeping the original characters in the name.  Set <code>lowernames='no'</code> to leave variable names as they were created in the original file export, or set <code>lowernames='yes'</code> to set all names to lower case whether they have mixed case or not.  For all options, a check is made to see if the name conversions would result in any duplicate names.  If so, the original names are retained and a warning message issued.</p>
</td></tr>
<tr><td><code id="fImport_+3A_und.">und.</code></td>
<td>
<p>set to <code>TRUE</code> to change all underscores in names to periods</p>
</td></tr>
<tr><td><code id="fImport_+3A_...">...</code></td>
<td>
<p>more arguments to pass to <code>rio::import()</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a front-end for the <code>rio</code> package's <code>import</code> function.  <code>fImport</code> includes options for setting variable names to lower case and to change underscores in names to periods.  Variables on the imported data frame that have <code>label</code>s are converted to Hmisc package <code>labelled</code> class so that subsetting the data frame will preserve the labels.
</p>


<h3>Value</h3>

<p>a data frame created by <code>rio</code>, unless a <code>rio</code> option is given to use another format
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><code>upData</code>, especially the <code>moveUnits</code> option
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Get a Stata dataset
d &lt;- fImport('http://www.principlesofeconometrics.com/stata/alcohol.dta')
contents(d)

## End(Not run)
</code></pre>

<hr>
<h2 id='find.matches'>
Find Close Matches
</h2><span id='topic+find.matches'></span><span id='topic+summary.find.matches'></span><span id='topic+print.find.matches'></span><span id='topic+matchCases'></span>

<h3>Description</h3>

<p>Compares each row in <code>x</code> against all the rows in <code>y</code>, finding rows in
<code>y</code> with all columns within a tolerance of the values a given row of
<code>x</code>.  The default tolerance
<code>tol</code> is zero, i.e., an exact match is required on all columns.
For qualifying matches, a distance measure is computed.  This is
the sum of squares of differences between <code>x</code> and <code>y</code> after scaling
the columns.  The default scaling values are <code>tol</code>, and for columns
with <code>tol=1</code> the scale values are set to 1.0 (since they are ignored
anyway).  Matches (up to <code>maxmatch</code> of them) are stored and listed in order of 
increasing distance.
<br />
The <code>summary</code> method prints a frequency distribution of the
number of matches per observation in <code>x</code>, the median of the minimum
distances for all matches per <code>x</code>, as a function of the number of matches,
and the frequency of selection of duplicate observations as those having
the smallest distance.  The <code>print</code> method prints the entire <code>matches</code>
and <code>distance</code> components of the result from <code>find.matches</code>.
<br />
<code>matchCases</code> finds all controls that match cases on a single variable
<code>x</code> within a tolerance of <code>tol</code>.  This is intended for prospective
cohort studies that use matching for confounder adjustment (even
though regression models usually work better).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find.matches(x, y, tol=rep(0, ncol(y)), scale=tol, maxmatch=10)
## S3 method for class 'find.matches'
summary(object, ...)
## S3 method for class 'find.matches'
print(x, digits, ...)

matchCases(xcase,    ycase,    idcase=names(ycase),
           xcontrol, ycontrol, idcontrol=names(ycontrol),
           tol=NULL,
           maxobs=max(length(ycase),length(ycontrol))*10,
           maxmatch=20, which=c('closest','random'))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find.matches_+3A_x">x</code></td>
<td>

<p>a numeric matrix or the result of <code>find.matches</code>
</p>
</td></tr>
<tr><td><code id="find.matches_+3A_y">y</code></td>
<td>

<p>a numeric matrix with same number of columns as <code>x</code>
</p>
</td></tr>
<tr><td><code id="find.matches_+3A_xcase">xcase</code></td>
<td>
<p>numeric vector to match on for cases</p>
</td></tr>
<tr><td><code id="find.matches_+3A_xcontrol">xcontrol</code></td>
<td>
<p>numeric vector to match on for controls, not necessarily
the same length as <code>xcase</code></p>
</td></tr>
<tr><td><code id="find.matches_+3A_ycase">ycase</code></td>
<td>
<p>a vector or matrix</p>
</td></tr>
<tr><td><code id="find.matches_+3A_ycontrol">ycontrol</code></td>
<td>

<p><code>ycase</code> and <code>ycontrol</code> are vectors or matrices, not necessarily having the same number of rows,
specifying a variable to carry along from cases and matching
controls.  If you instead want to carry along rows from a data frame,
let <code>ycase</code> and <code>ycontrol</code> be non-overlapping integer subscripts of
the donor data frame.
</p>
</td></tr>
<tr><td><code id="find.matches_+3A_tol">tol</code></td>
<td>

<p>a vector of tolerances with number of elements the same as the number
of columns of <code>y</code>, for <code>find.matches</code>.  For <code>matchCases</code>
is a scalar tolerance.
</p>
</td></tr>
<tr><td><code id="find.matches_+3A_scale">scale</code></td>
<td>

<p>a vector of scaling constants with number of elements the same as the
number of columns of <code>y</code>.
</p>
</td></tr>
<tr><td><code id="find.matches_+3A_maxmatch">maxmatch</code></td>
<td>

<p>maximum number of matches to allow.  For <code>matchCases</code>,
maximum number of controls to match with a case (default is 20).  If more than
<code>maxmatch</code> matching controls are available, a random sample without
replacement of <code>maxmatch</code> controls is used (if <code>which="random"</code>).
</p>
</td></tr>
<tr><td><code id="find.matches_+3A_object">object</code></td>
<td>
<p>an object created by <code>find.matches</code></p>
</td></tr>
<tr><td><code id="find.matches_+3A_digits">digits</code></td>
<td>
<p>number of digits to use in printing distances</p>
</td></tr>
<tr><td><code id="find.matches_+3A_idcase">idcase</code></td>
<td>
<p>vector the same length as <code>xcase</code></p>
</td></tr>
<tr><td><code id="find.matches_+3A_idcontrol">idcontrol</code></td>
<td>

<p><code>idcase</code> and <code>idcontrol</code> are vectors the same length as
<code>xcase</code> and <code>xcontrol</code> respectively, 
specifying the id of cases and controls.  Defaults are integers
specifying original element positions within each of cases and
controls.
</p>
</td></tr>
<tr><td><code id="find.matches_+3A_maxobs">maxobs</code></td>
<td>

<p>maximum number of cases and all matching controls combined (maximum
dimension of data frame resulting from <code>matchControls</code>).  Default is
ten times the maximum of the number of cases and number of controls.
<code>maxobs</code> is used to allocate space for the resulting data frame.
</p>
</td></tr>
<tr><td><code id="find.matches_+3A_which">which</code></td>
<td>

<p>set to <code>"closest"</code> (the default) to match cases with up to <code>maxmatch</code>
controls that most closely match on <code>x</code>.  Set <code>which="random"</code> to use
randomly chosen controls.  In either case, only those controls within
<code>tol</code> on <code>x</code> are allowed to be used.
</p>
</td></tr>
<tr><td><code id="find.matches_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>find.matches</code> returns a list of class <code>find.matches</code> with elements
<code>matches</code> and <code>distance</code>. 
Both elements are matrices with the number of rows equal to the number
of rows in <code>x</code>, and with <code>k</code> columns, where <code>k</code> is the maximum number of
matches (<code>&lt;= maxmatch</code>) that occurred.  The elements of <code>matches</code>
are row identifiers of <code>y</code> that match, with zeros if fewer than
<code>maxmatch</code> matches are found (blanks if <code>y</code> had row names).
<code>matchCases</code> returns a data frame with variables <code>idcase</code> (id of case
currently being matched), <code>type</code> (factor variable with levels <code>"case"</code>
and <code>"control"</code>), <code>id</code> (id of case if case row, or id of matching
case), and <code>y</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Ming K, Rosenbaum PR (2001): A note on optimal matching with variable
controls using the assignment algorithm.  J Comp Graph Stat
10:455&ndash;463.
</p>
<p>Cepeda MS, Boston R, Farrar JT, Strom BL (2003): Optimal matching with a
variable number of controls vs. a fixed number of controls for a cohort
study: trade-offs.  J Clin Epidemiology 56:230-237.
Note: These papers were not used for the functions here but
probably should have been.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+scale">scale</a></code>, <code><a href="base.html#topic+apply">apply</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rbind(c(.1, .2),c(.11, .22), c(.3, .4), c(.31, .41), c(.32, 5))
x &lt;- rbind(c(.09,.21), c(.29,.39))
y
x
w &lt;- find.matches(x, y, maxmatch=5, tol=c(.05,.05))


set.seed(111)       # so can replicate results
x &lt;- matrix(runif(500), ncol=2)
y &lt;- matrix(runif(2000), ncol=2)
w &lt;- find.matches(x, y, maxmatch=5, tol=c(.02,.03))
w$matches[1:5,]
w$distance[1:5,]
# Find first x with 3 or more y-matches
num.match &lt;- apply(w$matches, 1, function(x)sum(x &gt; 0))
j &lt;- ((1:length(num.match))[num.match &gt; 2])[1]
x[j,]
y[w$matches[j,],]


summary(w)


# For many applications would do something like this:
# attach(df1)
# x &lt;- cbind(age, sex) # Just do as.matrix(df1) if df1 has no factor objects
# attach(df2)
# y &lt;- cbind(age, sex)
# mat &lt;- find.matches(x, y, tol=c(5,0)) # exact match on sex, 5y on age


# Demonstrate matchCases
xcase     &lt;- c(1,3,5,12)
xcontrol  &lt;- 1:6
idcase    &lt;- c('A','B','C','D')
idcontrol &lt;- c('a','b','c','d','e','f')
ycase     &lt;- c(11,33,55,122)
ycontrol  &lt;- c(11,22,33,44,55,66)
matchCases(xcase, ycase, idcase,
           xcontrol, ycontrol, idcontrol, tol=1)


# If y is a binary response variable, the following code
# will produce a Mantel-Haenszel summary odds ratio that 
# utilizes the matching.
# Standard variance formula will not work here because
# a control will match more than one case
# WARNING: The M-H procedure exemplified here is suspect 
# because of the small strata and widely varying number
# of controls per case.


x    &lt;- c(1, 2, 3, 3, 3, 6, 7, 12,  1, 1:7)
y    &lt;- c(0, 0, 0, 1, 0, 1, 1,  1,  1, 0, 0, 0, 0, 1, 1, 1)
case &lt;- c(rep(TRUE, 8), rep(FALSE, 8))
id   &lt;- 1:length(x)


m &lt;- matchCases(x[case],  y[case],  id[case],
                x[!case], y[!case], id[!case], tol=1)
iscase &lt;- m$type=='case'
# Note: the first tapply on insures that event indicators are
# sorted by case id.  The second actually does something.
event.case    &lt;- tapply(m$y[iscase],  m$idcase[iscase],  sum)
event.control &lt;- tapply(m$y[!iscase], m$idcase[!iscase], sum)
n.control     &lt;- tapply(!iscase,      m$idcase,          sum)
n             &lt;- tapply(m$y,          m$idcase,          length)
or &lt;- sum(event.case * (n.control - event.control) / n) /
      sum(event.control * (1 - event.case) / n)
or


# Bootstrap this estimator by sampling with replacement from
# subjects.  Assumes id is unique when combine cases+controls
# (id was constructed this way above).  The following algorithms
# puts all sampled controls back with the cases to whom they were
# originally matched.


ids &lt;- unique(m$id)
idgroups &lt;- split(1:nrow(m), m$id)
B   &lt;- 50   # in practice use many more
ors &lt;- numeric(B)
# Function to order w by ids, leaving unassigned elements zero
align &lt;- function(ids, w) {
  z &lt;- structure(rep(0, length(ids)), names=ids)
  z[names(w)] &lt;- w
  z
}
for(i in 1:B) {
  j &lt;- sample(ids, replace=TRUE)
  obs &lt;- unlist(idgroups[j])
  u &lt;- m[obs,]
  iscase &lt;- u$type=='case'
  n.case &lt;- align(ids, tapply(u$type, u$idcase, 
                              function(v)sum(v=='case')))
  n.control &lt;- align(ids, tapply(u$type, u$idcase,
                                 function(v)sum(v=='control')))
  event.case &lt;- align(ids, tapply(u$y[iscase],  u$idcase[iscase],  sum))
  event.control &lt;- align(ids, tapply(u$y[!iscase], u$idcase[!iscase], sum))
  n &lt;- n.case + n.control
  # Remove sets having 0 cases or 0 controls in resample
  s             &lt;- n.case &gt; 0 &amp; n.control &gt; 0
  denom &lt;- sum(event.control[s] * (n.case[s] - event.case[s]) / n[s])
  or &lt;- if(denom==0) NA else 
   sum(event.case[s] * (n.control[s] - event.control[s]) / n[s]) / denom
  ors[i] &lt;- or
}
describe(ors)
</code></pre>

<hr>
<h2 id='first.word'>First Word in a String or Expression</h2><span id='topic+first.word'></span>

<h3>Description</h3>

<p><code>first.word</code> finds the first word in an expression.  A word is defined by
unlisting the elements of the expression found by the S parser and then
accepting any elements whose first character is either a letter or period.
The principal intended use is for the automatic generation of temporary
file names where it is important to exclude special characters from
the file name. For Microsoft Windows, periods in names are deleted and
only up to the first 8 characters of the word is returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>first.word(x, i=1, expr=substitute(x))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="first.word_+3A_x">x</code></td>
<td>

<p>any scalar character string
</p>
</td></tr>
<tr><td><code id="first.word_+3A_i">i</code></td>
<td>

<p>word number, default value = 1.  Used when the second or <code>i</code>th word is
wanted.  Currently only the <code>i=1</code> case is implemented.
</p>
</td></tr>
<tr><td><code id="first.word_+3A_expr">expr</code></td>
<td>

<p>any S object of mode <code>expression</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character string
</p>


<h3>Author(s)</h3>

<p>Frank E. Harrell, Jr.,
<br />
Department of Biostatistics,
<br />
Vanderbilt University,
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>
<p>Richard M. Heiberger,
<br />
Department of Statistics,
<br />
Temple University, Philadelphia, PA.
<br />
<a href="mailto:rmh@temple.edu">rmh@temple.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>first.word(expr=expression(y ~ x + log(w)))
</code></pre>

<hr>
<h2 id='format.df'>
Format a Data Frame or Matrix for LaTeX or HTML
</h2><span id='topic+format.df'></span>

<h3>Description</h3>

<p><code>format.df</code> does appropriate rounding and decimal alignment, and outputs
a character matrix containing the formatted data.  If <code>x</code> is a
<code>data.frame</code>, then do each component separately.
If <code>x</code> is a matrix, but not a data.frame, make it a data.frame
with individual components for the columns.
If a component <code>x$x</code> is a matrix, then do all columns the same.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>format.df(x, digits, dec=NULL, rdec=NULL, cdec=NULL,
          numeric.dollar=!dcolumn, na.blank=FALSE, na.dot=FALSE,
          blank.dot=FALSE, col.just=NULL, cdot=FALSE,
          dcolumn=FALSE, matrix.sep=' ', scientific=c(-4,4),
          math.row.names=FALSE, already.math.row.names=FALSE,
          math.col.names=FALSE, already.math.col.names=FALSE,
          double.slash=FALSE, format.Date="%m/%d/%Y",
          format.POSIXt="%m/%d/%Y %H:%M:%OS", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="format.df_+3A_x">x</code></td>
<td>

<p>a matrix (usually numeric) or data frame
</p>
</td></tr>
<tr><td><code id="format.df_+3A_digits">digits</code></td>
<td>

<p>causes all values in the table to be formatted to <code>digits</code> significant
digits.  <code>dec</code> is usually preferred.
</p>
</td></tr>
<tr><td><code id="format.df_+3A_dec">dec</code></td>
<td>

<p>If <code>dec</code> is a scalar, all elements of the matrix will be rounded
to <code>dec</code> 
decimal places to the right of the decimal. <code>dec</code> can also be a matrix 
whose elements correspond to <code>x</code>, for customized rounding of each element.
A matrix <code>dec</code> must have number of columns equal to number of columns
of input <code>x</code>.
A scalar <code>dec</code> is expanded to a vector <code>cdec</code> with number of
items equal to number of columns of input <code>x</code>.
</p>
</td></tr>
<tr><td><code id="format.df_+3A_rdec">rdec</code></td>
<td>

<p>a vector specifying the number of decimal places to the right for each row 
(<code>cdec</code> is more commonly used than <code>rdec</code>)
A vector <code>rdec</code> must have number of items equal to number of rows of input <code>x</code>.
<code>rdec</code> is expanded to matrix <code>dec</code>.
</p>
</td></tr>
<tr><td><code id="format.df_+3A_cdec">cdec</code></td>
<td>

<p>a vector specifying the number of decimal places for each column.
The vector must have number of items equal to number of columns or components
of input x.
</p>
</td></tr>
<tr><td><code id="format.df_+3A_cdot">cdot</code></td>
<td>

<p>Set to <code>TRUE</code> to use centered dots rather than ordinary periods in numbers.
The output uses a syntax appropriate for <code>latex</code>.
</p>
</td></tr>
<tr><td><code id="format.df_+3A_na.blank">na.blank</code></td>
<td>

<p>Set to <code>TRUE</code> to use blanks rather than <code>NA</code> for missing values.
This usually looks better in <code>latex</code>.
</p>
</td></tr>
<tr><td><code id="format.df_+3A_dcolumn">dcolumn</code></td>
<td>

<p>Set to <code>TRUE</code> to use David Carlisle's dcolumn style for
decimal alignment in <code>latex</code>.
Default is <code>FALSE</code>. You will probably want to
use <code>dcolumn</code> if you use <code>rdec</code>, as a column may then contain varying
number of places to the right of the decimal. <code>dcolumn</code> can line up
all such numbers on the decimal point, with integer values right
justified at the decimal point location of numbers that actually
contain decimal places.  When you use <code>dcolumn = TRUE</code>, 
<code>numeric.dollar</code> is set by default to <code>FALSE</code>.  When you
use <code>dcolumn = TRUE</code>, the
object attribute <code>"style"</code> set to &lsquo;<span class="samp">&#8288;dcolumn&#8288;</span>&rsquo; as the
<code>latex</code> <code>usepackage</code> must reference <code>[dcolumn]</code>.
The three files &lsquo;<span class="file">dcolumn.sty</span>&rsquo;, &lsquo;<span class="file">newarray.sty</span>&rsquo;, and
&lsquo;<span class="file">array.sty</span>&rsquo; will 
need to be in a directory in your <span class="env">TEXINPUTS</span> path.
When you use <code>dcolumn=TRUE</code>, <code>numeric.dollar</code> should be set to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="format.df_+3A_numeric.dollar">numeric.dollar</code></td>
<td>

<p>logical, default <code>!dcolumn</code>.  Set to <code>TRUE</code> to place dollar
signs around numeric values when <code>dcolumn = FALSE</code>.  This 
assures that <code>latex</code> will use minus signs rather than hyphens to indicate
negative numbers.  Set to <code>FALSE</code> when <code>dcolumn = TRUE</code>, as
<code>dcolumn.sty</code> automatically uses minus signs.
</p>
</td></tr>
<tr><td><code id="format.df_+3A_math.row.names">math.row.names</code></td>
<td>

<p>logical, set true to place dollar signs around the row names.
</p>
</td></tr>
<tr><td><code id="format.df_+3A_already.math.row.names">already.math.row.names</code></td>
<td>
<p>set to <code>TRUE</code> to prevent any math
mode changes to row names</p>
</td></tr>
<tr><td><code id="format.df_+3A_math.col.names">math.col.names</code></td>
<td>

<p>logical, set true to place dollar signs around the column names.
</p>
</td></tr>
<tr><td><code id="format.df_+3A_already.math.col.names">already.math.col.names</code></td>
<td>
<p>set to <code>TRUE</code> to prevent any math
mode changes to column names</p>
</td></tr>
<tr><td><code id="format.df_+3A_na.dot">na.dot</code></td>
<td>

<p>Set to <code>TRUE</code> to use periods rather than <code>NA</code> for missing
numeric values. 
This works with the <abbr><span class="acronym">SAS</span></abbr> convention that periods indicate missing values.
</p>
</td></tr>
<tr><td><code id="format.df_+3A_blank.dot">blank.dot</code></td>
<td>

<p>Set to <code>TRUE</code> to use periods rather than blanks for missing character values.
This works with the <abbr><span class="acronym">SAS</span></abbr> convention that periods indicate missing values.
</p>
</td></tr>
<tr><td><code id="format.df_+3A_col.just">col.just</code></td>
<td>

<p>Input vector <code>col.just</code> must have number of columns equal to
number of columns of the output matrix.  When <code>NULL</code>, the
default, the <code>col.just</code> attribute of the result is set to
&lsquo;<span class="samp">&#8288;l&#8288;</span>&rsquo; for character columns and to &lsquo;<span class="samp">&#8288;r&#8288;</span>&rsquo; for numeric
columns.  The user can override the default by an argument vector
whose length is equal to the number of columns of the result matrix.
When <code>format.df</code> is called by <code>latex.default</code>, the
<code>col.just</code> is used as the <code>cols</code> argument to the
<code>tabular</code> environment and the letters &lsquo;<span class="samp">&#8288;l&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;r&#8288;</span>&rsquo;,
and &lsquo;<span class="samp">&#8288;c&#8288;</span>&rsquo; are valid values.  When <code>format.df</code> is called by
<abbr><span class="acronym">SAS</span></abbr>, the <code>col.just</code> is used to determine whether a
&lsquo;<span class="samp">&#8288;\$&#8288;</span>&rsquo; is needed on the &lsquo;<span class="samp">&#8288;input&#8288;</span>&rsquo; line of the &lsquo;<span class="file">sysin</span>&rsquo; file,
and the letters &lsquo;<span class="samp">&#8288;l&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;r&#8288;</span>&rsquo; are valid values.  You can
pass specifications other than <code>l,r,c</code> in <code>col.just</code>,
e.g., <code>"p{3in}"</code> to get paragraph-formatted columns from
<code>latex()</code>. 
</p>
</td></tr>
<tr><td><code id="format.df_+3A_matrix.sep">matrix.sep</code></td>
<td>

<p>When <code>x</code> is a data frame containing a matrix, so that new column names
are constructed from the name of the matrix object and the names of
the individual columns of the matrix, <code>matrix.sep</code> specifies the
character to use to separate object names from individual column
names.
</p>
</td></tr>
<tr><td><code id="format.df_+3A_scientific">scientific</code></td>
<td>

<p>specifies ranges of exponents (or a logical vector) specifying values
not to convert to scientific notation.  See <code>format.default</code> for details.
</p>
</td></tr>
<tr><td><code id="format.df_+3A_double.slash">double.slash</code></td>
<td>

<p>should escaping backslashes be themselves escaped.
</p>
</td></tr>
<tr><td><code id="format.df_+3A_format.date">format.Date</code></td>
<td>

<p>String used to format objects of the Date class.
</p>
</td></tr>
<tr><td><code id="format.df_+3A_format.posixt">format.POSIXt</code></td>
<td>

<p>String used to format objects of the POSIXt class.
</p>
</td></tr>
<tr><td><code id="format.df_+3A_...">...</code></td>
<td>

<p>other arguments are accepted and passed to <code>format.default</code>.
For <code>latexVerbatim</code> these arguments are passed to the
<code>print</code> function.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character matrix with character images of properly rounded <code>x</code>.
Matrix components of input <code>x</code> are now just sets of columns of
character matrix.
Object attribute<code>"col.just"</code> repeats the value of the argument <code>col.just</code> when provided,
otherwise, it includes the recommended justification for columns of output.
See the discussion of the argument <code>col.just</code>.
The default justification is &lsquo;<span class="samp">&#8288;l&#8288;</span>&rsquo; for characters and factors,
&lsquo;<span class="samp">&#8288;r&#8288;</span>&rsquo; for numeric.
When <code>dcolumn==TRUE</code>, numerics will have &lsquo;<span class="samp">&#8288;.&#8288;</span>&rsquo; as the justification character.
</p>


<h3>Author(s)</h3>

<p>Frank E. Harrell, Jr.,
<br />
Department of Biostatistics,
<br />
Vanderbilt University,
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>
<p>Richard M. Heiberger,
<br />
Department of Statistics,
<br />
Temple University, Philadelphia, PA.
<br />
<a href="mailto:rmh@temple.edu">rmh@temple.edu</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+latex">latex</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
x &lt;- data.frame(a=1:2, b=3:4)
x$m &lt;- 10000*matrix(5:8,nrow=2)
names(x)
dim(x)
x
format.df(x, big.mark=",")
dim(format.df(x))

## End(Not run)
</code></pre>

<hr>
<h2 id='format.pval'>Format P Values</h2><span id='topic+format.pval'></span>

<h3>Description</h3>

<p><code>format.pval</code> is intended for formatting p-values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>format.pval(x, pv=x, digits = max(1, .Options$digits - 2),
            eps = .Machine$double.eps, na.form = "NA", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="format.pval_+3A_pv">pv</code></td>
<td>
<p>a numeric vector.</p>
</td></tr>
<tr><td><code id="format.pval_+3A_x">x</code></td>
<td>
<p>argument for method compliance.</p>
</td></tr>
<tr><td><code id="format.pval_+3A_digits">digits</code></td>
<td>
<p>how many significant digits are to be used.</p>
</td></tr>
<tr><td><code id="format.pval_+3A_eps">eps</code></td>
<td>
<p>a numerical tolerance: see Details.</p>
</td></tr>
<tr><td><code id="format.pval_+3A_na.form">na.form</code></td>
<td>
<p>character representation of <code>NA</code>s.</p>
</td></tr>
<tr><td><code id="format.pval_+3A_...">...</code></td>
<td>

<p>arguments passed to <code><a href="base.html#topic+format">format</a></code> in the <code>format.pval</code>
function body.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>format.pval</code> is mainly an auxiliary function for
<code><a href="stats.html#topic+print.summary.lm">print.summary.lm</a></code> etc., and does separate formatting for
fixed, floating point and very small values; those less than
<code>eps</code> are formatted as &ldquo;&lsquo;<span class="samp">&#8288;&lt; [eps]&#8288;</span>&rsquo;&rdquo; (where
&ldquo;&lsquo;<span class="samp">&#8288;[eps]&#8288;</span>&rsquo;&rdquo; stands for <code>format(eps, digits)</code>).
</p>


<h3>Value</h3>

<p>A character vector.
</p>


<h3>Note</h3>

<p>This is the base <code><a href="base.html#topic+format.pval">format.pval</a></code> function with the
ablitiy to pass the <code>nsmall</code> argument to <code><a href="base.html#topic+format">format</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>format.pval(c(runif(5), pi^-100, NA))
format.pval(c(0.1, 0.0001, 1e-27))
format.pval(c(0.1, 1e-27), nsmall=3)
</code></pre>

<hr>
<h2 id='gbayes'>
Gaussian Bayesian Posterior and Predictive Distributions
</h2><span id='topic+gbayes'></span><span id='topic+plot.gbayes'></span><span id='topic+gbayes2'></span><span id='topic+gbayesMixPredNoData'></span><span id='topic+gbayesMixPost'></span><span id='topic+gbayesMixPowerNP'></span><span id='topic+gbayes1PowerNP'></span>

<h3>Description</h3>

<p><code>gbayes</code> derives the (Gaussian) posterior and optionally the predictive
distribution when both the prior and the likelihood are Gaussian, and
when the statistic of interest comes from a 2-sample problem.
This function is especially useful in obtaining the expected power of
a statistical test, averaging over the distribution of the population
effect parameter (e.g., log hazard ratio) that is obtained using
pilot data.  <code>gbayes</code> is also useful for summarizing studies for
which the statistic of interest is approximately Gaussian with
known variance.  An example is given for comparing two proportions
using the angular transformation, for which the variance is
independent of unknown parameters except for very extreme probabilities.
A <code>plot</code> method is also given.  This plots the prior, posterior, and
predictive distributions on a single graph using a nice default for
the x-axis limits and using the <code>labcurve</code> function for automatic
labeling of the curves.
</p>
<p><code>gbayes2</code> uses the method of Spiegelhalter and Freedman (1986) to compute the
probability of correctly concluding that a new treatment is superior
to a control.  By this we mean that a 1-<code>alpha</code> normal
theory-based confidence interval for the new minus old treatment
effect lies wholly to the right of <code>delta.w</code>, where <code>delta.w</code> is the
minimally worthwhile treatment effect (which can be zero to be
consistent with ordinary null hypothesis testing, a method not always
making sense).  This kind of power function is averaged over a prior
distribution for the unknown treatment effect.  This procedure is
applicable to the situation where a prior distribution is not to be
used in constructing the test statistic or confidence interval, but is
only used for specifying the distribution of <code>delta</code>, the parameter of
interest.
</p>
<p>Even though <code>gbayes2</code>
assumes that the test statistic has a normal distribution with known
variance (which is strongly a function of the sample size in the two
treatment groups), the prior distribution function can be completely
general.  Instead of using a step-function for the prior distribution
as Spiegelhalter and Freedman used in their appendix, <code>gbayes2</code> uses
the built-in <code>integrate</code> function for numerical integration.
<code>gbayes2</code> also allows the variance of the test statistic to be general
as long as it is evaluated by the user.  The conditional power given the
parameter of interest <code>delta</code> is <code>1 - pnorm((delta.w - delta)/sd + z)</code>, where z
is the normal critical value corresponding to 1 - <code>alpha</code>/2.
</p>
<p><code>gbayesMixPredNoData</code> derives the predictive distribution of a
statistic that is Gaussian given <code>delta</code> when no data have yet been
observed and when the prior is a mixture of two Gaussians.
</p>
<p><code>gbayesMixPost</code> derives the posterior density, cdf, or posterior
mean of <code>delta</code> given 
the statistic <code>x</code>, when the prior for <code>delta</code> is a mixture of two
Gaussians and when <code>x</code> is Gaussian given <code>delta</code>.
</p>
<p><code>gbayesMixPowerNP</code> computes the power for a test for <code>delta</code> &gt; <code>delta.w</code>
for the case where (1) a Gaussian prior or mixture of two Gaussian priors
is used as the prior distribution, (2) this prior is used in forming
the statistical test or credible interval, (3) no prior is used for
the distribution of <code>delta</code> for computing power but instead a fixed
single <code>delta</code> is given (as in traditional frequentist hypothesis
tests), and (4) the test statistic has a Gaussian likelihood with
known variance (and mean equal to the specified <code>delta</code>).
<code>gbayesMixPowerNP</code> is handy where you want to use an earlier study in
testing for treatment effects in a new study, but you want to mix with
this prior a non-informative prior.  The mixing probability <code>mix</code> can
be thought of as the &quot;applicability&quot; of the previous study.  As with
<code>gbayes2</code>, power here means the probability that the new study will
yield a left credible interval that is to the right of <code>delta.w</code>.
<code>gbayes1PowerNP</code> is a special case of <code>gbayesMixPowerNP</code> when the
prior is a single Gaussian.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbayes(mean.prior, var.prior, m1, m2, stat, var.stat, 
       n1, n2, cut.prior, cut.prob.prior=0.025)

## S3 method for class 'gbayes'
plot(x, xlim, ylim, name.stat='z', ...)

gbayes2(sd, prior, delta.w=0, alpha=0.05, upper=Inf, prior.aux)

gbayesMixPredNoData(mix=NA, d0=NA, v0=NA, d1=NA, v1=NA,
                    what=c('density','cdf'))

gbayesMixPost(x=NA, v=NA, mix=1, d0=NA, v0=NA, d1=NA, v1=NA,
              what=c('density','cdf','postmean'))

gbayesMixPowerNP(pcdf, delta, v, delta.w=0, mix, interval,
                 nsim=0, alpha=0.05)

gbayes1PowerNP(d0, v0, delta, v, delta.w=0, alpha=0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gbayes_+3A_mean.prior">mean.prior</code></td>
<td>

<p>mean of the prior distribution
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_cut.prior">cut.prior</code>, <code id="gbayes_+3A_cut.prob.prior">cut.prob.prior</code>, <code id="gbayes_+3A_var.prior">var.prior</code></td>
<td>

<p>variance of the prior.  Use a large number such as 10000 to effectively
use a flat (noninformative) prior.  Sometimes it is useful to compute
the variance so that the prior probability that <code>stat</code> is greater than
some impressive value <code>u</code> is only <code>alpha</code>.  The correct
<code>var.prior</code> to use is then <code>((u-mean.prior)/qnorm(1-alpha))^2</code>.
You can specify <code>cut.prior=u</code> and <code>cut.prob.prior=alpha</code> (whose default is 0.025)
in place of <code>var.prior</code> to have <code>gbayes</code> compute the prior variance in this
manner. 
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_m1">m1</code></td>
<td>

<p>sample size in group 1
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_m2">m2</code></td>
<td>

<p>sample size in group 2
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_stat">stat</code></td>
<td>

<p>statistic comparing groups 1 and 2, e.g., log hazard ratio, difference
in means, difference in angular transformations of proportions
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_var.stat">var.stat</code></td>
<td>

<p>variance of <code>stat</code>, assumed to be known.  <code>var.stat</code> should either
be a constant (allowed if <code>n1</code> is not specified), or a function of
two arguments which specify the sample sizes in groups 1 and 2. 
Calculations will be approximate when the variance is estimated from the data.
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_x">x</code></td>
<td>

<p>an object returned by <code>gbayes</code> or the value of the statistic which
is an estimator of delta, the parameter of interest
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_sd">sd</code></td>
<td>

<p>the standard deviation of the treatment effect
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_prior">prior</code></td>
<td>

<p>a function of possibly a vector of unknown treatment effects,
returning the prior density at those values
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_pcdf">pcdf</code></td>
<td>

<p>a function computing the posterior CDF of the treatment effect
<code>delta</code>, such as a function created by <code>gbayesMixPost</code> with
<code>what="cdf"</code>.
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_delta">delta</code></td>
<td>

<p>a true unknown single treatment effect to detect
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_v">v</code></td>
<td>

<p>the variance of the statistic <code>x</code>, e.g., <code>s^2 * (1/n1 + 1/n2)</code>.
Neither <code>x</code> nor <code>v</code> need to be defined to
<code>gbayesMixPost</code>, as they can be defined at run time to the function
created by <code>gbayesMixPost</code>.
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_n1">n1</code></td>
<td>

<p>number of future observations in group 1, for obtaining a predictive
distribution
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_n2">n2</code></td>
<td>

<p>number of future observations in group 2
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_xlim">xlim</code></td>
<td>

<p>vector of 2 x-axis limits.  Default is the mean of the posterior plus or
minus 6 standard deviations of the posterior.
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_ylim">ylim</code></td>
<td>

<p>vector of 2 y-axis limits.  Default is the range over combined prior and 
posterior densities.
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_name.stat">name.stat</code></td>
<td>

<p>label for x-axis.  Default is <code>"z"</code>.
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_...">...</code></td>
<td>

<p>optional arguments passed to <code>labcurve</code> from <code>plot.gbayes</code>
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_delta.w">delta.w</code></td>
<td>

<p>the minimum worthwhile treatment difference to detech.  The default is
zero for a plain uninteristing null hypothesis.
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_alpha">alpha</code></td>
<td>

<p>type I error, or more accurately one minus the confidence level for a
two-sided confidence limit for the treatment effect
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_upper">upper</code></td>
<td>

<p>upper limit of integration over the prior distribution multiplied by
the normal likelihood for the treatment effect statistic.  Default is
infinity.
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_prior.aux">prior.aux</code></td>
<td>

<p>argument to pass to <code>prior</code> from <code>integrate</code> through <code>gbayes2</code>.
Inside of <code>power</code> the argument must be named <code>prior.aux</code> if it
exists.  You can pass multiple parameters by passing <code>prior.aux</code> as a
list and pulling off elements of the list inside <code>prior</code>.  This setup
was used because of difficulties in passing <code>...</code> arguments through
<code>integrate</code> for some situations.
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_mix">mix</code></td>
<td>

<p>mixing probability or weight for the Gaussian prior having mean <code>d0</code>
and variance <code>v0</code>.  <code>mix</code> must be between 0 and 1, inclusive.
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_d0">d0</code></td>
<td>

<p>mean of the first Gaussian distribution (only Gaussian for
<code>gbayes1PowerNP</code> and is a required argument)
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_v0">v0</code></td>
<td>

<p>variance of the first Gaussian (only Gaussian for
<code>gbayes1PowerNP</code> and is a required argument)
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_d1">d1</code></td>
<td>

<p>mean of the second Gaussian (if <code>mix</code> &lt; 1)
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_v1">v1</code></td>
<td>

<p>variance of the second Gaussian (if <code>mix</code> &lt; 1).  Any of these last 5
arguments can be omitted to <code>gbayesMixPredNoData</code> as they can be
provided at run time to the function created by <code>gbayesMixPredNoData</code>.
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_what">what</code></td>
<td>

<p>specifies whether the predictive density or the CDF is to be
computed.  Default is <code>"density"</code>.
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_interval">interval</code></td>
<td>

<p>a 2-vector containing the lower and upper limit for possible values of
the test statistic <code>x</code> that would result in a left credible interval
exceeding <code>delta.w</code> with probability 1-<code>alpha</code>/2
</p>
</td></tr>
<tr><td><code id="gbayes_+3A_nsim">nsim</code></td>
<td>

<p>defaults to zero, causing <code>gbayesMixPowerNP</code> to solve numerically for the
critical value of <code>x</code>, then to compute the power accordingly.  Specify
a nonzero number such as 20000 for <code>nsim</code> to instead have the function
estimate power by simulation.  In this case 0.95 confidence limits on
the estimated power are also computed.  This approach is sometimes
necessary if <code>uniroot</code> can't solve the equation for the critical value.
</p>
</td></tr></table>


<h3>Value</h3>

<p><code>gbayes</code> returns a list of class <code>"gbayes"</code> containing the following
names elements: <code>mean.prior</code>,<code>var.prior</code>,<code>mean.post</code>, <code>var.post</code>, and
if <code>n1</code> is specified, <code>mean.pred</code> and <code>var.pred</code>.  Note that
<code>mean.pred</code> is  identical to <code>mean.post</code>.  <code>gbayes2</code> returns a single
number which is the probability of correctly rejecting the null
hypothesis in favor of the new treatment.  <code>gbayesMixPredNoData</code>
returns a function that can be used to evaluate the predictive density
or cumulative distribution.  <code>gbayesMixPost</code> returns a function that
can be used to evaluate the posterior density or cdf.  <code>gbayesMixPowerNP</code>
returns a vector containing two values if <code>nsim</code> = 0.  The first value is the
critical value for the test statistic that will make the left credible
interval &gt; <code>delta.w</code>, and the second value is the power.  If <code>nsim</code> &gt; 0,
it returns the power estimate and confidence limits for it if <code>nsim</code> &gt;
0.  The examples show how to use these functions.  
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University School of Medicine
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Spiegelhalter DJ, Freedman LS, Parmar MKB (1994): Bayesian approaches to
randomized trials.  JRSS A 157:357&ndash;416.  Results for <code>gbayes</code> are derived from
Equations 1, 2, 3, and 6.
</p>
<p>Spiegelhalter DJ, Freedman LS (1986): A predictive approach to
selecting the size of a clinical trial, based on subjective clinical
opinion.  Stat in Med 5:1&ndash;13.
</p>
<p>Joseph, Lawrence and Belisle, Patrick (1997): Bayesian sample size
determination for normal means and differences between normal means.
The Statistician 46:209&ndash;226.
</p>
<p>Grouin, JM, Coste M, Bunouf P, Lecoutre B (2007): Bayesian sample size
determination in non-sequential clinical trials: Statistical aspects and
some regulatory considerations.  Stat in Med 26:4914&ndash;4924.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbayesSeqSim">gbayesSeqSim</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># Compare 2 proportions using the var stabilizing transformation
# arcsin(sqrt((x+3/8)/(n+3/4))) (Anscombe), which has variance 
# 1/[4(n+.5)]


m1 &lt;- 100;     m2 &lt;- 150
deaths1 &lt;- 10; deaths2 &lt;- 30


f &lt;- function(events,n) asin(sqrt((events+3/8)/(n+3/4)))
stat &lt;- f(deaths1,m1) - f(deaths2,m2)
var.stat &lt;- function(m1, m2) 1/4/(m1+.5) + 1/4/(m2+.5)
cat("Test statistic:",format(stat),"  s.d.:",
    format(sqrt(var.stat(m1,m2))), "\n")
#Use unbiased prior with variance 1000 (almost flat)
b &lt;- gbayes(0, 1000, m1, m2, stat, var.stat, 2*m1, 2*m2)
print(b)
plot(b)
#To get posterior Prob[parameter &gt; w] use 
# 1-pnorm(w, b$mean.post, sqrt(b$var.post))


#If g(effect, n1, n2) is the power function to
#detect an effect of 'effect' with samples size for groups 1 and 2
#of n1,n2, estimate the expected power by getting 1000 random
#draws from the posterior distribution, computing power for
#each value of the population effect, and averaging the 1000 powers
#This code assumes that g will accept vector-valued 'effect'
#For the 2-sample proportion problem just addressed, 'effect'
#could be taken approximately as the change in the arcsin of
#the square root of the probability of the event


g &lt;- function(effect, n1, n2, alpha=.05) {
  sd &lt;- sqrt(var.stat(n1,n2))
  z &lt;- qnorm(1 - alpha/2)
  effect &lt;- abs(effect)
  1 - pnorm(z - effect/sd) + pnorm(-z - effect/sd)
}


effects &lt;- rnorm(1000, b$mean.post, sqrt(b$var.post))
powers &lt;- g(effects, 500, 500)
hist(powers, nclass=35, xlab='Power')
describe(powers)




# gbayes2 examples
# First consider a study with a binary response where the
# sample size is n1=500 in the new treatment arm and n2=300
# in the control arm.  The parameter of interest is the 
# treated:control log odds ratio, which has variance
# 1/[n1 p1 (1-p1)] + 1/[n2 p2 (1-p2)].  This is not
# really constant so we average the variance over plausible
# values of the probabilities of response p1 and p2.  We
# think that these are between .4 and .6 and we take a 
# further short cut


v &lt;- function(n1, n2, p1, p2) 1/(n1*p1*(1-p1)) + 1/(n2*p2*(1-p2))
n1 &lt;- 500; n2 &lt;- 300
ps &lt;- seq(.4, .6, length=100)
vguess &lt;- quantile(v(n1, n2, ps, ps), .75)
vguess
#        75% 
# 0.02183459


# The minimally interesting treatment effect is an odds ratio
# of 1.1.  The prior distribution on the log odds ratio is
# a 50:50 mixture of a vague Gaussian (mean 0, sd 100) and
# an informative prior from a previous study (mean 1, sd 1)


prior &lt;- function(delta) 
  0.5*dnorm(delta, 0, 100)+0.5*dnorm(delta, 1, 1)
deltas &lt;- seq(-5, 5, length=150)
plot(deltas, prior(deltas), type='l')


# Now compute the power, averaged over this prior
gbayes2(sqrt(vguess), prior, log(1.1))
# [1] 0.6133338


# See how much power is lost by ignoring the previous
# study completely


gbayes2(sqrt(vguess), function(delta)dnorm(delta, 0, 100), log(1.1))
# [1] 0.4984588


# What happens to the power if we really don't believe the treatment
# is very effective?  Let's use a prior distribution for the log
# odds ratio that is uniform between log(1.2) and log(1.3).
# Also check the power against a true null hypothesis


prior2 &lt;- function(delta) dunif(delta, log(1.2), log(1.3))
gbayes2(sqrt(vguess), prior2, log(1.1))
# [1] 0.1385113


gbayes2(sqrt(vguess), prior2, 0)
# [1] 0.3264065


# Compare this with the power of a two-sample binomial test to
# detect an odds ratio of 1.25
bpower(.5, odds.ratio=1.25, n1=500, n2=300)
#     Power 
# 0.3307486


# For the original prior, consider a new study with equal
# sample sizes n in the two arms.  Solve for n to get a
# power of 0.9.  For the variance of the log odds ratio
# assume a common p in the center of a range of suspected
# probabilities of response, 0.3.  For this example we
# use a zero null value and the uniform prior above


v   &lt;- function(n) 2/(n*.3*.7)
pow &lt;- function(n) gbayes2(sqrt(v(n)), prior2)
uniroot(function(n) pow(n)-0.9, c(50,10000))$root
# [1] 2119.675
# Check this value
pow(2119.675)
# [1] 0.9


# Get the posterior density when there is a mixture of two priors,
# with mixing probability 0.5.  The first prior is almost
# non-informative (normal with mean 0 and variance 10000) and the
# second has mean 2 and variance 0.3.  The test statistic has a value
# of 3 with variance 0.4.
f &lt;- gbayesMixPost(3, 4, mix=0.5, d0=0, v0=10000, d1=2, v1=0.3)


args(f)


# Plot this density
delta &lt;- seq(-2, 6, length=150)
plot(delta, f(delta), type='l')


# Add to the plot the posterior density that used only
# the almost non-informative prior
lines(delta, f(delta, mix=1), lty=2)


# The same but for an observed statistic of zero
lines(delta, f(delta, mix=1, x=0), lty=3)


# Derive the CDF instead of the density
g &lt;- gbayesMixPost(3, 4, mix=0.5, d0=0, v0=10000, d1=2, v1=0.3,
                   what='cdf')
# Had mix=0 or 1, gbayes1PowerNP could have been used instead
# of gbayesMixPowerNP below


# Compute the power to detect an effect of delta=1 if the variance
# of the test statistic is 0.2
gbayesMixPowerNP(g, 1, 0.2, interval=c(-10,12))


# Do the same thing by simulation
gbayesMixPowerNP(g, 1, 0.2, interval=c(-10,12), nsim=20000)


# Compute by what factor the sample size needs to be larger
# (the variance needs to be smaller) so that the power is 0.9
ratios &lt;- seq(1, 4, length=50)
pow &lt;- single(50)
for(i in 1:50) 
  pow[i] &lt;- gbayesMixPowerNP(g, 1, 0.2/ratios[i], interval=c(-10,12))[2]


# Solve for ratio using reverse linear interpolation
approx(pow, ratios, xout=0.9)$y


# Check this by computing power
gbayesMixPowerNP(g, 1, 0.2/2.1, interval=c(-10,12))
# So the study will have to be 2.1 times as large as earlier thought
</code></pre>

<hr>
<h2 id='gbayesSeqSim'>gbayesSeqSim</h2><span id='topic+gbayesSeqSim'></span>

<h3>Description</h3>

<p>Simulate Bayesian Sequential Treatment Comparisons Using a Gaussian Model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbayesSeqSim(est, asserts)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gbayesSeqSim_+3A_est">est</code></td>
<td>
<p>data frame created by <code>estSeqSim()</code></p>
</td></tr>
<tr><td><code id="gbayesSeqSim_+3A_asserts">asserts</code></td>
<td>
<p>list of lists.  The first element of each list is the user-specified name for each assertion/prior combination, e.g., <code>"efficacy"</code>.  The other elements are, in order, a character string equal to &quot;&lt;&quot;, &quot;&gt;&quot;, or &quot;in&quot;, a parameter value <code>cutoff</code> (for &quot;&lt;&quot; and &quot;&gt;&quot;) or a 2-vector specifying an interval for &quot;in&quot;, and either a prior distribution mean and standard deviation named <code>mu</code> and <code>sigma</code> respectively, or a parameter value (<code>"cutprior"</code>) and tail area <code>"tailprob"</code>.  If the latter is used, <code>mu</code> is assumed to be zero and <code>sigma</code> is solved for such that P(parameter &gt; 'cutprior') = P(parameter &lt; - 'cutprior') = <code>tailprob</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Simulate a sequential trial under a Gaussian model for parameter estimates, and Gaussian priors using simulated estimates and variances returned by <code>estSeqSim</code>.  For each row of the data frame <code>est</code> and for each prior/assertion combination, computes the posterior probability of the assertion.
</p>


<h3>Value</h3>

<p>a data frame with number of rows equal to that of <code>est</code> with a number of new columns equal to the number of assertions added.  The new columns are named <code>p1</code>, <code>p2</code>, <code>p3</code>, ... (posterior probabilities), <code>mean1</code>, <code>mean2</code>, ... (posterior means), and <code>sd1</code>, <code>sd2</code>, ... (posterior standard deviations).  The returned data frame also has an attribute <code>asserts</code> added which is the original <code>asserts</code> augmented with any derived <code>mu</code> and <code>sigma</code> and converted to a data frame, and another attribute <code>alabels</code> which is a named vector used to map <code>p1</code>, <code>p2</code>, ... to the user-provided labels in <code>asserts</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><code>gbayes()</code>, <code>estSeqSim()</code>, <code>simMarkovOrd()</code>, <code>estSeqMarkovOrd()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Simulate Bayesian operating characteristics for an unadjusted
# proportional odds comparison (Wilcoxon test)
# For 100 simulations, 5 looks, 2 true parameter values, and
# 2 assertion/prior combinations, compute the posterior probability
# Use a low-level logistic regression call to speed up simuluations
# Use data.table to compute various summary measures
# Total simulation time: 2s
lfit &lt;- function(x, y) {
f &lt;- rms::lrm.fit(x, y)
  k &lt;- length(coef(f))
  c(coef(f)[k], vcov(f)[k, k])
}
gdat &lt;- function(beta, n1, n2) {
  # Cell probabilities for a 7-category ordinal outcome for the control group
  p &lt;- c(2, 1, 2, 7, 8, 38, 42) / 100

  # Compute cell probabilities for the treated group
  p2 &lt;- pomodm(p=p, odds.ratio=exp(beta))
  y1 &lt;- sample(1 : 7, n1, p,  replace=TRUE)
  y2 &lt;- sample(1 : 7, n2, p2, replace=TRUE)
  list(y1=y1, y2=y2)
}

# Assertion 1: log(OR) &lt; 0 under prior with prior mean 0.1 and sigma 1 on log OR scale
# Assertion 2: OR between 0.9 and 1/0.9 with prior mean 0 and sigma computed so that
# P(OR &gt; 2) = 0.05
asserts &lt;- list(list('Efficacy', '&lt;', 0, mu=0.1, sigma=1),
                list('Similarity', 'in', log(c(0.9, 1/0.9)),
                     cutprior=log(2), tailprob=0.05))

set.seed(1)
est &lt;- estSeqSim(c(0, log(0.7)), looks=c(50, 75, 95, 100, 200),
                   gendat=gdat,
                   fitter=lfit, nsim=100)
z &lt;- gbayesSeqSim(est, asserts)
head(z)
attr(z, 'asserts')

# Compute the proportion of simulations that hit targets (different target posterior
# probabilities for efficacy vs. similarity)

# For the efficacy assessment compute the first look at which the target
# was hit (set to infinity if never hit)
require(data.table)
z &lt;- data.table(z)
u &lt;- z[, .(first=min(p1 &gt; 0.95)), by=.(parameter, sim)]
# Compute the proportion of simulations that ever hit the target and
# that hit it by the 100th subject
u[, .(ever=mean(first &lt; Inf)),  by=.(parameter)]
u[, .(by75=mean(first &lt;= 100)), by=.(parameter)]

## End(Not run)
</code></pre>

<hr>
<h2 id='getabd'>getabd</h2><span id='topic+getabd'></span>

<h3>Description</h3>

<p>Data from The Analysis of Biological Data by Shitlock and Schluter
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getabd(name = "", lowernames = FALSE, allow = "_")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getabd_+3A_name">name</code></td>
<td>
<p>name of dataset to fetch.  Omit to get a data table listing all available datasets.</p>
</td></tr>
<tr><td><code id="getabd_+3A_lowernames">lowernames</code></td>
<td>
<p>set to <code>TRUE</code> to change variable names to lower case</p>
</td></tr>
<tr><td><code id="getabd_+3A_allow">allow</code></td>
<td>
<p>set to <code>NULL</code> to convert underscores in variable names to periods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fetches csv files for exercises in the book
</p>


<h3>Value</h3>

<p>data frame with attributes <code>label</code> and <code>url</code>
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

<hr>
<h2 id='getHdata'>
Download and Install Datasets for <span class="pkg">Hmisc</span>, <span class="pkg">rms</span>, and Statistical
Modeling
</h2><span id='topic+getHdata'></span>

<h3>Description</h3>

<p>This function downloads and makes ready to use datasets from the main
web site for the <span class="pkg">Hmisc</span> and <span class="pkg">rms</span> libraries.  For <span class="rlang"><b>R</b></span>, the
datasets were stored in compressed <code><a href="base.html#topic+save">save</a></code> format and
<code>getHdata</code> makes them available by running <code><a href="base.html#topic+load">load</a></code>
after download.  For S-Plus, the datasets were stored in
<code>data.dump</code> format and are made available by running
<code>data.restore</code> after import.  The dataset is run through the
<code><a href="#topic+cleanup.import">cleanup.import</a></code> function.  Calling <code>getHdata</code> with no
<code>file</code> argument provides a character vector of names of available
datasets that are currently on the web site.  For <span class="rlang"><b>R</b></span>, <span class="rlang"><b>R</b></span>'s default
browser can optionally be launched to view <code style="white-space: pre;">&#8288;html&#8288;</code> files that were
already prepared using the <span class="pkg">Hmisc</span> command
<code>html(contents())</code> or to view &lsquo;<span class="file">.txt</span>&rsquo; or &lsquo;<span class="file">.html</span>&rsquo; data
description files when available.
</p>
<p>If <code>options(localHfiles=TRUE)</code> the scripts are read from local directory
<code>~/web/data/repo</code> instead of from the web server.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getHdata(file, what = c("data", "contents", "description", "all"),
         where="https://hbiostat.org/data/repo")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getHdata_+3A_file">file</code></td>
<td>

<p>an unquoted name of a dataset on the web site, e.g. &lsquo;<span class="samp">&#8288;prostate&#8288;</span>&rsquo;.
Omit <code>file</code> to obtain a list of available datasets.
</p>
</td></tr>
<tr><td><code id="getHdata_+3A_what">what</code></td>
<td>

<p>specify <code>what="contents"</code> to browse the contents (metadata) for
the dataset rather than fetching the data themselves.  Specify
<code>what="description"</code> to browse a data description file if
available.  Specify <code>what="all"</code> to retrieve the data and see
the metadata and description.
</p>
</td></tr>
<tr><td><code id="getHdata_+3A_where">where</code></td>
<td>

<p><abbr><span class="acronym">URL</span></abbr> containing the data and metadata files
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>getHdata()</code> without a <code>file</code> argument returns a character
vector of dataset base names.  When a dataset is downloaded, the data
frame is placed in search position one and is not returned as value of
<code>getHdata</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="utils.html#topic+download.file">download.file</a></code>, <code><a href="#topic+cleanup.import">cleanup.import</a></code>,
<code><a href="foreign.html#topic+read.S">data.restore</a></code>, <code><a href="base.html#topic+load">load</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
getHdata()          # download list of available datasets
getHdata(prostate)  # downloads, load( ) or data.restore( )
                    # runs cleanup.import for S-Plus 6
getHdata(valung, "contents")   # open browser (options(browser="whatever"))
                    # after downloading valung.html
                    # (result of html(contents()))
getHdata(support, "all")  # download and open one browser window
datadensity(support)
attach(support)     # make individual variables available
getHdata(plasma,  "all")  # download and open two browser windows
                          # (description file is available for plasma)

## End(Not run)
</code></pre>

<hr>
<h2 id='getRs'>Interact with github rscripts Project</h2><span id='topic+getRs'></span>

<h3>Description</h3>

<p>The github rscripts project at
<a href="https://github.com/harrelfe/rscripts">https://github.com/harrelfe/rscripts</a> contains R scripts that are
primarily analysis templates for teaching with RStudio.  This function
allows the user to print an organized list of available scripts, to
download a script and <code>source()</code> it into the current session (the
default), to
download a script and load it into an RStudio script editor window, to
list scripts whose major category contains a given string (ignoring
case), or to list all major and minor categories.  If
<code>options(localHfiles=TRUE)</code> the scripts are read from local directory
<code>~/R/rscripts</code> instead of from github.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getRs(file=NULL, guser='harrelfe', grepo='rscripts', gdir='raw/master',
      dir=NULL, browse=c('local', 'browser'), cats=FALSE,
      put=c('source', 'rstudio'))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getRs_+3A_file">file</code></td>
<td>
<p>a character string containing a script file name.
Omit <code>file</code> to obtain a list of available scripts with major
and minor categories.</p>
</td></tr>
<tr><td><code id="getRs_+3A_guser">guser</code></td>
<td>
<p>GitHub user name, default is <code>'harrelfe'</code></p>
</td></tr>
<tr><td><code id="getRs_+3A_grepo">grepo</code></td>
<td>
<p>Github repository name, default is <code>'rscripts'</code></p>
</td></tr>
<tr><td><code id="getRs_+3A_gdir">gdir</code></td>
<td>
<p>Github directory under which to find retrievable files</p>
</td></tr>
<tr><td><code id="getRs_+3A_dir">dir</code></td>
<td>
<p>directory under <code>grepo</code> in which to find files</p>
</td></tr>
<tr><td><code id="getRs_+3A_browse">browse</code></td>
<td>
<p>When showing the rscripts contents directory, the
default is to list in tabular form in the console.  Specify
<code>browse='browser'</code> to open the online contents in a web
browser.</p>
</td></tr>
<tr><td><code id="getRs_+3A_cats">cats</code></td>
<td>
<p>Leave at the default (<code>FALSE</code>) to list whole contents
or download a script.  Specify <code>cats=TRUE</code> to list major and
minor categories available.  Specify a character string to list
all scripts whose major category contains the string (ignoring
case).</p>
</td></tr>
<tr><td><code id="getRs_+3A_put">put</code></td>
<td>
<p>Leave at the default (<code>'source'</code>) to <code>source()</code> the file.  This is useful when the file just defines a function you want to use in the session. Use load <code>put='rstudio'</code> to load the file into the RStudio script editor window using the <code>rstudioapi</code> <code>navigateToFile</code> function.  If RStudio is not running, <code>file.edit()</code> is used instead.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data frame or list, depending on arguments</p>


<h3>Author(s)</h3>

<p>Frank Harrell and Cole Beck</p>


<h3>See Also</h3>

<p><code><a href="utils.html#topic+download.file">download.file</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
getRs()             # list available scripts
scripts &lt;- getRs()  # likewise, but store in an object that can easily
                    # be viewed on demand in RStudio
getRs('introda.r')  # download introda.r and put in script editor
getRs(cats=TRUE)    # list available major and minor categories
categories &lt;- getRs(cats=TRUE)
# likewise but store results in a list for later viewing
getRs(cats='reg')   # list all scripts in a major category containing 'reg'
getRs('importREDCap.r')   # source() to define a function
# source() a new version of the Hmisc package's cut2 function:
getRs('cut2.s', grepo='Hmisc', dir='R')

## End(Not run)
</code></pre>

<hr>
<h2 id='getZip'>Open a Zip File From a URL</h2><span id='topic+getZip'></span>

<h3>Description</h3>

<p>Allows downloading and reading of a zip file containing one file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getZip(url, password=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getZip_+3A_url">url</code></td>
<td>
<p>either a path to a local file or a valid URL.</p>
</td></tr>
<tr><td><code id="getZip_+3A_password">password</code></td>
<td>
<p>required to decode password-protected zip files</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Allows downloading and reading of zip file containing one file.
The file may be password protected.  If a password is needed then one will be requested unless given.
</p>
<p>Note: to make password-protected zip file z.zip, do zip -e z myfile
</p>


<h3>Value</h3>

<p>Returns a file O/I pipe.
</p>


<h3>Author(s)</h3>

<p>Frank E. Harrell</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+pipe">pipe</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
read.csv(getZip('http://test.com/z.zip'))

## End(Not run)
</code></pre>

<hr>
<h2 id='ggfreqScatter'>Frequency Scatterplot</h2><span id='topic+ggfreqScatter'></span>

<h3>Description</h3>

<p>Uses <code>ggplot2</code> to plot a scatterplot or dot-like chart for the case
where there is a very large number of overlapping values.  This works
for continuous and categorical <code>x</code> and <code>y</code>.  For continuous
variables it serves the same purpose as hexagonal binning.  Counts for
overlapping points are grouped into quantile groups and level of
transparency and rainbow colors are used to provide count information.
</p>
<p>Instead, you can specify <code>stick=TRUE</code> not use color but to encode
cell frequencies 
with the height of a black line y-centered at the middle of the bins.
Relative frequencies are not transformed, and the maximum cell
frequency is shown in a caption.  Every point with at least a
frequency of one is depicted with a full-height light gray vertical
line, scaled to the above overall maximum frequency.  In this way to
relative frequency is to proportion of these light gray lines that are
black, and one can see points whose frequencies are too low to see the
black lines.
</p>
<p>The result can also be passed to <code>ggplotly</code>.  Actual cell
frequencies are added to the hover text in that case using the
<code>label</code> <code>ggplot2</code> aesthetic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ggfreqScatter(x, y, by=NULL, bins=50, g=10, cuts=NULL,
              xtrans = function(x) x,
              ytrans = function(y) y,
              xbreaks = pretty(x, 10),
              ybreaks = pretty(y, 10),
              xminor  = NULL, yminor = NULL,
              xlab = as.character(substitute(x)),
              ylab = as.character(substitute(y)),
              fcolors = viridis::viridis(10), nsize=FALSE,
              stick=FALSE, html=FALSE, prfreq=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggfreqScatter_+3A_x">x</code></td>
<td>
<p>x-variable</p>
</td></tr>
<tr><td><code id="ggfreqScatter_+3A_y">y</code></td>
<td>
<p>y-variable</p>
</td></tr>
<tr><td><code id="ggfreqScatter_+3A_by">by</code></td>
<td>
<p>an optional vector used to make separate plots for each
distinct value using <code>facet_wrap()</code></p>
</td></tr>
<tr><td><code id="ggfreqScatter_+3A_bins">bins</code></td>
<td>
<p>for continuous <code>x</code> or <code>y</code> is the number of bins to
create by rounding.  Ignored for categorical variables.  If a
2-vector, the first element corresponds to <code>x</code> and the second to
<code>y</code>.</p>
</td></tr>
<tr><td><code id="ggfreqScatter_+3A_g">g</code></td>
<td>
<p>number of quantile groups to make for frequency counts.  Use
<code>g=0</code> to use frequencies continuously for color 
coding.  This is recommended only when using <code>plotly</code>.</p>
</td></tr>
<tr><td><code id="ggfreqScatter_+3A_cuts">cuts</code></td>
<td>
<p>instead of using <code>g</code>, specify <code>cuts</code> to provide
the vector of cuts for categorizing frequencies for assignment to colors</p>
</td></tr>
<tr><td><code id="ggfreqScatter_+3A_xtrans">xtrans</code>, <code id="ggfreqScatter_+3A_ytrans">ytrans</code></td>
<td>
<p>functions specifying transformations to be made
before binning and plotting</p>
</td></tr>
<tr><td><code id="ggfreqScatter_+3A_xbreaks">xbreaks</code>, <code id="ggfreqScatter_+3A_ybreaks">ybreaks</code></td>
<td>
<p>vectors of values to label on axis, on original
scale</p>
</td></tr>
<tr><td><code id="ggfreqScatter_+3A_xminor">xminor</code>, <code id="ggfreqScatter_+3A_yminor">yminor</code></td>
<td>
<p>values at which to put minor tick marks, on
original scale</p>
</td></tr>
<tr><td><code id="ggfreqScatter_+3A_xlab">xlab</code>, <code id="ggfreqScatter_+3A_ylab">ylab</code></td>
<td>
<p>axis labels.  If not specified and variable has a
<code>label</code>, thatu label will be used.</p>
</td></tr>
<tr><td><code id="ggfreqScatter_+3A_fcolors">fcolors</code></td>
<td>
<p><code>colors</code> argument to pass to
<code>scale_color_gradientn</code> to color code frequencies.  Use
<code>fcolors=gray.colors(10, 0.75, 0)</code> to show gray
scale, for example.  Another good choice is
<code>fcolors=hcl.colors(10, 'Blue-Red')</code>.</p>
</td></tr> 
<tr><td><code id="ggfreqScatter_+3A_nsize">nsize</code></td>
<td>
<p>set to <code>TRUE</code> to not vary color or transparency but
instead to size the symbols in relation to the number of points.  Best
with both <code>x</code> and <code>y</code> are discrete.  <code>ggplot2</code>
<code>size</code> is taken as the fourth root of the frequency.  If there
are 15 or unique frequencies all the unique frequencies are used,
otherwise <code>g</code> quantile groups of frequencies are used.</p>
</td></tr>
<tr><td><code id="ggfreqScatter_+3A_stick">stick</code></td>
<td>
<p>set to <code>TRUE</code> to not use colors but instead use
varying-height black vertical lines to depict cell frequencies.</p>
</td></tr>
<tr><td><code id="ggfreqScatter_+3A_html">html</code></td>
<td>
<p>set to <code>TRUE</code> to use html in axis labels instead of
plotmath</p>
</td></tr>
<tr><td><code id="ggfreqScatter_+3A_prfreq">prfreq</code></td>
<td>
<p>set to <code>TRUE</code> to print the frequency distributions of
the binned coordinate frequencies</p>
</td></tr>
<tr><td><code id="ggfreqScatter_+3A_...">...</code></td>
<td>
<p>arguments to pass to <code>geom_point</code> such as <code>shape</code>
and <code>size</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>ggplot</code> object</p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="#topic+cut2">cut2</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>require(ggplot2)
set.seed(1)
x &lt;- rnorm(1000)
y &lt;- rnorm(1000)
count &lt;- sample(1:100, 1000, TRUE)
x &lt;- rep(x, count)
y &lt;- rep(y, count)
# color=alpha=NULL below makes loess smooth over all points
g &lt;- ggfreqScatter(x, y) +   # might add g=0 if using plotly
      geom_smooth(aes(color=NULL, alpha=NULL), se=FALSE) +
      ggtitle("Using Deciles of Frequency Counts, 2500 Bins")
g
# plotly::ggplotly(g, tooltip='label')  # use plotly, hover text = freq. only
# Plotly makes it somewhat interactive, with hover text tooltips

# Instead use varying-height sticks to depict frequencies
ggfreqScatter(x, y, stick=TRUE) +
 labs(subtitle='Relative height of black lines to gray lines
is proportional to cell frequency.
Note that points with even tiny frequency are visable
(gray line with no visible black line).')


# Try with x categorical
x1 &lt;- sample(c('cat', 'dog', 'giraffe'), length(x), TRUE)
ggfreqScatter(x1, y)

# Try with y categorical
y1 &lt;- sample(LETTERS[1:10], length(x), TRUE)
ggfreqScatter(x, y1)

# Both categorical, larger point symbols, box instead of circle
ggfreqScatter(x1, y1, shape=15, size=7)
# Vary box size instead
ggfreqScatter(x1, y1, nsize=TRUE, shape=15)
</code></pre>

<hr>
<h2 id='ggplotlyr'>ggplotlyr</h2><span id='topic+ggplotlyr'></span>

<h3>Description</h3>

<p>Render <code>plotly</code> Graphic from a <code>ggplot2</code> Object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ggplotlyr(ggobject, tooltip = "label", remove = "txt: ", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggplotlyr_+3A_ggobject">ggobject</code></td>
<td>
<p>an object produced by <code>ggplot</code></p>
</td></tr>
<tr><td><code id="ggplotlyr_+3A_tooltip">tooltip</code></td>
<td>
<p>attribute specified to <code>ggplot</code> to hold hover text</p>
</td></tr>
<tr><td><code id="ggplotlyr_+3A_remove">remove</code></td>
<td>
<p>extraneous text to remove from hover text.  Default is set to assume <code>tooltip='label'</code> and assumed the user specified <code>aes(..., label=txt)</code>.  If you instead specified <code>aes(..., label=myvar)</code> use <code>remove='myvar: '</code>.</p>
</td></tr>
<tr><td><code id="ggplotlyr_+3A_...">...</code></td>
<td>
<p>other arguments passed to <code>ggplotly</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses <code>plotly::ggplotly()</code> to render a <code>plotly</code> graphic with a specified tooltip attribute, removing extraneous text that <code>ggplotly</code> puts in hover text when <code>tooltip='label'</code>
</p>


<h3>Value</h3>

<p>a <code>plotly</code> object
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

<hr>
<h2 id='GiniMd'>Gini's Mean Difference</h2><span id='topic+GiniMd'></span>

<h3>Description</h3>

<p><code>GiniMD</code> computes Gini's mean difference on a
numeric vector.  This index is defined as the mean absolute difference
between any two distinct elements of a vector.  For a Bernoulli
(binary) variable with proportion of ones equal to <code class="reqn">p</code> and sample
size <code class="reqn">n</code>, Gini's mean difference is
<code class="reqn">2\frac{n}{n-1}p(1-p)</code>.  For a 
trinomial variable (e.g., predicted values for a 3-level categorical
predictor using two dummy variables) having (predicted)
values <code class="reqn">A, B, C</code> with corresponding proportions <code class="reqn">a, b, c</code>,
Gini's mean difference is
<code class="reqn">2\frac{n}{n-1}[ab|A-B|+ac|A-C|+bc|B-C|]</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GiniMd(x, na.rm=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GiniMd_+3A_x">x</code></td>
<td>
<p>a numeric vector (for <code>GiniMd</code>)</p>
</td></tr>
<tr><td><code id="GiniMd_+3A_na.rm">na.rm</code></td>
<td>
<p>set to <code>TRUE</code> if you suspect there may be <code>NA</code>s
in <code>x</code>; these will then be removed.  Otherwise an error will
result.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a scalar numeric</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>David HA (1968): Gini's mean difference rediscovered.  Biometrika 55:573&ndash;575.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x &lt;- rnorm(40)
# Test GiniMd against a brute-force solution
gmd &lt;- function(x) {
  n &lt;- length(x)
  sum(outer(x, x, function(a, b) abs(a - b))) / n / (n - 1)
  }
GiniMd(x)
gmd(x)

z &lt;- c(rep(0,17), rep(1,6))
n &lt;- length(z)
GiniMd(z)
2*mean(z)*(1-mean(z))*n/(n-1)

a &lt;- 12; b &lt;- 13; c &lt;- 7; n &lt;- a + b + c
A &lt;- -.123; B &lt;- -.707; C &lt;- 0.523
xx &lt;- c(rep(A, a), rep(B, b), rep(C, c))
GiniMd(xx)
2*(a*b*abs(A-B) + a*c*abs(A-C) + b*c*abs(B-C))/n/(n-1)
</code></pre>

<hr>
<h2 id='hashCheck'>hashCheck</h2><span id='topic+hashCheck'></span>

<h3>Description</h3>

<p>Check for Changes in List of Objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hashCheck(..., file, .print. = TRUE, .names. = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hashCheck_+3A_...">...</code></td>
<td>
<p>a list of objects including data frames, vectors, functions, and all other types of R objects that represent dependencies of a certain calculation</p>
</td></tr>
<tr><td><code id="hashCheck_+3A_file">file</code></td>
<td>
<p>name of file in which results are stored</p>
</td></tr>
<tr><td><code id="hashCheck_+3A_.print.">.print.</code></td>
<td>
<p>set to <code>FALSE</code> to suppress printing information messages about what has changed</p>
</td></tr>
<tr><td><code id="hashCheck_+3A_.names.">.names.</code></td>
<td>
<p>vector of names of original arguments if not calling <code>hashCheck</code> directly</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given an RDS file name and a list of objects, does the following:
</p>

<ul>
<li><p> makes a vector of hashes, one for each object.  Function objects are run through <code>deparse</code> so that the environment of the function will not be considered.
</p>
</li>
<li><p> see if the file exists; if not, return a list with result=NULL, <code>hash</code> = new vector of hashes, <code>changed='All'</code>
</p>
</li>
<li><p> if the file exists, read the file and its hash attribute as <code>prevhash</code>
</p>
</li>
<li><p> if <code>prevhash</code> is not identical to hash:
if <code>.print.=TRUE</code> (default), print to console a summary of what's changed
return a list with result=NULL, <code>hash</code> = new hash vector, changed
</p>
</li>
<li><p> if <code>prevhash = hash</code>, return a list with result=file object, <code>hash</code>=new hash,  changed=&rdquo;
</p>
</li></ul>

<p>Set <code>options(debughash=TRUE)</code> to trace results in <code style="white-space: pre;">&#8288;/tmp/debughash.txt&#8288;</code>
</p>


<h3>Value</h3>

<p>a <code>list</code> with elements <code>result</code> (the computations), <code>hash</code> (the new hash), and <code>changed</code> which details what changed to make computations need to be run
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

<hr>
<h2 id='hdquantile'>Harrell-Davis Distribution-Free Quantile Estimator</h2><span id='topic+hdquantile'></span>

<h3>Description</h3>

<p>Computes the Harrell-Davis (1982) quantile estimator and jacknife
standard errors of quantiles.  The quantile estimator is a weighted
linear combination or order statistics in which the order statistics
used in traditional nonparametric quantile estimators are given the
greatest weight.  In small samples the H-D estimator is more efficient
than traditional ones, and the two methods are asymptotically
equivalent.  The H-D estimator is the limit of a bootstrap average as
the number of bootstrap resamples becomes infinitely large.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hdquantile(x, probs = seq(0, 1, 0.25),
           se = FALSE, na.rm = FALSE, names = TRUE, weights=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hdquantile_+3A_x">x</code></td>
<td>
<p>a numeric vector</p>
</td></tr>
<tr><td><code id="hdquantile_+3A_probs">probs</code></td>
<td>
<p>vector of quantiles to compute</p>
</td></tr>
<tr><td><code id="hdquantile_+3A_se">se</code></td>
<td>
<p>set to <code>TRUE</code> to also compute standard errors</p>
</td></tr>
<tr><td><code id="hdquantile_+3A_na.rm">na.rm</code></td>
<td>
<p>set to <code>TRUE</code> to remove <code>NA</code>s from <code>x</code>
before computing quantiles</p>
</td></tr>
<tr><td><code id="hdquantile_+3A_names">names</code></td>
<td>
<p>set to <code>FALSE</code> to prevent names attributions from
being added to quantiles and standard errors</p>
</td></tr>
<tr><td><code id="hdquantile_+3A_weights">weights</code></td>
<td>
<p>set to <code>TRUE</code> to return a <code>"weights"</code>
attribution with the matrix of weights used in the H-D estimator
corresponding to order statistics, with columns corresponding to
quantiles.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A Fortran routine is used to compute the jackknife leave-out-one
quantile estimates.  Standard errors are not computed for quantiles 0 or
1 (<code>NA</code>s are returned).
</p>


<h3>Value</h3>

<p>A vector of quantiles.  If <code>se=TRUE</code> this vector will have an
attribute <code>se</code> added to it, containing the standard errors.  If
<code>weights=TRUE</code>, also has a <code>"weights"</code> attribute which is a matrix.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>References</h3>

<p>Harrell FE, Davis CE (1982): A new distribution-free quantile
estimator.  Biometrika 69:635-640.
</p>
<p>Hutson AD, Ernst MD (2000): The exact bootstrap mean and variance of
an L-estimator.  J Roy Statist Soc B 62:89-94.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+quantile">quantile</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x &lt;- runif(100)
hdquantile(x, (1:3)/4, se=TRUE)

## Not run: 
# Compare jackknife standard errors with those from the bootstrap
library(boot)
boot(x, function(x,i) hdquantile(x[i], probs=(1:3)/4), R=400)

## End(Not run)
</code></pre>

<hr>
<h2 id='hidingTOC'>Moving and Hiding Table of Contents</h2><span id='topic+hidingTOC'></span>

<h3>Description</h3>

<p>Moving and hiding table of contents for Rmd HTML documents
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hidingTOC(
  buttonLabel = "Contents",
  levels = 3,
  tocSide = c("right", "left"),
  buttonSide = c("right", "left"),
  posCollapse = c("margin", "top", "bottom"),
  hidden = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hidingTOC_+3A_buttonlabel">buttonLabel</code></td>
<td>
<p>the text on the button that hides and unhides the
table of contents. Defaults to <code>Contents</code>.</p>
</td></tr>
<tr><td><code id="hidingTOC_+3A_levels">levels</code></td>
<td>
<p>the max depth of the table of contents that it is desired to
have control over the display of.  (defaults to 3)</p>
</td></tr>
<tr><td><code id="hidingTOC_+3A_tocside">tocSide</code></td>
<td>
<p>which side of the page should the table of contents be placed
on. Can be either <code>'right'</code> or <code>'left'</code>. Defaults to
<code>'right'</code></p>
</td></tr>
<tr><td><code id="hidingTOC_+3A_buttonside">buttonSide</code></td>
<td>
<p>which side of the page should the button that hides the TOC
be placed on. Can be either <code>'right'</code> or <code>'left'</code>. Defaults to
<code>'right'</code></p>
</td></tr>
<tr><td><code id="hidingTOC_+3A_poscollapse">posCollapse</code></td>
<td>
<p>if <code>'margin'</code> then display the depth select buttons
vertically along the side of the page choosen by <code>buttonSide</code>. If
<code>'top'</code> then display the depth select buttons horizontally under the
button that hides the TOC. Defaults to <code>'margin'</code>. <code>'bottom'</code> is
currently unimplemented.</p>
</td></tr>
<tr><td><code id="hidingTOC_+3A_hidden">hidden</code></td>
<td>
<p>Logical should the table of contents be hidden at page load
Defaults to <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>hidingTOC</code> creates a table of contents in a Rmd document that
can be hidden at the press of a button. It also generate buttons that allow
the hiding or unhiding of the diffrent level depths of the table of contents.
</p>


<h3>Value</h3>

<p>a HTML formated text string to be inserted into an markdown document
</p>


<h3>Author(s)</h3>

<p>Thomas Dupont
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
hidingTOC()

## End(Not run)
</code></pre>

<hr>
<h2 id='hist.data.frame'>Histograms for Variables in a Data Frame</h2><span id='topic+hist.data.frame'></span>

<h3>Description</h3>

<p>This functions tries to compute the maximum number of histograms that
will fit on one page, then it draws a matrix of histograms.  If there
are more qualifying variables than will fit on a page, the function
waits for a mouse click before drawing the next page.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'data.frame'
hist(x, n.unique = 3, nclass = "compute",
                na.big = FALSE, rugs = FALSE, freq=TRUE, mtitl = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hist.data.frame_+3A_x">x</code></td>
<td>
<p>a data frame</p>
</td></tr>
<tr><td><code id="hist.data.frame_+3A_n.unique">n.unique</code></td>
<td>
<p>minimum number of unique values a variable must have
before a histogram is drawn</p>
</td></tr>
<tr><td><code id="hist.data.frame_+3A_nclass">nclass</code></td>
<td>
<p>number of bins.  Default is
max(2,trunc(min(n/10,25*log(n,10))/2)), where n is the number of
non-missing values for a variable.</p>
</td></tr>
<tr><td><code id="hist.data.frame_+3A_na.big">na.big</code></td>
<td>
<p>set to <code>TRUE</code> to draw the number of missing values
on the top of the histogram in addition to in a subtitle.  In the
subtitle, n is the number of non-missing values and m is the number
of missing values</p>
</td></tr>
<tr><td><code id="hist.data.frame_+3A_rugs">rugs</code></td>
<td>
<p>set to <code>TRUE</code> to add rug plots at the top of each
histogram</p>
</td></tr>
<tr><td><code id="hist.data.frame_+3A_freq">freq</code></td>
<td>
<p>see <code><a href="graphics.html#topic+hist">hist</a></code>.  Default is to show frequencies.</p>
</td></tr>
<tr><td><code id="hist.data.frame_+3A_mtitl">mtitl</code></td>
<td>
<p>set to a character string to set aside extra outside top
margin and to use the string for an overall title</p>
</td></tr>
<tr><td><code id="hist.data.frame_+3A_...">...</code></td>
<td>
<p>arguments passed to <code>scat1d</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>the number of pages drawn</p>


<h3>Author(s)</h3>

<p>Frank E Harrell Jr</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+hist">hist</a></code>, <code><a href="#topic+scat1d">scat1d</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>d &lt;- data.frame(a=runif(200), b=rnorm(200),
                w=factor(sample(c('green','red','blue'), 200, TRUE)))
hist.data.frame(d)   # in R, just say hist(d)
</code></pre>

<hr>
<h2 id='histbackback'>
Back to Back Histograms
</h2><span id='topic+histbackback'></span>

<h3>Description</h3>

<p>Takes two vectors or a list with <code>x</code> and <code>y</code> components, and produces 
back to back histograms of the two datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>histbackback(x, y, brks=NULL, xlab=NULL, axes=TRUE, probability=FALSE,
             xlim=NULL, ylab='', ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="histbackback_+3A_x">x</code>, <code id="histbackback_+3A_y">y</code></td>
<td>

<p>either two vectors or a list given as <code>x</code> with two components.  If the
components have names, they will be used to label the axis
(modification FEH).
</p>
</td></tr>
<tr><td><code id="histbackback_+3A_brks">brks</code></td>
<td>

<p>vector of the desired breakpoints for the histograms.
</p>
</td></tr>
<tr><td><code id="histbackback_+3A_xlab">xlab</code></td>
<td>

<p>a vector of two character strings naming the two datasets.
</p>
</td></tr>
<tr><td><code id="histbackback_+3A_axes">axes</code></td>
<td>

<p>logical flag stating whether or not to label the axes.
</p>
</td></tr>
<tr><td><code id="histbackback_+3A_probability">probability</code></td>
<td>

<p>logical flag: if <code>TRUE</code>, then the x-axis corresponds to the units for a
density.  If <code>FALSE</code>, then the units are counts.
</p>
</td></tr>
<tr><td><code id="histbackback_+3A_xlim">xlim</code></td>
<td>

<p>x-axis limits.  First value must be negative, as the left histogram is
placed at negative x-values.  Second value must be positive, for the
right histogram.  To make the limits symmetric, use e.g. <code>ylim=c(-20,20)</code>.
</p>
</td></tr>
<tr><td><code id="histbackback_+3A_ylab">ylab</code></td>
<td>

<p>label for y-axis.  Default is no label.
</p>
</td></tr>
<tr><td><code id="histbackback_+3A_...">...</code></td>
<td>

<p>additional graphics parameters may be given.
</p>
</td></tr></table>


<h3>Value</h3>

<p>a list is returned invisibly with the following components:
</p>
<table>
<tr><td><code>left</code></td>
<td>

<p>the counts for the dataset plotted on the left.
</p>
</td></tr>
<tr><td><code>right</code></td>
<td>

<p>the counts for the dataset plotted on the right.
</p>
</td></tr>
<tr><td><code>breaks</code></td>
<td>

<p>the breakpoints used.
</p>
</td></tr></table>


<h3>Side Effects</h3>

<p>a plot is produced on the current graphics device.
</p>


<h3>Author(s)</h3>

<p>Pat Burns
<br />
Salomon Smith Barney
<br />
London
<br />
<a href="mailto:pburns@dorado.sbi.com">pburns@dorado.sbi.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+hist">hist</a></code>, <code><a href="lattice.html#topic+histogram">histogram</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>options(digits=3)
set.seed(1)
histbackback(rnorm(20), rnorm(30))


fool &lt;- list(x=rnorm(40), y=rnorm(40))
histbackback(fool)
age &lt;- rnorm(1000,50,10)
sex &lt;- sample(c('female','male'),1000,TRUE)
histbackback(split(age, sex))
agef &lt;- age[sex=='female']; agem &lt;- age[sex=='male']
histbackback(list(Female=agef,Male=agem), probability=TRUE, xlim=c(-.06,.06))
</code></pre>

<hr>
<h2 id='histboxp'>Use plotly to Draw Stratified Spike Histogram and Box Plot Statistics</h2><span id='topic+histboxp'></span><span id='topic+histboxpM'></span><span id='topic+dhistboxp'></span>

<h3>Description</h3>

<p>Uses <code>plotly</code> to draw horizontal spike histograms stratified by
<code>group</code>, plus the mean (solid dot) and vertical bars for these
quantiles: 0.05 (red, short), 0.25 (blue, medium), 0.50 (black, long),
0.75 (blue, medium), and 0.95 (red, short).  The robust dispersion measure
Gini's mean difference and the SD may optionally be added.  These are
shown as horizontal lines starting at the minimum value of <code>x</code>
having a length equal to the mean difference or SD.  Even when Gini's
and SD are computed, they are not drawn unless the user clicks on their
legend entry.
</p>
<p>Spike histograms have the advantage of effectively showing the raw data for both
small and huge datasets, and unlike box plots allow multi-modality to be
easily seen.
</p>
<p><code>histboxpM</code> plots multiple histograms stacked vertically, for
variables in a data frame having a common <code>group</code> variable (if any)
and combined using <code>plotly::subplot</code>.
</p>
<p><code>dhistboxp</code> is like <code>histboxp</code> but no <code>plotly</code> graphics
are actually drawn.  Instead, a data frame suitable for use with
<code>plotlyM</code> is returned.  For <code>dhistboxp</code> an additional level of
stratification <code>strata</code> is implemented.  <code>group</code> causes a
different result here to produce back-to-back histograms (in the case of
two groups) for each level of <code>strata</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>histboxp(p = plotly::plot_ly(height=height), x, group = NULL,
         xlab=NULL, gmd=TRUE, sd=FALSE, bins = 100, wmax=190, mult=7,
         connect=TRUE, showlegend=TRUE)

dhistboxp(x, group = NULL, strata=NULL, xlab=NULL, 
          gmd=FALSE, sd=FALSE, bins = 100, nmin=5, ff1=1, ff2=1)

histboxpM(p=plotly::plot_ly(height=height, width=width), x, group=NULL,
          gmd=TRUE, sd=FALSE, width=NULL, nrows=NULL, ncols=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="histboxp_+3A_p">p</code></td>
<td>
<p><code>plotly</code> graphics object if already begun</p>
</td></tr>
<tr><td><code id="histboxp_+3A_x">x</code></td>
<td>
<p>a numeric vector, or for <code>histboxpM</code> a numeric vector or
a data frame of numeric vectors, hopefully with <code>label</code> and
<code>units</code> attributes</p>
</td></tr>
<tr><td><code id="histboxp_+3A_group">group</code></td>
<td>
<p>a discrete grouping variable.  If omitted, defaults to a
vector of ones</p>
</td></tr>
<tr><td><code id="histboxp_+3A_strata">strata</code></td>
<td>
<p>a discrete numeric stratification variable.  Values are
also used to space out different spike histograms.  Defaults
to a vector of ones.</p>
</td></tr>
<tr><td><code id="histboxp_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label, defaults to labelled version include units
of measurement if any</p>
</td></tr>
<tr><td><code id="histboxp_+3A_gmd">gmd</code></td>
<td>
<p>set to <code>FALSE</code> to not compute Gini's mean difference</p>
</td></tr>
<tr><td><code id="histboxp_+3A_sd">sd</code></td>
<td>
<p>set to <code>TRUE</code> to compute the SD</p>
</td></tr>
<tr><td><code id="histboxp_+3A_width">width</code></td>
<td>
<p>width in pixels</p>
</td></tr>
<tr><td><code id="histboxp_+3A_nrows">nrows</code></td>
<td>
<p>number of rows for layout of multiple plots</p>
</td></tr>
<tr><td><code id="histboxp_+3A_ncols">ncols</code></td>
<td>
<p>number of columns for layout of multiple plots.  At most
one of <code>nrows,ncols</code> should be specified.</p>
</td></tr>
<tr><td><code id="histboxp_+3A_bins">bins</code></td>
<td>
<p>number of equal-width bins to use for spike histogram.  If
the number of distinct values of <code>x</code> is less than <code>bins</code>,
the actual values of <code>x</code> are used.</p>
</td></tr>
<tr><td><code id="histboxp_+3A_nmin">nmin</code></td>
<td>
<p>minimum number of non-missing observations for a
group-stratum combination before the spike histogram and
quantiles are drawn</p>
</td></tr>
<tr><td><code id="histboxp_+3A_ff1">ff1</code>, <code id="histboxp_+3A_ff2">ff2</code></td>
<td>
<p>fudge factors for position and bar length for spike histograms</p>
</td></tr>
<tr><td><code id="histboxp_+3A_wmax">wmax</code>, <code id="histboxp_+3A_mult">mult</code></td>
<td>
<p>tweaks for margin to allocate</p>
</td></tr>
<tr><td><code id="histboxp_+3A_connect">connect</code></td>
<td>
<p>set to <code>FALSE</code> to suppress lines connecting
quantiles</p>
</td></tr>
<tr><td><code id="histboxp_+3A_showlegend">showlegend</code></td>
<td>
<p>used if producing multiple plots to be combined with
<code>subplot</code>; set to <code>FALSE</code> for all but one plot</p>
</td></tr>
<tr><td><code id="histboxp_+3A_...">...</code></td>
<td>
<p>other arguments for <code>histboxpM</code> that are passed to
<code>histboxp</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>plotly</code> object.  For <code>dhistboxp</code> a data frame as
expected by <code>plotlyM</code></p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="#topic+histSpike">histSpike</a></code>, <code><a href="#topic+plot.describe">plot.describe</a></code>,
<code><a href="#topic+scat1d">scat1d</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dist &lt;- c(rep(1, 500), rep(2, 250), rep(3, 600))
Distribution &lt;- factor(dist, 1 : 3, c('Unimodal', 'Bimodal', 'Trimodal'))
x &lt;- c(rnorm(500, 6, 1),
       rnorm(200, 3, .7), rnorm(50, 7, .4),
       rnorm(200, 2, .7), rnorm(300, 5.5, .4), rnorm(100, 8, .4))
histboxp(x=x, group=Distribution, sd=TRUE)
X &lt;- data.frame(x, x2=runif(length(x)))
histboxpM(x=X, group=Distribution, ncols=2)  # separate plots

## End(Not run)
</code></pre>

<hr>
<h2 id='hlab'>hlab</h2><span id='topic+hlab'></span>

<h3>Description</h3>

<p>Easy Extraction of Labels/Units Expressions for Plotting
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hlab(x, name = NULL, html = FALSE, plotmath = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hlab_+3A_x">x</code></td>
<td>
<p>a single variable name, unquoted</p>
</td></tr>
<tr><td><code id="hlab_+3A_name">name</code></td>
<td>
<p>a single character string providing an alternate way to name <code>x</code> that is useful when <code>hlab</code> is called from another function such as <code>hlabs</code></p>
</td></tr>
<tr><td><code id="hlab_+3A_html">html</code></td>
<td>
<p>set to <code>TRUE</code> to return HTML strings instead of <code>plotmath</code> expressions</p>
</td></tr>
<tr><td><code id="hlab_+3A_plotmath">plotmath</code></td>
<td>
<p>set to <code>FALSE</code> to use plain text instead of plotmath</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a single unquoted variable, first looks to see if a non-<code>NULL</code> <code>LabelsUnits</code> object exists (produced by <code>extractlabs()</code>).  When <code>LabelsUnits</code> does not exist or is <code>NULL</code>, looks up the attributes in the current dataset, which defaults to <code>d</code> or may be specified by <code>options(current_ds='name of the data frame/table')</code>.  Finally the existence of a variable of the given name in the global environment is checked. When a variable is not found in any of these three sources or has a blank <code>label</code> and <code>units</code>, an <code>expression()</code> with the variable name alone is returned.  If <code>html=TRUE</code>, HTML strings are constructed instead, suitable for <code>plotly</code> graphics.
</p>
<p>The result is useful for <code>xlab</code> and <code>ylab</code> in base plotting functions or in <code>ggplot2</code>, along with being useful for <code>labs</code> in <code>ggplot2</code>.  See example.
</p>


<h3>Value</h3>

<p>an expression created by <code>labelPlotmath</code> with <code>plotmath=TRUE</code>
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><code><a href="#topic+label">label()</a></code>, <code><a href="#topic+units">units()</a></code>, <code><a href="#topic+contents">contents()</a></code>, <code><a href="#topic+hlabs">hlabs()</a></code>, <code><a href="#topic+extractlabs">extractlabs()</a></code>, <a href="grDevices.html#topic+plotmath">plotmath</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d &lt;- data.frame(x=1:10, y=(1:10)/10)
d &lt;- upData(d, labels=c(x='X', y='Y'), units=c(x='mmHg'), print=FALSE)
hlab(x)
hlab(x, html=TRUE)
hlab(z)
require(ggplot2)
ggplot(d, aes(x, y)) + geom_point() + labs(x=hlab(x), y=hlab(y))
# Can use xlab(hlab(x)) + ylab(hlab(y)) also
# Store names, labels, units for all variables in d in object
LabelsUnits &lt;- extractlabs(d)
# Remove d; labels/units still found
rm(d)
hlab(x)
# Remove LabelsUnits and use a current dataset named
# d2 instead of the default d
rm(LabelsUnits)
options(current_ds='d2')
</code></pre>

<hr>
<h2 id='hlabs'>hlabs</h2><span id='topic+hlabs'></span>

<h3>Description</h3>

<p>Front-end to ggplot2 labs Function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hlabs(x, y, html = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hlabs_+3A_x">x</code></td>
<td>
<p>a single variable name, unquoted</p>
</td></tr>
<tr><td><code id="hlabs_+3A_y">y</code></td>
<td>
<p>a single variable name, unquoted</p>
</td></tr>
<tr><td><code id="hlabs_+3A_html">html</code></td>
<td>
<p>set to <code>TRUE</code> to render in html (for <code>plotly</code>), otherwise the result is <code>plotmath</code> expressions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Runs <code>x</code>, <code>y</code>, or both through <code><a href="#topic+hlab">hlab()</a></code> and passes the constructed labels to the <a href="ggplot2.html#topic+labs">ggplot2::labs</a> function to specify x- and y-axis labels specially formatted for units of measurement
</p>


<h3>Value</h3>

<p>result of <code><a href="ggplot2.html#topic+labs">ggplot2::labs()</a></code>
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Name the current dataset d, or specify a name with
# options(curr_ds='...') or run `extractlabs`, then
# ggplot(d, aes(x,y)) + geom_point() + hlabs(x,y)
# to specify only the x-axis label use hlabs(x), or to
# specify only the y-axis label use hlabs(y=...)
</code></pre>

<hr>
<h2 id='Hmisc-internal'>Internal Hmisc functions</h2><span id='topic+dataDensityString'></span><span id='topic+aregTran'></span><span id='topic+as.double.Cbind'></span><span id='topic+as.numeric.Cbind'></span><span id='topic+formatSep'></span><span id='topic+as.data.frame.impute'></span><span id='topic+as.data.frame.roundN'></span><span id='topic+as.data.frame.special.miss'></span><span id='topic+as.data.frame.substi'></span><span id='topic+substi'></span><span id='topic+substi.source'></span><span id='topic++5B.substi'></span><span id='topic+bpx'></span><span id='topic+convertPdate'></span><span id='topic+ddmmmyy'></span><span id='topic+expr.tree'></span><span id='topic+fillin'></span><span id='topic+formatCats'></span><span id='topic+formatCons'></span><span id='topic+formatDateTime'></span><span id='topic+formatTestStats'></span><span id='topic+format.timePOSIXt'></span><span id='topic+ftuss'></span><span id='topic+ftupwr'></span><span id='topic+get2rowHeads'></span><span id='topic+groupn'></span><span id='topic+htmlGreek'></span><span id='topic+htmlSpecial'></span><span id='topic+markupSpecs'></span><span id='topic+importConvertDateTime'></span><span id='topic+is.present'></span><span id='topic+lookupSASContents'></span><span id='topic+makeNames'></span><span id='topic+mask'></span><span id='topic+nafitted.delete'></span><span id='topic+na.include'></span><span id='topic+Names2names'></span><span id='topic+GetModelFrame'></span><span id='topic+naprint.keep'></span><span id='topic+naresid.keep'></span><span id='topic+naprint.delete'></span><span id='topic+naresid.delete'></span><span id='topic+napredict.delete'></span><span id='topic+oPar'></span><span id='topic+optionsCmds'></span><span id='topic+ordGridFun'></span><span id='topic+parGrid'></span><span id='topic+pasteFit'></span><span id='topic+plotpsummaryM'></span><span id='topic+print.substi'></span><span id='topic+print.timePOSIXt'></span><span id='topic+read.xportDataload'></span><span id='topic+readSAScsv'></span><span id='topic+rowsumFast'></span><span id='topic+sas.get.macro'></span><span id='topic+setParNro'></span><span id='topic+StatPlsmo'></span><span id='topic+stepfun.eval'></span><span id='topic+stripChart'></span><span id='topic+termsDrop'></span><span id='topic+testDateTime'></span><span id='topic+uncbind'></span><span id='topic+var.inner'></span><span id='topic+xInch'></span><span id='topic+xySortNoDupNoNA'></span><span id='topic+yInch'></span><span id='topic+zoom'></span><span id='topic+latex.responseSummary'></span><span id='topic+print.responseSummary'></span><span id='topic+responseSummary'></span><span id='topic+combine'></span><span id='topic+combine+3C-'></span><span id='topic+F_cidxcn'></span><span id='topic+F_cidxcp'></span><span id='topic+F_do_mchoice_match'></span><span id='topic+F_do_nstr'></span><span id='topic+F_hoeffd'></span><span id='topic+F_jacklins'></span><span id='topic+F_largrec'></span><span id='topic+F_maxempr'></span><span id='topic+F_rcorr'></span><span id='topic+F_wclosepw'></span><span id='topic+F_wclosest'></span>

<h3>Description</h3>

<p>Internal Hmisc functions.</p>


<h3>Details</h3>

<p>These are not to be called by the user or are undocumented.</p>

<hr>
<h2 id='HmiscOverview'>
Overview of Hmisc Library
</h2><span id='topic+HmiscOverview'></span><span id='topic+Hmisc.Overview'></span>

<h3>Description</h3>

<p>The Hmisc library contains many functions useful for data analysis,
high-level graphics, utility operations, functions for computing
sample size and power, translating SAS datasets into <span class="rlang"><b>R</b></span>, imputing
missing values, advanced table making, variable clustering, character
string manipulation, conversion of <span class="rlang"><b>R</b></span> objects to LaTeX code, recoding
variables, and bootstrap repeated measures analysis.  Most of these
functions were written by F Harrell, but a few were collected from
statlib and from s-news; other authors are indicated below.  This
collection of functions includes all of  Harrell's submissions to
statlib other than  the functions in the <span class="pkg">rms</span> and display
libraries.  A few of the functions do not  have &ldquo;Help&rdquo;
documentation.
</p>
<p>To make <span class="pkg">Hmisc</span> load silently, issue
<code>options(Hverbose=FALSE)</code> before <code>library(Hmisc)</code>.
</p>


<h3>Functions</h3>


<table>
<tr>
 <td style="text-align: left;">
<b>Function Name</b> </td><td style="text-align: left;">  <b>Purpose</b> </td>
</tr>
<tr>
 <td style="text-align: left;">
abs.error.pred  </td><td style="text-align: left;"> Computes various indexes of predictive accuracy based</td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;">    on absolute errors, for linear models</td>
</tr>
<tr>
 <td style="text-align: left;">
addMarginal     </td><td style="text-align: left;"> Add marginal observations over selected variables</td>
</tr>
<tr>
 <td style="text-align: left;">
all.is.numeric  </td><td style="text-align: left;"> Check if character strings are legal numerics</td>
</tr>
<tr>
 <td style="text-align: left;">
approxExtrap    </td><td style="text-align: left;"> Linear extrapolation</td>
</tr>
<tr>
 <td style="text-align: left;">
aregImpute      </td><td style="text-align: left;"> Multiple imputation based on additive regression,</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     bootstrapping, and predictive mean matching</td>
</tr>
<tr>
 <td style="text-align: left;">
areg.boot       </td><td style="text-align: left;"> Nonparametrically estimate transformations for both</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     sides of a multiple additive regression, and</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     bootstrap these estimates and <code class="reqn">R^2</code></td>
</tr>
<tr>
 <td style="text-align: left;">
ballocation     </td><td style="text-align: left;"> Optimum sample allocations in 2-sample proportion test</td>
</tr>
<tr>
 <td style="text-align: left;">
binconf         </td><td style="text-align: left;"> Exact confidence limits for a proportion and more accurate</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     (narrower!) score stat.-based Wilson interval</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     (Rollin Brant, mod. FEH)</td>
</tr>
<tr>
 <td style="text-align: left;">
bootkm          </td><td style="text-align: left;"> Bootstrap Kaplan-Meier survival or quantile estimates</td>
</tr>
<tr>
 <td style="text-align: left;">
bpower          </td><td style="text-align: left;"> Approximate power of 2-sided test for 2 proportions</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     Includes bpower.sim for exact power by simulation</td>
</tr>
<tr>
 <td style="text-align: left;">
bpplot          </td><td style="text-align: left;"> Box-Percentile plot </td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     (Jeffrey Banfield, <a href="mailto:umsfjban@bill.oscs.montana.edu">umsfjban@bill.oscs.montana.edu</a>)</td>
</tr>
<tr>
 <td style="text-align: left;">
bpplotM         </td><td style="text-align: left;"> Chart extended box plots for multiple variables</td>
</tr>
<tr>
 <td style="text-align: left;">
bsamsize        </td><td style="text-align: left;"> Sample size requirements for test of 2 proportions</td>
</tr>
<tr>
 <td style="text-align: left;">
bystats         </td><td style="text-align: left;"> Statistics on a single variable by levels of &gt;=1 factors</td>
</tr>
<tr>
 <td style="text-align: left;">
bystats2        </td><td style="text-align: left;"> 2-way statistics</td>
</tr>
<tr>
 <td style="text-align: left;">
character.table </td><td style="text-align: left;"> Shows numeric equivalents of all latin characters</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     Useful for putting many special chars. in graph titles</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     (Pierre Joyet, <a href="mailto:pierre.joyet@bluewin.ch">pierre.joyet@bluewin.ch</a>)</td>
</tr>
<tr>
 <td style="text-align: left;">
ciapower        </td><td style="text-align: left;"> Power of Cox interaction test</td>
</tr>
<tr>
 <td style="text-align: left;">
cleanup.import  </td><td style="text-align: left;"> More compactly store variables in a data frame, and clean up</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     problem data when e.g. Excel spreadsheet had a non-</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     numeric value in a numeric column</td>
</tr>
<tr>
 <td style="text-align: left;">
combine.levels  </td><td style="text-align: left;"> Combine infrequent levels of a categorical variable</td>
</tr>
<tr>
 <td style="text-align: left;">
confbar         </td><td style="text-align: left;"> Draws confidence bars on an existing plot using multiple</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     confidence levels distinguished using color or gray scale</td>
</tr>
<tr>
 <td style="text-align: left;">
contents        </td><td style="text-align: left;"> Print the contents (variables, labels, etc.) of a data frame</td>
</tr>
<tr>
 <td style="text-align: left;">
cpower          </td><td style="text-align: left;"> Power of Cox 2-sample test allowing for noncompliance</td>
</tr>
<tr>
 <td style="text-align: left;">
Cs              </td><td style="text-align: left;"> Vector of character strings from list of unquoted names</td>
</tr>
<tr>
 <td style="text-align: left;">
csv.get         </td><td style="text-align: left;"> Enhanced importing of comma separated files labels</td>
</tr>
<tr>
 <td style="text-align: left;">
cut2            </td><td style="text-align: left;"> Like cut with better endpoint label construction and allows</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     construction of quantile groups or groups with given n</td>
</tr>
<tr>
 <td style="text-align: left;">
datadensity     </td><td style="text-align: left;"> Snapshot graph of distributions of all variables in</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     a data frame.  For continuous variables uses scat1d.</td>
</tr>
<tr>
 <td style="text-align: left;">
dataRep         </td><td style="text-align: left;"> Quantify representation of new observations in a database</td>
</tr>
<tr>
 <td style="text-align: left;">
ddmmmyy         </td><td style="text-align: left;"> SAS &ldquo;date7&rdquo; output format for a chron object</td>
</tr>
<tr>
 <td style="text-align: left;">
deff            </td><td style="text-align: left;"> Kish design effect and intra-cluster correlation</td>
</tr>
<tr>
 <td style="text-align: left;">
describe        </td><td style="text-align: left;"> Function to describe different classes of objects.</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     Invoke by saying describe(object). It calls one of the</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     following:</td>
</tr>
<tr>
 <td style="text-align: left;">
describe.data.frame
                </td><td style="text-align: left;"> Describe all variables in a data frame (generalization</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     of SAS UNIVARIATE)</td>
</tr>
<tr>
 <td style="text-align: left;">
describe.default
                </td><td style="text-align: left;"> Describe a variable (generalization of SAS UNIVARIATE)</td>
</tr>
<tr>
 <td style="text-align: left;">
dotplot3        </td><td style="text-align: left;"> A more flexible version of dotplot</td>
</tr>
<tr>
 <td style="text-align: left;">
Dotplot         </td><td style="text-align: left;"> Enhancement of Trellis dotplot allowing for matrix</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     x-var., auto generation of Key function, superposition</td>
</tr>
<tr>
 <td style="text-align: left;">
drawPlot        </td><td style="text-align: left;"> Simple mouse-driven drawing program, including a function</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     for fitting Bezier curves</td>
</tr>
<tr>
 <td style="text-align: left;">
Ecdf            </td><td style="text-align: left;"> Empirical cumulative distribution function plot</td>
</tr>
<tr>
 <td style="text-align: left;">
errbar          </td><td style="text-align: left;"> Plot with error bars (Charles Geyer, U. Chi., mod FEH)</td>
</tr>
<tr>
 <td style="text-align: left;">
event.chart     </td><td style="text-align: left;"> Plot general event charts (Jack Lee, <a href="mailto:jjlee@mdanderson.org">jjlee@mdanderson.org</a>, </td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     Ken Hess, Joel Dubin; Am Statistician 54:63-70,2000)</td>
</tr>
<tr>
 <td style="text-align: left;">
event.history	</td><td style="text-align: left;"> Event history chart with time-dependent cov. status</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     (Joel Dubin, <a href="mailto:jdubin@uwaterloo.ca">jdubin@uwaterloo.ca</a>)</td>
</tr>
<tr>
 <td style="text-align: left;">
find.matches    </td><td style="text-align: left;"> Find matches (with tolerances) between columns of 2 matrices</td>
</tr>
<tr>
 <td style="text-align: left;">
first.word      </td><td style="text-align: left;"> Find the first word in an <span class="rlang"><b>R</b></span> expression (R Heiberger)</td>
</tr>
<tr>
 <td style="text-align: left;">
fit.mult.impute </td><td style="text-align: left;"> Fit most regression models over multiple transcan imputations,</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     compute imputation-adjusted variances and avg. betas</td>
</tr>
<tr>
 <td style="text-align: left;">
format.df       </td><td style="text-align: left;"> Format a matrix or data frame with much user control</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     (R Heiberger and FE Harrell)</td>
</tr>
<tr>
 <td style="text-align: left;">
ftupwr          </td><td style="text-align: left;"> Power of 2-sample binomial test using Fleiss, Tytun, Ury</td>
</tr>
<tr>
 <td style="text-align: left;">
ftuss           </td><td style="text-align: left;"> Sample size for 2-sample binomial test using  "  "  "  "</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     (Both by Dan Heitjan, <a href="mailto:dheitjan@biostats.hmc.psu.edu">dheitjan@biostats.hmc.psu.edu</a>)</td>
</tr>
<tr>
 <td style="text-align: left;">
gbayes          </td><td style="text-align: left;"> Bayesian posterior and predictive distributions when both</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;"> the prior and the likelihood are Gaussian</td>
</tr>
<tr>
 <td style="text-align: left;">
getHdata        </td><td style="text-align: left;"> Fetch and list datasets on our web site</td>
</tr>
<tr>
 <td style="text-align: left;">
hdquantile      </td><td style="text-align: left;"> Harrell-Davis nonparametric quantile estimator with s.e.</td>
</tr>
<tr>
 <td style="text-align: left;">
histbackback    </td><td style="text-align: left;"> Back-to-back histograms (Pat Burns, Salomon Smith</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     Barney, London, <a href="mailto:pburns@dorado.sbi.com">pburns@dorado.sbi.com</a>)</td>
</tr>
<tr>
 <td style="text-align: left;">
hist.data.frame </td><td style="text-align: left;"> Matrix of histograms for all numeric vars. in data frame</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     Use hist.data.frame(data.frame.name)</td>
</tr>
<tr>
 <td style="text-align: left;">
histSpike       </td><td style="text-align: left;"> Add high-resolution spike histograms or density estimates</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     to an existing plot</td>
</tr>
<tr>
 <td style="text-align: left;">
hoeffd          </td><td style="text-align: left;"> Hoeffding's D test (omnibus test of independence of X and Y)</td>
</tr>
<tr>
 <td style="text-align: left;">
impute          </td><td style="text-align: left;"> Impute missing data (generic method)</td>
</tr>
<tr>
 <td style="text-align: left;">
interaction     </td><td style="text-align: left;"> More flexible version of builtin function</td>
</tr>
<tr>
 <td style="text-align: left;">
is.present      </td><td style="text-align: left;"> Tests for non-blank character values or non-NA numeric values</td>
</tr>
<tr>
 <td style="text-align: left;">
james.stein     </td><td style="text-align: left;"> James-Stein shrinkage estimates of cell means from raw data</td>
</tr>
<tr>
 <td style="text-align: left;">
labcurve        </td><td style="text-align: left;"> Optimally label a set of curves that have been drawn on</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     an existing plot, on the basis of gaps between curves.</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     Also position legends automatically at emptiest rectangle.</td>
</tr>
<tr>
 <td style="text-align: left;">
label           </td><td style="text-align: left;"> Set or fetch a label for an <span class="rlang"><b>R</b></span>-object</td>
</tr>
<tr>
 <td style="text-align: left;">
Lag             </td><td style="text-align: left;"> Lag a vector, padding on the left with NA or ''</td>
</tr>
<tr>
 <td style="text-align: left;">
latex           </td><td style="text-align: left;"> Convert an <span class="rlang"><b>R</b></span> object to LaTeX (R Heiberger &amp; FE Harrell)</td>
</tr>
<tr>
 <td style="text-align: left;">
list.tree       </td><td style="text-align: left;"> Pretty-print the structure of any data object</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     (Alan Zaslavsky, <a href="mailto:zaslavsk@hcp.med.harvard.edu">zaslavsk@hcp.med.harvard.edu</a>)</td>
</tr>
<tr>
 <td style="text-align: left;">
Load            </td><td style="text-align: left;"> Enhancement of <code>load</code></td>
</tr>
<tr>
 <td style="text-align: left;">
mask            </td><td style="text-align: left;"> 8-bit logical representation of a short integer value</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     (Rick Becker)</td>
</tr>
<tr>
 <td style="text-align: left;">
matchCases      </td><td style="text-align: left;"> Match each case on one continuous variable</td>
</tr>
<tr>
 <td style="text-align: left;">
matxv           </td><td style="text-align: left;"> Fast matrix * vector, handling intercept(s) and NAs</td>
</tr>
<tr>
 <td style="text-align: left;">
mgp.axis        </td><td style="text-align: left;"> Version of axis() that uses appropriate mgp from </td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     mgp.axis.labels and gets around bug in axis(2, ...)</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     that causes it to assume las=1</td>
</tr>
<tr>
 <td style="text-align: left;">
mgp.axis.labels
                </td><td style="text-align: left;"> Used by survplot and plot in <span class="pkg">rms</span> library (and other</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     functions in the future) so that different spacing</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     between tick marks and axis tick mark labels may be</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     specified for x- and y-axes.  </td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     Use mgp.axis.labels('default') to set defaults.</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     Users can set values manually using </td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     mgp.axis.labels(x,y) where x and y are 2nd value of</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     par('mgp') to use.  Use mgp.axis.labels(type=w) to</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     retrieve values, where w='x', 'y', 'x and y', 'xy',</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     to get 3 mgp values (first 3 types) or 2 mgp.axis.labels.</td>
</tr>
<tr>
 <td style="text-align: left;">
minor.tick      </td><td style="text-align: left;"> Add minor tick marks to an existing plot</td>
</tr>
<tr>
 <td style="text-align: left;">
mtitle          </td><td style="text-align: left;"> Add outer titles and subtitles to a multiple plot layout</td>
</tr>
<tr>
 <td style="text-align: left;">
multLines       </td><td style="text-align: left;"> Draw multiple vertical lines at each x</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;"> in a line plot</td>
</tr>
<tr>
 <td style="text-align: left;">
%nin%         </td><td style="text-align: left;"> Opposite of %in%</td>
</tr>
<tr>
 <td style="text-align: left;">
nobsY           </td><td style="text-align: left;"> Compute no. non-NA observations for left hand formula side</td>
</tr>
<tr>
 <td style="text-align: left;">
nomiss          </td><td style="text-align: left;"> Return a matrix after excluding any row with an NA</td>
</tr>
<tr>
 <td style="text-align: left;">
panel.bpplot    </td><td style="text-align: left;"> Panel function for trellis bwplot - box-percentile plots</td>
</tr>
<tr>
 <td style="text-align: left;">
panel.plsmo     </td><td style="text-align: left;"> Panel function for trellis xyplot - uses plsmo</td>
</tr>
<tr>
 <td style="text-align: left;">
pBlock          </td><td style="text-align: left;"> Block variables for certain lattice charts</td>
</tr>
<tr>
 <td style="text-align: left;">
pc1             </td><td style="text-align: left;"> Compute first prin. component and get coefficients on</td>
</tr>
<tr>
 <td style="text-align: left;"></td><td style="text-align: left;">     original scale of variables</td>
</tr>
<tr>
 <td style="text-align: left;">
plotCorrPrecision  </td><td style="text-align: left;"> Plot precision of estimate of correlation coefficient</td>
</tr>
<tr>
 <td style="text-align: left;">
plsmo           </td><td style="text-align: left;"> Plot smoothed x vs. y with labeling and exclusion of NAs</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     Also allows a grouping variable and plots unsmoothed data</td>
</tr>
<tr>
 <td style="text-align: left;">
popower         </td><td style="text-align: left;"> Power and sample size calculations for ordinal responses</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     (two treatments, proportional odds model)</td>
</tr>
<tr>
 <td style="text-align: left;">
prn             </td><td style="text-align: left;"> prn(expression) does print(expression) but titles the</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     output with 'expression'.  Do prn(expression,txt) to add</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     a heading (&lsquo;txt&rsquo;) before the &lsquo;expression&rsquo; title</td>
</tr>
<tr>
 <td style="text-align: left;">
pstamp          </td><td style="text-align: left;"> Stamp a plot with date in lower right corner (pstamp())</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     Add ,pwd=T and/or ,time=T to add current directory </td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">      name or time</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     Put additional text for label as first argument, e.g.</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     pstamp('Figure 1')  will draw 'Figure 1  date'</td>
</tr>
<tr>
 <td style="text-align: left;">
putKey          </td><td style="text-align: left;"> Different way to use key()</td>
</tr>
<tr>
 <td style="text-align: left;">
putKeyEmpty     </td><td style="text-align: left;"> Put key at most empty part of existing plot</td>
</tr>
<tr>
 <td style="text-align: left;">
rcorr           </td><td style="text-align: left;"> Pearson or Spearman correlation matrix with pairwise deletion</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     of missing data</td>
</tr>
<tr>
 <td style="text-align: left;">
rcorr.cens      </td><td style="text-align: left;"> Somers' Dxy rank correlation with censored data</td>
</tr>
<tr>
 <td style="text-align: left;">
rcorrp.cens     </td><td style="text-align: left;"> Assess difference in concordance for paired predictors</td>
</tr>
<tr>
 <td style="text-align: left;">
rcspline.eval   </td><td style="text-align: left;"> Evaluate restricted cubic spline design matrix</td>
</tr>
<tr>
 <td style="text-align: left;">
rcspline.plot   </td><td style="text-align: left;"> Plot spline fit with nonparametric smooth and grouped estimates</td>
</tr>
<tr>
 <td style="text-align: left;">
rcspline.restate
                </td><td style="text-align: left;"> Restate restricted cubic spline in unrestricted form, and</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     create TeX expression to print the fitted function</td>
</tr>
<tr>
 <td style="text-align: left;">
reShape         </td><td style="text-align: left;"> Reshape a matrix into 3 vectors, reshape serial data</td>
</tr>
<tr>
 <td style="text-align: left;">
rm.boot         </td><td style="text-align: left;"> Bootstrap spline fit to repeated measurements model,</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     with simultaneous confidence region - least</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     squares using spline function in time</td>
</tr>
<tr>
 <td style="text-align: left;">
rMultinom       </td><td style="text-align: left;"> Generate multinomial random variables with varying prob.</td>
</tr>
<tr>
 <td style="text-align: left;">
samplesize.bin  </td><td style="text-align: left;"> Sample size for 2-sample binomial problem</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     (Rick Chappell, <a href="mailto:chappell@stat.wisc.edu">chappell@stat.wisc.edu</a>)</td>
</tr>
<tr>
 <td style="text-align: left;">
sas.get         </td><td style="text-align: left;"> Convert SAS dataset to S data frame</td>
</tr>
<tr>
 <td style="text-align: left;">
sasxport.get    </td><td style="text-align: left;"> Enhanced importing of SAS transport dataset in R</td>
</tr>
<tr>
 <td style="text-align: left;">
Save            </td><td style="text-align: left;"> Enhancement of <code>save</code></td>
</tr>
<tr>
 <td style="text-align: left;">
scat1d          </td><td style="text-align: left;"> Add 1-dimensional scatterplot to an axis of an existing plot</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     (like bar-codes, FEH/Martin Maechler, </td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     <a href="mailto:maechler@stat.math.ethz.ch">maechler@stat.math.ethz.ch</a>/Jens Oehlschlaegel-Akiyoshi,</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     <a href="mailto:oehl@psyres-stuttgart.de">oehl@psyres-stuttgart.de</a>)</td>
</tr>
<tr>
 <td style="text-align: left;">
score.binary    </td><td style="text-align: left;"> Construct a score from a series of binary variables or</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     expressions</td>
</tr>
<tr>
 <td style="text-align: left;">
sedit           </td><td style="text-align: left;"> A set of character handling functions written entirely</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     in <span class="rlang"><b>R</b></span>.  sedit() does much of what the UNIX sed</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     program does.  Other functions included are</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     substring.location, substring&lt;-, replace.string.wild,</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     and functions to check if a string is numeric or</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     contains only the digits 0-9</td>
</tr>
<tr>
 <td style="text-align: left;">
setTrellis      </td><td style="text-align: left;"> Set Trellis graphics to use blank conditioning panel strips,</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     line thickness 1 for dot plot reference lines: </td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     setTrellis(); 3 optional arguments</td>
</tr>
<tr>
 <td style="text-align: left;">
show.col        </td><td style="text-align: left;"> Show colors corresponding to col=0,1,...,99</td>
</tr>
<tr>
 <td style="text-align: left;">
show.pch        </td><td style="text-align: left;"> Show all plotting characters specified by pch=.</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     Just type show.pch() to draw the table on the </td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     current device.  </td>
</tr>
<tr>
 <td style="text-align: left;">
showPsfrag      </td><td style="text-align: left;"> Use LaTeX to compile, and dvips and ghostview to</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">    display a postscript graphic containing psfrag strings</td>
</tr>
<tr>
 <td style="text-align: left;">
solvet          </td><td style="text-align: left;"> Version of solve with argument tol passed to qr</td>
</tr>
<tr>
 <td style="text-align: left;">
somers2         </td><td style="text-align: left;"> Somers' rank correlation and c-index for binary y</td>
</tr>
<tr>
 <td style="text-align: left;">
spearman        </td><td style="text-align: left;"> Spearman rank correlation coefficient  spearman(x,y)</td>
</tr>
<tr>
 <td style="text-align: left;">
spearman.test   </td><td style="text-align: left;"> Spearman 1 d.f. and 2 d.f. rank correlation test</td>
</tr>
<tr>
 <td style="text-align: left;">
spearman2       </td><td style="text-align: left;"> Spearman multiple d.f. <code class="reqn">\rho^2</code>, adjusted <code class="reqn">\rho^2</code>, Wilcoxon-Kruskal-</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     Wallis test, for multiple predictors</td>
</tr>
<tr>
 <td style="text-align: left;">
spower          </td><td style="text-align: left;"> Simulate power of 2-sample test for survival under</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     complex conditions</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     Also contains the Gompertz2,Weibull2,Lognorm2
functions.</td>
</tr>
<tr>
 <td style="text-align: left;">
spss.get        </td><td style="text-align: left;"> Enhanced importing of SPSS files using read.spss
function </td>
</tr>
<tr>
 <td style="text-align: left;">
src             </td><td style="text-align: left;"> src(name) = source("name.s") with memory</td>
</tr>
<tr>
 <td style="text-align: left;">
store           </td><td style="text-align: left;"> store an object permanently (easy interface to assign function)</td>
</tr>
<tr>
 <td style="text-align: left;">
strmatch        </td><td style="text-align: left;"> Shortest unique identifier match </td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     (Terry Therneau, <a href="mailto:therneau@mayo.edu">therneau@mayo.edu</a>)</td>
</tr>
<tr>
 <td style="text-align: left;">
subset          </td><td style="text-align: left;"> More easily subset a data frame</td>
</tr>
<tr>
 <td style="text-align: left;">
substi          </td><td style="text-align: left;"> Substitute one var for another when observations NA</td>
</tr>
<tr>
 <td style="text-align: left;">
summarize       </td><td style="text-align: left;"> Generate a data frame containing stratified summary</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     statistics.  Useful for passing to trellis.</td>
</tr>
<tr>
 <td style="text-align: left;">
summary.formula </td><td style="text-align: left;"> General table making and plotting functions for summarizing</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;"> data</td>
</tr>
<tr>
 <td style="text-align: left;">
summaryD        </td><td style="text-align: left;"> Summarizing using user-provided formula and dotchart3</td>
</tr>
<tr>
 <td style="text-align: left;">
summaryM        </td><td style="text-align: left;"> Replacement for summary.formula(..., method='reverse')</td>
</tr>
<tr>
 <td style="text-align: left;">
summaryP        </td><td style="text-align: left;"> Multi-panel dot chart for summarizing proportions</td>
</tr>
<tr>
 <td style="text-align: left;">
summaryS        </td><td style="text-align: left;"> Summarize multiple response variables for multi-panel</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;"> dot chart or scatterplot</td>
</tr>
<tr>
 <td style="text-align: left;">
summaryRc       </td><td style="text-align: left;"> Summary for continuous variables using lowess</td>
</tr>
<tr>
 <td style="text-align: left;">
symbol.freq     </td><td style="text-align: left;"> X-Y Frequency plot with circles' area prop. to frequency</td>
</tr>
<tr>
 <td style="text-align: left;">
sys             </td><td style="text-align: left;"> Execute unix() or dos() depending on what's running</td>
</tr>
<tr>
 <td style="text-align: left;">
tabulr          </td><td style="text-align: left;"> Front-end to tabular function in the tables package</td>
</tr>
<tr>
 <td style="text-align: left;">
tex             </td><td style="text-align: left;"> Enclose a string with the correct syntax for using</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">    with the LaTeX psfrag package, for postscript graphics</td>
</tr>
<tr>
 <td style="text-align: left;">
transace        </td><td style="text-align: left;"> ace() packaged for easily automatically transforming all</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     variables in a matrix</td>
</tr>
<tr>
 <td style="text-align: left;">
transcan        </td><td style="text-align: left;"> automatic transformation and imputation of NAs for a</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     series of predictor variables</td>
</tr>
<tr>
 <td style="text-align: left;">
trap.rule       </td><td style="text-align: left;"> Area under curve defined by arbitrary x and y vectors,</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;"> using trapezoidal rule</td>
</tr>
<tr>
 <td style="text-align: left;">
trellis.strip.blank
                </td><td style="text-align: left;"> To make the strip titles in trellis more visible, you can </td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     make the backgrounds blank by saying trellis.strip.blank().</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     Use before opening the graphics device.</td>
</tr>
<tr>
 <td style="text-align: left;">
t.test.cluster  </td><td style="text-align: left;"> 2-sample t-test for cluster-randomized observations</td>
</tr>
<tr>
 <td style="text-align: left;">
uncbind         </td><td style="text-align: left;"> Form individual variables from a matrix</td>
</tr>
<tr>
 <td style="text-align: left;">
upData          </td><td style="text-align: left;"> Update a data frame (change names, labels, remove vars, etc.)</td>
</tr>
<tr>
 <td style="text-align: left;">
units           </td><td style="text-align: left;"> Set or fetch "units" attribute - units of measurement for var.</td>
</tr>
<tr>
 <td style="text-align: left;">
varclus         </td><td style="text-align: left;"> Graph hierarchical clustering of variables using squared</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;"> Pearson or Spearman correlations or Hoeffding D as similarities</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;"> Also includes the naclus function for examining similarities in</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;"> patterns of missing values across variables.</td>
</tr>
<tr>
 <td style="text-align: left;">
wtd.mean </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;"> wtd.var </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">  wtd.quantile </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">  wtd.Ecdf </td><td style="text-align: left;">
</td>
</tr>
<tr>
 <td style="text-align: left;"> wtd.table </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">  wtd.rank </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;"> wtd.loess.noiter </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
num.denom.setup </td><td style="text-align: left;"> Set of function for obtaining weighted estimates</td>
</tr>
<tr>
 <td style="text-align: left;">
xy.group        </td><td style="text-align: left;"> Compute mean x vs. function of y by groups of x</td>
</tr>
<tr>
 <td style="text-align: left;">
xYplot          </td><td style="text-align: left;"> Like trellis xyplot but supports error bars and multiple</td>
</tr>
<tr>
 <td style="text-align: left;">
                </td><td style="text-align: left;">     response variables that are connected as separate lines</td>
</tr>
<tr>
 <td style="text-align: left;">
ynbind          </td><td style="text-align: left;"> Combine a series of yes/no true/false present/absent variables into a matrix</td>
</tr>
<tr>
 <td style="text-align: left;">
zoom            </td><td style="text-align: left;"> Zoom in on any graphical display</td>
</tr>
<tr>
 <td style="text-align: left;"> </td><td style="text-align: left;">     (Bill Dunlap, <a href="mailto:bill@statsci.com">bill@statsci.com</a>)
</td>
</tr>

</table>


<h3>Copyright Notice</h3>

<p><b>GENERAL DISCLAIMER</b><br />
This program is free software; you can redistribute it
and/or modify it under the terms of the GNU General Public
License as published by the Free Software Foundation; either
version 2, or (at your option) any later version.<br />
</p>
<p>This program is distributed in the hope that it will be
useful, but WITHOUT ANY WARRANTY; without even the implied
warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR
PURPOSE.  See the GNU General Public License for more
details.<br />
</p>
<p>In short: You may use it any way you like, as long as you
don't charge money for it, remove this notice, or hold anyone liable
for its results.  Also, please acknowledge the source and communicate
changes to the author.<br />
</p>
<p>If this software is used is work presented for publication, kindly
reference it using for example:<br />
Harrell FE (2014): Hmisc: A package of miscellaneous R functions.
Programs available from <a href="https://hbiostat.org/R/Hmisc/">https://hbiostat.org/R/Hmisc/</a>.<br />
Be sure to reference <span class="rlang"><b>R</b></span> itself and other libraries used.
</p>


<h3>Author(s)</h3>

<p>Frank E Harrell Jr<br />
Professor of Biostatistics<br />
Vanderbilt University School of Medicine<br />
Nashville, Tennessee<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>See Alzola CF, Harrell FE (2004): An Introduction to S and the
Hmisc and Design Libraries at
<a href="https://hbiostat.org/R/doc/sintro.pdf">https://hbiostat.org/R/doc/sintro.pdf</a>
for extensive documentation and examples for the Hmisc package.
</p>

<hr>
<h2 id='hoeffd'>
Matrix of Hoeffding's D Statistics
</h2><span id='topic+hoeffd'></span><span id='topic+print.hoeffd'></span>

<h3>Description</h3>

<p>Computes a matrix of Hoeffding's (1948) <code>D</code> statistics for all
possible pairs of columns of a matrix.  <code>D</code> is a measure of the
distance between <code>F(x,y)</code> and <code>G(x)H(y)</code>, where <code>F(x,y)</code>
is the joint CDF of <code>X</code> and <code>Y</code>, and <code>G</code> and <code>H</code> are
marginal CDFs. Missing values are deleted in pairs rather than deleting
all rows of <code>x</code> having any missing variables.  The <code>D</code>
statistic is robust against a wide variety of alternatives to
independence, such as non-monotonic relationships.  The larger the value
of <code>D</code>, the more dependent are <code>X</code> and <code>Y</code> (for many
types of dependencies).  <code>D</code> used here is 30 times Hoeffding's
original <code>D</code>, and ranges from -0.5 to 1.0 if there are no ties in
the data.  <code>print.hoeffd</code> prints the information derived by
<code>hoeffd</code>.  The higher the value of <code>D</code>, the more dependent are
<code>x</code> and <code>y</code>.  <code>hoeffd</code> also computes the mean and maximum
absolute values of the difference between the joint empirical CDF and
the product of the marginal empirical CDFs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hoeffd(x, y)
## S3 method for class 'hoeffd'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hoeffd_+3A_x">x</code></td>
<td>

<p>a numeric matrix with at least 5 rows and at least 2 columns (if
<code>y</code> is absent), or an object created by <code>hoeffd</code>
</p>
</td></tr>
<tr><td><code id="hoeffd_+3A_y">y</code></td>
<td>

<p>a numeric vector or matrix which will be concatenated to <code>x</code>
</p>
</td></tr>
<tr><td><code id="hoeffd_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses midranks in case of ties, as described by Hollander and Wolfe.
P-values are approximated by linear interpolation on the table
in Hollander and Wolfe, which uses the asymptotically equivalent
Blum-Kiefer-Rosenblatt statistic.  For <code>P&lt;.0001</code> or <code>&gt;0.5</code>, <code>P</code> values are
computed using a well-fitting linear regression function in <code>log P</code> vs.
the test statistic.
Ranks (but not bivariate ranks) are computed using efficient
algorithms (see reference 3).
</p>


<h3>Value</h3>

<p>a list with elements <code>D</code>, the
matrix of D statistics, <code>n</code> the
matrix of number of observations used in analyzing each pair of variables,
and <code>P</code>, the asymptotic P-values.
Pairs with fewer than 5 non-missing values have the D statistic set to NA.
The diagonals of <code>n</code> are the number of non-NAs for the single variable
corresponding to that row and column.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Hoeffding W. (1948): A non-parametric test of independence.  Ann Math Stat
19:546&ndash;57.
</p>
<p>Hollander M. and Wolfe D.A. (1973).  Nonparametric Statistical Methods,
pp. 228&ndash;235, 423. New York: Wiley.
</p>
<p>Press WH, Flannery BP, Teukolsky SA, Vetterling, WT (1988): Numerical
Recipes in C.  Cambridge: Cambridge University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rcorr">rcorr</a></code>, <code><a href="#topic+varclus">varclus</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(-2, -1, 0, 1, 2)
y &lt;- c(4,   1, 0, 1, 4)
z &lt;- c(1,   2, 3, 4, NA)
q &lt;- c(1,   2, 3, 4, 5)
hoeffd(cbind(x,y,z,q))


# Hoeffding's test can detect even one-to-many dependency
set.seed(1)
x &lt;- seq(-10,10,length=200)
y &lt;- x*sign(runif(200,-1,1))
plot(x,y)
hoeffd(x,y)
</code></pre>

<hr>
<h2 id='html'>Convert an S object to HTML</h2><span id='topic+html'></span><span id='topic+html.latex'></span><span id='topic+html.data.frame'></span><span id='topic+html.default'></span><span id='topic+htmlVerbatim'></span>

<h3>Description</h3>

<p><code>html</code> is a generic function, for which only two methods are currently
implemented, <code>html.latex</code> and a rudimentary
<code>html.data.frame</code>.  The former uses the <code>HeVeA</code> LaTeX to HTML 
translator by Maranget to create an HTML file from a LaTeX file like
the one produced by <code>latex</code>.  <code>html.default</code> just runs
<code>html.data.frame</code>. 
<code>htmlVerbatim</code> prints all of its arguments to the console in an
html verbatim environment, using a specified percent of the prevailing
character size.  This is useful for R Markdown with <code>knitr</code>.
</p>
<p>Most of the html-producing functions in the Hmisc and rms packages
return a character vector passed through <code>htmltools::HTML</code> so that
<code>kintr</code> will correctly format the result without the need for the
user putting <code>results='asis'</code> in the chunk header.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>html(object, ...)
## S3 method for class 'latex'
html(object, file, where=c('cwd', 'tmp'),
  method=c('hevea', 'htlatex'),
  rmarkdown=FALSE, cleanup=TRUE, ...)
## S3 method for class 'data.frame'
html(object,
  file=paste(first.word(deparse(substitute(object))),'html',sep='.'), header,
     caption=NULL, rownames=FALSE, align='r', align.header='c',
     bold.header=TRUE, col.header='Black',
     border=2, width=NULL, size=100, translate=FALSE,
     append=FALSE, link=NULL, linkCol=1,
     linkType=c('href','name'), disableq=FALSE, ...) 
## Default S3 method:
html(object,
     file=paste(first.word(deparse(substitute(object))),'html',sep='.'),
     append=FALSE, link=NULL, linkCol=1, linkType=c('href','name'), ...)
htmlVerbatim(..., size=75, width=85, scroll=FALSE, rows=10, cols=100,
             propts=NULL, omit1b=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="html_+3A_object">object</code></td>
<td>
<p>a data frame or an object created by <code>latex</code>.
For the generic <code>html</code> is any object for which an <code>html</code>
method exists.</p>
</td></tr> 
<tr><td><code id="html_+3A_file">file</code></td>
<td>

<p>name of the file to create.  The default file
name is <code>object.html</code> where <code>object</code> is the first word in
the name of the argument for <code>object</code>.  For <code>html.latex</code>
specify <code>file=''</code> or <code>file=character(0)</code> to print html code to
the console, as when using <code>knitr</code>.  For the <code>data.frame</code>
method, <code>file</code> may be set to <code>FALSE</code> which causes a character
vector enclosed in <code>htmltools::HTML</code> to be returned instead of
writing to the console. 
</p>
</td></tr>
<tr><td><code id="html_+3A_where">where</code></td>
<td>
<p>for <code>html</code>.  Default is to put output files in current
working directory.  Specify <code>where='tmp'</code> to put in a system
temporary directory area.</p>
</td></tr>
<tr><td><code id="html_+3A_method">method</code></td>
<td>
<p>default is to use system command <code>hevea</code> to convert
from LaTeX to html.  Specify<code>method='htlatex'</code> to use system
command <code>htlatex</code>, assuming the 
system package <code>TeX4ht</code> is installed.</p>
</td></tr>
<tr><td><code id="html_+3A_rmarkdown">rmarkdown</code></td>
<td>
<p>set to <code>TRUE</code> if using RMarkdown (usually under
<code>knitr</code> and RStudio).  This causes html to be packaged for
RMarkdown and output to go into the console stream.  <code>file</code> is
ignored when <code>rmarkdown=TRUE</code>.</p>
</td></tr>
<tr><td><code id="html_+3A_cleanup">cleanup</code></td>
<td>
<p>if using <code>method='htlatex'</code> set to <code>FALSE</code> if
<code>where='cwd'</code> to	prevent deletion of auxiliary files created by
<code>htlatex</code> that are not needed when using the final <code>html</code>
document (only the <code>.css</code> file is needed in addition to
<code>.html</code>).  If using <code>method='hevea'</code>, <code>cleanup=TRUE</code>
causes deletion of the generated <code>.haux</code> file.</p>
</td></tr>
<tr><td><code id="html_+3A_header">header</code></td>
<td>
<p>vector of column names.  Defaults to names in
<code>object</code>.  Set to <code>NULL</code> to suppress column names.</p>
</td></tr>
<tr><td><code id="html_+3A_caption">caption</code></td>
<td>
<p>a character string to be used as a caption before the
table</p>
</td></tr>
<tr><td><code id="html_+3A_rownames">rownames</code></td>
<td>
<p>set to <code>FALSE</code> to ignore row names even if they are
present</p>
</td></tr>
<tr><td><code id="html_+3A_align">align</code></td>
<td>
<p>alignment for table columns (all are assumed to have the
same if is a scalar).  Specify <code>"c", "r", "l"</code> for center, right, or left
alignment.</p>
</td></tr>
<tr><td><code id="html_+3A_align.header">align.header</code></td>
<td>
<p>same coding as for <code>align</code> but pertains to
header</p>
</td></tr>
<tr><td><code id="html_+3A_bold.header">bold.header</code></td>
<td>
<p>set to <code>FALSE</code> to not bold face column headers</p>
</td></tr>
<tr><td><code id="html_+3A_col.header">col.header</code></td>
<td>
<p>color for column headers</p>
</td></tr>
<tr><td><code id="html_+3A_border">border</code></td>
<td>
<p>set to 0 to not include table cell borders, 1 to include
only outer borders, or 2 (the default) to put borders around cells too</p>
</td></tr>
<tr><td><code id="html_+3A_translate">translate</code></td>
<td>
<p>set to <code>TRUE</code> to run header and table cell text
through the <code>htmlTranslate</code> function</p>
</td></tr>
<tr><td><code id="html_+3A_width">width</code></td>
<td>
<p>optional table width for <code>html.data.frame</code>.  For full
page width use <code>width="100%"</code>, for use in <code>options()</code> for
printing objects.</p>
</td></tr>
<tr><td><code id="html_+3A_size">size</code></td>
<td>
<p>a number between 0 and 100 representing the percent of the
prevailing character size to be used by <code>htmlVerbatim</code> and the
data frame method.</p>
</td></tr>
<tr><td><code id="html_+3A_append">append</code></td>
<td>
<p>set to <code>TRUE</code> to append to an existing file</p>
</td></tr>
<tr><td><code id="html_+3A_link">link</code></td>
<td>
<p>character vector specifying hyperlink names to attach to
selected elements of the matrix or data frame.  No hyperlinks are used
if <code>link</code> is omitted or for elements of <code>link</code> that are
<code>""</code>.  To allow multiple links per link, <code>link</code> may also be
a character matrix shaped as <code>object</code> in which case
<code>linkCol</code> is ignored.</p>
</td></tr>
<tr><td><code id="html_+3A_linkcol">linkCol</code></td>
<td>
<p>column number of <code>object</code> to which hyperlinks are
attached.  Defaults to first column.</p>
</td></tr>
<tr><td><code id="html_+3A_linktype">linkType</code></td>
<td>
<p>defaults to <code>"href"</code></p>
</td></tr>
<tr><td><code id="html_+3A_disableq">disableq</code></td>
<td>
<p>set to <code>TRUE</code> to add code to the html table tag
that makes Quarto not use its usual table style</p>
</td></tr>
<tr><td><code id="html_+3A_...">...</code></td>
<td>
<p>ignored except for <code>htmlVerbatim</code> - is a list of
objects to <code>print()</code></p>
</td></tr>
<tr><td><code id="html_+3A_scroll">scroll</code></td>
<td>
<p>set to <code>TRUE</code> to put the html in a scrollable
<code>textarea</code></p>
</td></tr>
<tr><td><code id="html_+3A_rows">rows</code>, <code id="html_+3A_cols">cols</code></td>
<td>
<p>the number of rows and columns to devote to the visable
part of the scrollable box</p>
</td></tr>
<tr><td><code id="html_+3A_propts">propts</code></td>
<td>
<p>options, besides <code>quote=FALSE</code> to pass to the
<code>print</code> method, for <code>htmlVerbatim</code></p>
</td></tr>
<tr><td><code id="html_+3A_omit1b">omit1b</code></td>
<td>
<p>for <code>htmlVerbatim</code> if <code>TRUE</code> causes an initial
and a final line of output that is all blank to be deleted</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Frank E. Harrell, Jr.
<br />
Department of Biostatistics,
<br />
Vanderbilt University,
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Maranget, Luc.  HeVeA: a LaTeX to HTML translater.
URL: http://para.inria.fr/~maranget/hevea/
</p>


<h3>See Also</h3>

<p><code><a href="#topic+latex">latex</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
x &lt;- matrix(1:6, nrow=2, dimnames=list(c('a','b'),c('c','d','e')))
w &lt;- latex(x)
h &lt;- html(w) # run HeVeA to convert .tex to .html
h &lt;- html(x) # convert x directly to html
w &lt;- html(x, link=c('','B'))   # hyperlink first row first col to B

# Assuming system package tex4ht is installed, easily convert advanced
# LaTeX tables to html
getHdata(pbc)
s &lt;- summaryM(bili + albumin + stage + protime + sex + age + spiders ~ drug,
              data=pbc, test=TRUE)
w &lt;- latex(s, npct='slash', file='s.tex')
z &lt;- html(w)
browseURL(z$file)

d &lt;- describe(pbc)
w &lt;- latex(d, file='d.tex')
z &lt;- html(w)
browseURL(z$file)

## End(Not run)
</code></pre>

<hr>
<h2 id='htmltabv'>htmltabc</h2><span id='topic+htmltabv'></span>

<h3>Description</h3>

<p>Simple HTML Table of Verbatim Output
</p>


<h3>Usage</h3>

<pre><code class='language-R'>htmltabv(..., cols = 2, propts = list(quote = FALSE))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="htmltabv_+3A_...">...</code></td>
<td>
<p>objects to <code>print()</code>.  The arguments must be named with the labels you want to print before the verbatim <code>print()</code>.</p>
</td></tr>
<tr><td><code id="htmltabv_+3A_cols">cols</code></td>
<td>
<p>number of columns in the html table</p>
</td></tr>
<tr><td><code id="htmltabv_+3A_propts">propts</code></td>
<td>
<p>an option list of arguments to pass to the <code>print()</code> methods; default is to not quote character strings</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses <code>capture.output</code> to capture as character strings the results of
running <code>print()</code> on each element of <code>...</code>.  If an element of <code>...</code> has
length of 1 and is a blank string, nothing is printed for that cell
other than its name (not in verbatim).
</p>


<h3>Value</h3>

<p>character string of html
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

<hr>
<h2 id='impute'>
Generic Functions and Methods for Imputation
</h2><span id='topic+impute'></span><span id='topic+impute.default'></span><span id='topic+print.impute'></span><span id='topic+summary.impute'></span><span id='topic++5B.impute'></span><span id='topic+is.imputed'></span>

<h3>Description</h3>

<p>These functions do simple and <code>transcan</code> 
imputation and print, summarize, and subscript
variables that have NAs filled-in with imputed values.  The simple
imputation method involves filling in NAs with constants,
with a specified single-valued function of the non-NAs, or from
a sample (with replacement) from the non-NA values (this is useful
in multiple imputation).
More complex imputations can be done
with the <code>transcan</code> function, which also works with the generic methods
shown here, i.e., <code>impute</code> can take a <code>transcan</code> object and use  the
imputed values created by <code>transcan</code> (with <code>imputed=TRUE</code>)  to fill-in NAs.
The <code>print</code> method places * after variable values that were imputed.
The <code>summary</code> method summarizes all imputed values and then uses
the next <code>summary</code> method available for the variable.
The subscript method preserves attributes of the variable and subsets
the list of imputed values corresponding with how the variable was
subsetted.  The <code>is.imputed</code> function is for checking if observations
are imputed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>impute(x, ...)

## Default S3 method:
impute(x, fun=median, ...)

## S3 method for class 'impute'
print(x, ...)

## S3 method for class 'impute'
summary(object, ...)

is.imputed(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="impute_+3A_x">x</code></td>
<td>

<p>a vector or an object created by <code>transcan</code>, or a vector needing
basic unconditional imputation.  If there are no <code>NA</code>s and <code>x</code>
is a vector, it is returned unchanged.
</p>
</td></tr>
<tr><td><code id="impute_+3A_fun">fun</code></td>
<td>

<p>the name of a function to use in computing the (single) 
imputed value from the non-NAs.  The default is <code>median</code>.
If instead of specifying a function as <code>fun</code>, a single value or vector
(numeric, or character if <code>object</code> is a factor) is specified,
those values are used for insertion.  <code>fun</code> can also be the character
string <code>"random"</code> to draw random values for imputation, with the random
values not forced to be the same if there are multiple NAs.
For a vector of constants, the vector must be of length one
(indicating the same value replaces all NAs) or must be as long as
the number of NAs, in which case the values correspond to consecutive NAs
to replace.  For a factor <code>object</code>, constants for imputation may include
character values not in the current levels of <code>object</code>.  In that
case new levels are added.
If <code>object</code> is of class <code>"factor"</code>, <code>fun</code> is ignored and the
most frequent category is used for imputation.
</p>
</td></tr>
<tr><td><code id="impute_+3A_object">object</code></td>
<td>
<p>an object of class <code>"impute"</code></p>
</td></tr>
<tr><td><code id="impute_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector with class <code>"impute"</code> placed in front of existing classes.
For <code>is.imputed</code>, a vector of logical values is returned (all
<code>TRUE</code> if <code>object</code> is not of class <code>impute</code>).
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+transcan">transcan</a></code>, <code><a href="#topic+impute.transcan">impute.transcan</a></code>, <code><a href="#topic+describe">describe</a></code>, <code><a href="#topic+na.include">na.include</a></code>, <code><a href="base.html#topic+sample">sample</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>age &lt;- c(1,2,NA,4)
age.i &lt;- impute(age)
# Could have used impute(age,2.5), impute(age,mean), impute(age,"random")
age.i
summary(age.i)
is.imputed(age.i)
</code></pre>

<hr>
<h2 id='intMarkovOrd'>intMarkovOrd</h2><span id='topic+intMarkovOrd'></span>

<h3>Description</h3>

<p>Compute Parameters for Proportional Odds Markov Model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>intMarkovOrd(
  y,
  times,
  initial,
  absorb = NULL,
  intercepts,
  extra = NULL,
  g,
  target,
  t,
  ftarget = NULL,
  onlycrit = FALSE,
  constraints = NULL,
  printsop = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="intMarkovOrd_+3A_y">y</code></td>
<td>
<p>vector of possible y values in order (numeric, character, factor)</p>
</td></tr>
<tr><td><code id="intMarkovOrd_+3A_times">times</code></td>
<td>
<p>vector of measurement times</p>
</td></tr>
<tr><td><code id="intMarkovOrd_+3A_initial">initial</code></td>
<td>
<p>initial value of <code>y</code> (baseline state; numeric, character, or factor matching <code>y</code>).  If length 1 this value is used for all subjects, otherwise it is a vector of length <code>n</code>.</p>
</td></tr>
<tr><td><code id="intMarkovOrd_+3A_absorb">absorb</code></td>
<td>
<p>vector of absorbing states, a subset of <code>y</code> (numeric, character, or factor matching <code>y</code>).  The default is no absorbing states.  Observations are truncated when an absorbing state is simulated.</p>
</td></tr>
<tr><td><code id="intMarkovOrd_+3A_intercepts">intercepts</code></td>
<td>
<p>vector of initial guesses for the intercepts</p>
</td></tr>
<tr><td><code id="intMarkovOrd_+3A_extra">extra</code></td>
<td>
<p>an optional vector of intial guesses for other parameters passed to <code>g</code> such as regression coefficients for previous states and for general time trends.  Name the elements of <code>extra</code> for more informative output.</p>
</td></tr>
<tr><td><code id="intMarkovOrd_+3A_g">g</code></td>
<td>
<p>a user-specified function of three or more arguments which in order are <code>yprev</code> - the value of <code>y</code> at the previous time, the current time <code>t</code>, the <code>gap</code> between the previous time and the current time, an optional (usually named) covariate vector <code>X</code>, and optional arguments such as a regression coefficient value to simulate from.  The function needs to allow <code>yprev</code> to be a vector and <code>yprev</code> must not include any absorbing states.  The <code>g</code> function returns the linear predictor for the proportional odds model aside from <code>intercepts</code>.  The returned value must be a matrix with row names taken from <code>yprev</code>.  If the model is a proportional odds model, the returned value must be one column.  If it is a partial proportional odds model, the value must have one column for each distinct value of the response variable Y after the first one, with the levels of Y used as optional column names.  So columns correspond to <code>intercepts</code>. The different columns are used for <code>y</code>-specific contributions to the linear predictor (aside from <code>intercepts</code>) for a partial or constrained partial proportional odds model.  Parameters for partial proportional odds effects may be included in the ... arguments.</p>
</td></tr>
<tr><td><code id="intMarkovOrd_+3A_target">target</code></td>
<td>
<p>vector of target state occupancy probabilities at time <code>t</code>.  If <code>extra</code> is specified, <code>target</code> must be a matrix where row names are character versions of <code>t</code> and columns represent occupancy probabilities corresponding to values of <code>y</code> at the time given in the row.</p>
</td></tr>
<tr><td><code id="intMarkovOrd_+3A_t">t</code></td>
<td>
<p>target times.  Can have more than one element only if <code>extra</code> is given.</p>
</td></tr>
<tr><td><code id="intMarkovOrd_+3A_ftarget">ftarget</code></td>
<td>
<p>an optional function defining constraints that relate to transition probabilities.  The function returns a penalty which is a sum of absolute differences in probabilities from target probabilities over possibly multiple targets.  The <code>ftarget</code> function must have two arguments: <code>intercepts</code> and <code>extra</code>.</p>
</td></tr>
<tr><td><code id="intMarkovOrd_+3A_onlycrit">onlycrit</code></td>
<td>
<p>set to <code>TRUE</code> to only return the achieved objective criterion and not print anything</p>
</td></tr>
<tr><td><code id="intMarkovOrd_+3A_constraints">constraints</code></td>
<td>
<p>a function of two arguments: the vector of current intercept values and the vector of <code>extra</code> parameters, returning <code>TRUE</code> if that vector meets the constrains and <code>FALSE</code> otherwise</p>
</td></tr>
<tr><td><code id="intMarkovOrd_+3A_printsop">printsop</code></td>
<td>
<p>set to <code>TRUE</code> to print solved-for state occupancy probabilities for groups 1 and 2 and log odds ratios corresponding to them</p>
</td></tr>
<tr><td><code id="intMarkovOrd_+3A_...">...</code></td>
<td>
<p>optional arguments to pass to <code><a href="stats.html#topic+nlm">stats::nlm()</a></code>.  If this is specified, the arguments that <code>intMarkovOrd</code> normally sends to <code>nlm</code> are not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a vector <code>intercepts</code> of initial guesses at the intercepts in a Markov proportional odds model, and a vector <code>extra</code> if there are other parameters, solves for the <code>intercepts</code> and <code>extra</code> vectors that yields a set of occupancy probabilities at time <code>t</code> that equal, as closely as possible, a vector of target values.
</p>


<h3>Value</h3>

<p>list containing two vectors named <code>intercepts</code> and <code>extra</code> unless <code>oncrit=TRUE</code> in which case the best achieved sum of absolute errors is returned
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><a href="https://hbiostat.org/R/Hmisc/markov/">https://hbiostat.org/R/Hmisc/markov/</a>
</p>

<hr>
<h2 id='knitrSet'>knitr Setup and plotly Service Function</h2><span id='topic+knitrSet'></span><span id='topic+plotlySave'></span>

<h3>Description</h3>

<p><code>knitrSet</code> sets up knitr to use better default parameters for base graphics,
better code formatting, and to allow several arguments to be passed
from code chunk headers, such as <code>bty</code>, <code>mfrow</code>, <code>ps</code>,
<code>bot</code> (extra bottom margin for base graphics), <code>top</code> (extra
top margin), <code>left</code> (extra left margin), <code>rt</code> (extra right
margin), <code>lwd</code>, <code>mgp</code>, <code>las</code>, <code>tcl</code>, <code>axes</code>,
<code>xpd</code>, <code>h</code> (usually <code>fig.height</code> in knitr),
<code>w</code> (usually <code>fig.width</code> in knitr), <code>wo</code>
(<code>out.width</code> in knitr), <code>ho</code> (<code>out.height</code> in knitr),
<code>cap</code> (character
string containing figure caption), <code>scap</code> (character string
containing short figure caption for table of figures).  The
<code>capfile</code> argument facilities auto-generating a table of figures
for certain Rmarkdown report themes.  This is done by the addition of
a hook function that appends data to the <code>capfile</code> file each time
a chunk runs that has a long or short caption in the chunk header.
</p>
<p><code>plotlySave</code> saves a plotly graphic with name <code>foo.png</code>
where <code>foo</code> is the name of the current chunk.  You must have a
free <code>plotly</code> account from <code>plot.ly</code> to use this function,
and you must have run 
<code>Sys.setenv(plotly_username="your_plotly_username")</code> and
<code>Sys.setenv(plotly_api_key="your_api_key")</code>. The API key can be
found in one's profile settings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>knitrSet(basename=NULL, w=if(! bd) 4, h=if(! bd) 3, wo=NULL, ho=NULL,
         fig.path=if(length(basename)) basename else '',
         fig.align=if(! bd) 'center', fig.show='hold',
         fig.pos=if(! bd) 'htbp',
         fig.lp    = if(! bd) paste('fig', basename, sep=':'),
         dev=switch(lang, latex='pdf', markdown='png',
                    blogdown=NULL, quarto=NULL),
         tidy=FALSE, error=FALSE,
         messages=c('messages.txt', 'console'),
         width=61, decinline=5, size=NULL, cache=FALSE,
         echo=TRUE, results='markup', capfile=NULL,
         lang=c('latex','markdown','blogdown','quarto'))

plotlySave(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="knitrSet_+3A_basename">basename</code></td>
<td>
<p>base name to be added in front of graphics file
names. <code>basename</code> is followed by a minus sign.</p>
</td></tr>
<tr><td><code id="knitrSet_+3A_w">w</code>, <code id="knitrSet_+3A_h">h</code></td>
<td>
<p>default figure width and height in inches</p>
</td></tr>
<tr><td><code id="knitrSet_+3A_wo">wo</code>, <code id="knitrSet_+3A_ho">ho</code></td>
<td>
<p>default figure rendering width and height, in integer
pixels or percent as a character string, e.g. <code>'40%'</code></p>
</td></tr>
<tr><td><code id="knitrSet_+3A_fig.path">fig.path</code></td>
<td>
<p>path for figures.  To put figures in a subdirectory
specify e.g. <code>fig.path='folder/'</code>.  Ignored for blogdown.</p>
</td></tr>
<tr><td><code id="knitrSet_+3A_fig.align">fig.align</code>, <code id="knitrSet_+3A_fig.show">fig.show</code>, <code id="knitrSet_+3A_fig.pos">fig.pos</code>, <code id="knitrSet_+3A_fig.lp">fig.lp</code>, <code id="knitrSet_+3A_tidy">tidy</code>, <code id="knitrSet_+3A_cache">cache</code>, <code id="knitrSet_+3A_echo">echo</code>, <code id="knitrSet_+3A_results">results</code>, <code id="knitrSet_+3A_error">error</code>, <code id="knitrSet_+3A_size">size</code></td>
<td>
<p>see knitr documentation</p>
</td></tr> 
<tr><td><code id="knitrSet_+3A_dev">dev</code></td>
<td>
<p>graphics device, with default figured from <code>lang</code></p>
</td></tr>
<tr><td><code id="knitrSet_+3A_messages">messages</code></td>
<td>
<p>By default warning and other messages such as those
from loading packages are sent to file <code>'messages.txt'</code> in the
current working directory.  You can specify
<code>messages='console'</code> to send them directly to the console.</p>
</td></tr>
<tr><td><code id="knitrSet_+3A_width">width</code></td>
<td>
<p>text output width for R code and output</p>
</td></tr>
<tr><td><code id="knitrSet_+3A_decinline">decinline</code></td>
<td>
<p>number of digits to the right of the decimal point to
round numeric values appearing inside Sexpr</p>
</td></tr>
<tr><td><code id="knitrSet_+3A_capfile">capfile</code></td>
<td>
<p>the name of a file in the current working directory
that is used to accumulate chunk labels, figure cross-reference
tags, and figure short captions (long captions if no short caption
is defined) for the purpose of using
<code>markupSpecs$markdown$tof()</code> to insert a table of figures in a
report.  The file as appended to, which is useful if
<code>cache=TRUE</code> is used since this will keep some chunks from
running.  The <code>tof</code> function will remove earlier duplicated
figure tags if this is the case.  If not <code>cache</code>ing, the user
should initialize the file to empty at the top of the script.</p>
</td></tr>
<tr><td><code id="knitrSet_+3A_lang">lang</code></td>
<td>
<p>Default is <code>'latex'</code> to use LaTeX.  Set to
<code>'markdown'</code> when using R Markdown or <code>'blogdown'</code> or
<code>'quarto'</code>.  For 
<code>'blogdown'</code> and <code>'quarto'</code>, <code>par</code> and <code>knitr</code>
graphics-related hooks are not called as this would prevent
writing graphics files in the correct directory
for the blog system.</p>
</td></tr>
<tr><td><code id="knitrSet_+3A_x">x</code></td>
<td>
<p>a <code>plotly</code> graphics object or a named list of such
objects.  The resulting <code>png</code> file will go in the file path
given by the <code>knitr</code> <code>fig.path</code> value, and have a base
name equal to the current <code>knitr</code> chunk name.  If <code>x</code> is a
list, a minus sign followed by the chunk name are inserted before
<code>.png</code>.</p>
</td></tr>
<tr><td><code id="knitrSet_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code>plotly::plotly_IMAGE</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="knitr.html#topic+knit">knit</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Typical call (without # comment symbols):
# &lt;&lt;echo=FALSE&gt;&gt;=
# require(Hmisc)
# knitrSet()
# @

knitrSet()    # use all defaults and don't use a graphics file prefix
knitrSet('modeling')   # use modeling- prefix for a major section or chapter
knitrSet(cache=TRUE, echo=FALSE)  # global default to cache and not print code
knitrSet(w=5,h=3.75)   # override default figure width, height

# ```{r chunkname}
# p &lt;- plotly::plot_ly(...)
# plotlySave(p)   # creates fig.path/chunkname.png

## End(Not run)
</code></pre>

<hr>
<h2 id='labcurve'>Label Curves, Make Keys, and Interactively Draw Points and Curves</h2><span id='topic+labcurve'></span><span id='topic+putKey'></span><span id='topic+putKeyEmpty'></span><span id='topic+largest.empty'></span><span id='topic+drawPlot'></span><span id='topic+plot.drawPlot'></span><span id='topic+bezier'></span>

<h3>Description</h3>

<p><code>labcurve</code> optionally draws a set of curves then labels the curves.
A variety of methods for drawing labels are implemented, ranging from
positioning using the mouse to automatic labeling to automatic placement
of key symbols with manual placement of key legends to automatic
placement of legends.  For automatic positioning of labels or keys, a
curve is labeled at a point that is maximally separated from all of the
other curves.  Gaps occurring when curves do not start or end at the
same x-coordinates are given preference for positioning labels. If
labels are offset from the curves (the default behaviour), if the
closest curve to curve i is above curve i, curve i is labeled below its
line.  If the closest curve is below curve i, curve i is labeled above
its line.  These directions are reversed if the resulting labels would
appear outside the plot region.
</p>
<p>Both ordinary lines and step functions are handled, and there is an
option to draw the labels at the same angle as the curve within a
local window.
</p>
<p>Unless the mouse is used to position labels or plotting symbols are
placed along the curves to distinguish them, curves are examined at 100
(by default) equally spaced points over the range of x-coordinates in
the current plot area.  Linear interpolation is used to get
y-coordinates to line up (step function or constant interpolation is
used for step functions).  There is an option to instead examine all
curves at the set of unique x-coordinates found by unioning the
x-coordinates of all the curves.  This option is especially useful when
plotting step functions.  By setting <code>adj="auto"</code> you can have
<code>labcurve</code> try to optimally left- or right-justify labels depending
on the slope of the curves at the points at which labels would be
centered (plus a vertical offset).  This is especially useful when
labels must be placed on steep curve sections.
</p>
<p>You can use the <code>on top</code> method to write (short) curve names
directly on the curves (centered on the y-coordinate).  This is
especially useful when there are many curves whose full labels would run
into each other.  You can plot letters or numbers on the curves, for
example (using the <code>keys</code> option), and have <code>labcurve</code> use the
<code>key</code> function to provide long labels for these short ones (see the
end of the example).  There is another option for connecting labels to
curves using arrows.  When <code>keys</code> is a vector of integers, it is
taken to represent plotting symbols (<code>pch</code>s), and these symbols are
plotted at equally-spaced x-coordinates on each curve (by default, using
5 points per curve).  The points are offset in the x-direction between
curves so as to minimize the chance of collisions.
</p>
<p>To add a legend defining line types, colors, or line widths with no
symbols, specify <code>keys="lines"</code>, e.g., <code>labcurve(curves,
keys="lines", lty=1:2)</code>.
</p>
<p><code>putKey</code> provides a different way to use <code>key()</code> by allowing
the user to specify vectors for labels, line types, plotting characters,
etc.  Elements that do not apply (e.g., <code>pch</code> for lines
(<code>type="l"</code>)) may be <code>NA</code>.  When a series of points is
represented by both a symbol and a line, the corresponding elements of
both <code>pch</code> and <code>lty</code>, <code>col.</code>, or <code>lwd</code> will be
non-missing.
</p>
<p><code>putKeyEmpty</code>, given vectors of all the x-y coordinates that have been
plotted, uses <code>largest.empty</code> to find the largest empty rectangle large
enough to hold the key, and draws the key using <code>putKey</code>.
</p>
<p><code>drawPlot</code> is a simple mouse-driven function for drawing series of
lines, step functions, polynomials, Bezier curves, and points, and
automatically labeling the point groups using <code>labcurve</code> or
<code>putKeyEmpty</code>.  When <code>drawPlot</code> is invoked it creates
temporary functions <code>Points</code>, <code>Curve</code>, and <code>Abline</code>.
The user calls these functions inside
the call to <code>drawPlot</code> to define groups of points in the order they
are defined with the mouse.  <code>Abline</code> is used to call <code>abline</code>
and not actually great a group of points. For some curve types, the
curve generated to represent the corresponding series of points is drawn
after all points are entered for that series, and this curve may be
different than the simple curve obtained by connecting points at the
mouse clicks.  For example, to draw a general smooth Bezier curve the
user need only click on a few points, and she must overshoot the final
curve coordinates to define the curve.  The originally entered points
are not erased once the curve is drawn.  The same goes for step
functions and polynomials.  If you <code>plot()</code> the object returned by
<code>drawPlot</code>, however, only final curves will be shown.  The last
examples show how to use <code>drawPlot</code>.
</p>
<p>The <code>largest.empty</code> function finds the largest rectangle that is large
enough to hold a rectangle of a given height and width, such that the
rectangle does not contain any of a given set of points.  This is
used by <code>labcurve</code> and <code>putKeyEmpty</code> to position keys at the most
empty part of an existing plot.  The default method was created by Hans
Borchers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>labcurve(curves, labels=names(curves),
         method=NULL, keys=NULL, keyloc=c("auto","none"),
         type="l", step.type=c("left", "right"), 
         xmethod=if(any(type=="s")) "unique" else "grid", 
         offset=NULL, xlim=NULL,
         tilt=FALSE, window=NULL, npts=100, cex=NULL, 
         adj="auto", angle.adj.auto=30,
         lty=pr$lty, lwd=pr$lwd, col.=pr$col, transparent=TRUE,
         arrow.factor=1, point.inc=NULL, opts=NULL, key.opts=NULL,
         empty.method=c('area','maxdim'), numbins=25, 
         pl=!missing(add), add=FALSE, 
         ylim=NULL, xlab="", ylab="",
         whichLabel=1:length(curves),
         grid=FALSE, xrestrict=NULL, ...)

putKey(z, labels, type, pch, lty, lwd,
       cex=par('cex'), col=rep(par('col'),nc),
       transparent=TRUE, plot=TRUE, key.opts=NULL, grid=FALSE)

putKeyEmpty(x, y, labels, type=NULL,
            pch=NULL, lty=NULL, lwd=NULL,
            cex=par('cex'), col=rep(par('col'),nc),
            transparent=TRUE, plot=TRUE, key.opts=NULL,
            empty.method=c('area','maxdim'), 
            numbins=25, 
            xlim=pr$usr[1:2], ylim=pr$usr[3:4], grid=FALSE)

drawPlot(..., xlim=c(0,1), ylim=c(0,1), xlab='', ylab='',
         ticks=c('none','x','y','xy'),
         key=FALSE, opts=NULL)

# Points(label=' ', type=c('p','r'),
#        n, pch=pch.to.use[1], cex=par('cex'), col=par('col'),
#        rug = c('none','x','y','xy'), ymean)

# Curve(label=' ',
#       type=c('bezier','polygon','linear','pol','loess','step','gauss'),
#       n=NULL, lty=1, lwd=par('lwd'), col=par('col'), degree=2,
#      evaluation=100, ask=FALSE)

# Abline(\dots)

## S3 method for class 'drawPlot'
plot(x, xlab, ylab, ticks,
     key=x$key, keyloc=x$keyloc, ...)

largest.empty(x, y, width=0, height=0, 
              numbins=25, method=c('exhaustive','rexhaustive','area','maxdim'),
              xlim=pr$usr[1:2], ylim=pr$usr[3:4],
              pl=FALSE, grid=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="labcurve_+3A_curves">curves</code></td>
<td>

<p>a list of lists, each of which have at least two components: a vector of
<code>x</code> values and a vector of corresponding <code>y</code> values.  <code>curves</code> is
mandatory except when <code>method="mouse"</code> or <code>"locator"</code>, in which 
case <code>labels</code> is mandatory.  Each list in <code>curves</code> may optionally have
any of the parameters <code>type</code>, <code>lty</code>, <code>lwd</code>, or <code>col</code>
for that curve, as defined below (see one of the last examples).
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_z">z</code></td>
<td>

<p>a two-element list specifying the coordinate of the center of the key,
e.g. <code>locator(1)</code> to use the mouse for positioning
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_labels">labels</code></td>
<td>

<p>For <code>labcurve</code>, a vector of character strings used to label curves 
(which may contain newline characters to stack labels vertically).  The
default labels are taken from the names of the <code>curves</code> list.
Setting <code>labels=FALSE</code> will suppress drawing any labels (for
<code>labcurve</code> only). 
For <code>putKey</code> and <code>putKeyEmpty</code> is a vector of character strings
specifying group labels
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_x">x</code></td>
<td>
<p>see below</p>
</td></tr>
<tr><td><code id="labcurve_+3A_y">y</code></td>
<td>

<p>for <code>putKeyEmpty</code> and <code>largest.empty</code>, <code>x</code> and <code>y</code>
are same-length vectors specifying points that have been plotted.
<code>x</code> can also be an object created by <code>drawPlot</code>.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_...">...</code></td>
<td>

<p>For <code>drawPlot</code> is a series of invocations of <code>Points</code> and
<code>Curve</code> (see example).  Any number of point groups can be defined
in this way.  For <code>Abline</code> these may be any arguments to
<code>abline</code>. 
For <code>labcurve</code>, other parameters to pass to <code>text</code>.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_width">width</code></td>
<td>
<p>see below</p>
</td></tr>
<tr><td><code id="labcurve_+3A_height">height</code></td>
<td>

<p>for <code>largest.empty</code>, specifies the minimum allowable width in
<code>x</code> units and the minimum allowable height in <code>y</code> units
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_method">method</code></td>
<td>

<p><code>"offset"</code> (the default) offsets labels at largest gaps between
curves, and draws labels beside curves.  
<code>"on top"</code> draws labels on top of the curves (especially
good when using keys).  
<code>"arrow"</code> draws arrows connecting labels to the curves.
<code>"mouse"</code> or <code>"locator"</code> positions labels according to mouse clicks.
If <code>keys</code> is specified and is an integer vector or is <code>"lines"</code>, 
<code>method</code> defaults to <code>"on top"</code>.  If <code>keys</code> is character,
<code>method</code> defaults to <code>"offset"</code>.  Set <code>method="none"</code> to
suppress all curve labeling and key drawing, which is useful when
<code>pl=TRUE</code> and you only need <code>labcurve</code> to draw the curves and the
rest of the basic graph.
</p>
<p>For <code>largest.empty</code> specifies the method a rectangle that does not
collide with any of the (<code>x</code>, <code>y</code>) points.  The default
method, <code>'exhaustive'</code>, uses a Fortran translation of an R function
and algorithm developed by Hans Borchers.  The same result, more slowly,
may be obtained by using pure R code by specifying
<code>method='rexhaustive'</code>.  The original algorithms using binning (and
the only methods supported for S-Plus) are
still available.  For all methods, screening of candidate rectangles
having at least a given width in <code>x</code>-units of <code>width</code> or
having at least a given height in <code>y</code>-units of <code>height</code> is possible.
Use <code>method="area"</code> to use the binning method to find the rectangle
having the largest area, or <code>method="maxdim"</code> to use the binning
method to return with last rectangle searched that had both
the largest width and largest height over all previous rectangles.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_keys">keys</code></td>
<td>

<p>This causes keys (symbols or short text) to be drawn on or beside
curves, and if <code>keyloc</code> is not equal to <code>"none"</code>, a legend to be
automatically drawn.  The legend links keys with full curve labels
and optionally with colors and line types.
Set <code>keys</code> to a vector of character strings, or a
vector of integers specifying plotting character (<code>pch</code> values -
see <code>points</code>).  For the latter case, the default behavior is to
plot the symbols periodically, at equally spaced x-coordinates.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_keyloc">keyloc</code></td>
<td>

<p>When <code>keys</code> is specified, <code>keyloc</code> specifies how the legend
is to be positioned for drawing using the <code>key</code> function in
<code>trellis</code>.  The default is <code>"auto"</code>, for which the
<code>largest.empty</code> function to used to find the most empty part of the
plot.  If no empty rectangle large enough to hold the key is found, no
key will be drawn. Specify <code>keyloc="none"</code> to suppress drawing a
legend, or set <code>keyloc</code> to a 2-element list containing the x and y
coordinates for the center of the legend.  For example, use
<code>keyloc=locator(1)</code> to click the mouse at the center.
<code>keyloc</code> specifies the coordinates of the center of the
key to be drawn with <code>plot.drawPlot</code> when <code>key=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_type">type</code></td>
<td>

<p>for <code>labcurve</code>, a scalar or vector of character strings specifying the
method that the points in the curves were connected. <code>"l"</code> means
ordinary connections between points and <code>"s"</code> means step functions.
For <code>putKey</code> and <code>putKeyEmpty</code> is a vector of plotting types, <code>"l"</code>
for regular line, <code>"p"</code> for point, <code>"b"</code> for both point and line, and
<code>"n"</code> for none.  For <code>Points</code> is either <code>"p"</code> (the default) for
regular points, or <code>"r"</code> for rugplot (one-dimensional scatter diagram
to be drawn using the <code>scat1d</code> function).  For <code>Curve</code>, <code>type</code> is
<code>"bezier"</code> (the default) for drawing a smooth Bezier curves (which can
represent a non-1-to-1 function such as a circle), <code>"polygon"</code> for
orginary line segments, <code>"linear"</code> for a straight line defined by two
endpoints, <code>"pol"</code> for a <code>degree</code>-degree polynomial to be fitted to
the mouse-clicked points, <code>"step"</code> for a left-step-function, <code>"gauss"</code>
to plot a Gaussian density fitted to 3 clicked points, <code>"loess"</code> to
use the <code>lowess</code> function to smooth the clicked points, or a function
to draw a user-specified function, evaluated at <code>evaluation</code> points
spanning the whole x-axis.  For the density the user must click in the
left tail, at the highest value (at the mean), and in the right tail,
with the two tail values being approximately equidistant from the
mean.  The density is scaled to fit in the highest value regardless of
its area.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_step.type">step.type</code></td>
<td>

<p>type of step functions used (default is <code>"left"</code>)
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_xmethod">xmethod</code></td>
<td>

<p>method for generating the unique set of x-coordinates to examine (see above).  Default is <code>"grid"</code> for <code>type="l"</code> or <code>"unique"</code> for 
<code>type="s"</code>.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_offset">offset</code></td>
<td>

<p>distance in y-units between the center of the label and the line being
labeled.  Default is 0.75 times the height of an &quot;m&quot; that would be
drawn in a label.  For R grid/lattice you must specify offset using
the <code>grid</code> <code>unit</code> function, e.g., <code>offset=unit(2,"native")</code> or
<code>offset=unit(.25,"cm")</code> (<code>"native"</code> means data units)
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_xlim">xlim</code></td>
<td>

<p>limits for searching for label positions, and is also used to set up
plots when <code>pl=TRUE</code> and <code>add=FALSE</code>.  Default is total x-axis
range for current plot (<code>par("usr")[1:2]</code>).  For
<code>largest.empty</code>, <code>xlim</code> limits the search for largest
rectanges, but it has the same default as above. For
<code>pl=TRUE,add=FALSE</code> you may want to extend <code>xlim</code> somewhat to
allow large keys to fit, when using <code>keyloc="auto"</code>.  For
<code>drawPlot</code> default is <code>c(0,1)</code>.  When using
<code>largest.empty</code> with <code>ggplot2</code>, <code>xlim</code> and <code>ylim</code>
are mandatory.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_tilt">tilt</code></td>
<td>

<p>set to <code>TRUE</code> to tilt labels to follow the curves, for <code>method="offset"</code>
when <code>keys</code> is not given.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_window">window</code></td>
<td>

<p>width of a window, in x-units, to use in determining the local slope
for tilting labels.  Default is 0.5 times number of characters in the
label times the x-width of an &quot;m&quot; in the current character size and font.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_npts">npts</code></td>
<td>

<p>number of points to use if <code>xmethod="grid"</code>
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_cex">cex</code></td>
<td>

<p>character size to pass to <code>text</code> and <code>key</code>.  Default is current
<code>par("cex")</code>.  For <code>putKey</code>, <code>putKeyEmpty</code>, and <code>Points</code> is the size of the
plotting symbol.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_adj">adj</code></td>
<td>

<p>Default is <code>"auto"</code> which has <code>labcurve</code> figure justification
automatically when <code>method="offset"</code>.  This will cause centering to be used when the local angle
of the curve is less than <code>angle.adj.auto</code> in absolute value, left
justification if the angle is larger and either the label is under a
curve of positive slope or over a curve of negative slope, and right
justification otherwise.  For step functions, left justification is used
when the label is above the curve and right justifcation otherwise.
Set <code>adj=.5</code> to center labels at computed coordinates.  Set to 0 for
left-justification, 1 for right.  Set <code>adj</code> to a vector to vary adjustments
over the curves.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_angle.adj.auto">angle.adj.auto</code></td>
<td>

<p>see <code>adj</code>.  Does not apply to step functions.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_lty">lty</code></td>
<td>

<p>vector of line types which were used to draw the curves.
This is only used when keys are drawn. If all of the
line types, line widths, and line colors are the same, 
lines are not drawn in the key.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_lwd">lwd</code></td>
<td>

<p>vector of line widths which were used to draw the curves.
This is only used when keys are drawn.  See <code>lty</code> also.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_col.">col.</code></td>
<td>
<p>vector of integer color numbers</p>
</td></tr>
<tr><td><code id="labcurve_+3A_col">col</code></td>
<td>

<p>vector of integer color numbers for use in curve labels, symbols,
lines, and legends.  Default is <code>par("col")</code> for all curves.
See <code>lty</code> also.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_transparent">transparent</code></td>
<td>

<p>Default is <code>TRUE</code> to make <code>key</code> draw transparent legends, i.e., to
suppress drawing a solid rectangle background for the legend.
Set to <code>FALSE</code> otherwise.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_arrow.factor">arrow.factor</code></td>
<td>

<p>factor by which to multiply default arrow lengths
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_point.inc">point.inc</code></td>
<td>

<p>When <code>keys</code> is a vector of integers, <code>point.inc</code> specifies the x-increment
between the point symbols that are overlaid periodically on the curves.  
By default, <code>point.inc</code> is equal
to the range for the x-axis divided by 5.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_opts">opts</code></td>
<td>

<p>an optional list which can be used to specify any of the options
to <code>labcurve</code>, with the usual element name abbreviations allowed.
This is useful when <code>labcurve</code> is being called from another
function.  Example: <code>opts=list(method="arrow", cex=.8, np=200)</code>.
For <code>drawPlot</code> a list of <code>labcurve</code> options to pass as
<code>labcurve(..., opts=)</code>.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_key.opts">key.opts</code></td>
<td>

<p>a list of extra arguments you wish to pass to <code>key()</code>, e.g.,
<code>key.opts=list(background=1, between=3)</code>.  
The argument names must be spelled out in full.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_empty.method">empty.method</code></td>
<td>
<p>see below</p>
</td></tr>
<tr><td><code id="labcurve_+3A_numbins">numbins</code></td>
<td>

<p>These two arguments are passed to the <code>largest.empty</code> function's
<code>method</code> and <code>numbins</code> arguments (see below).
For <code>largest.empty</code> specifies the number of bins in which to
discretize both the <code>x</code> and <code>y</code> directions for searching for
rectangles.  Default is 25.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_pl">pl</code></td>
<td>

<p>set to <code>TRUE</code> (or specify <code>add</code>) to cause the curves in <code>curves</code> to be
drawn, under the control of <code>type</code>,<code>lty</code>,<code>lwd</code>,<code>col</code> parameters defined
either in the <code>curves</code> lists or in the separate arguments given to
<code>labcurve</code> or through <code>opts</code>.
For <code>largest.empty</code>, set <code>pl=TRUE</code> to show the rectangle the function 
found by drawing it with a solid color.  May not be used under <code>ggplot2</code>.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_add">add</code></td>
<td>

<p>By default, when curves are actually drawn by <code>labcurve</code> a new plot is
started.  To add to an existing plot, set <code>add=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_ylim">ylim</code></td>
<td>

<p>When a plot has already been started, <code>ylim</code> defaults to <code>par("usr")[3:4]</code>.
When <code>pl=TRUE</code>, <code>ylim</code> and <code>xlim</code> are determined from the ranges of the data.
Specify <code>ylim</code> yourself to take control of the plot construction.  
In some cases it is advisable to
make <code>ylim</code> larger than usual to allow for automatically-positioned keys.
For <code>largest.empty</code>, <code>ylim</code> specifies the limits on the y-axis to limit
the search for rectangle.  
Here <code>ylim</code> defaults to the same as above, i.e., the range
of the y-axis of an open plot from <code>par</code>.  For <code>drawPlot</code> the default
is <code>c(0,1)</code>.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_xlab">xlab</code></td>
<td>
<p>see below</p>
</td></tr>
<tr><td><code id="labcurve_+3A_ylab">ylab</code></td>
<td>

<p>x-axis and y-axis labels when <code>pl=TRUE</code> and <code>add=FALSE</code> or for
<code>drawPlot</code>.
Defaults to <code>""</code> unless the first curve has names for its first two
elements, in which case the names of these elements are taken as
<code>xlab</code> and <code>ylab</code>.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_whichlabel">whichLabel</code></td>
<td>

<p>integer vector corresponding to <code>curves</code> specifying which curves
are to be labelled or have a legend
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_grid">grid</code></td>
<td>

<p>set to <code>TRUE</code> if the R <code>grid</code> package was used to draw the
current plot.  This prevents <code>labcurve</code> from using
<code>par("usr")</code> etc.  If using R <code>grid</code> you can pass coordinates
and lengths having arbitrary units, as documented in the <code>unit</code>
function.  This is especially useful for <code>offset</code>.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_xrestrict">xrestrict</code></td>
<td>

<p>When having <code>labcurve</code> label curves where they are most
separated, you can restrict the search for this separation point to a
range of the x-axis, specified as a 2-vector <code>xrestrict</code>.  This
is useful when one part of the curve is very steep.  Even though
steep regions may have maximum separation, the labels will collide
when curves are steep.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_pch">pch</code></td>
<td>

<p>vector of plotting characters for <code>putKey</code> and <code>putKeyEmpty</code>.  Can be
any value including <code>NA</code> when only a line is used to indentify the
group.  Is a single plotting character for <code>Points</code>, with the default
being the next unused value from among 1, 2, 3, 4, 16, 17, 5, 6, 15,
18, 19.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_plot">plot</code></td>
<td>

<p>set to <code>FALSE</code> to keep <code>putKey</code> or <code>putKeyEmpty</code> from actually drawing the
key.  Instead, the size of the key will be return by <code>putKey</code>, or the
coordinates of the key by <code>putKeyEmpty</code>.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_ticks">ticks</code></td>
<td>

<p>tells <code>drawPlot</code> which axes to draw tick marks and tick labels.
Default is <code>"none"</code>.
</p>
</td></tr>
<tr><td><code id="labcurve_+3A_key">key</code></td>
<td>

<p>for <code>drawPlot</code> and <code>plot.drawPlot</code>.  Default is <code>FALSE</code> so that <code>labcurve</code>
is used to label points or curves.  Set to <code>TRUE</code> to use
<code>putKeyEmpty</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The internal functions <code>Points</code>, <code>Curve</code>, <code>Abline</code> have
unique arguments as follows.
</p>

<dl>
<dt><code>label</code>:</dt><dd><p>for <code>Points</code> and <code>Curve</code> is a single
character string to label that group of points</p>
</dd>
<dt><code>n</code>:</dt><dd><p>number of points to accept from the mouse.  Default
is to input points until a right mouse click.</p>
</dd>
<dt><code>rug</code>:</dt><dd><p>for <code>Points</code>.  Default is <code>"none"</code> to
not show the  marginal x or y distributions as rug plots, for the
points entered. Other possibilities are used to execute
<code>scat1d</code> to show the marginal distribution of x, y, or both
as rug plots.</p>
</dd> 
<dt><code>ymean</code>:</dt><dd><p>for <code>Points</code>, subtracts a constant from
each y-coordinate entered to make the overall mean <code>ymean</code></p>
</dd>
<dt><code>degree</code>:</dt><dd><p>degree of polynomial to fit to points by
<code>Curve</code></p>
</dd> 
<dt><code>evaluation</code>:</dt><dd><p>number of points at which to evaluate
Bezier curves, polynomials, and other functions in <code>Curve</code></p>
</dd>
<dt><code>ask</code>:</dt><dd><p>set <code>ask=TRUE</code> to give the user the
opportunity to try again at specifying points for Bezier curves,
step functions, and polynomials</p>
</dd>
</dl>

<p>The <code>labcurve</code> function used some code from the function <code>plot.multicurve</code> written
by Rod Tjoelker of The Boeing Company (<a href="mailto:tjoelker@espresso.rt.cs.boeing.com">tjoelker@espresso.rt.cs.boeing.com</a>).
</p>
<p>If there is only one curve, a label is placed at the middle x-value,
and no fancy features such as <code>angle</code> or positive/negative offsets are
used.
</p>
<p><code>key</code> is called once (with the argument <code>plot=FALSE</code>) to find the key
dimensions.  Then an empty rectangle with at least these dimensions is
searched for using <code>largest.empty</code>.  Then <code>key</code> is called again to draw
the key there, using the argument <code>corner=c(.5,.5)</code> so that the center
of the rectangle can be specified to <code>key</code>.
</p>
<p>If you want to plot the data, an easier way to use <code>labcurve</code> is
through <code>xYplot</code> as shown in some of its examples.
</p>


<h3>Value</h3>

<p><code>labcurve</code> returns an invisible list with components <code>x, y, offset, adj, cex, col</code>, and if <code>tilt=TRUE</code>, 
<code>angle</code>. <code>offset</code> is the amount to add to <code>y</code> to draw a label.
<code>offset</code> is negative if the label is drawn below the line.
<code>adj</code> is a vector containing the values 0, .5, 1.
</p>
<p><code>largest.empty</code> returns a list with elements <code>x</code> and <code>y</code>
specifying the coordinates of the center of the rectangle which was
found, and element <code>rect</code> containing the 4 <code>x</code> and <code>y</code>
coordinates of the corners of the found empty rectangle.  The
<code>area</code> of the rectangle is also returned.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+approx">approx</a></code>, <code><a href="graphics.html#topic+text">text</a></code>, <code><a href="graphics.html#topic+legend">legend</a></code>,
<code><a href="#topic+scat1d">scat1d</a></code>, <code><a href="#topic+xYplot">xYplot</a></code>, <code><a href="graphics.html#topic+abline">abline</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 2:8
m &lt;-  length(n)
type &lt;- c('l','l','l','l','s','l','l')
# s=step function l=ordinary line (polygon)
curves &lt;- vector('list', m)


plot(0,1,xlim=c(0,1),ylim=c(-2.5,4),type='n')


set.seed(39)


for(i in 1:m) {
  x &lt;- sort(runif(n[i]))
  y &lt;- rnorm(n[i])
  lines(x, y, lty=i, type=type[i], col=i)
  curves[[i]] &lt;- list(x=x,y=y)
}


labels &lt;- paste('Label for',letters[1:m])
labcurve(curves, labels, tilt=TRUE, type=type, col=1:m)


# Put only single letters on curves at points of 
# maximum space, and use key() to define the letters,
# with automatic positioning of the key in the most empty
# part of the plot
# Have labcurve do the plotting, leaving extra space for key


names(curves) &lt;- labels
labcurve(curves, keys=letters[1:m], type=type, col=1:m,
         pl=TRUE, ylim=c(-2.5,4))


# Put plotting symbols at equally-spaced points,
# with a key for the symbols, ignoring line types


labcurve(curves, keys=1:m, lty=1, type=type, col=1:m,
         pl=TRUE, ylim=c(-2.5,4))




# Plot and label two curves, with line parameters specified with data
set.seed(191)
ages.f &lt;- sort(rnorm(50,20,7))
ages.m &lt;- sort(rnorm(40,19,7))
height.f &lt;- pmin(ages.f,21)*.2+60
height.m &lt;- pmin(ages.m,21)*.16+63


labcurve(list(Female=list(ages.f,height.f,col=2),
              Male  =list(ages.m,height.m,col=3,lty='dashed')),
         xlab='Age', ylab='Height', pl=TRUE)
# add ,keys=c('f','m') to label curves with single letters
# For S-Plus use lty=2


# Plot power for testing two proportions vs. n for various odds ratios, 
# using 0.1 as the probability of the event in the control group.  
# A separate curve is plotted for each odds ratio, and the curves are
# labeled at points of maximum separation


n  &lt;- seq(10, 1000, by=10)
OR &lt;- seq(.2,.9,by=.1)
pow &lt;- lapply(OR, function(or,n)list(x=n,y=bpower(p1=.1,odds.ratio=or,n=n)),
              n=n)
names(pow) &lt;- format(OR)
labcurve(pow, pl=TRUE, xlab='n', ylab='Power')


# Plot some random data and find the largest empty rectangle
# that is at least .1 wide and .1 tall


x &lt;- runif(50)
y &lt;- runif(50)
plot(x, y)
z &lt;- largest.empty(x, y, .1, .1)
z
points(z,pch=3)  # mark center of rectangle, or
polygon(z$rect, col='blue')  # to draw the rectangle, or
#key(z$x, z$y, \dots stuff for legend)




# Use the mouse to draw a series of points using one symbol, and
# two smooth curves or straight lines (if two points are clicked), 
# none of these being labeled


# d &lt;- drawPlot(Points(), Curve(), Curve())
# plot(d)


## Not run: 
# Use the mouse to draw a Gaussian density, two series of points
# using 2 symbols, one Bezier curve, a step function, and raw data
# along the x-axis as a 1-d scatter plot (rug plot).  Draw a key.
# The density function is fit to 3 mouse clicks
# Abline draws a dotted horizontal reference line
d &lt;- drawPlot(Curve('Normal',type='gauss'),
              Points('female'), Points('male'), 
              Curve('smooth',ask=TRUE,lty=2), Curve('step',type='s',lty=3), 
              Points(type='r'), Abline(h=.5, lty=2),
              xlab='X', ylab='y', xlim=c(0,100), key=TRUE)
plot(d, ylab='Y')
plot(d, key=FALSE)  # label groups using labcurve

## End(Not run)
</code></pre>

<hr>
<h2 id='label'>
Label Attribute of an Object
</h2><span id='topic+label'></span><span id='topic+label+3C-'></span><span id='topic+label.default'></span><span id='topic+label.Surv'></span><span id='topic+label+3C-.default'></span><span id='topic+labelPlotmath'></span><span id='topic+labelLatex'></span><span id='topic++5B.labelled'></span><span id='topic+print.labelled'></span><span id='topic+Label'></span><span id='topic+Label.data.frame'></span><span id='topic+llist'></span><span id='topic+prList'></span><span id='topic+putHcap'></span><span id='topic+putHfig'></span><span id='topic+plotmathTranslate'></span><span id='topic+as.data.frame.labelled'></span><span id='topic+data.frame.labelled'></span><span id='topic+reLabelled'></span><span id='topic+label.data.frame'></span><span id='topic+label+3C-.data.frame'></span><span id='topic+relevel.labelled'></span><span id='topic+combineLabels'></span>

<h3>Description</h3>

<p><code>label(x)</code> retrieves the <code>label</code> attribute of <code>x</code>.
<code>label(x) &lt;- "a label"</code> stores the label attribute, and also puts
the class <code>labelled</code> as the first class of <code>x</code> (for S-Plus
this class is not used and methods for handling this class are
not defined so the <code>"label"</code> and <code>"units"</code> attributes are lost
upon subsetting).  The reason for having this class is so that the
subscripting method for <code>labelled</code>, <code>[.labelled</code>, can preserve
the <code>label</code> attribute in S.  Also, the <code>print</code>
method for <code>labelled</code> objects prefaces the print with the object's
<code>label</code> (and <code>units</code> if there).  If the variable is also given
a <code>"units"</code> attribute using the <code>units</code> function, subsetting
the variable (using <code>[.labelled</code>) will also retain the
<code>"units"</code> attribute.
</p>
<p><code>label</code> can optionally append a <code>"units"</code> attribute to the
string, and it can optionally return a string or expression (for <span class="rlang"><b>R</b></span>'s
<code>plotmath</code> facility) suitable for plotting.  <code>labelPlotmath</code>
is a function that also has this function, when the input arguments are
the <code>'label'</code> and <code>'units'</code> rather than a vector having those
attributes.  When <code>plotmath</code> mode is used to construct labels, the
<code>'label'</code> or <code>'units'</code> may contain math expressions but they
are typed verbatim if they contain percent signs, blanks, or
underscores.  <code>labelPlotmath</code> can optionally create the
expression as a character string, which is useful in building
<code>ggplot</code> commands.
</p>
<p>For <code>Surv</code> objects, <code>label</code> first looks to see if there is
an overall <code>"label"</code> attribute for the object, then it looks for
saved attributes that <code>Surv</code> put in the <code>"inputAttributes"</code>
object, looking first at the <code>event</code> variable, then <code>time2</code>,
and finally <code>time</code>.  You can restrict the looking by specifying
<code>type</code>.
</p>
<p><code>labelLatex</code> constructs suitable LaTeX labels a variable or from the
<code>label</code> and <code>units</code> arguments, optionally right-justifying
<code>units</code> if <code>hfill=TRUE</code>.  This is useful when making tables
when the variable in question is not a column heading.  If <code>x</code>
is specified, <code>label</code> and <code>units</code> values are extracted from
its attributes instead of from the other arguments.
</p>
<p><code>Label</code> (actually <code>Label.data.frame</code>) is a function which generates
S source code that makes the labels in all the variables in a data
frame easy to edit.
</p>
<p><code>llist</code> is like <code>list</code> except that it preserves the names or
labels of the component variables in the variables <code>label</code>
attribute.  This can be useful when looping over variables or using
<code>sapply</code> or <code>lapply</code>. By using <code>llist</code> instead of
<code>list</code> one can annotate the output with the current variable's name
or label.  <code>llist</code> also defines a <code>names</code> attribute for the
list and pulls the <code>names</code> from the arguments' expressions for
non-named arguments.
</p>
<p><code>prList</code> prints a list with element names (without the dollar
sign as in default list printing) and if an element of the list is an
unclassed list with a name, all of those elements are printed, with
titles of the form &quot;primary list name : inner list name&quot;.  This is
especially useful for Rmarkdown html notebooks when a user-written
function creates multiple html and graphical outputs to all be printed
in a code chunk.  Optionally the names can be printed after the
object, and the <code>htmlfig</code> option provides more capabilities when
making html reports.  <code>prList</code> does not work for regular html
documents. 
</p>
<p><code>putHfig</code> is similar to <code>prList</code> but for a single graphical
object that is rendered with a <code>print</code> method, making it easy to
specify long captions, and short captions for the table of contents in
HTML documents.
Table of contents entries are generated with the short caption, which
is taken as the long caption if there is none.  One can optionally not
make a table of contents entry.  If argument <code>table=TRUE</code> table
captions will be produced instead.  Using <code>expcoll</code>,
<code>markupSpecs</code> <code>html</code> function <code>expcoll</code> will be used to
make tables expand upon clicking an arrow rather than always appear.
</p>
<p><code>putHcap</code> is like <code>putHfig</code> except that it
assumes that users render the graphics or table outside of the
<code>putHcap</code> call.  This allows things to work in ordinary html
documents.  <code>putHcap</code> does not handle collapsed text.
</p>
<p><code>plotmathTranslate</code> is a simple function that translates certain
character strings to character strings that can be used as part of <span class="rlang"><b>R</b></span>
<code>plotmath</code> expressions.  If the input string has a space or percent
inside, the string is surrounded by a call to <code>plotmath</code>'s
<code>paste</code> function.
</p>
<p><code>as.data.frame.labelled</code> is a utility function that is called by
<code>[.data.frame</code>.  It is just a copy of <code>as.data.frame.vector</code>.
<code>data.frame.labelled</code> is another utility function, that adds a
class <code>"labelled"</code> to every variable in a data frame that has a
<code>"label"</code> attribute but not a <code>"labelled"</code> class.
</p>
<p><code>relevel.labelled</code> is a method for preserving <code>label</code>s with the <code>relevel</code> function.
</p>
<p><code>reLabelled</code> is used to add a <code>'labelled'</code> class back to
variables in data frame that have a 'label' attribute but no 'labelled'
class.  Useful for changing <code>cleanup.import()</code>'d S-Plus data
frames back to general form for <span class="rlang"><b>R</b></span> and old versions of S-Plus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>label(x, default=NULL, ...)

## Default S3 method:
label(x, default=NULL, units=plot, plot=FALSE,
      grid=FALSE, html=FALSE, ...)

## S3 method for class 'Surv'
label(x, default=NULL, units=plot, plot=FALSE,
      grid=FALSE, html=FALSE, type=c('any', 'time', 'event'), ...)

## S3 method for class 'data.frame'
label(x, default=NULL, self=FALSE, ...)

label(x, ...) &lt;- value

## Default S3 replacement method:
label(x, ...) &lt;- value

## S3 replacement method for class 'data.frame'
label(x, self=TRUE, ...) &lt;- value

labelPlotmath(label, units=NULL, plotmath=TRUE, html=FALSE, grid=FALSE,
              chexpr=FALSE)

labelLatex(x=NULL, label='', units='', size='smaller[2]',
           hfill=FALSE, bold=FALSE, default='', double=FALSE)

## S3 method for class 'labelled'
print(x, ...)   ## or x - calls print.labelled

Label(object, ...)

## S3 method for class 'data.frame'
Label(object, file='', append=FALSE, ...)

llist(..., labels=TRUE)

prList(x, lcap=NULL, htmlfig=0, after=FALSE)

putHfig(x, ..., scap=NULL, extra=NULL, subsub=TRUE, hr=TRUE,
        table=FALSE, file='', append=FALSE, expcoll=NULL)

putHcap(..., scap=NULL, extra=NULL, subsub=TRUE, hr=TRUE,
        table=FALSE, file='', append=FALSE)

plotmathTranslate(x)

data.frame.labelled(object)

## S3 method for class 'labelled'
relevel(x, ...)

reLabelled(object)

combineLabels(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="label_+3A_x">x</code></td>
<td>

<p>any object (for <code>plotmathTranslate</code> is a character string). For
<code>relevel</code> is a <code>factor</code> variable.  For <code>prList</code> is a
named list.  For <code>putHfig</code> is a graphical object for which a
<code>print</code> method will render the graphic (e.g., a <code>ggplot2</code>
or <code>plotly</code> object).
</p>
</td></tr>
<tr><td><code id="label_+3A_self">self</code></td>
<td>
<p>lgoical, where to interact with the object or its components</p>
</td></tr>
<tr><td><code id="label_+3A_units">units</code></td>
<td>

<p>set to <code>TRUE</code> to append the <code>'units'</code> attribute (if present)
to the returned label.  The <code>'units'</code> are surrounded
by brackets.  For <code>labelPlotmath</code> and <code>labelLatex</code> is a
character string containing the units of measurement.  When
<code>plot</code> is <code>TRUE</code>, <code>units</code> defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="label_+3A_plot">plot</code></td>
<td>

<p>set to <code>TRUE</code> to return a label suitable for <span class="rlang"><b>R</b></span>'s <code>plotmath</code>
facility (returns an expression instead of a character string) if R is
in effect.  If <code>units</code> is also <code>TRUE</code>, and if both
<code>'label'</code> and <code>'units'</code> attributes are present, the
<code>'units'</code> will appear after the label but in smaller type and
will not be surrounded by brackets.
</p>
</td></tr>
<tr><td><code id="label_+3A_default">default</code></td>
<td>

<p>if <code>x</code> does not have a <code>'label'</code> attribute and
<code>default</code> (a character string) is specified, the label will be
taken as <code>default</code>.  For <code>labelLatex</code> the <code>default</code>
is the name of the first argument if it is a variable and not a label.
</p>
</td></tr>
<tr><td><code id="label_+3A_grid">grid</code></td>
<td>

<p>Currently <span class="rlang"><b>R</b></span>'s <code>lattice</code> and <code>grid</code> functions do not support
<code>plotmath</code> expressions for <code>xlab</code> and <code>ylab</code>
arguments.  When using <code>lattice</code> functions in <span class="rlang"><b>R</b></span>, set the
argument <code>grid</code> to <code>TRUE</code> so that <code>labelPlotmath</code> can
return an ordinary character string instead of an expression.
</p>
</td></tr>
<tr><td><code id="label_+3A_html">html</code></td>
<td>
<p>set to <code>TRUE</code> to use HTML formatting instead of
plotmath expressions for constructing labels with units</p>
</td></tr>
<tr><td><code id="label_+3A_type">type</code></td>
<td>
<p>for <code>Surv</code> objects specifies the type of element for
which to restrict the search for a label</p>
</td></tr>
<tr><td><code id="label_+3A_label">label</code></td>
<td>
<p>a character string containing a variable's label</p>
</td></tr>
<tr><td><code id="label_+3A_plotmath">plotmath</code></td>
<td>

<p>set to <code>TRUE</code> to have <code>labelMathplot</code> return an expression
for plotting using <span class="rlang"><b>R</b></span>'s <code>plotmath</code> facility.  If <span class="rlang"><b>R</b></span> is not in
effect, an ordinary character string is returned.
</p>
</td></tr>
<tr><td><code id="label_+3A_chexpr">chexpr</code></td>
<td>
<p>set to <code>TRUE</code> to have <code>labelPlotmath</code> return a
character string of the form <code>"expression(...)"</code></p>
</td></tr>
<tr><td><code id="label_+3A_size">size</code></td>
<td>
<p>LaTeX size for <code>units</code>.  Default is two sizes smaller
than <code>label</code>, which assumes that the LaTeX <code>relsize</code>
package is in use.</p>
</td></tr>
<tr><td><code id="label_+3A_hfill">hfill</code></td>
<td>
<p>set to <code>TRUE</code> to right-justify <code>units</code> in the
field.  This is useful when multiple labels are being put into rows
in a LaTeX <code>tabular</code> environment, and will cause a problem if
the label is used in an environment where <code>hfill</code> is not
appropriate.</p>
</td></tr>
<tr><td><code id="label_+3A_bold">bold</code></td>
<td>
<p>set to <code>TRUE</code> to have <code>labelLatex</code> put the
<code>label</code> in bold face.</p>
</td></tr>
<tr><td><code id="label_+3A_double">double</code></td>
<td>
<p>set to <code>TRUE</code> to represent backslash in LaTeX as
four backslashes in place of two.  This is needed if, for example,
you need to convert the result using <code>as.formula</code></p>
</td></tr>
<tr><td><code id="label_+3A_value">value</code></td>
<td>

<p>the label of the object, or &quot;&quot;.
</p>
</td></tr>
<tr><td><code id="label_+3A_object">object</code></td>
<td>

<p>a data frame
</p>
</td></tr>
<tr><td><code id="label_+3A_...">...</code></td>
<td>

<p>a list of variables or expressions to be formed into a <code>list</code>.
Ignored for <code>print.labelled</code>.  For <code>relevel</code> is the
<code>level</code> (a single character string) to become the new reference
(first) category.  For <code>putHfig</code> and <code>putHcap</code> represents
one or more character strings that are pasted together, separated by
a blank. 
</p>
</td></tr>
<tr><td><code id="label_+3A_file">file</code></td>
<td>

<p>the name of a file to which to write S source code.  Default is
<code>""</code>, meaning standard output.  For <code>putHcap</code>, set
<code>file</code> to <code>FALSE</code> to return a character vector instead of
writing to <code>file</code>.</p>
</td></tr>
<tr><td><code id="label_+3A_append">append</code></td>
<td>

<p>set to <code>TRUE</code> to append code generated by <code>Label</code> to file
<code>file</code>.  Also used for <code>putHfig, putHcap</code>.
</p>
</td></tr>
<tr><td><code id="label_+3A_labels">labels</code></td>
<td>

<p>set to <code>FALSE</code> to make <code>llist</code> ignore the variables'
<code>label</code> attribute and  use the variables' names.
</p>
</td></tr>
<tr><td><code id="label_+3A_lcap">lcap</code></td>
<td>
<p>an optional vector of character strings corresponding to
elements in <code>x</code> for <code>prList</code>.  These contain long captions
that do not appear in the table of contents but which are printed
right after the short caption in the body, in the same font.</p>
</td></tr>
<tr><td><code id="label_+3A_htmlfig">htmlfig</code></td>
<td>
<p>for <code>prList</code> set to <code>1</code> to use HTML markup by
running the object names through <code>markupSpecs$html$cap</code> for
figure captions.  Set <code>htmlfig=2</code> to also preface the figure
caption with <code>"### "</code> so that it will appear in the table of
contents.</p>
</td></tr>
<tr><td><code id="label_+3A_after">after</code></td>
<td>
<p>set to <code>TRUE</code> to have <code>prList</code> put names after
the printed object instead of before</p>
</td></tr>
<tr><td><code id="label_+3A_scap">scap</code></td>
<td>
<p>a character string specifying the short (or possibly only)
caption.</p>
</td></tr>
<tr><td><code id="label_+3A_extra">extra</code></td>
<td>
<p>an optional vector of character strings.  When present
the long caption will be put in the first column of an HTML table
and the elements of <code>extra</code> in subsequent columns.  This allows
extra information to appear in the long caption in a way that is
right-justified to the right of the flowing caption text.</p>
</td></tr>
<tr><td><code id="label_+3A_subsub">subsub</code></td>
<td>
<p>set to <code>FALSE</code> to suppress <code>"### "</code> from being
placed in front of the short caption.  Set it to different character
string to use that instead.  Set it to <code>""</code> to ignore short
captions entirely.  For example to use second-level headings for the
table of contents specify <code>subsub="## "</code>.</p>
</td></tr>
<tr><td><code id="label_+3A_hr">hr</code></td>
<td>
<p>applies if a caption is present.  Specify <code>FALSE</code> to
not put a horizontal line before the caption and figure.</p>
</td></tr>
<tr><td><code id="label_+3A_table">table</code></td>
<td>
<p>set to <code>TRUE</code> to produce table captions instead of
figure captions</p>
</td></tr>
<tr><td><code id="label_+3A_expcoll">expcoll</code></td>
<td>
<p>character string to be visible, with a clickable arrow
following to allow initial hiding of a table and its captions.
Cannot be used with <code>table=FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>label</code> returns the label attribute of x, if any; otherwise, &quot;&quot;.
<code>label</code> is used
most often for the individual variables in data frames.  The function
<code>sas.get</code> copies labels over from SAS if they exist.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sas.get">sas.get</a></code>, <code><a href="#topic+describe">describe</a></code>,
<code><a href="#topic+extractlabs">extractlabs</a></code>, <code><a href="#topic+hlab">hlab</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>age &lt;- c(21,65,43)
y   &lt;- 1:3
label(age) &lt;- "Age in Years"
plot(age, y, xlab=label(age))

data &lt;- data.frame(age=age, y=y)
label(data)

label(data, self=TRUE) &lt;- "A data frame"
label(data, self=TRUE)

x1 &lt;- 1:10
x2 &lt;- 10:1
label(x2) &lt;- 'Label for x2'
units(x2) &lt;- 'mmHg'
x2
x2[1:5]
dframe &lt;- data.frame(x1, x2)
Label(dframe)

labelLatex(x2, hfill=TRUE, bold=TRUE)
labelLatex(label='Velocity', units='m/s')

##In these examples of llist, note that labels are printed after
##variable names, because of print.labelled
a &lt;- 1:3
b &lt;- 4:6
label(b) &lt;- 'B Label'
llist(a,b)
llist(a,b,d=0)
llist(a,b,0)


w &lt;- llist(a, b&gt;5, d=101:103)
sapply(w, function(x){
  hist(as.numeric(x), xlab=label(x))
  # locator(1)   ## wait for mouse click
})

# Or: for(u in w) {hist(u); title(label(u))}
</code></pre>

<hr>
<h2 id='Lag'>Lag a Numeric, Character, or Factor Vector</h2><span id='topic+Lag'></span>

<h3>Description</h3>

<p>Shifts a vector <code>shift</code> elements later.  Character or factor
variables are padded with <code>""</code>, numerics with <code>NA</code>.  The shift
may be negative.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Lag(x, shift = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Lag_+3A_x">x</code></td>
<td>
<p>a vector</p>
</td></tr>
<tr><td><code id="Lag_+3A_shift">shift</code></td>
<td>
<p>integer specifying the number of observations to
be shifted to the right.  Negative values imply shifts to the left.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A.ttributes of the original object are carried along to the new lagged
one.
</p>


<h3>Value</h3>

<p>a vector like <code>x</code>
</p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+lag">lag</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>Lag(1:5,2)
Lag(letters[1:4],2)
Lag(factor(letters[1:4]),-2)
# Find which observations are the first for a given subject
id &lt;- c('a','a','b','b','b','c')
id != Lag(id)
!duplicated(id)
</code></pre>

<hr>
<h2 id='latestFile'>latestFile</h2><span id='topic+latestFile'></span>

<h3>Description</h3>

<p>Find File With Latest Modification Time
</p>


<h3>Usage</h3>

<pre><code class='language-R'>latestFile(pattern, path = ".", verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="latestFile_+3A_pattern">pattern</code></td>
<td>
<p>a regular expression; see <code><a href="base.html#topic+list.files">base::list.files()</a></code></p>
</td></tr>
<tr><td><code id="latestFile_+3A_path">path</code></td>
<td>
<p>full path, defaulting to current working directory</p>
</td></tr>
<tr><td><code id="latestFile_+3A_verbose">verbose</code></td>
<td>
<p>set to <code>FALSE</code> to not report on total number of matching files</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Subject to matching on <code>pattern</code> finds the last modified file, and if <code>verbose</code> is <code>TRUE</code> reports on how many total files matched <code>pattern</code>.
</p>


<h3>Value</h3>

<p>the name of the last modified file
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+list.files">base::list.files()</a></code>
</p>

<hr>
<h2 id='latex'>
Convert an S object to LaTeX, and Related Utilities
</h2><span id='topic+latex'></span><span id='topic+latex.default'></span><span id='topic+latex.function'></span><span id='topic+latex.list'></span><span id='topic+latexTranslate'></span><span id='topic+htmlTranslate'></span><span id='topic+latexSN'></span><span id='topic+htmlSN'></span><span id='topic+latexVerbatim'></span><span id='topic+dvi'></span><span id='topic+print.dvi'></span><span id='topic+dvi.latex'></span><span id='topic+dvips'></span><span id='topic+dvips.latex'></span><span id='topic+dvips.dvi'></span><span id='topic+dvigv'></span><span id='topic+dvigv.latex'></span><span id='topic+dvigv.dvi'></span><span id='topic+print.latex'></span><span id='topic+show.latex'></span><span id='topic+show.dvi'></span>

<h3>Description</h3>

<p><code>latex</code> converts its argument to a &lsquo;<span class="file">.tex</span>&rsquo; file appropriate
for inclusion in a LaTeX2e document.  <code>latex</code> is a generic
function that calls one of <code>latex.default</code>,
<code>latex.function</code>, <code>latex.list</code>. 
</p>
<p><code>latex.default</code>
does appropriate rounding and decimal alignment and produces a
file containing a LaTeX tabular environment to print the matrix or data.frame
<code>x</code> as a table.
</p>
<p><code>latex.function</code> prepares an S function for printing by issuing <code>sed</code>
commands that are similar to those in the
<code>S.to.latex</code> procedure in the <code>s.to.latex</code> package (Chambers
and Hastie, 1993).  <code>latex.function</code> can also produce
<code>verbatim</code> output or output that works with the <code>Sweavel</code>
LaTeX style at <a href="https://biostat.app.vumc.org/wiki/Main/SweaveTemplate">https://biostat.app.vumc.org/wiki/Main/SweaveTemplate</a>.
</p>
<p><code>latex.list</code> calls <code>latex</code> recursively for each element in the argument.
</p>
<p><code>latexTranslate</code> translates particular items in character
strings to LaTeX format, e.g., makes &lsquo;<span class="samp">&#8288;a^2 = a\$^2\$&#8288;</span>&rsquo; for superscript within
variable labels.   LaTeX names of greek letters (e.g., <code>"alpha"</code>)
will have backslashes added if <code>greek==TRUE</code>.  Math mode is
inserted as needed. 
<code>latexTranslate</code> assumes that input text always has matches,
e.g. <code>[) [] (] ()</code>, and that surrounding  by &lsquo;<span class="samp">&#8288;\$\$&#8288;</span>&rsquo; is OK.
</p>
<p><code>htmlTranslate</code> is similar to <code>latexTranslate</code> but for html
translation.  It doesn't need math mode and assumes dollar signs are
just that.
</p>
<p><code>latexSN</code> converts a vector floating point numbers to character
strings using LaTeX exponents.  Dollar signs to enter math mode are not
added.  Similarly, <code>htmlSN</code> converts to scientific notation in html.
</p>
<p><code>latexVerbatim</code> on an object executes the object's <code>print</code> method,
capturing the output for a file inside a LaTeX verbatim environment.
</p>
<p><code>dvi</code> uses the system <code>latex</code> command to compile LaTeX code produced
by <code>latex</code>, including any needed styles.  <code>dvi</code>
will put a &lsquo;<span class="samp">&#8288;\documentclass{report}&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;\end{document}&#8288;</span>&rsquo; wrapper
around a file produced by <code>latex</code>.  By default, the &lsquo;<span class="samp">&#8288;geometry&#8288;</span>&rsquo; LaTeX package is
used to omit all margins and to set the paper size to a default of
5.5in wide by 7in tall.  The result of <code>dvi</code> is a .dvi file.  To both
format and screen display a non-default size, use for example
<code>print(dvi(latex(x), width=3, height=4),width=3,height=4)</code>.  Note that
you can use something like &lsquo;<span class="samp">&#8288;xdvi -geometry 460x650 -margins 2.25in
file&#8288;</span>&rsquo; without changing LaTeX defaults to emulate this.
</p>
<p><code>dvips</code> will use the system <code>dvips</code> command to print the .dvi file to
the default system printer, or create a postscript file if <code>file</code>
is specified.
</p>
<p><code>dvigv</code> uses the system <code>dvips</code> command to convert the input object
to a .dvi file, and uses the system <code>dvips</code> command to convert it to
postscript.  Then the postscript file is displayed using Ghostview
(assumed to be the system command <code>gv</code>).
</p>
<p>There are <code>show</code> methods for displaying typeset LaTeX
on the screen using the system <code>xdvi</code>
command.   If you <code>show</code> a LaTeX file created by
<code>latex</code> without running it through <code>dvi</code> using
<code>show.dvi(object)</code>, the 
<code>show</code> method will run it through <code>dvi</code> automatically.
These <code>show</code> 
methods are not S Version 4 methods so you have to use full names such
as <code>show.dvi</code> and <code>show.latex</code>.  Use the <code>print</code> methods for
more automatic display of typesetting, e.g. typing <code>latex(x)</code> will
invoke xdvi to view the typeset document.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>latex(object, ...)

## Default S3 method:
latex(object,
    title=first.word(deparse(substitute(object))),
    file=paste(title, ".tex", sep=""),
    append=FALSE, label=title,
    rowlabel=title, rowlabel.just="l",
    cgroup=NULL, n.cgroup=NULL,
    rgroup=NULL, n.rgroup=NULL,
    cgroupTexCmd="bfseries",
    rgroupTexCmd="bfseries",
    rownamesTexCmd=NULL,
    colnamesTexCmd=NULL,
    cellTexCmds=NULL,
    rowname, cgroup.just=rep("c",length(n.cgroup)),
    colheads=NULL,
    extracolheads=NULL, extracolsize='scriptsize',
    dcolumn=FALSE, numeric.dollar=!dcolumn, cdot=FALSE,
    longtable=FALSE, draft.longtable=TRUE, ctable=FALSE, booktabs=FALSE,
    table.env=TRUE, here=FALSE, lines.page=40,
    caption=NULL, caption.lot=NULL, caption.loc=c('top','bottom'),
    star=FALSE,
    double.slash=FALSE,
    vbar=FALSE, collabel.just=rep("c",nc), na.blank=TRUE,
    insert.bottom=NULL, insert.bottom.width=NULL,
    insert.top=NULL,
    first.hline.double=!(booktabs | ctable),
    where='!tbp', size=NULL,
    center=c('center','centering','centerline','none'),
    landscape=FALSE,
    multicol=TRUE,
    math.row.names=FALSE, already.math.row.names=FALSE,
    math.col.names=FALSE, already.math.col.names=FALSE,
    hyperref=NULL, continued='continued',
    ...) # x is a matrix or data.frame

## S3 method for class 'function'
latex(
	object,
	title=first.word(deparse(substitute(object))),
	file=paste(title, ".tex", sep=""),
	append=FALSE,
	assignment=TRUE,  type=c('example','verbatim','Sinput'),
    width.cutoff=70, size='', ...)

## S3 method for class 'list'
latex(
           object,
           title=first.word(deparse(substitute(object))),
           file=paste(title, ".tex", sep=""),
           append=FALSE,
           label,
           caption,
           caption.lot,
           caption.loc=c('top','bottom'),
           ...)

## S3 method for class 'latex'
print(x, ...)

latexTranslate(object, inn=NULL, out=NULL, pb=FALSE, greek=FALSE, na='',
               ...)

htmlTranslate(object, inn=NULL, out=NULL, greek=FALSE, na='',
              code=htmlSpecialType(), ...)

latexSN(x)

htmlSN(x, pretty=TRUE, ...)

latexVerbatim(x, title=first.word(deparse(substitute(x))),
    file=paste(title, ".tex", sep=""),
    append=FALSE, size=NULL, hspace=NULL,
    width=.Options$width, length=.Options$length, ...)

dvi(object, ...)
## S3 method for class 'latex'
dvi(object, prlog=FALSE, nomargins=TRUE, width=5.5, height=7, ...)
## S3 method for class 'dvi'
print(x, ...)
dvips(object, ...)
## S3 method for class 'latex'
dvips(object, ...)
## S3 method for class 'dvi'
dvips(object, file, ...)
## S3 method for class 'latex'
show(object)  # or show.dvi(object) or just object
dvigv(object, ...)
## S3 method for class 'latex'
dvigv(object, ...)       # or gvdvi(dvi(object))
## S3 method for class 'dvi'
dvigv(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="latex_+3A_object">object</code></td>
<td>

<p>For <code>latex</code>, any S object.  For <code>dvi</code> or <code>dvigv</code>, an object
created by <code>latex</code>.  For <code>latexTranslate</code> is a vector of
character strings to translate.  Any <code>NA</code>s are set to blank
strings before conversion.
</p>
</td></tr>
<tr><td><code id="latex_+3A_x">x</code></td>
<td>

<p>any object to be <code>print</code>ed verbatim for <code>latexVerbatim</code>.  For
<code>latexSN</code> or <code>htmlSN</code>, <code>x</code> is a numeric vector.
</p>
</td></tr>
<tr><td><code id="latex_+3A_title">title</code></td>
<td>

<p>name of file to create without the &lsquo;<span class="samp">&#8288;.tex&#8288;</span>&rsquo; extension.  If this
option is not set, value/string of <code>x</code> (see above) is printed
in the top left corner of the table.  Set <code>title=''</code> to
suppress this output.
</p>
</td></tr>
<tr><td><code id="latex_+3A_file">file</code></td>
<td>

<p>name of the file to create.  The default file name is &lsquo;<span class="file">x.tex</span>&rsquo; where
<code>x</code> is the first word in the name of the argument for <code>x</code>.
Set <code>file=""</code> to have the generated LaTeX code just printed to
standard output.  This is especially useful when running under Sweave in
R using its &lsquo;<span class="samp">&#8288;results=tex&#8288;</span>&rsquo; tag, to save having to manage many
small external files.  When <code>file=""</code>, <code>latex</code> keeps track of
LaTeX styles that are called for by creating or modifying an object
<code>latexStyles</code> (in <code>.GlobalTemp</code> in R or in frame 0 in
S-Plus).  <code>latexStyles</code> is a vector containing the base names of
all the unique LaTeX styles called for so far in the current session.
See the end of the examples section for a way to use this object to good
effect.  For <code>dvips</code>, <code>file</code> is the name of an output
postscript file.
</p>
</td></tr>
<tr><td><code id="latex_+3A_append">append</code></td>
<td>

<p>defaults to <code>FALSE</code>. Set to <code>TRUE</code> to append output to an existing file.
</p>
</td></tr>
<tr><td><code id="latex_+3A_label">label</code></td>
<td>

<p>a text string representing a symbolic label for the table for referencing
in the LaTeX &lsquo;<span class="samp">&#8288;\label&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;\ref&#8288;</span>&rsquo; commands.
<code>label</code> is only used if <code>caption</code> is given.
</p>
</td></tr>
<tr><td><code id="latex_+3A_rowlabel">rowlabel</code></td>
<td>

<p>If <code>x</code> has row dimnames, <code>rowlabel</code> is a character string containing the
column heading for the row dimnames. The default is the name of the
argument for <code>x</code>.
</p>
</td></tr>
<tr><td><code id="latex_+3A_rowlabel.just">rowlabel.just</code></td>
<td>

<p>If <code>x</code> has row dimnames, specifies the justification for printing them.
Possible values are <code>"l"</code>, <code>"r"</code>, <code>"c"</code>. The heading (<code>rowlabel</code>) itself
is left justified if <code>rowlabel.just="l"</code>, otherwise it is centered.
</p>
</td></tr>
<tr><td><code id="latex_+3A_cgroup">cgroup</code></td>
<td>

<p>a vector of character strings defining major column headings. The default is
to have none.
</p>
</td></tr>
<tr><td><code id="latex_+3A_n.cgroup">n.cgroup</code></td>
<td>

<p>a vector containing the number of columns for which each element in
cgroup is a heading.  For example, specify <code>cgroup=c("Major 1","Major 2")</code>,
<code>n.cgroup=c(3,3)</code> if <code>"Major 1"</code> is to span columns 1-3 and <code>"Major 2"</code> is
to span columns 4-6.  <code>rowlabel</code> does not count in the column numbers.
You can omit <code>n.cgroup</code> if all groups have the same number of columns.
</p>
</td></tr>
<tr><td><code id="latex_+3A_rgroup">rgroup</code></td>
<td>

<p>a vector of character strings containing headings for row groups.
<code>n.rgroup</code> must be present when <code>rgroup</code> is given. The first <code>n.rgroup[1]</code>
rows are sectioned off and <code>rgroup[1]</code> is used as a bold heading for
them. The usual row dimnames (which must be present if <code>rgroup</code> is) are 
indented. The next <code>n.rgroup[2]</code> rows are treated likewise, etc.
</p>
</td></tr>
<tr><td><code id="latex_+3A_n.rgroup">n.rgroup</code></td>
<td>

<p>integer vector giving the number of rows in each grouping. If <code>rgroup</code>
is not specified, <code>n.rgroup</code> is just used to divide off blocks of
rows by horizontal lines. If <code>rgroup</code> is given but <code>n.rgroup</code> is omitted,
<code>n.rgroup</code> will default so that each row group contains the same number
of rows.
</p>
</td></tr>
<tr><td><code id="latex_+3A_cgrouptexcmd">cgroupTexCmd</code></td>
<td>
<p>A character string specifying a LaTeX command to be
used to format column group labels.  The default, <code>"bfseries"</code>, sets
the current font to &lsquo;bold&rsquo;.  It is possible to supply a vector of
strings so that each column group label is formatted differently.
Please note that the first item of the vector is used to format the
title (even if a title is not used). Currently the user needs to handle
these issue.  Multiple effects can be achieved by creating custom
LaTeX commands; for example,
<code>"\providecommand{\redscshape}{\color{red}\scshape}"</code> creates a
LaTeX command called &lsquo;<span class="samp">&#8288;\redscshape&#8288;</span>&rsquo; that formats the text in red
small-caps.
</p>
</td></tr>
<tr><td><code id="latex_+3A_rgrouptexcmd">rgroupTexCmd</code></td>
<td>
<p>A character string specifying a LaTeX command to be
used to format row group labels.  The default, <code>"bfseries"</code>, sets the
current font to &lsquo;bold&rsquo;.  A vector of strings can be supplied to
format each row group label differently.  Normal recycling applies
if the vector is shorter than <code>n.rgroups</code>.  See also
<code>cgroupTexCmd</code> above regarding multiple effects.
</p>
</td></tr>
<tr><td><code id="latex_+3A_rownamestexcmd">rownamesTexCmd</code></td>
<td>
<p>A character string specifying a LaTeX
command to be used to format rownames. The default, <code>NULL</code>, applies no
command.  A vector of different commands can also be supplied.
See also <code>cgroupTexCmd</code> above regarding multiple effects.
</p>
</td></tr>
<tr><td><code id="latex_+3A_colnamestexcmd">colnamesTexCmd</code></td>
<td>
<p>A character string specifying a LaTeX command to be
used to format column labels. The default, <code>NULL</code>, applies no command.
It is possible to supply a vector of strings to format each column
label differently. If column groups are not used, the first item in
the vector will be used to format the title. Please note that if
column groups are used the first item of <code>cgroupTexCmd</code> and not
<code>colnamesTexCmd</code> is used to format the title. The user needs to allow for
these issues when supplying a vector of commands.  See also
<code>cgroupTexCmd</code> above regarding multiple effects.
</p>
</td></tr>
<tr><td><code id="latex_+3A_celltexcmds">cellTexCmds</code></td>
<td>
<p>A matrix of character strings which are LaTeX
commands to be
used to format each element, or cell, of the object.  The matrix
must have the same <code>NROW()</code> and <code>NCOL()</code> as the object.  The default,
NULL, applies no formats.  Empty strings also apply no formats, and
one way to start might be to create a matrix of empty strings with
<code>matrix(rep("", NROW(x) * NCOL(x)), nrow=NROW(x))</code> and then
selectively change appropriate elements of the matrix.  Note that
you might need to set <code>numeric.dollar=FALSE</code> (to disable math
mode) for some effects to work. See also <code>cgroupTexCmd</code> above
regarding multiple effects.
</p>
</td></tr>
<tr><td><code id="latex_+3A_na.blank">na.blank</code></td>
<td>

<p>Set to <code>TRUE</code> to use blanks rather than <code>NA</code> for missing values.
This usually looks better in <code>latex</code>.
</p>
</td></tr>
<tr><td><code id="latex_+3A_insert.bottom">insert.bottom</code></td>
<td>

<p>an optional character string to typeset at the bottom of the table.
For <code>"ctable"</code> style tables, this is placed in an unmarked footnote.
</p>
</td></tr>
<tr><td><code id="latex_+3A_insert.bottom.width">insert.bottom.width</code></td>
<td>

<p>character string; a tex width controlling the width of the
insert.bottom text.  Currently only does something with using
<code>longtable=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="latex_+3A_insert.top">insert.top</code></td>
<td>
<p>a character string to insert as a heading right
before beginning <code>tabular</code> environment.  Useful for multiple
sub-tables.</p>
</td></tr>
<tr><td><code id="latex_+3A_first.hline.double">first.hline.double</code></td>
<td>

<p>set to <code>FALSE</code> to use single horizontal rules for styles other than
<code>"bookmark"</code> or <code>"ctable"</code>
</p>
</td></tr>
<tr><td><code id="latex_+3A_rowname">rowname</code></td>
<td>

<p>rownames for <code>tabular</code> environment.  Default is rownames of matrix or
data.frame.  Specify <code>rowname=NULL</code> to suppress the use of row names.
</p>
</td></tr>
<tr><td><code id="latex_+3A_cgroup.just">cgroup.just</code></td>
<td>

<p>justification for labels for column groups.  Defaults to <code>"c"</code>.
</p>
</td></tr>
<tr><td><code id="latex_+3A_colheads">colheads</code></td>
<td>
<p>a character vector of column headings if you don't want
to use <code>dimnames(object)[[2]]</code>.  Specify <code>colheads=FALSE</code> to
suppress column headings.</p>
</td></tr>
<tr><td><code id="latex_+3A_extracolheads">extracolheads</code></td>
<td>

<p>an optional vector of extra column headings that will appear under the
main headings (e.g., sample sizes).  This character vector does not
need to include an empty space for any <code>rowname</code> in effect, as
this will be added automatically.  You can also form subheadings by
splitting character strings defining the column headings using the
usual backslash <code>n</code> newline character.</p>
</td></tr>
<tr><td><code id="latex_+3A_extracolsize">extracolsize</code></td>
<td>

<p>size for <code>extracolheads</code> or for any second lines in column names;
default is <code>"scriptsize"</code> 
</p>
</td></tr>
<tr><td><code id="latex_+3A_dcolumn">dcolumn</code></td>
<td>
<p>see <code><a href="#topic+format.df">format.df</a></code></p>
</td></tr>
<tr><td><code id="latex_+3A_numeric.dollar">numeric.dollar</code></td>
<td>

<p>logical, default <code>!dcolumn</code>.  Set to <code>TRUE</code> to place dollar
signs around numeric values when <code>dcolumn=FALSE</code>.  This 
assures that <code>latex</code> will use minus signs rather than hyphens to indicate
negative numbers.  Set to <code>FALSE</code> when <code>dcolumn=TRUE</code>, as
<code>dcolumn.sty</code> automatically uses minus signs.
</p>
</td></tr>
<tr><td><code id="latex_+3A_math.row.names">math.row.names</code></td>
<td>

<p>logical, set true to place dollar signs around the row names.
</p>
</td></tr>
<tr><td><code id="latex_+3A_already.math.row.names">already.math.row.names</code></td>
<td>
<p>set to <code>TRUE</code> to prevent any math
mode changes to row names</p>
</td></tr>
<tr><td><code id="latex_+3A_math.col.names">math.col.names</code></td>
<td>

<p>logical, set true to place dollar signs around the column names.
</p>
</td></tr>
<tr><td><code id="latex_+3A_already.math.col.names">already.math.col.names</code></td>
<td>
<p>set to <code>TRUE</code> to prevent any math
mode changes to column names</p>
</td></tr>
<tr><td><code id="latex_+3A_hyperref">hyperref</code></td>
<td>
<p>if <code>table.env=TRUE</code> is a character string used to
generate a LaTeX <code>hyperref</code> enclosure</p>
</td></tr>
<tr><td><code id="latex_+3A_continued">continued</code></td>
<td>
<p>a character string used to indicate pages after the
first when making a long table</p>
</td></tr>
<tr><td><code id="latex_+3A_cdot">cdot</code></td>
<td>
<p>see <code><a href="#topic+format.df">format.df</a></code></p>
</td></tr>
<tr><td><code id="latex_+3A_longtable">longtable</code></td>
<td>

<p>Set to <code>TRUE</code> to use David Carlisle's LaTeX <code>longtable</code> style, allowing
long tables to be split over multiple pages with headers repeated on
each page.
The <code>"style"</code> element is set to <code>"longtable"</code>. The <code>latex</code> &lsquo;<span class="samp">&#8288;\usepackage&#8288;</span>&rsquo;
must reference &lsquo;<span class="samp">&#8288;[longtable]&#8288;</span>&rsquo;.
The file &lsquo;<span class="file">longtable.sty</span>&rsquo; will
need to be in a directory in your <span class="env">TEXINPUTS</span> path.
</p>
</td></tr>
<tr><td><code id="latex_+3A_draft.longtable">draft.longtable</code></td>
<td>

<p>I forgot what this does.
</p>
</td></tr>
<tr><td><code id="latex_+3A_ctable">ctable</code></td>
<td>

<p>set to <code>TRUE</code> to use Wybo Dekker's &lsquo;<span class="samp">&#8288;ctable&#8288;</span>&rsquo; style from
<abbr><span class="acronym">CTAN</span></abbr>.  Even though for historical reasons it is not the
default, it is generally the preferred method.  Thicker but not
doubled &lsquo;<span class="samp">&#8288;\hline&#8288;</span>&rsquo;s are used to start a table when <code>ctable</code> is
in effect.
</p>
</td></tr>
<tr><td><code id="latex_+3A_booktabs">booktabs</code></td>
<td>

<p>set <code>booktabs=TRUE</code> to use the &lsquo;<span class="samp">&#8288;booktabs&#8288;</span>&rsquo; style of horizontal
rules for better tables.  In this case, double &lsquo;<span class="samp">&#8288;\hline&#8288;</span>&rsquo;s are not
used to start a table.
</p>
</td></tr>
<tr><td><code id="latex_+3A_table.env">table.env</code></td>
<td>

<p>Set <code>table.env=FALSE</code> to suppress enclosing the table in a LaTeX
&lsquo;<span class="samp">&#8288;table&#8288;</span>&rsquo; environment.  <code>table.env</code> only applies when
<code>longtable=FALSE</code>.  You may not specify a <code>caption</code> if
<code>table.env=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="latex_+3A_here">here</code></td>
<td>

<p>Set to <code>TRUE</code> if you are using <code>table.env=TRUE</code> with <code>longtable=FALSE</code> and you
have installed David Carlisle's &lsquo;<span class="file">here.sty</span>&rsquo; LaTeX style. This will cause
the LaTeX &lsquo;<span class="samp">&#8288;table&#8288;</span>&rsquo; environment to be set up with option &lsquo;<span class="samp">&#8288;H&#8288;</span>&rsquo; to guarantee
that the table will appear exactly where you think it will in the text.
The <code>"style"</code> element is set to <code>"here"</code>. The <code>latex</code> &lsquo;<span class="samp">&#8288;\usepackage&#8288;</span>&rsquo;
must reference &lsquo;<span class="samp">&#8288;[here]&#8288;</span>&rsquo;.  The file &lsquo;<span class="file">here.sty</span>&rsquo; will
need to be in a directory in your <span class="env">TEXINPUTS</span> path.  &lsquo;<span class="samp">&#8288;here&#8288;</span>&rsquo; is
largely obsolete with LaTeX2e.
</p>
</td></tr>
<tr><td><code id="latex_+3A_lines.page">lines.page</code></td>
<td>

<p>Applies if <code>longtable=TRUE</code>. No more than <code>lines.page</code> lines in the body
of a table will be placed on a single page. Page breaks will only
occur at <code>rgroup</code> boundaries.
</p>
</td></tr>
<tr><td><code id="latex_+3A_caption">caption</code></td>
<td>

<p>a text string to use as a caption to print at the top of the first
page of the table. Default is no caption.
</p>
</td></tr>
<tr><td><code id="latex_+3A_caption.lot">caption.lot</code></td>
<td>

<p>a text string representing a short caption to be used in the &ldquo;List of Tables&rdquo;.
By default, LaTeX will use <code>caption</code>.  If you get inexplicable &lsquo;<span class="samp">&#8288;latex&#8288;</span>&rsquo; errors,
you may need to supply <code>caption.lot</code> to make the errors go away.
</p>
</td></tr>
<tr><td><code id="latex_+3A_caption.loc">caption.loc</code></td>
<td>

<p>set to <code>"bottom"</code> to position a caption below
the table instead of the default of <code>"top"</code>.
</p>
</td></tr>
<tr><td><code id="latex_+3A_star">star</code></td>
<td>

<p>apply the star option for ctables to allow a table to spread over
two columns when in twocolumn mode.
</p>
</td></tr>
<tr><td><code id="latex_+3A_double.slash">double.slash</code></td>
<td>

<p>set to <code>TRUE</code> to output &lsquo;<span class="samp">&#8288;"\"&#8288;</span>&rsquo; as &lsquo;<span class="samp">&#8288;"\\"&#8288;</span>&rsquo; in LaTeX commands. Useful when you
are reading the output file back into an S vector for later output.
</p>
</td></tr>
<tr><td><code id="latex_+3A_vbar">vbar</code></td>
<td>

<p>logical. When <code>vbar==TRUE</code>, columns in the tabular environment are separated with
vertical bar characters.  When <code>vbar==FALSE</code>, columns are separated with white
space.  The default, <code>vbar==FALSE</code>, produces tables consistent with the style sheet
for the Journal of the American Statistical Association.
</p>
</td></tr>
<tr><td><code id="latex_+3A_collabel.just">collabel.just</code></td>
<td>

<p>justification for column labels.
</p>
</td></tr>
<tr><td><code id="latex_+3A_assignment">assignment</code></td>
<td>

<p>logical.  When <code>TRUE</code>, the default, the name of the function
and the assignment arrow are printed to the file.
</p>
</td></tr>
<tr><td><code id="latex_+3A_where">where</code></td>
<td>

<p>specifies placement of floats if a table environment is used.  Default
is <code>"!tbp"</code>.  To allow tables to appear in the middle of a page of
text you might specify <code>where="!htbp"</code> to <code>latex.default</code>.
</p>
</td></tr>
<tr><td><code id="latex_+3A_size">size</code></td>
<td>

<p>size of table text if a size change is needed (default is no change).
For example you might specify <code>size="small"</code> to use LaTeX font size
&ldquo;small&rdquo;.  For <code>latex.function</code> is a character string
that will be appended to <code>"Sinput"</code> such as <code>"small"</code>.
</p>
</td></tr>
<tr><td><code id="latex_+3A_center">center</code></td>
<td>

<p>default is <code>"center"</code> to enclose the table in a &lsquo;<span class="samp">&#8288;center&#8288;</span>&rsquo;
environment.  Use <code>center="centering"</code> or <code>"centerline"</code>
to instead use LaTeX 
&lsquo;<span class="samp">&#8288;centering&#8288;</span>&rsquo; or <code>centerline</code> directives, or
<code>center="none"</code> to use no 
centering.  <code>centerline</code> can be useful when objects besides a
<code>tabular</code> are enclosed in a single <code>table</code> environment.
This option was implemented by Markus Jäntti
<a href="mailto:markus.jantti@iki.fi">markus.jantti@iki.fi</a> of Abo Akademi University.
</p>
</td></tr>
<tr><td><code id="latex_+3A_landscape">landscape</code></td>
<td>

<p>set to <code>TRUE</code> to enclose the table in a &lsquo;<span class="samp">&#8288;landscape&#8288;</span>&rsquo;
environment.  When <code>ctable</code> is <code>TRUE</code>, will use the
<code>rotate</code> argument to <code>ctable</code>.
</p>
</td></tr>
<tr><td><code id="latex_+3A_type">type</code></td>
<td>

<p>The default uses the S <code>alltt</code> environment for <code>latex.function</code>,
Set <code>type="verbatim"</code> to instead use the LaTeX &lsquo;<span class="samp">&#8288;verbatim&#8288;</span>&rsquo;
environment.  Use <code>type="Sinput"</code> if using <code>Sweave</code>,
especially if you have customized the <code>Sinput</code> environment, for
example using the <code>Sweavel</code> style which uses the
<code>listings</code> LaTeX package.
</p>
</td></tr>
<tr><td><code id="latex_+3A_width.cutoff">width.cutoff</code></td>
<td>
<p>width of function text output in columns; see
<code>deparse</code></p>
</td></tr>
<tr><td><code id="latex_+3A_...">...</code></td>
<td>

<p>other arguments are accepted and ignored except that <code>latex</code>
passes arguments to <code>format.df</code> (e.g., <code>col.just</code> and other
formatting options like <code>dec</code>, <code>rdec</code>, and <code>cdec</code>).  For
<code>latexVerbatim</code> these arguments are passed to the <code>print</code>
function.  Ignored for <code>latexTranslate</code> and
<code>htmlTranslate</code>.  For <code>htmlSN</code>, these arguments are passed
to <code>prettyNum</code> or <code>format</code>.
</p>
</td></tr>
<tr><td><code id="latex_+3A_inn">inn</code>, <code id="latex_+3A_out">out</code></td>
<td>

<p>specify additional input and translated strings over the usual
defaults
</p>
</td></tr>
<tr><td><code id="latex_+3A_pb">pb</code></td>
<td>

<p>If <code>pb=TRUE</code>, <code>latexTranslate</code> also translates &lsquo;<span class="samp">&#8288;[()]&#8288;</span>&rsquo;
to math mode using &lsquo;<span class="samp">&#8288;\left, \right&#8288;</span>&rsquo;.
</p>
</td></tr>
<tr><td><code id="latex_+3A_greek">greek</code></td>
<td>
<p>set to <code>TRUE</code> to have <code>latexTranslate</code> put names
for greek letters in math mode and add backslashes. For
<code>htmlTranslate</code>, translates greek letters to corresponding html
characters, ignoring &quot;modes&quot;.</p>
</td></tr>
<tr><td><code id="latex_+3A_na">na</code></td>
<td>
<p>single character string to translate <code>NA</code> values to for
<code>latexTranslate</code> and <code>htmlTranslate</code></p>
</td></tr>
<tr><td><code id="latex_+3A_code">code</code></td>
<td>
<p>set to <code>'unicode'</code> to use HTML unicode characters
or <code>'&amp;'</code> to use the ampersand pound number format</p>
</td></tr>
<tr><td><code id="latex_+3A_pretty">pretty</code></td>
<td>
<p>set to <code>FALSE</code> to have <code>htmlSN</code> use
<code>format</code> instead of <code>prettyNum</code></p>
</td></tr>
<tr><td><code id="latex_+3A_hspace">hspace</code></td>
<td>

<p>horizontal space, e.g., extra left margin for verbatim text.  Default
is none.  Use e.g. <code>hspace="10ex"</code> to add 10 extra spaces to the left
of the text.
</p>
</td></tr>
<tr><td><code id="latex_+3A_length">length</code></td>
<td>
<p>for S-Plus only; is the length of the output page for
printing and capturing verbatim text</p>
</td></tr>
<tr><td><code id="latex_+3A_width">width</code>, <code id="latex_+3A_height">height</code></td>
<td>

<p>are the <code>options( )</code> to have in effect only for when <code>print</code> is
executed.  Defaults are current <code>options</code>.  For <code>dvi</code> these specify
the paper width and height in inches if <code>nomargins=TRUE</code>, with
defaults of 5.5 and 7, respectively.
</p>
</td></tr>
<tr><td><code id="latex_+3A_prlog">prlog</code></td>
<td>

<p>set to <code>TRUE</code> to have <code>dvi</code> print, to the S-Plus session, the LaTeX .log
file.
</p>
</td></tr>
<tr><td><code id="latex_+3A_multicol">multicol</code></td>
<td>

<p>set  to <code>FALSE</code> to not use &lsquo;<span class="samp">&#8288;\multicolumn&#8288;</span>&rsquo; in header
of table
</p>
</td></tr>
<tr><td><code id="latex_+3A_nomargins">nomargins</code></td>
<td>

<p>set to <code>FALSE</code> to use default LaTeX margins when making the .dvi file
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>latex.default</code> optionally outputs a LaTeX comment containing the calling
statement.  To output this comment, run
<code>options(omitlatexcom=FALSE)</code> before running.  The default behavior or suppressing the comment is helpful
when running RMarkdown to produce pdf output using LaTeX, as this uses
<code>pandoc</code> which is fooled into try to escape the percent
comment symbol.
</p>
<p>If running under Windows and using MikTeX, <code>latex</code> and <code>yap</code>
must be in your system path, and <code>yap</code> is used to browse
&lsquo;<span class="file">.dvi</span>&rsquo; files created by <code>latex</code>.  You should install the
&lsquo;<span class="file">geometry.sty</span>&rsquo; and &lsquo;<span class="file">ctable.sty</span>&rsquo; styles in MikTeX to make optimum use
of <code>latex()</code>.
</p>
<p>On Mac OS X, you may have to append the &lsquo;<span class="file">/usr/texbin</span>&rsquo; directory to the
system path.  Thanks to Kevin Thorpe
(<a href="mailto:kevin.thorpe@utoronto.ca">kevin.thorpe@utoronto.ca</a>) one way to set up Mac OS X is
to install &lsquo;<span class="samp">&#8288;X11&#8288;</span>&rsquo; and &lsquo;<span class="samp">&#8288;X11SDK&#8288;</span>&rsquo; if not already installed,
start &lsquo;<span class="samp">&#8288;X11&#8288;</span>&rsquo; within the R GUI, and issue the command
<code>Sys.setenv( PATH=paste(Sys.getenv("PATH"),"/usr/texbin",sep=":")
  )</code>.  To avoid any complications of using &lsquo;<span class="samp">&#8288;X11&#8288;</span>&rsquo; under MacOS, users
can install the &lsquo;<span class="samp">&#8288;TeXShop&#8288;</span>&rsquo; package, which will associate
&lsquo;<span class="file">.dvi</span>&rsquo; files with a viewer that displays a &lsquo;<span class="file">pdf</span>&rsquo; version of
the file after a hidden conversion from &lsquo;<span class="file">dvi</span>&rsquo; to &lsquo;<span class="file">pdf</span>&rsquo;.
</p>
<p>System options can be used to specify external commands to be used.
Defaults are given by <code>options(xdvicmd='xdvi')</code> or
<code>options(xdvicmd='yap')</code>, <code>options(dvipscmd='dvips')</code>,
<code>options(latexcmd='latex')</code>.  For MacOS specify
<code>options(xdvicmd='MacdviX')</code> or if TeXShop is installed,
<code>options(xdvicmd='open')</code>.
</p>
<p>To use &lsquo;<span class="samp">&#8288;pdflatex&#8288;</span>&rsquo; rather than &lsquo;<span class="samp">&#8288;latex&#8288;</span>&rsquo;, set
<code>options(latexcmd='pdflatex')</code>,
<code>options(dviExtension='pdf')</code>, and set
<code>options('xdvicmd')</code> to your chosen PDF previewer.
</p>
<p>If running S-Plus and your directory for temporary files is not
&lsquo;<span class="file">/tmp</span>&rsquo; (Unix/Linux) or &lsquo;<span class="file">\windows\temp</span>&rsquo; (Windows), add your
own <code>tempdir</code> function such as <code>
	tempdir &lt;- function() "/yourmaindirectory/yoursubdirectory"</code>
</p>
<p>To prevent the latex file from being displayed store the result of
<code>latex</code> in an object, e.g. <code>w &lt;- latex(object, file='foo.tex')</code>.
</p>


<h3>Value</h3>

<p><code>latex</code> and <code>dvi</code> return a
list of class <code>latex</code> or <code>dvi</code> containing character string
elements <code>file</code> and <code>style</code>.  <code>file</code> contains the name of the
generated file, and <code>style</code> is a vector (possibly empty) of styles to
be included using the LaTeX2e &lsquo;<span class="samp">&#8288;\usepackage&#8288;</span>&rsquo; command.
</p>
<p><code>latexTranslate</code> returns a vector of character strings
</p>


<h3>Side Effects</h3>

<p>creates various system files and runs various Linux/UNIX system
commands which are assumed to be in the system path.
</p>


<h3>Author(s)</h3>

<p>Frank E. Harrell, Jr.,<br />
Department of Biostatistics,<br />
Vanderbilt University,<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>
<p>Richard M. Heiberger,<br />
Department of Statistics,<br />
Temple University, Philadelphia, PA.<br />
<a href="mailto:rmh@temple.edu">rmh@temple.edu</a>
</p>
<p>David R. Whiting,<br />
School of Clinical Medical Sciences (Diabetes),<br />
University of Newcastle upon Tyne, UK.<br />
<a href="mailto:david.whiting@ncl.ac.uk">david.whiting@ncl.ac.uk</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+html">html</a></code>, <code><a href="#topic+format.df">format.df</a></code>, <code><a href="tools.html#topic+texi2dvi">texi2dvi</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(1:6, nrow=2, dimnames=list(c('a','b'),c('c','d','this that')))
## Not run: 
latex(x)   # creates x.tex in working directory
# The result of the above command is an object of class "latex"
# which here is automatically printed by the latex print method.
# The latex print method prepends and appends latex headers and
# calls the latex program in the PATH.  If the latex program is
# not in the PATH, you will get error messages from the operating
# system.

w &lt;- latex(x, file='/tmp/my.tex')
# Does not call the latex program as the print method was not invoked
print.default(w)
# Shows the contents of the w variable without attempting to latex it.

d &lt;- dvi(w)  # compile LaTeX document, make .dvi
             # latex assumed to be in path
d            # or show(d) : run xdvi (assumed in path) to display
w            # or show(w) : run dvi then xdvi
dvips(d)     # run dvips to print document
dvips(w)     # run dvi then dvips
library(tools)
texi2dvi('/tmp/my.tex')   # compile and produce pdf file in working dir.

## End(Not run)
latex(x, file="")   # just write out LaTeX code to screen

## Not run: 
# Use paragraph formatting to wrap text to 3 in. wide in a column
d &lt;- data.frame(x=1:2,
                y=c(paste("a",
                    paste(rep("very",30),collapse=" "),"long string"),
                "a short string"))
latex(d, file="", col.just=c("l", "p{3in}"), table.env=FALSE)

## End(Not run)

## Not run: 
# After running latex( ) multiple times with different special styles in
# effect, make a file that will call for the needed LaTeX packages when
# latex is run (especially when using Sweave with R)
if(exists(latexStyles))
  cat(paste('\usepackage{',latexStyles,'}',sep=''),
      file='stylesused.tex', sep='\n')
# Then in the latex job have something like:
# \documentclass{article}
# \input{stylesused}
# \begin{document}
# ...

## End(Not run)
</code></pre>

<hr>
<h2 id='latexCheckOptions'>Check whether the options for latex functions have been specified.</h2><span id='topic+latexCheckOptions'></span>

<h3>Description</h3>

<p>Check whether the options for latex functions have been specified.
If any of<br />
<code>options()[c("latexcmd","dviExtension","xdvicmd")]</code>
are <code>NULL</code>, an error message is displayed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>latexCheckOptions(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="latexCheckOptions_+3A_...">...</code></td>
<td>

<p>Any arguments are ignored.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If any <code>NULL</code> options are detected, the invisible text of the
error message.  If all three options have non-<code>NULL</code> values, NULL.
</p>


<h3>Author(s)</h3>

<p> Richard M. Heiberger &lt;rmh@temple.edu&gt; </p>


<h3>See Also</h3>

<p><code><a href="#topic+latex">latex</a></code>
</p>

<hr>
<h2 id='latexDotchart'>Enhanced Dot Chart for LaTeX Picture Environment with epic</h2><span id='topic+latexDotchart'></span>

<h3>Description</h3>

<p><code>latexDotchart</code>  is a translation of the <code>dotchart3</code> function
for producing a vector of character strings containing LaTeX picture
environment markup that mimics <code>dotchart3</code> output.  The LaTeX
<code>epic</code> and <code>color</code> packages are required.  The <code>add</code> and
<code>horizontal=FALSE</code> options are not available for
<code>latexDotchart</code>, however. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>latexDotchart(data, labels, groups=NULL, gdata=NA, 
  xlab='', auxdata, auxgdata=NULL, auxtitle,
  w=4, h=4, margin,      
  lines=TRUE, dotsize = .075, size='small', size.labels='small',
  size.group.labels='normalsize', ttlabels=FALSE, sort.=TRUE,
  xaxis=TRUE, lcolor='gray', ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="latexDotchart_+3A_data">data</code></td>
<td>
<p>a numeric vector whose values are shown on the x-axis</p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_labels">labels</code></td>
<td>
<p>a vector of labels for each point, corresponding to
<code>x</code>.  If omitted, <code>names(data)</code> are used, and if there are
no <code>names</code>, integers prefixed by <code>"#"</code> are used.</p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_groups">groups</code></td>
<td>
<p>an optional categorical variable indicating how
<code>data</code> values are grouped</p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_gdata">gdata</code></td>
<td>
<p>data values for groups, typically summaries such as group
medians</p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_xlab">xlab</code></td>
<td>
<p>x-axis title</p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_auxdata">auxdata</code></td>
<td>

<p>a vector of auxiliary data, of the same length
as the first (<code>data</code>) argument.  If present, this
vector of values will be printed outside the right margin of the dot
chart.  Usually <code>auxdata</code> represents cell sizes.
</p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_auxgdata">auxgdata</code></td>
<td>

<p>similar to <code>auxdata</code> but corresponding to the <code>gdata</code>
argument.  These usually represent overall sample sizes for each
group of lines.</p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_auxtitle">auxtitle</code></td>
<td>

<p>if <code>auxdata</code> is given, <code>auxtitle</code> specifies a column
heading for the extra printed data in the chart, e.g., <code>"N"</code></p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_w">w</code></td>
<td>
<p>width of picture in inches</p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_h">h</code></td>
<td>
<p>height of picture in inches</p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_margin">margin</code></td>
<td>
<p>a 4-vector representing, in inches, the margin to the
left of the x-axis, below the y-axis, to the right of the x-axis,
and above the y-axis.  By default these are computed making educated
cases about how to accommodate <code>auxdata</code> etc.</p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_lines">lines</code></td>
<td>
<p>set to <code>FALSE</code> to suppress drawing of reference
lines</p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_dotsize">dotsize</code></td>
<td>
<p>diameter of filled circles, in inches, for drawing dots</p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_size">size</code></td>
<td>
<p>size of text in picture.  This and the next two arguments
are LaTeX font commands without the opening backslash, e.g.,
<code>'normalsize'</code>, <code>'small'</code>, <code>'large'</code>, <code>smaller[2]</code>.</p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_size.labels">size.labels</code></td>
<td>
<p>size of labels</p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_size.group.labels">size.group.labels</code></td>
<td>
<p>size of labels corresponding to <code>groups</code></p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_ttlabels">ttlabels</code></td>
<td>
<p>set to <code>TRUE</code> to use typewriter monospaced font
for labels</p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_sort.">sort.</code></td>
<td>

<p>set to <code>FALSE</code> to keep <code>latexDotchart</code> from sorting the input
data, i.e., it will assume that the data are already properly
arranged.  This is especially useful when you are using <code>gdata</code>
and <code>groups</code> and you want to control the
order that groups appear on the chart (from top to bottom).</p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_xaxis">xaxis</code></td>
<td>
<p>set to <code>FALSE</code> to suppress drawing x-axis</p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_lcolor">lcolor</code></td>
<td>

<p>color for horizontal reference lines.  Default is <code>"gray"</code></p>
</td></tr>
<tr><td><code id="latexDotchart_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dotchart3">dotchart3</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
z &lt;- latexDotchart(c(.1,.2), c('a','bbAAb'), xlab='This Label',
                   auxdata=c(.1,.2), auxtitle='Zcriteria')
f &lt;- '/tmp/t.tex'
cat('\documentclass{article}\n\usepackage{epic,color}\n\begin{document}\n', file=f)
cat(z, sep='\n', file=f, append=TRUE)
cat('\end{document}\n', file=f, append=TRUE)

set.seed(135)
maj &lt;- factor(c(rep('North',13),rep('South',13)))
g &lt;- paste('Category',rep(letters[1:13],2))
n &lt;- sample(1:15000, 26, replace=TRUE)
y1 &lt;- runif(26)
y2 &lt;- pmax(0, y1 - runif(26, 0, .1))
z &lt;- latexDotchart(y1, g, groups=maj, auxdata=n, auxtitle='n', xlab='Y',
                   size.group.labels='large', ttlabels=TRUE)
f &lt;- '/tmp/t2.tex'
cat('\documentclass{article}\n\usepackage{epic,color}\n\begin{document}\n\framebox{', file=f)
cat(z, sep='\n', file=f, append=TRUE)
cat('}\end{document}\n', file=f, append=TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='latexTabular'>Convert a Data Frame or Matrix to a LaTeX Tabular</h2><span id='topic+latexTabular'></span>

<h3>Description</h3>

<p><code>latexTabular</code> creates a character vector representing a matrix or
data frame in a simple &lsquo;<span class="samp">&#8288;tabular&#8288;</span>&rsquo; environment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>latexTabular(x, headings=colnames(x),
             align =paste(rep('c',ncol(x)),collapse=''),
             halign=paste(rep('c',ncol(x)),collapse=''),
             helvetica=TRUE, translate=TRUE, hline=0, center=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="latexTabular_+3A_x">x</code></td>
<td>
<p>a matrix or data frame, or a vector that is automatically
converted to a matrix</p>
</td></tr>
<tr><td><code id="latexTabular_+3A_headings">headings</code></td>
<td>
<p>a vector of character strings specifying column
headings for &lsquo;<span class="samp">&#8288;latexTabular&#8288;</span>&rsquo;, defaulting to <code>x</code>'s
<code>colnames</code>.  To make multi-line headers use the newline character
inside elements of <code>headings</code>.</p>
</td></tr>
<tr><td><code id="latexTabular_+3A_align">align</code></td>
<td>
<p>a character strings specifying column
alignments for &lsquo;<span class="samp">&#8288;latexTabular&#8288;</span>&rsquo;, defaulting to
<code>paste(rep('c',ncol(x)),collapse='')</code> to center.  You may
specify <code>align='c|c'</code> and other LaTeX tabular formatting.</p>
</td></tr>
<tr><td><code id="latexTabular_+3A_halign">halign</code></td>
<td>
<p>a character strings specifying alignment for
column headings, defaulting to centered.</p>
</td></tr>
<tr><td><code id="latexTabular_+3A_helvetica">helvetica</code></td>
<td>
<p>set to <code>FALSE</code> to use default LaTeX font in
&lsquo;<span class="samp">&#8288;latexTabular&#8288;</span>&rsquo; instead of helvetica.</p>
</td></tr>
<tr><td><code id="latexTabular_+3A_translate">translate</code></td>
<td>
<p>set to <code>FALSE</code> if column headings and table
entries are already in
LaTeX format, otherwise <code>latexTabular</code> will run them through
<code>latexTranslate</code></p>
</td></tr>
<tr><td><code id="latexTabular_+3A_hline">hline</code></td>
<td>
<p>set to 1 to put <code>hline</code> after heading, 2 to also put
<code>hline</code>s before and after heading and at table end</p>
</td></tr>
<tr><td><code id="latexTabular_+3A_center">center</code></td>
<td>
<p>set to <code>TRUE</code> to enclose the tabular in a LaTeX
<code>center</code> environment</p>
</td></tr>
<tr><td><code id="latexTabular_+3A_...">...</code></td>
<td>
<p>if present, <code>x</code> is run through <code>format.df</code> with
those extra arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character string containing LaTeX markup</p>


<h3>Author(s)</h3>

<p>Frank E. Harrell, Jr.,<br />
Department of Biostatistics,<br />
Vanderbilt University,<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+latex.default">latex.default</a></code>, <code><a href="#topic+format.df">format.df</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(1:6, nrow=2, dimnames=list(c('a','b'),c('c','d','this that')))
latexTabular(x)   # a character string with LaTeX markup
</code></pre>

<hr>
<h2 id='latexTherm'>Create LaTeX Thermometers and Colored Needles</h2><span id='topic+latexTherm'></span><span id='topic+latexNeedle'></span><span id='topic+pngNeedle'></span>

<h3>Description</h3>

<p><code>latexTherm</code> creates a LaTeX picture environment for drawing a
series of thermometers 
whose heights depict the values of a variable <code>y</code> assumed to be
scaled from 0 to 1.  This is useful for showing fractions of sample
analyzed in any table or plot, intended for a legend.  For example, four
thermometers might be used to depict the fraction of enrolled patients
included in the current analysis, the fraction randomized, the fraction
of patients randomized to treatment A being analyzed, and the fraction
randomized to B being analyzed.  The picture is placed
inside a LaTeX macro definition for macro variable named <code>name</code>, to
be invoked by the user later in the LaTeX file using <code>name</code>
preceeded by a backslash.
</p>
<p>If <code>y</code> has an attribute <code>"table"</code>, it is assumed to contain a
character string with LaTeX code.  This code is used as a tooltip popup
for PDF using the LaTeX <code>ocgtools</code> package or using style
<code>tooltips</code>.  Typically the code will contain a <code>tabular</code>
environment.  The user must define a LaTeX macro <code>tooltipn</code> that
takes two arguments (original object and pop-up object) that does
the pop-up.
</p>
<p><code>latexNeedle</code> is similar to <code>latexTherm</code> except that vertical
needles are produced and each may have its own color.  A grayscale box
is placed around the needles and provides the 0-1 <code>y</code>-axis
reference.  Horizontal grayscale grid lines may be drawn.
</p>
<p><code>pngNeedle</code> is similar to <code>latexNeedle</code> but is for generating
small png graphics.  The full graphics file name is returned invisibly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>latexTherm(y, name, w = 0.075, h = 0.15, spacefactor = 1/2, extra = 0.07,
           file = "", append = TRUE)

latexNeedle(y, x=NULL, col='black', href=0.5, name, w=.05, h=.15,
            extra=0, file = "", append=TRUE)

pngNeedle(y, x=NULL, col='black', href=0.5, lwd=3.5, w=6, h=18,
          file=tempfile(fileext='.png'))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="latexTherm_+3A_y">y</code></td>
<td>
<p>a vector of 0-1 scaled values.  Boxes and their frames are
omitted for <code>NA</code> elements</p>
</td></tr>
<tr><td><code id="latexTherm_+3A_x">x</code></td>
<td>
<p>a vector corresponding to <code>y</code> giving x-coordinates.
Scaled accordingly, or defaults to equally-spaced values.</p>
</td></tr>
<tr><td><code id="latexTherm_+3A_name">name</code></td>
<td>
<p>name of LaTeX macro variable to be defined</p>
</td></tr>
<tr><td><code id="latexTherm_+3A_w">w</code></td>
<td>
<p>width of a single box (thermometer) in inches.  For
<code>latexNeedle</code> and <code>pngNeedle</code> is the spacing between
needles, the latter being in pixels.</p>
</td></tr>
<tr><td><code id="latexTherm_+3A_h">h</code></td>
<td>
<p>height of a single box in inches.  For <code>latexNeedle</code> and
<code>pngNeedle</code> is the height of the frame, the latter in pixels.</p>
</td></tr>
<tr><td><code id="latexTherm_+3A_spacefactor">spacefactor</code></td>
<td>
<p>fraction of <code>w</code> added for extra space between
boxes for <code>latexTherm</code></p>
</td></tr>
<tr><td><code id="latexTherm_+3A_extra">extra</code></td>
<td>
<p>extra space in inches to set aside to the right of and
above the series of boxes or frame</p>
</td></tr>
<tr><td><code id="latexTherm_+3A_file">file</code></td>
<td>
<p>name of file to which to write LaTeX code.  Default is the
console.  Also used as base file name for png graphic.  Default for
that is from <code>tempfile</code>.</p>
</td></tr>
<tr><td><code id="latexTherm_+3A_append">append</code></td>
<td>
<p>set to <code>FALSE</code> to write over <code>file</code></p>
</td></tr>
<tr><td><code id="latexTherm_+3A_col">col</code></td>
<td>
<p>a vector of colors corresponding to positions in <code>y</code>.
<code>col</code> is repeated if too short.</p>
</td></tr>
<tr><td><code id="latexTherm_+3A_href">href</code></td>
<td>
<p>values of <code>y</code> (0-1) for which horizontal grayscale
reference lines are drawn for <code>latexNeedle</code> and
<code>pngNeedle</code>.  Set to 
<code>NULL</code> to not draw any reference lines</p>
</td></tr>
<tr><td><code id="latexTherm_+3A_lwd">lwd</code></td>
<td>
<p>line width of needles for <code>pngNeedle</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# The following is in the Hmisc tests directory
# For a knitr example see latexTherm.Rnw in that directory
ct &lt;- function(...) cat(..., sep='')
ct('\documentclass{report}\begin{document}\n')
latexTherm(c(1, 1, 1, 1), name='lta')
latexTherm(c(.5, .7, .4, .2), name='ltb')
latexTherm(c(.5, NA, .75, 0), w=.3, h=1, name='ltc', extra=0)
latexTherm(c(.5, NA, .75, 0), w=.3, h=1, name='ltcc')
latexTherm(c(0, 0, 0, 0), name='ltd')
ct('This is a the first:\lta and the second:\ltb\\ and the third
without extra:\ltc END\\\nThird with extra:\ltcc END\\ 
\vspace{2in}\\ 
All data = zero, frame only:\ltd\\
\end{document}\n')
w &lt;- pngNeedle(c(.2, .5, .7))
cat(tobase64image(w))  # can insert this directly into an html file

## End(Not run)</code></pre>

<hr>
<h2 id='legendfunctions'>Legend Creation Functions</h2><span id='topic+legendfunctions'></span><span id='topic+Key'></span><span id='topic+sKey'></span><span id='topic+Key2'></span>

<h3>Description</h3>

<p>Wrapers to plot defined legend ploting functions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Key(...)
Key2(...)
sKey(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="legendfunctions_+3A_...">...</code></td>
<td>
<p>arguments to pass to wrapped functions</p>
</td></tr>
</table>

<hr>
<h2 id='list.tree'>
Pretty-print the Structure of a Data Object
</h2><span id='topic+list.tree'></span>

<h3>Description</h3>

<p>This is a function to pretty-print the structure of any data object
(usually a list).  It is similar to the R function <code>str</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list.tree(struct, depth=-1, numbers=FALSE, maxlen=22, maxcomp=12, 
          attr.print=TRUE, front="", fill=". ", name.of, size=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="list.tree_+3A_struct">struct</code></td>
<td>

<p>The object to be displayed
</p>
</td></tr>
<tr><td><code id="list.tree_+3A_depth">depth</code></td>
<td>

<p>Maximum depth of recursion (of lists within lists ...) to be printed; negative
value means no limit on depth.
</p>
</td></tr>
<tr><td><code id="list.tree_+3A_numbers">numbers</code></td>
<td>

<p>If TRUE, use numbers in leader  instead  of  dots  to
represent position in structure.
</p>
</td></tr>
<tr><td><code id="list.tree_+3A_maxlen">maxlen</code></td>
<td>

<p>Approximate maximum length (in characters) allowed on each line to give the
first few values of a vector.  maxlen=0 suppresses printing any values.
</p>
</td></tr>
<tr><td><code id="list.tree_+3A_maxcomp">maxcomp</code></td>
<td>

<p>Maximum number of components of any list that will be described.
</p>
</td></tr>
<tr><td><code id="list.tree_+3A_attr.print">attr.print</code></td>
<td>

<p>Logical flag, determining whether a description of attributes will be printed.
</p>
</td></tr>
<tr><td><code id="list.tree_+3A_front">front</code></td>
<td>

<p>Front material of a line, for internal use.
</p>
</td></tr>
<tr><td><code id="list.tree_+3A_fill">fill</code></td>
<td>

<p>Fill character used for each level of indentation.
</p>
</td></tr>
<tr><td><code id="list.tree_+3A_name.of">name.of</code></td>
<td>

<p>Name of object, for internal use (deparsed version  of  struct  by  default). 
</p>
</td></tr>
<tr><td><code id="list.tree_+3A_size">size</code></td>
<td>

<p>Logical flag, should the size of the object in bytes be printed?
</p>
<p>A description of the structure of struct will be printed in outline
form, with indentation
for each level of recursion, showing the internal storage mode, length,
class(es) if any, attributes, and first few elements of each data vector.
By default each level of list recursion is indicated by a &quot;.&quot; and 
attributes by &quot;A&quot;.
</p>
</td></tr></table>


<h3>Author(s)</h3>

<p>Alan Zaslavsky, <a href="mailto:zaslavsk@hcp.med.harvard.edu">zaslavsk@hcp.med.harvard.edu</a>
</p>


<h3>See Also</h3>

<p><code><a href="utils.html#topic+str">str</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- list(a=ordered(c(1:30,30:1)),b=c("Rick","John","Allan"),
          c=diag(300),e=cbind(p=1008:1019,q=4))
list.tree(X)
# In R you can say str(X)
</code></pre>

<hr>
<h2 id='makeNstr'> creates a string that is a repeat of a substring </h2><span id='topic+makeNstr'></span>

<h3>Description</h3>

<p>Takes a character and creates a string that is the character repeated <code>len</code> times.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeNstr(char, len)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makeNstr_+3A_char">char</code></td>
<td>
<p> character to be repeated </p>
</td></tr>
<tr><td><code id="makeNstr_+3A_len">len</code></td>
<td>
<p> number of times to repeat <code>char</code>. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A string that is <code>char</code> repeated <code>len</code> times.
</p>


<h3>Author(s)</h3>

<p> Charles Dupont </p>


<h3>See Also</h3>

 <p><code><a href="base.html#topic+paste">paste</a></code>, <code><a href="base.html#topic+rep">rep</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>makeNstr(" ", 5)


</code></pre>

<hr>
<h2 id='mApply'>Apply a Function to Rows of a Matrix or Vector</h2><span id='topic+mApply'></span>

<h3>Description</h3>

<p><code>mApply</code> is like <code>tapply</code> except that the first argument can
be a matrix or a vector, and the output is cleaned up if <code>simplify=TRUE</code>.
It uses code adapted from Tony Plate (<a href="mailto:tplate@blackmesacapital.com">tplate@blackmesacapital.com</a>) to
operate on grouped submatrices.
</p>
<p>As <code>mApply</code> can be much faster than using <code>by</code>, it is often
worth the trouble of converting a data frame to a numeric matrix for
processing by <code>mApply</code>.  <code>asNumericMatrix</code> will do this, and
<code>matrix2dataFrame</code> will convert a numeric matrix back into a data
frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mApply(X, INDEX, FUN, ..., simplify=TRUE, keepmatrix=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mApply_+3A_x">X</code></td>
<td>

<p>a vector or matrix capable of being operated on by the
function specified as the <code>FUN</code> argument
</p>
</td></tr>
<tr><td><code id="mApply_+3A_index">INDEX</code></td>
<td>

<p>list of factors, each of same number of rows as 'X' has.
</p>
</td></tr>
<tr><td><code id="mApply_+3A_fun">FUN</code></td>
<td>

<p>the function to be applied.  In the case of functions like
'+', '
</p>
</td></tr>
<tr><td><code id="mApply_+3A_...">...</code></td>
<td>

<p>optional arguments to 'FUN'.
</p>
</td></tr>
<tr><td><code id="mApply_+3A_simplify">simplify</code></td>
<td>

<p>set to 'FALSE' to suppress simplification of the result in to
an array, matrix, etc.
</p>
</td></tr>
<tr><td><code id="mApply_+3A_keepmatrix">keepmatrix</code></td>
<td>
<p>set to <code>TRUE</code> to keep result as a matrix even if
<code>simplify</code> is <code>TRUE</code>, in the case of only one stratum
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>mApply</code>, the returned value is a vector, matrix, or list.
If <code>FUN</code> returns more than one number, the result is an array if
<code>simplify=TRUE</code> and is a list otherwise.  If a matrix is returned,
its rows correspond to unique combinations of <code>INDEX</code>.  If
<code>INDEX</code> is a list with more than one vector, <code>FUN</code> returns
more than one number, and <code>simplify=FALSE</code>, the returned value is a
list that is an array with the first dimension corresponding to the last
vector in <code>INDEX</code>, the second dimension corresponding to the next
to last vector in <code>INDEX</code>, etc., and the elements of the list-array
correspond to the values computed by <code>FUN</code>.  In this situation the
returned value is a regular array if <code>simplify=TRUE</code>.   The order
of dimensions is as previously but the additional (last) dimension
corresponds to values computed by <code>FUN</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+asNumericMatrix">asNumericMatrix</a></code>, <code><a href="#topic+matrix2dataFrame">matrix2dataFrame</a></code>, <code><a href="base.html#topic+tapply">tapply</a></code>,
<code><a href="base.html#topic+sapply">sapply</a></code>, <code><a href="base.html#topic+lapply">lapply</a></code>, <code><a href="base.html#topic+mapply">mapply</a></code>, <code><a href="base.html#topic+by">by</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(datasets, TRUE)
a &lt;- mApply(iris[,-5], iris$Species, mean)
</code></pre>

<hr>
<h2 id='mChoice'>Methods for Storing and Analyzing Multiple Choice Variables</h2><span id='topic+mChoice'></span><span id='topic+format.mChoice'></span><span id='topic+print.mChoice'></span><span id='topic+summary.mChoice'></span><span id='topic+as.character.mChoice'></span><span id='topic+as.double.mChoice'></span><span id='topic+inmChoice'></span><span id='topic+inmChoicelike'></span><span id='topic+nmChoice'></span><span id='topic+match.mChoice'></span><span id='topic++5B.mChoice'></span><span id='topic+print.summary.mChoice'></span><span id='topic+is.mChoice'></span><span id='topic+Math.mChoice'></span><span id='topic+Ops.mChoice'></span><span id='topic+Summary.mChoice'></span>

<h3>Description</h3>

<p><code>mChoice</code> is a function that is useful for grouping 
variables that represent
individual choices on a multiple choice question.  These choices are
typically factor or character values but may be of any type.  Levels
of component factor variables need not be the same; all unique levels
(or unique character values) are collected over all of the multiple
variables.  Then a new character vector is formed with integer choice
numbers separated by semicolons.  Optimally, a database system would
have exported the semicolon-separated character strings with a
<code>levels</code> attribute containing strings defining value labels
corresponding to the integer choice numbers.  <code>mChoice</code> is a
function for creating a multiple-choice variable after the fact.
<code>mChoice</code> variables are explicitly handed by the <code>describe</code>
and <code>summary.formula</code> functions. <code>NA</code>s or blanks in input
variables are ignored. 
</p>
<p><code>format.mChoice</code> will convert the multiple choice representation
to text form by substituting <code>levels</code> for integer codes.
<code>as.double.mChoice</code> converts the <code>mChoice</code> object to a
binary numeric matrix, one column per used level (or all levels of
<code>drop=FALSE</code>.  This is called by
the user by invoking <code>as.numeric</code>.  There is a
<code>print</code> method and a <code>summary</code> method, and a <code>print</code>
method for the <code>summary.mChoice</code> object.  The <code>summary</code>
method computes frequencies of all two-way choice combinations, the
frequencies of the top 5 combinations, information about which other
choices are present when each given choice is present, and the
frequency distribution of the number of choices per observation.  This
<code>summary</code> output is used in the <code>describe</code> function.  The
<code>print</code> method returns an html character string if
<code>options(prType='html')</code> is in effect if <code>render=FALSE</code> or
renders the html otherwise.  This is used by <code>print.describe</code> and
is most effective when <code>short=TRUE</code> is specified to <code>summary</code>.
</p>
<p><code>in.mChoice</code> creates a logical vector the same length as <code>x</code>
whose elements are <code>TRUE</code> when the observation in <code>x</code>
contains at least one of the codes or value labels in the second
argument.
</p>
<p><code>match.mChoice</code> creates an integer vector of the indexes of all
elements in <code>table</code> which contain any of the speicified levels
</p>
<p><code>nmChoice</code> returns an integer vector of the number of choices
that were made
</p>
<p><code>is.mChoice</code> returns <code>TRUE</code> is the argument is a multiple
choice variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mChoice(..., label='',
        sort.levels=c('original','alphabetic'), 
        add.none=FALSE, drop=TRUE, ignoreNA=TRUE)

## S3 method for class 'mChoice'
format(x, minlength=NULL, sep=";", ...)

## S3 method for class 'mChoice'
as.double(x, drop=FALSE, ...)

## S3 method for class 'mChoice'
print(x, quote=FALSE, max.levels=NULL,
       width=getOption("width"), ...)

## S3 method for class 'mChoice'
as.character(x, ...)

## S3 method for class 'mChoice'
summary(object, ncombos=5, minlength=NULL,
  drop=TRUE, short=FALSE, ...)

## S3 method for class 'summary.mChoice'
print(x, prlabel=TRUE, render=TRUE, ...)

## S3 method for class 'mChoice'
x[..., drop=FALSE]

match.mChoice(x, table, nomatch=NA, incomparables=FALSE)

inmChoice(x, values, condition=c('any', 'all'))

inmChoicelike(x, values, condition=c('any', 'all'),
              ignore.case=FALSE, fixed=FALSE)

nmChoice(object)

is.mChoice(x)

## S3 method for class 'mChoice'
Summary(..., na.rm)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mChoice_+3A_na.rm">na.rm</code></td>
<td>

<p>Logical: remove <code>NA</code>'s from data
</p>
</td></tr>
<tr><td><code id="mChoice_+3A_table">table</code></td>
<td>

<p>a vector (mChoice) of values to be matched against.
</p>
</td></tr>
<tr><td><code id="mChoice_+3A_nomatch">nomatch</code></td>
<td>

<p>value to return if a value for <code>x</code> does not exist in
<code>table</code>.
</p>
</td></tr>
<tr><td><code id="mChoice_+3A_incomparables">incomparables</code></td>
<td>

<p>logical whether incomparable values should be compaired.
</p>
</td></tr>
<tr><td><code id="mChoice_+3A_...">...</code></td>
<td>

<p>a series of vectors
</p>
</td></tr>
<tr><td><code id="mChoice_+3A_label">label</code></td>
<td>

<p>a character string <code>label</code> attribute to attach to the matrix created
by <code>mChoice</code>
</p>
</td></tr>
<tr><td><code id="mChoice_+3A_sort.levels">sort.levels</code></td>
<td>

<p>set <code>sort.levels="alphabetic"</code> to sort the columns of the matrix
created by <code>mChoice</code> alphabetically by category rather than by the
original order of levels in component factor variables (if there were
any input variables that were factors)
</p>
</td></tr>
<tr><td><code id="mChoice_+3A_add.none">add.none</code></td>
<td>

<p>Set <code>add.none</code> to <code>TRUE</code> to make a new category
<code>'none'</code> if it doesn't already exist and if there is an
observations with no choices selected.
</p>
</td></tr>
<tr><td><code id="mChoice_+3A_drop">drop</code></td>
<td>

<p>set <code>drop=FALSE</code> to keep unused factor levels as columns of the matrix
produced by <code>mChoice</code>
</p>
</td></tr>
<tr><td><code id="mChoice_+3A_ignorena">ignoreNA</code></td>
<td>
<p>set to <code>FALSE</code> to keep any <code>NA</code>s present in
data as a real level.  Prior to Hmisc 4.7-2 <code>FALSE</code> was the
default.</p>
</td></tr>
<tr><td><code id="mChoice_+3A_x">x</code></td>
<td>

<p>an object of class <code>"mchoice"</code> such as that created by
<code>mChoice</code>.  For <code>is.mChoice</code> is any object.
</p>
</td></tr>
<tr><td><code id="mChoice_+3A_object">object</code></td>
<td>

<p>an object of class <code>"mchoice"</code> such as that created by
<code>mChoice</code>
</p>
</td></tr>
<tr><td><code id="mChoice_+3A_ncombos">ncombos</code></td>
<td>

<p>maximum number of combos.
</p>
</td></tr>
<tr><td><code id="mChoice_+3A_width">width</code></td>
<td>

<p>With of a line of text to be formated
</p>
</td></tr>
<tr><td><code id="mChoice_+3A_quote">quote</code></td>
<td>

<p>quote the output
</p>
</td></tr>
<tr><td><code id="mChoice_+3A_max.levels">max.levels</code></td>
<td>
<p>max levels to be displayed</p>
</td></tr>
<tr><td><code id="mChoice_+3A_minlength">minlength</code></td>
<td>

<p>By default no abbreviation of levels is done in
<code>format</code> and <code>summary</code>.  Specify a positive integer to use
abbreviation in those functions.  See <code><a href="base.html#topic+abbreviate">abbreviate</a></code>.
</p>
</td></tr>
<tr><td><code id="mChoice_+3A_short">short</code></td>
<td>
<p>set to <code>TRUE</code> to have <code>summary.mChoice</code> use
integer choice numbers in its tables, and to print the choice level
definitions at the top</p>
</td></tr>
<tr><td><code id="mChoice_+3A_sep">sep</code></td>
<td>
<p>character to use to separate levels when formatting</p>
</td></tr>
<tr><td><code id="mChoice_+3A_prlabel">prlabel</code></td>
<td>

<p>set to <code>FALSE</code> to keep
<code>print.summary.mChoice</code> from printing the variable label and
number of unique values.  Ignore for html output.
</p>
</td></tr>
<tr><td><code id="mChoice_+3A_render">render</code></td>
<td>
<p>applies of <code>options(prType='html')</code> is in
effect. Set to <code>FALSE</code> to return the html text instead of
rendering the html.</p>
</td></tr>
<tr><td><code id="mChoice_+3A_values">values</code></td>
<td>

<p>a scalar or vector.  If <code>values</code> is integer, it is
the choice codes, and if it is a character vector, it is assumed to
be value labels.  For <code>inmChoicelike</code> <code>values</code> must be
character strings which are pieces of choice labels.
</p>
</td></tr>
<tr><td><code id="mChoice_+3A_condition">condition</code></td>
<td>
<p>set to <code>'all'</code> for <code>inmChoice</code> to require
that all choices in <code>values</code> be present instead of the default of
any of them present.</p>
</td></tr>
<tr><td><code id="mChoice_+3A_ignore.case">ignore.case</code></td>
<td>
<p>set to <code>TRUE</code> to have <code>inmChoicelike</code>
ignore case in the data when matching on <code>values</code></p>
</td></tr>
<tr><td><code id="mChoice_+3A_fixed">fixed</code></td>
<td>
<p>see <code>grep</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>mChoice</code> returns a character vector of class <code>"mChoice"</code>
plus attributes <code>"levels"</code> and <code>"label"</code>.
<code>summary.mChoice</code> returns an object of class
<code>"summary.mChoice"</code>.  <code>inmChoice</code> and <code>inmChoicelike</code>
return a logical vector.
<code>format.mChoice</code> returns a character vector, and
<code>as.double.mChoice</code> returns a binary numeric matrix.
<code>nmChoice</code> returns an integer vector.
<code>print.summary.mChoice</code> returns an html character string if
<code>options(prType='html')</code> is in effect.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+label">label</a></code>, <code><a href="#topic+combplotp">combplotp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>options(digits=3)
set.seed(3)
n &lt;- 20
sex &lt;- factor(sample(c("m","f"), n, rep=TRUE))
age &lt;- rnorm(n, 50, 5)
treatment &lt;- factor(sample(c("Drug","Placebo"), n, rep=TRUE))


# Generate a 3-choice variable; each of 3 variables has 5 possible levels
symp &lt;- c('Headache','Stomach Ache','Hangnail',
          'Muscle Ache','Depressed')
symptom1 &lt;- sample(symp, n, TRUE)
symptom2 &lt;- sample(symp, n, TRUE)
symptom3 &lt;- sample(symp, n, TRUE)
cbind(symptom1, symptom2, symptom3)[1:5,]
Symptoms &lt;- mChoice(symptom1, symptom2, symptom3, label='Primary Symptoms')
Symptoms
print(Symptoms, long=TRUE)
format(Symptoms[1:5])
inmChoice(Symptoms,'Headache')
inmChoicelike(Symptoms, 'head', ignore.case=TRUE)
levels(Symptoms)
inmChoice(Symptoms, 3)
# Find all subjects with either of two symptoms
inmChoice(Symptoms, c('Headache','Hangnail'))
# Note: In this example, some subjects have the same symptom checked
# multiple times; in practice these redundant selections would be NAs
# mChoice will ignore these redundant selections
# Find all subjects with both symptoms
inmChoice(Symptoms, c('Headache', 'Hangnail'), condition='all')

meanage &lt;- N &lt;- numeric(5)
for(j in 1:5) {
 meanage[j] &lt;- mean(age[inmChoice(Symptoms,j)])
 N[j] &lt;- sum(inmChoice(Symptoms,j))
}
names(meanage) &lt;- names(N) &lt;- levels(Symptoms)
meanage
N

# Manually compute mean age for 2 symptoms
mean(age[symptom1=='Headache' | symptom2=='Headache' | symptom3=='Headache'])
mean(age[symptom1=='Hangnail' | symptom2=='Hangnail' | symptom3=='Hangnail'])

summary(Symptoms)

#Frequency table sex*treatment, sex*Symptoms
summary(sex ~ treatment + Symptoms, fun=table)
# Check:
ma &lt;- inmChoice(Symptoms, 'Muscle Ache')
table(sex[ma])

# could also do:
# summary(sex ~ treatment + mChoice(symptom1,symptom2,symptom3), fun=table)

#Compute mean age, separately by 3 variables
summary(age ~ sex + treatment + Symptoms)


summary(age ~ sex + treatment + Symptoms, method="cross")

f &lt;- summary(treatment ~ age + sex + Symptoms, method="reverse", test=TRUE)
f
# trio of numbers represent 25th, 50th, 75th percentile
print(f, long=TRUE)
</code></pre>

<hr>
<h2 id='mdb.get'>Read Tables in a Microsoft Access Database</h2><span id='topic+mdb.get'></span>

<h3>Description</h3>

<p>Assuming the <code>mdbtools</code> package has been installed on your
system and is in the system path, <code>mdb.get</code> imports
one or more tables in a Microsoft Access database.  Date-time
variables are converted to dates or <code>chron</code> package date-time
variables.  The <code>csv.get</code> function is used to import
automatically exported csv files.  If <code>tables</code> 
is unspecified all tables in the database are retrieved.  If more than
one table is imported, the result is a list of data frames.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdb.get(file, tables=NULL, lowernames=FALSE, allow=NULL,
        dateformat='%m/%d/%y', mdbexportArgs='-b strip', ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mdb.get_+3A_file">file</code></td>
<td>
<p>the file name containing the Access database</p>
</td></tr>
<tr><td><code id="mdb.get_+3A_tables">tables</code></td>
<td>
<p>character vector specifying the names of tables to
import.  Default is to import all tables.  Specify
<code>tables=TRUE</code> to return the list of available tables.</p>
</td></tr>
<tr><td><code id="mdb.get_+3A_lowernames">lowernames</code></td>
<td>
<p>set this to <code>TRUE</code> to change variable names to
lower case</p>
</td></tr>
<tr><td><code id="mdb.get_+3A_allow">allow</code></td>
<td>
<p>a vector of characters allowed by <span class="rlang"><b>R</b></span> that should not be
converted to periods in variable names.  By default, underscores in
variable names are converted to periods as with <span class="rlang"><b>R</b></span> before version
1.9.</p>
</td></tr>
<tr><td><code id="mdb.get_+3A_dateformat">dateformat</code></td>
<td>
<p>see <code><a href="#topic+cleanup.import">cleanup.import</a></code>.  Default is the
usual Access format used in the U.S.</p>
</td></tr>
<tr><td><code id="mdb.get_+3A_mdbexportargs">mdbexportArgs</code></td>
<td>
<p>command line arguments to issue to mdb-export.
Set to <code>''</code> to omit <code>'-b strip'</code>.</p>
</td></tr>
<tr><td><code id="mdb.get_+3A_...">...</code></td>
<td>
<p>arguments to pass to <code>csv.get</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses the <code>mdbtools</code> package executables <code>mdb-tables</code>,
<code>mdb-schema</code>, and <code>mdb-export</code> (with by default option
<code>-b strip</code> to drop any binary output).  In Debian/Ubuntu Linux run
<code>apt get install mdbtools</code>.
<code>cleanup.import</code> is invoked by <code>csv.get</code> to transform
variables and store them as efficiently as possible.
</p>


<h3>Value</h3>

<p>a new data frame or a list of data frames</p>


<h3>Author(s)</h3>

<p>Frank Harrell, Vanderbilt University</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+data.frame">data.frame</a></code>,
<code><a href="#topic+cleanup.import">cleanup.import</a></code>, <code><a href="#topic+csv.get">csv.get</a></code>,
<code><a href="base.html#topic+Date">Date</a></code>, <code><a href="chron.html#topic+chron">chron</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Read all tables in the Microsoft Access database Nwind.mdb
d &lt;- mdb.get('Nwind.mdb')
contents(d)
for(z in d) print(contents(z))
# Just print the names of tables in the database
mdb.get('Nwind.mdb', tables=TRUE)
# Import one table
Orders &lt;- mdb.get('Nwind.mdb', tables='Orders')

## End(Not run)
</code></pre>

<hr>
<h2 id='meltData'>meltData</h2><span id='topic+meltData'></span>

<h3>Description</h3>

<p>Melt a Dataset To Examine All Xs vs Y
</p>


<h3>Usage</h3>

<pre><code class='language-R'>meltData(
  formula,
  data,
  tall = c("right", "left"),
  vnames = c("labels", "names"),
  sepunits = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="meltData_+3A_formula">formula</code></td>
<td>
<p>a formula</p>
</td></tr>
<tr><td><code id="meltData_+3A_data">data</code></td>
<td>
<p>data frame or table</p>
</td></tr>
<tr><td><code id="meltData_+3A_tall">tall</code></td>
<td>
<p>see above</p>
</td></tr>
<tr><td><code id="meltData_+3A_vnames">vnames</code></td>
<td>
<p>set to <code>names</code> to always use variable names instead of labels for X</p>
</td></tr>
<tr><td><code id="meltData_+3A_sepunits">sepunits</code></td>
<td>
<p>set to <code>TRUE</code> to create a separate variable <code>Units</code> to hold units of measurement.  The variable is not created if no original variables have a non-blank <code>units</code> attribute.</p>
</td></tr>
<tr><td><code id="meltData_+3A_...">...</code></td>
<td>
<p>passed to <code>label()</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses a formula with one or more left hand side variables (Y) and one or more right hand side variables (X).  Uses <code><a href="data.table.html#topic+melt.data.table">data.table::melt()</a></code> to melt <code>data</code> so that each X is played against the same Y if <code>tall='right'</code> (the default) or each Y is played against the same X combination if <code>tall='left'</code>.  The resulting data table has variables Y with their original names (if <code>tall='right'</code>) or variables X with their original names (if <code>tall='left'</code>), <code>variable</code>, and <code>value</code>.  By default <code>variable</code> is taken as <code>label()</code>s of the <code>tall</code> variables.
</p>


<h3>Value</h3>

<p>data table
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><code><a href="#topic+label">label()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d &lt;- data.frame(y1=(1:10)/10, y2=(1:10)/100, x1=1:10, x2=101:110)
label(d$x1) &lt;- 'X1'
units(d$x1) &lt;- 'mmHg'
m=meltData(y1 + y2 ~ x1 + x2, data=d, units=TRUE) # consider also html=TRUE
print(m)
m=meltData(y1 + y2 ~ x1 + x2, data=d, tall='left')
print(m)
</code></pre>

<hr>
<h2 id='Merge'>Merge Multiple Data Frames or Data Tables</h2><span id='topic+Merge'></span>

<h3>Description</h3>

<p>Merges an arbitrarily large series of data frames or data tables containing common <code>id</code> variables.  Information about number of observations and number of unique <code>id</code>s in individual and final merged datasets is printed.  The first data frame/table has special meaning in that all of its observations are kept whether they match <code>id</code>s in other data frames or not.  For all other data frames, by default non-matching observations are dropped.  The first data frame is also the one against which counts of unique <code>id</code>s are compared.  Sometimes <code>merge</code> drops variable attributes such as <code>labels</code> and <code>units</code>.  These are restored by <code>Merge</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Merge(..., id = NULL, all = TRUE, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Merge_+3A_...">...</code></td>
<td>
<p>two or more dataframes or data tables</p>
</td></tr>
<tr><td><code id="Merge_+3A_id">id</code></td>
<td>
<p>a formula containing all the identification variables such that the combination of these variables uniquely identifies subjects or records of interest.  May be omitted for data tables; in that case the <code>key</code> function retrieves the id variables.</p>
</td></tr>
<tr><td><code id="Merge_+3A_all">all</code></td>
<td>
<p>set to <code>FALSE</code> to drop observations not found in second and later data frames (only applies if not using <code>data.table</code>)</p>
</td></tr>
<tr><td><code id="Merge_+3A_verbose">verbose</code></td>
<td>
<p>set to <code>FALSE</code> to not print information about observations</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
a &lt;- data.frame(sid=1:3, age=c(20,30,40))
b &lt;- data.frame(sid=c(1,2,2), bp=c(120,130,140))
d &lt;- data.frame(sid=c(1,3,4), wt=c(170,180,190))
all &lt;- Merge(a, b, d, id = ~ sid)
# First file should be the master file and must
# contain all ids that ever occur.  ids not in the master will
# not be merged from other datasets.
a &lt;- data.table(a); setkey(a, sid)
# data.table also does not allow duplicates without allow.cartesian=TRUE
b &lt;- data.table(sid=1:2, bp=c(120,130)); setkey(b, sid)
d &lt;- data.table(d); setkey(d, sid)
all &lt;- Merge(a, b, d)

## End(Not run)
</code></pre>

<hr>
<h2 id='mgp.axis'>Draw Axes With Side-Specific mgp Parameters</h2><span id='topic+mgp.axis'></span><span id='topic+mgp.axis.labels'></span>

<h3>Description</h3>

<p><code>mgp.axis</code> is a version of <code>axis</code> that uses the appropriate
side-specific <code>mgp</code> parameter (see <code><a href="graphics.html#topic+par">par</a></code>) to account
for different space requirements for axis labels vertical vs. horizontal
tick marks.  <code>mgp.axis</code> also fixes a bug in <code>axis(2,...)</code>
that causes it to assume <code>las=1</code>.
</p>
<p><code>mgp.axis.labels</code> is used so that different spacing between tick
marks and axis tick mark labels may be specified for x- and y-axes.  Use
<code>mgp.axis.labels('default')</code> to set defaults. Users can set values
manually using <code>mgp.axis.labels(x,y)</code> where <code>x</code> and <code>y</code>
are 2nd value of <code>par('mgp')</code> to use.  Use
<code>mgp.axis.labels(type=w)</code> to retrieve values, where <code>w='x'</code>,
<code>'y'</code>, <code>'x and y'</code>, <code>'xy'</code>, to get 3 <code>mgp</code> values
(first 3 types) or 2 <code>mgp.axis.labels</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mgp.axis(side, at = NULL, ...,
         mgp = mgp.axis.labels(type = if (side == 1 | side == 3) "x"
                               else "y"),
         axistitle = NULL, cex.axis=par('cex.axis'), cex.lab=par('cex.lab'))

mgp.axis.labels(value,type=c('xy','x','y','x and y'))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mgp.axis_+3A_side">side</code>, <code id="mgp.axis_+3A_at">at</code></td>
<td>
<p>see <code><a href="graphics.html#topic+par">par</a></code></p>
</td></tr>
<tr><td><code id="mgp.axis_+3A_...">...</code></td>
<td>
<p>arguments passed through to <code><a href="graphics.html#topic+axis">axis</a></code></p>
</td></tr>
<tr><td><code id="mgp.axis_+3A_mgp">mgp</code>, <code id="mgp.axis_+3A_cex.axis">cex.axis</code>, <code id="mgp.axis_+3A_cex.lab">cex.lab</code></td>
<td>
<p>see <code><a href="graphics.html#topic+par">par</a></code></p>
</td></tr>
<tr><td><code id="mgp.axis_+3A_axistitle">axistitle</code></td>
<td>
<p>if specified will cause <code>axistitle</code> to be drawn
on the appropriate axis as a title</p>
</td></tr>
<tr><td><code id="mgp.axis_+3A_value">value</code></td>
<td>
<p>vector of values to which to set system option
<code>mgp.axis.labels</code></p>
</td></tr>
<tr><td><code id="mgp.axis_+3A_type">type</code></td>
<td>
<p>see above</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>mgp.axis.labels</code> returns the value of <code>mgp</code> (only the
second element of <code>mgp</code> if <code>type="xy"</code> or a list with
elements <code>x</code> and <code>y</code> if <code>type="x or y"</code>, each list
element being a 3-vector) for the 
appropriate axis if <code>value</code> is not specified, otherwise it
returns nothing but the system option <code>mgp.axis.labels</code> is set.
</p>
<p><code>mgp.axis</code> returns nothing.
</p>


<h3>Side Effects</h3>

<p><code>mgp.axis.labels</code> stores the value in the
system option <code>mgp.axis.labels</code></p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+par">par</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
mgp.axis.labels(type='x')  # get default value for x-axis
mgp.axis.labels(type='y')  # get value for y-axis
mgp.axis.labels(type='xy') # get 2nd element of both mgps
mgp.axis.labels(type='x and y')  # get a list with 2 elements
mgp.axis.labels(c(3,.5,0), type='x')  # set
options('mgp.axis.labels')            # retrieve

plot(..., axes=FALSE)
mgp.axis(1, "X Label")
mgp.axis(2, "Y Label")


## End(Not run)</code></pre>

<hr>
<h2 id='mhgr'>Miscellaneous Functions for Epidemiology</h2><span id='topic+mhgr'></span><span id='topic+print.mhgr'></span><span id='topic+lrcum'></span><span id='topic+print.lrcum'></span>

<h3>Description</h3>

<p>The <code>mhgr</code> function computes the Cochran-Mantel-Haenszel stratified
risk ratio and its confidence limits using the Greenland-Robins variance
estimator.
</p>
<p>The <code>lrcum</code> function takes the results of a series of 2x2 tables
representing the relationship between test positivity and diagnosis and
computes positive and negative likelihood ratios (with all their
deficiencies) and the variance of
their logarithms.  Cumulative likelihood ratios and their confidence
intervals (assuming independence of tests) are computed, assuming a
string of all positive tests or a string of all negative tests.  The
method of Simel et al as described in Altman et al is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mhgr(y, group, strata, conf.int = 0.95)
## S3 method for class 'mhgr'
print(x, ...)

lrcum(a, b, c, d, conf.int = 0.95)
## S3 method for class 'lrcum'
print(x, dec=3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mhgr_+3A_y">y</code></td>
<td>
<p>a binary response variable</p>
</td></tr>
<tr><td><code id="mhgr_+3A_group">group</code></td>
<td>
<p>a variable with two unique values specifying comparison groups</p>
</td></tr>
<tr><td><code id="mhgr_+3A_strata">strata</code></td>
<td>
<p>the stratification variable</p>
</td></tr>
<tr><td><code id="mhgr_+3A_conf.int">conf.int</code></td>
<td>
<p>confidence level</p>
</td></tr>
<tr><td><code id="mhgr_+3A_x">x</code></td>
<td>
<p>an object created by <code>mhgr</code> or <code>lrcum</code></p>
</td></tr>
<tr><td><code id="mhgr_+3A_a">a</code></td>
<td>
<p>frequency of true positive tests</p>
</td></tr>
<tr><td><code id="mhgr_+3A_b">b</code></td>
<td>
<p>frequency of false positive tests</p>
</td></tr>
<tr><td><code id="mhgr_+3A_c">c</code></td>
<td>
<p>frequency of false negative tests</p>
</td></tr>
<tr><td><code id="mhgr_+3A_d">d</code></td>
<td>
<p>frequency of true negative tests</p>
</td></tr>
<tr><td><code id="mhgr_+3A_dec">dec</code></td>
<td>
<p>number of places to the right of the decimal to print for
<code>lrcum</code></p>
</td></tr>
<tr><td><code id="mhgr_+3A_...">...</code></td>
<td>
<p>addtitional arguments to be passed to other print functions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses equations 4 and 13 from Greenland and Robins.
</p>


<h3>Value</h3>

<p>a list of class <code>"mhgr"</code> or of class <code>"lrcum"</code>.
</p>


<h3>Author(s)</h3>

<p>Frank E Harrell Jr <a href="mailto:fh@fharrell.com">fh@fharrell.com</a></p>


<h3>References</h3>

<p>Greenland S, Robins JM (1985): Estimation of a common effect parameter
from sparse follow-up data.  Biometrics 41:55-68.
</p>
<p>Altman DG, Machin D, Bryant TN, Gardner MJ, Eds. (2000): Statistics with
Confidence, 2nd Ed.  Bristol: BMJ Books, 105-110.
</p>
<p>Simel DL, Samsa GP, Matchar DB (1991): Likelihood ratios with
confidence: sample size estimation for diagnostic test studies.  J
Clin Epi 44:763-770.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+logrank">logrank</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># Greate Migraine dataset used in Example 28.6 in the SAS PROC FREQ guide
d &lt;- expand.grid(response=c('Better','Same'),
                 treatment=c('Active','Placebo'),
                 sex=c('female','male'))
d$count &lt;- c(16, 11, 5, 20, 12, 16, 7, 19)
d
# Expand data frame to represent raw data
r &lt;- rep(1:8, d$count)
d &lt;- d[r,]
with(d, mhgr(response=='Better', treatment, sex))

# Discrete survival time example, to get Cox-Mantel relative risk and CL
# From Stokes ME, Davis CS, Koch GG, Categorical Data Analysis Using the
# SAS System, 2nd Edition, Sectino 17.3, p. 596-599
#
# Input data in Table 17.5
d &lt;- expand.grid(treatment=c('A','P'), center=1:3)
d$healed2w    &lt;- c(15,15,17,12, 7, 3)
d$healed4w    &lt;- c(17,17,17,13,17,17)
d$notHealed4w &lt;- c( 2, 7,10,15,16,18)
d
# Reformat to the way most people would collect raw data
d1 &lt;- d[rep(1:6, d$healed2w),]
d1$time &lt;- '2'
d1$y &lt;- 1
d2 &lt;- d[rep(1:6, d$healed4w),]
d2$time &lt;- '4'
d2$y &lt;- 1
d3 &lt;- d[rep(1:6, d$notHealed4w),]
d3$time &lt;- '4'
d3$y &lt;- 0
d &lt;- rbind(d1, d2, d3)
d$healed2w &lt;- d$healed4w &lt;- d$notHealed4w &lt;- NULL
d
# Finally, duplicate appropriate observations to create 2 and 4-week
# risk sets.  Healed and not healed at 4w need to be in the 2-week
# risk set as not healed
d2w      &lt;- subset(d, time=='4')
d2w$time &lt;- '2'
d2w$y    &lt;- 0
d24      &lt;- rbind(d, d2w)
with(d24, table(y, treatment, time, center))
# Matches Table 17.6

with(d24, mhgr(y, treatment, interaction(center, time, sep=';')))

# Get cumulative likelihood ratios and their 0.95 confidence intervals
# based on the following two tables
#
#          Disease       Disease
#          +     -       +     -
# Test +   39    3       20    5
# Test -   21   17       22   15

lrcum(c(39,20), c(3,5), c(21,22), c(17,15))
</code></pre>

<hr>
<h2 id='minor.tick'>Minor Tick Marks</h2><span id='topic+minor.tick'></span>

<h3>Description</h3>

<p>Adds minor tick marks to an existing plot.  All minor tick marks that
will fit on the axes will be drawn.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>minor.tick(nx=2, ny=2, tick.ratio=0.5, x.args = list(), y.args = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="minor.tick_+3A_nx">nx</code></td>
<td>

<p>number of intervals in which to divide the area between major tick marks on
the X-axis.  Set to 1 to suppress minor tick marks.
</p>
</td></tr>
<tr><td><code id="minor.tick_+3A_ny">ny</code></td>
<td>

<p>same as <code>nx</code> but for the Y-axis.
</p>
</td></tr>
<tr><td><code id="minor.tick_+3A_tick.ratio">tick.ratio</code></td>
<td>

<p>ratio of lengths of minor tick marks to major tick marks.  The length
of major tick marks is retrieved from <code>par("tck")</code>.</p>
</td></tr>
<tr><td><code id="minor.tick_+3A_x.args">x.args</code></td>
<td>

<p>additionl arguments (e.g. <code>post</code>, <code>lwd</code>) used by <code>axis()</code> function when rendering the X-axis.</p>
</td></tr>
<tr><td><code id="minor.tick_+3A_y.args">y.args</code></td>
<td>

<p>same as <code>x.args</code> but for Y-axis.</p>
</td></tr>
</table>


<h3>Side Effects</h3>

<p>plots
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
<br />
Earl Bellinger
<br />
Max Planck Institute
<br />
<a href="mailto:earlbellinger@gmail.com">earlbellinger@gmail.com</a>
<br />
Viktor Horvath
<br />
Brandeis University
<br />
<a href="mailto:vhorvath@brandeis.edu">vhorvath@brandeis.edu</a>
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+axis">axis</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Plot with default settings
plot(runif(20), runif(20))
minor.tick()

# Plot with arguments passed to axis()
plot(c(0,1), c(0,1), type = 'n', axes = FALSE, ann = FALSE)
# setting up a plot without axes and annotation
points(runif(20), runif(20))                       # plotting data
axis(1, pos = 0.5, lwd = 2)                        # showing X-axis at Y = 0.5 with formatting
axis(2, col = 2)                                   # formatted Y-axis
minor.tick( nx = 4, ny = 4, tick.ratio = 0.3,
            x.args = list(pos = 0.5, lwd = 2),     # X-minor tick format argumnets
            y.args = list(col = 2))                # Y-minor tick format arguments
</code></pre>

<hr>
<h2 id='Misc'>Miscellaneous Functions</h2><span id='topic+clowess'></span><span id='topic+confbar'></span><span id='topic+getLatestSource'></span><span id='topic+grType'></span><span id='topic+prType'></span><span id='topic+htmlSpecialType'></span><span id='topic+inverseFunction'></span><span id='topic+james.stein'></span><span id='topic+keepHattrib'></span><span id='topic+km.quick'></span><span id='topic+latexBuild'></span><span id='topic+lm.fit.qr.bare'></span><span id='topic+matxv'></span><span id='topic+makeSteps'></span><span id='topic+nomiss'></span><span id='topic+outerText'></span><span id='topic+plotlyParm'></span><span id='topic+plotp'></span><span id='topic+rendHTML'></span><span id='topic+restoreHattrib'></span><span id='topic+sepUnitsTrans'></span><span id='topic+strgraphwrap'></span><span id='topic+tobase64image'></span><span id='topic+trap.rule'></span><span id='topic+trellis.strip.blank'></span><span id='topic+unPaste'></span><span id='topic+whichClosest'></span><span id='topic+whichClosePW'></span><span id='topic+whichClosek'></span><span id='topic+xless'></span>

<h3>Description</h3>

<p>This documents miscellaneous small functions in Hmisc that may be of
interest to users.
</p>
<p><code>clowess</code> runs <code>lowess</code> but if the <code>iter</code> argument
exceeds zero, sometimes wild values can result, in which case
<code>lowess</code> is re-run with <code>iter=0</code>.
</p>
<p><code>confbar</code> draws multi-level confidence bars using small rectangles
that may be of different colors.
</p>
<p><code>getLatestSource</code> fetches and <code>source</code>s the most recent
source code for functions in GitHub.
</p>
<p><code>grType</code> retrieves the system option <code>grType</code>, which is
forced to be <code>"base"</code> if the <code>plotly</code> package is not
installed.
</p>
<p><code>prType</code> retrieves the system option <code>prType</code>, which is
set to <code>"plain"</code> if the option is not set.  <code>print</code> methods
that allow for markdown/html/latex can be automatically invoked by
setting <code>options(prType="html")</code> or
<code>options(prType='latex')</code>.
</p>
<p><code>htmlSpecialType</code> retrieves the system option
<code>htmlSpecialType</code>, which is set to <code>"unicode"</code> if the option
is not set.  <code>htmlSpecialType='unicode'</code> cause html-generating
functions in <code>Hmisc</code> and <code>rms</code> to use unicode for special
characters, and <code>htmlSpecialType='&amp;'</code> uses the older ampersand
3-digit format. 
</p>
<p><code>inverseFunction</code> generates a function to find all inverses of a
monotonic or nonmonotonic function that is tabulated at vectors (x,y),
typically 1000 points.  If the original function is monotonic, simple linear
interpolation is used and the result is a vector, otherwise linear
interpolation is used within each interval in which the function is
monotonic and the result is a matrix with number of columns equal to the
number of monotonic intervals.  If a requested y is not within any
interval, the extreme x that pertains to the nearest extreme y is
returned. Specifying what='sample' to the returned function will cause a
vector to be returned instead of a matrix, with elements taken as a
random choice of the possible inverses.
</p>
<p><code>james.stein</code> computes James-Stein shrunken estimates of cell
means given a response variable (which may be binary) and a grouping
indicator.
</p>
<p><code>keepHattrib</code> for an input variable or a data frame, creates a
list object saving special Hmisc attributes such as <code>label</code> and
<code>units</code> that might be lost during certain operations such as
running <code>data.table</code>.  <code>restoreHattrib</code> restores these attributes.
</p>
<p><code>km.quick</code> provides a fast way to invoke <code>survfitKM</code> in the
<code>survival</code> package to get Kaplan-Meier estimates for a
single stratum for a vector of time points (if <code>times</code> is given) or to
get a vector of survival time quantiles (if <code>q</code> is given).
</p>
<p><code>latexBuild</code> takes pairs of character strings and produces a
single character string containing concatenation of all of them, plus
an attribute <code>"close"</code> which is a character string containing the
LaTeX closure that will balance LaTeX code with respect to
parentheses, braces, brackets, or <code>begin</code> vs. <code>end</code>.  When
an even-numbered element of the vector is not a left parenthesis,
brace, or bracket, the element is taken as a word that was surrounded
by <code>begin</code> and braces, for which the corresponding <code>end</code> is
constructed in the returned attribute.
</p>
<p><code>lm.fit.qr.bare</code> is a fast stripped-down function for computing
regression coefficients, residuals, <code class="reqn">R^2</code>, and fitted values.  It
uses <code>lm.fit</code>. 
</p>
<p><code>matxv</code> multiplies a matrix by a vector, handling automatic
addition of intercepts if the matrix does not have a column of ones.
If the first argument is not a matrix, it will be converted to one.
An optional argument allows the second argument to be treated as a
matrix, useful when its rows represent bootstrap reps of
coefficients.  Then ab' is computed.  <code>matxv</code> respects the
<code>"intercepts"</code> attribute if it is stored on <code>b</code> by the
<code>rms</code> package.  This is used by <code><a href="rms.html#topic+orm">orm</a></code>
fits that are bootstrap-repeated by <code><a href="rms.html#topic+bootcov">bootcov</a></code> where
only the intercept corresponding to the median is retained.  If
<code>kint</code> has nonzero length, it is checked for consistency with the
attribute.
</p>
<p><code>makeSteps</code> is a copy of the dostep function inside the
<code>survival</code> package's <code>plot.survfit</code> function.  It expands a
series of points to include all the segments needed to plot step
functions.  This is useful for drawing polygons to shade confidence
bands for step functions.
</p>
<p><code>nomiss</code> returns a data frame (if its argument is one) with rows
corresponding to <code>NA</code>s removed, or it returns a matrix with rows
with any element missing removed.
</p>
<p><code>outerText</code> uses <code>axis()</code> to put right-justified text
strings in the right margin.  Placement depends on
<code>par('mar')[4]</code>
</p>
<p><code>plotlyParm</code> is a list of functions useful for specifying
parameters to <code>plotly</code> graphics.
</p>
<p><code>plotp</code> is a generic to handle <code>plotp</code> methods to make
<code>plotly</code> graphics.
</p>
<p><code>rendHTML</code> renders HTML in a character vector, first converting
to one character string with newline delimeters.  If <code>knitr</code> is
currently running, runs this string through <code>knitr::asis_output</code>
so that the user need not include <code>results='asis'</code> in the chunk
header for R Markdown or Quarto.  If <code>knitr</code> is not running, uses
<code>htmltools::browsable</code> and <code>htmltools::HTML</code> and prints the
result so that an RStudio viewer (if running inside RStudio) or
separate browser window displays the rendered HTML.  The HTML code is
surrounded by yaml markup to make Pandoc not fiddle with the HTML.
Set the argument <code>html=FALSE</code> to not add this, in case you are
really rendering markdown.  <code>html=FALSE</code> also invokes
<code>rmarkdown::render</code> to convert the character vector to HTML
before using <code>htmltools</code> to view, assuming the characters
represent RMarkdown/Quarto text other than the YAML header.  If
<code>options(rawmarkup=TRUE)</code> is in effect, <code>rendHTML</code> will just
<code>cat()</code> its first argument.  This is useful when rendering is
happening inside a Quarto margin, for example.
</p>
<p><code>sepUnitsTrans</code> converts character vectors containing values such
as <code>c("3 days","3day","4month","2 years","2weeks","7")</code> to
numeric vectors 
(here <code>c(3,3,122,730,14,7)</code>) in a flexible fashion.  The user can
specify a 
vector of units of measurements and conversion factors.  The units
with a conversion factor of <code>1</code> are taken as the target units,
and if those units are present in the character strings they are
ignored.  The target units are added to the resulting vector as the
<code>"units"</code> attribute.
</p>
<p><code>strgraphwrap</code> is like <code>strwrap</code> but is for the current
graphics environment.
</p>
<p><code>tobase64image</code> is a function written by Dirk Eddelbuettel that
uses the <code>base64enc</code> package to convert a png graphic file to
base64 encoding to include as an inline image in an html file.
</p>
<p><code>trap.rule</code> computes the area under a curve using the trapezoidal
rule, assuming <code>x</code> is sorted.
</p>
<p><code>trellis.strip.blank</code> sets up Trellis or Lattice graphs to have a
clear background on the strips for panel labels.
</p>
<p><code>unPaste</code> provides a version of the S-Plus <code>unpaste</code> that
works for <span class="rlang"><b>R</b></span> and S-Plus.
</p>
<p><code>whichClosePW</code> is a very fast function using weighted multinomial
sampling to determine which element of a vector is &quot;closest&quot; to each
element of another vector.  <code>whichClosest</code> quickly finds the closest
element without any randomness.
</p>
<p><code>whichClosek</code> is a slow function that finds, after jittering the
lookup table, the <code>k</code> closest matchest to each element of the
other vector, and chooses from among these one at random.
</p>
<p><code>xless</code> is a function for Linux/Unix users to invoke the system
<code>xless</code> command to pop up a window to display the result of
<code>print</code>ing an object.  For MacOS <code>xless</code> uses the system <code>open</code> command to pop up a <code>TextEdit</code> window.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confbar(at, est, se, width, q = c(0.7, 0.8, 0.9, 0.95, 0.99), 
        col = gray(c(0, 0.25, 0.5, 0.75, 1)),
        type = c("v", "h"), labels = TRUE, ticks = FALSE,
        cex = 0.5, side = "l", lwd = 5, clip = c(-1e+30, 1e+30),
        fun = function(x) x,
        qfun = function(x) ifelse(x == 0.5, qnorm(x),
                            ifelse(x &lt; 0.5, qnorm(x/2),
                            qnorm((1 +  x)/2))))
getLatestSource(x=NULL, package='Hmisc', recent=NULL, avail=FALSE)
grType()
prType()
htmlSpecialType()
inverseFunction(x, y)
james.stein(y, group)
keepHattrib(obj)
km.quick(S, times, q)
latexBuild(..., insert, sep='')
lm.fit.qr.bare(x, y, tolerance, intercept=TRUE, xpxi=FALSE, singzero=FALSE)
matxv(a, b, kint=1, bmat=FALSE)
nomiss(x)
outerText(string, y, cex=par('cex'), ...)
plotlyParm
plotp(data, ...)
rendHTML(x, html=TRUE)
restoreHattrib(obj, attribs)
sepUnitsTrans(x, conversion=c(day=1, month=365.25/12, year=365.25, week=7),
              round=FALSE, digits=0)
strgraphwrap(x, width = 0.9 * getOption("width"),
             indent = 0, exdent = 0,
             prefix = "", simplify = TRUE, units='user', cex=NULL)
tobase64image(file, Rd = FALSE, alt = "image")
trap.rule(x, y)
trellis.strip.blank()
unPaste(str, sep="/")
whichClosest(x, w)
whichClosePW(x, w, f=0.2)
whichClosek(x, w, k)
xless(x, ..., title)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Misc_+3A_a">a</code></td>
<td>
<p>a numeric matrix or vector</p>
</td></tr>
<tr><td><code id="Misc_+3A_alt">alt</code>, <code id="Misc_+3A_rd">Rd</code></td>
<td>
<p>see <code>base64::img</code></p>
</td></tr>
<tr><td><code id="Misc_+3A_at">at</code></td>
<td>
<p>x-coordinate for vertical confidence intervals, y-coordinate
for horizontal</p>
</td></tr>
<tr><td><code id="Misc_+3A_attribs">attribs</code></td>
<td>
<p>an object returned by <code>keepHattrib</code></p>
</td></tr>
<tr><td><code id="Misc_+3A_avail">avail</code></td>
<td>
<p>set to <code>TRUE</code> to have <code>getLatestSource</code> return
a data frame of available files and latest versions instead of
fetching any</p>
</td></tr>
<tr><td><code id="Misc_+3A_b">b</code></td>
<td>
<p>a numeric vector</p>
</td></tr>
<tr><td><code id="Misc_+3A_cex">cex</code></td>
<td>
<p>character expansion factor</p>
</td></tr>
<tr><td><code id="Misc_+3A_clip">clip</code></td>
<td>
<p>interval to truncate limits</p>
</td></tr>
<tr><td><code id="Misc_+3A_col">col</code></td>
<td>
<p>vector of colors</p>
</td></tr>
<tr><td><code id="Misc_+3A_conversion">conversion</code></td>
<td>
<p>a named numeric vector</p>
</td></tr>
<tr><td><code id="Misc_+3A_data">data</code></td>
<td>
<p>an object having a <code>plotp</code> method</p>
</td></tr>
<tr><td><code id="Misc_+3A_digits">digits</code></td>
<td>
<p>number of digits used for <code>round</code></p>
</td></tr>
<tr><td><code id="Misc_+3A_est">est</code></td>
<td>
<p>vector of point estimates for confidence limits</p>
</td></tr>
<tr><td><code id="Misc_+3A_f">f</code></td>
<td>
<p>a scaling constant</p>
</td></tr>
<tr><td><code id="Misc_+3A_file">file</code></td>
<td>
<p>a file name</p>
</td></tr>
<tr><td><code id="Misc_+3A_fun">fun</code></td>
<td>
<p>function to transform scale</p>
</td></tr>
<tr><td><code id="Misc_+3A_group">group</code></td>
<td>
<p>a categorical grouping variable</p>
</td></tr>
<tr><td><code id="Misc_+3A_html">html</code></td>
<td>
<p>set to <code>FALSE</code> to tell <code>rendHTML</code> to not
surround HTML code with yaml</p>
</td></tr>
<tr><td><code id="Misc_+3A_insert">insert</code></td>
<td>
<p>a list of 3-element lists for <code>latexBuild</code>.
The first of each 3-element
list is a character string with an environment name.  The second
specifies the order: <code>"before"</code> or <code>"after"</code>, the former
indicating that when the environment is found, the third element of
the list is inserted before or after it, according to the second
element.</p>
</td></tr>
<tr><td><code id="Misc_+3A_intercept">intercept</code></td>
<td>
<p>set to <code>FALSE</code> to not automatically add a column
of ones to the <code>x</code> matrix</p>
</td></tr>
<tr><td><code id="Misc_+3A_k">k</code></td>
<td>
<p>get the <code>k</code> closest matches</p>
</td></tr>
<tr><td><code id="Misc_+3A_kint">kint</code></td>
<td>
<p>which element of <code>b</code> to add to the result if <code>a</code>
does not contain a column for intercepts</p>
</td></tr>
<tr><td><code id="Misc_+3A_bmat">bmat</code></td>
<td>
<p>set to <code>TRUE</code> to consider <code>b</code> a matrix of
repeated coefficients, usually resampled estimates with rows
corresponding to resamples</p>
</td></tr>
<tr><td><code id="Misc_+3A_labels">labels</code></td>
<td>
<p>set to <code>FALSE</code> to omit drawing confidence
coefficients</p>
</td></tr>
<tr><td><code id="Misc_+3A_lwd">lwd</code></td>
<td>
<p>line widths</p>
</td></tr>
<tr><td><code id="Misc_+3A_package">package</code></td>
<td>
<p>name of package for <code>getLatestSource</code>, default is
<code>'Hmisc'</code></p>
</td></tr>
<tr><td><code id="Misc_+3A_obj">obj</code></td>
<td>
<p>a variable, data frame, or data table</p>
</td></tr>
<tr><td><code id="Misc_+3A_q">q</code></td>
<td>
<p>vector of confidence coefficients or quantiles</p>
</td></tr>
<tr><td><code id="Misc_+3A_qfun">qfun</code></td>
<td>
<p>quantiles on transformed scale</p>
</td></tr>
<tr><td><code id="Misc_+3A_recent">recent</code></td>
<td>
<p>an integer telling <code>getLatestSource</code> to get the
<code>recent</code> most recently modified files from the package</p>
</td></tr>
<tr><td><code id="Misc_+3A_round">round</code></td>
<td>
<p>set to <code>TRUE</code> to round converted values</p>
</td></tr>
<tr><td><code id="Misc_+3A_s">S</code></td>
<td>
<p>a <code><a href="survival.html#topic+Surv">Surv</a></code> object</p>
</td></tr>
<tr><td><code id="Misc_+3A_se">se</code></td>
<td>
<p>vector of standard errors</p>
</td></tr>
<tr><td><code id="Misc_+3A_sep">sep</code></td>
<td>
<p>a single character string specifying the delimiter.  For
<code>latexBuild</code> the default is <code>""</code>.</p>
</td></tr>
<tr><td><code id="Misc_+3A_side">side</code></td>
<td>
<p>for <code>confbar</code> is <code>"b","l","t","r"</code> for bottom,
left, top, right.</p>
</td></tr>
<tr><td><code id="Misc_+3A_str">str</code></td>
<td>
<p>a character string vector</p>
</td></tr>
<tr><td><code id="Misc_+3A_string">string</code></td>
<td>
<p>a character string vector</p>
</td></tr>
<tr><td><code id="Misc_+3A_ticks">ticks</code></td>
<td>
<p>set to <code>TRUE</code> to draw lines between rectangles</p>
</td></tr>
<tr><td><code id="Misc_+3A_times">times</code></td>
<td>
<p>a numeric vector of times</p>
</td></tr>
<tr><td><code id="Misc_+3A_title">title</code></td>
<td>
<p>a character string to title a window or plot.  Ignored for <code>xless</code> under MacOs.</p>
</td></tr>
<tr><td><code id="Misc_+3A_tolerance">tolerance</code></td>
<td>
<p>tolerance for judging singularity in matrix</p>
</td></tr>
<tr><td><code id="Misc_+3A_type">type</code></td>
<td>
<p><code>"v"</code> for vertical, <code>"h"</code> for horizontal.</p>
</td></tr>
<tr><td><code id="Misc_+3A_w">w</code></td>
<td>
<p>a numeric vector</p>
</td></tr>
<tr><td><code id="Misc_+3A_width">width</code></td>
<td>
<p>width of confidence rectanges in user units, or see
<code><a href="base.html#topic+strwrap">strwrap</a></code></p>
</td></tr> 
<tr><td><code id="Misc_+3A_x">x</code></td>
<td>
<p>a numeric vector (matrix for <code>lm.fit.qr.bare</code>) or data
frame.  For <code>xless</code> may be any object that is sensible to
<code>print</code>.  For <code>sepUnitsTrans</code> is a character or factor
variable.  For <code>getLatestSource</code> is a character string or
vector of character strings containing base file names to retrieve
from CVS.  Set <code>x='all'</code> to retrieve all source files.  For
<code>clowess</code>, <code>x</code> may also be a list with x and y
components.  For <code>inverseFunction</code>, <code>x</code> and <code>y</code>
contain evaluations of the function whose inverse is needed.
<code>x</code> is typically an equally-spaced grid of 1000 points.  For
<code>strgraphwrap</code> is a character vector.  For <code>rendHTML</code>
<code>x</code> is a character vector.</p>
</td></tr>
<tr><td><code id="Misc_+3A_xpxi">xpxi</code></td>
<td>
<p>set to <code>TRUE</code> to add an element to the result
containing the inverse of <code class="reqn">X'X</code></p>
</td></tr>
<tr><td><code id="Misc_+3A_singzero">singzero</code></td>
<td>
<p>set to <code>TRUE</code> to set coefficients corresponding
to singular variables to zero instead of <code>NA</code>.</p>
</td></tr>
<tr><td><code id="Misc_+3A_y">y</code></td>
<td>
<p>a numeric vector.  For <code>inverseFunction</code> <code>y</code> is the
evaluated function values at <code>x</code>.</p>
</td></tr>
<tr><td><code id="Misc_+3A_indent">indent</code>, <code id="Misc_+3A_exdent">exdent</code>, <code id="Misc_+3A_prefix">prefix</code></td>
<td>
<p>see <code><a href="base.html#topic+strwrap">strwrap</a></code></p>
</td></tr>
<tr><td><code id="Misc_+3A_simplify">simplify</code></td>
<td>
<p>see <code><a href="base.html#topic+sapply">sapply</a></code></p>
</td></tr>
<tr><td><code id="Misc_+3A_units">units</code></td>
<td>
<p>see <code><a href="graphics.html#topic+par">par</a></code></p>
</td></tr>
<tr><td><code id="Misc_+3A_...">...</code></td>
<td>
<p>arguments passed through to another function.  For
<code>latexBuild</code> represents pairs, with odd numbered elements being
character strings containing LaTeX code or a zero-length object to
ignore, and even-numbered elements representing LaTeX left
parenthesis, left brace, or left bracket, or environment name.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Frank Harrell and Charles Dupont</p>


<h3>Examples</h3>

<pre><code class='language-R'>

trap.rule(1:100,1:100)

unPaste(c('a;b or c','ab;d','qr;s'), ';')

sepUnitsTrans(c('3 days','4 months','2 years','7'))

set.seed(1)
whichClosest(1:100, 3:5)
whichClosest(1:100, rep(3,20))

whichClosePW(1:100, rep(3,20))
whichClosePW(1:100, rep(3,20), f=.05)
whichClosePW(1:100, rep(3,20), f=1e-10)

x &lt;- seq(-1, 1, by=.01)
y &lt;- x^2
h &lt;- inverseFunction(x,y)
formals(h)$turns   # vertex
a &lt;- seq(0, 1, by=.01)
plot(0, 0, type='n', xlim=c(-.5,1.5))
lines(a, h(a)[,1])            ## first inverse
lines(a, h(a)[,2], col='red') ## second inverse
a &lt;- c(-.1, 1.01, 1.1, 1.2)
points(a, h(a)[,1])

d &lt;- data.frame(x=1:2, y=3:4, z=5:6)
d &lt;- upData(d, labels=c(x='X', z='Z lab'), units=c(z='mm'))
a &lt;- keepHattrib(d)

d &lt;- data.frame(x=1:2, y=3:4, z=5:6)
d2 &lt;- restoreHattrib(d, a)
sapply(d2, attributes)

## Not run: 
getLatestSource(recent=5)  # source() most recent 5 revised files in Hmisc
getLatestSource('cut2')    # fetch and source latest cut2.s
getLatestSource('all')     # get everything
getLatestSource(avail=TRUE) # list available files and latest versions

## End(Not run)
</code></pre>

<hr>
<h2 id='movStats'>movStats</h2><span id='topic+movStats'></span>

<h3>Description</h3>

<p>Moving Estimates Using Overlapping Windows
</p>


<h3>Usage</h3>

<pre><code class='language-R'>movStats(
  formula,
  stat = NULL,
  discrete = FALSE,
  space = c("n", "x"),
  eps = if (space == "n") 15,
  varyeps = FALSE,
  nignore = 10,
  xinc = NULL,
  xlim = NULL,
  times = NULL,
  tunits = "year",
  msmooth = c("smoothed", "raw", "both"),
  tsmooth = c("supsmu", "lowess"),
  bass = 8,
  span = 1/4,
  maxdim = 6,
  penalty = NULL,
  trans = function(x) x,
  itrans = function(x) x,
  loess = FALSE,
  ols = FALSE,
  qreg = FALSE,
  lrm = FALSE,
  orm = FALSE,
  hare = FALSE,
  lrm_args = NULL,
  family = "logistic",
  k = 5,
  tau = (1:3)/4,
  melt = FALSE,
  data = environment(formula),
  pr = c("none", "kable", "plain", "margin")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="movStats_+3A_formula">formula</code></td>
<td>
<p>a formula with the analysis variable on the left and the x-variable on the right, following by optional stratification variables</p>
</td></tr>
<tr><td><code id="movStats_+3A_stat">stat</code></td>
<td>
<p>function of one argument that returns a named list of computed values.  Defaults to computing mean and quartiles + N except when y is binary in which case it computes moving proportions.  If y has two columns the default statistics are Kaplan-Meier estimates of cumulative incidence at a vector of <code>times</code>.</p>
</td></tr>
<tr><td><code id="movStats_+3A_discrete">discrete</code></td>
<td>
<p>set to <code>TRUE</code> if x-axis variable is discrete and no intervals should be created for windows</p>
</td></tr>
<tr><td><code id="movStats_+3A_space">space</code></td>
<td>
<p>defines whether intervals used fixed width or fixed sample size</p>
</td></tr>
<tr><td><code id="movStats_+3A_eps">eps</code></td>
<td>
<p>tolerance for window (half width of window).  For <code>space='x'</code> is in data units, otherwise is the sample size for half the window, not counting the middle target point.</p>
</td></tr>
<tr><td><code id="movStats_+3A_varyeps">varyeps</code></td>
<td>
<p>applies to <code>space='n'</code> and causes a smaller <code>eps</code> to be used in strata with fewer than &ldquo; observations so as to arrive at three x points</p>
</td></tr>
<tr><td><code id="movStats_+3A_nignore">nignore</code></td>
<td>
<p>see description, default is to exclude <code>nignore=10</code> points on the left and right tails from estimation and plotting</p>
</td></tr>
<tr><td><code id="movStats_+3A_xinc">xinc</code></td>
<td>
<p>increment in x to evaluate stats, default is xlim range/100 for <code>space='x'</code>.  For <code>space='n'</code> <code>xinc</code> defaults to m observations, where m = max(n/200, 1).</p>
</td></tr>
<tr><td><code id="movStats_+3A_xlim">xlim</code></td>
<td>
<p>2-vector of limits to evaluate if <code>space='x'</code> (default is <code>nignore</code> smallest to <code>nignore</code> largest)</p>
</td></tr>
<tr><td><code id="movStats_+3A_times">times</code></td>
<td>
<p>vector of times for evaluating one minus Kaplan-Meier estimates</p>
</td></tr>
<tr><td><code id="movStats_+3A_tunits">tunits</code></td>
<td>
<p>time units when <code>times</code> is given</p>
</td></tr>
<tr><td><code id="movStats_+3A_msmooth">msmooth</code></td>
<td>
<p>set to <code>'smoothed'</code> or <code>'both'</code> to compute <code>lowess</code>-smooth moving estimates. <code>msmooth='both'</code> will display both.  <code>'raw'</code> will display only the moving statistics.  <code>msmooth='smoothed'</code> (the default) will display only he smoothed moving estimates.</p>
</td></tr>
<tr><td><code id="movStats_+3A_tsmooth">tsmooth</code></td>
<td>
<p>defaults to the super-smoother <code>'supsmu'</code> for after-moving smoothing.  Use <code>tsmooth='lowess'</code> to instead use <code>lowess</code>.</p>
</td></tr>
<tr><td><code id="movStats_+3A_bass">bass</code></td>
<td>
<p>the <code>supsmu</code> <code>bass</code> parameter used to smooth the moving statistics if <code>tsmooth='supsmu'</code>.  The default of 8 represents quite heavy smoothing.</p>
</td></tr>
<tr><td><code id="movStats_+3A_span">span</code></td>
<td>
<p>the <code>lowess</code> <code>span</code> used to smooth the moving statistics</p>
</td></tr>
<tr><td><code id="movStats_+3A_maxdim">maxdim</code></td>
<td>
<p>passed to <code>hare</code>, default is 6</p>
</td></tr>
<tr><td><code id="movStats_+3A_penalty">penalty</code></td>
<td>
<p>passed to <code>hare</code>, default is to use BIC.  Specify 2 to use AIC.</p>
</td></tr>
<tr><td><code id="movStats_+3A_trans">trans</code></td>
<td>
<p>transformation to apply to x</p>
</td></tr>
<tr><td><code id="movStats_+3A_itrans">itrans</code></td>
<td>
<p>inverse transformation</p>
</td></tr>
<tr><td><code id="movStats_+3A_loess">loess</code></td>
<td>
<p>set to TRUE to also compute loess estimates</p>
</td></tr>
<tr><td><code id="movStats_+3A_ols">ols</code></td>
<td>
<p>set to TRUE to include rcspline estimate of mean using ols</p>
</td></tr>
<tr><td><code id="movStats_+3A_qreg">qreg</code></td>
<td>
<p>set to TRUE to include quantile regression estimates w rcspline</p>
</td></tr>
<tr><td><code id="movStats_+3A_lrm">lrm</code></td>
<td>
<p>set to TRUE to include logistic regression estimates w rcspline</p>
</td></tr>
<tr><td><code id="movStats_+3A_orm">orm</code></td>
<td>
<p>set to TRUE to include ordinal logistic regression estimates w rcspline (mean + quantiles in <code>tau</code>)</p>
</td></tr>
<tr><td><code id="movStats_+3A_hare">hare</code></td>
<td>
<p>set to TRUE to include hazard regression estimtes of incidence at <code>times</code>, using the <code>polspline</code> package</p>
</td></tr>
<tr><td><code id="movStats_+3A_lrm_args">lrm_args</code></td>
<td>
<p>a <code>list</code> of optional arguments to pass to <code>lrm</code> when <code>lrm=TRUE</code>, e.g., <code>list(maxit=20)</code></p>
</td></tr>
<tr><td><code id="movStats_+3A_family">family</code></td>
<td>
<p>link function for ordinal regression (see <code>rms::orm</code>)</p>
</td></tr>
<tr><td><code id="movStats_+3A_k">k</code></td>
<td>
<p>number of knots to use for ols and/or qreg rcspline</p>
</td></tr>
<tr><td><code id="movStats_+3A_tau">tau</code></td>
<td>
<p>quantile numbers to estimate with quantile regression</p>
</td></tr>
<tr><td><code id="movStats_+3A_melt">melt</code></td>
<td>
<p>set to TRUE to melt data table and derive Type and Statistic</p>
</td></tr>
<tr><td><code id="movStats_+3A_data">data</code></td>
<td>
<p>data.table or data.frame, default is calling frame</p>
</td></tr>
<tr><td><code id="movStats_+3A_pr">pr</code></td>
<td>
<p>defaults to no printing of window information.  Use <code>pr='plain'</code> to print in the ordinary way, <code style="white-space: pre;">&#8288;pr='kable&#8288;</code> to convert the object to <code>knitr::kable</code> and print, or <code>pr='margin'</code> to convert to <code>kable</code> and place in the <code>Quarto</code> right margin.  For the latter two <code>results='asis'</code> must be in the chunk header.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function to compute moving averages and other statistics as a function
of a continuous variable, possibly stratified by other variables.
Estimates are made by creating overlapping moving windows and
computing the statistics defined in the stat function for each window.
The default method, <code>space='n'</code> creates varying-width intervals each having a sample size of <code>2*eps +1</code>, and the smooth estimates are made every <code>xinc</code> observations.  Outer intervals are not symmetric in sample size (but the mean x in those intervals will reflect that) unless <code>eps=nignore</code>, as outer intervals are centered at observations <code>nignore</code> and <code>n - nignore + 1</code> where the default for <code>nignore</code> is 10.  The mean x-variable within each windows is taken to represent that window.  If <code>trans</code> and <code>itrans</code> are given, x means are computed on the <code>trans(x)</code> scale and then <code>itrans</code>'d.  For <code>space='x'</code>, by default estimates are made on to the <code>nignore</code> smallest to the <code>nignore</code> largest
observed values of the x variable to avoid extrapolation and to
help getting the moving statistics off on an adequate start for
the left tail.  Also by default the moving estimates are smoothed using <code>supsmu</code>.
When <code>melt=TRUE</code> you can feed the result into <code>ggplot</code> like this:
<code style="white-space: pre;">&#8288;ggplot(w, aes(x=age, y=crea, col=Type)) + geom_line() +&#8288;</code>
<code>facet_wrap(~ Statistic)</code>
</p>
<p>See <a href="https://hbiostat.org/rflow/analysis.html#sec-analysis-assoc">here</a> for several examples.
</p>


<h3>Value</h3>

<p>a data table, with attribute <code>infon</code> which is a data frame with rows corresponding to strata and columns <code>N</code>, <code>Wmean</code>, <code>Wmin</code>, <code>Wmax</code> if <code>stat</code> computed <code>N</code>.  These summarize the number of observations used in the windows.  If <code>varyeps=TRUE</code> there is an additional column <code>eps</code> with the computed per-stratum <code>eps</code>.  When <code>space='n'</code> and <code>xinc</code> is not given, the computed <code>xinc</code> also appears as a column.  An additional attribute <code>info</code> is a <code>kable</code> object ready for printing to describe the window characteristics.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

<hr>
<h2 id='mtitle'>
Margin Titles
</h2><span id='topic+mtitle'></span>

<h3>Description</h3>

<p>Writes overall titles and subtitles after a multiple image plot is drawn.
If <code>par()$oma==c(0,0,0,0)</code>, <code>title</code> is used instead of <code>mtext</code>, to draw
titles or subtitles that are inside the plotting region for a single plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mtitle(main, ll, lc,  
       lr=format(Sys.time(),'%d%b%y'),
       cex.m=1.75, cex.l=.5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mtitle_+3A_main">main</code></td>
<td>

<p>main title to be centered over entire figure, default is none
</p>
</td></tr>
<tr><td><code id="mtitle_+3A_ll">ll</code></td>
<td>

<p>subtitle for lower left of figure, default is none
</p>
</td></tr>
<tr><td><code id="mtitle_+3A_lc">lc</code></td>
<td>

<p>subtitle for lower center of figure, default is none
</p>
</td></tr>
<tr><td><code id="mtitle_+3A_lr">lr</code></td>
<td>

<p>subtitle for lower right of figure, default is today's date in format
23Jan91 for UNIX or R (Thu May 30 09:08:13 1996 format for Windows). 
Set to <code>""</code> to suppress lower right title.
</p>
</td></tr>
<tr><td><code id="mtitle_+3A_cex.m">cex.m</code></td>
<td>

<p>character size for main, default is 1.75
</p>
</td></tr>
<tr><td><code id="mtitle_+3A_cex.l">cex.l</code></td>
<td>

<p>character size for subtitles
</p>
</td></tr>
<tr><td><code id="mtitle_+3A_...">...</code></td>
<td>

<p>other arguments passed to <code>mtext</code>
</p>
</td></tr></table>


<h3>Value</h3>

<p>nothing
</p>


<h3>Side Effects</h3>

<p>plots
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics, Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+par">par</a></code>, <code><a href="graphics.html#topic+mtext">mtext</a></code>, <code><a href="graphics.html#topic+title">title</a></code>, <code><a href="base.html#topic+unix">unix</a></code>, <code><a href="#topic+pstamp">pstamp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Set up for 1 plot on figure, give a main title,
#use date for lr
plot(runif(20),runif(20))
mtitle("Main Title")


#Set up for 2 x 2 matrix of plots with a lower left subtitle and overall title
par(mfrow=c(2,2), oma=c(3,0,3,0))
plot(runif(20),runif(20))
plot(rnorm(20),rnorm(20))
plot(exp(rnorm(20)),exp(rnorm(20)))
mtitle("Main Title",ll="n=20")
</code></pre>

<hr>
<h2 id='multLines'>Plot Multiple Lines</h2><span id='topic+multLines'></span>

<h3>Description</h3>

<p>Plots multiple lines based on a vector <code>x</code> and a matrix <code>y</code>,
draws thin vertical lines connecting limits represented by columns of
<code>y</code> beyond the first.  It is assumed that either (1) the second
and third columns of <code>y</code> represent lower and upper confidence
limits, or that (2) there is an even number of columns beyond the
first and these represent ascending quantiles that are symmetrically
arranged around 0.5.  If <code>options(grType='plotly')</code> is in effect,
uses <code>plotly</code> graphics instead of <code>grid</code> or base graphics.
For <code>plotly</code> you may want to set the list of possible colors,
etc. using <code>pobj=plot_ly(colors=...)</code>.  <code>lwd,lty,lwd.vert</code>
are ignored under <code>plotly</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multLines(x, y, pos = c('left', 'right'), col='gray',
          lwd=1, lty=1, lwd.vert = .85, lty.vert = 1,
          alpha = 0.4, grid = FALSE,
          pobj=plotly::plot_ly(), xlim, name=colnames(y)[1], legendgroup=name,
          showlegend=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multLines_+3A_x">x</code></td>
<td>
<p>a numeric vector</p>
</td></tr>
<tr><td><code id="multLines_+3A_y">y</code></td>
<td>
<p>a numeric matrix with number of rows equal to the number of
<code>x</code> elements</p>
</td></tr>
<tr><td><code id="multLines_+3A_pos">pos</code></td>
<td>
<p>when <code>pos='left'</code> the vertical lines are drawn, right
to left, to the left of the point <code>(x, y[,1)</code>.  Otherwise lines
are drawn left to right to the right of the point.</p>
</td></tr>
<tr><td><code id="multLines_+3A_col">col</code></td>
<td>
<p>a color used to connect <code>(x, y[,1])</code> pairs.  The same
color but with transparency given by the <code>alpha</code> argument is
used to draw the vertical lines</p>
</td></tr>
<tr><td><code id="multLines_+3A_lwd">lwd</code></td>
<td>
<p>line width for main lines</p>
</td></tr>
<tr><td><code id="multLines_+3A_lty">lty</code></td>
<td>
<p>line types for main lines</p>
</td></tr>
<tr><td><code id="multLines_+3A_lwd.vert">lwd.vert</code></td>
<td>
<p>line width for vertical lines</p>
</td></tr>
<tr><td><code id="multLines_+3A_lty.vert">lty.vert</code></td>
<td>
<p>line type for vertical lines</p>
</td></tr>
<tr><td><code id="multLines_+3A_alpha">alpha</code></td>
<td>
<p>transparency</p>
</td></tr>
<tr><td><code id="multLines_+3A_grid">grid</code></td>
<td>
<p>set to <code>TRUE</code> when using <code>grid</code>/<code>lattice</code></p>
</td></tr>
<tr><td><code id="multLines_+3A_pobj">pobj</code></td>
<td>
<p>an already started <code>plotly</code> object to add to</p>
</td></tr>
<tr><td><code id="multLines_+3A_xlim">xlim</code></td>
<td>
<p>global x-axis limits (required if using <code>plotly</code>)</p>
</td></tr>
<tr><td><code id="multLines_+3A_name">name</code></td>
<td>
<p>trace name if using <code>plotly</code></p>
</td></tr>
<tr><td><code id="multLines_+3A_legendgroup">legendgroup</code></td>
<td>
<p>legend group name if using <code>plotly</code></p>
</td></tr>
<tr><td><code id="multLines_+3A_showlegend">showlegend</code></td>
<td>
<p>whether or not to show traces in legend, if using
<code>plotly</code></p>
</td></tr> 
<tr><td><code id="multLines_+3A_...">...</code></td>
<td>
<p>passed to <code>add_lines</code> or <code>add_segments</code> if
using <code>plotly</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (requireNamespace("plotly")) {
  x &lt;- 1:4
  y &lt;- cbind(x, x-3, x-2, x-1, x+1, x+2, x+3)
  plot(NA, NA, xlim=c(1,4), ylim=c(-2, 7))
  multLines(x, y, col='blue')
  multLines(x, y, col='red', pos='right')
}
</code></pre>

<hr>
<h2 id='na.delete'>
Row-wise Deletion na.action
</h2><span id='topic+na.delete'></span>

<h3>Description</h3>

<p>Does row-wise deletion as <code>na.omit</code>, but adds frequency of missing values
for each predictor
to the <code>"na.action"</code> attribute of the returned model frame.
Optionally stores further details if <code>options(na.detail.response=TRUE)</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>na.delete(frame)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="na.delete_+3A_frame">frame</code></td>
<td>

<p>a model frame
</p>
</td></tr></table>


<h3>Value</h3>

<p>a model frame with rows deleted and the <code>"na.action"</code> attribute added.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+na.omit">na.omit</a></code>, <code><a href="#topic+na.keep">na.keep</a></code>, <code><a href="#topic+na.detail.response">na.detail.response</a></code>, <code><a href="stats.html#topic+model.frame.default">model.frame.default</a></code>,
<code><a href="stats.html#topic+naresid">naresid</a></code>, <code><a href="stats.html#topic+naprint">naprint</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># options(na.action="na.delete")
# ols(y ~ x)
</code></pre>

<hr>
<h2 id='na.detail.response'>
Detailed Response Variable Information
</h2><span id='topic+na.detail.response'></span>

<h3>Description</h3>

<p>This function is called by certain <code>na.action</code> functions if
<code>options(na.detail.response=TRUE)</code> is set.  By default, this function
returns a matrix of counts of non-NAs and the mean of the response variable
computed separately by whether or not each predictor is NA.  The default
action uses the last column of a <code>Surv</code> object, in effect computing the
proportion of events.  Other summary functions may be specified by
using <code>options(na.fun.response="name of function")</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>na.detail.response(mf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="na.detail.response_+3A_mf">mf</code></td>
<td>

<p>a model frame
</p>
</td></tr></table>


<h3>Value</h3>

<p>a matrix, with rows representing the different statistics that are
computed for the response, and columns representing the different
subsets for each predictor (NA and non-NA value subsets).
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+na.omit">na.omit</a></code>, <code><a href="#topic+na.delete">na.delete</a></code>, <code><a href="stats.html#topic+model.frame.default">model.frame.default</a></code>, 
<code><a href="stats.html#topic+naresid">naresid</a></code>, <code><a href="stats.html#topic+naprint">naprint</a></code>, <code><a href="#topic+describe">describe</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sex
# [1] m f f m f f m m m m m m m m f f f m f m
# age
# [1] NA 41 23 30 44 22 NA 32 37 34 38 36 36 50 40 43 34 22 42 30
# y
# [1] 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 0
# options(na.detail.response=TRUE, na.action="na.delete", digits=3)
# lrm(y ~ age*sex)
#
# Logistic Regression Model
# 
# lrm(formula = y ~ age * sex)
#
#
# Frequencies of Responses
#   0 1 
#  10 8
#
# Frequencies of Missing Values Due to Each Variable
#  y age sex 
#  0   2   0
#
#
# Statistics on Response by Missing/Non-Missing Status of Predictors
#
#     age=NA age!=NA sex!=NA Any NA  No NA 
#   N    2.0  18.000   20.00    2.0 18.000
# Mean    0.5   0.444    0.45    0.5  0.444
#
# \dots\dots
# options(na.action="na.keep")
# describe(y ~ age*sex)
# Statistics on Response by Missing/Non-Missing Status of Predictors
#
#      age=NA age!=NA sex!=NA Any NA  No NA 
#    N    2.0  18.000   20.00    2.0 18.000
# Mean    0.5   0.444    0.45    0.5  0.444
#
# \dots
# options(na.fun.response="table")  #built-in function table()
# describe(y ~ age*sex)
#
# Statistics on Response by Missing/Non-Missing Status of Predictors
#
#   age=NA age!=NA sex!=NA Any NA No NA 
# 0      1      10      11      1    10
# 1      1       8       9      1     8
#
# \dots
</code></pre>

<hr>
<h2 id='na.keep'>
Do-nothing na.action
</h2><span id='topic+na.keep'></span>

<h3>Description</h3>

<p>Does not delete rows containing NAs, but does add details concerning
the distribution of the response variable if <code>options(na.detail.response=TRUE)</code>.
This <code>na.action</code> is primarily for use with <code>describe.formula</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>na.keep(mf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="na.keep_+3A_mf">mf</code></td>
<td>

<p>a model frame
</p>
</td></tr></table>


<h3>Value</h3>

<p>the same model frame with the <code>"na.action"</code> attribute
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+na.omit">na.omit</a></code>, <code><a href="#topic+na.delete">na.delete</a></code>, <code><a href="stats.html#topic+model.frame.default">model.frame.default</a></code>, <code><a href="#topic+na.detail.response">na.detail.response</a></code>,
<code><a href="stats.html#topic+naresid">naresid</a></code>, <code><a href="stats.html#topic+naprint">naprint</a></code>, <code><a href="#topic+describe">describe</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>options(na.action="na.keep", na.detail.response=TRUE)
x1 &lt;- runif(20)
x2 &lt;- runif(20)
x2[1:4] &lt;- NA
y &lt;- rnorm(20)
describe(y ~ x1*x2)
</code></pre>

<hr>
<h2 id='nCoincident'>nCoincident</h2><span id='topic+nCoincident'></span>

<h3>Description</h3>

<p>Number of Coincident Points
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nCoincident(x, y, bins = 400)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nCoincident_+3A_x">x</code></td>
<td>
<p>numeric vector</p>
</td></tr>
<tr><td><code id="nCoincident_+3A_y">y</code></td>
<td>
<p>numeric vector</p>
</td></tr>
<tr><td><code id="nCoincident_+3A_bins">bins</code></td>
<td>
<p>number of bins in both directions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the number of x,y pairs that are likely to be obscured in a regular scatterplot, in the sense of overlapping pairs after binning into <code>bins</code> x <code>bins</code> squares where <code>bins</code> defaults to 400.  <code>NA</code>s are removed first.
</p>


<h3>Value</h3>

<p>integer count
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nCoincident(c(1:5, 4:5), c(1:5, 4:5)/10)
</code></pre>

<hr>
<h2 id='nobsY'>Compute Number of Observations for Left Hand Side of Formula</h2><span id='topic+nobsY'></span>

<h3>Description</h3>

<p>After removing any artificial observations added by
<code>addMarginal</code>, computes the number of
non-missing observations for all left-hand-side variables in
<code>formula</code>.  If <code>formula</code> contains a term <code>id(variable)</code>
<code>variable</code> is assumed to be a subject ID variable, and only unique
subject IDs are counted.  If group is given and its value is the name of
a variable in the right-hand-side of the model, an additional object
<code>nobsg</code> is returned that is a matrix with as many columns as there
are left-hand variables, and as many rows as there are levels to the
<code>group</code> variable.  This matrix has the further breakdown of unique
non-missing observations by <code>group</code>.  The concatenation of all ID
variables, is returned in a <code>list</code> element <code>id</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nobsY(formula, group=NULL, data = NULL, subset = NULL,
      na.action = na.retain, matrixna=c('all', 'any'))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nobsY_+3A_formula">formula</code></td>
<td>
<p>a formula object</p>
</td></tr>
<tr><td><code id="nobsY_+3A_group">group</code></td>
<td>
<p>character string containing optional name of a
stratification variable for computing sample sizes</p>
</td></tr>
<tr><td><code id="nobsY_+3A_data">data</code></td>
<td>
<p>a data frame</p>
</td></tr>
<tr><td><code id="nobsY_+3A_subset">subset</code></td>
<td>
<p>an optional subsetting criterion</p>
</td></tr>
<tr><td><code id="nobsY_+3A_na.action">na.action</code></td>
<td>
<p>an optional <code>NA</code>-handling function</p>
</td></tr>
<tr><td><code id="nobsY_+3A_matrixna">matrixna</code></td>
<td>
<p>set to <code>"all"</code> if an observation is to be
considered <code>NA</code> if all the columns of the variable are
<code>NA</code>, otherwise use <code>matrixna="any"</code> to consider the row
missing if any of the columns are missing</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an integer, with an attribute <code>"formula"</code> containing the
original formula but with an <code>id</code> variable (if present) removed</p>


<h3>Examples</h3>

<pre><code class='language-R'>d &lt;- expand.grid(sex=c('female', 'male', NA),
                 country=c('US', 'Romania'),
                 reps=1:2)
d$subject.id &lt;- c(0, 0, 3:12)
dm &lt;- addMarginal(d, sex, country)
dim(dm)
nobsY(sex + country ~ 1, data=d)
nobsY(sex + country ~ id(subject.id), data=d)
nobsY(sex + country ~ id(subject.id) + reps, group='reps', data=d)
nobsY(sex ~ 1, data=d)
nobsY(sex ~ 1, data=dm)
nobsY(sex ~ id(subject.id), data=dm)
</code></pre>

<hr>
<h2 id='nstr'> Creates a string of arbitry length </h2><span id='topic+nstr'></span>

<h3>Description</h3>

<p>Creates a vector of strings which consists of the string segment given in
each element of the <code>string</code> vector repeated <code>times</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nstr(string, times)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nstr_+3A_string">string</code></td>
<td>
<p> character: vector of string segments to be
repeated.  Will be recycled if argument <code>times</code> is longer.</p>
</td></tr>
<tr><td><code id="nstr_+3A_times">times</code></td>
<td>
<p> integer: vector of number of times to repeat the
corisponding segment.  Will be recycled if argument <code>string</code> is
longer.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a character vector the same length as the longest of the two arguments.
</p>


<h3>Note</h3>

<p>Will throw a warning if the length of the longer argment is not a even
multiple of the shorter argument.
</p>


<h3>Author(s)</h3>

<p> Charles Dupont </p>


<h3>See Also</h3>

 <p><code><a href="base.html#topic+paste">paste</a></code>, <code><a href="base.html#topic+rep">rep</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>nstr(c("a"), c(0,3,4))

nstr(c("a", "b", "c"), c(1,2,3))

nstr(c("a", "b", "c"), 4)
</code></pre>

<hr>
<h2 id='num.intercepts'>Extract number of intercepts</h2><span id='topic+num.intercepts'></span>

<h3>Description</h3>

<p>Extract the number of intercepts from a model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>num.intercepts(fit, type=c('fit', 'var', 'coef'))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="num.intercepts_+3A_fit">fit</code></td>
<td>
<p>a model fit object</p>
</td></tr>
<tr><td><code id="num.intercepts_+3A_type">type</code></td>
<td>

<p>the default is to return the formal number of intercepts used when fitting
the model.  Set <code>type='var'</code> to return the actual number of
intercepts stored in the <code>var</code> object, or <code>type='coef'</code> to
return the actual number in the fitted coefficients.  The former will be
less than the number fitted for <code><a href="rms.html#topic+orm">orm</a></code> fits, and the
latter for <code>orm</code> fits passed through
<code><a href="#topic+fit.mult.impute">fit.mult.impute</a></code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>num.intercepts</code> returns an integer with the number of intercepts
in the model.
</p>


<h3>See Also</h3>

<p><code><a href="rms.html#topic+orm">orm</a></code>, <code><a href="#topic+fit.mult.impute">fit.mult.impute</a></code>
</p>

<hr>
<h2 id='pairUpDiff'>pairUpDiff</h2><span id='topic+pairUpDiff'></span>

<h3>Description</h3>

<p>Pair-up and Compute Differences
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pairUpDiff(
  x,
  major = NULL,
  minor = NULL,
  group,
  refgroup,
  lower = NULL,
  upper = NULL,
  minkeep = NULL,
  sortdiff = TRUE,
  conf.int = 0.95
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pairUpDiff_+3A_x">x</code></td>
<td>
<p>a numeric vector</p>
</td></tr>
<tr><td><code id="pairUpDiff_+3A_major">major</code></td>
<td>
<p>an optional factor or character vector</p>
</td></tr>
<tr><td><code id="pairUpDiff_+3A_minor">minor</code></td>
<td>
<p>an optional factor or character vector</p>
</td></tr>
<tr><td><code id="pairUpDiff_+3A_group">group</code></td>
<td>
<p>a required factor or character vector with two levels</p>
</td></tr>
<tr><td><code id="pairUpDiff_+3A_refgroup">refgroup</code></td>
<td>
<p>a character string specifying which level of <code>group</code> is to be subtracted</p>
</td></tr>
<tr><td><code id="pairUpDiff_+3A_lower">lower</code></td>
<td>
<p>an optional numeric vector giving the lower <code>conf.int</code> confidence limit for <code>x</code></p>
</td></tr>
<tr><td><code id="pairUpDiff_+3A_upper">upper</code></td>
<td>
<p>similar to <code>lower</code> but for the upper limit</p>
</td></tr>
<tr><td><code id="pairUpDiff_+3A_minkeep">minkeep</code></td>
<td>
<p>the minimum value of <code>x</code> required to keep the observation.  An observation is kept if either <code>group</code> has <code>x</code> exceeding or equalling <code>minkeep</code>.  Default is to keep all observations.</p>
</td></tr>
<tr><td><code id="pairUpDiff_+3A_sortdiff">sortdiff</code></td>
<td>
<p>set to <code>FALSE</code> to avoid sorting observations by descending between-<code>group</code> differences</p>
</td></tr>
<tr><td><code id="pairUpDiff_+3A_conf.int">conf.int</code></td>
<td>
<p>confidence level; must have been the value used to compute <code>lower</code> and <code>upper</code> if they are provided</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function sets up for plotting half-width confidence intervals for differences, sorting by descending order of differences within major categories, especially for dot charts as produced by <code><a href="#topic+dotchartpl">dotchartpl()</a></code>. Given a numeric vector <code>x</code> and a grouping (superpositioning) vector <code>group</code> with exactly two levels, computes differences in possibly transformed <code>x</code> between levels of <code>group</code> for the two observations that are equal on <code>major</code> and <code>minor</code>.  If <code>lower</code> and <code>upper</code> are specified, using <code>conf.int</code> and approximate normality on the transformed scale to backsolve for the standard errors of estimates, and uses approximate normality to get confidence intervals on differences by taking the square root of the sum of squares of the two standard errors.  Coordinates for plotting half-width confidence intervals are also computed.  These intervals may be plotted on the same scale as <code>x</code>, having the property that they overlap the two <code>x</code> values if and only if there is no &quot;significant&quot; difference at the <code>conf.int</code> level.
</p>


<h3>Value</h3>

<p>a list of two objects both sorted by descending values of differences in <code>x</code>.  The <code>X</code> object is a data frame that contains the original variables sorted by descending differences across <code>group</code> and in addition a variable <code>subscripts</code> denoting the subscripts of original observations with possible re-sorting and dropping depending on <code>sortdiff</code> and <code>minkeep</code>.  The <code>D</code> data frame contains sorted differences (<code>diff</code>), <code>major</code>, <code>minor</code>, <code>sd</code> of difference, <code>lower</code> and <code>upper</code> confidence limits for the difference, <code>mid</code>, the midpoint of the two <code>x</code> values involved in the difference, <code>lowermid</code>, the midpoint minus 1/2 the width of the confidence interval, and <code>uppermid</code>, the midpoint plus 1/2 the width of the confidence interval.  Another element returned is <code>dropped</code> which is a vector of <code>major</code> / <code>minor</code> combinations dropped due to <code>minkeep</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(1, 4, 7, 2, 5, 3, 6)
pairUpDiff(x, c(rep('A', 4), rep('B', 3)),
  c('u','u','v','v','z','z','q'),
  c('a','b','a','b','a','b','a'), 'a', x-.1, x+.1)
</code></pre>

<hr>
<h2 id='panel.bpplot'>
Box-Percentile Panel Function for Trellis
</h2><span id='topic+panel.bpplot'></span><span id='topic+bpplotM'></span><span id='topic+bpplt'></span><span id='topic+bppltp'></span>

<h3>Description</h3>

<p>For all their good points, box plots have a high ink/information ratio
in that they mainly display 3 quartiles.  Many practitioners have
found that the &quot;outer values&quot; are difficult to explain to
non-statisticians and many feel that the notion of &quot;outliers&quot; is too
dependent on (false) expectations that data distributions should be Gaussian.
</p>
<p><code>panel.bpplot</code> is a <code>panel</code> function for use with
<code>trellis</code>, especially for <code>bwplot</code>.  It draws box plots
(without the whiskers) with any number of user-specified &quot;corners&quot;
(corresponding to different quantiles), but it also draws box-percentile
plots similar to those drawn by Jeffrey Banfield's
(<a href="mailto:umsfjban@bill.oscs.montana.edu">umsfjban@bill.oscs.montana.edu</a>) <code>bpplot</code> function. 
To quote from Banfield, &quot;box-percentile plots supply more
information about the univariate distributions.  At any height the
width of the irregular 'box' is proportional to the percentile of that
height, up to the 50th percentile, and above the 50th percentile the
width is proportional to 100 minus the percentile.  Thus, the width at
any given height is proportional to the percent of observations that
are more extreme in that direction.  As in boxplots, the median, 25th
and 75th percentiles are marked with line segments across the box.&quot;
</p>
<p><code>panel.bpplot</code> can also be used with base graphics to add extended
box plots to an existing plot, by specifying <code>nogrid=TRUE, height=...</code>.
</p>
<p><code>panel.bpplot</code> is a generalization of <code>bpplot</code> and
<code><a href="lattice.html#topic+panel.bwplot">panel.bwplot</a></code> in 
that it works with <code>trellis</code> (making the plots horizontal so that
category labels are more visable), it allows the user to specify the
quantiles to connect and those for which to draw reference lines, 
and it displays means (by default using dots).
</p>
<p><code>bpplt</code> draws horizontal box-percentile plot much like those drawn
by <code>panel.bpplot</code> but taking as the starting point a matrix
containing quantiles summarizing the data.  <code>bpplt</code> is primarily
intended to be used internally by <code>plot.summary.formula.reverse</code> or
<code>plot.summaryM</code> 
but when used with no arguments has a general purpose: to draw an
annotated example box-percentile plot with the default quantiles used
and with the mean drawn with a solid dot.  This schematic plot is
rendered nicely in postscript with an image height of 3.5 inches.
</p>
<p><code>bppltp</code> is like <code>bpplt</code> but for <code>plotly</code> graphics, and
it does not draw an annotated extended box plot example.
</p>
<p><code>bpplotM</code> uses the <code>lattice</code> <code>bwplot</code> function to depict
multiple numeric continuous variables with varying scales in a single
<code>lattice</code> graph, after reshaping the dataset into a tall and thin
format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>panel.bpplot(x, y, box.ratio=1, means=TRUE, qref=c(.5,.25,.75),
             probs=c(.05,.125,.25,.375), nout=0,
             nloc=c('right lower', 'right', 'left', 'none'), cex.n=.7,
             datadensity=FALSE, scat1d.opts=NULL,
             violin=FALSE, violin.opts=NULL,
             font=box.dot$font, pch=box.dot$pch, 
             cex.means =box.dot$cex,  col=box.dot$col,
             nogrid=NULL, height=NULL, ...)

# E.g. bwplot(formula, panel=panel.bpplot, panel.bpplot.parameters)

bpplt(stats, xlim, xlab='', box.ratio = 1, means=TRUE,
      qref=c(.5,.25,.75), qomit=c(.025,.975),
      pch=16, cex.labels=par('cex'), cex.points=if(prototype)1 else 0.5,
      grid=FALSE)

bppltp(p=plotly::plot_ly(),
       stats, xlim, xlab='', box.ratio = 1, means=TRUE,
       qref=c(.5,.25,.75), qomit=c(.025,.975),
       teststat=NULL, showlegend=TRUE)

bpplotM(formula=NULL, groups=NULL, data=NULL, subset=NULL, na.action=NULL,
        qlim=0.01, xlim=NULL,
        nloc=c('right lower','right','left','none'),
        vnames=c('labels', 'names'), cex.n=.7, cex.strip=1,
        outerlabels=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="panel.bpplot_+3A_x">x</code></td>
<td>

<p>continuous variable whose distribution is to be examined
</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_y">y</code></td>
<td>

<p>grouping variable
</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_box.ratio">box.ratio</code></td>
<td>

<p>see <code><a href="lattice.html#topic+panel.bwplot">panel.bwplot</a></code>
</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_means">means</code></td>
<td>

<p>set to <code>FALSE</code> to suppress drawing a character at the mean value
</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_qref">qref</code></td>
<td>

<p>vector of quantiles for which to draw reference lines.  These do not
need to be included in <code>probs</code>.
</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_probs">probs</code></td>
<td>

<p>vector of quantiles to display in the box plot.  These should all be
less than 0.5; the mirror-image quantiles are added automatically.  By
default, <code>probs</code> is set to <code>c(.05,.125,.25,.375)</code> so that intervals
contain 0.9, 0.75, 0.5, and 0.25 of the data.
To draw all 99 percentiles, i.e., to draw a box-percentile plot,
set <code>probs=seq(.01,.49,by=.01)</code>.
To make a more traditional box plot, use <code>probs=.25</code>.
</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_nout">nout</code></td>
<td>

<p>tells the function to use <code>scat1d</code> to draw tick marks showing the
<code>nout</code> smallest and <code>nout</code> largest values if <code>nout &gt;= 1</code>, or to
show all values less than the <code>nout</code> quantile or greater than the
<code>1-nout</code> quantile if <code>0 &lt; nout &lt;= 0.5</code>.  If <code>nout</code> is a whole number,
only the first <code>n/2</code> observations are shown on either side of the
median, where <code>n</code> is the total number of observations. 
</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_nloc">nloc</code></td>
<td>
<p>location to plot number of non-<code>NA</code>
observations next to each box.  Specify <code>nloc='none'</code> to
suppress.  For <code>panel.bpplot</code>, the default <code>nloc</code> is
<code>'none'</code> if <code>nogrid=TRUE</code>.</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_cex.n">cex.n</code></td>
<td>
<p>character size for <code>nloc</code></p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_datadensity">datadensity</code></td>
<td>

<p>set to <code>TRUE</code> to invoke <code>scat1d</code> to draw a data density
(one-dimensional scatter diagram or rug plot) inside each box plot.
</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_scat1d.opts">scat1d.opts</code></td>
<td>

<p>a list containing named arguments (without abbreviations) to pass to
<code>scat1d</code> when <code>datadensity=TRUE</code> or <code>nout &gt; 0</code>
</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_violin">violin</code></td>
<td>
<p>set to <code>TRUE</code> to invoke <code>panel.violin</code> in
addition to drawing box-percentile plots</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_violin.opts">violin.opts</code></td>
<td>
<p>a list of options to pass to <code>panel.violin</code></p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_cex.means">cex.means</code></td>
<td>
<p>character size for dots representing means</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_font">font</code>, <code id="panel.bpplot_+3A_pch">pch</code>, <code id="panel.bpplot_+3A_col">col</code></td>
<td>
<p>see <code><a href="lattice.html#topic+panel.bwplot">panel.bwplot</a></code></p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_nogrid">nogrid</code></td>
<td>
<p>set to <code>TRUE</code> to use in base graphics</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_height">height</code></td>
<td>
<p>if <code>nogrid=TRUE</code>, specifies the height of the box in
user <code>y</code> units</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_...">...</code></td>
<td>
<p>arguments passed to <code>points</code> or <code>panel.bpplot</code> or
<code>bwplot</code></p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_stats">stats</code>, <code id="panel.bpplot_+3A_xlim">xlim</code>, <code id="panel.bpplot_+3A_xlab">xlab</code>, <code id="panel.bpplot_+3A_qomit">qomit</code>, <code id="panel.bpplot_+3A_cex.labels">cex.labels</code>, <code id="panel.bpplot_+3A_cex.points">cex.points</code>, <code id="panel.bpplot_+3A_grid">grid</code></td>
<td>

<p>undocumented arguments to <code>bpplt</code>.  For <code>bpplotM</code>,
<code>xlim</code> is a list with elements named as the <code>x</code>-axis
variables, 
to override the <code>qlim</code> calculations with user-specified
<code>x</code>-axis limits for selected variables.  Example:
<code>xlim=list(age=c(20,60))</code>.
</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_p">p</code></td>
<td>
<p>an already-started <code>plotly</code> object</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_teststat">teststat</code></td>
<td>
<p>an html expression containing a test statistic</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_showlegend">showlegend</code></td>
<td>
<p>set to <code>TRUE</code> to have <code>plotly</code> include
a legend.  Not recommended when plotting more than one variable.</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_formula">formula</code></td>
<td>
<p>a formula with continuous numeric analysis variables on
the left hand side and stratification variables on the right.
The first variable on the right is the one that will vary the
fastest, forming the <code>y</code>-axis.  <code>formula</code> may be
omitted, in which case all numeric variables with more than 5
unique values in <code>data</code> will be analyzed.  Or
<code>formula</code> may be a vector of variable names in <code>data</code>
to analyze.  In the latter two cases (and only those cases),
<code>groups</code> must be given, representing a character vector
with names of stratification variables.</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_groups">groups</code></td>
<td>
<p>see above</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_data">data</code></td>
<td>
<p>an optional data frame</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_subset">subset</code></td>
<td>
<p>an optional subsetting expression or logical vector</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_na.action">na.action</code></td>
<td>
<p>specifies a function to possibly subset the data
according to <code>NA</code>s (default is no such subsetting).</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_qlim">qlim</code></td>
<td>
<p>the outer quantiles to use for scaling each panel in
<code>bpplotM</code></p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_vnames">vnames</code></td>
<td>
<p>default is to use variable <code>label</code> attributes when
they exist, or use variable names otherwise.  Specify
<code>vnames='names'</code> to always use variable names for panel
labels in <code>bpplotM</code></p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_cex.strip">cex.strip</code></td>
<td>
<p>character size for panel strip labels</p>
</td></tr>
<tr><td><code id="panel.bpplot_+3A_outerlabels">outerlabels</code></td>
<td>
<p>if <code>TRUE</code>, pass the <code>lattice</code> graphics
through the <code>latticeExtra</code> package's <code>useOuterStrips</code>
function if there are two conditioning (paneling) variables, to
put panel labels in outer margins.</p>
</td></tr>			
</table>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University School of Medicine
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Esty WW, Banfield J: The box-percentile plot.  J Statistical
Software 8 No. 17, 2003.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bpplot">bpplot</a></code>, <code><a href="lattice.html#topic+panel.bwplot">panel.bwplot</a></code>,
<code><a href="#topic+scat1d">scat1d</a></code>, <code><a href="stats.html#topic+quantile">quantile</a></code>,
<code><a href="#topic+Ecdf">Ecdf</a></code>, <code><a href="#topic+summaryP">summaryP</a></code>,
<code><a href="latticeExtra.html#topic+useOuterStrips">useOuterStrips</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(13)
x &lt;- rnorm(1000)
g &lt;- sample(1:6, 1000, replace=TRUE)
x[g==1][1:20] &lt;- rnorm(20)+3   # contaminate 20 x's for group 1


# default trellis box plot
require(lattice)
bwplot(g ~ x)


# box-percentile plot with data density (rug plot)
bwplot(g ~ x, panel=panel.bpplot, probs=seq(.01,.49,by=.01), datadensity=TRUE)
# add ,scat1d.opts=list(tfrac=1) to make all tick marks the same size
# when a group has &gt; 125 observations


# small dot for means, show only .05,.125,.25,.375,.625,.75,.875,.95 quantiles
bwplot(g ~ x, panel=panel.bpplot, cex.means=.3)


# suppress means and reference lines for lower and upper quartiles
bwplot(g ~ x, panel=panel.bpplot, probs=c(.025,.1,.25), means=FALSE, qref=FALSE)


# continuous plot up until quartiles ("Tootsie Roll plot")
bwplot(g ~ x, panel=panel.bpplot, probs=seq(.01,.25,by=.01))


# start at quartiles then make it continuous ("coffin plot")
bwplot(g ~ x, panel=panel.bpplot, probs=seq(.25,.49,by=.01))


# same as previous but add a spike to give 0.95 interval
bwplot(g ~ x, panel=panel.bpplot, probs=c(.025,seq(.25,.49,by=.01)))


# decile plot with reference lines at outer quintiles and median
bwplot(g ~ x, panel=panel.bpplot, probs=c(.1,.2,.3,.4), qref=c(.5,.2,.8))


# default plot with tick marks showing all observations outside the outer
# box (.05 and .95 quantiles), with very small ticks
bwplot(g ~ x, panel=panel.bpplot, nout=.05, scat1d.opts=list(frac=.01))


# show 5 smallest and 5 largest observations
bwplot(g ~ x, panel=panel.bpplot, nout=5)


# Use a scat1d option (preserve=TRUE) to ensure that the right peak extends 
# to the same position as the extreme scat1d
bwplot(~x , panel=panel.bpplot, probs=seq(.00,.5,by=.001), 
       datadensity=TRUE, scat1d.opt=list(preserve=TRUE))

# Add an extended box plot to an existing base graphics plot
plot(x, 1:length(x))
panel.bpplot(x, 1070, nogrid=TRUE, pch=19, height=15, cex.means=.5)

# Draw a prototype showing how to interpret the plots
bpplt()

# Example for bpplotM
set.seed(1)
n &lt;- 800
d &lt;- data.frame(treatment=sample(c('a','b'), n, TRUE),
                sex=sample(c('female','male'), n, TRUE),
                age=rnorm(n, 40, 10),
                bp =rnorm(n, 120, 12),
                wt =rnorm(n, 190, 30))
label(d$bp) &lt;- 'Systolic Blood Pressure'
units(d$bp) &lt;- 'mmHg'
bpplotM(age + bp + wt ~ treatment, data=d)
bpplotM(age + bp + wt ~ treatment * sex, data=d, cex.strip=.8)
bpplotM(age + bp + wt ~ treatment*sex, data=d,
        violin=TRUE,
        violin.opts=list(col=adjustcolor('blue', alpha.f=.15),
                         border=FALSE))


bpplotM(c('age', 'bp', 'wt'), groups='treatment', data=d)
# Can use Hmisc Cs function, e.g. Cs(age, bp, wt)
bpplotM(age + bp + wt ~ treatment, data=d, nloc='left')

# Without treatment: bpplotM(age + bp + wt ~ 1, data=d)

## Not run: 
# Automatically find all variables that appear to be continuous
getHdata(support)
bpplotM(data=support, group='dzgroup',
        cex.strip=.4, cex.means=.3, cex.n=.45)

# Separate displays for categorical vs. continuous baseline variables
getHdata(pbc)
pbc &lt;- upData(pbc, moveUnits=TRUE)

s &lt;- summaryM(stage + sex + spiders ~ drug, data=pbc)
plot(s)
Key(0, .5)
s &lt;- summaryP(stage + sex + spiders ~ drug, data=pbc)
plot(s, val ~ freq | var, groups='drug', pch=1:3, col=1:3,
     key=list(x=.6, y=.8))

bpplotM(bili + albumin + protime + age ~ drug, data=pbc)

## End(Not run)
</code></pre>

<hr>
<h2 id='partition'>Patitions an object into different sets</h2><span id='topic+partition'></span><span id='topic+partition.vector'></span><span id='topic+partition.matrix'></span>

<h3>Description</h3>

<p>Partitions an object into subsets of length defined in the <code>sep</code>
argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>partition.vector(x, sep, ...)
partition.matrix(x, rowsep, colsep, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="partition_+3A_x">x</code></td>
<td>
<p>object to be partitioned. </p>
</td></tr>
<tr><td><code id="partition_+3A_sep">sep</code></td>
<td>
<p>determines how many elements should go into each set.  The
sum of <code>sep</code> should be equal to the length of <code>x</code>.</p>
</td></tr>
<tr><td><code id="partition_+3A_rowsep">rowsep</code></td>
<td>
<p>determins how many rows should go into each set.  The
sum of <code>rowsep</code> must equal the number of rows in <code>x</code>.</p>
</td></tr>
<tr><td><code id="partition_+3A_colsep">colsep</code></td>
<td>
<p>determins how many columns should go into each set.  The
sum of <code>colsep</code> must equal the number of columns in <code>x</code>.</p>
</td></tr>
<tr><td><code id="partition_+3A_...">...</code></td>
<td>
<p>arguments used in other methods of <code>partition</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of equal length as <code>sep</code> containing the partitioned objects.
</p>


<h3>Author(s)</h3>

<p>Charles Dupont</p>


<h3>See Also</h3>

 <p><code><a href="base.html#topic+split">split</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- 1:7
partition.vector(a, sep=c(1,3,2,1))
</code></pre>

<hr>
<h2 id='pc1'>First Principal Component</h2><span id='topic+pc1'></span>

<h3>Description</h3>

<p>Given a numeric matrix which may or may not contain <code>NA</code>s,
<code>pc1</code> standardizes the columns to have mean 0 and variance 1 and
computes the first principal component using <code><a href="stats.html#topic+prcomp">prcomp</a></code>.  The
proportion of variance explained by this component is printed, and so
are the coefficients of the original (not scaled) variables.  These
coefficients may be applied to the raw data to obtain the first PC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pc1(x, hi)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pc1_+3A_x">x</code></td>
<td>
<p>numeric matrix</p>
</td></tr>
<tr><td><code id="pc1_+3A_hi">hi</code></td>
<td>
<p>if specified, the first PC is scaled so that its maximum
value is <code>hi</code> and its minimum value is zero</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The vector of observations with the first PC.  An attribute
<code>"coef"</code> is attached to this vector.  <code>"coef"</code> contains the
raw-variable coefficients.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+prcomp">prcomp</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x1 &lt;- rnorm(100)
x2 &lt;- x1 + rnorm(100)
w &lt;- pc1(cbind(x1,x2))
attr(w,'coef')
</code></pre>

<hr>
<h2 id='plot.princmp'>plot.princmp</h2><span id='topic+plot.princmp'></span>

<h3>Description</h3>

<p>Plot Method for princmp
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'princmp'
plot(
  x,
  which = c("scree", "loadings"),
  k = x$k,
  offset = 0.8,
  col = 1,
  adj = 0,
  ylim = NULL,
  add = FALSE,
  abbrev = 25,
  nrow = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.princmp_+3A_x">x</code></td>
<td>
<p>results of 'princmp'</p>
</td></tr>
<tr><td><code id="plot.princmp_+3A_which">which</code></td>
<td>
<p>'&lsquo;scree'&rsquo; or '&lsquo;loadings&rsquo;'</p>
</td></tr>
<tr><td><code id="plot.princmp_+3A_k">k</code></td>
<td>
<p>number of components to show, default is 'k' specified to 'princmp'</p>
</td></tr>
<tr><td><code id="plot.princmp_+3A_offset">offset</code></td>
<td>
<p>controls positioning of text labels for cumulative fraction of variance explained</p>
</td></tr>
<tr><td><code id="plot.princmp_+3A_col">col</code></td>
<td>
<p>color of plotted text in scree plot</p>
</td></tr>
<tr><td><code id="plot.princmp_+3A_adj">adj</code></td>
<td>
<p>angle for plotting text in scree plot</p>
</td></tr>
<tr><td><code id="plot.princmp_+3A_ylim">ylim</code></td>
<td>
<p>y-axis scree plotting limits, a 2-vector</p>
</td></tr>
<tr><td><code id="plot.princmp_+3A_add">add</code></td>
<td>
<p>set to 'TRUE' to add a line to an existing scree plot without drawing axes</p>
</td></tr>
<tr><td><code id="plot.princmp_+3A_abbrev">abbrev</code></td>
<td>
<p>an integer specifying the variable name length above which names are passed through [abbreviate(..., minlength=abbrev)]</p>
</td></tr>
<tr><td><code id="plot.princmp_+3A_nrow">nrow</code></td>
<td>
<p>number of rows to use in plotting loadings.  Defaults to the 'ggplot2' 'facet_wrap' default.</p>
</td></tr>
<tr><td><code id="plot.princmp_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses base graphics to by default plot the scree plot from a [princmp()] result, showing cumultive proportion of variance explained.  Alternatively the standardized PC loadings are shown in a 'ggplot2' bar chart.
</p>


<h3>Value</h3>

<p>&lsquo;ggplot2' object if 'which=&rsquo;loadings''
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

<hr>
<h2 id='plotCorrM'>plotCorrM</h2><span id='topic+plotCorrM'></span>

<h3>Description</h3>

<p>Plot Correlation Matrix and Correlation vs. Time Gap
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotCorrM(
  r,
  what = c("plots", "data"),
  type = c("rectangle", "circle"),
  xlab = "",
  ylab = "",
  maxsize = 12,
  xangle = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotCorrM_+3A_r">r</code></td>
<td>
<p>correlation matrix</p>
</td></tr>
<tr><td><code id="plotCorrM_+3A_what">what</code></td>
<td>
<p>specifies whether to return plots or the data frame used in making the plots</p>
</td></tr>
<tr><td><code id="plotCorrM_+3A_type">type</code></td>
<td>
<p>specifies whether to use bottom-aligned rectangles (the default) or centered circles</p>
</td></tr>
<tr><td><code id="plotCorrM_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label for correlation matrix</p>
</td></tr>
<tr><td><code id="plotCorrM_+3A_ylab">ylab</code></td>
<td>
<p>y-axis label for correlation matrix</p>
</td></tr>
<tr><td><code id="plotCorrM_+3A_maxsize">maxsize</code></td>
<td>
<p>maximum circle size if <code>type='circle'</code></p>
</td></tr>
<tr><td><code id="plotCorrM_+3A_xangle">xangle</code></td>
<td>
<p>angle for placing x-axis labels, defaulting to 0.  Consider using <code>xangle=45</code> when labels are long.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Constructs two <code>ggplot2</code> graphics.  The first is a half matrix of rectangles where the height of the rectangle is proportional to the absolute value of the correlation coefficient, with positive and negative coefficients shown in different colors.  The second graphic is a variogram-like graph of correlation coefficients on the y-axis and absolute time gap on the x-axis, with a <code>loess</code> smoother added.  The times are obtained from the correlation matrix's row and column names if these are numeric.  If any names are not numeric, the times are taken as the integers 1, 2, 3, ...  The two graphics are <code>ggplotly</code>-ready if you use <code>plotly::ggplotly(..., tooltip='label')</code>.
</p>


<h3>Value</h3>

<p>a list containing two <code>ggplot2</code> objects if <code>what='plots'</code>, or a data frame if <code>what='data'</code>
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
r &lt;- cor(matrix(rnorm(100), ncol=10))
g &lt;- plotCorrM(r)
g[[1]]  # plot matrix
g[[2]]  # plot correlation vs gap time
# ggplotlyr(g[[2]])
# ggplotlyr uses ggplotly with tooltip='label' then removes
# txt: from hover text
</code></pre>

<hr>
<h2 id='plotCorrPrecision'>Plot Precision of Estimate of Pearson Correlation Coefficient</h2><span id='topic+plotCorrPrecision'></span>

<h3>Description</h3>

<p>This function plots the precision (margin of error) of the
product-moment linear 
correlation coefficient r vs. sample size, for a given vector of
correlation coefficients <code>rho</code>.  Precision is defined as the larger
of the upper confidence limit minus rho and rho minus the lower confidence
limit.  <code>labcurve</code> is used to automatically label the curves.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotCorrPrecision(rho = c(0, 0.5), n = seq(10, 400, length.out = 100),
                  conf.int = 0.95, offset=0.025, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotCorrPrecision_+3A_rho">rho</code></td>
<td>
<p>single or vector of true correlations.  A worst-case
precision graph results from rho=0</p>
</td></tr>
<tr><td><code id="plotCorrPrecision_+3A_n">n</code></td>
<td>
<p>vector of sample sizes to use on the x-axis</p>
</td></tr>
<tr><td><code id="plotCorrPrecision_+3A_conf.int">conf.int</code></td>
<td>
<p>confidence coefficient; default uses 0.95 confidence
limits</p>
</td></tr>
<tr><td><code id="plotCorrPrecision_+3A_offset">offset</code></td>
<td>
<p>see <code><a href="#topic+labcurve">labcurve</a></code></p>
</td></tr>
<tr><td><code id="plotCorrPrecision_+3A_...">...</code></td>
<td>
<p>other arguments to <code><a href="#topic+labcurve">labcurve</a></code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Xing Wang and Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="#topic+rcorr">rcorr</a></code>,<code><a href="stats.html#topic+cor">cor</a></code>,<code><a href="stats.html#topic+cor.test">cor.test</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>plotCorrPrecision()
plotCorrPrecision(rho=0)
</code></pre>

<hr>
<h2 id='plotlyM'>plotly Multiple</h2><span id='topic+plotlyM'></span>

<h3>Description</h3>

<p>Generates multiple plotly graphics, driven by specs in a data frame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotlyM(
  data,
  x = ~x,
  y = ~y,
  xhi = ~xhi,
  yhi = ~yhi,
  htext = NULL,
  multplot = NULL,
  strata = NULL,
  fitter = NULL,
  color = NULL,
  size = NULL,
  showpts = !length(fitter),
  rotate = FALSE,
  xlab = NULL,
  ylab = NULL,
  ylabpos = c("top", "y"),
  xlim = NULL,
  ylim = NULL,
  shareX = TRUE,
  shareY = FALSE,
  height = NULL,
  width = NULL,
  nrows = NULL,
  ncols = NULL,
  colors = NULL,
  alphaSegments = 1,
  alphaCline = 0.3,
  digits = 4,
  zeroline = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotlyM_+3A_data">data</code></td>
<td>
<p>input data frame</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_x">x</code></td>
<td>
<p>formula specifying the x-axis variable</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_y">y</code></td>
<td>
<p>formula for y-axis variable</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_xhi">xhi</code></td>
<td>
<p>formula for upper x variable limits (<code>x</code> taken to be lower value)</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_yhi">yhi</code></td>
<td>
<p>formula for upper y variable limit (<code>y</code> taken to be lower value)</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_htext">htext</code></td>
<td>
<p>formula for hovertext variable</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_multplot">multplot</code></td>
<td>
<p>formula specifying a variable in <code>data</code> that when stratified on produces a separate plot</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_strata">strata</code></td>
<td>
<p>formula specifying an optional stratification variable</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_fitter">fitter</code></td>
<td>
<p>a fitting such as <code>loess</code> that comes with a <code>predict</code> method.  Alternatively specify <code>fitter='ecdf'</code> to use an internal function for computing and displaying ECDFs, which moves the analysis variable from the y-axis to the x-axis</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_color">color</code></td>
<td>
<p><code>plotly</code> formula specifying a color variable or e.g. <code>~ I('black')</code>.  To keep colors constant over multiple plots you will need to specify an AsIs color when you don't have a variable representing color groups.</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_size">size</code></td>
<td>
<p><code>plotly</code> formula specifying a symbol size variable or AsIs</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_showpts">showpts</code></td>
<td>
<p>if <code>fitter</code> is given, set to <code>TRUE</code> to show raw data points in addition to smooth fits</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_rotate">rotate</code></td>
<td>
<p>set to <code>TRUE</code> to reverse the roles of <code>x</code> and <code>y</code>, for example to get horizontal dot charts with error bars</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label.  May contain html.</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_ylab">ylab</code></td>
<td>
<p>a named vector of y-axis labels, possibly containing html (see example below).  The names of the vector must correspond to levels of the <code>multplot</code> variable.  <code>ylab</code> can be unnamed if <code>multplot</code> is not used.</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_ylabpos">ylabpos</code></td>
<td>
<p>position of y-axis labels.  Default is on top left of plot.  Specify <code>ylabpos='y'</code> for usual y-axis placement.</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_xlim">xlim</code></td>
<td>
<p>2-vector of x-axis limits, optional</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_ylim">ylim</code></td>
<td>
<p>2-vector of y-axis limits, optional</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_sharex">shareX</code></td>
<td>
<p>specifies whether x-axes should be shared when they align vertically over multiple plots</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_sharey">shareY</code></td>
<td>
<p>specifies whether y-axes should be shared when they align horizontally over multiple plots</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_height">height</code></td>
<td>
<p>height of the combined image in pixels</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_width">width</code></td>
<td>
<p>width of the combined image in pixels</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_nrows">nrows</code></td>
<td>
<p>the number of rows to produce using <code>subplot</code></p>
</td></tr>
<tr><td><code id="plotlyM_+3A_ncols">ncols</code></td>
<td>
<p>the number of columns to produce using <code>subplot</code> (specify at most one of <code>nrows,ncols</code>)</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_colors">colors</code></td>
<td>
<p>the color palette.  Leave unspecified to use the default <code>plotly</code> palette</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_alphasegments">alphaSegments</code></td>
<td>
<p>alpha transparency for line segments (when <code>xhi</code> or <code>yhi</code> is not <code>NA</code>)</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_alphacline">alphaCline</code></td>
<td>
<p>alpha transparency for lines used to connect points</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_digits">digits</code></td>
<td>
<p>number of significant digits to use in constructing hovertext</p>
</td></tr>
<tr><td><code id="plotlyM_+3A_zeroline">zeroline</code></td>
<td>
<p>set to <code>FALSE</code> to suppress vertical line at x=0</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generates multiple <code>plotly</code> traces and combines them with <code>plotly::subplot</code>.  The traces are controlled by specifications in data frame <code>data</code> plus various arguments.  <code>data</code> must contain these variables: <code>x</code>, <code>y</code>, and <code>tracename</code> (if <code>color</code> is not an &quot;AsIs&quot; color such as <code>~ I('black')</code>), and can contain these optional variables: <code>xhi</code>, <code>yhi</code> (rows containing <code>NA</code> for both <code>xhi</code> and <code>yhi</code> represent points, and those with non-<code>NA</code> <code>xhi</code> or <code>yhi</code> represent segments, <code>connect</code> (set to <code>TRUE</code> for rows for points, to connect the symbols), <code>legendgroup</code> (see <code>plotly</code> documentation), and <code>htext</code> (hovertext).  If the <code>color</code> argument is given and it is not an &quot;AsIs&quot; color, the variable named in the <code>color</code> formula must also be in <code>data</code>.  Likewise for <code>size</code>.  If the <code>multplot</code> is given, the variable given in the formula must be in <code>data</code>.  If <code>strata</code> is present, another level of separate plots is generated by levels of <code>strata</code>, within levels of <code>multplot</code>.
</p>
<p>If <code>fitter</code> is specified, x,y coordinates for an individual plot are
run through <code>fitter</code>, and a line plot is made instead of showing data points.  Alternatively you can specify <code>fitter='ecdf'</code> to compute and plot emirical cumulative distribution functions.
</p>


<h3>Value</h3>

<p><code>plotly</code> object produced by <code>subplot</code>
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(1)
pts     &lt;- expand.grid(v=c('y1', 'y2', 'y3'), x=1:4, g=c('a', 'b'), yhi=NA,
                       tracename='mean', legendgroup='mean',
                       connect=TRUE, size=4)

pts$y   &lt;- round(runif(nrow(pts)), 2)

segs     &lt;- expand.grid(v=c('y1', 'y2', 'y3'), x=1:4, g=c('a', 'b'),
                        tracename='limits', legendgroup='limits',
                        connect=NA, size=6)
segs$y   &lt;- runif(nrow(pts))
segs$yhi &lt;- segs$y + runif(nrow(pts), .05, .15)

z &lt;- rbind(pts, segs)

xlab &lt;- labelPlotmath('X&lt;sub&gt;12&lt;/sub&gt;', 'm/sec&lt;sup&gt;2&lt;/sup&gt;', html=TRUE)
ylab &lt;- c(y1=labelPlotmath('Y1', 'cm', html=TRUE),
          y2='Y2',
          y3=labelPlotmath('Y3', 'mm', html=TRUE))

W=plotlyM(z, multplot=~v, color=~g, xlab=xlab, ylab=ylab, ncols=2,
          colors=c('black', 'blue'))

W2=plotlyM(z, multplot=~v, color=~I('black'), xlab=xlab, ylab=ylab,
           colors=c('black', 'blue'))


## End(Not run)
</code></pre>

<hr>
<h2 id='plsmo'>
Plot smoothed estimates
</h2><span id='topic+plsmo'></span><span id='topic+panel.plsmo'></span>

<h3>Description</h3>

<p>Plot smoothed estimates of x vs. y, handling missing data for lowess
or supsmu, and adding axis labels.  Optionally suppresses plotting
extrapolated estimates.  An optional <code>group</code> variable can be
specified to compute and plot the smooth curves by levels of
<code>group</code>.  When <code>group</code> is present, the <code>datadensity</code>
option will draw tick marks showing the location of the raw
<code>x</code>-values, separately for each curve.  <code>plsmo</code> has an
option to plot connected points for raw data, with no smoothing.  The
non-panel version of <code>plsmo</code> allows <code>y</code> to be a matrix, for
which smoothing is done separately over its columns.  If both
<code>group</code> and multi-column <code>y</code> are used, the number of curves
plotted is the product of the number of groups and the number of
<code>y</code> columns.
</p>
<p><code>method='intervals'</code> is often used when y is binary, as it may be
tricky to specify a reasonable smoothing parameter to <code>lowess</code> or
<code>supsmu</code> in this case.  The <code>'intervals'</code> method uses the
<code>cut2</code> function to form intervals of x containing a target of
<code>mobs</code> observations.  For each interval the <code>ifun</code> function
summarizes y, with the default being the mean (proportions for binary
y).  The results are plotted as step functions, with vertical
discontinuities drawn with a saturation of 0.15 of the original color.
A plus sign is drawn at the mean x within each interval.
For this approach, the default x-range is the entire raw data range,
and <code>trim</code> and <code>evaluate</code> are ignored.  For
<code>panel.plsmo</code> it is best to specify <code>type='l'</code> when using
<code>'intervals'</code>. 
</p>
<p><code>panel.plsmo</code> is a <code>panel</code> function for <code>trellis</code> for the
<code>xyplot</code> function that uses <code>plsmo</code> and its options to draw
one or more nonparametric function estimates on each panel.  This has
advantages over using <code>xyplot</code> with <code>panel.xyplot</code> and
<code>panel.loess</code>: (1) by default it will invoke <code>labcurve</code> to
label the curves where they are most separated, (2) the
<code>datadensity</code> option will put rug plots on each curve (instead of a
single rug plot at the bottom of the graph), and (3) when
<code>panel.plsmo</code> invokes <code>plsmo</code> it can use the &quot;super smoother&quot;
(<code>supsmu</code> function) instead of <code>lowess</code>, or pass
<code>method='intervals'</code>.  <code>panel.plsmo</code> 
senses when a <code>group</code> variable is specified to <code>xyplot</code> so
that it can invoke <code><a href="lattice.html#topic+panel.superpose">panel.superpose</a></code> instead of
<code>panel.xyplot</code>.  Using <code>panel.plsmo</code> through <code>trellis</code>
has some advantages over calling <code>plsmo</code> directly in that
conditioning variables are allowed and <code>trellis</code> uses nicer fonts
etc.
</p>
<p>When a <code>group</code> variable was used, <code>panel.plsmo</code> creates a function
<code>Key</code> in the session frame that the user can invoke to draw a key for
individual data point symbols used for the <code>group</code>s.  
By default, the key is positioned at the upper right
corner of the graph.  If <code>Key(locator(1))</code> is specified, the key will
appear so that its upper left corner is at the coordinates of the
mouse click.
</p>
<p>For <code>ggplot2</code> graphics the counterparts are
<code><a href="#topic+stat_plsmo">stat_plsmo</a></code> and <code><a href="#topic+histSpikeg">histSpikeg</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plsmo(x, y, method=c("lowess","supsmu","raw","intervals"), xlab, ylab, 
      add=FALSE, lty=1 : lc, col=par("col"), lwd=par("lwd"),
      iter=if(length(unique(y))&gt;2) 3 else 0, bass=0, f=2/3, mobs=30, trim, 
      fun, ifun=mean, group, prefix, xlim, ylim, 
      label.curves=TRUE, datadensity=FALSE, scat1d.opts=NULL,
      lines.=TRUE, subset=TRUE,
      grid=FALSE, evaluate=NULL, ...)


#To use panel function:
#xyplot(formula=y ~ x | conditioningvars, groups,
#       panel=panel.plsmo, type='b', 
#       label.curves=TRUE,
#       lwd = superpose.line$lwd, 
#       lty = superpose.line$lty, 
#       pch = superpose.symbol$pch, 
#       cex = superpose.symbol$cex, 
#       font = superpose.symbol$font, 
#       col = NULL, scat1d.opts=NULL, \dots)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plsmo_+3A_x">x</code></td>
<td>

<p>vector of x-values, NAs allowed
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_y">y</code></td>
<td>

<p>vector or matrix of y-values, NAs allowed
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_method">method</code></td>
<td>

<p><code>"lowess"</code> (the default), <code>"supsmu"</code>, <code>"raw"</code> to not
smooth at all, or <code>"intervals"</code> to use intervals (see above)
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_xlab">xlab</code></td>
<td>

<p>x-axis label iff add=F.  Defaults of label(x) or argument name.
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_ylab">ylab</code></td>
<td>

<p>y-axis label, like xlab.
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_add">add</code></td>
<td>

<p>Set to T to call lines instead of plot.  Assumes axes already labeled.
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_lty">lty</code></td>
<td>

<p>line type, default=1,2,3,..., corresponding to columns of <code>y</code> and
<code>group</code> combinations
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_col">col</code></td>
<td>

<p>color for each curve, corresponding to <code>group</code>.  Default is
current <code>par("col")</code>. 
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_lwd">lwd</code></td>
<td>

<p>vector of line widths for the curves, corresponding to <code>group</code>.
Default is current <code>par("lwd")</code>. 
<code>lwd</code> can also be specified as an element of <code>label.curves</code> if
<code>label.curves</code> is a list.
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_iter">iter</code></td>
<td>

<p>iter parameter if <code>method="lowess"</code>, default=0 if <code>y</code> is binary, and 3 otherwise.
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_bass">bass</code></td>
<td>

<p>bass parameter if <code>method="supsmu"</code>, default=0.
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_f">f</code></td>
<td>
<p>passed to the <code>lowess</code> function, for <code>method="lowess"</code></p>
</td></tr>
<tr><td><code id="plsmo_+3A_mobs">mobs</code></td>
<td>
<p>for <code>method='intervals'</code>, the target number of
observations per interval</p>
</td></tr>
<tr><td><code id="plsmo_+3A_trim">trim</code></td>
<td>

<p>only plots smoothed estimates between trim and 1-trim quantiles
of x.  Default is to use 10th smallest to 10th largest x in the group if the number of observations in the group exceeds 200 (0 otherwise).
Specify trim=0 to plot over entire range.
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_fun">fun</code></td>
<td>

<p>after computing the smoothed estimates, if <code>fun</code> is given the y-values
are transformed by <code>fun()</code>
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_ifun">ifun</code></td>
<td>
<p>a summary statistic function to apply to the
<code>y</code>-variable for <code>method='intervals'</code>.  Default is <code>mean</code>.</p>
</td></tr>
<tr><td><code id="plsmo_+3A_group">group</code></td>
<td>

<p>a variable, either a <code>factor</code> vector or one that will be converted to
<code>factor</code> by <code>plsmo</code>, that is used to stratify the data so that separate
smooths may be computed
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_prefix">prefix</code></td>
<td>

<p>a character string to appear in group of group labels.  The presence of
<code>prefix</code> ensures that <code>labcurve</code> will be called even when <code>add=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_xlim">xlim</code></td>
<td>

<p>a vector of 2 x-axis limits.  Default is observed range.
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_ylim">ylim</code></td>
<td>

<p>a vector of 2 y-axis limits.  Default is observed range.
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_label.curves">label.curves</code></td>
<td>

<p>set to <code>FALSE</code> to prevent <code>labcurve</code> from being called to label multiple
curves corresponding to <code>group</code>s.  Set to a list to pass options to
<code>labcurve</code>.  <code>lty</code> and <code>col</code> are passed to <code>labcurve</code> automatically.
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_datadensity">datadensity</code></td>
<td>

<p>set to <code>TRUE</code> to draw tick marks on each curve, using x-coordinates
of the raw data <code>x</code> values.  This is done using <code>scat1d</code>.
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_scat1d.opts">scat1d.opts</code></td>
<td>
<p>a list of options to hand to <code>scat1d</code></p>
</td></tr>
<tr><td><code id="plsmo_+3A_lines.">lines.</code></td>
<td>

<p>set to <code>FALSE</code> to suppress smoothed curves from being drawn.  This can
make sense if <code>datadensity=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_subset">subset</code></td>
<td>

<p>a logical or integer vector specifying a subset to use for processing,
with respect too all variables being analyzed
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_grid">grid</code></td>
<td>

<p>set to <code>TRUE</code> if the <span class="rlang"><b>R</b></span> <code>grid</code> package drew the current	plot</p>
</td></tr>
<tr><td><code id="plsmo_+3A_evaluate">evaluate</code></td>
<td>

<p>number of points to keep from smoother.  If specified, an
equally-spaced grid of <code>evaluate</code> <code>x</code> values will be obtained from the
smoother using linear interpolation.  This will keep from plotting an
enormous number of points if the dataset contains a very large number
of unique <code>x</code> values.</p>
</td></tr>
<tr><td><code id="plsmo_+3A_...">...</code></td>
<td>

<p>optional arguments that are passed to <code>scat1d</code>,
or optional parameters to pass to <code>plsmo</code> from
<code>panel.plsmo</code>.  See optional arguments for <code>plsmo</code> above.
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_type">type</code></td>
<td>

<p>set to <code>p</code> to have <code>panel.plsmo</code> plot points (and not call <code>plsmo</code>), 
<code>l</code> to call <code>plsmo</code> and not plot points, or use the default <code>b</code> to plot both.
</p>
</td></tr>
<tr><td><code id="plsmo_+3A_pch">pch</code>, <code id="plsmo_+3A_cex">cex</code>, <code id="plsmo_+3A_font">font</code></td>
<td>

<p>vectors of graphical parameters corresponding to the <code>group</code>s (scalars
if <code>group</code> is absent).  By default, the parameters set up by
<code>trellis</code> will be used.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>plsmo</code> returns a list of curves (x and y coordinates) that was passed to <code>labcurve</code>
</p>


<h3>Side Effects</h3>

<p>plots, and <code>panel.plsmo</code> creates the <code>Key</code> function in the session frame.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+lowess">lowess</a></code>, <code><a href="stats.html#topic+supsmu">supsmu</a></code>, <code><a href="#topic+label">label</a></code>,
<code><a href="stats.html#topic+quantile">quantile</a></code>, <code><a href="#topic+labcurve">labcurve</a></code>, <code><a href="#topic+scat1d">scat1d</a></code>,
<code><a href="lattice.html#topic+xyplot">xyplot</a></code>, <code><a href="lattice.html#topic+panel.superpose">panel.superpose</a></code>,
<code><a href="lattice.html#topic+panel.xyplot">panel.xyplot</a></code>, <code><a href="#topic+stat_plsmo">stat_plsmo</a></code>,
<code><a href="#topic+histSpikeg">histSpikeg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x &lt;- 1:100
y &lt;- x + runif(100, -10, 10)
plsmo(x, y, "supsmu", xlab="Time of Entry") 
#Use label(y) or "y" for ylab


plsmo(x, y, add=TRUE, lty=2)
#Add lowess smooth to existing plot, with different line type


age &lt;- rnorm(500, 50, 15)
survival.time &lt;- rexp(500)
sex &lt;- sample(c('female','male'), 500, TRUE)
race &lt;- sample(c('black','non-black'), 500, TRUE)
plsmo(age, survival.time &lt; 1, fun=qlogis, group=sex) # plot logit by sex

#Bivariate Y
sbp &lt;- 120 + (age - 50)/10 + rnorm(500, 0, 8) + 5 * (sex == 'male')
dbp &lt;-  80 + (age - 50)/10 + rnorm(500, 0, 8) - 5 * (sex == 'male')
Y &lt;- cbind(sbp, dbp)
plsmo(age, Y)
plsmo(age, Y, group=sex)


#Plot points and smooth trend line using trellis 
# (add type='l' to suppress points or type='p' to suppress trend lines)
require(lattice)
xyplot(survival.time ~ age, panel=panel.plsmo)


#Do this for multiple panels
xyplot(survival.time ~ age | sex, panel=panel.plsmo)

#Repeat this using equal sample size intervals (n=25 each) summarized by
#the median, then a proportion (mean of binary y)
xyplot(survival.time ~ age | sex, panel=panel.plsmo, type='l',
       method='intervals', mobs=25, ifun=median)
ybinary &lt;- ifelse(runif(length(sex)) &lt; 0.5, 1, 0)
xyplot(ybinary ~ age, groups=sex, panel=panel.plsmo, type='l',
       method='intervals', mobs=75, ifun=mean, xlim=c(0, 120))


#Do this for subgroups of points on each panel, show the data
#density on each curve, and draw a key at the default location
xyplot(survival.time ~ age | sex, groups=race, panel=panel.plsmo,
       datadensity=TRUE)
Key()


#Use wloess.noiter to do a fast weighted smooth
plot(x, y)
lines(wtd.loess.noiter(x, y))
lines(wtd.loess.noiter(x, y, weights=c(rep(1,50), 100, rep(1,49))), col=2)
points(51, y[51], pch=18)   # show overly weighted point
#Try to duplicate this smooth by replicating 51st observation 100 times
lines(wtd.loess.noiter(c(x,rep(x[51],99)),c(y,rep(y[51],99)),
      type='ordered all'), col=3)
#Note: These two don't agree exactly
</code></pre>

<hr>
<h2 id='popower'>Power and Sample Size for Ordinal Response</h2><span id='topic+multEventChart'></span><span id='topic+popower'></span><span id='topic+posamsize'></span><span id='topic+print.popower'></span><span id='topic+print.posamsize'></span><span id='topic+pomodm'></span><span id='topic+simPOcuts'></span><span id='topic+propsPO'></span><span id='topic+propsTrans'></span>

<h3>Description</h3>

<p><code>popower</code> computes the power for a two-tailed two sample comparison
of ordinal outcomes under the proportional odds ordinal logistic
model.  The power is the same as that of the Wilcoxon test but with
ties handled properly.  <code>posamsize</code> computes the total sample size
needed to achieve a given power.  Both functions compute the efficiency
of the design compared with a design in which the response variable
is continuous.  <code>print</code> methods exist for both functions.  Any of the
input arguments may be vectors, in which case a vector of powers or
sample sizes is returned.  These functions use the methods of
Whitehead (1993).
</p>
<p><code>pomodm</code> is a function that assists in translating odds ratios to
differences in mean or median on the original scale.
</p>
<p><code>simPOcuts</code> simulates simple unadjusted two-group comparisons under
a PO model to demonstrate the natural sampling variability that causes
estimated odds ratios to vary over cutoffs of Y.
</p>
<p><code>propsPO</code> uses <code><a href="ggplot2.html#topic+ggplot2">ggplot2</a></code> to plot a stacked bar chart of
proportions stratified by a grouping variable (and optionally a stratification variable), with an optional
additional graph showing what the proportions would be had proportional
odds held and an odds ratio was applied to the proportions in a
reference group.  If the result is passed to <code>ggplotly</code>, customized
tooltip hover text will appear.
</p>
<p><code>propsTrans</code> uses <code><a href="ggplot2.html#topic+ggplot2">ggplot2</a></code> to plot all successive
transition proportions.  <code>formula</code> has the state variable on the
left hand side, the first right-hand variable is time, and the second
right-hand variable is a subject ID variable.\
</p>
<p><code>multEventChart</code> uses <code><a href="ggplot2.html#topic+ggplot2">ggplot2</a></code> to plot event charts
showing state transitions, account for absorbing states/events.  It is
based on code written by Lucy D'Agostino McGowan posted at <a href="https://livefreeordichotomize.com/posts/2020-05-21-survival-model-detective-1/">https://livefreeordichotomize.com/posts/2020-05-21-survival-model-detective-1/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>popower(p, odds.ratio, n, n1, n2, alpha=0.05)
## S3 method for class 'popower'
print(x, ...)
posamsize(p, odds.ratio, fraction=.5, alpha=0.05, power=0.8)
## S3 method for class 'posamsize'
print(x, ...)
pomodm(x=NULL, p, odds.ratio=1)
simPOcuts(n, nsim=10, odds.ratio=1, p)
propsPO(formula, odds.ratio=NULL, ref=NULL, data=NULL, ncol=NULL, nrow=NULL )
propsTrans(formula, data=NULL, labels=NULL, arrow='\u2794',
           maxsize=12, ncol=NULL, nrow=NULL)
multEventChart(formula, data=NULL, absorb=NULL, sortbylast=FALSE,
   colorTitle=label(y), eventTitle='Event',
   palette='OrRd',
   eventSymbols=c(15, 5, 1:4, 6:10),
   timeInc=min(diff(unique(x))/2))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="popower_+3A_p">p</code></td>
<td>

<p>a vector of marginal cell probabilities which must add up to one.
For <code>popower</code> and <code>posamsize</code>, The <code>i</code>th element specifies the probability that a patient will be
in response level <code>i</code>, averaged over the two treatment groups.  For
<code>pomodm</code> and <code>simPOcuts</code>, <code>p</code> is the vector of cell
probabilities to be translated under a given odds ratio.  For
<code>simPOcuts</code>, if <code>p</code> has names, those names are taken as the
ordered distinct Y-values.  Otherwise Y-values are taken as the integers
1, 2, ... up to the length of <code>p</code>.
</p>
</td></tr>
<tr><td><code id="popower_+3A_odds.ratio">odds.ratio</code></td>
<td>

<p>the odds ratio to be able to detect.  It doesn't
matter which group is in the numerator.  For <code>propsPO</code>,
<code>odds.ratio</code> is a function of the grouping (right hand side)
variable value.  The value of the function specifies the odds ratio to
apply to the refernce group to get all other group's expected proportions
were proportional odds to hold against the first group.  Normally the
function should return 1.0 when its <code>x</code> argument corresponds to the
<code>ref</code> group.  For <code>pomodm</code> and <code>simPOcuts</code> is the odds
ratio to apply to convert the given cell probabilities.</p>
</td></tr>
<tr><td><code id="popower_+3A_n">n</code></td>
<td>

<p>total sample size for <code>popower</code>.  You must specify either <code>n</code> or
<code>n1</code> and <code>n2</code>.  If you specify <code>n</code>, <code>n1</code> and
<code>n2</code> are set to <code>n/2</code>. For <code>simPOcuts</code> is a single number
equal to the combined sample sizes of two groups.
</p>
</td></tr>
<tr><td><code id="popower_+3A_n1">n1</code></td>
<td>
<p>for <code>popower</code>, the number of subjects in treatment group 1</p>
</td></tr>
<tr><td><code id="popower_+3A_n2">n2</code></td>
<td>
<p>for <code>popower</code>, the number of subjects in group 2</p>
</td></tr>
<tr><td><code id="popower_+3A_nsim">nsim</code></td>
<td>
<p>number of simulated studies to create by <code>simPOcuts</code></p>
</td></tr>
<tr><td><code id="popower_+3A_alpha">alpha</code></td>
<td>
<p>type I error</p>
</td></tr>
<tr><td><code id="popower_+3A_x">x</code></td>
<td>
<p>an object created by <code>popower</code> or <code>posamsize</code>, or a
vector of data values given to <code>pomodm</code> that corresponds to the
vector <code>p</code> of probabilities.  If <code>x</code> is omitted for
<code>pomodm</code>, the <code>odds.ratio</code> will be applied and the new
vector of individual probabilities will be returned.  Otherwise if
<code>x</code> is given to <code>pomodm</code>, a 2-vector with the mean and
median <code>x</code> after applying the odds ratio is returned.</p>
</td></tr>
<tr><td><code id="popower_+3A_fraction">fraction</code></td>
<td>

<p>for <code>posamsize</code>, the fraction of subjects that will be allocated to group 1
</p>
</td></tr>
<tr><td><code id="popower_+3A_power">power</code></td>
<td>

<p>for <code>posamsize</code>, the desired power (default is 0.8)
</p>
</td></tr>
<tr><td><code id="popower_+3A_formula">formula</code></td>
<td>
<p>an R formula expressure for <code>proposPO</code> where the
outcome categorical variable is on the left hand side and the grouping
variable is on the right.  It is assumed that the left hand variable is
either already a factor or will have its levels in the right order for
an ordinal model when it is converted to factor.  For
<code>multEventChart</code> the left hand variable is a categorial status
variable, the first right hand side variable represents time, and the
second right side variable is a unique subject ID.  One line is
produced per subject.</p>
</td></tr>
<tr><td><code id="popower_+3A_ref">ref</code></td>
<td>
<p>for <code>propsPO</code> specifies the reference group (value of
the right hand side <code>formula</code> variable) to use in computing
proportions on which too translate proportions in other groups, under
the proportional odds assumption.</p>
</td></tr>
<tr><td><code id="popower_+3A_data">data</code></td>
<td>
<p>a data frame or <code>data.table</code></p>
</td></tr>
<tr><td><code id="popower_+3A_labels">labels</code></td>
<td>
<p>for <code>propsTrans</code> is an optional character vector
corresponding to y=1,2,3,... that is used to construct <code>plotly</code>
hovertext as a <code>label</code> attribute in the <code>ggplot2</code>
aesthetic.  Used with y is integer on axes but you want long labels in
hovertext.</p>
</td></tr>
<tr><td><code id="popower_+3A_arrow">arrow</code></td>
<td>
<p>character to use as the arrow symbol for transitions in
<code>propsTrans.  The default is the dingbats heavy wide-headed
		rightwards arror.</code></p>
</td></tr>
<tr><td><code id="popower_+3A_nrow">nrow</code>, <code id="popower_+3A_ncol">ncol</code></td>
<td>
<p>see <code><a href="ggplot2.html#topic+facet_wrap">facet_wrap</a></code></p>
</td></tr>
<tr><td><code id="popower_+3A_maxsize">maxsize</code></td>
<td>
<p>maximum symbol size</p>
</td></tr>
<tr><td><code id="popower_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
<tr><td><code id="popower_+3A_absorb">absorb</code></td>
<td>
<p>character vector specifying the subset of levels of the
left hand side variable that are absorbing states such as death or
hospital discharge</p>
</td></tr>
<tr><td><code id="popower_+3A_sortbylast">sortbylast</code></td>
<td>
<p>set to <code>TRUE</code> to sort the subjects by the
severity of the status at the last time point</p>
</td></tr>
<tr><td><code id="popower_+3A_colortitle">colorTitle</code></td>
<td>
<p>label for legend for status</p>
</td></tr>
<tr><td><code id="popower_+3A_eventtitle">eventTitle</code></td>
<td>
<p>label for legend for <code>absorb</code></p>
</td></tr>
<tr><td><code id="popower_+3A_palette">palette</code></td>
<td>
<p>a single character string specifying the
<code><a href="ggplot2.html#topic+scale_fill_brewer">scale_fill_brewer</a></code> color palette</p>
</td></tr>
<tr><td><code id="popower_+3A_eventsymbols">eventSymbols</code></td>
<td>
<p>vector of symbol codes.  Default for first two
symbols is a solid square and an open diamond.</p>
</td></tr>
<tr><td><code id="popower_+3A_timeinc">timeInc</code></td>
<td>
<p>time increment for the x-axis.  Default is 1/2 the
shortest gap between any two distincttimes in the data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing <code>power</code>, <code>eff</code> (relative efficiency), and
<code>approx.se</code> (approximate standard error of log odds ratio) for
<code>popower</code>, or containing <code>n</code> and <code>eff</code> for <code>posamsize</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University School of Medicine
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Whitehead J (1993): Sample size calculations for ordered categorical
data.  Stat in Med 12:2257&ndash;2271.
</p>
<p>Julious SA, Campbell MJ (1996): Letter to the Editor.  Stat in Med 15:
1065&ndash;1066.  Shows accuracy of formula for binary response case.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+simRegOrd">simRegOrd</a></code>, <code><a href="#topic+bpower">bpower</a></code>, <code><a href="#topic+cpower">cpower</a></code>, <code><a href="rms.html#topic+impactPO">impactPO</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># For a study of back pain (none, mild, moderate, severe) here are the
# expected proportions (averaged over 2 treatments) that will be in
# each of the 4 categories:


p &lt;- c(.1,.2,.4,.3)
popower(p, 1.2, 1000)   # OR=1.2, total n=1000
posamsize(p, 1.2)
popower(p, 1.2, 3148)
# If p was the vector of probabilities for group 1, here's how to
# compute the average over the two groups:
# p2   &lt;- pomodm(p=p, odds.ratio=1.2)
# pavg &lt;- (p + p2) / 2

# Compare power to test for proportions for binary case,
# proportion of events in control group of 0.1
p &lt;- 0.1; or &lt;- 0.85; n &lt;- 4000
popower(c(1 - p, p), or, n)    # 0.338
bpower(p, odds.ratio=or, n=n)  # 0.320
# Add more categories, starting with 0.1 in middle
p &lt;- c(.8, .1, .1)
popower(p, or, n)   # 0.543
p &lt;- c(.7, .1, .1, .1)
popower(p, or, n)   # 0.67
# Continuous scale with final level have prob. 0.1
p &lt;- c(rep(1 / n, 0.9 * n), 0.1)
popower(p, or, n)   # 0.843

# Compute the mean and median x after shifting the probability
# distribution by an odds ratio under the proportional odds model
x &lt;- 1 : 5
p &lt;- c(.05, .2, .2, .3, .25)
# For comparison make up a sample that looks like this
X &lt;- rep(1 : 5, 20 * p)
c(mean=mean(X), median=median(X))
pomodm(x, p, odds.ratio=1)  # still have to figure out the right median
pomodm(x, p, odds.ratio=0.5)

# Show variation of odds ratios over possible cutoffs of Y even when PO
# truly holds.  Run 5 simulations for a total sample size of 300.
# The two groups have 150 subjects each.
s &lt;- simPOcuts(300, nsim=5, odds.ratio=2, p=p)
round(s, 2)

# An ordinal outcome with levels a, b, c, d, e is measured at 3 times
# Show the proportion of values in each outcome category stratified by
# time.  Then compute what the proportions would be had the proportions
# at times 2 and 3 been the proportions at time 1 modified by two odds ratios 

set.seed(1)
d   &lt;- expand.grid(time=1:3, reps=1:30)
d$y &lt;- sample(letters[1:5], nrow(d), replace=TRUE)
propsPO(y ~ time, data=d, odds.ratio=function(time) c(1, 2, 4)[time])
# To show with plotly, save previous result as object p and then:
# plotly::ggplotly(p, tooltip='label')

# Add a stratification variable and don't consider an odds ratio
d   &lt;- expand.grid(time=1:5, sex=c('female', 'male'), reps=1:30)
d$y &lt;- sample(letters[1:5], nrow(d), replace=TRUE)
propsPO(y ~ time + sex, data=d)  # may add nrow= or ncol=

# Show all successive transition proportion matrices
d   &lt;- expand.grid(id=1:30, time=1:10)
d$state &lt;- sample(LETTERS[1:4], nrow(d), replace=TRUE)
propsTrans(state ~ time + id, data=d)

pt1 &lt;- data.frame(pt=1, day=0:3,
   status=c('well', 'well', 'sick', 'very sick'))
pt2 &lt;- data.frame(pt=2, day=c(1,2,4,6),
   status=c('sick', 'very sick', 'coma', 'death'))
pt3 &lt;- data.frame(pt=3, day=1:5,
   status=c('sick', 'very sick', 'sick', 'very sick', 'discharged'))
pt4 &lt;- data.frame(pt=4, day=c(1:4, 10),
   status=c('well', 'sick', 'very sick', 'well', 'discharged'))
d &lt;- rbind(pt1, pt2, pt3, pt4)
d$status &lt;- factor(d$status, c('discharged', 'well', 'sick',
                               'very sick', 'coma', 'death'))
label(d$day) &lt;- 'Day'
require(ggplot2)
multEventChart(status ~ day + pt, data=d,
               absorb=c('death', 'discharged'),
               colorTitle='Status', sortbylast=TRUE) +
               theme_classic() +
               theme(legend.position='bottom')
</code></pre>

<hr>
<h2 id='princmp'>princmp</h2><span id='topic+princmp'></span>

<h3>Description</h3>

<p>Enhanced Output for Principal and Sparse Principal Components
</p>


<h3>Usage</h3>

<pre><code class='language-R'>princmp(
  formula,
  data = environment(formula),
  method = c("regular", "sparse"),
  k = min(5, p - 1),
  kapprox = min(5, k),
  cor = TRUE,
  sw = FALSE,
  nvmax = 5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="princmp_+3A_formula">formula</code></td>
<td>
<p>a formula with no left hand side, or a numeric matrix</p>
</td></tr>
<tr><td><code id="princmp_+3A_data">data</code></td>
<td>
<p>a data frame or table.  By default variables come from the calling environment.</p>
</td></tr>
<tr><td><code id="princmp_+3A_method">method</code></td>
<td>
<p>specifies whether to use regular or sparse principal components are computed</p>
</td></tr>
<tr><td><code id="princmp_+3A_k">k</code></td>
<td>
<p>the number of components to plot, display, and return</p>
</td></tr>
<tr><td><code id="princmp_+3A_kapprox">kapprox</code></td>
<td>
<p>the number of components to approximate with stepwise regression when <code>sw=TRUE</code></p>
</td></tr>
<tr><td><code id="princmp_+3A_cor">cor</code></td>
<td>
<p>set to <code>FALSE</code> to compute PCs on the original data scale, which is useful if all variables have the same units of measurement</p>
</td></tr>
<tr><td><code id="princmp_+3A_sw">sw</code></td>
<td>
<p>set to <code>TRUE</code> to run stepwise regression PC prediction/approximation</p>
</td></tr>
<tr><td><code id="princmp_+3A_nvmax">nvmax</code></td>
<td>
<p>maximum number of predictors to allow in stepwise regression PC approximations</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Expands any categorical predictors into indicator variables, and calls <code>princomp</code> (if <code>method='regular'</code> (the default)) or <code>sPCAgrid</code> in the <code>pcaPP</code> package (<code>method='sparse'</code>) to compute lasso-penalized sparse principal components.  By default all variables are first scaled by their standard deviation after observations with any <code>NA</code>s on any variables in <code>formula</code> are removed.  Loadings of standardized variables, and if <code>orig=TRUE</code> loadings on the original data scale are printed.  If <code>pl=TRUE</code> a scree plot is drawn with text added to indicate cumulative proportions of variance explained.  If <code>sw=TRUE</code>, the <code>leaps</code> package <code>regsubsets</code> function is used to approximate the PCs using forward stepwise regression with the original variables as individual predictors.
</p>
<p>A <code>print</code> method prints the results and a <code>plot</code> method plots the scree plot of variance explained.
</p>


<h3>Value</h3>

<p>a list of class <code>princmp</code> with elements <code>scores</code>, a k-column matrix with principal component scores, with <code>NA</code>s when the input data had an <code>NA</code>, and other components useful for printing and plotting.  If <code>k=1</code> <code>scores</code> is a vector.  Other components include <code>vars</code> (vector of variances explained), <code>method</code>, <code>k</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

<hr>
<h2 id='print.char.list'> prints a list of lists in a visually readable format. </h2><span id='topic+print.char.list'></span>

<h3>Description</h3>

<p>Takes a list that is composed of other lists and matrixes and prints
it in a visually readable format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'char.list'
print(x, ..., hsep = c("|"), vsep = c("-"), csep = c("+"), print.it = TRUE,
                rowname.halign = c("left", "centre", "right"),
                rowname.valign = c("top", "centre", "bottom"),
                colname.halign = c("centre", "left", "right"),
                colname.valign = c("centre", "top", "bottom"),
                text.halign = c("right", "centre", "left"),
                text.valign = c("top", "centre", "bottom"),
                rowname.width, rowname.height,
                min.colwidth = .Options$digits, max.rowheight = NULL,
                abbreviate.dimnames = TRUE, page.width = .Options$width,
                colname.width, colname.height, prefix.width,
                superprefix.width = prefix.width)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.char.list_+3A_x">x</code></td>
<td>

<p>list object to be printed
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_...">...</code></td>
<td>

<p>place for extra arguments to reside.
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_hsep">hsep</code></td>
<td>

<p>character used to separate horizontal fields
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_vsep">vsep</code></td>
<td>

<p>character used to separate veritcal feilds
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_csep">csep</code></td>
<td>

<p>character used where horizontal and veritcal separators meet.
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_print.it">print.it</code></td>
<td>

<p>should the value be printed to the console or returned as a string.
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_rowname.halign">rowname.halign</code></td>
<td>

<p>horizontal justification of row names.
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_rowname.valign">rowname.valign</code></td>
<td>

<p>verical justification of row names.
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_colname.halign">colname.halign</code></td>
<td>

<p>horizontal justification of column names.
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_colname.valign">colname.valign</code></td>
<td>

<p>verical justification of column names.
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_text.halign">text.halign</code></td>
<td>

<p>horizontal justification of cell text.
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_text.valign">text.valign</code></td>
<td>

<p>vertical justification of cell text.
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_rowname.width">rowname.width</code></td>
<td>

<p>minimum width of row name strings.
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_rowname.height">rowname.height</code></td>
<td>

<p>minimum height of row name strings.
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_min.colwidth">min.colwidth</code></td>
<td>

<p>minimum column width.
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_max.rowheight">max.rowheight</code></td>
<td>

<p>maximum row height.
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_abbreviate.dimnames">abbreviate.dimnames</code></td>
<td>

<p>should the row and column names be abbreviated.
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_page.width">page.width</code></td>
<td>

<p>width of the page being printed on.
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_colname.width">colname.width</code></td>
<td>

<p>minimum width of the column names.
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_colname.height">colname.height</code></td>
<td>

<p>minimum height of the column names
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_prefix.width">prefix.width</code></td>
<td>

<p>maximum width of the rowname columns
</p>
</td></tr>
<tr><td><code id="print.char.list_+3A_superprefix.width">superprefix.width</code></td>
<td>

<p>maximum width of the super rowname columns
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>String that formated table of the list object.
</p>


<h3>Author(s)</h3>

<p> Charles Dupont </p>

<hr>
<h2 id='print.char.matrix'> Function to print a matrix with stacked cells </h2><span id='topic+print.char.matrix'></span>

<h3>Description</h3>

<p>Prints a dataframe or matrix in stacked cells.  Line break charcters
in a matrix element will result in a line break in that cell, but tab
characters are not supported.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'char.matrix'
print(x, file = "", col.name.align = "cen", col.txt.align = "right", 
    cell.align = "cen", hsep = "|", vsep = "-", csep = "+", row.names = TRUE, 
    col.names = FALSE, append = FALSE,
    top.border = TRUE, left.border = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.char.matrix_+3A_x">x</code></td>
<td>
<p>a matrix or dataframe</p>
</td></tr>
<tr><td><code id="print.char.matrix_+3A_file">file</code></td>
<td>
<p>name of file if file output is desired.  If left empty,
output will be to the screen</p>
</td></tr>
<tr><td><code id="print.char.matrix_+3A_col.name.align">col.name.align</code></td>
<td>
<p>if column names are used, they can be aligned
right, left or centre. Default <code>"cen"</code> results in names centred
between the sides of the columns they name. If the width of the text
in the columns is less than the width of the name, <code>col.name.align</code>
will have no effect. Other options are <code>"right"</code> and <code>"left"</code>.</p>
</td></tr>
<tr><td><code id="print.char.matrix_+3A_col.txt.align">col.txt.align</code></td>
<td>
<p>how character columns are aligned.  Options
are the same as for <code>col.name.align</code> with no effect when the width of
the column is greater than its name.</p>
</td></tr>
<tr><td><code id="print.char.matrix_+3A_cell.align">cell.align</code></td>
<td>
<p>how numbers are displayed in columns</p>
</td></tr>
<tr><td><code id="print.char.matrix_+3A_hsep">hsep</code></td>
<td>
<p>character string to use as horizontal separator,
i.e. what separates columns</p>
</td></tr>
<tr><td><code id="print.char.matrix_+3A_vsep">vsep</code></td>
<td>
<p>character string to use as vertical separator,
i.e. what separates rows.  Length cannot be more than one.</p>
</td></tr>
<tr><td><code id="print.char.matrix_+3A_csep">csep</code></td>
<td>
<p>character string to use where vertical and horizontal
separators cross.  If <code>hsep</code> is more than one character,
<code>csep</code> will need to be the same length.  There is no provision
for multiple vertical separators</p>
</td></tr>
<tr><td><code id="print.char.matrix_+3A_row.names">row.names</code></td>
<td>
<p>logical: are we printing the names of the rows?</p>
</td></tr>
<tr><td><code id="print.char.matrix_+3A_col.names">col.names</code></td>
<td>
<p>logical: are we printing the names of the columns?</p>
</td></tr>
<tr><td><code id="print.char.matrix_+3A_append">append</code></td>
<td>
<p>logical: if <code>file</code> is not <code>""</code>, are we appending to
the file or overwriting?</p>
</td></tr>
<tr><td><code id="print.char.matrix_+3A_top.border">top.border</code></td>
<td>
<p>logical: do we want a border along the top above the
columns?</p>
</td></tr>
<tr><td><code id="print.char.matrix_+3A_left.border">left.border</code></td>
<td>
<p>logical: do we want a border along the left of the
first column?</p>
</td></tr>
<tr><td><code id="print.char.matrix_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If any column of <code>x</code> is a mixture of character and numeric, the
distinction between character and numeric columns will be lost. This
is especially so if the matrix is of a form where you would not want
to print the column names, the column information being in the rows at
the beginning of the matrix.
</p>
<p>Row names, if not specified in the making of the matrix will simply be
numbers. To prevent printing them, set <code>row.names = FALSE</code>.</p>


<h3>Value</h3>

<p>No value is returned.  The matrix or dataframe will be printed to file
or to the screen.
</p>


<h3>Author(s)</h3>

<p>Patrick Connolly <a href="mailto:p.connolly@hortresearch.co.nz">p.connolly@hortresearch.co.nz</a></p>


<h3>See Also</h3>

<p><code>write</code>,  <code>write.table</code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(HairEyeColor)
print.char.matrix(HairEyeColor[ , , "Male"], col.names = TRUE)
print.char.matrix(HairEyeColor[ , , "Female"], col.txt.align = "left", col.names = TRUE)


z &lt;- rbind(c("", "N", "y"),
           c("[ 1.34,40.3)\n[40.30,48.5)\n[48.49,58.4)\n[58.44,87.8]",
             " 50\n 50\n 50\n 50",
             "0.530\n0.489\n0.514\n0.507"),
           c("female\nmale", " 94\n106", "0.552\n0.473"  ),
           c("", "200", "0.510"))
dimnames(z) &lt;- list(c("", "age", "sex", "Overall"),NULL)

print.char.matrix(z)
</code></pre>

<hr>
<h2 id='print.princmp'>print.princmp</h2><span id='topic+print.princmp'></span>

<h3>Description</h3>

<p>Print Results of princmp
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'princmp'
print(x, which = c("none", "standardized", "original", "both"), k = x$k, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.princmp_+3A_x">x</code></td>
<td>
<p>results of <code>princmp</code></p>
</td></tr>
<tr><td><code id="print.princmp_+3A_which">which</code></td>
<td>
<p>specifies which loadings to print, the default being <code>'none'</code> and other values being <code>'standardized'</code>, <code>'original'</code>, or <code>'both'</code></p>
</td></tr>
<tr><td><code id="print.princmp_+3A_k">k</code></td>
<td>
<p>number of components to show, defaults to <code>k</code> specified to <code>princmp</code></p>
</td></tr>
<tr><td><code id="print.princmp_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Simple print method for <code><a href="#topic+princmp">princmp()</a></code>
</p>


<h3>Value</h3>

<p>nothing
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

<hr>
<h2 id='printL'>printL</h2><span id='topic+printL'></span>

<h3>Description</h3>

<p>Print an object or a named list of objects.  When multiple objects are given, their names are printed before their contents.  When an object is a vector that is not longer than <code>maxoneline</code> and its elements are not named, all the elements will be printed on one line separated by commas.  When <code>dec</code> is given, numeric vectors or numeric columns of data frames or data tables are rounded to the nearest <code>dec</code> before printing.  This function is especially helpful when printing objects in a Quarto or RMarkdown document and the code is not currently being shown to place the output in context.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>printL(..., dec = NULL, maxoneline = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="printL_+3A_...">...</code></td>
<td>
<p>any number of objects to <code>print()</code></p>
</td></tr>
<tr><td><code id="printL_+3A_dec">dec</code></td>
<td>
<p>optional decimal places to the right of the decimal point for rounding</p>
</td></tr>
<tr><td><code id="printL_+3A_maxoneline">maxoneline</code></td>
<td>
<p>controls how many elements may be printed on a single line for <code>vector</code> objects</p>
</td></tr>
</table>


<h3>Value</h3>

<p>nothing
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prn">prn()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>w &lt;- pi + 1 : 2
printL(w=w)
printL(w, dec=3)
printL('this is it'=c(pi, pi, 1, 2),
       yyy=pi,
       z=data.frame(x=pi+1:2, y=3:4, z=c('a', 'b')),
       qq=1:10,
       dec=4)
       
</code></pre>

<hr>
<h2 id='prnz'>
Print and Object with its Name
</h2><span id='topic+prn'></span>

<h3>Description</h3>

<p>Prints an object with its name and with an optional descriptive
text string.  This is useful for annotating analysis output files and
for debugging.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prn(x, txt, file)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prnz_+3A_x">x</code></td>
<td>
<p>any object</p>
</td></tr>
<tr><td><code id="prnz_+3A_txt">txt</code></td>
<td>
<p>optional text string</p>
</td></tr>
<tr><td><code id="prnz_+3A_file">file</code></td>
<td>
<p>optional file name.  By default, writes to console.
<code>append=TRUE</code> is assumed.</p>
</td></tr>
</table>


<h3>Side Effects</h3>

<p>prints
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+print">print</a></code>, <code><a href="base.html#topic+cat">cat</a></code>, <code><a href="#topic+printL">printL</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 1:5
prn(x)
# prn(fit, 'Full Model Fit')
</code></pre>

<hr>
<h2 id='prselect'>Selectively Print Lines of a Text Vector</h2><span id='topic+prselect'></span>

<h3>Description</h3>

<p>Given one or two regular expressions or exact text matches, removes
elements of the input vector that match these specifications. Omitted
lines are replaced by ....  This is useful for selectively
suppressing some of the printed output of R functions such as
regression fitting functions, especially in the context of making
statistical reports using Sweave or Odfweave.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prselect(x, start = NULL, stop = NULL, i = 0, j = 0, pr = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prselect_+3A_x">x</code></td>
<td>

<p>input character vector
</p>
</td></tr>
<tr><td><code id="prselect_+3A_start">start</code></td>
<td>

<p>text or regular expression to look for starting line to omit.  If
omitted, deletions start at the first line.
</p>
</td></tr>
<tr><td><code id="prselect_+3A_stop">stop</code></td>
<td>

<p>text or regular expression to look for ending line to omit. If
omitted, deletions proceed until the last line.
</p>
</td></tr>
<tr><td><code id="prselect_+3A_i">i</code></td>
<td>

<p>increment in number of first line to delete after match is found
</p>
</td></tr>
<tr><td><code id="prselect_+3A_j">j</code></td>
<td>

<p>increment in number of last line to delete after match is found
</p>
</td></tr>
<tr><td><code id="prselect_+3A_pr">pr</code></td>
<td>

<p>set to <code>FALSE</code> to suppress printing
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an invisible vector of retained lines of text</p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="utils.html#topic+Sweave">Sweave</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c('the','cat','ran','past','the','dog')
prselect(x, 'big','bad')     # omit nothing- no match
prselect(x, 'the','past')    # omit first 4 lines
prselect(x,'the','junk')     # omit nothing- no match for stop
prselect(x,'ran','dog')      # omit last 4 lines
prselect(x,'cat')            # omit lines 2-
prselect(x,'cat',i=1)        # omit lines 3-
prselect(x,'cat','past')     # omit lines 2-4
prselect(x,'cat','past',j=1) # omit lines 2-5
prselect(x,'cat','past',j=-1)# omit lines 2-3
prselect(x,'t$','dog')       # omit lines 2-6; t must be at end

# Example for Sweave: run a regression analysis with the rms package
# then selectively output only a portion of what print.ols prints.
# (Thanks to \email{romain.francois@dbmail.com})
# &lt;&lt;z,eval=FALSE,echo=T&gt;&gt;=
# library(rms)
# y &lt;- rnorm(20); x1 &lt;- rnorm(20); x2 &lt;- rnorm(20)
# ols(y ~ x1 + x2)
# &lt;&lt;echo=F&gt;&gt;=
# z &lt;- capture.output( {
# &lt;&lt;z&gt;&gt;
#    } )
# prselect(z, 'Residuals:') # keep only summary stats; or:
# prselect(z, stop='Coefficients', j=-1)  # keep coefficients, rmse, R^2; or:
# prselect(z, 'Coefficients', 'Residual standard error', j=-1) # omit coef
# @
</code></pre>

<hr>
<h2 id='pstamp'>Date/Time/Directory Stamp the Current Plot</h2><span id='topic+pstamp'></span>

<h3>Description</h3>

<p>Date-time stamp the current plot in the extreme lower right
corner. Optionally add the current working directory and arbitrary other
text to the stamp.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pstamp(txt, pwd = FALSE, time. = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pstamp_+3A_txt">txt</code></td>
<td>
<p>an optional single text string</p>
</td></tr>
<tr><td><code id="pstamp_+3A_pwd">pwd</code></td>
<td>
<p>set to <code>TRUE</code> to add the current working directory
name to the stamp</p>
</td></tr>
<tr><td><code id="pstamp_+3A_time.">time.</code></td>
<td>
<p>set to <code>FALSE</code> to use the date without the time</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Certain functions are not supported for S-Plus under Windows.  For <span class="rlang"><b>R</b></span>,
results may not be satisfactory if <code>par(mfrow=)</code> is in effect.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>Examples</h3>

<pre><code class='language-R'>plot(1:20)
pstamp(pwd=TRUE, time=FALSE)
</code></pre>

<hr>
<h2 id='qcrypt'>qcrypt</h2><span id='topic+qcrypt'></span>

<h3>Description</h3>

<p>Store and Encrypt R Objects or Files or Read and Decrypt Them
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qcrypt(obj, base, service = "R-keyring-service", file)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qcrypt_+3A_obj">obj</code></td>
<td>
<p>an R object to write to disk and encrypt (if <code>base</code> is specified) or the base file name to read and uncrypted (if <code>base</code> is not specified).  Not used when <code>file</code> is given.</p>
</td></tr>
<tr><td><code id="qcrypt_+3A_base">base</code></td>
<td>
<p>base file name when creating a file.  Not used when <code>file</code> is given.</p>
</td></tr>
<tr><td><code id="qcrypt_+3A_service">service</code></td>
<td>
<p>a fairly arbitrary <code>keyring</code> service name.  The default is almost always OK unless you need to use different passwords for different files.</p>
</td></tr>
<tr><td><code id="qcrypt_+3A_file">file</code></td>
<td>
<p>full name of file to encrypt or decrypt</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>qcrypt</code> is used to protect sensitive information on a user's computer or when transmitting a copy of the file to another R user.  Unencrypted information only exists for a moment, and the encryption password does not appear in the user's script but instead is managed by the <code>keyring</code> package to remember the password across R sessions, and the <code>getPass</code> package, which pops up a password entry window and does not allow the password to be visible.  The password is requested only once, except perhaps when the user logs out of their operating system session or reboots.
</p>
<p>The keyring can be bypassed and the password entered in a popup window by specifying <code>service=NA</code>.  This is the preferred approach when sending an encrypted file to a user on a different computer.
</p>
<p><code>qcrypt</code> writes R objects to disk in a temporary file using the <code>qs</code> package <code>qsave</code> function.  The file is quickly encrypted using the <code>safer</code> package, and the temporary unencrypted <code>qs</code> file is deleted.  When reading an encrypted file the process is reversed.
</p>
<p>To save an object in an encrypted file, specify the object as the first argument <code>obj</code> and specify a base file name as a character string in the second argument <code>base</code>.  The full <code>qs</code> file name will be of the form <code>base.qs.encrypted</code> in the user's current working directory.  To unencrypt the file into a short-lived temporary file and use <code>qs::qread</code> to read it, specify the base file name as a character string with the first argument, and do not specify the <code>base</code> argument.
</p>
<p>Alternatively, <code>qcrypt</code> can be used to encrypt or decrypt existing files of any type using the same password and keyring mechanism.  The former is done by specifying <code>file</code> that does not end in <code>'.encrypted'</code> and the latter is done by ending <code>file</code> with <code>'.encrypted'</code>.  When <code>file</code> does not contain a path it is assumed to be in the current working directory.  When a file is encrypted the original file is removed.  Files are decrypted into a temporary directory created by <code>tempdir()</code>, with the name of the file being the value of <code>file</code> with <code>'.encrypted'</code> removed.
</p>
<p>Interactive password provision works when running <code>R</code>, <code>Rscript</code>, <code>RStudio</code>, or <code>Quarto</code> but does not work when running <code style="white-space: pre;">&#8288;R CMD BATCH&#8288;</code>.  <code>getPass</code> fails under <code>RStudio</code> on Macs.
</p>
<p>See <a href="https://hbiostat.org/rflow/fcreate.html#sec-fcreate-secure">R Workflow</a> for more information.
</p>


<h3>Value</h3>

<p>(invisibly) the full encrypted file name if writing the file, or the restored R object if reading the file.  When decrypting a general file with <code style="white-space: pre;">&#8288;file=&#8288;</code>, the returned value is the full path to a temporary file containing the decrypted data.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Suppose x is a data.table or data.frame
# The first time qcrypt is run with a service a password will
# be requested.  It will be remembered across sessions thanks to
# the keyring package
qcrypt(x, 'x')   # creates x.qs.encrypted in current working directory
x &lt;- qcrypt('x') # unencrypts x.qs.encrypted into a temporary
                 # directory, uses qs::qread to read it, and
                 # stores the result in x
# Encrypt a general file using a different password
qcrypt(file='report.pdf', service='pdfkey')
# Decrypt that file
fi &lt;- qcrypt(file='report.pdf.encrypted', service='pdfkey')
fi contains the full unencrypted file name which is in a temporary directory
# Encrypt without using a keyring
qcrypt(x, 'x', service=NA)
x &lt;- qcrypt('x', service=NA)

## End(Not run)
</code></pre>

<hr>
<h2 id='r2describe'>r2describe</h2><span id='topic+r2describe'></span>

<h3>Description</h3>

<p>Summarize Strength of Relationships Using R-Squared From Linear Regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2describe(x, nvmax = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r2describe_+3A_x">x</code></td>
<td>
<p>numeric matrix with 2 or more columns</p>
</td></tr>
<tr><td><code id="r2describe_+3A_nvmax">nvmax</code></td>
<td>
<p>maxmum number of columns of x to use in predicting a given column</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function to use <code>leaps::regsubsets()</code> to briefly describe which variables more strongly predict another variable.  Variables are in a numeric matrix and are assumed to be transformed so that relationships are linear (e.g., using <code>redun()</code> or <code>transcan()</code>.)
</p>


<h3>Value</h3>

<p>nothing
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
r &lt;- redun(...)
r2describe(r$scores)

## End(Not run)
</code></pre>

<hr>
<h2 id='R2Measures'>R2Measures</h2><span id='topic+R2Measures'></span>

<h3>Description</h3>

<p>Generalized R^2 Measures
</p>


<h3>Usage</h3>

<pre><code class='language-R'>R2Measures(lr, p, n, ess = NULL, padj = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="R2Measures_+3A_lr">lr</code></td>
<td>
<p>likelihoood ratio chi-square statistic</p>
</td></tr>
<tr><td><code id="R2Measures_+3A_p">p</code></td>
<td>
<p>number of non-intercepts in the model that achieved <code>lr</code></p>
</td></tr>
<tr><td><code id="R2Measures_+3A_n">n</code></td>
<td>
<p>raw number of observations</p>
</td></tr>
<tr><td><code id="R2Measures_+3A_ess">ess</code></td>
<td>
<p>if a single number, is the effective sample size.  If a vector of numbers is assumed to be the frequency tabulation of all distinct values of the outcome variable, from which the effective sample size is computed.</p>
</td></tr>
<tr><td><code id="R2Measures_+3A_padj">padj</code></td>
<td>
<p>set to 2 to use the classical adjusted R^2 penalty, 1 (the default) to subtract <code>p</code> from <code>lr</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes various generalized R^2 measures related to the Maddala-Cox-Snell (MCS) R^2 for regression models fitted with maximum likelihood.  The original MCS R^2 is labeled <code>R2</code> in the result.  This measure uses the raw sample size <code>n</code> and does not penalize for the number of free parameters, so it can be rewarded for overfitting.  A measure adjusted for the number of fitted regression coefficients <code>p</code> uses the analogy to R^2 in linear models by computing <code>1 - exp(- lr / n) * (n-1)/(n-p-1)</code> if <code>padj=2</code>, which is approximately <code>1 - exp(- (lr - p) / n)</code>, the version used if <code>padj=1</code> (the default).  The latter measure is appealing because the expected value of the likelihood ratio chi-square statistic <code>lr</code> is <code>p</code> under the global null hypothesis of no predictors being associated with the response variable.  See <a href="https://hbiostat.org/bib/r2.html">https://hbiostat.org/bib/r2.html</a> for more details.
</p>
<p>It is well known that in logistic regression the MCS R^2 cannot achieve a value of 1.0 even with a perfect model, which prompted Nagelkerke to divide the R^2 measure by its maximum attainable value.  This is not necessarily the best recalibration of R^2 throughout its range.  An alternative is to use the formulas above but to replace the raw sample size <code>n</code> with the effective sample size, which for data with many ties can be significantly lower than the number of observations.  As used in the <code>popower()</code> and <code>describe()</code> functions, in the context of a Wilcoxon test or the proportional odds model, the effective sample size is <code>n * (1 - f)</code> where <code>f</code> is the sums of cubes of the proportion of observations at each distict value of the response variable.  Whitehead derived this from an approximation to the variance of a log odds ratio in a proportional odds model.  To obtain R^2 measures using the effective sample size, either provide <code>ess</code> as a single number specifying the effective sample size, or specify a vector of frequencies of distinct Y values from which the effective sample size will be computed.  In the context of survival analysis, the single number effective sample size you may wish to specify is the number of uncensored observations.  This is exactly correct when estimating the hazard rate from a simple exponential distribution or when using the Cox PH/log-rank test.  For failure time distributions with a very high early hazard, censored observations contain enough information that the effective sample size is greater than the number of events.  See Benedetti et al, 1982.
</p>
<p>If the effective sample size equals the raw sample size, measures involving the effective sample size are set to <code>NA</code>.
</p>


<h3>Value</h3>

<p>named vector of R2 measures.  The notation for results is <code>R^2(p, n)</code> where the <code>p</code> component is empty for unadjusted estimates and <code>n</code> is the sample size used (actual sample size for first measures, effective sample size for remaining ones).  For indexes that are not adjusted, only <code>n</code> appears.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>References</h3>

<p>Smith TJ and McKenna CM (2013): A comparison of logistic regression pseudo R^2 indices.  Multiple Linear Regression Viewpoints 39:17-26.  <a href="https://www.glmj.org/archives/articles/Smith_v39n2.pdf">https://www.glmj.org/archives/articles/Smith_v39n2.pdf</a>
</p>
<p>Benedetti JK, et al (1982): Effective sample size for tests of censored survival data.  Biometrika 69:343&ndash;349.
</p>
<p>Mittlbock M, Schemper M (1996): Explained variation for logistic regression.  Stat in Med 15:1987-1997.
</p>
<p>Date, S: R-squared, adjusted R-squared and pseudo R-squared. <a href="https://timeseriesreasoning.com/contents/r-squared-adjusted-r-squared-pseudo-r-squared/">https://timeseriesreasoning.com/contents/r-squared-adjusted-r-squared-pseudo-r-squared/</a>
</p>
<p>UCLA: What are pseudo R-squareds? <a href="https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/">https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/</a>
</p>
<p>Allison P (2013): What's the beset R-squared for logistic regression? <a href="https://statisticalhorizons.com/r2logistic/">https://statisticalhorizons.com/r2logistic/</a>
</p>
<p>Menard S (2000): Coefficients of determination for multiple logistic regression analysis.  The Am Statistician 54:17-24.
</p>
<p>Whitehead J (1993): Sample size calculations for ordered categorical data.  Stat in Med 12:2257-2271.  See errata (1994) 13:871 and letter to the editor by Julious SA, Campbell MJ (1996) 15:1065-1066 showing that for 2-category Y the Whitehead sample size formula agrees closely with the usual formula for comparing two proportions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(rep(0, 50), rep(1, 50))
y &lt;- x
# f &lt;- lrm(y ~ x)
# f   # Nagelkerke R^2=1.0
# lr &lt;- f$stats['Model L.R.']
# 1 - exp(- lr / 100)  # Maddala-Cox-Snell (MCS) 0.75
lr &lt;- 138.6267  # manually so don't need rms package

R2Measures(lr, 1, 100, c(50, 50))  # 0.84 Effective n=75
R2Measures(lr, 1, 100, 50)         # 0.94
# MCS requires unreasonable effective sample size = minimum outcome
# frequency to get close to the 1.0 that Nagelkerke R^2 achieves
</code></pre>

<hr>
<h2 id='rcorr'>Matrix of Correlations and P-values</h2><span id='topic+rcorr'></span><span id='topic+print.rcorr'></span>

<h3>Description</h3>

<p><code>rcorr</code> Computes a matrix of Pearson's <code>r</code> or Spearman's
<code>rho</code> rank correlation coefficients for all possible pairs of
columns of a matrix.  Missing values are deleted in pairs rather than
deleting all rows of <code>x</code> having any missing variables.  Ranks are
computed using efficient algorithms (see reference 2), using midranks
for ties.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rcorr(x, y, type=c("pearson","spearman"))

## S3 method for class 'rcorr'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rcorr_+3A_x">x</code></td>
<td>

<p>a numeric matrix with at least 5 rows and at least 2 columns (if
<code>y</code> is absent).  For <code>print</code>, <code>x</code> is an object
produced by <code>rcorr</code>.
</p>
</td></tr>
<tr><td><code id="rcorr_+3A_y">y</code></td>
<td>

<p>a numeric vector or matrix which will be concatenated to <code>x</code>.  If
<code>y</code> is omitted for <code>rcorr</code>, <code>x</code> must be a matrix.
</p>
</td></tr>
<tr><td><code id="rcorr_+3A_type">type</code></td>
<td>

<p>specifies the type of correlations to compute.  Spearman correlations
are the Pearson linear correlations computed on the ranks of non-missing
elements, using midranks for ties.
</p>
</td></tr>
<tr><td><code id="rcorr_+3A_...">...</code></td>
<td>
<p>argument for method compatiblity.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses midranks in case of ties, as described by Hollander and Wolfe.
P-values are approximated by using the <code>t</code> or <code>F</code> distributions.
</p>


<h3>Value</h3>

<p><code>rcorr</code> returns a list with elements <code>r</code>, the
matrix of correlations, <code>n</code> the
matrix of number of observations used in analyzing each pair of variables,
and <code>P</code>, the asymptotic P-values.
Pairs with fewer than 2 non-missing values have the r values set to NA.
The diagonals of <code>n</code> are the number of non-NAs for the single variable
corresponding to that row and column.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Hollander M. and Wolfe D.A. (1973).  Nonparametric Statistical Methods.
New York: Wiley.
</p>
<p>Press WH, Flannery BP, Teukolsky SA, Vetterling, WT (1988): Numerical
Recipes in C.  Cambridge: Cambridge University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hoeffd">hoeffd</a></code>, <code><a href="stats.html#topic+cor">cor</a></code>, <code><a href="#topic+combine.levels">combine.levels</a></code>,
<code><a href="#topic+varclus">varclus</a></code>, <code><a href="#topic+dotchart3">dotchart3</a></code>, <code><a href="#topic+impute">impute</a></code>,
<code><a href="stats.html#topic+chisq.test">chisq.test</a></code>, <code><a href="#topic+cut2">cut2</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(-2, -1, 0, 1, 2)
y &lt;- c(4,   1, 0, 1, 4)
z &lt;- c(1,   2, 3, 4, NA)
v &lt;- c(1,   2, 3, 4, 5)
rcorr(cbind(x,y,z,v))
</code></pre>

<hr>
<h2 id='rcorr.cens'>
Rank Correlation for Censored Data
</h2><span id='topic+rcorr.cens'></span><span id='topic+rcorrcens'></span><span id='topic+rcorrcens.formula'></span>

<h3>Description</h3>

<p>Computes the c index and the corresponding
generalization of Somers' Dxy rank correlation for a censored response
variable. Also works for uncensored and binary responses, 
although its use of all possible pairings
makes it slow for this purpose.  Dxy and c are related by
<code class="reqn">Dxy=2(c-0.5)</code>.
</p>
<p><code>rcorr.cens</code> handles one predictor variable.  <code>rcorrcens</code>
computes rank correlation measures separately by a series of
predictors.  In addition, <code>rcorrcens</code> has a rough way of handling
categorical predictors.  If a categorical (factor) predictor has two
levels, it is coverted to a numeric having values 1 and 2.  If it has
more than 2 levels, an indicator variable is formed for the most
frequently level vs. all others, and another indicator for the second
most frequent level and all others.  The correlation is taken as the
maximum of the two (in absolute value).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rcorr.cens(x, S, outx=FALSE)

## S3 method for class 'formula'
rcorrcens(formula, data=NULL, subset=NULL,
          na.action=na.retain, exclude.imputed=TRUE, outx=FALSE,
          ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rcorr.cens_+3A_x">x</code></td>
<td>

<p>a numeric predictor variable
</p>
</td></tr>
<tr><td><code id="rcorr.cens_+3A_s">S</code></td>
<td>

<p>an <code>Surv</code> object or a vector.  If a vector, assumes that every
observation is uncensored.
</p>
</td></tr>
<tr><td><code id="rcorr.cens_+3A_outx">outx</code></td>
<td>

<p>set to <code>TRUE</code> to not count pairs of observations tied on <code>x</code> as a
relevant pair.  This results in a Goodman&ndash;Kruskal gamma type rank
correlation.
</p>
</td></tr>
<tr><td><code id="rcorr.cens_+3A_formula">formula</code></td>
<td>

<p>a formula with a <code>Surv</code> object or a numeric vector
on the left-hand side
</p>
</td></tr>
<tr><td><code id="rcorr.cens_+3A_data">data</code>, <code id="rcorr.cens_+3A_subset">subset</code>, <code id="rcorr.cens_+3A_na.action">na.action</code></td>
<td>

<p>the usual options for models.  Default for <code>na.action</code> is to retain
all values, NA or not, so that NAs can be deleted in only a pairwise
fashion.
</p>
</td></tr>
<tr><td><code id="rcorr.cens_+3A_exclude.imputed">exclude.imputed</code></td>
<td>

<p>set to <code>FALSE</code> to include imputed values (created by
<code>impute</code>) in the calculations.
</p>
</td></tr>
<tr><td><code id="rcorr.cens_+3A_...">...</code></td>
<td>

<p>extra arguments passed to <code><a href="#topic+biVar">biVar</a></code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>rcorr.cens</code> returns a vector with the following named elements:
<code>C Index</code>, <code>Dxy</code>, <code>S.D.</code>, <code>n</code>, <code>missing</code>,
<code>uncensored</code>, <code>Relevant Pairs</code>, <code>Concordant</code>, and
<code>Uncertain</code>
</p>
<table>
<tr><td><code>n</code></td>
<td>
<p>number of observations not missing on any input variables</p>
</td></tr>
<tr><td><code>missing</code></td>
<td>
<p>number of observations missing on <code>x</code> or <code>S</code></p>
</td></tr>
<tr><td><code>relevant</code></td>
<td>
<p>number of pairs of non-missing observations for which
<code>S</code> could be ordered</p>
</td></tr>
<tr><td><code>concordant</code></td>
<td>
<p>number of relevant pairs for which <code>x</code> and <code>S</code>
are concordant.</p>
</td></tr>
<tr><td><code>uncertain</code></td>
<td>
<p>number of pairs of non-missing observations for which
censoring prevents classification of concordance of <code>x</code> and
<code>S</code>.
</p>
</td></tr>
</table>
<p><code>rcorrcens.formula</code> returns an object of class <code>biVar</code>
which is documented with the <code><a href="#topic+biVar">biVar</a></code> function.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Newson R: Confidence intervals for rank statistics: Somers' D and extensions.  Stata Journal 6:309-334; 2006.
</p>


<h3>See Also</h3>

<p><code><a href="survival.html#topic+concordance">concordance</a></code>, <code><a href="#topic+somers2">somers2</a></code>, <code><a href="#topic+biVar">biVar</a></code>, <code><a href="#topic+rcorrp.cens">rcorrp.cens</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x &lt;- round(rnorm(200))
y &lt;- rnorm(200)
rcorr.cens(x, y, outx=TRUE)   # can correlate non-censored variables
library(survival)
age &lt;- rnorm(400, 50, 10)
bp  &lt;- rnorm(400,120, 15)
bp[1]  &lt;- NA
d.time &lt;- rexp(400)
cens   &lt;- runif(400,.5,2)
death  &lt;- d.time &lt;= cens
d.time &lt;- pmin(d.time, cens)
rcorr.cens(age, Surv(d.time, death))
r &lt;- rcorrcens(Surv(d.time, death) ~ age + bp)
r
plot(r)

# Show typical 0.95 confidence limits for ROC areas for a sample size
# with 24 events and 62 non-events, for varying population ROC areas
# Repeat for 138 events and 102 non-events
set.seed(8)
par(mfrow=c(2,1))
for(i in 1:2) {
 n1 &lt;- c(24,138)[i]
 n0 &lt;- c(62,102)[i]
 y &lt;- c(rep(0,n0), rep(1,n1))
 deltas &lt;- seq(-3, 3, by=.25)
 C &lt;- se &lt;- deltas
 j &lt;- 0
 for(d in deltas) {
  j &lt;- j + 1
  x &lt;- c(rnorm(n0, 0), rnorm(n1, d))
  w &lt;- rcorr.cens(x, y)
  C[j]  &lt;- w['C Index']
  se[j] &lt;- w['S.D.']/2
 }
 low &lt;- C-1.96*se; hi &lt;- C+1.96*se
 print(cbind(C, low, hi))
 errbar(deltas, C, C+1.96*se, C-1.96*se,
        xlab='True Difference in Mean X',
        ylab='ROC Area and Approx. 0.95 CI')
 title(paste('n1=',n1,'  n0=',n0,sep=''))
 abline(h=.5, v=0, col='gray')
 true &lt;- 1 - pnorm(0, deltas, sqrt(2))
 lines(deltas, true, col='blue')
}
par(mfrow=c(1,1))
</code></pre>

<hr>
<h2 id='rcorrp.cens'>
Rank Correlation for Paired Predictors with a Possibly Censored
Response, and Integrated Discrimination Index
</h2><span id='topic+rcorrp.cens'></span><span id='topic+improveProb'></span><span id='topic+print.improveProb'></span>

<h3>Description</h3>

<p>Computes U-statistics to test for whether predictor X1 is more
concordant than predictor X2, extending <code>rcorr.cens</code>.  For
<code>method=1</code>, estimates the fraction of pairs for which the
<code>x1</code> difference is more impressive than the <code>x2</code>
difference. For <code>method=2</code>, estimates the fraction of pairs for
which <code>x1</code> is concordant with <code>S</code> but <code>x2</code> is not.
</p>
<p>For binary responses the function <code>improveProb</code> provides several
assessments of whether one set of predicted probabilities is better
than another, using the methods describe in 
<cite>Pencina et al (2007)</cite>. This involves NRI and IDI to test for
whether predictions from model <code>x1</code> are significantly different
from those obtained from predictions from model <code>x2</code>. This is a
distinct improvement over comparing ROC areas, sensitivity, or
specificity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rcorrp.cens(x1, x2, S, outx=FALSE, method=1)

improveProb(x1, x2, y)

## S3 method for class 'improveProb'
print(x, digits=3, conf.int=.95, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rcorrp.cens_+3A_x1">x1</code></td>
<td>

<p>first predictor (a probability, for <code>improveProb</code>)
</p>
</td></tr> 
<tr><td><code id="rcorrp.cens_+3A_x2">x2</code></td>
<td>

<p>second predictor (a probability, for <code>improveProb</code>)
</p>
</td></tr>
<tr><td><code id="rcorrp.cens_+3A_s">S</code></td>
<td>

<p>a possibly right-censored <code><a href="survival.html#topic+Surv">Surv</a></code> object.  If
<code>S</code> is a vector instead, it is converted to a
<code><a href="survival.html#topic+Surv">Surv</a></code> object and it is assumed that no
observations are censored.
</p>
</td></tr>
<tr><td><code id="rcorrp.cens_+3A_outx">outx</code></td>
<td>

<p>set to <code>TRUE</code> to exclude pairs tied on <code>x1</code> or <code>x2</code>
from consideration
</p>
</td></tr>
<tr><td><code id="rcorrp.cens_+3A_method">method</code></td>
<td>

<p>see above
</p>
</td></tr>
<tr><td><code id="rcorrp.cens_+3A_y">y</code></td>
<td>

<p>a binary 0/1 outcome variable
</p>
</td></tr>
<tr><td><code id="rcorrp.cens_+3A_x">x</code></td>
<td>

<p>the result from <code>improveProb</code>
</p>
</td></tr>
<tr><td><code id="rcorrp.cens_+3A_digits">digits</code></td>
<td>

<p>number of significant digits for use in printing the result of
<code>improveProb</code>
</p>
</td></tr> 
<tr><td><code id="rcorrp.cens_+3A_conf.int">conf.int</code></td>
<td>

<p>level for confidence limits
</p>
</td></tr>
<tr><td><code id="rcorrp.cens_+3A_...">...</code></td>
<td>

<p>unused
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>x1</code>,<code>x2</code> represent predictions from models, these
functions assume either that you are using a separate sample from the
one used to build the model, or that the amount of overfitting in
<code>x1</code> equals the amount of overfitting in <code>x2</code>.  An example
of the latter is giving both models equal opportunity to be complex so
that both models have the same number of effective degrees of freedom,
whether a predictor was included in the model or was screened out by a
variable selection scheme.
</p>
<p>Note that in the first part of their paper, <cite>Pencina et al.</cite>
presented measures that required binning the predicted probabilities.
Those measures were then replaced with better continuous measures that
are implementedhere.
</p>


<h3>Value</h3>

<p>a vector of statistics for <code>rcorrp.cens</code>, or a list with class
<code>improveProb</code> of statistics for <code>improveProb</code>:
<br />
</p>
<table>
<tr><td><code>n</code></td>
<td>
<p>number of cases</p>
</td></tr>
<tr><td><code>na</code></td>
<td>
<p>number of events</p>
</td></tr>
<tr><td><code>nb</code></td>
<td>
<p>number of non-events</p>
</td></tr>
<tr><td><code>pup.ev</code></td>
<td>

<p>mean of pairwise differences in probabilities for those with events
and a pairwise difference of <code class="reqn">\mbox{probabilities}&gt;0</code>
</p>
</td></tr>
<tr><td><code>pup.ne</code></td>
<td>

<p>mean of pairwise differences in probabilities for those without
events and a pairwise difference of <code class="reqn">\mbox{probabilities}&gt;0</code>
</p>
</td></tr> 
<tr><td><code>pdown.ev</code></td>
<td>

<p>mean of pairwise differences in probabilities for those with events
and a pairwise difference of <code class="reqn">\mbox{probabilities}&gt;0</code>
</p>
</td></tr> 
<tr><td><code>pdown.ne</code></td>
<td>

<p>mean of pairwise differences in probabilities for those without
events and a pairwise difference of <code class="reqn">\mbox{probabilities}&gt;0</code>
</p>
</td></tr>
<tr><td><code>nri</code></td>
<td>

<p>Net Reclassification Index =
<code class="reqn">(pup.ev-pdown.ev)-(pup.ne-pdown.ne)</code>
</p>
</td></tr>
<tr><td><code>se.nri</code></td>
<td>
<p>standard error of NRI</p>
</td></tr>
<tr><td><code>z.nri</code></td>
<td>
<p>Z score for NRI</p>
</td></tr>
<tr><td><code>nri.ev</code></td>
<td>
<p>Net Reclassification Index = <code class="reqn">pup.ev-pdown.ev</code></p>
</td></tr>
<tr><td><code>se.nri.ev</code></td>
<td>
<p>SE of NRI of events</p>
</td></tr>
<tr><td><code>z.nri.ev</code></td>
<td>
<p>Z score for NRI of events</p>
</td></tr>
<tr><td><code>nri.ne</code></td>
<td>
<p>Net Reclassification Index = <code class="reqn">pup.ne-pdown.ne</code></p>
</td></tr>
<tr><td><code>se.nri.ne</code></td>
<td>
<p>SE of NRI of non-events</p>
</td></tr>
<tr><td><code>z.nri.ne</code></td>
<td>
<p>Z score for NRI of non-events</p>
</td></tr>
<tr><td><code>improveSens</code></td>
<td>
<p>improvement in sensitivity</p>
</td></tr>
<tr><td><code>improveSpec</code></td>
<td>
<p>improvement in specificity</p>
</td></tr>
<tr><td><code>idi</code></td>
<td>
<p>Integrated Discrimination Index</p>
</td></tr>
<tr><td><code>se.idi</code></td>
<td>
<p>SE of IDI</p>
</td></tr>
<tr><td><code>z.idi</code></td>
<td>
<p>Z score of IDI</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Frank Harrell  <br />
Department of Biostatistics, Vanderbilt University  <br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>
<p>Scott Williams  <br />
Division of Radiation Oncology  <br />
Peter MacCallum Cancer Centre, Melbourne, Australia  <br />
<a href="mailto:scott.williams@petermac.org">scott.williams@petermac.org</a>
</p>


<h3>References</h3>

<p>Pencina MJ, D'Agostino Sr RB, D'Agostino Jr RB, Vasan RS (2008):
Evaluating the added predictive ability of a new marker: From area
under the ROC curve to reclassification and beyond.  Stat in Med 27:157-172.
DOI: 10.1002/sim.2929
</p>
<p>Pencina MJ, D'Agostino Sr RB, D'Agostino Jr RB, Vasan RS:
Rejoinder: Comments on Integrated discrimination and net reclassification
improvements-Practical advice. Stat in Med 2007; DOI: 10.1002/sim.3106  
</p>
<p>Pencina MJ, D'Agostino RB, Steyerberg EW (2011): Extensions of net
reclassification improvement calculations to measure usefulness of new
biomarkers.  Stat in Med 30:11-21; DOI: 10.1002/sim.4085
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rcorr.cens">rcorr.cens</a></code>, <code><a href="#topic+somers2">somers2</a></code>,
<code><a href="survival.html#topic+Surv">Surv</a></code>, <code><a href="rms.html#topic+val.prob">val.prob</a></code>,
<code><a href="survival.html#topic+concordance">concordance</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
library(survival)

x1 &lt;- rnorm(400)
x2 &lt;- x1 + rnorm(400)
d.time &lt;- rexp(400) + (x1 - min(x1))
cens   &lt;- runif(400,.5,2)
death  &lt;- d.time &lt;= cens
d.time &lt;- pmin(d.time, cens)
rcorrp.cens(x1, x2, Surv(d.time, death))
#rcorrp.cens(x1, x2, y) ## no censoring

set.seed(1)
x1 &lt;- runif(1000)
x2 &lt;- runif(1000)
y  &lt;- sample(0:1, 1000, TRUE)
rcorrp.cens(x1, x2, y)
improveProb(x1, x2, y)
</code></pre>

<hr>
<h2 id='rcspline.eval'>
Restricted Cubic Spline Design Matrix
</h2><span id='topic+rcspline.eval'></span>

<h3>Description</h3>

<p>Computes matrix that expands a single variable into the terms needed
to fit a restricted cubic spline (natural spline) function using the
truncated power basis. Two normalization options are given for
somewhat reducing problems of ill-conditioning.  The antiderivative
function can be optionally created. If knot locations are not given,
they will be estimated from the marginal distribution of <code>x</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rcspline.eval(x, knots, nk=5, inclx=FALSE, knots.only=FALSE, 
              type="ordinary", norm=2, rpm=NULL, pc=FALSE,
              fractied=0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rcspline.eval_+3A_x">x</code></td>
<td>

<p>a vector representing a predictor variable
</p>
</td></tr>
<tr><td><code id="rcspline.eval_+3A_knots">knots</code></td>
<td>

<p>knot locations. If not given, knots will be estimated using default
quantiles of <code>x</code>. For 3 knots, the outer quantiles used are 0.10
and 0.90. For 4-6 knots, the outer quantiles used are 0.05 and
0.95. For <code class="reqn">\code{nk}&gt;6</code>, the outer quantiles are 0.025 and 0.975. The
knots are equally spaced between these on the quantile scale. For
fewer than 100 non-missing values of <code>x</code>, the outer knots are
the 5th smallest and largest <code>x</code>.
</p>
</td></tr>
<tr><td><code id="rcspline.eval_+3A_nk">nk</code></td>
<td>

<p>number of knots. Default is 5. The minimum value is 3.
</p>
</td></tr>
<tr><td><code id="rcspline.eval_+3A_inclx">inclx</code></td>
<td>

<p>set to <code>TRUE</code> to add <code>x</code> as the first column of the
returned matrix
</p>
</td></tr>
<tr><td><code id="rcspline.eval_+3A_knots.only">knots.only</code></td>
<td>

<p>return the estimated knot locations but not the expanded matrix
</p>
</td></tr>
<tr><td><code id="rcspline.eval_+3A_type">type</code></td>
<td>

<p>&lsquo;<span class="samp">&#8288;"ordinary"&#8288;</span>&rsquo; to fit the function, &lsquo;<span class="samp">&#8288;"integral"&#8288;</span>&rsquo; to fit its
anti-derivative.
</p>
</td></tr>
<tr><td><code id="rcspline.eval_+3A_norm">norm</code></td>
<td>

<p>&lsquo;<span class="samp">&#8288;0&#8288;</span>&rsquo; to use the terms as originally given by <cite>Devlin and
Weeks (1986)</cite>, &lsquo;<span class="samp">&#8288;1&#8288;</span>&rsquo; to normalize non-linear terms by the cube
of the spacing between the last two knots, &lsquo;<span class="samp">&#8288;2&#8288;</span>&rsquo; to normalize by
the square of the spacing between the first and last knots (the
default). <code>norm=2</code> has the advantage of making all nonlinear
terms beon the x-scale.
</p>
</td></tr>
<tr><td><code id="rcspline.eval_+3A_rpm">rpm</code></td>
<td>

<p>If given, any <code>NA</code>s in <code>x</code> will be replaced with the value
<code>rpm</code> after estimating any knot locations.
</p>
</td></tr>
<tr><td><code id="rcspline.eval_+3A_pc">pc</code></td>
<td>

<p>Set to <code>TRUE</code> to replace the design matrix with orthogonal
(uncorrelated) principal components computed on the scaled, centered
design matrix
</p>
</td></tr>
<tr><td><code id="rcspline.eval_+3A_fractied">fractied</code></td>
<td>

<p>If the fraction of observations tied at the lowest and/or highest
values of <code>x</code> is greater than or equal to <code>fractied</code>, the
algorithm attempts to use a different algorithm for knot finding
based on quantiles of <code>x</code> after excluding the one or two values
with excessive ties.  And if the number of unique <code>x</code> values
excluding these values is small, the unique values will be used as
the knots.  If the number of knots to use other than these exterior
values is only one, that knot will be at the median of the
non-extreme <code>x</code>.  This algorithm is not used if any interior
values of <code>x</code> also have a proportion of ties equal to or
exceeding <code>fractied</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>knots.only=TRUE</code>, returns a vector of knot
locations. Otherwise returns a matrix with <code>x</code> (if
<code>inclx=TRUE</code>) followed by <code class="reqn">\code{nk}-2</code> nonlinear terms. The
matrix has an attribute <code>knots</code> which is the vector of knots
used.  When <code>pc</code> is <code>TRUE</code>, an additional attribute is
stored: <code>pcparms</code>, which contains the <code>center</code> and
<code>scale</code> vectors and the <code>rotation</code> matrix.
</p>


<h3>References</h3>

<p>Devlin TF and Weeks BJ (1986): Spline functions for logistic regression
modeling. Proc 11th Annual SAS Users Group Intnl Conf, p. 646&ndash;651.
Cary NC: SAS Institute, Inc.
</p>


<h3>See Also</h3>

<p><code><a href="splines.html#topic+ns">ns</a></code>, <code><a href="#topic+rcspline.restate">rcspline.restate</a></code>,
<code><a href="rms.html#topic+rcs">rcs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 1:100
rcspline.eval(x, nk=4, inclx=TRUE)
#lrm.fit(rcspline.eval(age,nk=4,inclx=TRUE), death)
x &lt;- 1:1000
attributes(rcspline.eval(x))
x &lt;- c(rep(0, 744),rep(1,6), rep(2,4), rep(3,10),rep(4,2),rep(6,6),
  rep(7,3),rep(8,2),rep(9,4),rep(10,2),rep(11,9),rep(12,10),rep(13,13),
  rep(14,5),rep(15,5),rep(16,10),rep(17,6),rep(18,3),rep(19,11),rep(20,16),
  rep(21,6),rep(22,16),rep(23,17), 24, rep(25,8), rep(26,6),rep(27,3),
  rep(28,7),rep(29,9),rep(30,10),rep(31,4),rep(32,4),rep(33,6),rep(34,6),
  rep(35,4), rep(36,5), rep(38,6), 39, 39, 40, 40, 40, 41, 43, 44, 45)
attributes(rcspline.eval(x, nk=3))
attributes(rcspline.eval(x, nk=5))
u &lt;- c(rep(0,30), 1:4, rep(5,30))
attributes(rcspline.eval(u))
</code></pre>

<hr>
<h2 id='rcspline.plot'>
Plot Restricted Cubic Spline Function
</h2><span id='topic+rcspline.plot'></span>

<h3>Description</h3>

<p>Provides plots of the estimated restricted cubic spline function
relating a single predictor to the response for a logistic or Cox
model. The <code>rcspline.plot</code> function does not allow for
interactions as do <code><a href="rms.html#topic+lrm">lrm</a></code> and <code><a href="rms.html#topic+cph">cph</a></code>, but it can
provide detailed output for checking spline fits. This function uses
the <code><a href="#topic+rcspline.eval">rcspline.eval</a></code>, <code><a href="rms.html#topic+lrm.fit">lrm.fit</a></code>, and Therneau's
<code><a href="survival.html#topic+survival-internal">coxph.fit</a></code> functions and plots the estimated spline
regression and confidence limits, placing summary statistics on the
graph. If there are no adjustment variables, <code>rcspline.plot</code> can
also plot two alternative estimates of the regression function when
<code>model="logistic"</code>: proportions or logit proportions on grouped
data, and a nonparametric estimate. The nonparametric regression
estimate is based on smoothing the binary responses and taking the
logit transformation of the smoothed estimates, if desired. The
smoothing uses <code><a href="stats.html#topic+supsmu">supsmu</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rcspline.plot(x,y,model=c("logistic", "cox", "ols"), xrange, event, nk=5,
              knots=NULL, show=c("xbeta","prob"), adj=NULL, xlab, ylab,
              ylim, plim=c(0,1), plotcl=TRUE, showknots=TRUE, add=FALSE,
              subset, lty=1, noprint=FALSE, m, smooth=FALSE, bass=1,
              main="auto", statloc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rcspline.plot_+3A_x">x</code></td>
<td>

<p>a numeric predictor
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_y">y</code></td>
<td>

<p>a numeric response. For binary logistic regression, <code>y</code> should
be either 0 or 1.
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_model">model</code></td>
<td>

<p><code>"logistic"</code> or <code>"cox"</code>. For <code>"cox"</code>, uses the
<code>coxph.fit</code> function with <code>method="efron"</code> arguement set.
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_xrange">xrange</code></td>
<td>

<p>range for evaluating <code>x</code>, default is f and
<code class="reqn">1 - f</code> quantiles of <code>x</code>, where
<code class="reqn">f = \frac{10}{\max{(n, 200)}}</code>
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_event">event</code></td>
<td>

<p>event/censoring indicator if <code>model="cox"</code>. If <code>event</code> is
present, <code>model</code> is assumed to be <code>"cox"</code>
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_nk">nk</code></td>
<td>

<p>number of knots
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_knots">knots</code></td>
<td>

<p>knot locations, default based on quantiles of <code>x</code> (by
<code><a href="#topic+rcspline.eval">rcspline.eval</a></code>) 
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_show">show</code></td>
<td>

<p><code>"xbeta"</code> or <code>"prob"</code> - what is plotted on <code style="white-space: pre;">&#8288;y&#8288;</code>-axis
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_adj">adj</code></td>
<td>

<p>optional matrix of adjustment variables
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_xlab">xlab</code></td>
<td>

<p><code style="white-space: pre;">&#8288;x&#8288;</code>-axis label, default is the &ldquo;label&rdquo; attribute of
<code>x</code> 
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_ylab">ylab</code></td>
<td>

<p><code style="white-space: pre;">&#8288;y&#8288;</code>-axis label, default is the &ldquo;label&rdquo; attribute of
<code>y</code>
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_ylim">ylim</code></td>
<td>

<p><code style="white-space: pre;">&#8288;y&#8288;</code>-axis limits for logit or log hazard
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_plim">plim</code></td>
<td>

<p><code style="white-space: pre;">&#8288;y&#8288;</code>-axis limits for probability scale
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_plotcl">plotcl</code></td>
<td>

<p>plot confidence limits
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_showknots">showknots</code></td>
<td>

<p>show knot locations with arrows
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_add">add</code></td>
<td>

<p>add this plot to an already existing plot
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_subset">subset</code></td>
<td>

<p>subset of observations to process, e.g. <code>sex == "male"</code>
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_lty">lty</code></td>
<td>

<p>line type for plotting estimated spline function
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_noprint">noprint</code></td>
<td>

<p>suppress printing regression coefficients and standard errors
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_m">m</code></td>
<td>

<p>for <code>model="logistic"</code>, plot grouped estimates with
triangles. Each group contains <code>m</code> ordered observations on
<code>x</code>.
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_smooth">smooth</code></td>
<td>

<p>plot nonparametric estimate if <code>model="logistic"</code> and
<code>adj</code> is not specified
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_bass">bass</code></td>
<td>

<p>smoothing parameter (see <code>supsmu</code>)
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_main">main</code></td>
<td>

<p>main title, default is <code>"Estimated Spline Transformation"</code>
</p>
</td></tr>
<tr><td><code id="rcspline.plot_+3A_statloc">statloc</code></td>
<td>

<p>location of summary statistics. Default positioning by clicking left
mouse button where upper left corner of statistics should
appear. Alternative is <code>"ll"</code> to place below the graph on the
lower left, or the actual <code>x</code> and <code>y</code> coordinates. Use
<code>"none"</code> to suppress statistics.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with components (&lsquo;<span class="samp">&#8288;knots&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;x&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;xbeta&#8288;</span>&rsquo;,
&lsquo;<span class="samp">&#8288;lower&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;upper&#8288;</span>&rsquo;) which are respectively the knot locations,
design matrix, linear predictor, and lower and upper confidence limits
</p>


<h3>Author(s)</h3>

<p>Frank Harrell  <br />
Department of Biostatistics, Vanderbilt University  <br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="rms.html#topic+lrm">lrm</a></code>, <code><a href="rms.html#topic+cph">cph</a></code>, <code><a href="#topic+rcspline.eval">rcspline.eval</a></code>,
<code><a href="graphics.html#topic+plot">plot</a></code>, <code><a href="stats.html#topic+supsmu">supsmu</a></code>,
<code><a href="survival.html#topic+survival-internal">coxph.fit</a></code>,
<code><a href="rms.html#topic+lrm.fit">lrm.fit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#rcspline.plot(cad.dur, tvdlm, m=150)
#rcspline.plot(log10(cad.dur+1), tvdlm, m=150)
</code></pre>

<hr>
<h2 id='rcspline.restate'>
Re-state Restricted Cubic Spline Function
</h2><span id='topic+rcspline.restate'></span><span id='topic+rcsplineFunction'></span>

<h3>Description</h3>

<p>This function re-states a restricted cubic spline function in
the un-linearly-restricted form. Coefficients for that form are
returned, along with an <span class="rlang"><b>R</b></span> functional representation of this function
and a LaTeX character representation of the function.
<code>rcsplineFunction</code> is a fast function that creates a function to
compute a restricted cubic spline function with given coefficients and
knots, without reformatting the function to be pretty (i.e., into
unrestricted form).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rcspline.restate(knots, coef,
                 type=c("ordinary","integral"),
                 x="X", lx=nchar(x),
                 norm=2, columns=65, before="&amp; &amp;", after="\\",
                 begin="", nbegin=0, digits=max(8, .Options$digits))

rcsplineFunction(knots, coef, norm=2, type=c('ordinary', 'integral'))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rcspline.restate_+3A_knots">knots</code></td>
<td>

<p>vector of knots used in the regression fit
</p>
</td></tr>
<tr><td><code id="rcspline.restate_+3A_coef">coef</code></td>
<td>

<p>vector of coefficients from the fit. If the length of <code>coef</code> is
<code class="reqn">k-1</code>, where k is equal to the <code>length(knots)</code>, the
first coefficient must be for the linear term and remaining
<code class="reqn">k-2</code> coefficients must be for the constructed terms (e.g., from
<code>rcspline.eval</code>). If the length of <code>coef</code> is k, an
intercept is assumed to be in the first element (or a zero is
prepended to <code>coef</code> for <code>rcsplineFunction</code>).
</p>
</td></tr>
<tr><td><code id="rcspline.restate_+3A_type">type</code></td>
<td>

<p>The default is to represent the cubic spline function corresponding
to the coefficients and knots.  Set <code>type = "integral"</code> to
instead represent its anti-derivative.
</p>
</td></tr>
<tr><td><code id="rcspline.restate_+3A_x">x</code></td>
<td>

<p>a character string to use as the variable name in the LaTeX expression
for the formula.
</p>
</td></tr>
<tr><td><code id="rcspline.restate_+3A_lx">lx</code></td>
<td>

<p>length of <code>x</code> to count with respect to <code>columns</code>. Default
is length of character string contained by <code>x</code>. You may want to
set <code>lx</code> smaller than this if it includes non-printable LaTeX
commands.
</p>
</td></tr>
<tr><td><code id="rcspline.restate_+3A_norm">norm</code></td>
<td>

<p>normalization that was used in deriving the original nonlinear terms
used in the fit. See <code>rcspline.eval</code> for definitions.
</p>
</td></tr>
<tr><td><code id="rcspline.restate_+3A_columns">columns</code></td>
<td>

<p>maximum number of symbols in the LaTeX expression to allow before
inserting a newline (&lsquo;<span class="samp">&#8288;\\&#8288;</span>&rsquo;) command. Set to a very large
number to keep text all on one line.
</p>
</td></tr>
<tr><td><code id="rcspline.restate_+3A_before">before</code></td>
<td>

<p>text to place before each line of LaTeX output. Use &lsquo;<span class="samp">&#8288;"&amp; &amp;"&#8288;</span>&rsquo;
for an equation array environment in LaTeX where you want to have a
left-hand prefix e.g. &lsquo;<span class="samp">&#8288;"f(X) &amp; = &amp;"&#8288;</span>&rsquo; or using
&lsquo;<span class="samp">&#8288;"\lefteqn"&#8288;</span>&rsquo;.
</p>
</td></tr>
<tr><td><code id="rcspline.restate_+3A_after">after</code></td>
<td>

<p>text to place at the end of each line of output.
</p>
</td></tr>
<tr><td><code id="rcspline.restate_+3A_begin">begin</code></td>
<td>

<p>text with which to start the first line of output. Useful when
adding LaTeX output to part of an existing formula
</p>
</td></tr>
<tr><td><code id="rcspline.restate_+3A_nbegin">nbegin</code></td>
<td>

<p>number of columns of printable text in <code>begin</code>
</p>
</td></tr>
<tr><td><code id="rcspline.restate_+3A_digits">digits</code></td>
<td>

<p>number of significant digits to write for coefficients and knots
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>rcspline.restate</code> returns a vector of coefficients. The
coefficients are un-normalized and two coefficients are added that are
linearly dependent on the other coefficients and knots. The vector of
coefficients has four attributes. <code>knots</code> is a vector of knots,
<code>latex</code> is a vector of text strings with the LaTeX
representation of the formula. <code>columns.used</code> is the number of
columns used in the output string since the last newline command.
<code>function</code> is an <span class="rlang"><b>R</b></span> function, which is also return in character
string format as the <code>text</code> attribute.  <code>rcsplineFunction</code>
returns an <span class="rlang"><b>R</b></span> function with arguments <code>x</code> (a user-supplied
numeric vector at which to evaluate the function), and some
automatically-supplied other arguments. 
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics, Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rcspline.eval">rcspline.eval</a></code>, <code><a href="splines.html#topic+ns">ns</a></code>, <code><a href="rms.html#topic+rcs">rcs</a></code>,
<code><a href="#topic+latex">latex</a></code>, <code><a href="#topic+Function.transcan">Function.transcan</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x &lt;- 1:100
y &lt;- (x - 50)^2 + rnorm(100, 0, 50)
plot(x, y)
xx &lt;- rcspline.eval(x, inclx=TRUE, nk=4)
knots &lt;- attr(xx, "knots")
coef &lt;- lsfit(xx, y)$coef
options(digits=4)
# rcspline.restate must ignore intercept
w &lt;- rcspline.restate(knots, coef[-1], x="{\\rm BP}")
# could also have used coef instead of coef[-1], to include intercept
cat(attr(w,"latex"), sep="\n")


xtrans &lt;- eval(attr(w, "function"))
# This is an S function of a single argument
lines(x, coef[1] + xtrans(x), type="l")
# Plots fitted transformation

xtrans &lt;- rcsplineFunction(knots, coef)
xtrans
lines(x, xtrans(x), col='blue')


#x &lt;- blood.pressure
xx.simple &lt;- cbind(x, pmax(x-knots[1],0)^3, pmax(x-knots[2],0)^3,
                       pmax(x-knots[3],0)^3, pmax(x-knots[4],0)^3)
pred.value &lt;- coef[1] + xx.simple %*% w
plot(x, pred.value, type='l')   # same as above
</code></pre>

<hr>
<h2 id='redun'>Redundancy Analysis</h2><span id='topic+redun'></span><span id='topic+print.redun'></span>

<h3>Description</h3>

<p>Uses flexible parametric additive models (see <code><a href="#topic+areg">areg</a></code> and its
use of regression splines), or alternatively to run a regular regression
after replacing continuous variables with ranks, to
determine how well each variable can be predicted from the remaining
variables.  Variables are dropped in a stepwise fashion, removing the
most predictable variable at each step. The remaining variables are used
to predict.  The process continues until no variable still in the list
of predictors can be predicted with an <code class="reqn">R^2</code> or adjusted <code class="reqn">R^2</code>
of at least <code>r2</code> or until dropping the variable with the highest
<code class="reqn">R^2</code> (adjusted or ordinary) would cause a variable that was dropped
earlier to no longer be predicted at least at the <code>r2</code> level from
the now smaller list of predictors.
</p>
<p>There is also an option <code>qrank</code> to expand each variable into two
columns containing the rank and square of the rank.  Whenever ranks are
used, they are computed as fractional ranks for numerical reasons.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>redun(formula, data=NULL, subset=NULL, r2 = 0.9,
      type = c("ordinary", "adjusted"), nk = 3, tlinear = TRUE,
      rank=qrank, qrank=FALSE,
      allcat=FALSE, minfreq=0, iterms=FALSE, pc=FALSE, pr = FALSE, ...)
## S3 method for class 'redun'
print(x, digits=3, long=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="redun_+3A_formula">formula</code></td>
<td>
<p>a formula.  Enclose a variable in <code>I()</code> to force
linearity.  Alternately, can be a numeric matrix, in which case the
data are not run through <code>dataframeReduce</code>.  This is useful when
running the data through <code>transcan</code> first for nonlinearly
transforming the data.</p>
</td></tr>
<tr><td><code id="redun_+3A_data">data</code></td>
<td>
<p>a data frame, which must be omitted if <code>formula</code> is a
matrix</p>
</td></tr>
<tr><td><code id="redun_+3A_subset">subset</code></td>
<td>
<p>usual subsetting expression</p>
</td></tr>
<tr><td><code id="redun_+3A_r2">r2</code></td>
<td>
<p>ordinary or adjusted <code class="reqn">R^2</code> cutoff for redundancy</p>
</td></tr>
<tr><td><code id="redun_+3A_type">type</code></td>
<td>
<p>specify <code>"adjusted"</code> to use adjusted <code class="reqn">R^2</code></p>
</td></tr>
<tr><td><code id="redun_+3A_nk">nk</code></td>
<td>
<p>number of knots to use for continuous variables.  Use
<code>nk=0</code> to force linearity for all variables.</p>
</td></tr>
<tr><td><code id="redun_+3A_tlinear">tlinear</code></td>
<td>
<p>set to <code>FALSE</code> to allow a variable to be automatically
nonlinearly transformed (see <code>areg</code>) while being predicted.  By
default, only continuous variables on the right hand side (i.e., while
they are being predictors) are automatically transformed, using
regression splines.  Estimating transformations for target (dependent)
variables causes more overfitting than doing so for predictors.</p>
</td></tr>
<tr><td><code id="redun_+3A_rank">rank</code></td>
<td>
<p>set to <code>TRUE</code> to replace non-categorical varibles
with ranks before running the analysis.  This causes <code>nk</code> to be
set to zero.</p>
</td></tr>
<tr><td><code id="redun_+3A_qrank">qrank</code></td>
<td>
<p>set to <code>TRUE</code> to also include squares of ranks to
allow for non-monotonic transformations</p>
</td></tr>
<tr><td><code id="redun_+3A_allcat">allcat</code></td>
<td>
<p>set to <code>TRUE</code> to ensure that all categories of
categorical variables having more than two categories are redundant
(see details below)</p>
</td></tr>
<tr><td><code id="redun_+3A_minfreq">minfreq</code></td>
<td>
<p>For a binary or categorical variable, there must be at
least two categories with at least <code>minfreq</code> observations or
the variable will be dropped and not checked for redundancy against
other variables.  <code>minfreq</code> also specifies the minimum
frequency of a category or its complement 
before that category is considered when <code>allcat=TRUE</code>.</p>
</td></tr>
<tr><td><code id="redun_+3A_iterms">iterms</code></td>
<td>
<p>set to <code>TRUE</code> to consider derived terms (dummy
variables and nonlinear spline components) as separate variables.
This will perform a redundancy analysis on pieces of the variables.</p>
</td></tr>
<tr><td><code id="redun_+3A_pc">pc</code></td>
<td>
<p>if <code>iterms=TRUE</code> you can set <code>pc</code> to <code>TRUE</code>
to replace the submatrix of terms corresponding to each variable
with the orthogonal principal components before doing the redundancy
analysis.  The components are based on the correlation matrix.</p>
</td></tr>
<tr><td><code id="redun_+3A_pr">pr</code></td>
<td>
<p>set to <code>TRUE</code> to monitor progress of the stepwise algorithm</p>
</td></tr>
<tr><td><code id="redun_+3A_...">...</code></td>
<td>
<p>arguments to pass to <code>dataframeReduce</code> to remove
&quot;difficult&quot; variables from <code>data</code> if <code>formula</code> is
<code>~.</code> to use all variables in <code>data</code> (<code>data</code> must be
specified when these arguments are used).  Ignored for <code>print</code>.</p>
</td></tr>
<tr><td><code id="redun_+3A_x">x</code></td>
<td>
<p>an object created by <code>redun</code></p>
</td></tr>
<tr><td><code id="redun_+3A_digits">digits</code></td>
<td>
<p>number of digits to which to round <code class="reqn">R^2</code> values when
printing</p>
</td></tr>
<tr><td><code id="redun_+3A_long">long</code></td>
<td>
<p>set to <code>FALSE</code> to prevent the <code>print</code> method
from printing the <code class="reqn">R^2</code> history and the original <code class="reqn">R^2</code> with
which each variable can be predicted from ALL other variables.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A categorical variable is deemed
redundant if a linear combination of dummy variables representing it can
be predicted from a linear combination of other variables.  For example,
if there were 4 cities in the data and each city's rainfall was also
present as a variable, with virtually the same rainfall reported for all
observations for a city, city would be redundant given rainfall (or
vice-versa; the one declared redundant would be the first one in the
formula). If two cities had the same rainfall, <code>city</code> might be
declared redundant even though tied cities might be deemed non-redundant
in another setting.  To ensure that all categories may be predicted well
from other variables, use the <code>allcat</code> option.  To ignore
categories that are too infrequent or too frequent, set <code>minfreq</code>
to a nonzero integer.  When the number of observations in the category
is below this number or the number of observations not in the category
is below this number, no attempt is made to predict observations being
in that category individually for the purpose of redundancy detection.</p>


<h3>Value</h3>

<p>an object of class <code>"redun"</code> including an element <code>"scores"</code>, a numeric matrix with all transformed values when each variable was the dependent variable and the first canonical variate was computed </p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+areg">areg</a></code>, <code><a href="#topic+dataframeReduce">dataframeReduce</a></code>,
<code><a href="#topic+transcan">transcan</a></code>, <code><a href="#topic+varclus">varclus</a></code>, <code><a href="#topic+r2describe">r2describe</a></code>,
<code>subselect::genetic</code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n &lt;- 100
x1 &lt;- runif(n)
x2 &lt;- runif(n)
x3 &lt;- x1 + x2 + runif(n)/10
x4 &lt;- x1 + x2 + x3 + runif(n)/10
x5 &lt;- factor(sample(c('a','b','c'),n,replace=TRUE))
x6 &lt;- 1*(x5=='a' | x5=='c')
redun(~x1+x2+x3+x4+x5+x6, r2=.8)
redun(~x1+x2+x3+x4+x5+x6, r2=.8, minfreq=40)
redun(~x1+x2+x3+x4+x5+x6, r2=.8, allcat=TRUE)
# x5 is no longer redundant but x6 is
redun(~x1+x2+x3+x4+x5+x6, r2=.8, rank=TRUE)
redun(~x1+x2+x3+x4+x5+x6, r2=.8, qrank=TRUE)

# To help decode which variables made a particular variable redundant:
# r &lt;- redun(...)
# r2describe(r$scores)
</code></pre>

<hr>
<h2 id='reShape'>Reshape Matrices and Serial Data</h2><span id='topic+reShape'></span>

<h3>Description</h3>

<p>If the first argument is a matrix, <code>reShape</code> strings out its values
and creates row and column vectors specifying the row and column each
element came from.  This is useful for sending matrices to Trellis
functions, for analyzing or plotting results of <code>table</code> or
<code>crosstabs</code>, or for reformatting serial data stored in a matrix (with
rows representing multiple time points) into vectors.  The number of
observations in the new variables will be the product of the number of
rows and number of columns in the input matrix.  If the first
argument is a vector, the <code>id</code> and <code>colvar</code> variables are used to
restructure it into a matrix, with <code>NA</code>s for elements that corresponded
to combinations of <code>id</code> and <code>colvar</code> values that did not exist in the
data.  When more than one vector is given, multiple matrices are
created.  This is useful for restructuring irregular serial data into
regular matrices.  It is also useful for converting data produced by
<code>expand.grid</code> into a matrix (see the last example).  The number of
rows of the new matrices equals the number of unique values of <code>id</code>,
and the number of columns equals the number of unique values of
<code>colvar</code>.
</p>
<p>When the first argument is a vector and the <code>id</code> is a data frame
(even with only one variable),
<code>reShape</code> will produce a data frame, and the unique groups are
identified by combinations of the values of all variables in <code>id</code>.
If a data frame <code>constant</code> is specified, the variables in this data
frame are assumed to be constant within combinations of <code>id</code>
variables (if not, an arbitrary observation in <code>constant</code> will be
selected for each group).  A row of <code>constant</code> corresponding to the
target <code>id</code> combination is then carried along when creating the
data frame result.
</p>
<p>A different behavior of <code>reShape</code> is achieved when <code>base</code> and <code>reps</code>
are specified.  In that case <code>x</code> must be a list or data frame, and
those data are assumed to contain one or more non-repeating
measurements (e.g., baseline measurements) and one or more repeated
measurements represented by variables named by pasting together the
character strings in the vector <code>base</code> with the integers 1, 2, ...,
<code>reps</code>.  The input data are rearranged by repeating each value of the
baseline variables <code>reps</code> times and by transposing each observation's
values of one of the set of repeated measurements as <code>reps</code>
observations under the variable whose name does not have an integer
pasted to the end.  if <code>x</code> has a <code>row.names</code> attribute, those
observation identifiers are each repeated <code>reps</code> times in the output
object.  See the last example.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reShape(x, ..., id, colvar, base, reps, times=1:reps,
        timevar='seqno', constant=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reShape_+3A_x">x</code></td>
<td>

<p>a matrix or vector, or, when <code>base</code> is specified, a list or data frame
</p>
</td></tr>
<tr><td><code id="reShape_+3A_...">...</code></td>
<td>

<p>other optional vectors, if <code>x</code> is a vector
</p>
</td></tr>
<tr><td><code id="reShape_+3A_id">id</code></td>
<td>

<p>A numeric, character, category, or factor variable containing subject
identifiers, or a data frame of such variables that in combination form
groups of interest.  Required if <code>x</code> is a vector, ignored otherwise.
</p>
</td></tr>
<tr><td><code id="reShape_+3A_colvar">colvar</code></td>
<td>

<p>A numeric, character, category, or factor variable containing column
identifiers.  <code>colvar</code> is using a &quot;time of data collection&quot; variable.
Required if <code>x</code> is a vector, ignored otherwise.
</p>
</td></tr>
<tr><td><code id="reShape_+3A_base">base</code></td>
<td>

<p>vector of character strings containing base names of repeated
measurements
</p>
</td></tr>
<tr><td><code id="reShape_+3A_reps">reps</code></td>
<td>

<p>number of times variables named in <code>base</code> are repeated.  This must be
a constant.
</p>
</td></tr>
<tr><td><code id="reShape_+3A_times">times</code></td>
<td>

<p>when <code>base</code> is given, <code>times</code> is the vector of times to create
if you do not want to use consecutive integers beginning with 1.
</p>
</td></tr>
<tr><td><code id="reShape_+3A_timevar">timevar</code></td>
<td>

<p>specifies the name of the time variable to create if <code>times</code> is
given, if you do not want to use <code>seqno</code>
</p>
</td></tr>
<tr><td><code id="reShape_+3A_constant">constant</code></td>
<td>

<p>a data frame with the same number of rows in <code>id</code> and <code>x</code>,
containing auxiliary information to be merged into the resulting data
frame.  Logically, the rows of <code>constant</code> within each group
should have the same value of all of its variables.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In converting <code>dimnames</code> to vectors, the resulting variables are
numeric if all elements of the matrix dimnames can be converted to
numeric, otherwise the corresponding row or column variable remains
character.  When the <code>dimnames</code> if <code>x</code> have a <code>names</code> attribute, those
two names become the new variable names.  If <code>x</code> is a vector and
another vector is also given (in <code>...</code>), the matrices in the resulting
list are named the same as the input vector calling arguments.  You
can specify customized names for these on-the-fly by using
e.g. <code>reShape(X=x, Y=y, id= , colvar= )</code>.  The new names will then be
<code>X</code> and <code>Y</code> instead of <code>x</code> and <code>y</code>.   A new variable named <code>seqnno</code> is
also added to the resulting object.  <code>seqno</code> indicates the sequential
repeated measurement number.  When <code>base</code> and <code>times</code> are
specified, this new variable is named the character value of <code>timevar</code> and the values
are given by a table lookup into the vector <code>times</code>.
</p>


<h3>Value</h3>

<p>If <code>x</code> is a matrix, returns a list containing the row variable, the
column variable, and the <code>as.vector(x)</code> vector, named the same as the
calling argument was called for <code>x</code>.  If <code>x</code> is a vector and no other
vectors were specified as <code>...</code>, the result is a matrix.  If at least
one vector was given to <code>...</code>, the result is a list containing <code>k</code>
matrices, where <code>k</code> one plus the number of vectors in <code>...</code>.  If <code>x</code>
is a list or data frame, the same type of object is returned.  If
<code>x</code> is a vector and <code>id</code> is a data frame, a data frame will be
the result.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University School of Medicine<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+reshape">reshape</a></code>, <code><a href="base.html#topic+vector">as.vector</a></code>,
<code><a href="base.html#topic+matrix">matrix</a></code>, <code><a href="base.html#topic+dimnames">dimnames</a></code>,
<code><a href="base.html#topic+outer">outer</a></code>, <code><a href="base.html#topic+table">table</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
Solder  &lt;- factor(sample(c('Thin','Thick'),200,TRUE),c('Thin','Thick'))
Opening &lt;- factor(sample(c('S','M','L'),  200,TRUE),c('S','M','L'))

tab &lt;- table(Opening, Solder)
tab
reShape(tab)
# attach(tab)  # do further processing

# An example where a matrix is created from irregular vectors
follow &lt;- data.frame(id=c('a','a','b','b','b','d'),
                     month=c(1, 2,  1,  2,  3,  2),
                     cholesterol=c(225,226, 320,319,318, 270))
follow
attach(follow)
reShape(cholesterol, id=id, colvar=month)
detach('follow')
# Could have done :
# reShape(cholesterol, triglyceride=trig, id=id, colvar=month)

# Create a data frame, reshaping a long dataset in which groups are
# formed not just by subject id but by combinations of subject id and
# visit number.  Also carry forward a variable that is supposed to be
# constant within subject-visit number combinations.  In this example,
# it is not constant, so an arbitrary visit number will be selected.
w &lt;- data.frame(id=c('a','a','a','a','b','b','b','d','d','d'),
             visit=c(  1,  1,  2,  2,  1,  1,  2,  2,  2,  2),
                 k=c('A','A','B','B','C','C','D','E','F','G'),
               var=c('x','y','x','y','x','y','y','x','y','z'),
               val=1:10)
with(w,
     reShape(val, id=data.frame(id,visit),
             constant=data.frame(k), colvar=var))

# Get predictions from a regression model for 2 systematically
# varying predictors.  Convert the predictions into a matrix, with
# rows corresponding to the predictor having the most values, and
# columns corresponding to the other predictor
# d &lt;- expand.grid(x2=0:1, x1=1:100)
# pred &lt;- predict(fit, d)
# reShape(pred, id=d$x1, colvar=d$x2)  # makes 100 x 2 matrix


# Reshape a wide data frame containing multiple variables representing
# repeated measurements (3 repeats on 2 variables; 4 subjects)
set.seed(33)
n &lt;- 4
w &lt;- data.frame(age=rnorm(n, 40, 10),
                sex=sample(c('female','male'), n,TRUE),
                sbp1=rnorm(n, 120, 15),
                sbp2=rnorm(n, 120, 15),
                sbp3=rnorm(n, 120, 15),
                dbp1=rnorm(n,  80, 15),
                dbp2=rnorm(n,  80, 15),
                dbp3=rnorm(n,  80, 15), row.names=letters[1:n])
options(digits=3)
w


u &lt;- reShape(w, base=c('sbp','dbp'), reps=3)
u
reShape(w, base=c('sbp','dbp'), reps=3, timevar='week', times=c(0,3,12))
</code></pre>

<hr>
<h2 id='rlegend'>Special Version of legend for R</h2><span id='topic+rlegend'></span><span id='topic+rlegendg'></span>

<h3>Description</h3>

<p><code>rlegend</code> is a version of <code><a href="graphics.html#topic+legend">legend</a></code> for <span class="rlang"><b>R</b></span> that implements
<code>plot=FALSE</code>, adds <code>grid=TRUE</code>, and defaults <code>lty</code>,
<code>lwd</code>, <code>pch</code> to <code>NULL</code> and checks for <code>length&gt;0</code>
rather than <code>missing()</code>, so it's easier to deal with
non-applicable parameters.  But when <span class="pkg">grid</span> is in effect, the
preferred function to use is <code>rlegendg</code>, which calls the
<span class="pkg">lattice</span> <code><a href="lattice.html#topic+draw.key">draw.key</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rlegend(x, y, legend, fill, col = "black", lty = NULL, lwd = NULL,
        pch = NULL, angle = NULL, density = NULL, bty = "o",
        bg = par("bg"), pt.bg = NA, cex = 1, xjust = 0, yjust = 1,
        x.intersp = 1, y.intersp = 1, adj = 0, text.width = NULL,
        merge = do.lines &amp;&amp; has.pch, trace = FALSE, ncol = 1,
        horiz = FALSE, plot = TRUE, grid = FALSE, ...)

rlegendg(x, y, legend, col=pr$col[1], lty=NULL,
         lwd=NULL, pch=NULL, cex=pr$cex[1], other=NULL) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rlegend_+3A_x">x</code>, <code id="rlegend_+3A_y">y</code>, <code id="rlegend_+3A_legend">legend</code>, <code id="rlegend_+3A_fill">fill</code>, <code id="rlegend_+3A_col">col</code>, <code id="rlegend_+3A_lty">lty</code>, <code id="rlegend_+3A_lwd">lwd</code>, <code id="rlegend_+3A_pch">pch</code>, <code id="rlegend_+3A_angle">angle</code>, <code id="rlegend_+3A_density">density</code>, <code id="rlegend_+3A_bty">bty</code>, <code id="rlegend_+3A_bg">bg</code>, <code id="rlegend_+3A_pt.bg">pt.bg</code>, <code id="rlegend_+3A_cex">cex</code>, <code id="rlegend_+3A_xjust">xjust</code>, <code id="rlegend_+3A_yjust">yjust</code>, <code id="rlegend_+3A_x.intersp">x.intersp</code>, <code id="rlegend_+3A_y.intersp">y.intersp</code>, <code id="rlegend_+3A_adj">adj</code>, <code id="rlegend_+3A_text.width">text.width</code>, <code id="rlegend_+3A_merge">merge</code>, <code id="rlegend_+3A_trace">trace</code>, <code id="rlegend_+3A_ncol">ncol</code>, <code id="rlegend_+3A_horiz">horiz</code></td>
<td>

<p>see <code><a href="graphics.html#topic+legend">legend</a></code>
</p>
</td></tr>
<tr><td><code id="rlegend_+3A_plot">plot</code></td>
<td>
<p>set to <code>FALSE</code> to suppress drawing the legend.  This
is used the compute the size needed for when the legend is drawn
with a later call to <code>rlegend</code>.
</p>
</td></tr>
<tr><td><code id="rlegend_+3A_grid">grid</code></td>
<td>
<p>set to <code>TRUE</code> if the <span class="pkg">grid</span> package is in effect</p>
</td></tr>
<tr><td><code id="rlegend_+3A_...">...</code></td>
<td>
<p>see <code><a href="graphics.html#topic+legend">legend</a></code></p>
</td></tr>
<tr><td><code id="rlegend_+3A_other">other</code></td>
<td>

<p>a list containing other arguments to pass to
<code>draw.key</code>.  See the help file for <code><a href="lattice.html#topic+xyplot">xyplot</a></code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with elements <code>rect</code> and <code>text</code>.  <code>rect</code> has
elements <code>w, h, left, top</code> with size/position information.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell and R-Core</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+legend">legend</a></code>, <code><a href="lattice.html#topic+draw.key">draw.key</a></code>, <code><a href="lattice.html#topic+xyplot">xyplot</a></code>
</p>

<hr>
<h2 id='rm.boot'>
Bootstrap Repeated Measurements Model
</h2><span id='topic+rm.boot'></span><span id='topic+plot.rm.boot'></span>

<h3>Description</h3>

<p>For a dataset containing a time variable, a scalar response variable,
and an optional subject identification variable, obtains least squares
estimates of the coefficients of a restricted cubic spline function or
a linear regression in time after adjusting for subject effects
through the use of subject dummy variables.  Then the fit is
bootstrapped <code>B</code> times, either by treating time and subject ID as
fixed (i.e., conditioning the analysis on them) or as random
variables.  For the former, the residuals from the original model fit
are used as the basis of the bootstrap distribution.  For the latter,
samples are taken jointly from the time, subject ID, and response
vectors to obtain unconditional distributions.
</p>
<p>If a subject <code>id</code> variable is given, the bootstrap sampling will
be based on samples with replacement from subjects rather than from
individual data points.  In other words, either none or all of a given
subject's data will appear in a bootstrap sample.  This cluster
sampling takes into account any correlation structure that might exist
within subjects, so that confidence limits are corrected for
within-subject correlation.  Assuming that ordinary least squares
estimates, which ignore the correlation structure, are consistent
(which is almost always true) and efficient (which would not be true
for certain correlation structures or for datasets in which the number
of observation times vary greatly from subject to subject), the
resulting analysis will be a robust, efficient repeated measures
analysis for the one-sample problem.
</p>
<p>Predicted values of the fitted models are evaluated by default at a
grid of 100 equally spaced time points ranging from the minimum to
maximum observed time points.  Predictions are for the average subject
effect.  Pointwise confidence intervals are optionally computed
separately for each of the points on the time grid.  However,
simultaneous confidence regions that control the level of confidence
for the entire regression curve lying within a band are often more
appropriate, as they allow the analyst to draw conclusions about
nuances in the mean time response profile that were not stated
apriori.  The method of <cite>Tibshirani (1997)</cite> is used to easily
obtain simultaneous confidence sets for the set of coefficients of the
spline or linear regression function as well as the average  intercept
parameter (over subjects).  Here one computes the objective criterion
(here both the -2 log likelihood evaluated at the bootstrap estimate
of beta but with respect to the original design matrix and response
vector, and the sum of squared errors in predicting the original
response vector) for the original fit as well as for all of the
bootstrap fits.  The confidence set of the regression coefficients is
the set of all coefficients that are associated with objective
function values that are less than or equal to say the 0.95 quantile
of the vector of <code class="reqn">\code{B} + 1</code> objective function values.  For
the coefficients satisfying this condition, predicted curves are
computed at the time grid, and minima and maxima of these curves are
computed separately at each time point toderive the final
simultaneous confidence band.
</p>
<p>By default, the log likelihoods that are computed for obtaining the
simultaneous confidence band assume independence within subject.  This
will cause problems unless such log likelihoods have very high rank
correlation with the log likelihood allowing for dependence.  To allow
for correlation or to estimate the correlation function, see the
<code>cor.pattern</code> argument below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rm.boot(time, y, id=seq(along=time), subset,
        plot.individual=FALSE,
        bootstrap.type=c('x fixed','x random'),
        nk=6, knots, B=500, smoother=supsmu, 
        xlab, xlim, ylim=range(y), 
        times=seq(min(time), max(time), length=100),
        absorb.subject.effects=FALSE, 
        rho=0, cor.pattern=c('independent','estimate'), ncor=10000,
        ...)


## S3 method for class 'rm.boot'
plot(x, obj2, conf.int=.95,
     xlab=x$xlab, ylab=x$ylab, 
     xlim, ylim=x$ylim,
     individual.boot=FALSE,
     pointwise.band=FALSE,
     curves.in.simultaneous.band=FALSE,
     col.pointwise.band=2,
     objective=c('-2 log L','sse','dep -2 log L'), add=FALSE, ncurves,
     multi=FALSE, multi.method=c('color','density'),
     multi.conf   =c(.05,.1,.2,.3,.4,.5,.6,.7,.8,.9,.95,.99),
     multi.density=c( -1,90,80,70,60,50,40,30,20,10,  7,  4),
     multi.col    =c(  1, 8,20, 5, 2, 7,15,13,10,11,  9, 14),
     subtitles=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rm.boot_+3A_time">time</code></td>
<td>

<p>numeric time vector
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_y">y</code></td>
<td>

<p>continuous numeric response vector of length the same as <code>time</code>.
Subjects having multiple measurements have the measurements strung out.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_x">x</code></td>
<td>

<p>an object returned from <code>rm.boot</code>
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_id">id</code></td>
<td>

<p>subject ID variable.  If omitted, it is assumed that each
time-response pair is measured on a different subject.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_subset">subset</code></td>
<td>

<p>subset of observations to process if not all the data
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_plot.individual">plot.individual</code></td>
<td>

<p>set to <code>TRUE</code> to plot nonparametrically smoothed time-response
curves for each subject
</p>
</td></tr>  
<tr><td><code id="rm.boot_+3A_bootstrap.type">bootstrap.type</code></td>
<td>

<p>specifies whether to treat the time and subject ID variables as
fixed or random
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_nk">nk</code></td>
<td>

<p>number of knots in the restricted cubic spline function fit.  The
number of knots may be 0 (denoting linear regression) or an integer
greater than 2 in which k knots results in <code class="reqn">k - 1</code>
regression coefficients excluding the intercept.  The default is 6
knots.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_knots">knots</code></td>
<td>

<p>vector of knot locations.  May be specified if <code>nk</code> is
omitted. 
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_b">B</code></td>
<td>

<p>number of bootstrap repetitions.  Default is 500.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_smoother">smoother</code></td>
<td>

<p>a smoothing function that is used if <code>plot.individual=TRUE</code>.
Default is <code><a href="stats.html#topic+supsmu">supsmu</a></code>.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_xlab">xlab</code></td>
<td>

<p>label for x-axis.  Default is <code>"units"</code> attribute of the
original <code>time</code> variable, or <code>"Time"</code> if no such
attribute was defined using the <code>units</code> function.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_xlim">xlim</code></td>
<td>

<p>specifies x-axis plotting limits.  Default is to use range of times
specified to <code>rm.boot</code>.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_ylim">ylim</code></td>
<td>

<p>for <code>rm.boot</code> this is a vector of y-axis limits used if
<code>plot.individual=TRUE</code>.  It is also passed along for later use
by <code>plot.rm.boot</code>.  For <code>plot.rm.boot</code>, <code>ylim</code> can
be specified, to override the value stored in the object stored by
<code>rm.boot</code>.  The default is the actual range of <code>y</code> in the
input data.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_times">times</code></td>
<td>

<p>a sequence of times at which to evaluated fitted values and
confidence limits.  Default is 100 equally spaced points in the
observed range of <code>time</code>.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_absorb.subject.effects">absorb.subject.effects</code></td>
<td>

<p>If <code>TRUE</code>, adjusts the response vector <code>y</code> before
re-sampling so that the subject-specific effects in the initial
model fit are all zero.  Then in re-sampling, subject effects are
not used in the models.  This will downplay one of the sources of
variation.  This option is used mainly for checking for consistency
of results, as the re-sampling analyses are simpler when
<code>absort.subject.effects=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_rho">rho</code></td>
<td>

<p>The log-likelihood function that is used as the basis of
simultaneous confidence bands assumes normality with independence
within subject. To check the robustness of this assumption, if
<code>rho</code> is not zero, the log-likelihood under multivariate
normality within subject, with constant correlation <code>rho</code>
between any two time points, is also computed.  If the two
log-likelihoods have the same ranks across re-samples, alllowing
the correlation structure does not matter.  The agreement in ranks
is quantified using the Spearman rank correlation coefficient.  The
<code><a href="base.html#topic+plot">plot</a></code> method allows the non-zero intra-subject
correlation log-likelihood to be used in deriving the simultaneous
confidence band.  Note that this approach does assume
homoscedasticity.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_cor.pattern">cor.pattern</code></td>
<td>

<p>More generally than using an equal-correlation structure, you can
specify a function of two time vectors that generates as many
correlations as the length of these vectors.  For example,
<code>cor.pattern=function(time1,time2) 0.2^(abs(time1-time2)/10)</code>
would specify a dampening serial correlation pattern.
<code>cor.pattern</code> can also be a list containing vectors <code>x</code>
(a vector of absolute time differences) and <code>y</code> (a
corresponding vector of correlations).  To estimate the correlation
function as a function of absolute time differences within
subjects, specify <code>cor.pattern="estimate"</code>.  The products of
all possible pairs of residuals (or at least up to <code>ncor</code> of
them) within subjects will be related to the absolute time
difference.  The correlation function is estimated by computing the
sample mean of the products of standardized residuals, stratified
by absolute time difference.  The correlation for a zero time
difference is set to 1 regardless of the <code><a href="stats.html#topic+lowess">lowess</a></code>
estimate.  NOTE: This approach fails in the presence of large
subject effects; correcting for such effects removes too much of
the correlation structure in the residuals.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_ncor">ncor</code></td>
<td>

<p>the maximum number of pairs of time values used in estimating the
correlation function if <code>cor.pattern="estimate"</code>
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_...">...</code></td>
<td>

<p>other arguments to pass to <code>smoother</code> if <code>plot.individual=TRUE</code>
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_obj2">obj2</code></td>
<td>

<p>a second object created by <code>rm.boot</code> that can also be passed
to <code>plot.rm.boot</code>.  This is used for two-sample problems for
which the time profiles are allowed to differ between the two
groups.  The bootstrapped predicted y values for the second fit are
subtracted from the fitted values for the first fit so that the
predicted mean response for group 1 minus the predicted mean
response for group 2 is what is plotted. The confidence bands that
are plotted are also for this difference.  For the simultaneous
confidence band, the objective criterion is taken to be the sum of
the objective criteria (-2 log L or sum of squared errors) for the
separate fits for the two groups. The <code>times</code> vectors must
have been identical for both calls to <code>rm.boot</code>, although
<code>NA</code>s can be inserted by the user of one or both of the time
vectors in the <code>rm.boot</code> objects so as to suppress certain
sections of the difference curve from being plotted.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_conf.int">conf.int</code></td>
<td>

<p>the confidence level to use in constructing simultaneous, and
optionally pointwise, bands.  Default is 0.95.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_ylab">ylab</code></td>
<td>

<p>label for y-axis.  Default is the <code>"label"</code> attribute of the
original <code>y</code> variable, or <code>"y"</code> if no label was assigned
to <code>y</code> (using the <code>label</code> function, for example).
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_individual.boot">individual.boot</code></td>
<td>

<p>set to <code>TRUE</code> to plot the first 100 bootstrap regression fits
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_pointwise.band">pointwise.band</code></td>
<td>

<p>set to <code>TRUE</code> to draw a pointwise confidence band in addition
to the simultaneous band
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_curves.in.simultaneous.band">curves.in.simultaneous.band</code></td>
<td>

<p>set to <code>TRUE</code> to draw all bootstrap regression fits that had a
sum of squared errors (obtained by predicting the original <code>y</code>
vector from the original <code>time</code> vector and <code>id</code> vector)
that was less that or equal to the <code>conf.int</code> quantile of all
bootstrapped models (plus the original model). This will show how
the point by point max and min were computed to form the
simultaneous confidence band.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_col.pointwise.band">col.pointwise.band</code></td>
<td>

<p>color for the pointwise confidence band.  Default is &lsquo;<span class="samp">&#8288;2&#8288;</span>&rsquo;,
which defaults to red for default Windows S-PLUS setups.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_objective">objective</code></td>
<td>

<p>the default is to use the -2 times log of the Gaussian likelihood
for computing the simultaneous confidence region.  If neither
<code>cor.pattern</code> nor <code>rho</code> was specified to <code>rm.boot</code>,
the independent homoscedastic Gaussian likelihood is
used. Otherwise the dependent homoscedastic likelihood is used
according to the specified or estimated correlation
pattern. Specify <code>objective="sse"</code> to instead use the sum of
squared errors.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_add">add</code></td>
<td>

<p>set to <code>TRUE</code> to add curves to an existing plot.  If you do
this, titles and subtitles are omitted.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_ncurves">ncurves</code></td>
<td>

<p>when using <code>individual.boot=TRUE</code> or
<code>curves.in.simultaneous.band=TRUE</code>, you can plot a random
sample of <code>ncurves</code> of the fitted curves instead of plotting
up to <code>B</code> of them.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_multi">multi</code></td>
<td>

<p>set to <code>TRUE</code> to draw multiple simultaneous confidence bands
shaded with different colors.  Confidence levels vary over the
values in the <code>multi.conf</code> vector.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_multi.method">multi.method</code></td>
<td>

<p>specifies the method of shading when <code>multi=TRUE</code>.  Default is
to use colors, with the default colors chosen so that when the
graph is printed under S-Plus for Windows 4.0 to an HP LaserJet
printer, the confidence regions are naturally ordered by darkness
of gray-scale. Regions closer to the point estimates (i.e., the
center) are darker. Specify <code>multi.method="density"</code> to
instead use densities of lines drawn per inch in the confidence
regions, with all regions drawn with the default color. The
<code><a href="graphics.html#topic+polygon">polygon</a></code> function is used to shade the regions.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_multi.conf">multi.conf</code></td>
<td>

<p>vector of confidence levels, in ascending order.  Default is to use
12 confidence levels ranging from 0.05 to 0.99.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_multi.density">multi.density</code></td>
<td>

<p>vector of densities in lines per inch corresponding to
<code>multi.conf</code>. As is the convention in the
<code><a href="graphics.html#topic+polygon">polygon</a></code> function, a density of -1 indicates a solid
region.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_multi.col">multi.col</code></td>
<td>

<p>vector of colors corresponding to <code>multi.conf</code>.  See
<code>multi.method</code> for rationale.
</p>
</td></tr>
<tr><td><code id="rm.boot_+3A_subtitles">subtitles</code></td>
<td>

<p>set to <code>FALSE</code> to suppress drawing subtitles for the plot
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Observations having missing <code>time</code> or <code>y</code> are excluded from
the analysis.
</p>
<p>As most repeated measurement studies consider the times as design
points, the fixed covariable case is the default. Bootstrapping the
residuals from the initial fit assumes that the model is correctly
specified.  Even if the covariables are fixed, doing an unconditional
bootstrap is still appropriate, and for large sample sizes
unconditional confidence intervals are only slightly wider than
conditional ones.  For moderate to small sample sizes, the
<code>bootstrap.type="x random"</code> method can be fairly conservative.
</p>
<p>If not all subjects have the same number of observations (after
deleting observations containing missing values) and if
<code>bootstrap.type="x fixed"</code>, bootstrapped residual vectors may
have a length m that is different from the number of original
observations n.  If <code class="reqn">m &gt; n</code> for a bootstrap
repetition, the first n elements of the randomly drawn residuals
are used. If <code class="reqn">m &lt; n</code>, the residual vector is appended
with a random sample with replacement of length <code class="reqn">n - m</code> from itself.  A warning message is issued if this happens.
If the number of time points per subject varies, the bootstrap results
for <code>bootstrap.type="x fixed"</code> can still be invalid, as this
method assumes that a vector (over subjects) of all residuals can be
added to the original yhats, and varying number of points will cause
mis-alignment.
</p>
<p>For <code>bootstrap.type="x random"</code> in the presence of significant
subject effects, the analysis is approximate as the subjects used in
any one bootstrap fit will not be the entire list of subjects.  The
average (over subjects used in the bootstrap sample) intercept is used
from that bootstrap sample as a predictor of average subject effects
in the overall sample.
</p>
<p>Once the bootstrap coefficient matrix is stored by <code>rm.boot</code>,
<code>plot.rm.boot</code> can be run multiple times with different options
(e.g, different confidence levels).
</p>
<p>See <code><a href="rms.html#topic+bootcov">bootcov</a></code> in the <span class="pkg">rms</span> library for a general
approach to handling repeated measurement data for ordinary linear
models, binary and ordinal models, and survival models, using the
unconditional bootstrap.  <code><a href="rms.html#topic+bootcov">bootcov</a></code> does not handle bootstrapping
residuals.
</p>


<h3>Value</h3>

<p>an object of class <code>rm.boot</code> is returned by <code>rm.boot</code>. The
principal object stored in the returned object is a matrix of
regression coefficients for the original fit and all of the bootstrap
repetitions (object <code>Coef</code>), along with vectors of the
corresponding -2 log likelihoods are sums of squared errors. The
original fit object from <code>lm.fit.qr</code> is stored in
<code>fit</code>. For this fit, a cell means model is used for the
<code>id</code> effects.
</p>
<p><code>plot.rm.boot</code> returns a list containing the vector of times used
for plotting along with the overall fitted values, lower and upper
simultaneous confidence limits, and optionally the pointwise
confidence limits.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell  <br />
Department of Biostatistics  <br />
Vanderbilt University School of Medicine  <br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Feng Z, McLerran D, Grizzle J (1996): A comparison of statistical methods for
clustered data analysis with Gaussian error.  Stat in Med 15:1793&ndash;1806.
</p>
<p>Tibshirani R, Knight K (1997):Model search and inference by bootstrap 
&quot;bumping&quot;.  Technical Report, Department of Statistics, University of Toronto.
<br />
<a href="https://www.jstor.org/stable/1390820">https://www.jstor.org/stable/1390820</a>. Presented at the Joint Statistical
Meetings, Chicago, August 1996.
</p>
<p>Efron B, Tibshirani R (1993): An Introduction to the Bootstrap.
New York: Chapman and Hall.
</p>
<p>Diggle PJ, Verbyla AP (1998): Nonparametric estimation of covariance
structure in logitudinal data.  Biometrics 54:401&ndash;415.
</p>
<p>Chapman IM, Hartman ML, et al (1997): Effect of aging on the
sensitivity of growth hormone secretion to insulin-like growth
factor-I negative feedback.  J Clin Endocrinol Metab 82:2996&ndash;3004.
</p>
<p>Li Y, Wang YG (2008): Smooth bootstrap methods for analysis of
longitudinal data.  Stat in Med 27:937-953. (potential improvements to
cluster bootstrap; not implemented here)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rcspline.eval">rcspline.eval</a></code>, <code><a href="stats.html#topic+lm">lm</a></code>, <code><a href="stats.html#topic+lowess">lowess</a></code>,
<code><a href="stats.html#topic+supsmu">supsmu</a></code>, <code><a href="rms.html#topic+bootcov">bootcov</a></code>,
<code><a href="#topic+units">units</a></code>, <code><a href="#topic+label">label</a></code>, <code><a href="graphics.html#topic+polygon">polygon</a></code>,
<code><a href="#topic+reShape">reShape</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate multivariate normal responses with equal correlations (.7)
# within subjects and no correlation between subjects
# Simulate realizations from a piecewise linear population time-response
# profile with large subject effects, and fit using a 6-knot spline
# Estimate the correlation structure from the residuals, as a function
# of the absolute time difference


# Function to generate n p-variate normal variates with mean vector u and
# covariance matrix S
# Slight modification of function written by Bill Venables
# See also the built-in function rmvnorm
mvrnorm &lt;- function(n, p = 1, u = rep(0, p), S = diag(p)) {
  Z &lt;- matrix(rnorm(n * p), p, n)
  t(u + t(chol(S)) %*% Z)
}


n     &lt;- 20         # Number of subjects
sub   &lt;- .5*(1:n)   # Subject effects


# Specify functional form for time trend and compute non-stochastic component
times &lt;- seq(0, 1, by=.1)
g     &lt;- function(times) 5*pmax(abs(times-.5),.3)
ey    &lt;- g(times)


# Generate multivariate normal errors for 20 subjects at 11 times
# Assume equal correlations of rho=.7, independent subjects


nt    &lt;- length(times)
rho   &lt;- .7


        
set.seed(19)        
errors &lt;- mvrnorm(n, p=nt, S=diag(rep(1-rho,nt))+rho)
# Note:  first random number seed used gave rise to mean(errors)=0.24!


# Add E[Y], error components, and subject effects
y      &lt;- matrix(rep(ey,n), ncol=nt, byrow=TRUE) + errors + 
          matrix(rep(sub,nt), ncol=nt)


# String out data into long vectors for times, responses, and subject ID
y      &lt;- as.vector(t(y))
times  &lt;- rep(times, n)
id     &lt;- sort(rep(1:n, nt))


# Show lowess estimates of time profiles for individual subjects
f &lt;- rm.boot(times, y, id, plot.individual=TRUE, B=25, cor.pattern='estimate',
             smoother=lowess, bootstrap.type='x fixed', nk=6)
# In practice use B=400 or 500
# This will compute a dependent-structure log-likelihood in addition
# to one assuming independence.  By default, the dep. structure
# objective will be used by the plot method  (could have specified rho=.7)
# NOTE: Estimating the correlation pattern from the residual does not
# work in cases such as this one where there are large subject effects


# Plot fits for a random sample of 10 of the 25 bootstrap fits
plot(f, individual.boot=TRUE, ncurves=10, ylim=c(6,8.5))


# Plot pointwise and simultaneous confidence regions
plot(f, pointwise.band=TRUE, col.pointwise=1, ylim=c(6,8.5))


# Plot population response curve at average subject effect
ts &lt;- seq(0, 1, length=100)
lines(ts, g(ts)+mean(sub), lwd=3)


## Not run: 
#
# Handle a 2-sample problem in which curves are fitted 
# separately for males and females and we wish to estimate the
# difference in the time-response curves for the two sexes.  
# The objective criterion will be taken by plot.rm.boot as the 
# total of the two sums of squared errors for the two models
#
knots &lt;- rcspline.eval(c(time.f,time.m), nk=6, knots.only=TRUE)
# Use same knots for both sexes, and use a times vector that 
# uses a range of times that is included in the measurement 
# times for both sexes
#
tm &lt;- seq(max(min(time.f),min(time.m)),
          min(max(time.f),max(time.m)),length=100)


f.female &lt;- rm.boot(time.f, bp.f, id.f, knots=knots, times=tm)
f.male   &lt;- rm.boot(time.m, bp.m, id.m, knots=knots, times=tm)
plot(f.female)
plot(f.male)
# The following plots female minus male response, with 
# a sequence of shaded confidence band for the difference
plot(f.female,f.male,multi=TRUE)


# Do 1000 simulated analyses to check simultaneous coverage 
# probability.  Use a null regression model with Gaussian errors


n.per.pt &lt;- 30
n.pt     &lt;- 10


null.in.region &lt;- 0


for(i in 1:1000) {
  y    &lt;- rnorm(n.pt*n.per.pt)
  time &lt;- rep(1:n.per.pt, n.pt)
#  Add the following line and add ,id=id to rm.boot to use clustering
#  id   &lt;- sort(rep(1:n.pt, n.per.pt))
#  Because we are ignoring patient id, this simulation is effectively
#  using 1 point from each of 300 patients, with times 1,2,3,,,30 


  f &lt;- rm.boot(time, y, B=500, nk=5, bootstrap.type='x fixed')
  g &lt;- plot(f, ylim=c(-1,1), pointwise=FALSE)
  null.in.region &lt;- null.in.region + all(g$lower&lt;=0 &amp; g$upper&gt;=0)
  prn(c(i=i,null.in.region=null.in.region))
}


# Simulation Results: 905/1000 simultaneous confidence bands 
# fully contained the horizontal line at zero

## End(Not run)
</code></pre>

<hr>
<h2 id='rMultinom'>Generate Multinomial Random Variables with Varying Probabilities</h2><span id='topic+rMultinom'></span>

<h3>Description</h3>

<p>Given a matrix of multinomial probabilities where rows correspond to
observations and columns to categories (and each row sums to 1),
generates a matrix with the same number of rows as has <code>probs</code> and
with <code>m</code> columns.  The columns represent multinomial cell numbers,
and within a row the columns are all samples from the same multinomial
distribution.  The code is a modification of that in the
<code>impute.polyreg</code> function in the <code>MICE</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rMultinom(probs, m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rMultinom_+3A_probs">probs</code></td>
<td>
<p>matrix of probabilities</p>
</td></tr>
<tr><td><code id="rMultinom_+3A_m">m</code></td>
<td>
<p>number of samples for each row of <code>probs</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>an integer matrix having <code>m</code> columns
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+rbinom">rbinom</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
w &lt;- rMultinom(rbind(c(.1,.2,.3,.4),c(.4,.3,.2,.1)),200)
t(apply(w, 1, table)/200)
</code></pre>

<hr>
<h2 id='runifChanged'>runifChanged</h2><span id='topic+runifChanged'></span>

<h3>Description</h3>

<p>Re-run Code if an Input Changed
</p>


<h3>Usage</h3>

<pre><code class='language-R'>runifChanged(fun, ..., file = NULL, .print. = TRUE, .inclfun. = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="runifChanged_+3A_fun">fun</code></td>
<td>
<p>the (usually slow) function to run</p>
</td></tr>
<tr><td><code id="runifChanged_+3A_...">...</code></td>
<td>
<p>input objects the result of running the function is dependent on</p>
</td></tr>
<tr><td><code id="runifChanged_+3A_file">file</code></td>
<td>
<p>file in which to store the result of <code>fun</code> augmented by attributes containing hash digests</p>
</td></tr>
<tr><td><code id="runifChanged_+3A_.print.">.print.</code></td>
<td>
<p>set to <code>TRUE</code> to list which objects changed that neessitated re-running <code>f</code></p>
</td></tr>
<tr><td><code id="runifChanged_+3A_.inclfun.">.inclfun.</code></td>
<td>
<p>set to <code>FALSE</code> to not include <code>fun</code> in the hash digest, i.e., to not require re-running <code>fun</code> if only <code>fun</code> itself has changed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses <code>hashCheck</code> to run a function and save the results if specified inputs have changed, otherwise to retrieve results from a file.  This makes it easy to see if any objects changed that require re-running a long simulation, and reports on any changes.  The file name is taken as the chunk name appended with <code>.rds</code> unless it is given as <code style="white-space: pre;">&#8288;file=&#8288;</code>.  <code>fun</code> has no arguments.  Set <code>.inclfun.=FALSE</code> to not include <code>fun</code> in the hash check (for legacy uses).  The typical workflow is as follows.
</p>
<div class="sourceCode"><pre>f &lt;- function(       ) {
# . . . do the real work with multiple function calls ...
}
seed &lt;- 3
set.seed(seed)
w &lt;- runifChanged(f, seed, obj1, obj2, ....)
</pre></div>
<p><code style="white-space: pre;">&#8288;seed, obj1, obj2&#8288;</code>, ... are all the objects that <code>f()</code> uses that if changed
would give a different result of <code>f()</code>.  This can include functions such as
those in a package, and <code>f</code> will be re-run if any of the function's code
changes.  <code>f</code> is also re-run if the code inside <code>f</code> changes.
The result of <code>f</code> is stored with <code>saveRDS</code> by default in file named <code>xxx.rds</code>
where <code>xxx</code> is the label for the current chunk.  To control this use instead
<code>file=xxx.rds</code> add the file argument to <code>runifChanged(...)</code>.  If nothing has
changed and the file already exists, the file is read to create the result
object (e.g., <code>w</code> above).  If <code>f()</code> needs to be run, the hashed input objects
are stored as attributes for the result then the enhanced result is written to the file.
</p>
<p>See <a href="https://hbiostat.org/rflow/caching.html">here</a> for examples.
</p>


<h3>Value</h3>

<p>the result of running <code>fun</code>
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

<hr>
<h2 id='runParallel'>runParallel</h2><span id='topic+runParallel'></span>

<h3>Description</h3>

<p>parallel Package Easy Front-End
</p>


<h3>Usage</h3>

<pre><code class='language-R'>runParallel(
  onecore,
  reps,
  seed = round(runif(1, 0, 10000)),
  cores = max(1, parallel::detectCores() - 1),
  simplify = TRUE,
  along
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="runParallel_+3A_onecore">onecore</code></td>
<td>
<p>function to run the analysis on one core</p>
</td></tr>
<tr><td><code id="runParallel_+3A_reps">reps</code></td>
<td>
<p>total number of repetitions</p>
</td></tr>
<tr><td><code id="runParallel_+3A_seed">seed</code></td>
<td>
<p>species the base random number seed.  The seed used for core i will be <code>seed</code> + <code>i</code>.</p>
</td></tr>
<tr><td><code id="runParallel_+3A_cores">cores</code></td>
<td>
<p>number of cores to use, defaulting to one less than the number available</p>
</td></tr>
<tr><td><code id="runParallel_+3A_simplify">simplify</code></td>
<td>
<p>set to FALSE to not create an outer list if a <code>onecore</code> result has only one element</p>
</td></tr>
<tr><td><code id="runParallel_+3A_along">along</code></td>
<td>
<p>see Details</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a function <code>onecore</code> that runs the needed set of simulations on
one CPU core, and given a total number of repetitions <code>reps</code>, determines
the number of available cores and by default uses one less than that.
By default the number of cores is one less than the number available
on your machine.
reps is divided as evenly as possible over these cores, and batches
are run on the cores using the <code>parallel</code> package <code>mclapply</code> function.
The current per-core repetition number is continually updated in
your system's temporary directory (/tmp for Linux and Mac, TEMP for Windows)
in a file name progressX.log where X is the core number.
The random number seed is set for each core and is equal to
the scalar <code>seed</code> - core number + 1.  The default seed is a random
number between 0 and 10000 but it's best if the user provides the
seed so the simulation is reproducible.
The total run time is computed and printed
onefile must create a named list of all the results created during
that one simulation batch.  Elements of this list must be data frames,
vectors, matrices, or arrays.   Upon completion of all batches,
all the results are rbind'd and saved in a single list.
</p>
<p>onecore must have an argument <code>reps</code> that will tell the function
how many simulations to run for one batch, another argument <code>showprogress</code>
which is a function to be called inside onecore to write to the
progress file for the current core and repetition, and an argument <code>core</code>
which informs <code>onecore</code> which sequential core number (batch number) it is
processing.
When calling <code>showprogress</code> inside <code>onecore</code>, the arguments, in order,
must be the integer value of the repetition to be noted, the number of reps,
<code>core</code>, an optional 4th argument <code>other</code> that can contain a single
character string to add to the output, and an optional 5th argument <code>pr</code>.
You can set <code>pr=FALSE</code> to suppress printing and have <code>showprogress</code>
return the file name for holding progress information if you want to
customize printing.
</p>
<p>If any of the objects appearing as list elements produced by onecore
are multi-dimensional arrays, you must specify an integer value for
<code>along</code>.  This specifies to the <code>abind</code> package <code>abind</code> function
the dimension along which to bind the arrays.  For example, if the
first dimension of the array corresponding to repetitions, you would
specify along=1.   All arrays present must use the same <code>along</code> unless
<code>along</code> is a named vector and the names match elements of the
simulation result object.
Set <code>simplify=FALSE</code> if you don't want the result simplified if
onecore produces only one list element.  The default returns the
first (and only) list element rather than the list if there is only one
element.
</p>
<p>See <a href="https://hbiostat.org/rflow/parallel.html">here</a> for examples.
</p>


<h3>Value</h3>

<p>result from combining all the parallel runs, formatting as similar to the result produced from one run as possible
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

<hr>
<h2 id='samplesize.bin'>
Sample Size for 2-sample Binomial
</h2><span id='topic+samplesize.bin'></span>

<h3>Description</h3>

<p>Computes sample size(s) for 2-sample binomial problem given vector or
scalar probabilities in the two groups.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>samplesize.bin(alpha, beta, pit, pic, rho=0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="samplesize.bin_+3A_alpha">alpha</code></td>
<td>

<p>scalar ONE-SIDED test size, or two-sided size/2
</p>
</td></tr>
<tr><td><code id="samplesize.bin_+3A_beta">beta</code></td>
<td>

<p>scalar or vector of powers
</p>
</td></tr>
<tr><td><code id="samplesize.bin_+3A_pit">pit</code></td>
<td>

<p>hypothesized treatment probability of success
</p>
</td></tr>
<tr><td><code id="samplesize.bin_+3A_pic">pic</code></td>
<td>

<p>hypothesized control probability of success
</p>
</td></tr>
<tr><td><code id="samplesize.bin_+3A_rho">rho</code></td>
<td>

<p>proportion of the sample devoted to treated group (<code class="reqn">0 &lt;\code{rho} &lt; 1</code>)
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TOTAL sample size(s)
</p>


<h3>AUTHOR</h3>

<p>Rick Chappell<br />
Dept. of Statistics and Human Oncology<br />
University of Wisconsin at Madison<br />
<a href="mailto:chappell@stat.wisc.edu">chappell@stat.wisc.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>alpha &lt;- .05
beta &lt;- c(.70,.80,.90,.95)


# N1 is a matrix of total sample sizes whose
# rows vary by hypothesized treatment success probability and
# columns vary by power
# See Meinert's book for formulae.


N1 &lt;- samplesize.bin(alpha, beta, pit=.55, pic=.5)
N1 &lt;- rbind(N1, samplesize.bin(alpha, beta, pit=.60, pic=.5))
N1 &lt;- rbind(N1, samplesize.bin(alpha, beta, pit=.65, pic=.5))
N1 &lt;- rbind(N1, samplesize.bin(alpha, beta, pit=.70, pic=.5))
attr(N1,"dimnames") &lt;- NULL


#Accounting for 5% noncompliance in the treated group
inflation &lt;- (1/.95)**2
print(round(N1*inflation+.5,0))
</code></pre>

<hr>
<h2 id='sas.get'>Convert a SAS Dataset to an S Data Frame</h2><span id='topic+sas.get'></span><span id='topic+is.special.miss'></span><span id='topic++5B.special.miss'></span><span id='topic+print.special.miss'></span><span id='topic+format.special.miss'></span><span id='topic+sas.codes'></span><span id='topic+code.levels'></span><span id='topic+timePOSIXt'></span>

<h3>Description</h3>

<p>Converts a <abbr><span class="acronym">SAS</span></abbr> dataset into an S data frame.  
You may choose to extract only a subset of variables 
or a subset of observations in the <abbr><span class="acronym">SAS</span></abbr> dataset.
You may have the function automatically convert </p>
<pre>PROC FORMAT</pre><p>-coded
variables to factor objects.  The original <abbr><span class="acronym">SAS</span></abbr> codes are stored in an
attribute called <code>sas.codes</code> and these may be added back to the
<code>levels</code> of a <code>factor</code> variable using the <code>code.levels</code> function.
Information about special missing values may be captured in an attribute
of each variable having special missing values.  This attribute is
called <code>special.miss</code>, and such variables are given class <code>special.miss</code>.
There are <code>print</code>, <code>[]</code>, <code>format</code>, and <code>is.special.miss</code>
methods for such variables.
The <code>chron</code> function is used to set up date, time, and date-time variables.
If using S-Plus 5 or 6 or later, the <code>timeDate</code> function is used
instead.
Under R, <code><a href="base.html#topic+Dates">Dates</a></code> is used for dates and <code><a href="chron.html#topic+chron">chron</a></code>
for date-times.  For times without
dates, these still need to be stored in date-time format in POSIX.
Such <abbr><span class="acronym">SAS</span></abbr> time variables are given a major class of <code>POSIXt</code> and a
<code>format.POSIXt</code> function so that the date portion (which will
always be 1/1/1970) will not print by default.
If a date variable represents a partial date (0.5 added if
month missing, 0.25 added if day missing, 0.75 if both), an attribute
<code>partial.date</code> is added to the variable, and the variable also becomes
a class <code>imputed</code> variable.
The <code>describe</code> function uses information about partial dates and
special missing values.
There is an option to automatically uncompress (or <code>gunzip</code>) compressed
<abbr><span class="acronym">SAS</span></abbr> datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sas.get(libraryName, member, variables=character(0), ifs=character(0),
     format.library=libraryName, id,
     dates.=c("sas","yymmdd","yearfrac","yearfrac2"),
     keep.log=TRUE, log.file="_temp_.log", macro=sas.get.macro,
     data.frame.out=existsFunction("data.frame"), clean.up=FALSE, quiet=FALSE,
     temp=tempfile("SaS"), formats=TRUE, recode=formats,
     special.miss=FALSE, sasprog="sas", 
     as.is=.5, check.unique.id=TRUE, force.single=FALSE,
     pos, uncompress=FALSE, defaultencoding="latin1", var.case="lower")

is.special.miss(x, code)

## S3 method for class 'special.miss'
x[..., drop=FALSE]

## S3 method for class 'special.miss'
print(x, ...)

## S3 method for class 'special.miss'
format(x, ...)

sas.codes(object)

code.levels(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sas.get_+3A_libraryname">libraryName</code></td>
<td>

<p>character string naming the directory in which the dataset is kept.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_drop">drop</code></td>
<td>

<p>logical. If <code>TRUE</code> the result is coerced to the
lowest possible dimension.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_member">member</code></td>
<td>

<p>character string giving the second part of the two part <abbr><span class="acronym">SAS</span></abbr> dataset name.  
(The first part is irrelevant here - it is mapped to the UNIX directory name.)
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_x">x</code></td>
<td>

<p>a variable that may have been created by <code>sas.get</code> with
<code>special.miss=T</code> or with <code>recode</code> in effect.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_variables">variables</code></td>
<td>

<p>vector of character strings naming the variables in the <abbr><span class="acronym">SAS</span></abbr> dataset.  
The S dataset will contain only those variables from the
<abbr><span class="acronym">SAS</span></abbr> dataset.  
To get all of the variables (the default), an empty string may be given.
It is a fatal error if any one of the variables is not
in the <abbr><span class="acronym">SAS</span></abbr> dataset.  You can use <code>sas.contents</code> to get
the variables in the <abbr><span class="acronym">SAS</span></abbr> dataset.
If you have retrieved a subset of the variables
in the <abbr><span class="acronym">SAS</span></abbr> dataset and which to retrieve the same list of variables
from another dataset, you can program the value of <code>variables</code> - see
one of the last examples.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_ifs">ifs</code></td>
<td>

<p>a vector of character strings, each containing one <abbr><span class="acronym">SAS</span></abbr> &ldquo;subsetting if&rdquo;
statement.  
These will be used to extract a subset of the observations in the <abbr><span class="acronym">SAS</span></abbr> dataset.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_format.library">format.library</code></td>
<td>

<p>The UNIX directory containing the file &lsquo;<span class="file">formats.sct</span>&rsquo;, which contains
the definitions of the user defined formats used in this dataset.
By default, we look for the formats in the same directory as the data.
The user defined formats must be available (so <abbr><span class="acronym">SAS</span></abbr> can read the data).
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_formats">formats</code></td>
<td>

<p>Set <code>formats</code> to <code>FALSE</code> to keep <code>sas.get</code> from telling the <abbr><span class="acronym">SAS</span></abbr> macro to 
retrieve value label formats from <code>format.library</code>.  When you do not
specify <code>formats</code> or <code>recode</code>, <code>sas.get</code> will set <code>format</code> to <code>TRUE</code> if a
<abbr><span class="acronym">SAS</span></abbr> format catalog (&lsquo;<span class="file">.sct</span>&rsquo; or &lsquo;<span class="file">.sc2</span>&rsquo;) file exists in <code>format.library</code>.
Value label formats if present are stored as the <code>formats</code> attribute of the returned
object (see below). A format is used if it is referred to by one or more 
variables
in the dataset, if it contains no ranges of values (i.e., it identifies
value labels for single values), and if it is a character format
or a numeric format that is not used just to label missing values.
If you set <code>recode</code> to <code>TRUE</code>, 1, or 2, <code>formats</code> defaults to <code>TRUE</code>.
To fetch the values and labels for variable <code>x</code> in the dataset <code>d</code> you
could type:
<br />
<code>f &lt;- attr(d\$x, "format")</code>
<br />
<code>formats &lt;- attr(d, "formats")</code>
<br />
<code>formats\$f\$values; formats\$f\$labels</code>
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_recode">recode</code></td>
<td>

<p>This parameter defaults to <code>TRUE</code> if <code>formats</code> is <code>TRUE</code>.  If it is
<code>TRUE</code>, variables that have an appropriate format (see above) are
recoded as <code>factor</code> objects, which map the values
to the value labels for the format.  Alternatively, set <code>recode</code> to
1 to use labels of the form value:label, e.g. 1:good 2:better 3:best.
Set <code>recode</code> to 2 to use labels such as good(1) better(2) best(3).
Since <code>sas.codes</code> and <code>code.levels</code> add flexibility, the usual choice
for <code>recode</code> is <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_special.miss">special.miss</code></td>
<td>

<p>For numeric variables, any missing values are stored as NA in S.
You can recover special missing values by setting <code>special.miss</code> to
<code>TRUE</code>.  This will cause the <code>special.miss</code> attribute and the
<code>special.miss</code> class to be added
to each variable that has at least one special missing value.  
Suppose that variable  <code>y</code> was .E in observation 3 and .G
in observation 544.  The <code>special.miss</code> attribute for <code>y</code> then has the
value
<br />
<code>list(codes=c("E","G"),obs=c(3,544))</code>
<br />
To fetch this information for variable <code>y</code> you would say for example
<br />
<code>s &lt;- attr(y, "special.miss")</code>
<br />
<code>s\$codes; s\$obs</code>
<br />
or use <code>is.special.miss(x)</code> or the <code>print.special.miss</code> method, which
will replace <code>NA</code> values for the variable with &lsquo;<span class="samp">&#8288;E&#8288;</span>&rsquo; or &lsquo;<span class="samp">&#8288;G&#8288;</span>&rsquo; if they
correspond to special missing values.
The describe
function uses this information in printing a data summary.  
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_id">id</code></td>
<td>

<p>The name of the variable to be used as the row names of the S dataset.
The id variable becomes the <code>row.names</code> attribute of a data frame, but
the id variable is still retained as a variable in the data frame.
(if <code>data.frame.out</code> is <code>FALSE</code>, this will be the attribute &lsquo;<span class="samp">&#8288;id&#8288;</span>&rsquo; of the <span class="rlang"><b>R</b></span>
dataset.)  You can also specify a vector of variable names as the <code>id</code>
parameter.  After fetching the data from <abbr><span class="acronym">SAS</span></abbr>, all these variables will be
converted to character format and concatenated (with a space as a separator)
to form a (hopefully) unique identification variable.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_dates.">dates.</code></td>
<td>

<p>specifies the format for storing <abbr><span class="acronym">SAS</span></abbr> dates in the
resulting data frame
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_as.is">as.is</code></td>
<td>

<p>IF <code>data.frame.out = TRUE</code>, <abbr><span class="acronym">SAS</span></abbr> character variables are converted to S factor
objects if <code>as.is = FALSE</code> or if <code>as.is</code> is a number between 0 and 1 inclusive and
the number of unique values of the variable is less than
the number of observations (<code>n</code>) times <code>as.is</code>.  The default if <code>as.is</code> is 0.5,
so character variables are converted to factors only if they have fewer
than <code>n/2</code> unique values.  The primary purpose of this is to keep unique
identification variables as character values in the data frame instead
of using more space to store both the integer factor codes and the
factor labels.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_check.unique.id">check.unique.id</code></td>
<td>

<p>If <code>id</code> is specified, the row names are checked for
uniqueness if <code>check.unique.id = TRUE</code>.  If any are duplicated, a warning
is printed.  Note that if a data frame is being created with duplicate
row names, statements such as <code>my.data.frame["B23",]</code> will retrieve
only the first row with a row name of </p>
<pre>B23</pre><p>.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_force.single">force.single</code></td>
<td>

<p>By default, <abbr><span class="acronym">SAS</span></abbr> numeric variables having <code class="reqn">LENGTH &gt; 4</code> are stored as
S double precision numerics, which allow for the same precision as
a <abbr><span class="acronym">SAS</span></abbr> </p>
<pre>LENGTH</pre><p> 8 variable.  Set <code>force.single = TRUE</code> to store every
numeric variable in single precision (7 digits of precision).
This option is useful when the creator of the <abbr><span class="acronym">SAS</span></abbr> dataset has
failed to use a </p>
<pre>LENGTH</pre><p> statement.
R does not have single precision, so no attempt is made to convert to
single if running R.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_dates">dates</code></td>
<td>

<p>One of the character strings <code>"sas"</code>, <code>"yearfrac"</code>, <code>"yearfrac2"</code>, <code>"yymmdd"</code>.
If a <abbr><span class="acronym">SAS</span></abbr> variable has a date format (one of <code>"DATE"</code>, <code>"MMDDYY"</code>, <code>"YYMMDD"</code>,
<code>"DDMMYY"</code>, <code>"YYQ"</code>, <code>"MONYY"</code>, <code>"JULIAN"</code>), it will be converted to the format
specified by <code>dates</code> before being given to S.  <code>"sas"</code> gives
days from 1/1/1960 (from 1/1/1970 if using <code>chron</code>), 
<code>"yearfrac"</code> gives days from 1/1/1900 divided by
365.25, <code>"yearfrac2"</code> gives year plus fraction of current year,
and <code>"yymmdd"</code> gives a 6 digit number </p>
<pre>YYMMDD</pre><p> (year%%100, month, day).
Note that <span class="rlang"><b>R</b></span> will store these as numbers, not as
character strings.  If <code>dates="sas"</code> and a variable has one of the <abbr><span class="acronym">SAS</span></abbr>
date formats listed above, the variable will be given a class of &lsquo;<span class="samp">&#8288;date&#8288;</span>&rsquo;
to work with Terry Therneau's implementation of the &lsquo;<span class="samp">&#8288;date&#8288;</span>&rsquo; class in S.
If the <code>chron</code> package or <code>timeDate</code> function is available, these are
used instead.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_keep.log">keep.log</code></td>
<td>

<p>logical flag: if <code>FALSE</code>, delete the <abbr><span class="acronym">SAS</span></abbr> log file upon completion.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_log.file">log.file</code></td>
<td>

<p>the name of the <abbr><span class="acronym">SAS</span></abbr> log file.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_macro">macro</code></td>
<td>

<p>the name of an S object in the current search path that contains the text of
the <abbr><span class="acronym">SAS</span></abbr> macro called by <span class="rlang"><b>R</b></span>. The <span class="rlang"><b>R</b></span> object is a character vector that
can be edited using for example <code>sas.get.macro &lt;- editor(sas.get.macro)</code>.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_data.frame.out">data.frame.out</code></td>
<td>

<p>logical flag: if <code>TRUE</code>, the return value will be an S data frame,
otherwise it will be a list.  
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_clean.up">clean.up</code></td>
<td>

<p>logical flag: if <code>TRUE</code>, remove all temporary files when finished.  You
may want to keep these while debugging the <abbr><span class="acronym">SAS</span></abbr> macro.  Not needed for <span class="rlang"><b>R</b></span>.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_quiet">quiet</code></td>
<td>

<p>logical flag: if <code>FALSE</code>, print the contents of the <abbr><span class="acronym">SAS</span></abbr> log file if 
there has been an error.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_temp">temp</code></td>
<td>

<p>the prefix to use for the temporary files.  Two characters
will be added to this, the resulting name
must fit on your file system.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_sasprog">sasprog</code></td>
<td>

<p>the name of the system command to invoke <abbr><span class="acronym">SAS</span></abbr>
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_uncompress">uncompress</code></td>
<td>

<p>set to <code>TRUE</code> to automatically invoke the <abbr><span class="acronym">UNIX</span></abbr> <code>gunzip</code> command
(if &lsquo;<span class="file"><var>member</var>.ssd01.gz</span>&rsquo; exists) or the <code>uncompress</code> command 
(if &lsquo;<span class="file"><var>member</var>.ssd01.Z</span>&rsquo; exists) to uncompress the <abbr><span class="acronym">SAS</span></abbr> dataset before
proceeding.  This assumes you have the file permissions to allow
uncompressing in place.  If the file is already uncompressed, this
option is ignored.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_pos">pos</code></td>
<td>

<p>by default, a list or data frame which contains all the variables is returned.
If you specify <code>pos</code>, each individual variable is placed into a
separate object (whose name is the name of the variable) using the
<code>assign</code> function with the <code>pos</code> argument.  For example, you can
put each variable in its own file in a directory, which in some cases
may save memory over attaching a data frame.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_code">code</code></td>
<td>

<p>a special missing value code (&lsquo;<span class="samp">&#8288;A&#8288;</span>&rsquo; through &lsquo;<span class="samp">&#8288;Z&#8288;</span>&rsquo; or &lsquo;<span class="samp">&#8288;\_&#8288;</span>&rsquo;) to check
against.  If <code>code</code> is omitted, <code>is.special.miss</code> will return
a <code>TRUE</code> for each observation that has any special missing value.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_defaultencoding">defaultencoding</code></td>
<td>

<p>encoding to assume if the SAS dataset does not specify one. Defaults to &quot;latin1&quot;.
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_var.case">var.case</code></td>
<td>
<p>default is to change case of SAS variable names to
lower case.  Specify alternatively <code>"upper"</code> or <code>"preserve"</code>.</p>
</td></tr>
<tr><td><code id="sas.get_+3A_object">object</code></td>
<td>

<p>a variable in a data frame created by <code>sas.get</code>
</p>
</td></tr>
<tr><td><code id="sas.get_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If you specify <code>special.miss = TRUE</code> and there are no special missing
values in the data <abbr><span class="acronym">SAS</span></abbr> dataset, the <abbr><span class="acronym">SAS</span></abbr> step will bomb.
</p>
<p>For variables having a </p>
<pre>PROC FORMAT VALUE</pre>
<p>format with some of the levels undefined, <code>sas.get</code> will interpret those
values as <code>NA</code> if you are using <code>recode</code>.
</p>
<p>The <abbr><span class="acronym">SAS</span></abbr> macro &lsquo;<span class="file">sas_get</span>&rsquo; uses record lengths of up to 4096 in two
places.  If you are exporting records that are very long (because of
a large number of variables and/or long character variables), you
may want to edit these </p>
<pre>LRECL</pre><p>s to quadruple them, for example.
</p>


<h3>Value</h3>

<p>if <code>data.frame.out</code> is <code>TRUE</code>, the output will
be a data frame resembling the <abbr><span class="acronym">SAS</span></abbr> dataset.  If <code>id</code>
was specified, that column of the data frame will be used
as the row names of the data frame.  Each variable in the data frame
or vector in the list will have the attributes <code>label</code> and <code>format</code>
containing <abbr><span class="acronym">SAS</span></abbr> labels and formats.  Underscores in formats are
converted to periods.  Formats for character variables have <code>\$</code> placed
in front of their names.
If <code>formats</code> is <code>TRUE</code> and there are any 
appropriate format definitions in <code>format.library</code>, the returned
object will have attribute <code>formats</code> containing lists named the
same as the format names (with periods substituted for underscores and
character formats prefixed by <code>\$</code>).
Each of these lists has a vector called <code>values</code> and one called
<code>labels</code> with the </p>
<pre>PROC FORMAT; VALUE ...</pre><p>   definitions.
</p>
<p>If <code>data.frame.out</code> is <code>FALSE</code>, the output will
be a list of vectors, each containing a variable from the <abbr><span class="acronym">SAS</span></abbr>
dataset.  If <code>id</code> was specified, that element of the list will
be used as the <code>id</code> attribute of the entire list.
</p>


<h3>Side Effects</h3>

<p>if a <abbr><span class="acronym">SAS</span></abbr> error occurs and <code>quiet</code> is <code>FALSE</code>, then the <abbr><span class="acronym">SAS</span></abbr> log file will be
printed under the control of  the <code>less</code> pager.
</p>


<h3>BACKGROUND</h3>

<p>The references cited below explain the structure of <abbr><span class="acronym">SAS</span></abbr> datasets and how
they are stored under <abbr><span class="acronym">UNIX</span></abbr>.
See <em><abbr><span class="acronym">SAS</span></abbr> Language</em> 
for a discussion of the &ldquo;subsetting if&rdquo; statement.
</p>


<h3>Note</h3>

<p>You must be able to run <abbr><span class="acronym">SAS</span></abbr> (by typing <code>sas</code>) on your system.
If the S command <code>!sas</code> does not start <abbr><span class="acronym">SAS</span></abbr>, then this function cannot work.
</p>
<p>If you are reading time or
date-time variables, you will need to execute the command <code>library(chron)</code>
to print those variables or the data frame if the <code>timeDate</code> function
is not available.
</p>


<h3>Author(s)</h3>

<p>Terry Therneau, Mayo Clinic
<br />
Frank Harrell, Vanderbilt University
<br />
Bill Dunlap, University of Washington and Insightful Corporation
<br />
Michael W. Kattan, Cleveland Clinic Foundation
<br />
Reinhold Koch (encoding)
</p>


<h3>References</h3>

<p><abbr><span class="acronym">SAS</span></abbr> Institute Inc. (1990).
<em><abbr><span class="acronym">SAS</span></abbr> Language: Reference, Version 6.</em>
First Edition.
<abbr><span class="acronym">SAS</span></abbr> Institute Inc., Cary, North Carolina.
</p>
<p><abbr><span class="acronym">SAS</span></abbr> Institute Inc. (1988).
<abbr><span class="acronym">SAS</span></abbr> Technical Report P-176,
<em>Using the <abbr><span class="acronym">SAS</span></abbr> System, Release 6.03, under UNIX Operating Systems and Derivatives.  </em>
<abbr><span class="acronym">SAS</span></abbr> Institute Inc., Cary, North Carolina.
</p>
<p><abbr><span class="acronym">SAS</span></abbr> Institute Inc. (1985).
<em><abbr><span class="acronym">SAS</span></abbr> Introductory Guide.</em>
Third Edition.
<abbr><span class="acronym">SAS</span></abbr> Institute Inc., Cary, North Carolina.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+data.frame">data.frame</a></code>, <code><a href="#topic+describe">describe</a></code>,
<code><a href="#topic+label">label</a></code>,
<code><a href="#topic+upData">upData</a></code>,
<code><a href="#topic+upData">cleanup.import</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sas.contents("saslib", "mice")
# [1] "dose"  "ld50"  "strain"  "lab_no"
attr(, "n"):
# [1] 117
mice &lt;- sas.get("saslib", mem="mice", var=c("dose", "strain", "ld50"))
plot(mice$dose, mice$ld50)


nude.mice &lt;- sas.get(lib=unix("echo $HOME/saslib"), mem="mice",
	ifs="if strain='nude'")


nude.mice.dl &lt;- sas.get(lib=unix("echo $HOME/saslib"), mem="mice",
	var=c("dose", "ld50"), ifs="if strain='nude'")


# Get a dataset from current directory, recode PROC FORMAT; VALUE \dots 
# variables into factors with labels of the form "good(1)" "better(2)",
# get special missing values, recode missing codes .D and .R into new
# factor levels "Don't know" and "Refused to answer" for variable q1
d &lt;- sas.get(".", "mydata", recode=2, special.miss=TRUE)
attach(d)
nl &lt;- length(levels(q1))
lev &lt;- c(levels(q1), "Don't know", "Refused")
q1.new &lt;- as.integer(q1)
q1.new[is.special.miss(q1,"D")] &lt;- nl+1
q1.new[is.special.miss(q1,"R")] &lt;- nl+2
q1.new &lt;- factor(q1.new, 1:(nl+2), lev)
# Note: would like to use factor() in place of as.integer \dots but
# factor in this case adds "NA" as a category level


d &lt;- sas.get(".", "mydata")
sas.codes(d$x)    # for PROC FORMATted variables returns original data codes
d$x &lt;- code.levels(d$x)   # or attach(d); x &lt;- code.levels(x)
# This makes levels such as "good" "better" "best" into e.g.
# "1:good" "2:better" "3:best", if the original SAS values were 1,2,3


# Retrieve the same variables from another dataset (or an update of
# the original dataset)
mydata2 &lt;- sas.get('mydata2', var=names(d))
# This only works if none of the original SAS variable names contained _
mydata2 &lt;- cleanup.import(mydata2) # will make true integer variables

# Code from Don MacQueen to generate SAS dataset to test import of
# date, time, date-time variables
# data ssd.test;
#     d1='3mar2002'd ;
#     dt1='3mar2002 9:31:02'dt;
#     t1='11:13:45't;
#     output;
#
#     d1='3jun2002'd ;
#     dt1='3jun2002 9:42:07'dt;
#     t1='11:14:13't;
#     output;
#     format d1 mmddyy10. dt1 datetime. t1 time.;
# run;

## End(Not run)
</code></pre>

<hr>
<h2 id='sasxport.get'>Enhanced Importing of SAS Transport Files using read.xport</h2><span id='topic+sasxport.get'></span><span id='topic+sasdsLabels'></span>

<h3>Description</h3>

<p>Uses the <code>read.xport</code> and <code>lookup.xport</code> functions in the
<code>foreign</code> library to import SAS datasets.  SAS date, time, and
date/time variables are converted respectively to <code>Date</code>, 
POSIX, or <code>POSIXct</code> objects in <span class="rlang"><b>R</b></span>, 
variable names are converted to lower case, SAS labels are associated
with variables, and (by default) integer-valued variables are converted
from storage mode <code>double</code> to <code>integer</code>.  If the user ran
<code>PROC FORMAT CNTLOUT=</code> in SAS and included the resulting dataset in
the SAS version 5 transport file, variables having customized formats
that do not include any ranges (i.e., variables having standard
<code>PROC FORMAT; VALUE</code> label formats) will have their format labels looked
up, and these variables are converted to S <code>factor</code>s.
</p>
<p>For those users having access to SAS, <code>method='csv'</code> is preferred
when importing several SAS datasets.
Run SAS macro <code>exportlib.sas</code> available from
<a href="https://github.com/harrelfe/Hmisc/blob/master/src/sas/exportlib.sas">https://github.com/harrelfe/Hmisc/blob/master/src/sas/exportlib.sas</a>
to convert all SAS datasets in a SAS data library (from any engine
supported by your system) into <code>CSV</code> files.  If any customized
formats are used, it is assumed that the <code>PROC FORMAT CNTLOUT=</code>
dataset is in the data library as a regular SAS dataset, as above.
</p>
<p><code>SASdsLabels</code> reads a file containing <code>PROC CONTENTS</code>
printed output to parse dataset labels, assuming that <code>PROC
CONTENTS</code> was run on an entire library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sasxport.get(file, lowernames=TRUE, force.single = TRUE,
             method=c('read.xport','dataload','csv'), formats=NULL, allow=NULL,
             out=NULL, keep=NULL, drop=NULL, as.is=0.5, FUN=NULL)
sasdsLabels(file)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sasxport.get_+3A_file">file</code></td>
<td>
<p>name of a file containing the SAS transport file.
<code>file</code> may be a URL beginning with <code>https://</code>.  For
<code>sasdsLabels</code>, <code>file</code> is the name of a file containing a
<code>PROC CONTENTS</code> output listing.  For <code>method='csv'</code>,
<code>file</code> is the name of the directory containing all the <code>CSV</code>
files created by running the <code>exportlib</code> SAS macro.
</p>
</td></tr>
<tr><td><code id="sasxport.get_+3A_lowernames">lowernames</code></td>
<td>
<p>set to <code>FALSE</code> to keep from converting SAS
variable names to lower case</p>
</td></tr>
<tr><td><code id="sasxport.get_+3A_force.single">force.single</code></td>
<td>
<p>set to <code>FALSE</code> to keep integer-valued
variables not exceeding <code class="reqn">2^31-1</code> in value from being converted to
<code>integer</code> storage mode</p>
</td></tr>
<tr><td><code id="sasxport.get_+3A_method">method</code></td>
<td>
<p>set to <code>"dataload"</code> if you have the <code>dataload</code>
executable installed and want to use it instead of
<code>read.xport</code>.  This seems to correct some errors in which
rarely some factor variables are always missing when read by
<code>read.xport</code> when in fact they have some non-missing values.</p>
</td></tr>
<tr><td><code id="sasxport.get_+3A_formats">formats</code></td>
<td>
<p>a data frame or list (like that created by
<code>read.xport</code>) containing <code>PROC FORMAT</code>
output, if such output is not stored in the main transport file.</p>
</td></tr>
<tr><td><code id="sasxport.get_+3A_allow">allow</code></td>
<td>
<p>a vector of characters allowed by <span class="rlang"><b>R</b></span> that should not be
converted to periods in variable names.  By default, underscores in
variable names are converted to periods as with <span class="rlang"><b>R</b></span> before version 1.9.</p>
</td></tr>
<tr><td><code id="sasxport.get_+3A_out">out</code></td>
<td>
<p>a character string specifying a directory in which to write
separate <span class="rlang"><b>R</b></span> <code>save</code> files (<code>.rda</code> files) for each regular
dataset.  Each file and the data frame inside it is named with the
SAS dataset name translated to lower case and with underscores
changed to periods.  The default <code>NULL</code> value of <code>out</code>
results in a data frame or a list of data frames being returned.
When <code>out</code> is given, <code>sasxport.get</code> returns only metadata (see
below), invisibly.
<code>out</code> only works with <code>methods='csv'</code>.  <code>out</code> should
not have a trailing slash.</p>
</td></tr>
<tr><td><code id="sasxport.get_+3A_keep">keep</code></td>
<td>
<p>a vector of names of SAS datasets to process (original SAS
upper case names).  Must include <code>PROC FORMAT</code> dataset if it
exists, and if the kept datasets use any of its value label formats.</p>
</td></tr>
<tr><td><code id="sasxport.get_+3A_drop">drop</code></td>
<td>
<p>a vector of names of SAS datasets to ignore (original SAS
upper case names)</p>
</td></tr>
<tr><td><code id="sasxport.get_+3A_as.is">as.is</code></td>
<td>

<p>SAS character variables are converted to S factor
objects if <code>as.is=FALSE</code> or if <code>as.is</code> is a number between
0 and 1 inclusive and the number of unique values of the variable is
less than the number of observations (<code>n</code>) times <code>as.is</code>.
The default if <code>as.is</code> is .5, so character variables are
converted to factors only if they have fewer than <code>n/2</code> unique
values.  The primary purpose of this is to keep unique
identification variables as character values in the data frame
instead of using more space to store both the integer factor codes
and the factor labels.
</p>
</td></tr>
<tr><td><code id="sasxport.get_+3A_fun">FUN</code></td>
<td>
<p>an optional function that will be run on each data frame
created, when <code>method='csv'</code> and <code>out</code> are specified.  The
result of all the <code>FUN</code> calls is made into a list corresponding
to the SAS datasets that are read.  This list is the <code>FUN</code>
attribute of the result returned by <code>sasxport.get</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+contents.list">contents.list</a></code> for a way to print the
directory of SAS datasets when more than one was imported.</p>


<h3>Value</h3>

<p>If there is more than one dataset in the transport file other than the
<code>PROC FORMAT</code> file, the result is a list of data frames
containing all the non-<code>PROC FORMAT</code> datasets.  Otherwise the
result is the single data frame.  There is an exception if <code>out</code>
is specified; that causes separate <span class="rlang"><b>R</b></span> <code>save</code> files to be written
and the returned value to be a list corresponding to the SAS datasets,
with key <code>PROC CONTENTS</code> information in a data frame making up
each part of the list.
<code>sasdsLabels</code> returns a named
vector of dataset labels, with names equal to the dataset names.
</p>


<h3>Author(s)</h3>

<p>Frank E Harrell Jr</p>


<h3>See Also</h3>

<p><code><a href="foreign.html#topic+read.xport">read.xport</a></code>,<code><a href="#topic+label">label</a></code>,<code><a href="#topic+sas.get">sas.get</a></code>,
<code><a href="base.html#topic+Dates">Dates</a></code>,<code><a href="base.html#topic+DateTimeClasses">DateTimeClasses</a></code>,
<code><a href="foreign.html#topic+lookup.xport">lookup.xport</a></code>,<code><a href="#topic+contents">contents</a></code>,<code><a href="#topic+describe">describe</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# SAS code to generate test dataset:
# libname y SASV5XPT "test2.xpt";
#
# PROC FORMAT; VALUE race 1=green 2=blue 3=purple; RUN;
# PROC FORMAT CNTLOUT=format;RUN;  * Name, e.g. 'format', unimportant;
# data test;
# LENGTH race 3 age 4;
# age=30; label age="Age at Beginning of Study";
# race=2;
# d1='3mar2002'd ;
# dt1='3mar2002 9:31:02'dt;
# t1='11:13:45't;
# output;
#
# age=31;
# race=4;
# d1='3jun2002'd ;
# dt1='3jun2002 9:42:07'dt;
# t1='11:14:13't;
# output;
# format d1 mmddyy10. dt1 datetime. t1 time. race race.;
# run;
# data z; LENGTH x3 3 x4 4 x5 5 x6 6 x7 7 x8 8;
#    DO i=1 TO 100;
#        x3=ranuni(3);
#        x4=ranuni(5);
#        x5=ranuni(7);
#        x6=ranuni(9);
#        x7=ranuni(11);
#        x8=ranuni(13);
#        output;
#        END;
#    DROP i;
#    RUN;
# PROC MEANS; RUN;
# PROC COPY IN=work OUT=y;SELECT test format z;RUN; *Creates test2.xpt;
w &lt;- sasxport.get('test2.xpt')
# To use an existing copy of test2.xpt available on the web:
w &lt;- sasxport.get('https://github.com/harrelfe/Hmisc/raw/master/inst/tests/test2.xpt')

describe(w$test)   # see labels, format names for dataset test
# Note: if only one dataset (other than format) had been exported,
# just do describe(w) as sasxport.get would not create a list for that
lapply(w, describe)# see descriptive stats for both datasets
contents(w$test)   # another way to see variable attributes
lapply(w, contents)# show contents of both datasets
options(digits=7)  # compare the following matrix with PROC MEANS output
t(sapply(w$z, function(x)
 c(Mean=mean(x),SD=sqrt(var(x)),Min=min(x),Max=max(x))))

## End(Not run)
</code></pre>

<hr>
<h2 id='Save'>Faciliate Use of save and load to Remote Directories</h2><span id='topic+Save'></span><span id='topic+Load'></span>

<h3>Description</h3>

<p>These functions are slightly enhanced versions of <code>save</code> and
<code>load</code> that allow a target directory to be specified using
<code>options(LoadPath="pathname")</code>.  If the <code>LoadPath</code> option is
not set, the current working directory is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># options(LoadPath='mypath')
Save(object, name=deparse(substitute(object)), compress=TRUE)
Load(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Save_+3A_object">object</code></td>
<td>
<p>the name of an object, usually a data frame.  It must
not be quoted.</p>
</td></tr>
<tr><td><code id="Save_+3A_name">name</code></td>
<td>
<p>an optional name to assign to the object and file name
prefix, if the argument name is not used</p>
</td></tr>
<tr><td><code id="Save_+3A_compress">compress</code></td>
<td>
<p>see <code><a href="base.html#topic+save">save</a></code>.  Default is <code>TRUE</code>
which corresponds to <code>gzip</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>Save</code> creates a temporary version of the object under the name
given by the user, so that <code>save</code> will internalize this name.
Then subsequent <code>Load</code> or <code>load</code> will cause an object of the
original name to be created in the global environment.  The name of
the <span class="rlang"><b>R</b></span> data file is assumed to be the name of the object (or the value
of <code>name</code>) appended with <code>".rda"</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+save">save</a></code>, <code><a href="base.html#topic+load">load</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
d &lt;- data.frame(x=1:3, y=11:13)
options(LoadPath='../data/rda')
Save(d)   # creates ../data/rda/d.rda
Load(d)   # reads   ../data/rda/d.rda
Save(d, 'D')   # creates object D and saves it in .../D.rda

## End(Not run)
</code></pre>

<hr>
<h2 id='scat1d'>One-Dimensional Scatter Diagram, Spike Histogram, or Density</h2><span id='topic+scat1d'></span><span id='topic+jitter2'></span><span id='topic+jitter2.default'></span><span id='topic+jitter2.data.frame'></span><span id='topic+datadensity'></span><span id='topic+datadensity.data.frame'></span><span id='topic+histSpike'></span><span id='topic+histSpikeg'></span><span id='topic+ecdfpM'></span>

<h3>Description</h3>

<p><code>scat1d</code> adds tick marks (bar codes. rug plot) on any of the four
sides of an existing plot, corresponding with non-missing values of a
vector <code>x</code>.  This is used to show the data density.  Can also
place the tick marks along a curve by specifying y-coordinates to go
along with the <code>x</code> values.
</p>
<p>If any two values of <code>x</code> are within <code class="reqn">\code{eps}*w</code> of
each other, where <code>eps</code> defaults to .001 and w is the span
of the intended axis, values of <code>x</code> are jittered by adding a
value uniformly distributed in <code class="reqn">[-\code{jitfrac}*w,
  \code{jitfrac}*w]</code>, where <code>jitfrac</code> defaults to
.008. Specifying <code>preserve=TRUE</code> invokes <code>jitter2</code> with a
different logic of jittering. Allows plotting random sub-segments to
handle very large <code>x</code> vectors (see<code>tfrac</code>).
</p>
<p><code>jitter2</code> is a generic method for jittering, which does not add
random noise. It retains unique values and ranks, and randomly spreads
duplicate values at equidistant positions within limits of enclosing
values. <code>jitter2</code> is especially useful for numeric variables with
discrete values, like rating scales. Missing values are allowed and
are returned. Currently implemented methods are <code>jitter2.default</code>
for vectors and <code>jitter2.data.frame</code> which returns a data.frame
with each numeric column jittered.
</p>
<p><code>datadensity</code> is a generic method used to show data densities in
more complex situations.  Here, another <code>datadensity</code> method is
defined for data frames. Depending on the <code>which</code> argument, some
or all of the variables in a data frame will be displayed, with
<code>scat1d</code> used to display continuous variables and, by default,
bars used to display frequencies of categorical, character, or
discrete numeric variables.  For such variables, when the total length
of value labels exceeds 200, only the first few characters from each
level are used. By default, <code>datadensity.data.frame</code> will
construct one axis (i.e., one strip) per variable in the data frame.
Variable names appear to the left of the axes, and the number of
missing values (if greater than zero) appear to the right of the axes.
An optional <code>group</code> variable can be used for stratification,
where the different strata are depicted using different colors.  If
the <code>q</code> vector is specified, the desired quantiles (over all
<code>group</code>s) are displayed with solid triangles below each axis.
</p>
<p>When the sample size exceeds 2000 (this value may be modified using
the <code>nhistSpike</code> argument, <code>datadensity</code> calls
<code>histSpike</code> instead of <code>scat1d</code> to show the data density for
numeric variables.  This results in a histogram-like display that
makes the resulting graphics file much smaller.  In this case,
<code>datadensity</code> uses the <code>minf</code> argument (see below) so that
very infrequent data values will not be lost on the variable's axis,
although this will slightly distortthe histogram.
</p>
<p><code>histSpike</code> is another method for showing a high-resolution data
distribution that is particularly good for very large datasets (say
<code class="reqn">\code{n} &gt; 1000</code>).  By default, <code>histSpike</code> bins the
continuous <code>x</code> variable into 100 equal-width bins and then
computes the frequency counts within bins (if <code>n</code> does not exceed
10, no binning is done). If <code>add=FALSE</code> (the default), the
function displays either proportions or frequencies as in a vertical
histogram.  Instead of bars, spikes are used to depict the
frequencies.  If <code>add=FALSE</code>, the function assumes you are adding
small density displays that are intended to take up a small amount of
space in the margins of the overall plot.  The <code>frac</code> argument is
used as with <code>scat1d</code> to determine the relative length of the
whole plot that is used to represent the maximum frequency.  No
jittering is done by <code>histSpike</code>.
</p>
<p><code>histSpike</code> can also graph a kernel density estimate for
<code>x</code>, or add a small density curve to any of 4 sides of an
existing plot.  When <code>y</code> or <code>curve</code> is specified, the
density or spikes are drawn with respect to the curve rather than the
x-axis.
</p>
<p><code>histSpikeg</code> is similar to <code>histSpike</code> but is for adding layers
to a <code>ggplot2</code> graphics object or traces to a <code>plotly</code>
object.
<code>histSpikeg</code> can also add <code>lowess</code> curves to the plot.
</p>
<p><code>ecdfpM</code> makes a <code>plotly</code> graph or series of graphs showing
possibly superposed empirical cumulative distribution functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scat1d(x, side=3, frac=0.02, jitfrac=0.008, tfrac,
       eps=ifelse(preserve,0,.001),
       lwd=0.1, col=par("col"),
       y=NULL, curve=NULL,
       bottom.align=FALSE,
       preserve=FALSE, fill=1/3, limit=TRUE, nhistSpike=2000, nint=100,
       type=c('proportion','count','density'), grid=FALSE, ...)

jitter2(x, ...)

## Default S3 method:
jitter2(x, fill=1/3, limit=TRUE, eps=0,
        presorted=FALSE, ...)

## S3 method for class 'data.frame'
jitter2(x, ...)

datadensity(object, ...)

## S3 method for class 'data.frame'
datadensity(object, group,
            which=c("all","continuous","categorical"),
            method.cat=c("bar","freq"),
            col.group=1:10,
            n.unique=10, show.na=TRUE, nint=1, naxes,
            q, bottom.align=nint&gt;1,
            cex.axis=sc(.5,.3), cex.var=sc(.8,.3),
            lmgp=NULL, tck=sc(-.009,-.002),
            ranges=NULL, labels=NULL, ...)
# sc(a,b) means default to a if number of axes &lt;= 3, b if &gt;=50, use
# linear interpolation within 3-50

histSpike(x, side=1, nint=100, bins=NULL, frac=.05, minf=NULL, mult.width=1,
          type=c('proportion','count','density'),
          xlim=range(x), ylim=c(0,max(f)), xlab=deparse(substitute(x)), 
          ylab=switch(type,proportion='Proportion',
                           count     ='Frequency',
                           density   ='Density'),
          y=NULL, curve=NULL, add=FALSE, minimal=FALSE,
          bottom.align=type=='density', col=par('col'), lwd=par('lwd'),
          grid=FALSE, ...)

histSpikeg(formula=NULL, predictions=NULL, data, plotly=NULL,
           lowess=FALSE, xlim=NULL, ylim=NULL,
           side=1, nint=100,
           frac=function(f) 0.01 + 0.02*sqrt(f-1)/sqrt(max(f,2)-1),
           span=3/4, histcol='black', showlegend=TRUE)

ecdfpM(x, group=NULL, what=c('F','1-F','f','1-f'), q=NULL,
       extra=c(0.025, 0.025), xlab=NULL, ylab=NULL, height=NULL, width=NULL,
       colors=NULL, nrows=NULL, ncols=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scat1d_+3A_x">x</code></td>
<td>

<p>a vector of numeric data, or a data frame (for <code>jitter2</code> or
<code>ecdfpM</code>) 
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_object">object</code></td>
<td>

<p>a data frame or list (even with unequal number of observations per
variable, as long as <code>group</code> is notspecified)
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_side">side</code></td>
<td>

<p>axis side to use (1=bottom (default for <code>histSpike</code>), 2=left,
3=top (default for <code>scat1d</code>), 4=right)
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_frac">frac</code></td>
<td>

<p>fraction of smaller of vertical and horizontal axes for tick mark
lengths. Can be negative to move tick marks outside of plot. For
<code>histSpike</code>, this is the relative y-direction length to be used for the
largest frequency. When <code>scat1d</code> calls <code>histSpike</code>, it
multiplies its <code>frac</code> argument by 2.5.  For <code>histSpikeg</code>,
<code>frac</code> is a function of <code>f</code>, the vector of all frequencies.  The
default function scales tick marks so that they are between 0.01 and
0.03 of the y range, linearly scaled in the square root of the
frequency less one.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_jitfrac">jitfrac</code></td>
<td>

<p>fraction of axis for jittering.  If
<code class="reqn">\code{jitfrac} \le 0</code>, no
jittering is done. If <code>preserve=TRUE</code>, the amount of
jittering is independent of jitfrac.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_tfrac">tfrac</code></td>
<td>

<p>Fraction of tick mark to actually draw.  If <code class="reqn">\code{tfrac}&lt;1</code>,
will draw a random fraction <code>tfrac</code>  of the line segment at
each point. This is useful for very large samples or ones with some
very dense points. The default value is 1 if the number of
non-missing observations <code>n</code>  is less than 125, and
<code class="reqn">\max{(.1, 125/n)}</code> otherwise.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_eps">eps</code></td>
<td>

<p>fraction of axis for determining overlapping points in <code>x</code>. For
<code>preserve=TRUE</code> the default is 0 and original unique values are
retained, bigger values of eps tends to bias observations from dense
to sparse regions, but ranks are still preserved.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_lwd">lwd</code></td>
<td>

<p>line width for tick marks, passed to <code>segments</code>
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_col">col</code></td>
<td>

<p>color for tick marks, passed to <code>segments</code>
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_y">y</code></td>
<td>

<p>specify a vector the same length as <code>x</code> to draw tick marks
along a curve instead of by one of the axes.  The <code>y</code> values
are often predicted values from a model.  The <code>side</code> argument
is ignored when <code>y</code> is given.  If the curve is already
represented as a table look-up, you may specify it using the
<code>curve</code> argument instead.  <code>y</code> may be a scalar to use a
constant verticalplacement.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_curve">curve</code></td>
<td>

<p>a list containing elements <code>x</code> and <code>y</code> for which linear
interpolation is used to derive <code>y</code> values corresponding to
values of <code>x</code>.  This results in tick marks being drawn along
the curve.  For <code>histSpike</code>, interpolated <code>y</code> values are
derived for binmidpoints.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_minimal">minimal</code></td>
<td>
<p>for <code>histSpike</code> set <code>minimal=TRUE</code> to draw a
minimalist spike histogram with no y-axis.  This works best when
produce graphics images that are short, e.g., have a height of
two inches.  <code>add</code> is forced to be <code>FALSE</code> in this case
so that a standalone graph is produced.  Only base graphics are
used.</p>
</td></tr>
<tr><td><code id="scat1d_+3A_bottom.align">bottom.align</code></td>
<td>

<p>set to <code>TRUE</code> to have the bottoms of tick marks (for
<code>side=1</code> or <code>side=3</code>) aligned at the y-coordinate.  The
default behavior is to center the tick marks.  For
<code>datadensity.data.frame</code>, <code>bottom.align</code> defaults to
<code>TRUE</code> if <code>nint&gt;1</code>.  In other words, if you are only
labeling the first and last axis tick mark, the <code>scat1d</code> tick
marks are centered on the variable's axis.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_preserve">preserve</code></td>
<td>

<p>set to <code>TRUE</code> to invoke <code>jitter2</code>
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_fill">fill</code></td>
<td>

<p>maximum fraction of the axis filled by jittered values. If <code>d</code>
are duplicated values between a lower value l and upper value
u, then d will be spread within
<code class="reqn">\pm \code{fill}*\min{(u-d,d-l)}/2</code>.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_limit">limit</code></td>
<td>

<p>specifies a limit for maximum shift in jittered values. Duplicate
values will be spread within
<code class="reqn">\pm\code{fill}*\min{(u-d,d-l)}/2</code>. The
default <code>TRUE</code> restricts jittering to the smallest
<code class="reqn">\min{(u-d,d-l)}/2</code> observed and results
in equal amount of jittering for all d. Setting to
<code>FALSE</code> allows for locally different amount of jittering, using
maximum space available.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_nhistspike">nhistSpike</code></td>
<td>

<p>If the number of observations exceeds or equals <code>nhistSpike</code>,
<code>scat1d</code> will automatically call <code>histSpike</code> to draw the
data density, to prevent the graphics file from being too large.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_type">type</code></td>
<td>

<p>used by or passed to <code>histSpike</code>.  Set to <code>"count"</code> to
display frequency counts rather than relative frequencies, or
<code>"density"</code> to display a kernel density estimate computed using
the <code>density</code> function.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_grid">grid</code></td>
<td>

<p>set to <code>TRUE</code> if the <span class="rlang"><b>R</b></span> <code>grid</code> package is in effect for
the current plot
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_nint">nint</code></td>
<td>

<p>number of intervals to divide each continuous variable's axis for
<code>datadensity</code>. For <code>histSpike</code>, is the number of
equal-width intervals for which to bin <code>x</code>, and if instead
<code>nint</code> is a character string (e.g.,<code>nint="all"</code>), the
frequency tabulation is done with no binning.  In other words,
frequencies for all unique values of <code>x</code> are derived and
plotted.  For <code>histSpikeg</code>, if <code>x</code> has no more than
<code>nint</code> unique values, all observed values are used, otherwise
the data are rounded before tabulation so that there are no more
than <code>nint</code> intervals.  For <code>histSpike</code>, <code>nint</code> is
ignored if <code>bins</code> is given.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_bins">bins</code></td>
<td>
<p>for <code>histSpike</code> specifies the actual cutpoints to use
for binning <code>x</code>.  The default is to use <code>nint</code> in
conjunction with <code>xlim</code>.</p>
</td></tr>
<tr><td><code id="scat1d_+3A_...">...</code></td>
<td>

<p>optional arguments passed to <code>scat1d</code> from <code>datadensity</code>
or to <code>histSpike</code> from <code>scat1d</code>.  For <code>histSpikep</code>
are passed to the <code>lines</code> list to <code>add_trace</code>.  For
<code>ecdfpM</code> these arguments are passed to <code>add_lines</code>.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_presorted">presorted</code></td>
<td>

<p>set to <code>TRUE</code> to prevent from sorting for determining the order
<code class="reqn">l&lt;d&lt;u</code>. This is usefull if an existing
meaningfull local order would be destroyed by sorting, as in
<code class="reqn">\sin{(\pi*\code{sort}(\code{round}(\code{runif}(1000,0,10),1)))}</code>.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_group">group</code></td>
<td>

<p>an optional stratification variable, which is converted to a
<code>factor</code> vector if it is not one already
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_which">which</code></td>
<td>

<p>set <code>which="continuous"</code> to only plot continuous variables, or
<code>which="categorical"</code> to only plot categorical, character, or
discrete numeric ones.  By default, all types of variables are
depicted.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_method.cat">method.cat</code></td>
<td>

<p>set <code>method.cat="freq"</code> to depict frequencies of categorical
variables with digits representing the cell frequencies, with size
proportional to the square root of the frequency.  By default,
vertical bars are used.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_col.group">col.group</code></td>
<td>

<p>colors representing the <code>group</code> strata.  The vector of colors
is recycled to be the same length as the levels of <code>group</code>.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_n.unique">n.unique</code></td>
<td>

<p>number of unique values a numeric variable must have before it is
considered to be a continuous variable
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_show.na">show.na</code></td>
<td>

<p>set to <code>FALSE</code> to suppress drawing the number of <code>NA</code>s to
the right of each axis
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_naxes">naxes</code></td>
<td>

<p>number of axes to draw on each page before starting a new plot.  You
can set <code>naxes</code> larger than the number of variables in the data
frame if you want to compress the plot vertically.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_q">q</code></td>
<td>

<p>a vector of quantiles to display.  By default, quantiles are not
shown.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_extra">extra</code></td>
<td>
<p>a two-vector specifying the fraction of the x
range to add on the left and the fraction to add on the right</p>
</td></tr>
<tr><td><code id="scat1d_+3A_cex.axis">cex.axis</code></td>
<td>

<p>character size for draw labels for axis tick marks
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_cex.var">cex.var</code></td>
<td>

<p>character size for variable names and frequence of <code>NA</code>s
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_lmgp">lmgp</code></td>
<td>

<p>spacing between numeric axis labels and axis (see <code>par</code> for
<code>mgp</code>)
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_tck">tck</code></td>
<td>

<p>see <code>tck</code> under <code><a href="graphics.html#topic+par">par</a></code>
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_ranges">ranges</code></td>
<td>

<p>a list containing ranges for some or all of the numeric variables.
If <code>ranges</code> is not given or if a certain variable is not found
in the list, the empirical range, modified by <code>pretty</code>, is
used.  Example:
<code>ranges=list(age=c(10,100), pressure=c(50,150))</code>.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_labels">labels</code></td>
<td>

<p>a vector of labels to use in labeling the axes for
<code>datadensity.data.frame</code>.  Default is to use the names of the
variable in the input data frame.  Note: margin widths computed for
setting aside names of variables use the names, and not these
labels.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_minf">minf</code></td>
<td>

<p>For <code>histSpike</code>, if <code>minf</code> is specified low bin
frequencies are set to a minimum value of <code>minf</code> times the
maximum bin frequency, so that rare data points will remain visible.
A good choice of <code>minf</code> is 0.075.
<code>datadensity.data.frame</code> passes <code>minf=0.075</code> to
<code>scat1d</code> to pass to <code>histSpike</code>.  Note that specifying
<code>minf</code> will cause the shape of the histogram to be distorted
somewhat.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_mult.width">mult.width</code></td>
<td>

<p>multiplier for the smoothing window width computed by
<code>histSpike</code> when <code>type="density"</code>
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_xlim">xlim</code></td>
<td>

<p>a 2-vector specifying the outer limits of <code>x</code> for binning (and
plotting, if <code>add=FALSE</code> and <code>nint</code> is a number).  For
<code>histSpikeg</code>, observations outside the <code>xlim</code> range are ignored.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_ylim">ylim</code></td>
<td>

<p>y-axis range for plotting (if <code>add=FALSE</code>).  Often needed for
<code>histSpikeg</code> to help scale the tick mark line segments.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_xlab">xlab</code></td>
<td>

<p>x-axis label (<code>add=FALSE</code> or for <code>ecdfpM</code>); default is
name of input argument, or for <code>ecdfpM</code> comes from
<code>label</code> and <code>units</code> attributes of the analysis
variable.  For <code>ecdfpM</code> <code>xlab</code> may be a vector if there
is more than one analysis variable.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_ylab">ylab</code></td>
<td>

<p>y-axis label (<code>add=FALSE</code> or for <code>ecdfpM</code>)
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_add">add</code></td>
<td>

<p>set to <code>TRUE</code> to add the spike-histogram to an existing plot,
to show marginal data densities
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_formula">formula</code></td>
<td>

<p>a formula of the form <code>y ~ x1</code> or <code>y ~ x1 + ...</code> where
<code>y</code> is the name of the <code>y</code>-axis variable being plotted
with <code>ggplot</code>, <code>x1</code> is the name of the <code>x</code>-axis
variable, and optional ... are variables used by
<code>ggplot</code> to produce multiple curves on a panel and/or facets.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_predictions">predictions</code></td>
<td>

<p>the data frame being plotted by <code>ggplot</code>, containing <code>x</code>
and <code>y</code> coordinates of curves.  If omitted, spike histograms
are drawn at the bottom (default) or top of the plot according to
<code>side</code>.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_data">data</code></td>
<td>

<p>for <code>histSpikeg</code> is a mandatory data frame containing raw data whose
frequency distribution is to be summarized, using variables in
<code>formula</code>. 
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_plotly">plotly</code></td>
<td>

<p>an existing <code>plotly</code> object.  If not <code>NULL</code>,
<code>histSpikeg</code> uses <code>plotly</code> instead of <code>ggplot</code>.</p>
</td></tr>
<tr><td><code id="scat1d_+3A_lowess">lowess</code></td>
<td>

<p>set to <code>TRUE</code> to have <code>histSpikeg</code> add a <code>geom_line</code>
layer to the <code>ggplot2</code> graphic, containing
<code>lowess()</code> nonparametric smoothers.  This causes the
returned value of <code>histSpikeg</code> to be a list with two
components: <code>"hist"</code> and <code>"lowess"</code> each containing
a layer.  Fortunately, <code>ggplot2</code> plots both layers
automatically.  If the dependent variable is binary,
<code>iter=0</code> is passed to <code>lowess</code> so that outlier
detection is turned off; otherwise <code>iter=3</code> is passed.
</p>
</td></tr>
<tr><td><code id="scat1d_+3A_span">span</code></td>
<td>
<p>passed to <code>lowess</code> as the <code>f</code> argument</p>
</td></tr>
<tr><td><code id="scat1d_+3A_histcol">histcol</code></td>
<td>
<p>color of line segments (tick marks) for
<code>histSpikeg</code>.  Default is black.  Set to any color or to
<code>"default"</code> to use the prevailing colors for the
graphic.</p>
</td></tr>
<tr><td><code id="scat1d_+3A_showlegend">showlegend</code></td>
<td>
<p>set to <code>FALSE</code> too have the added <code>plotly</code>
traces not have entries in the plot legend</p>
</td></tr>
<tr><td><code id="scat1d_+3A_what">what</code></td>
<td>
<p>set to <code>"1-F"</code> to plot 1 minus the ECDF instead of the
ECDF, <code>"f"</code> to plot cumulative frequency, or <code>"1-f"</code> to
plot the inverse cumulative frequency</p>
</td></tr>
<tr><td><code id="scat1d_+3A_height">height</code>, <code id="scat1d_+3A_width">width</code></td>
<td>
<p>passed to <code>plot_ly</code></p>
</td></tr>
<tr><td><code id="scat1d_+3A_colors">colors</code></td>
<td>
<p>a vector of colors to pas to <code>add_lines</code></p>
</td></tr>
<tr><td><code id="scat1d_+3A_nrows">nrows</code>, <code id="scat1d_+3A_ncols">ncols</code></td>
<td>
<p>passed to <code>plotly::subplot</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>scat1d</code> the length of line segments used is
<code>frac*min(par()$pin)/par()$uin[opp]</code> data units, where
opp is the index of the opposite axis and <code>frac</code> defaults
to .02.  Assumes that <code>plot</code> has already been called.  Current
<code>par("usr")</code> is used to determine the range of data for the axis
of the current plot.  This range is used in jittering and in
constructing line segments.
</p>


<h3>Value</h3>

<p><code>histSpike</code> returns the actual range of <code>x</code> used in its binning
</p>


<h3>Side Effects</h3>

<p><code>scat1d</code> adds line segments to plot.
<code>datadensity.data.frame</code> draws a complete plot.  <code>histSpike</code>
draws a complete plot or adds to an existing plot.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
Nashville TN, USA<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>
<p>Martin Maechler (improved <code>scat1d</code>)<br />
Seminar fuer Statistik<br />
ETH Zurich SWITZERLAND<br />
<a href="mailto:maechler@stat.math.ethz.ch">maechler@stat.math.ethz.ch</a>
</p>
<p>Jens Oehlschlaegel-Akiyoshi (wrote <code>jitter2</code>)<br />
Center for Psychotherapy Research<br />
Christian-Belser-Strasse 79a<br />
D-70597 Stuttgart Germany<br />
<a href="mailto:oehl@psyres-stuttgart.de">oehl@psyres-stuttgart.de</a>
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+segments">segments</a></code>, <code><a href="base.html#topic+jitter">jitter</a></code>, <code><a href="graphics.html#topic+rug">rug</a></code>,
<code><a href="#topic+plsmo">plsmo</a></code>, <code><a href="stats.html#topic+lowess">lowess</a></code>, <code><a href="graphics.html#topic+stripplot">stripplot</a></code>,
<code><a href="#topic+hist.data.frame">hist.data.frame</a></code>,<code><a href="#topic+Ecdf">Ecdf</a></code>, <code><a href="graphics.html#topic+hist">hist</a></code>,
<code><a href="lattice.html#topic+histogram">histogram</a></code>, <code><a href="base.html#topic+table">table</a></code>,
<code><a href="stats.html#topic+density">density</a></code>, <code><a href="#topic+stat_plsmo">stat_plsmo</a></code>, <code><a href="#topic+histboxp">histboxp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>plot(x &lt;- rnorm(50), y &lt;- 3*x + rnorm(50)/2 )
scat1d(x)                 # density bars on top of graph
scat1d(y, 4)              # density bars at right
histSpike(x, add=TRUE)       # histogram instead, 100 bins
histSpike(y, 4, add=TRUE)
histSpike(x, type='density', add=TRUE)  # smooth density at bottom
histSpike(y, 4, type='density', add=TRUE)


smooth &lt;- lowess(x, y)    # add nonparametric regression curve
lines(smooth)             # Note: plsmo() does this
scat1d(x, y=approx(smooth, xout=x)$y) # data density on curve
scat1d(x, curve=smooth)   # same effect as previous command
histSpike(x, curve=smooth, add=TRUE) # same as previous but with histogram
histSpike(x, curve=smooth, type='density', add=TRUE)  
# same but smooth density over curve


plot(x &lt;- rnorm(250), y &lt;- 3*x + rnorm(250)/2)
scat1d(x, tfrac=0)        # dots randomly spaced from axis
scat1d(y, 4, frac=-.03)   # bars outside axis
scat1d(y, 2, tfrac=.2)    # same bars with smaller random fraction


x &lt;- c(0:3,rep(4,3),5,rep(7,10),9)
plot(x, jitter2(x))       # original versus jittered values
abline(0,1)               # unique values unjittered on abline
points(x+0.1, jitter2(x, limit=FALSE), col=2)
                          # allow locally maximum jittering
points(x+0.2, jitter2(x, fill=1), col=3); abline(h=seq(0.5,9,1), lty=2)
                          # fill 3/3 instead of 1/3
x &lt;- rnorm(200,0,2)+1; y &lt;- x^2
x2 &lt;- round((x+rnorm(200))/2)*2
x3 &lt;- round((x+rnorm(200))/4)*4
dfram &lt;- data.frame(y,x,x2,x3)
plot(dfram$x2, dfram$y)   # jitter2 via scat1d
scat1d(dfram$x2, y=dfram$y, preserve=TRUE, col=2)
scat1d(dfram$x2, preserve=TRUE, frac=-0.02, col=2)
scat1d(dfram$y, 4, preserve=TRUE, frac=-0.02, col=2)


pairs(jitter2(dfram))     # pairs for jittered data.frame
# This gets reasonable pairwise scatter plots for all combinations of
# variables where
#
# - continuous variables (with unique values) are not jittered at all, thus
#   all relations between continuous variables are shown as they are,
#   extreme values have exact positions.
#
# - discrete variables get a reasonable amount of jittering, whether they
#   have 2, 3, 5, 10, 20 \dots levels
#
# - different from adding noise, jitter2() will use the available space
#   optimally and no value will randomly mask another
#
# If you want a scatterplot with lowess smooths on the *exact* values and
# the point clouds shown jittered, you just need
#
pairs( dfram ,panel=function(x,y) { points(jitter2(x),jitter2(y))
                                    lines(lowess(x,y)) } )




datadensity(dfram)     # graphical snapshot of entire data frame
datadensity(dfram, group=cut2(dfram$x2,g=3))
                          # stratify points and frequencies by
                          # x2 tertiles and use 3 colors


# datadensity.data.frame(split(x, grouping.variable))
# need to explicitly invoke datadensity.data.frame when the
# first argument is a list

## Not run: 
require(rms)
require(ggplot2)
f &lt;- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)),
         data=d)
p &lt;- Predict(f, cholesterol, sex)
g &lt;- ggplot(p, aes(x=cholesterol, y=yhat, color=sex)) + geom_line() +
  xlab(xl2) + ylim(-1, 1)
g &lt;- g + geom_ribbon(data=p, aes(ymin=lower, ymax=upper), alpha=0.2,
                linetype=0, show_guide=FALSE)
g + histSpikeg(yhat ~ cholesterol + sex, p, d)

# colors &lt;- c('red', 'blue')
# p &lt;- plot_ly(x=x, y=y, color=g, colors=colors, mode='markers')
# histSpikep(p, x, y, z, color=g, colors=colors)

w &lt;- data.frame(x1=rnorm(100), x2=exp(rnorm(100)))
g &lt;- c(rep('a', 50), rep('b', 50))
ecdfpM(w, group=g, ncols=2)

## End(Not run)
</code></pre>

<hr>
<h2 id='score.binary'>
Score a Series of Binary Variables
</h2><span id='topic+score.binary'></span>

<h3>Description</h3>

<p>Creates a new variable from a series of logical conditions.  The new
variable can be a hierarchical category or score derived from considering
the rightmost <code>TRUE</code> value among the input variables, an additive point
score, a union, or any of several others by specifying a function using the
<code>fun</code> argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>score.binary(..., fun=max, points=1:p, 
             na.rm=funtext == "max", retfactor=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="score.binary_+3A_...">...</code></td>
<td>

<p>a list of variables or expressions which are considered to be binary
or logical
</p>
</td></tr>
<tr><td><code id="score.binary_+3A_fun">fun</code></td>
<td>

<p>a function to compute on each row of the matrix represented by
a specific observation of all the variables in <code>...</code>
</p>
</td></tr>
<tr><td><code id="score.binary_+3A_points">points</code></td>
<td>

<p>points to assign to successive elements of <code>...</code> .  The default is
<code>1, 2, ..., p</code>, where <code>p</code> is the number of elements.  If you specify
one number for <code>points</code>, that number will be duplicated (i.e., equal weights
are assumed).
</p>
</td></tr>
<tr><td><code id="score.binary_+3A_na.rm">na.rm</code></td>
<td>

<p>set to <code>TRUE</code> to remove <code>NA</code>s from consideration when processing
each row of the matrix of variables in <code>...</code> .  For <code>fun=max</code>,
<code>na.rm=TRUE</code> is the default since <code>score.binary</code> assumes that a
hierarchical scale is based on available information.  Otherwise,
<code>na.rm=FALSE</code> is assumed.  For <code>fun=mean</code> you may want to specify
<code>na.rm=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="score.binary_+3A_retfactor">retfactor</code></td>
<td>

<p>applies if <code>fun=max</code>, in which case <code>retfactor=TRUE</code> makes <code>score.binary</code>
return a <code>factor</code> object since a hierarchical scale implies
a unique choice.
</p>
</td></tr></table>


<h3>Value</h3>

<p>a <code>factor</code> object if <code>retfactor=TRUE</code> and <code>fun=max</code> or a numeric vector
otherwise.  Will not contain NAs if <code>na.rm=TRUE</code> unless every variable in
a row is <code>NA</code>.  If a <code>factor</code> object
is returned, it has levels <code>"none"</code> followed by character
string versions of the arguments given in <code>...</code> .
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+any">any</a></code>, <code><a href="base.html#topic+sum">sum</a></code>, <code><a href="base.html#topic+max">max</a></code>, <code><a href="base.html#topic+factor">factor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
age &lt;- rnorm(25, 70, 15)
previous.disease &lt;- sample(0:1, 25, TRUE)
#Hierarchical scale, highest of 1:age&gt;70  2:previous.disease
score.binary(age&gt;70, previous.disease, retfactor=FALSE)
#Same as above but return factor variable with levels "none" "age&gt;70" 
# "previous.disease"
score.binary(age&gt;70, previous.disease)


#Additive scale with weights 1:age&gt;70  2:previous.disease
score.binary(age&gt;70, previous.disease, fun=sum)
#Additive scale, equal weights
score.binary(age&gt;70, previous.disease, fun=sum, points=c(1,1))
#Same as saying points=1


#Union of variables, to create a new binary variable
score.binary(age&gt;70, previous.disease, fun=any)
</code></pre>

<hr>
<h2 id='sedit'>
Character String Editing and Miscellaneous Character Handling Functions
</h2><span id='topic+sedit'></span><span id='topic+substring.location'></span><span id='topic+substring2'></span><span id='topic+substring2+3C-'></span><span id='topic+replace.substring.wild'></span><span id='topic+numeric.string'></span><span id='topic+all.digits'></span>

<h3>Description</h3>

<p>This suite of functions was written to implement many of the features
of the UNIX <code>sed</code> program entirely within S (function <code>sedit</code>).
The <code>substring.location</code> function returns the first and last position
numbers that a sub-string occupies in a larger string.  The <code>substring2&lt;-</code>
function does the opposite of the builtin function <code>substring</code>.
It is named <code>substring2</code> because for S-Plus there is a built-in
function <code>substring</code>, but it does not handle multiple replacements in
a single string.
<code>replace.substring.wild</code> edits character strings in the fashion of
&quot;change xxxxANYTHINGyyyy to aaaaANYTHINGbbbb&quot;, if the &quot;ANYTHING&quot;
passes an optional user-specified <code>test</code> function.  Here, the
&quot;yyyy&quot; string is searched for from right to left to handle
balancing parentheses, etc.  <code>numeric.string</code>
and <code>all.digits</code> are two examples of <code>test</code> functions, to check,
respectively if each of a vector of strings is a legal numeric or if it contains only
the digits 0-9.  For the case where <code>old="*$" or "^*"</code>, or for
<code>replace.substring.wild</code> with the same values of <code>old</code> or with
<code>front=TRUE</code> or <code>back=TRUE</code>, <code>sedit</code> (if <code>wild.literal=FALSE</code>) and
<code>replace.substring.wild</code> will edit the largest substring
satisfying <code>test</code>.
</p>
<p><code>substring2</code> is just a copy of <code>substring</code> so that
<code>substring2&lt;-</code> will work.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sedit(text, from, to, test, wild.literal=FALSE)
substring.location(text, string, restrict)
# substring(text, first, last) &lt;- setto   # S-Plus only
replace.substring.wild(text, old, new, test, front=FALSE, back=FALSE)
numeric.string(string)
all.digits(string)
substring2(text, first, last)
substring2(text, first, last) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sedit_+3A_text">text</code></td>
<td>

<p>a vector of character strings for <code>sedit, substring2, substring2&lt;-</code>
or a single character string for <code>substring.location,
      replace.substring.wild</code>.
</p>
</td></tr>
<tr><td><code id="sedit_+3A_from">from</code></td>
<td>

<p>a vector of character strings to translate from, for <code>sedit</code>.
A single asterisk wild card, meaning allow any sequence of characters
(subject to the <code>test</code> function, if any) in place of the <code>"*"</code>.
An element of <code>from</code> may begin with <code>"^"</code> to force the match to
begin at the beginning of <code>text</code>, and an element of <code>from</code> can end with
<code>"$"</code> to force the match to end at the end of <code>text</code>.
</p>
</td></tr>
<tr><td><code id="sedit_+3A_to">to</code></td>
<td>

<p>a vector of character strings to translate to, for <code>sedit</code>.
If a corresponding element in <code>from</code> had an <code>"*"</code>, the element
in <code>to</code> may also have an <code>"*"</code>.  Only single asterisks are allowed.
If <code>to</code> is not the same length as <code>from</code>, the <code>rep</code> function
is used to make it the same length.
</p>
</td></tr>
<tr><td><code id="sedit_+3A_string">string</code></td>
<td>

<p>a single character string, for <code>substring.location</code>, <code>numeric.string</code>,
<code>all.digits</code>
</p>
</td></tr>
<tr><td><code id="sedit_+3A_first">first</code></td>
<td>

<p>a vector of integers specifying the first position to replace for
<code>substring2&lt;-</code>.  <code>first</code> may also be a vector of character strings
that are passed to <code>sedit</code> to use as patterns for replacing
substrings with <code>setto</code>.  See one of the last examples below.
</p>
</td></tr>
<tr><td><code id="sedit_+3A_last">last</code></td>
<td>

<p>a vector of integers specifying the ending positions of the character
substrings to be replaced.  The default is to go to the end of
the string.  When <code>first</code> is character, <code>last</code> must be
omitted.
</p>
</td></tr>
<tr><td><code id="sedit_+3A_setto">setto</code></td>
<td>

<p>a character string or vector of character strings used as replacements,
in <code>substring2&lt;-</code>
</p>
</td></tr>
<tr><td><code id="sedit_+3A_old">old</code></td>
<td>

<p>a character string to translate from for <code>replace.substring.wild</code>.
May be <code>"*$"</code> or <code>"^*"</code> or any string containing a single <code>"*"</code> but
not beginning with <code>"^"</code> or ending with <code>"$"</code>.
</p>
</td></tr>
<tr><td><code id="sedit_+3A_new">new</code></td>
<td>

<p>a character string to translate to for <code>replace.substring.wild</code>
</p>
</td></tr>
<tr><td><code id="sedit_+3A_test">test</code></td>
<td>

<p>a function of a vector of character strings returning a logical vector
whose elements are <code>TRUE</code> or <code>FALSE</code> according
to whether that string element qualifies as the wild card string for
<code>sedit, replace.substring.wild</code>
</p>
</td></tr>
<tr><td><code id="sedit_+3A_wild.literal">wild.literal</code></td>
<td>

<p>set to <code>TRUE</code> to not treat asterisks as wild cards and to not look for
<code>"^"</code> or <code>"$"</code> in <code>old</code>
</p>
</td></tr>
<tr><td><code id="sedit_+3A_restrict">restrict</code></td>
<td>

<p>a vector of two integers for <code>substring.location</code> which specifies a
range to which the search for matches should be restricted
</p>
</td></tr>
<tr><td><code id="sedit_+3A_front">front</code></td>
<td>

<p>specifying <code>front = TRUE</code> and <code>old = "*"</code> is the same as
specifying <code>old = "^*"</code>
</p>
</td></tr>
<tr><td><code id="sedit_+3A_back">back</code></td>
<td>

<p>specifying <code>back = TRUE</code> and <code>old = "*"</code> is the same as
specifying <code>old = "*$"</code>
</p>
</td></tr>
<tr><td><code id="sedit_+3A_value">value</code></td>
<td>
<p>a character vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>sedit</code> returns a vector of character strings the same length as <code>text</code>.
<code>substring.location</code> returns a list with components named <code>first</code>
and <code>last</code>, each specifying a vector of character positions corresponding
to matches.  <code>replace.substring.wild</code> returns a single character string.
<code>numeric.string</code> and <code>all.digits</code> return a single logical value.
</p>


<h3>Side Effects</h3>

<p><code>substring2&lt;-</code> modifies its first argument
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University School of Medicine
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+grep">grep</a></code>, <code><a href="base.html#topic+substring">substring</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- 'this string'
substring2(x, 3, 4) &lt;- 'IS'
x
substring2(x, 7) &lt;- ''
x


substring.location('abcdefgabc', 'ab')
substring.location('abcdefgabc', 'ab', restrict=c(3,999))


replace.substring.wild('this is a cat','this*cat','that*dog')
replace.substring.wild('there is a cat','is a*', 'is not a*')
replace.substring.wild('this is a cat','is a*', 'Z')


qualify &lt;- function(x) x==' 1.5 ' | x==' 2.5 '
replace.substring.wild('He won 1.5 million $','won*million',
                       'lost*million', test=qualify)
replace.substring.wild('He won 1 million $','won*million',
                       'lost*million', test=qualify)
replace.substring.wild('He won 1.2 million $','won*million',
                       'lost*million', test=numeric.string)


x &lt;- c('a = b','c &lt; d','hello')
sedit(x, c('=','he*o'),c('==','he*'))


sedit('x23', '*$', '[*]', test=numeric.string)
sedit('23xx', '^*', 'Y_{*} ', test=all.digits)


replace.substring.wild("abcdefabcdef", "d*f", "xy")


x &lt;- "abcd"
substring2(x, "bc") &lt;- "BCX"
x
substring2(x, "B*d") &lt;- "B*D"
x
</code></pre>

<hr>
<h2 id='seqFreq'>seqFreq</h2><span id='topic+seqFreq'></span>

<h3>Description</h3>

<p>Find Sequential Exclusions Due to NAs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>seqFreq(..., labels = NULL, noneNA = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="seqFreq_+3A_...">...</code></td>
<td>
<p>any number of variables</p>
</td></tr>
<tr><td><code id="seqFreq_+3A_labels">labels</code></td>
<td>
<p>if specified variable labels will be used in place of variable names</p>
</td></tr>
<tr><td><code id="seqFreq_+3A_nonena">noneNA</code></td>
<td>
<p>set to <code>TRUE</code> to not include 'none' as a level in the result</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Finds the variable with the highest number of <code>NA</code>s.  From the non-<code>NA</code>s on that variable find the next variable from those remaining with the highest number of <code>NA</code>s.  Proceed in like fashion.  The resulting variable summarizes sequential exclusions in a hierarchical fashion.  See <a href="https://hbiostat.org/rflow/doverview.html#sec-doverview-filter">this</a> for more information.
</p>


<h3>Value</h3>

<p><code>factor</code> variable with <code>obs.per.numcond</code> attribute
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>

<hr>
<h2 id='show.pch'>Display Colors, Plotting Symbols, and Symbol Numeric Equivalents</h2><span id='topic+show.pch'></span><span id='topic+show.col'></span><span id='topic+character.table'></span>

<h3>Description</h3>

<p><code>show.pch</code> plots the definitions of the <code>pch</code> parameters.
<code>show.col</code> plots definitions of integer-valued colors.
<code>character.table</code> draws numeric equivalents of all latin
characters; the character on line <code>xy</code> and column <code>z</code> of the
table has numeric code <code>"xyz"</code>, which you would surround in quotes
and preceed by a backslash.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>show.pch(object = par("font"))
show.col(object=NULL)
character.table(font=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show.pch_+3A_object">object</code></td>
<td>
<p>font for <code>show.pch</code>, ignored for <code>show.col</code>.</p>
</td></tr>
<tr><td><code id="show.pch_+3A_font">font</code></td>
<td>
<p>font</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Pierre Joyet <a href="mailto:pierre.joyet@bluewin.ch">pierre.joyet@bluewin.ch</a>, Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+points">points</a></code>, <code><a href="graphics.html#topic+text">text</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
show.pch()
show.col()
character.table()

## End(Not run)</code></pre>

<hr>
<h2 id='showPsfrag'>
Display image from psfrag LaTeX strings
</h2><span id='topic+showPsfrag'></span>

<h3>Description</h3>

<p><code>showPsfrag</code> is used to display (using ghostview) a postscript
image that contained psfrag LaTeX strings, by building a small LaTeX
script and running <code>latex</code> and <code>dvips</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>showPsfrag(filename)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="showPsfrag_+3A_filename">filename</code></td>
<td>

<p>name or character string or character vector specifying file
prefix.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Grant MC, Carlisle (1998): The PSfrag System, Version 3.  Full
documentation is obtained by searching www.ctan.org for &lsquo;<span class="file">pfgguide.ps</span>&rsquo;.
</p>


<h3>See Also</h3>

<p><code><a href="grDevices.html#topic+postscript">postscript</a></code>, <code><a href="graphics.html#topic+par">par</a></code>, <code><a href="grDevices.html#topic+ps.options">ps.options</a></code>,
<code><a href="#topic+mgp.axis.labels">mgp.axis.labels</a></code>, <code><a href="grDevices.html#topic+pdf">pdf</a></code>,
<code><a href="lattice.html#topic+trellis.device">trellis.device</a></code>, <code><a href="#topic+setTrellis">setTrellis</a></code>
</p>

<hr>
<h2 id='simMarkovOrd'>simMarkovOrd</h2><span id='topic+simMarkovOrd'></span>

<h3>Description</h3>

<p>Simulate Ordinal Markov Process
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simMarkovOrd(
  n = 1,
  y,
  times,
  initial,
  X = NULL,
  absorb = NULL,
  intercepts,
  g,
  carry = FALSE,
  rdsample = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simMarkovOrd_+3A_n">n</code></td>
<td>
<p>number of subjects to simulate</p>
</td></tr>
<tr><td><code id="simMarkovOrd_+3A_y">y</code></td>
<td>
<p>vector of possible y values in order (numeric, character, factor)</p>
</td></tr>
<tr><td><code id="simMarkovOrd_+3A_times">times</code></td>
<td>
<p>vector of measurement times</p>
</td></tr>
<tr><td><code id="simMarkovOrd_+3A_initial">initial</code></td>
<td>
<p>initial value of <code>y</code> (baseline state; numeric, character, or factor matching <code>y</code>).  If length 1 this value is used for all subjects, otherwise it is a vector of length <code>n</code>.</p>
</td></tr>
<tr><td><code id="simMarkovOrd_+3A_x">X</code></td>
<td>
<p>an optional vector of matrix of baseline covariate values passed to <code>g</code>.  If a vector, <code>X</code> represents a set of single values for all the covariates and those values are used for every subject.  Otherwise <code>X</code> is a matrix with rows corresponding to subjects and columns corresponding to covariates which <code>g</code> must know how to handle.  <code>g</code> only sees one row of <code>X</code> at a time.</p>
</td></tr>
<tr><td><code id="simMarkovOrd_+3A_absorb">absorb</code></td>
<td>
<p>vector of absorbing states, a subset of <code>y</code> (numeric, character, or factor matching <code>y</code>).  The default is no absorbing states.  Observations are truncated when an absorbing state is simulated.</p>
</td></tr>
<tr><td><code id="simMarkovOrd_+3A_intercepts">intercepts</code></td>
<td>
<p>vector of intercepts in the proportional odds model.  There must be one fewer of these than the length of <code>y</code>.</p>
</td></tr>
<tr><td><code id="simMarkovOrd_+3A_g">g</code></td>
<td>
<p>a user-specified function of three or more arguments which in order are <code>yprev</code> - the value of <code>y</code> at the previous time, the current time <code>t</code>, the <code>gap</code> between the previous time and the current time, an optional (usually named) covariate vector <code>X</code>, and optional arguments such as a regression coefficient value to simulate from.  The function needs to allow <code>yprev</code> to be a vector and <code>yprev</code> must not include any absorbing states.  The <code>g</code> function returns the linear predictor for the proportional odds model aside from <code>intercepts</code>.  The returned value must be a matrix with row names taken from <code>yprev</code>.  If the model is a proportional odds model, the returned value must be one column.  If it is a partial proportional odds model, the value must have one column for each distinct value of the response variable Y after the first one, with the levels of Y used as optional column names.  So columns correspond to <code>intercepts</code>. The different columns are used for <code>y</code>-specific contributions to the linear predictor (aside from <code>intercepts</code>) for a partial or constrained partial proportional odds model.  Parameters for partial proportional odds effects may be included in the ... arguments.</p>
</td></tr>
<tr><td><code id="simMarkovOrd_+3A_carry">carry</code></td>
<td>
<p>set to <code>TRUE</code> to carry absorbing state forward after it is first hit; the default is to end records for the subject once the absorbing state is hit</p>
</td></tr>
<tr><td><code id="simMarkovOrd_+3A_rdsample">rdsample</code></td>
<td>
<p>an optional function to do response-dependent sampling.  It is a function of these arguments, which are vectors that stop at any absorbing state: <code>times</code> (ascending measurement times for one subject), <code>y</code> (vector of ordinal outcomes at these times for one subject.  The function returns <code>NULL</code> if no observations are to be dropped, returns the vector of new times to sample.</p>
</td></tr>
<tr><td><code id="simMarkovOrd_+3A_...">...</code></td>
<td>
<p>additional arguments to pass to <code>g</code> such as a regresson coefficient</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Simulates longitudinal data for subjects following a first-order Markov process under a proportional odds model.  Optionally, response-dependent sampling can be done, e.g., if a subject hits a specified state at time t, measurements are removed for times t+1, t+3, t+5, ...  This is applicable when for example a study of hospitalized patients samples every day, Y=1 denotes patient discharge to home, and sampling is less frequent outside the hospital.  This example assumes that arriving home is not an absorbing state, i.e., a patient could return to the hospital.
</p>


<h3>Value</h3>

<p>data frame with one row per subject per time, and columns id, time, yprev, y, values in ...
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><a href="https://hbiostat.org/R/Hmisc/markov/">https://hbiostat.org/R/Hmisc/markov/</a>
</p>

<hr>
<h2 id='simplifyDims'>List Simplification</h2><span id='topic+simplifyDims'></span>

<h3>Description</h3>

<p>Takes a list where each element is a group of rows that have been
spanned by a multirow row and combines it into one large matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simplifyDims(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simplifyDims_+3A_x">x</code></td>
<td>
<p>list of spanned rows</p>
</td></tr>
</table>


<h3>Details</h3>

<p>All rows must have the same number of columns.  This is used to format
the list for printing.
</p>


<h3>Value</h3>

<p>a matrix that contains all of the spanned rows.
</p>


<h3>Author(s)</h3>

<p>Charles Dupont</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+rbind">rbind</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- list(a = matrix(1:25, ncol=5), b = matrix(1:10, ncol=5), c = 1:5)

simplifyDims(a)
</code></pre>

<hr>
<h2 id='simRegOrd'>Simulate Power for Adjusted Ordinal Regression Two-Sample Test</h2><span id='topic+simRegOrd'></span>

<h3>Description</h3>

<p>This function simulates the power of a two-sample test from a
proportional odds ordinal logistic model for a continuous response
variable- a generalization of the Wilcoxon test.  The continuous data
model is normal with equal variance.  Nonlinear covariate
adjustment is allowed, and the user can optionally specify discrete
ordinal level overrides to the continuous response.  For example, if
the main response is systolic blood pressure, one can add two ordinal
categories higher than the highest observed blood pressure to capture
heart attack or death.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simRegOrd(n, nsim=1000, delta=0, odds.ratio=1, sigma,
          p=NULL, x=NULL, X=x, Eyx, alpha=0.05, pr=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simRegOrd_+3A_n">n</code></td>
<td>
<p>combined sample size (both groups combined)</p>
</td></tr>
<tr><td><code id="simRegOrd_+3A_nsim">nsim</code></td>
<td>
<p>number of simulations to run</p>
</td></tr>
<tr><td><code id="simRegOrd_+3A_delta">delta</code></td>
<td>
<p>difference in means to detect, for continuous portion of
response variable</p>
</td></tr>
<tr><td><code id="simRegOrd_+3A_odds.ratio">odds.ratio</code></td>
<td>
<p>odds ratio to detect for ordinal overrides of
continuous portion</p>
</td></tr>
<tr><td><code id="simRegOrd_+3A_sigma">sigma</code></td>
<td>
<p>standard deviation for continuous portion of response</p>
</td></tr>
<tr><td><code id="simRegOrd_+3A_p">p</code></td>
<td>
<p>a vector of marginal cell probabilities which must add up to one.
The <code>i</code>th element specifies the probability that a patient will be
in response level <code>i</code> for the control arm for the discrete
ordinal overrides.</p>
</td></tr>
<tr><td><code id="simRegOrd_+3A_x">x</code></td>
<td>
<p>optional covariate to adjust for - a vector of length
<code>n</code></p>
</td></tr>
<tr><td><code id="simRegOrd_+3A_x">X</code></td>
<td>
<p>a design matrix for the adjustment covariate <code>x</code> if
present.  This could represent for example <code>x</code> and <code>x^2</code>
or cubic spline components.</p>
</td></tr>
<tr><td><code id="simRegOrd_+3A_eyx">Eyx</code></td>
<td>
<p>a function of <code>x</code> that provides the mean response for
the control arm treatment</p>
</td></tr>
<tr><td><code id="simRegOrd_+3A_alpha">alpha</code></td>
<td>
<p>type I error</p>
</td></tr>
<tr><td><code id="simRegOrd_+3A_pr">pr</code></td>
<td>
<p>set to <code>TRUE</code> to see iteration progress</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing <code>n, delta, sigma, power, betas, se, pvals</code> where
<code>power</code> is the estimated power (scalar), and <code>betas, se,
	pvals</code> are <code>nsim</code>-vectors containing, respectively, the ordinal
model treatment effect estimate, standard errors, and 2-tailed
p-values.  When a model fit failed, the corresponding entries in
<code>betas, se, pvals</code> are <code>NA</code> and <code>power</code> is the proportion
of non-failed iterations for which the treatment p-value is significant
at the <code>alpha</code> level.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University School of Medicine
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+popower">popower</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## First use no ordinal high-end category overrides, and compare power
## to t-test when there is no covariate

n &lt;- 100
delta &lt;- .5
sd &lt;- 1
require(pwr)
power.t.test(n = n / 2, delta=delta, sd=sd, type='two.sample')  # 0.70
set.seed(1)
w &lt;- simRegOrd(n, delta=delta, sigma=sd, pr=TRUE)     # 0.686

## Now do ANCOVA with a quadratic effect of a covariate
n &lt;- 100
x &lt;- rnorm(n)
w &lt;- simRegOrd(n, nsim=400, delta=delta, sigma=sd, x=x,
               X=cbind(x, x^2),
               Eyx=function(x) x + x^2, pr=TRUE)
w$power  # 0.68

## Fit a cubic spline to some simulated pilot data and use the fitted
## function as the true equation in the power simulation
require(rms)
N &lt;- 1000
set.seed(2)
x &lt;- rnorm(N)
y &lt;- x + x^2 + rnorm(N, 0, sd=sd)
f &lt;- ols(y ~ rcs(x, 4), x=TRUE)

n &lt;- 100
j &lt;- sample(1 : N, n, replace=n &gt; N)
x &lt;-   x[j]
X &lt;- f$x[j,]
w &lt;- simRegOrd(n, nsim=400, delta=delta, sigma=sd, x=x,
               X=X,
               Eyx=Function(f), pr=TRUE)
w$power  ## 0.70

## Finally, add discrete ordinal category overrides and high end of y
## Start with no effect of treatment on these ordinal event levels (OR=1.0)

w &lt;- simRegOrd(n, nsim=400, delta=delta, odds.ratio=1, sigma=sd,
               x=x, X=X, Eyx=Function(f),
               p=c(.98, .01, .01),
               pr=TRUE)
w$power  ## 0.61   (0.3 if p=.8 .1 .1, 0.37 for .9 .05 .05, 0.50 for .95 .025 .025)

## Now assume that odds ratio for treatment is 2.5
## First compute power for clinical endpoint portion of Y alone
or &lt;- 2.5
p &lt;- c(.9, .05, .05)
popower(p, odds.ratio=or, n=100)   # 0.275
## Compute power of t-test on continuous part of Y alone
power.t.test(n = 100 / 2, delta=delta, sd=sd, type='two.sample')  # 0.70
## Note this is the same as the p.o. model power from simulation above
## Solve for OR that gives the same power estimate from popower
popower(rep(.01, 100), odds.ratio=2.4, n=100)   # 0.706
## Compute power for continuous Y with ordinal override
w &lt;- simRegOrd(n, nsim=400, delta=delta, odds.ratio=or, sigma=sd,
               x=x, X=X, Eyx=Function(f),
               p=c(.9, .05, .05),
               pr=TRUE)
w$power  ## 0.72

## End(Not run)
</code></pre>

<hr>
<h2 id='smean.sd'>
Compute Summary Statistics on a Vector
</h2><span id='topic+smean.cl.normal'></span><span id='topic+smean.sd'></span><span id='topic+smean.sdl'></span><span id='topic+smean.cl.boot'></span><span id='topic+smedian.hilow'></span>

<h3>Description</h3>

<p>A number of statistical summary functions is provided for use
with <code>summary.formula</code> and <code>summarize</code> (as well as
<code>tapply</code> and by themselves).
<code>smean.cl.normal</code> computes 3 summary variables: the sample mean and
lower and upper Gaussian confidence limits based on the t-distribution.
<code>smean.sd</code> computes the mean and standard deviation.
<code>smean.sdl</code> computes the mean plus or minus a constant times the
standard deviation.
<code>smean.cl.boot</code> is a very fast implementation of the basic
nonparametric bootstrap for obtaining confidence limits for the
population mean without assuming normality.
These functions all delete NAs automatically.
<code>smedian.hilow</code> computes the sample median and a selected pair of
outer quantiles having equal tail areas.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smean.cl.normal(x, mult=qt((1+conf.int)/2,n-1), conf.int=.95, na.rm=TRUE)

smean.sd(x, na.rm=TRUE)

smean.sdl(x, mult=2, na.rm=TRUE)

smean.cl.boot(x, conf.int=.95, B=1000, na.rm=TRUE, reps=FALSE)

smedian.hilow(x, conf.int=.95, na.rm=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smean.sd_+3A_x">x</code></td>
<td>

<p>for summary functions <code>smean.*</code>, <code>smedian.hilow</code>, a numeric vector
from which NAs will be removed automatically
</p>
</td></tr>
<tr><td><code id="smean.sd_+3A_na.rm">na.rm</code></td>
<td>

<p>defaults to <code>TRUE</code> unlike built-in functions, so that by
default <code>NA</code>s are automatically removed
</p>
</td></tr>
<tr><td><code id="smean.sd_+3A_mult">mult</code></td>
<td>

<p>for <code>smean.cl.normal</code> is the multiplier of the standard error of the
mean to use in obtaining confidence limits of the population mean
(default is appropriate quantile of the t distribution).  For
<code>smean.sdl</code>, <code>mult</code> is the multiplier of the standard deviation used
in obtaining a coverage interval about the sample mean.  The default
is <code>mult=2</code> to use plus or minus 2 standard deviations.
</p>
</td></tr>
<tr><td><code id="smean.sd_+3A_conf.int">conf.int</code></td>
<td>

<p>for <code>smean.cl.normal</code> and <code>smean.cl.boot</code> specifies the confidence
level (0-1) for interval estimation of the population mean.  For
<code>smedian.hilow</code>, <code>conf.int</code> is the coverage probability the outer
quantiles should target.  When the default, 0.95, is used, the lower
and upper quantiles computed are 0.025 and 0.975.
</p>
</td></tr>
<tr><td><code id="smean.sd_+3A_b">B</code></td>
<td>

<p>number of bootstrap resamples for <code>smean.cl.boot</code>
</p>
</td></tr>
<tr><td><code id="smean.sd_+3A_reps">reps</code></td>
<td>

<p>set to <code>TRUE</code> to have <code>smean.cl.boot</code> return the vector of bootstrapped
means as the <code>reps</code> attribute of the returned object
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of summary statistics
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summarize">summarize</a></code>, <code><a href="#topic+summary.formula">summary.formula</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x &lt;- rnorm(100)
smean.sd(x)
smean.sdl(x)
smean.cl.normal(x)
smean.cl.boot(x)
smedian.hilow(x, conf.int=.5)  # 25th and 75th percentiles

# Function to compute 0.95 confidence interval for the difference in two means
# g is grouping variable
bootdif &lt;- function(y, g) {
 g &lt;- as.factor(g)
 a &lt;- attr(smean.cl.boot(y[g==levels(g)[1]], B=2000, reps=TRUE),'reps')
 b &lt;- attr(smean.cl.boot(y[g==levels(g)[2]], B=2000, reps=TRUE),'reps')
 meandif &lt;- diff(tapply(y, g, mean, na.rm=TRUE))
 a.b &lt;- quantile(b-a, c(.025,.975))
 res &lt;- c(meandif, a.b)
 names(res) &lt;- c('Mean Difference','.025','.975')
 res
}

</code></pre>

<hr>
<h2 id='solvet'>
solve Function with tol argument
</h2><span id='topic+solvet'></span>

<h3>Description</h3>

<p>A slightly modified version of <code>solve</code> that allows a tolerance argument
for singularity (<code>tol</code>) which is passed to <code>qr</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>solvet(a, b, tol=1e-09)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="solvet_+3A_a">a</code></td>
<td>
<p>a square numeric matrix</p>
</td></tr>
<tr><td><code id="solvet_+3A_b">b</code></td>
<td>
<p>a numeric vector or matrix</p>
</td></tr>
<tr><td><code id="solvet_+3A_tol">tol</code></td>
<td>
<p>tolerance for detecting linear dependencies in columns of
<code>a</code></p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="base.html#topic+solve">solve</a></code>
</p>

<hr>
<h2 id='somers2'>
Somers' Dxy Rank Correlation
</h2><span id='topic+somers2'></span>

<h3>Description</h3>

<p>Computes Somers' Dxy rank correlation between a variable <code>x</code> and a
binary (0-1) variable <code>y</code>, and the corresponding receiver operating
characteristic curve area <code>c</code>. Note that <code>Dxy = 2(c-0.5)</code>.  
<code>somers</code> allows for a <code>weights</code> variable, which specifies frequencies
to associate with each observation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>somers2(x, y, weights=NULL, normwt=FALSE, na.rm=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="somers2_+3A_x">x</code></td>
<td>

<p>typically a predictor variable. <code>NA</code>s are allowed.
</p>
</td></tr>
<tr><td><code id="somers2_+3A_y">y</code></td>
<td>

<p>a numeric outcome variable coded <code>0-1</code>. <code>NA</code>s are allowed.
</p>
</td></tr>
<tr><td><code id="somers2_+3A_weights">weights</code></td>
<td>

<p>a numeric vector of observation weights (usually frequencies).  Omit
or specify a zero-length vector to do an unweighted analysis.
</p>
</td></tr>
<tr><td><code id="somers2_+3A_normwt">normwt</code></td>
<td>

<p>set to <code>TRUE</code> to make <code>weights</code> sum to the actual number of non-missing
observations.
</p>
</td></tr>
<tr><td><code id="somers2_+3A_na.rm">na.rm</code></td>
<td>

<p>set to <code>FALSE</code> to suppress checking for NAs.
</p>
</td></tr></table>


<h3>Details</h3>

<p>The <code>rcorr.cens</code> function, which although slower than <code>somers2</code> for large
sample sizes, can also be used to obtain Dxy for non-censored binary
<code>y</code>, and it has the advantage of computing the standard deviation of
the correlation index.
</p>


<h3>Value</h3>

<p>a vector with the named elements <code>C</code>, <code>Dxy</code>, <code>n</code> (number of non-missing
pairs), and <code>Missing</code>. Uses the formula 
<code>C = (mean(rank(x)[y == 1]) - (n1 + 1)/2)/(n - n1)</code>, where <code>n1</code> is the
frequency of <code>y=1</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University School of Medicine
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="survival.html#topic+concordance">concordance</a></code>, <code><a href="#topic+rcorr.cens">rcorr.cens</a></code>, <code><a href="base.html#topic+rank">rank</a></code>, <code><a href="#topic+wtd.rank">wtd.rank</a></code>, 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
predicted &lt;- runif(200)
dead      &lt;- sample(0:1, 200, TRUE)
roc.area &lt;- somers2(predicted, dead)["C"]

# Check weights
x &lt;- 1:6
y &lt;- c(0,0,1,0,1,1)
f &lt;- c(3,2,2,3,2,1)
somers2(x, y)
somers2(rep(x, f), rep(y, f))
somers2(x, y, f)
</code></pre>

<hr>
<h2 id='soprobMarkovOrd'>soprobMarkovOrd</h2><span id='topic+soprobMarkovOrd'></span>

<h3>Description</h3>

<p>State Occupancy Probabilities for First-Order Markov Ordinal Model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>soprobMarkovOrd(y, times, initial, absorb = NULL, intercepts, g, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="soprobMarkovOrd_+3A_y">y</code></td>
<td>
<p>a vector of possible y values in order (numeric, character, factor)</p>
</td></tr>
<tr><td><code id="soprobMarkovOrd_+3A_times">times</code></td>
<td>
<p>vector of measurement times</p>
</td></tr>
<tr><td><code id="soprobMarkovOrd_+3A_initial">initial</code></td>
<td>
<p>initial value of <code>y</code> (baseline state; numeric, character, factr)</p>
</td></tr>
<tr><td><code id="soprobMarkovOrd_+3A_absorb">absorb</code></td>
<td>
<p>vector of absorbing states, a subset of <code>y</code>.  The default is no absorbing states. (numeric, character, factor)</p>
</td></tr>
<tr><td><code id="soprobMarkovOrd_+3A_intercepts">intercepts</code></td>
<td>
<p>vector of intercepts in the proportional odds model, with length one less than the length of <code>y</code></p>
</td></tr>
<tr><td><code id="soprobMarkovOrd_+3A_g">g</code></td>
<td>
<p>a user-specified function of three or more arguments which in order are <code>yprev</code> - the value of <code>y</code> at the previous time, the current time <code>t</code>, the <code>gap</code> between the previous time and the current time, an optional (usually named) covariate vector <code>X</code>, and optional arguments such as a regression coefficient value to simulate from.  The function needs to allow <code>yprev</code> to be a vector and <code>yprev</code> must not include any absorbing states.  The <code>g</code> function returns the linear predictor for the proportional odds model aside from <code>intercepts</code>.  The returned value must be a matrix with row names taken from <code>yprev</code>.  If the model is a proportional odds model, the returned value must be one column.  If it is a partial proportional odds model, the value must have one column for each distinct value of the response variable Y after the first one, with the levels of Y used as optional column names.  So columns correspond to <code>intercepts</code>. The different columns are used for <code>y</code>-specific contributions to the linear predictor (aside from <code>intercepts</code>) for a partial or constrained partial proportional odds model.  Parameters for partial proportional odds effects may be included in the ... arguments.</p>
</td></tr>
<tr><td><code id="soprobMarkovOrd_+3A_...">...</code></td>
<td>
<p>additional arguments to pass to <code>g</code> such as covariate settings</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix with rows corresponding to times and columns corresponding to states, with values equal to exact state occupancy probabilities
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><a href="https://hbiostat.org/R/Hmisc/markov/">https://hbiostat.org/R/Hmisc/markov/</a>
</p>

<hr>
<h2 id='soprobMarkovOrdm'>soprobMarkovOrdm</h2><span id='topic+soprobMarkovOrdm'></span>

<h3>Description</h3>

<p>State Occupancy Probabilities for First-Order Markov Ordinal Model from a Model Fit
</p>


<h3>Usage</h3>

<pre><code class='language-R'>soprobMarkovOrdm(
  object,
  data,
  times,
  ylevels,
  absorb = NULL,
  tvarname = "time",
  pvarname = "yprev",
  gap = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="soprobMarkovOrdm_+3A_object">object</code></td>
<td>
<p>a fit object created by <code>blrm</code>, <code>lrm</code>, <code>orm</code>, <code>VGAM::vglm()</code>, or <code>VGAM::vgam()</code></p>
</td></tr>
<tr><td><code id="soprobMarkovOrdm_+3A_data">data</code></td>
<td>
<p>a single observation list or data frame with covariate settings, including the initial state for Y</p>
</td></tr>
<tr><td><code id="soprobMarkovOrdm_+3A_times">times</code></td>
<td>
<p>vector of measurement times</p>
</td></tr>
<tr><td><code id="soprobMarkovOrdm_+3A_ylevels">ylevels</code></td>
<td>
<p>a vector of ordered levels of the outcome variable (numeric or character)</p>
</td></tr>
<tr><td><code id="soprobMarkovOrdm_+3A_absorb">absorb</code></td>
<td>
<p>vector of absorbing states, a subset of <code>ylevels</code>.  The default is no absorbing states. (numeric, character, factor)</p>
</td></tr>
<tr><td><code id="soprobMarkovOrdm_+3A_tvarname">tvarname</code></td>
<td>
<p>name of time variable, defaulting to <code>time</code></p>
</td></tr>
<tr><td><code id="soprobMarkovOrdm_+3A_pvarname">pvarname</code></td>
<td>
<p>name of previous state variable, defaulting to <code>yprev</code></p>
</td></tr>
<tr><td><code id="soprobMarkovOrdm_+3A_gap">gap</code></td>
<td>
<p>name of time gap variable, defaults assuming that gap time is not in the model</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes state occupancy probabilities for a single setting of baseline covariates.  If the model fit was from <code>rms::blrm()</code>, these probabilities are from all the posterior draws of the basic model parameters.  Otherwise they are maximum likelihood point estimates.
</p>


<h3>Value</h3>

<p>if <code>object</code> was not a Bayesian model, a matrix with rows corresponding to times and columns corresponding to states, with values equal to exact state occupancy probabilities.  If <code>object</code> was created by <code>blrm</code>, the result is a 3-dimensional array with the posterior draws as the first dimension.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><a href="https://hbiostat.org/R/Hmisc/markov/">https://hbiostat.org/R/Hmisc/markov/</a>
</p>

<hr>
<h2 id='spikecomp'>spikecomp</h2><span id='topic+spikecomp'></span>

<h3>Description</h3>

<p>Compute Elements of a Spike Histogram
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spikecomp(
  x,
  method = c("tryactual", "simple", "grid"),
  lumptails = 0.01,
  normalize = TRUE,
  y,
  trans = NULL,
  tresult = c("list", "segments", "roundeddata")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spikecomp_+3A_x">x</code></td>
<td>
<p>a numeric variable</p>
</td></tr>
<tr><td><code id="spikecomp_+3A_method">method</code></td>
<td>
<p>specifies the binning and output method.  The default is <code>'tryactual'</code> and is intended to be used for spike histograms plotted in a way that allows for random x-coordinates and data gaps.  No binning is done if there are less than 100 distinct values and the closest distinct <code>x</code> values are distinguishable (not with 1/500th of the data range of each other).  Binning uses <code>pretty</code>.  When <code>trans</code> is specified to transform <code>x</code> to reduce long tails due to outliers, <code>pretty</code> rounding is not done, and <code>lumptails</code> is ignored.  <code>method='grid'</code> is intended for sparkline spike histograms drawn with bar charts, where plotting is done in a way that x-coordinates must be equally spaced.  For this method, extensive binning information is returned.  For either <code>'tryactual'</code> or <code>'grid'</code>, the default if <code>trans</code> is omitted is to put all values beyond the 0.01 or 0.99 quantiles into a single bin so that outliers will not create long nearly empty tails.  When <code>y</code> is specified, <code>method</code> is ignored.</p>
</td></tr>
<tr><td><code id="spikecomp_+3A_lumptails">lumptails</code></td>
<td>
<p>the quantile to use for lumping values into a single left and a single right bin for two of the methods.  When outer quantiles using <code>lumptails</code> equal outer quantiles using <code>2*lumptails</code>, <code>lumptails</code> is ignored as this indicates a large number of ties in the tails of the distribution.</p>
</td></tr>
<tr><td><code id="spikecomp_+3A_normalize">normalize</code></td>
<td>
<p>set to <code>FALSE</code> to not divide frequencies by maximum frequency</p>
</td></tr>
<tr><td><code id="spikecomp_+3A_y">y</code></td>
<td>
<p>a vector of frequencies corresponding to <code>x</code> if you want the (<code>x</code>, <code>y</code>) pairs to be taken as a possibly irregular-spaced frequency tabulation for which you want to convert to a regularly-spaced tabulation like <code>count='tabulate'</code> produces.  If there is a constant gap between <code>x</code> values, the original pairs are return, with possible removal of <code>NA</code>s.</p>
</td></tr>
<tr><td><code id="spikecomp_+3A_trans">trans</code></td>
<td>
<p>a list with three elements: the name of a transformation to make on <code>x</code>, the transformation function, and the inverse transformation function.  The latter is used for <code>method='grid'</code>.  When <code>trans</code> is given <code>lumptails</code> is ignored.  <code>trans</code> applies only to <code>method='tryactual'</code>.</p>
</td></tr>
<tr><td><code id="spikecomp_+3A_tresult">tresult</code></td>
<td>
<p>applies only to <code>method='tryactual'</code>.  The default <code>'list'</code> returns a list with elements <code>x</code>, <code>y</code>, and <code>roundedTo</code>.  <code>method='segments'</code> returns a list suitable for drawing line segments, with elements <code style="white-space: pre;">&#8288;x, y1, y2&#8288;</code>.  <code>method='roundeddata'</code> returns a list with elements <code>x</code> (non-tabulated rounded data vector after excluding <code>NA</code>s) and vector <code>roundedTo</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Derives the line segment coordinates need to draw a spike histogram.  This is useful for adding elements to <code>ggplot2</code> plots and for the <code>describe</code> function to construct spike histograms.  Date/time variables are handled by doing calculations on the underlying numeric scale then converting back to the original class.  For them the left endpoint of the first bin is taken as the minimal data value instead of rounded using <code>pretty()</code>.
</p>


<h3>Value</h3>

<p>when <code>y</code> is specified, a list with elements <code>x</code> and <code>y</code>.  When <code>method='tryactual'</code> the returned value depends on <code>tresult</code>.  For <code>method='grid'</code>, a list with elements <code>x</code> and <code>y</code> and scalar element <code>roundedTo</code> containing the typical bin width.  Here <code>x</code> is a character string.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>Examples</h3>

<pre><code class='language-R'>spikecomp(1:1000)
spikecomp(1:1000, method='grid')
## Not run: 
On a data.table d use ggplot2 to make spike histograms by country and sex groups
s &lt;- d[, spikecomp(x, tresult='segments'), by=.(country, sex)]
ggplot(s) + geom_segment(aes(x=x, y=y1, xend=x, yend=y2, alpha=I(0.3))) +
   scale_y_continuous(breaks=NULL, labels=NULL) + ylab('') +
   facet_grid(country ~ sex)

## End(Not run)
</code></pre>

<hr>
<h2 id='spower'>
Simulate Power of 2-Sample Test for Survival under Complex Conditions
</h2><span id='topic+spower'></span><span id='topic+print.spower'></span><span id='topic+Quantile2'></span><span id='topic+print.Quantile2'></span><span id='topic+plot.Quantile2'></span><span id='topic+logrank'></span><span id='topic+Gompertz2'></span><span id='topic+Lognorm2'></span><span id='topic+Weibull2'></span>

<h3>Description</h3>

<p>Given functions to generate random variables for survival times and
censoring times, <code>spower</code> simulates the power of a user-given
2-sample test for censored data.  By default, the logrank (Cox
2-sample) test is used, and a <code>logrank</code> function for comparing 2
groups is provided. Optionally a Cox model is fitted for each each
simulated dataset and the log hazard ratios are saved (this requires
the <code>survival</code> package). A <code>print</code> method prints various
measures from these.  For composing <span class="rlang"><b>R</b></span> functions to generate random
survival times under complex conditions, the <code>Quantile2</code> function
allows the user to specify the intervention:control hazard ratio as a
function of time, the probability of a control subject actually
receiving the intervention (dropin) as a function of time, and the
probability that an intervention subject receives only the control
agent as a function of time (non-compliance, dropout).
<code>Quantile2</code> returns a function that generates either control or
intervention uncensored survival times subject to non-constant
treatment effect, dropin, and dropout.  There is a <code>plot</code> method
for plotting the results of <code>Quantile2</code>, which will aid in
understanding the effects of the two types of non-compliance and
non-constant treatment effects.  <code>Quantile2</code> assumes that the
hazard function for either treatment group is a mixture of the control
and intervention hazard functions, with mixing proportions defined by
the dropin and dropout probabilities.  It computes hazards and
survival distributions by numerical differentiation and integration
using a grid of (by default) 7500 equally-spaced time points.
</p>
<p>The <code>logrank</code> function is intended to be used with <code>spower</code>
but it can be used by itself.  It returns the 1 degree of freedom
chi-square statistic, with the associated Pike hazard ratio estimate as an attribute.
</p>
<p>The <code>Weibull2</code> function accepts as input two vectors, one
containing two times and one containing two survival probabilities, and
it solves for the scale and shape parameters of the Weibull distribution
(<code class="reqn">S(t) = e^{-\alpha {t}^{\gamma}}</code>)
which will yield
those estimates.  It creates an <span class="rlang"><b>R</b></span> function to evaluate survival
probabilities from this Weibull distribution.  <code>Weibull2</code> is
useful in creating functions to pass as the first argument to
<code>Quantile2</code>.
</p>
<p>The <code>Lognorm2</code> and <code>Gompertz2</code> functions are similar to
<code>Weibull2</code> except that they produce survival functions for the
log-normal and Gompertz distributions.
</p>
<p>When <code>cox=TRUE</code> is specified to <code>spower</code>, the analyst may wish
to extract the two margins of error by using the <code>print</code> method
for <code>spower</code> objects (see example below) and take the maximum of
the two.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spower(rcontrol, rinterv, rcens, nc, ni, 
       test=logrank, cox=FALSE, nsim=500, alpha=0.05, pr=TRUE)

## S3 method for class 'spower'
print(x, conf.int=.95, ...)

Quantile2(scontrol, hratio, 
          dropin=function(times)0, dropout=function(times)0,
          m=7500, tmax, qtmax=.001, mplot=200, pr=TRUE, ...)

## S3 method for class 'Quantile2'
print(x, ...)

## S3 method for class 'Quantile2'
plot(x, 
     what=c("survival", "hazard", "both", "drop", "hratio", "all"),
     dropsep=FALSE, lty=1:4, col=1, xlim, ylim=NULL,
     label.curves=NULL, ...)

logrank(S, group)

Gompertz2(times, surv)
Lognorm2(times, surv)
Weibull2(times, surv)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spower_+3A_rcontrol">rcontrol</code></td>
<td>

<p>a function of n which returns n random uncensored
failure times for the control group.  <code>spower</code> assumes that
non-compliance (dropin) has been taken into account by this
function.
</p>
</td></tr>
<tr><td><code id="spower_+3A_rinterv">rinterv</code></td>
<td>

<p>similar to <code>rcontrol</code> but for the intervention group
</p>
</td></tr>
<tr><td><code id="spower_+3A_rcens">rcens</code></td>
<td>

<p>a function of n which returns n random censoring times.
It is assumed that both treatment groups have the same censoring
distribution.
</p>
</td></tr>
<tr><td><code id="spower_+3A_nc">nc</code></td>
<td>

<p>number of subjects in the control group
</p>
</td></tr>
<tr><td><code id="spower_+3A_ni">ni</code></td>
<td>

<p>number in the intervention group
</p>
</td></tr>
<tr><td><code id="spower_+3A_scontrol">scontrol</code></td>
<td>

<p>a function of a time vector which returns the survival probabilities
for the control group at those times assuming that all patients are
compliant.
</p>
</td></tr>
<tr><td><code id="spower_+3A_hratio">hratio</code></td>
<td>

<p>a function of time which specifies the intervention:control hazard
ratio (treatment effect)
</p>
</td></tr>
<tr><td><code id="spower_+3A_x">x</code></td>
<td>

<p>an object of class &ldquo;Quantile2&rdquo; created by <code>Quantile2</code>,
or of class &ldquo;spower&rdquo; created by <code>spower</code>
</p>
</td></tr>
<tr><td><code id="spower_+3A_conf.int">conf.int</code></td>
<td>

<p>confidence level for determining fold-change margins of error in
estimating the hazard ratio
</p>
</td></tr>
<tr><td><code id="spower_+3A_s">S</code></td>
<td>

<p>a <code>Surv</code> object or other two-column matrix for right-censored
survival times 
</p>
</td></tr>
<tr><td><code id="spower_+3A_group">group</code></td>
<td>

<p>group indicators have length equal to the number of rows in <code>S</code>
argument.
</p>
</td></tr>
<tr><td><code id="spower_+3A_times">times</code></td>
<td>

<p>a vector of two times
</p>
</td></tr>
<tr><td><code id="spower_+3A_surv">surv</code></td>
<td>

<p>a vector of two survival probabilities
</p>
</td></tr>
<tr><td><code id="spower_+3A_test">test</code></td>
<td>

<p>any function of a <code>Surv</code> object and a grouping variable which
computes a chi-square for a two-sample censored data test.  The
default is <code>logrank</code>.
</p>
</td></tr>
<tr><td><code id="spower_+3A_cox">cox</code></td>
<td>

<p>If true <code>TRUE</code> the two margins of error are available by using
the <code>print</code> method for <code>spower</code> objects (see example
below) and taking the maximum of the two.
</p>
</td></tr>
<tr><td><code id="spower_+3A_nsim">nsim</code></td>
<td>

<p>number of simulations to perform (default=500)
</p>
</td></tr>
<tr><td><code id="spower_+3A_alpha">alpha</code></td>
<td>

<p>type I error (default=.05)
</p>
</td></tr>
<tr><td><code id="spower_+3A_pr">pr</code></td>
<td>

<p>If <code>FALSE</code> prevents <code>spower</code> from printing progress notes for
simulations. 
If <code>FALSE</code> prevents <code>Quantile2</code> from printing <code>tmax</code>
when it calculates <code>tmax</code>.
</p>
</td></tr>
<tr><td><code id="spower_+3A_dropin">dropin</code></td>
<td>

<p>a function of time specifying the probability that a control subject
actually is treated with the new intervention at the corresponding
time
</p>
</td></tr>
<tr><td><code id="spower_+3A_dropout">dropout</code></td>
<td>

<p>a function of time specifying the probability of an intervention
subject dropping out to control conditions.  As a function of time,
<code>dropout</code> specifies the probability that a patient is treated
with the control therapy at time t.  <code>dropin</code> and
<code>dropout</code> form mixing proportions for control and intervention
hazard functions.
</p>
</td></tr>
<tr><td><code id="spower_+3A_m">m</code></td>
<td>

<p>number of time points used for approximating functions (default is
7500)
</p>
</td></tr>
<tr><td><code id="spower_+3A_tmax">tmax</code></td>
<td>

<p>maximum time point to use in the grid of <code>m</code> times.  Default is
the time such that <code>scontrol(time)</code> is <code>qtmax</code>.
</p>
</td></tr>
<tr><td><code id="spower_+3A_qtmax">qtmax</code></td>
<td>

<p>survival probability corresponding to the last time point used for
approximating survival and hazard functions.  Default is 0.001.  For
<code>qtmax</code> of the time for which a simulated time is needed which
corresponds to a survival probability of less than <code>qtmax</code>, the
simulated value will be <code>tmax</code>.
</p>
</td></tr>
<tr><td><code id="spower_+3A_mplot">mplot</code></td>
<td>

<p>number of points used for approximating functions for use in
plotting (default is 200 equally spaced points)
</p>
</td></tr>
<tr><td><code id="spower_+3A_...">...</code></td>
<td>

<p>optional arguments passed to the <code>scontrol</code> function when it's
evaluated by <code>Quantile2</code>.  Unused for <code>print.spower</code>.
</p>
</td></tr>
<tr><td><code id="spower_+3A_what">what</code></td>
<td>

<p>a single character constant (may be abbreviated) specifying which
functions to plot.  The default is &lsquo;<span class="samp">&#8288;"both"&#8288;</span>&rsquo; meaning both
survival and hazard functions.  Specify <code>what="drop"</code> to just
plot the dropin and dropout functions, <code>what="hratio"</code> to plot
the hazard ratio functions, or &lsquo;<span class="samp">&#8288;"all"&#8288;</span>&rsquo; to make 4 separate plots
showing all functions (6 plots if <code>dropsep=TRUE</code>).
</p>
</td></tr>
<tr><td><code id="spower_+3A_dropsep">dropsep</code></td>
<td>

<p>If <code>TRUE</code> makes <code>plot.Quantile2</code> separate pure and
contaminated functions onto separate plots
</p>
</td></tr>
<tr><td><code id="spower_+3A_lty">lty</code></td>
<td>

<p>vector of line types
</p>
</td></tr>
<tr><td><code id="spower_+3A_col">col</code></td>
<td>

<p>vector of colors
</p>
</td></tr>
<tr><td><code id="spower_+3A_xlim">xlim</code></td>
<td>

<p>optional x-axis limits
</p>
</td></tr>
<tr><td><code id="spower_+3A_ylim">ylim</code></td>
<td>

<p>optional y-axis limits
</p>
</td></tr>
<tr><td><code id="spower_+3A_label.curves">label.curves</code></td>
<td>

<p>optional list which is passed as the <code>opts</code> argument to
<code><a href="#topic+labcurve">labcurve</a></code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>spower</code> returns the power estimate (fraction of simulated
chi-squares greater than the alpha-critical value).  If
<code>cox=TRUE</code>, <code>spower</code> returns an object of class
&ldquo;spower&rdquo; containing the power and various other quantities.
</p>
<p><code>Quantile2</code> returns an <span class="rlang"><b>R</b></span> function of class &ldquo;Quantile2&rdquo;
with attributes that drive the <code>plot</code> method.  The major
attribute is a list containing several lists.  Each of these sub-lists
contains a <code>Time</code> vector along with one of the following:
survival probabilities for either treatment group and with or without
contamination caused by non-compliance, hazard rates in a similar way,
intervention:control hazard ratio function with and without
contamination, and dropin and dropout functions.
</p>
<p><code>logrank</code> returns a single chi-square statistic and an attribute <code>hr</code> which is the Pike hazard ratio estimate.
</p>
<p><code>Weibull2</code>, <code>Lognorm2</code> and <code>Gompertz2</code> return an <span class="rlang"><b>R</b></span>
function with three arguments, only the first of which (the vector of
<code>times</code>) is intended to be specified by the user.
</p>


<h3>Side Effects</h3>

<p><code>spower</code> prints the interation number every 10 iterations if
<code>pr=TRUE</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University School of Medicine
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Lakatos E (1988): Sample sizes based on the log-rank statistic in complex
clinical trials.  Biometrics 44:229&ndash;241 (Correction 44:923).
</p>
<p>Cuzick J, Edwards R, Segnan N (1997): Adjusting for non-compliance and 
contamination in randomized clinical trials. Stat in Med 16:1017&ndash;1029.
</p>
<p>Cook, T (2003): Methods for mid-course corrections in clinical trials
with survival outcomes.  Stat in Med 22:3431&ndash;3447.
</p>
<p>Barthel FMS, Babiker A et al (2006): Evaluation of sample size and power
for multi-arm survival trials allowing for non-uniform accrual,
non-proportional hazards, loss to follow-up and cross-over.  Stat in Med
25:2521&ndash;2542.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cpower">cpower</a></code>, <code><a href="#topic+ciapower">ciapower</a></code>, <code><a href="#topic+bpower">bpower</a></code>,
<code><a href="rms.html#topic+cph">cph</a></code>, <code><a href="survival.html#topic+coxph">coxph</a></code>,
<code><a href="#topic+labcurve">labcurve</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulate a simple 2-arm clinical trial with exponential survival so
# we can compare power simulations of logrank-Cox test with cpower()
# Hazard ratio is constant and patients enter the study uniformly
# with follow-up ranging from 1 to 3 years
# Drop-in probability is constant at .1 and drop-out probability is
# constant at .175.  Two-year survival of control patients in absence
# of drop-in is .8 (mortality=.2).  Note that hazard rate is -log(.8)/2
# Total sample size (both groups combined) is 1000
# % mortality reduction by intervention (if no dropin or dropout) is 25
# This corresponds to a hazard ratio of 0.7283 (computed by cpower)


cpower(2, 1000, .2, 25, accrual=2, tmin=1, 
       noncomp.c=10, noncomp.i=17.5)


ranfun &lt;- Quantile2(function(x)exp(log(.8)/2*x),
                    hratio=function(x)0.7283156,
                    dropin=function(x).1,
                    dropout=function(x).175)


rcontrol &lt;- function(n) ranfun(n, what='control')
rinterv  &lt;- function(n) ranfun(n, what='int')
rcens    &lt;- function(n) runif(n, 1, 3)


set.seed(11)   # So can reproduce results
spower(rcontrol, rinterv, rcens, nc=500, ni=500, 
       test=logrank, nsim=50)  # normally use nsim=500 or 1000

## Not run: 
# Run the same simulation but fit the Cox model for each one to
# get log hazard ratios for the purpose of assessing the tightness
# confidence intervals that are likely to result

set.seed(11)
u &lt;- spower(rcontrol, rinterv, rcens, nc=500, ni=500, 
       test=logrank, nsim=50, cox=TRUE)
u
v &lt;- print(u)
v[c('MOElower','MOEupper','SE')]

## End(Not run)

# Simulate a 2-arm 5-year follow-up study for which the control group's
# survival distribution is Weibull with 1-year survival of .95 and
# 3-year survival of .7.  All subjects are followed at least one year,
# and patients enter the study with linearly increasing probability  after that
# Assume there is no chance of dropin for the first 6 months, then the
# probability increases linearly up to .15 at 5 years
# Assume there is a linearly increasing chance of dropout up to .3 at 5 years
# Assume that the treatment has no effect for the first 9 months, then
# it has a constant effect (hazard ratio of .75)


# First find the right Weibull distribution for compliant control patients
sc &lt;- Weibull2(c(1,3), c(.95,.7))
sc


# Inverse cumulative distribution for case where all subjects are followed
# at least a years and then between a and b years the density rises
# as (time - a) ^ d is a + (b-a) * u ^ (1/(d+1))


rcens &lt;- function(n) 1 + (5-1) * (runif(n) ^ .5)
# To check this, type hist(rcens(10000), nclass=50)


# Put it all together


f &lt;- Quantile2(sc, 
      hratio=function(x)ifelse(x&lt;=.75, 1, .75),
      dropin=function(x)ifelse(x&lt;=.5, 0, .15*(x-.5)/(5-.5)),
      dropout=function(x).3*x/5)


par(mfrow=c(2,2))
# par(mfrow=c(1,1)) to make legends fit
plot(f, 'all', label.curves=list(keys='lines'))


rcontrol &lt;- function(n) f(n, 'control')
rinterv  &lt;- function(n) f(n, 'intervention')


set.seed(211)
spower(rcontrol, rinterv, rcens, nc=350, ni=350, 
       test=logrank, nsim=50)  # normally nsim=500 or more
par(mfrow=c(1,1))

# Compose a censoring time generator function such that at 1 year
# 5% of subjects are accrued, at 3 years 70% are accured, and at 10
# years 100% are accrued.  The trial proceeds two years past the last
# accrual for a total of 12 years of follow-up for the first subject.
# Use linear interporation between these 3 points

rcens &lt;- function(n)
{
  times &lt;- c(0,1,3,10)
  accrued &lt;- c(0,.05,.7,1)
  # Compute inverse of accrued function at U(0,1) random variables
  accrual.times &lt;- approx(accrued, times, xout=runif(n))$y
  censor.times &lt;- 12 - accrual.times
  censor.times
}

censor.times &lt;- rcens(500)
# hist(censor.times, nclass=20)
accrual.times &lt;- 12 - censor.times
# Ecdf(accrual.times)
# lines(c(0,1,3,10), c(0,.05,.7,1), col='red')
# spower(..., rcens=rcens, ...)

## Not run: 
# To define a control survival curve from a fitted survival curve
# with coordinates (tt, surv) with tt[1]=0, surv[1]=1:

Scontrol &lt;- function(times, tt, surv) approx(tt, surv, xout=times)$y
tt &lt;- 0:6
surv &lt;- c(1, .9, .8, .75, .7, .65, .64)
formals(Scontrol) &lt;- list(times=NULL, tt=tt, surv=surv)

# To use a mixture of two survival curves, with e.g. mixing proportions
# of .2 and .8, use the following as a guide:
#
# Scontrol &lt;- function(times, t1, s1, t2, s2)
#  .2*approx(t1, s1, xout=times)$y + .8*approx(t2, s2, xout=times)$y
# t1 &lt;- ...; s1 &lt;- ...; t2 &lt;- ...; s2 &lt;- ...;
# formals(Scontrol) &lt;- list(times=NULL, t1=t1, s1=s1, t2=t2, s2=s2)

# Check that spower can detect a situation where generated censoring times
# are later than all failure times

rcens &lt;- function(n) runif(n, 0, 7)
f &lt;- Quantile2(scontrol=Scontrol, hratio=function(x).8, tmax=6)
cont &lt;- function(n) f(n, what='control')
int  &lt;- function(n) f(n, what='intervention')
spower(rcontrol=cont, rinterv=int, rcens=rcens, nc=300, ni=300, nsim=20)

# Do an unstratified logrank test
library(survival)
# From SAS/STAT PROC LIFETEST manual, p. 1801
days &lt;- c(179,256,262,256,255,224,225,287,319,264,237,156,270,257,242,
          157,249,180,226,268,378,355,319,256,171,325,325,217,255,256,
          291,323,253,206,206,237,211,229,234,209)
status &lt;- c(1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,0,
            0,rep(1,19))
treatment &lt;- c(rep(1,10), rep(2,10), rep(1,10), rep(2,10))
sex &lt;- Cs(F,F,M,F,M,F,F,M,M,M,F,F,M,M,M,F,M,F,F,M,
          M,M,M,M,F,M,M,F,F,F,M,M,M,F,F,M,F,F,F,F)
data.frame(days, status, treatment, sex)
table(treatment, status)
logrank(Surv(days, status), treatment)  # agrees with p. 1807
# For stratified tests the picture is puzzling.
# survdiff(Surv(days,status) ~ treatment + strata(sex))$chisq
# is 7.246562, which does not agree with SAS (7.1609)
# But summary(coxph(Surv(days,status) ~ treatment + strata(sex)))
# yields 7.16 whereas summary(coxph(Surv(days,status) ~ treatment))
# yields 5.21 as the score test, not agreeing with SAS or logrank() (5.6485)

## End(Not run)
</code></pre>

<hr>
<h2 id='spss.get'>Enhanced Importing of SPSS Files</h2><span id='topic+spss.get'></span>

<h3>Description</h3>

<p><code>spss.get</code> invokes the <code>read.spss</code> function in the
<span class="pkg">foreign</span> package to read an SPSS file, with a default output
format of <code>"data.frame"</code>.  The <code>label</code> function is used to
attach labels to individual variables instead of to the data frame as
done by <code>read.spss</code>.  By default, integer-valued variables are
converted to a storage mode of integer unless
<code>force.single=FALSE</code>.  Date variables are converted to <span class="rlang"><b>R</b></span> <code>Date</code>
variables.  By default, underscores in names are converted to periods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spss.get(file, lowernames=FALSE, datevars = NULL,
         use.value.labels = TRUE, to.data.frame = TRUE,
         max.value.labels = Inf, force.single=TRUE,
         allow=NULL, charfactor=FALSE, reencode = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spss.get_+3A_file">file</code></td>
<td>
<p>input SPSS save file.  May be a file on the WWW, indicated
by <code>file</code> starting with <code>'http://'</code> or <code>'https://'</code>.</p>
</td></tr>
<tr><td><code id="spss.get_+3A_lowernames">lowernames</code></td>
<td>
<p>set to <code>TRUE</code> to convert variable names to
lower case</p>
</td></tr>
<tr><td><code id="spss.get_+3A_datevars">datevars</code></td>
<td>
<p>vector of variable names containing dates to be
converted to <span class="rlang"><b>R</b></span> internal format</p>
</td></tr>
<tr><td><code id="spss.get_+3A_use.value.labels">use.value.labels</code></td>
<td>
<p>see <code><a href="foreign.html#topic+read.spss">read.spss</a></code></p>
</td></tr>
<tr><td><code id="spss.get_+3A_to.data.frame">to.data.frame</code></td>
<td>
<p>see <code><a href="foreign.html#topic+read.spss">read.spss</a></code>; default is
<code>TRUE</code> for <code>spss.get</code></p>
</td></tr>
<tr><td><code id="spss.get_+3A_max.value.labels">max.value.labels</code></td>
<td>
<p>see <code><a href="foreign.html#topic+read.spss">read.spss</a></code></p>
</td></tr>
<tr><td><code id="spss.get_+3A_force.single">force.single</code></td>
<td>
<p>set to <code>FALSE</code> to prevent integer-valued
variables from being converted from storage mode <code>double</code> to
<code>integer</code></p>
</td></tr>
<tr><td><code id="spss.get_+3A_allow">allow</code></td>
<td>
<p>a vector of characters allowed by <span class="rlang"><b>R</b></span> that should not be
converted to periods in variable names.  By default, underscores in
variable names are converted to periods as with <span class="rlang"><b>R</b></span> before version 1.9.</p>
</td></tr>
<tr><td><code id="spss.get_+3A_charfactor">charfactor</code></td>
<td>
<p>set to <code>TRUE</code> to change character variables to
factors if they have fewer than n/2 unique values.  Blanks and null
strings are converted to <code>NA</code>s.</p>
</td></tr>
<tr><td><code id="spss.get_+3A_reencode">reencode</code></td>
<td>
<p>see <code><a href="foreign.html#topic+read.spss">read.spss</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data frame or list
</p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="foreign.html#topic+read.spss">read.spss</a></code>,<code><a href="#topic+cleanup.import">cleanup.import</a></code>,<code><a href="#topic+sas.get">sas.get</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
w &lt;- spss.get('/tmp/my.sav', datevars=c('birthdate','deathdate'))
  
## End(Not run)
</code></pre>

<hr>
<h2 id='src'>Source a File from the Current Working Directory</h2><span id='topic+src'></span>

<h3>Description</h3>

<p><code>src</code> concatenates <code>".s"</code> to its argument, quotes the result,
and <code>source</code>s in the file.  It sets <code>options(last.source)</code> to
this file name so that <code>src()</code> can be issued to re-<code>source</code>
the file when it is edited.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>src(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="src_+3A_x">x</code></td>
<td>
<p>an unquoted file name aside from <code>".s"</code>.  This base file
name must be a legal S name.</p>
</td></tr>
</table>


<h3>Side Effects</h3>

<p>Sets system option <code>last.source</code>
</p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+source">source</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
src(myfile)   # source("myfile.s")
src()         # re-source myfile.s

## End(Not run)
</code></pre>

<hr>
<h2 id='stat_plsmo'>Add a lowess smoother without counfidence bands.</h2><span id='topic+stat_plsmo'></span>

<h3>Description</h3>

<p>Automatically selects <code>iter=0</code> for <code>lowess</code> if <code>y</code> is binary, otherwise uses <code>iter=3</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stat_plsmo(
  mapping = NULL,
  data = NULL,
  geom = "smooth",
  position = "identity",
  n = 80,
  fullrange = FALSE,
  span = 2/3,
  fun = function(x) x,
  na.rm = FALSE,
  show.legend = NA,
  inherit.aes = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stat_plsmo_+3A_mapping">mapping</code>, <code id="stat_plsmo_+3A_data">data</code>, <code id="stat_plsmo_+3A_geom">geom</code>, <code id="stat_plsmo_+3A_position">position</code>, <code id="stat_plsmo_+3A_show.legend">show.legend</code>, <code id="stat_plsmo_+3A_inherit.aes">inherit.aes</code></td>
<td>
<p>see ggplot2 documentation</p>
</td></tr>
<tr><td><code id="stat_plsmo_+3A_n">n</code></td>
<td>
<p>number of points to evaluate smoother at</p>
</td></tr>
<tr><td><code id="stat_plsmo_+3A_fullrange">fullrange</code></td>
<td>
<p>should the fit span the full range of the plot, or just
the data</p>
</td></tr>
<tr><td><code id="stat_plsmo_+3A_span">span</code></td>
<td>
<p>see <code>f</code> argument to <code>lowess</code></p>
</td></tr>
<tr><td><code id="stat_plsmo_+3A_fun">fun</code></td>
<td>
<p>a function to transform smoothed <code>y</code></p>
</td></tr>
<tr><td><code id="stat_plsmo_+3A_na.rm">na.rm</code></td>
<td>
<p>If <code>FALSE</code> (the default), removes missing values with
a warning.  If <code>TRUE</code> silently removes missing values.</p>
</td></tr>
<tr><td><code id="stat_plsmo_+3A_...">...</code></td>
<td>
<p>other arguments are passed to smoothing function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with additional columns
</p>
<table>
<tr><td><code>y</code></td>
<td>
<p>predicted value</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+lowess">lowess</a></code> for <code>loess</code> smoother.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require(ggplot2)
c &lt;- ggplot(mtcars, aes(qsec, wt))
c + stat_plsmo()
c + stat_plsmo() + geom_point()

c + stat_plsmo(span = 0.1) + geom_point()

# Smoothers for subsets
c &lt;- ggplot(mtcars, aes(y=wt, x=mpg)) + facet_grid(. ~ cyl)
c + stat_plsmo() + geom_point()
c + stat_plsmo(fullrange = TRUE) + geom_point()

# Geoms and stats are automatically split by aesthetics that are factors
c &lt;- ggplot(mtcars, aes(y=wt, x=mpg, colour=factor(cyl)))
c + stat_plsmo() + geom_point()
c + stat_plsmo(aes(fill = factor(cyl))) + geom_point()
c + stat_plsmo(fullrange=TRUE) + geom_point()

# Example with logistic regression
data("kyphosis", package="rpart")
qplot(Age, as.numeric(Kyphosis) - 1, data = kyphosis) + stat_plsmo()

</code></pre>

<hr>
<h2 id='stata.get'>Enhanced Importing of STATA Files</h2><span id='topic+stata.get'></span>

<h3>Description</h3>

<p>Reads a file in Stata version 5-11 binary format format into a
data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stata.get(file, lowernames = FALSE, convert.dates = TRUE,
          convert.factors = TRUE, missing.type = FALSE,
          convert.underscore = TRUE, warn.missing.labels = TRUE,
          force.single = TRUE, allow=NULL, charfactor=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stata.get_+3A_file">file</code></td>
<td>
<p>input <abbr><span class="acronym">SPSS</span></abbr> save file.  May be a file on the <abbr><span class="acronym">WWW</span></abbr>, indicated
by <code>file</code> starting with &lsquo;<span class="samp">&#8288;'https://'&#8288;</span>&rsquo;.</p>
</td></tr>
<tr><td><code id="stata.get_+3A_lowernames">lowernames</code></td>
<td>
<p>set to <code>TRUE</code> to convert variable names to
lower case</p>
</td></tr>
<tr><td><code id="stata.get_+3A_convert.dates">convert.dates</code></td>
<td>
<p>see <code><a href="foreign.html#topic+read.dta">read.dta</a></code></p>
</td></tr>
<tr><td><code id="stata.get_+3A_convert.factors">convert.factors</code></td>
<td>
<p>see <code><a href="foreign.html#topic+read.dta">read.dta</a></code></p>
</td></tr>
<tr><td><code id="stata.get_+3A_missing.type">missing.type</code></td>
<td>
<p>see <code><a href="foreign.html#topic+read.dta">read.dta</a></code></p>
</td></tr>
<tr><td><code id="stata.get_+3A_convert.underscore">convert.underscore</code></td>
<td>
<p>see <code><a href="foreign.html#topic+read.dta">read.dta</a></code></p>
</td></tr>
<tr><td><code id="stata.get_+3A_warn.missing.labels">warn.missing.labels</code></td>
<td>
<p>see <code><a href="foreign.html#topic+read.dta">read.dta</a></code></p>
</td></tr>
<tr><td><code id="stata.get_+3A_force.single">force.single</code></td>
<td>
<p>set to <code>FALSE</code> to prevent integer-valued
variables from being converted from storage mode <code>double</code> to
<code>integer</code></p>
</td></tr>
<tr><td><code id="stata.get_+3A_allow">allow</code></td>
<td>
<p>a vector of characters allowed by <span class="rlang"><b>R</b></span> that should not be
converted to periods in variable names.  By default, underscores in
variable names are converted to periods as with <span class="rlang"><b>R</b></span> before version 1.9.</p>
</td></tr>
<tr><td><code id="stata.get_+3A_charfactor">charfactor</code></td>
<td>
<p>set to <code>TRUE</code> to change character variables to
factors if they have fewer than n/2 unique values.  Blanks and null
strings are converted to <code>NA</code>s.</p>
</td></tr>
<tr><td><code id="stata.get_+3A_...">...</code></td>
<td>
<p>arguments passed to <code><a href="foreign.html#topic+read.dta">read.dta</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>stata.get</code> invokes the <code><a href="foreign.html#topic+read.dta">read.dta</a></code> function in the
<span class="pkg">foreign</span> package to read an STATA file, with a default output
format of <code><a href="base.html#topic+data.frame">data.frame</a></code>.  The <code><a href="#topic+label">label</a></code> function is used to
attach labels to individual variables instead of to the data frame as
done by <code><a href="foreign.html#topic+read.dta">read.dta</a></code>.  By default, integer-valued variables are
converted to a storage mode of integer unless
<code>force.single=FALSE</code>.  Date variables are converted to <span class="rlang"><b>R</b></span>
<code><a href="base.html#topic+Date">Date</a></code> variables.  By default, underscores in names are converted to periods.
</p>


<h3>Value</h3>

<p>A data frame</p>


<h3>Author(s)</h3>

<p>Charles Dupont</p>


<h3>See Also</h3>

<p><code><a href="foreign.html#topic+read.dta">read.dta</a></code>,<code><a href="#topic+cleanup.import">cleanup.import</a></code>,<code><a href="#topic+label">label</a></code>,<code><a href="base.html#topic+data.frame">data.frame</a></code>,<code><a href="base.html#topic+Date">Date</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
w &lt;- stata.get('/tmp/my.dta')

## End(Not run)
</code></pre>

<hr>
<h2 id='string.bounding.box'>Determine Dimensions of Strings</h2><span id='topic+string.bounding.box'></span>

<h3>Description</h3>

<p>This determines the number of rows and maximum number of columns of
each string in a vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>string.bounding.box(string, type = c("chars", "width"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="string.bounding.box_+3A_string">string</code></td>
<td>
<p>vector of strings</p>
</td></tr>
<tr><td><code id="string.bounding.box_+3A_type">type</code></td>
<td>
<p>character: whether to count characters or screen columns</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>rows</code></td>
<td>
<p>vector containing the number of character rows in each string</p>
</td></tr>
<tr><td><code>columns</code></td>
<td>
<p>vector containing the maximum number of character
columns in each string</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Charles Dupont</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+nchar">nchar</a></code>, <code><a href="#topic+stringDims">stringDims</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- c("this is a single line string", "This is a\nmulti-line string")
stringDims(a)
</code></pre>

<hr>
<h2 id='string.break.line'>Break a String into Many Lines at Newlines</h2><span id='topic+string.break.line'></span>

<h3>Description</h3>

<p>Takes a string and breaks it into seperate substrings where there are
newline characters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>string.break.line(string)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="string.break.line_+3A_string">string</code></td>
<td>
<p>character vector to be separated into many lines.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list that is the same length of as the <code>string</code>
argument.
</p>
<p>Each list element is a character vector.
</p>
<p>Each character vectors elements are the
split lines of the corresponding element in the <code>string</code> argument vector.
</p>


<h3>Author(s)</h3>

<p>Charles Dupont</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+strsplit">strsplit</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- c('', 'this is a single line string',
       'This is a\nmulti-line string.')

b &lt;- string.break.line(a)
</code></pre>

<hr>
<h2 id='stringDims'>String Dimentions</h2><span id='topic+stringDims'></span>

<h3>Description</h3>

<p>Finds the height and width of all the string in a character vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stringDims(string)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stringDims_+3A_string">string</code></td>
<td>
<p>vector of strings</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>stringDims</code> finds the number of characters in width and number of
lines in height for each string in the <code>string</code> argument.
</p>


<h3>Value</h3>

<table>
<tr><td><code>height</code></td>
<td>
<p>a vector of the number of lines in each string.</p>
</td></tr>
<tr><td><code>width</code></td>
<td>
<p>a vector with the number of character columns in the
longest line.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Charles Dupont</p>


<h3>See Also</h3>

<p><code><a href="#topic+string.bounding.box">string.bounding.box</a></code>, <code><a href="base.html#topic+nchar">nchar</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- c("this is a single line string", "This is a\nmulty line string")
stringDims(a)
</code></pre>

<hr>
<h2 id='subplot'>Embed a new plot within an existing plot</h2><span id='topic+subplot'></span>

<h3>Description</h3>

<p>Subplot will embed a new plot within an existing plot at the
coordinates specified (in user units of the existing plot).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>subplot(fun, x, y, size=c(1,1), vadj=0.5, hadj=0.5, pars=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="subplot_+3A_fun">fun</code></td>
<td>
<p>an expression or function defining the new plot to be embedded.</p>
</td></tr>
<tr><td><code id="subplot_+3A_x">x</code></td>
<td>
<p><code>x</code>-coordinate(s) of the new plot (in user coordinates
of the existing plot).</p>
</td></tr>
<tr><td><code id="subplot_+3A_y">y</code></td>
<td>
<p><code>y</code>-coordinate(s) of the new plot, <code>x</code> and <code>y</code>
can be specified in any of the ways understood by <code>xy.coords</code>.</p>
</td></tr>
<tr><td><code id="subplot_+3A_size">size</code></td>
<td>
<p>The size of the embedded plot in inches if <code>x</code> and
<code>y</code> have length 1.</p>
</td></tr>
<tr><td><code id="subplot_+3A_vadj">vadj</code></td>
<td>
<p>vertical adjustment of the plot when <code>y</code> is a scalar,
the default is to center vertically, 0 means place the bottom of the
plot at <code>y</code>, 1 places the top of the plot at <code>y</code>.</p>
</td></tr>
<tr><td><code id="subplot_+3A_hadj">hadj</code></td>
<td>
<p>horizontal adjustment of the plot when <code>x</code> is a
scalar, the default is to center horizontally, 0 means place the
left edge of the plot at <code>x</code>, and 1 means place the right edge
of the plot at <code>x</code>.</p>
</td></tr>
<tr><td><code id="subplot_+3A_pars">pars</code></td>
<td>
<p>a list of parameters to be passed to <code>par</code> before
running <code>fun</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The coordinates <code>x</code> and <code>y</code> can be scalars or vectors of
length 2.  If vectors of length 2 then they determine the opposite
corners of the rectangle for the embedded plot (and the parameters
<code>size</code>, <code>vadj</code>, and <code>hadj</code> are all ignored.
</p>
<p>If <code>x</code> and <code>y</code> are given as scalars then the plot position
relative to the point and the size of the plot will be determined by
the arguments <code>size</code>, <code>vadj</code>, and <code>hadj</code>.  The default
is to center a 1 inch by 1 inch plot at <code>x,y</code>.  Setting
<code>vadj</code> and <code>hadj</code> to <code>(0,0)</code> will position the lower
left corner of the plot at <code>(x,y)</code>.
</p>
<p>The rectangle defined by <code>x</code>, <code>y</code>, <code>size</code>, <code>vadj</code>,
and <code>hadj</code> will be used as the plotting area of the new plot.
Any tick marks, axis labels, main and sub titles will be outside of
this rectangle.
</p>
<p>Any graphical parameter settings that you would like to be in place
before <code>fun</code> is evaluated can be specified in the <code>pars</code>
argument (warning: specifying layout parameters here (<code>plt</code>,
<code>mfrow</code>, etc.) may cause unexpected results).
</p>
<p>After the function completes the graphical parameters will have been
reset to what they were before calling the function (so you can
continue to augment the original plot).
</p>


<h3>Value</h3>

<p>An invisible list with the graphical parameters that were in effect
when the subplot was created.  Passing this list to <code>par</code> will
enable you to augment the embedded plot.
</p>


<h3>Author(s)</h3>

<p>Greg Snow <a href="mailto:greg.snow@imail.org">greg.snow@imail.org</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+cnvrt.coords">cnvrt.coords</a></code>, <code><a href="graphics.html#topic+par">par</a></code>, <code><a href="graphics.html#topic+symbols">symbols</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># make an original plot
plot( 11:20, sample(51:60) )

# add some histograms

subplot( hist(rnorm(100)), 15, 55)
subplot( hist(runif(100),main='',xlab='',ylab=''), 11, 51, hadj=0, vadj=0)
subplot( hist(rexp(100, 1/3)), 20, 60, hadj=1, vadj=1, size=c(0.5,2) )
subplot( hist(rt(100,3)), c(12,16), c(57,59), pars=list(lwd=3,ask=FALSE) )

tmp &lt;- rnorm(25)
qqnorm(tmp)
qqline(tmp)
tmp2 &lt;- subplot( hist(tmp,xlab='',ylab='',main=''), 
		cnvrt.coords(0.1,0.9,'plt')$usr, vadj=1, hadj=0 )
abline(v=0, col='red') # wrong way to add a reference line to histogram

# right way to add a reference line to histogram
op &lt;- par(no.readonly=TRUE)
par(tmp2)
abline(v=0, col='green')
par(op)


</code></pre>

<hr>
<h2 id='summarize'>Summarize Scalars or Matrices by Cross-Classification</h2><span id='topic+summarize'></span><span id='topic+asNumericMatrix'></span><span id='topic+matrix2dataFrame'></span>

<h3>Description</h3>

<p><code>summarize</code> is a fast version of <code>summary.formula(formula,
method="cross",overall=FALSE)</code> for producing stratified summary statistics
and storing them in a data frame for plotting (especially with trellis
<code>xyplot</code> and <code>dotplot</code> and Hmisc <code>xYplot</code>).  Unlike
<code>aggregate</code>, <code>summarize</code> accepts a matrix as its first
argument and a multi-valued <code>FUN</code>
argument and <code>summarize</code> also labels the variables in the new data
frame using their original names.  Unlike methods based on
<code>tapply</code>, <code>summarize</code> stores the values of the stratification
variables using their original types, e.g., a numeric <code>by</code> variable
will remain a numeric variable in the collapsed data frame.
<code>summarize</code> also retains <code>"label"</code> attributes for variables.
<code>summarize</code> works especially well with the Hmisc <code>xYplot</code>
function for displaying multiple summaries of a single variable on each
panel, such as means and upper and lower confidence limits.
</p>
<p><code>asNumericMatrix</code> converts a data frame into a numeric matrix,
saving attributes to reverse the process by <code>matrix2dataframe</code>.
It saves attributes that are commonly preserved across row
subsetting (i.e., it does not save <code>dim</code>, <code>dimnames</code>, or
<code>names</code> attributes).
</p>
<p><code>matrix2dataFrame</code> converts a numeric matrix back into a data
frame if it was created by <code>asNumericMatrix</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summarize(X, by, FUN, ..., 
          stat.name=deparse(substitute(X)),
          type=c('variables','matrix'), subset=TRUE,
          keepcolnames=FALSE)

asNumericMatrix(x)

matrix2dataFrame(x, at=attr(x, 'origAttributes'), restoreAll=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summarize_+3A_x">X</code></td>
<td>

<p>a vector or matrix capable of being operated on by the
function specified as the <code>FUN</code> argument
</p>
</td></tr>
<tr><td><code id="summarize_+3A_by">by</code></td>
<td>

<p>one or more stratification variables.  If a single
variable, <code>by</code> may be a vector, otherwise it should be a list.
Using the Hmisc <code>llist</code> function instead of <code>list</code> will result
in individual variable names being accessible to <code>summarize</code>.  For
example, you can specify <code>llist(age.group,sex)</code> or
<code>llist(Age=age.group,sex)</code>.  The latter gives <code>age.group</code> a
new temporary name, <code>Age</code>. 
</p>
</td></tr>
<tr><td><code id="summarize_+3A_fun">FUN</code></td>
<td>

<p>a function of a single vector argument, used to create the statistical
summaries for <code>summarize</code>.  <code>FUN</code> may compute any number of
statistics. 
</p>
</td></tr>
<tr><td><code id="summarize_+3A_...">...</code></td>
<td>
<p>extra arguments are passed to <code>FUN</code></p>
</td></tr>
<tr><td><code id="summarize_+3A_stat.name">stat.name</code></td>
<td>

<p>the name to use when creating the main summary variable.  By default,
the name of the <code>X</code> argument is used.  Set <code>stat.name</code> to
<code>NULL</code> to suppress this name replacement.
</p>
</td></tr>
<tr><td><code id="summarize_+3A_type">type</code></td>
<td>

<p>Specify <code>type="matrix"</code> to store the summary variables (if there are
more than one) in a matrix.
</p>
</td></tr>
<tr><td><code id="summarize_+3A_subset">subset</code></td>
<td>

<p>a logical vector or integer vector of subscripts used to specify the
subset of data to use in the analysis.  The default is to use all
observations in the data frame.
</p>
</td></tr>
<tr><td><code id="summarize_+3A_keepcolnames">keepcolnames</code></td>
<td>
<p>by default when <code>type="matrix"</code>, the first
column of the computed matrix is the name of the first argument to
<code>summarize</code>.  Set <code>keepcolnames=TRUE</code> to retain the name of
the first column created by <code>FUN</code></p>
</td></tr>
<tr><td><code id="summarize_+3A_x">x</code></td>
<td>

<p>a data frame (for <code>asNumericMatrix</code>) or a numeric matrix (for
<code>matrix2dataFrame</code>).
</p>
</td></tr>
<tr><td><code id="summarize_+3A_at">at</code></td>
<td>
<p>List containing attributes of original data frame that survive
subsetting. Defaults to attribute <code>"origAttributes"</code> of the
object <code>x</code>, created by the call to <code>asNumericMatrix</code></p>
</td></tr>
<tr><td><code id="summarize_+3A_restoreall">restoreAll</code></td>
<td>

<p>set to <code>FALSE</code> to only restore attributes <code>label</code>,
<code>units</code>, and <code>levels</code> instead of all attributes
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>summarize</code>, a data frame containing the <code>by</code> variables and the
statistical summaries (the first of which is named the same as the <code>X</code>
variable unless <code>stat.name</code> is given).  If <code>type="matrix"</code>, the
summaries are stored in a single variable in the data frame, and this
variable is a matrix.
</p>
<p><code>asNumericMatrix</code> returns a numeric matrix and stores an object
<code>origAttributes</code> as an attribute of the returned object, with original
attributes of component variables, the <code>storage.mode</code>. 
</p>
<p><code>matrix2dataFrame</code> returns a data frame.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+label">label</a></code>, <code><a href="#topic+cut2">cut2</a></code>, <code><a href="#topic+llist">llist</a></code>, <code><a href="base.html#topic+by">by</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
s &lt;- summarize(ap&gt;1, llist(size=cut2(sz, g=4), bone), mean,
               stat.name='Proportion')
dotplot(Proportion ~ size | bone, data=s7)

## End(Not run)

set.seed(1)
temperature &lt;- rnorm(300, 70, 10)
month &lt;- sample(1:12, 300, TRUE)
year  &lt;- sample(2000:2001, 300, TRUE)
g &lt;- function(x)c(Mean=mean(x,na.rm=TRUE),Median=median(x,na.rm=TRUE))
summarize(temperature, month, g)
mApply(temperature, month, g)

mApply(temperature, month, mean, na.rm=TRUE)
w &lt;- summarize(temperature, month, mean, na.rm=TRUE)
library(lattice)
xyplot(temperature ~ month, data=w) # plot mean temperature by month

w &lt;- summarize(temperature, llist(year,month), 
               quantile, probs=c(.5,.25,.75), na.rm=TRUE, type='matrix')
xYplot(Cbind(temperature[,1],temperature[,-1]) ~ month | year, data=w)
mApply(temperature, llist(year,month),
       quantile, probs=c(.5,.25,.75), na.rm=TRUE)

# Compute the median and outer quartiles.  The outer quartiles are
# displayed using "error bars"
set.seed(111)
dfr &lt;- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
attach(dfr)
y &lt;- abs(month-6.5) + 2*runif(length(month)) + year-1997
s &lt;- summarize(y, llist(month,year), smedian.hilow, conf.int=.5)
s
mApply(y, llist(month,year), smedian.hilow, conf.int=.5)

xYplot(Cbind(y,Lower,Upper) ~ month, groups=year, data=s, 
       keys='lines', method='alt')
# Can also do:
s &lt;- summarize(y, llist(month,year), quantile, probs=c(.5,.25,.75),
               stat.name=c('y','Q1','Q3'))
xYplot(Cbind(y, Q1, Q3) ~ month, groups=year, data=s, keys='lines')
# To display means and bootstrapped nonparametric confidence intervals
# use for example:
s &lt;- summarize(y, llist(month,year), smean.cl.boot)
xYplot(Cbind(y, Lower, Upper) ~ month | year, data=s)

# For each subject use the trapezoidal rule to compute the area under
# the (time,response) curve using the Hmisc trap.rule function
x &lt;- cbind(time=c(1,2,4,7, 1,3,5,10),response=c(1,3,2,4, 1,3,2,4))
subject &lt;- c(rep(1,4),rep(2,4))
trap.rule(x[1:4,1],x[1:4,2])
summarize(x, subject, function(y) trap.rule(y[,1],y[,2]))

## Not run: 
# Another approach would be to properly re-shape the mm array below
# This assumes no missing cells.  There are many other approaches.
# mApply will do this well while allowing for missing cells.
m &lt;- tapply(y, list(year,month), quantile, probs=c(.25,.5,.75))
mm &lt;- array(unlist(m), dim=c(3,2,12), 
            dimnames=list(c('lower','median','upper'),c('1997','1998'),
                          as.character(1:12)))
# aggregate will help but it only allows you to compute one quantile
# at a time; see also the Hmisc mApply function
dframe &lt;- aggregate(y, list(Year=year,Month=month), quantile, probs=.5)

# Compute expected life length by race assuming an exponential
# distribution - can also use summarize
g &lt;- function(y) { # computations for one race group
  futime &lt;- y[,1]; event &lt;- y[,2]
  sum(futime)/sum(event)  # assume event=1 for death, 0=alive
}
mApply(cbind(followup.time, death), race, g)

# To run mApply on a data frame:
xn &lt;- asNumericMatrix(x)
m &lt;- mApply(xn, race, h)
# Here assume h is a function that returns a matrix similar to x
matrix2dataFrame(m)


# Get stratified weighted means
g &lt;- function(y) wtd.mean(y[,1],y[,2])
summarize(cbind(y, wts), llist(sex,race), g, stat.name='y')
mApply(cbind(y,wts), llist(sex,race), g)

# Compare speed of mApply vs. by for computing 
d &lt;- data.frame(sex=sample(c('female','male'),100000,TRUE),
                country=sample(letters,100000,TRUE),
                y1=runif(100000), y2=runif(100000))
g &lt;- function(x) {
  y &lt;- c(median(x[,'y1']-x[,'y2']),
         med.sum =median(x[,'y1']+x[,'y2']))
  names(y) &lt;- c('med.diff','med.sum')
  y
}

system.time(by(d, llist(sex=d$sex,country=d$country), g))
system.time({
             x &lt;- asNumericMatrix(d)
             a &lt;- subsAttr(d)
             m &lt;- mApply(x, llist(sex=d$sex,country=d$country), g)
            })
system.time({
             x &lt;- asNumericMatrix(d)
             summarize(x, llist(sex=d$sex, country=d$country), g)
            })

# An example where each subject has one record per diagnosis but sex of
# subject is duplicated for all the rows a subject has.  Get the cross-
# classified frequencies of diagnosis (dx) by sex and plot the results
# with a dot plot

count &lt;- rep(1,length(dx))
d &lt;- summarize(count, llist(dx,sex), sum)
Dotplot(dx ~ count | sex, data=d)

## End(Not run)
d &lt;- list(x=1:10, a=factor(rep(c('a','b'), 5)),
          b=structure(letters[1:10], label='label for b'),
          d=c(rep(TRUE,9), FALSE), f=pi*(1 : 10))
x &lt;- asNumericMatrix(d)
attr(x, 'origAttributes')
matrix2dataFrame(x)

detach('dfr')

# Run summarize on a matrix to get column means
x &lt;- c(1:19,NA)
y &lt;- 101:120
z &lt;- cbind(x, y)
g &lt;- c(rep(1, 10), rep(2, 10))
summarize(z, g, colMeans, na.rm=TRUE, stat.name='x')
# Also works on an all numeric data frame
summarize(as.data.frame(z), g, colMeans, na.rm=TRUE, stat.name='x')
</code></pre>

<hr>
<h2 id='summary.formula'>Summarize Data for Making Tables and Plots</h2><span id='topic+summary.formula'></span><span id='topic+stratify'></span><span id='topic+print.summary.formula.response'></span><span id='topic+plot.summary.formula.response'></span><span id='topic+latex.summary.formula.response'></span><span id='topic+print.summary.formula.reverse'></span><span id='topic+plot.summary.formula.reverse'></span><span id='topic+latex.summary.formula.reverse'></span><span id='topic++5B.summary.formula.response'></span><span id='topic+print.summary.formula.cross'></span><span id='topic+latex.summary.formula.cross'></span><span id='topic+formula.summary.formula.cross'></span><span id='topic+na.retain'></span><span id='topic+cumcategory'></span><span id='topic+conTestkw'></span><span id='topic+catTestchisq'></span><span id='topic+ordTestpo'></span>

<h3>Description</h3>

<p><code>summary.formula</code> summarizes the variables listed in an S formula,
computing descriptive statistics (including ones in a
user-specified function).  The summary statistics may be passed to
<code>print</code> methods, <code>plot</code> methods for making annotated dot charts, and
<code>latex</code> methods for typesetting tables using LaTeX. 
<code>summary.formula</code> has three methods for computing descriptive
statistics on univariate or multivariate responses, subsetted by
categories of other variables.  The method of summarization is
specified in the parameter <code>method</code> (see details below).  For the
<code>response</code> and <code>cross</code> methods, the statistics used to
summarize the data 
may be specified in a very flexible way (e.g., the geometric mean,
33rd percentile, Kaplan-Meier 2-year survival estimate, mixtures of
several statistics).  The default summary statistic for these methods
is the mean (the proportion of positive responses for a binary
response variable).  The <code>cross</code> method is useful for creating data
frames which contain summary statistics that are passed to <code>trellis</code>
as raw data (to make multi-panel dot charts, for example).  The
<code>print</code> methods use the <code>print.char.matrix</code> function to print boxed
tables.
</p>
<p>The right hand side of <code>formula</code> may contain <code>mChoice</code>
(&ldquo;multiple choice&rdquo;) variables.  When <code>test=TRUE</code> each choice is
tested separately as a binary categorical response.
</p>
<p>The <code>plot</code> method for <code>method="reverse"</code> creates a temporary
function <code>Key</code> in frame 0 as is done by the <code>xYplot</code> and
<code>Ecdf.formula</code> functions.  After <code>plot</code> runs, you can type
<code>Key()</code> to put a legend in a default location, or
e.g. <code>Key(locator(1))</code> to draw a legend where you click the left
mouse button.  This key is for categorical variables, so to have the
opportunity to put the key on the graph you will probably want to use
the command <code>plot(object, which="categorical")</code>.  A second function
<code>Key2</code> is created if continuous variables are being plotted.  It is
used the same as <code>Key</code>.  If the <code>which</code> argument is not
specified to <code>plot</code>, two pages of plots will be produced.  If you
don't define <code>par(mfrow=)</code> yourself,
<code>plot.summary.formula.reverse</code> will try to lay out a multi-panel
graph to best fit all the individual dot charts for continuous
variables.
</p>
<p>There is a subscripting method for objects created with
<code>method="response"</code>. 
This can be used to print or plot selected variables or summary statistics
where there would otherwise be too many on one page.
</p>
<p><code>cumcategory</code> is a utility function useful when summarizing an ordinal
response variable.  It converts such a variable having <code>k</code> levels to a
matrix with <code>k-1</code> columns, where column <code>i</code> is a vector of zeros and
ones indicating that the categorical response is in level <code>i+1</code> or
greater.  When the left hand side of <code>formula</code> is <code>cumcategory(y)</code>,
the default <code>fun</code> will summarize it by computing all of the relevant
cumulative proportions.
</p>
<p>Functions <code>conTestkw</code>, <code>catTestchisq</code>, <code>ordTestpo</code> are
the default statistical test functions for <code>summary.formula</code>.
These defaults are: Wilcoxon-Kruskal-Wallis test for continuous
variables, Pearson chi-square test for categorical variables, and the
likelihood ratio chi-square test from the proportional odds model for
ordinal variables.  These three functions serve also as templates for
the user to create her own testing functions that are self-defining in
terms of how the results are printed or rendered in LaTeX, or plotted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'formula'
summary(formula, data=NULL, subset=NULL,
        na.action=NULL, fun = NULL,
        method = c("response", "reverse", "cross"),
        overall = method == "response" | method == "cross",
        continuous = 10, na.rm = TRUE, na.include = method != "reverse",
        g = 4, quant = c(0.025, 0.05, 0.125, 0.25, 0.375, 0.5, 0.625,
                         0.75, 0.875, 0.95, 0.975),
        nmin = if (method == "reverse") 100
               else 0,
        test = FALSE, conTest = conTestkw, catTest = catTestchisq,
        ordTest = ordTestpo, ...)

## S3 method for class 'summary.formula.response'
x[i, j, drop=FALSE]

## S3 method for class 'summary.formula.response'
print(x, vnames=c('labels','names'), prUnits=TRUE,
      abbreviate.dimnames=FALSE,
      prefix.width, min.colwidth, formatArgs=NULL, markdown=FALSE, ...)

## S3 method for class 'summary.formula.response'
plot(x, which = 1, vnames = c('labels','names'), xlim, xlab,
     pch = c(16, 1, 2, 17, 15, 3, 4, 5, 0), superposeStrata = TRUE,
     dotfont = 1, add = FALSE, reset.par = TRUE, main, subtitles = TRUE,
     ...)

## S3 method for class 'summary.formula.response'
latex(object, title = first.word(deparse(substitute(object))), caption,
      trios, vnames = c('labels', 'names'), prn = TRUE, prUnits = TRUE,
      rowlabel = '', cdec = 2, ncaption = TRUE, ...)

## S3 method for class 'summary.formula.reverse'
print(x, digits, prn = any(n != N), pctdig = 0,
      what=c('%', 'proportion'),
      npct = c('numerator', 'both', 'denominator', 'none'),
      exclude1 = TRUE, vnames = c('labels', 'names'), prUnits = TRUE,
      sep = '/', abbreviate.dimnames = FALSE,
      prefix.width = max(nchar(lab)), min.colwidth, formatArgs=NULL, round=NULL,
      prtest = c('P','stat','df','name'), prmsd = FALSE, long = FALSE,
      pdig = 3, eps = 0.001, ...)

## S3 method for class 'summary.formula.reverse'
plot(x, vnames = c('labels', 'names'), what = c('proportion', '%'),
     which = c('both', 'categorical', 'continuous'),
     xlim = if(what == 'proportion') c(0,1)
            else c(0,100), 
     xlab = if(what=='proportion') 'Proportion'
            else 'Percentage', 
     pch = c(16, 1, 2, 17, 15, 3, 4, 5, 0), exclude1 = TRUE,
     dotfont = 1, main,
     prtest = c('P', 'stat', 'df', 'name'), pdig = 3, eps = 0.001,
     conType = c('dot', 'bp', 'raw'), cex.means = 0.5, ...)

## S3 method for class 'summary.formula.reverse'
latex(object, title = first.word(deparse(substitute(object))), digits,
      prn = any(n != N), pctdig = 0, what=c('%', 'proportion'),
      npct = c("numerator", "both", "denominator", "slash", "none"),
      npct.size = 'scriptsize', Nsize = "scriptsize", exclude1 = TRUE,
      vnames=c("labels", "names"), prUnits = TRUE, middle.bold = FALSE,
      outer.size = "scriptsize", caption, rowlabel = "",
      insert.bottom = TRUE, dcolumn = FALSE, formatArgs=NULL, round = NULL,
      prtest = c('P', 'stat', 'df', 'name'), prmsd = FALSE,
      msdsize = NULL, long = dotchart, pdig = 3, eps = 0.001,
      auxCol = NULL, dotchart=FALSE, ...)

## S3 method for class 'summary.formula.cross'
print(x, twoway = nvar == 2, prnmiss = any(stats$Missing &gt; 0), prn = TRUE,
      abbreviate.dimnames = FALSE, prefix.width = max(nchar(v)),
      min.colwidth, formatArgs = NULL, ...)

## S3 method for class 'summary.formula.cross'
latex(object, title = first.word(deparse(substitute(object))),
      twoway = nvar == 2, prnmiss = TRUE, prn = TRUE,
      caption=attr(object, "heading"), vnames=c("labels", "names"),
      rowlabel="", ...)

stratify(..., na.group = FALSE, shortlabel = TRUE)

## S3 method for class 'summary.formula.cross'
formula(x, ...)

cumcategory(y)

conTestkw(group, x)
catTestchisq(tab)
ordTestpo(group, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.formula_+3A_formula">formula</code></td>
<td>

<p>An <span class="rlang"><b>R</b></span> formula with additive effects.  For <code>method="response"</code> or
<code>"cross"</code>, the dependent variable has the usual connotation.  For
<code>method="reverse"</code>, the dependent variable is what is usually thought
of as an independent variable, and it is one that is used to stratify
all of the right hand side variables.  For <code>method="response"</code>
(only), the <code>formula</code> may contain one or more invocations of the
<code>stratify</code> function whose arguments are defined below.  This causes
the entire analysis to be stratified by cross-classifications of the
combined list of stratification factors.  This stratification will be
reflected as major column groupings in the resulting table, or as more
response columns for plotting.  If <code>formula</code> has no dependent variable
<code>method="reverse"</code> is the only legal value and so <code>method</code> defaults to
<code>"reverse"</code> in this case.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_x">x</code></td>
<td>
<p>an object created by <code>summary.formula</code>.  For
<code>conTestkw</code> a numeric vector, and for <code>ordTestpo</code>, a numeric
or factor variable that can be considered ordered</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_y">y</code></td>
<td>

<p>a numeric, character, category, or factor vector for <code>cumcategory</code>.
Is converted to a categorical variable is needed.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_drop">drop</code></td>
<td>

<p>logical. If <code>TRUE</code> the result is coerced to the
lowest possible dimension.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_data">data</code></td>
<td>

<p>name or number of a data frame.  Default is the current frame.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_subset">subset</code></td>
<td>

<p>a logical vector or integer vector of subscripts used to specify the
subset of data to use in the analysis.  The default is to use all
observations in the data frame.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_na.action">na.action</code></td>
<td>

<p>function for handling missing data in the input data.  The default is
a function defined here called <code>na.retain</code>, which keeps all
observations for processing, with missing variables or not.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_fun">fun</code></td>
<td>

<p>function for summarizing data in each cell.  Default is to take the
mean of each column of the possibly multivariate response variable.
You can specify <code>fun="%"</code> to compute percentages (100 times the mean of a 
series of logical or binary variables).
User&ndash;specified functions can also return a matrix.  For example, you might 
compute quartiles on a bivariate response.  Does not apply to
<code>method="reverse"</code>. 
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_method">method</code></td>
<td>

<p>The default is <code>"response"</code>, in which case the response variable may
be multivariate and any number of statistics may be used to summarize
them.  Here the responses are summarized separately for each of any
number of independent variables.  Continuous independent variables
(see the <code>continuous</code> parameter below) are automatically stratified
into <code>g</code> (see below) quantile groups (if you want to control the
discretization for selected variables, use the <code>cut2</code> function on them).  
Otherwise, the data are
subsetted by all levels of discrete right hand side variables.  For
multivariate responses, subjects are considered to be missing if any
of the columns is missing.  
</p>
<p>The <code>method="reverse"</code> option is
typically used to make baseline characteristic tables, for example.
The single left hand side variable must be categorical (e.g.,
treatment), and the right hand side variables are broken down one at a
time by the &quot;dependent&quot; variable.  Continuous variables are described
by three quantiles (quartiles by default) along with 
outer quantiles (used only for scaling x-axes when plotting quartiles;
all are used when plotting box-percentile plots), and
categorical ones are
described by counts and percentages.  If there is no left hand side
variable, <code>summary</code> assumes that there is only one group in the data,
so that only one column of summaries will appear.
If there is no dependent variable in <code>formula</code>, <code>method</code> defaults to
<code>"reverse"</code> automatically.
</p>
<p>The <code>method="cross"</code> option allows for a multivariate dependent
variable and for up to three independents.  Continuous independent
variables (those with at least <code>continuous</code> unique values) are
automatically divided into <code>g</code> quantile groups.
The independents are cross-classified, and marginal statistics may optionally be computed.
The output of <code>summary.formula</code> in this case is a data frame
containing the independent variable combinations (with levels of
<code>"All"</code> corresponding to marginals) and the corresponding summary
statistics in the matrix <code>S</code>.  The output data frame is suitable for
direct use in <code>trellis</code>.  The <code>print</code> and <code>latex</code> typesetting methods for this
method allows for a special two-way format if there are two right
hand variables.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_overall">overall</code></td>
<td>

<p>For <code>method="reverse"</code>, setting <code>overall=TRUE</code> makes a new column with
overall statistics for the whole sample.  For <code>method="cross"</code>,
<code>overall=TRUE</code> (the default) results in all marginal statistics being
computed.  For <code>trellis</code> displays (usually multi-panel dot plots), 
these marginals just form other categories.  For <code>"response"</code>, the
default is <code>overall=TRUE</code>, causing a final row of global summary
statistics to appear in tables and dot charts.  If <code>test=TRUE</code> these
marginal statistics are ignored in doing statistical tests.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_continuous">continuous</code></td>
<td>

<p>specifies the threshold for when a variable is considered to be
continuous (when there are at least <code>continuous</code> unique values).
<code>factor</code> variables are always considered to be categorical no matter
how many levels they have.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_na.rm">na.rm</code></td>
<td>

<p><code>TRUE</code> (the default) to exclude <code>NA</code>s before passing data to
<code>fun</code> to compute statistics, <code>FALSE</code> otherwise.
<code>na.rm=FALSE</code> is useful if the response variable is a matrix and
you do not wish to exclude a row of the matrix if any of the columns
in that row are <code>NA</code>.  <code>na.rm</code> also applies to summary
statistic functions such as <code>smean.cl.normal</code>.  For these <code>na.rm</code>
defaults to <code>TRUE</code> unlike built-in functions.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_na.include">na.include</code></td>
<td>

<p>for <code>method="response"</code>, set <code>na.include=FALSE</code> to exclude missing values from
being counted as their own category when subsetting the response(s)
by levels of a categorical variable.  For <code>method="reverse"</code> set
<code>na.include=TRUE</code> to keep missing values of categorical variables from
being excluded from the table.  
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_g">g</code></td>
<td>

<p>number of quantile groups to use when variables are automatically
categorized with <code>method="response"</code> or <code>"cross"</code> using <code>cut2</code>
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_nmin">nmin</code></td>
<td>

<p>if fewer than <code>nmin</code> observations exist in a category for <code>"response"</code>
(over all strata combined), that category will be ignored.  For
<code>"reverse"</code>, for categories of the response variable in which there
are less than or equal to <code>nmin</code> non-missing observations, the raw
data are retained for later plotting in place of box plots.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_test">test</code></td>
<td>

<p>applies if <code>method="reverse"</code>.  Set to <code>TRUE</code> to compute test
statistics using tests specified in <code>conTest</code> and <code>catTest</code>.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_contest">conTest</code></td>
<td>

<p>a function of two arguments (grouping variable and a continuous
variable) that returns a list with components <code>P</code> (the computed
P-value), <code>stat</code> (the test statistic, either chi-square or F),
<code>df</code> (degrees of freedom), <code>testname</code> (test name), <code>statname</code>
(statistic name), <code>namefun</code> (<code>"chisq", "fstat"</code>), an
optional component <code>latexstat</code> (LaTeX 
representation of <code>statname</code>), an optional component
<code>plotmathstat</code> (for R - the <code>plotmath</code> representation of
<code>statname</code>, as a character string),  and an
optional component <code>note</code> 
that contains a character string note about the test (e.g.,
<code>"test not done because n &lt; 5"</code>).  <code>conTest</code> is applied to continuous variables
on the right-hand-side of the formula when <code>method="reverse"</code>.  The
default uses the <code>spearman2</code> function to run the Wilcoxon or
Kruskal-Wallis test using the F distribution.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_cattest">catTest</code></td>
<td>

<p>a function of a frequency table (an integer matrix) that returns a
list with the same components as created by <code>conTest</code>.  By default,
the Pearson chi-square test is done, without continuity correction
(the continuity correction would make the test conservative like the
Fisher exact test).
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_ordtest">ordTest</code></td>
<td>

<p>a function of a frequency table (an integer matrix) that returns a
list with the same components as created by <code>conTest</code>.  By default,
the Proportional odds likelihood ratio test is done.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_...">...</code></td>
<td>

<p>for <code>summary.formula</code> these are optional
arguments for <code>cut2</code> when variables are automatically categorized.
For <code>plot</code> methods these arguments are passed to <code>dotchart2</code>.
For <code>Key</code> and <code>Key2</code> these arguments are passed to <code>key</code>,
<code>text</code>, or <code>mtitle</code>.  For <code>print</code> methods these are
optional arguments to <code>print.char.matrix</code>. For <code>latex</code> methods
these are passed to <code>latex.default</code>.  One of the most important of
these is <code>file</code>.  Specifying <code>file=""</code> will cause LaTeX code
to just be printed to standard output rather than be stored in a
permanent file.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_object">object</code></td>
<td>
<p>an object created by <code>summary.formula</code></p>
</td></tr>
<tr><td><code id="summary.formula_+3A_quant">quant</code></td>
<td>

<p>vector of quantiles to use for summarizing data with
<code>method="reverse"</code>.  This must be numbers between 0 and 1
inclusive and must include the numbers 0.5, 0.25, and 0.75 which are
used for printing and for plotting 
quantile intervals.  The outer quantiles are used for scaling the x-axes
for such plots.  Specify outer quantiles as <code>0</code> and <code>1</code> to
scale the x-axes using the whole observed data ranges instead of the
default (a 0.95 quantile interval).  Box-percentile plots are drawn
using all but the outer quantiles.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_vnames">vnames</code></td>
<td>

<p>By default, tables and plots are usually labeled with variable labels
(see the <code>label</code> and <code>sas.get</code> functions).  To use the shorter
variable names, specify <code>vnames="name"</code>.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_pch">pch</code></td>
<td>

<p>vector of plotting characters to represent different groups, in order
of group levels.  For <code>method="response"</code> the characters
correspond to levels of the <code>stratify</code> variable if
<code>superposeStrata=TRUE</code>, and if no 
<code>strata</code> are used or if <code>superposeStrata=FALSE</code>, the 
<code>pch</code> vector corresponds to the <code>which</code> argument for
<code>method="response"</code>.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_superposestrata">superposeStrata</code></td>
<td>

<p>If <code>stratify</code> was used, set <code>superposeStrata=FALSE</code> to make
separate dot charts for each level of the <code>stratification</code>
variable, for <code>method='response'</code>.  The default is to
superposition all strata on one dot chart.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_dotfont">dotfont</code></td>
<td>
<p>font for plotting points</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_reset.par">reset.par</code></td>
<td>
<p>set to <code>FALSE</code> to suppress the restoring of the
old par values in <code>plot.summary.formula.response</code>
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_abbreviate.dimnames">abbreviate.dimnames</code></td>
<td>
<p>see <code>print.char.matrix</code></p>
</td></tr>
<tr><td><code id="summary.formula_+3A_prefix.width">prefix.width</code></td>
<td>
<p>see <code>print.char.matrix</code></p>
</td></tr>
<tr><td><code id="summary.formula_+3A_min.colwidth">min.colwidth</code></td>
<td>

<p>minimum column width to use for boxes printed with <code>print.char.matrix</code>.
The default is the maximum of the minimum column label length and the minimum
length of entries in the data cells.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_formatargs">formatArgs</code></td>
<td>

<p>a list containing other arguments to pass to <code>format.default</code> such as
<code>scientific</code>, e.g., <code>formatArgs=list(scientific=c(-5,5))</code>.  For
<code>print.summary.formula.reverse</code> and
<code>format.summary.formula.reverse</code>, <code>formatArgs</code> applies only to
statistics computed on continuous variables, not to percents,
numerators, and denominators.  The <code>round</code> argument may be preferred.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_markdown">markdown</code></td>
<td>
<p>for <code>print.summary.formula.response</code> set to
<code>TRUE</code> to use <code>knitr::kable</code> to produce the table in
markdown format rather than using raw text output created by <code>print.char.matrix</code></p>
</td></tr>
<tr><td><code id="summary.formula_+3A_digits">digits</code></td>
<td>

<p>number of significant digits to print.  Default is to use the current
value of the <code>digits</code> system option.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_prn">prn</code></td>
<td>

<p>set to <code>TRUE</code> to print the number of non-missing observations on the
current (row) variable.  The default is to print these only if any of
the counts of non-missing values differs from the total number of
non-missing values of the left-hand-side variable.
For <code>method="cross"</code> the default is to always print <code>N</code>.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_prnmiss">prnmiss</code></td>
<td>

<p>set to <code>FALSE</code> to suppress printing counts of missing values for <code>"cross"</code>
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_what">what</code></td>
<td>

<p>for <code>method="reverse"</code> specifies whether proportions or percentages
are to be plotted
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_pctdig">pctdig</code></td>
<td>

<p>number of digits to the right of the decimal place for printing
percentages. The default is zero, so percents will be rounded to the
nearest percent.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_npct">npct</code></td>
<td>

<p>specifies which counts are to be printed to the right of percentages.
The default is to print the frequency (numerator of the percent) in
parentheses.  You can specify <code>"both"</code> to print both numerator and
denominator, <code>"denominator"</code>, <code>"slash"</code> to
typeset horizontally using a forward slash, or <code>"none"</code>.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_npct.size">npct.size</code></td>
<td>

<p>the size for typesetting <code>npct</code> information which appears after percents.
The default is <code>"scriptsize"</code>.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_nsize">Nsize</code></td>
<td>

<p>When a second row of column headings is added showing sample sizes,
<code>Nsize</code> specifies the LaTeX size for these subheadings.  Default
is <code>"scriptsize"</code>.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_exclude1">exclude1</code></td>
<td>

<p>by default, <code>method="reverse"</code> objects will be printed, plotted,  or typeset by
removing redundant entries from percentage tables for categorical
variables.  For example, if you print the percent of females, you
don't need to print the percent of males.  To override this, set <code>exclude1=FALSE</code>.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_prunits">prUnits</code></td>
<td>

<p>set to <code>FALSE</code> to suppress printing or latexing <code>units</code>
attributes of variables, when <code>method='reverse'</code> or <code>'response'</code>
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_sep">sep</code></td>
<td>

<p>character to use to separate quantiles when printing
<code>method="reverse"</code> tables
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_prtest">prtest</code></td>
<td>

<p>a vector of test statistic components to print if <code>test=TRUE</code> was in
effect when <code>summary.formula</code> was called.  Defaults to printing all
components.  Specify <code>prtest=FALSE</code> or <code>prtest="none"</code> to not
print any tests.  This applies to <code>print</code>, <code>latex</code>, and
<code>plot</code> methods for <code>method='reverse'</code>.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_round">round</code></td>
<td>

<p>for <code>print.summary.formula.reverse</code> and
<code>latex.summary.formula.reverse</code> specify <code>round</code> to round
the quantiles and optional mean and standard deviation to
<code>round</code> digits after the decimal point
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_prmsd">prmsd</code></td>
<td>

<p>set to <code>TRUE</code> to print mean and SD after the three quantiles, for
continuous variables with <code>method="reverse"</code>
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_msdsize">msdsize</code></td>
<td>

<p>defaults to <code>NULL</code> to use the current font size for the mean and
standard deviation if <code>prmsd</code> is <code>TRUE</code>.  Set to a character
string to specify an alternate LaTeX font size.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_long">long</code></td>
<td>

<p>set to <code>TRUE</code> to print the results for the first category on its own
line, not on the same line with the variable label (for
<code>method="reverse"</code> with <code>print</code> and <code>latex</code> methods)
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_pdig">pdig</code></td>
<td>

<p>number of digits to the right of the decimal place for printing
P-values.  Default is <code>3</code>.  This is passed to <code>format.pval</code>.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_eps">eps</code></td>
<td>

<p>P-values less than <code>eps</code> will be printed as <code>&lt; eps</code>.  See
<code>format.pval</code>.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_auxcol">auxCol</code></td>
<td>

<p>an optional auxiliary column of information, right justified, to add
in front of statistics typeset by
<code>latex.summary.formula.reverse</code>.  This argument is a list with a
single element that has a name specifying the column heading.  If this
name includes a newline character, the portions of the string before
and after the newline form respectively the main heading and the
subheading (typically set in smaller font), respectively.  See the
<code>extracolheads</code> argument to <code>latex.default</code>.  <code>auxCol</code>
is filled with blanks when a variable being summarized takes up more
than one row in the output.  This happens with categorical variables.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_twoway">twoway</code></td>
<td>

<p>for <code>method="cross"</code> with two right hand side variables, <code>twoway</code>
controls whether the resulting table will be printed in enumeration
format or as a two-way table (the default)
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_which">which</code></td>
<td>

<p>For <code>method="response"</code> specifies the sequential number or a vector of
subscripts of statistics to plot.  If you had any <code>stratify</code>
variables, these are counted as if more statistics were computed.
For <code>method="reverse"</code> specifies whether to plot results 
for categorical variables, continuous variables, or both (the default).
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_contype">conType</code></td>
<td>

<p>For plotting <code>method="reverse"</code> plots for continuous variables,
dot plots showing quartiles are drawn by default.  Specify
<code>conType='bp'</code> to draw box-percentile plots using all the
quantiles in <code>quant</code> except the outermost ones.  Means are drawn
with a solid dot and vertical reference lines are placed at the three
quartiles.  Specify <code>conType='raw'</code> to make a strip chart showing
the raw data.  This can only be used if the sample size for each
left-hand-side group is less than or equal to <code>nmin</code>.</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_cex.means">cex.means</code></td>
<td>

<p>character size for means in box-percentile plots; default is .5</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_xlim">xlim</code></td>
<td>

<p>vector of length two specifying x-axis limits.  For
<code>method="reverse"</code>, this is only used for plotting categorical
variables.  Limits for continuous variables are determined by the
outer quantiles specified in <code>quant</code>.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_xlab">xlab</code></td>
<td>

<p>x-axis label
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_add">add</code></td>
<td>

<p>set to <code>TRUE</code> to add to an existing plot
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_main">main</code></td>
<td>

<p>a main title.  For <code>method="reverse"</code> this applies only to the plot
for categorical variables.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_subtitles">subtitles</code></td>
<td>

<p>set to <code>FALSE</code> to suppress automatic subtitles
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_caption">caption</code></td>
<td>

<p>character string containing LaTeX table captions.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_title">title</code></td>
<td>

<p>name of resulting LaTeX file omitting the <code>.tex</code> suffix.  Default
is the name of the <code>summary</code> object.  If <code>caption</code> is specied,
<code>title</code> is also used for the table's symbolic reference label. 
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_trios">trios</code></td>
<td>

<p>If for <code>method="response"</code> you summarized the response(s) by using
three quantiles, specify <code>trios=TRUE</code> or <code>trios=v</code> to group each set of
three statistics into one column for <code>latex</code> output, using the format
a B c, where the outer quantiles are in smaller font
(<code>scriptsize</code>).  For <code>trios=TRUE</code>, the overall column names are taken
from the column names of the original data matrix.  To give new
column names, specify <code>trios=v</code>, where <code>v</code> is a vector of column
names, of length <code>m/3</code>, where <code>m</code> is the original number of columns
of summary statistics.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_rowlabel">rowlabel</code></td>
<td>

<p>see <code>latex.default</code> (under the help file <code>latex</code>)
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_cdec">cdec</code></td>
<td>

<p>number of decimal places to the right of the decimal point for
<code>latex</code>.  This value should be a scalar (which will be properly
replicated), or a vector with length equal to the number of columns
in the table.  For <code>"response"</code> tables, this length does not count
the column for <code>N</code>.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_ncaption">ncaption</code></td>
<td>

<p>set to <code>FALSE</code> to not have <code>latex.summary.formula.response</code>
put sample sizes in captions
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_i">i</code></td>
<td>

<p>a vector of integers, or character strings containing variable names
to subset on.  Note that each row subsetted on in an <code>summary.formula.reverse</code>
object subsets on all the levels that make up the corresponding variable
(automatically).
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_j">j</code></td>
<td>

<p>a vector of integers representing column numbers
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_middle.bold">middle.bold</code></td>
<td>

<p>set to <code>TRUE</code> to have LaTeX use bold face for the middle quantile for
<code>method="reverse"</code> 
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_outer.size">outer.size</code></td>
<td>

<p>the font size for outer quantiles for <code>"reverse"</code> tables
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_insert.bottom">insert.bottom</code></td>
<td>

<p>set to <code>FALSE</code> to suppress inclusion of definitions placed at the
bottom of LaTeX tables for <code>method="reverse"</code>
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_dcolumn">dcolumn</code></td>
<td>

<p>see <code>latex</code>
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_na.group">na.group</code></td>
<td>

<p>set to <code>TRUE</code> to have missing stratification variables given their own
category (<code>NA</code>)
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_shortlabel">shortlabel</code></td>
<td>

<p>set to <code>FALSE</code> to include stratification variable names and equal signs
in labels for strata levels
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_dotchart">dotchart</code></td>
<td>

<p>set to <code>TRUE</code> to output a dotchart in the latex table being
generated.
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_group">group</code></td>
<td>
<p>for <code>conTest</code> and <code>ordTest</code>, a numeric or
factor variable with length the same as <code>x</code>
</p>
</td></tr>
<tr><td><code id="summary.formula_+3A_tab">tab</code></td>
<td>
<p>for <code>catTest</code>, a frequency table such as that created
by <code>table()</code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>summary.formula</code> returns a data frame or list depending on
<code>method</code>.  <code>plot.summary.formula.reverse</code> returns the number
of pages of plots that were made.
</p>


<h3>Side Effects</h3>

<p><code>plot.summary.formula.reverse</code> creates a function <code>Key</code> and
<code>Key2</code> in frame 0 that will draw legends.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Harrell FE (2007): Statistical tables and plots using S and LaTeX.
Document available from <a href="https://hbiostat.org/R/Hmisc/summary.pdf">https://hbiostat.org/R/Hmisc/summary.pdf</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mChoice">mChoice</a></code>, <code><a href="#topic+smean.sd">smean.sd</a></code>, <code><a href="#topic+summarize">summarize</a></code>,
<code><a href="#topic+label">label</a></code>, <code><a href="survival.html#topic+strata">strata</a></code>, <code><a href="#topic+dotchart2">dotchart2</a></code>,
<code><a href="#topic+print.char.matrix">print.char.matrix</a></code>, <code><a href="stats.html#topic+update">update</a></code>,
<code><a href="stats.html#topic+formula">formula</a></code>, <code><a href="#topic+cut2">cut2</a></code>, <code><a href="#topic+llist">llist</a></code>,
<code><a href="base.html#topic+format.default">format.default</a></code>, <code><a href="#topic+latex">latex</a></code>,
<code><a href="#topic+latexTranslate">latexTranslate</a></code> <code><a href="#topic+bpplt">bpplt</a></code>,
<code><a href="#topic+summaryM">summaryM</a></code>, <code><a href="base.html#topic+summary">summary</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>options(digits=3)
set.seed(173)
sex &lt;- factor(sample(c("m","f"), 500, rep=TRUE))
age &lt;- rnorm(500, 50, 5)
treatment &lt;- factor(sample(c("Drug","Placebo"), 500, rep=TRUE))

# Generate a 3-choice variable; each of 3 variables has 5 possible levels
symp &lt;- c('Headache','Stomach Ache','Hangnail',
          'Muscle Ache','Depressed')
symptom1 &lt;- sample(symp, 500,TRUE)
symptom2 &lt;- sample(symp, 500,TRUE)
symptom3 &lt;- sample(symp, 500,TRUE)
Symptoms &lt;- mChoice(symptom1, symptom2, symptom3, label='Primary Symptoms')
table(Symptoms)

# Note: In this example, some subjects have the same symptom checked
# multiple times; in practice these redundant selections would be NAs
# mChoice will ignore these redundant selections

#Frequency table sex*treatment, sex*Symptoms
summary(sex ~ treatment + Symptoms, fun=table)
# could also do summary(sex ~ treatment +
#  mChoice(symptom1,symptom2,symptom3), fun=table)


#Compute mean age, separately by 3 variables
summary(age ~ sex + treatment + Symptoms)


f &lt;- summary(treatment ~ age + sex + Symptoms, method="reverse", test=TRUE)
f
# trio of numbers represent 25th, 50th, 75th percentile
print(f, long=TRUE)
plot(f)
plot(f, conType='bp', prtest='P')
bpplt()    # annotated example showing layout of bp plot

#Compute predicted probability from a logistic regression model
#For different stratifications compute receiver operating
#characteristic curve areas (C-indexes)
predicted &lt;- plogis(.4*(sex=="m")+.15*(age-50))
positive.diagnosis &lt;- ifelse(runif(500)&lt;=predicted, 1, 0)
roc &lt;- function(z) {
   x &lt;- z[,1];
   y &lt;- z[,2];
   n &lt;- length(x);
   if(n&lt;2)return(c(ROC=NA));
   n1 &lt;- sum(y==1);
   c(ROC= (mean(rank(x)[y==1])-(n1+1)/2)/(n-n1) );
 }
y &lt;- cbind(predicted, positive.diagnosis)
options(digits=2)
summary(y ~ age + sex, fun=roc)


options(digits=3)
summary(y ~ age + sex, fun=roc, method="cross")

#Use stratify() to produce a table in which time intervals go down the
#page and going across 3 continuous variables are summarized using
#quartiles, and are stratified by two treatments

set.seed(1)
d &lt;- expand.grid(visit=1:5, treat=c('A','B'), reps=1:100)
d$sysbp &lt;- rnorm(100*5*2, 120, 10)
label(d$sysbp) &lt;- 'Systolic BP'
d$diasbp &lt;- rnorm(100*5*2, 80,  7)
d$diasbp[1] &lt;- NA
d$age    &lt;- rnorm(100*5*2, 50, 12)
g &lt;- function(y) {
  N &lt;- apply(y, 2, function(w) sum(!is.na(w)))
  h &lt;- function(x) {
    qu &lt;- quantile(x, c(.25,.5,.75), na.rm=TRUE)
    names(qu) &lt;- c('Q1','Q2','Q3')
    c(N=sum(!is.na(x)), qu)
}
  w &lt;- as.vector(apply(y, 2, h))
  names(w) &lt;- as.vector( outer(c('N','Q1','Q2','Q3'), dimnames(y)[[2]],
                                function(x,y) paste(y,x)))
  w
}
#Use na.rm=FALSE to count NAs separately by column
s &lt;- summary(cbind(age,sysbp,diasbp) ~ visit + stratify(treat),
             na.rm=FALSE, fun=g, data=d)
#The result is very wide.  Re-do, putting treatment vertically
x &lt;- with(d, factor(paste('Visit', visit, treat)))
summary(cbind(age,sysbp,diasbp) ~ x, na.rm=FALSE, fun=g, data=d)

#Compose LaTeX code directly
g &lt;- function(y) {
  h &lt;- function(x) {
    qu &lt;- format(round(quantile(x, c(.25,.5,.75), na.rm=TRUE),1),nsmall=1)
    paste('{\\scriptsize(',sum(!is.na(x)),
          ')} \\hfill{\\scriptsize ', qu[1], '} \\textbf{', qu[2],
          '} {\\scriptsize ', qu[3],'}', sep='')
  }
  apply(y, 2, h)
}
s &lt;- summary(cbind(age,sysbp,diasbp) ~ visit + stratify(treat),
             na.rm=FALSE, fun=g, data=d)
# latex(s, prn=FALSE)
## need option in latex to not print n
#Put treatment vertically
s &lt;- summary(cbind(age,sysbp,diasbp) ~ x, fun=g, data=d, na.rm=FALSE)
# latex(s, prn=FALSE)

#Plot estimated mean life length (assuming an exponential distribution) 
#separately by levels of 4 other variables.  Repeat the analysis
#by levels of a stratification variable, drug.  Automatically break
#continuous variables into tertiles.
#We are using the default, method='response'
## Not run: 
life.expect &lt;- function(y) c(Years=sum(y[,1])/sum(y[,2]))
attach(pbc)
require(survival)
S &lt;- Surv(follow.up.time, death)
s2 &lt;- summary(S ~ age + albumin + ascites + edema + stratify(drug),
                         fun=life.expect, g=3)


#Note: You can summarize other response variables using the same 
#independent variables using e.g. update(s2, response~.), or you 
#can change the list of independent variables using e.g. 
#update(s2, response ~.- ascites) or update(s2, .~.-ascites)
#You can also print, typeset, or plot subsets of s2, e.g.
#plot(s2[c('age','albumin'),]) or plot(s2[1:2,])


s2    # invokes print.summary.formula.response


#Plot results as a separate dot chart for each of the 3 strata levels
par(mfrow=c(2,2))
plot(s2, cex.labels=.6, xlim=c(0,40), superposeStrata=FALSE)


#Typeset table, creating s2.tex
w &lt;- latex(s2, cdec=1)
#Typeset table but just print LaTeX code
latex(s2, file="")    # useful for Sweave


#Take control of groups used for age.  Compute 3 quartiles for
#both cholesterol and bilirubin (excluding observations that are missing
#on EITHER ONE)


age.groups &lt;- cut2(age, c(45,60))
g &lt;- function(y) apply(y, 2, quantile, c(.25,.5,.75))
y &lt;- cbind(Chol=chol,Bili=bili)
label(y) &lt;- 'Cholesterol and Bilirubin'
#You can give new column names that are not legal S names
#by enclosing them in quotes, e.g. 'Chol (mg/dl)'=chol


s &lt;- summary(y ~ age.groups + ascites, fun=g)


par(mfrow=c(1,2), oma=c(3,0,3,0))   # allow outer margins for overall
for(ivar in 1:2) {                  # title 
  isub &lt;- (1:3)+(ivar-1)*3          # *3=number of quantiles/var.
  plot(s3, which=isub, main='', 
       xlab=c('Cholesterol','Bilirubin')[ivar],
       pch=c(91,16,93))            # [, closed circle, ]
  }
mtext(paste('Quartiles of', label(y)), adj=.5, outer=TRUE, cex=1.75)  
#Overall (outer) title


prlatex(latex(s3, trios=TRUE)) 
# trios -&gt; collapse 3 quartiles


#Summarize only bilirubin, but do it with two statistics:
#the mean and the median.  Make separate tables for the two randomized
#groups and make plots for the active arm.


g &lt;- function(y) c(Mean=mean(y), Median=median(y))


for(sub in c("D-penicillamine", "placebo")) {
  ss &lt;- summary(bili ~ age.groups + ascites + chol, fun=g,
                subset=drug==sub)
  cat('\n',sub,'\n\n')
  print(ss)


  if(sub=='D-penicillamine') {
    par(mfrow=c(1,1))
    plot(s4, which=1:2, dotfont=c(1,-1), subtitles=FALSE, main='')
    #1=mean, 2=median     -1 font = open circle
    title(sub='Closed circle: mean;  Open circle: median', adj=0)
    title(sub=sub, adj=1)
  }


  w &lt;- latex(ss, append=TRUE, fi='my.tex', 
             label=if(sub=='placebo') 's4b' else 's4a',
             caption=paste(label(bili),' {\\em (',sub,')}', sep=''))
  #Note symbolic labels for tables for two subsets: s4a, s4b
  prlatex(w)
}


#Now consider examples in 'reverse' format, where the lone dependent
#variable tells the summary function how to stratify all the 
#'independent' variables.  This is typically used to make tables 
#comparing baseline variables by treatment group, for example.


s5 &lt;- summary(drug ~ bili + albumin + stage + protime + sex + 
                     age + spiders,
              method='reverse')
#To summarize all variables, use summary(drug ~., data=pbc)
#To summarize all variables with no stratification, use
#summary(~a+b+c) or summary(~.,data=\dots)


options(digits=1)
print(s5, npct='both')
#npct='both' : print both numerators and denominators
plot(s5, which='categorical')
Key(locator(1))  # draw legend at mouse click
par(oma=c(3,0,0,0))  # leave outer margin at bottom
plot(s5, which='continuous')
Key2()           # draw legend at lower left corner of plot
                 # oma= above makes this default key fit the page better


options(digits=3)
w &lt;- latex(s5, npct='both', here=TRUE)     
# creates s5.tex


#Turn to a different dataset and do cross-classifications on possibly 
#more than one independent variable.  The summary function with 
#method='cross' produces a data frame containing the cross-
#classifications.  This data frame is suitable for multi-panel 
#trellis displays, although `summarize' works better for that.


attach(prostate)
size.quartile &lt;- cut2(sz, g=4)
bone &lt;- factor(bm,labels=c("no mets","bone mets"))


s7 &lt;- summary(ap&gt;1 ~ size.quartile + bone, method='cross')
#In this case, quartiles are the default so could have said sz + bone


options(digits=3)
print(s7, twoway=FALSE)
s7   # same as print(s7)
w &lt;- latex(s7, here=TRUE)   # Make s7.tex


library(trellis,TRUE)
invisible(ps.options(reset=TRUE))
trellis.device(postscript, file='demo2.ps')


dotplot(S ~ size.quartile|bone, data=s7, #s7 is name of summary stats
                  xlab="Fraction ap&gt;1", ylab="Quartile of Tumor Size")
#Can do this more quickly with summarize:
# s7 &lt;- summarize(ap&gt;1, llist(size=cut2(sz, g=4), bone), mean,
#                 stat.name='Proportion')
# dotplot(Proportion ~ size | bone, data=s7)


summary(age ~ stage, method='cross')
summary(age ~ stage, fun=quantile, method='cross')
summary(age ~ stage, fun=smean.sd, method='cross')
summary(age ~ stage, fun=smedian.hilow, method='cross')
summary(age ~ stage, fun=function(x) c(Mean=mean(x), Median=median(x)),
        method='cross')
#The next statements print real two-way tables
summary(cbind(age,ap) ~ stage + bone, 
        fun=function(y) apply(y, 2, quantile, c(.25,.75)),
        method='cross')
options(digits=2)
summary(log(ap) ~ sz + bone,
        fun=function(y) c(Mean=mean(y), quantile(y)),
        method='cross')


#Summarize an ordered categorical response by all of the needed
#cumulative proportions
summary(cumcategory(disease.severity) ~ age + sex)


## End(Not run)
</code></pre>

<hr>
<h2 id='summaryM'>Summarize Mixed Data Types vs. Groups</h2><span id='topic+summaryM'></span><span id='topic+print.summaryM'></span><span id='topic+plot.summaryM'></span><span id='topic+latex.summaryM'></span><span id='topic+html.summaryM'></span><span id='topic+printsummaryM'></span>

<h3>Description</h3>

<p><code>summaryM</code> summarizes the variables listed in an S formula,
computing descriptive statistics and optionally statistical tests for
group differences.  This function is typically used when there are
multiple left-hand-side variables that are independently against by
groups marked by a single right-hand-side variable.  The summary
statistics may be passed to <code>print</code> methods, <code>plot</code> methods
for making annotated dot charts and extended box plots, and 
<code>latex</code> methods for typesetting tables using LaTeX.  The
<code>html</code> method uses <code>htmlTable::htmlTable</code> to typeset the
table in html, by passing information to the <code>latex</code> method with
<code>html=TRUE</code>.  This is for use with Quarto/RMarkdown.
The <code>print</code> methods use the <code>print.char.matrix</code> function to
print boxed tables when <code>options(prType=)</code> has not been given or
when <code>prType='plain'</code>.  For plain tables, <code>print</code> calls the
internal function <code>printsummaryM</code>.  When <code>prType='latex'</code>
the <code>latex</code> method is invoked, and when <code>prType='html'</code> html
is rendered.  In Quarto/RMarkdown, proper rendering will result even
if <code>results='asis'</code> does not appear in the chunk header.  When
rendering in html at the console due to having <code>options(prType='html')</code>
the table will be rendered in a viewer.
</p>
<p>The <code>plot</code> method creates <code>plotly</code> graphics if
<code>options(grType='plotly')</code>, otherwise base graphics are used.
<code>plotly</code> graphics provide extra information such as which
quantile is being displayed when hovering the mouse.  Test statistics
are displayed by hovering over the mean.
</p>
<p>Continuous variables are described by three quantiles (quartiles by
default) when printing, or by the following quantiles when plotting
expended box plots using the <code><a href="#topic+bpplt">bpplt</a></code> function:
0.05, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 0.95.  The box
plots are scaled to the 0.025 and 0.975 quantiles of each continuous
left-hand-side variable.  Categorical variables are 
described by counts and percentages.
</p>
<p>The left hand side of <code>formula</code> may contain <code>mChoice</code>
(&quot;multiple choice&quot;) variables.  When <code>test=TRUE</code> each choice is
tested separately as a binary categorical response.
</p>
<p>The <code>plot</code> method for <code>method="reverse"</code> creates a temporary
function <code>Key</code> as is done by the <code>xYplot</code> and
<code>Ecdf.formula</code> functions.  After <code>plot</code>
runs, you can type <code>Key()</code> to put a legend in a default location, or
e.g. <code>Key(locator(1))</code> to draw a legend where you click the left
mouse button.  This key is for categorical variables, so to have the
opportunity to put the key on the graph you will probably want to use
the command <code>plot(object, which="categorical")</code>.  A second function
<code>Key2</code> is created if continuous variables are being plotted.  It is
used the same as <code>Key</code>.  If the <code>which</code> argument is not
specified to <code>plot</code>, two pages of plots will be produced.  If you
don't define <code>par(mfrow=)</code> yourself,
<code>plot.summaryM</code> will try to lay out a multi-panel
graph to best fit all the individual charts for continuous
variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summaryM(formula, groups=NULL, data=NULL, subset, na.action=na.retain,
         overall=FALSE, continuous=10, na.include=FALSE,
         quant=c(0.025, 0.05, 0.125, 0.25, 0.375, 0.5, 0.625,
                 0.75, 0.875, 0.95, 0.975),
         nmin=100, test=FALSE,
         conTest=conTestkw, catTest=catTestchisq,
         ordTest=ordTestpo)

## S3 method for class 'summaryM'
print(...)

printsummaryM(x, digits, prn = any(n != N),
      what=c('proportion', '%'), pctdig = if(what == '%') 0 else 2,
      npct = c('numerator', 'both', 'denominator', 'none'),
      exclude1 = TRUE, vnames = c('labels', 'names'), prUnits = TRUE,
      sep = '/', abbreviate.dimnames = FALSE,
      prefix.width = max(nchar(lab)), min.colwidth, formatArgs=NULL, round=NULL,
      prtest = c('P','stat','df','name'), prmsd = FALSE, long = FALSE,
      pdig = 3, eps = 0.001, prob = c(0.25, 0.5, 0.75), prN = FALSE, ...)

## S3 method for class 'summaryM'
plot(x, vnames = c('labels', 'names'),
     which = c('both', 'categorical', 'continuous'), vars=NULL,
     xlim = c(0,1),
     xlab = 'Proportion',
     pch = c(16, 1, 2, 17, 15, 3, 4, 5, 0), exclude1 = TRUE,
     main, ncols=2,
     prtest = c('P', 'stat', 'df', 'name'), pdig = 3, eps = 0.001,
     conType = c('bp', 'dot', 'raw'), cex.means = 0.5, cex=par('cex'),
     height='auto', width=700, ...)

## S3 method for class 'summaryM'
latex(object, title =
      first.word(deparse(substitute(object))),
      file=paste(title, 'tex', sep='.'), append=FALSE, digits, 
      prn = any(n != N), what=c('proportion', '%'),
      pctdig = if(what == '%') 0 else 2,
      npct = c('numerator', 'both', 'denominator', 'slash', 'none'),
      npct.size = if(html) mspecs$html$smaller else 'scriptsize',
      Nsize = if(html) mspecs$html$smaller else 'scriptsize',
      exclude1 = TRUE,
      vnames=c("labels", "names"), prUnits = TRUE, middle.bold = FALSE,
      outer.size = if(html) mspecs$html$smaller else "scriptsize",
      caption, rowlabel = "", rowsep=html,
      insert.bottom = TRUE, dcolumn = FALSE, formatArgs=NULL, round=NULL,
      prtest = c('P', 'stat', 'df', 'name'), prmsd = FALSE,
      msdsize = if(html) function(x) x else NULL, brmsd=FALSE,
      long = FALSE, pdig = 3, eps = 0.001,
      auxCol = NULL, table.env=TRUE, tabenv1=FALSE, prob=c(0.25, 0.5, 0.75),
      prN=FALSE, legend.bottom=FALSE, html=FALSE,
      mspecs=markupSpecs, ...)

## S3 method for class 'summaryM'
html(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summaryM_+3A_formula">formula</code></td>
<td>

<p>An S formula with additive effects.  There may be several variables
on the right hand side separated by &quot;+&quot;,
or the numeral <code>1</code>, indicating that
there is no grouping variable so that only margin summaries are
produced.  The right hand side variable, if present, must be a
discrete variable producing a limited number of groups.  On the
left hand side there may be any number of variables, separated by
&quot;+&quot;, and these may be of mixed types.  These variables are analyzed
separately by the grouping variable.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_groups">groups</code></td>
<td>
<p>if there is more than one right-hand variable, specify
<code>groups</code> as a character string containing the name of the
variable used to produce columns of the table.  The remaining right
hand variables are combined to produce levels that cause separate
tables or plots to be produced.</p>
</td></tr>
<tr><td><code id="summaryM_+3A_x">x</code></td>
<td>
<p>an object created by <code>summaryM</code>.  For
<code>conTestkw</code> a numeric vector, and for <code>ordTestpo</code>, a numeric
or factor variable that can be considered ordered</p>
</td></tr>
<tr><td><code id="summaryM_+3A_data">data</code></td>
<td>

<p>name or number of a data frame.  Default is the current frame.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_subset">subset</code></td>
<td>

<p>a logical vector or integer vector of subscripts used to specify the
subset of data to use in the analysis.  The default is to use all
observations in the data frame.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_na.action">na.action</code></td>
<td>

<p>function for handling missing data in the input data.  The default is
a function defined here called <code>na.retain</code>, which keeps all
observations for processing, with missing variables or not.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_overall">overall</code></td>
<td>

<p>Setting <code>overall=TRUE</code> makes a new column with
overall statistics for the whole sample.  If <code>test=TRUE</code> these
marginal statistics are ignored in doing statistical tests.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_continuous">continuous</code></td>
<td>

<p>specifies the threshold for when a variable is considered to be
continuous (when there are at least <code>continuous</code> unique values).
<code>factor</code> variables are always considered to be categorical no matter
how many levels they have.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_na.include">na.include</code></td>
<td>

<p>Set <code>na.include=TRUE</code> to keep missing values of categorical
variables from being excluded from the table.  
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_nmin">nmin</code></td>
<td>

<p>For categories of the response variable in which there
are less than or equal to <code>nmin</code> non-missing observations, the raw
data are retained for later plotting in place of box plots.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_test">test</code></td>
<td>

<p>Set to <code>TRUE</code> to compute test
statistics using tests specified in <code>conTest</code> and <code>catTest</code>.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_contest">conTest</code></td>
<td>

<p>a function of two arguments (grouping variable and a continuous
variable) that returns a list with components <code>P</code> (the computed
P-value), <code>stat</code> (the test statistic, either chi-square or F),
<code>df</code> (degrees of freedom), <code>testname</code> (test name),
<code>namefun</code> (<code>"chisq", "fstat"</code>), <code>statname</code>
(statistic name), an optional component <code>latexstat</code> (LaTeX
representation of <code>statname</code>), an optional component
<code>plotmathstat</code> (for R - the <code>plotmath</code> representation of
<code>statname</code>, as a character string),  and an
optional component <code>note</code> 
that contains a character string note about the test (e.g.,
<code>"test not done because n &lt; 5"</code>).  <code>conTest</code> is applied to
continuous variables 
on the right-hand-side of the formula when <code>method="reverse"</code>.  The
default uses the <code>spearman2</code> function to run the Wilcoxon or
Kruskal-Wallis test using the F distribution.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_cattest">catTest</code></td>
<td>

<p>a function of a frequency table (an integer matrix) that returns a
list with the same components as created by <code>conTest</code>.  By default,
the Pearson chi-square test is done, without continuity correction
(the continuity correction would make the test conservative like the
Fisher exact test).
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_ordtest">ordTest</code></td>
<td>

<p>a function of a frequency table (an integer matrix) that returns a
list with the same components as created by <code>conTest</code>.  By default,
the Proportional odds likelihood ratio test is done.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_...">...</code></td>
<td>

<p>For <code>Key</code> and <code>Key2</code> these arguments are passed to <code>key</code>,
<code>text</code>, or <code>mtitle</code>.  For <code>print</code> methods these are
optional arguments to <code>print.char.matrix</code>. For <code>latex</code> methods
these are passed to <code>latex.default</code>.  For <code>html</code> the
arguments are passed the <code>latex.summaryM</code>, and the arguments
may not include <code>file</code>.  For <code>print</code> the arguments are
passed to <code>printsummaryM</code> or <code>latex.summaryM</code> depending on
<code>options(prType=)</code>.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_object">object</code></td>
<td>
<p>an object created by <code>summaryM</code></p>
</td></tr>
<tr><td><code id="summaryM_+3A_quant">quant</code></td>
<td>

<p>vector of quantiles to use for summarizing continuous variables.
These must be numbers between 0 and 1
inclusive and must include the numbers 0.5, 0.25, and 0.75 which are
used for printing and for plotting 
quantile intervals.  The outer quantiles are used for scaling the x-axes
for such plots.  Specify outer quantiles as <code>0</code> and <code>1</code> to
scale the x-axes using the whole observed data ranges instead of the
default (a 0.95 quantile interval).  Box-percentile plots are drawn
using all but the outer quantiles.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_prob">prob</code></td>
<td>

<p>vector of quantiles to use for summarizing continuous variables.
These must be numbers between 0 and 1 inclusive and have previously been
included in the <code>quant</code> argument of <code>summaryM</code>.  The vector
must be of length three.  By default it contains 0.25, 0.5, and 0.75.
</p>
<p>Warning: specifying 0 and 1 as two of the quantiles will result in
computing the minimum and maximum of the variable.  As for many random
variables the minimum will continue to become smaller as the sample size
grows, and the maximum will continue to get larger.  Thus the min and max
are not recommended as summary statistics.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_vnames">vnames</code></td>
<td>

<p>By default, tables and plots are usually labeled with variable labels
(see the <code>label</code> and <code>sas.get</code> functions).  To use the shorter
variable names, specify <code>vnames="name"</code>.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_pch">pch</code></td>
<td>

<p>vector of plotting characters to represent different groups, in order
of group levels.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_abbreviate.dimnames">abbreviate.dimnames</code></td>
<td>
<p>see <code>print.char.matrix</code></p>
</td></tr>
<tr><td><code id="summaryM_+3A_prefix.width">prefix.width</code></td>
<td>
<p>see <code>print.char.matrix</code></p>
</td></tr>
<tr><td><code id="summaryM_+3A_min.colwidth">min.colwidth</code></td>
<td>

<p>minimum column width to use for boxes printed with <code>print.char.matrix</code>.
The default is the maximum of the minimum column label length and
the minimum length of entries in the data cells.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_formatargs">formatArgs</code></td>
<td>

<p>a list containing other arguments to pass to <code>format.default</code> such as
<code>scientific</code>, e.g., <code>formatArgs=list(scientific=c(-5,5))</code>.  For
<code>print.summary.formula.reverse</code> and
<code>format.summary.formula.reverse</code>, <code>formatArgs</code> applies only to
statistics computed on continuous variables, not to percents,
numerators, and denominators.  The <code>round</code> argument may be preferred.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_digits">digits</code></td>
<td>

<p>number of significant digits to print.  Default is to use the current
value of the <code>digits</code> system option.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_what">what</code></td>
<td>
<p>specifies whether proportions or percentages are to be
printed or LaTeX'd</p>
</td></tr> 
<tr><td><code id="summaryM_+3A_pctdig">pctdig</code></td>
<td>

<p>number of digits to the right of the decimal place for printing
percentages or proportions. The default is zero if <code>what='%'</code>,
so percents will be rounded to the nearest percent.  The default is
2 for proportions.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_prn">prn</code></td>
<td>

<p>set to <code>TRUE</code> to print the number of non-missing observations on the
current (row) variable.  The default is to print these only if any of
the counts of non-missing values differs from the total number of
non-missing values of the left-hand-side variable.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_prn">prN</code></td>
<td>

<p>set to <code>TRUE</code> to print the number of non-missing observations on
rows that contain continuous variables.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_npct">npct</code></td>
<td>

<p>specifies which counts are to be printed to the right of percentages.
The default is to print the frequency (numerator of the percent) in
parentheses.  You can specify <code>"both"</code> to print both numerator and
denominator as a fraction, <code>"denominator"</code>, <code>"slash"</code> to
typeset horizontally using a forward slash, or <code>"none"</code>.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_npct.size">npct.size</code></td>
<td>

<p>the size for typesetting <code>npct</code> information which appears after
percents. The default is <code>"scriptsize"</code>.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_nsize">Nsize</code></td>
<td>

<p>When a second row of column headings is added showing sample sizes,
<code>Nsize</code> specifies the LaTeX size for these subheadings.  Default
is <code>"scriptsize"</code>.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_exclude1">exclude1</code></td>
<td>

<p>By default, <code>summaryM</code> objects will be printed, plotted,  or typeset by
removing redundant entries from percentage tables for categorical
variables.  For example, if you print the percent of females, you
don't need to print the percent of males.  To override this, set
<code>exclude1=FALSE</code>. 
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_prunits">prUnits</code></td>
<td>

<p>set to <code>FALSE</code> to suppress printing or latexing <code>units</code>
attributes of variables, when <code>method='reverse'</code> or <code>'response'</code>
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_sep">sep</code></td>
<td>

<p>character to use to separate quantiles when printing tables
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_prtest">prtest</code></td>
<td>

<p>a vector of test statistic components to print if <code>test=TRUE</code> was in
effect when <code>summaryM</code> was called.  Defaults to printing all
components.  Specify <code>prtest=FALSE</code> or <code>prtest="none"</code> to not
print any tests.  This applies to <code>print</code>, <code>latex</code>, and
<code>plot</code> methods.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_round">round</code></td>
<td>

<p>Specify <code>round</code> to round
the quantiles and optional mean and standard deviation to
<code>round</code> digits after the decimal point.  Set <code>round='auto'</code>
to try an automatic choice.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_prmsd">prmsd</code></td>
<td>

<p>set to <code>TRUE</code> to print mean and SD after the three quantiles, for
continuous variables
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_msdsize">msdsize</code></td>
<td>

<p>defaults to <code>NULL</code> to use the current font size for the mean and
standard deviation if <code>prmsd</code> is <code>TRUE</code>.  Set to a character
string or function to specify an alternate LaTeX font size.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_brmsd">brmsd</code></td>
<td>
<p>set to <code>TRUE</code> to put the mean and standard deviation
on a separate line, for html</p>
</td></tr>
<tr><td><code id="summaryM_+3A_long">long</code></td>
<td>

<p>set to <code>TRUE</code> to print the results for the first category on its own
line, not on the same line with the variable label
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_pdig">pdig</code></td>
<td>

<p>number of digits to the right of the decimal place for printing
P-values.  Default is <code>3</code>.  This is passed to <code>format.pval</code>.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_eps">eps</code></td>
<td>

<p>P-values less than <code>eps</code> will be printed as <code>&lt; eps</code>.  See
<code>format.pval</code>.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_auxcol">auxCol</code></td>
<td>

<p>an optional auxiliary column of information, right justified, to add
in front of statistics typeset by
<code>latex.summaryM</code>.  This argument is a list with a
single element that has a name specifying the column heading.  If this
name includes a newline character, the portions of the string before
and after the newline form respectively the main heading and the
subheading (typically set in smaller font), respectively.  See the
<code>extracolheads</code> argument to <code>latex.default</code>.  <code>auxCol</code>
is filled with blanks when a variable being summarized takes up more
than one row in the output.  This happens with categorical variables.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_table.env">table.env</code></td>
<td>
<p>set to <code>FALSE</code> to use <code>tabular</code> environment
with no caption</p>
</td></tr>
<tr><td><code id="summaryM_+3A_tabenv1">tabenv1</code></td>
<td>
<p>set to <code>TRUE</code> in the case of stratification when
you want only the first stratum's table to be in a table
environment.  This is useful when using <code>hyperref</code>.</p>
</td></tr>
<tr><td><code id="summaryM_+3A_which">which</code></td>
<td>
<p>Specifies whether to plot results for categorical variables,
continuous variables, or both (the default).</p>
</td></tr>
<tr><td><code id="summaryM_+3A_vars">vars</code></td>
<td>
<p>Subscripts (indexes) of variables to plot for
<code>plotly</code> graphics.  Default is to plot all variables of each
type (categorical or continuous).</p>
</td></tr>
<tr><td><code id="summaryM_+3A_contype">conType</code></td>
<td>

<p>For drawing plots for continuous variables,
extended box plots (box-percentile-type plots) are drawn by default,
using all quantiles in <code>quant</code> except for the outermost ones
which are using for scaling the overall plot based on the
non-stratified marginal distribution of the current response variable.
Specify <code>conType='dot'</code> to draw dot plots showing the three
quartiles instead.  For extended box plots, means are drawn
with a solid dot and vertical reference lines are placed at the three
quartiles.  Specify <code>conType='raw'</code> to make a strip chart showing
the raw data.  This can only be used if the sample size for each
right-hand-side group is less than or equal to <code>nmin</code>.</p>
</td></tr>
<tr><td><code id="summaryM_+3A_cex.means">cex.means</code></td>
<td>

<p>character size for means in box-percentile plots; default is .5</p>
</td></tr>
<tr><td><code id="summaryM_+3A_cex">cex</code></td>
<td>
<p>character size for other plotted items</p>
</td></tr>
<tr><td><code id="summaryM_+3A_height">height</code>, <code id="summaryM_+3A_width">width</code></td>
<td>
<p>dimensions in pixels for the <code>plotly</code>
<code>subplot</code> object containing all the extended box plots.  If
<code>height="auto"</code>, <code>plot.summaryM</code> will set <code>height</code>
based on the number of 
continuous variables and <code>ncols</code> or for dot charts it will use
<code>Hmisc::plotlyHeightDotchart</code>.  At present <code>height</code> is
ignored for extended box plots due to vertical spacing problem with
<code>plotly</code> graphics.</p>
</td></tr>
<tr><td><code id="summaryM_+3A_xlim">xlim</code></td>
<td>

<p>vector of length two specifying x-axis limits.  This is only used
for plotting categorical variables.  Limits for continuous
variables are determined by the outer quantiles specified in
<code>quant</code>. 
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label</p>
</td></tr>
<tr><td><code id="summaryM_+3A_main">main</code></td>
<td>
<p>a main title.  This applies only to the plot for
categorical variables.</p>
</td></tr>
<tr><td><code id="summaryM_+3A_ncols">ncols</code></td>
<td>
<p>number of columns for <code>plotly</code> graphics for extended
box plots.  Defaults to 2.  Recommendation is for 1-2.</p>
</td></tr>
<tr><td><code id="summaryM_+3A_caption">caption</code></td>
<td>
<p>character string containing LaTeX table captions.</p>
</td></tr>
<tr><td><code id="summaryM_+3A_title">title</code></td>
<td>

<p>name of resulting LaTeX file omitting the <code>.tex</code> suffix.  Default
is the name of the <code>summary</code> object.  If <code>caption</code> is specied,
<code>title</code> is also used for the table's symbolic reference label. 
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_file">file</code></td>
<td>
<p>name of file to write LaTeX code to. Specifying
<code>file=""</code> will cause LaTeX code to just be printed to
standard output rather than be stored in a  permanent file.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_append">append</code></td>
<td>
<p>specify <code>TRUE</code> to add code to an existing file</p>
</td></tr>
<tr><td><code id="summaryM_+3A_rowlabel">rowlabel</code></td>
<td>
<p>see <code>latex.default</code> (under the help file
<code>latex</code>)</p>
</td></tr>
<tr><td><code id="summaryM_+3A_rowsep">rowsep</code></td>
<td>
<p>if <code>html</code> is <code>TRUE</code>, instructs the function to
use a horizontal line to separate variables from one another.
Recommended if <code>brmsd</code> is <code>TRUE</code>.  Ignored for LaTeX.</p>
</td></tr>
<tr><td><code id="summaryM_+3A_middle.bold">middle.bold</code></td>
<td>

<p>set to <code>TRUE</code> to have LaTeX use bold face for the middle
quantile
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_outer.size">outer.size</code></td>
<td>
<p>the font size for outer quantiles
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_insert.bottom">insert.bottom</code></td>
<td>

<p>set to <code>FALSE</code> to suppress inclusion of definitions placed at the
bottom of LaTeX tables.  You can also specify a character string
containing other text that overrides the automatic text.  At
present such text always appears in the main caption for LaTeX.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_legend.bottom">legend.bottom</code></td>
<td>

<p>set to <code>TRUE</code> to separate the table caption and legend.  This
will place table legends at the bottom of LaTeX tables.
</p>
</td></tr>
<tr><td><code id="summaryM_+3A_html">html</code></td>
<td>
<p>set to <code>TRUE</code> to typeset with html</p>
</td></tr>
<tr><td><code id="summaryM_+3A_mspecs">mspecs</code></td>
<td>
<p>list defining markup syntax for various languages,
defaults to Hmisc <code>markupSpecs</code> which the user can use as a
starting point for editing</p>
</td></tr>
<tr><td><code id="summaryM_+3A_dcolumn">dcolumn</code></td>
<td>
<p>see <code>latex</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list.  <code>plot.summaryM</code> returns the number
of pages of plots that were made if using base graphics, or
<code>plotly</code> objects created by <code>plotly::subplot</code> otherwise.
If both categorical and continuous variables were plotted, the
returned object is a list with two named elements <code>Categorical</code>
and <code>Continuous</code> each containing <code>plotly</code> objects.
Otherwise a <code>plotly</code> object is returned.
The <code>latex</code> method returns attributes <code>legend</code> and
<code>nstrata</code>.
</p>


<h3>Side Effects</h3>

<p><code>plot.summaryM</code> creates a function <code>Key</code> and
<code>Key2</code> in frame 0 that will draw legends, if base graphics are
being used.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Harrell FE (2004): Statistical tables and plots using S and LaTeX.
Document available from
<a href="https://hbiostat.org/R/Hmisc/summary.pdf">https://hbiostat.org/R/Hmisc/summary.pdf</a>. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mChoice">mChoice</a></code>, <code><a href="#topic+label">label</a></code>, <code><a href="#topic+dotchart3">dotchart3</a></code>,
<code><a href="#topic+print.char.matrix">print.char.matrix</a></code>, <code><a href="stats.html#topic+update">update</a></code>,
<code><a href="stats.html#topic+formula">formula</a></code>, 
<code><a href="base.html#topic+format.default">format.default</a></code>, <code><a href="#topic+latex">latex</a></code>,
<code><a href="#topic+latexTranslate">latexTranslate</a></code>, <code><a href="#topic+bpplt">bpplt</a></code>,
<code><a href="#topic+tabulr">tabulr</a></code>, <code><a href="#topic+bpplotM">bpplotM</a></code>, <code><a href="#topic+summaryP">summaryP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>options(digits=3)
set.seed(173)
sex &lt;- factor(sample(c("m","f"), 500, rep=TRUE))
country &lt;- factor(sample(c('US', 'Canada'), 500, rep=TRUE))
age &lt;- rnorm(500, 50, 5)
sbp &lt;- rnorm(500, 120, 12)
label(sbp) &lt;- 'Systolic BP'
units(sbp) &lt;- 'mmHg'
treatment &lt;- factor(sample(c("Drug","Placebo"), 500, rep=TRUE))
treatment[1]
sbp[1] &lt;- NA

# Generate a 3-choice variable; each of 3 variables has 5 possible levels
symp &lt;- c('Headache','Stomach Ache','Hangnail',
          'Muscle Ache','Depressed')
symptom1 &lt;- sample(symp, 500,TRUE)
symptom2 &lt;- sample(symp, 500,TRUE)
symptom3 &lt;- sample(symp, 500,TRUE)
Symptoms &lt;- mChoice(symptom1, symptom2, symptom3, label='Primary Symptoms')
table(as.character(Symptoms))

# Note: In this example, some subjects have the same symptom checked
# multiple times; in practice these redundant selections would be NAs
# mChoice will ignore these redundant selections

f &lt;- summaryM(age + sex + sbp + Symptoms ~ treatment, test=TRUE)
f
# trio of numbers represent 25th, 50th, 75th percentile
print(f, long=TRUE)
plot(f)    # first specify options(grType='plotly') to use plotly
plot(f, conType='dot', prtest='P')
bpplt()    # annotated example showing layout of bp plot

# Produce separate tables by country
f &lt;- summaryM(age + sex + sbp + Symptoms ~ treatment + country,
              groups='treatment', test=TRUE)
f

## Not run: 
getHdata(pbc)
s5 &lt;- summaryM(bili + albumin + stage + protime + sex + 
               age + spiders ~ drug, data=pbc)

print(s5, npct='both')
# npct='both' : print both numerators and denominators
plot(s5, which='categorical')
Key(locator(1))  # draw legend at mouse click
par(oma=c(3,0,0,0))  # leave outer margin at bottom
plot(s5, which='continuous')  # see also bpplotM
Key2()           # draw legend at lower left corner of plot
                 # oma= above makes this default key fit the page better

options(digits=3)
w &lt;- latex(s5, npct='both', here=TRUE, file='')

options(grType='plotly')
pbc &lt;- upData(pbc, moveUnits = TRUE)
s &lt;- summaryM(bili + albumin + alk.phos + copper + spiders + sex ~
              drug, data=pbc, test=TRUE)
# Render html
options(prType='html')
s   # invokes print.summaryM
a &lt;- plot(s)
a$Categorical
a$Continuous
plot(s, which='con')

## End(Not run)
</code></pre>

<hr>
<h2 id='summaryP'>Multi-way Summary of Proportions</h2><span id='topic+summaryP'></span><span id='topic+plot.summaryP'></span><span id='topic+ggplot.summaryP'></span><span id='topic+latex.summaryP'></span>

<h3>Description</h3>

<p><code>summaryP</code> produces a tall and thin data frame containing
numerators (<code>freq</code>) and denominators (<code>denom</code>) after
stratifying the data by a series of variables.  A special capability
to group a series of related yes/no variables is included through the
use of the <code><a href="#topic+ynbind">ynbind</a></code> function, for which the user specials a final
argument <code>label</code> used to label the panel created for that group
of related variables.
</p>
<p>If <code>options(grType='plotly')</code> is not in effect,
the <code>plot</code> method for <code>summaryP</code>	displays proportions as a
multi-panel dot chart using the <code>lattice</code> package's <code>dotplot</code>
function with a special <code>panel</code> function.  Numerators and
denominators of proportions are also included as text, in the same
colors as used by an optional <code>groups</code> variable.  The
<code>formula</code> argument used in the <code>dotplot</code> call is constructed,
but the user can easily reorder the variables by specifying
<code>formula</code>, with elements named <code>val</code> (category levels),
<code>var</code> (classification variable name), <code>freq</code> (calculated
result) plus the overall cross-classification variables excluding
<code>groups</code>.  If <code>options(grType='plotly')</code> is in effect, the
<code>plot</code> method makes an entirely different display using
<code>Hmisc::dotchartpl</code> with <code>plotly</code> if <code>marginVal</code> is
specified, whereby a stratification
variable causes more finely stratified estimates to be shown slightly
below the lines, with smaller and translucent symbols if <code>data</code>
has been run through <code>addMarginal</code>.  The marginal summaries are
shown as the main estimates and the user can turn off display of the
stratified estimates, or view their details with hover text.
</p>
<p>The <code>ggplot</code> method for <code>summaryP</code> does not draw numerators
and denominators but the chart is more compact than using the
<code>plot</code> method with base graphics because <code>ggplot2</code>
does not repeat category names the same way as <code>lattice</code> does.
Variable names that are too long to fit in panel strips are renamed
(1), (2), etc. and an attribute <code>"fnvar"</code> is added to the result;
this attribute is a character string defining the abbreviations,
useful in a figure caption.  The <code>ggplot2</code> object has
<code>label</code>s for points plotted, used by <code>plotly::ggplotly</code> as
hover text (see example).
</p>
<p>The <code>latex</code> method produces one or more LaTeX <code>tabular</code>s
containing a table representation of the result, with optional
side-by-side display if <code>groups</code> is specified.  Multiple
<code>tabular</code>s result from the presence of non-group stratification
factors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summaryP(formula, data = NULL, subset = NULL,
         na.action = na.retain, sort=TRUE,
         asna = c("unknown", "unspecified"), ...)
## S3 method for class 'summaryP'
plot(x, formula=NULL, groups=NULL,
         marginVal=NULL, marginLabel=marginVal,
         refgroup=NULL, exclude1=TRUE,  xlim = c(-.05, 1.05),
         text.at=NULL, cex.values = 0.5,
         key = list(columns = length(groupslevels), x = 0.75,
                    y = -0.04, cex = 0.9,
                    col = lattice::trellis.par.get('superpose.symbol')$col,
                    corner=c(0,1)),
         outerlabels=TRUE, autoarrange=TRUE,
         col=colorspace::rainbow_hcl, ...)
## S3 method for class 'summaryP'
ggplot(data, mapping, groups=NULL, exclude1=TRUE,
           xlim=c(0, 1), col=NULL, shape=NULL, size=function(n) n ^ (1/4),
           sizerange=NULL, abblen=5, autoarrange=TRUE, addlayer=NULL,
           ..., environment)
## S3 method for class 'summaryP'
latex(object, groups=NULL, exclude1=TRUE, file='', round=3,
                           size=NULL, append=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summaryP_+3A_formula">formula</code></td>
<td>
<p>a formula with the variables for whose levels
proportions are computed on the left hand side, and major
classification variables on the right.  The formula need to include
any variable later used as <code>groups</code>, as the data summarization
does not distinguish between superpositioning and paneling.  For the
plot method, <code>formula</code> can provide an overall to the default
formula for <code>dotplot()</code>.</p>
</td></tr>
<tr><td><code id="summaryP_+3A_data">data</code></td>
<td>
<p>an optional data frame.  For <code>ggplot.summaryP</code>
<code>data</code> is the result of <code>summaryP</code>.</p>
</td></tr>
<tr><td><code id="summaryP_+3A_subset">subset</code></td>
<td>
<p>an optional subsetting expression or vector</p>
</td></tr>
<tr><td><code id="summaryP_+3A_na.action">na.action</code></td>
<td>
<p>function specifying how to handle <code>NA</code>s.  The
default is to keep all <code>NA</code>s in the analysis frame.</p>
</td></tr>
<tr><td><code id="summaryP_+3A_sort">sort</code></td>
<td>
<p>set to <code>FALSE</code> to not sort category levels in
descending order of global proportions</p>
</td></tr>
<tr><td><code id="summaryP_+3A_asna">asna</code></td>
<td>
<p>character vector specifying level names to consider the
same as <code>NA</code>.  Set <code>asna=NULL</code> to not consider any.</p>
</td></tr>
<tr><td><code id="summaryP_+3A_x">x</code></td>
<td>
<p>an object produced by <code>summaryP</code></p>
</td></tr>
<tr><td><code id="summaryP_+3A_groups">groups</code></td>
<td>
<p>a character string containing the name of a
superpositioning variable for obtaining 
further stratification within a horizontal line in the dot chart.</p>
</td></tr>
<tr><td><code id="summaryP_+3A_marginval">marginVal</code></td>
<td>
<p>if <code>options(grType='plotly')</code> is in effect and
the data given to <code>summaryP</code> were run through <code>addMarginal</code>,
specifies the category name that represents marginal summaries
(usually <code>"All"</code>).</p>
</td></tr>
<tr><td><code id="summaryP_+3A_marginlabel">marginLabel</code></td>
<td>
<p>specifies a different character string to use than
the value of <code>marginVal</code>.  For example, if marginal proportions
were computed over all <code>region</code>s, one may specify
<code>marginVal="All", marginLabel="All Regions"</code>.  <code>marginLabel</code>
is only used for formatting graphical output.</p>
</td></tr>
<tr><td><code id="summaryP_+3A_refgroup">refgroup</code></td>
<td>
<p>used when doing a <code>plotly</code> chart and a two-level
group variable was used, resulting in the half-width confidence
interval for the difference in two proportions to be shown, and the
actual confidence limits and the difference added to hover text.  See
<code>dotchartpl</code> for more details.</p>
</td></tr>
<tr><td><code id="summaryP_+3A_exclude1">exclude1</code></td>
<td>
<p>By default, <code>ggplot</code>, <code>plot</code>, and
<code>latex</code> methods for <code>summaryP</code> remove redundant entries 
from tables for variables with only two levels.  For example, if you
print the proportion of females, you don't need to print the
proportion of males.  To override this, set <code>exclude1=FALSE</code>.</p>
</td></tr>
<tr><td><code id="summaryP_+3A_xlim">xlim</code></td>
<td>
<p><code>x</code>-axis limits.  Default is <code>c(0,1)</code>.</p>
</td></tr>
<tr><td><code id="summaryP_+3A_text.at">text.at</code></td>
<td>
<p>specify to leave unused space to the right of each
panel to prevent numerators and denominators from touching data
points.  <code>text.at</code> is the upper limit for scaling panels'
<code>x</code>-axes but tick marks are only labeled up to <code>max(xlim)</code>.</p>
</td></tr>
<tr><td><code id="summaryP_+3A_cex.values">cex.values</code></td>
<td>
<p>character size to use for plotting numerators and
denominators</p>
</td></tr>
<tr><td><code id="summaryP_+3A_key">key</code></td>
<td>
<p>a list to pass to the <code>auto.key</code> argument of
<code>dotplot</code>.  To place a key above the entire chart use
<code>auto.key=list(columns=2)</code> for example.</p>
</td></tr>
<tr><td><code id="summaryP_+3A_outerlabels">outerlabels</code></td>
<td>
<p>by default if there are two conditioning variables
besides <code>groups</code>, the <code>latticeExtra</code> package's
<code>useOuterStrips</code> function is used to put strip labels in the
margins, usually resulting in a much prettier chart.  Set to
<code>FALSE</code> to prevent usage of <code>useOuterStrips</code>.</p>
</td></tr>
<tr><td><code id="summaryP_+3A_autoarrange">autoarrange</code></td>
<td>
<p>If <code>TRUE</code>, the formula is re-arranged so that
if there are two conditioning (paneling) variables, the variable with
the most levels is taken as the vertical condition.</p>
</td></tr>
<tr><td><code id="summaryP_+3A_col">col</code></td>
<td>
<p>a vector of colors to use to override defaults in
<code>ggplot</code>.  When <code>options(grType='plotly')</code>, see <code>dotchartpl</code>.</p>
</td></tr>
<tr><td><code id="summaryP_+3A_shape">shape</code></td>
<td>
<p>a vector of plotting symbols to override <code>ggplot</code>
defaults</p>
</td></tr>
<tr><td><code id="summaryP_+3A_mapping">mapping</code>, <code id="summaryP_+3A_environment">environment</code></td>
<td>
<p>not used; needed because of rules for generics</p>
</td></tr>
<tr><td><code id="summaryP_+3A_size">size</code></td>
<td>
<p>for <code>ggplot</code>, a function that transforms denominators
into metrics used for the <code>size</code> aesthetic.  Default is the
fourth root function so that the area of symbols is proportional to
the square root of sample size.  Specify <code>NULL</code> to not vary point
sizes. 	<code>size=sqrt</code> is a reasonable 	alternative.  Set
<code>size</code> to an integer to categorize the denominators into
<code>size</code> quantile groups using <code>cut2</code>. Unless <code>size</code> is
an integer, the legend for sizes uses the minimum and maximum
denominators and 6-tiles using <code>quantile(..., type=1)</code> so that
actually occurring sample sizes are used as labels.  <code>size</code> is
overridden to <code>NULL</code> if the range in denominators is less than 10
or the ratio of the maximum to the minimum is less than 1.2.
For	<code>latex</code>, <code>size</code> is an optional font size such as
<code>"small"</code></p>
</td></tr>  
<tr><td><code id="summaryP_+3A_sizerange">sizerange</code></td>
<td>
<p>a 2-vector specifying the <code>range</code> argument to the
<code>ggplot2</code> <code>scale_size_...</code> function, which is the
range of sizes allowed for the points according to the denominator.
The default is <code>sizerange=c(.7, 3.25)</code> but the lower limit is
increased according to the ratio of maximum to minimum sample sizes.</p>
</td></tr>
<tr><td><code id="summaryP_+3A_abblen">abblen</code></td>
<td>
<p>labels of variables having only one level and having
their name longer than <code>abblen</code> characters are 
abbreviated and documented in <code>fnvar</code> (described elsewhere
here).  The default <code>abblen=5</code> is good for labels plotted
vertically.  If labels are rotated using <code>theme</code> a better value
would be 12.</p>
</td></tr>
<tr><td><code id="summaryP_+3A_...">...</code></td>
<td>
<p>used only for <code>plotly</code> graphics and these arguments
are passed to <code>dotchartpl</code></p>
</td></tr>
<tr><td><code id="summaryP_+3A_object">object</code></td>
<td>
<p>an object produced by <code>summaryP</code></p>
</td></tr>
<tr><td><code id="summaryP_+3A_file">file</code></td>
<td>
<p>file name, defaults to writing to console</p>
</td></tr>
<tr><td><code id="summaryP_+3A_round">round</code></td>
<td>
<p>number of digits to the right of the decimal place for
proportions</p>
</td></tr>
<tr><td><code id="summaryP_+3A_append">append</code></td>
<td>
<p>set to <code>FALSE</code> to start output over</p>
</td></tr>
<tr><td><code id="summaryP_+3A_addlayer">addlayer</code></td>
<td>
<p>a <code>ggplot</code> layer to add to the plot object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>summaryP</code> produces a data frame of class
<code>"summaryP"</code>.  The <code>plot</code> method produces a <code>lattice</code>
object of class <code>"trellis"</code>.  The <code>latex</code> method produces an
object of class <code>"latex"</code> with an additional attribute
<code>ngrouplevels</code> specifying the number of levels of any
<code>groups</code> variable and an attribute <code>nstrata</code> specifying the
number of strata.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+bpplotM">bpplotM</a></code>, <code><a href="#topic+summaryM">summaryM</a></code>,
<code><a href="#topic+ynbind">ynbind</a></code>, <code><a href="#topic+pBlock">pBlock</a></code>,
<code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>, <code><a href="#topic+colorFacet">colorFacet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 100
f &lt;- function(na=FALSE) {
  x &lt;- sample(c('N', 'Y'), n, TRUE)
  if(na) x[runif(100) &lt; .1] &lt;- NA
  x
}
set.seed(1)
d &lt;- data.frame(x1=f(), x2=f(), x3=f(), x4=f(), x5=f(), x6=f(), x7=f(TRUE),
                age=rnorm(n, 50, 10),
                race=sample(c('Asian', 'Black/AA', 'White'), n, TRUE),
                sex=sample(c('Female', 'Male'), n, TRUE),
                treat=sample(c('A', 'B'), n, TRUE),
                region=sample(c('North America','Europe'), n, TRUE))
d &lt;- upData(d, labels=c(x1='MI', x2='Stroke', x3='AKI', x4='Migraines',
                 x5='Pregnant', x6='Other event', x7='MD withdrawal',
                 race='Race', sex='Sex'))
dasna &lt;- subset(d, region=='North America')
with(dasna, table(race, treat))
s &lt;- summaryP(race + sex + ynbind(x1, x2, x3, x4, x5, x6, x7, label='Exclusions') ~
              region + treat, data=d)
# add exclude1=FALSE below to include female category
plot(s, groups='treat')
require(ggplot2)
ggplot(s, groups='treat')

plot(s, val ~ freq | region * var, groups='treat', outerlabels=FALSE)
# Much better looking if omit outerlabels=FALSE; see output at
# https://hbiostat.org/R/Hmisc/summaryFuns.pdf
# See more examples under bpplotM

## For plotly interactive graphic that does not handle variable size
## panels well:
## require(plotly)
## g &lt;- ggplot(s, groups='treat')
## ggplotly(g, tooltip='text')

## For nice plotly interactive graphic:
## options(grType='plotly')
## s &lt;- summaryP(race + sex + ynbind(x1, x2, x3, x4, x5, x6, x7,
##                                   label='Exclusions') ~
##               treat, data=subset(d, region='Europe'))
##
## plot(s, groups='treat', refgroup='A')  # refgroup='A' does B-A differences


# Make a chart where there is a block of variables that
# are only analyzed for males.  Keep redundant sex in block for demo.
# Leave extra space for numerators, denominators
sb &lt;- summaryP(race + sex +
               pBlock(race, sex, label='Race: Males', subset=sex=='Male') ~
               region, data=d)
plot(sb, text.at=1.3)
plot(sb, groups='region', layout=c(1,3), key=list(space='top'),
     text.at=1.15)
ggplot(sb, groups='region')
## Not run: 
plot(s, groups='treat')
# plot(s, groups='treat', outerlabels=FALSE) for standard lattice output
plot(s, groups='region', key=list(columns=2, space='bottom'))
require(ggplot2)
colorFacet(ggplot(s))

plot(summaryP(race + sex ~ region, data=d), exclude1=FALSE, col='green')

require(lattice)
# Make your own plot using data frame created by summaryP
useOuterStrips(dotplot(val ~ freq | region * var, groups=treat, data=s,
        xlim=c(0,1), scales=list(y='free', rot=0), xlab='Fraction',
        panel=function(x, y, subscripts, ...) {
          denom &lt;- s$denom[subscripts]
          x &lt;- x / denom
          panel.dotplot(x=x, y=y, subscripts=subscripts, ...) }))

# Show marginal summary for all regions combined
s &lt;- summaryP(race + sex ~ region, data=addMarginal(d, region))
plot(s, groups='region', key=list(space='top'), layout=c(1,2))

# Show marginal summaries for both race and sex
s &lt;- summaryP(ynbind(x1, x2, x3, x4, label='Exclusions', sort=FALSE) ~
              race + sex, data=addMarginal(d, race, sex))
plot(s, val ~ freq | sex*race)

## End(Not run)
</code></pre>

<hr>
<h2 id='summaryRc'>Graphical Summarization of Continuous Variables Against a Response</h2><span id='topic+summaryRc'></span>

<h3>Description</h3>

<p><code>summaryRc</code> is a continuous version of <code><a href="#topic+summary.formula">summary.formula</a></code>
with <code>method='response'</code>.  It uses the <code><a href="#topic+plsmo">plsmo</a></code>
function to compute the possibly stratified <code><a href="stats.html#topic+lowess">lowess</a></code>
nonparametric regression estimates, and plots them along with the data
density, with selected quantiles of the overall distribution (over
strata) of each <code>x</code> shown as arrows on top of the graph.  All the
<code>x</code> variables must be numeric and continuous or nearly continuous.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summaryRc(formula, data=NULL, subset=NULL,
          na.action=NULL, fun = function(x) x,
          na.rm = TRUE, ylab=NULL, ylim=NULL, xlim=NULL,
          nloc=NULL, datadensity=NULL,
          quant = c(0.05, 0.1, 0.25, 0.5, 0.75,
                    0.90, 0.95), quantloc=c('top','bottom'),
          cex.quant=.6, srt.quant=0,
          bpplot = c('none', 'top', 'top outside', 'top inside', 'bottom'),
          height.bpplot=0.08,
          trim=NULL, test = FALSE, vnames = c('labels', 'names'), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summaryRc_+3A_formula">formula</code></td>
<td>

<p>An <span class="rlang"><b>R</b></span> formula with additive effects.  The <code>formula</code> may contain
one or more invocations of the <code>stratify</code> function whose
arguments are defined below.  This causes 
the entire analysis to be stratified by cross-classifications of the
combined list of stratification factors.  This stratification will be
reflected as separate <code>lowess</code> curves.</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_data">data</code></td>
<td>

<p>name or number of a data frame.  Default is the current frame.
</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_subset">subset</code></td>
<td>

<p>a logical vector or integer vector of subscripts used to specify the
subset of data to use in the analysis.  The default is to use all
observations in the data frame.
</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_na.action">na.action</code></td>
<td>

<p>function for handling missing data in the input data.  The default is
a function defined here called <code>na.retain</code>, which keeps all
observations for processing, with missing variables or not.
</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_fun">fun</code></td>
<td>

<p>function for transforming <code>lowess</code> estimates.  Default is the
identity function.</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_na.rm">na.rm</code></td>
<td>

<p><code>TRUE</code> (the default) to exclude <code>NA</code>s before passing data to
<code>fun</code> to compute statistics, <code>FALSE</code> otherwise.
</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_ylab">ylab</code></td>
<td>
<p><code>y</code>-axis label.  Default is label attribute of
<code>y</code> variable, or its name.</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_ylim">ylim</code></td>
<td>
<p><code>y</code>-axis limits.  By default each graph is scaled on
its own.</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_xlim">xlim</code></td>
<td>
<p>a list with elements named as the variable names appearing
on the <code>x</code>-axis, with each element being a 2-vector specifying
lower and upper limits.  Any variable not appearing in the list will
have its limits computed and possibly <code>trim</code>med.</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_nloc">nloc</code></td>
<td>
<p>location for sample size.  Specify <code>nloc=FALSE</code> to
suppress, or <code>nloc=list(x=,y=)</code> where <code>x,y</code> are relative
coordinates in the data window.  Default position is in the largest
empty space.</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_datadensity">datadensity</code></td>
<td>
<p>see <code><a href="#topic+plsmo">plsmo</a></code>.  Defaults to <code>TRUE</code>
if there is a <code>stratify</code> variable, <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_quant">quant</code></td>
<td>

<p>vector of quantiles to use for summarizing the marginal distribution
of each <code>x</code>. This must be numbers between 0 and 1
inclusive.  Use <code>NULL</code> to omit quantiles.
</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_quantloc">quantloc</code></td>
<td>
<p>specify <code>quantloc='bottom'</code> to place at the
bottom of each plot rather than the default</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_cex.quant">cex.quant</code></td>
<td>
<p>character size for writing which quantiles are
represented.  Set to <code>0</code> to suppress quantile labels.</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_srt.quant">srt.quant</code></td>
<td>
<p>angle for text for quantile labels</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_bpplot">bpplot</code></td>
<td>
<p>if not <code>'none'</code> will draw extended box plot at
location given by <code>bpplot</code>, and quantiles discussed above will
be suppressed.  Specifying <code>bpplot='top'</code> is the same as
specifying <code>bpplot='top inside'</code>.</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_height.bpplot">height.bpplot</code></td>
<td>
<p>height in inches of the horizontal extended box plot</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_trim">trim</code></td>
<td>
<p>The default is to plot from the 10th smallest to the 10th
largest <code>x</code> if the number of non-NAs exceeds 200, otherwise to
use the entire range of <code>x</code>.  Specify another quantile to use
other limits, e.g.,  <code>trim=0.01</code> will use the first and last
percentiles</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_test">test</code></td>
<td>

<p>Set to <code>TRUE</code> to plot test statistics (not yet implemented).
</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_vnames">vnames</code></td>
<td>

<p>By default, plots are usually labeled with variable labels
(see the <code>label</code> and <code>sas.get</code> functions).  To use the shorter
variable names, specify <code>vnames="names"</code>.
</p>
</td></tr>
<tr><td><code id="summaryRc_+3A_...">...</code></td>
<td>
<p>arguments passed to <code><a href="#topic+plsmo">plsmo</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>no value is returned</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plsmo">plsmo</a></code>, <code><a href="#topic+stratify">stratify</a></code>,
<code><a href="#topic+label">label</a></code>, <code><a href="stats.html#topic+formula">formula</a></code>, <code><a href="#topic+panel.bpplot">panel.bpplot</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>options(digits=3)
set.seed(177)
sex &lt;- factor(sample(c("m","f"), 500, rep=TRUE))
age &lt;- rnorm(500, 50, 5)
bp  &lt;- rnorm(500, 120, 7)
units(age) &lt;- 'Years'; units(bp) &lt;- 'mmHg'
label(bp) &lt;- 'Systolic Blood Pressure'
L &lt;- .5*(sex == 'm') + 0.1 * (age - 50)
y &lt;- rbinom(500, 1, plogis(L))
par(mfrow=c(1,2))
summaryRc(y ~ age + bp)
# For x limits use 1st and 99th percentiles to frame extended box plots
summaryRc(y ~ age + bp, bpplot='top', datadensity=FALSE, trim=.01)
summaryRc(y ~ age + bp + stratify(sex),
          label.curves=list(keys='lines'), nloc=list(x=.1, y=.05))
y2 &lt;- rbinom(500, 1, plogis(L + .5))
Y &lt;- cbind(y, y2)
summaryRc(Y ~ age + bp + stratify(sex),
          label.curves=list(keys='lines'), nloc=list(x=.1, y=.05))
</code></pre>

<hr>
<h2 id='summaryS'>Summarize Multiple Response Variables and Make Multipanel Scatter
or Dot Plot</h2><span id='topic+summaryS'></span><span id='topic+plot.summaryS'></span><span id='topic+plotp.summaryS'></span><span id='topic+mbarclPanel'></span><span id='topic+medvPanel'></span><span id='topic+mbarclpl'></span><span id='topic+medvpl'></span>

<h3>Description</h3>

<p>Multiple left-hand formula variables along with right-hand side
conditioning variables are reshaped into a &quot;tall and thin&quot; data frame if
<code>fun</code> is not specified.  The resulting raw data can be plotted with
the <code>plot</code> method using user-specified <code>panel</code> functions for
<code>lattice</code> graphics, typically to make a scatterplot or <code>loess</code>
smooths, or both.  The <code>Hmisc</code> <code>panel.plsmo</code> function is handy
in this context.  Instead, if <code>fun</code> is specified, this function
takes individual response variables (which may be matrices, as in
<code><a href="survival.html#topic+Surv">Surv</a></code> objects) and creates one or more summary
statistics that will be computed while the resulting data frame is being
collapsed to one row per condition.  The <code>plot</code> method in this case
plots a multi-panel dot chart using the <code>lattice</code>
<code><a href="lattice.html#topic+dotplot">dotplot</a></code> function if <code>panel</code> is not specified
to <code>plot</code>.  There is an option to print
selected statistics as text on the panels.  <code>summaryS</code> pays special
attention to <code>Hmisc</code> variable annotations: <code>label, units</code>.
When <code>panel</code> is specified in addition to <code>fun</code>, a special
<code>x-y</code> plot is made that assumes that the <code>x</code>-axis variable
(typically time) is discrete.  This is used for example to plot multiple
quantile intervals as vertical lines next to the main point.  A special
panel function <code>mvarclPanel</code> is provided for this purpose.
</p>
<p>The <code>plotp</code> method produces corresponding <code>plotly</code> graphics.
</p>
<p>When <code>fun</code> is given and <code>panel</code> is omitted, and the result of
<code>fun</code> is a vector of more than one 
statistic, the first statistic is taken as the main one.  Any columns
with names not in <code>textonly</code> will figure into the calculation of
axis limits.  Those in <code>textonly</code> will be printed right under the
dot lines in the dot chart.  Statistics with names in <code>textplot</code>
will figure into limits, be plotted, and printed.  <code>pch.stats</code> can
be used to specify symbols for statistics after the first column.  When
<code>fun</code> computed three columns that are plotted, columns two and
three are taken as confidence limits for which horizontal &quot;error bars&quot;
are drawn.  Two levels with different thicknesses are drawn if there are
four plotted summary statistics beyond the first.
</p>
<p><code>mbarclPanel</code> is used to draw multiple vertical lines around the
main points, such as a series of quantile intervals stratified by
<code>x</code> and paneling variables.  If <code>mbarclPanel</code> finds a column
of an arument <code>yother</code> that is named <code>"se"</code>, and if there are
exactly two levels to a superpositioning variable, the half-height of
the approximate 0.95 confidence interval for the difference between two
point estimates is shown, positioned at the midpoint of the two point
estimates at an <code>x</code> value.  This assume normality of point
estimates, and the standard error of the difference is the square root
of the sum of squares of the two standard errors.  By positioning the
intervals in this fashion, a failure of the two point estimates to touch
the half-confidence interval is consistent with rejecting the null
hypothesis of no difference at the 0.05 level.
</p>
<p><code>mbarclpl</code> is the <code>sfun</code> function corresponding to
<code>mbarclPanel</code> for <code>plotp</code>, and <code>medvpl</code> is the
<code>sfun</code> replacement for <code>medvPanel</code>.
</p>
<p><code>medvPanel</code> takes raw data and plots median <code>y</code> vs. <code>x</code>,
along with confidence intervals and half-interval for the difference in
medians as with <code>mbarclPanel</code>.  Quantile intervals are optional.
Very transparent vertical violin plots are added by default.  Unlike
<code>panel.violin</code>, only half of the violin is plotted, and when there
are two superpose groups they are side-by-side in different colors.
</p>
<p>For <code>plotp</code>, the function corresponding to <code>medvPanel</code> is
<code>medvpl</code>, which draws back-to-back spike histograms, optional Gini
mean difference, optional SD, quantiles (thin line version of box
plot with 0.05 0.25 0.5 0.75 0.95 quantiles), and half-width confidence
interval for differences in medians.  For quantiles, the Harrell-Davis
estimator is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summaryS(formula, fun = NULL, data = NULL, subset = NULL,
         na.action = na.retain, continuous=10, ...)

## S3 method for class 'summaryS'
plot(x, formula=NULL, groups=NULL, panel=NULL,
           paneldoesgroups=FALSE, datadensity=NULL, ylab='',
           funlabel=NULL, textonly='n', textplot=NULL,
           digits=3, custom=NULL,
           xlim=NULL, ylim=NULL, cex.strip=1, cex.values=0.5, pch.stats=NULL,
           key=list(columns=length(groupslevels),
             x=.75, y=-.04, cex=.9,
             col=lattice::trellis.par.get('superpose.symbol')$col,
             corner=c(0,1)),
           outerlabels=TRUE, autoarrange=TRUE, scat1d.opts=NULL, ...)

## S3 method for class 'summaryS'
plotp(data, formula=NULL, groups=NULL, sfun=NULL,
           fitter=NULL, showpts=! length(fitter), funlabel=NULL,
           digits=5, xlim=NULL, ylim=NULL,
           shareX=TRUE, shareY=FALSE, autoarrange=TRUE, ...)

mbarclPanel(x, y, subscripts, groups=NULL, yother, ...)

medvPanel(x, y, subscripts, groups=NULL, violin=TRUE, quantiles=FALSE, ...)

mbarclpl(x, y, groups=NULL, yother, yvar=NULL, maintracename='y',
         xlim=NULL, ylim=NULL, xname='x', alphaSegments=0.45, ...)

medvpl(x, y, groups=NULL, yvar=NULL, maintracename='y',
       xlim=NULL, ylim=NULL, xlab=xname, ylab=NULL, xname='x',
       zeroline=FALSE, yother=NULL, alphaSegments=0.45,
       dhistboxp.opts=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summaryS_+3A_formula">formula</code></td>
<td>
<p>a formula with possibly multiple left and right-side
variables separated by <code>+</code>.  Analysis (response) variables are
on the left and are typically numeric.  For <code>plot</code>,
<code>formula</code> is optional and overrides the default formula
inferred for the reshaped data frame.</p>
</td></tr>
<tr><td><code id="summaryS_+3A_fun">fun</code></td>
<td>
<p>an optional summarization function, e.g., <code><a href="#topic+smean.sd">smean.sd</a></code></p>
</td></tr>
<tr><td><code id="summaryS_+3A_data">data</code></td>
<td>
<p>optional input data frame.  For <code>plotp</code> is the object
produced by <code>summaryS</code>.</p>
</td></tr>
<tr><td><code id="summaryS_+3A_subset">subset</code></td>
<td>
<p>optional subsetting criteria</p>
</td></tr>
<tr><td><code id="summaryS_+3A_na.action">na.action</code></td>
<td>
<p>function for dealing with <code>NA</code>s when
constructing the model data frame</p>
</td></tr>
<tr><td><code id="summaryS_+3A_continuous">continuous</code></td>
<td>
<p>minimum number of unique values for a numeric
variable to have to be considered continuous</p>
</td></tr>
<tr><td><code id="summaryS_+3A_...">...</code></td>
<td>
<p>ignored for <code>summaryS</code> and <code>mbarclPanel</code>,
passed to <code>strip</code> and <code>panel</code> for <code>plot</code>.  Passed to
the <code><a href="stats.html#topic+density">density</a></code> function by <code>medvPanel</code>.  For
<code>plotp</code>, are passed to <code>plotlyM</code> and <code>sfun</code>.  For
<code>mbarclpl</code>, passed to <code>plotlyM</code>.</p>
</td></tr>
<tr><td><code id="summaryS_+3A_x">x</code></td>
<td>
<p>an object created by <code>summaryS</code>.  For <code>mbarclPanel</code>
is an <code>x</code>-axis argument provided by <code>lattice</code></p>
</td></tr>
<tr><td><code id="summaryS_+3A_groups">groups</code></td>
<td>
<p>a character string or factor specifying that one of the
conditioning variables is used for superpositioning and not
paneling</p>
</td></tr>
<tr><td><code id="summaryS_+3A_panel">panel</code></td>
<td>
<p>optional <code>lattice</code> <code>panel</code> function</p>
</td></tr>
<tr><td><code id="summaryS_+3A_paneldoesgroups">paneldoesgroups</code></td>
<td>
<p>set to <code>TRUE</code> if, like
<code><a href="#topic+panel.plsmo">panel.plsmo</a></code>, the paneling function internally
handles superpositioning for <code>groups</code></p>
</td></tr>
<tr><td><code id="summaryS_+3A_datadensity">datadensity</code></td>
<td>
<p>set to <code>TRUE</code> to add rug plots etc. using
<code><a href="#topic+scat1d">scat1d</a></code></p>
</td></tr>
<tr><td><code id="summaryS_+3A_ylab">ylab</code></td>
<td>
<p>optional <code>y</code>-axis label</p>
</td></tr>
<tr><td><code id="summaryS_+3A_funlabel">funlabel</code></td>
<td>
<p>optional axis label for when <code>fun</code> is given</p>
</td></tr>
<tr><td><code id="summaryS_+3A_textonly">textonly</code></td>
<td>
<p>names of statistics to print and not plot.  By
default, any statistic named <code>"n"</code> is only printed.</p>
</td></tr>
<tr><td><code id="summaryS_+3A_textplot">textplot</code></td>
<td>
<p>names of statistics to print and plot</p>
</td></tr>
<tr><td><code id="summaryS_+3A_digits">digits</code></td>
<td>
<p>used if any statistics are printed as text (including
<code>plotly</code> hovertext), to specify
the number of significant digits to render</p>
</td></tr>
<tr><td><code id="summaryS_+3A_custom">custom</code></td>
<td>
<p>a function that customizes formatting of statistics that
are printed as text.  This is useful for generating plotmath
notation.  See the example in the tests directory.</p>
</td></tr>
<tr><td><code id="summaryS_+3A_xlim">xlim</code></td>
<td>
<p>optional <code>x</code>-axis limits</p>
</td></tr>
<tr><td><code id="summaryS_+3A_ylim">ylim</code></td>
<td>
<p>optional <code>y</code>-axis limits</p>
</td></tr>			 
<tr><td><code id="summaryS_+3A_cex.strip">cex.strip</code></td>
<td>
<p>size of strip labels</p>
</td></tr>
<tr><td><code id="summaryS_+3A_cex.values">cex.values</code></td>
<td>
<p>size of statistics printed as text</p>
</td></tr>
<tr><td><code id="summaryS_+3A_pch.stats">pch.stats</code></td>
<td>
<p>symbols to use for statistics (not included the one
one in columne one) that are plotted.  This is a named
vectors, with names exactly matching those created by
<code>fun</code>.  When a column does not have an entry in
<code>pch.stats</code>, no point is drawn for that column.</p>
</td></tr>
<tr><td><code id="summaryS_+3A_key">key</code></td>
<td>
<p><code>lattice</code> <code>key</code> specification</p>
</td></tr>
<tr><td><code id="summaryS_+3A_outerlabels">outerlabels</code></td>
<td>
<p>set to <code>FALSE</code> to not pass two-way charts
through <code><a href="latticeExtra.html#topic+useOuterStrips">useOuterStrips</a></code></p>
</td></tr>
<tr><td><code id="summaryS_+3A_autoarrange">autoarrange</code></td>
<td>
<p>set to <code>FALSE</code> to prevent <code>plot</code> from
trying to optimize which conditioning variable is vertical</p>
</td></tr>
<tr><td><code id="summaryS_+3A_scat1d.opts">scat1d.opts</code></td>
<td>
<p>a list of options to specify to <code><a href="#topic+scat1d">scat1d</a></code></p>
</td></tr>
<tr><td><code id="summaryS_+3A_y">y</code>, <code id="summaryS_+3A_subscripts">subscripts</code></td>
<td>
<p>provided by <code>lattice</code></p>
</td></tr>
<tr><td><code id="summaryS_+3A_yother">yother</code></td>
<td>
<p>passed to the panel function from the <code>plot</code> method
based on multiple statistics computed</p>
</td></tr>
<tr><td><code id="summaryS_+3A_violin">violin</code></td>
<td>
<p>controls whether violin plots are included</p>
</td></tr>
<tr><td><code id="summaryS_+3A_quantiles">quantiles</code></td>
<td>
<p>controls whether quantile intervals are included</p>
</td></tr>
<tr><td><code id="summaryS_+3A_sfun">sfun</code></td>
<td>
<p>a function called by <code>plotp.summaryS</code> to compute and
plot user-specified summary measures.  Two functions for doing
this are provided here: <code>mbarclpl, medvpl</code>.</p>
</td></tr>
<tr><td><code id="summaryS_+3A_fitter">fitter</code></td>
<td>
<p>a fitting function such as <code>loess</code> to smooth
points.  The smoothed values over a systematic grid will be
evaluated and plotted as curves.</p>
</td></tr>
<tr><td><code id="summaryS_+3A_showpts">showpts</code></td>
<td>
<p>set to <code>TRUE</code> to show raw data points in additon
to smoothed curves</p>
</td></tr>
<tr><td><code id="summaryS_+3A_sharex">shareX</code></td>
<td>
<p><code>TRUE</code> to cause <code>plotly</code> to share a single
x-axis when graphs are aligned vertically</p>
</td></tr>
<tr><td><code id="summaryS_+3A_sharey">shareY</code></td>
<td>
<p><code>TRUE</code> to cause <code>plotly</code> to share a single
y-axis when graphs are aligned horizontally</p>
</td></tr>
<tr><td><code id="summaryS_+3A_yvar">yvar</code></td>
<td>
<p>a character or factor variable used to stratify the
analysis into multiple y-variables</p>
</td></tr>
<tr><td><code id="summaryS_+3A_maintracename">maintracename</code></td>
<td>
<p>a default trace name when it can't be inferred</p>
</td></tr>
<tr><td><code id="summaryS_+3A_xname">xname</code></td>
<td>
<p>x-axis variable name for hover text when it can't be
inferred</p>
</td></tr>
<tr><td><code id="summaryS_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label when it can't be inferred</p>
</td></tr>
<tr><td><code id="summaryS_+3A_alphasegments">alphaSegments</code></td>
<td>
<p>alpha saturation to draw line segments for
<code>plotly</code></p>
</td></tr>
<tr><td><code id="summaryS_+3A_dhistboxp.opts">dhistboxp.opts</code></td>
<td>
<p><code>list</code> of options to pass to <code>dhistboxp</code></p>
</td></tr>
<tr><td><code id="summaryS_+3A_zeroline">zeroline</code></td>
<td>
<p>set to <code>FALSE</code> to suppress <code>plotly</code> zero
line at x=0</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data frame with added attributes for <code>summaryS</code> or a
<code>lattice</code> object ready to render for <code>plot</code></p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+summary">summary</a></code>, <code><a href="#topic+summarize">summarize</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># See tests directory file summaryS.r for more examples, and summarySp.r
# for plotp examples
require(survival)
n &lt;- 100
set.seed(1)
d &lt;- data.frame(sbp=rnorm(n, 120, 10),
                dbp=rnorm(n, 80, 10),
                age=rnorm(n, 50, 10),
                days=sample(1:n, n, TRUE),
                S1=Surv(2*runif(n)), S2=Surv(runif(n)),
                race=sample(c('Asian', 'Black/AA', 'White'), n, TRUE),
                sex=sample(c('Female', 'Male'), n, TRUE),
                treat=sample(c('A', 'B'), n, TRUE),
                region=sample(c('North America','Europe'), n, TRUE),
                meda=sample(0:1, n, TRUE), medb=sample(0:1, n, TRUE))

d &lt;- upData(d, labels=c(sbp='Systolic BP', dbp='Diastolic BP',
            race='Race', sex='Sex', treat='Treatment',
            days='Time Since Randomization',
            S1='Hospitalization', S2='Re-Operation',
            meda='Medication A', medb='Medication B'),
            units=c(sbp='mmHg', dbp='mmHg', age='Year', days='Days'))

s &lt;- summaryS(age + sbp + dbp ~ days + region + treat,  data=d)
# plot(s)   # 3 pages
plot(s, groups='treat', datadensity=TRUE,
     scat1d.opts=list(lwd=.5, nhistSpike=0))
plot(s, groups='treat', panel=lattice::panel.loess,
     key=list(space='bottom', columns=2),
     datadensity=TRUE, scat1d.opts=list(lwd=.5))

# To make a plotly graph when the stratification variable region is not
# present, run the following (showpts adds raw data points):
# plotp(s, groups='treat', fitter=loess, showpts=TRUE)

# Make your own plot using data frame created by summaryP
# xyplot(y ~ days | yvar * region, groups=treat, data=s,
#        scales=list(y='free', rot=0))

# Use loess to estimate the probability of two different types of events as
# a function of time
s &lt;- summaryS(meda + medb ~ days + treat + region, data=d)
pan &lt;- function(...)
   panel.plsmo(..., type='l', label.curves=max(which.packet()) == 1,
               datadensity=TRUE)
plot(s, groups='treat', panel=pan, paneldoesgroups=TRUE,
     scat1d.opts=list(lwd=.7), cex.strip=.8)

# Repeat using intervals instead of nonparametric smoother
pan &lt;- function(...)  # really need mobs &gt; 96 to est. proportion
  panel.plsmo(..., type='l', label.curves=max(which.packet()) == 1,
              method='intervals', mobs=5)

plot(s, groups='treat', panel=pan, paneldoesgroups=TRUE, xlim=c(0, 150))


# Demonstrate dot charts of summary statistics
s &lt;- summaryS(age + sbp + dbp ~ region + treat, data=d, fun=mean)
plot(s)
plot(s, groups='treat', funlabel=expression(bar(X)))
# Compute parametric confidence limits for mean, and include sample
# sizes by naming a column "n"

f &lt;- function(x) {
  x &lt;- x[! is.na(x)]
  c(smean.cl.normal(x, na.rm=FALSE), n=length(x))
}
s &lt;- summaryS(age + sbp + dbp ~ region + treat, data=d, fun=f)
plot(s, funlabel=expression(bar(X) %+-% t[0.975] %*% s))
plot(s, groups='treat', cex.values=.65,
     key=list(space='bottom', columns=2,
       text=c('Treatment A:','Treatment B:')))

# For discrete time, plot Harrell-Davis quantiles of y variables across
# time using different line characteristics to distinguish quantiles
d &lt;- upData(d, days=round(days / 30) * 30)
g &lt;- function(y) {
  probs &lt;- c(0.05, 0.125, 0.25, 0.375)
  probs &lt;- sort(c(probs, 1 - probs))
  y &lt;- y[! is.na(y)]
  w &lt;- hdquantile(y, probs)
  m &lt;- hdquantile(y, 0.5, se=TRUE)
  se &lt;- as.numeric(attr(m, 'se'))
  c(Median=as.numeric(m), w, se=se, n=length(y))
}
s &lt;- summaryS(sbp + dbp ~ days + region, fun=g, data=d)
plot(s, panel=mbarclPanel)
plot(s, groups='region', panel=mbarclPanel, paneldoesgroups=TRUE)

# For discrete time, plot median y vs x along with CL for difference,
# using Harrell-Davis median estimator and its s.e., and use violin
# plots

s &lt;- summaryS(sbp + dbp ~ days + region, data=d)
plot(s, groups='region', panel=medvPanel, paneldoesgroups=TRUE)

# Proportions and Wilson confidence limits, plus approx. Gaussian
# based half/width confidence limits for difference in probabilities
g &lt;- function(y) {
  y &lt;- y[!is.na(y)]
  n &lt;- length(y)
  p &lt;- mean(y)
  se &lt;- sqrt(p * (1. - p) / n)
  structure(c(binconf(sum(y), n), se=se, n=n),
            names=c('Proportion', 'Lower', 'Upper', 'se', 'n'))
}
s &lt;- summaryS(meda + medb ~ days + region, fun=g, data=d)
plot(s, groups='region', panel=mbarclPanel, paneldoesgroups=TRUE)
</code></pre>

<hr>
<h2 id='symbol.freq'>Graphic Representation of a Frequency Table</h2><span id='topic+symbol.freq'></span>

<h3>Description</h3>

<p>This function can be used to represent
contingency tables graphically.  Frequency counts are represented as
the heights of &quot;thermometers&quot; by default; you can also specify
<code>symbol='circle'</code> to the function.  There is an option to include
marginal frequencies, which are plotted on a halved scale so as to not
overwhelm the plot.   If you do not ask for marginal frequencies to be
plotted using <code>marginals=T</code>, <code>symbol.freq</code> will ask you to click
the mouse where a reference symbol is to be drawn to assist in reading
the scale of the frequencies.
</p>
<p><code>label</code> attributes, if present, are used for x- and y-axis labels.
Otherwise, names of calling arguments are used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>symbol.freq(x, y, symbol = c("thermometer", "circle"),
            marginals = FALSE, orig.scale = FALSE,
            inches = 0.25, width = 0.15, subset, srtx = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="symbol.freq_+3A_x">x</code></td>
<td>
<p>first variable to cross-classify</p>
</td></tr>
<tr><td><code id="symbol.freq_+3A_y">y</code></td>
<td>
<p>second variable</p>
</td></tr>
<tr><td><code id="symbol.freq_+3A_symbol">symbol</code></td>
<td>
<p>specify <code>"thermometer"</code> (the default) or <code>"circle"</code></p>
</td></tr>
<tr><td><code id="symbol.freq_+3A_marginals">marginals</code></td>
<td>
<p>set to <code>TRUE</code> to add marginal frequencies
(scaled by half) to the plot</p>
</td></tr>
<tr><td><code id="symbol.freq_+3A_orig.scale">orig.scale</code></td>
<td>
<p>set to <code>TRUE</code> when the first two arguments are
numeric variables; this uses their original values for x and y
coordinates)</p>
</td></tr> 
<tr><td><code id="symbol.freq_+3A_inches">inches</code></td>
<td>
<p>see <code><a href="graphics.html#topic+symbols">symbols</a></code></p>
</td></tr>
<tr><td><code id="symbol.freq_+3A_width">width</code></td>
<td>
<p>see <code>thermometers</code> option in <code>symbols</code></p>
</td></tr>
<tr><td><code id="symbol.freq_+3A_subset">subset</code></td>
<td>
<p>the usual subsetting vector</p>
</td></tr>
<tr><td><code id="symbol.freq_+3A_srtx">srtx</code></td>
<td>
<p>rotation angle for x-axis labels</p>
</td></tr>
<tr><td><code id="symbol.freq_+3A_...">...</code></td>
<td>
<p>other arguments to pass to <code>symbols</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+symbols">symbols</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
getHdata(titanic)
attach(titanic)
age.tertile &lt;- cut2(titanic$age, g=3)
symbol.freq(age.tertile, pclass, marginals=T, srtx=45)
detach(2)

## End(Not run)</code></pre>

<hr>
<h2 id='sys'>
Run Unix or Dos Depending on System
</h2><span id='topic+sys'></span>

<h3>Description</h3>

<p>Runs <code>unix</code> or <code>dos</code> depending on the current operating system.  For
<span class="rlang"><b>R</b></span>, just runs <code>system</code> with optional concatenation of first two
arguments which are assumed named <code>command</code> and <code>text</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sys(command, text=NULL, output=TRUE)
# S-Plus: sys(\dots, minimized=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sys_+3A_command">command</code></td>
<td>

<p>system command to execute
</p>
</td></tr>
<tr><td><code id="sys_+3A_text">text</code></td>
<td>

<p>text to concatenate to system command, if any (typically options or file
names or both)
</p>
</td></tr>
<tr><td><code id="sys_+3A_output">output</code></td>
<td>

<p>set to <code>FALSE</code> to not return output of command as a character
vector
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>see <code>unix</code> or <code>dos</code>
</p>


<h3>Side Effects</h3>

<p>executes system commands
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+unix">unix</a></code>, <code><a href="base.html#topic+system">system</a></code>
</p>

<hr>
<h2 id='t.test.cluster'>t-test for Clustered Data</h2><span id='topic+t.test.cluster'></span><span id='topic+print.t.test.cluster'></span>

<h3>Description</h3>

<p>Does a 2-sample t-test for clustered data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>t.test.cluster(y, cluster, group, conf.int = 0.95)
## S3 method for class 't.test.cluster'
print(x, digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="t.test.cluster_+3A_y">y</code></td>
<td>
<p>normally distributed response variable to test</p>
</td></tr>
<tr><td><code id="t.test.cluster_+3A_cluster">cluster</code></td>
<td>
<p>cluster identifiers, e.g. subject ID</p>
</td></tr>
<tr><td><code id="t.test.cluster_+3A_group">group</code></td>
<td>
<p>grouping variable with two values</p>
</td></tr>
<tr><td><code id="t.test.cluster_+3A_conf.int">conf.int</code></td>
<td>
<p>confidence coefficient to use for confidence limits</p>
</td></tr>
<tr><td><code id="t.test.cluster_+3A_x">x</code></td>
<td>
<p>an object created by <code>t.test.cluster</code></p>
</td></tr>
<tr><td><code id="t.test.cluster_+3A_digits">digits</code></td>
<td>
<p>number of significant digits to print</p>
</td></tr>
<tr><td><code id="t.test.cluster_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix of statistics of class <code>t.test.cluster</code>
</p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>References</h3>

<p>Donner A, Birkett N, Buck C, Am J Epi 114:906-914, 1981.
</p>
<p>Donner A, Klar N, J Clin Epi 49:435-439, 1996.
</p>
<p>Hsieh FY, Stat in Med 8:1195-1201, 1988.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+t.test">t.test</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
y &lt;- rnorm(800)
group &lt;- sample(1:2, 800, TRUE)
cluster &lt;- sample(1:40, 800, TRUE)
table(cluster,group)
t.test(y ~ group)   # R only
t.test.cluster(y, cluster, group)
# Note: negate estimates of differences from t.test to
# compare with t.test.cluster
</code></pre>

<hr>
<h2 id='tabulr'>Interface to Tabular Function</h2><span id='topic+tabulr'></span><span id='topic+table_trio'></span><span id='topic+table_N'></span><span id='topic+table_freq'></span><span id='topic+table_pc'></span><span id='topic+table_latexdefs'></span><span id='topic+table_formatpct'></span><span id='topic+nFm'></span>

<h3>Description</h3>

<p><code><a href="#topic+tabulr">tabulr</a></code> is a front-end to the <code>tables</code> package's
<code><a href="tables.html#topic+tabular">tabular</a></code> function so that the user can take
advantage of variable annotations used by the <code>Hmisc</code> package,
particular those created by the <code><a href="#topic+label">label</a></code>, <code><a href="#topic+units">units</a></code>, and
<code><a href="#topic+upData">upData</a></code> functions.  When a variable appears in a
<code><a href="tables.html#topic+tabular">tabular</a></code> function, the 
variable <code>x</code> is found in the <code>data</code> argument or in the parent
environment, and the <code><a href="#topic+labelLatex">labelLatex</a></code> function is used to create
a LaTeX label.  By default any units of measurement are right justified
in the current LaTeX tabular field using <code>hfill</code>; use <code>nofill</code>
to list variables for which <code>units</code> are not right-justified with
<code>hfill</code>.  Once the label is constructed, the variable name is
preceeded by <code>Heading("LaTeX label")*x</code> in the formula before it is
passed to <code><a href="tables.html#topic+tabular">tabular</a></code>.  <code>nolabel</code> can be used to
specify variables for which labels are ignored.
</p>
<p><code>tabulr</code> also replaces <code>trio</code> with <code>table_trio</code>, <code>N</code>
with <code>table_N</code>,  and <code>freq</code> with <code>table_freq</code> in the
formula.  
</p>
<p><code>table_trio</code> is a function that takes a numeric vector and computes
the three quartiles and optionally the mean and standard deviation, and
outputs a LaTeX-formatted character string representing the results.  By
default, calculated statistics are formatted with 3 digits to the left
and 1 digit to the right of the decimal point.  Running
<code><a href="tables.html#topic+table_options">table_options</a>(left=l, right=r)</code> will use <code>l</code>
and <code>r</code> digits instead.  Other options that can be given to
<code>table_options</code> are <code>prmsd=TRUE</code> to add mean +/- standard
deviation to the result, <code>pn=TRUE</code> to add the sample size,
<code>bold=TRUE</code> to set the median in bold face, <code>showfreq='all',
	'low', 'high'</code> used by the <code>table_freq</code> function, <code>pctdec</code>,
specifying the number of places to the right of the decimal point for
percentages (default is zero), and
<code>npct='both','numerator','denominator','none'</code> used by
<code>table_formatpct</code> to control what appears after the percent.
Option <code>pnformat</code> may be specified to control the formatting for
<code>pn</code>.  The default is <code>"(n=..)"</code>.  Specify
<code>pnformat="non"</code> to suppress <code>"n="</code>.  <code>pnwhen</code> specifies
when to print the number of observations.  The default is
<code>"always"</code>.  Specify <code>pnwhen="ifna"</code> to include <code>n</code> only
if there are missing values in the vector being processed.
</p>
<p><code>tabulr</code> substitutes <code>table_N</code> for <code>N</code> in the formula.
This is used to create column headings for the number of observations,
without a row label.
</p>
<p><code>table_freq</code> analyzes a character variable to compute, for a single
output cell, the percents, numerator, and denominator for each category,
or optimally just the maximum or minimum, as specified by
<code>table_options(showfreq)</code>. 
</p>
<p><code>table_formatpct</code> is a function that formats percents depending on
settings of options in <code>table_options</code>.
</p>
<p><code>nFm</code> is a function that calls <code><a href="base.html#topic+sprintf">sprintf</a></code> to format
numeric values to have a specific number of digits to the <code>left</code>
and to the <code>right</code> of the point.
</p>
<p><code>table_latexdefs</code> writes (by default) to the console a set of LaTeX
definitions that can be invoked at any point thereafter in a <code>knitr</code> or
<code>sweave</code> document by naming the macro, preceeded by a single
slash.  The <code>blfootnote</code> macro is called with a single LaTeX
argument which will appear as a footnote without a number.
<code>keytrio</code> invokes <code>blfootnote</code> to define the output of
<code>table_trio</code> if mean and SD are not included.  If mean and SD are
included, use <code>keytriomsd</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tabulr(formula, data = NULL, nolabel=NULL, nofill=NULL, ...)
table_trio(x)
table_freq(x)
table_formatpct(num, den)
nFm(x, left, right, neg=FALSE, pad=FALSE, html=FALSE)
table_latexdefs(file='')
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tabulr_+3A_formula">formula</code></td>
<td>
<p>a formula suitable for <code><a href="tables.html#topic+tabular">tabular</a></code>
except for the addition of <code>.(variable name)</code>,
<code>.n()</code>, <code>trio</code>.</p>
</td></tr>
<tr><td><code id="tabulr_+3A_data">data</code></td>
<td>
<p>a data frame or list.  If omitted, the parent environment
is assumed to contain the variables.</p>
</td></tr>
<tr><td><code id="tabulr_+3A_nolabel">nolabel</code></td>
<td>
<p>a formula such as <code>~ x1 + x2</code> containing the list
of variables for which labels are to be ignored, forcing use of the
variable name</p>
</td></tr>
<tr><td><code id="tabulr_+3A_nofill">nofill</code></td>
<td>
<p>a formula such as <code>~ x1 + x2</code> contaning the list of
variables for which units of measurement are not to be
right-justified in the field using the LaTeX <code>hfill</code> directive</p>
</td></tr>
<tr><td><code id="tabulr_+3A_...">...</code></td>
<td>
<p>other arguments to <code>tabular</code></p>
</td></tr>
<tr><td><code id="tabulr_+3A_x">x</code></td>
<td>
<p>a numeric vector</p>
</td></tr>
<tr><td><code id="tabulr_+3A_num">num</code></td>
<td>
<p>a single numerator or vector of numerators</p>
</td></tr>
<tr><td><code id="tabulr_+3A_den">den</code></td>
<td>
<p>a single denominator</p>
</td></tr>
<tr><td><code id="tabulr_+3A_left">left</code>, <code id="tabulr_+3A_right">right</code></td>
<td>
<p>number of places to the left and right of the
decimal point, respectively</p>
</td></tr>
<tr><td><code id="tabulr_+3A_neg">neg</code></td>
<td>
<p>set to <code>TRUE</code> if negative <code>x</code> values are allowed,
to add one more space to the left of the decimal place</p>
</td></tr>
<tr><td><code id="tabulr_+3A_pad">pad</code></td>
<td>
<p>set to <code>TRUE</code> to replace blanks with the LaTeX tilde
placeholder</p>
</td></tr>
<tr><td><code id="tabulr_+3A_html">html</code></td>
<td>
<p>set to <code>TRUE</code> to make <code>pad</code> use an HTML space
character instead of a LaTeX tilde space</p>
</td></tr>
<tr><td><code id="tabulr_+3A_file">file</code></td>
<td>
<p>location of output of <code>table_latexdefs</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>tabulr</code> returns an object of class <code>"tabular"</code></p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="tables.html#topic+tabular">tabular</a></code>, <code><a href="#topic+label">label</a></code>,
<code><a href="#topic+latex">latex</a></code>, <code><a href="#topic+summaryM">summaryM</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
n &lt;- 400
set.seed(1)
d &lt;- data.frame(country=factor(sample(c('US','Canada','Mexico'), n, TRUE)),
                sex=factor(sample(c('Female','Male'), n, TRUE)),
                age=rnorm(n, 50, 10),
                sbp=rnorm(n, 120, 8))
d &lt;- upData(d,
            preghx=ifelse(sex=='Female', sample(c('No','Yes'), n, TRUE), NA),
            labels=c(sbp='Systolic BP', age='Age', preghx='Pregnancy History'),
            units=c(sbp='mmHg', age='years'))
contents(d)
require(tables)
invisible(booktabs())  # use booktabs LaTeX style for tabular
g &lt;- function(x) {
  x &lt;- x[!is.na(x)]
  if(length(x) == 0) return('')
  paste(latexNumeric(nFm(mean(x), 3, 1)),
        ' \hfill{\smaller[2](', length(x), ')}', sep='')
}
tab &lt;- tabulr((age + Heading('Females')*(sex == 'Female')*sbp)*
              Heading()*g + (age + sbp)*Heading()*trio ~ 
              Heading()*country*Heading()*sex, data=d)
# Formula after interpretation by tabulr:
# (Heading('Age\hfill {\smaller[2] years}') * age + Heading("Females")
# * (sex == "Female") * Heading('Systolic BP {\smaller[2] mmHg}') * sbp)
# * Heading() * g + (age + sbp) * Heading() * table_trio ~ Heading()
# * country * Heading() * sex
cat('\begin{landscape}\n')
cat('\begin{minipage}{\textwidth}\n')
cat('\keytrio\n')
latex(tab)
cat('\end{minipage}\end{landscape}\n')

getHdata(pbc)
pbc &lt;- upData(pbc, moveUnits=TRUE)
# Convert to character to prevent tabular from stratifying
for(x in c('sex', 'stage', 'spiders')) {
  pbc[[x]] &lt;- as.character(pbc[[x]])
  label(pbc[[x]]) &lt;- paste(toupper(substring(x, 1, 1)), substring(x, 2), sep='')
}
table_options(pn=TRUE, showfreq='all')
tab &lt;- tabulr((bili + albumin + protime + age) *
              Heading()*trio +
              (sex + stage + spiders)*Heading()*freq ~ drug, data=pbc)
latex(tab)

## End(Not run)
</code></pre>

<hr>
<h2 id='testCharDateTime'>testCharDateTime</h2><span id='topic+testCharDateTime'></span>

<h3>Description</h3>

<p>Test Character Variables for Dates and Times
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testCharDateTime(x, p = 0.5, m = 0, convert = FALSE, existing = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="testCharDateTime_+3A_x">x</code></td>
<td>
<p>input vector of any type, but interesting cases are for character <code>x</code></p>
</td></tr>
<tr><td><code id="testCharDateTime_+3A_p">p</code></td>
<td>
<p>minimum proportion of non-missing non-blank values of <code>x</code> for which the format is one of the formats described before considering <code>x</code> to be of that type</p>
</td></tr>
<tr><td><code id="testCharDateTime_+3A_m">m</code></td>
<td>
<p>if greater than 0, a test is applied: the number of distinct illegal values of <code>x</code> (values containing a letter or underscore) must not exceed <code>m</code>, or type <code>character</code> will be returned.  <code>p</code> is set to <code>1.0</code> when <code>m</code> &gt; 0.</p>
</td></tr>
<tr><td><code id="testCharDateTime_+3A_convert">convert</code></td>
<td>
<p>set to <code>TRUE</code> to convert the variable under the dominant format.  If all values are <code>NA</code>, <code>type</code> will be set to <code>'character'</code>.</p>
</td></tr>
<tr><td><code id="testCharDateTime_+3A_existing">existing</code></td>
<td>
<p>set to <code>TRUE</code> to return a character string with the current type of variable without examining pattern matches</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a vector <code>x</code>, if it is already a date-time, date, or time variable, the type is returned if <code>convert=FALSE</code>, or a list with that type, the original vector, and <code>numna=0</code> is returned.  Otherwise if <code>x</code> is not a character vector, a type of <code>notcharacter</code> is returned, or a list that includes the original <code>x</code> and <code>type='notcharacter'</code>.  When <code>x</code> is character, the main logic is applied.  The default logic (when <code>m=0</code>) is to consider <code>x</code> a date-time variable when its format is YYYY-MM-DD HH:MM:SS (:SS is optional) in more than 1/2 of the non-missing observations.  It is considered to be a date if its format is YYYY-MM-DD or MM/DD/YYYY or DD-MMM-YYYY in more than 1/2 of the non-missing observations (MMM=3-letter month).  A time variable has the format HH:MM:SS or HH:MM.  Blank values of <code>x</code> (after trimming) are set to <code>NA</code> before proceeding.
</p>


<h3>Value</h3>

<p>if <code>convert=FALSE</code>, a single character string with the type of <code>x</code>: <code style="white-space: pre;">&#8288;"character", "datetime", "date", "time"&#8288;</code>.  If <code>convert=TRUE</code>, a list with components named <code>type</code>, <code>x</code> (converted to <code>POSIXct</code>, <code>Date</code>, or <code>chron</code> times format), and <code>numna</code>, the number of originally non-<code>NA</code> values of <code>x</code> that could not be converted to the predominant format.  If there were any non-covertible dates/times,
the returned vector is given an additional class <code>special.miss</code> and an
attribute <code>special.miss</code> which is a list with original character values
(<code>codes</code>) and observation numbers (<code>obs</code>).  These are summarized by
<code>describe()</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>Examples</h3>

<pre><code class='language-R'>for(conv in c(FALSE, TRUE)) {
  print(testCharDateTime(c('2023-03-11', '2023-04-11', 'a', 'b', 'c'), convert=conv))
  print(testCharDateTime(c('2023-03-11', '2023-04-11', 'a', 'b'), convert=conv))
  print(testCharDateTime(c('2023-03-11 11:12:13', '2023-04-11 11:13:14', 'a', 'b'), convert=conv))
  print(testCharDateTime(c('2023-03-11 11:12', '2023-04-11 11:13', 'a', 'b'), convert=conv))
  print(testCharDateTime(c('3/11/2023', '4/11/2023', 'a', 'b'), convert=conv))
}
x &lt;- c(paste0('2023-03-0', 1:9), 'a', 'a', 'a', 'b')
y &lt;- testCharDateTime(x, convert=TRUE)$x
describe(y)  # note counts of special missing values a, b
</code></pre>

<hr>
<h2 id='tex'>
function for use in graphs that are used with the psfrag package in LaTeX
</h2><span id='topic+tex'></span>

<h3>Description</h3>

<p><code>tex</code> is a little function to save typing when including TeX
commands in graphs that are used with the psfrag package in LaTeX to
typeset any LaTeX text inside a postscript graphic.  <code>tex</code>
surrounds the input character string with &lsquo;<span class="samp">&#8288;\tex[options]{}&#8288;</span>&rsquo;.
This is especially useful for getting Greek letters and math symbols
in postscript graphs.  By default <code>tex</code> returns a string with
<code>psfrag</code> commands specifying that the string be centered, not
rotated, and not specially enlarged or shrunk.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tex(string, lref='c', psref='c', scale=1, srt=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tex_+3A_string">string</code></td>
<td>

<p>a character string to be processed by <code>psfrag</code> in LaTeX.
</p>
</td></tr>
<tr><td><code id="tex_+3A_lref">lref</code></td>
<td>

<p>LaTeX reference point for <code>string</code>.  See the <code>psfrag</code>
documentation referenced below.  Default is <code>"c"</code> for centered
(this is also the default for <code>psref</code>).
</p>
</td></tr>
<tr><td><code id="tex_+3A_psref">psref</code></td>
<td>

<p>PostScript reference point.
</p>
</td></tr>
<tr><td><code id="tex_+3A_scale">scale</code></td>
<td>

<p>scall factor, default is 1
</p>
</td></tr>
<tr><td><code id="tex_+3A_srt">srt</code></td>
<td>

<p>rotation for <code>string</code> in degrees (default is zero)
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>tex</code> returns a modified character string.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell<br />
Department of Biostatistics<br />
Vanderbilt University<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Grant MC, Carlisle (1998): The PSfrag System, Version 3.  Full
documentation is obtained by searching www.ctan.org for &lsquo;<span class="file">pfgguide.ps</span>&rsquo;.
</p>


<h3>See Also</h3>

<p><code><a href="grDevices.html#topic+postscript">postscript</a></code>, <code><a href="graphics.html#topic+par">par</a></code>, <code><a href="grDevices.html#topic+ps.options">ps.options</a></code>,
<code><a href="#topic+mgp.axis.labels">mgp.axis.labels</a></code>, <code><a href="grDevices.html#topic+pdf">pdf</a></code>,
<code><a href="lattice.html#topic+trellis.device">trellis.device</a></code>, <code><a href="#topic+setTrellis">setTrellis</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
pdf('test.pdf')
x &lt;- seq(0,15,length=100)
plot(x, dchisq(x, 5), xlab=tex('$x$'),
        ylab=tex('$f(x)$'), type='l')
title(tex('Density Function of the $\chi_{5}^{2}$ Distribution'))
dev.off()
# To process this file in LaTeX do something like
#\documentclass{article}
#\usepackage[scanall]{psfrag}
#\begin{document}
#\begin{figure}
#\includegraphics{test.ps}
#\caption{This is an example}
#\end{figure}
#\end{document}

## End(Not run)
</code></pre>

<hr>
<h2 id='transace'>
Additive Regression and Transformations using ace or avas
</h2><span id='topic+transace'></span><span id='topic+ggplot.transace'></span><span id='topic+print.transace'></span><span id='topic+areg.boot'></span><span id='topic+print.areg.boot'></span><span id='topic+plot.areg.boot'></span><span id='topic+predict.areg.boot'></span><span id='topic+summary.areg.boot'></span><span id='topic+print.summary.areg.boot'></span><span id='topic+Function.areg.boot'></span><span id='topic+Mean'></span><span id='topic+Mean.areg.boot'></span><span id='topic+Quantile'></span><span id='topic+Quantile.areg.boot'></span><span id='topic+monotone'></span><span id='topic+smearingEst'></span>

<h3>Description</h3>

<p><code>transace</code> is <code><a href="acepack.html#topic+ace">ace</a></code> packaged for easily automatically
transforming all variables in a formula without a left-hand side.
<code>transace</code> is a fast 
one-iteration version of <code><a href="#topic+transcan">transcan</a></code> without imputation of
<code>NA</code>s.  The <code>ggplot</code> method makes nice transformation plots
using <code>ggplot2</code>.  Binary variables are automatically kept linear,
and character or factor variables are automatically treated as categorical.
</p>
<p><code>areg.boot</code> uses <code><a href="#topic+areg">areg</a></code> or
<code><a href="acepack.html#topic+avas">avas</a></code> to fit additive regression models allowing
all variables in the model (including the left-hand-side) to be
transformed, with transformations chosen so as to optimize certain
criteria.  The default method uses <code><a href="#topic+areg">areg</a></code> whose goal it is
to maximize <code class="reqn">R^2</code>. <code>method="avas"</code> explicity tries to
transform the response variable so as to stabilize the variance of the
residuals. All-variables-transformed models tend to inflate <code>R^2</code>
and it can be difficult to get confidence limits for each
transformation. <code>areg.boot</code> solves both of these problems using
the bootstrap.  As with the <code><a href="rms.html#topic+validate">validate</a></code> function in the
<span class="pkg">rms</span> library, the Efron bootstrap is used to estimate the
optimism in the apparent <code class="reqn">R^2</code>, and this optimism is subtracted
from the apparent <code class="reqn">R^2</code> to optain a bias-corrected <code class="reqn">R^2</code>.
This is done however on the transformed response variable scale.
</p>
<p>Tests with 3 predictors show that the <code><a href="acepack.html#topic+avas">avas</a></code> and
<code><a href="acepack.html#topic+ace">ace</a></code> estimates are unstable unless the sample size
exceeds 350.  Apparent <code class="reqn">R^2</code> with low sample sizes can be very
inflated, and bootstrap estimates of <code class="reqn">R^2</code> can be even more
unstable in such cases, resulting in optimism-corrected <code class="reqn">R^2</code> that
are much lower even than the actual <code class="reqn">R^2</code>.  The situation can be
improved a little by restricting predictor transformations to be
monotonic.  On the other hand, the <code>areg</code> approach allows one to
control overfitting by specifying the number of knots to use for each
continuous variable in a restricted cubic spline function.
</p>
<p>For <code>method="avas"</code> the response transformation is restricted to
be monotonic.  You can specify restrictions for transformations of
predictors (and linearity for the response).  When the first argument
is a formula, the function automatically determines which variables
are categorical (i.e., <code>factor</code>, <code>category</code>, or character
vectors).  Specify linear transformations by enclosing variables by
the identify function (<code>I()</code>), and specify monotonicity by using
<code>monotone(variable)</code>.  Monotonicity restrictions are not
allowed with <code>method="areg"</code>.
</p>
<p>The <code><a href="base.html#topic+summary">summary</a></code> method for <code>areg.boot</code> computes
bootstrap estimates of standard errors of differences in predicted
responses (usually on the original scale) for selected levels of each
predictor against the lowest level of the predictor.  The smearing
estimator (see below) can be used here to estimate differences in
predicted means, medians, or many other statistics.  By default,
quartiles are used for continuous predictors and all levels are used
for categorical ones.  See <cite>Details</cite> below.  There is also a
<code><a href="base.html#topic+plot">plot</a></code> method for plotting transformation estimates,
transformations for individual bootstrap re-samples, and pointwise
confidence limits for transformations. Unless you already have a
<code>par(mfrow=)</code> in effect with more than one row or column,
<code>plot</code> will try to fit the plots on one page.  A
<code><a href="stats.html#topic+predict">predict</a></code> method computes predicted values on the original
or transformed response scale, or a matrix of transformed
predictors. There is a <code><a href="#topic+Function">Function</a></code> method for producing a
list of <span class="rlang"><b>R</b></span> functions that perform the final fitted transformations.
There is also a <code><a href="base.html#topic+print">print</a></code> method for <code>areg.boot</code>
objects.
</p>
<p>When estimated means (or medians or other statistical parameters) are
requested for models fitted with <code>areg.boot</code> (by
<code>summary.areg.boot</code> or <code>predict.areg.boot</code>), the
&ldquo;smearing&rdquo; estimator of <cite>Duan (1983)</cite> is used.  Here we
estimate the mean of the untransformed response by computing the
arithmetic mean of <code class="reqn">ginverse(lp + residuals)</code>,
where ginverse is the inverse of the nonparametric
transformation of the response (obtained by reverse linear
interpolation), lp is the linear predictor for an individual
observation on the transformed scale, and residuals is the
entire vector of residuals estimated from the fitted model, on the
transformed scales (n residuals for n original observations).  The
<code>smearingEst</code> function computes the general smearing estimate.
For efficiency <code>smearingEst</code> recognizes that quantiles are
transformation-preserving, i.e., when one wishes to estimate a
quantile of the untransformed distribution one just needs to compute
the inverse transformation of the transformed estimate after the
chosen quantile of the vector of residuals is added to it. When the
median is desired, the estimate is
<code class="reqn">ginverse(lp + \mbox{median}(residuals))</code>.
See the last example for how <code>smearingEst</code> can be used outside of
<code>areg.boot</code>.
</p>
<p><code>Mean</code> is a generic function that returns an <span class="rlang"><b>R</b></span> function to
compute the estimate of the mean of a variable.  Its input is
typically some kind of model fit object.  Likewise, <code>Quantile</code> is
a generic quantile function-producing function.  <code>Mean.areg.boot</code>
and <code>Quantile.areg.boot</code> create functions of a vector of linear
predictors that transform them into the smearing estimates of the mean
or quantile of the response variable,
respectively. <code>Quantile.areg.boot</code> produces exactly the same
value as <code>predict.areg.boot</code> or <code>smearingEst</code>.  <code>Mean</code>
approximates the mapping of linear predictors to means over an evenly
spaced grid of by default 200 points.  Linear interpolation is used
between these points.  This approximate method is much faster than the
full smearing estimator once <code>Mean</code> creates the function.  These
functions are especially useful in <code><a href="rms.html#topic+nomogram">nomogram</a></code> (see the
example on hypothetical data).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transace(formula, trim=0.01, data=environment(formula))

## S3 method for class 'transace'
print(x, ...)

## S3 method for class 'transace'
ggplot(data, mapping, ..., environment, nrow=NULL)

areg.boot(x, data, weights, subset, na.action=na.delete, 
          B=100, method=c("areg","avas"), nk=4, evaluation=100, valrsq=TRUE, 
          probs=c(.25,.5,.75), tolerance=NULL)

## S3 method for class 'areg.boot'
print(x, ...)

## S3 method for class 'areg.boot'
plot(x, ylim, boot=TRUE, col.boot=2, lwd.boot=.15,
     conf.int=.95, ...)

smearingEst(transEst, inverseTrans, res,
            statistic=c('median','quantile','mean','fitted','lp'),
            q)

## S3 method for class 'areg.boot'
summary(object, conf.int=.95, values, adj.to,
        statistic='median', q, ...)

## S3 method for class 'summary.areg.boot'
print(x, ...)

## S3 method for class 'areg.boot'
predict(object, newdata,
        statistic=c("lp", "median",
                    "quantile", "mean", "fitted", "terms"),
        q=NULL, ...) 

## S3 method for class 'areg.boot'
Function(object, type=c('list','individual'),
         ytype=c('transformed','inverse'),
         prefix='.', suffix='', pos=-1, ...)

Mean(object, ...)

Quantile(object, ...)

## S3 method for class 'areg.boot'
Mean(object, evaluation=200, ...)

## S3 method for class 'areg.boot'
Quantile(object, q=.5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="transace_+3A_formula">formula</code></td>
<td>
<p>a formula without a left-hand-side variable.  Variables
may be enclosed in <code>monotone(), linear(), categorical()</code> to
make certain assumptions about transformations.  <code>categorical</code>
and <code>linear</code> need not be specified if they can be summized from
the variable values.</p>
</td></tr>
<tr><td><code id="transace_+3A_x">x</code></td>
<td>

<p>for <code>areg.boot</code> <code>x</code> 
is a formula.  For <code>print</code> or <code>plot</code>, an object created by
<code>areg.boot</code> or <code>transace</code>.  For
<code>print.summary.areg.boot</code>, and object 
created by <code>summary.areg.boot</code>.  For <code>ggplot</code> is
the result of <code>transace</code>.
</p>
</td></tr>
<tr><td><code id="transace_+3A_object">object</code></td>
<td>

<p>an object created by <code>areg.boot</code>, or a model fit object
suitable for <code>Mean</code> or <code>Quantile</code>.
</p>
</td></tr>
<tr><td><code id="transace_+3A_transest">transEst</code></td>
<td>

<p>a vector of transformed values.  In log-normal regression these
could be predicted log(Y) for example.
</p>
</td></tr>
<tr><td><code id="transace_+3A_inversetrans">inverseTrans</code></td>
<td>

<p>a function specifying the inverse transformation needed to change
<code>transEst</code> to the original untransformed scale.
<code>inverseTrans</code> may also be a 2-element list defining a mapping
from the transformed values to untransformed values.  Linear
interpolation is used in this case to obtain untransform values.
</p>
</td></tr>
<tr><td><code id="transace_+3A_trim">trim</code></td>
<td>
<p>quantile to which to trim original and transformed values
for continuous variables for purposes of plotting the
transformations with <code>ggplot.transace</code></p>
</td></tr>
<tr><td><code id="transace_+3A_nrow">nrow</code></td>
<td>
<p>the number of rows to graph for <code>transace</code>
transformations, with the default chosen by <code>ggplot2</code></p>
</td></tr>
<tr><td><code id="transace_+3A_data">data</code></td>
<td>

<p>data frame to use if <code>x</code> is a formula and variables are not
already in the search list.  For <code>ggplot</code> is a <code>transace</code> object.
</p>
</td></tr>
<tr><td><code id="transace_+3A_environment">environment</code>, <code id="transace_+3A_mapping">mapping</code></td>
<td>
<p>ignored</p>
</td></tr>
<tr><td><code id="transace_+3A_weights">weights</code></td>
<td>

<p>a numeric vector of observation weights.  By default, all
observations are weighted equally.
</p>
</td></tr>
<tr><td><code id="transace_+3A_subset">subset</code></td>
<td>

<p>an expression to subset data if <code>x</code> is a formula
</p>
</td></tr>
<tr><td><code id="transace_+3A_na.action">na.action</code></td>
<td>

<p>a function specifying how to handle <code>NA</code>s.  Default is <code><a href="#topic+na.delete">na.delete</a></code>.
</p>
</td></tr>
<tr><td><code id="transace_+3A_b">B</code></td>
<td>

<p>number of bootstrap samples (default=100)
</p>
</td></tr>
<tr><td><code id="transace_+3A_method">method</code></td>
<td>

<p><code>"areg"</code> (the default) or <code>"avas"</code>
</p>
</td></tr>
<tr><td><code id="transace_+3A_nk">nk</code></td>
<td>

<p>number of knots for continuous variables not restricted to be
linear.  Default is 4.  One or two is not allowed.  <code>nk=0</code>
forces linearity for all continuous variables.
</p>
</td></tr>
<tr><td><code id="transace_+3A_evaluation">evaluation</code></td>
<td>

<p>number of equally-spaced points at which to evaluate (and save) the
nonparametric transformations derived by <code><a href="acepack.html#topic+avas">avas</a></code> or
<code><a href="acepack.html#topic+ace">ace</a></code>.  Default is 100.  For <code>Mean.areg.boot</code>,
<code>evaluation</code> is the number of points at which to evaluate exact
smearing estimates, to approximate them using linear interpolation
(default is 200).
</p>
</td></tr>
<tr><td><code id="transace_+3A_valrsq">valrsq</code></td>
<td>

<p>set to <code>TRUE</code> to more quickly do bootstrapping without
validating <code class="reqn">R^2</code>
</p>
</td></tr>
<tr><td><code id="transace_+3A_probs">probs</code></td>
<td>

<p>vector probabilities denoting the quantiles of continuous predictors
to use in estimating effects of those predictors
</p>
</td></tr>
<tr><td><code id="transace_+3A_tolerance">tolerance</code></td>
<td>

<p>singularity criterion; list source code for the
<code><a href="#topic+lm.fit.qr.bare">lm.fit.qr.bare</a></code> function.
</p>
</td></tr>
<tr><td><code id="transace_+3A_res">res</code></td>
<td>

<p>a vector of residuals from the transformed model.  Not required when
<code>statistic="lp"</code> or <code>statistic="fitted"</code>.
</p>
</td></tr>
<tr><td><code id="transace_+3A_statistic">statistic</code></td>
<td>

<p>statistic to estimate with the smearing estimator.  For
<code>smearingEst</code>, the default results in computation of the sample
median of the model residuals, then <code>smearingEst</code> adds the
median residual and back-transforms to get estimated median
responses on the original scale.  <code>statistic="lp"</code> causes
predicted transformed responses to be computed.  For
<code>smearingEst</code>, the result (for <code>statistic="lp"</code>) is the
input argument <code>transEst</code>.  <code>statistic="fitted"</code> gives
predicted untransformed responses, i.e.,
<code class="reqn">ginverse(lp)</code>, where ginverse is the inverse
of the estimated response transformation, estimated by reverse
linear interpolation on the tabulated nonparametric response
transformation or by using an explicit analytic
function. <code>statistic="quantile"</code> generalizes <code>"median"</code> to
any single quantile <code>q</code> which must be specified.  <code>"mean"</code>
causes the population mean response to be estimated.  For
<code>predict.areg.boot</code>, <code>statistic="terms"</code> returns a matrix
of transformed predictors. <code>statistic</code> can also be any <span class="rlang"><b>R</b></span>
function that computes a single value on a vector of values, such as
<code>statistic=var</code>.  Note that in this case the function name is
not quoted.
</p>
</td></tr>
<tr><td><code id="transace_+3A_q">q</code></td>
<td>

<p>a single quantile of the original response scale to estimate, when
<code>statistic="quantile"</code>, or for <code>Quantile.areg.boot</code>.
</p>
</td></tr>
<tr><td><code id="transace_+3A_ylim">ylim</code></td>
<td>

<p>2-vector of y-axis limits
</p>
</td></tr>
<tr><td><code id="transace_+3A_boot">boot</code></td>
<td>

<p>set to <code>FALSE</code> to not plot any bootstrapped transformations.
Set it to an integer k to plot the first k bootstrap
estimates.
</p>
</td></tr>
<tr><td><code id="transace_+3A_col.boot">col.boot</code></td>
<td>

<p>color for bootstrapped transformations
</p>
</td></tr>
<tr><td><code id="transace_+3A_lwd.boot">lwd.boot</code></td>
<td>

<p>line width for bootstrapped transformations
</p>
</td></tr>
<tr><td><code id="transace_+3A_conf.int">conf.int</code></td>
<td>

<p>confidence level (0-1) for pointwise bootstrap confidence limits and
for estimated effects of predictors in <code>summary.areg.boot</code>. The
latter assumes normality of the estimated effects.
</p>
</td></tr>
<tr><td><code id="transace_+3A_values">values</code></td>
<td>

<p>a list of vectors of settings of the predictors, for predictors for
which you want to overide settings determined from <code>probs</code>.
The list must have named components, with names corresponding to the
predictors.  Example:
<code>values=list(x1=c(2,4,6,8), x2=c(-1,0,1))</code> specifies that
<code>summary</code> is to estimate the effect on <code>y</code> of changing
<code>x1</code> from 2 to 4, 2 to 6, 2 to 8, and separately, of changing
<code>x2</code> from -1 to 0 and -1 to 1.  
</p>
</td></tr>
<tr><td><code id="transace_+3A_adj.to">adj.to</code></td>
<td>

<p>a named vector of adjustment constants, for setting all other
predictors when examining the effect of a single predictor in
<code>summary</code>.  The more nonlinear is the transformation of
<code>y</code> the more the adjustment settings will matter.  Default
values are the medians of the values defined by <code>values</code> or
<code>probs</code>.  You only need to name the predictors for which you
are overriding the default settings. Example:
<code>adj.to=c(x2=0,x5=10)</code> will set <code>x2</code> to 0 and <code>x5</code> to
10 when assessing the impact of variation in the other predictors.
</p>
</td></tr>
<tr><td><code id="transace_+3A_newdata">newdata</code></td>
<td>

<p>a data frame or list containing the same number of values of all of
the predictors used in the fit.  For <code><a href="base.html#topic+factor">factor</a></code> predictors
the &lsquo;<span class="samp">&#8288;levels&#8288;</span>&rsquo; attribute do not need to be in the same order as
those used in the original fit, and not all levels need to be
represented.  If <code>newdata</code> is omitted, you can still obtain
linear predictors (on the transformed response scale) and fitted
values (on the original response scale), but not <code>"terms"</code>.
</p>
</td></tr>
<tr><td><code id="transace_+3A_type">type</code></td>
<td>

<p>specifies how <code><a href="#topic+Function">Function</a></code> is to return the series of
functions that define the transformations of all variables.  By
default a list is created, with the names of the list elements being
the names of the variables.  Specify <code>type="individual"</code> to
have separate functions created in the current environment
(<code>pos=-1</code>, the default) or in location defined by <code>pos</code>
if <code>where</code> is specified.  For the latter method, the names of
the objects created are the names of the corresponding variables,
prefixed by <code>prefix</code> and with <code>suffix</code> appended to the
end. If any of <code>pos</code>, <code>prefix</code>, or
<code>suffix</code> is specified, <code>type</code> is automatically set to
<code>"individual"</code>.
</p>
</td></tr>
<tr><td><code id="transace_+3A_ytype">ytype</code></td>
<td>

<p>By default the first function created by <code>Function</code> is the
y-transformation.  Specify <code>ytype="inverse"</code> to instead create
the inverse of the transformation, to be able to obtain originally
scaled y-values.
</p>
</td></tr>
<tr><td><code id="transace_+3A_prefix">prefix</code></td>
<td>

<p>character string defining the prefix for function names created when
<code>type="individual"</code>.  By default, the function specifying the
transformation for variable <code>x</code> will be named <code>.x</code>.
</p>
</td></tr>
<tr><td><code id="transace_+3A_suffix">suffix</code></td>
<td>

<p>character string defining the suffix for the function names
</p>
</td></tr>
<tr><td><code id="transace_+3A_pos">pos</code></td>
<td>

<p>See <code><a href="base.html#topic+assign">assign</a></code>.
</p>
</td></tr>
<tr><td><code id="transace_+3A_...">...</code></td>
<td>

<p>arguments passed to other functions.  Ignored for
<code>print.transace</code> and <code>ggplot.transace</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As <code>transace</code> only does one iteration over the predictors, it may
not find optimal transformations and it will be dependent on the order
of the predictors in <code>x</code>.
</p>
<p><code><a href="acepack.html#topic+ace">ace</a></code> and <code><a href="acepack.html#topic+avas">avas</a></code> standardize transformed variables to have
mean zero and variance one for each bootstrap sample, so if a
predictor is not important it will still consistently have a positive
regression coefficient.  Therefore using the bootstrap to estimate
standard errors of the additive least squares regression coefficients
would not help in drawing inferences about the importance of the
predictors.  To do this, <code>summary.areg.boot</code> computes estimates
of, e.g., the inter-quartile range effects of predictors in predicting
the response variable (after untransforming it).  As an example, at
each bootstrap repetition the estimated transformed value of one of
the predictors is computed at the lower quartile, median, and upper
quartile of the raw value of the predictor.  These transformed x
values are then multipled by the least squares estimate of the partial
regression coefficient for that transformed predictor in predicting
transformed y.  Then these weighted transformed x values have the
weighted transformed x value corresponding to the lower quartile
subtracted from them, to estimate an x effect accounting for
nonlinearity.  The last difference computed is then the standardized
effect of raising x from its lowest to its highest quartile.  Before
computing differences, predicted values are back-transformed to be on
the original y scale in a way depending on <code>statistic</code> and
<code>q</code>. The sample standard deviation of these effects (differences)
is taken over the bootstrap samples, and this is used to compute
approximate confidence intervals for effects andapproximate P-values,
both assuming normality.
</p>
<p><code>predict</code> does not re-insert <code>NA</code>s corresponding to
observations that were dropped before the fit, when <code>newdata</code> is
omitted.
</p>
<p><code>statistic="fitted"</code> estimates the same quantity as
<code>statistic="median"</code> if the residuals on the transformed response
have a symmetric distribution. The two provide identical estimates
when the sample median of the residuals is exactly zero. The sample
mean of the residuals is constrained to be exactly zero although this
does not simplify anything.
</p>


<h3>Value</h3>

<p><code>transace</code> returns a list of class <code>transace</code> containing
these elements: <code>n</code> (number of non-missing observations used), <code>transformed</code> (a matrix containing transformed values), <code>rsq</code> (vector of <code class="reqn">R^2</code> with which each
variable can be predicted from the others), <code>omitted</code> (row
numbers of data that were deleted due to <code>NA</code>s),
<code>trantab</code> (compact transformation lookups), <code>levels</code>
(original levels of character and factor	varibles if the input was a
data frame), <code>trim</code> (value of <code>trim</code> passed to
<code>transace</code>), <code>limits</code> (the limits for plotting raw and
transformed variables, computed from <code>trim</code>), and <code>type</code> (a
vector of transformation types used for the variables).
</p>
<p><code>areg.boot</code> returns a list of class &lsquo;<span class="samp">&#8288;areg.boot&#8288;</span>&rsquo; containing
many elements, including (if <code>valrsq</code> is <code>TRUE</code>)
<code>rsquare.app</code> and <code>rsquare.val</code>.  <code>summary.areg.boot</code>
returns a list of class &lsquo;<span class="samp">&#8288;summary.areg.boot&#8288;</span>&rsquo; containing a matrix
of results for each predictor and a vector of adjust-to settings.  It
also contains the call and a &lsquo;<span class="samp">&#8288;label&#8288;</span>&rsquo; for the statistic that was
computed.  A <code>print</code> method for these objects handles the
printing.  <code>predict.areg.boot</code> returns a vector unless
<code>statistic="terms"</code>, in which case it returns a
matrix. <code>Function.areg.boot</code> returns by default a list of
functions whose argument is one of the variables (on the original
scale) and whose returned values are the corresponding transformed
values.  The names of the list of functions correspond to the names of
the original variables.  When <code>type="individual"</code>,
<code>Function.areg.boot</code> invisibly returns the vector of names of the
created function objects. <code>Mean.areg.boot</code> and
<code>Quantile.areg.boot</code> also return functions.
</p>
<p><code>smearingEst</code> returns a vector of estimates of distribution
parameters of class &lsquo;<span class="samp">&#8288;labelled&#8288;</span>&rsquo; so that <code>print.labelled</code> wil
print a label documenting the estimate that was used (see
<code><a href="#topic+label">label</a></code>).  This label can be retrieved for other purposes
by using e.g. <code>label(obj)</code>, where obj was the vector
returned by <code>smearingEst</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University School of Medicine
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Harrell FE, Lee KL, Mark DB (1996): Stat in Med 15:361&ndash;387.
</p>
<p>Duan N (1983): Smearing estimate: A nonparametric retransformation
method.  JASA 78:605&ndash;610.
</p>
<p>Wang N, Ruppert D (1995): Nonparametric estimation of the
transformation in the transform-both-sides regression model.  JASA
90:522&ndash;534. 
</p>
<p>See <code><a href="acepack.html#topic+avas">avas</a></code>, <code><a href="acepack.html#topic+ace">ace</a></code> for primary references.
</p>


<h3>See Also</h3>

<p><code><a href="acepack.html#topic+avas">avas</a></code>, <code><a href="acepack.html#topic+ace">ace</a></code>,
<code><a href="rms.html#topic+ols">ols</a></code>, <code><a href="rms.html#topic+validate">validate</a></code>,
<code><a href="rms.html#topic+predab.resample">predab.resample</a></code>, <code><a href="#topic+label">label</a></code>,
<code><a href="rms.html#topic+nomogram">nomogram</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># xtrans &lt;- transace(~ monotone(age) + sex + blood.pressure + categorical(race.code))
# print(xtrans)  # show R^2s and a few other things
# ggplot(xtrans) # show transformations

# Generate random data from the model y = exp(x1 + epsilon/3) where
# x1 and epsilon are Gaussian(0,1)
set.seed(171)  # to be able to reproduce example
x1 &lt;- rnorm(200)
x2 &lt;- runif(200)  # a variable that is really unrelated to y]
x3 &lt;- factor(sample(c('cat','dog','cow'), 200,TRUE))  # also unrelated to y
y  &lt;- exp(x1 + rnorm(200)/3)
f  &lt;- areg.boot(y ~ x1 + x2 + x3, B=40)
f
plot(f)
# Note that the fitted transformation of y is very nearly log(y)
# (the appropriate one), the transformation of x1 is nearly linear,
# and the transformations of x2 and x3 are essentially flat 
# (specifying monotone(x2) if method='avas' would have resulted
# in a smaller confidence band for x2)


summary(f)


# use summary(f, values=list(x2=c(.2,.5,.8))) for example if you
# want to use nice round values for judging effects


# Plot Y hat vs. Y (this doesn't work if there were NAs)
plot(fitted(f), y)  # or: plot(predict(f,statistic='fitted'), y)


# Show fit of model by varying x1 on the x-axis and creating separate
# panels for x2 and x3.  For x2 using only a few discrete values
newdat &lt;- expand.grid(x1=seq(-2,2,length=100),x2=c(.25,.75),
                      x3=c('cat','dog','cow'))
yhat &lt;- predict(f, newdat, statistic='fitted')  
# statistic='mean' to get estimated mean rather than simple inverse trans.
xYplot(yhat ~ x1 | x2, groups=x3, type='l', data=newdat)


## Not run: 
# Another example, on hypothetical data
f &lt;- areg.boot(response ~ I(age) + monotone(blood.pressure) + race)
# use I(response) to not transform the response variable
plot(f, conf.int=.9)
# Check distribution of residuals
plot(fitted(f), resid(f))
qqnorm(resid(f))
# Refit this model using ols so that we can draw a nomogram of it.
# The nomogram will show the linear predictor, median, mean.
# The last two are smearing estimators.
Function(f, type='individual')  # create transformation functions
f.ols &lt;- ols(.response(response) ~ age + 
             .blood.pressure(blood.pressure) + .race(race))
# Note: This model is almost exactly the same as f but there
# will be very small differences due to interpolation of
# transformations
meanr &lt;- Mean(f)      # create function of lp computing mean response
medr  &lt;- Quantile(f)  # default quantile is .5
nomogram(f.ols, fun=list(Mean=meanr,Median=medr))


# Create S functions that will do the transformations
# This is a table look-up with linear interpolation
g &lt;- Function(f)
plot(blood.pressure, g$blood.pressure(blood.pressure))
# produces the central curve in the last plot done by plot(f)

## End(Not run)


# Another simulated example, where y has a log-normal distribution
# with mean x and variance 1.  Untransformed y thus has median
# exp(x) and mean exp(x + .5sigma^2) = exp(x + .5)
# First generate data from the model y = exp(x + epsilon),
# epsilon ~ Gaussian(0, 1)


set.seed(139)
n &lt;- 1000
x &lt;- rnorm(n)
y &lt;- exp(x + rnorm(n))
f &lt;- areg.boot(y ~ x, B=20)
plot(f)       # note log shape for y, linear for x.  Good!
xs &lt;- c(-2, 0, 2)
d &lt;- data.frame(x=xs)
predict(f, d, 'fitted')
predict(f, d, 'median')   # almost same; median residual=-.001
exp(xs)                   # population medians
predict(f, d, 'mean')
exp(xs + .5)              # population means


# Show how smearingEst works
res &lt;- c(-1,0,1)          # define residuals
y &lt;- 1:5
ytrans &lt;- log(y)
ys &lt;- seq(.1,15,length=50)
trans.approx &lt;- list(x=log(ys), y=ys)
options(digits=4)
smearingEst(ytrans, exp, res, 'fitted')          # ignores res
smearingEst(ytrans, trans.approx, res, 'fitted') # ignores res 
smearingEst(ytrans, exp, res, 'median')          # median res=0
smearingEst(ytrans, exp, res+.1, 'median')       # median res=.1
smearingEst(ytrans, trans.approx, res, 'median')
smearingEst(ytrans, exp, res, 'mean')
mean(exp(ytrans[2] + res))                       # should equal 2nd # above
smearingEst(ytrans, trans.approx, res, 'mean')
smearingEst(ytrans, trans.approx, res, mean)
# Last argument can be any statistical function operating
# on a vector that returns a single value
</code></pre>

<hr>
<h2 id='transcan'>
Transformations/Imputations using Canonical Variates
</h2><span id='topic+transcan'></span><span id='topic+summary.transcan'></span><span id='topic+print.transcan'></span><span id='topic+plot.transcan'></span><span id='topic+ggplot.transcan'></span><span id='topic+impute.transcan'></span><span id='topic+predict.transcan'></span><span id='topic+Function'></span><span id='topic+Function.transcan'></span><span id='topic+fit.mult.impute'></span><span id='topic+vcov.default'></span><span id='topic+vcov.fit.mult.impute'></span><span id='topic++5B.transcan'></span><span id='topic+invertTabulated'></span>

<h3>Description</h3>

<p><code>transcan</code> is a nonlinear additive transformation and imputation
function, and there are several functions for using and operating on
its results.  <code>transcan</code> automatically transforms continuous and
categorical variables to have maximum correlation with the best linear
combination of the other variables.  There is also an option to use a
substitute criterion - maximum correlation with the first principal
component of the other variables.  Continuous variables are expanded
as restricted cubic splines and categorical variables are expanded as
contrasts (e.g., dummy variables).  By default, the first canonical
variate is used to find optimum linear combinations of component
columns.  This function is similar to <code><a href="acepack.html#topic+ace">ace</a></code> except that
transformations for continuous variables are fitted using restricted
cubic splines, monotonicity restrictions are not allowed, and
<code>NA</code>s are allowed.  When a variable has any <code>NA</code>s,
transformed scores for that variable are imputed using least squares
multiple regression incorporating optimum transformations, or
<code>NA</code>s are optionally set to constants.  Shrinkage can be used to
safeguard against overfitting when imputing.  Optionally, imputed
values on the original scale are also computed and returned.  For this
purpose, recursive partitioning or multinomial logistic models can
optionally be used to impute categorical variables, using what is
predicted to be the most probable category.
</p>
<p>By default, <code>transcan</code> imputes <code>NA</code>s with &ldquo;best
guess&rdquo; expected values of transformed variables, back transformed to
the original scale. Values thus imputed are most like conditional
medians assuming the transformations make variables' distributions
symmetric (imputed values are similar to conditionl modes for
categorical variables).  By instead specifying <code>n.impute</code>,
<code>transcan</code> does approximate multiple imputation from the
distribution of each variable conditional on all other variables.
This is done by sampling <code>n.impute</code> residuals from the
transformed variable, with replacement (a la bootstrapping), or by
default, using Rubin's approximate Bayesian bootstrap, where a sample
of size n with replacement is selected from the residuals on
n non-missing values of the target variable, and then a sample
of size m with replacement is chosen from this sample, where
m is the number of missing values needing imputation for the
current multiple imputation  repetition.  Neither of these bootstrap
procedures assume normality or even symmetry of residuals. For
sometimes-missing categorical variables, optimal scores are computed
by adding the &ldquo;best guess&rdquo; predicted mean score to random
residuals off this score.  Then categories having scores closest to
these predicted scores are taken as the random multiple imputations
(<code>impcat = "rpart"</code> is not currently allowed
with <code>n.impute</code>).  The literature recommends using <code>n.impute
  = 5</code> or greater. <code>transcan</code> provides only an approximation to
multiple imputation, especially since it &ldquo;freezes&rdquo; the
imputation model before drawing the multiple imputations rather than
using different estimates of regression coefficients for each
imputation.  For multiple imputation, the <code><a href="#topic+aregImpute">aregImpute</a></code> function
provides a much better approximation to the full Bayesian approach
while still not requiring linearity assumptions.
</p>
<p>When you specify <code>n.impute</code> to <code>transcan</code> you can use
<code>fit.mult.impute</code> to re-fit any model <code>n.impute</code> times based
on <code>n.impute</code> completed datasets (if there are any sometimes
missing variables not specified to <code>transcan</code>, some observations
will still be dropped from these fits).  After fitting <code>n.impute</code>
models, <code>fit.mult.impute</code> will return the fit object from the
last imputation, with <code>coefficients</code> replaced by the average of
the <code>n.impute</code> coefficient vectors and with a component
<code>var</code> equal to the imputation-corrected variance-covariance
matrix using Rubin's rule.  <code>fit.mult.impute</code> can also use the object created by the
<code><a href="mice.html#topic+mice">mice</a></code> function in the <span class="pkg">mice</span> library to draw the
multiple imputations, as well as objects created by
<code><a href="#topic+aregImpute">aregImpute</a></code>.  The following components of fit objects are
also replaced with averages over the <code>n.impute</code> model fits:
<code>linear.predictors</code>, <code>fitted.values</code>, <code>stats</code>,
<code>means</code>, <code>icoef</code>, <code>scale</code>, <code>center</code>,
<code>y.imputed</code>.
</p>
<p>By specifying <code>fun</code> to <code>fit.mult.impute</code> you can run any
function on the fit objects from completed datasets, with the results
saved in an element named <code>funresults</code>.  This facilitates
running bootstrap or cross-validation separately on each completed
dataset and storing all these results in a list for later processing,
e.g., with the <code>rms</code> package <code>processMI</code> function.  Note that for
<code>rms</code>-type validation you will need to specify
<code>fitargs=list(x=TRUE,y=TRUE)</code> to <code>fit.mult.impute</code> and to
use special names for <code>fun</code> result components, such as
<code>validate</code> and <code>calibrate</code> so that the result can be
processed with <code>processMI</code>. When simultaneously running multiple
imputation and resampling model validation you may not need values for
<code>n.impute</code> or <code>B</code> (number of bootstraps) as high as usual,
as the total number of repetitions will be <code>n.impute * B</code>.
</p>
<p><code>fit.mult.impute</code> can incorporate robust sandwich variance estimates into
Rubin's rule if <code>robust=TRUE</code>.
</p>
<p>For <code>ols</code> models fitted by <code>fit.mult.impute</code> with stacking,
the <code class="reqn">R^2</code> measure in the stacked model fit is OK, and
<code>print.ols</code> computes adjusted <code class="reqn">R^2</code> using the real sample
size so it is also OK because <code>fit.mult.compute</code> corrects the
stacked error degrees of freedom in the stacked fit object to reflect
the real sample size. 
</p>
<p>The <code><a href="base.html#topic+summary">summary</a></code> method for <code>transcan</code> prints the function
call, <code class="reqn">R^2</code> achieved in transforming each variable, and for each
variable the coefficients of all other transformed variables that are
used to estimate the transformation of the initial variable.  If
<code>imputed=TRUE</code> was used in the call to transcan, also uses the
<code>describe</code> function to print a summary of imputed values.  If
<code>long = TRUE</code>, also prints all imputed values with observation
identifiers.  There is also a simple function <code>print.transcan</code>
which merely prints the transformation matrix and the function call.
It has an optional argument <code>long</code>, which if set to <code>TRUE</code>
causes detailed parameters to be printed.  Instead of plotting while
<code>transcan</code> is running, you can plot the final transformations
after the fact using <code>plot.transcan</code> or <code>ggplot.transcan</code>,
if the option <code>trantab  = TRUE</code> was specified to <code>transcan</code>.
If in addition the option 
<code>imputed = TRUE</code> was specified to <code>transcan</code>,
<code>plot</code> and <code>ggplot</code> will show the location of imputed values
(including multiples) along the axes.  For <code>ggplot</code>, imputed
values are shown as red plus signs.
</p>
<p><code><a href="#topic+impute">impute</a></code> method for <code>transcan</code> does imputations for a
selected original data variable, on the original scale (if
<code>imputed=TRUE</code> was given to <code>transcan</code>).  If you do not
specify a variable to <code>impute</code>, it will do imputations for all
variables given to <code>transcan</code> which had at least one missing
value.  This assumes that the original variables are accessible (i.e.,
they have been attached) and that you want the imputed variables to
have the same names are the original variables. If <code>n.impute</code> was
specified to <code>transcan</code> you must tell <code><a href="#topic+impute">impute</a></code> which
<code>imputation</code> to use.   Results are stored in <code>.GlobalEnv</code>
when <code>list.out</code> is not specified  (it is recommended to use
<code>list.out=TRUE</code>).   
</p>
<p>The <code><a href="stats.html#topic+predict">predict</a></code> method for <code>transcan</code> computes
predicted variables and imputed values from a matrix of new data.
This matrix should have the same column variables as the original
matrix used with <code>transcan</code>, and in the same order (unless a
formula was used with <code>transcan</code>).
</p>
<p>The <code><a href="#topic+Function">Function</a></code> function is a generic function
generator. <code>Function.transcan</code> creates <span class="rlang"><b>R</b></span> functions to transform
variables using transformations created by <code>transcan</code>. These
functions are useful for getting predicted values with predictors set
to values on the original scale.
</p>
<p>The <code><a href="stats.html#topic+vcov">vcov</a></code> methods are defined here so that
imputation-corrected variance-covariance matrices are readily
extracted from <code>fit.mult.impute</code> objects, and so that
<code>fit.mult.impute</code> can easily compute traditional covariance
matrices for individual completed datasets.
</p>
<p>The subscript method for <code>transcan</code> preserves attributes.
</p>
<p>The <code>invertTabulated</code> function does either inverse linear
interpolation or uses sampling to sample qualifying x-values having
y-values near the desired values.  The latter is used to get inverse
values having a reasonable distribution (e.g., no floor or ceiling
effects) when the transformation has a flat or nearly flat segment,
resulting in a many-to-one transformation in that region.  Sampling
weights are a combination of the frequency of occurrence of x-values
that are within <code>tolInverse</code> times the range of <code>y</code> and the
squared distance between the associated y-values and the target
y-value (<code>aty</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transcan(x, method=c("canonical","pc"),
         categorical=NULL, asis=NULL, nk, imputed=FALSE, n.impute,
         boot.method=c('approximate bayesian', 'simple'),
         trantab=FALSE, transformed=FALSE, 
         impcat=c("score", "multinom", "rpart"),
         mincut=40, 
         inverse=c('linearInterp','sample'), tolInverse=.05,
         pr=TRUE, pl=TRUE, allpl=FALSE, show.na=TRUE, 
         imputed.actual=c('none','datadensity','hist','qq','ecdf'),
         iter.max=50, eps=.1, curtail=TRUE, 
         imp.con=FALSE, shrink=FALSE, init.cat="mode", 
         nres=if(boot.method=='simple')200 else 400,
         data, subset, na.action, treeinfo=FALSE, 
         rhsImp=c('mean','random'), details.impcat='', ...)

## S3 method for class 'transcan'
summary(object, long=FALSE, digits=6, ...)

## S3 method for class 'transcan'
print(x, long=FALSE, ...)

## S3 method for class 'transcan'
plot(x, ...)

## S3 method for class 'transcan'
ggplot(data, mapping, scale=FALSE, ..., environment)

## S3 method for class 'transcan'
impute(x, var, imputation, name, pos.in, data, 
       list.out=FALSE, pr=TRUE, check=TRUE, ...)

fit.mult.impute(formula, fitter, xtrans, data, n.impute, fit.reps=FALSE,
                dtrans, derived, fun, vcovOpts=NULL,
                robust=FALSE, cluster, robmethod=c('huber', 'efron'),
                method=c('ordinary', 'stack', 'only stack'),
                funstack=TRUE, lrt=FALSE,
                pr=TRUE, subset, fitargs)

## S3 method for class 'transcan'
predict(object, newdata, iter.max=50, eps=0.01, curtail=TRUE,
        type=c("transformed","original"),
        inverse, tolInverse, check=FALSE, ...)

Function(object, ...)

## S3 method for class 'transcan'
Function(object, prefix=".", suffix="", pos=-1, ...)

invertTabulated(x, y, freq=rep(1,length(x)), 
                aty, name='value',
                inverse=c('linearInterp','sample'),
                tolInverse=0.05, rule=2)

## Default S3 method:
vcov(object, regcoef.only=FALSE, ...)

## S3 method for class 'fit.mult.impute'
vcov(object, regcoef.only=TRUE,
                intercepts='mid', ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="transcan_+3A_x">x</code></td>
<td>

<p>a matrix containing continuous variable values and codes for
categorical variables.  The matrix must have column names
(<code>dimnames</code>).  If row names are present, they are used in
forming the <code>names</code> attribute of imputed values if
<code>imputed = TRUE</code>.  <code>x</code> may also be a formula, in which
case the model matrix is created automatically, using data in the
calling frame.  Advantages of using a formula are that
<code style="white-space: pre;">&#8288;categorical&#8288;</code> variables can be determined automatically by a
variable being a <code><a href="base.html#topic+factor">factor</a></code> variable, and variables with
two unique levels are modeled <code style="white-space: pre;">&#8288;asis&#8288;</code>. Variables with 3 unique
values are considered to be <code style="white-space: pre;">&#8288;categorical&#8288;</code> if a formula is
specified.  For a formula you may also specify that a variable is to
remain untransformed by enclosing its name with the identify
function, e.g. <code>I(x3)</code>.  The user may add other variable names
to the <code>asis</code> and <code>categorical</code> vectors.  For
<code>invertTabulated</code>, <code>x</code> is a vector or a list with three
components: the x vector, the corresponding vector of transformed
values, and the corresponding vector of frequencies of the pair of
original and transformed variables. For <code>print</code>, <code>plot</code>,
<code>ggplot</code>, <code>impute</code>, and <code>predict</code>, <code>x</code> is an
object created by <code>transcan</code>.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_formula">formula</code></td>
<td>

<p>any <span class="rlang"><b>R</b></span> model formula
</p>
</td></tr>
<tr><td><code id="transcan_+3A_fitter">fitter</code></td>
<td>

<p>any <span class="rlang"><b>R</b></span>, <code>rms</code>, modeling function (not in quotes) that computes
a vector of <code><a href="stats.html#topic+coefficients">coefficients</a></code> and for which
<code><a href="stats.html#topic+vcov">vcov</a></code> will return a variance-covariance matrix.  E.g.,
<code>fitter = <a href="stats.html#topic+lm">lm</a></code>, <code><a href="stats.html#topic+glm">glm</a></code>,
<code><a href="rms.html#topic+ols">ols</a></code>. At present models 
involving non-regression parameters (e.g., scale parameters in
parametric survival models) are not handled fully.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_xtrans">xtrans</code></td>
<td>

<p>an object created by <code>transcan</code>, <code><a href="#topic+aregImpute">aregImpute</a></code>, or
<code><a href="mice.html#topic+mice">mice</a></code>
</p>
</td></tr>
<tr><td><code id="transcan_+3A_method">method</code></td>
<td>

<p>use <code>method="canonical"</code> or any abbreviation thereof, to use
canonical variates (the default). <code>method="pc"</code> transforms a
variable instead so as to maximize the correlation with the first
principal component of the other variables.  For
<code>fit.mult.impute</code>, <code>method</code> specifies whether to use
standard multiple imputation (the default <code>method='ordinary'</code>)
or whether to get final coefficients from stacking all 
completed datasets and fitting one model.  Stacking is required if
likelihood ratio tests accounting for imputation are to be done.
<code>method='stack'</code> means to do regular MI and stacking, which
results in more valid standard errors of coefficient estimates.
<code>method='only stack'</code> means that model fits are not done on
individual completed datasets, and standard errors will not be very
accurate.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_categorical">categorical</code></td>
<td>

<p>a character vector of names of variables in <code>x</code> which are
categorical, for which the ordering of re-scored values is not
necessarily preserved. If <code>categorical</code> is omitted, it is
assumed that all variables are continuous (or binary).  Set
<code>categorical="*"</code> to treat all variables as categorical.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_asis">asis</code></td>
<td>

<p>a character vector of names of variables that are not to be
transformed. For these variables, the guts of
<code><a href="stats.html#topic+lm.fit">lm.fit</a></code> <code>method="qr"</code> is used to impute
missing values. You may want to treat binary variables <code style="white-space: pre;">&#8288;asis&#8288;</code>
(this is automatic if using a formula).  If <code>imputed = TRUE</code>,
you may want to use &lsquo;<span class="samp">&#8288;"categorical"&#8288;</span>&rsquo; for binary variables if you
want to force imputed values to be one of the original data
values. Set <code>asis="*"</code> to treat all variables <code style="white-space: pre;">&#8288;asis&#8288;</code>.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_nk">nk</code></td>
<td>

<p>number of knots to use in expanding each continuous variable (not
listed in <code>asis</code>) in a restricted cubic spline function.
Default is 3 (yielding 2 parameters for a variable) if
<code class="reqn">n &lt; 30</code>, 4 if
<code class="reqn">30 &lt;= n &lt; 100</code>, and 5 if
<code class="reqn">n \ge 100</code> (4 parameters).
</p>
</td></tr>
<tr><td><code id="transcan_+3A_imputed">imputed</code></td>
<td>

<p>Set to <code>TRUE</code> to return a list containing imputed values on the
original scale. If the transformation for a variable is
non-monotonic, imputed values are not unique.  <code>transcan</code> uses
the <code><a href="stats.html#topic+approx">approx</a></code> function, which returns the highest value
of the variable with the transformed score equalling the imputed
score. <code>imputed=TRUE</code> also causes original-scale imputed values
to be shown as tick marks on the top margin of each graph when
<code>show.na=TRUE</code> (for the final iteration only). For categorical
predictors, these imputed values are passed through the
<code><a href="base.html#topic+jitter">jitter</a></code> function so that their frequencies can be
visualized.  When <code>n.impute</code> is used, each <code>NA</code> will have
<code>n.impute</code> tick marks.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_n.impute">n.impute</code></td>
<td>

<p>number of multiple imputations.  If omitted, single predicted
expected value imputation is used.  <code>n.impute=5</code> is frequently
recommended.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_boot.method">boot.method</code></td>
<td>

<p>default is to use the approximate Bayesian bootstrap (sample with
replacement from sample with replacement of the vector of residuals).
You can also specify <code>boot.method="simple"</code> to use the usual
bootstrap one-stage sampling with replacement.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_trantab">trantab</code></td>
<td>

<p>Set to <code>TRUE</code> to add an attribute <code>trantab</code> to the
returned matrix. This contains a vector of lists each with
components <code>x</code> and <code>y</code> containing the unique values and
corresponding transformed values for the columns of <code>x</code>.  This
is set up to be used easily with the <code><a href="stats.html#topic+approx">approx</a></code> function.
You must specify <code>trantab=TRUE</code> if you want to later use the
<code>predict.transcan</code> function with <code>type = "original"</code>.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_transformed">transformed</code></td>
<td>

<p>set to <code>TRUE</code> to cause <code>transcan</code> to return an object
<code>transformed</code> containing the matrix of transformed variables  
</p>
</td></tr>
<tr><td><code id="transcan_+3A_impcat">impcat</code></td>
<td>

<p>This argument tells how to impute categorical variables on the
original scale.  The default is <code>impcat="score"</code> to impute the
category whose canonical variate score is closest to the predicted
score. Use <code>impcat="rpart"</code> to impute categorical variables
using the values of all other transformed predictors in conjunction
with the <code><a href="rpart.html#topic+rpart">rpart</a></code> function.  A better but somewhat
slower approach is to 
use <code>impcat="multinom"</code> to fit a multinomial logistic model to
the categorical variable, at the last iteraction of the
<code>transcan</code> algorithm.  This uses the <code><a href="nnet.html#topic+multinom">multinom</a></code>
function in the <span class="pkg">nnet</span> library of the <span class="pkg">MASS</span> package (which
is assumed to have been installed by the user) to fit a polytomous
logistic model to the current working transformations of all the
other variables (using conditional mean imputation for missing
predictors).  Multiple imputations are made by drawing multinomial
values from the vector of predicted probabilities of category
membership for the missing categorical values.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_mincut">mincut</code></td>
<td>

<p>If <code>imputed=TRUE</code>, there are categorical variables, and
<code>impcat = "rpart"</code>, <code>mincut</code> specifies the lowest node size
that will be allowed to be split.  The default is 40.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_inverse">inverse</code></td>
<td>

<p>By default, imputed values are back-solved on the original scale
using inverse linear interpolation on the fitted tabulated
transformed values. This will cause distorted distributions of
imputed values (e.g., floor and ceiling effects) when the estimated
transformation has a flat or nearly flat section.  To instead use
the <code>invertTabulated</code> function (see above) with the
<code>"sample"</code> option, specify <code>inverse="sample"</code>.  
</p>
</td></tr>
<tr><td><code id="transcan_+3A_tolinverse">tolInverse</code></td>
<td>

<p>the multiplyer of the range of transformed values, weighted by
<code>freq</code> and by the distance measure, for determining the set of
x values having y values within a tolerance of the value of
<code>aty</code> in <code>invertTabulated</code>.  For <code>predict.transcan</code>,
<code>inverse</code> and <code>tolInverse</code> are obtained from options that
were specified to <code>transcan</code> by default.  Otherwise, if not
specified by the user, these default to the defaults used to
<code>invertTabulated</code>.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_pr">pr</code></td>
<td>

<p>For <code>transcan</code>, set to <code>FALSE</code> to suppress printing
<code class="reqn">R^2</code> and shrinkage factors.  Set <code>impute.transcan=FALSE</code>
to suppress messages concerning the number of <code>NA</code> values
imputed. Set <code>fit.mult.impute=FALSE</code> to suppress printing
variance inflation factors accounting for imputation, rate of
missing information, and degrees of freedom.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_pl">pl</code></td>
<td>

<p>Set to <code>FALSE</code> to suppress plotting the final transformations
with distribution of scores for imputed values (if
<code>show.na=TRUE</code>).
</p>
</td></tr>
<tr><td><code id="transcan_+3A_allpl">allpl</code></td>
<td>

<p>Set to <code>TRUE</code> to plot transformations for intermediate iterations.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_show.na">show.na</code></td>
<td>

<p>Set to <code>FALSE</code> to suppress the distribution of scores assigned
to missing values (as tick marks on the right margin of each
graph). See also <code>imputed</code>.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_imputed.actual">imputed.actual</code></td>
<td>

<p>The default is &lsquo;<span class="samp">&#8288;"none"&#8288;</span>&rsquo; to suppress plotting of actual
vs. imputed values for all variables having any <code>NA</code> values.
Other choices are &lsquo;<span class="samp">&#8288;"datadensity"&#8288;</span>&rsquo; to use
<code><a href="#topic+datadensity">datadensity</a></code> to make a single plot, &lsquo;<span class="samp">&#8288;"hist"&#8288;</span>&rsquo; to
make a series of back-to-back histograms, &lsquo;<span class="samp">&#8288;"qq"&#8288;</span>&rsquo; to make a
series of q-q plots, or &lsquo;<span class="samp">&#8288;"ecdf"&#8288;</span>&rsquo; to make a series of empirical
cdfs.  For <code>imputed.actual="datadensity"</code> for example you get a
rug plot of the non-missing values for the variable with beneath it
a rug plot of the imputed values. When <code>imputed.actual</code> is not
&lsquo;<span class="samp">&#8288;"none"&#8288;</span>&rsquo;, <code>imputed</code> is automatically set to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_iter.max">iter.max</code></td>
<td>

<p>maximum number of iterations to perform for <code>transcan</code> or
<code>predict</code>. For <code><a href="stats.html#topic+predict">predict</a></code>, only one iteration is
used if there are no <code>NA</code> values in the data or if
<code>imp.con</code> was used.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_eps">eps</code></td>
<td>

<p>convergence criterion for <code>transcan</code> and <code><a href="stats.html#topic+predict">predict</a></code>.
<code>eps</code> is the maximum change in transformed values from one
iteration to the next.  If for a given iteration all new
transformations of variables differ by less than <code>eps</code> (with or
without negating the transformation to allow for &ldquo;flipping&rdquo;)
from the transformations in the previous iteration, one more
iteration is done for <code>transcan</code>. During this last iteration,
individual transformations are not updated but coefficients of
transformations are.  This improves stability of coefficients of
canonical variates on the right-hand-side. <code>eps</code> is ignored
when <code>rhsImp="random"</code>.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_curtail">curtail</code></td>
<td>

<p>for <code>transcan</code>, causes imputed values on the transformed scale
to be truncated so that their ranges are within the ranges of
non-imputed transformed values. For <code><a href="stats.html#topic+predict">predict</a></code>,
<code>curtail</code> defaults to <code>TRUE</code> to truncate predicted
transformed values to their ranges in the original fit (<code>xt</code>).
</p>
</td></tr>
<tr><td><code id="transcan_+3A_imp.con">imp.con</code></td>
<td>

<p>for <code>transcan</code>, set to <code>TRUE</code> to impute <code>NA</code> values
on the original scales with constants (medians or most frequent
category codes).  Set to a vector of constants to instead always use
these constants for imputation. These imputed values are ignored
when fitting the current working transformation for asingle
variable.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_shrink">shrink</code></td>
<td>

<p>default is <code>FALSE</code> to use ordinary least squares or canonical
variate estimates. For the purposes of imputing <code>NA</code>s, you may
want to set <code>shrink=TRUE</code> to avoid overfitting when developing
a prediction equation to predict each variables from all the others
(see details below).
</p>
</td></tr>
<tr><td><code id="transcan_+3A_init.cat">init.cat</code></td>
<td>

<p>method for initializing scorings of categorical variables.  Default
is &lsquo;<span class="samp">&#8288;"mode"&#8288;</span>&rsquo; to use a dummy variable set to 1 if the value is
the most frequent value (this is the default). Use &lsquo;<span class="samp">&#8288;"random"&#8288;</span>&rsquo;
to use a random 0-1 variable.  Set to &lsquo;<span class="samp">&#8288;"asis"&#8288;</span>&rsquo; to use the
original integer codes asstarting scores.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_nres">nres</code></td>
<td>

<p>number of residuals to store if <code>n.impute</code> is specified.  If
the dataset has fewer than <code>nres</code> observations, all residuals
are saved. Otherwise a random sample of the residuals of length
<code>nres</code> without replacement is saved.  The default for
<code>nres</code> is higher if <code>boot.method="approximate bayesian"</code>.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_data">data</code></td>
<td>

<p>Data frame used to fill the formula.  For <code>ggplot</code> is the
result of <code>transcan</code> with <code>trantab=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_subset">subset</code></td>
<td>

<p>an integer or logical vector specifying the subset of observations
to fit
</p>
</td></tr>
<tr><td><code id="transcan_+3A_na.action">na.action</code></td>
<td>

<p>These may be used if <code>x</code> is a formula.  The default
<code>na.action</code> is <code>na.retain</code> (defined by <code>transcan</code>)
which keeps all observations with any <code>NA</code> values. For
<code>impute.transcan</code>, <code>data</code> is a data frame to use as the
source of variables to be imputed, rather than using
<code>pos.in</code>.  For <code>fit.mult.impute</code>, <code>data</code> is
mandatory and is a data frame containing the data to be used in
fitting the model but before imputations are applied.  Variables
omitted from <code>data</code> are assumed to be available from frame1
and do not need to be imputed.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_treeinfo">treeinfo</code></td>
<td>

<p>Set to <code>TRUE</code> to get additional information printed when
<code>impcat="rpart"</code>, such as the predicted probabilities of
category membership.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_rhsimp">rhsImp</code></td>
<td>

<p>Set to &lsquo;<span class="samp">&#8288;"random"&#8288;</span>&rsquo; to use random draw imputation when a
sometimes missing variable is moved to be a predictor of other
sometimes missing variables.  Default is <code>rhsImp="mean"</code>, which
uses conditional mean imputation on the transformed scale.
Residuals used are residuals from the transformed scale.  When
&lsquo;<span class="samp">&#8288;"random"&#8288;</span>&rsquo; is used, <code>transcan</code> runs 5 iterations and
ignores <code>eps</code>.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_details.impcat">details.impcat</code></td>
<td>

<p>set to a character scalar that is the name of a category variable to
include in the resulting <code>transcan</code> object an element
<code>details.impcat</code> containing details of how the categorical
variable was multiply imputed.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_...">...</code></td>
<td>

<p>arguments passed to <code><a href="#topic+scat1d">scat1d</a></code>. For <code>ggplot.transcan</code>,
these arguments are passed to <code>facet_wrap</code>, e.g. <code>ncol=2</code>.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_long">long</code></td>
<td>

<p>for <code><a href="base.html#topic+summary">summary</a></code>, set to <code>TRUE</code> to print all imputed
values. For <code><a href="base.html#topic+print">print</a></code>, set to <code>TRUE</code> to print details
of transformations/imputations.  
</p>
</td></tr>
<tr><td><code id="transcan_+3A_digits">digits</code></td>
<td>

<p>number of significant digits for printing values by
<code><a href="base.html#topic+summary">summary</a></code>
</p>
</td></tr>
<tr><td><code id="transcan_+3A_scale">scale</code></td>
<td>
<p>for <code>ggplot.transcan</code> set <code>scale=TRUE</code> to
scale transformed values to [0,1] before plotting.</p>
</td></tr>
<tr><td><code id="transcan_+3A_mapping">mapping</code>, <code id="transcan_+3A_environment">environment</code></td>
<td>
<p>not used; needed because of rules about generics</p>
</td></tr>
<tr><td><code id="transcan_+3A_var">var</code></td>
<td>

<p>For <code><a href="#topic+impute">impute</a></code>, is a variable that was originally a column
in <code>x</code>, for which imputated values are to be filled
in. <code>imputed=TRUE</code> must have been used in <code>transcan</code>.
Omit <code>var</code> to impute all variables, creating new variables in
position <code>pos</code> (see <code><a href="base.html#topic+assign">assign</a></code>).
</p>
</td></tr>
<tr><td><code id="transcan_+3A_imputation">imputation</code></td>
<td>

<p>specifies which of the multiple imputations to use for filling in
<code>NA</code> values
</p>
</td></tr>
<tr><td><code id="transcan_+3A_name">name</code></td>
<td>

<p>name of variable to impute, for <code><a href="#topic+impute">impute</a></code> function.
Default is character string version of the second argument
(<code>var</code>) in the call to <code><a href="#topic+impute">impute</a></code>. For
<code>invertTabulated</code>, is the name of variable being transformed
(used only for warning messages).
</p>
</td></tr>
<tr><td><code id="transcan_+3A_pos.in">pos.in</code></td>
<td>

<p>location as defined by <code><a href="base.html#topic+assign">assign</a></code> to find variables that
need to be 
imputed, when all variables are to be imputed automatically by
<code>impute.transcan</code> (i.e., when no input variable name is
specified).  Default is position that contains
the first variable to be imputed.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_list.out">list.out</code></td>
<td>

<p>If <code>var</code> is not specified, you can set <code>list.out=TRUE</code> to
have <code>impute.transcan</code> return a list containing variables with
needed values imputed.  This list will contain a single imputation.
Variables not needing imputation are copied to the list as-is.  You
can use this list for analysis just like a data frame.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_check">check</code></td>
<td>

<p>set to <code>FALSE</code> to suppress certain warning messages
</p>
</td></tr>
<tr><td><code id="transcan_+3A_newdata">newdata</code></td>
<td>

<p>a new data matrix for which to compute transformed
variables. Categorical variables must use the same integer codes as
were used in the call to <code>transcan</code>.  If a formula was
originally specified to <code>transcan</code> (instead of a data matrix),
<code>newdata</code> is optional and if given must be a data frame; a
model frame is generated automatically from the previous formula.
The <code>na.action</code> is handled automatically, and the levels for
factor variables must be the same and in the same order as were used
in the original variables specified in the formula given to
<code>transcan</code>.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_fit.reps">fit.reps</code></td>
<td>

<p>set to <code>TRUE</code> to save all fit objects from the fit for each
imputation in <code>fit.mult.impute</code>.  Then the object returned will
have a component <code>fits</code> which is a list whose i'th
element is the i'th fit object. 
</p>
</td></tr>
<tr><td><code id="transcan_+3A_dtrans">dtrans</code></td>
<td>

<p>provides an approach to creating derived variables from a single
filled-in dataset.  The function specified as <code>dtrans</code> can even
reshape the imputed dataset.  An example of such usage is fitting
time-dependent covariates in a Cox model that are created by
&ldquo;start,stop&rdquo; intervals.  Imputations may be done on a one
record per subject data frame that is converted by <code>dtrans</code> to
multiple records per subject.  The imputation can enforce
consistency of certain variables across records so that for example
a missing value of sex will not be imputed as &lsquo;<span class="samp">&#8288;male&#8288;</span>&rsquo; for
one of the subject's records and &lsquo;<span class="samp">&#8288;female&#8288;</span>&rsquo; as another.  An
example of how <code>dtrans</code> might be specified is
<code>dtrans=function(w) {w$age &lt;- w$years + w$months/12; w}</code>
where <code>months</code> might havebeen imputed but <code>years</code> was
never missing.  An outline for using 'dtrans' to impute missing
baseline variables in a longitudinal analysis appears in Details below.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_derived">derived</code></td>
<td>

<p>an expression containing <span class="rlang"><b>R</b></span> expressions for computing derived
variables that are used in the model formula.  This is useful when
multiple imputations are done for component variables but the actual
model uses combinations of these (e.g., ratios or other
derivations). For a single derived variable you can specify for
example <code>derived=expression(ratio &lt;- weight/height)</code>.  For
multiple derived variables use the form
<code>derived=expression({ratio &lt;- weight/height; product &lt;-
      weight*height})</code> or put the expression on separate input lines.
To monitor the multiply-imputed derived variables you can add to the
<code>expression</code> a command such as <code>print(describe(ratio))</code>.
See the example below.  Note that <code>derived</code> is not yet
implemented.  
</p>
</td></tr>
<tr><td><code id="transcan_+3A_fun">fun</code></td>
<td>
<p>a function of a fit made on one of the completed datasets.
Typical uses are bootstrap model validations.  The result of
<code>fun</code> for imputation <code>i</code> is placed in the <code>i</code>th
element of a list that is returned in the <code>fit.mult.impute</code>
object element named <code>funresults</code>. See
the <code>rms</code> <code>processMI</code> function for help in processing
these results for the cases of <code>validate</code> and <code>calibrate</code>.</p>
</td></tr>
<tr><td><code id="transcan_+3A_vcovopts">vcovOpts</code></td>
<td>
<p>a list of named additional arguments to pass to the
<code>vcov</code> method for <code>fitter</code>.  Useful for <code>orm</code> models
for retaining all intercepts
(<code>vcovOpts=list(intercepts='all')</code>) instead of just the middle
one.</p>
</td></tr>
<tr><td><code id="transcan_+3A_robust">robust</code></td>
<td>
<p>set to <code>TRUE</code> to have <code>fit.mult.impute</code> call the
<code>rms</code> package <code>robcov</code> function on each fit on a
completed dataset.  When <code>cluster</code> is given, <code>robust</code> is
forced to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="transcan_+3A_cluster">cluster</code></td>
<td>
<p>a vector of cluster IDs that is the same length of the number
of rows in the dataset being analyzed.  When specified, <code>robust</code> is
assumed to be <code>TRUE</code>, and the <code>rms</code> <code>robcov</code> function is 
called with the <code>cluster</code> vector given as its second argument.</p>
</td></tr>
<tr><td><code id="transcan_+3A_robmethod">robmethod</code></td>
<td>
<p>see the <code>robcov</code> function's <code>method</code>
argument</p>
</td></tr>
<tr><td><code id="transcan_+3A_funstack">funstack</code></td>
<td>
<p>set to <code>FALSE</code> to not run <code>fun</code> on the
stacked dataset, making an <code>n.impute</code>+1 element of
<code>funresults</code></p>
</td></tr>
<tr><td><code id="transcan_+3A_lrt">lrt</code></td>
<td>
<p>set to <code>TRUE</code> to have <code>method, fun, fitargs</code> set
appropriately automatically so that <code>processMI</code> can be used to
get likelihood ratio tests.  When doing this, <code>fun</code> may not be specified by the user.</p>
</td></tr>
<tr><td><code id="transcan_+3A_fitargs">fitargs</code></td>
<td>
<p>a list of extra arguments to pass to <code>fitter</code>,
used especially with <code>fun</code>.  When <code>robust=TRUE</code> the arguments
<code>x=TRUE, y=TRUE</code> are automatically added to <code>fitargs</code>.</p>
</td></tr>
<tr><td><code id="transcan_+3A_type">type</code></td>
<td>

<p>By default, the matrix of transformed variables is returned, with
imputed values on the transformed scale.  If you had specified
<code>trantab=TRUE</code> to <code>transcan</code>, specifying
<code>type="original"</code> does the table look-ups with linear
interpolation to return the input matrix <code>x</code> but with imputed
values on the original scale inserted for <code>NA</code> values.  For
categorical variables, the method used here is to select the
category code having a corresponding scaled value closest to the
predicted transformed value.  This corresponds to the default
<code>impcat</code>.  Note: imputed values
thus returned when <code>type="original"</code> are single expected value
imputations even in <code>n.impute</code> is given.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_object">object</code></td>
<td>

<p>an object created by <code>transcan</code>, or an object to be converted to
<span class="rlang"><b>R</b></span> function code, typically a model fit object of some sort  
</p>
</td></tr>
<tr><td><code id="transcan_+3A_prefix">prefix</code>, <code id="transcan_+3A_suffix">suffix</code></td>
<td>

<p>When creating separate <span class="rlang"><b>R</b></span> functions for each variable in <code>x</code>,
the name of the new function will be <code>prefix</code> placed in front of
the variable name, and <code>suffix</code> placed in back of the name.  The
default is to use names of the form &lsquo;<span class="samp">&#8288;.varname&#8288;</span>&rsquo;, where
varname is the variable name.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_pos">pos</code></td>
<td>

<p>position as in <code><a href="base.html#topic+assign">assign</a></code> at which to store new functions
(for <code><a href="#topic+Function">Function</a></code>). Default is <code>pos=-1</code>.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_y">y</code></td>
<td>

<p>a vector corresponding to <code>x</code> for <code>invertTabulated</code>, if its
first argument <code>x</code> is not a list
</p>
</td></tr>
<tr><td><code id="transcan_+3A_freq">freq</code></td>
<td>

<p>a vector of frequencies corresponding to cross-classified <code>x</code>
and <code>y</code> if <code>x</code> is not a list.  Default is a vector of ones.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_aty">aty</code></td>
<td>

<p>vector of transformed values at which inverses are desired
</p>
</td></tr>
<tr><td><code id="transcan_+3A_rule">rule</code></td>
<td>

<p>see <code><a href="stats.html#topic+approx">approx</a></code>.  <code>transcan</code> assumes <code>rule</code> is
always 2.
</p>
</td></tr>
<tr><td><code id="transcan_+3A_regcoef.only">regcoef.only</code></td>
<td>

<p>set to <code>TRUE</code> to make <code>vcov.default</code> delete positions in
the covariance matrix for any non-regression coefficients (e.g., log
scale parameter from <code><a href="rms.html#topic+psm">psm</a></code> or <code><a href="survival.html#topic+survreg">survreg</a></code>)
</p>
</td></tr>
<tr><td><code id="transcan_+3A_intercepts">intercepts</code></td>
<td>
<p>this is primarily for <code><a href="rms.html#topic+orm">orm</a></code>
objects.  Set to <code>"none"</code> to discard all intercepts from the
covariance matrix, or to <code>"all"</code> or <code>"mid"</code> to keep all
elements generated by <code>orm</code> (<code>orm</code> only outputs the
covariance matrix for the intercept corresponding to the median).
You can also set <code>intercepts</code> to a vector of subscripts for
selecting particular intercepts in a multi-intercept model.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The starting approximation to the transformation for each variable is
taken to be the original coding of the variable.  The initial
approximation for each missing value is taken to be the median of the
non-missing values for the variable (for continuous ones) or the most
frequent category (for categorical ones).  Instead, if <code>imp.con</code>
is a vector, its values are used for imputing <code>NA</code> values.  When
using each variable as a dependent variable, <code>NA</code> values on that
variable cause all observations to be temporarily deleted.  Once a new
working transformation is found for the variable, along with a model
to predict that transformation from all the other variables, that
latter model is used to impute <code>NA</code> values in the selected
dependent variable if <code>imp.con</code> is not specified.
</p>
<p>When that variable is used to predict a new dependent variable, the
current working imputed values are inserted.  Transformations are
updated after each variable becomes a dependent variable, so the order
of variables on <code>x</code> could conceivably make a difference in the
final estimates.  For obtaining out-of-sample
predictions/transformations, <code><a href="stats.html#topic+predict">predict</a></code> uses the same
iterative procedure as <code>transcan</code> for imputation, with the same
starting values for fill-ins as were used by <code>transcan</code>.  It also
(by default) uses a conservative approach of curtailing transformed
variables to be within the range of the original ones. Even when
<code>method = "pc"</code> is specified, canonical variables are used for
imputing missing values.
</p>
<p>Note that fitted transformations, when evaluated at imputed variable
values (on the original scale), will not precisely match the
transformed imputed values returned in <code>xt</code>.  This is because
<code>transcan</code> uses an approximate method based on linear
interpolation to back-solve for imputed values on the original scale.
</p>
<p>Shrinkage uses the method of
<cite>Van Houwelingen and Le Cessie (1990)</cite> (similar to
<cite>Copas, 1983</cite>).  The shrinkage factor is
</p>
<p style="text-align: center;"><code class="reqn">\frac{1-\frac{(1-R2)(n-1)}{n-k-1}}{R2}</code>
</p>

<p>where R2 is the apparent <code class="reqn">R^2</code>d for predicting the
variable, n is the number of non-missing values, and k is
the effective number of degrees of freedom (aside from intercepts).  A
heuristic estimate is used for k:
<code>A - 1 + sum(max(0,Bi - 1))/m + m</code>, where
A is the number of d.f. required to represent the variable being
predicted, the Bi are the number of columns required to
represent all the other variables, and m is the number of all
other variables.  Division by m is done because the
transformations for the other variables are fixed at their current
transformations the last time they were being predicted.  The
<code class="reqn">+ m</code> term comes from the number of coefficients estimated
on the right hand side, whether by least squares or canonical
variates.  If a shrinkage factor is negative, it is set to 0.  The
shrinkage factor is the ratio of the adjusted <code class="reqn">R^2</code>d to
the ordinary <code class="reqn">R^2</code>d. The adjusted <code class="reqn">R^2</code>d is
</p>
<p style="text-align: center;"><code class="reqn">1-\frac{(1-R2)(n-1)}{n-k-1}</code>
</p>

<p>which is also set to zero if it is negative.  If <code>shrink=FALSE</code>
and the adjusted <code class="reqn">R^2</code>s are much smaller than the
ordinary <code class="reqn">R^2</code>s, you may want to run <code>transcan</code>
with <code>shrink=TRUE</code>.
</p>
<p>Canonical variates are scaled to have variance of 1.0, by multiplying
canonical coefficients from <code><a href="stats.html#topic+cancor">cancor</a></code> by 
<code class="reqn">\sqrt{n-1}</code>.
</p>
<p>When specifying a non-<span class="pkg">rms</span> library fitting function to
<code>fit.mult.impute</code> (e.g., <code><a href="stats.html#topic+lm">lm</a></code>, <code><a href="stats.html#topic+glm">glm</a></code>),
running the result of <code>fit.mult.impute</code> through that fit's
<code><a href="base.html#topic+summary">summary</a></code> method will not use the imputation-adjusted
variances.  You may obtain the new variances using <code>fit$var</code> or
<code>vcov(fit)</code>.
</p>
<p>When you specify a <span class="pkg">rms</span> function to <code>fit.mult.impute</code> (e.g.
<code><a href="rms.html#topic+lrm">lrm</a></code>, <code><a href="rms.html#topic+ols">ols</a></code>, <code><a href="rms.html#topic+cph">cph</a></code>,
<code><a href="rms.html#topic+psm">psm</a></code>, <code><a href="rms.html#topic+bj">bj</a></code>, <code><a href="rms.html#topic+Rq">Rq</a></code>,
<code><a href="rms.html#topic+Gls">Gls</a></code>, <code><a href="rms.html#topic+Glm">Glm</a></code>), automatically computed
transformation  parameters (e.g., knot locations for
<code><a href="rms.html#topic+rcs">rcs</a></code>) that are estimated for the first imputation are
used for all other imputations.  This ensures that knot locations will
not vary, which would change the meaning of the regression
coefficients.
</p>
<p>Warning: even though <code>fit.mult.impute</code> takes imputation into
account when estimating variances of regression coefficient, it does
not take into account the variation that results from estimation of
the shapes and regression coefficients of the customized imputation
equations. Specifying <code>shrink=TRUE</code> solves a small part of this
problem.  To fully account for all sources of variation you should
consider putting the <code>transcan</code> invocation inside a bootstrap or
loop, if execution time allows.  Better still, use
<code><a href="#topic+aregImpute">aregImpute</a></code> or a package such as  as <span class="pkg">mice</span> that uses
real Bayesian posterior realizations to multiply impute missing values
correctly.
</p>
<p>It is strongly recommended that you use the <span class="pkg">Hmisc</span> <code><a href="#topic+naclus">naclus</a></code>
function to determine is there is a good basis for imputation.
<code><a href="#topic+naclus">naclus</a></code> will tell you, for example, if systolic blood
pressure is missing whenever diastolic blood pressure is missing.  If
the only variable that is well correlated with diastolic bp is
systolic bp, there is no basis for imputing diastolic bp in this case.
</p>
<p>At present, <code>predict</code> does not work with multiple imputation.
</p>
<p>When calling <code>fit.mult.impute</code> with <code><a href="stats.html#topic+glm">glm</a></code> as the
<code>fitter</code> argument, if you need to pass a <code>family</code> argument
to <code><a href="stats.html#topic+glm">glm</a></code> do it by quoting the family, e.g.,
<code>family="binomial"</code>.
</p>
<p><code>fit.mult.impute</code> will not work with proportional odds models
when regression imputation was used (as opposed to predictive mean
matching).  That's because regression imputation will create values of
the response variable that did not exist in the dataset, altering the
intercept terms in the model.
</p>
<p>You should be able to use a variable in the formula given to
<code>fit.mult.impute</code> as a numeric variable in the regression model
even though it was a factor variable in the invocation of
<code>transcan</code>.  Use for example <code>fit.mult.impute(y ~ codes(x),
    lrm, trans)</code> (thanks to Trevor Thompson
<a href="mailto:trevor@hp5.eushc.org">trevor@hp5.eushc.org</a>).
</p>
<p>Here is an outline of the steps necessary to impute baseline variables
using the <code>dtrans</code> argument, when the analysis to be repeated by
<code>fit.mult.impute</code> is a longitudinal analysis (using
e.g. <code>Gls</code>).
</p>

<ol>
<li><p> Create a one row per subject data frame containing baseline
variables plus follow-up variables that are assigned to windows.  For
example, you may have dozens of repeated measurements over years but
you capture the measurements at the times measured closest to 1, 2,
and 3 years after study entry
</p>
</li>
<li><p> Make sure the dataset contains the subject ID
</p>
</li>
<li><p> This dataset becomes the one passed to <code>aregImpute</code> as
<code>data=</code>.  You will be imputing missing baseline variables from
follow-up measurements defined at fixed times.
</p>
</li>
<li><p> Have another dataset with all the non-missing follow-up values
on it, one record per measurement time per subject.  This dataset
should not have the baseline variables on it, and the follow-up
measurements should not be named the same as the baseline variable(s);
the subject ID must also appear
</p>
</li>
<li><p> Add the dtrans argument to <code>fit.mult.impute</code> to define a
function with one argument representing the one record per subject
dataset with missing values filled it from the current imputation.
This function merges the above 2 datasets; the returned value of this
function is the merged data frame.
</p>
</li>
<li><p> This merged-on-the-fly dataset is the one handed by <code>fit.mult.impute</code> to your fitting function, so  variable names in the formula given to <code>fit.mult.impute</code> must matched the names created by the merge
</p>
</li></ol>



<h3>Value</h3>

<p>For <code>transcan</code>, a list of class &lsquo;<span class="samp">&#8288;transcan&#8288;</span>&rsquo; with elements
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p> (with the function call)</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p> (number of  iterations done)</p>
</td></tr>
<tr><td><code>rsq</code>, <code>rsq.adj</code></td>
<td>

<p>containing the <code class="reqn">R^2</code>s and adjusted
<code class="reqn">R^2</code>s achieved in predicting each variable from all
the others
</p>
</td></tr>
<tr><td><code>categorical</code></td>
<td>

<p>the values supplied for <code>categorical</code>
</p>
</td></tr>
<tr><td><code>asis</code></td>
<td>

<p>the values supplied for <code>asis</code>
</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>

<p>the within-variable coefficients used to compute the first
canonical variate
</p>
</td></tr>
<tr><td><code>xcoef</code></td>
<td>

<p>the (possibly shrunk) across-variables coefficients of the first
canonical variate that predicts each variable in-turn.
</p>
</td></tr>
<tr><td><code>parms</code></td>
<td>

<p>the parameters of the transformation (knots for splines, contrast
matrix for categorical variables)  
</p>
</td></tr>
<tr><td><code>fillin</code></td>
<td>

<p>the initial estimates for missing values (<code>NA</code> if variable
never missing)
</p>
</td></tr>
<tr><td><code>ranges</code></td>
<td>

<p>the matrix of ranges of the transformed variables (min and max in
first and secondrow)
</p>
</td></tr>
<tr><td><code>scale</code></td>
<td>

<p>a vector of scales used to determine convergence for a
transformation.
</p>
</td></tr>
<tr><td><code>formula</code></td>
<td>

<p>the formula (if <code>x</code> was a formula)
</p>
</td></tr>
</table>
<p>, and optionally a vector of shrinkage factors used for predicting
each variable from the others.  For <code>asis</code> variables, the scale
is the average absolute difference about the median.  For other
variables it is unity, since canonical variables are standardized.
For <code>xcoef</code>, row i has the coefficients to predict
transformed variable i, with the column for the coefficient of
variable i set to <code>NA</code>.  If <code>imputed=TRUE</code> was given,
an optional element <code>imputed</code> also appears.  This is a list with
the vector of imputed values (on the original scale) for each variable
containing <code>NA</code>s.  Matrices rather than vectors are returned if
<code>n.impute</code> is given.  If <code>trantab=TRUE</code>, the <code>trantab</code>
element also appears, as described above.  If <code>n.impute &gt; 0</code>,
<code>transcan</code> also returns a list <code>residuals</code> that can be used
for future multiple imputation.
</p>
<p><code>impute</code> returns a vector (the same length as <code>var</code>) of
class &lsquo;<span class="samp">&#8288;impute&#8288;</span>&rsquo; with <code>NA</code> values imputed.  
</p>
<p><code>predict</code> returns a matrix with the same number of columns or
variables as were in <code>x</code>.
</p>
<p><code>fit.mult.impute</code> returns a fit object that is a modification of
the fit object created by fitting the completed dataset for the final
imputation.  The <code>var</code> matrix in the fit object has the
imputation-corrected variance-covariance matrix.  <code>coefficients</code>
is the average (over imputations) of the coefficient vectors,
<code>variance.inflation.impute</code> is a vector containing the ratios of
the diagonals of the between-imputation variance matrix to the
diagonals of the average apparent (within-imputation) variance
matrix. <code>missingInfo</code> is
<cite>Rubin's rate of missing information</cite> and <code>dfmi</code> is
<cite>Rubin's degrees of freedom for a t-statistic</cite>
for testing a single parameter.  The last two objects are vectors
corresponding to the diagonal of the variance matrix.  The class
<code>"fit.mult.impute"</code> is prepended to the other classes produced by
the fitting function.
</p>
<p>When <code>method</code> is not <code>'ordinary'</code>, i.e., stacking is used,
<code>fit.mult.impute</code> returns a modified fit object that is computed
on all completed datasets combined, with most all statistics that are
functions of the sample size corrected to the real sample size.
Elements in the fit such as <code>residuals</code> will have length equal to
the real sample size times the number of imputations.
</p>
<p><code>fit.mult.impute</code> stores <code>intercepts</code> attributes in the
coefficient matrix and in <code>var</code> for <code>orm</code> fits.
</p>


<h3>Side Effects</h3>

<p>prints, plots, and <code>impute.transcan</code> creates new variables.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell  <br />
Department of Biostatistics  <br />
Vanderbilt University  <br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Kuhfeld, Warren F: The PRINQUAL Procedure.  SAS/STAT User's Guide, Fourth
Edition, Volume 2, pp. 1265&ndash;1323, 1990.
</p>
<p>Van Houwelingen JC, Le Cessie S: Predictive value of statistical models.
Statistics in Medicine 8:1303&ndash;1325, 1990.
</p>
<p>Copas JB: Regression, prediction and shrinkage. JRSS B 45:311&ndash;354, 1983.
</p>
<p>He X, Shen L: Linear regression after spline transformation.
Biometrika 84:474&ndash;481, 1997.
</p>
<p>Little RJA, Rubin DB: Statistical Analysis with Missing Data.  New
York: Wiley, 1987.
</p>
<p>Rubin DJ, Schenker N: Multiple imputation in health-care databases: An
overview and some applications.  Stat in Med 10:585&ndash;598, 1991.
</p>
<p>Faris PD, Ghali WA, et al:Multiple imputation versus data enhancement
for dealing with missing data in observational health care outcome
analyses.  J Clin Epidem 55:184&ndash;191, 2002.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+aregImpute">aregImpute</a></code>, <code><a href="#topic+impute">impute</a></code>, <code><a href="#topic+naclus">naclus</a></code>,
<code><a href="#topic+naplot">naplot</a></code>, <code><a href="acepack.html#topic+ace">ace</a></code>,
<code><a href="acepack.html#topic+avas">avas</a></code>, <code><a href="stats.html#topic+cancor">cancor</a></code>,
<code><a href="stats.html#topic+prcomp">prcomp</a></code>, <code><a href="#topic+rcspline.eval">rcspline.eval</a></code>,
<code><a href="stats.html#topic+lsfit">lsfit</a></code>, <code><a href="stats.html#topic+approx">approx</a></code>, <code><a href="#topic+datadensity">datadensity</a></code>,
<code><a href="mice.html#topic+mice">mice</a></code>, <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>,
<code><a href="rms.html#topic+processMI">processMI</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
x &lt;- cbind(age, disease, blood.pressure, pH)  
#cbind will convert factor object `disease' to integer
par(mfrow=c(2,2))
x.trans &lt;- transcan(x, categorical="disease", asis="pH",
                    transformed=TRUE, imputed=TRUE)
summary(x.trans)  #Summary distribution of imputed values, and R-squares
f &lt;- lm(y ~ x.trans$transformed)   #use transformed values in a regression
#Now replace NAs in original variables with imputed values, if not
#using transformations
age            &lt;- impute(x.trans, age)
disease        &lt;- impute(x.trans, disease)
blood.pressure &lt;- impute(x.trans, blood.pressure)
pH             &lt;- impute(x.trans, pH)
#Do impute(x.trans) to impute all variables, storing new variables under
#the old names
summary(pH)       #uses summary.impute to tell about imputations
                  #and summary.default to tell about pH overall
# Get transformed and imputed values on some new data frame xnew
newx.trans     &lt;- predict(x.trans, xnew)
w              &lt;- predict(x.trans, xnew, type="original")
age            &lt;- w[,"age"]            #inserts imputed values
blood.pressure &lt;- w[,"blood.pressure"]
Function(x.trans)  #creates .age, .disease, .blood.pressure, .pH()
#Repeat first fit using a formula
x.trans &lt;- transcan(~ age + disease + blood.pressure + I(pH), 
                    imputed=TRUE)
age &lt;- impute(x.trans, age)
predict(x.trans, expand.grid(age=50, disease="pneumonia",
        blood.pressure=60:260, pH=7.4))
z &lt;- transcan(~ age + factor(disease.code),  # disease.code categorical
              transformed=TRUE, trantab=TRUE, imputed=TRUE, pl=FALSE)
ggplot(z, scale=TRUE)
plot(z$transformed)

## End(Not run)


# Multiple imputation and estimation of variances and covariances of
# regression coefficient estimates accounting for imputation
set.seed(1)
x1 &lt;- factor(sample(c('a','b','c'),100,TRUE))
x2 &lt;- (x1=='b') + 3*(x1=='c') + rnorm(100)
y  &lt;- x2 + 1*(x1=='c') + rnorm(100)
x1[1:20] &lt;- NA
x2[18:23] &lt;- NA
d &lt;- data.frame(x1,x2,y)
n &lt;- naclus(d)
plot(n); naplot(n)  # Show patterns of NAs
f  &lt;- transcan(~y + x1 + x2, n.impute=10, shrink=FALSE, data=d)
options(digits=3)
summary(f)


f  &lt;- transcan(~y + x1 + x2, n.impute=10, shrink=TRUE, data=d)
summary(f)


h &lt;- fit.mult.impute(y ~ x1 + x2, lm, f, data=d)
# Add ,fit.reps=TRUE to save all fit objects in h, then do something like:
# for(i in 1:length(h$fits)) print(summary(h$fits[[i]]))


diag(vcov(h))


h.complete &lt;- lm(y ~ x1 + x2, na.action=na.omit)
h.complete
diag(vcov(h.complete))


# Note: had the rms ols function been used in place of lm, any
# function run on h (anova, summary, etc.) would have automatically
# used imputation-corrected variances and covariances


# Example demonstrating how using the multinomial logistic model
# to impute a categorical variable results in a frequency
# distribution of imputed values that matches the distribution
# of non-missing values of the categorical variable


## Not run: 
set.seed(11)
x1 &lt;- factor(sample(letters[1:4], 1000,TRUE))
x1[1:200] &lt;- NA
table(x1)/sum(table(x1))
x2 &lt;- runif(1000)
z  &lt;- transcan(~ x1 + I(x2), n.impute=20, impcat='multinom')
table(z$imputed$x1)/sum(table(z$imputed$x1))

# Here is how to create a completed dataset
d &lt;- data.frame(x1, x2)
z &lt;- transcan(~x1 + I(x2), n.impute=5, data=d)
imputed &lt;- impute(z, imputation=1, data=d,
                  list.out=TRUE, pr=FALSE, check=FALSE)
sapply(imputed, function(x)sum(is.imputed(x)))
sapply(imputed, function(x)sum(is.na(x)))

## End(Not run)

# Do single imputation and create a filled-in data frame
z &lt;- transcan(~x1 + I(x2), data=d, imputed=TRUE)
imputed &lt;- as.data.frame(impute(z, data=d, list.out=TRUE))

# Example where multiple imputations are for basic variables and
# modeling is done on variables derived from these


set.seed(137)
n &lt;- 400
x1 &lt;- runif(n)
x2 &lt;- runif(n)
y  &lt;- x1*x2 + x1/(1+x2) + rnorm(n)/3
x1[1:5] &lt;- NA
d &lt;- data.frame(x1,x2,y)
w &lt;- transcan(~ x1 + x2 + y, n.impute=5, data=d)
# Add ,show.imputed.actual for graphical diagnostics
## Not run: 
g &lt;- fit.mult.impute(y ~ product + ratio, ols, w,
                     data=data.frame(x1,x2,y),
                     derived=expression({
                       product &lt;- x1*x2
                       ratio   &lt;- x1/(1+x2)
                       print(cbind(x1,x2,x1*x2,product)[1:6,])}))

## End(Not run)


# Here's a method for creating a permanent data frame containing
# one set of imputed values for each variable specified to transcan
# that had at least one NA, and also containing all the variables
# in an original data frame.  The following is based on the fact
# that the default output location for impute.transcan is
# given by the global environment


## Not run: 
xt &lt;- transcan(~. , data=mine,
               imputed=TRUE, shrink=TRUE, n.impute=10, trantab=TRUE)
attach(mine, use.names=FALSE)
impute(xt, imputation=1) # use first imputation
# omit imputation= if using single imputation
detach(1, 'mine2')

## End(Not run)


# Example of using invertTabulated outside transcan
x    &lt;- c(1,2,3,4,5,6,7,8,9,10)
y    &lt;- c(1,2,3,4,5,5,5,5,9,10)
freq &lt;- c(1,1,1,1,1,2,3,4,1,1)
# x=5,6,7,8 with prob. .1 .2 .3 .4 when y=5
# Within a tolerance of .05*(10-1) all y's match exactly
# so the distance measure does not play a role
set.seed(1)      # so can reproduce
for(inverse in c('linearInterp','sample'))
 print(table(invertTabulated(x, y, freq, rep(5,1000), inverse=inverse)))


# Test inverse='sample' when the estimated transformation is
# flat on the right.  First show default imputations
set.seed(3)
x &lt;- rnorm(1000)
y &lt;- pmin(x, 0)
x[1:500] &lt;- NA
for(inverse in c('linearInterp','sample')) {
par(mfrow=c(2,2))
  w &lt;- transcan(~ x + y, imputed.actual='hist',
                inverse=inverse, curtail=FALSE,
                data=data.frame(x,y))
  if(inverse=='sample') next
# cat('Click mouse on graph to proceed\n')
# locator(1)
}

## Not run: 
# While running multiple imputation for a logistic regression model
# Run the rms package validate and calibrate functions and save the
# results in w$funresults
a &lt;- aregImpute(~ x1 + x2 + y, data=d, n.impute=10)
require(rms)
g &lt;- function(fit)
  list(validate=validate(fit, B=50), calibrate=calibrate(fit, B=75))
w &lt;- fit.mult.impute(y ~ x1 + x2, lrm, a, data=d, fun=g,
                     fitargs=list(x=TRUE, y=TRUE))
# Get all validate results in it's own list of length 10
r &lt;- w$funresults
val &lt;- lapply(r, function(x) x$validate)
cal &lt;- lapply(r, function(x) x$calibrate)
# See rms processMI and https://hbiostat.org/rmsc/validate.html#sec-val-mival

## End(Not run)

## Not run: 
# Account for within-subject correlation using the robust cluster sandwich
# covariance estimate in conjunction with Rubin's rule for multiple imputation
# rms package must be installed
a &lt;- aregImpute(..., data=d)
f &lt;- fit.mult.impute(y ~ x1 + x2, lrm, a, n.impute=30, data=d, cluster=d$id)
# Get likelihood ratio chi-square tests accounting for missingness
a &lt;- aregImpute(..., data=d)
h &lt;- fit.mult.impute(y ~ x1 + x2, lrm, a, n.impute=40, data=d, lrt=TRUE)
processMI(h, which='anova')   # processMI is in rms

## End(Not run)
</code></pre>

<hr>
<h2 id='translate'>
Translate Vector or Matrix of Text Strings
</h2><span id='topic+translate'></span>

<h3>Description</h3>

<p>Uses the UNIX tr command to translate any character in <code>old</code> in
<code>text</code> to the corresponding character in <code>new</code>.  If multichar=T
or <code>old</code> and <code>new</code> have more than one element, or each have one element
but they have different numbers of characters,
uses the UNIX <code>sed</code> command to translate the series of characters in
<code>old</code> to the series in <code>new</code> when these characters occur in <code>text</code>.
If <code>old</code> or <code>new</code> contain a backslash, you sometimes have to quadruple
it to make the UNIX command work. If they contain a forward slash,
preceed it by two backslashes.  Invokes the builtin chartr function if
<code>multichar=FALSE</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>translate(text, old, new, multichar=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="translate_+3A_text">text</code></td>
<td>

<p>scalar, vector, or matrix of character strings to translate.
</p>
</td></tr>
<tr><td><code id="translate_+3A_old">old</code></td>
<td>

<p>vector old characters
</p>
</td></tr>
<tr><td><code id="translate_+3A_new">new</code></td>
<td>

<p>corresponding vector of new characters
</p>
</td></tr>
<tr><td><code id="translate_+3A_multichar">multichar</code></td>
<td>
<p>See above.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object like text but with characters translated
</p>


<h3>See Also</h3>

<p>grep</p>


<h3>Examples</h3>

<pre><code class='language-R'>translate(c("ABC","DEF"),"ABCDEFG", "abcdefg")
translate("23.12","[.]","\\cdot ") # change . to \cdot
translate(c("dog","cat","tiger"),c("dog","cat"),c("DOG","CAT"))
# S-Plus gives  [1] "DOG"   "CAT"   "tiger" - check discrepency
translate(c("dog","cat2","snake"),c("dog","cat"),"animal")
# S-Plus gives  [1] "animal"  "animal2" "snake" 
</code></pre>

<hr>
<h2 id='trunc.POSIXt'>Return the floor, ceiling, or rounded value of date or time to
specified unit.</h2><span id='topic+truncPOSIXt'></span><span id='topic+ceil.POSIXt'></span><span id='topic+ceil'></span><span id='topic+ceil.default'></span><span id='topic+roundPOSIXt'></span>

<h3>Description</h3>

<p><code>truncPOSIXt</code> returns the date truncated to the specified unit.
<code>ceil.POSIXt</code> returns next ceiling of the date at the unit selected in
<code>units</code>.
<code>roundPOSIXt</code> returns the date or time value rounded to nearest
specified unit selected in <code>digits</code>.
</p>
<p><code>truncPOSIXt</code> and <code>roundPOSIXt</code> have been extended from
the <code>base</code> package functions <code>trunc.POSIXt</code> and
<code>round.POSIXt</code> which in the future will add the other time units
we need.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ceil(x, units,...)
## Default S3 method:
ceil(x, units, ...)
truncPOSIXt(x, units = c("secs", "mins", "hours", "days",
"months", "years"), ...)
## S3 method for class 'POSIXt'
ceil(x, units = c("secs", "mins", "hours", "days",
"months", "years"), ...)
roundPOSIXt(x, digits = c("secs", "mins", "hours", "days", "months", "years"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trunc.POSIXt_+3A_x">x</code></td>
<td>
<p> date to be ceilinged, truncated, or rounded </p>
</td></tr>
<tr><td><code id="trunc.POSIXt_+3A_units">units</code></td>
<td>
<p> unit to that is is rounded up or down to. </p>
</td></tr>
<tr><td><code id="trunc.POSIXt_+3A_digits">digits</code></td>
<td>

<p>same as <code>units</code> but different name to be compatible
with <code><a href="base.html#topic+round">round</a></code> generic.
</p>
</td></tr>
<tr><td><code id="trunc.POSIXt_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>POSIXlt</code>.
</p>


<h3>Author(s)</h3>

<p> Charles Dupont </p>


<h3>See Also</h3>

 <p><code><a href="base.html#topic+Date">Date</a></code> <code><a href="base.html#topic+POSIXt">POSIXt</a></code> <code><a href="base.html#topic+POSIXlt">POSIXlt</a></code> <code><a href="base.html#topic+DateTimeClasses">DateTimeClasses</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>date &lt;- ISOdate(1832, 7, 12)
ceil(date, units='months')  # '1832-8-1'
truncPOSIXt(date, units='years')     # '1832-1-1'
roundPOSIXt(date, digits='months')    # '1832-7-1'
</code></pre>

<hr>
<h2 id='units'>
Units Attribute of a Vector  
</h2><span id='topic+units'></span><span id='topic+units.default'></span><span id='topic+units.Surv'></span><span id='topic+units+3C-.default'></span>

<h3>Description</h3>

<p>Sets or retrieves the <code>"units"</code> attribute of an object.
For <code>units.default</code> replaces the builtin
version, which only works for time series objects.  If the variable is
also given a <code>label</code>, subsetting (using <code>[.labelled</code>) will
retain the <code>"units"</code> attribute.  For a <code>Surv</code> object,
<code>units</code> first looks for an overall <code>"units"</code> attribute, then
it looks for <code>units</code> for the <code>time2</code> variable then for <code>time1</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>units(x, ...)
## Default S3 method:
units(x, none='', ...)
## S3 method for class 'Surv'
units(x, none='', ...)
## Default S3 replacement method:
units(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="units_+3A_x">x</code></td>
<td>
<p>any object</p>
</td></tr>
<tr><td><code id="units_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
<tr><td><code id="units_+3A_value">value</code></td>
<td>
<p>the units of the object, or &quot;&quot;</p>
</td></tr>
<tr><td><code id="units_+3A_none">none</code></td>
<td>
<p>value to which to set result if no appropriate attribute is
found</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the units attribute of x, if any; otherwise, the <code>units</code> attribute of
the <code>tspar</code> attribute of <code>x</code> if any; otherwise the value
<code>none</code>.  Handling for <code>Surv</code> objects is different (see above).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+label">label</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>require(survival)
fail.time &lt;- c(10,20)
units(fail.time) &lt;- "Day"
describe(fail.time)
S &lt;- Surv(fail.time)
units(S)

label(fail.time) &lt;- 'Failure Time'
fail.time
</code></pre>

<hr>
<h2 id='upData'>
Update a Data Frame or Cleanup a Data Frame after Importing
</h2><span id='topic+cleanup.import'></span><span id='topic+upData'></span><span id='topic+dataframeReduce'></span>

<h3>Description</h3>

<p><code>cleanup.import</code> will correct errors and shrink
the size of data frames.  By default, double precision numeric
variables are changed to integer when they contain no fractional components. 
Infinite values or values greater than 1e20 in absolute value are set
to NA.  This solves problems of importing Excel spreadsheets that
contain occasional character values for numeric columns, as S
converts these to <code>Inf</code> without warning.  There is also an option to
convert variable names to lower case and to add labels to variables.
The latter can be made easier by importing a CNTLOUT dataset created
by SAS PROC FORMAT and using the <code>sasdict</code> option as shown in the
example below.  <code>cleanup.import</code> can also transform character or
factor variables to dates.
</p>
<p><code>upData</code> is a function facilitating the updating of a data frame
without attaching it in search position one.  New variables can be
added, old variables can be modified, variables can be removed or renamed, and
<code>"labels"</code> and <code>"units"</code> attributes can be provided.
Observations can be subsetted.  Various checks
are made for errors and inconsistencies, with warnings issued to help
the user.  Levels of factor variables can be replaced, especially
using the <code>list</code> notation of the standard <code>merge.levels</code>
function.  Unless <code>force.single</code> is set to <code>FALSE</code>, 
<code>upData</code> also converts double precision vectors to integer if no
fractional values are present in 
a vector.  <code>upData</code> is also used to process R workspace objects
created by StatTransfer, which puts variable and value labels as attributes on
the data frame rather than on each variable. If such attributes are
present, they are used to define all the labels and value labels
(through conversion to factor variables) before any label changes
take place, and <code>force.single</code> is set to a default of
<code>FALSE</code>, as StatTransfer already does conversion to integer.
</p>
<p>Variables having labels but not classed <code>"labelled"</code> (e.g., data
imported using the <code>haven</code> package) have that class added to them
by <code>upData</code>.
</p>
<p>The <code>dataframeReduce</code> function removes variables from a data frame
that are problematic for certain analyses.  Variables can be removed
because the fraction of missing values exceeds a threshold, because they
are character or categorical variables having too many levels, or
because they are binary and have too small a prevalence in one of the
two values.  Categorical variables can also have their levels combined
when a level is of low prevalence.  A data frame listing actions take
is return as attribute <code>"info"</code> to the main returned data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cleanup.import(obj, labels, lowernames=FALSE, 
               force.single=TRUE, force.numeric=TRUE, rmnames=TRUE,
               big=1e20, sasdict, print, datevars=NULL, datetimevars=NULL,
               dateformat='%F',
               fixdates=c('none','year'),
               autodate=FALSE, autonum=FALSE, fracnn=0.3,
               considerNA=NULL, charfactor=FALSE)

upData(object, ..., 
       subset, rename, drop, keep, labels, units, levels, force.single=TRUE,
       lowernames=FALSE, caplabels=FALSE, moveUnits=FALSE,
       charfactor=FALSE, print=TRUE, html=FALSE)

dataframeReduce(data, fracmiss=1, maxlevels=NULL,  minprev=0, print=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="upData_+3A_obj">obj</code></td>
<td>
<p>a data frame or list</p>
</td></tr>
<tr><td><code id="upData_+3A_object">object</code></td>
<td>
<p>a data frame or list</p>
</td></tr>
<tr><td><code id="upData_+3A_data">data</code></td>
<td>
<p>a data frame</p>
</td></tr>
<tr><td><code id="upData_+3A_force.single">force.single</code></td>
<td>

<p>By default, double precision variables are converted to single precision
(in S-Plus only) unless <code>force.single=FALSE</code>.
<code>force.single=TRUE</code> will also convert vectors having only integer
values to have a storage mode of integer, in R or S-Plus.
</p>
</td></tr>
<tr><td><code id="upData_+3A_force.numeric">force.numeric</code></td>
<td>

<p>Sometimes importing will cause a numeric variable to be
changed to a factor vector.  By default, <code>cleanup.import</code> will check
each factor variable to see if the levels contain only numeric values
and <code>""</code>.  In that case, the variable will be converted to numeric,
with <code>""</code> converted to NA.  Set <code>force.numeric=FALSE</code> to prevent
this behavior. 
</p>
</td></tr>
<tr><td><code id="upData_+3A_rmnames">rmnames</code></td>
<td>

<p>set to &lsquo;F&rsquo; to not have &lsquo;cleanup.import&rsquo; remove &lsquo;names&rsquo; or &lsquo;.Names&rsquo;
attributes from variables
</p>
</td></tr>
<tr><td><code id="upData_+3A_labels">labels</code></td>
<td>

<p>a character vector the same length as the number of variables in
<code>obj</code>.  These character values are taken to be variable labels in the
same order of variables in <code>obj</code>.
For <code>upData</code>, <code>labels</code> is a named list or named vector
with variables in no specific order.
</p>
</td></tr>
<tr><td><code id="upData_+3A_lowernames">lowernames</code></td>
<td>

<p>set this to <code>TRUE</code> to change variable names to lower case.
<code>upData</code> does this before applying any other changes, so variable
names given inside arguments to <code>upData</code> need to be lower case if
<code>lowernames==TRUE</code>. 
</p>
</td></tr>
<tr><td><code id="upData_+3A_big">big</code></td>
<td>

<p>a value such that values larger than this in absolute value are set to
missing by <code>cleanup.import</code>
</p>
</td></tr>
<tr><td><code id="upData_+3A_sasdict">sasdict</code></td>
<td>

<p>the name of a data frame containing a raw imported SAS PROC CONTENTS
CNTLOUT= dataset.  This is used to define variable names and to add
attributes to the new data frame specifying the original SAS dataset
name and label.
</p>
</td></tr>
<tr><td><code id="upData_+3A_print">print</code></td>
<td>

<p>set to <code>TRUE</code> or <code>FALSE</code> to force or prevent printing of the current
variable number being processed.  By default, such messages are printed if the
product of the number of variables and number of observations in <code>obj</code>
exceeds 500,000.  For <code>dataframeReduce</code> set <code>print</code> to
<code>FALSE</code> to suppress printing information about dropped or
modified variables.  Similar for <code>upData</code>.</p>
</td></tr>
<tr><td><code id="upData_+3A_datevars">datevars</code></td>
<td>
<p>character vector of names (after <code>lowernames</code> is
applied) of variables to consider as a factor or character vector
containing dates in a format matching <code>dateformat</code>.  The
default is <code>"%F"</code> which uses the yyyy-mm-dd format.</p>
</td></tr>
<tr><td><code id="upData_+3A_datetimevars">datetimevars</code></td>
<td>
<p>character vector of names (after <code>lowernames</code>
is applied) of variables to consider to be date-time variables, with
date formats as described under <code>datevars</code> followed by a space
followed by time in hh:mm:ss format.  <code>chron</code> is used to store
date-time variables.  If all times in the variable
are 00:00:00 the variable will be converted to an ordinary date variable.</p>
</td></tr>
<tr><td><code id="upData_+3A_dateformat">dateformat</code></td>
<td>
<p>for <code>cleanup.import</code> is the input format (see
<code><a href="base.html#topic+strptime">strptime</a></code>)</p>
</td></tr>
<tr><td><code id="upData_+3A_fixdates">fixdates</code></td>
<td>
<p>for any of the variables listed in <code>datevars</code>
that have a <code>dateformat</code> that <code>cleanup.import</code> understands,
specifying <code>fixdates</code> allows corrections of certain formatting
inconsistencies before the fields are attempted to be converted to
dates (the default is to assume that the <code>dateformat</code> is followed
for all observation for <code>datevars</code>).  Currently
<code>fixdates='year'</code> is implemented, which will cause 2-digit or
4-digit years to be shifted to the alternate number of digits when
<code>dateform</code> is the default <code>"%F"</code> or is <code>"%y-%m-%d"</code>,
<code>"%m/%d/%y"</code>, or <code>"%m/%d/%Y"</code>.  Two-digits years are padded with <code>20</code>
on the left.  Set <code>dateformat</code> to the desired format, not the
exceptional format.
</p>
</td></tr>
<tr><td><code id="upData_+3A_autodate">autodate</code></td>
<td>
<p>set to <code>TRUE</code> to have <code>cleanup.import</code>
determine and automatically handle factor or character 
vectors that mainly contain dates of the form YYYY-mm-dd,
mm/dd/YYYY, YYYY, or mm/YYYY, where the later two are imputed to,
respectively, July 3 and the 15th of the month.  Takes effect when
the fraction of non-dates (of non-missing values) is less than
<code>fracnn</code> to allow for some free text such as <code>"unknown"</code>.
Attributes 
<code>special.miss</code> and <code>imputed</code> are created for the vector so
that <code>describe()</code> will inform the user.  Illegal values are
converted to <code>NA</code>s and stored in the <code>special.miss</code> attribute.</p>
</td></tr>
<tr><td><code id="upData_+3A_autonum">autonum</code></td>
<td>
<p>set to <code>TRUE</code> to have <code>cleanup.import</code>
examine (after <code>autodate</code>) character and factor variables to
see if they are legal numerics exact for at most a fraction of
<code>fracnn</code> of non-missing non-numeric values.  Qualifying variables are
converted to numeric, and illegal values set to <code>NA</code> and stored in
the <code>special.miss</code> attribute to enhance <code>describe</code> output.</p>
</td></tr>
<tr><td><code id="upData_+3A_fracnn">fracnn</code></td>
<td>
<p>see <code>autodate</code> and <code>autonum</code></p>
</td></tr>
<tr><td><code id="upData_+3A_considerna">considerNA</code></td>
<td>
<p>for <code>autodate</code> and <code>autonum</code>, considers
character values in the vector <code>considerNA</code> to be the same as
<code>NA</code>.  Leading and trailing white space and upper/lower case
are ignored.</p>
</td></tr> 
<tr><td><code id="upData_+3A_charfactor">charfactor</code></td>
<td>
<p>set to <code>TRUE</code> to change character variables to
factors if they have fewer than n/2 unique values.  Null strings and
blanks are converted to <code>NA</code>s.</p>
</td></tr>
<tr><td><code id="upData_+3A_...">...</code></td>
<td>

<p>for <code>upData</code>, one or more expressions of the form
<code>variable=expression</code>, to derive new variables or change old ones.
</p>
</td></tr>
<tr><td><code id="upData_+3A_subset">subset</code></td>
<td>
<p>an expression that evaluates to a logical vector
specifying which rows of <code>object</code> should be retained.  The
expressions should use the original variable names, i.e., before any
variables are renamed but after <code>lowernames</code> takes effect.</p>
</td></tr>
<tr><td><code id="upData_+3A_rename">rename</code></td>
<td>

<p>list or named vector specifying old and new names for variables.  Variables are
renamed before any other operations are done.  For example, to rename
variables <code>age</code> and <code>sex</code> to respectively <code>Age</code> and
<code>gender</code>, specify <code>rename=list(age="Age", sex="gender")</code> or
<code>rename=c(age=...)</code>. 
</p>
</td></tr>
<tr><td><code id="upData_+3A_drop">drop</code></td>
<td>
<p>a vector of variable names to remove from the data frame</p>
</td></tr>
<tr><td><code id="upData_+3A_keep">keep</code></td>
<td>
<p>a vector of variable names to keep, with all other
variables dropped</p>
</td></tr>
<tr><td><code id="upData_+3A_units">units</code></td>
<td>

<p>a named vector or list defining <code>"units"</code> attributes of
variables, in no specific order
</p>
</td></tr>
<tr><td><code id="upData_+3A_levels">levels</code></td>
<td>

<p>a named list defining <code>"levels"</code> attributes for factor variables, in
no specific order.  The values in this list may be character vectors
redefining <code>levels</code> (in order) or another list (see
<code>merge.levels</code> if using S-Plus).
</p>
</td></tr>
<tr><td><code id="upData_+3A_caplabels">caplabels</code></td>
<td>

<p>set to <code>TRUE</code> to capitalize the first letter of each word in
each variable label
</p>
</td></tr>
<tr><td><code id="upData_+3A_moveunits">moveUnits</code></td>
<td>

<p>set to <code>TRUE</code> to look for units of measurements in variable
labels and move them to a <code>"units"</code> attribute.  If an expression
in a label is enclosed in parentheses or brackets it is assumed to be
units if <code>moveUnits=TRUE</code>.</p>
</td></tr>
<tr><td><code id="upData_+3A_html">html</code></td>
<td>
<p>set to <code>TRUE</code> to print conversion information as html
vertabim at 0.6 size.  The user will need to put
<code>results='asis'</code> in a <code>knitr</code> chunk header to properly
render this output.</p>
</td></tr>
<tr><td><code id="upData_+3A_fracmiss">fracmiss</code></td>
<td>
<p>the maximum permissable proportion of <code>NA</code>s for a
variable to be kept.  Default is to keep all variables no matter how
many <code>NA</code>s are present.</p>
</td></tr>
<tr><td><code id="upData_+3A_maxlevels">maxlevels</code></td>
<td>
<p>the maximum number of levels of a character or
categorical or factor variable before the variable is dropped</p>
</td></tr>
<tr><td><code id="upData_+3A_minprev">minprev</code></td>
<td>
<p>the minimum proportion of non-missing observations in a
category for a binary variable to be retained, and the minimum
relative frequency of a category before it will be combined with other
small categories</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a new data frame</p>


<h3>Author(s)</h3>

<p>Frank Harrell, Vanderbilt University
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sas.get">sas.get</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code>, <code><a href="#topic+describe">describe</a></code>,
<code><a href="#topic+label">label</a></code>, <code><a href="utils.html#topic+read.csv">read.csv</a></code>, <code><a href="base.html#topic+strptime">strptime</a></code>,
<code><a href="base.html#topic+POSIXct">POSIXct</a></code>,<code><a href="base.html#topic+Date">Date</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dat &lt;- read.table('myfile.asc')
dat &lt;- cleanup.import(dat)

## End(Not run)
dat &lt;- data.frame(a=1:3, d=c('01/02/2004',' 1/3/04',''))
cleanup.import(dat, datevars='d', dateformat='%m/%d/%y', fixdates='year')

dat &lt;- data.frame(a=(1:3)/7, y=c('a','b1','b2'), z=1:3)
dat2 &lt;- upData(dat, x=x^2, x=x-5, m=x/10, 
               rename=c(a='x'), drop='z',
               labels=c(x='X', y='test'),
               levels=list(y=list(a='a',b=c('b1','b2'))))
dat2
describe(dat2)
dat &lt;- dat2    # copy to original name and delete dat2 if OK
rm(dat2)
dat3 &lt;- upData(dat, X=X^2, subset = x &lt; (3/7)^2 - 5, rename=c(x='X'))

# Remove hard to analyze variables from a redundancy analysis of all
# variables in the data frame
d &lt;- dataframeReduce(dat, fracmiss=.1, minprev=.05, maxlevels=5)
# Could run redun(~., data=d) at this point or include dataframeReduce
# arguments in the call to redun

# If you import a SAS dataset created by PROC CONTENTS CNTLOUT=x.datadict,
# the LABELs from this dataset can be added to the data.  Let's also
# convert names to lower case for the main data file
## Not run: 
mydata2 &lt;- cleanup.import(mydata2, lowernames=TRUE, sasdict=datadict)

## End(Not run)
</code></pre>

<hr>
<h2 id='upFirst'>Change First Letters to Upper Case</h2><span id='topic+upFirst'></span>

<h3>Description</h3>

<p>Changes the first letter of each word in a string to upper case, keeping selected words in lower case.  Words containing at least 2 capital letters are kept as-is.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>upFirst(txt, lower = FALSE, alllower = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="upFirst_+3A_txt">txt</code></td>
<td>
<p>a character vector</p>
</td></tr>
<tr><td><code id="upFirst_+3A_lower">lower</code></td>
<td>
<p>set to <code>TRUE</code> to make only the very first letter of the string upper case, and to keep words with at least 2 capital letters in their original form</p>
</td></tr>
<tr><td><code id="upFirst_+3A_alllower">alllower</code></td>
<td>
<p>set to <code>TRUE</code> to make every word start with lower case unless it has at least 2 caps</p>
</td></tr>
</table>


<h3>References</h3>

<p><a href="https://en.wikipedia.org/wiki/Letter_case#Headings_and_publication_titles">https://en.wikipedia.org/wiki/Letter_case#Headings_and_publication_titles</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>upFirst(c('this and that','that is Beyond question'))
</code></pre>

<hr>
<h2 id='valueTags'>Store Descriptive Information About an Object</h2><span id='topic+valueTags'></span><span id='topic+valueTags+3C-'></span><span id='topic+valueLabel'></span><span id='topic+valueLabel+3C-'></span><span id='topic+valueUnit'></span><span id='topic+valueUnit+3C-'></span><span id='topic+valueName'></span><span id='topic+valueName+3C-'></span>

<h3>Description</h3>

<p>Functions get or set useful information about the contents of the
object for later use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>valueTags(x)
valueTags(x) &lt;- value

valueLabel(x)
valueLabel(x) &lt;- value

valueName(x)
valueName(x) &lt;- value

valueUnit(x)
valueUnit(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="valueTags_+3A_x">x</code></td>
<td>

<p>an object
</p>
</td></tr>
<tr><td><code id="valueTags_+3A_value">value</code></td>
<td>

<p>for <code>valueTags&lt;-</code> a named list of value tags.
a character vector of length 1, or <code>NULL</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions store the a short name of for the contents, a longer
label that is useful for display, and the units of the contents that
is useful for display.
</p>
<p><code>valueTag</code> is an accessor, and <code>valueTag&lt;-</code> is a replacement
function for all of the value's information.
</p>
<p><code>valueName</code> is an accessor, and <code>valueName&lt;-</code> is a
replacement function for the value's name.  This name is used when a
plot or a latex table needs a short name and the variable name is not
useful.
</p>
<p><code>valueLabel</code> is an accessor, and <code>valueLabel&lt;-</code> is a
replacement function for the value's label.  The label is used in a
plots or latex tables when they need a descriptive name.
</p>
<p><code>valueUnit</code> is an accessor, and <code>valueUnit&lt;-</code> is a
replacement function for the value's unit.  The unit is used to add
unit information to the R output.
</p>


<h3>Value</h3>

<p><code>valueTag</code> returns <code>NULL</code> or a named list with each of the
named values <code>name</code>, <code>label</code>, <code>unit</code> set if they exists
in the object.
</p>
<p>For <code>valueTag&lt;-</code> returns <code>list</code>
</p>
<p>For <code>valueName</code>, <code>valueLable</code>, and <code>valueUnit</code>  returns
<code>NULL</code> or character vector of length 1.
</p>
<p>For <code>valueName&lt;-</code>, <code>valueLabel&lt;-</code>, and <code>valueUnit</code> returns <code>value</code>
</p>


<h3>Author(s)</h3>

<p>Charles Dupont</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+names">names</a></code>, <code><a href="base.html#topic+attributes">attributes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>age &lt;- c(21,65,43)
y   &lt;- 1:3
valueLabel(age) &lt;- "Age in Years"
plot(age, y, xlab=valueLabel(age))


x1 &lt;- 1:10
x2 &lt;- 10:1
valueLabel(x2) &lt;- 'Label for x2'
valueUnit(x2) &lt;- 'mmHg'
x2
x2[1:5]
dframe &lt;- data.frame(x1, x2)
Label(dframe)


##In these examples of llist, note that labels are printed after
##variable names, because of print.labelled
a &lt;- 1:3
b &lt;- 4:6
valueLabel(b) &lt;- 'B Label'
</code></pre>

<hr>
<h2 id='varclus'>
Variable Clustering
</h2><span id='topic+varclus'></span><span id='topic+print.varclus'></span><span id='topic+plot.varclus'></span><span id='topic+naclus'></span><span id='topic+naplot'></span><span id='topic+plotMultSim'></span><span id='topic+na.pattern'></span>

<h3>Description</h3>

<p>Does a hierarchical cluster analysis on variables, using the Hoeffding
D statistic, squared Pearson or Spearman correlations, or proportion
of observations for which two variables are both positive as similarity
measures.  Variable clustering is used for assessing collinearity,
redundancy, and for separating variables into clusters that can be
scored as a single variable, thus resulting in data reduction.  For
computing any of the three similarity measures, pairwise deletion of
NAs is done.  The clustering is done by <code>hclust()</code>.  A small function
<code>naclus</code> is also provided which depicts similarities in which
observations are missing for variables in a data frame.  The
similarity measure is the fraction of <code>NAs</code> in common between any two
variables.  The diagonals of this <code>sim</code> matrix are the fraction of NAs
in each variable by itself.  <code>naclus</code> also computes <code>na.per.obs</code>, the
number of missing variables in each observation, and <code>mean.na</code>, a
vector whose ith element is the mean number of missing variables other
than variable i, for observations in which variable i is missing.  The
<code>naplot</code> function makes several plots (see the <code>which</code> argument).
</p>
<p>So as to not generate too many dummy variables for multi-valued
character or categorical predictors, <code>varclus</code> will automatically
combine infrequent cells of such variables using
<code><a href="#topic+combine.levels">combine.levels</a></code>.
</p>
<p><code>plotMultSim</code> plots multiple similarity matrices, with the similarity
measure being on the x-axis of each subplot.
</p>
<p><code>na.pattern</code> prints a frequency table of all combinations of
missingness for multiple variables.  If there are 3 variables, a
frequency table entry labeled <code>110</code> corresponds to the number of
observations for which the first and second variables were missing but
the third variable was not missing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>varclus(x, similarity=c("spearman","pearson","hoeffding","bothpos","ccbothpos"),
        type=c("data.matrix","similarity.matrix"), 
        method="complete",
        data=NULL, subset=NULL, na.action=na.retain,
        trans=c("square", "abs", "none"), ...)
## S3 method for class 'varclus'
print(x, abbrev=FALSE, ...)
## S3 method for class 'varclus'
plot(x, ylab, abbrev=FALSE, legend.=FALSE, loc, maxlen, labels, ...)

naclus(df, method)
naplot(obj, which=c('all','na per var','na per obs','mean na',
                    'na per var vs mean na'), ...)

plotMultSim(s, x=1:dim(s)[3],
            slim=range(pretty(c(0,max(s,na.rm=TRUE)))),
            slimds=FALSE,
            add=FALSE, lty=par('lty'), col=par('col'),
            lwd=par('lwd'), vname=NULL, h=.5, w=.75, u=.05,
            labelx=TRUE, xspace=.35)

na.pattern(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="varclus_+3A_x">x</code></td>
<td>

<p>a formula,
a numeric matrix of predictors, or a similarity matrix.  If <code>x</code> is
a formula, <code>model.matrix</code> is used to convert it to a design matrix.
If the formula excludes an intercept (e.g., <code>~ a + b -1</code>),
the first categorical (<code>factor</code>) variable in the formula will have
dummy variables generated for all levels instead of omitting one for
the first level.  For
<code>plot</code> and <code>print</code>, <code>x</code> is an object created by
<code>varclus</code>.  For <code>na.pattern</code>, <code>x</code> is a data table, data frame,
or matrix.
</p>
<p>For <code>plotMultSim</code>, is a numeric vector specifying the ordered
unique values on the x-axis, corresponding to the third dimension of
<code>s</code>.
</p>
</td></tr>
<tr><td><code id="varclus_+3A_df">df</code></td>
<td>
<p>a data frame</p>
</td></tr>
<tr><td><code id="varclus_+3A_s">s</code></td>
<td>

<p>an array of similarity matrices.  The third dimension of this array
corresponds to different computations of similarities.  The first two
dimensions come from a single similarity matrix.  This is useful for
displaying similarity matrices computed by <code>varclus</code>, for example.  A
use for this might be to show pairwise similarities of variables
across time in a longitudinal study (see the example below).  If
<code>vname</code> is not given, <code>s</code> must have <code>dimnames</code>.
</p>
</td></tr>
<tr><td><code id="varclus_+3A_similarity">similarity</code></td>
<td>

<p>the default is to use squared Spearman correlation coefficients, which
will detect monotonic but nonlinear relationships.  You can also
specify linear correlation or Hoeffding's (1948) D statistic, which
has the advantage of being sensitive to many types
of dependence, including highly non-monotonic relationships.  For
binary data, or data to be made binary, <code>similarity="bothpos"</code> uses as
a similarity measure the proportion of observations for which two
variables are both positive.  <code>similarity="ccbothpos"</code> uses a
chance-corrected measure which is the proportion of observations for
which both variables are positive minus the product of the two
marginal proportions.  This difference is expected to be zero under
independence.  For diagonals, <code>"ccbothpos"</code> still uses the proportion
of positives for the single variable.  So <code>"ccbothpos"</code> is not really
a similarity measure, and clustering is not done.  This measure is
useful for plotting with <code>plotMultSim</code> (see the last example).
</p>
</td></tr>
<tr><td><code id="varclus_+3A_type">type</code></td>
<td>

<p>if <code>x</code> is not a formula, it may be a data matrix or a similarity matrix.
By default, it is assumed to be a data matrix.
</p>
</td></tr>
<tr><td><code id="varclus_+3A_method">method</code></td>
<td>

<p>see <code>hclust</code>.  The default, for both <code>varclus</code> and <code>naclus</code>, is
<code>"compact"</code> (for <span class="rlang"><b>R</b></span> it is <code>"complete"</code>).
</p>
</td></tr>
<tr><td><code id="varclus_+3A_data">data</code></td>
<td>
<p>a data frame, data table, or list</p>
</td></tr>
<tr><td><code id="varclus_+3A_subset">subset</code></td>
<td>
<p>a standard subsetting expression</p>
</td></tr>
<tr><td><code id="varclus_+3A_na.action">na.action</code></td>
<td>

<p>These may be specified if <code>x</code> is a formula.  The default
<code>na.action</code> is <code>na.retain</code>, defined by <code>varclus</code>.  This
causes all observations to be kept in the model frame, with later
pairwise deletion of <code>NA</code>s.</p>
</td></tr>
<tr><td><code id="varclus_+3A_trans">trans</code></td>
<td>
<p>By default, when the similarity measure is based on
Pearson's or Spearman's correlation coefficients, the coefficients are
squared.  Specify <code>trans="abs"</code> to take absolute values or
<code>trans="none"</code> to use the coefficients as they stand.</p>
</td></tr>
<tr><td><code id="varclus_+3A_...">...</code></td>
<td>
<p>for <code>varclus</code> these are optional arguments to pass to
the <code><a href="#topic+dataframeReduce">dataframeReduce</a></code> function.  Otherwise,
passed to <code>plclust</code> (or to <code>dotchart</code> or <code>dotchart2</code> for
<code>naplot</code>).
</p>
</td></tr>
<tr><td><code id="varclus_+3A_ylab">ylab</code></td>
<td>

<p>y-axis label.  Default is constructed on the basis of <code>similarity</code>.
</p>
</td></tr>
<tr><td><code id="varclus_+3A_legend.">legend.</code></td>
<td>

<p>set to <code>TRUE</code> to plot a legend defining the abbreviations
</p>
</td></tr>
<tr><td><code id="varclus_+3A_loc">loc</code></td>
<td>

<p>a list with elements <code>x</code> and <code>y</code> defining coordinates of the
upper left corner of the legend.  Default is <code>locator(1)</code>.
</p>
</td></tr>
<tr><td><code id="varclus_+3A_maxlen">maxlen</code></td>
<td>

<p>if a legend is plotted describing abbreviations, original labels
longer than <code>maxlen</code> characters are truncated at <code>maxlen</code>.
</p>
</td></tr>
<tr><td><code id="varclus_+3A_labels">labels</code></td>
<td>

<p>a vector of character strings containing labels corresponding to
columns in the similar matrix, if the column names of that matrix are
not to be used
</p>
</td></tr>
<tr><td><code id="varclus_+3A_obj">obj</code></td>
<td>
<p>an object created by <code>naclus</code></p>
</td></tr>
<tr><td><code id="varclus_+3A_which">which</code></td>
<td>

<p>defaults to <code>"all"</code> meaning to have <code>naplot</code> make 4 separate
plots.  To 
make only one of the plots, use <code>which="na per var"</code> (dot chart of
fraction of NAs for each variable), ,<code>"na per obs"</code> (dot chart showing
frequency distribution of number of variables having NAs in an
observation), <code>"mean na"</code> (dot chart showing mean number of other
variables missing when the indicated variable is missing), or 
<code>"na per var vs mean na"</code>, a scatterplot showing on the x-axis the
fraction of NAs in the variable and on the y-axis the mean number of
other variables that are NA when the indicated variable is NA.
</p>
</td></tr>
<tr><td><code id="varclus_+3A_abbrev">abbrev</code></td>
<td>

<p>set to <code>TRUE</code> to abbreviate variable names for plotting or
printing.  Is set to <code>TRUE</code> automatically if <code>legend=TRUE</code>.
</p>
</td></tr>
<tr><td><code id="varclus_+3A_slim">slim</code></td>
<td>

<p>2-vector specifying the range of similarity values for scaling the
y-axes.  By default this is the observed range over all of <code>s</code>.
</p>
</td></tr>
<tr><td><code id="varclus_+3A_slimds">slimds</code></td>
<td>
<p>set to <code>slimds</code> to <code>TRUE</code> to scale diagonals and
off-diagonals separately</p>
</td></tr>
<tr><td><code id="varclus_+3A_add">add</code></td>
<td>

<p>set to <code>TRUE</code> to add similarities to an existing plot (usually
specifying <code>lty</code> or <code>col</code>)
</p>
</td></tr>
<tr><td><code id="varclus_+3A_lty">lty</code>, <code id="varclus_+3A_col">col</code>, <code id="varclus_+3A_lwd">lwd</code></td>
<td>

<p>line type, color, or line thickness for <code>plotMultSim</code>
</p>
</td></tr>
<tr><td><code id="varclus_+3A_vname">vname</code></td>
<td>

<p>optional vector of variable names, in order, used in <code>s</code>
</p>
</td></tr>
<tr><td><code id="varclus_+3A_h">h</code></td>
<td>

<p>relative height for subplot
</p>
</td></tr>
<tr><td><code id="varclus_+3A_w">w</code></td>
<td>

<p>relative width for subplot
</p>
</td></tr>
<tr><td><code id="varclus_+3A_u">u</code></td>
<td>

<p>relative extra height and width to leave unused inside the subplot.
Also used as the space between y-axis tick mark labels and graph border.
</p>
</td></tr>
<tr><td><code id="varclus_+3A_labelx">labelx</code></td>
<td>

<p>set to <code>FALSE</code> to suppress drawing of labels in the x direction
</p>
</td></tr>
<tr><td><code id="varclus_+3A_xspace">xspace</code></td>
<td>

<p>amount of space, on a scale of 1:<code>n</code> where <code>n</code> is the number
of variables, to set aside for y-axis labels
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>options(contrasts= c("contr.treatment", "contr.poly"))</code> is issued 
temporarily by <code>varclus</code> to make sure that ordinary dummy variables
are generated for <code>factor</code> variables.  Pass arguments to the
<code><a href="#topic+dataframeReduce">dataframeReduce</a></code> function to remove problematic variables
(especially if analyzing all variables in a data frame).
</p>


<h3>Value</h3>

<p>for <code>varclus</code> or <code>naclus</code>, a list of class <code>varclus</code> with elements
<code>call</code> (containing the calling statement), <code>sim</code> (similarity matrix),
<code>n</code> (sample size used if <code>x</code> was not a correlation matrix already -
<code>n</code> is a matrix), <code>hclust</code>, the object created by <code>hclust</code>,
<code>similarity</code>, and <code>method</code>.  <code>naclus</code> also returns the
two vectors listed under 
description, and <code>naplot</code> returns an invisible vector that is the
frequency table of the number of missing variables per observation.
<code>plotMultSim</code> invisibly returns the limits of similarities used in
constructing the y-axes of each subplot.  For <code>similarity="ccbothpos"</code>
the <code>hclust</code> object is <code>NULL</code>.
</p>
<p><code>na.pattern</code> creates an integer vector of frequencies.
</p>


<h3>Side Effects</h3>

<p>plots
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics, Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
</p>


<h3>References</h3>

<p>Sarle, WS: The VARCLUS Procedure.  SAS/STAT User's Guide, 4th Edition,
1990.  Cary NC: SAS Institute, Inc.
</p>
<p>Hoeffding W. (1948): A non-parametric test of independence.  Ann Math Stat
19:546&ndash;57.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+hclust">hclust</a></code>, <code><a href="stats.html#topic+plclust">plclust</a></code>, <code><a href="#topic+hoeffd">hoeffd</a></code>, <code><a href="#topic+rcorr">rcorr</a></code>, <code><a href="stats.html#topic+cor">cor</a></code>, <code><a href="stats.html#topic+model.matrix">model.matrix</a></code>,
<code><a href="graphics.html#topic+locator">locator</a></code>, <code><a href="#topic+na.pattern">na.pattern</a></code>, <code><a href="#topic+cut2">cut2</a></code>, <code><a href="#topic+combine.levels">combine.levels</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x1 &lt;- rnorm(200)
x2 &lt;- rnorm(200)
x3 &lt;- x1 + x2 + rnorm(200)
x4 &lt;- x2 + rnorm(200)
x &lt;- cbind(x1,x2,x3,x4)
v &lt;- varclus(x, similarity="spear")  # spearman is the default anyway
v    # invokes print.varclus
print(round(v$sim,2))
plot(v)


# plot(varclus(~ age + sys.bp + dias.bp + country - 1), abbrev=TRUE)
# the -1 causes k dummies to be generated for k countries
# plot(varclus(~ age + factor(disease.code) - 1))
#
#
# use varclus(~., data= fracmiss= maxlevels= minprev=) to analyze all
# "useful" variables - see dataframeReduce for details about arguments


df &lt;- data.frame(a=c(1,2,3),b=c(1,2,3),c=c(1,2,NA),d=c(1,NA,3),
                 e=c(1,NA,3),f=c(NA,NA,NA),g=c(NA,2,3),h=c(NA,NA,3))
par(mfrow=c(2,2))
for(m in c("ward","complete","median")) {
  plot(naclus(df, method=m))
  title(m)
}
naplot(naclus(df))
n &lt;- naclus(df)
plot(n); naplot(n)
na.pattern(df)

# plotMultSim example: Plot proportion of observations
# for which two variables are both positive (diagonals
# show the proportion of observations for which the
# one variable is positive).  Chance-correct the
# off-diagonals by subtracting the product of the
# marginal proportions.  On each subplot the x-axis
# shows month (0, 4, 8, 12) and there is a separate
# curve for females and males
d &lt;- data.frame(sex=sample(c('female','male'),1000,TRUE),
                month=sample(c(0,4,8,12),1000,TRUE),
                x1=sample(0:1,1000,TRUE),
                x2=sample(0:1,1000,TRUE),
                x3=sample(0:1,1000,TRUE))
s &lt;- array(NA, c(3,3,4))
opar &lt;- par(mar=c(0,0,4.1,0))  # waste less space
for(sx in c('female','male')) {
  for(i in 1:4) {
    mon &lt;- (i-1)*4
    s[,,i] &lt;- varclus(~x1 + x2 + x3, sim='ccbothpos', data=d,
                      subset=d$month==mon &amp; d$sex==sx)$sim
    }
  plotMultSim(s, c(0,4,8,12), vname=c('x1','x2','x3'),
              add=sx=='male', slimds=TRUE,
              lty=1+(sx=='male'))
  # slimds=TRUE causes separate  scaling for diagonals and
  # off-diagonals
}
par(opar)
</code></pre>

<hr>
<h2 id='vlab'>vlab</h2><span id='topic+vlab'></span>

<h3>Description</h3>

<p>Easily Retrieve Text Form of Labels/Units
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vlab(x, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vlab_+3A_x">x</code></td>
<td>
<p>a single variable name, unquoted</p>
</td></tr>
<tr><td><code id="vlab_+3A_name">name</code></td>
<td>
<p>optional character string to use as variable name</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses the same search method as <code>hlab</code> returns label and units in a character string with units, if present, in brackets
</p>


<h3>Value</h3>

<p>character string
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hlab">hlab()</a></code>
</p>

<hr>
<h2 id='wtd.stats'>
Weighted Statistical Estimates
</h2><span id='topic+wtd.mean'></span><span id='topic+wtd.var'></span><span id='topic+wtd.quantile'></span><span id='topic+wtd.Ecdf'></span><span id='topic+wtd.table'></span><span id='topic+wtd.rank'></span><span id='topic+wtd.loess.noiter'></span><span id='topic+num.denom.setup'></span>

<h3>Description</h3>

<p>These functions compute various weighted versions of standard
estimators.  In most cases the <code>weights</code> vector is a vector the same
length of <code>x</code>, containing frequency counts that in effect expand <code>x</code>
by these counts.  <code>weights</code> can also be sampling weights, in which
setting <code>normwt</code> to <code>TRUE</code> will often be appropriate.  This results in
making <code>weights</code> sum to the length of the non-missing elements in
<code>x</code>.  <code>normwt=TRUE</code> thus reflects the fact that the true sample size is
the length of the <code>x</code> vector and not the sum of the original values of
<code>weights</code> (which would be appropriate had <code>normwt=FALSE</code>).  When <code>weights</code>
is all ones, the estimates are all identical to unweighted estimates
(unless one of the non-default quantile estimation options is
specified to <code>wtd.quantile</code>).  When missing data have already been
deleted for, <code>x</code>, <code>weights</code>, and (in the case of <code>wtd.loess.noiter</code>) <code>y</code>,
specifying <code>na.rm=FALSE</code> will save computation time.  Omitting the
<code>weights</code> argument or specifying <code>NULL</code> or a zero-length vector will
result in the usual unweighted estimates.
</p>
<p><code>wtd.mean</code>, <code>wtd.var</code>, and <code>wtd.quantile</code> compute
weighted means, variances, and quantiles, respectively.  <code>wtd.Ecdf</code>
computes a weighted empirical distribution function.  <code>wtd.table</code>
computes a weighted frequency table (although only one stratification
variable is supported at present).  <code>wtd.rank</code> computes weighted
ranks, using mid&ndash;ranks for ties.  This can be used to obtain Wilcoxon
tests and rank correlation coefficients.  <code>wtd.loess.noiter</code> is a
weighted version of <code>loess.smooth</code> when no iterations for outlier
rejection are desired. This results in especially good smoothing when
<code>y</code> is binary.  <code>wtd.quantile</code> removes any observations with
zero weight at the beginning.  Previously, these were changing the
quantile estimates.
</p>
<p><code>num.denom.setup</code> is a utility function that allows one to deal with
observations containing numbers of events and numbers of trials, by
outputting two observations when the number of events and non-events
(trials - events) exceed zero.  A vector of subscripts is generated
that will do the proper duplications of observations, and a new binary
variable <code>y</code> is created along with usual cell frequencies (<code>weights</code>)
for each of the <code>y=0</code>, <code>y=1</code> cells per observation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wtd.mean(x, weights=NULL, normwt="ignored", na.rm=TRUE)
wtd.var(x, weights=NULL, normwt=FALSE, na.rm=TRUE,
        method=c('unbiased', 'ML'))
wtd.quantile(x, weights=NULL, probs=c(0, .25, .5, .75, 1), 
             type=c('quantile','(i-1)/(n-1)','i/(n+1)','i/n'), 
             normwt=FALSE, na.rm=TRUE)
wtd.Ecdf(x, weights=NULL, 
         type=c('i/n','(i-1)/(n-1)','i/(n+1)'), 
         normwt=FALSE, na.rm=TRUE)
wtd.table(x, weights=NULL, type=c('list','table'), 
          normwt=FALSE, na.rm=TRUE)
wtd.rank(x, weights=NULL, normwt=FALSE, na.rm=TRUE)
wtd.loess.noiter(x, y, weights=rep(1,n),
                 span=2/3, degree=1, cell=.13333, 
                 type=c('all','ordered all','evaluate'), 
                 evaluation=100, na.rm=TRUE)
num.denom.setup(num, denom)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wtd.stats_+3A_x">x</code></td>
<td>

<p>a numeric vector (may be a character or <code>category</code> or <code>factor</code> vector
for <code>wtd.table</code>)
</p>
</td></tr>
<tr><td><code id="wtd.stats_+3A_num">num</code></td>
<td>

<p>vector of numerator frequencies
</p>
</td></tr>
<tr><td><code id="wtd.stats_+3A_denom">denom</code></td>
<td>

<p>vector of denominators (numbers of trials)
</p>
</td></tr>
<tr><td><code id="wtd.stats_+3A_weights">weights</code></td>
<td>

<p>a numeric vector of weights
</p>
</td></tr>
<tr><td><code id="wtd.stats_+3A_normwt">normwt</code></td>
<td>

<p>specify <code>normwt=TRUE</code> to make <code>weights</code> sum to
<code>length(x)</code> after deletion of <code>NA</code>s.  If <code>weights</code> are
frequency weights, then <code>normwt</code> should be <code>FALSE</code>, and if
<code>weights</code> are normalization (aka reliability) weights, then
<code>normwt</code> should be <code>TRUE</code>. In the case of the former, no check
is made that <code>weights</code> are valid frequencies. 
</p>
</td></tr>
<tr><td><code id="wtd.stats_+3A_na.rm">na.rm</code></td>
<td>

<p>set to <code>FALSE</code> to suppress checking for NAs
</p>
</td></tr>
<tr><td><code id="wtd.stats_+3A_method">method</code></td>
<td>
<p>determines the estimator type; if <code>'unbiased'</code> (the
default) then the usual unbiased estimate (using Bessel's correction)
is returned, if <code>'ML'</code> then it is the maximum likelihood estimate
for a Gaussian distribution. In the case of the latter, the
<code>normwt</code> argument has no effect.  Uses <code>stats:cov.wt</code> for
both methods.</p>
</td></tr>
<tr><td><code id="wtd.stats_+3A_probs">probs</code></td>
<td>

<p>a vector of quantiles to compute.  Default is 0 (min), .25, .5, .75, 1
(max).
</p>
</td></tr>
<tr><td><code id="wtd.stats_+3A_type">type</code></td>
<td>

<p>For <code>wtd.quantile</code>, <code>type</code> defaults to <code>quantile</code> to use the same
interpolated order statistic method as <code>quantile</code>.  Set <code>type</code> to 
<code>"(i-1)/(n-1)"</code>,<code>"i/(n+1)"</code>, or <code>"i/n"</code> to use the inverse of the
empirical distribution function, using, respectively, (wt - 1)/T,
wt/(T+1), or wt/T, where wt is the cumulative weight and T is the
total weight (usually total sample size).  These three values of
<code>type</code> are the possibilities for <code>wtd.Ecdf</code>.  For <code>wtd.table</code> the
default <code>type</code> is <code>"list"</code>, meaning that the function is to return a
list containing two vectors: <code>x</code> is the sorted unique values of <code>x</code>
and <code>sum.of.weights</code> is the sum of weights for that <code>x</code>.  This is the
default so that you don't have to convert the <code>names</code> attribute of the
result that can be obtained with <code>type="table"</code> to a numeric variable
when <code>x</code> was originally numeric.  <code>type="table"</code> for <code>wtd.table</code>
results in an object that is the same structure as those returned from
<code>table</code>.  For <code>wtd.loess.noiter</code> the default <code>type</code> is <code>"all"</code>,
indicating that the function is to return a list containing all the
original values of <code>x</code> (including duplicates and without sorting) and
the smoothed <code>y</code> values corresponding to them.  Set <code>type="ordered
all"</code> to sort by <code>x</code>, and <code>type="evaluate"</code> to evaluate the smooth
only at <code>evaluation</code> equally spaced points between the observed limits
of <code>x</code>.
</p>
</td></tr>
<tr><td><code id="wtd.stats_+3A_y">y</code></td>
<td>
<p>a numeric vector the same length as <code>x</code></p>
</td></tr>
<tr><td><code id="wtd.stats_+3A_span">span</code>, <code id="wtd.stats_+3A_degree">degree</code>, <code id="wtd.stats_+3A_cell">cell</code>, <code id="wtd.stats_+3A_evaluation">evaluation</code></td>
<td>

<p>see <code>loess.smooth</code>.  The default is linear (<code>degree</code>=1) and 100 points
to evaluation (if <code>type="evaluate"</code>).
</p>
</td></tr></table>


<h3>Details</h3>

<p>The functions correctly combine weights of observations having
duplicate values of <code>x</code> before computing estimates.
</p>
<p>When <code>normwt=FALSE</code> the weighted variance will not equal the
unweighted variance even if the weights are identical.  That is because
of the subtraction of 1 from the sum of the weights in the denominator
of the variance formula.  If you want the weighted variance to equal the
unweighted variance when weights do not vary, use <code>normwt=TRUE</code>.
The articles by Gatz and Smith discuss alternative approaches, to arrive
at estimators of the standard error of a weighted mean.
</p>
<p><code>wtd.rank</code> does not handle NAs as elegantly as <code>rank</code> if
<code>weights</code> is specified.
</p>


<h3>Value</h3>

<p><code>wtd.mean</code> and <code>wtd.var</code> return scalars.  <code>wtd.quantile</code> returns a
vector the same length as <code>probs</code>.  <code>wtd.Ecdf</code> returns a list whose
elements <code>x</code> and <code>Ecdf</code> correspond to unique sorted values of <code>x</code>.
If the first CDF estimate is greater than zero, a point (min(x),0) is
placed at the beginning of the estimates.
See above for <code>wtd.table</code>.  <code>wtd.rank</code> returns a vector the same
length as <code>x</code> (after removal of NAs, depending on <code>na.rm</code>).  See above
for <code>wtd.loess.noiter</code>.
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University School of Medicine
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
<br />
Benjamin Tyner
<br />
<a href="mailto:btyner@gmail.com">btyner@gmail.com</a>
</p>


<h3>References</h3>

<p>Research Triangle Institute (1995): SUDAAN User's Manual, Release
6.40, pp. 8-16 to 8-17.
</p>
<p>Gatz DF, Smith L (1995): The standard error of a weighted mean
concentration&ndash;I.  Bootstrapping vs other methods.  Atmospheric Env
11:1185-1193.
</p>
<p>Gatz DF, Smith L (1995): The standard error of a weighted mean
concentration&ndash;II.  Estimating confidence intervals.  Atmospheric Env
29:1195-1200.
</p>
<p>https://en.wikipedia.org/wiki/Weighted_arithmetic_mean
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+mean">mean</a></code>, <code><a href="stats.html#topic+var">var</a></code>, <code><a href="stats.html#topic+quantile">quantile</a></code>, <code><a href="base.html#topic+table">table</a></code>, <code><a href="base.html#topic+rank">rank</a></code>, <code><a href="stats.html#topic+loess.smooth">loess.smooth</a></code>, <code><a href="stats.html#topic+lowess">lowess</a></code>,
<code><a href="#topic+plsmo">plsmo</a></code>, <code><a href="#topic+Ecdf">Ecdf</a></code>, <code><a href="#topic+somers2">somers2</a></code>, <code><a href="#topic+describe">describe</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x &lt;- runif(500)
wts &lt;- sample(1:6, 500, TRUE)
std.dev &lt;- sqrt(wtd.var(x, wts))
wtd.quantile(x, wts)
death &lt;- sample(0:1, 500, TRUE)
plot(wtd.loess.noiter(x, death, wts, type='evaluate'))
describe(~x, weights=wts)
# describe uses wtd.mean, wtd.quantile, wtd.table
xg &lt;- cut2(x,g=4)
table(xg)
wtd.table(xg, wts, type='table')

# Here is a method for getting stratified weighted means
y &lt;- runif(500)
g &lt;- function(y) wtd.mean(y[,1],y[,2])
summarize(cbind(y, wts), llist(xg), g, stat.name='y')

# Empirically determine how methods used by wtd.quantile match with
# methods used by quantile, when all weights are unity
set.seed(1)
u &lt;-  eval(formals(wtd.quantile)$type)
v &lt;- as.character(1:9)
r &lt;- matrix(0, nrow=length(u), ncol=9, dimnames=list(u,v))

for(n in c(8, 13, 22, 29))
  {
    x &lt;- rnorm(n)
    for(i in 1:5) {
      probs &lt;- sort( runif(9))
      for(wtype in u) {
        wq &lt;- wtd.quantile(x, type=wtype, weights=rep(1,length(x)), probs=probs)
        for(qtype in 1:9) {
          rq &lt;- quantile(x, type=qtype, probs=probs)
          r[wtype, qtype] &lt;- max(r[wtype,qtype], max(abs(wq-rq)))
        }
      }
    }
  }

r

# Restructure data to generate a dichotomous response variable
# from records containing numbers of events and numbers of trials
num   &lt;- c(10,NA,20,0,15)   # data are 10/12 NA/999 20/20 0/25 15/35
denom &lt;- c(12,999,20,25,35)
w     &lt;- num.denom.setup(num, denom)
w
# attach(my.data.frame[w$subs,])
</code></pre>

<hr>
<h2 id='xtfrm.labelled'>
Auxiliary Function Method for Sorting and Ranking
</h2><span id='topic+xtfrm.labelled'></span>

<h3>Description</h3>

<p>An auxiliary function method that is a workaround for bug in the
implementation of xtfrm handles inheritance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'labelled'
xtfrm(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xtfrm.labelled_+3A_x">x</code></td>
<td>

<p>any object of class labelled.
</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="base.html#topic+xtfrm">xtfrm</a></code>
</p>

<hr>
<h2 id='xy.group'>
Mean x vs. function of y in groups of x
</h2><span id='topic+xy.group'></span>

<h3>Description</h3>

<p>Compute mean x vs. a function of y (e.g. median) by quantile
groups of x or by x grouped to have a given average number of
observations.  Deletes NAs in x and y before doing computations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xy.group(x, y, m=150, g, fun=mean, result="list")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xy.group_+3A_x">x</code></td>
<td>

<p>a vector, may contain NAs
</p>
</td></tr>
<tr><td><code id="xy.group_+3A_y">y</code></td>
<td>

<p>a vector of same length as x, may contain NAs
</p>
</td></tr>
<tr><td><code id="xy.group_+3A_m">m</code></td>
<td>

<p>number of observations per group
</p>
</td></tr>
<tr><td><code id="xy.group_+3A_g">g</code></td>
<td>

<p>number of quantile groups
</p>
</td></tr>
<tr><td><code id="xy.group_+3A_fun">fun</code></td>
<td>

<p>function of y such as median or mean (the default)
</p>
</td></tr>
<tr><td><code id="xy.group_+3A_result">result</code></td>
<td>

<p>&quot;list&quot; (the default), or &quot;matrix&quot;
</p>
</td></tr></table>


<h3>Value</h3>

<p>if result=&quot;list&quot;, a list with components x and y suitable for plotting.
if result=&quot;matrix&quot;, matrix with rows corresponding to x-groups and columns named
n, x, and y.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cut2">cut2</a></code>, <code><a href="base.html#topic+tapply">tapply</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># plot(xy.group(x, y, g=10))	#Plot mean y by deciles of x
# xy.group(x, y, m=100, result="matrix")	#Print table, 100 obs/group
</code></pre>

<hr>
<h2 id='xYplot'>xyplot and dotplot with Matrix Variables to Plot Error Bars and Bands</h2><span id='topic+xYplot'></span><span id='topic+panel.xYplot'></span><span id='topic+prepanel.xYplot'></span><span id='topic+Dotplot'></span><span id='topic+panel.Dotplot'></span><span id='topic+prepanel.Dotplot'></span><span id='topic+Cbind'></span><span id='topic++5B.Cbind'></span><span id='topic+setTrellis'></span><span id='topic+numericScale'></span>

<h3>Description</h3>

<p>A utility function <code>Cbind</code> returns the first argument as a vector and
combines all other arguments into a matrix stored as an attribute called
<code>"other"</code>.  The arguments can be named (e.g.,
<code>Cbind(pressure=y,ylow,yhigh)</code>) or a <code>label</code> attribute may be pre-attached
to the first argument. In either case, the name or label of the first
argument is stored as an attribute <code>"label"</code> of the object returned by
<code>Cbind</code>.  Storing other vectors as a matrix attribute facilitates plotting
error bars, etc., as <code>trellis</code> really wants the x- and y-variables to be
vectors, not matrices. If a single argument is given to <code>Cbind</code> and that
argument is a matrix with column dimnames, the first column is taken as the
main vector and remaining columns are taken as <code>"other"</code>. A subscript
method for <code>Cbind</code> objects subscripts the <code>other</code> matrix along
with the main <code>y</code> vector.
</p>
<p>The <code>xYplot</code> function is a substitute for <code>xyplot</code> that allows for
simulated multi-column <code>y</code>. It uses by default the <code>panel.xYplot</code> and
<code>prepanel.xYplot</code> functions to do the actual work. The <code>method</code> argument
passed to <code>panel.xYplot</code> from <code>xYplot</code> allows you to make error bars, the
upper-only or lower-only portions of error bars, alternating lower-only and
upper-only bars, bands, or filled bands.  <code>panel.xYplot</code> decides how to
alternate upper and lower bars according to whether the median <code>y</code> value of
the current main data line is above the median <code>y</code> for all <code>groups</code> of
lines or not.  If the median is above the overall median, only the upper
bar is drawn. For <code>bands</code> (but not 'filled bands'), any number of other
columns of <code>y</code> will be drawn as lines having the same thickness, color, and
type as the main data line.  If plotting bars, bands, or filled bands and
only one additional column is specified for the response variable, that
column is taken as the half width of a precision interval for <code>y</code>, and the
lower and upper values are computed automatically as <code>y</code> plus or minus the
value of the additional column variable.
</p>
<p>When a <code>groups</code> variable is present, <code>panel.xYplot</code> will create a function
in frame 0 (<code>.GlobalEnv</code> in <span class="rlang"><b>R</b></span>) called <code>Key</code> that when
invoked will draw a key describing the 
<code>groups</code> labels, point symbols, and colors. By default, the key is outside
the graph.  For S-Plus, if <code>Key(locator(1))</code> is specified, the key will appear so that
its upper left corner is at the coordinates of the mouse click.  For
R/Lattice the first two arguments of <code>Key</code> (<code>x</code> and <code>y</code>) are fractions
of the page, measured from the lower left corner, and the default
placement is at <code>x=0.05, y=0.95</code>.  For <span class="rlang"><b>R</b></span>, an optional argument
to <code>sKey</code>, <code>other</code>, may contain a list of arguments to pass to <code>draw.key</code> (see
<code><a href="lattice.html#topic+xyplot">xyplot</a></code> for a list of possible arguments, under
the <code>key</code> option).  
</p>
<p>When <code>method="quantile"</code> is specified, <code>xYplot</code> automatically groups the
<code>x</code> variable into intervals containing a target of <code>nx</code> observations each,
and within each <code>x</code> group computes three quantiles of <code>y</code> and plots these
as three lines. The mean <code>x</code> within each <code>x</code> group is taken as the
<code>x</code>-coordinate. This will make a useful empirical display for large
datasets in which scatterdiagrams are too busy to see patterns of central
tendency and variability.  You can also specify a general function of a
data vector that returns a matrix of statistics for the <code>method</code> argument.
Arguments can be passed to that function via a list <code>methodArgs</code>.  The
statistic in the first column should be the measure of central tendency.
Examples of useful <code>method</code> functions are those listed under the help file
for <code>summary.formula</code> such as <code>smean.cl.normal</code>.
</p>
<p><code>xYplot</code> can also produce bubble plots.  This is done when
<code>size</code> is specified to <code>xYplot</code>.  When <code>size</code> is used, a
function <code>sKey</code> is generated for drawing a key to the character
sizes.  See the bubble plot example.  <code>size</code> can also specify a
vector where the first character of each observation is used as the
plotting symbol, if <code>rangeCex</code> is set to a single <code>cex</code>
value.  An optional argument to <code>sKey</code>, <code>other</code>, may contain
a list of arguments to pass to <code>draw.key</code> (see
<code><a href="lattice.html#topic+xyplot">xyplot</a></code> for a list of possible arguments, under
the <code>key</code> option).  See the bubble plot example.
</p>
<p><code>Dotplot</code> is a substitute for <code>dotplot</code> allowing for a matrix x-variable,
automatic superpositioning when <code>groups</code> is present, and creation of a
<code>Key</code> function.  When the x-variable (created by <code>Cbind</code> to simulate a
matrix) contains a total of 3 columns, the first column specifies where the
dot is positioned, and the last 2 columns specify starting and ending
points for intervals.  The intervals are shown using line type, width, and
color from the trellis <code>plot.line</code> list. By default, you will usually see a
darker line segment for the low and high values, with the dotted reference
line elsewhere. A good choice of the <code>pch</code> argument for such plots is <code>3</code>
(plus sign) if you want to emphasize the interval more than the point
estimate.  When the x-variable contains a total of 5 columns, the 2nd and
5th columns are treated as the 2nd and 3rd are treated above, and the 3rd
and 4th columns define an inner line segment that will have twice the
thickness of the outer segments. In addition, tick marks separate the outer
and inner segments.  This type of display (an example of which appeared in
<em>The Elements of Graphing Data</em> by Cleveland) is very suitable for
displaying two confidence levels (e.g., 0.9 and 0.99) or the 0.05, 0.25,
0.75, 0.95 sample quantiles, for example.  For this display, the central
point displays well with a default circle symbol.
</p>
<p><code>setTrellis</code> sets nice defaults for Trellis graphics, assuming that the
graphics device has already been opened if using postscript, etc. By
default, it sets panel strips to blank and reference dot lines to thickness
1 instead of the Trellis default of 2.
</p>
<p><code>numericScale</code> is a utility function that facilitates using
<code>xYplot</code> to 
plot variables that are not considered to be numeric but which can readily
be converted to numeric using <code>as.numeric()</code>.  <code>numericScale</code>
by default will keep the name of the input variable as a <code>label</code>
attribute for the new numeric variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Cbind(...)

xYplot(formula, data = sys.frame(sys.parent()), groups,
       subset, xlab=NULL, ylab=NULL, ylim=NULL,
       panel=panel.xYplot, prepanel=prepanel.xYplot, scales=NULL,
       minor.ticks=NULL, sub=NULL, ...)

panel.xYplot(x, y, subscripts, groups=NULL, 
             type=if(is.function(method) || method=='quantiles') 
               'b' else 'p',
             method=c("bars", "bands", "upper bars", "lower bars", 
                      "alt bars", "quantiles", "filled bands"), 
             methodArgs=NULL, label.curves=TRUE, abline,
             probs=c(.5,.25,.75), nx=NULL,
             cap=0.015, lty.bar=1, 
             lwd=plot.line$lwd, lty=plot.line$lty, pch=plot.symbol$pch, 
             cex=plot.symbol$cex, font=plot.symbol$font, col=NULL, 
             lwd.bands=NULL, lty.bands=NULL, col.bands=NULL, 
             minor.ticks=NULL, col.fill=NULL,
             size=NULL, rangeCex=c(.5,3), ...)

prepanel.xYplot(x, y, ...)

Dotplot(formula, data = sys.frame(sys.parent()), groups, subset, 
        xlab = NULL, ylab = NULL, ylim = NULL,
        panel=panel.Dotplot, prepanel=prepanel.Dotplot,
        scales=NULL, xscale=NULL, ...)

prepanel.Dotplot(x, y, ...)

panel.Dotplot(x, y, groups = NULL,
              pch  = dot.symbol$pch, 
              col  = dot.symbol$col, cex = dot.symbol$cex, 
              font = dot.symbol$font, abline, ...)

setTrellis(strip.blank=TRUE, lty.dot.line=2, lwd.dot.line=1)

numericScale(x, label=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xYplot_+3A_...">...</code></td>
<td>

<p>for <code>Cbind</code> <code>...</code> is any number of additional numeric
vectors. Unless you are using <code>Dotplot</code> (which allows for either 2
or 4 &quot;other&quot; variables) or <code>xYplot</code> with <code>method="bands"</code>,
vectors after the first two are ignored.  If drawing bars and only one
extra variable is given in <code>...</code>, upper and lower values are
computed as described above. If the second argument to <code>Cbind</code> is a
matrix, that matrix is stored in the <code>"other"</code> attribute and
arguments after the second are ignored.  For bubble plots, name an
argument <code>cex</code>.
</p>
<p>Also can be other arguments to pass to <code>labcurve</code>.
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_formula">formula</code></td>
<td>

<p>a <code>trellis</code> formula consistent with <code>xyplot</code> or <code>dotplot</code> 
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_x">x</code></td>
<td>

<p><code>x</code>-axis variable.  For <code>numericScale</code> <code>x</code> is any vector
such as <code>as.numeric(x)</code> returns a numeric vector suitable for x- or
y-coordinates.
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_y">y</code></td>
<td>

<p>a vector, or an object created by <code>Cbind</code> for <code>xYplot</code>.
<code>y</code> represents the main variable to plot, i.e., the variable used to
draw the main lines. For <code>Dotplot</code> the first argument to
<code>Cbind</code> will be the main <code>x</code>-axis variable.  
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_data">data</code>, <code id="xYplot_+3A_subset">subset</code>, <code id="xYplot_+3A_ylim">ylim</code>, <code id="xYplot_+3A_subscripts">subscripts</code>, <code id="xYplot_+3A_groups">groups</code>, <code id="xYplot_+3A_type">type</code>, <code id="xYplot_+3A_scales">scales</code>, <code id="xYplot_+3A_panel">panel</code>, <code id="xYplot_+3A_prepanel">prepanel</code>, <code id="xYplot_+3A_xlab">xlab</code>, <code id="xYplot_+3A_ylab">ylab</code></td>
<td>

<p>see <code>trellis.args</code>.  <code>xlab</code> and <code>ylab</code> get default values from
<code>"label"</code> attributes.
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_xscale">xscale</code></td>
<td>
<p>allows one to use the default <code>scales</code> but specify
only the <code>x</code> component of it for <code>Dotplot</code></p>
</td></tr>
<tr><td><code id="xYplot_+3A_method">method</code></td>
<td>

<p>defaults to <code>"bars"</code> to draw error-bar type plots.  See meaning of other
values above.  <code>method</code> can be a function.  Specifying <code>method=quantile</code>,
<code>methodArgs=list(probs=c(.5,.25,.75))</code> is the same as specifying
<code>method="quantile"</code> without specifying <code>probs</code>.
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_methodargs">methodArgs</code></td>
<td>

<p>a list containing optional arguments to be passed to the function specified
in <code>method</code>
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_label.curves">label.curves</code></td>
<td>

<p>set to <code>FALSE</code> to suppress invocation of <code>labcurve</code> to label primary curves
where they are most separated or to draw a legend in an empty spot on the
panel.  You can also set <code>label.curves</code> to a list of options to pass to
<code>labcurve</code>.  These options can also be passed as <code>...</code> to <code>xYplot</code>. See the
examples below.
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_abline">abline</code></td>
<td>

<p>a list of arguments to pass to <code>panel.abline</code> for each panel, e.g.
<code>list(a=0, b=1, col=3)</code> to draw the line of identity using color
3.  To make multiple calls to <code>panel.abline</code>, pass a list of
unnamed lists as <code>abline</code>, e.g., <code>abline=list(list(h=0),list(v=1))</code>.
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_probs">probs</code></td>
<td>

<p>a vector of three quantiles with the quantile corresponding to the central
line listed first. By default <code>probs=c(.5, .25, .75)</code>. You can also specify
<code>probs</code> through <code>methodArgs=list(probs=...)</code>.
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_nx">nx</code></td>
<td>

<p>number of target observations for each <code>x</code> group (see <code>cut2</code> <code>m</code> argument).
<code>nx</code> defaults to the minimum of 40 and the number of points in the current
stratum divided by 4. Set <code>nx=FALSE</code> or <code>nx=0</code> if <code>x</code> is already discrete and
requires no grouping.
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_cap">cap</code></td>
<td>

<p>the half-width of horizontal end pieces for error bars, as a fraction of
the length of the <code>x</code>-axis
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_lty.bar">lty.bar</code></td>
<td>

<p>line type for bars
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_lwd">lwd</code>, <code id="xYplot_+3A_lty">lty</code>, <code id="xYplot_+3A_pch">pch</code>, <code id="xYplot_+3A_cex">cex</code>, <code id="xYplot_+3A_font">font</code>, <code id="xYplot_+3A_col">col</code></td>
<td>

<p>see <code>trellis.args</code>.  These are vectors when <code>groups</code> is present, and the
order of their elements corresponds to the different <code>groups</code>, regardless
of how many bands or bars are drawn. If you don't specify <code>lty.bands</code>, for
example, all band lines within each group will have the same <code>lty</code>.
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_lty.bands">lty.bands</code>, <code id="xYplot_+3A_lwd.bands">lwd.bands</code>, <code id="xYplot_+3A_col.bands">col.bands</code></td>
<td>

<p>used to allow <code>lty</code>, <code>lwd</code>, <code>col</code> to vary across the different band lines
for different <code>groups</code>. These parameters are vectors or lists whose
elements correspond to the added band lines (i.e., they ignore the central
line, whose line characteristics are defined by <code>lty</code>, <code>lwd</code>, <code>col</code>). For
example, suppose that 4 lines are drawn in addition to the central line.
Specifying <code>lwd.bands=1:4</code> will cause line widths of 1:4 to be used for
every group, regardless of the value of <code>lwd</code>.  To vary characteristics
over the <code>groups</code> use e.g. <code>lwd.bands=list(rep(1,4), rep(2,4))</code> or
<code>list(c(1,2,1,2), c(3,4,3,4))</code>.
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_minor.ticks">minor.ticks</code></td>
<td>

<p>a list with elements <code>at</code> and <code>labels</code> specifying positions
and labels for minor tick marks to be used on the x-axis of each panel,
if any.
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_sub">sub</code></td>
<td>
<p>an optional subtitle</p>
</td></tr>
<tr><td><code id="xYplot_+3A_col.fill">col.fill</code></td>
<td>

<p>used to override default colors used for the bands in method='filled
bands'. This is a vector when <code>groups</code> is present, and the order of the
elements corresponds to the different <code>groups</code>, regardless of how many
bands are drawn.  The default colors for 'filled bands' are pastel colors
matching the default colors superpose.line$col (plot.line$col)
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_size">size</code></td>
<td>

<p>a vector the same length as <code>x</code> giving a variable whose values
are a linear function of the size of the symbol drawn.  This is used
for example for bubble plots.
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_rangecex">rangeCex</code></td>
<td>

<p>a vector of two values specifying the range in character sizes to use
for the <code>size</code> variable (lowest first, highest second).
<code>size</code> values are linearly translated to this range, based on the
observed range of <code>size</code> when <code>x</code> and <code>y</code> coordinates
are not missing.  Specify a single numeric <code>cex</code> value for
<code>rangeCex</code> to use the first character of each observations's
<code>size</code> as the plotting symbol.
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_strip.blank">strip.blank</code></td>
<td>

<p>set to <code>FALSE</code> to not make the panel strip backgrounds blank 
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_lty.dot.line">lty.dot.line</code></td>
<td>

<p>line type for dot plot reference lines (default = 1 for dotted; use 2 for
dotted)
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_lwd.dot.line">lwd.dot.line</code></td>
<td>

<p>line thickness for reference lines for dot plots (default = 1) 
</p>
</td></tr>
<tr><td><code id="xYplot_+3A_label">label</code></td>
<td>

<p>a scalar character string to be used as a variable label after
<code>numericScale</code> converts the 
variable to numeric form 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Unlike <code>xyplot</code>, <code>xYplot</code> senses the presence of a <code>groups</code> variable and
automatically invokes <code>panel.superpose</code> instead of <code>panel.xyplot</code>. The same
is true for <code>Dotplot</code> vs. <code>dotplot</code>.
</p>


<h3>Value</h3>

<p><code>Cbind</code> returns a matrix with attributes.  Other functions return standard
<code>trellis</code> results.
</p>


<h3>Side Effects</h3>

<p>plots, and <code>panel.xYplot</code> may create temporary <code>Key</code> and
<code>sKey</code> functions in the session frame. 
</p>


<h3>Author(s)</h3>

<p>Frank Harrell
<br />
Department of Biostatistics
<br />
Vanderbilt University
<br />
<a href="mailto:fh@fharrell.com">fh@fharrell.com</a>
<br />
Madeline Bauer
<br />
Department of Infectious Diseases
<br />
University of Southern California School of Medicine
<br />
<a href="mailto:mbauer@usc.edu">mbauer@usc.edu</a>
</p>


<h3>See Also</h3>

<p><code><a href="lattice.html#topic+xyplot">xyplot</a></code>, <code><a href="lattice.html#topic+panel.xyplot">panel.xyplot</a></code>, <code><a href="#topic+summarize">summarize</a></code>, <code><a href="#topic+label">label</a></code>, <code><a href="#topic+labcurve">labcurve</a></code>,
<code><a href="#topic+errbar">errbar</a></code>, <code><a href="lattice.html#topic+xyplot">dotplot</a></code>, 
<code><a href="#topic+reShape">reShape</a></code>, <code><a href="#topic+cut2">cut2</a></code>, <code><a href="lattice.html#topic+panel.functions">panel.abline</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Plot 6 smooth functions.  Superpose 3, panel 2.
# Label curves with p=1,2,3 where most separated 
d &lt;- expand.grid(x=seq(0,2*pi,length=150), p=1:3, shift=c(0,pi)) 
xYplot(sin(x+shift)^p ~ x | shift, groups=p, data=d, type='l') 
# Use a key instead, use 3 line widths instead of 3 colors 
# Put key in most empty portion of each panel
xYplot(sin(x+shift)^p ~ x | shift, groups=p, data=d, 
       type='l', keys='lines', lwd=1:3, col=1) 
# Instead of implicitly using labcurve(), put a 
# single key outside of panels at lower left corner
xYplot(sin(x+shift)^p ~ x | shift, groups=p, data=d, 
       type='l', label.curves=FALSE, lwd=1:3, col=1, lty=1:3) 
Key()

# Bubble plots
x &lt;- y &lt;- 1:8
x[2] &lt;- NA
units(x) &lt;- 'cm^2'
z &lt;- 101:108
p &lt;- factor(rep(c('a','b'),4))
g &lt;- c(rep(1,7),2)
data.frame(p, x, y, z, g)
xYplot(y ~ x | p, groups=g, size=z)
 Key(other=list(title='g', cex.title=1.2))  # draw key for colors
sKey(.2,.85,other=list(title='Z Values', cex.title=1.2))
# draw key for character sizes

# Show the median and quartiles of height given age, stratified 
# by sex and race.  Draws 2 sets (male, female) of 3 lines per panel.
# xYplot(height ~ age | race, groups=sex, method='quantiles')


# Examples of plotting raw data
dfr &lt;- expand.grid(month=1:12, continent=c('Europe','USA'), 
                   sex=c('female','male'))
set.seed(1)
dfr &lt;- upData(dfr,
              y=month/10 + 1*(sex=='female') + 2*(continent=='Europe') + 
                runif(48,-.15,.15),
              lower=y - runif(48,.05,.15),
              upper=y + runif(48,.05,.15))


xYplot(Cbind(y,lower,upper) ~ month,subset=sex=='male' &amp; continent=='USA',
       data=dfr)
xYplot(Cbind(y,lower,upper) ~ month|continent, subset=sex=='male',data=dfr)
xYplot(Cbind(y,lower,upper) ~ month|continent, groups=sex, data=dfr); Key() 
# add ,label.curves=FALSE to suppress use of labcurve to label curves where
# farthest apart


xYplot(Cbind(y,lower,upper) ~ month,groups=sex,
                              subset=continent=='Europe', data=dfr) 
xYplot(Cbind(y,lower,upper) ~ month,groups=sex, type='b',
                              subset=continent=='Europe', keys='lines',
                              data=dfr)
# keys='lines' causes labcurve to draw a legend where the panel is most empty


xYplot(Cbind(y,lower,upper) ~ month,groups=sex, type='b', data=dfr,
                              subset=continent=='Europe',method='bands') 
xYplot(Cbind(y,lower,upper) ~ month,groups=sex, type='b', data=dfr,
                              subset=continent=='Europe',method='upper')


label(dfr$y) &lt;- 'Quality of Life Score'   
# label is in Hmisc library = attr(y,'label') &lt;- 'Quality\dots'; will be
# y-axis label 
# can also specify Cbind('Quality of Life Score'=y,lower,upper) 
xYplot(Cbind(y,lower,upper) ~ month, groups=sex,
       subset=continent=='Europe', method='alt bars',
        offset=grid::unit(.1,'inches'), type='b', data=dfr)   
# offset passed to labcurve to label .4 y units away from curve
# for R (using grid/lattice), offset is specified using the grid
# unit function, e.g., offset=grid::unit(.4,'native') or
# offset=grid::unit(.1,'inches') or grid::unit(.05,'npc')


# The following example uses the summarize function in Hmisc to 
# compute the median and outer quartiles.  The outer quartiles are 
# displayed using "error bars"
set.seed(111)
dfr &lt;- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
month &lt;- dfr$month; year &lt;- dfr$year
y &lt;- abs(month-6.5) + 2*runif(length(month)) + year-1997
s &lt;- summarize(y, llist(month,year), smedian.hilow, conf.int=.5) 
xYplot(Cbind(y,Lower,Upper) ~ month, groups=year, data=s, 
       keys='lines', method='alt', type='b')
# Can also do:
s &lt;- summarize(y, llist(month,year), quantile, probs=c(.5,.25,.75),
               stat.name=c('y','Q1','Q3')) 
xYplot(Cbind(y, Q1, Q3) ~ month, groups=year, data=s, 
       type='b', keys='lines') 
# Or:
xYplot(y ~ month, groups=year, keys='lines', nx=FALSE, method='quantile',
       type='b') 
# nx=FALSE means to treat month as a discrete variable


# To display means and bootstrapped nonparametric confidence intervals 
# use:
s &lt;- summarize(y, llist(month,year), smean.cl.boot) 
s
xYplot(Cbind(y, Lower, Upper) ~ month | year, data=s, type='b')
# Can also use Y &lt;- cbind(y, Lower, Upper); xYplot(Cbind(Y) ~ ...) 
# Or:
xYplot(y ~ month | year, nx=FALSE, method=smean.cl.boot, type='b')


# This example uses the summarize function in Hmisc to 
# compute the median and outer quartiles.  The outer quartiles are 
# displayed using "filled bands"


s &lt;- summarize(y, llist(month,year), smedian.hilow, conf.int=.5) 


# filled bands: default fill = pastel colors matching solid colors
# in superpose.line (this works differently in R)
xYplot ( Cbind ( y, Lower, Upper ) ~ month, groups=year, 
     method="filled bands" , data=s, type="l")


# note colors based on levels of selected subgroups, not first two colors
xYplot ( Cbind ( y, Lower, Upper ) ~ month, groups=year, 
     method="filled bands" , data=s, type="l",
     subset=(year == 1998 | year == 2000), label.curves=FALSE )


# filled bands using black lines with selected solid colors for fill
xYplot ( Cbind ( y, Lower, Upper ) ~ month, groups=year, 
     method="filled bands" , data=s, label.curves=FALSE,
     type="l", col=1, col.fill = 2:3)
Key(.5,.8,col = 2:3) #use fill colors in key


# A good way to check for stable variance of residuals from ols 
# xYplot(resid(fit) ~ fitted(fit), method=smean.sdl) 
# smean.sdl is defined with summary.formula in Hmisc


# Plot y vs. a special variable x
# xYplot(y ~ numericScale(x, label='Label for X') | country) 
# For this example could omit label= and specify 
#    y ~ numericScale(x) | country, xlab='Label for X'


# Here is an example of using xYplot with several options
# to change various Trellis parameters,
# xYplot(y ~ x | z, groups=v, pch=c('1','2','3'),
#        layout=c(3,1),     # 3 panels side by side
#        ylab='Y Label', xlab='X Label',
#        main=list('Main Title', cex=1.5),
#        par.strip.text=list(cex=1.2),
#        strip=function(\dots) strip.default(\dots, style=1),
#        scales=list(alternating=FALSE))


#
# Dotplot examples
#


s &lt;- summarize(y, llist(month,year), smedian.hilow, conf.int=.5) 


setTrellis()            # blank conditioning panel backgrounds 
Dotplot(month ~ Cbind(y, Lower, Upper) | year, data=s) 
# or Cbind(\dots), groups=year, data=s


# Display a 5-number (5-quantile) summary (2 intervals, dot=median) 
# Note that summarize produces a matrix for y, and Cbind(y) trusts the 
# first column to be the point estimate (here the median) 
s &lt;- summarize(y, llist(month,year), quantile,
               probs=c(.5,.05,.25,.75,.95), type='matrix') 
Dotplot(month ~ Cbind(y) | year, data=s) 
# Use factor(year) to make actual years appear in conditioning title strips

# Plot proportions and their Wilson confidence limits
set.seed(3)
d &lt;- expand.grid(continent=c('USA','Europe'), year=1999:2001,
                 reps=1:100)
# Generate binary events from a population probability of 0.2
# of the event, same for all years and continents
d$y &lt;- ifelse(runif(6*100) &lt;= .2, 1, 0)
s &lt;- with(d,
          summarize(y, llist(continent,year),
                    function(y) {
                     n &lt;- sum(!is.na(y))
                     s &lt;- sum(y, na.rm=TRUE)
                     binconf(s, n)
                    }, type='matrix')
)

Dotplot(year ~ Cbind(y) | continent,  data=s, ylab='Year',
        xlab='Probability')


# Dotplot(z ~ x | g1*g2)                 
# 2-way conditioning 
# Dotplot(z ~ x | g1, groups=g2); Key()  
# Key defines symbols for g2


# If the data are organized so that the mean, lower, and upper 
# confidence limits are in separate records, the Hmisc reShape 
# function is useful for assembling these 3 values as 3 variables 
# a single observation, e.g., assuming type has values such as 
# c('Mean','Lower','Upper'):
# a &lt;- reShape(y, id=month, colvar=type) 
# This will make a matrix with 3 columns named Mean Lower Upper 
# and with 1/3 as many rows as the original data 
</code></pre>

<hr>
<h2 id='yearDays'> Get Number of Days in Year or Month </h2><span id='topic+yearDays'></span><span id='topic+monthDays'></span>

<h3>Description</h3>

<p>Returns the number of days in a specific year or month.</p>


<h3>Usage</h3>

<pre><code class='language-R'>yearDays(time)

monthDays(time)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="yearDays_+3A_time">time</code></td>
<td>

<p>A POSIXt or Date object describing the month or year in
question.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Charles Dupont </p>


<h3>See Also</h3>

 <p><code><a href="base.html#topic+POSIXt">POSIXt</a></code>, <code><a href="base.html#topic+Date">Date</a></code> </p>

<hr>
<h2 id='ynbind'>Combine Variables in a Matrix</h2><span id='topic+ynbind'></span><span id='topic++5B.ynbind'></span><span id='topic+pBlock'></span><span id='topic++5B.pBlock'></span>

<h3>Description</h3>

<p><code>ynbind</code> column binds a series of related yes/no variables,
allowing for a final argument <code>label</code> used to label the panel
created for the group.  <code>label</code>s for individual variables are
collected into a vector attribute <code>"labels"</code> for the result;
original variable names are used in place of labels for those variables
without labels.  A positive response is taken to be <code>y, yes,
present</code> (ignoring case) or a <code>logical</code> <code>TRUE</code> value.  By
default, the columns are sorted be ascending order or the overall
proportion of positives.  A subsetting method is provided for objects of
class <code>"ynbind"</code>.
</p>
<p><code>pBlock</code> creates a matrix similarly labeled, from a general set of
variables (without special handling of binaries), and sets to <code>NA</code>
any observation not in <code>subset</code> so that when that block of
variables is analyzed it will be only for that subset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ynbind(..., label = deparse(substitute(...)),
       asna = c("unknown", "unspecified"), sort = TRUE)

pBlock(..., subset=NULL, label = deparse(substitute(...)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ynbind_+3A_...">...</code></td>
<td>
<p>a series of vectors</p>
</td></tr>
<tr><td><code id="ynbind_+3A_label">label</code></td>
<td>
<p>a label for the group, to be attached to the resulting
matrix as a <code>"label"</code> attribute, used by <code><a href="#topic+summaryP">summaryP</a></code>.</p>
</td></tr>
<tr><td><code id="ynbind_+3A_asna">asna</code></td>
<td>
<p>a vector of character strings specifying levels that are
to be treated the same as <code>NA</code> if present</p>
</td></tr>
<tr><td><code id="ynbind_+3A_sort">sort</code></td>
<td>
<p>set to <code>FALSE</code> to not sort the columns by their
proportions</p>
</td></tr>
<tr><td><code id="ynbind_+3A_subset">subset</code></td>
<td>
<p>subset criteria - either a vector of logicals or subscripts</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix of class <code>"ynbind"</code> or
<code>"pBlock"</code> with <code>"label"</code> and <code>"labels"</code> attributes.
For <code>"pBlock"</code>, factor input vectors will have values converted
to <code>character</code>. 
</p>


<h3>Author(s)</h3>

<p>Frank Harrell</p>


<h3>See Also</h3>

<p><code><a href="#topic+summaryP">summaryP</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x1 &lt;- c('yEs', 'no', 'UNKNOWN', NA)
x2 &lt;- c('y', 'n', 'no', 'present')
label(x2) &lt;- 'X2'
X &lt;- ynbind(x1, x2, label='x1-2')
X[1:3,]

pBlock(x1, x2, subset=2:3, label='x1-2')
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
