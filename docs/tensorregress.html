<!DOCTYPE html><html lang="en"><head><title>Help for package tensorregress</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tensorregress}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#as.tensor'><p>Tensor Conversion</p></a></li>
<li><a href='#dim-methods'><p>Mode Getter for Tensor</p></a></li>
<li><a href='#fold'><p>General Folding of Matrix</p></a></li>
<li><a href='#HCP'><p>HCP data</p></a></li>
<li><a href='#hosvd'><p>(Truncated-)Higher-order SVD</p></a></li>
<li><a href='#kronecker_list'><p>List Kronecker Product</p></a></li>
<li><a href='#nations'><p>nations data</p></a></li>
<li><a href='#rand_tensor'><p>Tensor with Random Entries</p></a></li>
<li><a href='#sele_rank'><p>Rank selection</p></a></li>
<li><a href='#sim_data'><p>Simulation of supervised tensor decomposition models</p></a></li>
<li><a href='#tensor_regress'><p>Supervised Tensor Decomposition with Interactive Side Information</p></a></li>
<li><a href='#Tensor-class'><p>S4 Class for a Tensor</p></a></li>
<li><a href='#ttl'><p>Tensor Times List</p></a></li>
<li><a href='#ttm'><p>Tensor Matrix Product (m-Mode Product)</p></a></li>
<li><a href='#tucker'><p>Tucker Decomposition</p></a></li>
<li><a href='#unfold-methods'><p>Tensor Unfolding</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Supervised Tensor Decomposition with Side Information</td>
</tr>
<tr>
<td>Version:</td>
<td>5.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-06-20</td>
</tr>
<tr>
<td>Description:</td>
<td>Implement the alternating algorithm for supervised tensor decomposition with interactive side information. Details can be found in the publication Hu, Jiaxin, Chanwoo Lee, and Miaoyan Wang. "Generalized Tensor Decomposition with features on multiple modes." Journal of Computational and Graphical Statistics, Vol. 31, No. 1, 204-218, 2022 &lt;<a href="https://doi.org/10.1080%2F10618600.2021.1978471">doi:10.1080/10618600.2021.1978471</a>&gt;.</td>
</tr>
<tr>
<td>Imports:</td>
<td>pracma,MASS,methods</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jiaxin Hu &lt;jhu267@wisc.edu&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Author:</td>
<td>Jiaxin Hu [aut, cre, cph],
  Chen Zhang [aut, cph],
  Chanwoo Lee [aut, cph],
  Miaoyan Wang [aut, cph]</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-06-20 17:03:06 UTC; jiaxinhu</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-06-20 17:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='as.tensor'>Tensor Conversion</h2><span id='topic+as.tensor'></span>

<h3>Description</h3>

<p>Create a <code><a href="#topic+Tensor-class">Tensor-class</a></code> object from an <code>array</code>, <code>matrix</code>, or <code>vector</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.tensor(x, drop = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as.tensor_+3A_x">x</code></td>
<td>
<p>an instance of <code>array</code>, <code>matrix</code>, or <code>vector</code></p>
</td></tr>
<tr><td><code id="as.tensor_+3A_drop">drop</code></td>
<td>
<p>whether or not modes of 1 should be dropped</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="#topic+Tensor-class">Tensor-class</a></code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#From vector
vec &lt;- runif(100); vecT &lt;- as.tensor(vec); vecT
#From matrix
mat &lt;- matrix(runif(1000),nrow=100,ncol=10)
matT &lt;- as.tensor(mat); matT
#From array
indices &lt;- c(10,20,30,40)
arr &lt;- array(runif(prod(indices)), dim = indices)
arrT &lt;- as.tensor(arr); arrT
</code></pre>

<hr>
<h2 id='dim-methods'>Mode Getter for Tensor</h2><span id='topic+dim-methods'></span><span id='topic+dim+2CTensor-method'></span>

<h3>Description</h3>

<p>Return the vector of modes from a tensor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Tensor'
dim(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dim-methods_+3A_x">x</code></td>
<td>
<p>the Tensor instance</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>dim(x)</code>
</p>


<h3>Value</h3>

<p>an integer vector of the modes associated with <code>x</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor()
dim(tnsr)
</code></pre>

<hr>
<h2 id='fold'>General Folding of Matrix</h2><span id='topic+fold'></span>

<h3>Description</h3>

<p>General folding of a matrix into a Tensor. This is designed to be the inverse function to <code><a href="#topic+unfold-methods">unfold-methods</a></code>, with the same ordering of the indices. This amounts to following: if we were to unfold a Tensor using a set of <code>row_idx</code> and <code>col_idx</code>, then we can fold the resulting matrix back into the original Tensor using the same <code>row_idx</code> and <code>col_idx</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fold(mat, row_idx = NULL, col_idx = NULL, modes = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fold_+3A_mat">mat</code></td>
<td>
<p>matrix to be folded into a Tensor</p>
</td></tr>
<tr><td><code id="fold_+3A_row_idx">row_idx</code></td>
<td>
<p>the indices of the modes that are mapped onto the row space</p>
</td></tr>
<tr><td><code id="fold_+3A_col_idx">col_idx</code></td>
<td>
<p>the indices of the modes that are mapped onto the column space</p>
</td></tr>
<tr><td><code id="fold_+3A_modes">modes</code></td>
<td>
<p>the modes of the output Tensor</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses <code>aperm</code> as the primary workhorse.
</p>


<h3>Value</h3>

<p>Tensor object with modes given by <code>modes</code>
</p>


<h3>References</h3>

<p>T. Kolda, B. Bader, &quot;Tensor decomposition and applications&quot;. SIAM Applied Mathematics and Applications 2009, Vol. 51, No. 3 (September 2009), pp. 455-500. URL: https://www.jstor.org/stable/25662308.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+unfold-methods">unfold-methods</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- new('Tensor',3L,c(3L,4L,5L),data=runif(60))
matT3&lt;-unfold(tnsr,row_idx=2,col_idx=c(3,1))
identical(fold(matT3,row_idx=2,col_idx=c(3,1),modes=c(3,4,5)),tnsr)
</code></pre>

<hr>
<h2 id='HCP'>HCP data</h2><span id='topic+HCP'></span>

<h3>Description</h3>

<p>The array &quot;tensor&quot; is a 68 × 68 × 136 binary tensor consisting of structural connectivity patterns among 68 brain regions for 136 individuals from Human Connectome Project (HCP). All the individual images were preprocessed following a standard pipeline (Zhang et al., 2018), and the brain was parcellated to 68 regions-of-interest following the Desikan atlas (Desikan et al., 2006). The tensor entries encode the presence or absence of fiber connections between those 68 brain regions for each of the 136 individuals.
The data frame &quot;attr&quot; is a 136 × 573 matrix consisting of 573 personal features for 136 individuals. The full list of covariates can be found at:
https://wiki.humanconnectome.org/display/PublicData/
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(HCP)
</code></pre>


<h3>Format</h3>

<p>A list. Includes a 68-68-136 binary array named &quot;tensor&quot; and a 136-573 data frame named &quot;attr&quot;.
</p>

<hr>
<h2 id='hosvd'>(Truncated-)Higher-order SVD</h2><span id='topic+hosvd'></span>

<h3>Description</h3>

<p>Higher-order SVD of a K-Tensor. Write the K-Tensor as a (m-mode) product of a core Tensor (possibly smaller modes) and K orthogonal factor matrices. Truncations can be specified via <code>ranks</code> (making them smaller than the original modes of the K-Tensor will result in a truncation). For the mathematical details on HOSVD, consult Lathauwer et. al. (2000).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hosvd(tnsr, ranks = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hosvd_+3A_tnsr">tnsr</code></td>
<td>
<p>Tensor with K modes</p>
</td></tr>
<tr><td><code id="hosvd_+3A_ranks">ranks</code></td>
<td>
<p>a vector of desired modes in the output core tensor, default is <code>tnsr@modes</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>A progress bar is included to help monitor operations on large tensors.
</p>


<h3>Value</h3>

<p>a list containing the following:</p>

<dl>
<dt><code>Z</code></dt><dd><p>core tensor with modes speficied by <code>ranks</code></p>
</dd>
<dt><code>U</code></dt><dd><p>a list of orthogonal matrices, one for each mode</p>
</dd>
<dt><code>est</code></dt><dd><p>estimate of <code>tnsr</code> after compression</p>
</dd>
<dt><code>fnorm_resid</code></dt><dd><p>the Frobenius norm of the error <code>fnorm(est-tnsr)</code> - if there was no truncation, then this is on the order of mach_eps * fnorm. </p>
</dd>
</dl>



<h3>Note</h3>

<p>The length of <code>ranks</code> must match <code>tnsr@num_modes</code>.
</p>


<h3>References</h3>

<p>L. Lathauwer, B.Moor, J. Vandewalle, &quot;A multilinear singular value decomposition&quot;. Journal of Matrix Analysis and Applications 2000, Vol. 21, No. 4, pp. 1253–1278.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tucker">tucker</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor(c(6,7,8))
hosvdD &lt;-hosvd(tnsr)
hosvdD$fnorm_resid
hosvdD2 &lt;-hosvd(tnsr,ranks=c(3,3,4))
hosvdD2$fnorm_resid
</code></pre>

<hr>
<h2 id='kronecker_list'>List Kronecker Product</h2><span id='topic+kronecker_list'></span>

<h3>Description</h3>

<p>Returns the Kronecker product from a list of matrices or vectors. Commonly used for n-mode products and various Tensor decompositions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kronecker_list(L)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kronecker_list_+3A_l">L</code></td>
<td>
<p>list of matrices or vectors</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix that is the Kronecker product
</p>


<h3>Examples</h3>

<pre><code class='language-R'>smalllizt &lt;- list('mat1' = matrix(runif(12),ncol=4), 
'mat2' = matrix(runif(12),ncol=4),
'mat3' = matrix(runif(12),ncol=4))
dim(kronecker_list(smalllizt))
</code></pre>

<hr>
<h2 id='nations'>nations data</h2><span id='topic+nations'></span>

<h3>Description</h3>

<p>The array &quot;R&quot; is a 14 × 14 × 56 binary tensor consisting of 56 political relations of 14 countries between 1950 and 1965. The tensor entry indicates the presence or absence of a political action, such as “treaties”, “sends tourists to”, between the nations. Please set the diagonal elements Y(i,i,k) = 0 in the analysis.
The matrix &quot;cov&quot; is a 14 × 6 matrix describing a few important country attributes, e.g. whether a nation is actively involved in medicine NGO, law NGO, or belongs to a catholic nation, etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(nations)
</code></pre>


<h3>Format</h3>

<p>A list. Includes a 14-14-56 binary array named &quot;R&quot; and a 14-6 matrix named &quot;cov&quot;.
</p>

<hr>
<h2 id='rand_tensor'>Tensor with Random Entries</h2><span id='topic+rand_tensor'></span>

<h3>Description</h3>

<p>Generate a Tensor with specified modes with iid normal(0,1) entries.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rand_tensor(modes = c(3, 4, 5), drop = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rand_tensor_+3A_modes">modes</code></td>
<td>
<p>the modes of the output Tensor</p>
</td></tr>
<tr><td><code id="rand_tensor_+3A_drop">drop</code></td>
<td>
<p>whether or not modes equal to 1 should be dropped</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Tensor object with modes given by <code>modes</code>
</p>


<h3>Note</h3>

<p>Default <code>rand_tensor()</code> generates a 3-Tensor with modes <code>c(3,4,5)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rand_tensor()
rand_tensor(c(4,4,4))
rand_tensor(c(10,2,1),TRUE)
</code></pre>

<hr>
<h2 id='sele_rank'>Rank selection</h2><span id='topic+sele_rank'></span>

<h3>Description</h3>

<p>Estimate the Tucker rank of tensor decomposition based on BIC criterion. The choice of BIC
aims to balance between the goodness-of-fit for the data and the degree of freedom in the population model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sele_rank(
  tsr,
  X_covar1 = NULL,
  X_covar2 = NULL,
  X_covar3 = NULL,
  rank_range,
  niter = 10,
  cons = "non",
  lambda = 0.1,
  alpha = 1,
  solver = "CG",
  dist,
  initial = c("random", "QR_tucker")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sele_rank_+3A_tsr">tsr</code></td>
<td>
<p>response tensor with 3 modes</p>
</td></tr>
<tr><td><code id="sele_rank_+3A_x_covar1">X_covar1</code></td>
<td>
<p>side information on first mode</p>
</td></tr>
<tr><td><code id="sele_rank_+3A_x_covar2">X_covar2</code></td>
<td>
<p>side information on second mode</p>
</td></tr>
<tr><td><code id="sele_rank_+3A_x_covar3">X_covar3</code></td>
<td>
<p>side information on third mode</p>
</td></tr>
<tr><td><code id="sele_rank_+3A_rank_range">rank_range</code></td>
<td>
<p>a matrix containing rank candidates on each row</p>
</td></tr>
<tr><td><code id="sele_rank_+3A_niter">niter</code></td>
<td>
<p>max number of iterations if update does not convergence</p>
</td></tr>
<tr><td><code id="sele_rank_+3A_cons">cons</code></td>
<td>
<p>the constraint method, &quot;non&quot; for without constraint, &quot;vanilla&quot; for global scale down at each iteration,
</p>
<p>&quot;penalty&quot; for adding log-barrier penalty to object function.</p>
</td></tr>
<tr><td><code id="sele_rank_+3A_lambda">lambda</code></td>
<td>
<p>penalty coefficient for &quot;penalty&quot; constraint</p>
</td></tr>
<tr><td><code id="sele_rank_+3A_alpha">alpha</code></td>
<td>
<p>max norm constraint on linear predictor</p>
</td></tr>
<tr><td><code id="sele_rank_+3A_solver">solver</code></td>
<td>
<p>solver for solving object function when using &quot;penalty&quot; constraint, see &quot;details&quot;</p>
</td></tr>
<tr><td><code id="sele_rank_+3A_dist">dist</code></td>
<td>
<p>distribution of response tensor, see &quot;details&quot;</p>
</td></tr>
<tr><td><code id="sele_rank_+3A_initial">initial</code></td>
<td>
<p>initialization of the alternating optimiation, &quot;random&quot; for random initialization, &quot;QR_tucker&quot; for deterministic initialization using tucker decomposition</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For rank selection, recommend using non-constraint version.
</p>
<p>Constraint <code>penalty</code> adds log-barrier regularizer to
general object function (negative log-likelihood). The main function uses solver in function &quot;optim&quot; to
solve the objective function. The &quot;solver&quot; passes to the argument &quot;method&quot; in function &quot;optim&quot;.
</p>
<p><code>dist</code> specifies three distributions of response tensor: binary, poisson and normal distributions.
</p>


<h3>Value</h3>

<p>a list containing the following:
</p>
<p><code>rank</code> a vector with selected rank with minimal BIC
</p>
<p><code>result</code>  a matrix containing rank candidate and its loglikelihood and BIC on each row
</p>


<h3>Examples</h3>

<pre><code class='language-R'>seed=24
dist='binary'
data=sim_data(seed, whole_shape = c(20,20,20),
core_shape=c(3,3,3),p=c(5,5,5),dist=dist, dup=5, signal=4)
rank_range = rbind(c(3,3,3),c(3,3,2),c(3,2,2),c(2,2,2),c(3,2,3))
re = sele_rank(data$tsr[[1]],data$X_covar1,data$X_covar2,data$X_covar3,
 rank_range = rank_range,niter=10,cons = 'non',dist = dist,initial = "random")
</code></pre>

<hr>
<h2 id='sim_data'>Simulation of supervised tensor decomposition models</h2><span id='topic+sim_data'></span>

<h3>Description</h3>

<p>Generate tensor data with multiple side information matrices under different simulation models, specifically for tensors with 3 modes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim_data(
  seed = NA,
  whole_shape = c(20, 20, 20),
  core_shape = c(3, 3, 3),
  p = c(3, 3, 0),
  dist,
  dup,
  signal,
  block = rep(FALSE, 3),
  ortho = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sim_data_+3A_seed">seed</code></td>
<td>
<p>a random seed for generating data</p>
</td></tr>
<tr><td><code id="sim_data_+3A_whole_shape">whole_shape</code></td>
<td>
<p>a vector containing dimension of the tensor</p>
</td></tr>
<tr><td><code id="sim_data_+3A_core_shape">core_shape</code></td>
<td>
<p>a vector containing Tucker rank of the tensor decomposition</p>
</td></tr>
<tr><td><code id="sim_data_+3A_p">p</code></td>
<td>
<p>a vector containing numbers of side information on each mode, see &quot;details&quot;</p>
</td></tr>
<tr><td><code id="sim_data_+3A_dist">dist</code></td>
<td>
<p>distribution of response tensor, see &quot;details&quot;</p>
</td></tr>
<tr><td><code id="sim_data_+3A_dup">dup</code></td>
<td>
<p>number of simulated tensors from the same linear predictor</p>
</td></tr>
<tr><td><code id="sim_data_+3A_signal">signal</code></td>
<td>
<p>a scalar controlling the max norm of the linear predictor</p>
</td></tr>
<tr><td><code id="sim_data_+3A_block">block</code></td>
<td>
<p>a vector containing boolean variables, see &quot;details&quot;</p>
</td></tr>
<tr><td><code id="sim_data_+3A_ortho">ortho</code></td>
<td>
<p>if &quot;TRUE&quot;, generate side information matrices with orthogonal columns; if &quot;FLASE&quot; (default), generate side information matrices with gaussian entries</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default non-positive entry in <code>p</code> indicates no covariate on the corresponding mode of the tensor.
</p>
<p><code>dist</code> specifies three distributions of response tensor: binary, poisson or normal distribution.
</p>
<p><code>block</code> specifies whether the coefficient factor matrix is a membership matrix, set to <code>TRUE</code> when utilizing the stochastic block model
</p>


<h3>Value</h3>

<p>a list containing the following:
</p>
<p><code>tsr</code> a list of simulated tensors, with the number of replicates specified by <code>dup</code>
</p>
<p><code>X_covar1</code>  a matrix, side information on first mode
</p>
<p><code>X_covar2</code>  a matrix, side information on second mode
</p>
<p><code>X_covar3</code>  a matrix, side information on third mode
</p>
<p><code>W</code> a list of orthogonal factor matrices - one for each mode, with the number of columns given by <code>core_shape</code>
</p>
<p><code>G</code>  an array, core tensor with size specified by <code>core_shape</code>
</p>
<p><code>C_ts</code>  an array, coefficient tensor, Tucker product of <code>G</code>,<code>A</code>,<code>B</code>,<code>C</code>
</p>
<p><code>U</code> an array, linear predictor,i.e. Tucker product of <code>C_ts</code>,<code>X_covar1</code>,<code>X_covar2</code>,<code>X_covar3</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>seed = 34
dist = 'binary'
data=sim_data(seed, whole_shape = c(20,20,20), core_shape=c(3,3,3),
p=c(5,5,5),dist=dist, dup=5, signal=4)
</code></pre>

<hr>
<h2 id='tensor_regress'>Supervised Tensor Decomposition with Interactive Side Information</h2><span id='topic+tensor_regress'></span>

<h3>Description</h3>

<p>Supervised tensor decomposition with interactive side information on multiple modes. Main function in the package. The function takes a response tensor, multiple side information matrices,
and a desired Tucker rank as input. The output is a rank-constrained M-estimate of the core tensor and factor matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tensor_regress(
  tsr,
  X_covar1 = NULL,
  X_covar2 = NULL,
  X_covar3 = NULL,
  core_shape,
  niter = 20,
  cons = c("non", "vanilla", "penalty"),
  lambda = 0.1,
  alpha = 1,
  solver = "CG",
  dist = c("binary", "poisson", "normal"),
  traj_long = FALSE,
  initial = c("random", "QR_tucker")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tensor_regress_+3A_tsr">tsr</code></td>
<td>
<p>response tensor with 3 modes</p>
</td></tr>
<tr><td><code id="tensor_regress_+3A_x_covar1">X_covar1</code></td>
<td>
<p>side information on first mode</p>
</td></tr>
<tr><td><code id="tensor_regress_+3A_x_covar2">X_covar2</code></td>
<td>
<p>side information on second mode</p>
</td></tr>
<tr><td><code id="tensor_regress_+3A_x_covar3">X_covar3</code></td>
<td>
<p>side information on third mode</p>
</td></tr>
<tr><td><code id="tensor_regress_+3A_core_shape">core_shape</code></td>
<td>
<p>the Tucker rank of the tensor decomposition</p>
</td></tr>
<tr><td><code id="tensor_regress_+3A_niter">niter</code></td>
<td>
<p>max number of iterations if update does not convergence</p>
</td></tr>
<tr><td><code id="tensor_regress_+3A_cons">cons</code></td>
<td>
<p>the constraint method, &quot;non&quot; for without constraint, &quot;vanilla&quot; for global scale down at each iteration, &quot;penalty&quot; for adding log-barrier penalty to object function</p>
</td></tr>
<tr><td><code id="tensor_regress_+3A_lambda">lambda</code></td>
<td>
<p>penalty coefficient for &quot;penalty&quot; constraint</p>
</td></tr>
<tr><td><code id="tensor_regress_+3A_alpha">alpha</code></td>
<td>
<p>max norm constraint on linear predictor</p>
</td></tr>
<tr><td><code id="tensor_regress_+3A_solver">solver</code></td>
<td>
<p>solver for solving object function when using &quot;penalty&quot; constraint, see &quot;details&quot;</p>
</td></tr>
<tr><td><code id="tensor_regress_+3A_dist">dist</code></td>
<td>
<p>distribution of the response tensor, see &quot;details&quot;</p>
</td></tr>
<tr><td><code id="tensor_regress_+3A_traj_long">traj_long</code></td>
<td>
<p>if &quot;TRUE&quot;, set the minimal iteration number to 8; if &quot;FALSE&quot;, set the minimal iteration number to 0</p>
</td></tr>
<tr><td><code id="tensor_regress_+3A_initial">initial</code></td>
<td>
<p>initialization of the alternating optimiation, &quot;random&quot; for random initialization, &quot;QR_tucker&quot; for deterministic initialization using tucker decomposition</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Constraint <code>penalty</code> adds log-barrier regularizer to
general object function (negative log-likelihood). The main function uses solver in function &quot;optim&quot; to
solve the objective function. The &quot;solver&quot; passes to the argument &quot;method&quot; in function &quot;optim&quot;.
</p>
<p><code>dist</code> specifies three distributions of response tensor: binary, poisson and normal distribution.
</p>
<p>If <code>dist</code> is set to &quot;normal&quot; and <code>initial</code> is set to &quot;QR_tucker&quot;, then the function returns the results after initialization.
</p>


<h3>Value</h3>

<p>a list containing the following:
</p>
<p><code>W</code> a list of orthogonal factor matrices - one for each mode, with the number of columns given by <code>core_shape</code>
</p>
<p><code>G</code>  an array, core tensor with the size specified by <code>core_shape</code>
</p>
<p><code>C_ts</code>  an array, coefficient tensor, Tucker product of <code>G</code>,<code>A</code>,<code>B</code>,<code>C</code>
</p>
<p><code>U</code> linear predictor,i.e. Tucker product of <code>C_ts</code>,<code>X_covar1</code>,<code>X_covar2</code>,<code>X_covar3</code>
</p>
<p><code>lglk</code> a vector containing loglikelihood at convergence
</p>
<p><code>sigma</code> a scalar, estimated error variance (for Gaussian tensor) or dispersion parameter (for Bernoulli and Poisson tensors)
</p>
<p><code>violate</code> a vector listing whether each iteration violates the max norm constraint on the linear predictor, <code>1</code> indicates violation
</p>


<h3>Examples</h3>

<pre><code class='language-R'>seed = 34
dist = 'binary'
data=sim_data(seed, whole_shape = c(20,20,20), core_shape=c(3,3,3),
p=c(5,5,5),dist=dist, dup=5, signal=4)
re = tensor_regress(data$tsr[[1]],data$X_covar1,data$X_covar2,data$X_covar3,
core_shape=c(3,3,3),niter=10, cons = 'non', dist = dist,initial = "random")
</code></pre>

<hr>
<h2 id='Tensor-class'>S4 Class for a Tensor</h2><span id='topic+Tensor-class'></span><span id='topic+Tensor'></span>

<h3>Description</h3>

<p>An S4 class for a tensor with arbitrary number of modes. The Tensor class extends the base &quot;array&quot; class to include additional tensor manipulation (folding, unfolding, reshaping, subsetting) as well as a formal class definition that enables more explicit tensor algebra.
</p>


<h3>Slots</h3>


<dl>
<dt>num_modes</dt><dd><p>number of modes (integer)</p>
</dd>
<dt>modes</dt><dd><p>vector of modes (integer), aka sizes/extents/dimensions</p>
</dd>
<dt>data</dt><dd><p>actual data of the tensor, which can be 'array' or 'vector'</p>
</dd>
</dl>



<h3>Note</h3>

<p>All of the decompositions and regression models in this package require a Tensor input.
</p>


<h3>Author(s)</h3>

<p>James Li <a href="mailto:jamesyili@gmail.com">jamesyili@gmail.com</a>
</p>


<h3>References</h3>

<p>James Li, Jacob Bien, Martin T. Wells (2018). rTensor: An R Package for Multidimensional Array (Tensor) Unfolding, Multiplication, and Decomposition. Journal of Statistical Software, Vol. 87, No. 10, 1-31. URL: http://www.jstatsoft.org/v087/i10/.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+as.tensor">as.tensor</a></code>
</p>

<hr>
<h2 id='ttl'>Tensor Times List</h2><span id='topic+ttl'></span>

<h3>Description</h3>

<p>Contracted (m-Mode) product between a Tensor of arbitrary number of modes and a list of matrices. The result is folded back into Tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ttl(tnsr, list_mat, ms = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ttl_+3A_tnsr">tnsr</code></td>
<td>
<p>Tensor object with K modes</p>
</td></tr>
<tr><td><code id="ttl_+3A_list_mat">list_mat</code></td>
<td>
<p>a list of matrices</p>
</td></tr>
<tr><td><code id="ttl_+3A_ms">ms</code></td>
<td>
<p>a vector of modes to contract on (order should match the order of <code>list_mat</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Performs <code>ttm</code> repeated for a single Tensor and a list of matrices on multiple modes. For instance, suppose we want to do multiply a Tensor object <code>tnsr</code> with three matrices <code>mat1</code>, <code>mat2</code>, <code>mat3</code> on modes 1, 2, and 3. We could do <code>ttm(ttm(ttm(tnsr,mat1,1),mat2,2),3)</code>, or we could do <code>ttl(tnsr,list(mat1,mat2,mat3),c(1,2,3))</code>. The order of the matrices in the list should obviously match the order of the modes. This is a common operation for various Tensor decompositions such as CP and Tucker. For the math on the m-Mode Product, see Kolda and Bader (2009).
</p>


<h3>Value</h3>

<p>Tensor object with K modes
</p>


<h3>Note</h3>

<p>The returned Tensor does not drop any modes equal to 1.
</p>


<h3>References</h3>

<p>T. Kolda, B. Bader, &quot;Tensor decomposition and applications&quot;. SIAM Applied Mathematics and Applications 2009, Vol. 51, No. 3 (September 2009), pp. 455-500. URL: https://www.jstor.org/stable/25662308
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ttm">ttm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- new('Tensor',3L,c(3L,4L,5L),data=runif(60))
lizt &lt;- list('mat1' = matrix(runif(30),ncol=3), 
'mat2' = matrix(runif(40),ncol=4),
'mat3' = matrix(runif(50),ncol=5))
ttl(tnsr,lizt,ms=c(1,2,3))
</code></pre>

<hr>
<h2 id='ttm'>Tensor Matrix Product (m-Mode Product)</h2><span id='topic+ttm'></span>

<h3>Description</h3>

<p>Contracted (m-Mode) product between a Tensor of arbitrary number of modes and a matrix. The result is folded back into Tensor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ttm(tnsr, mat, m = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ttm_+3A_tnsr">tnsr</code></td>
<td>
<p>Tensor object with K modes</p>
</td></tr>
<tr><td><code id="ttm_+3A_mat">mat</code></td>
<td>
<p>input matrix with same number columns as the <code>m</code>th mode of <code>tnsr</code></p>
</td></tr>
<tr><td><code id="ttm_+3A_m">m</code></td>
<td>
<p>the mode to contract on</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By definition, the number of columns in <code>mat</code> must match the <code>m</code>th mode of <code>tnsr</code>. For the math on the m-Mode Product, see Kolda and Bader (2009).
</p>


<h3>Value</h3>

<p>a Tensor object with K modes
</p>


<h3>Note</h3>

<p>The <code>m</code>th mode of <code>tnsr</code> must match the number of columns in <code>mat</code>. By default, the returned Tensor does not drop any modes equal to 1.
</p>


<h3>References</h3>

<p>T. Kolda, B. Bader, &quot;Tensor decomposition and applications&quot;. SIAM Applied Mathematics and Applications 2009, Vol. 51, No. 3 (September 2009), pp. 455-500. URL: https://www.jstor.org/stable/25662308
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ttl">ttl</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- new('Tensor',3L,c(3L,4L,5L),data=runif(60))
mat &lt;- matrix(runif(50),ncol=5)
ttm(tnsr,mat,m=3)
</code></pre>

<hr>
<h2 id='tucker'>Tucker Decomposition</h2><span id='topic+tucker'></span>

<h3>Description</h3>

<p>The Tucker decomposition of a tensor. Approximates a K-Tensor using a n-mode product of a core tensor (with modes specified by <code>ranks</code>) with orthogonal factor matrices. If there is no truncation in all the modes (i.e. <code>ranks = tnsr@modes</code>), then this is the same as the HOSVD, <code><a href="#topic+hosvd">hosvd</a></code>. This is an iterative algorithm, with two possible stopping conditions: either relative error in Frobenius norm has gotten below <code>tol</code>, or the <code>max_iter</code> number of iterations has been reached. For more details on the Tucker decomposition, consult Kolda and Bader (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tucker(tnsr, ranks = NULL, max_iter = 25, tol = 1e-05)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tucker_+3A_tnsr">tnsr</code></td>
<td>
<p>Tensor with K modes</p>
</td></tr>
<tr><td><code id="tucker_+3A_ranks">ranks</code></td>
<td>
<p>a vector of the modes of the output core Tensor</p>
</td></tr>
<tr><td><code id="tucker_+3A_max_iter">max_iter</code></td>
<td>
<p>maximum number of iterations if error stays above <code>tol</code></p>
</td></tr>
<tr><td><code id="tucker_+3A_tol">tol</code></td>
<td>
<p>relative Frobenius norm error tolerance</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses the Alternating Least Squares (ALS) estimation procedure also known as Higher-Order Orthogonal Iteration (HOOI). Intialized using a (Truncated-)HOSVD. A progress bar is included to help monitor operations on large tensors.
</p>


<h3>Value</h3>

<p>a list containing the following:</p>

<dl>
<dt><code>Z</code></dt><dd><p>the core tensor, with modes specified by <code>ranks</code></p>
</dd>
<dt><code>U</code></dt><dd><p>a list of orthgonal factor matrices - one for each mode, with the number of columns of the matrices given by <code>ranks</code></p>
</dd>
<dt><code>conv</code></dt><dd><p>whether or not <code>resid</code> &lt; <code>tol</code> by the last iteration</p>
</dd>
<dt><code>est</code></dt><dd><p>estimate of <code>tnsr</code> after compression</p>
</dd>
<dt><code>norm_percent</code></dt><dd><p>the percent of Frobenius norm explained by the approximation</p>
</dd>
<dt><code>fnorm_resid</code></dt><dd><p>the Frobenius norm of the error <code>fnorm(est-tnsr)</code></p>
</dd>
<dt><code>all_resids</code></dt><dd><p>vector containing the Frobenius norm of error for all the iterations</p>
</dd>
</dl>



<h3>Note</h3>

<p>The length of <code>ranks</code> must match <code>tnsr@num_modes</code>.
</p>


<h3>References</h3>

<p>T. Kolda, B. Bader, &quot;Tensor decomposition and applications&quot;. SIAM Applied Mathematics and Applications 2009, Vol. 51, No. 3 (September 2009), pp. 455-500. URL: https://www.jstor.org/stable/25662308
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hosvd">hosvd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor(c(4,4,4,4))
tuckerD &lt;- tucker(tnsr,ranks=c(2,2,2,2))
tuckerD$conv 
tuckerD$norm_percent
plot(tuckerD$all_resids)
</code></pre>

<hr>
<h2 id='unfold-methods'>Tensor Unfolding</h2><span id='topic+unfold-methods'></span><span id='topic+unfold'></span><span id='topic+unfold+2CTensor-method'></span>

<h3>Description</h3>

<p>Unfolds the tensor into a matrix, with the modes in <code>rs</code> onto the rows and modes in <code>cs</code> onto the columns. Note that <code>c(rs,cs)</code> must have the same elements (order doesn't matter) as <code>x@modes</code>. Within the rows and columns, the order of the unfolding is determined by the order of the modes. This convention is consistent with Kolda and Bader (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unfold(tnsr, row_idx, col_idx)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="unfold-methods_+3A_tnsr">tnsr</code></td>
<td>
<p>the Tensor instance</p>
</td></tr>
<tr><td><code id="unfold-methods_+3A_row_idx">row_idx</code></td>
<td>
<p>the indices of the modes to map onto the row space</p>
</td></tr>
<tr><td><code id="unfold-methods_+3A_col_idx">col_idx</code></td>
<td>
<p>the indices of the modes to map onto the column space</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>unfold(tnsr,row_idx=NULL,col_idx=NULL)</code>
</p>


<h3>Value</h3>

<p>matrix with <code>prod(row_idx)</code> rows and <code>prod(col_idx)</code> columns
</p>


<h3>References</h3>

<p>T. Kolda, B. Bader, &quot;Tensor decomposition and applications&quot;. SIAM Applied Mathematics and Applications 2009, Vol. 51, No. 3 (September 2009), pp. 455-500. URL: https://www.jstor.org/stable/25662308.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tnsr &lt;- rand_tensor()
matT3&lt;-unfold(tnsr,row_idx=2,col_idx=c(3,1))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
