<!DOCTYPE html><html><head><title>Help for package regtools</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {regtools}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#regtools-package'><p>Overview and Package Reference Guide</p></a></li>
<li><a href='#courseRecords'>
<p>Records from several offerings of a certain course.</p></a></li>
<li><a href='#currency'>
<p>Pre-Euro Era Currency Fluctuations</p></a></li>
<li><a href='#day,day1'>
<p>Bike sharing data.</p></a></li>
<li><a href='#english'>
<p>English vocabulary data</p></a></li>
<li><a href='#factorsToDummies'><p>Factor Conversion Utilities</p></a></li>
<li><a href='#falldetection'>
<p>Fall Detection Data</p></a></li>
<li><a href='#fineTuning,knnFineTune'><p>Grid Search Plus More</p></a></li>
<li><a href='#knnest,meany,vary,loclin,predict.knn,preprocessx,kmin,parvsnonparplot,nonparvsxplot,l1,l2,kNN,bestKperPoint'><p>k-NN Nonparametric Regression and Classification</p></a></li>
<li><a href='#krsFit'><p>Tools for Neural Networks</p></a></li>
<li><a href='#lmac,makeNA,coef.lmac,vcov.lmac,pcac,loglinac,tbltofakedf'><p>Available Cases Method for Missing Data</p></a></li>
<li><a href='#ltrfreqs'>
<p>Letter Frequencies</p></a></li>
<li><a href='#misc'><p>Utilities</p></a></li>
<li><a href='#mlb'>
<p>Major Leage Baseball player data set.</p></a></li>
<li><a href='#mlens'>
<p>MovieLens User Summary Data</p></a></li>
<li><a href='#mm'><p>Method of Moments, Including Possible Regression Terms</p></a></li>
<li><a href='#multiclass routines'><p>Classification with More Than 2 Classes</p></a></li>
<li><a href='#newadult'>
<p>UCI adult income data set, adapted</p></a></li>
<li><a href='#nlshc'><p>Heteroscedastic Nonlinear Regression</p></a></li>
<li><a href='#oliveoils'>
<p>Italian olive oils data set.</p></a></li>
<li><a href='#Penrose Linear'><p>Penrose-Inverse Linear Models and Polynomial Regression</p></a></li>
<li><a href='#phoneme'>
<p>Phoneme Data</p></a></li>
<li><a href='#prgeng'>
<p>Silicon Valley programmers and engineers data</p></a></li>
<li><a href='#quizDocs'>
<p>Course quiz documents</p></a></li>
<li><a href='#ridgelm,plot.rlm'><p>Ridge Regression</p></a></li>
<li><a href='#SwissRoll'>
<p>Swiss Roll</p></a></li>
<li><a href='#textToXY,textToXYpred'><p>Tools for Text Classification</p></a></li>
<li><a href='#TStoX'><p>Transform Time Series to Rectangular Form</p></a></li>
<li><a href='#unscale'><p>Miscellaneous Utilities</p></a></li>
<li><a href='#weatherTS'>
<p>Weather Time Series</p></a></li>
<li><a href='#xyzPlot'><p>Misc. Graphics</p></a></li>
<li><a href='#yell10k'>
<p>New York Taxi Data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.7.0</td>
</tr>
<tr>
<td>Title:</td>
<td>Regression and Classification Tools</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Norm Matloff &lt;matloff@cs.ucdavis.edu&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0),FNN,gtools</td>
</tr>
<tr>
<td>Imports:</td>
<td>R.utils,mvtnorm,sandwich,MASS,car,data.table,glmnet,rje,text2vec,
polyreg</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, OpenImageR, cdparcoord, keras, magick,
partools</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Description:</td>
<td>Tools for linear, nonlinear and nonparametric regression
             and classification.  Novel graphical methods for assessment 
             of parametric models using nonparametric methods. One 
             vs. All and All vs. All multiclass classification, optional
             class probabilities adjustment.  Nonparametric regression 
             (k-NN) for general dimension, local-linear option.  Nonlinear 
             regression with Eickert-White method for dealing with 
             heteroscedasticity.  Utilities for converting time series
             to rectangular form.  Utilities for conversion between
             factors and indicator variables.  Some code related to
             "Statistical Regression and Classification: from Linear
             Models to Machine Learning", N. Matloff, 2017, CRC,
             ISBN 9781498710916.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/matloff/regtools">https://github.com/matloff/regtools</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/matloff/regtools/issues">https://github.com/matloff/regtools/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-03-30 16:15:10 UTC; normanmatloff</td>
</tr>
<tr>
<td>Author:</td>
<td>Norm Matloff <a href="https://orcid.org/0000-0001-9179-6785"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Robin Yancey [aut],
  Bochao Xin [ctb],
  Kenneth Lee [ctb],
  Rongkui Han [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-03-30 17:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='regtools-package'>Overview and Package Reference Guide</h2><span id='topic+regtools-package'></span><span id='topic+regtools'></span>

<h3>Description</h3>

 
<p>This package provides a broad collection of functions useful for
regression and classification analysis, and machine learning.
</p>


<h3>Function List</h3>

<p><b>Parametric modeling:</b>
</p>

<ul>
<li><p> nonlinear regression:  nlshc
</p>
</li>
<li><p> ridge regression:  ridgelm, plot
</p>
</li>
<li><p> missing values (also see our <span class="pkg">toweranNA</span> package):
lmac,makeNA,coef.lmac,vcov.lmac,pcac
</p>
</li></ul>

<p><b>Diagnostic plots:</b>
</p>

<ul>
<li><p> regression diagnostics:  parvsnonparplot, nonparvsxplot,
nonparvarplot
</p>
</li>
<li><p> other: boundaryplot, nonparvsxplot
</p>
</li></ul>

<p><b>Classification:</b>
</p>

<ul>
<li><p> unbalanced data: classadjust (see <b>UnbalancedClasses.md</b>)
</p>
</li>
<li><p> All vs. All: avalogtrn, avalogpred
</p>
</li>
<li><p> k-NN reweighting: exploreExpVars, plotExpVars, knnFineTune
</p>
</li></ul>

<p><b>Machine learning (also see qeML package):</b>
</p>

<ul>
<li><p> k-NN: kNN, kmin, knnest, knntrn, preprocessx, meany, vary, loclin,
predict, kmin, pwplot, bestKperPoint, knnFineTune
</p>
</li>
<li><p> neural networks: krsFit,multCol
</p>
</li>
<li><p> advanced grid search: fineTuning, fineTuningPar, plot.tuner,
knnFineTune
</p>
</li>
<li><p> loss: l1, l2, MAPE, ROC
</p>
</li></ul>

<p><b>Dummies and R factors Utilities:</b>
</p>

<ul>
<li><p> conversion between factors and dummies: dummiesToFactor,
dummiesToInt, factorsToDummies, factorToDummies, factorTo012etc,
dummiesToInt, hasFactors, charsToFactors, makeAllNumeric
</p>
</li>
<li><p> dealing with superset and subsets of factors: toSuperFactor,
toSubFactor
</p>
</li></ul>

<p><b>Statistics:</b>
</p>

<ul>
<li><p> mm
</p>
</li></ul>

<p><b>Matrix:</b>
</p>

<ul>
<li><p> multCols, constCols
</p>
</li></ul>

<p><b>Time series:</b>
</p>

<ul>
<li><p> convert rectangular to TS: TStoX
</p>
</li></ul>

<p><b>Text processing:</b>
</p>

<ul>
<li><p> textToXY
</p>
</li></ul>

<p><b>Misc.:</b>
</p>

<ul>
<li><p> scaling:  mmscale, unscale
</p>
</li>
<li><p> data frames: catDFRow, tabletofakedf
</p>
</li>
<li><p> R: getNamedArgs, ulist
</p>
</li>
<li><p> discretize
</p>
</li></ul>


<hr>
<h2 id='courseRecords'>
Records from several offerings of a certain course.
</h2><span id='topic+courseRecords'></span>

<h3>Description</h3>

<p>The data are in the form of an R list.  Each element of the list
corresponds to one offering of the course.  Fields are:  Class level;
major (two different computer science majors, LCSI in Letters and
Science and ECSE in engineering); quiz grade average (scale of 4.0, A+
counting as 4.3); homework grade average (same scale); and course letter
grade.
</p>

<hr>
<h2 id='currency'>
Pre-Euro Era Currency Fluctuations
</h2><span id='topic+currency'></span>

<h3>Description</h3>

<p>From Wai Mun Fong and Sam Ouliaris, &quot;Spectral Tests of the Martingale       
Hypothesis for Exchange Rates&quot;, Journal of Applied Econometrics, Vol.
10, No. 3, 1995, pp. 255-271.  Weekly exchange rates against US dollar,
over the period 7 August 1974 to 29 March 1989.        
</p>

<hr>
<h2 id='day+2Cday1'>
Bike sharing data.
</h2><span id='topic+day'></span><span id='topic+day1'></span><span id='topic+day2'></span>

<h3>Description</h3>

<p>This is the Bike Sharing dataset (day records only) from the UC Irvine
Machine Learning Dataset Repository.  Included here with 
permission of Dr. Hadi Fanaee.  
</p>
<p>The <code>day</code> data is as on UCI; <code>day1</code> is modified so that the
numeric weather variables are on their original scale.
</p>
<p>The <code>day2</code> is the same as <code>day1</code>, except that <code>dteday</code>
has been removed, and <code>season</code>, <code>mnth</code>, <code>weekday</code> and
<code>weathersit</code> have been converted to R factors.
</p>
<p>See <a href="https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset">https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset</a>
for details.
</p>

<hr>
<h2 id='english'>
English vocabulary data
</h2><span id='topic+english'></span>

<h3>Description</h3>

<p>The Stanford WordBank data on vocabulary acquisition in young children.
The file consists of about 5500 rows. (There are many NA values, though,
and only about 2800 complete cases.) Variables are age, birth order,
sex, mother's education and vocabulary size.
</p>

<hr>
<h2 id='factorsToDummies'>Factor Conversion Utilities</h2><span id='topic+factorToDummies'></span><span id='topic+factorsToDummies'></span><span id='topic+dummiesToFactor'></span><span id='topic+charsToFactors'></span><span id='topic+factorTo012etc'></span><span id='topic+getDFclasses'></span><span id='topic+hasCharacters'></span><span id='topic+hasFactors'></span><span id='topic+toAllNumeric'></span><span id='topic+toSubFactor'></span><span id='topic+toSuperFactor'></span><span id='topic+toAllNumeric'></span><span id='topic+discretize'></span><span id='topic+dummiesToInt'></span><span id='topic+xyDataframeToMatrix'></span>

<h3>Description</h3>

<p>Utilities from converting back and forth between factors and dummy
variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xyDataframeToMatrix(xy)
dummiesToInt(dms,inclLast=FALSE)
factorToDummies(f,fname,omitLast=FALSE,factorInfo=NULL)
factorsToDummies(dfr,omitLast=FALSE,factorsInfo=NULL,dfOut=FALSE)
dummiesToFactor(dms,inclLast=FALSE) 
charsToFactors(dtaf)
factorTo012etc(f,earlierLevels = NULL)
discretize(x,endpts)
getDFclasses(dframe)
hasCharacters(dfr)
hasFactors(x)
toAllNumeric(w,factorsInfo=NULL)
toSubFactor(f,saveLevels,lumpedLevel="zzzOther")
toSuperFactor(inFactor,superLevels)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="factorsToDummies_+3A_dfout">dfOut</code></td>
<td>
<p>If TRUE, return a data frame, otherwise a matrix.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_dms">dms</code></td>
<td>
<p>Matrix or data frame of dummy columns.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_incllast">inclLast</code></td>
<td>
<p>When forming a factor from dummies, include the last
dummy as a level if this is TRUE.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_xy">xy</code></td>
<td>
<p>A data frame mentioned for prediction, &quot;Y&quot; in last column.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_savelevels">saveLevels</code></td>
<td>
<p>In collapsing a factor, which levels to retain.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_lumpedlevel">lumpedLevel</code></td>
<td>
<p>Name of new level to be created from levels not retained.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_x">x</code></td>
<td>
<p>A numeric vector, except in <code>hasFactors</code>, where it is a
data frame.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_endpts">endpts</code></td>
<td>
<p>Vector to be used as <code>breaks</code> in call to
<code>cut</code>. To avoid NAs, range of the vector must cover the 
range of the input vector.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_f">f</code></td>
<td>
<p>A factor.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_infactor">inFactor</code></td>
<td>
<p>Original factor, to be extended.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_superlevels">superLevels</code></td>
<td>
<p>New levels to be added to the original factor.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_earlierlevels">earlierLevels</code></td>
<td>
<p>Previous levels found for this factor.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_fname">fname</code></td>
<td>
<p>A factor name.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_dfr">dfr</code></td>
<td>
<p>A data frame.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_w">w</code></td>
<td>
<p>A data frame.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_dframe">dframe</code></td>
<td>
<p>A data frame, for which we wish to find the column classes.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_omitlast">omitLast</code></td>
<td>
<p>If TRUE, then generate only k-1 dummies from k factor
levels.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_factorsinfo">factorsInfo</code></td>
<td>
<p>Attribute from output of <code>factorsToDummies</code>.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_factorinfo">factorInfo</code></td>
<td>
<p>Attribute from output of <code>factorToDummies</code>.</p>
</td></tr>
<tr><td><code id="factorsToDummies_+3A_dtaf">dtaf</code></td>
<td>
<p>A data frame.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Many R users prefer to express categorical data as R factors, or often
work with data that is of this type to begin with.  On the other hand,
many regression packages, e.g. <span class="pkg">lars</span>, disallow factors.  These
utilities facilitate conversion from one form to another.
</p>
<p>Here is an overview of the roles of the various functions:
</p>

<ul>
<li> <p><code>factorToDummies</code>: Convert one factor to dummies, yielding a
matrix of dummies corresponding to that factor.
</p>
</li>
<li> <p><code>factorsToDummies</code>: Convert all factors to dummies, yielding
a matrix of dummies, corresponding to all factors in the input data
frame.
</p>
</li>
<li> <p><code>dummiesToFactor</code>: Convert a set of related dummies to a
factor.
</p>
</li>
<li> <p><code>factorTo012etc</code>: Convert a factor to a numeric code,
starting at 0.
</p>
</li>
<li> <p><code>dummiesToInt</code>: Convert a related set of dummies to a numeric code,
starting at 0.
</p>
</li>
<li> <p><code>charsToFactors</code>:  Convert all character columns in a data
frame to factors.
</p>
</li>
<li> <p><code>toAllNumeric</code>: Convert all factors in a data frame to
dummies, yielding a new version of the data frame, including its
original nonfactor columns.
</p>
</li>
<li> <p><code>toSubFactor</code>: Coalesce some levels of a factor, yielding a
new factor.
</p>
</li>
<li> <p><code>toSuperFactor</code>: Add levels to a factor. Typically used in
prediction contexts, in which a factor in a data point to be predicted
does not have all the levels of the same factor in the training set.
</p>
<p>\item <code>xyDataframeToMatrix</code>: Given a data frame to be used in
a training set, with &quot;Y&quot; a factor in the last column, change to all
numeric, with dummies in place of all &quot;X&quot; factors and in place of the
&quot;Y&quot; factor.
</p>
</li></ul>

<p>The optional argument <code>factorsInfo</code> is intended for use in prediction
contexts.  Typically a set of new cases will not have all levels of the
factor in the training set.  Without this argument, only an incomplete
set of dummies would be generated for the set of new cases.
</p>
<p>A key point about changing factors to dummies is that, for later
prediction after fitting a model in our training set, one needs to use
the same transformations.  Say a factor has levels 'abc', 'de' and 'f'
(and <code>omitLast = FALSE</code>).  If we later have a set of say two new
cases to predict, and their values for this factor are 'de' and 'f', we
would generate dummies for them but not for 'abc', incompatible with the
three dummies used in the training set.
</p>
<p>Thus the factor names and levels are saved in attributes, and can be
used as input:  The relations are as follows:
</p>

<ul>
<li> <p><code>factorsToDummies</code> calls <code>factorToDummies</code> on each
factor it finds in its input data frame
</p>
</li>
<li> <p><code>factorToDummies</code> outputs and later inputs <code>factorsInfo</code>
</p>
</li>
<li> <p><code>factorsToDummies</code> outputs and later inputs <code>factorsInfo</code>
</p>
</li></ul>

<p>Other functions:
</p>

<ul>
<li> <p><code>getDFclasses</code>: Return a vector of the classes of the columns
of a data frame.
</p>
</li>
<li> <p><code>discretize</code>: Partition range of a vector into (not
necessarily equal-length) intervals, and construct a factor from the
labels of the intervals that the input elements fall into.
</p>
</li>
<li> <p><code>hasCharacters, hasFactors</code>: Logical scalars, TRUE if the
input data frame has any character or factor columns.
</p>
</li></ul>



<h3>Value</h3>

<p>The function <code>factorToDummies</code> returns a matrix of dummy
variables, while <code>factorsToDummies</code> returns a new version of the
input data frame, in which each factor is replaced by columns of
dummies.  The function <code>factorToDummies</code> is similar, but changes
character vectors to factors.
</p>


<h3>Author(s)</h3>

<p>Norm Matloff
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- factor(c('abc','de','f','de'))
xd &lt;- factorToDummies(x,'x')  
xd 
#      x.abc x.de
# [1,]     1    0
# [2,]     0    1
# [3,]     0    0
# [4,]     0    1
# attr(,"factorInfo")
# attr(,"factorInfo")$fname
# [1] "x"
# 
# attr(,"factorInfo")$omitLast
# [1] TRUE
# 
# attr(,"factorInfo")$fullLvls
# [1] "abc" "de"  "f"  
w &lt;- factor(c('de','abc','abc'))
wd &lt;- factorToDummies(w,'x',factorInfo=attr(xd,'factorInfo')) 
wd 
#      x.abc x.de
# [1,]     0    1
# [2,]     1    0
# [3,]     1    0
# attr(,"factorInfo")
# attr(,"factorInfo")$fname
# [1] "x"
# 
# attr(,"factorInfo")$omitLast
# [1] TRUE
# 
# attr(,"factorInfo")$fullLvls
# [1] "abc" "de"  "f"  

</code></pre>

<hr>
<h2 id='falldetection'>
Fall Detection Data
</h2><span id='topic+falldetection'></span>

<h3>Description</h3>

<p>Detection falls in the elderly via physiological measurements.
Obtained from Kaggle, but is no longer there.
</p>

<hr>
<h2 id='fineTuning+2CknnFineTune'>Grid Search Plus More</h2><span id='topic+fineTuning'></span><span id='topic+knnFineTune'></span><span id='topic+fineTuningPar'></span><span id='topic+plot.tuner'></span>

<h3>Description</h3>

<p>Adds various extra features to grid search for specified tuning 
parameter/hyperparameter combinations:  There is a plot() function, using
parallel coordinates graphs to show trends among the different
combinations; and Bonferroni confidence intervals are computed to avoid
p-hacking.  An experimental smoothing facility is also included.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fineTuning(dataset,pars,regCall,nCombs=NULL,specCombs=NULL,nTst=500,
   nXval=1,up=TRUE,k=NULL,dispOrderSmoothed=FALSE,
   showProgress=TRUE,...)
## S3 method for class 'tuner'
plot(x,...)
knnFineTune(data,yName,k,expandVars,ws,classif=FALSE,seed=9999)
fineTuningPar(cls,dataset,pars,regCall,nCombs=NULL,specCombs=NULL,
   nTst=500,nXval=1,up=TRUE,k=NULL,dispOrderSmoothed=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_...">...</code></td>
<td>
<p>Arguments to be passed on by <code>fineTuning</code> or
<code>plot.tuner</code>.</p>
</td></tr>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_x">x</code></td>
<td>
<p>Output object from <code>fineTuning</code>.</p>
</td></tr>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_cls">cls</code></td>
<td>
<p>A <code>parallel</code> cluster.</p>
</td></tr>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_dataset">dataset</code></td>
<td>
<p>Data frame etc. containing the data to be analyzed.</p>
</td></tr>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_data">data</code></td>
<td>
<p>The data to be analyzed.</p>
</td></tr>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_yname">yName</code></td>
<td>
<p>Quoted name of &quot;Y&quot; in the column names of <code>data</code>.</p>
</td></tr>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_expandvars">expandVars</code></td>
<td>
<p>Indices of columns in <code>data</code> to be weighted in
distance calculations.</p>
</td></tr>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_ws">ws</code></td>
<td>
<p>Weights to be used for <code>expandVars</code>.</p>
</td></tr>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_classif">classif</code></td>
<td>
<p>Set to TRUE for classification problems.</p>
</td></tr>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_seed">seed</code></td>
<td>
<p>Seed for random number generation.</p>
</td></tr>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_pars">pars</code></td>
<td>
<p>R list, showing the desired tuning parameter values.</p>
</td></tr>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_regcall">regCall</code></td>
<td>
<p>Function to be called at each parameter combination,
performing the model fit etc.</p>
</td></tr>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_ncombs">nCombs</code></td>
<td>
<p>Number of parameter combinations to run.  If Null, all
will be run</p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_ntst">nTst</code></td>
<td>
<p>Number of data points to be in the test set.</p>
</td></tr>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_nxval">nXval</code></td>
<td>
<p>Number of folds to be run for a given data partition and
parameter combination.</p>
</td></tr>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_k">k</code></td>
<td>
<p>Nearest-neighbor smoothing parameter.</p>
</td></tr>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_up">up</code></td>
<td>
<p>If TRUE, display results in ascending order of performance
value.</p>
</td></tr>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_dispordersmoothed">dispOrderSmoothed</code></td>
<td>
<p>Display in order of smoothed results.</p>
</td></tr>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_showprogress">showProgress</code></td>
<td>
<p>If TRUE, print each output line as it becomes ready.</p>
</td></tr>
<tr><td><code id="fineTuning+2B2CknnFineTune_+3A_speccombs">specCombs</code></td>
<td>
<p>A data frame in which the user specifies 
#      hyperparameter parameter combinations to evaluate.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The user specifies the values for each tuning parameter in 
<code>pars</code>.  This leads to a number of possible combinations of the
parameters.  In many cases, there are more combinations than the user
wishes to try, so <code>nCombs</code> of them will be chosen at random.
</p>
<p>For each combination, the function will run the analysis specified by
the user in <code>regCall</code>.  The latter must have the call form
</p>
<p><code>ftnName(dtrn,dtst,cmbi</code>
</p>
<p>Again, note that it is <code>fineTuning</code> that calls this function.  It
will provide the training and test sets <code>dtrn</code> and <code>dtst</code>, as
well as <code>cmbi</code> (&quot;combination i&quot;), the particular parameter
combination to be run at this moment.
</p>
<p>Each chosen combination is run in <code>nXval</code> folds.  All specified
combinations are run fully, as opposed to a directional &quot;hill descent&quot;
search that hopes it might eliminate poor combinations early in the process.
</p>
<p>The function <code>knnFineTune</code> is a wrapper for <code>fineTuning</code> for
k-NN problems.
</p>
<p>The function <code>plot.tuner</code> draws a parallel coordinates plot to
visualize the grid. The argument <code>x</code> is the output of
<code>fineTuning</code>.  Arguments to specify in the ellipsis are:
<code>col</code> is the column to be plotted;
<code>disp</code> is the number to display, with <code>0</code>, <code>-m</code> and
<code>+m</code> meaning cases with the <code>m</code> smallest 'smoothed' values, all
cases and the <code>m</code> largest values of 'smoothed', respectively;
<code>jit</code> avoids plotting coincident lines by adding jitter in the
amount <code>jit * range(x) * runif(n,-0.5,0.5)</code>.
</p>


<h3>Value</h3>

<p>Object of class **&rdquo;tuner'**.  Contains the grid results, including
upper bounds of approximate one-sided 95
univariate and Bonferroni-Dunn (adjusted for the
number of parameter combinations).
</p>


<h3>Author(s)</h3>

<p>Norm Matloff
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# mlb data set, predict weight using k-NN, try various values of k

tc &lt;- function(dtrn,dtst,cmbi,...)
{
   knnout &lt;- kNN(dtrn[,-3],dtrn[,3],dtst[,-3],as.integer(cmbi[1]))
   preds &lt;- knnout$regests
   mean(abs(preds - dtst[,3]))
}

data(mlb)
mlb &lt;- mlb[,3:6]
mlb.d &lt;- factorsToDummies(mlb)
fineTuning(mlb.d,list(k=c(5,25)),tc,nTst=100,nXval=2)

</code></pre>

<hr>
<h2 id='knnest+2Cmeany+2Cvary+2Cloclin+2Cpredict.knn+2Cpreprocessx+2Ckmin+2Cparvsnonparplot+2Cnonparvsxplot+2Cl1+2Cl2+2CkNN+2CbestKperPoint'>k-NN Nonparametric Regression and Classification</h2><span id='topic+kNN'></span><span id='topic+knnest'></span><span id='topic+predict.knn'></span><span id='topic+meany'></span><span id='topic+vary'></span><span id='topic+loclin'></span><span id='topic+preprocessx'></span><span id='topic+kmin'></span><span id='topic+parvsnonparplot'></span><span id='topic+nonparvsxplot'></span><span id='topic+nonparvarplot'></span><span id='topic+l2'></span><span id='topic+l1'></span><span id='topic+MAPE'></span><span id='topic+bestKperPoint'></span><span id='topic+kNNallK'></span><span id='topic+kNNxv'></span><span id='topic+knntrn'></span><span id='topic+loclogit'></span><span id='topic+mediany'></span><span id='topic+plotExpVars'></span><span id='topic+exploreExpVars'></span>

<h3>Description</h3>

<p>Full set of tools for k-NN regression and classification, including both
for direct usage and as tools for assessing the fit of parametric
models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kNN(x,y,newx=x,kmax,scaleX=TRUE,PCAcomps=0,expandVars=NULL,expandVals=NULL,
   smoothingFtn=mean,allK=FALSE,leave1out=FALSE, classif=FALSE,
   startAt1=TRUE,saveNhbrs=FALSE,savedNhbrs=NULL)
knnest(y,xdata,k,nearf=meany)
preprocessx(x,kmax,xval=FALSE)
meany(nearIdxs,x,y,predpt) 
mediany(nearIdxs,x,y,predpt) 
vary(nearIdxs,x,y,predpt) 
loclin(nearIdxs,x,y,predpt) 
## S3 method for class 'knn'
predict(object,...)
kmin(y,xdata,lossftn=l2,nk=5,nearf=meany) 
parvsnonparplot(lmout,knnout,cex=1.0) 
nonparvsxplot(knnout,lmout=NULL) 
nonparvarplot(knnout,returnPts=FALSE)
l2(y,muhat)
l1(y,muhat)
MAPE(yhat,y)
bestKperPoint(x,y,maxK,lossFtn="MAPE",classif=FALSE)
kNNallK(x,y,newx=x,kmax,scaleX=TRUE,PCAcomps=0,
   expandVars=NULL,expandVals=NULL,smoothingFtn=mean,
   allK=FALSE,leave1out=FALSE,classif=FALSE,startAt1=TRUE)
kNNxv(x,y,k,scaleX=TRUE,PCAcomps=0,smoothingFtn=mean,
   nSubSam=500)
knnest(y,xdata,k,nearf=meany)
loclogit(nearIdxs,x,y,predpt)
mediany(nearIdxs,x,y,predpt) 
exploreExpVars(xtrn, ytrn, xtst, ytst, k, eVar, maxEVal, lossFtn, 
    eValIncr = 0.05, classif = FALSE, leave1out = FALSE) 
plotExpVars(xtrn,ytrn,xtst,ytst,k,eVars,maxEVal,lossFtn,
   ylim,eValIncr=0.05,classif=FALSE,leave1out=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_nearf">nearf</code></td>
<td>
<p>Function to be applied to a neighborhood.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_ylim">ylim</code></td>
<td>
<p>Range of Y values for plot.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_lossftn">lossFtn</code></td>
<td>
<p>Loss function for plot.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_evar">eVar</code></td>
<td>
<p>Variable to be expanded.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_evars">eVars</code></td>
<td>
<p>Variables to be expanded.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_maxeval">maxEVal</code></td>
<td>
<p>Maximum expansion value.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_evalincr">eValIncr</code></td>
<td>
<p>Increment in range of expansion value.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_xtrn">xtrn</code></td>
<td>
<p>Training set for X.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_ytrn">ytrn</code></td>
<td>
<p>Training set for Y.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_xtst">xtst</code></td>
<td>
<p>Test set for X.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_ytst">ytst</code></td>
<td>
<p>Test set for Y.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_nearidxs">nearIdxs</code></td>
<td>
<p>Indices of the neighbors.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_nsubsam">nSubSam</code></td>
<td>
<p>Number of folds.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_x">x</code></td>
<td>
<p>&quot;X&quot; data, predictors, one row per data point, in the training
set.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_y">y</code></td>
<td>
<p>Response variable data in the training set. Vector or matrix,
the latter case for vector-valued response, e.g. multiclass
classification.  In that case, can be a vector, either (0,1,2,...,)
or (1,2,3,...), which automatically is converted into a matrix of
dummies.</p>
</td></tr>  
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_newx">newx</code></td>
<td>
<p>New data points to be predicted.  If NULL in <code>kNN</code>,
compute regression functions estimates on <code>x</code> and save for
future prediction with <code>predict.kNN</code></p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_scalex">scaleX</code></td>
<td>
<p>If TRUE, call <code>scale</code> on <code>x</code> and <code>newx</code></p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_pcacomps">PCAcomps</code></td>
<td>
<p>If positive, transform <code>x</code> and <code>newx</code> by
PCA, using the top <code>PCAcomps</code> principal components.  Disabled.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_expandvars">expandVars</code></td>
<td>
<p>Indices of columns in <code>x</code> to expand.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_expandvals">expandVals</code></td>
<td>
<p>The corresponding expansion values.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_smoothingftn">smoothingFtn</code></td>
<td>
<p>Function to apply to the &quot;Y&quot; values in the 
set of nearest neighbors.  Built-in choices are <code>meany</code>,
<code>mediany</code>, <code>vary</code> and <code>loclin</code>.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_allk">allK</code></td>
<td>
<p>If TRUE, find regression estimates for all <code>k</code>
through <code>kmax</code>.  Currently disabled.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_leave1out">leave1out</code></td>
<td>
<p>If TRUE, omit the 1-nearest neighbor from analysis</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_classif">classif</code></td>
<td>
<p>If TRUE, compute the predicted class labels, not just
the regression function values</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_startat1">startAt1</code></td>
<td>
<p>If TRUE, class labels start at 1, else 0.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbors</p>
</td></tr> 
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_savenhbrs">saveNhbrs</code></td>
<td>
<p>If TRUE, place output of <code>FNN::get.knnx</code> 
into <code>nhbrs</code> of component in return value</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_savednhbrs">savedNhbrs</code></td>
<td>
<p>If non-NULL, this is the <code>nhbrs</code> component 
in the return value of a previous call; <code>newx</code> must be the
same in both calls</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_...">...</code></td>
<td>
<p>Needed for consistency with generic.  See Details below for
'arguments.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_xdata">xdata</code></td>
<td>
<p>X and associated neighbor indices. Output of
<code>preprocessx</code>.</p>
</td></tr> 
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_object">object</code></td>
<td>
<p>Output of <code>knnest</code>.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_predpt">predpt</code></td>
<td>
<p>One point on which to predict, as a vector.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_kmax">kmax</code></td>
<td>
<p>Maximal number of nearest neighbors to find.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_maxk">maxK</code></td>
<td>
<p>Maximal number of nearest neighbors to find.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_xval">xval</code></td>
<td>
<p>Cross-validation flag. If TRUE, then the set of nearest 
neighbors of a point will not include the point itself.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_lossftn">lossftn</code></td>
<td>
<p>Loss function to be used in cross-validation
determination of &quot;best&quot; <code>k</code>.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_nk">nk</code></td>
<td>
<p>Number of values of <code>k</code> to try in cross-validation.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_lmout">lmout</code></td>
<td>
<p>Output of <code>lm</code>.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_knnout">knnout</code></td>
<td>
<p>Output of <code>knnest</code>.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_cex">cex</code></td>
<td>
<p>R parameter to control dot size in plot.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_muhat">muhat</code></td>
<td>
<p>Vector of estimated regression function values.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_yhat">yhat</code></td>
<td>
<p>Vector of estimated regression function values.</p>
</td></tr>
<tr><td><code id="knnest+2B2Cmeany+2B2Cvary+2B2Cloclin+2B2Cpredict.knn+2B2Cpreprocessx+2B2Ckmin+2B2Cparvsnonparplot+2B2Cnonparvsxplot+2B2Cl1+2B2Cl2+2B2CkNN+2B2CbestKperPoint_+3A_returnpts">returnPts</code></td>
<td>
<p>If TRUE, return matrix of plotted points.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>kNN</code> function is the main tool here; <code>knnest</code> is being
deprecated.  (Note too <code>qeKNN</code>, a wrapper for <code>kNN</code>; more
on this below.)  Here are the capabilities:
</p>
<p>In its most basic form, the function will input training data and
output predictions for new cases <code>newx</code>.  By default this is
done for a single value of the number <code>k</code> of nearest neighbors,
but by setting <code>allK</code> to TRUE, the user can request that it be
done for all <code>k</code> through the specified maximum.
</p>
<p>In the second form, <code>newx</code> is set to NULL in the call to
<code>kNN</code>.  No predictions are made; instead, the regression function 
is estimated on all data points in <code>x</code>, which are saved in the return 
value.  Future new cases can then be predicted from this saved object, 
via <code>predict.kNN</code> (called via the generic <code>predict</code>).  
The call form is <code>predict(knnout,newx,newxK</code>, with a 
default value of 1 for <code>newxK</code>.  
</p>
<p>In this second form, the closest <code>k</code> points to the <code>newx</code> in
<code>x</code> are determined as usual, but instead of averaging their Y
values, the average is taken over the fitted regression estimates at
those points.  In this manner, there is almost no computational cost
in the prediction stage.  
</p>
<p>The second form is intended more for production use, so that neighbor
distances need not be repeatedly recomputed.
</p>
<p>Nearest-neighbor computation can be time-consuming.  If more than one
value of <code>k</code> is anticipated, for the same <code>x</code>, <code>y</code> and
<code>newx</code>, first run with the largest anticipated value of
<code>k</code>, with <code>saveNhbrs</code> set to TRUE.  Then for other values
of <code>k</code>, set <code>savedNhbrs</code> to the <code>nhbrs</code> component in
the return value of the first call.
</p>
<p>In addition, a novel feature allows the user to weight some
predictors more than others.  This is done by scaling the given
predictor up or down, according to a specified value.  Normally, this
should be done with <code>scaleX = TRUE</code>, which applies
<code>scale()</code> to the data.  In other words, first we create a &quot;level
playing field&quot; in which all predictors have standard deviation 1.0,
then scale some of them up or down.
</p>
<p>Alternatives are provided to calculating the mean Y in the given
neighborhood, such as the median and the variance, the latter of
possible use in dealing with heterogeneity in linear models.
</p>
<p>Another choice of note is to allow local-linear smoothing, by
setting <code>smoothingFtn</code> to <code>loclin</code>.  Here the value of the
regression function at a point is predicted from a linear fit to the
point's neighbors.  This may be especially helpful to counteract bias
near the edges of the data.  As in any regression fit, the number of
predictors should be considerably less than the number of neighbors.
</p>
<p>Custom functions for smoothing can easily be written, say following
the pattern of <code>loclin</code>.
</p>
<p>The main alternative to <code>kNN</code> is <code>qeKNN</code> in the qe* (&quot;quick
and easy&quot;) series. It is more convenient, e.g. allowing factor
inputs, but less flexible.
</p>
<p>The functions <code>ovaknntrn</code> and <code>ovaknnpred</code> are multiclass
wrappers for <code>knnest</code> and <code>knnpred</code>, thus also deprecated.
Here <code>y</code> is coded 0,1,...,<code>m</code>-1 for the <code>m</code> classes.
</p>
<p>The tools here can be useful for fit assessment of parametric models.
The <code>parvsnonparplot</code> function plots fitted values of
parameteric model vs. kNN fitted, <code>nonparvsxplot</code> k-NN fitted
values against each predictor, one by one.
</p>
<p>The functions <code>l2</code> and <code>l1</code> are used to define L2 and L1
loss.
</p>




<h3>Author(s)</h3>

<p>Norm Matloff
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- rbind(c(1,0),c(2,5),c(0,5),c(3,3),c(6,3))
y &lt;- c(8,3,10,11,4)
newx &lt;- c(0,0)

kNN(x,y,newx,2,scaleX=FALSE)
# $whichClosest
#      [,1] [,2]
# [1,]    1    4
# $regests
# [1] 9.5

kNN(x,y,newx,3,scaleX=FALSE,smoothingFtn=loclin)$regests
# 7.307692

knnout &lt;- kNN(x,y,newx,2,scaleX=FALSE)
knnout
# $whichClosest
#      [,1] [,2]
# [1,]    1    4
# ...

## Not run: 
data(mlb) 
mlb &lt;- mlb[,c(4,6,5)]  # height, age, weight
# fit, then predict 75", age 21, and 72", age 32
knnout &lt;- kNN(mlb[,1:2],mlb[,3],rbind(c(75,21),c(72,32)),25) 
knnout$regests
# [1] 202.72 195.72

# fit now, predict later
knnout &lt;- kNN(mlb[,1:2],mlb[,3],NULL,25) 
predict(knnout,c(70,28)) 
# [1] 186.48

data(peDumms) 
names(peDumms) 
ped &lt;- peDumms[,c(1,20,22:27,29,31,32)] 
names(ped) 

# fit, and predict income of a 35-year-old man, MS degree, occupation 101,
# worked 50 weeks, using 25 nearest neighbors
kNN(ped[,-10],ped[,10],c(35,1,0,0,1,0,0,0,1,50),25) $regests
# [1] 67540

# fit, and predict occupation 101 for a 35-year-old man, MS degree, 
# wage $55K, worked 50 weeks, using 25 nearest neighbors
z &lt;- kNN(ped[,-c(4:8)],ped[,4],c(35,1,0,1,55,50),25,classif=TRUE)
z$regests
# [1] 0.16  16
z$ypreds
# [1] 0  class 0, i.e. not occupation 101; round(0.24) = 0, 
# computed by user request, classif = TRUE

# the y argument must be either a vector (2-class setting) or a matrix
# (multiclass setting)
occs &lt;- as.matrix(ped[, 4:8])
z &lt;- kNN(ped[,-c(4:8)],occs,c(35,1,0,1,72000,50),25,classif=TRUE)
z$ypreds
# [1] 3   occupation 3, i.e. 102, is predicted

# predict occupation in general; let's bring occ.141 back in (was
# excluded as a predictor due to redundancy)
names(peDumms)
#  [1] "age"     "cit.1"   "cit.2"   "cit.3"   "cit.4"   "cit.5"   "educ.1" 
#  [8] "educ.2"  "educ.3"  "educ.4"  "educ.5"  "educ.6"  "educ.7"  "educ.8" 
# [15] "educ.9"  "educ.10" "educ.11" "educ.12" "educ.13" "educ.14" "educ.15"
# [22] "educ.16" "occ.100" "occ.101" "occ.102" "occ.106" "occ.140" "occ.141"
# [29] "sex.1"   "sex.2"   "wageinc" "wkswrkd" "yrentry"
occs &lt;- as.matrix(peDumms[,23:28])  
z &lt;- kNN(ped[,-c(4:8)],occs,c(35,1,0,1,72000,50),25,classif=TRUE)
z$ypreds
# [1] 3   prediction is occ.102

# try weight age 0.5, wkswrked 1.5; use leave1out to avoid overfit
knnout &lt;- kNN(ped[,-10],ped[,10],ped[,-10],25,leave1out=TRUE)
mean(abs(knnout$regests - ped[,10]))
# [1] 25341.6

# use of the weighted distance feature; deweight age by a factor of 0.5,
# put increased weight on weeks worked, factor of 1.5
knnout &lt;- kNN(ped[,-10],ped[,10],ped[,-10],25,
   expandVars=c(1,10),expandVals=c(0.5,1.5),leave1out=TRUE)
mean(abs(knnout$regests - ped[,10]))
# [1] 25196.61




## End(Not run)

</code></pre>

<hr>
<h2 id='krsFit'>Tools for Neural Networks</h2><span id='topic+krsFit'></span><span id='topic+krsFitImg'></span><span id='topic+diagNeural'></span><span id='topic+predict.krsFit'></span>

<h3>Description</h3>

<p>Tools to complement existing neural networks software, notably 
a more &quot;R-like&quot; wrapper to fitting data with R's <span class="pkg">keras</span> package.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>krsFit(x,y,hidden,acts=rep("relu",length(hidden)),learnRate=0.001,
   conv=NULL,xShape=NULL,classif=TRUE,nClass=NULL,nEpoch=30,
   scaleX=TRUE,scaleY=TRUE)
krsFitImg(x,y,hidden=c(100,100),acts=rep("relu",length(hidden)),
    nClass,nEpoch=30) 
## S3 method for class 'krsFit'
predict(object,...)
diagNeural(krsFitOut)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="krsFit_+3A_object">object</code></td>
<td>
<p>An object of class 'krsFit'.</p>
</td></tr>
<tr><td><code id="krsFit_+3A_...">...</code></td>
<td>
<p>Data points to be predicted, 'newx'.</p>
</td></tr>
<tr><td><code id="krsFit_+3A_x">x</code></td>
<td>
<p>X data, predictors, one row per data point, in the training
set.  Must be a matrix.</p>
</td></tr>
<tr><td><code id="krsFit_+3A_y">y</code></td>
<td>
<p>Numeric vector of Y values.  In classification case
must be integers, not an R factor, and take on the values 0,1,2,...,
<code>nClass</code>-1</p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="krsFit_+3A_hidden">hidden</code></td>
<td>
<p>Vector of number of units per 
hidden layer, or the rate for a dropout layer.</p>
</td></tr> 
<tr><td><code id="krsFit_+3A_acts">acts</code></td>
<td>
<p>Vector of names of the activation functions, one per
hidden layer.  Choices inclde 'relu', 'sigmoid', 'tanh', 'softmax',
'elu', 'selu'.</p>
</td></tr>
<tr><td><code id="krsFit_+3A_learnrate">learnRate</code></td>
<td>
<p>Learning rate.</p>
</td></tr> 
<tr><td><code id="krsFit_+3A_conv">conv</code></td>
<td>
<p>R list specifying the convolutional layers, if any.</p>
</td></tr>
<tr><td><code id="krsFit_+3A_xshape">xShape</code></td>
<td>
<p>Vector giving the number of rows and columns, and in the
convolutional case with multiple channels, the number of channels.</p>
</td></tr>
<tr><td><code id="krsFit_+3A_classif">classif</code></td>
<td>
<p>If TRUE, indicates a classification problem.</p>
</td></tr>
<tr><td><code id="krsFit_+3A_nclass">nClass</code></td>
<td>
<p>Number of classes.</p>
</td></tr>
<tr><td><code id="krsFit_+3A_nepoch">nEpoch</code></td>
<td>
<p>Number of epochs.</p>
</td></tr>
<tr><td><code id="krsFit_+3A_krsfitout">krsFitOut</code></td>
<td>
<p>An object returned by <code>krstFit</code>.</p>
</td></tr>
<tr><td><code id="krsFit_+3A_scalex">scaleX</code></td>
<td>
<p>If TRUE, scale X columns.</p>
</td></tr>
<tr><td><code id="krsFit_+3A_scaley">scaleY</code></td>
<td>
<p>If TRUE, scale Y columns.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>krstFit</code> function is a wrapper for the entire pipeline
in fitting the R <span class="pkg">keras</span> package to a dataset:  Defining the model,
compiling, stating the inputs and so on.  As a result, the wrapper
allows the user to skip those details (or not need to even know them),
and define the model in a manner more familiar to R users.
</p>
<p>The paired <code>predict.krsFit</code> takes as its first argument the output
of <code>krstFit</code>, and <code>newx</code>, the points to be predicted.
</p>


<h3>Author(s)</h3>

<p>Norm Matloff
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
library(keras)
data(peDumms) 
ped &lt;- peDumms[,c(1,20,22:27,29,32,31)]
# predict wage income
x &lt;- ped[,-11] 
y &lt;- ped[,11] 
z &lt;- krsFit(x,y,c(50,50,50),classif=FALSE,nEpoch=25) 
preds &lt;- predict(z,x) 
mean(abs(preds-y))  # something like 25000

x &lt;- ped[,-(4:8)] 
y &lt;- ped[,4:8] 
y &lt;- dummiesToInt(y,FALSE) - 1
z &lt;- krsFit(x,y,c(50,50,0.20,50),classif=TRUE,nEpoch=175,nClass=6) 
preds &lt;- predict(z,x)
mean(preds == y)   # something like 0.39

# obtain MNIST training and test sets; the following then uses the
# example network of 

# https://databricks-prod-cloudfront.cloud.databricks.com/
# public/4027ec902e239c93eaaa8714f173bcfc/2961012104553482/
# 4462572393058129/1806228006848429/latest.html

# converted to use the krsFit wrapper

x &lt;- mntrn[,-785] / 255 
y &lt;- mntrn[,785]
xShape &lt;- c(28,28)

# define convolutional layers
conv1 &lt;- list(type='conv2d',filters=32,kern=3)
conv2 &lt;- list(type='pool',kern=2)
conv3 &lt;- list(type='conv2d',filters=64,kern=3) 
conv4 &lt;- list(type='pool',kern=2)
conv5 &lt;- list(type='drop',drop=0.5)

# call wrapper, 1 dense hidden layer of 128 units, then dropout layer
# with proportion 0.5
z &lt;- krsFit(x,y,conv=list(conv1,conv2,conv3,conv4,conv5),c(128,0.5),
   classif=TRUE,nClass=10,nEpoch=10,xShape=c(28,28),scaleX=FALSE,scaleY=FALSE)

# try on test set
preds &lt;- predict(z,mntst[,-785]/255)
mean(preds == mntst[,785])  # 0.98 in my sample run


## End(Not run)

</code></pre>

<hr>
<h2 id='lmac+2CmakeNA+2Ccoef.lmac+2Cvcov.lmac+2Cpcac+2Cloglinac+2Ctbltofakedf'>Available Cases Method for Missing Data</h2><span id='topic+lmac'></span><span id='topic+pcac'></span><span id='topic+coef.lmac'></span><span id='topic+vcov.lmac'></span><span id='topic+loglinac'></span><span id='topic+tbltofakedf'></span><span id='topic+makeNA'></span><span id='topic+NAsTo0s'></span><span id='topic+ZerosToNAs'></span>

<h3>Description</h3>

<p>Various estimators that handle missing data via the Available Cases Method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lmac(xy,nboot=0) 
makeNA(m,probna)
NAsTo0s(x)
ZerosToNAs(x,replaceVal=0)
## S3 method for class 'lmac'
coef(object,...)
## S3 method for class 'lmac'
vcov(object,...)
pcac(indata,scale=FALSE) 
loglinac(x,margin) 
tbltofakedf(tbl)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lmac+2B2CmakeNA+2B2Ccoef.lmac+2B2Cvcov.lmac+2B2Cpcac+2B2Cloglinac+2B2Ctbltofakedf_+3A_replaceval">replaceVal</code></td>
<td>
<p>Value to be replaced by NA.</p>
</td></tr>
<tr><td><code id="lmac+2B2CmakeNA+2B2Ccoef.lmac+2B2Cvcov.lmac+2B2Cpcac+2B2Cloglinac+2B2Ctbltofakedf_+3A_xy">xy</code></td>
<td>
<p>Matrix or data frame, X values in the first columns, Y
in the last column.</p>
</td></tr>
<tr><td><code id="lmac+2B2CmakeNA+2B2Ccoef.lmac+2B2Cvcov.lmac+2B2Cpcac+2B2Cloglinac+2B2Ctbltofakedf_+3A_indata">indata</code></td>
<td>
<p>Matrix or data frame.</p>
</td></tr>
<tr><td><code id="lmac+2B2CmakeNA+2B2Ccoef.lmac+2B2Cvcov.lmac+2B2Cpcac+2B2Cloglinac+2B2Ctbltofakedf_+3A_x">x</code></td>
<td>
<p>Matrix or data frame, one column per variable.</p>
</td></tr>
<tr><td><code id="lmac+2B2CmakeNA+2B2Ccoef.lmac+2B2Cvcov.lmac+2B2Cpcac+2B2Cloglinac+2B2Ctbltofakedf_+3A_nboot">nboot</code></td>
<td>
<p>If positive, number of bootstrap samples to take.</p>
</td></tr>
<tr><td><code id="lmac+2B2CmakeNA+2B2Ccoef.lmac+2B2Cvcov.lmac+2B2Cpcac+2B2Cloglinac+2B2Ctbltofakedf_+3A_probna">probna</code></td>
<td>
<p>Probability that an element will be NA.</p>
</td></tr>
<tr><td><code id="lmac+2B2CmakeNA+2B2Ccoef.lmac+2B2Cvcov.lmac+2B2Cpcac+2B2Cloglinac+2B2Ctbltofakedf_+3A_scale">scale</code></td>
<td>
<p>If TRUE, call <code>cor</code> instead of <code>cov</code>.</p>
</td></tr>
<tr><td><code id="lmac+2B2CmakeNA+2B2Ccoef.lmac+2B2Cvcov.lmac+2B2Cpcac+2B2Cloglinac+2B2Ctbltofakedf_+3A_tbl">tbl</code></td>
<td>
<p>An R table.</p>
</td></tr>
<tr><td><code id="lmac+2B2CmakeNA+2B2Ccoef.lmac+2B2Cvcov.lmac+2B2Cpcac+2B2Cloglinac+2B2Ctbltofakedf_+3A_m">m</code></td>
<td>
<p>Number of synthetic NAs to insert.</p>
</td></tr>
<tr><td><code id="lmac+2B2CmakeNA+2B2Ccoef.lmac+2B2Cvcov.lmac+2B2Cpcac+2B2Cloglinac+2B2Ctbltofakedf_+3A_object">object</code></td>
<td>
<p>Output from <code>lmac</code>.</p>
</td></tr>
<tr><td><code id="lmac+2B2CmakeNA+2B2Ccoef.lmac+2B2Cvcov.lmac+2B2Cpcac+2B2Cloglinac+2B2Ctbltofakedf_+3A_...">...</code></td>
<td>
<p>Needed for consistency with generic function.  Not used.</p>
</td></tr>
<tr><td><code id="lmac+2B2CmakeNA+2B2Ccoef.lmac+2B2Cvcov.lmac+2B2Cpcac+2B2Cloglinac+2B2Ctbltofakedf_+3A_margin">margin</code></td>
<td>
<p>A list of vectors specifying the model, as in
<code>loglin</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Available Cases (AC) approach applies to statistical methods that
depend only on products of k of the variables, so that cases having
non-NA values for those k variables can be used, as opposed to using
only cases that are fully intact in all variables, the Complete Cases
(CC) approach.  In the case of linear regression, for instance, the
estimated coefficients depend only on covariances between the
variables (both predictors and response).  This approach assumes thst
the cases with missing values have the same distribution as the
intact cases.
</p>
<p>The <code>lmac</code> function forms OLS estimates as with <code>lm</code>, but
applying AC, in contrast to <code>lm</code>, which uses the CC method.
</p>
<p>The <code>pcac</code> function is an AC substitute for <code>prcomp</code>. The
data is centered, corresponding to a fixed value of <code>center =
   TRUE</code> in <code>prcomp</code>.  It is also scaled if <code>scale</code> is TRUE,
corresponding <code>scale = TRUE</code> in <code>prcomp</code>. Due to AC,
there is a small chance of negative eigenvalues, in which case
<code>stop</code> will be called.
</p>
<p>The <code>loglinac</code> function is an AC substitute for <code>loglin</code>.
The latter takes tables as input, but <code>loglinac</code> takes the raw
data. If you have just the table, use <code>tbltofakedf</code> to
regenerate a usable data frame.
</p>
<p>The <code>makeNA</code> function is used to insert random NA values into
data, for testing purposes.
</p>


<h3>Value</h3>

<p>For <code>lmac</code>, an object of class <code>lmac</code>, with components
</p>

<ul>
<li><p>coefficients, as with <code>lm</code>; 
accessible directly or by calling <code>coef</code>, as with <code>lm</code>
</p>
</li>
<li><p>fitted.values, as with <code>lm</code>
</p>
</li>
<li><p>residuals, as with <code>lm</code>
</p>
</li>
<li><p>r2, (unadjusted) R-squared
</p>
</li>
<li><p>cov, for <code>nboot &gt; 0</code> the estimated covariance matrix
of the vector of estimated regression coefficients; accessible
directly or by calling <code>vcov</code>, as with <code>lm</code> 
</p>
</li></ul>

<p>For <code>pcac</code>, an R list, with components
</p>

<ul>
<li><p>sdev, as with <code>prcomp</code>
</p>
</li>
<li><p>rotation, as with <code>prcomp</code>
</p>
</li></ul>

<p>For <code>loglinac</code>, an R list, with components
</p>

<ul>
<li><p>param, estimated coefficients, as in <code>loglin</code>
</p>
</li>
<li><p>fit, estimated expected call counts, as in <code>loglin</code>
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Norm Matloff
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 25000
w &lt;- matrix(rnorm(2*n),ncol=2)  # x and epsilon
x &lt;- w[,1]
y &lt;- x + w[,2]
# insert some missing values
nmiss &lt;- round(0.1*n)
x[sample(1:n,nmiss)] &lt;- NA
nmiss &lt;- round(0.2*n)
y[sample(1:n,nmiss)] &lt;- NA
acout &lt;- lmac(cbind(x,y))
coef(acout)  # should be near pop. values 0 and 1
</code></pre>

<hr>
<h2 id='ltrfreqs'>
Letter Frequencies
</h2><span id='topic+ltrfreqs'></span>

<h3>Description</h3>

<p>This is data consists of capital letter frequencies obtained at
http://www.math.cornell.edu/~mec/2003-2004/cryptography/subs/frequencies.h
tml
</p>

<hr>
<h2 id='misc'>Utilities</h2><span id='topic+replicMeans'></span><span id='topic+stdErrPred'></span><span id='topic+pythonBlankSplit'></span><span id='topic+stopBrowser'></span><span id='topic+doPCA'></span><span id='topic+PCAwithFactors'></span><span id='topic+ulist'></span><span id='topic+prToFile'></span><span id='topic+partTrnTst'></span><span id='topic+findOverallLoss'></span><span id='topic+getNamedArgs'></span><span id='topic+multCols'></span><span id='topic+probIncorrectClass'></span><span id='topic+propMisclass'></span>

<h3>Description</h3>

<p>Various helper functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>replicMeans(nrep,toReplic,timing=FALSE)
stdErrPred(regObj,xnew)
pythonBlankSplit(s)
stopBrowser(msg = stop("msg not supplied"))
doPCA(x,pcaProp)
PCAwithFactors(x, nComps = ncol(x))
ulist(lst)
prToFile(filename)
partTrnTst(fullData,nTest=min(1000,round(0.2*nrow(fullData))))
findOverallLoss(regests,y,lossFtn = MAPE) 
getNamedArgs(argVec)
multCols(x,cols,vals)
probIncorrectClass(yhat, y, startAt1 = TRUE)
propMisclass(y,yhat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="misc_+3A_regests">regests</code></td>
<td>
<p>Fitted regression estimates, training set.</p>
</td></tr>
<tr><td><code id="misc_+3A_y">y</code></td>
<td>
<p>Y values, training set.</p>
</td></tr>
<tr><td><code id="misc_+3A_yhat">yhat</code></td>
<td>
<p>Predicted Y values</p>
</td></tr>
<tr><td><code id="misc_+3A_startat1">startAt1</code></td>
<td>
<p>TRUE if indexing starts at 1, FALSE if starting at 0.</p>
</td></tr>
<tr><td><code id="misc_+3A_lossftn">lossFtn</code></td>
<td>
<p>Loss functin.</p>
</td></tr>
<tr><td><code id="misc_+3A_fulldata">fullData</code></td>
<td>
<p>A data frame or matrix.</p>
</td></tr>
<tr><td><code id="misc_+3A_ntest">nTest</code></td>
<td>
<p>Number of rows for the test set.</p>
</td></tr>
<tr><td><code id="misc_+3A_filename">filename</code></td>
<td>
<p>Name of output file.</p>
</td></tr>
<tr><td><code id="misc_+3A_lst">lst</code></td>
<td>
<p>An R list.</p>
</td></tr>
<tr><td><code id="misc_+3A_x">x</code></td>
<td>
<p>Matrix or data frame.</p>
</td></tr>
<tr><td><code id="misc_+3A_pcaprop">pcaProp</code></td>
<td>
<p>Fraction in [0,1], specifying number of PCA components
to compute, in terms of fraction of total variance.</p>
</td></tr>
<tr><td><code id="misc_+3A_ncomps">nComps</code></td>
<td>
<p>Number of PCA components.</p>
</td></tr>
<tr><td><code id="misc_+3A_regobj">regObj</code></td>
<td>
<p>An object of class <code>'lm'</code> or similar, for which 
there is a <code>vcov</code> generic function.</p>
</td></tr>
<tr><td><code id="misc_+3A_xnew">xnew</code></td>
<td>
<p>New X value to be predicted.</p>
</td></tr>
<tr><td><code id="misc_+3A_nrep">nrep</code></td>
<td>
<p>Number of replications.</p>
</td></tr>
<tr><td><code id="misc_+3A_s">s</code></td>
<td>
<p>A character string.</p>
</td></tr>
<tr><td><code id="misc_+3A_toreplic">toReplic</code></td>
<td>
<p>Function call(s), as a quoted string, separated by
semicolons if more than one call.</p>
</td></tr>
<tr><td><code id="misc_+3A_timing">timing</code></td>
<td>
<p>If TRUE, find average elapsed time over the replicates.</p>
</td></tr>
<tr><td><code id="misc_+3A_msg">msg</code></td>
<td>
<p>Character string, error message for existing debug browser.</p>
</td></tr>
<tr><td><code id="misc_+3A_argvec">argVec</code></td>
<td>
<p>R list or vector with named elements.</p>
</td></tr>
<tr><td><code id="misc_+3A_cols">cols</code></td>
<td>
<p>A set of column numbers.</p>
</td></tr>
<tr><td><code id="misc_+3A_vals">vals</code></td>
<td>
<p>A set of positive expansion numbers.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>PCAwithFactors</code> is a wrapper for
<code>stats::prcomp</code>, to be used on data frames that contain at least on
R factor.
</p>


<h3>Value</h3>

<p>The function <code>PCAwithFactors</code> returns an object of class
'PCAwithFactors'. with components <code>pcout</code>, the object returned by
the wrapped call to <code>prcomp</code>; <code>factorsInfo</code>, factor conversion
information to be used with <code>predict</code>; and <code>preds</code>, the PCA
version of <code>x</code>.
</p>
<p>The function <code>getNamedArgs</code> will assign in the caller's space
variables with the names and values in <code>argVec</code>.
</p>


<h3>Author(s)</h3>

<p>Norm Matloff
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
w &lt;- list(a=3,b=8)
getNamedArgs(w)  
a
b
u &lt;- c(5,12,13)
names(u) &lt;- c('x','y','z')
getNamedArgs(u)
x
y
z

</code></pre>

<hr>
<h2 id='mlb'>
Major Leage Baseball player data set.
</h2><span id='topic+mlb'></span>

<h3>Description</h3>

<p>Heights, weights, ages etc. of major league baseball players.  A new
variable has been added, consolidating positions into Infielders,
Outfielders, Catchers and Pitchers.
</p>
<p>Included here with the permission of the UCLA Statistics Department.
</p>

<hr>
<h2 id='mlens'>
MovieLens User Summary Data
</h2><span id='topic+mlens'></span>

<h3>Description</h3>

<p>The MovieLens dataset, <a href="https://grouplens.org/">https://grouplens.org/</a>,
is a standard example in the recommender systems literature.  Here we
give demographic data for each user, plus the mean rating and number of
ratings.  One may explore, for instance, the relation between ratings
and age.
</p>

<hr>
<h2 id='mm'>Method of Moments, Including Possible Regression Terms</h2><span id='topic+mm'></span>

<h3>Description</h3>

<p>Method of Moments computation for almost any statistical problem that
has derivatives with respect to theta.  Capable of handling models that
include parametric regression terms, but not need be a regression
problem.  (This is not <em>Generalized</em> Method of Moments; see the
package <span class="pkg">gmm</span> for the latter.)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mm(m,g,x,init=rep(0.5,length(m)),eps=0.0001,maxiters=1000) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mm_+3A_m">m</code></td>
<td>
<p>Vector of sample moments, &quot;left-hand sides&quot; of moment
equations.</p>
</td></tr>
<tr><td><code id="mm_+3A_g">g</code></td>
<td>
<p>Function of parameter estimates, forming the &quot;right-hand
sides.&quot; This is a multivariate-valued function, of dimensionality
equal to that of <code>m</code></p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="mm_+3A_init">init</code></td>
<td>
<p>Vector of initial guesses for parameter estimates.  If 
components are named, these will be used as labels in the output.</p>
</td></tr>
<tr><td><code id="mm_+3A_eps">eps</code></td>
<td>
<p>Convergence criterion.</p>
</td></tr> 
<tr><td><code id="mm_+3A_maxiters">maxiters</code></td>
<td>
<p>Maximum number of iterations.</p>
</td></tr>
<tr><td><code id="mm_+3A_x">x</code></td>
<td>
<p>Input data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Standard Newton-Raphson methods are used to solve for the parameter
estimates, with <code>numericDeriv</code> being used to find the
approximate derivatives.
</p>


<h3>Value</h3>

<p>R list consisting of components <code>tht</code>, the vector of parameter
estimates, and <code>numiters</code>, the number of iterations performed. 
</p>


<h3>Author(s)</h3>

<p>Norm Matloff
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rgamma(1000,2)
m &lt;- c(mean(x),var(x))
g &lt;- function(x,theta) {  # from theoretical properties of gamma distr.
   g1 &lt;-  theta[1] / theta[2]
   g2 &lt;-  theta[1] / theta[2]^2
   c(g1,g2)
}
# should output about 2 and 1
mm(m,g,x)

## Not run: 
library(mfp)
data(bodyfat)
# model as a beta distribution 
g &lt;- function(x,theta) {
   t1 &lt;- theta[1]
   t2 &lt;- theta[2]
   t12 &lt;- t1 + t2
   meanb &lt;- t1 / t12
   m1 &lt;- meanb 
   m2 &lt;- t1*t2 / (t12^2 * (t12+1)) 
   c(m1,m2)
}
x &lt;- bodyfat$brozek/100
m &lt;- c(mean(x),var(x))
# about 4.65 and 19.89
mm(m,g,x)

## End(Not run)

</code></pre>

<hr>
<h2 id='multiclass+20routines'>Classification with More Than 2 Classes</h2><span id='topic+boundaryplot'></span><span id='topic+ovalogtrn'></span><span id='topic+ovaknntrn'></span><span id='topic+ovalogpred'></span><span id='topic+avalogtrn'></span><span id='topic+avalogpred'></span><span id='topic+predict.ovaknn'></span><span id='topic+classadjust'></span><span id='topic+confusion'></span><span id='topic+factorTo012ec'></span><span id='topic+classadjust'></span>

<h3>Description</h3>

<p>Tools for multiclass classification, parametric and nonparametric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>avalogtrn(trnxy,yname)
ovaknntrn(trnxy,yname,k,xval=FALSE)
avalogpred()
classadjust(econdprobs,wrongprob1,trueprob1) 
boundaryplot(y01,x,regests,pairs=combn(ncol(x),2),pchvals=2+y01,cex=0.5,band=0.10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multiclass+2B20routines_+3A_pchvals">pchvals</code></td>
<td>
<p>Point size in base-R graphics.</p>
</td></tr>
<tr><td><code id="multiclass+2B20routines_+3A_trnxy">trnxy</code></td>
<td>
<p>Data matrix, Y last.</p>
</td></tr>
<tr><td><code id="multiclass+2B20routines_+3A_xval">xval</code></td>
<td>
<p>If TRUE, use leaving-one-out method.</p>
</td></tr>
<tr><td><code id="multiclass+2B20routines_+3A_y01">y01</code></td>
<td>
<p>Y vector (1s and 0s).</p>
</td></tr>
<tr><td><code id="multiclass+2B20routines_+3A_regests">regests</code></td>
<td>
<p>Estimated regression function values.</p>
</td></tr>
<tr><td><code id="multiclass+2B20routines_+3A_x">x</code></td>
<td>
<p>X data frame or matrix.</p>
</td></tr>
<tr><td><code id="multiclass+2B20routines_+3A_pairs">pairs</code></td>
<td>
<p>Two-row matrix, column i of which is a pair of predictor
variables to graph.</p>
</td></tr>
<tr><td><code id="multiclass+2B20routines_+3A_cex">cex</code></td>
<td>
<p>Symbol size for plotting.</p>
</td></tr>
<tr><td><code id="multiclass+2B20routines_+3A_band">band</code></td>
<td>
<p>If <code>band</code> is non-NULL, only points within <code>band</code>, 
say 0.1, of est. P(Y = 1) are displayed, for a contour-like effect.</p>
</td></tr>
<tr><td><code id="multiclass+2B20routines_+3A_yname">yname</code></td>
<td>
<p>Name of the Y column.</p>
</td></tr>
<tr><td><code id="multiclass+2B20routines_+3A_k">k</code></td>
<td>
<p>Number of nearest neighbors.</p>
</td></tr> 
<tr><td><code id="multiclass+2B20routines_+3A_econdprobs">econdprobs</code></td>
<td>
<p>Estimated conditional class probabilities, given the
predictors.</p>
</td></tr>
<tr><td><code id="multiclass+2B20routines_+3A_wrongprob1">wrongprob1</code></td>
<td>
<p>Incorrect, data-provenanced, unconditional P(Y = 1).</p>
</td></tr>
<tr><td><code id="multiclass+2B20routines_+3A_trueprob1">trueprob1</code></td>
<td>
<p>Correct unconditional P(Y = 1).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions aid classification in the multiclass setting.  
</p>
<p>The function <code>boundaryplot</code> serves as a visualization technique,
for the two-class setting.  It draws the boundary between predicted Y =
1 and predicted Y = 0 data points in 2-dimensional feature space, as
determined by the argument <code>regests</code>.  Used to visually assess
goodness of fit, typically running this function twice, say one for
<code>glm</code> then for <code>kNN</code>.  If there is much discrepancy and the
analyst wishes to still use glm(), he/she may wish to add polynomial
terms.
</p>
<p>The functions not listed above are largely deprecated, e.g. in favor of
<code>qeLogit</code> and the other <code>qe</code>-series functions.
</p>


<h3>Author(s)</h3>

<p>Norm Matloff
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 


data(oliveoils) 
oo &lt;- oliveoils[,-1] 

# toy example
set.seed(9999)
x &lt;- runif(25)
y &lt;- sample(0:2,25,replace=TRUE)
xd &lt;- preprocessx(x,2,xval=FALSE)
kout &lt;- ovaknntrn(y,xd,m=3,k=2)
kout$regest  # row 2:  0.0,0.5,0.5
predict(kout,predpts=matrix(c(0.81,0.55,0.15),ncol=1))  # 0,2,0or2
yd &lt;- factorToDummies(as.factor(y),'y',FALSE)
kNN(x,yd,c(0.81,0.55,0.15),2)  # predicts 0, 1or2, 2

data(peDumms)  # prog/engr data 
ped &lt;- peDumms[,-33] 
ped &lt;- as.matrix(ped)
x &lt;- ped[,-(23:28)]
y &lt;- ped[,23:28]
knnout &lt;- kNN(x,y,x,25,leave1out=TRUE) 
truey &lt;- apply(y,1,which.max) - 1
mean(knnout$ypreds == truey)  # about 0.37
xd &lt;- preprocessx(x,25,xval=TRUE)
kout &lt;- knnest(y,xd,25)
preds &lt;- predict(kout,predpts=x)
hats &lt;- apply(preds,1,which.max) - 1
mean(yhats == truey)  # about 0.37

data(peFactors)
# discard the lower educ-level cases, which are rare
edu &lt;- peFactors$educ 
numedu &lt;- as.numeric(edu) 
idxs &lt;- numedu &gt;= 12 
pef &lt;- peFactors[idxs,]
numedu &lt;- numedu[idxs]
pef$educ &lt;- as.factor(numedu)
pef1 &lt;- pef[,c(1,3,5,7:9)]

# ovalog
ovaout &lt;- ovalogtrn(pef1,"occ")
preds &lt;- predict(ovaout,predpts=pef1[,-3])
mean(preds == factorTo012etc(pef1$occ))  # about 0.39

# avalog

avaout &lt;- avalogtrn(pef1,"occ")  
preds &lt;- predict(avaout,predpts=pef1[,-3]) 
mean(preds == factorTo012etc(pef1$occ))  # about 0.39 

# knn

knnout &lt;- ovalogtrn(pef1,"occ",25)
preds &lt;- predict(knnout,predpts=pef1[,-3])
mean(preds == factorTo012etc(pef1$occ))  # about 0.43

data(oliveoils)
oo &lt;- oliveoils
oo &lt;- oo[,-1]
knnout &lt;- ovaknntrn(oo,'Region',10)
# predict a new case that is like oo1[1,] but with palmitic = 950
newx &lt;- oo[1,2:9,drop=FALSE]
newx[,1] &lt;- 950
predict(knnout,predpts=newx)  # predicts class 2, South


## End(Not run)

</code></pre>

<hr>
<h2 id='newadult'>
UCI adult income data set, adapted
</h2><span id='topic+newadult'></span><span id='topic+newAdult'></span>

<h3>Description</h3>

<p>This data set is adapted from
the Adult data from the UCI Machine Learning Repository,
which was in turn adapted from Census data on adult incomes and other 
demographic variables.  The UCI data is used here with permission 
from Ronny Kohavi.
</p>
<p>The variables are:
</p>

<ul>
<li> <p><code>gt50</code>, which converts the original <code>&gt;50K</code> variable
to an indicator variable; 1 for income greater than $50,000, else 0
</p>
</li>
<li> <p><code>edu</code>, which converts a set of education levels to
approximate number of years of schooling
</p>
</li>
<li> <p><code>age</code>
</p>
</li>
<li> <p><code>gender</code>, 1 for male, 0 for female
</p>
</li>
<li> <p><code>mar</code>, 1 for married, 0 for single
</p>
</li></ul>

<p>Note that the education variable is now numeric.
</p>

<hr>
<h2 id='nlshc'>Heteroscedastic Nonlinear Regression</h2><span id='topic+nlshc'></span>

<h3>Description</h3>

<p>Extension of <code>nls</code> to the heteroscedastic case.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlshc(nlsout,type='HC')
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nlshc_+3A_nlsout">nlsout</code></td>
<td>
<p>Object of type 'nls'.</p>
</td></tr>
<tr><td><code id="nlshc_+3A_type">type</code></td>
<td>
<p>Eickert-White algorithm to use.  See documentation for
<span class="pkg">nls</span>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calls <code>nls</code> but then forms a different estimated covariance 
matrix for the estimated regression coefficients, applying the
Eickert-White technique to handle heteroscedasticity.  This then
gives valid statistical inference in that setting.
</p>
<p>Some users may prefer to use <code>nlsLM</code> of the package
<span class="pkg">minpack.lm</span> instead of <code>nls</code>. This is fine, as both
functions return objects of class 'nls'.
</p>


<h3>Value</h3>

<p>Estimated covariance matrix
</p>


<h3>Author(s)</h3>

<p>Norm Matloff
</p>


<h3>References</h3>

<p> Zeileis A (2006), Object-Oriented Computation of Sandwich
Estimators.  <em>Journal of Statistical Software</em>, <b>16</b>(9),
1&ndash;16, <a href="https://www.jstatsoft.org/v16/i09/">https://www.jstatsoft.org/v16/i09/</a>.  </p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate data from a setting in which mean Y is 
# 1 / (b1 * X1 + b2 * X2)
n &lt;- 250
b &lt;- 1:2
x &lt;- matrix(rexp(2*n),ncol=2)
meany &lt;- 1 / (x %*% b)  # reg ftn
y &lt;- meany + (runif(n) - 0.5) * meany  # heterosced epsilon
xy &lt;- cbind(x,y)
xy &lt;- data.frame(xy)
# see nls() docs
nlout &lt;- nls(X3 ~ 1 / (b1*X1+b2*X2),
   data=xy,start=list(b1 = 1,b2=1))
nlshc(nlout)
</code></pre>

<hr>
<h2 id='oliveoils'>
Italian olive oils data set.
</h2><span id='topic+oliveoils'></span>

<h3>Description</h3>

<p>Italian olive oils data set, as used in <em>Graphics of Large 
Datasets: Visualizing a  Million</em>, by  Antony Unwin, Martin Theus and 
Heike Hofmann, Springer, 2006.  Included here with permission of Dr. 
Martin Theus.  
</p>

<hr>
<h2 id='Penrose+20Linear'>Penrose-Inverse Linear Models and Polynomial Regression</h2><span id='topic+penroseLM'></span><span id='topic+ridgePoly'></span><span id='topic+penrosePoly'></span><span id='topic+predict.penroseLM'></span><span id='topic+predict.penrosePoly'></span>

<h3>Description</h3>

<p>Provides mininum-norm solutions to linear models, identical to OLS in
standard situations, but allowing exploration of overfitting in the
overparameterized case. Also provides a wrapper for the polynomial
case.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>penroseLM(d,yName)
penrosePoly(d,yName,deg,maxInteractDeg=deg)
ridgePoly(d,yName,deg,maxInteractDeg=deg)
## S3 method for class 'penroseLM'
predict(object,...)
## S3 method for class 'penrosePoly'
predict(object,...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Penrose+2B20Linear_+3A_...">...</code></td>
<td>
<p>Arguments for the <code>predict</code> functions.</p>
</td></tr>
<tr><td><code id="Penrose+2B20Linear_+3A_d">d</code></td>
<td>
<p>Dataframe, training set.</p>
</td></tr> 
<tr><td><code id="Penrose+2B20Linear_+3A_yname">yName</code></td>
<td>
<p>Name of the class labels column.</p>
</td></tr>
<tr><td><code id="Penrose+2B20Linear_+3A_deg">deg</code></td>
<td>
<p>Polynomial degree.</p>
</td></tr>
<tr><td><code id="Penrose+2B20Linear_+3A_maxinteractdeg">maxInteractDeg</code></td>
<td>
<p>Maximum degree of interaction terms.</p>
</td></tr>
<tr><td><code id="Penrose+2B20Linear_+3A_object">object</code></td>
<td>
<p>A value returned by <code>penroseLM</code> or
<code>penrosePoly</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>First, provides a convenient wrapper to the <span class="pkg">polyreg</span> package for
polynomial regression. (See <code>qePoly</code> here for an even higher-level
wrapper.)  Note that this computes true polynomials, with
cross-product/interaction terms rather than just powers, and that dummy
variables are handled properly (to NOT compute powers).
</p>
<p>Second, provides a tool for exploring the &quot;double descent&quot; phenomenon,
in which prediction error may improve upon fitting past the
interpolation point.
</p>


<h3>Author(s)</h3>

<p>Norm Matloff
</p>

<hr>
<h2 id='phoneme'>
Phoneme Data
</h2><span id='topic+phoneme'></span>

<h3>Description</h3>

<p>Phoneme detection, 2 types.  Features are from harmonic analysis of th
voice.  From OpenML, <a href="https://www.openml.org/d/1489">https://www.openml.org/d/1489</a>.
</p>

<hr>
<h2 id='prgeng'>
Silicon Valley programmers and engineers data
</h2><span id='topic+prgeng'></span><span id='topic+peDumms'></span><span id='topic+peFactors'></span><span id='topic+pef'></span>

<h3>Description</h3>

<p>This data set is adapted from the 2000 Census (5% sample, person
records).  It is mainly restricted to programmers and engineers in the 
Silicon Valley area.  (Apparently due to errors, there are some from
other ZIP codes.)
</p>
<p>There are three versions:
</p>

<ul>
<li><p><code>prgeng</code>, the original data, with categorical variables,
e.g. Occupation, in their original codes
</p>
</li>
<li><p><code>peDumms</code>, same but with categorical variables
converted to dummies; due to the large number of levels the birth
and PUMA data is not included
</p>
</li>
<li><p><code>peFactors</code>, same but with categorical variables
converted to factors
</p>
</li>
<li><p><code>pef</code>, same as <code>peFactors</code>, but having only columns
for age, education, occupation, gender, wage income and weeks
worked.  The education column has been collapsed to Master's degree,
PhD and other.
</p>
</li></ul>

<p>The variable codes, e.g. occupational codes, are available from 
<a href="https://usa.ipums.org/usa/volii/occ2000.shtml">https://usa.ipums.org/usa/volii/occ2000.shtml</a>.
(Short code lists are given in the record layout, but longer ones are in
the appendix Code Lists.)
</p>
<p>The variables are:
</p>

<ul>
<li><p><code>age</code>, with a U(0,1) variate added for jitter
</p>
</li>
<li><p><code>cit</code>, citizenship; 1-4 code various categories of
citizens; 5 means noncitizen (including permanent residents)
</p>
</li>
<li><p><code>educ</code>: 01-09 code no college; 10-12 means some college;
13 is a bachelor's degree, 14 a master's, 15 a professional degree and
16 is a doctorate
</p>
</li>
<li><p><code>occ</code>, occupation
</p>
</li>
<li><p><code>birth</code>, place of birth
</p>
</li>
<li><p><code>wageinc</code>, wage income
</p>
</li>
<li><p><code>wkswrkd</code>, number of weeks worked
</p>
</li>
<li><p><code>yrentry</code>, year of entry to the U.S. (0 for natives)
</p>
</li>
<li><p><code>powpuma</code>, location of work 
</p>
</li>
<li><p><code>gender</code>, 1 for male, 2 for female
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>data(prgeng)
data(peDumms)
data(peFactors)
</code></pre>

<hr>
<h2 id='quizDocs'>
Course quiz documents
</h2><span id='topic+quizDocs'></span><span id='topic+quizzes'></span>

<h3>Description</h3>

<p>This data is suitable for NLP analysis.  It consists of all the quizzes
I've given in undergraduate courses, 143 quizzes in all.  
</p>
<p>It is available in two forms.  First, <code>quizzes</code> is a data.frame,
143 rows and 2 columns.  Row i consists of a single character vector
comprising the entire quiz i, followed by the course name (as an R
factor).  The second form is an R list, 143 elements.  Each list element
is a character vector, one vector element per line of the quiz.
</p>
<p>The original documents were LaTeX files.  They have been run through the
<code>detex</code> utility to remove most LaTeX commands, as well as removing
the LaTeX preambles separately.
</p>
<p>The names of the list elements are the course names, as follows:
</p>
<p>ECS 50:  a course in machine organization
</p>
<p>ECS 132:  an undergraduate course in probabilistic modeling
</p>
<p>ECS 145:  a course in scripting languages (Python, R)
</p>
<p>ECS 158:  an undergraduate course in parallel computation
</p>
<p>ECS 256:  a graduate course in probabilistic modeling
</p>

<hr>
<h2 id='ridgelm+2Cplot.rlm'>Ridge Regression</h2><span id='topic+ridgelm'></span><span id='topic+plot.rlm'></span>

<h3>Description</h3>

<p>Similar to <code>lm.ridge</code> in <code>MASS</code> packaged included
with R, but with a different kind of scaling and a little nicer
plotting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgelm(xy,lambda = seq(0.01,1,0.01),mapback=TRUE) 
## S3 method for class 'rlm'
plot(x,y,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgelm+2B2Cplot.rlm_+3A_xy">xy</code></td>
<td>
<p>Data, response variable in the last column.</p>
</td></tr>
<tr><td><code id="ridgelm+2B2Cplot.rlm_+3A_lambda">lambda</code></td>
<td>
<p>Vector of desired values for the ridge parameter.</p>
</td></tr>
<tr><td><code id="ridgelm+2B2Cplot.rlm_+3A_mapback">mapback</code></td>
<td>
<p>If TRUE, the scaling that had been applied to the
original data will be map back to the original scale, so that the
estimated regression coefficients are now on the scale of the original
data.</p>
</td></tr>
<tr><td><code id="ridgelm+2B2Cplot.rlm_+3A_x">x</code></td>
<td>
<p>Object of type 'rlm', output of <code>ridgelm</code>.</p>
</td></tr>
<tr><td><code id="ridgelm+2B2Cplot.rlm_+3A_y">y</code></td>
<td>
<p>Needed for consistency with the generic. Not used.</p>
</td></tr>
<tr><td><code id="ridgelm+2B2Cplot.rlm_+3A_...">...</code></td>
<td>
<p>Needed for consistency with the generic.  Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Centers and scales the predictors X, and centers the response
variable Y.  Computes X'X and then solves [(X'X)/n + lambda I]b =
X'Y/n for b.  The 1/n factors are important, making the diagonal
elements of (X'X)/n all 1s and thus facilitating choices for the
lambdas in a manner independent of the data.
</p>
<p>Calling <code>plot</code> on the output of <code>ridgelm</code> dispatches to
<code>plot.rlm</code>, thus diplaying the ridge traces.
</p>


<h3>Value</h3>

<p>The function <code>ridgelm</code> returns an object of class 'rlm', with
components <code>bhats</code>, the estimated beta vectors, one column per
lambda value, and <code>lambda</code>, a copy of the input.
</p>


<h3>Author(s)</h3>

<p>Norm Matloff
</p>

<hr>
<h2 id='SwissRoll'>
Swiss Roll
</h2><span id='topic+SwissRoll'></span><span id='topic+sw'></span>

<h3>Description</h3>

<p>See <a href="http://people.cs.uchicago.edu/~dinoj/manifold/swissroll.html">http://people.cs.uchicago.edu/~dinoj/manifold/swissroll.html</a>
for this version of Swiss Roll.
</p>
<p>Running <code>data(SwissRoll)</code> produces an object <code>sw</code>.
</p>

<hr>
<h2 id='textToXY+2CtextToXYpred'>Tools for Text Classification</h2><span id='topic+textToXY'></span><span id='topic+textToXYpred'></span>

<h3>Description</h3>

<p>&quot;R-style,&quot; classification-oriented wrappers for the <span class="pkg">text2vec</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    textToXY(docs,labels,kTop=50,stopWords='a') 
    textToXYpred(ttXYout,predDocs) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textToXY+2B2CtextToXYpred_+3A_docs">docs</code></td>
<td>
<p>Character vector, one element per document.</p>
</td></tr>
<tr><td><code id="textToXY+2B2CtextToXYpred_+3A_preddocs">predDocs</code></td>
<td>
<p>Character vector, one element per document.</p>
</td></tr>
<tr><td><code id="textToXY+2B2CtextToXYpred_+3A_labels">labels</code></td>
<td>
<p>Class labels, as numeric, character or factor.  NULL is
used at the prediction stage.</p>
</td></tr>
<tr><td><code id="textToXY+2B2CtextToXYpred_+3A_ktop">kTop</code></td>
<td>
<p>The number of most-frequent words to retain; 0 means 
retain all.</p>
</td></tr>
<tr><td><code id="textToXY+2B2CtextToXYpred_+3A_stopwords">stopWords</code></td>
<td>
<p>Character vector of common words, e.g. prepositions
to delete. Recommended is <code>tm::stopwords('english')</code>.</p>
</td></tr>
<tr><td><code id="textToXY+2B2CtextToXYpred_+3A_ttxyout">ttXYout</code></td>
<td>
<p>Output object from <code>textToXY</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A typical classification/machine learning package will have as arguments
a feature matrix X and a labels vector/factor Y.  For a &quot;bag of
words&quot; analysis in the text case, each row of X would be a document
and each column a word.
</p>
<p>The functions here are basically wrappers for generating X.  Wrappers
are convenient in that:
</p>

<ul>
<li><p> The <span class="pkg">text2vec</span> package is rather arcane, so a &quot;R-style&quot; 
wrapper would be useful.
</p>
</li>
<li><p> The <span class="pkg">text2vec</span> are not directly set up to do
classification, so the functions here provide the &quot;glue&quot; to do
that.
</p>
</li></ul>

<p>The typical usage pattern is thus:
</p>

<ul>
<li><p> Run the documents vector and labels vector/factor through
<code>textToXY</code>, generating X and Y.
</p>
</li>
<li><p> Apply your favorite classification/machine learning package
p to X and Y, returning o.
</p>
</li>
<li><p> When predicting a new document d, run o and d through
<code>textToXY</code>, producing x.
</p>
</li>
<li><p> Run x on p's <code>predict</code> function.
</p>
</li></ul>



<h3>Value</h3>

<p>The function <code>textToXY</code> returns an R list with components
<code>x</code> and <code>y</code> for X and Y, and a copy of the input
<code>stopWords</code>.
</p>
<p>The function <code>textToXY</code> returns X.
</p>


<h3>Author(s)</h3>

<p>Norm Matloff
</p>

<hr>
<h2 id='TStoX'>Transform Time Series to Rectangular Form</h2><span id='topic+TStoX'></span><span id='topic+TStoXmv'></span>

<h3>Description</h3>

<p>Input a time series and transform it to a form suitable for prediction
using <code>lm</code> etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TStoX(x,lg)
TStoXmv(xmat,lg,y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TStoX_+3A_x">x</code></td>
<td>
<p>A vector.</p>
</td></tr>
<tr><td><code id="TStoX_+3A_lg">lg</code></td>
<td>
<p>Lag, a positive integer.</p>
</td></tr>
<tr><td><code id="TStoX_+3A_xmat">xmat</code></td>
<td>
<p>A matrix, data frame etc., a multivariate time series.
Each column is a time series, over a common time period.</p>
</td></tr>
<tr><td><code id="TStoX_+3A_y">y</code></td>
<td>
<p>A time series, again on that common time period.  If NULL in
<code>TStoXmv</code>, then <code>y</code> is set to <code>x</code> (i.e. for a 
univariate time series in which older values predict newer ones).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Similar to <code>stats::embed</code>, but in lagged form, with applications
such as <code>lm</code> in mind.
</p>
<p><code>TStoX</code> is for transforming vectors, while <code>TStoXmv</code>
handles the multivariate time series case.  Intended for use with
<code>lm</code> or other regression/machine learning  model, predicting 
<code>y[i]</code> from observations <code>i-lg, i-lg+1,...,i-1</code>.
</p>


<h3>Value</h3>

<p>As noted, the idea is to set up something like <code>lm(Y ~ X)</code>.
Let <code>m</code> denote length of <code>x</code>, and in the matrix input
case, the number of rows in <code>xmat</code>.  Let <code>p</code> be 1 in the
vector case, <code>ncol(xmat)</code> in the matrix case.  The return value
is a matrix with <code>m-lg</code> rows.  There will be <code>p*lg+1</code>
columns, with &quot;Y,&quot; the numbers to be predicted in the last column.
</p>
<p>In the output in the multivariate case, let k denote
<code>ncol(xmat)</code>.  Then the first k columns of the output  will be
the k series at lag <code>lg</code>, the second k columns will be the k
series at lag <code>lg-1</code>, ..., and the <code>lg</code>-th set of k
columns will be the k series at lag 1,
</p>


<h3>Author(s)</h3>

<p>Norm Matloff
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x1 &lt;- c(5,12,13,8,88,6) 
x2 &lt;- c(5,4,3,18,168,0) 
y &lt;- 1:6 
xmat &lt;- cbind(x1,x2) 

TStoX(x1,2)
#      [,1] [,2] [,3]
# [1,]    5   12   13
# [2,]   12   13    8
# [3,]   13    8   88
# [4,]    8   88    6

xy &lt;- TStoXmv(xmat,2,y)
xy
#      [,1] [,2] [,3] [,4] [,5]
# [1,]    5    5   12    4    3
# [2,]   12    4   13    3    4
# [3,]   13    3    8   18    5
# [4,]    8   18   88  168    6

lm(xy[,5] ~ xy[,-5])
# Coefficients:
# (Intercept)    xy[, -5]1    xy[, -5]2    xy[, -5]3    xy[, -5]4
#       -65.6          3.2         18.2         -3.2           NA
# need n &gt; 7 here for useful lm() call, but this illustrates the idea
</code></pre>

<hr>
<h2 id='unscale'>Miscellaneous Utilities</h2><span id='topic+unscale'></span><span id='topic+mmscale'></span><span id='topic+catDFRow'></span><span id='topic+constCols'></span><span id='topic+allNumeric'></span>

<h3>Description</h3>

<p>Utilities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unscale(scaledx,ctrs=NULL,sds=NULL)
mmscale(m,scalePars=NULL,p=NULL)
catDFRow(dfRow)
constCols(d)
allNumeric(lst)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unscale_+3A_scaledx">scaledx</code></td>
<td>
<p>A matrix.</p>
</td></tr>
<tr><td><code id="unscale_+3A_m">m</code></td>
<td>
<p>A matrix.</p>
</td></tr>
<tr><td><code id="unscale_+3A_ctrs">ctrs</code></td>
<td>
<p>Take the original means to be <code>ctrs</code></p>
</td></tr>
<tr><td><code id="unscale_+3A_lst">lst</code></td>
<td>
<p>An R list.</p>
</td></tr>
<tr><td><code id="unscale_+3A_sds">sds</code></td>
<td>
<p>Take the original standard deviations to be <code>sds</code></p>
</td></tr>
<tr><td><code id="unscale_+3A_dfrow">dfRow</code></td>
<td>
<p>A row in a data frame.</p>
</td></tr>
<tr><td><code id="unscale_+3A_d">d</code></td>
<td>
<p>A data frame or matrix.</p>
</td></tr>
<tr><td><code id="unscale_+3A_scalepars">scalePars</code></td>
<td>
<p>If not NULL, a 2-row matrix, with column <code>i</code> storing
the min and max values to be used in scaling column <code>i</code> of <code>m</code>.
Typically, one has previously called <code>mmscale</code> on a dataset and
saved the resulting scale parameters, and we wish to use those
same scale parameters on new data.</p>
</td></tr>
<tr><td><code id="unscale_+3A_p">p</code></td>
<td>
<p>If <code>m</code> is a vector, this specifies the 
number of columns it should have as a matrix. The code will try to take 
care of this by itself if <code>p</code> is left at NULL.</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>The function <code>mmscale</code> is meant as a better-behaved alternative to
<code>scale</code>.  Using minimum and maximum values, it maps variables to
[0,1], thus avoiding the problems arising from very small standard
deviations in <code>scale</code>.
</p>
<p>The function <code>catDFRow</code> nicely prints a row of a data frame.
</p>
<p>The function <code>constCols</code> determines which columns of a data frame
or matrix are constant, if any.
</p>


<h3>Value</h3>

<p>The function <code>unscale</code> returns the original object to which
<code>scale</code> had been applied.  Or, the attributes <code>ctrs</code> and
<code>sds</code> can be specified by the user.
</p>


<h3>Author(s)</h3>

<p>Norm Matloff
</p>

<hr>
<h2 id='weatherTS'>
Weather Time Series
</h2><span id='topic+weatherTS'></span>

<h3>Description</h3>

<p>Various measurements on weather variables collected by NASA.  Downloaded
via <code>nasapower</code>; see that package for documentation.
</p>

<hr>
<h2 id='xyzPlot'>Misc. Graphics</h2><span id='topic+xyzPlot'></span>

<h3>Description</h3>

<p>Graphics utiliites.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xyzPlot(xyz,clrs=NULL,cexText=1.0,xlim=NULL,ylim=NULL,
   xlab=NULL,ylab=NULL,legendPos=NULL,plotType='l') 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xyzPlot_+3A_xyz">xyz</code></td>
<td>
<p>A matrix or data frame of at least 3 columns, the first
3 serving as 'x', 'y' and 'z' coordinates of points to be plotted. 
Grouping, if any, is specified in column 4, in which case <code>xyz</code>
must be a data frame.</p>
</td></tr>
<tr><td><code id="xyzPlot_+3A_clrs">clrs</code></td>
<td>
<p>Colors to be used in the grouped case.</p>
</td></tr>
<tr><td><code id="xyzPlot_+3A_cextext">cexText</code></td>
<td>
<p>Text size, proportional to standard.</p>
</td></tr>
<tr><td><code id="xyzPlot_+3A_xlim">xlim</code></td>
<td>
<p>As in <code>plot</code>.</p>
</td></tr>
<tr><td><code id="xyzPlot_+3A_ylim">ylim</code></td>
<td>
<p>As in <code>plot</code>.</p>
</td></tr>
<tr><td><code id="xyzPlot_+3A_xlab">xlab</code></td>
<td>
<p>As in <code>plot</code>.</p>
</td></tr>
<tr><td><code id="xyzPlot_+3A_ylab">ylab</code></td>
<td>
<p>As in <code>plot</code>.</p>
</td></tr>
<tr><td><code id="xyzPlot_+3A_legendpos">legendPos</code></td>
<td>
<p>As in <code>legend</code>.</p>
</td></tr>
<tr><td><code id="xyzPlot_+3A_plottype">plotType</code></td>
<td>
<p>Coded 'l' for lines, 'p' for points.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A way to display 3-dimensional data in 2 dimensions.  For each plotted
point (x,y), a z value is written in text over the point.  A grouping
variable is also allowed, with different colors used to plot different
groups.
</p>
<p>A group (including the entire data in the case of one group) can be
displayed either as a polygonal line, or just as a point cloud.  The
user should experiment with different argument settings to get the most
visually impactful plot.
</p>


<h3>Author(s)</h3>

<p>Norm Matloff
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

xyzPlot(mtcars[,c(3,6,1)],plotType='l',cexText=0.75)
xyzPlot(mtcars[,c(3,6,1)],plotType='p',cexText=0.75)
xyzPlot(mtcars[,c(3,6,1)],plotType='l',cexText=0.75)
xyzPlot(mtcars[,c(3,6,1,2)],clrs=c('red','darkgreen','blue'),plotType='l',cexText=0.75)


## End(Not run)

</code></pre>

<hr>
<h2 id='yell10k'>
New York Taxi Data
</h2><span id='topic+yell10k'></span>

<h3>Description</h3>

<p>From public data on New York City taxi trips.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
