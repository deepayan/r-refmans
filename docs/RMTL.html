<!DOCTYPE html><html><head><title>Help for package RMTL</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {RMTL}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#calcError'><p>Calculate the prediction error</p></a></li>
<li><a href='#Create_simulated_data'><p>Create an example dataset for testing the MTL algorithm</p></a></li>
<li><a href='#cvMTL'><p>K-fold cross-validation</p></a></li>
<li><a href='#MTL'><p>Train a multi-task learning model.</p></a></li>
<li><a href='#plot.cvMTL'><p>Plot the cross-validation curve</p></a></li>
<li><a href='#plotObj'><p>Plot the historical values of objective function</p></a></li>
<li><a href='#predict.MTL'><p>Predict the outcomes of new individuals</p></a></li>
<li><a href='#print.MTL'><p>Print the  meta information of the model</p></a></li>
<li><a href='#RMTL-package'><p>RMTL: Regularized Multi-Task Learning</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Regularized Multi-Task Learning</td>
</tr>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Version:</td>
<td>0.9.9</td>
</tr>
<tr>
<td>Description:</td>
<td>Efficient solvers for 10 regularized multi-task learning algorithms applicable for regression, classification, joint feature selection, task clustering, low-rank learning, sparse learning and network incorporation. Based on the accelerated gradient descent method, the algorithms feature a state-of-art computational complexity O(1/k^2). Sparse model structure is induced by the solving the proximal operator. The detail of the package is described in the paper of Han Cao and Emanuel Schwarz (2018) &lt;<a href="https://doi.org/10.1093%2Fbioinformatics%2Fbty831">doi:10.1093/bioinformatics/bty831</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/transbioZI/RMTL/">https://github.com/transbioZI/RMTL/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/transbioZI/RMTL/issues/">https://github.com/transbioZI/RMTL/issues/</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS (&ge; 7.3-50), psych (&ge; 1.8.4), corpcor (&ge; 1.6.9),
doParallel (&ge; 1.0.14), foreach (&ge; 1.4.4)</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-04-29</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Author:</td>
<td>Han Cao [cre, aut, cph],
  Emanuel Schwarz [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Han Cao &lt;hank9cao@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-05-02 15:16:53 UTC; hank</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-05-02 16:10:09 UTC</td>
</tr>
</table>
<hr>
<h2 id='calcError'>Calculate the prediction error</h2><span id='topic+calcError'></span>

<h3>Description</h3>

<p>Calculate the averaged prediction error across tasks. For
classification problem, the miss-classification rate is returned, and for
regression problem, the mean square error(MSE) is returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calcError(m, newX = NULL, newY = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calcError_+3A_m">m</code></td>
<td>
<p>A MTL model</p>
</td></tr>
<tr><td><code id="calcError_+3A_newx">newX</code></td>
<td>
<p>The feature matrices of new individuals</p>
</td></tr>
<tr><td><code id="calcError_+3A_newy">newY</code></td>
<td>
<p>The responses of new individuals</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The averaged prediction error
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#create example data
data&lt;-Create_simulated_data(Regularization="L21", type="Regression")
#train a model 
model&lt;-MTL(data$X, data$Y, type="Regression", Regularization="L21",
    Lam1=0.1, Lam2=0, opts=list(init=0,  tol=10^-6, maxIter=1500)) 
#calculate the training error
calcError(model, newX=data$X, newY=data$Y)
#calculate the test error
calcError(model, newX=data$tX, newY=data$tY)

</code></pre>

<hr>
<h2 id='Create_simulated_data'>Create an example dataset for testing the MTL algorithm</h2><span id='topic+Create_simulated_data'></span>

<h3>Description</h3>

<p>Create an example dataset which contains 1), training datasets (X: feature matrices, Y: response vectors); 2), test datasets 
(tX: feature matrices, tY: response vectors); 3), the ground truth model (W: coefficient matrix) and 4), extra
information for some algorithms (i.e. a matrix for encoding the network information is necessary for calling the MTL method with network 
structure(<code>Regularization=Graph</code> )
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Create_simulated_data(
  t = 5,
  p = 50,
  n = 20,
  type = "Regression",
  Regularization = "L21"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Create_simulated_data_+3A_t">t</code></td>
<td>
<p>Number of tasks</p>
</td></tr>
<tr><td><code id="Create_simulated_data_+3A_p">p</code></td>
<td>
<p>Number of features</p>
</td></tr>
<tr><td><code id="Create_simulated_data_+3A_n">n</code></td>
<td>
<p>Number of samples of each task. For simplicity, all tasks
contain the same number of samples.</p>
</td></tr>
<tr><td><code id="Create_simulated_data_+3A_type">type</code></td>
<td>
<p>The type of problem, must be &quot;Regression&quot; or
&quot;Classification&quot;</p>
</td></tr>
<tr><td><code id="Create_simulated_data_+3A_regularization">Regularization</code></td>
<td>
<p>The type of MTL algorithm (cross-task regularizer). The value must be
one of {<code>L21</code>, <code>Lasso</code>, <code>Trace</code>, <code>Graph</code>, <code>CMTL</code> }</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The example dataset.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data&lt;-Create_simulated_data(t=5,p=50, n=20, type="Regression", Regularization="L21")
str(data)
</code></pre>

<hr>
<h2 id='cvMTL'>K-fold cross-validation</h2><span id='topic+cvMTL'></span>

<h3>Description</h3>

<p>Perform the k-fold cross-validation to estimate the <code class="reqn">\lambda_1</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvMTL(
  X,
  Y,
  type = "Classification",
  Regularization = "L21",
  Lam1_seq = 10^seq(1, -4, -1),
  Lam2 = 0,
  G = NULL,
  k = 2,
  opts = list(init = 0, tol = 10^-3, maxIter = 1000),
  stratify = FALSE,
  nfolds = 5,
  ncores = 2,
  parallel = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvMTL_+3A_x">X</code></td>
<td>
<p>A set of feature matrices</p>
</td></tr>
<tr><td><code id="cvMTL_+3A_y">Y</code></td>
<td>
<p>A set of responses, could be binary (classification
problem) or continues (regression problem). The valid
value of binary outcome <code class="reqn">\in\{1, -1\}</code></p>
</td></tr>
<tr><td><code id="cvMTL_+3A_type">type</code></td>
<td>
<p>The type of problem, must be <code>Regression</code> or
<code>Classification</code></p>
</td></tr>
<tr><td><code id="cvMTL_+3A_regularization">Regularization</code></td>
<td>
<p>The type of MTL algorithm (cross-task regularizer). The value must be
one of {<code>L21</code>, <code>Lasso</code>, <code>Trace</code>, <code>Graph</code>, <code>CMTL</code> }</p>
</td></tr>
<tr><td><code id="cvMTL_+3A_lam1_seq">Lam1_seq</code></td>
<td>
<p>A positive sequence of <code>Lam1</code> which controls the
cross-task regularization</p>
</td></tr>
<tr><td><code id="cvMTL_+3A_lam2">Lam2</code></td>
<td>
<p>A positive constant <code class="reqn">\lambda_{2}</code> to improve the
generalization performance</p>
</td></tr>
<tr><td><code id="cvMTL_+3A_g">G</code></td>
<td>
<p>A matrix to encode the network information. This parameter
is only used in the MTL with graph structure (<code>Regularization=Graph</code> )</p>
</td></tr>
<tr><td><code id="cvMTL_+3A_k">k</code></td>
<td>
<p>A positive number to modulate the structure of clusters
with the default of 2. This parameter is only used in MTL with
clustering structure (<code>Regularization=CMTL</code> ) Note, the larger number is adapted to more
complex clustering structure.</p>
</td></tr>
<tr><td><code id="cvMTL_+3A_opts">opts</code></td>
<td>
<p>Options of the optimization procedure. One can set the
initial search point, the tolerance and the maximized number of
iterations through the parameter. The default value is
<code>list(init=0,  tol=10^-3, maxIter=1000)</code></p>
</td></tr>
<tr><td><code id="cvMTL_+3A_stratify">stratify</code></td>
<td>
<p><code>stratify=TRUE</code> is used for stratified
cross-validation</p>
</td></tr>
<tr><td><code id="cvMTL_+3A_nfolds">nfolds</code></td>
<td>
<p>The number of folds</p>
</td></tr>
<tr><td><code id="cvMTL_+3A_ncores">ncores</code></td>
<td>
<p>The number of cores used for parallel computing with the default value of 2</p>
</td></tr>
<tr><td><code id="cvMTL_+3A_parallel">parallel</code></td>
<td>
<p><code>parallel=TRUE</code> is used for parallel computing</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The estimated <code class="reqn">\lambda_1</code> and related information
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#create the example data
data&lt;-Create_simulated_data(Regularization="L21", type="Classification")
#perform the cross validation
cvfit&lt;-cvMTL(data$X, data$Y, type="Classification", Regularization="L21", 
    Lam2=0, opts=list(init=0,  tol=10^-6, maxIter=1500), nfolds=5,
    stratify=TRUE, Lam1_seq=10^seq(1,-4, -1))
#show meta-infomration
str(cvfit)
#plot the CV accuracies across lam1 sequence
plot(cvfit)
</code></pre>

<hr>
<h2 id='MTL'>Train a multi-task learning model.</h2><span id='topic+MTL'></span>

<h3>Description</h3>

<p>Train a multi-task learning model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MTL(
  X,
  Y,
  type = "Classification",
  Regularization = "L21",
  Lam1 = 0.1,
  Lam1_seq = NULL,
  Lam2 = 0,
  opts = list(init = 0, tol = 10^-3, maxIter = 1000),
  G = NULL,
  k = 2
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MTL_+3A_x">X</code></td>
<td>
<p>A set of feature matrices</p>
</td></tr>
<tr><td><code id="MTL_+3A_y">Y</code></td>
<td>
<p>A set of responses, could be binary (classification
problem) or continues (regression problem). The valid
value of binary outcome <code class="reqn">\in\{1, -1\}</code></p>
</td></tr>
<tr><td><code id="MTL_+3A_type">type</code></td>
<td>
<p>The type of problem, must be <code>Regression</code> or
<code>Classification</code></p>
</td></tr>
<tr><td><code id="MTL_+3A_regularization">Regularization</code></td>
<td>
<p>The type of MTL algorithm (cross-task regularizer). The value must be
one of {<code>L21</code>, <code>Lasso</code>, <code>Trace</code>, <code>Graph</code>, <code>CMTL</code> }</p>
</td></tr>
<tr><td><code id="MTL_+3A_lam1">Lam1</code></td>
<td>
<p>A positive constant <code class="reqn">\lambda_{1}</code> to control the
cross-task regularization</p>
</td></tr>
<tr><td><code id="MTL_+3A_lam1_seq">Lam1_seq</code></td>
<td>
<p>A positive sequence of <code>Lam1</code>. If the parameter
is given, the model is trained using warm-start technique. Otherwise, the
model is trained based on the <code>Lam1</code> and the initial search point (<code>opts$init</code>).</p>
</td></tr>
<tr><td><code id="MTL_+3A_lam2">Lam2</code></td>
<td>
<p>A non-negative constant <code class="reqn">\lambda_{2}</code> to improve the
generalization performance with the default value of 0 (except for
<code>Regularization=CMTL</code>)</p>
</td></tr>
<tr><td><code id="MTL_+3A_opts">opts</code></td>
<td>
<p>Options of the optimization procedure. One can set the
initial search point, the tolerance and the maximized number of
iterations using this parameter. The default value is
<code>list(init=0,  tol=10^-3, maxIter=1000)</code></p>
</td></tr>
<tr><td><code id="MTL_+3A_g">G</code></td>
<td>
<p>A matrix to encode the network information. This parameter
is only used in the MTL with graph structure (<code>Regularization=Graph</code> )</p>
</td></tr>
<tr><td><code id="MTL_+3A_k">k</code></td>
<td>
<p>A positive number to modulate the structure of clusters
with the default of 2. This parameter is only used in MTL with
clustering structure (<code>Regularization=CMTL</code> ) Note, the larger number is adapted to more
complex clustering structure.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The trained model including the coefficient matrix <code>W</code>
and intercepts <code>C</code> and related meta information
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#create the example data
data&lt;-Create_simulated_data(Regularization="L21", type="Regression")
#train a MTL model
#cold-start
model&lt;-MTL(data$X, data$Y, type="Regression", Regularization="L21",
    Lam1=0.1, Lam2=0, opts=list(init=0,  tol=10^-6, maxIter=1500))
#warm-start
model&lt;-MTL(data$X, data$Y, type="Regression", Regularization="L21",
    Lam1=0.1, Lam1_seq=10^seq(1,-4, -1), Lam2=0, opts=list(init=0,  tol=10^-6, maxIter=1500))
#meta-information
str(model)
#plot the historical objective values
plotObj(model)
</code></pre>

<hr>
<h2 id='plot.cvMTL'>Plot the cross-validation curve</h2><span id='topic+plot.cvMTL'></span>

<h3>Description</h3>

<p>Plot the cross-validation curve
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cvMTL'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.cvMTL_+3A_x">x</code></td>
<td>
<p>The returned object of function <code>cvMTL</code></p>
</td></tr>
<tr><td><code id="plot.cvMTL_+3A_...">...</code></td>
<td>
<p>Other parameters</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>#create the example data
data&lt;-Create_simulated_data(Regularization="L21", type="Classification")
#perform the cv
cvfit&lt;-cvMTL(data$X, data$Y, type="Classification", Regularization="L21", 
    Lam2=0, opts=list(init=0,  tol=10^-6, maxIter=1500), nfolds=5,
    stratify=TRUE, Lam1_seq=10^seq(1,-4, -1))
#plot the curve
plot(cvfit)
</code></pre>

<hr>
<h2 id='plotObj'>Plot the historical values of objective function</h2><span id='topic+plotObj'></span>

<h3>Description</h3>

<p>Plot the values of objective function across iterations in the
optimization procedure. This function indicates the &quot;inner status&quot; of the
solver during the optimization, and could be used for diagnosis of the
solver and training procedure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotObj(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotObj_+3A_m">m</code></td>
<td>
<p>A trained MTL model</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>#create the example date
data&lt;-Create_simulated_data(Regularization="L21", type="Regression")
#Train a MTL model
model&lt;-MTL(data$X, data$Y, type="Regression", Regularization="L21",
    Lam1=0.1, Lam2=0, opts=list(init=0,  tol=10^-6, maxIter=1500))
#plot the objective values
plotObj(model)
</code></pre>

<hr>
<h2 id='predict.MTL'>Predict the outcomes of new individuals</h2><span id='topic+predict.MTL'></span>

<h3>Description</h3>

<p>Predict the outcomes of new individuals. For classification, the
probability of the individual being assigned to positive label P(y==1) is estimated, and for regression, the
prediction score is estimated
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'MTL'
predict(object, newX = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.MTL_+3A_object">object</code></td>
<td>
<p>A trained MTL model</p>
</td></tr>
<tr><td><code id="predict.MTL_+3A_newx">newX</code></td>
<td>
<p>The feature matrices of new individuals</p>
</td></tr>
<tr><td><code id="predict.MTL_+3A_...">...</code></td>
<td>
<p>Other parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The predictive outcome
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Create data
data&lt;-Create_simulated_data(Regularization="L21", type="Regression")
#Train
model&lt;-MTL(data$X, data$Y, type="Regression", Regularization="L21",
    Lam1=0.1, Lam2=0, opts=list(init=0,  tol=10^-6, maxIter=1500)) 
predict(model, newX=data$tX)

</code></pre>

<hr>
<h2 id='print.MTL'>Print the  meta information of the model</h2><span id='topic+print.MTL'></span>

<h3>Description</h3>

<p>Print the  meta information of the model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'MTL'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.MTL_+3A_x">x</code></td>
<td>
<p>A trained MTL model</p>
</td></tr>
<tr><td><code id="print.MTL_+3A_...">...</code></td>
<td>
<p>Other parameters</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>#create data
data&lt;-Create_simulated_data(Regularization="L21", type="Regression")
#train a MTL model
model&lt;-MTL(data$X, data$Y, type="Regression", Regularization="L21",
    Lam1=0.1, Lam2=0, opts=list(init=0,  tol=10^-6, maxIter=1500)) 
#print the information of the model
print(model)
</code></pre>

<hr>
<h2 id='RMTL-package'>RMTL: Regularized Multi-Task Learning</h2><span id='topic+RMTL-package'></span>

<h3>Description</h3>

<p>This package provides an efficient implementation of regularized
multi-task learning (MTL) comprising 10 algorithms applicable for
regression, classification, joint feature selection, task clustering,
low-rank learning, sparse learning and network incorporation. All
algorithms are implemented based on the accelerated gradient descent
method and feature a complexity of O(1/k^2). Parallel computing is allowed to improve the efficiency. Sparse model structure
is induced by the solving the proximal operator.
</p>


<h3>Details</h3>

<p>This package provides 10 multi-task learning  algorithms (5
classification and 5 regression), which incorporate five 
regularization strategies for knowledge transferring among tasks. All
algorithms share the same framework:
</p>
<p style="text-align: center;"><code class="reqn">\min\limits_{W,C}
   \sum_{i}^{t}{L(W_i, C_i|X_i, Y_i)} + \lambda_1\Omega(W) + \lambda_2{||W||}_F^2</code>
</p>

<p>where <code class="reqn">L(\circ)</code> is the loss function (logistic loss for classification or least square loss for linear regression). 
<code class="reqn">\Omega(\circ)</code> is the cross-task regularization for knowledge transfer, and <code class="reqn">||W||_F^2</code> is used for improving the 
generalization. <code class="reqn">X=\{X_i= n_i \times p | i \in \{1,...,t\}\}</code> and <code class="reqn">Y=\{Y_i=n_i \times 1 | i \in \{1,...,t\}\}</code> are 
predictors matrices and responses of <code class="reqn">t</code> tasks respectively, while each task <code class="reqn">i</code> contains <code class="reqn">n_i</code> subjects and <code class="reqn">p</code> 
predictors. <code class="reqn">W=p \times t</code> is the coefficient matrix, where <code class="reqn">W_i</code>, the <code class="reqn">i</code>th column of <code class="reqn">W</code>, 
refers to the coefficient vector of task <code class="reqn">i</code>. 
</p>
<p>The function <code class="reqn">\Omega(W)</code> jointly modulates multi-task models(<code class="reqn">\{W_1, W_2, ..., W_t\}</code>) according to specific 
prior structure of <code class="reqn">W</code>. In this package, 5 common regularization methods are implemented to incorporate different priors, i.e.  
sparse structure (<code class="reqn">\Omega(W)=||W||_1</code>), joint feature selection (<code class="reqn">\Omega(W)=||W||_{2,1}</code>), low-rank structure
(<code class="reqn">\Omega(W)=||W||_*</code>), network-based relatedness across tasks (<code class="reqn">\Omega(W)=||WG||_F^2</code>) and task clustering
(<code class="reqn">\Omega(W)=tr(W^TW)-tr(F^TW^TWF)</code>). To call a specific method correctly, the corresponding &quot;short name&quot; has to be given. 
Follow the above sequence of methods, the short names are defined: <code>L21</code>, <code>Lasso</code>, <code>Trace</code>, <code>Graph</code> 
and <code>CMTL</code> 
</p>
<p>For all algorithms, we implemented an solver based on the accelerated
gradient descent method, which takes advantage of information from the
previous two iterations to calculate the current gradient and then
achieves an improved convergent rate. To solve the non-smooth and convex
regularizer, the proximal operator is applied. Moreover, backward
line search is used to determine the appropriate step-size in each
iteration. Overall, the solver achieves a complexity of
<code class="reqn">O(\frac{1}{k^2})</code> and is optimal among first-order gradient
descent methods.
</p>
<p>For the academic references of the implemented algorithms, the users are referred to the paper (doi:10.1093/bioinformatics/bty831) or 
the vignettes in the package.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
