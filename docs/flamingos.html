<!DOCTYPE html><html><head><title>Help for package flamingos</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {flamingos}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cemMixRHLP'><p>cemMixRHLP implements the CEM algorithm to fit a MixRHLP model.</p></a></li>
<li><a href='#emMixHMM'><p>emMixHMM implemens the EM (Baum-Welch) algorithm to fit a mixture of HMM</p>
models.</a></li>
<li><a href='#emMixHMMR'><p>emMixHMMR implements the EM algorithm to fit a mixture if HMMR models.</p></a></li>
<li><a href='#emMixRHLP'><p>emMixRHLP implements the EM algorithm to fit a mixture of RHLP models.</p></a></li>
<li><a href='#FData-class'><p>A Reference Class which represents functional data.</p></a></li>
<li><a href='#flamingos-package'><p>FLaMingos: Functional Latent datA Models for clusterING heterogeneOus curveS</p></a></li>
<li><a href='#mkStochastic'><p>mkStochastic ensures that it is a stochastic vector, matrix or array.</p></a></li>
<li><a href='#ModelMixHMM-class'><p>A Reference Class which represents a fitted Mixture of HMM model.</p></a></li>
<li><a href='#ModelMixHMMR-class'><p>A Reference Class which represents a fitted mixture of HMMR model.</p></a></li>
<li><a href='#ModelMixRHLP-class'><p>A Reference Class which represents a fitted mixture of RHLP model.</p></a></li>
<li><a href='#ParamMixHMM-class'><p>A Reference Class which contains parameters of a mixture of HMM models.</p></a></li>
<li><a href='#ParamMixHMMR-class'><p>A Reference Class which contains parameters of a mixture of HMMR models.</p></a></li>
<li><a href='#ParamMixRHLP-class'><p>A Reference Class which contains parameters of a mixture of RHLP models.</p></a></li>
<li><a href='#StatMixHMM-class'><p>A Reference Class which contains statistics of a mixture of HMM model.</p></a></li>
<li><a href='#StatMixHMMR-class'><p>A Reference Class which contains statistics of a mixture of HMMR models.</p></a></li>
<li><a href='#StatMixRHLP-class'><p>A Reference Class which contains statistics of a mixture of RHLP models.</p></a></li>
<li><a href='#toydataset'><p>A dataset composed of simulated time series with regime changes.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Functional Latent Data Models for Clustering Heterogeneous
Curves ('FLaMingos')</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides a variety of original and flexible user-friendly 
    statistical latent variable models for the simultaneous clustering and 
    segmentation of heterogeneous functional data (i.e time series, or more 
    generally longitudinal data, fitted by unsupervised algorithms, including 
    EM algorithms. Functional Latent Data Models for Clustering heterogeneous 
    curves ('FLaMingos') are originally introduced and written in 'Matlab' by
    Faicel Chamroukhi 
    <a href="https://github.com/fchamroukhi?utf8=?&amp;amp;tab=repositories&amp;amp;q=mix&amp;amp;type=public&amp;amp;language=matlab">https://github.com/fchamroukhi?utf8=?&amp;tab=repositories&amp;q=mix&amp;type=public&amp;language=matlab</a>. 
    The references are mainly the following ones.
    Chamroukhi F. (2010) <a href="https://chamroukhi.com/FChamroukhi-PhD.pdf">https://chamroukhi.com/FChamroukhi-PhD.pdf</a>.
    Chamroukhi F., Same A., Govaert, G. and Aknin P. (2010) &lt;<a href="https://doi.org/10.1016%2Fj.neucom.2009.12.023">doi:10.1016/j.neucom.2009.12.023</a>&gt;.
    Chamroukhi F., Same A., Aknin P. and Govaert G. (2011). &lt;<a href="https://doi.org/10.1109%2FIJCNN.2011.6033590">doi:10.1109/IJCNN.2011.6033590</a>&gt;.
    Same A., Chamroukhi F., Govaert G. and Aknin, P. (2011) &lt;<a href="https://doi.org/10.1007%2Fs11634-011-0096-5">doi:10.1007/s11634-011-0096-5</a>&gt;.
    Chamroukhi F., and Glotin H. (2012) &lt;<a href="https://doi.org/10.1109%2FIJCNN.2012.6252818">doi:10.1109/IJCNN.2012.6252818</a>&gt;.
    Chamroukhi F., Glotin H. and Same A. (2013) &lt;<a href="https://doi.org/10.1016%2Fj.neucom.2012.10.030">doi:10.1016/j.neucom.2012.10.030</a>&gt;.
    Chamroukhi F. (2015) <a href="https://chamroukhi.com/FChamroukhi-HDR.pdf">https://chamroukhi.com/FChamroukhi-HDR.pdf</a>.
    Chamroukhi F. and Nguyen H-D. (2019) &lt;<a href="https://doi.org/10.1002%2Fwidm.1298">doi:10.1002/widm.1298</a>&gt;.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/fchamroukhi/FLaMingos">https://github.com/fchamroukhi/FLaMingos</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/fchamroukhi/FLaMingos/issues">https://github.com/fchamroukhi/FLaMingos/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, stats, Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Collate:</td>
<td>flamingos-package.R RcppExports.R utils.R kmeans.R
mkStochastic.R FData.R ParamMixHMM.R ParamMixHMMR.R
ParamMixRHLP.R StatMixHMM.R StatMixHMMR.R StatMixRHLP.R
ModelMixHMMR.R ModelMixHMM.R ModelMixRHLP.R emMixHMM.R
emMixHMMR.R emMixRHLP.R cemMixRHLP.R data-toydataset.R</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-08-05 19:08:18 UTC; lecocq191</td>
</tr>
<tr>
<td>Author:</td>
<td>Faicel Chamroukhi <a href="https://orcid.org/0000-0002-5894-3103"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Florian Lecocq [aut, trl, cre] (R port),
  Marius Bartcus [aut, trl] (R port)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Florian Lecocq &lt;florian.lecocq@outlook.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-08-06 09:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='cemMixRHLP'>cemMixRHLP implements the CEM algorithm to fit a MixRHLP model.</h2><span id='topic+cemMixRHLP'></span>

<h3>Description</h3>

<p>cemMixRHLP implements the maximum complete likelihood parameter estimation of
mixture of RHLP models by the Classification Expectation-Maximization
algorithm (CEM algorithm).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cemMixRHLP(X, Y, K, R, p = 3, q = 1,
  variance_type = c("heteroskedastic", "homoskedastic"),
  init_kmeans = TRUE, n_tries = 1, max_iter = 100,
  threshold = 1e-05, verbose = FALSE, verbose_IRLS = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cemMixRHLP_+3A_x">X</code></td>
<td>
<p>Numeric vector of length <em>m</em> representing the covariates/inputs
<code class="reqn">x_{1},\dots,x_{m}</code>.</p>
</td></tr>
<tr><td><code id="cemMixRHLP_+3A_y">Y</code></td>
<td>
<p>Matrix of size <code class="reqn">(n, m)</code> representing the observed
responses/outputs. <code>Y</code> consists of <em>n</em> functions of <code>X</code> observed at
points <code class="reqn">1,\dots,m</code>.</p>
</td></tr>
<tr><td><code id="cemMixRHLP_+3A_k">K</code></td>
<td>
<p>The number of clusters (Number of RHLP models).</p>
</td></tr>
<tr><td><code id="cemMixRHLP_+3A_r">R</code></td>
<td>
<p>The number of regimes (RHLP components) for each cluster.</p>
</td></tr>
<tr><td><code id="cemMixRHLP_+3A_p">p</code></td>
<td>
<p>Optional. The order of the polynomial regression. By default, <code>p</code> is
set at 3.</p>
</td></tr>
<tr><td><code id="cemMixRHLP_+3A_q">q</code></td>
<td>
<p>Optional. The dimension of the logistic regression. For the purpose
of segmentation, it must be set to 1 (which is the default value).</p>
</td></tr>
<tr><td><code id="cemMixRHLP_+3A_variance_type">variance_type</code></td>
<td>
<p>Optional character indicating if the model is
&quot;homoskedastic&quot; or &quot;heteroskedastic&quot;. By default the model is
&quot;heteroskedastic&quot;.</p>
</td></tr>
<tr><td><code id="cemMixRHLP_+3A_init_kmeans">init_kmeans</code></td>
<td>
<p>Optional. A logical indicating whether or not the curve
partition should be initialized by the K-means algorithm. Otherwise the
curve partition is initialized randomly.</p>
</td></tr>
<tr><td><code id="cemMixRHLP_+3A_n_tries">n_tries</code></td>
<td>
<p>Optional. Number of runs of the EM algorithm. The solution
providing the highest log-likelihood will be returned.
</p>
<p>If <code>n_tries</code> &gt; 1, then for the first run, parameters are initialized by
uniformly segmenting the data into R segments, and for the next runs,
parameters are initialized by randomly segmenting the data into R
contiguous segments.</p>
</td></tr>
<tr><td><code id="cemMixRHLP_+3A_max_iter">max_iter</code></td>
<td>
<p>Optional. The maximum number of iterations for the EM
algorithm.</p>
</td></tr>
<tr><td><code id="cemMixRHLP_+3A_threshold">threshold</code></td>
<td>
<p>Optional. A numeric value specifying the threshold for the
relative difference of log-likelihood between two steps of the EM as
stopping criteria.</p>
</td></tr>
<tr><td><code id="cemMixRHLP_+3A_verbose">verbose</code></td>
<td>
<p>Optional. A logical value indicating whether or not values of
the log-likelihood should be printed during EM iterations.</p>
</td></tr>
<tr><td><code id="cemMixRHLP_+3A_verbose_irls">verbose_IRLS</code></td>
<td>
<p>Optional. A logical value indicating whether or not
values of the criterion optimized by IRLS should be printed at each step of
the EM algorithm.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>cemMixRHLP function implements the CEM algorithm. This function
starts with an initialization of the parameters done by the method
<code>initParam</code> of the class <a href="#topic+ParamMixRHLP">ParamMixRHLP</a>, then it alternates
between the E-Step, the C-Step (methods of the class
<a href="#topic+StatMixRHLP">StatMixRHLP</a>), and the CM-Step (method of the class
<a href="#topic+ParamMixRHLP">ParamMixRHLP</a>) until convergence (until the relative
variation of log-likelihood between two steps of the EM algorithm is less
than the <code>threshold</code> parameter).
</p>


<h3>Value</h3>

<p>EM returns an object of class <a href="#topic+ModelMixRHLP">ModelMixRHLP</a>.
</p>


<h3>See Also</h3>

<p><a href="#topic+ModelMixRHLP">ModelMixRHLP</a>, <a href="#topic+ParamMixRHLP">ParamMixRHLP</a>, <a href="#topic+StatMixRHLP">StatMixRHLP</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(toydataset)

#' # Let's fit a mixRHLP model on a dataset containing 2 clusters:
data &lt;- toydataset[1:190,1:21]
x &lt;- data$x
Y &lt;- t(data[,2:ncol(data)])

mixrhlp &lt;- cemMixRHLP(X = x, Y = Y, K = 2, R = 2, p = 1, verbose = TRUE)

mixrhlp$summary()

mixrhlp$plot()
</code></pre>

<hr>
<h2 id='emMixHMM'>emMixHMM implemens the EM (Baum-Welch) algorithm to fit a mixture of HMM
models.</h2><span id='topic+emMixHMM'></span>

<h3>Description</h3>

<p>emMixHMM implements the maximum-likelihood parameter estimation of a mixture
of HMM models by the Expectation-Maximization (EM) algorithm, known as
Baum-Welch algorithm in the context of mixHMM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emMixHMM(Y, K, R, variance_type = c("heteroskedastic", "homoskedastic"),
  order_constraint = TRUE, init_kmeans = TRUE, n_tries = 1,
  max_iter = 1000, threshold = 1e-06, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emMixHMM_+3A_y">Y</code></td>
<td>
<p>Matrix of size <code class="reqn">(n, m)</code> representing the observed
responses/outputs. <code>Y</code> consists of <em>n</em> functions of <code>X</code> observed at
points <code class="reqn">1,\dots,m</code>.</p>
</td></tr>
<tr><td><code id="emMixHMM_+3A_k">K</code></td>
<td>
<p>The number of clusters (Number of HMM models).</p>
</td></tr>
<tr><td><code id="emMixHMM_+3A_r">R</code></td>
<td>
<p>The number of regimes (HMM components) for each cluster.</p>
</td></tr>
<tr><td><code id="emMixHMM_+3A_variance_type">variance_type</code></td>
<td>
<p>Optional character indicating if the model is
&quot;homoskedastic&quot; or &quot;heteroskedastic&quot;. By default the model is
&quot;heteroskedastic&quot;.</p>
</td></tr>
<tr><td><code id="emMixHMM_+3A_order_constraint">order_constraint</code></td>
<td>
<p>Optional. A logical indicating whether or not a mask
of order one should be applied to the transition matrix of the Markov chain
to provide ordered states. For the purpose of segmentation, it must be set
to <code>TRUE</code> (which is the default value).</p>
</td></tr>
<tr><td><code id="emMixHMM_+3A_init_kmeans">init_kmeans</code></td>
<td>
<p>Optional. A logical indicating whether or not the curve
partition should be initialized by the K-means algorithm. Otherwise the
curve partition is initialized randomly.</p>
</td></tr>
<tr><td><code id="emMixHMM_+3A_n_tries">n_tries</code></td>
<td>
<p>Optional. Number of runs of the EM algorithm. The solution
providing the highest log-likelihood will be returned.
</p>
<p>If <code>n_tries</code> &gt; 1, then for the first run, parameters are initialized by
uniformly segmenting the data into K segments, and for the next runs,
parameters are initialized by randomly segmenting the data into K
contiguous segments.</p>
</td></tr>
<tr><td><code id="emMixHMM_+3A_max_iter">max_iter</code></td>
<td>
<p>Optional. The maximum number of iterations for the EM
algorithm.</p>
</td></tr>
<tr><td><code id="emMixHMM_+3A_threshold">threshold</code></td>
<td>
<p>Optional. A numeric value specifying the threshold for the
relative difference of log-likelihood between two steps of the EM as
stopping criteria.</p>
</td></tr>
<tr><td><code id="emMixHMM_+3A_verbose">verbose</code></td>
<td>
<p>Optional. A logical value indicating whether or not values of
the log-likelihood should be printed during EM iterations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>emMixHMM function implements the EM algorithm. This function starts
with an initialization of the parameters done by the method <code>initParam</code> of
the class <a href="#topic+ParamMixHMM">ParamMixHMM</a>, then it alternates between the E-Step
(method of the class <a href="#topic+StatMixHMM">StatMixHMM</a>) and the M-Step (method of
the class <a href="#topic+ParamMixHMM">ParamMixHMM</a>) until convergence (until the relative
variation of log-likelihood between two steps of the EM algorithm is less
than the <code>threshold</code> parameter).
</p>


<h3>Value</h3>

<p>EM returns an object of class <a href="#topic+ModelMixHMM">ModelMixHMM</a>.
</p>


<h3>See Also</h3>

<p><a href="#topic+ModelMixHMM">ModelMixHMM</a>, <a href="#topic+ParamMixHMM">ParamMixHMM</a>, <a href="#topic+StatMixHMM">StatMixHMM</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(toydataset)
Y &lt;- t(toydataset[,2:ncol(toydataset)])

mixhmm &lt;- emMixHMM(Y = Y, K = 3, R = 3, verbose = TRUE)

mixhmm$summary()

mixhmm$plot()
</code></pre>

<hr>
<h2 id='emMixHMMR'>emMixHMMR implements the EM algorithm to fit a mixture if HMMR models.</h2><span id='topic+emMixHMMR'></span>

<h3>Description</h3>

<p>emMixHMMR implements the maximum-likelihood parameter estimation of a mixture
of HMMR models by the Expectation-Maximization (EM) algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emMixHMMR(X, Y, K, R, p = 3, variance_type = c("heteroskedastic",
  "homoskedastic"), order_constraint = TRUE, init_kmeans = TRUE,
  n_tries = 1, max_iter = 1000, threshold = 1e-06, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emMixHMMR_+3A_x">X</code></td>
<td>
<p>Numeric vector of length <em>m</em> representing the covariates/inputs
<code class="reqn">x_{1},\dots,x_{m}</code>.</p>
</td></tr>
<tr><td><code id="emMixHMMR_+3A_y">Y</code></td>
<td>
<p>Matrix of size <code class="reqn">(n, m)</code> representing the observed
responses/outputs. <code>Y</code> consists of <em>n</em> functions of <code>X</code> observed at
points <code class="reqn">1,\dots,m</code>.</p>
</td></tr>
<tr><td><code id="emMixHMMR_+3A_k">K</code></td>
<td>
<p>The number of clusters (Number of HMMR models).</p>
</td></tr>
<tr><td><code id="emMixHMMR_+3A_r">R</code></td>
<td>
<p>The number of regimes (HMMR components) for each cluster.</p>
</td></tr>
<tr><td><code id="emMixHMMR_+3A_p">p</code></td>
<td>
<p>Optional. The order of the polynomial regression. By default, <code>p</code> is
set at 3.</p>
</td></tr>
<tr><td><code id="emMixHMMR_+3A_variance_type">variance_type</code></td>
<td>
<p>Optional. character indicating if the model is
&quot;homoskedastic&quot; or &quot;heteroskedastic&quot;. By default the model is
&quot;heteroskedastic&quot;.</p>
</td></tr>
<tr><td><code id="emMixHMMR_+3A_order_constraint">order_constraint</code></td>
<td>
<p>Optional. A logical indicating whether or not a mask
of order one should be applied to the transition matrix of the Markov chain
to provide ordered states. For the purpose of segmentation, it must be set
to <code>TRUE</code> (which is the default value).</p>
</td></tr>
<tr><td><code id="emMixHMMR_+3A_init_kmeans">init_kmeans</code></td>
<td>
<p>Optional. A logical indicating whether or not the curve
partition should be initialized by the K-means algorithm. Otherwise the
curve partition is initialized randomly.</p>
</td></tr>
<tr><td><code id="emMixHMMR_+3A_n_tries">n_tries</code></td>
<td>
<p>Optional. Number of runs of the EM algorithm. The solution
providing the highest log-likelihood will be returned.
</p>
<p>If <code>n_tries</code> &gt; 1, then for the first run, parameters are initialized by
uniformly segmenting the data into K segments, and for the next runs,
parameters are initialized by randomly segmenting the data into K
contiguous segments.</p>
</td></tr>
<tr><td><code id="emMixHMMR_+3A_max_iter">max_iter</code></td>
<td>
<p>Optional. The maximum number of iterations for the EM
algorithm.</p>
</td></tr>
<tr><td><code id="emMixHMMR_+3A_threshold">threshold</code></td>
<td>
<p>Optional. A numeric value specifying the threshold for the
relative difference of log-likelihood between two steps of the EM as
stopping criteria.</p>
</td></tr>
<tr><td><code id="emMixHMMR_+3A_verbose">verbose</code></td>
<td>
<p>Optional. A logical value indicating whether or not values of
the log-likelihood should be printed during EM iterations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>emMixHMMR function implements the EM algorithm. This function starts
with an initialization of the parameters done by the method <code>initParam</code> of
the class <a href="#topic+ParamMixHMMR">ParamMixHMMR</a>, then it alternates between the
E-Step (method of the class <a href="#topic+StatMixHMMR">StatMixHMMR</a>) and the M-Step
(method of the class <a href="#topic+ParamMixHMMR">ParamMixHMMR</a>) until convergence (until
the relative variation of log-likelihood between two steps of the EM
algorithm is less than the <code>threshold</code> parameter).
</p>


<h3>Value</h3>

<p>EM returns an object of class <a href="#topic+ModelMixHMMR">ModelMixHMMR</a>.
</p>


<h3>See Also</h3>

<p><a href="#topic+ModelMixHMMR">ModelMixHMMR</a>, <a href="#topic+ParamMixHMMR">ParamMixHMMR</a>, <a href="#topic+StatMixHMMR">StatMixHMMR</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(toydataset)
x &lt;- toydataset$x
Y &lt;- t(toydataset[,2:ncol(toydataset)])

mixhmmr &lt;- emMixHMMR(X = x, Y = Y, K = 3, R = 3, p = 1, verbose = TRUE)

mixhmmr$summary()

mixhmmr$plot()
</code></pre>

<hr>
<h2 id='emMixRHLP'>emMixRHLP implements the EM algorithm to fit a mixture of RHLP models.</h2><span id='topic+emMixRHLP'></span>

<h3>Description</h3>

<p>emMixRHLP implements the maximum-likelihood parameter estimation of a mixture
of RHLP models by the Expectation-Maximization (EM) algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emMixRHLP(X, Y, K, R, p = 3, q = 1,
  variance_type = c("heteroskedastic", "homoskedastic"),
  init_kmeans = TRUE, n_tries = 1, max_iter = 1000,
  threshold = 1e-05, verbose = FALSE, verbose_IRLS = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emMixRHLP_+3A_x">X</code></td>
<td>
<p>Numeric vector of length <em>m</em> representing the covariates/inputs
<code class="reqn">x_{1},\dots,x_{m}</code>.</p>
</td></tr>
<tr><td><code id="emMixRHLP_+3A_y">Y</code></td>
<td>
<p>Matrix of size <code class="reqn">(n, m)</code> representing the observed
responses/outputs. <code>Y</code> consists of <em>n</em> functions of <code>X</code> observed at
points <code class="reqn">1,\dots,m</code>.</p>
</td></tr>
<tr><td><code id="emMixRHLP_+3A_k">K</code></td>
<td>
<p>The number of clusters (Number of RHLP models).</p>
</td></tr>
<tr><td><code id="emMixRHLP_+3A_r">R</code></td>
<td>
<p>The number of regimes (RHLP components) for each cluster.</p>
</td></tr>
<tr><td><code id="emMixRHLP_+3A_p">p</code></td>
<td>
<p>Optional. The order of the polynomial regression. By default, <code>p</code> is
set at 3.</p>
</td></tr>
<tr><td><code id="emMixRHLP_+3A_q">q</code></td>
<td>
<p>Optional. The dimension of the logistic regression. For the purpose
of segmentation, it must be set to 1 (which is the default value).</p>
</td></tr>
<tr><td><code id="emMixRHLP_+3A_variance_type">variance_type</code></td>
<td>
<p>Optional character indicating if the model is
&quot;homoskedastic&quot; or &quot;heteroskedastic&quot;. By default the model is
&quot;heteroskedastic&quot;.</p>
</td></tr>
<tr><td><code id="emMixRHLP_+3A_init_kmeans">init_kmeans</code></td>
<td>
<p>Optional. A logical indicating whether or not the curve
partition should be initialized by the K-means algorithm. Otherwise the
curve partition is initialized randomly.</p>
</td></tr>
<tr><td><code id="emMixRHLP_+3A_n_tries">n_tries</code></td>
<td>
<p>Optional. Number of runs of the EM algorithm. The solution
providing the highest log-likelihood will be returned.
</p>
<p>If <code>n_tries</code> &gt; 1, then for the first run, parameters are initialized by
uniformly segmenting the data into R segments, and for the next runs,
parameters are initialized by randomly segmenting the data into R
contiguous segments.</p>
</td></tr>
<tr><td><code id="emMixRHLP_+3A_max_iter">max_iter</code></td>
<td>
<p>Optional. The maximum number of iterations for the EM
algorithm.</p>
</td></tr>
<tr><td><code id="emMixRHLP_+3A_threshold">threshold</code></td>
<td>
<p>Optional. A numeric value specifying the threshold for the
relative difference of log-likelihood between two steps of the EM as
stopping criteria.</p>
</td></tr>
<tr><td><code id="emMixRHLP_+3A_verbose">verbose</code></td>
<td>
<p>Optional. A logical value indicating whether or not values of
the log-likelihood should be printed during EM iterations.</p>
</td></tr>
<tr><td><code id="emMixRHLP_+3A_verbose_irls">verbose_IRLS</code></td>
<td>
<p>Optional. A logical value indicating whether or not
values of the criterion optimized by IRLS should be printed at each step of
the EM algorithm.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>emMixRHLP function implements the EM algorithm. This function starts
with an initialization of the parameters done by the method <code>initParam</code> of
the class <a href="#topic+ParamMixRHLP">ParamMixRHLP</a>, then it alternates between the
E-Step (method of the class <a href="#topic+StatMixRHLP">StatMixRHLP</a>) and the M-Step
(method of the class <a href="#topic+ParamMixRHLP">ParamMixRHLP</a>) until convergence (until
the relative variation of log-likelihood between two steps of the EM
algorithm is less than the <code>threshold</code> parameter).
</p>


<h3>Value</h3>

<p>EM returns an object of class <a href="#topic+ModelMixRHLP">ModelMixRHLP</a>.
</p>


<h3>See Also</h3>

<p><a href="#topic+ModelMixRHLP">ModelMixRHLP</a>, <a href="#topic+ParamMixRHLP">ParamMixRHLP</a>, <a href="#topic+StatMixRHLP">StatMixRHLP</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(toydataset)

# Let's fit a mixRHLP model on a dataset containing 2 clusters:
data &lt;- toydataset[1:190,1:21]
x &lt;- data$x
Y &lt;- t(data[,2:ncol(data)])

mixrhlp &lt;- emMixRHLP(X = x, Y = Y, K = 2, R = 2, p = 1, verbose = TRUE)

mixrhlp$summary()

mixrhlp$plot()
</code></pre>

<hr>
<h2 id='FData-class'>A Reference Class which represents functional data.</h2><span id='topic+FData-class'></span><span id='topic+FData'></span>

<h3>Description</h3>

<p>FData is a reference class which represents general independent and
identically distributed (i.i.d.) functional objects. The data can be ordered
by time (functional time series). In the last case, the field <code>X</code> represents
the time.
</p>


<h3>Fields</h3>


<dl>
<dt><code>X</code></dt><dd><p>Numeric vector of length <em>m</em> representing the
covariates/inputs.</p>
</dd>
<dt><code>Y</code></dt><dd><p>Matrix of size <code class="reqn">(n, m)</code> representing the observed
responses/outputs. <code>Y</code> consists of <em>n</em> functions of <code>X</code> observed at
points <code class="reqn">1,\dots,m</code>.</p>
</dd>
</dl>

<hr>
<h2 id='flamingos-package'>FLaMingos: Functional Latent datA Models for clusterING heterogeneOus curveS</h2><span id='topic+flamingos'></span><span id='topic+flamingos-package'></span>

<h3>Description</h3>

<p><code>flamingos</code> is an open-source toolbox for the simultaneous
clustering (or classification) and segmentation of heterogeneous functional
data (i.e time-series ore more generally longitudinal data), with original
and flexible functional latent variable models, fitted by unsupervised
algorithms, including EM algorithms.
</p>
<p><code>flamingos</code> contains the following time series clustering and segmentation models:
</p>

<ul>
<li><p> mixRHLP;
</p>
</li>
<li><p> mixHMM;
</p>
</li>
<li><p> mixHMMR.
</p>
</li></ul>

<p>For the advantages/differences of each of them, the user is referred to our
mentioned paper references.
</p>
<p>To learn more about <code>flamingos</code>, start with the vignettes:
<code>browseVignettes(package = "flamingos")</code>
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Florian Lecocq <a href="mailto:florian.lecocq@outlook.com">florian.lecocq@outlook.com</a> (R port) [translator]
</p>
<p>Authors:
</p>

<ul>
<li><p> Faicel Chamroukhi <a href="mailto:faicel.chamroukhi@unicaen.fr">faicel.chamroukhi@unicaen.fr</a> (0000-0002-5894-3103)
</p>
</li>
<li><p> Marius Bartcus <a href="mailto:marius.bartcus@gmail.com">marius.bartcus@gmail.com</a> (R port) [translator]
</p>
</li></ul>



<h3>References</h3>

<p>Chamroukhi, Faicel, and Hien D. Nguyen. 2019. <em>Model-Based Clustering and Classification of Functional Data.</em> Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery. <a href="https://chamroukhi.com/papers/MBCC-FDA.pdf">https://chamroukhi.com/papers/MBCC-FDA.pdf</a>.
</p>
<p>Chamroukhi, F. 2016. <em>Unsupervised Learning of Regression Mixture Models with Unknown Number of Components.</em> Journal of Statistical Computation and Simulation 86 (November): 2308&ndash;34. <a href="https://chamroukhi.com/papers/Chamroukhi-JSCS-2015.pdf">https://chamroukhi.com/papers/Chamroukhi-JSCS-2015.pdf</a>.
</p>
<p>Chamroukhi, Faicel. 2016. <em>Piecewise Regression Mixture for Simultaneous Functional Data Clustering and Optimal Segmentation.</em> Journal of Classification 33 (3): 374&ndash;411. <a href="https://chamroukhi.com/papers/Chamroukhi-PWRM-JournalClassif-2016.pdf">https://chamroukhi.com/papers/Chamroukhi-PWRM-JournalClassif-2016.pdf</a>.
</p>
<p>Chamroukhi, F. 2015. <em>Statistical Learning of Latent Data Models for Complex Data Analysis.</em> Habilitation Thesis (HDR), Universite de Toulon. <a href="https://chamroukhi.com/Dossier/FChamroukhi-Habilitation.pdf">https://chamroukhi.com/Dossier/FChamroukhi-Habilitation.pdf</a>.
</p>
<p>Chamroukhi, F., H. Glotin, and A. Same. 2013. <em>Model-Based Functional Mixture Discriminant Analysis with Hidden Process Regression for Curve Classification.</em> Neurocomputing 112: 153&ndash;63. <a href="https://chamroukhi.com/papers/chamroukhi_et_al_neucomp2013a.pdf">https://chamroukhi.com/papers/chamroukhi_et_al_neucomp2013a.pdf</a>.
</p>
<p>Chamroukhi, F., and H. Glotin. 2012. <em>Mixture Model-Based Functional Discriminant Analysis for Curve Classification.</em> In Proceedings of the International Joint Conference on Neural Networks (IJCNN), IEEE, 1&ndash;8. Brisbane, Australia. <a href="https://chamroukhi.com/papers/Chamroukhi-ijcnn-2012.pdf">https://chamroukhi.com/papers/Chamroukhi-ijcnn-2012.pdf</a>.
</p>
<p>Same, A., F. Chamroukhi, Gerard Govaert, and P. Aknin. 2011. <em>Model-Based Clustering and Segmentation of Time Series with Changes in Regime.</em> Advances in Data Analysis and Classification 5 (4): 301&ndash;21. <a href="https://chamroukhi.com/papers/adac-2011.pdf">https://chamroukhi.com/papers/adac-2011.pdf</a>.
</p>
<p>Chamroukhi, F., A. Same, P. Aknin, and G. Govaert. 2011. <em>Model-Based Clustering with Hidden Markov Model Regression for Time Series with Regime Changes.</em> In Proceedings of the International Joint Conference on Neural Networks (IJCNN), IEEE, 2814&ndash;21. <a href="https://chamroukhi.com/papers/Chamroukhi-ijcnn-2011.pdf">https://chamroukhi.com/papers/Chamroukhi-ijcnn-2011.pdf</a>.
</p>
<p>Chamroukhi, F., A. Same, G. Govaert, and P. Aknin. 2010. <em>A Hidden Process Regression Model for Functional Data Description. Application to Curve Discrimination.</em> Neurocomputing 73 (7-9): 1210&ndash;21. <a href="https://chamroukhi.com/papers/chamroukhi_neucomp_2010.pdf">https://chamroukhi.com/papers/chamroukhi_neucomp_2010.pdf</a>.
</p>
<p>Chamroukhi, F. 2010. <em>Hidden Process Regression for Curve Modeling, Classification and Tracking.</em> Ph.D. Thesis, Universite de Technologie de Compiegne. <a href="https://chamroukhi.com/papers/FChamroukhi-Thesis.pdf">https://chamroukhi.com/papers/FChamroukhi-Thesis.pdf</a>.
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/fchamroukhi/FLaMingos">https://github.com/fchamroukhi/FLaMingos</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/fchamroukhi/FLaMingos/issues">https://github.com/fchamroukhi/FLaMingos/issues</a>
</p>
</li></ul>


<hr>
<h2 id='mkStochastic'>mkStochastic ensures that it is a stochastic vector, matrix or array.</h2><span id='topic+mkStochastic'></span>

<h3>Description</h3>

<p>mkStochastic ensures that it is a stochastic vector, matrix or array.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mkStochastic(M)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mkStochastic_+3A_m">M</code></td>
<td>
<p>A vector, matrix or array to transform.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>mkStochastic ensures that the giving argument is a stochastic
vector, matrix or array, i.e., that the sum over the last dimension is 1.
</p>


<h3>Value</h3>

<p>A vector, matrix or array for which the sum over the last dimension
is 1.
</p>

<hr>
<h2 id='ModelMixHMM-class'>A Reference Class which represents a fitted Mixture of HMM model.</h2><span id='topic+ModelMixHMM-class'></span><span id='topic+ModelMixHMM'></span>

<h3>Description</h3>

<p>ModelMixHMM represents an estimated mixture of HMM model.
</p>


<h3>Fields</h3>


<dl>
<dt><code>param</code></dt><dd><p>A <a href="#topic+ParamMixHMM">ParamMixHMM</a> object. It contains the estimated
values of the parameters.</p>
</dd>
<dt><code>stat</code></dt><dd><p>A <a href="#topic+StatMixHMM">StatMixHMM</a> object. It contains all the statistics
associated to the MixHMM model.</p>
</dd>
</dl>


<h3>Methods</h3>


<dl>
<dt><code>plot(what = c("clustered", "smoothed", "loglikelihood"), ...)</code></dt><dd><p>Plot method
</p>

<dl>
<dt><code>what</code></dt><dd><p>The type of graph requested:
</p>

<ul>
<li> <p><code>"clustered" = </code> Clustered curves (field
<code>klas</code> of class <a href="#topic+StatMixHMM">StatMixHMM</a>).
</p>
</li>
<li> <p><code>"smoothed" = </code> Smoothed signal (field
<code>smoothed</code> of class StatMixHMM).
</p>
</li>
<li> <p><code>"loglikelihood" = </code> Value of the log-likelihood for
each iteration (field <code>stored_loglik</code> of class
<a href="#topic+StatMixHMM">StatMixHMM</a>).
</p>
</li></ul>

</dd>
<dt><code>...</code></dt><dd><p>Other graphics parameters.</p>
</dd>
</dl>
</dd>
<dt><code>summary(digits = getOption("digits"))</code></dt><dd><p>Summary method.
</p>

<dl>
<dt><code>digits</code></dt><dd><p>The number of significant digits to use when
printing.</p>
</dd>
</dl>
</dd>
</dl>


<h3>See Also</h3>

<p><a href="#topic+ParamMixHMM">ParamMixHMM</a>, <a href="#topic+StatMixHMM">StatMixHMM</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(toydataset)
Y &lt;- t(toydataset[,2:ncol(toydataset)])

mixhmm &lt;- emMixHMM(Y = Y, K = 3, R = 3, verbose = TRUE)

# mixhmm is a ModelMixHMM object. It contains some methods such as 'summary' and 'plot'
mixhmm$summary()
mixhmm$plot()

# mixhmm has also two fields, stat and param which are reference classes as well

# Log-likelihood:
mixhmm$stat$loglik

# Means
mixhmm$param$mu
</code></pre>

<hr>
<h2 id='ModelMixHMMR-class'>A Reference Class which represents a fitted mixture of HMMR model.</h2><span id='topic+ModelMixHMMR-class'></span><span id='topic+ModelMixHMMR'></span>

<h3>Description</h3>

<p>ModelMixHMMR represents an estimated mixture of HMMR model.
</p>


<h3>Fields</h3>


<dl>
<dt><code>param</code></dt><dd><p>A <a href="#topic+ParamMixHMMR">ParamMixHMMR</a> object. It contains the estimated
values of the parameters.</p>
</dd>
<dt><code>stat</code></dt><dd><p>A <a href="#topic+StatMixHMMR">StatMixHMMR</a> object. It contains all the
statistics associated to the MixHMMR model.</p>
</dd>
</dl>


<h3>Methods</h3>


<dl>
<dt><code>plot(what = c("clustered", "smoothed", "loglikelihood"), ...)</code></dt><dd><p>Plot method
</p>

<dl>
<dt><code>what</code></dt><dd><p>The type of graph requested:
</p>

<ul>
<li> <p><code>"clustered" = </code> Clustered curves (field
<code>klas</code> of class <a href="#topic+StatMixHMMR">StatMixHMMR</a>).
</p>
</li>
<li> <p><code>"smoothed" = </code> Smoothed signal (field
<code>smoothed</code> of class StatMixHMMR).
</p>
</li>
<li> <p><code>"loglikelihood" = </code> Value of the log-likelihood for
each iteration (field <code>stored_loglik</code> of class
<a href="#topic+StatMixHMMR">StatMixHMMR</a>).
</p>
</li></ul>

</dd>
<dt><code>...</code></dt><dd><p>Other graphics parameters.</p>
</dd>
</dl>
</dd>
<dt><code>summary(digits = getOption("digits"))</code></dt><dd><p>Summary method.
</p>

<dl>
<dt><code>digits</code></dt><dd><p>The number of significant digits to use when
printing.</p>
</dd>
</dl>
</dd>
</dl>


<h3>See Also</h3>

<p><a href="#topic+ParamMixHMMR">ParamMixHMMR</a>, <a href="#topic+StatMixHMMR">StatMixHMMR</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(toydataset)
x &lt;- toydataset$x
Y &lt;- t(toydataset[,2:ncol(toydataset)])

mixhmmr &lt;- emMixHMMR(X = x, Y = Y, K = 3, R = 3, p = 1, verbose = TRUE)

# mixhmmr is a ModelMixHMMR object. It contains some methods such as 'summary' and 'plot'
mixhmmr$summary()
mixhmmr$plot()

# mixhmmr has also two fields, stat and param which are reference classes as well

# Log-likelihood:
mixhmmr$stat$loglik

# Parameters of the polynomial regressions:
mixhmmr$param$beta
</code></pre>

<hr>
<h2 id='ModelMixRHLP-class'>A Reference Class which represents a fitted mixture of RHLP model.</h2><span id='topic+ModelMixRHLP-class'></span><span id='topic+ModelMixRHLP'></span>

<h3>Description</h3>

<p>ModelMixRHLP represents an estimated mixture of RHLP model.
</p>


<h3>Fields</h3>


<dl>
<dt><code>param</code></dt><dd><p>A <a href="#topic+ParamMixRHLP">ParamMixRHLP</a> object. It contains the estimated
values of the parameters.</p>
</dd>
<dt><code>stat</code></dt><dd><p>A <a href="#topic+StatMixRHLP">StatMixRHLP</a> object. It contains all the
statistics associated to the MixRHLP model.</p>
</dd>
</dl>


<h3>Methods</h3>


<dl>
<dt><code>plot(what = c("estimatedsignal", "regressors", "loglikelihood"), ...)</code></dt><dd><p>Plot method.
</p>

<dl>
<dt><code>what</code></dt><dd><p>The type of graph requested:
</p>

<ul>
<li> <p><code>"estimatedsignal" = </code> Estimated signal (field
<code>Ey</code> of class <a href="#topic+StatMixRHLP">StatMixRHLP</a>).
</p>
</li>
<li> <p><code>"regressors" = </code> Polynomial regression components
(fields <code>polynomials</code> and <code>pi_jkr</code> of class
<a href="#topic+StatMixRHLP">StatMixRHLP</a>).
</p>
</li>
<li> <p><code>"loglikelihood" = </code> Value of the log-likelihood for
each iteration (field <code>stored_loglik</code> of class
<a href="#topic+StatMixRHLP">StatMixRHLP</a>).
</p>
</li></ul>

</dd>
<dt><code>...</code></dt><dd><p>Other graphics parameters.</p>
</dd>
</dl>

<p>By default, all the above graphs are produced.</p>
</dd>
<dt><code>summary(digits = getOption("digits"))</code></dt><dd><p>Summary method.
</p>

<dl>
<dt><code>digits</code></dt><dd><p>The number of significant digits to use when
printing.</p>
</dd>
</dl>
</dd>
</dl>


<h3>See Also</h3>

<p><a href="#topic+ParamMixRHLP">ParamMixRHLP</a>, <a href="#topic+StatMixRHLP">StatMixRHLP</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(toydataset)

# Let's fit a mixRHLP model on a dataset containing 2 clusters:
data &lt;- toydataset[1:190,1:21]
x &lt;- data$x
Y &lt;- t(data[,2:ncol(data)])

mixrhlp &lt;- cemMixRHLP(X = x, Y = Y, K = 2, R = 2, p = 1, verbose = TRUE)

# mixrhlp is a ModelMixRHLP object. It contains some methods such as 'summary' and 'plot'
mixrhlp$summary()
mixrhlp$plot()

# mixrhlp has also two fields, stat and param which are reference classes as well

# Log-likelihood:
mixrhlp$stat$loglik

# Parameters of the polynomial regressions:
mixrhlp$param$beta
</code></pre>

<hr>
<h2 id='ParamMixHMM-class'>A Reference Class which contains parameters of a mixture of HMM models.</h2><span id='topic+ParamMixHMM-class'></span><span id='topic+ParamMixHMM'></span>

<h3>Description</h3>

<p>ParamMixHMM contains all the parameters of a mixture of HMM models.
</p>


<h3>Fields</h3>


<dl>
<dt><code>fData</code></dt><dd><p><a href="#topic+FData">FData</a> object representing the sample (covariates/inputs
<code>X</code> and observed responses/outputs <code>Y</code>).</p>
</dd>
<dt><code>K</code></dt><dd><p>The number of clusters (Number of HMM models).</p>
</dd>
<dt><code>R</code></dt><dd><p>The number of regimes (HMM components) for each cluster.</p>
</dd>
<dt><code>variance_type</code></dt><dd><p>Character indicating if the model is homoskedastic
(<code>variance_type = "homoskedastic"</code>) or heteroskedastic (<code>variance_type = "heteroskedastic"</code>). By default the model is heteroskedastic.</p>
</dd>
<dt><code>order_constraint</code></dt><dd><p>A logical indicating whether or not a mask of order
one should be applied to the transition matrix of the Markov chain to
provide ordered states. For the purpose of segmentation, it must be set to
<code>TRUE</code> (which is the default value).</p>
</dd>
<dt><code>alpha</code></dt><dd><p>Cluster weights. Matrix of dimension <code class="reqn">(K, 1)</code>.</p>
</dd>
<dt><code>prior</code></dt><dd><p>The prior probabilities of the Markov chains. <code>prior</code> is a
matrix of dimension <code class="reqn">(R, K)</code>. The k-th column represents the prior
distribution of the Markov chain asociated to the cluster k.</p>
</dd>
<dt><code>trans_mat</code></dt><dd><p>The transition matrices of the Markov chains. <code>trans_mat</code> is
an array of dimension <code class="reqn">(R, R, K)</code>.</p>
</dd>
<dt><code>mask</code></dt><dd><p>Mask applied to the transition matrices <code>trans_mat</code>. By default,
a mask of order one is applied.</p>
</dd>
<dt><code>mu</code></dt><dd><p>Means. Matrix of dimension <code class="reqn">(R, K)</code>. The k-th column gives
represents the k-th cluster and gives the means for the <code>R</code> regimes.</p>
</dd>
<dt><code>sigma2</code></dt><dd><p>The variances for the <code>K</code> clusters. If MixHMM model is
heteroskedastic (<code>variance_type = "heteroskedastic"</code>) then <code>sigma2</code> is a
matrix of size <code class="reqn">(R, K)</code> (otherwise MixHMM model is homoskedastic
(<code>variance_type = "homoskedastic"</code>) and <code>sigma2</code> is a matrix of size
<code class="reqn">(1, K)</code>).</p>
</dd>
<dt><code>nu</code></dt><dd><p>The degrees of freedom of the MixHMM model representing the
complexity of the model.</p>
</dd>
</dl>


<h3>Methods</h3>


<dl>
<dt><code>initGaussParamHmm(Y, k, R, variance_type, try_algo)</code></dt><dd><p>Initialize the means <code>mu</code> and <code>sigma2</code> for the cluster
<code>k</code>.</p>
</dd>
<dt><code>initParam(init_kmeans = TRUE, try_algo = 1)</code></dt><dd><p>Method to initialize parameters <code>alpha</code>, <code>prior</code>, <code>trans_mat</code>,
<code>mu</code> and <code>sigma2</code>.
</p>
<p>If <code>init_kmeans = TRUE</code> then the curve partition is initialized by
the K-means algorithm. Otherwise the curve partition is initialized
randomly.
</p>
<p>If <code>try_algo = 1</code> then <code>mu</code> and <code>sigma2</code> are
initialized by segmenting  the time series <code>Y</code> uniformly into
<code>R</code> contiguous segments. Otherwise, <code>mu</code> and
<code>sigma2</code> are initialized by segmenting randomly the time series
<code>Y</code> into <code>R</code> segments.</p>
</dd>
<dt><code>MStep(statMixHMM)</code></dt><dd><p>Method which implements the M-step of the EM algorithm to learn the
parameters of the MixHMM model based on statistics provided by the object
<code>statMixHMM</code> of class <a href="#topic+StatMixHMM">StatMixHMM</a> (which contains the
E-step).</p>
</dd>
</dl>

<hr>
<h2 id='ParamMixHMMR-class'>A Reference Class which contains parameters of a mixture of HMMR models.</h2><span id='topic+ParamMixHMMR-class'></span><span id='topic+ParamMixHMMR'></span>

<h3>Description</h3>

<p>ParamMixHMMR contains all the parameters of a mixture of HMMR models.
</p>


<h3>Fields</h3>


<dl>
<dt><code>fData</code></dt><dd><p><a href="#topic+FData">FData</a> object representing the sample (covariates/inputs
<code>X</code> and observed responses/outputs <code>Y</code>).</p>
</dd>
<dt><code>K</code></dt><dd><p>The number of clusters (Number of HMMR models).</p>
</dd>
<dt><code>R</code></dt><dd><p>The number of regimes (HMMR components) for each cluster.</p>
</dd>
<dt><code>p</code></dt><dd><p>The order of the polynomial regression.</p>
</dd>
<dt><code>variance_type</code></dt><dd><p>Character indicating if the model is homoskedastic
(<code>variance_type = "homoskedastic"</code>) or heteroskedastic (<code>variance_type = "heteroskedastic"</code>). By default the model is heteroskedastic.</p>
</dd>
<dt><code>order_constraint</code></dt><dd><p>A logical indicating whether or not a mask of order
one should be applied to the transition matrix of the Markov chain to
provide ordered states. For the purpose of segmentation, it must be set to
<code>TRUE</code> (which is the default value).</p>
</dd>
<dt><code>alpha</code></dt><dd><p>Cluster weights. Matrix of dimension <code class="reqn">(K, 1)</code>.</p>
</dd>
<dt><code>prior</code></dt><dd><p>The prior probabilities of the Markov chains. <code>prior</code> is a
matrix of dimension <code class="reqn">(R, K)</code>. The k-th column represents the prior
distribution of the Markov chain asociated to the cluster k.</p>
</dd>
<dt><code>trans_mat</code></dt><dd><p>The transition matrices of the Markov chains. <code>trans_mat</code> is
an array of dimension <code class="reqn">(R, R, K)</code>.</p>
</dd>
<dt><code>mask</code></dt><dd><p>Mask applied to the transition matrices <code>trans_mat</code>. By default,
a mask of order one is applied.</p>
</dd>
<dt><code>beta</code></dt><dd><p>Parameters of the polynomial regressions. <code>beta</code> is an array of
dimension <code class="reqn">(p + 1, R, K)</code>, with <code>p</code> the order of the polynomial
regression. <code>p</code> is fixed to 3 by default.</p>
</dd>
<dt><code>sigma2</code></dt><dd><p>The variances for the <code>K</code> clusters. If MixHMMR model is
heteroskedastic (<code>variance_type = "heteroskedastic"</code>) then <code>sigma2</code> is a
matrix of size <code class="reqn">(R, K)</code> (otherwise MixHMMR model is homoskedastic
(<code>variance_type = "homoskedastic"</code>) and <code>sigma2</code> is a matrix of size</p>
</dd>
<dt><code>nu</code></dt><dd><p>The degree of freedom of the MixHMMR model representing the
complexity of the model.</p>
</dd>
<dt><code>phi</code></dt><dd><p>A list giving the regression design matrix for the polynomial regressions.</p>
</dd>
</dl>


<h3>Methods</h3>


<dl>
<dt><code>initParam(init_kmeans = TRUE, try_algo = 1)</code></dt><dd><p>Method to initialize parameters <code>alpha</code>, <code>prior</code>,
<code>trans_mat</code>, <code>beta</code> and <code>sigma2</code>.
</p>
<p>If <code>init_kmeans = TRUE</code> then the curve partition is initialized by
the K-means algorithm. Otherwise the curve partition is initialized
randomly.
</p>
<p>If <code>try_algo = 1</code> then <code>beta</code> and <code>sigma2</code> are
initialized by segmenting  the time series <code>Y</code> uniformly into
<code>R</code> contiguous segments. Otherwise, <code>beta</code> and
<code>sigma2</code> are initialized by segmenting randomly the time series
<code>Y</code> into <code>R</code> segments.</p>
</dd>
<dt><code>initRegressionParam(Y, k, R, phi, variance_type, try_algo)</code></dt><dd><p>Initialize <code>beta</code> and <code>sigma2</code> for the cluster <code>k</code>.</p>
</dd>
<dt><code>MStep(statMixHMMR)</code></dt><dd><p>Method which implements the M-step of the EM algorithm to learn the
parameters of the MixHMMR model based on statistics provided by the
object <code>statMixHMMR</code> of class <a href="#topic+StatMixHMMR">StatMixHMMR</a> (which contains
the E-step).</p>
</dd>
</dl>

<hr>
<h2 id='ParamMixRHLP-class'>A Reference Class which contains parameters of a mixture of RHLP models.</h2><span id='topic+ParamMixRHLP-class'></span><span id='topic+ParamMixRHLP'></span>

<h3>Description</h3>

<p>ParamMixRHLP contains all the parameters of a mixture of RHLP models.
</p>


<h3>Fields</h3>


<dl>
<dt><code>fData</code></dt><dd><p><a href="#topic+FData">FData</a> object representing the sample (covariates/inputs
<code>X</code> and observed responses/outputs <code>Y</code>).</p>
</dd>
<dt><code>K</code></dt><dd><p>The number of clusters (Number of RHLP models).</p>
</dd>
<dt><code>R</code></dt><dd><p>The number of regimes (RHLP components) for each cluster.</p>
</dd>
<dt><code>p</code></dt><dd><p>The order of the polynomial regression.</p>
</dd>
<dt><code>q</code></dt><dd><p>The dimension of the logistic regression. For the purpose of
segmentation, it must be set to 1.</p>
</dd>
<dt><code>variance_type</code></dt><dd><p>Character indicating if the model is homoskedastic
(<code>variance_type = "homoskedastic"</code>) or heteroskedastic (<code>variance_type = "heteroskedastic"</code>). By default the model is heteroskedastic.</p>
</dd>
<dt><code>alpha</code></dt><dd><p>Cluster weights. Matrix of dimension <code class="reqn">(1, K)</code>.</p>
</dd>
<dt><code>W</code></dt><dd><p>Parameters of the logistic process. <code class="reqn">\boldsymbol{W} =
  (\boldsymbol{w}_{1},\dots,\boldsymbol{w}_{K})</code> is
an array of dimension <code class="reqn">(q + 1, R - 1, K)</code>, with <code class="reqn">\boldsymbol{w}_{k}
  = (\boldsymbol{w}_{k,1},\dots,\boldsymbol{w}_{k,R-1})</code>, <code class="reqn">k = 1,\dots,K</code>, and <code>q</code> the order of the
logistic regression. <code>q</code> is fixed to 1 by default.</p>
</dd>
<dt><code>beta</code></dt><dd><p>Parameters of the polynomial regressions. <code class="reqn">\boldsymbol{\beta}
  = (\boldsymbol{\beta}_{1},\dots,\boldsymbol{\beta}_{K})</code> is an array of dimension <code class="reqn">(p + 1, R, K)</code>,
with <code class="reqn">\boldsymbol{\beta}_{k} =
  (\boldsymbol{\beta}_{k,1},\dots,\boldsymbol{\beta}_{k,R})</code>, <code class="reqn">k = 1,\dots,K</code>, <code>p</code> the order of the
polynomial regression. <code>p</code> is fixed to 3 by default.</p>
</dd>
<dt><code>sigma2</code></dt><dd><p>The variances for the <code>K</code> clusters. If MixRHLP model is
heteroskedastic (<code>variance_type = "heteroskedastic"</code>) then <code>sigma2</code> is a
matrix of size <code class="reqn">(R, K)</code> (otherwise MixRHLP model is homoskedastic
(<code>variance_type = "homoskedastic"</code>) and <code>sigma2</code> is a matrix of size
<code class="reqn">(K, 1)</code>).</p>
</dd>
<dt><code>nu</code></dt><dd><p>The degree of freedom of the MixRHLP model representing the
complexity of the model.</p>
</dd>
<dt><code>phi</code></dt><dd><p>A list giving the regression design matrices for the polynomial
and the logistic regressions.</p>
</dd>
</dl>


<h3>Methods</h3>


<dl>
<dt><code>CMStep(statMixRHLP, verbose_IRLS = FALSE)</code></dt><dd><p>Method which implements the M-step of the CEM algorithm to learn the
parameters of the MixRHLP model based on statistics provided by the
object <code>statMixRHLP</code> of class <a href="#topic+StatMixRHLP">StatMixRHLP</a> (which contains
the E-step and the C-step).</p>
</dd>
<dt><code>initParam(init_kmeans = TRUE, try_algo = 1)</code></dt><dd><p>Method to initialize parameters <code>alpha</code>, <code>W</code>, <code>beta</code>
and <code>sigma2</code>.
</p>
<p>If <code>init_kmeans = TRUE</code> then the curve partition is initialized by
the R-means algorithm. Otherwise the curve partition is initialized
randomly.
</p>
<p>If <code>try_algo = 1</code> then <code>beta</code> and <code>sigma2</code> are
initialized by segmenting  the time series <code>Y</code> uniformly into
<code>R</code> contiguous segments. Otherwise, <code>W</code>, <code>beta</code> and
<code>sigma2</code> are initialized by segmenting randomly the time series
<code>Y</code> into <code>R</code> segments.</p>
</dd>
<dt><code>initRegressionParam(Yk, k, try_algo = 1)</code></dt><dd><p>Initialize the matrix of polynomial regression coefficients beta_k for
the cluster <code>k</code>.</p>
</dd>
<dt><code>MStep(statMixRHLP, verbose_IRLS = FALSE)</code></dt><dd><p>Method which implements the M-step of the EM algorithm to learn the
parameters of the MixRHLP model based on statistics provided by the
object <code>statMixRHLP</code> of class <a href="#topic+StatMixRHLP">StatMixRHLP</a> (which contains
the E-step).</p>
</dd>
</dl>

<hr>
<h2 id='StatMixHMM-class'>A Reference Class which contains statistics of a mixture of HMM model.</h2><span id='topic+StatMixHMM-class'></span><span id='topic+StatMixHMM'></span>

<h3>Description</h3>

<p>StatMixHMM contains all the statistics associated to a <a href="#topic+ParamMixHMM">MixHMM</a>
model, in particular the E-Step of the EM algorithm.
</p>


<h3>Fields</h3>


<dl>
<dt><code>tau_ik</code></dt><dd><p>Matrix of size <code class="reqn">(n, K)</code> giving the posterior probabilities
that the curve <code class="reqn">\boldsymbol{y}_{i}</code> originates from the
<code class="reqn">k</code>-th HMM model.</p>
</dd>
<dt><code>gamma_ikjr</code></dt><dd><p>Array of size <code class="reqn">(nm, R, K)</code> giving the posterior
probabilities that the observation <code class="reqn">\boldsymbol{y}_{ij}</code>
originates from the <code class="reqn">r</code>-th regime of the <code class="reqn">k</code>-th HMM model.</p>
</dd>
<dt><code>loglik</code></dt><dd><p>Numeric. Log-likelihood of the MixHMM model.</p>
</dd>
<dt><code>stored_loglik</code></dt><dd><p>Numeric vector. Stored values of the log-likelihood at
each iteration of the EM algorithm.</p>
</dd>
<dt><code>klas</code></dt><dd><p>Row matrix of the labels issued from <code>tau_ik</code>. Its elements are
<code class="reqn">klas[i] = z\_i</code>, <code class="reqn">i = 1,\dots,n</code>.</p>
</dd>
<dt><code>z_ik</code></dt><dd><p>Hard segmentation logical matrix of dimension <code class="reqn">(n, K)</code>
obtained by the Maximum a posteriori (MAP) rule: <code class="reqn">z\_ik = 1 \
  \textrm{if} \ z\_i = \textrm{arg} \ \textrm{max}_{k} \ P(z_{ik} = 1 |
  \boldsymbol{y}_{i}; \boldsymbol{\Psi}) = tau\_tk;\ 0 \
  \textrm{otherwise}</code>.</p>
</dd>
<dt><code>smoothed</code></dt><dd><p>Matrix of size <code class="reqn">(m, K)</code> giving the smoothed time series.
The smoothed time series are computed by combining the time series
<code class="reqn">\boldsymbol{y}_{i}</code> with both the estimated posterior regime
probabilities <code>gamma_ikjr</code> and the corresponding estimated posterior
cluster probability <code>tau_ik</code>. The k-th column gives the estimated mean
series of cluster k.</p>
</dd>
<dt><code>BIC</code></dt><dd><p>Numeric. Value of BIC (Bayesian Information Criterion).</p>
</dd>
<dt><code>AIC</code></dt><dd><p>Numeric. Value of AIC (Akaike Information Criterion).</p>
</dd>
<dt><code>ICL1</code></dt><dd><p>Numeric. Value of ICL (Integrated Completed Likelihood
Criterion).</p>
</dd>
<dt><code>log_alpha_k_fyi</code></dt><dd><p>Private. Only defined for calculations.</p>
</dd>
<dt><code>exp_num_trans</code></dt><dd><p>Private. Only defined for calculations.</p>
</dd>
<dt><code>exp_num_trans_from_l</code></dt><dd><p>Private. Only defined for calculations.</p>
</dd>
</dl>


<h3>Methods</h3>


<dl>
<dt><code>computeStats(paramMixHMM)</code></dt><dd><p>Method used in the EM algorithm to compute statistics based on
parameters provided by the object <code>paramMixHMM</code> of class
<a href="#topic+ParamMixHMM">ParamMixHMM</a>.</p>
</dd>
<dt><code>EStep(paramMixHMM)</code></dt><dd><p>Method used in the EM algorithm to update statistics based on parameters
provided by the object <code>paramMixHMM</code> of class <a href="#topic+ParamMixHMM">ParamMixHMM</a>
(prior and posterior probabilities).</p>
</dd>
<dt><code>MAP()</code></dt><dd><p>MAP calculates values of the fields <code>z_ik</code> and <code>klas</code>
by applying the Maximum A Posteriori Bayes allocation rule.
</p>
<p><code class="reqn">z\_ik = 1 \ \textrm{if} \ z\_i = \textrm{arg} \
      \textrm{max}_{k} \ P(z_{ik} = 1 | \boldsymbol{y}_{i};
      \boldsymbol{\Psi}) = tau\_tk;\ 0 \ \textrm{otherwise}</code>.</p>
</dd>
</dl>


<h3>See Also</h3>

<p><a href="#topic+ParamMixHMM">ParamMixHMM</a>
</p>

<hr>
<h2 id='StatMixHMMR-class'>A Reference Class which contains statistics of a mixture of HMMR models.</h2><span id='topic+StatMixHMMR-class'></span><span id='topic+StatMixHMMR'></span>

<h3>Description</h3>

<p>StatMixHMMR contains all the statistics associated to a
<a href="#topic+ParamMixHMMR">MixHMMR</a> model, in particular the E-Step of the EM algorithm.
</p>


<h3>Fields</h3>


<dl>
<dt><code>tau_ik</code></dt><dd><p>Matrix of size <code class="reqn">(n, K)</code> giving the posterior probabilities
that the curve <code class="reqn">\boldsymbol{y}_{i}</code> originates from the
<code class="reqn">k</code>-th HMMR model.</p>
</dd>
<dt><code>gamma_ikjr</code></dt><dd><p>Array of size <code class="reqn">(nm, R, K)</code> giving the posterior
probabilities that the observation <code class="reqn">\boldsymbol{y}_{ij}</code>
originates from the <code class="reqn">r</code>-th regime of the <code class="reqn">k</code>-th HMM model.</p>
</dd>
<dt><code>loglik</code></dt><dd><p>Numeric. Log-likelihood of the MixHMMR model.</p>
</dd>
<dt><code>stored_loglik</code></dt><dd><p>Numeric vector. Stored values of the log-likelihood at
each iteration of the EM algorithm.</p>
</dd>
<dt><code>klas</code></dt><dd><p>Row matrix of the labels issued from <code>tau_ik</code>. Its elements are
<code class="reqn">klas[i] = z\_i</code>, <code class="reqn">i = 1,\dots,n</code>.</p>
</dd>
<dt><code>z_ik</code></dt><dd><p>Hard segmentation logical matrix of dimension <code class="reqn">(n, K)</code>
obtained by the Maximum a posteriori (MAP) rule: <code class="reqn">z\_ik = 1 \
  \textrm{if} \ z\_i = \textrm{arg} \ \textrm{max}_{k} \ P(z_{ik} = 1 |
  \boldsymbol{y}_{i}; \boldsymbol{\Psi}) = tau\_ik;\ 0 \
  \textrm{otherwise}</code>.</p>
</dd>
<dt><code>smoothed</code></dt><dd><p>Matrix of size <code class="reqn">(m, K)</code> giving the smoothed time series.
The smoothed time series are computed by combining the polynomial
regression components with both the estimated posterior regime
probabilities <code>gamma_ikjr</code> and the corresponding estimated posterior
cluster probability <code>tau_ik</code>. The k-th column gives the estimated mean
series of cluster k.</p>
</dd>
<dt><code>BIC</code></dt><dd><p>Numeric. Value of BIC (Bayesian Information Criterion).</p>
</dd>
<dt><code>AIC</code></dt><dd><p>Numeric. Value of AIC (Akaike Information Criterion).</p>
</dd>
<dt><code>ICL1</code></dt><dd><p>Numeric. Value of ICL (Integrated Completed Likelihood
Criterion).</p>
</dd>
<dt><code>log_alpha_k_fyi</code></dt><dd><p>Private. Only defined for calculations.</p>
</dd>
<dt><code>exp_num_trans</code></dt><dd><p>Private. Only defined for calculations.</p>
</dd>
<dt><code>exp_num_trans_from_l</code></dt><dd><p>Private. Only defined for calculations.</p>
</dd>
</dl>


<h3>Methods</h3>


<dl>
<dt><code>computeStats(paramMixHMMR)</code></dt><dd><p>Method used in the EM algorithm to compute statistics based on
parameters provided by the object <code>paramMixHMMR</code> of class
<a href="#topic+ParamMixHMMR">ParamMixHMMR</a>.</p>
</dd>
<dt><code>EStep(paramMixHMMR)</code></dt><dd><p>Method used in the EM algorithm to update statistics based on parameters
provided by the object <code>paramMixHMMR</code> of class <a href="#topic+ParamMixHMMR">ParamMixHMMR</a>
(prior and posterior probabilities).</p>
</dd>
<dt><code>MAP()</code></dt><dd><p>MAP calculates values of the fields <code>z_ik</code> and <code>klas</code>
by applying the Maximum A Posteriori Bayes allocation rule.
</p>
<p><code class="reqn">z\_ik = 1 \ \textrm{if} \ z\_i = \textrm{arg} \
      \textrm{max}_{k} \ P(z_{ik} = 1 | \boldsymbol{y}_{i};
      \boldsymbol{\Psi}) = tau\_ik;\ 0 \ \textrm{otherwise}</code>.</p>
</dd>
</dl>


<h3>See Also</h3>

<p><a href="#topic+ParamMixHMMR">ParamMixHMMR</a>
</p>

<hr>
<h2 id='StatMixRHLP-class'>A Reference Class which contains statistics of a mixture of RHLP models.</h2><span id='topic+StatMixRHLP-class'></span><span id='topic+StatMixRHLP'></span>

<h3>Description</h3>

<p>StatMixRHLP contains all the statistics associated to a
<a href="#topic+ParamMixRHLP">MixRHLP</a> model, in particular the E-Step (and C-Step) of the
(C)EM algorithm.
</p>


<h3>Fields</h3>


<dl>
<dt><code>pi_jkr</code></dt><dd><p>Array of size <code class="reqn">(nm, R, K)</code> representing the logistic
proportion for cluster k.</p>
</dd>
<dt><code>tau_ik</code></dt><dd><p>Matrix of size <code class="reqn">(n, K)</code> giving the posterior probabilities
(fuzzy segmentation matrix) that the curve <code class="reqn">\boldsymbol{y}_{i}</code>
originates from the <code class="reqn">k</code>-th RHLP model.</p>
</dd>
<dt><code>z_ik</code></dt><dd><p>Hard segmentation logical matrix of dimension <code class="reqn">(n, K)</code>
obtained by the Maximum a posteriori (MAP) rule: <code class="reqn">z\_ik = 1 \
  \textrm{if} \ z\_i = \textrm{arg} \ \textrm{max}_{k} \ tau\_ik;\ 0 \
  \textrm{otherwise}</code>.</p>
</dd>
<dt><code>klas</code></dt><dd><p>Column matrix of the labels issued from <code>z_ik</code>. Its elements are
<code class="reqn">klas[i] = z\_i</code>, <code class="reqn">i = 1,\dots,n</code>.</p>
</dd>
<dt><code>gamma_ijkr</code></dt><dd><p>Array of size <code class="reqn">(nm, R, K)</code> giving the posterior
probabilities that the observation <code class="reqn">\boldsymbol{y}_{ij}</code>
originates from the <code class="reqn">r</code>-th regime of the <code class="reqn">k</code>-th RHLP model.</p>
</dd>
<dt><code>polynomials</code></dt><dd><p>Array of size <code class="reqn">(m, R, K)</code> giving the values of the
estimated polynomial regression components.</p>
</dd>
<dt><code>weighted_polynomials</code></dt><dd><p>Array of size <code class="reqn">(m, R, K)</code> giving the values
of the estimated polynomial regression components weighted by the prior
probabilities <code>pi_jkr</code>.</p>
</dd>
<dt><code>Ey</code></dt><dd><p>Matrix of size <em>(m, K)</em>. <code>Ey</code> is the curve expectation
(estimated signal): sum of the polynomial components weighted by the
logistic probabilities <code>pi_jkr</code>.</p>
</dd>
<dt><code>loglik</code></dt><dd><p>Numeric. Observed-data log-likelihood of the MixRHLP model.</p>
</dd>
<dt><code>com_loglik</code></dt><dd><p>Numeric. Complete-data log-likelihood of the MixRHLP model.</p>
</dd>
<dt><code>stored_loglik</code></dt><dd><p>Numeric vector. Stored values of the log-likelihood at
each EM iteration.</p>
</dd>
<dt><code>stored_com_loglik</code></dt><dd><p>Numeric vector. Stored values of the Complete
log-likelihood at each EM iteration.</p>
</dd>
<dt><code>BIC</code></dt><dd><p>Numeric. Value of BIC (Bayesian Information Criterion).</p>
</dd>
<dt><code>ICL</code></dt><dd><p>Numeric. Value of ICL (Integrated Completed Likelihood).</p>
</dd>
<dt><code>AIC</code></dt><dd><p>Numeric. Value of AIC (Akaike Information Criterion).</p>
</dd>
<dt><code>log_fk_yij</code></dt><dd><p>Matrix of size <code class="reqn">(n, K)</code> giving the values of the
probability density function <code class="reqn">f(\boldsymbol{y}_{i} | z_i = k,
  \boldsymbol{x}, \boldsymbol{\Psi})</code>, <code class="reqn">i =
  1,\dots,n</code>.</p>
</dd>
<dt><code>log_alphak_fk_yij</code></dt><dd><p>Matrix of size <code class="reqn">(n, K)</code> giving the values of the
logarithm of the joint probability density function
<code class="reqn">f(\boldsymbol{y}_{i}, \ z_{i} = k | \boldsymbol{x},
  \boldsymbol{\Psi})</code>, <code class="reqn">i = 1,\dots,n</code>.</p>
</dd>
<dt><code>log_gamma_ijkr</code></dt><dd><p>Array of size <code class="reqn">(nm, R, K)</code> giving the logarithm of
<code>gamma_ijkr</code>.</p>
</dd>
</dl>


<h3>Methods</h3>


<dl>
<dt><code>computeStats(paramMixRHLP)</code></dt><dd><p>Method used in the EM algorithm to compute statistics based on
parameters provided by the object <code>paramMixRHLP</code> of class
<a href="#topic+ParamMixRHLP">ParamMixRHLP</a>.</p>
</dd>
<dt><code>CStep(reg_irls)</code></dt><dd><p>Method used in the CEM algorithm to update statistics.</p>
</dd>
<dt><code>EStep(paramMixRHLP)</code></dt><dd><p>Method used in the EM algorithm to update statistics based on parameters
provided by the object <code>paramMixRHLP</code> of class <a href="#topic+ParamMixRHLP">ParamMixRHLP</a>
(prior and posterior probabilities).</p>
</dd>
<dt><code>MAP()</code></dt><dd><p>MAP calculates values of the fields <code>z_ik</code> and <code>klas</code>
by applying the Maximum A Posteriori Bayes allocation rule.
</p>
<p><code class="reqn">z\_ik = 1 \ \textrm{if} \ z\_i = \textrm{arg} \
      \textrm{max}_{k} \ tau\_ik;\ 0 \ \textrm{otherwise}
      </code>.</p>
</dd>
</dl>


<h3>See Also</h3>

<p><a href="#topic+ParamMixRHLP">ParamMixRHLP</a>
</p>

<hr>
<h2 id='toydataset'>A dataset composed of simulated time series with regime changes.</h2><span id='topic+toydataset'></span>

<h3>Description</h3>

<p>A dataset composed of 30 simulated time series with regime changes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>toydataset
</code></pre>


<h3>Format</h3>

<p>A data frame with 350 rows and 31 variables:
</p>

<dl>
<dt>x</dt><dd><p>The covariate variable which is the time in that case.</p>
</dd>
<dt>y1</dt><dd><p>Times series with a wave form shape and for which a normally
distributed random noise has been added.</p>
</dd>
<dt>y2</dt><dd><p>Same as <code>y1</code>.</p>
</dd>
<dt>y3</dt><dd><p>Same as <code>y1</code>.</p>
</dd>
<dt>y4</dt><dd><p>Same as <code>y1</code>.</p>
</dd>
<dt>y5</dt><dd><p>Same as <code>y1</code>.</p>
</dd>
<dt>y6</dt><dd><p>Same as <code>y1</code>.</p>
</dd>
<dt>y7</dt><dd><p>Same as <code>y1</code>.</p>
</dd>
<dt>y8</dt><dd><p>Same as <code>y1</code>.</p>
</dd>
<dt>y9</dt><dd><p>Same as <code>y1</code>.</p>
</dd>
<dt>y10</dt><dd><p>Same as <code>y1</code>.</p>
</dd>
<dt>y11</dt><dd><p>Time series generated as follows:
</p>

<ul>
<li><p>  First regime: 120 values of Normally distributed random numbers
with mean 5 and variance 1.
</p>
</li>
<li><p> Second regime: 70 values of Normally distributed random numbers
with mean 7 and variance 1.
</p>
</li>
<li><p> Third regime: 160 values of Normally distributed random numbers
with mean 5 variance 1.
</p>
</li></ul>

</dd>
<dt>y12</dt><dd><p>Same as <code>y11</code>.</p>
</dd>
<dt>y13</dt><dd><p>Same as <code>y11</code>.</p>
</dd>
<dt>y14</dt><dd><p>Same as <code>y11</code>.</p>
</dd>
<dt>y15</dt><dd><p>Same as <code>y11</code>.</p>
</dd>
<dt>y16</dt><dd><p>Same as <code>y11</code>.</p>
</dd>
<dt>y17</dt><dd><p>Same as <code>y11</code>.</p>
</dd>
<dt>y18</dt><dd><p>Same as <code>y11</code>.</p>
</dd>
<dt>y19</dt><dd><p>Same as <code>y11</code>.</p>
</dd>
<dt>y20</dt><dd><p>Same as <code>y11</code>.</p>
</dd>
<dt>y21</dt><dd><p>Time series generated as follows:
</p>

<ul>
<li><p>  First regime: 80 values of Normally distributed random numbers
with mean 7 variance 1.
</p>
</li>
<li><p> Second regime: 130 values of Normally distributed random numbers
with mean 5 variance 1.
</p>
</li>
<li><p> Third regime: 140 values of Normally distributed random numbers
with mean 4 variance 1.
</p>
</li></ul>

</dd>
<dt>y22</dt><dd><p>Same as <code>y21</code>.</p>
</dd>
<dt>y23</dt><dd><p>Same as <code>y21</code>.</p>
</dd>
<dt>y24</dt><dd><p>Same as <code>y21</code>.</p>
</dd>
<dt>y25</dt><dd><p>Same as <code>y21</code>.</p>
</dd>
<dt>y26</dt><dd><p>Same as <code>y21</code>.</p>
</dd>
<dt>y27</dt><dd><p>Same as <code>y21</code>.</p>
</dd>
<dt>y28</dt><dd><p>Same as <code>y21</code>.</p>
</dd>
<dt>y29</dt><dd><p>Same as <code>y21</code>.</p>
</dd>
<dt>y30</dt><dd><p>Same as <code>y21</code>.</p>
</dd>
</dl>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
