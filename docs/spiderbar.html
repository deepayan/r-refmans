<!DOCTYPE html><html><head><title>Help for package spiderbar</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {spiderbar}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#can_fetch'><p>Test URL paths against a <code>robxp</code> <code>robots.txt</code> object</p></a></li>
<li><a href='#crawl_delays'><p>Retrieve all agent crawl delay values in a <code>robxp</code> <code>robots.txt</code> object</p></a></li>
<li><a href='#print.robxp'><p>Custom printer for 'robxp&ldquo; objects</p></a></li>
<li><a href='#robxp'><p>Parse a 'robots.txt' file &amp; create a 'robxp' object</p></a></li>
<li><a href='#sitemaps'><p>Retrieve a character vector of sitemaps from a parsed robots.txt object</p></a></li>
<li><a href='#spiderbar'><p>Parse and Test Robots Exclusion Protocol Files and Rules</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Parse and Test Robots Exclusion Protocol Files and Rules</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-02-07</td>
</tr>
<tr>
<td>Author:</td>
<td>Bob Rudis (bob@rud.is) [aut, cre], SEOmoz, Inc [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Bob Rudis &lt;bob@rud.is&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>The 'Robots Exclusion Protocol' <a href="https://www.robotstxt.org/orig.html">https://www.robotstxt.org/orig.html</a> documents
    a set of standards for allowing or excluding robot/spider crawling of different areas of
    site content. Tools are provided which wrap The 'rep-cpp' <a href="https://github.com/seomoz/rep-cpp">https://github.com/seomoz/rep-cpp</a>
    C++ library for processing these 'robots.txt' files.</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/hrbrmstr/spiderbar">https://github.com/hrbrmstr/spiderbar</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/hrbrmstr/spiderbar/issues">https://github.com/hrbrmstr/spiderbar/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, robotstxt, tinytest</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.0)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-02-09 16:08:55 UTC; hrbrmstr</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-02-11 10:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='can_fetch'>Test URL paths against a <code>robxp</code> <code>robots.txt</code> object</h2><span id='topic+can_fetch'></span>

<h3>Description</h3>

<p>Provide a character vector of URL paths plus optional user agent and this function will
return a logical vector indicating whether you have permission to fetch the content
at the respective path.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>can_fetch(obj, path = "/", user_agent = "*")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="can_fetch_+3A_obj">obj</code></td>
<td>
<p><code>robxp</code> object</p>
</td></tr>
<tr><td><code id="can_fetch_+3A_path">path</code></td>
<td>
<p>path to test</p>
</td></tr>
<tr><td><code id="can_fetch_+3A_user_agent">user_agent</code></td>
<td>
<p>user agent to test</p>
</td></tr>
</table>


<h3>Value</h3>

<p>logical vector indicating whether you have permission to fetch the content
</p>


<h3>Examples</h3>

<pre><code class='language-R'>gh &lt;- paste0(readLines(system.file("extdata", "github-robots.txt",
             package="spiderbar")), collapse="\n")
gh_rt &lt;- robxp(gh)

can_fetch(gh_rt, "/humans.txt", "*") # TRUE
can_fetch(gh_rt, "/login", "*") # FALSE
can_fetch(gh_rt, "/oembed", "CCBot") # FALSE

can_fetch(gh_rt, c("/humans.txt", "/login", "/oembed"))
</code></pre>

<hr>
<h2 id='crawl_delays'>Retrieve all agent crawl delay values in a <code>robxp</code> <code>robots.txt</code> object</h2><span id='topic+crawl_delays'></span>

<h3>Description</h3>

<p>Retrieve all agent crawl delay values in a <code>robxp</code> <code>robots.txt</code> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crawl_delays(obj)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crawl_delays_+3A_obj">obj</code></td>
<td>
<p><code>robxp</code> object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data frame of agents and their crawl delays
</p>


<h3>Note</h3>

<p><code>-1</code> will be returned for any listed agent <em>without</em> a crawl delay setting
</p>


<h3>Examples</h3>

<pre><code class='language-R'>gh &lt;- paste0(readLines(system.file("extdata", "github-robots.txt",
             package="spiderbar")), collapse="\n")
gh_rt &lt;- robxp(gh)
crawl_delays(gh_rt)

imdb &lt;- paste0(readLines(system.file("extdata", "imdb-robots.txt",
               package="spiderbar")), collapse="\n")
imdb_rt &lt;- robxp(imdb)
crawl_delays(imdb_rt)
</code></pre>

<hr>
<h2 id='print.robxp'>Custom printer for 'robxp&ldquo; objects</h2><span id='topic+print.robxp'></span>

<h3>Description</h3>

<p>Custom printer for 'robxp&ldquo; objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'robxp'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.robxp_+3A_x">x</code></td>
<td>
<p>object to print</p>
</td></tr>
<tr><td><code id="print.robxp_+3A_...">...</code></td>
<td>
<p>unused</p>
</td></tr>
</table>

<hr>
<h2 id='robxp'>Parse a 'robots.txt' file &amp; create a 'robxp' object</h2><span id='topic+robxp'></span>

<h3>Description</h3>

<p>This function takes in a single element character vector and parses it into
a 'robxp' object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>robxp(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="robxp_+3A_x">x</code></td>
<td>
<p>either an atomic character vector containing a complete 'robots.txt&ldquo; file
_or_ a length &gt;1 character vector that will concatenated into a single string _or_
a 'connection' object that will be passed to [readLines()], the result of which
will be concatenated into a single string and parsed and the connection will be closed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a classed object holding an external pointer to parsed robots.txt data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>imdb &lt;- paste0(readLines(system.file("extdata", "imdb-robots.txt",
               package="spiderbar")), collapse="\n")
rt &lt;- robxp(imdb)
</code></pre>

<hr>
<h2 id='sitemaps'>Retrieve a character vector of sitemaps from a parsed robots.txt object</h2><span id='topic+sitemaps'></span>

<h3>Description</h3>

<p>Retrieve a character vector of sitemaps from a parsed robots.txt object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sitemaps(xp)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sitemaps_+3A_xp">xp</code></td>
<td>
<p>A <code>robxp</code> object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>charcter vector of all sitemaps found in the parsed <code>robots.txt</code> file
</p>


<h3>Examples</h3>

<pre><code class='language-R'>imdb &lt;- paste0(readLines(system.file("extdata", "imdb-robots.txt",
               package="rep")), collapse="\n")
rt &lt;- robxp(imdb)
sitemaps(rt)
</code></pre>

<hr>
<h2 id='spiderbar'>Parse and Test Robots Exclusion Protocol Files and Rules</h2><span id='topic+spiderbar'></span>

<h3>Description</h3>

<p>The 'Robots Exclusion Protocol' (<a href="https://www.robotstxt.org/orig.html">https://www.robotstxt.org/orig.html</a>) documents a set
of standards for allowing or excluding robot/spider crawling of different areas of
site content. Tools are provided which wrap The <code>rep-cpp</code> <a href="https://github.com/seomoz/rep-cpp">https://github.com/seomoz/rep-cpp</a>
C++ library for processing these 'robots.txt&ldquo; files.
</p>


<h3>Author(s)</h3>

<p>Bob Rudis (bob@rud.is)
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
