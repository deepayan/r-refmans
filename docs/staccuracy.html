<!DOCTYPE html><html lang="en-US"><head><title>Help for package staccuracy</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {staccuracy}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#staccuracy-package'><p>Standardized Accuracy and Other Model Performance Metrics</p></a></li>
<li><a href='#aucroc'><p>Area under the ROC curve</p></a></li>
<li><a href='#mae'><p>Regression error and deviation measures</p></a></li>
<li><a href='#reg_aucroc'><p>Area under the ROC curve for regression target outcomes</p></a></li>
<li><a href='#sa_diff'><p>Statistical tests for the differences between standardized accuracies (staccuracies)</p></a></li>
<li><a href='#staccuracy'><p>Standardized accuracy (staccuracy) functions.</p></a></li>
<li><a href='#winsorize'><p>Winsorize a numeric vector</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Standardized Accuracy and Other Model Performance Metrics</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.2</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>Description:</td>
<td>Standardized accuracy (staccuracy) is a framework for expressing accuracy scores such that 50% represents a reference level of performance and 100% is a perfect prediction. The 'staccuracy' package provides tools for creating staccuracy functions as well as some recommended staccuracy measures. It also provides functions for some classic performance metrics such as mean absolute error (MAE), root mean squared error (RMSE), and area under the receiver operating characteristic curve (AUCROC), as well as their winsorized versions when applicable.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>autogam, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>cli, dplyr, methods, purrr, rlang, stringr, tidyr</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.2.0)</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/tripartio/staccuracy">https://github.com/tripartio/staccuracy</a>,
<a href="https://tripartio.github.io/staccuracy/">https://tripartio.github.io/staccuracy/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/tripartio/staccuracy/issues">https://github.com/tripartio/staccuracy/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-23 14:24:04 UTC; chitu</td>
</tr>
<tr>
<td>Author:</td>
<td>Chitu Okoli <a href="https://orcid.org/0000-0001-5574-7572"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Chitu Okoli &lt;Chitu.Okoli@skema.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-23 14:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='staccuracy-package'>Standardized Accuracy and Other Model Performance Metrics</h2><span id='topic+staccuracy-package'></span>

<h3>Description</h3>

<p>Standardized accuracy (staccuracy) is a framework for expressing accuracy scores such that 50% represents a reference level of performance and 100% is perfect prediction. The 'staccuracy' package provides tools for creating staccuracy functions as well as some recommended staccuracy measures. It also provides functions for some classic performance metrics such as mean absolute error (MAE), root mean squared error (RMSE), and area under the receiver operating characteristic curve (AUCROC), as well as their winsorized versions when applicable.
</p>


<h3>Author(s)</h3>

<p>Chitu Okoli <a href="mailto:Chitu.Okoli@skema.edu">Chitu.Okoli@skema.edu</a>
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/tripartio/staccuracy">https://github.com/tripartio/staccuracy</a>
</p>
</li>
<li> <p><a href="https://tripartio.github.io/staccuracy/">https://tripartio.github.io/staccuracy/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/tripartio/staccuracy/issues">https://github.com/tripartio/staccuracy/issues</a>
</p>
</li></ul>


<hr>
<h2 id='aucroc'>Area under the ROC curve</h2><span id='topic+aucroc'></span>

<h3>Description</h3>

<p>Returns the area under the ROC curve based on comparing the predicted scores to the actual binary values. Tied predictions are handled by calculating the optimistic AUC (positive cases sorted first, resulting in higher AUC) and the pessimistic AUC (positive cases sorted last, resulting in lower AUC) and then returning the average of the two. For the ROC, a &quot;tie&quot; means at least one pair of <code>pred</code> predictions whose value is identical yet their corresponding values of <code>actual</code> are different. (If the value of <code>actual</code> are the same for identical predictions, then these are unproblematic and are not considered &quot;ties&quot;.)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aucroc(
  actual,
  pred,
  na.rm = FALSE,
  positive = NULL,
  sample_size = 10000,
  seed = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="aucroc_+3A_actual">actual</code></td>
<td>
<p>any atomic vector. Actual label values from a dataset. They must be binary; that is, there must be exactly two distinct values (other than missing values, which are allowed). The &quot;true&quot; or &quot;positive&quot; class is determined by coercing <code>actual</code> to logical <code>TRUE</code> and <code>FALSE</code> following the rules of <code><a href="base.html#topic+as.logical">as.logical()</a></code>. If this is not the intended meaning of &quot;positive&quot;, then specify which of the two values should be considered <code>TRUE</code> with the argument <code>positive</code>.</p>
</td></tr>
<tr><td><code id="aucroc_+3A_pred">pred</code></td>
<td>
<p>numeric vector. Predictions corresponding to each respective element in <code>actual</code>. Any numeric value (not only probabilities) are permissible.</p>
</td></tr>
<tr><td><code id="aucroc_+3A_na.rm">na.rm</code></td>
<td>
<p>logical(1). <code>TRUE</code> if missing values should be removed; <code>FALSE</code> if they should be retained. If <code>TRUE</code>, then if any element of either <code>actual</code> or <code>pred</code> is missing, its paired element will be also removed.</p>
</td></tr>
<tr><td><code id="aucroc_+3A_positive">positive</code></td>
<td>
<p>any single atomic value. The value of <code>actual</code> that is considered <code>TRUE</code>; any other value of <code>actual</code> is considered <code>FALSE</code>. For example, if <code>2</code> means <code>TRUE</code> and <code>1</code> means <code>FALSE</code>, then set <code>positive = 2</code>.</p>
</td></tr>
<tr><td><code id="aucroc_+3A_sample_size">sample_size</code></td>
<td>
<p>single positive integer. To keep the computation relatively rapid, when <code>actual</code> and <code>pred</code> are longer than <code>sample_size</code> elements, then a random sample of <code>sample_size</code> of <code>actual</code> and <code>pred</code> will be selected and the ROC and AUC will be calculated on this sample. To disable random sampling for long inputs, set <code>sample_size = NA</code>.</p>
</td></tr>
<tr><td><code id="aucroc_+3A_seed">seed</code></td>
<td>
<p>numeric(1). Random seed used only if <code>length(actual) &gt; sample_size</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with the following elements:
</p>

<ul>
<li> <p><code>roc_opt</code>: tibble with optimistic ROC data. &quot;Optimistic&quot; means that when predictions are tied, the TRUE/positive actual values are ordered before the FALSE/negative ones.
</p>
</li>
<li> <p><code>roc_pess</code>: tibble with pessimistic ROC data. &quot;Pessimistic&quot; means that when predictions are tied, the FALSE/negative actual values are ordered before the TRUE/positive ones. Note that this difference is not merely in the sort order: when there are ties, the way that true positives, true negatives, etc. are counted is different for optimistic and pessimistic approaches. If there are no tied predictions, then <code>roc_opt</code> and <code>roc_pess</code> are identical.
</p>
</li>
<li> <p><code>auc_opt</code>: area under the ROC curve for optimistic ROC.
</p>
</li>
<li> <p><code>auc_pess</code>: area under the ROC curve for pessimistic ROC.
</p>
</li>
<li> <p><code>auc</code>: mean of <code>auc_opt</code> and <code>auc_pess</code>. If there are no tied predictions, then <code>auc_opt</code>, <code>auc_pess</code>, and <code>auc</code> are identical.
</p>
</li>
<li> <p><code>ties</code>: <code>TRUE</code> if there are two or more tied predictions; <code>FALSE</code> if there are no ties.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>set.seed(0)
# Generate some simulated "actual" data
a &lt;- sample(c(TRUE, FALSE), 50, replace = TRUE)

# Generate some simulated predictions
p &lt;- runif(50) |&gt; round(2)
p[c(7, 8, 22, 35, 40, 41)] &lt;- 0.5

# Calculate AUCROC with its components
ar &lt;- aucroc(a, p)
ar$auc

</code></pre>

<hr>
<h2 id='mae'>Regression error and deviation measures</h2><span id='topic+mae'></span><span id='topic+rmse'></span><span id='topic+mad'></span>

<h3>Description</h3>

<p>These are standard error and deviation measures for numeric data. &quot;Deviation&quot; means the natural variation of the values of a numeric vector around its central tendency (usually the mean or median). &quot;Error&quot; means the average discrepancy between the actual values of a numeric vector and its predicted values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mae(actual, pred, na.rm = FALSE)

rmse(actual, pred, na.rm = FALSE)

mad(x, na.rm = FALSE, version = "mean", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mae_+3A_actual">actual</code></td>
<td>
<p>numeric vector. Actual (true) values of target outcome data.</p>
</td></tr>
<tr><td><code id="mae_+3A_pred">pred</code></td>
<td>
<p>numeric vector. Predictions corresponding to each respective element in <code>actual</code>.</p>
</td></tr>
<tr><td><code id="mae_+3A_na.rm">na.rm</code></td>
<td>
<p>logical(1). <code>TRUE</code> if missing values should be removed; <code>FALSE</code> if they should be retained. If <code>TRUE</code>, then if any element of either <code>actual</code> or <code>pred</code> is missing, its paired element will be also removed.</p>
</td></tr>
<tr><td><code id="mae_+3A_x">x</code></td>
<td>
<p>numeric vector. Values for which to calculate the MAD.</p>
</td></tr>
<tr><td><code id="mae_+3A_version">version</code></td>
<td>
<p>character(1). By default (<code>version = 'mean'</code>), <code>mad()</code> returns the mean absolute deviation (MAD) of values relative to their mean. If <code>version = 'median'</code>, it calls the <code>stats::mad()</code> function instead, the median absolute deviation relative to their median (MedAD, sometimes also called MAD). Any other value gives an error. See details.</p>
</td></tr>
<tr><td><code id="mae_+3A_...">...</code></td>
<td>
<p>Arguments to pass to <code>stats::mad()</code> if <code>version = 'median'</code>. See the <code>version</code> argument for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Mean absolute deviation (MAD)</strong>
</p>
<p><code>mad()</code> returns the mean absolute deviation (MAD) of values relative to their mean. This is useful as a default benchmark for the mean absolute error (MAE), as the standard deviation (SD) is a default benchmark for the root mean square error (RMSE).
</p>
<p><strong>NOTE:</strong> This function name overrides <code>stats::mad()</code> (median absolute deviation relative to their median). To maintain the functionality of <code>stats::mad()</code>, specify the <code>version</code> argument.
</p>


<h3>Value</h3>

<p>In all cases, if any value in <code>actual</code> or <code>pred</code> is <code>NA</code> and <code>na.rm = FALSE</code>, then the function returns <code>NA</code>.
</p>
<p><code>mae()</code> returns the mean absolute error (MAE) of predicted values <code>pred</code> compared to the <code>actual</code> values.
</p>
<p><code>rmse()</code> returns the root mean squared error (RMSE) of predicted values <code>pred</code> compared to the <code>actual</code> values.
</p>
<p><code>mad()</code> returns either the mean absolute deviation (MAD) of values relative to their mean (default) or the median absolute deviation relative to their median. See details.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- c(3, 5, 2, 7, 9, 4, 6, 8, 1, 10)
p &lt;- c(2.5, 5.5, 2, 6.5, 9.5, 3.5, 6, 7.5, 1.5, 9.5)
mae(a, p)

rmse(a, p)

mad(a)

</code></pre>

<hr>
<h2 id='reg_aucroc'>Area under the ROC curve for regression target outcomes</h2><span id='topic+reg_aucroc'></span>

<h3>Description</h3>

<p>Area under the ROC curve (AUCROC) is a classification measure. By dichotomizing the range of <code>actual</code> values, <code>reg_aucroc()</code> turns regression evaluation into classification evaluation for any regression model. Note that the model that generates the predictions is assumed to be a regression model; however, any numeric inputs are allowed for the <code>pred</code> argument, so there is no check for the nature of the source model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reg_aucroc(
  actual,
  pred,
  num_quants = 100,
  ...,
  cuts = NULL,
  imbalance = 0.05,
  na.rm = FALSE,
  sample_size = 10000,
  seed = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="reg_aucroc_+3A_actual">actual</code></td>
<td>
<p>numeric vector. Actual label values from a dataset. They must be numeric.</p>
</td></tr>
<tr><td><code id="reg_aucroc_+3A_pred">pred</code></td>
<td>
<p>numeric vector. Predictions corresponding to each respective element in <code>actual</code>.</p>
</td></tr>
<tr><td><code id="reg_aucroc_+3A_num_quants">num_quants</code></td>
<td>
<p>scalar positive integer. If <code>cuts</code> is <code>NULL</code> (default), <code>actual</code> will be dichotomized into <code>quants</code> quantiles and that many ROCs will be returned in the <code>rocs</code> element. However, if <code>cuts</code> is specified, then <code>quants</code> is ignored.</p>
</td></tr>
<tr><td><code id="reg_aucroc_+3A_...">...</code></td>
<td>
<p>Not used. Forces explicit naming of the arguments that follow.</p>
</td></tr>
<tr><td><code id="reg_aucroc_+3A_cuts">cuts</code></td>
<td>
<p>numeric vector. If <code>cuts</code> is provided, it overrides <code>quants</code> to specify the cut points for dichotomization of <code>actual</code> for the creation of <code>cuts + 1</code> ROCs.</p>
</td></tr>
<tr><td><code id="reg_aucroc_+3A_imbalance">imbalance</code></td>
<td>
<p>numeric(1) in (0, 0.5]. The result element <code>mean_auc</code> averages the AUCs over three regions (see details of the return value). <code>imbalance</code> is the supposed percentage of the less frequent class in the data. If not provided, defaults to 0.05 (5%).</p>
</td></tr>
<tr><td><code id="reg_aucroc_+3A_na.rm">na.rm</code></td>
<td>
<p>See documentation for <code>aucroc()</code></p>
</td></tr>
<tr><td><code id="reg_aucroc_+3A_sample_size">sample_size</code></td>
<td>
<p>See documentation for <code>aucroc()</code>. In addition to those notes, for <code>reg_aucroc()</code>, any sampling is conducted before the dichotomization of <code>actual</code> so that all classification ROCs are based on identical data.</p>
</td></tr>
<tr><td><code id="reg_aucroc_+3A_seed">seed</code></td>
<td>
<p>See documentation for <code>aucroc()</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The ROC data and AUCROC values are calculated with <code>aucroc()</code>.
</p>


<h3>Value</h3>

<p>List with the following elements:
</p>

<ul>
<li> <p><code>rocs</code>: List of results for <code>aucroc()</code> for each dichotomized segment of <code>actual</code>.
</p>
</li>
<li> <p><code>auc</code>: named numeric vector of AUC extracted from each element of <code>rocs</code>. Named by the percentile that the AUC represents.
</p>
</li>
<li> <p><code>mean_auc</code>: named numeric(3). The average AUC over the low, middle, and high quantiles of dichotomization:
</p>
</li>
<li> <p><code>lo</code>: average AUC with <code>imbalance</code>% (e.g., 5%) or less of the actual target values;
</p>
</li>
<li> <p><code>mid</code>: average AUC in between <code>lo</code> and <code>hi</code>;
</p>
</li>
<li> <p><code>hi</code>: average AUC with (1 - <code>imbalance</code>)% (e.g., 95%) or more of the actual target values;
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Remove rows with missing values from airquality dataset
airq &lt;- airquality |&gt;
  na.omit()

# Create binary version where the target variable 'Ozone' is dichotomized based on its median
airq_bin &lt;- airq
airq_bin$Ozone &lt;- airq_bin$Ozone &gt;= median(airq_bin$Ozone)

# Create a generic regression model; use autogam
req_aq   &lt;- autogam::autogam(airq, 'Ozone', family = gaussian())
req_aq$perf$sa_wmae_mad  # Standardized accuracy for regression

# Create a generic classification model; use autogam
class_aq &lt;- autogam::autogam(airq_bin, 'Ozone', family = binomial())
class_aq$perf$auc  # AUC (standardized accuracy for classification)

# Compute AUC for regression predictions
reg_auc_aq &lt;- reg_aucroc(
  airq$Ozone,
  predict(req_aq)
)

# Average AUC over the lo, mid, and hi quantiles of dichotomization:
reg_auc_aq$mean_auc


</code></pre>

<hr>
<h2 id='sa_diff'>Statistical tests for the differences between standardized accuracies (staccuracies)</h2><span id='topic+sa_diff'></span>

<h3>Description</h3>

<p>Because the distribution of staccuracies is uncertain (and indeed, different staccuracies likely have different distributions), bootstrapping is used to empirically estimate the distributions and calculate the p-values. See the return value description for details on what the function provides.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sa_diff(
  actual,
  preds,
  ...,
  na.rm = FALSE,
  sa = NULL,
  pct = c(0.01, 0.02, 0.03, 0.04, 0.05),
  boot_alpha = 0.05,
  boot_it = 1000,
  seed = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sa_diff_+3A_actual">actual</code></td>
<td>
<p>numeric vector. The actual (true) labels.</p>
</td></tr>
<tr><td><code id="sa_diff_+3A_preds">preds</code></td>
<td>
<p>named list of at least two numeric vectors. Each element is a vector of the same length as actual with predictions for each row corresponding to each element of actual. The names of the list elements should be the names of the models that produced each respective prediction; these names will be used to distinguish the results.</p>
</td></tr>
<tr><td><code id="sa_diff_+3A_...">...</code></td>
<td>
<p>not used. Forces explicit naming of subsequent arguments.</p>
</td></tr>
<tr><td><code id="sa_diff_+3A_na.rm">na.rm</code></td>
<td>
<p>See documentation for <code><a href="#topic+staccuracy">staccuracy()</a></code></p>
</td></tr>
<tr><td><code id="sa_diff_+3A_sa">sa</code></td>
<td>
<p>list of functions. Each element is the unquoted name of a valid staccuracy function (see <code><a href="#topic+staccuracy">staccuracy()</a></code> for the required function signature.) If an element is named, the name will be displayed as the value of the <code>sa</code> column of the result. Otherwise, the function name will be displayed. If NULL (default), staccuracy functions will be automatically selected based on the datatypes of actual and <code>preds</code>.</p>
</td></tr>
<tr><td><code id="sa_diff_+3A_pct">pct</code></td>
<td>
<p>numeric with values from (0, 1). The percentage values on which the difference in staccuracies will be tested.</p>
</td></tr>
<tr><td><code id="sa_diff_+3A_boot_alpha">boot_alpha</code></td>
<td>
<p>numeric(1) from 0 to 1. Alpha for percentile-based confidence interval range for the bootstrapped means; the bootstrap confidence intervals will be the lowest and highest <code>(1 - 0.05) / 2</code> percentiles. For example, if <code>boot_alpha = 0.05</code> (default), the intervals will be at the 2.5 and 97.5 percentiles.</p>
</td></tr>
<tr><td><code id="sa_diff_+3A_boot_it">boot_it</code></td>
<td>
<p>positive integer(1). The number of bootstrap iterations.</p>
</td></tr>
<tr><td><code id="sa_diff_+3A_seed">seed</code></td>
<td>
<p>integer(1). Random seed for the bootstrap sampling. Supply this between runs to assure identical results.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>tibble with staccuracy difference results:
</p>

<ul>
<li> <p><code>staccuracy</code>: name of staccuracy measure
</p>
</li>
<li> <p><code>pred</code>: Each named element (model name) in the input <code>preds</code>. The row values give the staccuracy for that prediction. When <code>pred</code> is <code>NA</code>, the row represents the difference between prediction staccuracies (<code>diff</code>) instead of  staccuracies themselves.
</p>
</li>
<li> <p><code>diff</code>: When <code>diff</code> takes the form 'model1-model2', then the row values give the difference in staccuracies between two named elements (model names) in the input <code>preds</code>. When <code>diff</code> is <code>NA</code>, the row instead represents the staccuracy of a specific model prediction (<code>pred</code>).
</p>
</li>
<li> <p><code>lo</code>, <code>mean</code>, <code>hi</code>: The lower bound, mean, and upper bound of the bootstrapped staccuracy. The lower and upper bounds are confidence intervals specified by the input <code>boot_alpha</code>.
</p>
</li>
<li> <p><code>p__</code>: p-values that the difference in staccuracies are at least the specified percentage amount or greater.  E.g., for the default input <code>pct = c(0.01, 0.02, 0.03, 0.04, 0.05)</code>, these columns would be <code>p01</code>, <code>p02</code>, <code>p03</code>, <code>p04</code>, and  <code>p05</code>. As they apply only to differences between staccuracies, they are provided only for <code>diff</code> rows and are <code>NA</code> for <code>pred</code> rows. As an example of their meaning, if the <code>mean</code> difference for 'model1-model2' is 0.0832 with <code>p01</code> of 0.012 and <code>p02</code> of 0.035, then 1.2% of bootstrapped staccuracies had a model1 - model2 difference of less than 0.01 and 3.5% were less than 0.02. (That is, 98.8% of differences were greater than 0.01 and 96.5% were greater than 0.02.)
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>lm_attitude_all &lt;- lm(rating ~ ., data = attitude)
lm_attitude__a &lt;- lm(rating ~ . - advance, data = attitude)
lm_attitude__c &lt;- lm(rating ~ . - complaints, data = attitude)

sdf &lt;- sa_diff(
  attitude$rating,
  list(
    all = predict(lm_attitude_all),
    madv = predict(lm_attitude__a),
    mcmp = predict(lm_attitude__c)
  ),
  boot_it = 10
)
sdf

</code></pre>

<hr>
<h2 id='staccuracy'>Standardized accuracy (staccuracy) functions.</h2><span id='topic+staccuracy'></span><span id='topic+sa_mae_mad'></span><span id='topic+sa_wmae_mad'></span><span id='topic+sa_rmse_sd'></span><span id='topic+sa_wrmse_sd'></span>

<h3>Description</h3>

<p>Standardized accuracy (staccuracy) represents error or accuracy measures on a scale where 1 or 100% means perfect prediction and 0.5 or 50% is a reference comparison of some specified standard performance. Higher than 0.5 is better than the reference and below 0.5 is worse. 0 might or might not have a special meaning; sometimes negative scores are possible, but these often indicate modelling errors.
</p>
<p>The core function is <code>staccuracy()</code>, which receives as input a generic error function and a reference function against which to compare the error function performance. In addition, the following recommended staccuracy functions are provided:
</p>

<ul>
<li> <p><code>sa_mae_mad</code>: standardized accuracy of the mean absolute error (MAE) based on the mean absolute deviation (MAD)
</p>
</li>
<li> <p><code>sa_rmse_sd</code>: standardized accuracy of the root mean squared error (RMSE) based on the standard deviation (SD)
</p>
</li>
<li> <p><code>sa_wmae_mad</code>: standardized accuracy of the winsorized mean absolute error (MAE) based on the mean absolute deviation (MAD)
</p>
</li>
<li> <p><code>sa_wrmse_sd</code>: standardized accuracy of the winsorized root mean squared error (RMSE) based on the standard deviation (SD)
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>staccuracy(error_fun, ref_fun)

sa_mae_mad(actual, pred, na.rm = FALSE)

sa_wmae_mad(actual, pred, na.rm = FALSE)

sa_rmse_sd(actual, pred, na.rm = FALSE)

sa_wrmse_sd(actual, pred, na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="staccuracy_+3A_error_fun">error_fun</code></td>
<td>
<p>function. The unquoted name of the function that calculates the error (or accuracy) measure. This function must be of the signature <code style="white-space: pre;">&#8288;function(actual, pred, na.rm = FALSE)&#8288;</code>.</p>
</td></tr>
<tr><td><code id="staccuracy_+3A_ref_fun">ref_fun</code></td>
<td>
<p>function. The unquoted name of the function that calculates the reference error, accuracy, or deviation measure. This function must be of the signature <code>ref_fun(actual, na.rm = FALSE)</code>.</p>
</td></tr>
<tr><td><code id="staccuracy_+3A_actual">actual</code></td>
<td>
<p>numeric. The true (actual) labels.</p>
</td></tr>
<tr><td><code id="staccuracy_+3A_pred">pred</code></td>
<td>
<p>numeric. The predicted estimates. Must be the same length as <code>actual</code>.</p>
</td></tr>
<tr><td><code id="staccuracy_+3A_na.rm">na.rm</code></td>
<td>
<p>logical(1). Whether NA values should be removed (<code>TRUE</code>) or not (<code>FALSE</code>, default).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The core function <code>staccuracy()</code> receives as input a generic error function and a reference function against which to compare the error function's performance. These input functions must have the following signatures (see the argument specifications for details of the arguments):
</p>

<ul>
<li> <p><code>error_fun</code>: <code style="white-space: pre;">&#8288;function(actual, pred, na.rm = na.rm)&#8288;</code>; the output must be a scalar numeric (that is, a single number).
</p>
</li>
<li> <p><code>error_fun</code>: <code style="white-space: pre;">&#8288;function(actual, pred, na.rm = na.rm)&#8288;</code>; the output must be a scalar numeric (that is, a single number).
</p>
</li></ul>



<h3>Value</h3>

<p><code>staccuracy()</code> returns a function with signature <code style="white-space: pre;">&#8288;function(actual, pred, na.rm = FALSE)&#8288;</code> that receives an <code>actual</code> and a <code>pred</code> vector as inputs and returns the staccuracy of the originally input error function based on the input reference function.
</p>
<p>The convenience <code style="white-space: pre;">&#8288;sa_*()&#8288;</code> functions return the staccuracy measures specified above.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Here's some data
actual_1 &lt;- c(2.3, 4.5, 1.8, 7.6, 3.2)

# Here are some predictions of that data
predicted_1 &lt;- c(2.5, 4.2, 1.9, 7.4, 3.0)

# MAE measures the average error in the predictions
mae(actual_1, predicted_1)

# But how good is that?
# MAD gives the natural variation in the actual data; this is a point of comparison.
mad(actual_1)

# So, our predictions are better (lower) than the MAD, but how good, really?
# Create a standardized accuracy function to give us an easily interpretable metric:
my_mae_vs_mad_sa &lt;- staccuracy(mae, mad)

# Now use it
my_mae_vs_mad_sa(actual_1, predicted_1)

# That's 94.2% standardized accuracy compared to the MAD. Pretty good!


</code></pre>

<hr>
<h2 id='winsorize'>Winsorize a numeric vector</h2><span id='topic+winsorize'></span><span id='topic+win_mae'></span><span id='topic+win_rmse'></span>

<h3>Description</h3>

<p>Winsorization means truncating the extremes of a numeric range by replacing extreme values with a predetermined minimum and maximum. <code>winsorize()</code> returns the input vector values with values less than or greater than the provided minimum or maximum replaced by the provided minimum or maximum, respectively.
</p>
<p><code>win_mae()</code> and <code>win_rmse()</code> return MAE and RMSE respectively with winsorized predictions. The fundamental idea underlying the winsorization of predictions is that if the actual data has well-defined bounds, then models should not be penalized for being overzealous in predicting beyond the extremes of the data. Models that are overzealous in the boundaries might sometimes be superior within normal ranges; the extremes can be easily corrected by winsorization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>winsorize(x, win_range)

win_mae(actual, pred, win_range = range(actual), na.rm = FALSE)

win_rmse(actual, pred, win_range = range(actual), na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="winsorize_+3A_x">x</code></td>
<td>
<p>numeric vector.</p>
</td></tr>
<tr><td><code id="winsorize_+3A_win_range">win_range</code></td>
<td>
<p>numeric(2). The minimum and maximum allowable values for the <code>pred</code> predictions or for <code>x</code>. For functions with <code>pred</code>, <code>win_range</code> defaults to the minimum and maximum values of the provided <code>actual</code> values. For functions with <code>x</code>, there is no default.</p>
</td></tr>
<tr><td><code id="winsorize_+3A_actual">actual</code></td>
<td>
<p>numeric vector. Actual (true) values of target outcome data.</p>
</td></tr>
<tr><td><code id="winsorize_+3A_pred">pred</code></td>
<td>
<p>numeric vector. Predictions corresponding to each respective element in <code>actual</code>.</p>
</td></tr>
<tr><td><code id="winsorize_+3A_na.rm">na.rm</code></td>
<td>
<p>logical(1). <code>TRUE</code> if missing values should be removed; <code>FALSE</code> if they should be retained. If <code>TRUE</code>, then if any element of either <code>actual</code> or <code>pred</code> is missing, its paired element will be also removed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>winsorize()</code> returns a winsorized vector.
</p>
<p><code>win_mae()</code> returns the mean absolute error (MAE) of winsorized predicted values <code>pred</code> compared to the <code>actual</code> values. See <code>mae()</code> for details.
</p>
<p><code>win_rmse()</code> returns the root mean squared error (RMSE) of winsorized predicted values <code>pred</code> compared to the <code>actual</code> values. See <code>rmse()</code> for details.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- c(3, 5, 2, 7, 9, 4, 6, 8, 2, 10)
p &lt;- c(2.5, 5.5, 1.5, 6.5, 10.5, 3.5, 6, 7.5, 0.5, 11.5)

a  # the original data
winsorize(a, c(2, 8))  # a winsorized on defined boundaries

# range of the original data
a
range(a)

# some overzealous predictions
p
range(p)

# MAE penalizes overzealous predictions
mae(a, p)

# Winsorized MAE forgives overzealous predictions
win_mae(a, p)

# RMSE penalizes overzealous predictions
rmse(a, p)

# Winsorized RMSE forgives overzealous predictions
win_rmse(a, p)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
