<!DOCTYPE html><html lang="en-US"><head><title>Help for package alookr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {alookr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cleanse.data.frame'><p>Cleansing the dataset for classification modeling</p></a></li>
<li><a href='#cleanse.split_df'><p>Cleansing the dataset for classification modeling</p></a></li>
<li><a href='#compare_diag'><p>Diagnosis of train set and test set of split_df object</p></a></li>
<li><a href='#compare_performance'><p>Compare model performance</p></a></li>
<li><a href='#compare_plot'><p>Comparison plot of train set and test set</p></a></li>
<li><a href='#compare_target_category'><p>Comparison of categorical variables of train set and test set</p></a></li>
<li><a href='#compare_target_numeric'><p>Comparison of numerical variables of train set and test set</p></a></li>
<li><a href='#extract_set'><p>Extract train/test dataset</p></a></li>
<li><a href='#matthews'><p>Compute Matthews Correlation Coefficient</p></a></li>
<li><a href='#performance_metric'><p>Calculate metrics for model evaluation</p></a></li>
<li><a href='#plot_cutoff'><p>Visualization for cut-off selection</p></a></li>
<li><a href='#plot_performance'><p>Visualization for ROC curve</p></a></li>
<li><a href='#run_models'><p>Fit binary classification model</p></a></li>
<li><a href='#run_performance'><p>Apply calculate performance metrics for model evaluation</p></a></li>
<li><a href='#run_predict'><p>Predict binary classification model</p></a></li>
<li><a href='#sampling_target'><p>Extract the data to fit the model</p></a></li>
<li><a href='#split_by'><p>Split Data into Train and Test Set</p></a></li>
<li><a href='#summary.split_df'><p>Summarizing split_df information</p></a></li>
<li><a href='#treatment_corr'><p>Diagnosis and removal of highly correlated variables</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Model Classifier for Binary Classification</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.9</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-02-11</td>
</tr>
<tr>
<td>Description:</td>
<td>A collection of tools that support data splitting, predictive modeling, and model evaluation. 
    A typical function is to split a dataset into a training dataset and a test dataset. 
    Then compare the data distribution of the two datasets.
    Another feature is to support the development of predictive models and to compare the performance of several predictive models, 
    helping to select the best model. </td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.0), ggplot2 (&ge; 3.0.0), randomForest</td>
</tr>
<tr>
<td>Imports:</td>
<td>caTools, cli (&ge; 1.1.0), dlookr, dplyr (&ge; 0.7.6), future,
ggmosaic, MASS, MLmetrics, methods, parallelly, party, purrr,
ROCR, ranger, rlang, rpart, stats, tibble, tidyr, tidyselect,
xgboost, glmnet</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, ISLR, mice, mlbench, rmarkdown, stringi</td>
</tr>
<tr>
<td>Author:</td>
<td>Choonghyun Ryu [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Choonghyun Ryu &lt;choonghyun.ryu@gmail.com&gt;</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/choonghyunryu/alookr/issues">https://github.com/choonghyunryu/alookr/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-11 00:53:57 UTC; choonghyunryu</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-11 07:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='cleanse.data.frame'>Cleansing the dataset for classification modeling</h2><span id='topic+cleanse.data.frame'></span><span id='topic+cleanse'></span>

<h3>Description</h3>

<p>The cleanse() cleanse the dataset for classification modeling
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'data.frame'
cleanse(
  .data,
  uniq = TRUE,
  uniq_thres = 0.1,
  char = TRUE,
  missing = FALSE,
  verbose = TRUE,
  ...
)

cleanse(.data, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cleanse.data.frame_+3A_.data">.data</code></td>
<td>
<p>a data.frame or a <code>tbl_df</code>.</p>
</td></tr>
<tr><td><code id="cleanse.data.frame_+3A_uniq">uniq</code></td>
<td>
<p>logical. Set whether to remove the variables whose unique value is one.</p>
</td></tr>
<tr><td><code id="cleanse.data.frame_+3A_uniq_thres">uniq_thres</code></td>
<td>
<p>numeric. Set a threshold to removing variables when the ratio of unique values(number of unique values / number of observation) is greater than the set value.</p>
</td></tr>
<tr><td><code id="cleanse.data.frame_+3A_char">char</code></td>
<td>
<p>logical. Set the change the character to factor.</p>
</td></tr>
<tr><td><code id="cleanse.data.frame_+3A_missing">missing</code></td>
<td>
<p>logical. Set whether to removing variables including missing value</p>
</td></tr>
<tr><td><code id="cleanse.data.frame_+3A_verbose">verbose</code></td>
<td>
<p>logical. Set whether to echo information to the console at runtime.</p>
</td></tr>
<tr><td><code id="cleanse.data.frame_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is useful when fit the classification model.
This function does the following.:
Remove the variable with only one value. And remove variables that have a unique number of values relative to the number of observations for a character or categorical variable. In this case, it is a variable that corresponds to an identifier or an identifier. And converts the character to factor.
</p>


<h3>Value</h3>

<p>An object of data.frame or train_df. and return value is an object of the same type as the .data argument.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create sample dataset
set.seed(123L)
id &lt;- sapply(1:1000, function(x)
  paste(c(sample(letters, 5), x), collapse = ""))

year &lt;- "2018"

set.seed(123L)
count &lt;- sample(1:10, size = 1000, replace = TRUE)

set.seed(123L)
alpha &lt;- sample(letters, size = 1000, replace = TRUE)

set.seed(123L)
flag &lt;- sample(c("Y", "N"), size = 1000, prob = c(0.1, 0.9), replace = TRUE)

dat &lt;- data.frame(id, year, count, alpha, flag, stringsAsFactors = FALSE)
# structure of dataset
str(dat)

# cleansing dataset
newDat &lt;- cleanse(dat)

# structure of cleansing dataset
str(newDat)

# cleansing dataset
newDat &lt;- cleanse(dat, uniq = FALSE)

# structure of cleansing dataset
str(newDat)

# cleansing dataset
newDat &lt;- cleanse(dat, uniq_thres = 0.3)

# structure of cleansing dataset
str(newDat)

# cleansing dataset
newDat &lt;- cleanse(dat, char = FALSE)

# structure of cleansing dataset
str(newDat)

</code></pre>

<hr>
<h2 id='cleanse.split_df'>Cleansing the dataset for classification modeling</h2><span id='topic+cleanse.split_df'></span>

<h3>Description</h3>

<p>Diagnosis of similarity between datasets splitted by train set and set included in the &quot;split_df&quot; class. and cleansing the &quot;split_df&quot; class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'split_df'
cleanse(.data, add_character = FALSE, uniq_thres = 0.9, missing = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cleanse.split_df_+3A_.data">.data</code></td>
<td>
<p>an object of class &quot;split_df&quot;, usually, a result of a call to split_df().</p>
</td></tr>
<tr><td><code id="cleanse.split_df_+3A_add_character">add_character</code></td>
<td>
<p>logical. Decide whether to include text variables in the
compare of categorical data. The default value is FALSE, which also not includes character variables.</p>
</td></tr>
<tr><td><code id="cleanse.split_df_+3A_uniq_thres">uniq_thres</code></td>
<td>
<p>numeric. Set a threshold to removing variables when the ratio of unique values(number of unique values / number of observation) is greater than the set value.</p>
</td></tr>
<tr><td><code id="cleanse.split_df_+3A_missing">missing</code></td>
<td>
<p>logical. Set whether to removing variables including missing value</p>
</td></tr>
<tr><td><code id="cleanse.split_df_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Remove the detected variables from the diagnosis using the compare_diag() function.
</p>


<h3>Value</h3>

<p>An object of class &quot;split_df&quot;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

# Credit Card Default Data
head(ISLR::Default)

# Generate data for the example
sb &lt;- ISLR::Default %&gt;%
  split_by(default)

sb %&gt;%
  cleanse

</code></pre>

<hr>
<h2 id='compare_diag'>Diagnosis of train set and test set of split_df object</h2><span id='topic+compare_diag'></span>

<h3>Description</h3>

<p>Diagnosis of similarity between datasets splitted by train set and set included in the &quot;split_df&quot; class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare_diag(
  .data,
  add_character = FALSE,
  uniq_thres = 0.01,
  miss_msg = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compare_diag_+3A_.data">.data</code></td>
<td>
<p>an object of class &quot;split_df&quot;, usually, a result of a call to split_df().</p>
</td></tr>
<tr><td><code id="compare_diag_+3A_add_character">add_character</code></td>
<td>
<p>logical. Decide whether to include text variables in the
compare of categorical data. The default value is FALSE, which also not includes character variables.</p>
</td></tr>
<tr><td><code id="compare_diag_+3A_uniq_thres">uniq_thres</code></td>
<td>
<p>numeric. Set a threshold to removing variables when the ratio of unique values(number of unique values / number of observation) is greater than the set value.</p>
</td></tr>
<tr><td><code id="compare_diag_+3A_miss_msg">miss_msg</code></td>
<td>
<p>logical. Set whether to output a message when diagnosing missing value.</p>
</td></tr>
<tr><td><code id="compare_diag_+3A_verbose">verbose</code></td>
<td>
<p>logical. Set whether to echo information to the console at runtime.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the two split datasets, a variable with a single value, a variable with a level not found in any dataset, and a variable with a high ratio to the number of levels are diagnosed.
</p>


<h3>Value</h3>

<p>list.
Variables of tbl_df for first component named &quot;single_value&quot;:
</p>

<ul>
<li><p> variables : character. variable name
</p>
</li>
<li><p> train_uniq : character. the type of unique value in train set. it is divided into &quot;single&quot; and &quot;multi&quot;.
</p>
</li>
<li><p> test_uniq : character. the type of unique value in test set. it is divided into &quot;single&quot; and &quot;multi&quot;.
</p>
</li></ul>

<p>Variables of tbl_df for second component named &quot;uniq_rate&quot;:
</p>

<ul>
<li><p> variables : character. categorical variable name
</p>
</li>
<li><p> train_uniqcount : numeric. the number of unique value in train set
</p>
</li>
<li><p> train_uniqrate : numeric. the ratio of unique values(number of unique values / number of observation) in train set
</p>
</li>
<li><p> test_uniqcount : numeric. the number of unique value in test set
</p>
</li>
<li><p> test_uniqrate : numeric. the ratio of unique values(number of unique values / number of observation) in test set
</p>
</li></ul>

<p>Variables of tbl_df for third component named &quot;missing_level&quot;:
</p>

<ul>
<li><p> variables : character. variable name
</p>
</li>
<li><p> n_levels : integer. count of level of categorical variable
</p>
</li>
<li><p> train_missing_nlevel : integer. the number of non-existent levels in the train set
</p>
</li>
<li><p> test_missing_nlevel : integer. he number of non-existent levels in the test set
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

# Credit Card Default Data
head(ISLR::Default)

defaults &lt;- ISLR::Default
defaults$id &lt;- seq(NROW(defaults))

set.seed(1)
defaults[sample(seq(NROW(defaults)), 3), "student"] &lt;- NA
set.seed(2)
defaults[sample(seq(NROW(defaults)), 10), "balance"] &lt;- NA

sb &lt;- defaults %&gt;%
  split_by(default)

sb %&gt;%
  compare_diag()

sb %&gt;%
  compare_diag(add_character = TRUE)

sb %&gt;%
  compare_diag(uniq_thres = 0.0005)

</code></pre>

<hr>
<h2 id='compare_performance'>Compare model performance</h2><span id='topic+compare_performance'></span>

<h3>Description</h3>

<p>compare_performance() compares the performance of a model with several model performance metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare_performance(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compare_performance_+3A_model">model</code></td>
<td>
<p>A model_df. results of predicted model that created by run_predict().</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list. results of compared model performance.
list has the following components:
</p>

<ul>
<li><p> recommend_model : character. The name of the model that is recommended as the best among the various models.
</p>
</li>
<li><p> top_count : numeric. The number of best performing performance metrics by model.
</p>
</li>
<li><p> mean_rank : numeric. Average of ranking individual performance metrics by model.
</p>
</li>
<li><p> top_metric : list. The name of the performance metric with the best performance on individual performance metrics by model.
</p>
</li></ul>

<p>The performance metrics calculated are as follows.:
</p>

<ul>
<li><p> ZeroOneLoss : Normalized Zero-One Loss(Classification Error Loss).
</p>
</li>
<li><p> Accuracy : Accuracy.
</p>
</li>
<li><p> Precision : Precision.
</p>
</li>
<li><p> Recall : Recall.
</p>
</li>
<li><p> Specificity : Specificity.
</p>
</li>
<li><p> F1_Score : F1 Score.
</p>
</li>
<li><p> LogLoss : Log loss / Cross-Entropy Loss.
</p>
</li>
<li><p> AUC : Area Under the Receiver Operating Characteristic Curve (ROC AUC).
</p>
</li>
<li><p> Gini : Gini Coefficient.
</p>
</li>
<li><p> PRAUC : Area Under the Precision-Recall Curve (PR AUC).
</p>
</li>
<li><p> LiftAUC : Area Under the Lift Chart.
</p>
</li>
<li><p> GainAUC : Area Under the Gain Chart.
</p>
</li>
<li><p> KS_Stat : Kolmogorov-Smirnov Statistic.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)

# Divide the train data set and the test data set.
sb &lt;- rpart::kyphosis %&gt;%
  split_by(Kyphosis)

# Extract the train data set from original data set.
train &lt;- sb %&gt;%
  extract_set(set = "train")

# Extract the test data set from original data set.
test &lt;- sb %&gt;%
  extract_set(set = "test")

# Sampling for unbalanced data set using SMOTE(synthetic minority over-sampling technique).
train &lt;- sb %&gt;%
  sampling_target(seed = 1234L, method = "ubSMOTE")

# Cleaning the set.
train &lt;- train %&gt;%
  cleanse

# Run the model fitting.
result &lt;- run_models(.data = train, target = "Kyphosis", positive = "present")

# Predict the model.
pred &lt;- run_predict(result, test)

# Compare the model performance
compare_performance(pred)


</code></pre>

<hr>
<h2 id='compare_plot'>Comparison plot of train set and test set</h2><span id='topic+compare_plot'></span>

<h3>Description</h3>

<p>Plot compare information of the train set and test set included
in the &quot;split_df&quot; class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare_plot(.data, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compare_plot_+3A_.data">.data</code></td>
<td>
<p>an object of class &quot;split_df&quot;, usually, a result of a call to split_df().</p>
</td></tr>
<tr><td><code id="compare_plot_+3A_...">...</code></td>
<td>
<p>one or more unquoted expressions separated by commas.
Select the variable you want to plotting.
You can treat variable names like they are positions.
Positive values select variables; negative values to drop variables.
If the first expression is negative, compare_target_category() will automatically
start with all variables.
These arguments are automatically quoted and evaluated in a context where column names
represent column positions.
They support unquoting and splicing.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The numerical variables are density plots and the categorical variables are
mosaic plots to compare the distribution of train sets and test sets.
</p>


<h3>Value</h3>

<p>There is no return value. Draw only the plot.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

# Credit Card Default Data
head(ISLR::Default)

# Generate data for the example
sb &lt;- ISLR::Default %&gt;%
  split_by(default)

sb %&gt;%
  compare_plot("income")

sb %&gt;%
  compare_plot()
</code></pre>

<hr>
<h2 id='compare_target_category'>Comparison of categorical variables of train set and test set</h2><span id='topic+compare_target_category'></span>

<h3>Description</h3>

<p>Compare the statistics of the categorical variables of
the train set and test set included in the &quot;split_df&quot; class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare_target_category(.data, ..., add_character = FALSE, margin = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compare_target_category_+3A_.data">.data</code></td>
<td>
<p>an object of class &quot;split_df&quot;, usually, a result of a call to split_df().</p>
</td></tr>
<tr><td><code id="compare_target_category_+3A_...">...</code></td>
<td>
<p>one or more unquoted expressions separated by commas.
Select the categorical variable you want to compare.
You can treat variable names like they are positions.
Positive values select variables; negative values to drop variables.
If the first expression is negative, compare_target_category() will automatically
start with all variables.
These arguments are automatically quoted and evaluated in a context where column names
represent column positions.
They support unquoting and splicing.</p>
</td></tr>
<tr><td><code id="compare_target_category_+3A_add_character">add_character</code></td>
<td>
<p>logical. Decide whether to include text variables in the
compare of categorical data. The default value is FALSE, which also not includes character variables.</p>
</td></tr>
<tr><td><code id="compare_target_category_+3A_margin">margin</code></td>
<td>
<p>logical. Choose to calculate the marginal frequency information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Compare the statistics of the numerical variables of the train set and
the test set to determine whether the raw data is well separated into two data sets.
</p>


<h3>Value</h3>

<p>tbl_df.
Variables of tbl_df for comparison:
</p>

<ul>
<li><p> variable : character. categorical variable name
</p>
</li>
<li><p> level : factor. level of categorical variables
</p>
</li>
<li><p> train : numeric. the relative frequency of the level in the train set
</p>
</li>
<li><p> test : numeric. the relative frequency of the level in the test set
</p>
</li>
<li><p> abs_diff : numeric. the absolute value of the difference between two
relative frequencies
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

# Credit Card Default Data
head(ISLR::Default)

# Generate data for the example
sb &lt;- ISLR::Default %&gt;%
  split_by(default)

sb %&gt;%
  compare_target_category()

sb %&gt;%
  compare_target_category(add_character = TRUE)

sb %&gt;%
  compare_target_category(margin = TRUE)

sb %&gt;%
  compare_target_category(student)

sb %&gt;%
  compare_target_category(student, margin = TRUE)

</code></pre>

<hr>
<h2 id='compare_target_numeric'>Comparison of numerical variables of train set and test set</h2><span id='topic+compare_target_numeric'></span>

<h3>Description</h3>

<p>Compare the statistics of the numerical variables of
the train set and test set included in the &quot;split_df&quot; class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare_target_numeric(.data, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compare_target_numeric_+3A_.data">.data</code></td>
<td>
<p>an object of class &quot;split_df&quot;, usually, a result of a call to split_df().</p>
</td></tr>
<tr><td><code id="compare_target_numeric_+3A_...">...</code></td>
<td>
<p>one or more unquoted expressions separated by commas.
Select the numeric variable you want to compare.
You can treat variable names like they are positions.
Positive values select variables; negative values to drop variables.
If the first expression is negative, compare_target_numeric() will automatically
start with all variables.
These arguments are automatically quoted and evaluated in a context where column names
represent column positions.
They support unquoting and splicing.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Compare the statistics of the numerical variables of the train set and
the test set to determine whether the raw data is well separated into two data sets.
</p>


<h3>Value</h3>

<p>tbl_df.
Variables for comparison:
</p>

<ul>
<li><p> variable : character. numeric variable name
</p>
</li>
<li><p> train_mean : numeric. arithmetic mean of train set
</p>
</li>
<li><p> test_mean : numeric. arithmetic mean of test set
</p>
</li>
<li><p> train_sd : numeric. standard deviation of train set
</p>
</li>
<li><p> test_sd : numeric. standard deviation of test set
</p>
</li>
<li><p> train_z : numeric. the arithmetic mean of the train set divided by
the standard deviation
</p>
</li>
<li><p> test_z : numeric. the arithmetic mean of the test set divided by
the standard deviation
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

# Credit Card Default Data
head(ISLR::Default)

# Generate data for the example
sb &lt;- ISLR::Default %&gt;%
  split_by(default)

sb %&gt;%
  compare_target_numeric()

sb %&gt;%
  compare_target_numeric(balance)

</code></pre>

<hr>
<h2 id='extract_set'>Extract train/test dataset</h2><span id='topic+extract_set'></span>

<h3>Description</h3>

<p>Extract train set or test set from split_df class object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_set(x, set = c("train", "test"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extract_set_+3A_x">x</code></td>
<td>
<p>an object of class &quot;split_df&quot;, usually, a result of a call to split_df().</p>
</td></tr>
<tr><td><code id="extract_set_+3A_set">set</code></td>
<td>
<p>character. Specifies whether the extracted data is a train set or a test set.
You can use &quot;train&quot; or &quot;test&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Extract the train or test sets based on the parameters you defined when creating split_df with split_by().
</p>


<h3>Value</h3>

<p>an object of class &quot;tbl_df&quot;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

# Credit Card Default Data
head(ISLR::Default)

# Generate data for the example
sb &lt;- ISLR::Default %&gt;%
  split_by(default)

train &lt;- sb %&gt;%
  extract_set(set = "train")

test &lt;- sb %&gt;%
  extract_set(set = "test")

</code></pre>

<hr>
<h2 id='matthews'>Compute Matthews Correlation Coefficient</h2><span id='topic+matthews'></span>

<h3>Description</h3>

<p>compute the Matthews correlation coefficient with actual and predict values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matthews(predicted, y, positive)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="matthews_+3A_predicted">predicted</code></td>
<td>
<p>numeric. the predicted value of binary classification</p>
</td></tr>
<tr><td><code id="matthews_+3A_y">y</code></td>
<td>
<p>factor or character. the actual value of binary classification</p>
</td></tr>
<tr><td><code id="matthews_+3A_positive">positive</code></td>
<td>
<p>level of positive class of binary classification</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Matthews Correlation Coefficient has a value between -1 and 1, and the closer to 1,
the better the performance of the binary classification.
</p>


<h3>Value</h3>

<p>numeric. The Matthews Correlation Coefficient.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate actual data
set.seed(123L)
actual &lt;- sample(c("Y", "N"), size = 100, prob = c(0.3, 0.7), replace = TRUE)
actual

# simulate predict data
set.seed(123L)
pred &lt;- sample(c("Y", "N"), size = 100, prob = c(0.2, 0.8), replace = TRUE)
pred

# simulate confusion matrix
table(pred, actual)

matthews(pred, actual, "Y")
</code></pre>

<hr>
<h2 id='performance_metric'>Calculate metrics for model evaluation</h2><span id='topic+performance_metric'></span>

<h3>Description</h3>

<p>Calculate some representative metrics for binary classification model evaluation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>performance_metric(
  pred,
  actual,
  positive,
  metric = c("ZeroOneLoss", "Accuracy", "Precision", "Recall", "Sensitivity",
    "Specificity", "F1_Score", "Fbeta_Score", "LogLoss", "AUC", "Gini", "PRAUC",
    "LiftAUC", "GainAUC", "KS_Stat", "ConfusionMatrix"),
  cutoff = 0.5,
  beta = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="performance_metric_+3A_pred">pred</code></td>
<td>
<p>numeric. Probability values that predicts the positive class of the target variable.</p>
</td></tr>
<tr><td><code id="performance_metric_+3A_actual">actual</code></td>
<td>
<p>factor. The value of the actual target variable.</p>
</td></tr>
<tr><td><code id="performance_metric_+3A_positive">positive</code></td>
<td>
<p>character. Level of positive class of binary classification.</p>
</td></tr>
<tr><td><code id="performance_metric_+3A_metric">metric</code></td>
<td>
<p>character. The performance metrics you want to calculate. See details.</p>
</td></tr>
<tr><td><code id="performance_metric_+3A_cutoff">cutoff</code></td>
<td>
<p>numeric. Threshold for classifying predicted probability values into positive and negative classes.</p>
</td></tr>
<tr><td><code id="performance_metric_+3A_beta">beta</code></td>
<td>
<p>numeric. Weight of precision in harmonic mean for F-Beta Score.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cutoff argument applies only if the metric argument is &quot;ZeroOneLoss&quot;, &quot;Accuracy&quot;, &quot;Precision&quot;, &quot;Recall&quot;,
&quot;Sensitivity&quot;, &quot;Specificity&quot;, &quot;F1_Score&quot;, &quot;Fbeta_Score&quot;, &quot;ConfusionMatrix&quot;.
</p>


<h3>Value</h3>

<p>numeric or table object.
Confusion Matrix return by table object. and otherwise is numeric.:
The performance metrics calculated are as follows.:
</p>

<ul>
<li><p> ZeroOneLoss : Normalized Zero-One Loss(Classification Error Loss).
</p>
</li>
<li><p> Accuracy : Accuracy.
</p>
</li>
<li><p> Precision : Precision.
</p>
</li>
<li><p> Recall : Recall.
</p>
</li>
<li><p> Sensitivity : Sensitivity.
</p>
</li>
<li><p> Specificity : Specificity.
</p>
</li>
<li><p> F1_Score : F1 Score.
</p>
</li>
<li><p> Fbeta_Score : F-Beta Score.
</p>
</li>
<li><p> LogLoss : Log loss / Cross-Entropy Loss.
</p>
</li>
<li><p> AUC : Area Under the Receiver Operating Characteristic Curve (ROC AUC).
</p>
</li>
<li><p> Gini : Gini Coefficient.
</p>
</li>
<li><p> PRAUC : Area Under the Precision-Recall Curve (PR AUC).
</p>
</li>
<li><p> LiftAUC : Area Under the Lift Chart.
</p>
</li>
<li><p> GainAUC : Area Under the Gain Chart.
</p>
</li>
<li><p> KS_Stat : Kolmogorov-Smirnov Statistic.
</p>
</li>
<li><p> ConfusionMatrix : Confusion Matrix.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

# Divide the train data set and the test data set.
sb &lt;- rpart::kyphosis %&gt;%
  split_by(Kyphosis)

# Extract the train data set from original data set.
train &lt;- sb %&gt;%
  extract_set(set = "train")

# Extract the test data set from original data set.
test &lt;- sb %&gt;%
  extract_set(set = "test")

# Sampling for unbalanced data set using SMOTE(synthetic minority over-sampling technique).
train &lt;- sb %&gt;%
  sampling_target(seed = 1234L, method = "ubSMOTE")

# Cleaning the set.
train &lt;- train %&gt;%
  cleanse

# Run the model fitting.
result &lt;- run_models(.data = train, target = "Kyphosis", positive = "present")
result

# Predict the model.
pred &lt;- run_predict(result, test)
pred

# Calculate Accuracy.
performance_metric(attr(pred$predicted[[1]], "pred_prob"), test$Kyphosis,
  "present", "Accuracy")
# Calculate Confusion Matrix.
performance_metric(attr(pred$predicted[[1]], "pred_prob"), test$Kyphosis,
  "present", "ConfusionMatrix")
# Calculate Confusion Matrix by cutoff = 0.55.
performance_metric(attr(pred$predicted[[1]], "pred_prob"), test$Kyphosis,
  "present", "ConfusionMatrix", cutoff = 0.55)
   
</code></pre>

<hr>
<h2 id='plot_cutoff'>Visualization for cut-off selection</h2><span id='topic+plot_cutoff'></span>

<h3>Description</h3>

<p>plot_cutoff() visualizes a plot to select a cut-off that separates positive and
negative from the probabilities that are predictions of a binary classification,
and suggests a cut-off.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_cutoff(
  predicted,
  y,
  positive,
  type = c("mcc", "density", "prob"),
  measure = c("mcc", "cross", "half")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_cutoff_+3A_predicted">predicted</code></td>
<td>
<p>numeric. the predicted value of binary classification</p>
</td></tr>
<tr><td><code id="plot_cutoff_+3A_y">y</code></td>
<td>
<p>factor or character. the actual value of binary classification</p>
</td></tr>
<tr><td><code id="plot_cutoff_+3A_positive">positive</code></td>
<td>
<p>level of positive class of binary classification</p>
</td></tr>
<tr><td><code id="plot_cutoff_+3A_type">type</code></td>
<td>
<p>character. Visualization type. &quot;mcc&quot; draw the Matthews Correlation Coefficient scatter plot,
&quot;density&quot; draw the density plot of negative and positive,
and &quot;prob&quot; draws line or points plots of the predicted probability.</p>
</td></tr>
<tr><td><code id="plot_cutoff_+3A_measure">measure</code></td>
<td>
<p>character. The kind of measure that calculates the cutoff.
&quot;mcc&quot; is the Matthews Correlation Coefficient, &quot;cross&quot; is the point where the positive
and negative densities cross, and &quot;half&quot; is the median of the probability, 0.5</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the type argument is &quot;prob&quot;, visualize the points plot if the number of observations
is less than 100. If the observation is greater than 100, draw a line plot.
In this case, the speed of visualization can be slow.
</p>


<h3>Value</h3>

<p>numeric. cut-off value
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
library(rpart)
data(kyphosis)

fit &lt;- glm(Kyphosis ~., family = binomial, kyphosis)
pred &lt;- predict(fit, type = "response")

cutoff &lt;- plot_cutoff(pred, kyphosis$Kyphosis, "present", type = "mcc")
cutoff
plot_cutoff(pred, kyphosis$Kyphosis, "present", type = "mcc", measure = "cross")
plot_cutoff(pred, kyphosis$Kyphosis, "present", type = "mcc", measure = "half")

plot_cutoff(pred, kyphosis$Kyphosis, "present", type = "density", measure = "mcc")
plot_cutoff(pred, kyphosis$Kyphosis, "present", type = "density", measure = "cross")
plot_cutoff(pred, kyphosis$Kyphosis, "present", type = "density", measure = "half")

plot_cutoff(pred, kyphosis$Kyphosis, "present", type = "prob", measure = "mcc")
plot_cutoff(pred, kyphosis$Kyphosis, "present", type = "prob", measure = "cross")
plot_cutoff(pred, kyphosis$Kyphosis, "present", type = "prob", measure = "half")

</code></pre>

<hr>
<h2 id='plot_performance'>Visualization for ROC curve</h2><span id='topic+plot_performance'></span>

<h3>Description</h3>

<p>plot_performance() visualizes a plot to ROC curve that separates model algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_performance(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_performance_+3A_model">model</code></td>
<td>
<p>A model_df. results of predicted model that created by run_predict().</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The ROC curve is output for each model included in the model_df class object specified as a model argument.
</p>


<h3>Value</h3>

<p>There is no return value. Only the plot is drawn.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)

# Divide the train data set and the test data set.
sb &lt;- rpart::kyphosis %&gt;%
  split_by(Kyphosis)

# Extract the train data set from original data set.
train &lt;- sb %&gt;%
  extract_set(set = "train")

# Extract the test data set from original data set.
test &lt;- sb %&gt;%
  extract_set(set = "test")

# Sampling for unbalanced data set using SMOTE(synthetic minority over-sampling technique).
train &lt;- sb %&gt;%
  sampling_target(seed = 1234L, method = "ubSMOTE")

# Cleaning the set.
train &lt;- train %&gt;%
  cleanse

# Run the model fitting.
result &lt;- run_models(.data = train, target = "Kyphosis", positive = "present")

# Predict the model.
pred &lt;- run_predict(result, test)

# Plot ROC curve
plot_performance(pred)


</code></pre>

<hr>
<h2 id='run_models'>Fit binary classification model</h2><span id='topic+run_models'></span>

<h3>Description</h3>

<p>Fit some representative binary classification models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_models(
  .data,
  target,
  positive,
  models = c("logistic", "rpart", "ctree", "randomForest", "ranger", "xgboost", "lasso")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="run_models_+3A_.data">.data</code></td>
<td>
<p>A train_df. Train data to fit the model. It also supports tbl_df, 
tbl, and data.frame objects.</p>
</td></tr>
<tr><td><code id="run_models_+3A_target">target</code></td>
<td>
<p>character. Name of target variable.</p>
</td></tr>
<tr><td><code id="run_models_+3A_positive">positive</code></td>
<td>
<p>character. Level of positive class of binary classification.</p>
</td></tr>
<tr><td><code id="run_models_+3A_models">models</code></td>
<td>
<p>character. Algorithm types of model to fit. See details. 
default value is c(&quot;logistic&quot;, &quot;rpart&quot;, &quot;ctree&quot;, &quot;randomForest&quot;, &quot;ranger&quot;, &quot;lasso&quot;).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Supported models are functions supported by the representative 
model package used in R environment.
The following binary classifications are supported:
</p>

<ul>
<li><p> &quot;logistic&quot; : logistic regression by glm() in stats package.
</p>
</li>
<li><p> &quot;rpart&quot; : recursive partitioning tree model by rpart() in rpart package.
</p>
</li>
<li><p> &quot;ctree&quot; : conditional inference tree model by ctree() in party package.
</p>
</li>
<li><p> &quot;randomForest&quot; : random forest model by randomForest() in 
randomForest package.
</p>
</li>
<li><p> &quot;ranger&quot; : random forest model by ranger() in ranger package.
</p>
</li>
<li><p> &quot;xgboost&quot; : XGBoosting model by xgboost() in xgboost package.
</p>
</li>
<li><p> &quot;lasso&quot; : lasso model by glmnet() in glmnet package.
</p>
</li></ul>

<p>run_models() executes the process in parallel when fitting the model. 
However, it is not supported in MS-Windows operating system and RStudio 
environment.
</p>


<h3>Value</h3>

<p>model_df. results of fitted model.
model_df is composed of tbl_df and contains the following variables.:
</p>

<ul>
<li><p> step : character. The current stage in the model fit process. 
The result of calling run_models() is returned as &quot;1.Fitted&quot;.
</p>
</li>
<li><p> model_id : character. Type of fit model.
</p>
</li>
<li><p> target : character. Name of target variable.
</p>
</li>
<li><p> is_factor : logical. Indicates whether the target variable is a factor. 
</p>
</li>
<li><p> positive : character. Level of positive class of binary classification.
</p>
</li>
<li><p> negative : character. Level of negative class of binary classification. 
</p>
</li>
<li><p> fitted_model : list. Fitted model object.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

# Divide the train data set and the test data set.
sb &lt;- rpart::kyphosis %&gt;%
  split_by(Kyphosis)

# Extract the train data set from original data set.
train &lt;- sb %&gt;%
  extract_set(set = "train")

# Extract the test data set from original data set.
test &lt;- sb %&gt;%
  extract_set(set = "test")

# Sampling for unbalanced data set using SMOTE(synthetic minority over-sampling technique).
train &lt;- sb %&gt;%
  sampling_target(seed = 1234L, method = "ubSMOTE")

# Cleaning the set.
train &lt;- train %&gt;%
  cleanse

# Run the model fitting.
result &lt;- run_models(.data = train, target = "Kyphosis", positive = "present")
result

# Run the several kinds model fitting by dplyr
train %&gt;%
  run_models(target = "Kyphosis", positive = "present")

</code></pre>

<hr>
<h2 id='run_performance'>Apply calculate performance metrics for model evaluation</h2><span id='topic+run_performance'></span>

<h3>Description</h3>

<p>Apply calculate performance metrics for binary classification model evaluation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_performance(model, actual = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="run_performance_+3A_model">model</code></td>
<td>
<p>A model_df. results of predicted model that created by run_predict().</p>
</td></tr>
<tr><td><code id="run_performance_+3A_actual">actual</code></td>
<td>
<p>factor. A data of target variable to evaluate the model. It supports factor that has binary class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>run_performance() is performed in parallel when calculating the performance evaluation index. 
However, it is not supported in MS-Windows operating system and RStudio environment.
</p>


<h3>Value</h3>

<p>model_df. results of predicted model.
model_df is composed of tbl_df and contains the following variables.:
</p>

<ul>
<li><p> step : character. The current stage in the model fit process. The result of calling run_performance() is returned as &quot;3.Performanced&quot;.
</p>
</li>
<li><p> model_id : character. Type of fit model.
</p>
</li>
<li><p> target : character. Name of target variable.
</p>
</li>
<li><p> positive : character. Level of positive class of binary classification.
</p>
</li>
<li><p> fitted_model : list. Fitted model object.
</p>
</li>
<li><p> predicted : list. Predicted value by individual model. Each value has a predict_class class object.
</p>
</li>
<li><p> performance : list. Calculate metrics by individual model. Each value has a numeric vector.
</p>
</li></ul>

<p>The performance metrics calculated are as follows.:
</p>

<ul>
<li><p> ZeroOneLoss : Normalized Zero-One Loss(Classification Error Loss).
</p>
</li>
<li><p> Accuracy : Accuracy.
</p>
</li>
<li><p> Precision : Precision.
</p>
</li>
<li><p> Recall : Recall.
</p>
</li>
<li><p> Sensitivity : Sensitivity.
</p>
</li>
<li><p> Specificity : Specificity.
</p>
</li>
<li><p> F1_Score : F1 Score.
</p>
</li>
<li><p> Fbeta_Score : F-Beta Score.
</p>
</li>
<li><p> LogLoss : Log loss / Cross-Entropy Loss.
</p>
</li>
<li><p> AUC : Area Under the Receiver Operating Characteristic Curve (ROC AUC).
</p>
</li>
<li><p> Gini : Gini Coefficient.
</p>
</li>
<li><p> PRAUC : Area Under the Precision-Recall Curve (PR AUC).
</p>
</li>
<li><p> LiftAUC : Area Under the Lift Chart.
</p>
</li>
<li><p> GainAUC : Area Under the Gain Chart.
</p>
</li>
<li><p> KS_Stat : Kolmogorov-Smirnov Statistic.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)

# Divide the train data set and the test data set.
sb &lt;- rpart::kyphosis %&gt;%
  split_by(Kyphosis)

# Extract the train data set from original data set.
train &lt;- sb %&gt;%
  extract_set(set = "train")

# Extract the test data set from original data set.
test &lt;- sb %&gt;%
  extract_set(set = "test")

# Sampling for unbalanced data set using SMOTE(synthetic minority over-sampling technique).
train &lt;- sb %&gt;%
  sampling_target(seed = 1234L, method = "ubSMOTE")

# Cleaning the set.
train &lt;- train %&gt;%
  cleanse

# Run the model fitting.
result &lt;- run_models(.data = train, target = "Kyphosis", positive = "present")
result

# Predict the model. (Case 1)
pred &lt;- run_predict(result, test)
pred

# Calculate performace metrics. (Case 1)
perf &lt;- run_performance(pred)
perf
perf$performance

# Predict the model. (Case 2)
pred &lt;- run_predict(result, test[, -1])
pred

# Calculate performace metrics. (Case 2)
perf &lt;- run_performance(pred, pull(test[, 1]))
perf
perf$performance

# Convert to matrix for compare performace.
sapply(perf$performance, "c")


</code></pre>

<hr>
<h2 id='run_predict'>Predict binary classification model</h2><span id='topic+run_predict'></span>

<h3>Description</h3>

<p>Predict some representative binary classification models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_predict(model, .data, cutoff = 0.5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="run_predict_+3A_model">model</code></td>
<td>
<p>A model_df. results of fitted model that created by run_models().</p>
</td></tr>
<tr><td><code id="run_predict_+3A_.data">.data</code></td>
<td>
<p>A tbl_df. The data set to predict the model. It also supports tbl, and data.frame objects.</p>
</td></tr>
<tr><td><code id="run_predict_+3A_cutoff">cutoff</code></td>
<td>
<p>numeric. Cut-off that determines the positive from the probability of predicting the positive.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Supported models are functions supported by the representative model package used in R environment.
The following binary classifications are supported:
</p>

<ul>
<li><p> &quot;logistic&quot; : logistic regression by predict.glm() in stats package.
</p>
</li>
<li><p> &quot;rpart&quot; : recursive partitioning tree model by predict.rpart() in rpart package.
</p>
</li>
<li><p> &quot;ctree&quot; : conditional inference tree model by predict() in stats package.
</p>
</li>
<li><p> &quot;randomForest&quot; : random forest model by predict.randomForest() in randomForest package.
</p>
</li>
<li><p> &quot;ranger&quot; : random forest model by predict.ranger() in ranger package.
</p>
</li>
<li><p> &quot;xgboost&quot; : random forest model by predict.xgb.Booster() in xgboost package.
</p>
</li>
<li><p> &quot;lasso&quot; : random forest model by predict.glmnet() in glmnet package.
</p>
</li></ul>

<p>run_predict() is executed in parallel when predicting by model. 
However, it is not supported in MS-Windows operating system and RStudio environment.
</p>


<h3>Value</h3>

<p>model_df. results of predicted model.
model_df is composed of tbl_df and contains the following variables.:
</p>

<ul>
<li><p> step : character. The current stage in the model fit process. The result of calling run_predict() is returned as &quot;2.Predicted&quot;.
</p>
</li>
<li><p> model_id : character. Type of fit model.
</p>
</li>
<li><p> target : character. Name of target variable.
</p>
</li>
<li><p> is_factor : logical. Indicates whether the target variable is a factor.
</p>
</li>
<li><p> positive : character. Level of positive class of binary classification.
</p>
</li>
<li><p> negative : character. Level of negative class of binary classification.
</p>
</li>
<li><p> fitted_model : list. Fitted model object.
</p>
</li>
<li><p> predicted : list. Predicted value by individual model. Each value has a predict_class class object.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

# Divide the train data set and the test data set.
sb &lt;- rpart::kyphosis %&gt;%
  split_by(Kyphosis)

# Extract the train data set from original data set.
train &lt;- sb %&gt;%
  extract_set(set = "train")

# Extract the test data set from original data set.
test &lt;- sb %&gt;%
  extract_set(set = "test")

# Sampling for unbalanced data set using SMOTE(synthetic minority over-sampling technique).
train &lt;- sb %&gt;%
  sampling_target(seed = 1234L, method = "ubSMOTE")

# Cleaning the set.
train &lt;- train %&gt;%
  cleanse

# Run the model fitting.
result &lt;- run_models(.data = train, target = "Kyphosis", positive = "present")
result

# Run the several kinds model predict by dplyr
result %&gt;%
  run_predict(test)

</code></pre>

<hr>
<h2 id='sampling_target'>Extract the data to fit the model</h2><span id='topic+sampling_target'></span>

<h3>Description</h3>

<p>To solve the imbalanced class, perform sampling in the train set of split_df.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampling_target(
  .data,
  method = c("ubUnder", "ubOver", "ubSMOTE"),
  seed = NULL,
  perc = 50,
  k = ifelse(method == "ubSMOTE", 5, 0),
  perc.over = 200,
  perc.under = 200
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sampling_target_+3A_.data">.data</code></td>
<td>
<p>an object of class &quot;split_df&quot;, usually, a result of a call to split_df().</p>
</td></tr>
<tr><td><code id="sampling_target_+3A_method">method</code></td>
<td>
<p>character. sampling methods. &quot;ubUnder&quot; is under-sampling,
and &quot;ubOver&quot; is over-sampling, &quot;ubSMOTE&quot; is SMOTE(Synthetic Minority Over-sampling TEchnique).</p>
</td></tr>
<tr><td><code id="sampling_target_+3A_seed">seed</code></td>
<td>
<p>integer. random seed used for sampling</p>
</td></tr>
<tr><td><code id="sampling_target_+3A_perc">perc</code></td>
<td>
<p>integer. The percentage of positive class in the final dataset.
It is used only in under-sampling. The default is 50. perc can not exceed 50.</p>
</td></tr>
<tr><td><code id="sampling_target_+3A_k">k</code></td>
<td>
<p>integer. It is used only in over-sampling and SMOTE.
If over-sampling and if K=0: sample with replacement from the minority class until
we have the same number of instances in each class. under-sampling and if K&gt;0:
sample with replacement from the minority class until we have k-times
the original number of minority instances.
If SMOTE, the number of neighbours to consider as the pool from where the new
examples are generated</p>
</td></tr>
<tr><td><code id="sampling_target_+3A_perc.over">perc.over</code></td>
<td>
<p>integer. It is used only in SMOTE. per.over/100 is the number of new instances
generated for each rare instance. If perc.over &lt; 100 a single instance is generated.</p>
</td></tr>
<tr><td><code id="sampling_target_+3A_perc.under">perc.under</code></td>
<td>
<p>integer. It is used only in SMOTE. perc.under/100 is the number
of &quot;normal&quot; (majority class) instances that are randomly selected for each smoted
observation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In order to solve the problem of imbalanced class, sampling is performed by under sampling,
over sampling, SMOTE method.
</p>


<h3>Value</h3>

<p>An object of train_df.
</p>


<h3>attributes of train_df class</h3>

<p>The attributes of the train_df class are as follows.:
</p>

<ul>
<li><p> sample_seed : integer. random seed used for sampling
</p>
</li>
<li><p> method : character. sampling methods.
</p>
</li>
<li><p> perc : integer. perc argument value
</p>
</li>
<li><p> k : integer. k argument value
</p>
</li>
<li><p> perc.over : integer. perc.over argument value
</p>
</li>
<li><p> perc.under : integer. perc.under argument value
</p>
</li>
<li><p> binary : logical. whether the target variable is a binary class
</p>
</li>
<li><p> target : character. target variable name
</p>
</li>
<li><p> minority : character. the level of the minority class
</p>
</li>
<li><p> majority : character. the level of the majority class
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

# Credit Card Default Data
head(ISLR::Default)

# Generate data for the example
sb &lt;- ISLR::Default %&gt;%
  split_by(default)

# under-sampling with random seed
under &lt;- sb %&gt;%
  sampling_target(seed = 1234L)

under %&gt;%
  count(default)

# under-sampling with random seed, and minority class frequency is 40%
under40 &lt;- sb %&gt;%
  sampling_target(seed = 1234L, perc = 40)

under40 %&gt;%
  count(default)

# over-sampling with random seed
over &lt;- sb %&gt;%
  sampling_target(method = "ubOver", seed = 1234L)

over %&gt;%
  count(default)

# over-sampling with random seed, and k = 10
over10 &lt;- sb %&gt;%
  sampling_target(method = "ubOver", seed = 1234L, k = 10)

over10 %&gt;%
  count(default)

# SMOTE with random seed
smote &lt;- sb %&gt;%
  sampling_target(method = "ubSMOTE", seed = 1234L)

smote %&gt;%
  count(default)

# SMOTE with random seed, and perc.under = 250
smote250 &lt;- sb %&gt;%
  sampling_target(method = "ubSMOTE", seed = 1234L, perc.under = 250)

smote250 %&gt;%
  count(default)

</code></pre>

<hr>
<h2 id='split_by'>Split Data into Train and Test Set</h2><span id='topic+split_by'></span><span id='topic+split_by.data.frame'></span>

<h3>Description</h3>

<p>The split_by() splits the data.frame or tbl_df into a train set and a test set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>split_by(.data, ...)

## S3 method for class 'data.frame'
split_by(.data, target, ratio = 0.7, seed = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="split_by_+3A_.data">.data</code></td>
<td>
<p>a data.frame or a <code>tbl_df</code>.</p>
</td></tr>
<tr><td><code id="split_by_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="split_by_+3A_target">target</code></td>
<td>
<p>unquoted expression or variable name. the name of the target variable</p>
</td></tr>
<tr><td><code id="split_by_+3A_ratio">ratio</code></td>
<td>
<p>numeric. the ratio of the train dataset. default is 0.7</p>
</td></tr>
<tr><td><code id="split_by_+3A_seed">seed</code></td>
<td>
<p>random seed used for splitting</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The split_df class is created, which contains the split information and criteria to separate the training and the test set.
</p>


<h3>Value</h3>

<p>An object of split_by.
</p>


<h3>attributes of split_by</h3>

<p>The attributes of the split_df class are as follows.:
</p>

<ul>
<li><p> split_seed : integer. random seed used for splitting
</p>
</li>
<li><p> target : character. the name of the target variable
</p>
</li>
<li><p> binary : logical. whether the target variable is binary class
</p>
</li>
<li><p> minority : character. the name of the minority class
</p>
</li>
<li><p> majority : character. the name of the majority class
</p>
</li>
<li><p> minority_rate : numeric. the rate of the minority class
</p>
</li>
<li><p> majority_rate : numeric. the rate of the majority class
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

# Credit Card Default Data
head(ISLR::Default)

# Generate data for the example
sb &lt;- ISLR::Default %&gt;%
  split_by(default)

sb

</code></pre>

<hr>
<h2 id='summary.split_df'>Summarizing split_df information</h2><span id='topic+summary.split_df'></span>

<h3>Description</h3>

<p>summary method for &quot;split_df&quot; class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'split_df'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.split_df_+3A_object">object</code></td>
<td>
<p>an object of class &quot;split_df&quot;, usually, a result of a call to split_df().</p>
</td></tr>
<tr><td><code id="summary.split_df_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>summary.split_df provides information on the number of two split data sets, minority class and majority class.
</p>


<h3>Value</h3>

<p>NULL is returned. 
However, the split train set and test set information are displayed. The output information is as follows.:
</p>

<ul>
<li><p> Random seed
</p>
</li>
<li><p> Number of train sets and test sets
</p>
</li>
<li><p> Name of target variable
</p>
</li>
<li><p> Target variable minority class and majority class information (label and ratio)
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

# Credit Card Default Data
head(ISLR::Default)

# Generate data for the example
sb &lt;- ISLR::Default %&gt;%
  split_by(default)

sb
summary(sb)

</code></pre>

<hr>
<h2 id='treatment_corr'>Diagnosis and removal of highly correlated variables</h2><span id='topic+treatment_corr'></span>

<h3>Description</h3>

<p>The treatment_corr() diagnose pairs of highly correlated variables or remove on of them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>treatment_corr(.data, corr_thres = 0.8, treat = TRUE, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="treatment_corr_+3A_.data">.data</code></td>
<td>
<p>a data.frame or a <code>tbl_df</code>.</p>
</td></tr>
<tr><td><code id="treatment_corr_+3A_corr_thres">corr_thres</code></td>
<td>
<p>numeric. Set a threshold to detecting variables when correlation greater then threshold.</p>
</td></tr>
<tr><td><code id="treatment_corr_+3A_treat">treat</code></td>
<td>
<p>logical. Set whether to removing variables</p>
</td></tr>
<tr><td><code id="treatment_corr_+3A_verbose">verbose</code></td>
<td>
<p>logical. Set whether to echo information to the console at runtime.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The correlation coefficient of pearson is obtained for continuous variables and the correlation coefficient of spearman for categorical variables.
</p>


<h3>Value</h3>

<p>An object of data.frame or train_df. and return value is an object of the same type as the .data argument. However, several variables can be excluded by correlation between variables.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># numerical variable
x1 &lt;- 1:100
set.seed(12L)
x2 &lt;- sample(1:3, size = 100, replace = TRUE) * x1 + rnorm(1)
set.seed(1234L)
x3 &lt;- sample(1:2, size = 100, replace = TRUE) * x1 + rnorm(1)

# categorical variable
x4 &lt;- factor(rep(letters[1:20], time = 5))
set.seed(100L)
x5 &lt;- factor(rep(letters[1:20 + sample(1:6, size = 20, replace = TRUE)], time = 5))
set.seed(200L)
x6 &lt;- factor(rep(letters[1:20 + sample(1:3, size = 20, replace = TRUE)], time = 5))
set.seed(300L)
x7 &lt;- factor(sample(letters[1:5], size = 100, replace = TRUE))

exam &lt;- data.frame(x1, x2, x3, x4, x5, x6, x7)
str(exam)
head(exam)

# default case
treatment_corr(exam)

# not removing variables
treatment_corr(exam, treat = FALSE)

# Set a threshold to detecting variables when correlation greater then 0.9
treatment_corr(exam, corr_thres = 0.9, treat = FALSE)

# not verbose mode
treatment_corr(exam, verbose = FALSE)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
