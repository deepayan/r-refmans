<!DOCTYPE html><html><head><title>Help for package l2boost</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {l2boost}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#l2boost-package'><p>Efficient implementation of Friedman's boosting algorithm for linear regression using an</p>
l2-loss function and coordinate direction (design matrix columns) basis functions.</a></li>
<li><a href='#coef.l2boost'><p>Extract model coefficients from an l2boost model object at any point along the solution path indexed by step m.</p>
</p>
<p><code>coef</code> is a generic function which extracts model coefficients from objects returned by modeling functions.</p></a></li>
<li><a href='#cv.l2boost'><p>K-fold cross-validation using <code>l2boost</code>.</p></a></li>
<li><a href='#diabetes'><p>Blood and other measurements in diabetics [Hastie and Efron (2012)]</p></a></li>
<li><a href='#elasticNetSim'><p>A blocked correlated data simulation.</p></a></li>
<li><a href='#error.bars'><p>nice standard errors for plots</p></a></li>
<li><a href='#fitted.l2boost'><p>Extract the fitted model estimates along the solution path for an l2boost model.</p></a></li>
<li><a href='#l2boost'><p>Generic gradient descent boosting method for linear regression.</p></a></li>
<li><a href='#mvnorm.l2boost'><p>multivariate normal data simulations.</p></a></li>
<li><a href='#plot.l2boost'><p>Plotting for <code>l2boost</code> objects.</p></a></li>
<li><a href='#plot.lines'><p>plots.lines is used by <code>plot.l2boost</code> to the path lines (each j, against each r-step)</p></a></li>
<li><a href='#predict.l2boost'><p>predict method for l2boost models.</p></a></li>
<li><a href='#print.l2boost'><p>print method for <code>l2boost</code> and <code>cv.l2boost</code> objects.</p></a></li>
<li><a href='#print.summary.l2boost'><p>Unimplemented generic function</p>
These are placeholders right now.</a></li>
<li><a href='#residuals.l2boost'><p>Model residuals for the training set of an l2boost model object</p></a></li>
<li><a href='#summary.l2boost'><p>Unimplemented generic function</p>
These are placeholders right now.</a></li>
<li><a href='#VAR'><p>This is a hidden function of the l2boost package.</p>
VAR is a helper function that specifically returns NA if all
values of the argument x are NA, otherwise, it returns a var
object.</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Exploring Friedman's Boosting Algorithm for Regularized Linear
Regression</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-02-10</td>
</tr>
<tr>
<td>Description:</td>
<td>Efficient implementation of Friedman's boosting algorithm with
    l2-loss function and coordinate direction (design matrix columns) basis
    functions.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.14.0), MASS, parallel</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-02-10 17:21:09 UTC; jehrling</td>
</tr>
<tr>
<td>Author:</td>
<td>John Ehrlinger [aut, cre],
  Hemant Ishwaran [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>John Ehrlinger &lt;john.ehrlinger@gmail.com&gt;</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-02-11 12:10:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='l2boost-package'>Efficient implementation of Friedman's boosting algorithm for linear regression using an
l2-loss function and coordinate direction (design matrix columns) basis functions.</h2><span id='topic+l2boost-package'></span>

<h3>Description</h3>

<p>The l2boost package implements a generic boosting method [Friedman (2001)] for linear regression settings using an 
l2-loss function. The basis functions are simply the column vectors of the design matrix. <code><a href="#topic+l2boost">l2boost</a></code> 
scales the design matrix such that the boosting coefficients correspond to the gradient direction for each
covariate. Friedman's gradient descent boosting algorithm proceeds at each step along the covariate direction closest
(in L2 distance) to the maximal gradient descent direction.
</p>
<p>The <code><a href="#topic+l2boost">l2boost</a></code> function uses an arbitrary L1-regularization parameter (nu), and includes the elementary 
data augmentation of Ehrlinger and Ishwaran (2012), to add an L2-penalization (lambda) similar to the elastic net 
[Zou and Hastie (2005)]. The L2-regularization reverses repressibility, a condition where one variable acts as 
a boosting surrogate for other, possibly informative, variables. Along with the decorrelation 
effect, this elasticBoost regularization circumvents L2Boost deficiencies in correlated settings. 
</p>
<p>We include a series of S3 functions for working  with <code><a href="#topic+l2boost">l2boost</a></code> objects:
</p>

<ul>
<li> <p><code><a href="base.html#topic+print">print</a></code> (<code><a href="#topic+print.l2boost">print.l2boost</a></code>) prints a summary of the l2boost model fit.
</p>
</li>
<li> <p><code><a href="stats.html#topic+coef">coef</a></code> (<code><a href="#topic+coef.l2boost">coef.l2boost</a></code>) returns the model regression coefficients at any point along the solution path indexed by step m. 
</p>
</li>
<li> <p><code><a href="stats.html#topic+fitted">fitted</a></code> (<code><a href="#topic+fitted.l2boost">fitted.l2boost</a></code>) returns the fitted response values from the training set at any point along the solution path.
</p>
</li>
<li> <p><code><a href="stats.html#topic+residuals">residuals</a></code> (<code><a href="#topic+residuals.l2boost">residuals.l2boost</a></code>) returns the training set residuals at any point along the solution path.
</p>
</li>
<li> <p><code><a href="graphics.html#topic+plot">plot</a></code> (<code><a href="#topic+plot.l2boost">plot.l2boost</a></code>) for graphing either beta coefficients or gradient-correlation as a function of boosting steps.
</p>
</li>
<li> <p><code><a href="stats.html#topic+predict">predict</a></code> (<code><a href="#topic+predict.l2boost">predict.l2boost</a></code>) for boosting prediction on possibly new observations at any point along the solution path.
</p>
</li></ul>

<p>A cross-validation method (<code><a href="#topic+cv.l2boost">cv.l2boost</a></code>) is also included for L2boost and elasticBoost 
cross-validating regularization parameter optimizations.
</p>
<p><em>Example Datasets</em>
We have repackaged the <code><a href="#topic+diabetes">diabetes</a></code> data set from Efron et. al. (2004) for demonstration purposes.
We also include data simulation functions for reproducing the elastic net 
simulation (<code><a href="#topic+elasticNetSim">elasticNetSim</a></code>) of Zou and Hastie (2005) and the example multivariate normal simulations
(<code><a href="#topic+mvnorm.l2boost">mvnorm.l2boost</a></code>) of Ehrlinger and Ishwaran (2012).
</p>


<h3>References</h3>

<p>Friedman J. (2001) Greedy function approximation: A gradient boosting machine. <em>Ann. Statist.</em>, 29:1189-1232
</p>
<p>Ehrlinger J., and Ishwaran H. (2012). &quot;Characterizing l2boosting&quot; <em>Ann. Statist.</em>, 40 (2), 1074-1101
</p>
<p>Zou H. and Hastie T (2005) &quot;Regularization and variable selection via the elastic net&quot;  <em>J. R. Statist. Soc. B</em>, 67, Part 2, pp. 301-320
</p>
<p>Efron B., Hastie T., Johnstone I., and Tibshirani R. (2004). &quot;Least Angle Regression&quot; <em>Ann. Statist.</em> 32:407-499
</p>

<hr>
<h2 id='coef.l2boost'>Extract model coefficients from an l2boost model object at any point along the solution path indexed by step m.
<code><a href="stats.html#topic+coef">coef</a></code> is a generic function which extracts model coefficients from objects returned by modeling functions.</h2><span id='topic+coef.l2boost'></span>

<h3>Description</h3>

<p>By default, <code><a href="#topic+coef.l2boost">coef.l2boost</a></code> returns the model (beta) coefficients from the last step, 
M of the <code><a href="#topic+l2boost">l2boost</a></code> model. For a <code><a href="#topic+cv.l2boost">cv.l2boost</a></code> object, the default returns the coefficients from 
model at the cross-validation optimal step (<em>m = opt.step</em> return value).
</p>
<p>Coefficients from alternative steps along the solution can be obtained using the <em>m</em> parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'l2boost'
coef(object, m = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.l2boost_+3A_object">object</code></td>
<td>
<p>an l2boost fit object (<code><a href="#topic+l2boost">l2boost</a></code> or <code><a href="#topic+cv.l2boost">cv.l2boost</a></code>)</p>
</td></tr>
<tr><td><code id="coef.l2boost_+3A_m">m</code></td>
<td>
<p>the iteration number within the l2boost solution path. If m=NULL, the coefficients are obtained from the last iteration M.</p>
</td></tr>
<tr><td><code id="coef.l2boost_+3A_...">...</code></td>
<td>
<p>other arguments passed to generic function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of coefficient estimates for <code><a href="#topic+l2boost">l2boost</a></code> objects. 
The estimates correspond to the given iteration number <em>m</em>, or the final step <em>M</em>.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+coef">coef</a></code> and <code><a href="#topic+l2boost">l2boost</a></code>, <code><a href="#topic+cv.l2boost">cv.l2boost</a></code> and 
<code><a href="#topic+predict.l2boost">predict.l2boost</a></code> methods of l2boost.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#--------------------------------------------------------------------------
# Example: Diabetes data 
#  
# See Efron B., Hastie T., Johnstone I., and Tibshirani R. 
# Least angle regression. Ann. Statist., 32:407-499, 2004.
data(diabetes, package='l2boost')

object &lt;- l2boost(diabetes$x,diabetes$y, M=1000, nu=.01)
coef(object)

# At the m=500 step
coef(object, m=500)

</code></pre>

<hr>
<h2 id='cv.l2boost'>K-fold cross-validation using <code><a href="#topic+l2boost">l2boost</a></code>.</h2><span id='topic+cv.l2boost'></span>

<h3>Description</h3>

<p>Calculate the K-fold cross-validation prediction error for <code><a href="#topic+l2boost">l2boost</a></code> models.
The prediction error is calculated using mean squared error (MSE). The optimal boosting step (<em>m=opt.step</em>)
is obtained by selecting the step <em>m</em> resulting in the minimal MSE.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.l2boost(
  x,
  y,
  K = 10,
  M = NULL,
  nu = 1e-04,
  lambda = NULL,
  trace = FALSE,
  type = c("discrete", "hybrid", "friedman", "lars"),
  cores = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.l2boost_+3A_x">x</code></td>
<td>
<p>the design matrix</p>
</td></tr>
<tr><td><code id="cv.l2boost_+3A_y">y</code></td>
<td>
<p>the response vector</p>
</td></tr>
<tr><td><code id="cv.l2boost_+3A_k">K</code></td>
<td>
<p>number of cross-validation folds (default: 10)</p>
</td></tr>
<tr><td><code id="cv.l2boost_+3A_m">M</code></td>
<td>
<p>the total number of iterations passed to <code><a href="#topic+l2boost">l2boost</a></code>.</p>
</td></tr>
<tr><td><code id="cv.l2boost_+3A_nu">nu</code></td>
<td>
<p>l1 shrinkage parameter (default: 1e-4)</p>
</td></tr>
<tr><td><code id="cv.l2boost_+3A_lambda">lambda</code></td>
<td>
<p>l2 shrinkage parameter for elasticBoost (default: NULL = no l2-regularization)</p>
</td></tr>
<tr><td><code id="cv.l2boost_+3A_trace">trace</code></td>
<td>
<p>Show computation/debugging output? (default: FALSE)</p>
</td></tr>
<tr><td><code id="cv.l2boost_+3A_type">type</code></td>
<td>
<p>Type of l2boost fit with (default: discrete) see <code><a href="#topic+l2boost">l2boost</a></code> for description.</p>
</td></tr>
<tr><td><code id="cv.l2boost_+3A_cores">cores</code></td>
<td>
<p>number of cores to parallel the cv analysis. If not specified, detects the 
number of cores. If more than 1 core, use n-1 for cross-validation. Implemented using 
multicore (mclapply), or clusterApply on Windows machines.</p>
</td></tr>
<tr><td><code id="cv.l2boost_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="#topic+l2boost">l2boost</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cross-validation method splits the test data set into K mutually exclusive subsets. An <code><a href="#topic+l2boost">l2boost</a></code> model 
is built on K different training data sets, each created from a subsample of the full data set by sequentially leaving 
out one of the K subsets. The prediction error estimate is calculated by averaging the mean square error of each K test 
sets of the all of the K training datasets. The optimal step <em>m</em> is obtained at the step with a minimal averaged 
mean square error.
</p>
<p>The full <code><a href="#topic+l2boost">l2boost</a></code> model is run after the cross-validation models, on the full data set. This model is
run for the full number of iteration steps <em>M</em> and returned in the cv.l2boost$fit object. 
</p>
<p><code><a href="#topic+cv.l2boost">cv.l2boost</a></code> only optimizes along the iteration count <em>m</em> for a given value of <em>nu</em>. This is
equivalent to an L1-regularization optimization. In order to optimize an elasticBoost model on the L2-regularization 
parameter lambda, a manual two way cross-validation can be obtained by sequentially optimizing over a range of lambda 
values, and selecting the lambda/opt.step pair resulting in the minimal cross-validated mean square error. See the 
examples below.
</p>
<p><code><a href="#topic+cv.l2boost">cv.l2boost</a></code> uses the parallel package internally to speed up the cross-validation process on multicore 
machines. Parallel is packaged with base R &gt;= 2.14, for earlier releases the multicore package provides the same 
functionality. By default, <code><a href="#topic+cv.l2boost">cv.l2boost</a></code> will use all cores available except 1. Each fold is run on it's
own core and results are combined automatically. The number of cores can be overridden using the <em>cores</em> function
argument.
</p>


<h3>Value</h3>

<p>A list of cross-validation results:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>   
<tr><td><code>type</code></td>
<td>
<p>Choice of l2boost algorithm from  &quot;discrete&quot;, &quot;hybrid&quot;, &quot;friedman&quot;,&quot;lars&quot;. see <code><a href="#topic+l2boost">l2boost</a></code></p>
</td></tr>       
<tr><td><code>names</code></td>
<td>
<p>design matrix column names used in the model</p>
</td></tr>
<tr><td><code>nu</code></td>
<td>
<p>The L1 boosting shrinkage parameter value</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>The L2 elasticBoost shrinkage parameter value</p>
</td></tr>                   
<tr><td><code>K</code></td>
<td>
<p>number of folds used for cross-validation</p>
</td></tr>             
<tr><td><code>mse</code></td>
<td>
<p>Optimal cross-validation mean square error estimate </p>
</td></tr>      
<tr><td><code>mse.list</code></td>
<td>
<p>list of <em>K</em> vectors of mean square errors at each step <em>m</em></p>
</td></tr>            
<tr><td><code>coef</code></td>
<td>
<p>beta coefficient estimates from the full model at opt.step</p>
</td></tr>     
<tr><td><code>coef.stand</code></td>
<td>
<p>standardized beta coefficient estimates from full model at opt.step</p>
</td></tr>    
<tr><td><code>opt.step</code></td>
<td>
<p>optimal step m calculated by minimizing cross-validation error among all K training sets</p>
</td></tr>           
<tr><td><code>opt.norm</code></td>
<td>
<p>L1 norm of beta coefficients at opt.step</p>
</td></tr>        
<tr><td><code>fit</code></td>
<td>
<p><code><a href="#topic+l2boost">l2boost</a></code> fit of full model</p>
</td></tr>
<tr><td><code>yhat</code></td>
<td>
<p>estimate of response from full model at opt.step</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+l2boost">l2boost</a></code>, <code><a href="#topic+plot.l2boost">plot.l2boost</a></code>, 
<code><a href="#topic+predict.l2boost">predict.l2boost</a></code> and <code><a href="#topic+coef.l2boost">coef.l2boost</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#--------------------------------------------------------------------------
# Example: ElasticBoost simulation
# Compare l2boost and elasticNetBoosting using 10-fold CV
# 
# Elastic net simulation, see Zou H. and Hastie T. Regularization and 
# variable selection via the elastic net. J. Royal Statist. Soc. B, 
# 67(2):301-320, 2005
set.seed(1025)
dta &lt;- elasticNetSim(n=100)

# The default values set up the signal on 3 groups of 5 variables,
# Color the signal variables red, others are grey.
sig &lt;- c(rep("red", 15), rep("grey", 40-15))

# Set the boosting parameters
Mtarget = 1000
nuTarget = 1.e-2

# For CRAN, only use 2 cores in the CV method
cvCores=2

# 10 fold l2boost CV  
cv.obj &lt;- cv.l2boost(dta$x,dta$y,M=Mtarget, nu=nuTarget, cores=cvCores)

# Plot the results
par(mfrow=c(2,3))
plot(cv.obj)
abline(v=cv.obj$opt.step, lty=2, col="grey")
plot(cv.obj$fit, type="coef", ylab=expression(beta[i]))
abline(v=cv.obj$opt.step, lty=2, col="grey")
plot(coef(cv.obj$fit, m=cv.obj$opt.step), cex=.5, 
  ylab=expression(beta[i]), xlab="Column Index", ylim=c(0,140), col=sig)

# elasticBoost l1-regularization parameter lambda=0.1 
# 5 fold elasticNet CV
cv.eBoost &lt;- cv.l2boost(dta$x,dta$y,M=Mtarget, K=5, nu=nuTarget, lambda=.1, cores=cvCores) 

# plot the results
plot(cv.eBoost)
abline(v=cv.eBoost$opt.step, lty=2, col="grey")
plot(cv.eBoost$fit, type="coef", ylab=expression(beta[i]))
abline(v=cv.eBoost$opt.step, lty=2, col="grey")
plot(coef(cv.eBoost$fit, m=cv.obj$opt.step), cex=.5, 
  ylab=expression(beta[i]), xlab="Column Index", ylim=c(0,140), col=sig)

## End(Not run)

</code></pre>

<hr>
<h2 id='diabetes'>Blood and other measurements in diabetics [Hastie and Efron (2012)]</h2><span id='topic+diabetes'></span>

<h3>Description</h3>

<p>A repackaged diabetes dataset [Hastie and Efron (2012)] is a list of two different design  matrices 
and a response vector with 442 observations [Efron et. al. (2004)]
</p>
<p>The x matrix has been standardized to have unit L2 norm in each
column and zero mean. The matrix x2 consists of x plus 54 interaction terms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diabetes
</code></pre>


<h3>Format</h3>

<p>A list of 3 data objects, 
</p>

<ul>
<li><p> x: A data frame with 10 variables and 442 observations
</p>
</li>
<li><p> y: a numeric response vector of 442 observations
</p>
</li>
<li><p> x2: a design matrix including interaction terms with 64 columns and 442 observations.
</p>
</li></ul>



<h3>References</h3>

<p>Efron B., Hastie T., Johnstone I., and Tibshirani R. 
&quot;Least Angle Regression&quot; <em>Annals of Statistics</em> 32:407-499, 2004.
</p>
<p>Hastie T. and Efron B. (2012). lars: Least Angle Regression, Lasso and Forward Stagewise. 
R package version 1.1. http://CRAN.R-project.org/package=lars
</p>

<hr>
<h2 id='elasticNetSim'>A blocked correlated data simulation.</h2><span id='topic+elasticNetSim'></span>

<h3>Description</h3>

<p>Creates a data simulation of <em>n</em> observations with <em>signal</em> 
groups of (<em>p0/signal</em>) signal variables and (<em>p-p0</em>) noise variables. Random noise is 
added to all columns. The default values, with <em>n=100</em> create
the simulation of Zou and Hastie (2005).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>elasticNetSim(
  n,
  p = 40,
  p0 = 15,
  signal = 3,
  sigma = sqrt(0.01),
  beta.true = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="elasticNetSim_+3A_n">n</code></td>
<td>
<p>number of observations</p>
</td></tr>
<tr><td><code id="elasticNetSim_+3A_p">p</code></td>
<td>
<p>number of coordinate directions in the design matrix (default 40)</p>
</td></tr>
<tr><td><code id="elasticNetSim_+3A_p0">p0</code></td>
<td>
<p>number of signal coordinate directions in the design matrix (default 15)</p>
</td></tr>
<tr><td><code id="elasticNetSim_+3A_signal">signal</code></td>
<td>
<p>number of signal groups (default 3)</p>
</td></tr>
<tr><td><code id="elasticNetSim_+3A_sigma">sigma</code></td>
<td>
<p>within group correlation coefficient (default sqrt(0.01))</p>
</td></tr>
<tr><td><code id="elasticNetSim_+3A_beta.true">beta.true</code></td>
<td>
<p>specify the true simulation parameters. (default NULL = generated from other arguments)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of 
</p>

<ul>
<li><p> x simulated design matrix
</p>
</li>
<li><p> y simulated response vector
</p>
</li>
<li><p> beta.true true beta parameters used to create the simulation
</p>
</li></ul>



<h3>References</h3>

<p>Zou, H. and Hastie, T. (2005) Regularization and variable selection via the elastic net 
<em>J. R. Statist. Soc. B</em>, 67, Part 2, pp. 301-320
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#--------------------------------------------------------------------------
# Example: Elastic net simulation
#  
# For elastic net simulation data, see Zou, H. and Hastie, T. (2005) 
# Regularization and variable selection via the elastic net J. R. Statist. Soc. B
# , 67, Part 2, pp. 301-320
  # Set the RNG seed to create a reproducible simulation
  set.seed(432) # Takes an integer argument
  
  # Creata simulation with 100 observations.
  dta &lt;- elasticNetSim(n=100)
  
  # The simulation contains a design matrix x, and response vector y
  dim(dta$x)
  length(dta$y)
  print(dta$x[1:5,])

</code></pre>

<hr>
<h2 id='error.bars'>nice standard errors for plots</h2><span id='topic+error.bars'></span>

<h3>Description</h3>

<p>nice standard errors for plots
</p>


<h3>Usage</h3>

<pre><code class='language-R'>error.bars(x, upper, lower, width = 0.001, max.M = 100, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="error.bars_+3A_x">x</code></td>
<td>
<p>Vector of error bar x value locations</p>
</td></tr>
<tr><td><code id="error.bars_+3A_upper">upper</code></td>
<td>
<p>Vector of upper error bar limits</p>
</td></tr>
<tr><td><code id="error.bars_+3A_lower">lower</code></td>
<td>
<p>Vector of limit error bar limits</p>
</td></tr>
<tr><td><code id="error.bars_+3A_width">width</code></td>
<td>
<p>errorbar line width (default: 0.001)</p>
</td></tr>
<tr><td><code id="error.bars_+3A_max.m">max.M</code></td>
<td>
<p>maximum number of bars to show in a plot (default: 100)</p>
</td></tr>
<tr><td><code id="error.bars_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to segment function</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+segments">segments</a></code>
</p>

<hr>
<h2 id='fitted.l2boost'>Extract the fitted model estimates along the solution path for an l2boost model.</h2><span id='topic+fitted.l2boost'></span>

<h3>Description</h3>

<p><code><a href="stats.html#topic+fitted">fitted</a></code> is a generic function which extracts fitted values from objects 
returned by modeling functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'l2boost'
fitted(object, m = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitted.l2boost_+3A_object">object</code></td>
<td>
<p>an l2boost object</p>
</td></tr>
<tr><td><code id="fitted.l2boost_+3A_m">m</code></td>
<td>
<p>the iteration number with the l2boost path. (default m=NULL)</p>
</td></tr>
<tr><td><code id="fitted.l2boost_+3A_...">...</code></td>
<td>
<p>other arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code><a href="#topic+fitted.l2boost">fitted.l2boost</a></code> returns the function estimates obtained 
from  the training set observations of an l2boost model object at any point along the solution path. 
The estimate, F_m(x) is evaluated at iteration m using the training data set x. By default, 
<code><a href="#topic+fitted.l2boost">fitted.l2boost</a></code> returns the estimate at the last iteration step M, unless a specific 
iteration step m is specified.
</p>


<h3>Value</h3>

<p>The vector of fitted response estimates at the given iteration m. By default,
the coefficients are obtained from the last iteration m=M.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+fitted">fitted</a></code> and <code><a href="#topic+l2boost">l2boost</a></code> and <code><a href="#topic+predict.l2boost">predict.l2boost</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#--------------------------------------------------------------------------
# Example: Diabetes 
#  
# See Efron B., Hastie T., Johnstone I., and Tibshirani R. 
# Least angle regression. Ann. Statist., 32:407-499, 2004.
data(diabetes, package="l2boost")

l2.object &lt;- l2boost(diabetes$x,diabetes$y, M=1000, nu=.01)

# return the fitted values
fitted(l2.object)
fitted(l2.object, m=500)

#' # Create diagnostic plots
par(mfrow=c(2,2))
qqnorm(fitted(l2.object), ylim=c(0, 300))
qqline(fitted(l2.object), col=2)

qqnorm(fitted(l2.object, m=500), ylim=c(0, 300))
qqline(fitted(l2.object, m=500), col=2)

# Tukey-Anscombe's plot
plot(y=residuals(l2.object), x=fitted(l2.object), main="Tukey-Anscombe's plot",
  ylim=c(-3e-13, 3e-13))
lines(smooth.spline(fitted(l2.object), residuals(l2.object), df=4), type="l", 
  lty=2, col="red", lwd=2)
abline(h=0, lty=2, col = 'gray')

plot(y=residuals(l2.object, m=500), x=fitted(l2.object, m=500), 
  main="Tukey-Anscombe's plot", ylim=c(-3e-13, 3e-13))
lines(smooth.spline(fitted(l2.object,m=500), residuals(l2.object, m=500), df=4), 
  type="l", lty=2, col="red", lwd=2)
abline(h=0, lty=2, col = 'gray')

</code></pre>

<hr>
<h2 id='l2boost'>Generic gradient descent boosting method for linear regression.</h2><span id='topic+l2boost'></span><span id='topic+l2boost.default'></span><span id='topic+l2boost.formula'></span>

<h3>Description</h3>

<p>Efficient implementation of Friedman's boosting algorithm  [Friedman (2001)] with L2-loss function and coordinate
direction (design matrix columns) basis functions. This includes the elasticNet data augmentation of Ehrlinger and Ishwaran (2012), 
which adds an L2-penalization (lambda) similar to the elastic net [Zou and Hastie (2005)].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>l2boost(x, ...)

## Default S3 method:
l2boost(x, y, M, nu, lambda, trace, type , qr.tolerance, eps.tolerance, ...)

## S3 method for class 'formula'
l2boost(formula, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="l2boost_+3A_x">x</code></td>
<td>
<p>design matrix of dimension n x p</p>
</td></tr>
<tr><td><code id="l2boost_+3A_...">...</code></td>
<td>
<p>other arguments (currently unused)</p>
</td></tr>
<tr><td><code id="l2boost_+3A_y">y</code></td>
<td>
<p>response variable of length n</p>
</td></tr>
<tr><td><code id="l2boost_+3A_m">M</code></td>
<td>
<p>number of steps to run boost algorithm (M &gt;1)</p>
</td></tr>
<tr><td><code id="l2boost_+3A_nu">nu</code></td>
<td>
<p>L1 shrinkage parameter (0 &lt; nu &lt;= 1)</p>
</td></tr>
<tr><td><code id="l2boost_+3A_lambda">lambda</code></td>
<td>
<p>L2 shrinkage parameter used for elastic net boosting (lambda &gt; 0 || lambda = NULL)</p>
</td></tr>
<tr><td><code id="l2boost_+3A_trace">trace</code></td>
<td>
<p>show runtime messages (default: FALSE)</p>
</td></tr>
<tr><td><code id="l2boost_+3A_type">type</code></td>
<td>
<p>Choice of l2boost algorithm from &quot;discrete&quot;, &quot;hybrid&quot;, &quot;friedman&quot;,&quot;lars&quot;. See details below. (default &quot;discrete&quot;)</p>
</td></tr>
<tr><td><code id="l2boost_+3A_qr.tolerance">qr.tolerance</code></td>
<td>
<p>tolerance limit for use in <code><a href="base.html#topic+qr.solve">qr.solve</a></code> (default: 1e-30)</p>
</td></tr>
<tr><td><code id="l2boost_+3A_eps.tolerance">eps.tolerance</code></td>
<td>
<p>dynamic step size lower limit (default: .Machine$double.eps)</p>
</td></tr>
<tr><td><code id="l2boost_+3A_formula">formula</code></td>
<td>
<p>an object of class <code><a href="stats.html#topic+formula">formula</a></code> 
(or one that can be coerced to that class): a symbolic 
description of the model to be fitted. The details of 
model specification are given under <code><a href="stats.html#topic+formula">formula</a></code>.</p>
</td></tr>
<tr><td><code id="l2boost_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment 
(or object coercible by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code> to 
a data frame) containing the variables in the model used in the
<code><a href="stats.html#topic+formula">formula</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="#topic+l2boost">l2boost</a></code> function is an efficient implementation of a generic boosting method [Friedman (2001)] for
linear regression using an L2-loss function. The basis functions are the column vectors of the design matrix. 
<code><a href="#topic+l2boost">l2boost</a></code> scales the design matrix such that the coordinate columns of the design correspond to the
gradient directions for each covariate. The boosting coefficients are equivalent to the gradient-correlation of each 
covariate. Friedman's gradient descent boosting algorithm proceeds at each step along the covariate direction closest
(in L2 distance) to the maximal gradient descent direction.
</p>
<p>We include a series of algorithms to solve the boosting optimization. These are selected through the <em>type</em> argument
</p>

<ul>
<li> <p><em>friedman</em> - The original, bare-bones l2boost (Friedman (2001)). This method takes a fixed step size of length
<em>nu</em>.
</p>
</li>
<li> <p><em>lars</em> - The l2boost-lars-limit (See Efron et.al (2004)). This algorithm takes a single step of the 
optimal length to the critical point required for a new coordinate direction to become favorable. Although optimal
in the number of steps required to reach the OLS solution, this method may be computationally expensive for large p
problems, as the method requires a matrix inversion to calculate the step length. 
</p>
</li>
<li> <p><em>discrete</em> - Optimized Friedman algorithm to reduce number of evaluations required 
[Ehrlinger and Ishwaran 2012]. The algorithm dynamically determines the number of steps of length <em>nu</em> to take along
a descent direction. The discrete method allows the algorithm to take step sizes of multiples of <em>nu</em> at any evaluation.
</p>
</li>
<li> <p><em>hybrid</em> - Similar to discrete, however only allows combining steps along the first descent direction. 
<em>hybrid</em> Works best if <em>nu</em> is moderate, but not too small. In this case, Friedman's algorithm would take 
many steps along the first coordinate direction, and then cycle when multiple coordinates have similar gradient 
directions (by the L2 measure).
</p>
</li></ul>

<p><code><a href="#topic+l2boost">l2boost</a></code> keeps track of all gradient-correlation coefficients (<em>rho</em>) at each iteration in addition to the maximal
descent direction taken by the method. Visualizing these coefficients can be informative of the inner workings of gradient boosting 
(see the examples in the <code><a href="#topic+plot.l2boost">plot.l2boost</a></code> method).
</p>
<p>The <code><a href="#topic+l2boost">l2boost</a></code> function uses an arbitrary L1-regularization parameter (nu), and includes the elementary 
data augmentation of Ehrlinger and Ishwaran (2012), to add an L2-penalization (lambda) similar to the elastic net 
[Zou and Hastie (2005)]. The L2-regularization reverses repressibility, a condition where one variable acts as 
a boosting surrogate for other, possibly informative, variables. Along with the decorrelation 
effect, this <em>elasticBoost</em> regularization circumvents L2Boost deficiencies in correlated settings. 
</p>
<p>We include a series of S3 functions for working  with <code><a href="#topic+l2boost">l2boost</a></code> objects:
</p>

<ul>
<li> <p><code><a href="base.html#topic+print">print</a></code> (<code><a href="#topic+print.l2boost">print.l2boost</a></code>) prints a summary of the <code><a href="#topic+l2boost">l2boost</a></code> fit.
</p>
</li>
<li> <p><code><a href="stats.html#topic+coef">coef</a></code> (<code><a href="#topic+coef.l2boost">coef.l2boost</a></code>) returns the <code><a href="#topic+l2boost">l2boost</a></code> model regression coefficients at any point 
along the solution path. 
</p>
</li>
<li> <p><code><a href="stats.html#topic+fitted">fitted</a></code> (<code><a href="#topic+fitted.l2boost">fitted.l2boost</a></code>) returns the fitted <code><a href="#topic+l2boost">l2boost</a></code> response estimates (from
the training dataset) along the solution path. 
</p>
</li>
<li> <p><code><a href="stats.html#topic+residuals">residuals</a></code> (<code><a href="#topic+residuals.l2boost">residuals.l2boost</a></code>) returns the training set <code><a href="#topic+l2boost">l2boost</a></code> residuals along the
solution path.
</p>
</li>
<li> <p><code><a href="graphics.html#topic+plot">plot</a></code> (<code><a href="#topic+plot.l2boost">plot.l2boost</a></code>) for graphing model coefficients of an <code><a href="#topic+l2boost">l2boost</a></code> object.
</p>
</li>
<li> <p><code><a href="stats.html#topic+predict">predict</a></code> (<code><a href="#topic+predict.l2boost">predict.l2boost</a></code>) for generating <code><a href="#topic+l2boost">l2boost</a></code> prediction estimates on possibly 
new test set observations.
</p>
</li></ul>

<p>A cross-validation method (<code><a href="#topic+cv.l2boost">cv.l2boost</a></code>) is also included for L2boost and elasticBoost, for cross-validated error estimates 
and regularization parameter optimizations.
</p>


<h3>Value</h3>

<p>A &quot;l2boost&quot; object is returned, for which print, plot, predict, and coef methods exist.
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>Choice of l2boost algorithm from &quot;friedman&quot;, &quot;discrete&quot;, &quot;hybrid&quot;, &quot;lars&quot;</p>
</td></tr>
<tr><td><code>nu</code></td>
<td>
<p>The L1 boosting shrinkage parameter value</p>
</td></tr>    
<tr><td><code>lambda</code></td>
<td>
<p>The L2 elasticNet shrinkage parameter value</p>
</td></tr>  
<tr><td><code>x</code></td>
<td>
<p>The training dataset</p>
</td></tr>
<tr><td><code>x.na</code></td>
<td>
<p>Columns of original design matrix with values na, these have been removed from x</p>
</td></tr>
<tr><td><code>x.attr</code></td>
<td>
<p>scale attributes of design matrix</p>
</td></tr>
<tr><td><code>names</code></td>
<td>
<p>Column names of design matrix</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>training response vector associated with x, centered about the mean value ybar</p>
</td></tr>
<tr><td><code>ybar</code></td>
<td>
<p>mean value of training response vector</p>
</td></tr>
<tr><td><code>mjk</code></td>
<td>
<p>measure to favorability. This is a matrix of size p by m. Each coordinate j has a measure at each step m</p>
</td></tr>
<tr><td><code>stepSize</code></td>
<td>
<p>vector of step lengths taken (<code>NULL</code> unless <code>type = "lars"</code>)</p>
</td></tr>       
<tr><td><code>l.crit</code></td>
<td>
<p>vector of column index of critical direction</p>
</td></tr> 
<tr><td><code>L.crit</code></td>
<td>
<p>number of steps along each l.crit direction</p>
</td></tr>      
<tr><td><code>S.crit</code></td>
<td>
<p>The critical step value where a direction change occurs</p>
</td></tr>
<tr><td><code>path.Fm</code></td>
<td>
<p>estimates of response at each step m</p>
</td></tr>
<tr><td><code>Fm</code></td>
<td>
<p>estimate of response at final step M</p>
</td></tr>
<tr><td><code>rhom.path</code></td>
<td>
<p>boosting parameter estimate at each step m</p>
</td></tr>   
<tr><td><code>betam.path</code></td>
<td>
<p>beta parameter estimates at each step m. List of m vectors of length p</p>
</td></tr>
<tr><td><code>betam</code></td>
<td>
<p>beta parameter estimate at final step M</p>
</td></tr>
</table>
<p>The notation for the return values is described in Ehrlinger and Ishwaran (2012).
</p>


<h3>References</h3>

<p>Friedman J. (2001) Greedy function approximation: A gradient boosting machine. <em>Ann. Statist.</em>, 29:1189-1232
</p>
<p>Ehrlinger J., and Ishwaran H. (2012). &quot;Characterizing l2boosting&quot; <em>Ann. Statist.</em>, 40 (2), 1074-1101
</p>
<p>Zou H. and Hastie T (2005) &quot;Regularization and variable selection via the elastic net&quot;  <em>J. R. Statist. Soc. B</em>, 67, Part 2, pp. 301-320
</p>
<p>Efron B., Hastie T., Johnstone I., and Tibshirani R. (2004). &quot;Least Angle Regression&quot; <em>Ann. Statist.</em> 32:407-499
</p>


<h3>See Also</h3>

<p><code><a href="#topic+print.l2boost">print.l2boost</a></code>, <code><a href="#topic+plot.l2boost">plot.l2boost</a></code>, <code><a href="#topic+predict.l2boost">predict.l2boost</a></code>, 
<code><a href="#topic+coef.l2boost">coef.l2boost</a></code>, <code><a href="#topic+residuals.l2boost">residuals.l2boost</a></code>, <code><a href="#topic+fitted.l2boost">fitted.l2boost</a></code> methods of l2boost 
and <code><a href="#topic+cv.l2boost">cv.l2boost</a></code> for K fold cross-validation of the l2boost method.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#--------------------------------------------------------------------------
# Example 1: Diabetes data
#  
# See Efron B., Hastie T., Johnstone I., and Tibshirani R. 
# Least angle regression. Ann. Statist., 32:407-499, 2004.
data(diabetes, package="l2boost")

l2.object &lt;- l2boost(diabetes$x,diabetes$y, M=1000, nu=.01)

# Plot the boosting rho, and regression beta coefficients as a function of
# boosting steps m
#
# Note: The selected coordinate trajectories are colored in red after selection, and 
# blue before. Unselected coordinates are colored grey.
#
par(mfrow=c(2,2))
plot(l2.object)
plot(l2.object, type="coef")

# increased shrinkage and number of iterations.
l2.shrink &lt;- l2boost(diabetes$x,diabetes$y,M=5000, nu=1.e-3) 
plot(l2.shrink)
plot(l2.shrink, type="coef")

## Not run: 
#--------------------------------------------------------------------------
# Example 2: elasticBoost simulation
# Compare l2boost and elastic net boosting
# 
# See Zou H. and Hastie T. Regularization and variable selection via the 
# elastic net. J. Royal Statist. Soc. B, 67(2):301-320, 2005
set.seed(1025)

# The default simulation uses 40 covariates with signal concentrated on 
# 3 groups of 5 correlated covariates (for 15 signal covariates)
dta &lt;- elasticNetSim(n=100)

# l2boost the simulated data with groups of correlated coordinates
l2.object &lt;- l2boost(dta$x,dta$y,M=10000, nu=1.e-3, lambda=NULL)

par(mfrow=c(2,2))
# plot the l2boost trajectories over all M
plot(l2.object, main="l2Boost nu=1.e-3")
# Then zoom into the first m=500 steps
plot(l2.object, xlim=c(0,500), ylim=c(.25,.5), main="l2Boost nu=1.e-3")

# elasticNet same data with L1 parameter lambda=0.1
en.object &lt;- l2boost(dta$x,dta$y,M=10000, nu=1.e-3, lambda=.1) 

# plot the elasticNet trajectories over all M
#
# Note 2: The elasticBoost selects all coordinates close to the selection boundary,
# where l2boost leaves some unselected (in grey)
plot(en.object, main="elasticBoost nu=1.e-3, lambda=.1")
# Then zoom into the first m=500 steps
plot(en.object, xlim=c(0,500), ylim=c(.25,.5),
  main="elasticBoost nu=1.e-3, lambda=.1")

## End(Not run)

</code></pre>

<hr>
<h2 id='mvnorm.l2boost'>multivariate normal data simulations.</h2><span id='topic+mvnorm.l2boost'></span>

<h3>Description</h3>

<p>Create simulated dataset from a multivariate normal. Used to recreate data simulations
from Ehrlinger and Ishwaran (2012).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mvnorm.l2boost(n = 100, p = 100, beta = NULL, which.beta = NULL, rho = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mvnorm.l2boost_+3A_n">n</code></td>
<td>
<p>number of observations</p>
</td></tr>
<tr><td><code id="mvnorm.l2boost_+3A_p">p</code></td>
<td>
<p>number of coordinate directions in the design matrix</p>
</td></tr>
<tr><td><code id="mvnorm.l2boost_+3A_beta">beta</code></td>
<td>
<p>a &quot;true&quot; beta vector of length p (default=NULL) See details.</p>
</td></tr>
<tr><td><code id="mvnorm.l2boost_+3A_which.beta">which.beta</code></td>
<td>
<p>indicator vector for which beta coefficients to include as signal in simulation (default=NULL) see details</p>
</td></tr>
<tr><td><code id="mvnorm.l2boost_+3A_rho">rho</code></td>
<td>
<p>correlation coefficient between coordinate directions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, mvnorm.l2boost creates a data set of n multivariate normal random observations of p covariates 
(see MASS:mvrnorm). The correlation matrix is constructed with 1 on the diagonals and the correlation 
coefficient <em>rho</em> on the off diagonals. 
</p>
<p>The response is constructed as follows: If a true beta vector is not supplied, the first 10 beta coefficients carry 
the signal with a value of 5, and the remaining p-10 values are set to zero. Given a <em>beta.true</em> vector, all 
values are used as specified. The coefficent vector is truncated to have <em>p</em> signal terms if 
length(<em>beta.true</em>) &gt; <em>p</em>, and noise coordinates are added if length(<em>beta.true</em>) &lt; <em>p</em>.
</p>
<p>It is possible to pass an indicator vector <em>which.beta</em> to select specific signal elements from the 
full vector <em>beta.true</em>.
</p>


<h3>Value</h3>


<ul>
<li><p> call Matched function call
</p>
</li>
<li><p> x design matrix of size <em>n</em> x <em>p</em>
</p>
</li>
<li><p> y response vector of length <em>n</em>
</p>
</li></ul>



<h3>References</h3>

<p>Ehrlinger J., and Ishwaran H. (2012). &quot;Characterizing l2boosting&quot; <em>Ann. Statist.</em>, 40 (2), 1074-1101
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#--------------------------------------------------------------------------
# Example: Multivariate normal data simulation

# Create a (reproducable) data set of size 100 x 100
set.seed(1024)
n&lt;- 100
p&lt;- 100

# Set 10 signal variables using a uniform beta=5, the remaining (p-10)=90 are
# set to zero indicating random noise.  
beta &lt;- c(rep(5,10), rep(0,p-10))

# Example with orthogonal design matrix columns (orthogonal + noise)
ortho.data &lt;- mvnorm.l2boost(n, p, beta)
cbind(ortho.data$y[1:5],ortho.data$x[1:5,])

# Example with correlation between design matrix columns
corr.data &lt;- mvnorm.l2boost(n, p, beta, rho=0.65)
cbind(corr.data$y[1:5],corr.data$x[1:5,])


</code></pre>

<hr>
<h2 id='plot.l2boost'>Plotting for <code><a href="#topic+l2boost">l2boost</a></code> objects.</h2><span id='topic+plot.l2boost'></span>

<h3>Description</h3>

<p>plotting methods for <code><a href="#topic+l2boost">l2boost</a></code> objects (<code><a href="#topic+l2boost">l2boost</a></code> and <code><a href="#topic+cv.l2boost">cv.l2boost</a></code>). 
</p>
<p>By default, plotting an <code><a href="#topic+l2boost">l2boost</a></code> object produces a gradient-correlation vs iteration steps (m) plot.
Plotting a <code><a href="#topic+cv.l2boost">cv.l2boost</a></code> object produces a cross-validation error plot, and prints the minimal CV MSE value
and optimal step opt.step to the R console.
</p>
<p>Many generic arguments to <code><a href="graphics.html#topic+plot">plot</a></code> are passed through the <code><a href="#topic+plot.l2boost">plot.l2boost</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'l2boost'
plot(
  x,
  type = c("rho", "coef"),
  standardize = TRUE,
  active.set = NULL,
  xvar = c("step", "norm"),
  xlab = NULL,
  ylab = NULL,
  trim = TRUE,
  clip = NULL,
  col = NULL,
  ylim = NULL,
  xlim = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.l2boost_+3A_x">x</code></td>
<td>
<p>l2boost or cv.l2boost object</p>
</td></tr>
<tr><td><code id="plot.l2boost_+3A_type">type</code></td>
<td>
<p>which type of plot. <em>rho</em> plots gradient-correlation, <em>coef</em> regression (beta) 
coefficients vs the step number m along the x-axis</p>
</td></tr>
<tr><td><code id="plot.l2boost_+3A_standardize">standardize</code></td>
<td>
<p>Should we plot standardized gradient-correlation (default: TRUE)</p>
</td></tr>
<tr><td><code id="plot.l2boost_+3A_active.set">active.set</code></td>
<td>
<p>Vector of indices of the coordinates for highlighting with 
color=col (default: NULL shows all active coordinates)</p>
</td></tr>
<tr><td><code id="plot.l2boost_+3A_xvar">xvar</code></td>
<td>
<p>what measure do we plot on the x-axis? <em>step</em> plots the step m, <em>norm</em> plots the 
normalized distance (1-nu)^(m-1)</p>
</td></tr>
<tr><td><code id="plot.l2boost_+3A_xlab">xlab</code></td>
<td>
<p>specific x-axis label (NULL results in default value depending on xvar)</p>
</td></tr>
<tr><td><code id="plot.l2boost_+3A_ylab">ylab</code></td>
<td>
<p>specific y-axis label (NULL results in default value depending on type)</p>
</td></tr>
<tr><td><code id="plot.l2boost_+3A_trim">trim</code></td>
<td>
<p>(default: TRUE)</p>
</td></tr>
<tr><td><code id="plot.l2boost_+3A_clip">clip</code></td>
<td>
<p>Do we want to c</p>
</td></tr>
<tr><td><code id="plot.l2boost_+3A_col">col</code></td>
<td>
<p>Color to highlight active.set coordinates (NULL indicates default all active set at 
step M in blue, changes to red after selection</p>
</td></tr>
<tr><td><code id="plot.l2boost_+3A_ylim">ylim</code></td>
<td>
<p>Control plotted y-values (default: NULL for auto range)</p>
</td></tr>
<tr><td><code id="plot.l2boost_+3A_xlim">xlim</code></td>
<td>
<p>Control plotted x-values (default: NULL for auto domain )</p>
</td></tr>
<tr><td><code id="plot.l2boost_+3A_...">...</code></td>
<td>
<p>other arguments passed to plot functions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Gradient-correlation plots are created by tracing out the boosting coefficient (rho) for each candidate
direction. The coefficient and gradient-correlation are equivalent under standard scaling (zero intercept with 
design matrix columns scaled to have mean=0 and variance=1).  
</p>
<p>Unless explicitly set using <em>col</em> argument, the plot function colors the gradient-correlation paths along each
direction by the following criteria: 
</p>

<ul>
<li><p> Red: indicates the coordinate direction has been selected in the boosting path at some step &lt;= m. 
</p>
</li>
<li><p> Blue: indicates the coordinate will be selected within the specified number of steps M (and switch to 
red upon selection).
</p>
</li>
<li><p> Grey: indicates coordinates have not and will not be selected by the algorithm over all iterations. 
</p>
</li></ul>

<p>The colors are set using the <em>l.crit</em> return value from the <code><a href="#topic+l2boost">l2boost</a></code> object.
</p>


<h3>Value</h3>

<p><code>NULL</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+l2boost">l2boost</a></code>, <code><a href="#topic+print.l2boost">print.l2boost</a></code>, <code><a href="#topic+predict.l2boost">predict.l2boost</a></code> methods of l2boost 
and <code><a href="#topic+cv.l2boost">cv.l2boost</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#--------------------------------------------------------------------------
# Example: Diabetes 
#  
# See Efron B., Hastie T., Johnstone I., and Tibshirani R. 
# Least angle regression. Ann. Statist., 32:407-499, 2004.
data(diabetes, package = "l2boost")

l2.object &lt;- l2boost(diabetes$x,diabetes$y, M=1000, nu=.01)

# Plot the gradient-correlation, and regression beta coefficients as a function of
# boosting steps m
par(mfrow=c(2,2))
plot(l2.object)
abline(v=500, lty=2, col="grey")
plot(l2.object, type="coef")
abline(v=500, lty=2, col="grey")

# limit the plot to only the first 500 steps of the algorithm 
# (grey vertical line in previous plots).
plot(l2.object, xlim=c(0,500))
plot(l2.object, type="coef", xlim=c(0,500))

## Not run: 
#--------------------------------------------------------------------------
# Example: Plotting cross-validation objects
dta &lt;- elasticNetSim(n=100)
# Set the boosting parameters
Mtarget = 1000
nuTarget = 1.e-2

cv.l2 &lt;- cv.l2boost(dta$x,dta$y,M=Mtarget, nu=nuTarget, lambda=NULL)

# Show the CV MSE plot, with a marker at the "optimal iteration"
plot(cv.l2)
abline(v=cv.l2$opt.step, lty=2, col="grey")

# Show the l2boost object plots.
plot(cv.l2$fit)
abline(v=cv.l2$opt.step, lty=2, col="grey")
 
plot(cv.l2$fit, type="coef")
abline(v=cv.l2$opt.step, lty=2, col="grey")

# Create a color vector of length p=40 (from elasticNetSim defaults)
clr &lt;- rep("black", 40)
# Set coordinates in the boosting path to color red.
clr[unique(cv.l2$fit$l.crit)] = "red"

# Show the "optimal" coefficient values, 
# red points are selected in boosting algorithm.
plot(coef(cv.l2$fit, m=cv.l2$opt.step), col=clr, ylab=expression(beta))

## End(Not run)

</code></pre>

<hr>
<h2 id='plot.lines'>plots.lines is used by <code><a href="#topic+plot.l2boost">plot.l2boost</a></code> to the path lines (each j, against each r-step)</h2><span id='topic+plot.lines'></span>

<h3>Description</h3>

<p>plots.lines is used by <code><a href="#topic+plot.l2boost">plot.l2boost</a></code> to the path lines (each j, against each r-step)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lines'
plot(xval = NULL, ind, path, l.crit, active = TRUE, col = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.lines_+3A_xval">xval</code></td>
<td>
<p>vector of x-values corresponding to the path y-values (default: NULL index of path)</p>
</td></tr>
<tr><td><code id="plot.lines_+3A_ind">ind</code></td>
<td>
<p>Coordinate of the path (for coloring individual paths)</p>
</td></tr>
<tr><td><code id="plot.lines_+3A_path">path</code></td>
<td>
<p>Plot the path values along the y-axis</p>
</td></tr>
<tr><td><code id="plot.lines_+3A_l.crit">l.crit</code></td>
<td>
<p>change the color at the value of m=l.crit</p>
</td></tr>
<tr><td><code id="plot.lines_+3A_active">active</code></td>
<td>
<p>active set coloring (default: TRUE)</p>
</td></tr>
<tr><td><code id="plot.lines_+3A_col">col</code></td>
<td>
<p>vector of color values length &gt;= 1 (default: NULL use built in scheme)</p>
</td></tr>
</table>

<hr>
<h2 id='predict.l2boost'>predict method for l2boost models.</h2><span id='topic+predict.l2boost'></span>

<h3>Description</h3>

<p><code><a href="stats.html#topic+predict">predict</a></code> is a generic function for predictions from the results 
of various model fitting functions. 
</p>
<p>@details <code><a href="#topic+predict.l2boost">predict.l2boost</a></code> takes the optional <em>xnew</em> (equivalent <em>newdata</em>) <code><a href="base.html#topic+data.frame">data.frame</a></code>
and returns the model estimates from an <code><a href="#topic+l2boost">l2boost</a></code> object. If neither <em>xnew</em> or <em>newdata</em> are
provided, <code><a href="stats.html#topic+predict">predict</a></code> returns estimates for the <code><a href="#topic+l2boost">l2boost</a></code> training data set.
</p>
<p>By default, <code><a href="#topic+predict.l2boost">predict.l2boost</a></code> returns the function estimates, unless type=&quot;coef&quot; then the 
set of regression coefficients (beta) are returned from the <code><a href="#topic+l2boost">l2boost</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'l2boost'
predict(object, xnew = NULL, type = c("fit", "coef"), newdata = xnew, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.l2boost_+3A_object">object</code></td>
<td>
<p>an l2boost object</p>
</td></tr>
<tr><td><code id="predict.l2boost_+3A_xnew">xnew</code></td>
<td>
<p>a new design matrix to fit with the l2boost object</p>
</td></tr>
<tr><td><code id="predict.l2boost_+3A_type">type</code></td>
<td>
<p>&quot;fit&quot; or &quot;coef&quot; determins the values returned. &quot;fit&quot; returns model estimates, &quot;coef&quot; returns the 
model coefficients</p>
</td></tr>
<tr><td><code id="predict.l2boost_+3A_newdata">newdata</code></td>
<td>
<p>a new design matrix to fit with the l2boost object</p>
</td></tr>
<tr><td><code id="predict.l2boost_+3A_...">...</code></td>
<td>
<p>other arguments (currently not used)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>function estimates for type=fit, coefficient estimates for type=coef
</p>

<ul>
<li><p>yhatvector of n function estimates from the final step M
</p>
</li>
<li><p>yhat.pathlist of M function estimates, one  at each step m
</p>
</li></ul>

<p>or
</p>

<ul>
<li><p>coefvector of p beta coefficient estimates from final step M          
</p>
</li>
<li><p>coef.standvector of p standardized beta coefficient estimates from final step M     
</p>
</li>
<li><p>coef.pathlist of vectors of p beta coefficient estimates, one for each step m  
</p>
</li>
<li><p>coef.stand.pathlist of vectors of p standardized beta coefficient estimates, one for each step m  
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="stats.html#topic+predict">predict</a></code> and <code><a href="#topic+l2boost">l2boost</a></code>, <code><a href="#topic+coef.l2boost">coef.l2boost</a></code>,  
<code><a href="#topic+fitted.l2boost">fitted.l2boost</a></code>, <code><a href="#topic+residuals.l2boost">residuals.l2boost</a></code> and <code><a href="#topic+cv.l2boost">cv.l2boost</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#--------------------------------------------------------------------------
# Example 1: Diabetes 
#  
# See Efron B., Hastie T., Johnstone I., and Tibshirani R. 
# Least angle regression. Ann. Statist., 32:407-499, 2004.
data(diabetes)

object &lt;- l2boost(diabetes$x,diabetes$y, M=1000, nu=.01)

# With no arguments returns the estimates at the full M from the training data.
prd &lt;- predict(object)
prd$yhat

# at step m=600
prd$yhat.path[[600]]

# Also can return coefficient estimates. This is equivalent to \code{\link{coef.l2boost}}
cf &lt;- predict(object, type="coef")
cf$coef

# at step m=600
cf$coef.path[[600]]

# Or used to predict new data, in this case a subset of training data
cbind(diabetes$y[1:5], predict(object, xnew=diabetes$x[1:5,])$yhat)

</code></pre>

<hr>
<h2 id='print.l2boost'>print method for <code><a href="#topic+l2boost">l2boost</a></code> and <code><a href="#topic+cv.l2boost">cv.l2boost</a></code> objects.</h2><span id='topic+print.l2boost'></span>

<h3>Description</h3>

<p><code><a href="base.html#topic+print">print</a></code> is a generic function for displaying model summaries
</p>
<p><code><a href="#topic+print.l2boost">print.l2boost</a></code> returns a model summary for <code><a href="#topic+l2boost">l2boost</a></code> and <code><a href="#topic+cv.l2boost">cv.l2boost</a></code> objects including 
the coefficient estimates at the specified step m. By default, <code><a href="#topic+print.l2boost">print.l2boost</a></code> returns the summary for the
object at the  final iteration step M
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'l2boost'
print(x, m = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.l2boost_+3A_x">x</code></td>
<td>
<p>an l2boost object</p>
</td></tr>
<tr><td><code id="print.l2boost_+3A_m">m</code></td>
<td>
<p>return the result from iteration m</p>
</td></tr>
<tr><td><code id="print.l2boost_+3A_...">...</code></td>
<td>
<p>other arguments passed to helper functions</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+l2boost">l2boost</a></code>, <code><a href="#topic+cv.l2boost">cv.l2boost</a></code> and <code><a href="#topic+coef.l2boost">coef.l2boost</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#--------------------------------------------------------------------------
# Example 1: Diabetes 
#  
# See Efron B., Hastie T., Johnstone I., and Tibshirani R. 
# Least angle regression. Ann. Statist., 32:407-499, 2004.
data(diabetes)

object &lt;- l2boost(diabetes$x,diabetes$y, M=1000, nu=.01)

# A summary of the l2boost object at M=1000
print(object)

# Similar at m=100
print(object, m=100)

</code></pre>

<hr>
<h2 id='print.summary.l2boost'>Unimplemented generic function
These are placeholders right now.</h2><span id='topic+print.summary.l2boost'></span>

<h3>Description</h3>

<p>Unimplemented generic function
These are placeholders right now.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'summary.l2boost'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.l2boost_+3A_x">x</code></td>
<td>
<p>an l2boost object</p>
</td></tr>
<tr><td><code id="print.summary.l2boost_+3A_...">...</code></td>
<td>
<p>other arguments (not used)</p>
</td></tr>
</table>

<hr>
<h2 id='residuals.l2boost'>Model residuals for the training set of an l2boost model object</h2><span id='topic+residuals.l2boost'></span>

<h3>Description</h3>

<p><code><a href="stats.html#topic+residuals">residuals</a></code> is a generic function which extracts model residuals 
from objects returned by modeling functions.
</p>
<p><code><a href="#topic+residuals.l2boost">residuals.l2boost</a></code> returns the training set residuals from an <code><a href="#topic+l2boost">l2boost</a></code>
object. By default, the residuals are returned at the final iteration step m=M.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'l2boost'
residuals(object, m = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals.l2boost_+3A_object">object</code></td>
<td>
<p>an l2boost object for the extraction of model coefficients.</p>
</td></tr>
<tr><td><code id="residuals.l2boost_+3A_m">m</code></td>
<td>
<p>the iteration number with the l2boost path. 
If m=NULL, the coefficients are obtained from the last iteration M.</p>
</td></tr>
<tr><td><code id="residuals.l2boost_+3A_...">...</code></td>
<td>
<p>arguments (unused)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of n residuals
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+residuals">residuals</a></code> and <code><a href="#topic+l2boost">l2boost</a></code> and <code><a href="#topic+predict.l2boost">predict.l2boost</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#--------------------------------------------------------------------------
# Example: Diabetes 
#  
# For diabetes data set, see Efron B., Hastie T., Johnstone I., and Tibshirani R. 
# Least angle regression. Ann. Statist., 32:407-499, 2004.
data(diabetes, package = "l2boost")

l2.object &lt;- l2boost(diabetes$x,diabetes$y, M=1000, nu=.01)
rsd&lt;-residuals(l2.object)
rsd.mid &lt;- residuals(l2.object, m=500)

# Create diagnostic plots
par(mfrow=c(2,2))
qqnorm(residuals(l2.object), ylim=c(-3e-13, 3e-13))
qqline(residuals(l2.object), col=2)

qqnorm(residuals(l2.object, m=500), ylim=c(-3e-13, 3e-13))
qqline(residuals(l2.object, m=500), col=2)

# Tukey-Anscombe's plot
plot(y=residuals(l2.object), x=fitted(l2.object), main="Tukey-Anscombe's plot",
   ylim=c(-3e-13, 3e-13))
lines(smooth.spline(fitted(l2.object), residuals(l2.object), df=4), type="l", 
  lty=2, col="red", lwd=2)
abline(h=0, lty=2, col = 'gray')

plot(y=residuals(l2.object, m=500), x=fitted(l2.object, m=500), main="Tukey-Anscombe's plot", 
  ylim=c(-3e-13, 3e-13))
lines(smooth.spline(fitted(l2.object,m=500), residuals(l2.object, m=500), df=4), type="l", 
  lty=2, col="red", lwd=2)
abline(h=0, lty=2, col = 'gray')


</code></pre>

<hr>
<h2 id='summary.l2boost'>Unimplemented generic function
These are placeholders right now.</h2><span id='topic+summary.l2boost'></span>

<h3>Description</h3>

<p>Unimplemented generic function
These are placeholders right now.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'l2boost'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.l2boost_+3A_object">object</code></td>
<td>
<p>an l2boost object</p>
</td></tr>
<tr><td><code id="summary.l2boost_+3A_...">...</code></td>
<td>
<p>other arguments (unused)</p>
</td></tr>
</table>

<hr>
<h2 id='VAR'>This is a hidden function of the l2boost package.
VAR is a helper function that specifically returns NA if all 
values of the argument x are NA, otherwise, it returns a var 
object.</h2><span id='topic+VAR'></span>

<h3>Description</h3>

<p>This is a hidden function of the l2boost package.
VAR is a helper function that specifically returns NA if all 
values of the argument x are NA, otherwise, it returns a var 
object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>VAR(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="VAR_+3A_x">x</code></td>
<td>
<p>return variance of x matrix.</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
