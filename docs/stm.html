<!DOCTYPE html><html><head><title>Help for package stm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {stm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#stm-package'><p>Structural Topic Model</p></a></li>
<li><a href='#alignCorpus'><p>Align the vocabulary of a new corpus to an old corpus</p></a></li>
<li><a href='#asSTMCorpus'><p>STM Corpus Coercion</p></a></li>
<li><a href='#calcfrex'><p>Calculate FREX (FRequency and EXclusivity) Words</p></a></li>
<li><a href='#calclift'><p>Calculate Lift Words</p></a></li>
<li><a href='#calcscore'><p>Calculate Score Words</p></a></li>
<li><a href='#checkBeta'><p>Looks for words that load exclusively onto a topic</p></a></li>
<li><a href='#checkResiduals'><p>Residual dispersion test for topic number</p></a></li>
<li><a href='#cloud'><p>Plot a wordcloud</p></a></li>
<li><a href='#convertCorpus'><p>Convert <span class="pkg">stm</span> formatted documents to another format</p></a></li>
<li><a href='#estimateEffect'><p>Estimates regressions using an STM object</p></a></li>
<li><a href='#exclusivity'><p>Exclusivity</p></a></li>
<li><a href='#findThoughts'><p>Find Thoughts</p></a></li>
<li><a href='#findTopic'><p>Find topics that contain user specified words.</p></a></li>
<li><a href='#fitNewDocuments'><p>Fit New Documents</p></a></li>
<li><a href='#gadarian'><p>Gadarian and Albertson data</p></a></li>
<li><a href='#js.estimate'><p>A James-Stein Estimator Shrinking to a Uniform Distribution</p></a></li>
<li><a href='#labelTopics'><p>Label topics</p></a></li>
<li><a href='#make.dt'><p>Make a <code>data.table</code> of topic proportions.</p></a></li>
<li><a href='#make.heldout'><p>Heldout Likelihood by Document Completion</p></a></li>
<li><a href='#makeDesignMatrix'><p>Make a Design Matrix</p></a></li>
<li><a href='#manyTopics'><p>Performs model selection across separate STM's that each assume different</p>
numbers of topics.</a></li>
<li><a href='#multiSTM'><p>Analyze Stability of Local STM Mode</p></a></li>
<li><a href='#optimizeDocument'><p>Optimize Document</p></a></li>
<li><a href='#permutationTest'><p>Permutation test of a binary covariate.</p></a></li>
<li><a href='#plot.estimateEffect'><p>Plot effect of covariates on topics</p></a></li>
<li><a href='#plot.MultimodDiagnostic'><p>Plotting Method for Multimodality Diagnostic Objects</p></a></li>
<li><a href='#plot.searchK'><p>Plots diagnostic values resulting from searchK</p></a></li>
<li><a href='#plot.STM'><p>Functions for plotting STM objects</p></a></li>
<li><a href='#plot.STMpermute'><p>Plot an STM permutation test.</p></a></li>
<li><a href='#plot.topicCorr'><p>Plot a topic correlation graph</p></a></li>
<li><a href='#plotModels'><p>Plots semantic coherence and exclusivity for high likelihood models</p>
outputted from selectModel.</a></li>
<li><a href='#plotQuote'><p>Plots strings</p></a></li>
<li><a href='#plotRemoved'><p>Plot documents, words and tokens removed at various word thresholds</p></a></li>
<li><a href='#plotTopicLoess'><p>Plot some effects with loess</p></a></li>
<li><a href='#poliblog5k'><p>CMU 2008 Political Blog Corpus</p></a></li>
<li><a href='#prepDocuments'><p>Prepare documents for analysis with <code>stm</code></p></a></li>
<li><a href='#readCorpus'><p>Read in a corpus file.</p></a></li>
<li><a href='#readLdac'><p>Read in a .ldac Formatted File</p></a></li>
<li><a href='#rmvnorm'><p>Draw from a Multivariate Normal</p></a></li>
<li><a href='#s'><p>Make a B-spline Basis Function</p></a></li>
<li><a href='#sageLabels'><p>Displays verbose labels that describe topics and topic-covariate groups in</p>
depth.</a></li>
<li><a href='#searchK'><p>Computes diagnostic values for models with different values of K (number of</p>
topics).</a></li>
<li><a href='#selectModel'><p>Assists the user in selecting the best STM model.</p></a></li>
<li><a href='#semanticCoherence'><p>Semantic Coherence</p></a></li>
<li><a href='#stm'><p>Variational EM for the Structural Topic Model</p></a></li>
<li><a href='#summary.estimateEffect'><p>Summary for estimateEffect</p></a></li>
<li><a href='#summary.STM'><p>Summary Function for the STM objects</p></a></li>
<li><a href='#textProcessor'><p>Process a vector of raw texts</p></a></li>
<li><a href='#thetaPosterior'><p>Draw from Theta Posterior</p></a></li>
<li><a href='#toLDAvis'><p>Wrapper to launch LDAvis topic browser.</p></a></li>
<li><a href='#toLDAvisJson'><p>Wrapper to create Json mapping for LDAvis. This can be useful in indirect render</p>
e.g. Shiny Dashboards</a></li>
<li><a href='#topicCorr'><p>Estimate topic correlation</p></a></li>
<li><a href='#topicLasso'><p>Plot predictions using topics</p></a></li>
<li><a href='#topicQuality'><p>Plots semantic coherence and exclusivity for each topic.</p></a></li>
<li><a href='#unpack.glmnet'><p>Unpack a <span class="pkg">glmnet</span> object</p></a></li>
<li><a href='#writeLdac'><p>Write a .ldac file</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Estimation of the Structural Topic Model</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3.7</td>
</tr>
<tr>
<td>Description:</td>
<td>The Structural Topic Model (STM) allows researchers 
  to estimate topic models with document-level covariates. 
  The package also includes tools for model selection, visualization,
  and estimation of topic-covariate regressions. Methods developed in
  Roberts et. al. (2014) &lt;<a href="https://doi.org/10.1111%2Fajps.12103">doi:10.1111/ajps.12103</a>&gt; and 
  Roberts et. al. (2016) &lt;<a href="https://doi.org/10.1080%2F01621459.2016.1141684">doi:10.1080/01621459.2016.1141684</a>&gt;. Vignette
  is Roberts et. al. (2019) &lt;<a href="https://doi.org/10.18637%2Fjss.v091.i02">doi:10.18637/jss.v091.i02</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), methods</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.11.3), data.table, glmnet, grDevices, graphics,
lda, Matrix, matrixStats, parallel, quadprog, quanteda, slam,
splines, stats, stringr, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>clue, geometry, huge, igraph, LDAvis, KernSmooth, NLP, rsvd,
Rtsne, SnowballC, spelling, testthat, tm (&ge; 0.6), wordcloud</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://www.structuraltopicmodel.com/">http://www.structuraltopicmodel.com/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/bstewart/stm/issues">https://github.com/bstewart/stm/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-01 14:43:38 UTC; bms4</td>
</tr>
<tr>
<td>Author:</td>
<td>Margaret Roberts [aut],
  Brandon Stewart [aut, cre],
  Dustin Tingley [aut],
  Kenneth Benoit [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Brandon Stewart &lt;bms4@princeton.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-01 22:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='stm-package'>Structural Topic Model</h2><span id='topic+stm-package'></span>

<h3>Description</h3>

<p>This package implements the Structural Topic Model, a general approach to
including document-level metadata within mixed-membership topic models. To
read the vignette use <code>vignette('stmVignette')</code>.
</p>


<h3>Details</h3>

<p>Functions to manipulate documents: <code><a href="#topic+textProcessor">textProcessor</a></code>
<code><a href="#topic+readCorpus">readCorpus</a></code> <code><a href="#topic+prepDocuments">prepDocuments</a></code>
</p>
<p>Functions to fit the model: <code><a href="#topic+stm">stm</a></code> <code><a href="#topic+selectModel">selectModel</a></code>
<code><a href="#topic+manyTopics">manyTopics</a></code> <code><a href="#topic+searchK">searchK</a></code>
</p>
<p>Functions to summarize a model: <code><a href="#topic+labelTopics">labelTopics</a></code>
<code><a href="#topic+summary.STM">summary.STM</a></code> <code><a href="#topic+findThoughts">findThoughts</a></code>
</p>
<p>Functions for Post-Estimation: <code><a href="#topic+estimateEffect">estimateEffect</a></code>
<code><a href="#topic+topicCorr">topicCorr</a></code> <code><a href="#topic+permutationTest">permutationTest</a></code>
</p>
<p>Plotting Functions: <code><a href="#topic+plot.STM">plot.STM</a></code> <code><a href="#topic+plot.estimateEffect">plot.estimateEffect</a></code>
<code><a href="#topic+plot.topicCorr">plot.topicCorr</a></code> <code><a href="#topic+plot.STMpermute">plot.STMpermute</a></code>
<code><a href="#topic+plotQuote">plotQuote</a></code> <code><a href="#topic+plotTopicLoess">plotTopicLoess</a></code>
<code><a href="#topic+plotModels">plotModels</a></code> <code><a href="#topic+topicQuality">topicQuality</a></code>
</p>
<p>Pre-Fit Models and Data: <code><a href="#topic+gadarian">gadarian</a></code> <code><a href="#topic+gadarianFit">gadarianFit</a></code>
<code><a href="#topic+poliblog5k">poliblog5k</a></code>
</p>


<h3>Author(s)</h3>

<p>Author: Margaret E. Roberts, Brandon M. Stewart and Dustin Tingley
</p>
<p>Maintainer: Brandon Stewart &lt;bms4@princeton.edu&gt;
</p>


<h3>References</h3>

<p>Roberts, M., Stewart, B., Tingley, D., and Airoldi, E. (2013)
&quot;The structural topic model and applied social science.&quot; In Advances in
Neural Information Processing Systems Workshop on Topic Models: Computation,
Application, and Evaluation.
</p>
<p>Roberts, M., Stewart, B., Tingley, D., Lucas, C., Leder-Luis, J., Gadarian,
S., Albertson, B., Albertson, B. and Rand, D. (2014). &quot;Structural topic
models for open ended survey responses.&quot; American Journal of Political
Science.
</p>
<p>Additional papers at: structuraltopicmodel.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+stm">stm</a></code>
</p>

<hr>
<h2 id='alignCorpus'>Align the vocabulary of a new corpus to an old corpus</h2><span id='topic+alignCorpus'></span>

<h3>Description</h3>

<p>Function that takes in a list of documents, vocab and (optionally) metadata
for a corpus of previously unseen documents and aligns them to an old vocabulary. 
Helps preprocess documents for <code><a href="#topic+fitNewDocuments">fitNewDocuments</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>alignCorpus(new, old.vocab, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="alignCorpus_+3A_new">new</code></td>
<td>
<p>a list (such as those produced by <code>textProcessor</code> or <code>prepDocuments</code>) 
containing a list of documents in <code><a href="#topic+stm">stm</a></code> format, a character vector 
containing the vocabulary and optional a <code>data.frame</code> containing meta data.
These should be labeled <code>documents</code>, <code>vocab</code>,and <code>meta</code> respectively.
This is the new set of unseen documents which will be returned with the vocab renumbered
and all words not appearing in <code>old</code> removed.</p>
</td></tr>
<tr><td><code id="alignCorpus_+3A_old.vocab">old.vocab</code></td>
<td>
<p>a character vector containing the vocabulary that you want to align to.
In general this will be the vocab used in your original stm model fit which from an stm
object called <code>mod</code> can be accessed as <code>mod$vocab</code>.</p>
</td></tr>
<tr><td><code id="alignCorpus_+3A_verbose">verbose</code></td>
<td>
<p>a logical indicating whether information about the new corpus should be
printed to the screen. Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When estimating topic proportions for previously unseen documents using
<code><a href="#topic+fitNewDocuments">fitNewDocuments</a></code> the new documents must have the same vocabulary
ordered in the same was as the original model.  This function helps with that
process.
</p>
<p>Note: the code is not really built for speed or memory efficiency- if you are trying
to do this with a really large corpus of new texts you might consider building the object
yourself using <span class="pkg">quanteda</span> or some other option.
</p>


<h3>Value</h3>

<table>
<tr><td><code>documents</code></td>
<td>
<p>A list containing the documents in the stm format.</p>
</td></tr>
<tr><td><code>vocab</code></td>
<td>
<p>Character vector of vocabulary.</p>
</td></tr> <tr><td><code>meta</code></td>
<td>
<p>Data frame or
matrix containing the user-supplied metadata for the retained documents.</p>
</td></tr>
<tr><td><code>docs.removed</code></td>
<td>
<p>document indices (corresponding to the original data passed) of
documents removed because they contain no words</p>
</td></tr>
<tr><td><code>words.removed</code></td>
<td>
<p>words dropped from <code>new</code></p>
</td></tr>
<tr><td><code>tokens.removed</code></td>
<td>
<p>the total number of tokens dropped from the new documents.</p>
</td></tr>
<tr><td><code>wordcounts</code></td>
<td>
<p>counts of times the old vocab appears in the new documents</p>
</td></tr>
<tr><td><code>prop.overlap</code></td>
<td>
<p>length two vector used to populate the message printed by verbose.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+prepDocuments">prepDocuments</a></code> <code><a href="#topic+fitNewDocuments">fitNewDocuments</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#we process an original set that is just the first 100 documents
temp&lt;-textProcessor(documents=gadarian$open.ended.response[1:100],metadata=gadarian[1:100,])
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta)
set.seed(02138)
#Maximum EM its is set low to make this run fast, run models to convergence!
mod.out &lt;- stm(out$documents, out$vocab, 3, prevalence=~treatment + s(pid_rep), 
              data=out$meta, max.em.its=5)
#now we process the remaining documents
temp&lt;-textProcessor(documents=gadarian$open.ended.response[101:nrow(gadarian)],
                    metadata=gadarian[101:nrow(gadarian),])
#note we don't run prepCorpus here because we don't want to drop any words- we want 
#every word that showed up in the old documents.
newdocs &lt;- alignCorpus(new=temp, old.vocab=mod.out$vocab)
#we get some helpful feedback on what has been retained and lost in the print out.
#and now we can fit our new held-out documents
fitNewDocuments(model=mod.out, documents=newdocs$documents, newData=newdocs$meta,
                origData=out$meta, prevalence=~treatment + s(pid_rep),
                prevalencePrior="Covariate")
</code></pre>

<hr>
<h2 id='asSTMCorpus'>STM Corpus Coercion</h2><span id='topic+asSTMCorpus'></span>

<h3>Description</h3>

<p>Convert a set of document term counts and associated metadata to
the form required for processing by the <code><a href="#topic+stm">stm</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>asSTMCorpus(documents, vocab, data = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="asSTMCorpus_+3A_documents">documents</code></td>
<td>
<p>A documents-by-term matrix of counts, or a set of
counts in the format returned by <code><a href="#topic+prepDocuments">prepDocuments</a></code>. Supported
matrix formats include <span class="pkg">quanteda</span> <a href="quanteda.html#topic+dfm">dfm</a>
and <span class="pkg">Matrix</span> sparse matrix objects in <code>"dgCMatrix"</code> or
<code>"dgTMatrix"</code> format.</p>
</td></tr>
<tr><td><code id="asSTMCorpus_+3A_vocab">vocab</code></td>
<td>
<p>Character vector specifying the words in the corpus in the
order of the vocab indices in documents. Each term in the vocabulary index
must appear at least once in the documents.  See <code><a href="#topic+prepDocuments">prepDocuments</a></code>
for dropping unused items in the vocabulary.  If <code>documents</code> is a
sparse matrix or <span class="pkg">quanteda</span> <a href="quanteda.html#topic+dfm">dfm</a> object, then <code>vocab</code> should not
(and must not) be supplied.  It is contained already inside the column
names of the matrix.</p>
</td></tr>
<tr><td><code id="asSTMCorpus_+3A_data">data</code></td>
<td>
<p>An optional data frame containing the prevalence and/or content
covariates.  If unspecified the variables are taken from the active
environment.</p>
</td></tr>
<tr><td><code id="asSTMCorpus_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components <code>"documents"</code>, <code>"vocab"</code>, and
<code>"data"</code> in the form needed for further processing by the <code>stm</code>
function.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prepDocuments">prepDocuments</a></code>, <code><a href="#topic+stm">stm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(quanteda)
gadarian_corpus &lt;- corpus(gadarian, text_field = "open.ended.response")
gadarian_dfm &lt;- dfm(gadarian_corpus, 
                     remove = stopwords("english"),
                     stem = TRUE)
asSTMCorpus(gadarian_dfm)

</code></pre>

<hr>
<h2 id='calcfrex'>Calculate FREX (FRequency and EXclusivity) Words</h2><span id='topic+calcfrex'></span>

<h3>Description</h3>

<p>A primarily internal function for calculating FREX words.
We expect most users will use <code><a href="#topic+labelTopics">labelTopics</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calcfrex(logbeta, w = 0.5, wordcounts = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calcfrex_+3A_logbeta">logbeta</code></td>
<td>
<p>a K by V matrix containing the log probabilities of seeing word v conditional on topic k</p>
</td></tr>
<tr><td><code id="calcfrex_+3A_w">w</code></td>
<td>
<p>a value between 0 and 1 indicating the proportion of the weight assigned to frequency</p>
</td></tr>
<tr><td><code id="calcfrex_+3A_wordcounts">wordcounts</code></td>
<td>
<p>a vector of word counts.  If provided, a James-Stein type shrinkage estimator is 
applied to stabilize the exclusivity probabilities. This helps with the concern that the rarest words
will always be completely exclusive.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>FREX attempts to find words which are both frequent in and exclusive to a topic of interest.
Balancing these two traits is important as frequent words are often by themselves simply functional
words necessary to discuss any topic.  While completely exclusive words can be so rare as to not
be informative. This accords with a long-running trend in natural language processing which is best exemplified
by the Term frequency-Inverse document frequency metric.  
</p>
<p>Our notion of FREX comes from a paper by Bischof and Airoldi (2012) which proposed a Hierarchical
Poisson Deconvolution model.  It relies on a known hierarchical structure in the documents and requires
a rather complicated estimation scheme.  We wanted a metric that would capture their core insight but
still be fast to compute.
</p>
<p>Bischof and Airoldi consider as a summary for a word's contribution to a topic the harmonic mean of the
word's rank in terms of exclusivity and frequency.  The harmonic mean is attractive here because it 
does not allow a high rank along one of the dimensions to compensate for the lower rank in another. Thus
words with a high score must be high along both dimensions.
</p>
<p>The formula is ' 
</p>
<p style="text-align: center;"><code class="reqn">FREX = \left(\frac{w}{F} + \frac{1-w}{E}\right)^{-1}</code>
</p>
 
<p>where F is the frequency score given by the empirical CDF of the word in it's topic distribution.  Exclusivity
is calculated by column-normalizing the beta matrix (thus representing the conditional probability of seeing
the topic given the word).  Then the empirical CDF of the word is computed within the topic.  Thus words with
high values are those where most of the mass for that word is assigned to the given topic.
</p>
<p>For rare words exclusivity will always be very high because there simply aren't many instances of the word.
If <code>wordcounts</code> are passed, the function will calculate a regularized form of this distribution using a
James-Stein type estimator described in <code><a href="#topic+js.estimate">js.estimate</a></code>.
</p>


<h3>References</h3>

<p>Bischof and Airoldi (2012) &quot;Summarizing topical content with word frequency and exclusivity&quot;
In Proceedings of the International Conference on Machine Learning.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+labelTopics">labelTopics</a></code> <code><a href="#topic+js.estimate">js.estimate</a></code>
</p>

<hr>
<h2 id='calclift'>Calculate Lift Words</h2><span id='topic+calclift'></span>

<h3>Description</h3>

<p>A primarily internal function for calculating words according to the lift metric.
We expect most users will use <code><a href="#topic+labelTopics">labelTopics</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calclift(logbeta, wordcounts)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calclift_+3A_logbeta">logbeta</code></td>
<td>
<p>a K by V matrix containing the log probabilities of seeing word v conditional on topic k</p>
</td></tr>
<tr><td><code id="calclift_+3A_wordcounts">wordcounts</code></td>
<td>
<p>a V length vector indicating the number of times each word appears in the corpus.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Lift is the calculated by dividing the topic-word distribution by the empirical
word count probability distribution.  In other words the Lift for word v in topic
k can be calculated as:
</p>
<p style="text-align: center;"><code class="reqn">Lift = \beta_{k,v}/(w_v/\sum_v w_v)</code>
</p>
 
<p>We include this after seeing it used effectively in Matt Taddy's work including his
excellent <span class="pkg">maptpx</span> package. Definitions are given in Taddy(2012).
</p>


<h3>References</h3>

<p>Taddy, Matthew. 2012. &quot;On Estimation and Selection for Topic Models.&quot; AISTATS JMLR W&amp;CP 22
</p>


<h3>See Also</h3>

<p><code><a href="#topic+labelTopics">labelTopics</a></code>
</p>

<hr>
<h2 id='calcscore'>Calculate Score Words</h2><span id='topic+calcscore'></span>

<h3>Description</h3>

<p>A primarily internal function for calculating words according to the score metric.
We expect most users will use <code><a href="#topic+labelTopics">labelTopics</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calcscore(logbeta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calcscore_+3A_logbeta">logbeta</code></td>
<td>
<p>a K by V matrix containing the log probabilities of seeing word v conditional on topic k</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Score is a metric which we include because it is used effectively in the 
<span class="pkg">lda</span> package by Jonathan Chang. It is calculated as:
</p>
<p style="text-align: center;"><code class="reqn">\beta_{v, k} (\log \beta_{w,k} - 1 / K \sum_{k'} \log \beta_{v,k'})</code>
</p>



<h3>References</h3>

<p>Jonathan Chang (2015). lda: Collapsed Gibbs Sampling Methods for Topic Models. R package version 1.4.2.
https://CRAN.R-project.org/package=lda
</p>


<h3>See Also</h3>

<p><code><a href="#topic+labelTopics">labelTopics</a></code>
</p>

<hr>
<h2 id='checkBeta'>Looks for words that load exclusively onto a topic</h2><span id='topic+checkBeta'></span>

<h3>Description</h3>

<p>Checks the log beta matrix for values too close to 0, which reflect words
that load onto a single topic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>checkBeta(stmobject, tolerance = 0.01)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="checkBeta_+3A_stmobject">stmobject</code></td>
<td>
<p>STM Model Output</p>
</td></tr>
<tr><td><code id="checkBeta_+3A_tolerance">tolerance</code></td>
<td>
<p>User specified input reflecting closeness to 1.  E.g. a
tolerance of .01 will flag any values greater than .99.  Tolerance must be
above 1e-6.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function checks the log beta matrix for values that exceed the tolerance
threshold, indicating that a word has loaded onto a single topics. The output
gives the user lists of which topics have problems, which words in which
topics have problems, as well as a count of the total problems in topics and
the total number of problem words.
</p>
<p>Note that if the tolerance value is below 1e-6, this function will throw an
error.
</p>


<h3>Value</h3>

<table>
<tr><td><code>problemTopics</code></td>
<td>
<p>A list of vectors, each vector corresponding to
the set of topics in the relevant beta matrix that contain words with too
high of a loading to that topic </p>
</td></tr> <tr><td><code>topicErrorTotal</code></td>
<td>
<p>A list of integers,
each corresponding to the total number of topics with problems in the
relevant beta matrix</p>
</td></tr> <tr><td><code>problemWords</code></td>
<td>
<p>A list of matrices, each
corresponding to a relevant beta matrix, which gives the topic and word
index of each word with too high of a topic loading</p>
</td></tr> <tr><td><code>wordErrorTotal</code></td>
<td>
<p>A
list of integers, each corresponding to the total words with problems for
the relevant beta matrix</p>
</td></tr> <tr><td><code>check</code></td>
<td>
<p>A boolean representing if the check
was passed. If wordErrorTotal is all 0s (no errors), check is True.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Antonio Coppola
</p>


<h3>Examples</h3>

<pre><code class='language-R'>checkBeta(gadarianFit)
</code></pre>

<hr>
<h2 id='checkResiduals'>Residual dispersion test for topic number</h2><span id='topic+checkResiduals'></span>

<h3>Description</h3>

<p>Computes the multinomial dispersion of the STM residuals as in Taddy (2012)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>checkResiduals(stmobj, documents, tol = 0.01)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="checkResiduals_+3A_stmobj">stmobj</code></td>
<td>
<p>An <code>STM</code> model object for which to compute residuals.</p>
</td></tr>
<tr><td><code id="checkResiduals_+3A_documents">documents</code></td>
<td>
<p>The documents corresponding to <code>stmobj</code> as in
<code><a href="#topic+stm">stm</a></code>.</p>
</td></tr>
<tr><td><code id="checkResiduals_+3A_tol">tol</code></td>
<td>
<p>The tolerance parameter for calculating the degrees of freedom.
Defaults to 1/100 as in Taddy(2012)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements the residual-based diagnostic method of Taddy
(2012).  The basic idea is that when the model is correctly specified the
multinomial likelihood implies a dispersion of the residuals:
<code class="reqn">\sigma^2=1</code>.  If we calculate the sample dispersion and the value is
greater than one, this implies that the number of topics is set too low,
because the latent topics are not able to account for the overdispersion. In
practice this can be a very demanding criterion, especially if the documents
are long.  However, when coupled with other tools it can provide a valuable
perspective on model fit. The function is based on the Taddy 2012 paper as well as code
found in maptpx package.
</p>
<p>Further details are available in the referenced paper, but broadly speaking
the dispersion is derived from the mean of the squared adjusted residuals.
We get the sample dispersion by dividing by the degrees of freedom
parameter.  In estimating the degrees of freedom, we follow Taddy (2012) in
approximating the parameter <code class="reqn">\hat{N}</code> by the number of expected counts
exceeding a tolerance parameter.  The default value of 1/100 given in the
Taddy paper can be changed by setting the <code>tol</code> argument.
</p>
<p>The function returns the estimated sample dispersion (which equals 1 under
the data generating process) and the p-value of a chi-squared test where the
null hypothesis is that <code class="reqn">\sigma^2=1</code> vs the alternative <code class="reqn">\sigma^2
&gt;1</code>. As Taddy notes and we echo, rejection of the null 'provides a very
rough measure for evidence in favor of a larger number of topics.'
</p>


<h3>References</h3>

<p>Taddy, M. 'On Estimation and Selection for Topic Models'.
AISTATS 2012, JMLR W&amp;CP 22
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#An example using the Gadarian data.  From Raw text to fitted model.
temp&lt;-textProcessor(documents=gadarian$open.ended.response,metadata=gadarian)
meta&lt;-temp$meta
vocab&lt;-temp$vocab
docs&lt;-temp$documents
out &lt;- prepDocuments(docs, vocab, meta)
docs&lt;-out$documents
vocab&lt;-out$vocab
meta &lt;-out$meta
set.seed(02138)
#maximum EM iterations set very low so example will run quickly.  
#Run your models to convergence!
mod.out &lt;- stm(docs, vocab, 3, prevalence=~treatment + s(pid_rep), data=meta,
               max.em.its=5)
checkResiduals(mod.out, docs)
</code></pre>

<hr>
<h2 id='cloud'>Plot a wordcloud</h2><span id='topic+cloud'></span>

<h3>Description</h3>

<p>Use the <span class="pkg">wordcloud</span> package to plot a wordcloud for a particular topic
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cloud(
  stmobj,
  topic = NULL,
  type = c("model", "documents"),
  documents,
  thresh = 0.9,
  max.words = 100,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cloud_+3A_stmobj">stmobj</code></td>
<td>
<p>The STM model object to be used in making the word cloud.</p>
</td></tr>
<tr><td><code id="cloud_+3A_topic">topic</code></td>
<td>
<p>NULL to plot the marginal distribution of words in the corpus,
or a single integer indicating the topic number.</p>
</td></tr>
<tr><td><code id="cloud_+3A_type">type</code></td>
<td>
<p>Specifies how the wordcloud is constructed.  The type
<code>"model"</code> which is used by default is based on the probability of the
word given the topic.  The type <code>"documents"</code> plots words within
documents that have a topic proportion of higher than <code>thresh</code>.  This
requires that the <code>documents</code> argument also be specified.</p>
</td></tr>
<tr><td><code id="cloud_+3A_documents">documents</code></td>
<td>
<p>The documents object of the same kind as passed to
<code><a href="#topic+stm">stm</a></code>.  This is only necessary if <code>type="documents"</code>.</p>
</td></tr>
<tr><td><code id="cloud_+3A_thresh">thresh</code></td>
<td>
<p>The threshold for including a document in the
<code>type="documents"</code> setting.</p>
</td></tr>
<tr><td><code id="cloud_+3A_max.words">max.words</code></td>
<td>
<p>The maximum number of words to be plotted.</p>
</td></tr>
<tr><td><code id="cloud_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to <code>wordcloud</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses the <span class="pkg">wordcloud</span> package to make a word cloud of a particular topic.
The option <code>"model"</code> uses the topic-word model parameters.  Thus it
shows words weighted by their probability conditional that the word comes
from a particular topic.  With content covariates it averages over the
values for all levels of the content covariate weighted by the empirical
frequency in the dataset.  The option <code>"documents"</code> plots the words
which appear in documents that have a topic proportion higher than
<code>thresh</code>.  Thus <code>"model"</code> gives a pure model based interpretation
of the topic while <code>"documents"</code> gives a picture of all the words in
documents which are highly associated with the topic.
</p>


<h3>References</h3>

<p>Ian Fellows (2014). wordcloud: Word Clouds. R package version
2.5.  <a href="https://cran.r-project.org/package=wordcloud">https://cran.r-project.org/package=wordcloud</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.STM">plot.STM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cloud(gadarianFit, 1)
</code></pre>

<hr>
<h2 id='convertCorpus'>Convert <span class="pkg">stm</span> formatted documents to another format</h2><span id='topic+convertCorpus'></span>

<h3>Description</h3>

<p>Takes an <span class="pkg">stm</span> formatted documents and vocab object and returns formats
usable in other packages.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convertCorpus(documents, vocab, type = c("slam", "lda", "Matrix"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convertCorpus_+3A_documents">documents</code></td>
<td>
<p>the documents object in <span class="pkg">stm</span> format</p>
</td></tr>
<tr><td><code id="convertCorpus_+3A_vocab">vocab</code></td>
<td>
<p>the vocab object in <span class="pkg">stm</span> format</p>
</td></tr>
<tr><td><code id="convertCorpus_+3A_type">type</code></td>
<td>
<p>the output type desired.  See Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We also recommend the <span class="pkg">quanteda</span> and <span class="pkg">tm</span> packages for text preparation
etc.  The <code>convertCorpus</code> function is provided as a helpful utility for 
moving formats around, but if you intend to do text processing with a variety
of output formats, you likely want to start with <span class="pkg">quanteda</span> or <span class="pkg">tm</span>.
</p>
<p>The various type conversions are described below: 
</p>

<dl>
<dt><code>type = "slam"</code></dt><dd><p>Converts to the simple triplet matrix
representation used by the <span class="pkg">slam</span> package.  This is the format used
internally by <span class="pkg">tm</span>.</p>
</dd> 
<dt><code>type = "lda"</code></dt><dd><p>Converts to the format
used by the <span class="pkg">lda</span> package.  This is a very minor change as the format in
<span class="pkg">stm</span> is based on <span class="pkg">lda</span>'s data representation.  The difference as
noted in <code><a href="#topic+stm">stm</a></code> involves how the numbers are indexed.
Accordingly this type returns a list containing the new documents object and
the unchanged vocab object.</p>
</dd> 
<dt><code>type = "Matrix"</code></dt><dd><p>Converts to the
sparse matrix representation used by <span class="pkg">Matrix</span>.  This is the format used
internally by numerous other text analysis packages.</p>
</dd> </dl>

<p>If you want to write
out a file containing the sparse matrix representation popularized by David
Blei's <code>C</code> code <code>ldac</code> see the function <code><a href="#topic+writeLdac">writeLdac</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+writeLdac">writeLdac</a></code> <code><a href="#topic+readCorpus">readCorpus</a></code>
<code><a href="#topic+poliblog5k">poliblog5k</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#convert the poliblog5k data to slam package format
poliSlam &lt;- convertCorpus(poliblog5k.docs, poliblog5k.voc, type="slam")
class(poliSlam)
poliMatrix &lt;- convertCorpus(poliblog5k.docs, poliblog5k.voc, type="Matrix")
class(poliMatrix)
poliLDA &lt;- convertCorpus(poliblog5k.docs, poliblog5k.voc, type="lda")
str(poliLDA)
</code></pre>

<hr>
<h2 id='estimateEffect'>Estimates regressions using an STM object</h2><span id='topic+estimateEffect'></span>

<h3>Description</h3>

<p>Estimates a regression where documents are the units, the outcome is the
proportion of each document about a topic in an STM model and the covariates
are document-meta data.  This procedure incorporates measurement uncertainty
from the STM model using the method of composition.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimateEffect(
  formula,
  stmobj,
  metadata = NULL,
  uncertainty = c("Global", "Local", "None"),
  documents = NULL,
  nsims = 25,
  prior = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimateEffect_+3A_formula">formula</code></td>
<td>
<p>A formula for the regression.  It should have an integer or
vector of numbers on the left-hand side and an equation with covariates on
the right hand side.  See Details for more information.</p>
</td></tr>
<tr><td><code id="estimateEffect_+3A_stmobj">stmobj</code></td>
<td>
<p>Model output from STM</p>
</td></tr>
<tr><td><code id="estimateEffect_+3A_metadata">metadata</code></td>
<td>
<p>A dataframe where all predictor variables in the formula can
be found. If <code>NULL</code> R will look for the variables in the global
namespace.  It will not look for them in the <code>STM</code> object which for
memory efficiency only stores the transformed design matrix and thus will
not in general have the original covariates.</p>
</td></tr>
<tr><td><code id="estimateEffect_+3A_uncertainty">uncertainty</code></td>
<td>
<p>Which procedure should be used to approximate the
measurement uncertainty in the topic proportions.  See details for more
information.  Defaults to the Global approximation.</p>
</td></tr>
<tr><td><code id="estimateEffect_+3A_documents">documents</code></td>
<td>
<p>If uncertainty is set to <code>Local</code>, the user needs to
provide the documents object (see <code><a href="#topic+stm">stm</a></code> for format).</p>
</td></tr>
<tr><td><code id="estimateEffect_+3A_nsims">nsims</code></td>
<td>
<p>The number of simulated draws from the variational posterior.
Defaults to 25.  This can often go even lower without affecting the results
too dramatically.</p>
</td></tr>
<tr><td><code id="estimateEffect_+3A_prior">prior</code></td>
<td>
<p>This argument allows the user to specify a ridge penalty to be
added to the least squares solution for the purposes of numerical stability.
If its a scalar it is added to all coefficients.  If its a matrix it should
be the prior precision matrix (a diagonal matrix of the same dimension as
the <code>ncol(X)</code>).  When the design matrix is collinear but this argument
is not specified, a warning will pop up and the function will estimate with
a small default penalty.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs a regression where topic-proportions are the outcome
variable.  This allows us to conditional expectation of topic prevalence
given document characteristics.  Use of the method of composition allows us
to incorporate our estimation uncertainty in the dependent variable. Mechanically
this means we draw a set of topic proportions from the variational posterior,
compute our coefficients, then repeat.  To compute quantities of interest we
simulate within each batch of coefficients and then average over all our results.
</p>
<p>The formula specifies the nature of the linear model.  On the left hand-side
we use a vector of integers to indicate the topics to be included as outcome
variables.  If left blank then the default of all topics is used. On the
right hand-side we can specify a linear model of covariates including
standard transformations.  Thus the model <code>2:4 ~ var1 + s(var2)</code> would
indicate that we want to run three regressions on Topics 2, 3 and 4 with
predictor variables <code>var1</code> and a b-spline transformed <code>var2</code>.  We
encourage the use of spline functions for non-linear transformations of
variables.
</p>
<p>The function allows the user to specify any variables in the model.
However, we caution that for the assumptions of the method of composition to
be the most plausible the topic model should contain at least all the
covariates contained in the <code>estimateEffect</code> regression.  However the
inverse need not be true.  The function will automatically check whether the
covariate matrix is singular which generally results from linearly dependent
columns.  Some common causes include a factor variable with an unobserved
level, a spline with degrees of freedom that are too high, or a spline with
a continuous variable where a gap in the support of the variable results in
several empty basis functions.  In these cases the function will still
estimate by adding a small ridge penalty to the likelihood.  However, we
emphasize that while this will produce an estimate it is only identified by
the penalty.  In many cases this will be an indication that the user should
specify a different model.
</p>
<p>The function can handle factors and numeric variables.  Dates should be
converted to numeric variables before analysis.
</p>
<p>We offer several different methods of incorporating uncertainty.  Ideally we
would want to use the covariance matrix that governs the variational
posterior for each document (<code class="reqn">\nu</code>).  The updates for the global
parameters rely only on the sum of these matrices and so we do not store
copies for each individual document.  The default uncertainty method
<code>Global</code> uses an approximation to the average covariance matrix formed
using the global parameters.  The uncertainty method <code>Local</code> steps
through each document and updates the parameters calculating and then saving
the local covariance matrix.  The option <code>None</code> simply uses the map
estimates for <code class="reqn">\theta</code> and does not incorporate any uncertainty.  We
strongly recommend the <code>Global</code> approximation as it provides the best
tradeoff of accuracy and computational tractability.
</p>
<p>Effects are plotted based on the results of <code><a href="#topic+estimateEffect">estimateEffect</a></code>
which contains information on how the estimates are constructed.  Note that
in some circumstances the expected value of a topic proportion given a
covariate level can be above 1 or below 0.  This is because we use a Normal
distribution rather than something constrained to the range between 0 and 1.
If a continuous variable goes above 0 or 1 within the range of the data it
may indicate that a more flexible non-linear specification is needed (such
as using a spline or a spline with greater degrees of freedom).
</p>


<h3>Value</h3>

<table>
<tr><td><code>parameters</code></td>
<td>
<p>A list of K elements each corresponding to a
topic.  Each element is itself a list of n elements one per simulation.
Each simulation contains the MLE of the parameter vector and the variance
covariance matrix</p>
</td></tr> <tr><td><code>topics</code></td>
<td>
<p>The topic vector</p>
</td></tr> <tr><td><code>call</code></td>
<td>
<p>The original
call</p>
</td></tr> <tr><td><code>uncertainty</code></td>
<td>
<p>The user choice of uncertainty measure</p>
</td></tr>
<tr><td><code>formula</code></td>
<td>
<p>The formula object</p>
</td></tr> <tr><td><code>data</code></td>
<td>
<p>The original user provided
meta data.</p>
</td></tr> <tr><td><code>modelframe</code></td>
<td>
<p>The model frame created from the formula and
data</p>
</td></tr> <tr><td><code>varlist</code></td>
<td>
<p>A variable list useful for mapping terms with columns
in the design matrix</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+plot.estimateEffect">plot.estimateEffect</a></code> <code><a href="#topic+summary.estimateEffect">summary.estimateEffect</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Just one topic (note we need c() to indicate it is a vector)
prep &lt;- estimateEffect(c(1) ~ treatment, gadarianFit, gadarian)
summary(prep)
plot(prep, "treatment", model=gadarianFit, method="pointestimate")

#three topics at once
prep &lt;- estimateEffect(1:3 ~ treatment, gadarianFit, gadarian)
summary(prep)
plot(prep, "treatment", model=gadarianFit, method="pointestimate")

#with interactions
prep &lt;- estimateEffect(1 ~ treatment*s(pid_rep), gadarianFit, gadarian)
summary(prep)
</code></pre>

<hr>
<h2 id='exclusivity'>Exclusivity</h2><span id='topic+exclusivity'></span>

<h3>Description</h3>

<p>Calculate an exclusivity metric for an STM model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>exclusivity(model, M = 10, frexw = 0.7)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="exclusivity_+3A_model">model</code></td>
<td>
<p>the STM object</p>
</td></tr>
<tr><td><code id="exclusivity_+3A_m">M</code></td>
<td>
<p>the number of top words to consider per topic</p>
</td></tr>
<tr><td><code id="exclusivity_+3A_frexw">frexw</code></td>
<td>
<p>the frex weight</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In Roberts et al 2014 we proposed using the Mimno et al 2011 <code><a href="#topic+semanticCoherence">semanticCoherence</a></code> metric 
for helping with topic model selection. We found that semantic coherence alone is relatively easy to
achieve by having only a couple of topics which all are dominated by the most common words.  Thus we
also proposed an exclusivity measure.  
</p>
<p>Our exclusivity measure includes some information on word frequency as well.  It is based on the FREX
labeling metric (<code><a href="#topic+calcfrex">calcfrex</a></code>) with the weight set to .7 in favor of exclusivity by default.
</p>
<p>This function is currently marked with the keyword internal because it does not have much error checking.
</p>


<h3>Value</h3>

<p>a numeric vector containing exclusivity for each topic
</p>


<h3>References</h3>

<p>Mimno, D., Wallach, H. M., Talley, E., Leenders, M., &amp; McCallum, A. (2011, July). 
&quot;Optimizing semantic coherence in topic models.&quot; In Proceedings of the Conference on Empirical Methods in 
Natural Language Processing (pp. 262-272). Association for Computational Linguistics. Chicago
</p>
<p>Bischof and Airoldi (2012) &quot;Summarizing topical content with word frequency and exclusivity&quot;
In Proceedings of the International Conference on Machine Learning.
</p>
<p>Roberts, M., Stewart, B., Tingley, D., Lucas, C., Leder-Luis, J., Gadarian, S., Albertson, B., et al. (2014). 
&quot;Structural topic models for open ended survey responses.&quot; American Journal of Political Science, 58(4), 1064-1082.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+searchK">searchK</a></code> <code><a href="#topic+plot.searchK">plot.searchK</a></code> <code><a href="#topic+semanticCoherence">semanticCoherence</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>exclusivity(gadarianFit)
</code></pre>

<hr>
<h2 id='findThoughts'>Find Thoughts</h2><span id='topic+findThoughts'></span><span id='topic+print.findThoughts'></span><span id='topic+plot.findThoughts'></span>

<h3>Description</h3>

<p>Outputs most representative documents for a particular topic. Use this in
order to get a better sense of the content of actual documents with a high
topical content.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findThoughts(
  model,
  texts = NULL,
  topics = NULL,
  n = 3,
  thresh = NULL,
  where = NULL,
  meta = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findThoughts_+3A_model">model</code></td>
<td>
<p>Model object created by <code>stm</code>.</p>
</td></tr>
<tr><td><code id="findThoughts_+3A_texts">texts</code></td>
<td>
<p>A character vector where each entry contains the text of a
document.  Must be in the same order as the documents object. NOTE: This is not the
documents which are passed to <code>stm</code> and come out of <code>prepDocuments</code>, 
this is the actual text of the document.</p>
</td></tr>
<tr><td><code id="findThoughts_+3A_topics">topics</code></td>
<td>
<p>The topic number or vector of topic numbers for which you want
to find thoughts.  Defaults to all topics.</p>
</td></tr>
<tr><td><code id="findThoughts_+3A_n">n</code></td>
<td>
<p>The number of desired documents to be displayed per topic.</p>
</td></tr>
<tr><td><code id="findThoughts_+3A_thresh">thresh</code></td>
<td>
<p>Sets a minimum threshold for the estimated topic proportion
for displayed documents.  It defaults to imposing no restrictions.</p>
</td></tr>
<tr><td><code id="findThoughts_+3A_where">where</code></td>
<td>
<p>An expression in the form of a <code>data.table</code> query. This is passed to the <code>i</code> argument in data.table and a custom query is passed to <code>j</code>.  This cannot be used with <code>thresh</code>.  See below for more details.</p>
</td></tr>
<tr><td><code id="findThoughts_+3A_meta">meta</code></td>
<td>
<p>The meta data object to be used with <code>where</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns the top <code>n</code> documents ranked by the MAP estimate of the topic's
theta value (which captures the modal estimate of the proportion of word
tokens assigned to the topic under the model). Setting the <code>thresh</code>
argument allows the user to specify a minimal value of theta for returned
documents. Returns document indices and top thoughts.
</p>
<p>Sometimes you may want to find thoughts which have more conditions than simply 
a minimum threshold.  For example, you may want to grab all documents which satisfy
certain conditions on the metadata or other topics.  You can supply a query in the
style of <span class="pkg">data.table</span> to the <code>where</code> argument.  Note that in <code>data.table</code>
variables are referenced by their names in the <code>data.table</code> object.  The topics
themselves are labeled <code>Topic1</code>, <code>Topic2</code> etc.  If you supply the metadata
to the <code>meta</code> argument, you can also query based on any available metadata. 
See below for examples.
</p>
<p>If you want to pass even more complicated queries, you can use the function <code><a href="#topic+make.dt">make.dt</a></code> 
to generate a <code>data.table</code> object where you can write your own queries.
</p>
<p>The <code>plot.findThoughts</code> function is a shortcut for the <code>plotQuote</code>
function.
</p>


<h3>Value</h3>

<p>A <code>findThoughts</code> object:
</p>
<table>
<tr><td><code>index</code></td>
<td>
<p>List with one entry per
topic.  Each entry is a vector of document indices.</p>
</td></tr> 
<tr><td><code>docs</code></td>
<td>
<p>List with
one entry per topic.  Each entry is a character vector of the corresponding
texts.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+plotQuote">plotQuote</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>findThoughts(gadarianFit, texts=gadarian$open.ended.response, topics=c(1,2), n=3)

#We can plot findThoughts objects using plot() or plotQuote
thought &lt;- findThoughts(gadarianFit, texts=gadarian$open.ended.response, topics=1, n=3)

#plotQuote takes a set of sentences
plotQuote(thought$docs[[1]])

#we can use the generic plot as a shorthand which will make one plot per topic
plot(thought)

#we can select a subset of examples as well using either approach
plot(thought,2:3)
plotQuote(thought$docs[[1]][2:3])


#gather thoughts for only treated documents
thought &lt;- findThoughts(gadarianFit, texts=gadarian$open.ended.response, topics=c(1,2), n=3, 
                       where = treatment==1, meta=gadarian)
plot(thought)
#you can also query in terms of other topics
thought &lt;- findThoughts(gadarianFit, texts=gadarian$open.ended.response, topics=c(1), n=3, 
                        where = treatment==1 &amp; Topic2&gt;.2, meta=gadarian)
plot(thought)         
#these queries can be really complex if you like
thought &lt;- findThoughts(gadarianFit, texts=gadarian$open.ended.response, topics=c(1), n=3, 
                       where = (treatment==1 | pid_rep &gt; .5) &amp; Topic3&gt;.2, meta=gadarian)
plot(thought)         
</code></pre>

<hr>
<h2 id='findTopic'>Find topics that contain user specified words.</h2><span id='topic+findTopic'></span>

<h3>Description</h3>

<p>Find topics that contain user specified words.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findTopic(
  x,
  list,
  n = 20,
  type = c("prob", "frex", "lift", "score"),
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findTopic_+3A_x">x</code></td>
<td>
<p>The STM model object to be searched. May also be the output from
sageLabels.</p>
</td></tr>
<tr><td><code id="findTopic_+3A_list">list</code></td>
<td>
<p>Character vector containing words to be searched.</p>
</td></tr>
<tr><td><code id="findTopic_+3A_n">n</code></td>
<td>
<p>Number of words to consider</p>
</td></tr>
<tr><td><code id="findTopic_+3A_type">type</code></td>
<td>
<p>Type of words to be searched.</p>
</td></tr>
<tr><td><code id="findTopic_+3A_verbose">verbose</code></td>
<td>
<p>A logical indicating whether details should be printed to the
screen.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+findThoughts">findThoughts</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
lab &lt;- sageLabels(gadarianFit, n=5)
findTopic(lab, c("poor", "immigr", "peopl"))
findTopic(gadarianFit, c("poor", "immigr", "peopl"))

</code></pre>

<hr>
<h2 id='fitNewDocuments'>Fit New Documents</h2><span id='topic+fitNewDocuments'></span>

<h3>Description</h3>

<p>A function for predicting thetas for an unseen document based on the previously fit model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fitNewDocuments(
  model = NULL,
  documents = NULL,
  newData = NULL,
  origData = NULL,
  prevalence = NULL,
  betaIndex = NULL,
  prevalencePrior = c("Average", "Covariate", "None"),
  contentPrior = c("Average", "Covariate"),
  returnPosterior = FALSE,
  returnPriors = FALSE,
  designMatrix = NULL,
  test = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitNewDocuments_+3A_model">model</code></td>
<td>
<p>the originally fit STM object.</p>
</td></tr>
<tr><td><code id="fitNewDocuments_+3A_documents">documents</code></td>
<td>
<p>the new documents to be fit. These documents must be in the stm format and
be numbered in the same way as the documents in the original model with the same dimension of vocabulary.
See <code><a href="#topic+alignCorpus">alignCorpus</a></code> or the <span class="pkg">quanteda</span> feature <a href="quanteda.html#topic+dfm_select">dfm_select</a> 
for a way to do this.</p>
</td></tr>
<tr><td><code id="fitNewDocuments_+3A_newdata">newData</code></td>
<td>
<p>the metadata for the prevalence prior which goes with the unseen documents. As in
the original data this cannot have any missing data.</p>
</td></tr>
<tr><td><code id="fitNewDocuments_+3A_origdata">origData</code></td>
<td>
<p>the original metadata used to fit the STM object.</p>
</td></tr>
<tr><td><code id="fitNewDocuments_+3A_prevalence">prevalence</code></td>
<td>
<p>the original formula passed to prevalence when <code>stm</code> was called. The function
will try to reconstruct this.</p>
</td></tr>
<tr><td><code id="fitNewDocuments_+3A_betaindex">betaIndex</code></td>
<td>
<p>a vector which indicates which level of the content covariate is used
for each unseen document. If originally passed as a factor, this can be passed as a factor or 
character vector as well but it must not have any levels not included in the original factor.</p>
</td></tr>
<tr><td><code id="fitNewDocuments_+3A_prevalenceprior">prevalencePrior</code></td>
<td>
<p>three options described in detail below.  Defaults to &quot;Average&quot; when
<code>data</code> is not provided and &quot;Covariate&quot; when it is.</p>
</td></tr>
<tr><td><code id="fitNewDocuments_+3A_contentprior">contentPrior</code></td>
<td>
<p>two options described in detail below. Defaults to &quot;Average&quot; when 
<code>betaIndex</code> is not provided and &quot;Covariate&quot; when it is.</p>
</td></tr>
<tr><td><code id="fitNewDocuments_+3A_returnposterior">returnPosterior</code></td>
<td>
<p>the function always returns the posterior mode of theta
(document-topic proportions),  If set to TRUE this will return the full variational
posterior.  Note that this will return a dense K-by-K matrix for every document
which can be very memory intensive if you are processing a lot of documents.</p>
</td></tr>
<tr><td><code id="fitNewDocuments_+3A_returnpriors">returnPriors</code></td>
<td>
<p>the function always returns the options that were set for the prior
(either by the user or chosen internally by the defaults).  In the case of content covariates
using the covariate prior this will be a set of indices to the original beta matrix so as
not to make the object too large.</p>
</td></tr>
<tr><td><code id="fitNewDocuments_+3A_designmatrix">designMatrix</code></td>
<td>
<p>an option for advanced users to pass an already constructed design matrix for
prevalence covariates.  This will override the options in <code>newData</code>, <code>origData</code> and
<code>test</code>.  See details below- please do not attempt to use without reading carefully.</p>
</td></tr>
<tr><td><code id="fitNewDocuments_+3A_test">test</code></td>
<td>
<p>a test of the functions ability to reconstruct the original functions.</p>
</td></tr>
<tr><td><code id="fitNewDocuments_+3A_verbose">verbose</code></td>
<td>
<p>Should a dot be printed every time 1 percent of the documents are fit.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Due to the existence of the metadata in the model, this isn't as simple as in models
without side information such as Latent Dirichlet Allocation. There are four scenarios:
models without covariate information, models with prevalence covariates only, models with
content covariates only and models with both.  When there is not covariate information the
choice is essentially whether or not to use prior information.
</p>
<p>We offer three types of choices (and may offer more in the future):
</p>

<dl>
<dt>&quot;None&quot;</dt><dd><p>No prior is used.  In the prevalence case this means that the model simply
maximizes the likelihood of seeing the words given the word-topic distribution.  This will
in general produce more sharply peaked topic distributions than the prior. This can be used
even without the covariates.  This is not an option for topical content covariate models.  If
you do not observe the topical content covariate, use the &quot;Average&quot; option.</p>
</dd> 
<dt>&quot;Average&quot;</dt><dd><p>We use a prior that is based on the average over the documents in the training
set.  This does not require the unseen documents to observe the covariates.  In a model that originally
had covariates we need to adjust our estimate of the variance-covariance matrix sigma to accommodate that
we no longer have the covariate information.  So we recalculate the variance based on what it would have
been if we didn't have any covariates.  This helps avoid an edge case where the covariates are extremely
influential and we don't want that strength applied to the new covariate-less setting.  In the case of 
content covariates this essentially use the <code><a href="#topic+sageLabels">sageLabels</a></code> approach to create a
marginalized distribution over words for each topic.
</p>
</dd>
<dt>&quot;Covariate&quot;</dt><dd><p>We use the same covariate driven prior that existed in the original model.
This requires that the test covariates be observed for all previously unseen documents.</p>
</dd>
</dl>

<p>If you fit a document that was used during training with the options to replicate the initial
<code><a href="#topic+stm">stm</a></code> model fit you will not necessarily get exactly the same result.  <code><a href="#topic+stm">stm</a></code>
updates the topic-word distributions last so they may shifted since the document-topic proportions
were updated.  If the original model converged, they should be very close.
</p>
<p>By default the function returns only the MAP estimate of the normalized document-topic proportions
theta.  By selecting <code>returnPrior=TRUE</code> you can get the various model parameters used to complete
the fit.  By selecting <code>returnPosterior=TRUE</code> you can get the full variational posterior.  Please
note that the full variational posterior is very memory intensive.  For a sense of scale it requires an
extra <code class="reqn">K^2 + K \times (V'+1) + 1</code> doubles per document where V' is the number of unique tokens in the document. 
</p>
<p><strong>Testing:</strong> Getting the prevalence covariates right in the unseen documents can be tricky.  However
as long as you leave <code>test</code> set to <code>TRUE</code> the code will automatically run a test to make sure
that everything lines up.  See the internal function <code><a href="#topic+makeDesignMatrix">makeDesignMatrix</a></code> for more on what is 
going on here.
</p>
<p><strong>Passing a Design Matrix</strong>  Advanced users may wish to circumvent this process and pass their
own design matrix possibly because they used their own function for transforming the original input
variables.  This can be done by passing the design matrix using the <code>designMatrix</code> argument
The columns need to match the ordering of the design matrix for the original <code>stm</code> object.  
The design matrix in an stm model called <code>stmobj</code> can be found in <code>stmobj$settings$covariates$X</code> 
which can in turn be used to check that you have formatted your result correctly. If you are going to 
try this we recommend that you read the documentation for <code><a href="#topic+makeDesignMatrix">makeDesignMatrix</a></code> to understand
some of the challenges involved.  
</p>
<p>If you want even more fine-grained control we recommend you directly use the 
optimization function <code><a href="#topic+optimizeDocument">optimizeDocument</a></code>
</p>


<h3>Value</h3>

<p>an object of class fitNewDocuments
</p>
<table>
<tr><td><code>theta</code></td>
<td>
<p>a matrix with one row per document contain the document-topic proportions at the posterior mode</p>
</td></tr>
<tr><td><code>eta</code></td>
<td>
<p>the mean of the variational posterior, only provided when posterior is requested. 
Matrix of same dimension as theta</p>
</td></tr>
<tr><td><code>nu</code></td>
<td>
<p>a list with one element per document containing the covariance matrix of the variational posterior.
Only provided when posterior is requested.</p>
</td></tr>
<tr><td><code>phis</code></td>
<td>
<p>a list with one element per K by V' matrix containing the variational distribution for each token 
(where V' is the number of unique words in the given document.  They are in the order of appearance in the document. 
For words repeated more than once the sum of the column is the number of times that token appeared.  This is only
provided if the posterior is requested.</p>
</td></tr>
<tr><td><code>bound</code></td>
<td>
<p>a vector with one element per document containing the approximate variational lower bound. This is only
provided if the posterior is requested.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>A list where each element contains the unlogged topic-word distribution for each level of the content covariate.
This is only provided if prior is requested.</p>
</td></tr>
<tr><td><code>betaindex</code></td>
<td>
<p>a vector with one element per document indicating which element of the beta list the documents pairs with.
This is only provided if prior is requested.</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>a matrix where each column includes the K-1 dimension prior mean for each document. This is only provided if prior is requested.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>a K-1 by K-1 matrix containing the prior covariance. This is only provided if prior is requested.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+alignCorpus">alignCorpus</a></code> <code><a href="#topic+optimizeDocument">optimizeDocument</a></code> <code><a href="#topic+make.heldout">make.heldout</a></code> <code><a href="#topic+makeDesignMatrix">makeDesignMatrix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#An example using the Gadarian data.  From Raw text to fitted model.
#(for a case where documents are all not processed at once see the help
# file for alignCorpus)
temp&lt;-textProcessor(documents=gadarian$open.ended.response,metadata=gadarian)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta)
set.seed(02138)
#Maximum EM its is set low to make this run fast, run models to convergence!
mod.out &lt;- stm(out$documents, out$vocab, 3, prevalence=~treatment + s(pid_rep), 
              data=out$meta, max.em.its=5)
fitNewDocuments(model=mod.out, documents=out$documents[1:5], newData=out$meta[1:5,],
               origData=out$meta, prevalence=~treatment + s(pid_rep),
               prevalencePrior="Covariate")
</code></pre>

<hr>
<h2 id='gadarian'>Gadarian and Albertson data</h2><span id='topic+gadarian'></span><span id='topic+gadarianFit'></span>

<h3>Description</h3>

<p>This data set
contains variables from Gadarian and Albertson (2014). The experiment had
those in the treatment condition write about what made them anxious about
immigration. The control condition just had subjects write about
immigration.
</p>


<h3>Format</h3>

<p>A data frame with 351 observations on the following 3 variables.
</p>
 
<dl>
<dt><code>MetaID</code></dt><dd><p>A numeric vector containing identification
numbers; not used for analysis</p>
</dd> 
<dt><code>treatment</code></dt><dd><p>A numeric vector
indicating treatment condition</p>
</dd> 
<dt><code>pid_rep</code></dt><dd><p>A numeric vector of
party identification</p>
</dd> 
<dt><code>open.ended.response</code></dt><dd><p>A character vector
of the subject's open ended response</p>
</dd> 
</dl>



<h3>Source</h3>

<p>Gadarian, Shana Kushner, and Bethany Albertson. &quot;Anxiety,
immigration, and the search for information.&quot; Political Psychology 35.2
(2014): 133-164.
</p>
<p>Roberts, Margaret E., Brandon M. Stewart, Dustin Tingley, Christopher Lucas,
Jetson Leder-Luis, Shana Kushner Gadarian, Bethany Albertson, and David G.
Rand.  &quot;Structural Topic Models for Open-Ended Survey Responses.&quot; American
Journal of Political Science 58, no 4 (2014): 1064-1082.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

head(gadarian)
#Process the data for analysis.
temp&lt;-textProcessor(documents=gadarian$open.ended.response,metadata=gadarian)
meta&lt;-temp$meta
vocab&lt;-temp$vocab
docs&lt;-temp$documents
out &lt;- prepDocuments(docs, vocab, meta)
docs&lt;-out$documents
vocab&lt;-out$vocab
meta &lt;-out$meta

</code></pre>

<hr>
<h2 id='js.estimate'>A James-Stein Estimator Shrinking to a Uniform Distribution</h2><span id='topic+js.estimate'></span>

<h3>Description</h3>

<p>A primarily internal function used in <code><a href="#topic+calcfrex">calcfrex</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>js.estimate(prob, ct)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="js.estimate_+3A_prob">prob</code></td>
<td>
<p>the MLE estimate of the discrete probability distribution</p>
</td></tr>
<tr><td><code id="js.estimate_+3A_ct">ct</code></td>
<td>
<p>the count of words observed to estimate that distribution</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This calculates a James-Stein type shrinkage estimator for a discrete probability
distribution regularizing towards a uniform distribution. The amount of shrinkage
is a function of the variance of MLE and the L2 norm distance from the uniform.
</p>
<p>This function is based off the ideas in Hausser and Strimmer (2009)
</p>


<h3>References</h3>

<p>Hausser, Jean, and Korbinian Strimmer. &quot;Entropy inference and the James-Stein estimator, 
with application to nonlinear gene association networks.&quot; Journal of Machine Learning Research 
10.Jul (2009): 1469-1484.
</p>

<hr>
<h2 id='labelTopics'>Label topics</h2><span id='topic+labelTopics'></span><span id='topic+print.labelTopics'></span>

<h3>Description</h3>

<p>Generate a set of words describing each topic from a fitted STM object.
Uses a variety of labeling algorithms (see details).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>labelTopics(model, topics = NULL, n = 7, frexweight = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="labelTopics_+3A_model">model</code></td>
<td>
<p>An <code>STM</code> model object.</p>
</td></tr>
<tr><td><code id="labelTopics_+3A_topics">topics</code></td>
<td>
<p>A vector of numbers indicating the topics to include.  Default
is all topics.</p>
</td></tr>
<tr><td><code id="labelTopics_+3A_n">n</code></td>
<td>
<p>The desired number of words (per type) used to label each topic.
Must be 1 or greater.</p>
</td></tr>
<tr><td><code id="labelTopics_+3A_frexweight">frexweight</code></td>
<td>
<p>A weight used in our approximate FREX scoring algorithm
(see details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Four different types of word weightings are printed with label topics.
</p>
<p>Highest Prob: are the words within each topic with the highest probability
(inferred directly from topic-word distribution parameter <code class="reqn">\beta</code>).
</p>
<p>FREX: are the words that are both frequent and exclusive, identifying words
that distinguish topics.  This is calculated by taking the harmonic mean of
rank by probability within the topic (frequency) and rank by distribution of
topic given word <code class="reqn">p(z|w=v)</code> (exclusivity).  In estimating exclusivity we
use a James-Stein type shrinkage estimator of the distribution
<code class="reqn">p(z|w=v)</code>.  More information can be found in the documentation for the
internal function <code><a href="#topic+calcfrex">calcfrex</a></code> and <code><a href="#topic+js.estimate">js.estimate</a></code>.
</p>
<p>Score and Lift are measures provided in two other popular text mining
packages. For more information on type Score, see the R package
<code><a href="MASS.html#topic+lda">lda</a></code> or the internal function <code><a href="#topic+calcscore">calcscore</a></code>.  
For more information on type Lift, see the R package <code>maptpx</code>
or or the internal function <code><a href="#topic+calclift">calclift</a></code>.
</p>


<h3>Value</h3>

<p>A labelTopics object (list) </p>
<table>
<tr><td><code>prob</code></td>
<td>
<p>matrix of highest
probability words</p>
</td></tr> <tr><td><code>frex</code></td>
<td>
<p>matrix of highest ranking frex words</p>
</td></tr>
<tr><td><code>lift</code></td>
<td>
<p>matrix of highest scoring words by lift</p>
</td></tr> <tr><td><code>score</code></td>
<td>
<p>matrix
of best words by score</p>
</td></tr> <tr><td><code>topicnums</code></td>
<td>
<p>a vector of topic numbers which
correspond to the rows</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+stm">stm</a></code> <code><a href="#topic+plot.STM">plot.STM</a></code> 
<code><a href="#topic+calcfrex">calcfrex</a></code> <code><a href="#topic+js.estimate">js.estimate</a></code> <code><a href="#topic+calcscore">calcscore</a></code> <code><a href="#topic+calclift">calclift</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>labelTopics(gadarianFit)
</code></pre>

<hr>
<h2 id='make.dt'>Make a <code>data.table</code> of topic proportions.</h2><span id='topic+make.dt'></span>

<h3>Description</h3>

<p>Combines the document-topic loadings (theta) with metadata to create a <code>data.table</code> object for easy querying.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.dt(model, meta = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.dt_+3A_model">model</code></td>
<td>
<p>The <code>stm</code> model.</p>
</td></tr>
<tr><td><code id="make.dt_+3A_meta">meta</code></td>
<td>
<p>Optionally, the metadata object passed to the <code>stm</code> model.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a simple utility function that creates a <span class="pkg">data.table</span> object which you can use to create 
more complicated queries than via <code><a href="#topic+findThoughts">findThoughts</a></code>.  Topics are named via the convention 
<code>Topic#</code>, for example <code>Topic1</code>, <code>Topic2</code> etc.  The object also contains <code>docnum</code>
which gives the index of the document so you can set keys without worrying about the texts getting
disconnected.
</p>
<p>We expect that for the vast majority of users the functionality in <code><a href="#topic+findThoughts">findThoughts</a></code> will be
sufficient.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+findThoughts">findThoughts</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dt &lt;- make.dt(gadarianFit, meta=gadarian)
#now we can do any query.  For example the 5 least associated documents with Topic 2 in
#the treated group
dt[treatment==0, docnum[order(Topic2, decreasing=FALSE)][1:5]]

</code></pre>

<hr>
<h2 id='make.heldout'>Heldout Likelihood by Document Completion</h2><span id='topic+make.heldout'></span><span id='topic+eval.heldout'></span>

<h3>Description</h3>

<p>Tools for making and evaluating heldout datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.heldout(
  documents,
  vocab,
  N = floor(0.1 * length(documents)),
  proportion = 0.5,
  seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.heldout_+3A_documents">documents</code></td>
<td>
<p>the documents to be modeled (see <code><a href="#topic+stm">stm</a></code> for format).</p>
</td></tr>
<tr><td><code id="make.heldout_+3A_vocab">vocab</code></td>
<td>
<p>the vocabulary item</p>
</td></tr>
<tr><td><code id="make.heldout_+3A_n">N</code></td>
<td>
<p>number of docs to be partially held out</p>
</td></tr>
<tr><td><code id="make.heldout_+3A_proportion">proportion</code></td>
<td>
<p>proportion of docs to be held out.</p>
</td></tr>
<tr><td><code id="make.heldout_+3A_seed">seed</code></td>
<td>
<p>the seed, set for replicability</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are used to create and evaluate heldout likelihood using the
document completion method.  The basic idea is to hold out some fraction of
the words in a set of documents, train the model and use the document-level
latent variables to evaluate the probability of the heldout portion. See the
example for the basic workflow.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
prep &lt;- prepDocuments(poliblog5k.docs, poliblog5k.voc,
                      poliblog5k.meta,subsample=500,
                      lower.thresh=20,upper.thresh=200)
heldout &lt;- make.heldout(prep$documents, prep$vocab)
documents &lt;- heldout$documents
vocab &lt;- heldout$vocab
meta &lt;- prep$meta

stm1&lt;- stm(documents, vocab, 5,
           prevalence =~ rating+ s(day),
           init.type="Random",
           data=meta, max.em.its=5)
eval.heldout(stm1, heldout$missing)

</code></pre>

<hr>
<h2 id='makeDesignMatrix'>Make a Design Matrix</h2><span id='topic+makeDesignMatrix'></span>

<h3>Description</h3>

<p>Create a sparse model matrix which respects the basis functions
of the original data on which it was created. Primarily for internal
use but may be of some independent interest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeDesignMatrix(formula, origData, newData, test = TRUE, sparse = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makeDesignMatrix_+3A_formula">formula</code></td>
<td>
<p>the formula describing the design matrix.  Any responses will be deleted</p>
</td></tr>
<tr><td><code id="makeDesignMatrix_+3A_origdata">origData</code></td>
<td>
<p>the original dataset as a dataframe</p>
</td></tr>
<tr><td><code id="makeDesignMatrix_+3A_newdata">newData</code></td>
<td>
<p>a dataframe containing any of the variables in the formula.  This will provide
the data in the returned model matrix.</p>
</td></tr>
<tr><td><code id="makeDesignMatrix_+3A_test">test</code></td>
<td>
<p>when set to TRUE runs a test that the matrix was constructed correctly
see details for more.</p>
</td></tr>
<tr><td><code id="makeDesignMatrix_+3A_sparse">sparse</code></td>
<td>
<p>by default returns a sparse matrix using <code><a href="Matrix.html#topic+sparse.model.matrix">sparse.model.matrix</a></code> in 
<span class="pkg">Matrix</span></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This functions is designed to be used in settings where we need
to make a prediction using a model matrix.  The practical challenge
here is ensuring that the representation of the data lines up
with the original representation.  This becomes challenging for
functions that produce a different representation depending on their
inputs. A simple conceptual example is factor variables.  If we run 
our original model using a factor with levels <code>c("A","B", "C")</code>
then when we try to make predictions for data having only levels
<code>c("A","C")</code> we need to adjust for the missing level.  Base
R functions like <code><a href="stats.html#topic+predict.lm">predict.lm</a></code> in <span class="pkg">stats</span> handle 
this gracefully and this function is essentially a version of 
<code><a href="stats.html#topic+predict.lm">predict.lm</a></code> that only constructs the model matrix.
</p>
<p>Beyond factors the key use case for this are basis functions like
splines.  For a function like this to work it must either depend only
on the observation it is transforming (e.g. <code><a href="base.html#topic+log">log</a></code>) or it must
have a generic for <code><a href="stats.html#topic+predict">predict</a></code> and <code><a href="stats.html#topic+makepredictcall">makepredictcall</a></code>.
The spline wrapper <code><a href="#topic+s">s</a></code> has both and so should work.
</p>
<p>When a function lacks these methods it will still produce a design matrix
but the values will be wrong.  To catch these settings we implement a quick
test when <code>test=TRUE</code> as it is by default.  To test we simply split
the original data in half and ensure that looking at each half separately produces
the same values as the complete original data.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fitNewDocuments">fitNewDocuments</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>foo &lt;- data.frame(response=rnorm(30),
                  predictor=as.factor(rep(c("A","B","C"),10)), 
                  predictor2=rnorm(30))
foo.new &lt;- data.frame(predictor=as.factor(c("A","C","C")), 
                      predictor2=foo$predictor2[1:3])
makeDesignMatrix(~predictor + s(predictor2), foo, foo.new)
</code></pre>

<hr>
<h2 id='manyTopics'>Performs model selection across separate STM's that each assume different
numbers of topics.</h2><span id='topic+manyTopics'></span>

<h3>Description</h3>

<p>Works the same as selectModel, except user specifies a range of numbers of
topics that they want the model fitted for. For example, models with 5, 10,
and 15 topics.  Then, for each number of topics, selectModel is run multiple
times. The output is then processed through a function that takes a pareto
dominant run of the model in terms of exclusivity and semantic coherence. If
multiple runs are candidates (i.e., none weakly dominates the others), a
single model run is randomly chosen from the set of undominated runs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>manyTopics(
  documents,
  vocab,
  K,
  prevalence = NULL,
  content = NULL,
  data = NULL,
  max.em.its = 100,
  verbose = TRUE,
  init.type = "LDA",
  emtol = 1e-05,
  seed = NULL,
  runs = 50,
  frexw = 0.7,
  net.max.em.its = 2,
  netverbose = FALSE,
  M = 10,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="manyTopics_+3A_documents">documents</code></td>
<td>
<p>The documents to be modeled.  Object must be a list of with
each element corresponding to a document.  Each document is represented as
an integer matrix with two rows, and columns equal to the number of unique
vocabulary words in the document.  The first row contains the 1-indexed
vocabulary entry and the second row contains the number of times that term
appears.
</p>
<p>This is similar to the format in the <span class="pkg">lda</span> package except that
(following R convention) the vocabulary is indexed from one. Corpora can be
imported using the reader function and manipulated using the
<code><a href="#topic+prepDocuments">prepDocuments</a></code>.</p>
</td></tr>
<tr><td><code id="manyTopics_+3A_vocab">vocab</code></td>
<td>
<p>Character vector specifying the words in the corpus in the
order of the vocab indices in documents. Each term in the vocabulary index
must appear at least once in the documents.  See
<code><a href="#topic+prepDocuments">prepDocuments</a></code> for dropping unused items in the vocabulary.</p>
</td></tr>
<tr><td><code id="manyTopics_+3A_k">K</code></td>
<td>
<p>A vector of positive integers representing the desired number of
topics for separate runs of selectModel.</p>
</td></tr>
<tr><td><code id="manyTopics_+3A_prevalence">prevalence</code></td>
<td>
<p>A formula object with no response variable or a matrix
containing topic prevalence covariates.  Use <code>s()</code>, <code>ns()</code> or
<code>bs()</code> to specify smooth terms. See details for more information.</p>
</td></tr>
<tr><td><code id="manyTopics_+3A_content">content</code></td>
<td>
<p>A formula containing a single variable, a factor variable or
something which can be coerced to a factor indicating the category of the
content variable for each document.</p>
</td></tr>
<tr><td><code id="manyTopics_+3A_data">data</code></td>
<td>
<p>Dataset which contains prevalence and content covariates.</p>
</td></tr>
<tr><td><code id="manyTopics_+3A_max.em.its">max.em.its</code></td>
<td>
<p>The maximum number of EM iterations.  If convergence has
not been met at this point, a message will be printed.</p>
</td></tr>
<tr><td><code id="manyTopics_+3A_verbose">verbose</code></td>
<td>
<p>A logical flag indicating whether information should be
printed to the screen.</p>
</td></tr>
<tr><td><code id="manyTopics_+3A_init.type">init.type</code></td>
<td>
<p>The method of initialization.  See <code><a href="#topic+stm">stm</a></code>.</p>
</td></tr>
<tr><td><code id="manyTopics_+3A_emtol">emtol</code></td>
<td>
<p>Convergence tolerance.</p>
</td></tr>
<tr><td><code id="manyTopics_+3A_seed">seed</code></td>
<td>
<p>Seed for the random number generator. <code>stm</code> saves the seed
it uses on every run so that any result can be exactly reproduced.  When
attempting to reproduce a result with that seed, it should be specified
here.</p>
</td></tr>
<tr><td><code id="manyTopics_+3A_runs">runs</code></td>
<td>
<p>Total number of STM runs used in the cast net stage.
Approximately 15 percent of these runs will be used for running a STM until
convergence.</p>
</td></tr>
<tr><td><code id="manyTopics_+3A_frexw">frexw</code></td>
<td>
<p>Weight used to calculate exclusivity</p>
</td></tr>
<tr><td><code id="manyTopics_+3A_net.max.em.its">net.max.em.its</code></td>
<td>
<p>Maximum EM iterations used when casting the net</p>
</td></tr>
<tr><td><code id="manyTopics_+3A_netverbose">netverbose</code></td>
<td>
<p>Whether verbose should be used when calculating net
models.</p>
</td></tr>
<tr><td><code id="manyTopics_+3A_m">M</code></td>
<td>
<p>Number of words used to calculate semantic coherence and
exclusivity.  Defaults to 10.</p>
</td></tr>
<tr><td><code id="manyTopics_+3A_...">...</code></td>
<td>
<p>Additional options described in details of stm.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Does not work with models that have a content variable (at this point).
</p>


<h3>Value</h3>

<table>
<tr><td><code>out</code></td>
<td>
<p>List of model outputs the user has to choose from.  Take
the same form as the output from a stm model.</p>
</td></tr> 
<tr><td><code>semcoh</code></td>
<td>
<p>Semantic
coherence values for each topic within each model selected for each number
of topics.</p>
</td></tr> 
<tr><td><code>exclusivity</code></td>
<td>
<p>Exclusivity values for each topic within each
model selected.  Only calculated for models without a content covariate.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

temp&lt;-textProcessor(documents=gadarian$open.ended.response,metadata=gadarian)
meta&lt;-temp$meta
vocab&lt;-temp$vocab
docs&lt;-temp$documents
out &lt;- prepDocuments(docs, vocab, meta)
docs&lt;-out$documents
vocab&lt;-out$vocab
meta &lt;-out$meta

set.seed(02138)
storage&lt;-manyTopics(docs,vocab,K=3:4, prevalence=~treatment + s(pid_rep),data=meta, runs=10)
#This chooses the output, a single run of STM that was selected,
#from the runs of the 3 topic model
t&lt;-storage$out[[1]]
#This chooses the output, a single run of STM that was selected,
#from the runs of the 4 topic model
t&lt;-storage$out[[2]]
#Please note that the way to extract a result for manyTopics is different from selectModel.

## End(Not run)
</code></pre>

<hr>
<h2 id='multiSTM'>Analyze Stability of Local STM Mode</h2><span id='topic+multiSTM'></span><span id='topic+print.MultimodDiagnostic'></span>

<h3>Description</h3>

<p>This function performs a suite of tests aimed at assessing the global
behavior of an STM model, which may have multiple modes. The function takes
in a collection of differently initialized STM fitted objects and selects a
reference model against which all others are benchmarked for stability. The
function returns an output of S3 class 'MultimodDiagnostic', with associated
plotting methods for quick inspection of the test results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multiSTM(
  mod.out = NULL,
  ref.model = NULL,
  align.global = FALSE,
  mass.threshold = 1,
  reg.formula = NULL,
  metadata = NULL,
  reg.nsims = 100,
  reg.parameter.index = 2,
  verbose = TRUE,
  from.disk = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multiSTM_+3A_mod.out">mod.out</code></td>
<td>
<p>The output of a <code>selectModel()</code> run. This is a list of
model outputs the user has to choose from, which all take the same form as
the output from a STM model. Currently only works with models without
content covariates.</p>
</td></tr>
<tr><td><code id="multiSTM_+3A_ref.model">ref.model</code></td>
<td>
<p>An integer referencing the element of the list in
<code>mod.out</code> which contains the desired reference model.  When set to the
default value of <code>NULL</code> this chooses the model with the largest value
of the approximate variational bound.</p>
</td></tr>
<tr><td><code id="multiSTM_+3A_align.global">align.global</code></td>
<td>
<p>A boolean parameter specifying how to align the topics
of two different STM fitted models. The alignment is performed by solving
the linear sum assignment problem using the Hungarian algorithm. If
<code>align.global</code> is set to <code>TRUE</code>, the Hungarian algorithm is run
globally on the topic-word matrices of the two models that are being
compared. The rows of the matrices are aligned such as to minimize the sum
of their inner products. This results in each topic in the current runout
being matched to a unique topic in the reference model. If
<code>align.global</code> is, conversely, set to <code>FALSE</code>, the alignment
problem is solved locally. Each topic in the current runout is matched to
the one topic in the reference models that yields minimum inner product.
This means that multiple topics in the current runout can be matched to a
single topic in the reference model, and does not guarantee that all the
topics in the reference model will be matched.</p>
</td></tr>
<tr><td><code id="multiSTM_+3A_mass.threshold">mass.threshold</code></td>
<td>
<p>A parameter specifying the portion of the probability
mass of topics to be used for model analysis. The tail of the probability
mass is disregarded accordingly. If <code>mass.threshold</code> is different from
1, both the full-mass and partial-mass analyses are carried out.</p>
</td></tr>
<tr><td><code id="multiSTM_+3A_reg.formula">reg.formula</code></td>
<td>
<p>A formula for estimating a regression for each model in
the ensemble, where the documents are the units, the outcome is the
proportion of each document about a topic in an STM model, and the
covariates are the document-level metadata. The formula should have an
integer or a vector of numbers on the left-hand side, and an equation with
covariates on the right-hand side. If the left-hand side is left blank, the
regression is performed on all topics in the model. The formula is
exclusively used for building calls to <code>estimateEffect()</code>, so see the
documentation for <code>estimateEffect()</code> for greater detail about the
regression procedure. If <code>reg.formula</code> is null, the covariate effect
stability analysis routines are not performed. The regressions incorporate
uncertainty by using an approximation to the average covariance matrix
formed using the global parameters.</p>
</td></tr>
<tr><td><code id="multiSTM_+3A_metadata">metadata</code></td>
<td>
<p>A dataframe where the predictor variables in
<code>reg.formula</code> can be found. It is necessary to include this argument if
<code>reg.formula</code> is specified.</p>
</td></tr>
<tr><td><code id="multiSTM_+3A_reg.nsims">reg.nsims</code></td>
<td>
<p>The number of simulated draws from the variational
posterior for each call of <code>estimateEffect()</code>. Defaults to 100.</p>
</td></tr>
<tr><td><code id="multiSTM_+3A_reg.parameter.index">reg.parameter.index</code></td>
<td>
<p>If <code>reg.formula</code> is specified, the function
analyzes the stability across runs of the regression coefficient for one
particular predictor variable. This argument specifies which predictor
variable is to be analyzed. A value of 1 corresponds to the intercept, a
value of 2 correspond to the first predictor variable in <code>reg.formula</code>,
and so on. Support for multiple concurrent covariate effect stability
analyses is forthcoming.</p>
</td></tr>
<tr><td><code id="multiSTM_+3A_verbose">verbose</code></td>
<td>
<p>If set to <code>TRUE</code>, the function will report progress.</p>
</td></tr>
<tr><td><code id="multiSTM_+3A_from.disk">from.disk</code></td>
<td>
<p>If set to <code>TRUE</code>, <code>multiSTM()</code> will load the
input models from disk rather than from RAM. This option is particularly
useful for dealing with large numbers of models, and is intended to be used
in conjunction with the <code>to.disk</code> option of <code>selectModel()</code>.
<code>multiSTM()</code> inspects the current directory for RData files.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The purpose of this function is to automate and generalize the stability
analysis routines for topic models that are introduced in Roberts, Margaret
E., Brandon M. Stewart, and Dustin Tingley: &quot;Navigating the Local Modes of
Big Data: The Case of Topic Models&quot; (2014). For more detailed discussion
regarding the background and motivation for multimodality analysis, please
refer to the original article. See also the documentation for
<code><a href="#topic+plot.MultimodDiagnostic">plot.MultimodDiagnostic</a></code> for help with the plotting methods
associated with this function.
</p>


<h3>Value</h3>

<p>An object of 'MultimodDiagnostic' S3 class, consisting of a list
with the following components: </p>
<table>
<tr><td><code>N</code></td>
<td>
<p>The number of fitted models in the
list of model outputs that was supplied to the function for the purpose of
stability analysis.</p>
</td></tr> <tr><td><code>K</code></td>
<td>
<p>The number of topics in the models.</p>
</td></tr>
<tr><td><code>glob.max</code></td>
<td>
<p>The index of the reference model in the list of model
outputs (<code>mod.out</code>) that was supplied to the function. The reference
model is selected as the one with the maximum bound value at convergence.</p>
</td></tr>
<tr><td><code>lb</code></td>
<td>
<p>A list of the maximum bound value at convergence for each of the
fitted models in the list of model outputs. The list has length N.</p>
</td></tr>
<tr><td><code>lmat</code></td>
<td>
<p>A K-by-N matrix reporting the L1-distance of each topic from the
corresponding one in the reference model. This is defined as:
</p>
<p style="text-align: center;"><code class="reqn">L_{1}=\sum_{v}|\beta_{k,v}^{ref}-\beta_{k,v}^{cand}|</code>
</p>
<p> Where the beta
matrices are the topic-word matrices for the reference and the candidate
model.</p>
</td></tr> <tr><td><code>tmat</code></td>
<td>
<p>A K-by-N matrix reporting the number of &quot;top documents&quot;
shared by the reference model and the candidate model. The &quot;top documents&quot;
for a given topic are defined as the 10 documents in the reference corpus
with highest topical frequency.</p>
</td></tr> <tr><td><code>wmat</code></td>
<td>
<p>A K-by-N matrix reporting the
number of &quot;top words&quot; shared by the reference model and the candidate model.
The &quot;top words&quot; for a given topic are defined as the 10 highest-frequency
words.</p>
</td></tr> <tr><td><code>lmod</code></td>
<td>
<p>A vector of length N consisting of the row sums of the
<code>lmat</code> matrix.</p>
</td></tr> <tr><td><code>tmod</code></td>
<td>
<p>A vector of length N consisting of the row
sums of the <code>tmat</code> matrix.</p>
</td></tr> <tr><td><code>wmod</code></td>
<td>
<p>A vector of length N consisting
of the row sums of the <code>wmat</code> matrix.</p>
</td></tr> <tr><td><code>semcoh</code></td>
<td>
<p>Semantic coherence
values for each topic within each model in the list of model outputs.</p>
</td></tr>
<tr><td><code>L1mat</code></td>
<td>
<p>A K-by-N matrix reporting the limited-mass L1-distance of each
topic from the corresponding one in the reference model. Similar to
<code>lmat</code>, but computed using only the top portion of the probability mass
for each topic, as specified by the <code>mass.threshol</code> parameter.
<code>NULL</code> if <code>mass.treshold==1</code>.</p>
</td></tr> <tr><td><code>L1mod</code></td>
<td>
<p>A vector of length N
consisting of the row means of the <code>L1mat</code> matrix.</p>
</td></tr>
<tr><td><code>mass.threshold</code></td>
<td>
<p>The mass threshold argument that was supplied to the
function.</p>
</td></tr> <tr><td><code>cov.effects</code></td>
<td>
<p>A list of length N containing the output of
the run of <code>estimateEffect()</code> on each candidate model with the given
regression formula. <code>NULL</code> if no regression formula is given.</p>
</td></tr>
<tr><td><code>var.matrix</code></td>
<td>
<p>A K-by-N matrix containing the estimated variance for each
of the fitted regression parameters. <code>NULL</code> if no regression formula is
given.</p>
</td></tr> <tr><td><code>confidence.ratings</code></td>
<td>
<p>A vector of length N, where each entry
specifies the proportion of regression coefficient estimates in a candidate
model that fall within the .95 confidence interval for the corresponding
estimate in the reference model.</p>
</td></tr> <tr><td><code>align.global</code></td>
<td>
<p>The alignment control
argument that was supplied to the function.</p>
</td></tr> <tr><td><code>reg.formula</code></td>
<td>
<p>The
regression formula that was supplied to the function.</p>
</td></tr> <tr><td><code>reg.nsims</code></td>
<td>
<p>The
<code>reg.nsims</code> argument that was supplied to the function.</p>
</td></tr>
<tr><td><code>reg.parameter.index</code></td>
<td>
<p>The <code>reg.parameter.index</code> argument that was
supplied to the function.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Antonio Coppola (Harvard University), Brandon Stewart (Princeton
University), Dustin Tingley (Harvard University)
</p>


<h3>References</h3>

<p>Roberts, M., Stewart, B., &amp; Tingley, D. (2016).
&quot;Navigating the Local Modes of Big Data: The Case of Topic Models. In Data
Analytics in Social Science, Government, and Industry.&quot; New York: Cambridge
University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.MultimodDiagnostic">plot.MultimodDiagnostic</a></code> <code><a href="#topic+selectModel">selectModel</a></code>
<code><a href="#topic+estimateEffect">estimateEffect</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

# Example using Gadarian data
temp&lt;-textProcessor(documents=gadarian$open.ended.response, 
                    metadata=gadarian)
meta&lt;-temp$meta
vocab&lt;-temp$vocab
docs&lt;-temp$documents
out &lt;- prepDocuments(docs, vocab, meta)
docs&lt;-out$documents
vocab&lt;-out$vocab
meta &lt;-out$meta
set.seed(02138)
mod.out &lt;- selectModel(docs, vocab, K=3, 
                       prevalence=~treatment + s(pid_rep), 
                       data=meta, runs=20)

out &lt;- multiSTM(mod.out, mass.threshold = .75, 
                reg.formula = ~ treatment,
                metadata = gadarian)
plot(out)

# Same example as above, but loading from disk
mod.out &lt;- selectModel(docs, vocab, K=3, 
                       prevalence=~treatment + s(pid_rep), 
                       data=meta, runs=20, to.disk=T)

out &lt;- multiSTM(from.disk=T, mass.threshold = .75, 
                reg.formula = ~ treatment,
                metadata = gadarian)

## End(Not run)
</code></pre>

<hr>
<h2 id='optimizeDocument'>Optimize Document</h2><span id='topic+optimizeDocument'></span>

<h3>Description</h3>

<p>A primarily internal use function for optimizing the document-level
parameters of the variational distribution.  
Included here for advanced users who want to design new
post-processing features. This help file assumes knowledge of our 
notation which follows the mathematical notation used in our vignette
and other papers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimizeDocument(
  document,
  eta,
  mu,
  beta,
  sigma = NULL,
  sigmainv = NULL,
  sigmaentropy = NULL,
  method = "BFGS",
  control = list(maxit = 500),
  posterior = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimizeDocument_+3A_document">document</code></td>
<td>
<p>a single matrix containing the document in the <code><a href="#topic+stm">stm</a></code> format</p>
</td></tr>
<tr><td><code id="optimizeDocument_+3A_eta">eta</code></td>
<td>
<p>a vector of length K-1 containing the initial starting value for eta</p>
</td></tr>
<tr><td><code id="optimizeDocument_+3A_mu">mu</code></td>
<td>
<p>a vector of length K-1 containing the prevalence prior</p>
</td></tr>
<tr><td><code id="optimizeDocument_+3A_beta">beta</code></td>
<td>
<p>a matrix containing the complete topic-word distribution for the document.
If using a content covariate model it is presumed that you have already passed the correct content
covariate level's beta.</p>
</td></tr>
<tr><td><code id="optimizeDocument_+3A_sigma">sigma</code></td>
<td>
<p>a K-1 by K-1 matrix containing the covariance matrix of the MVN prior. If you supply this
you do not need to supply <code>sigmainv</code> or <code>sigmaentropy</code>.  See below.</p>
</td></tr>
<tr><td><code id="optimizeDocument_+3A_sigmainv">sigmainv</code></td>
<td>
<p>a K-1 by K-1 matrix containing the precision matrix of the MVN prior.  If you supplied
<code>sigma</code> you do not need to supply this. See below.</p>
</td></tr>
<tr><td><code id="optimizeDocument_+3A_sigmaentropy">sigmaentropy</code></td>
<td>
<p>the entropy term calculated from sigma.  If you supplied <code>sigma</code> you do not
need to supply this.  See below.</p>
</td></tr>
<tr><td><code id="optimizeDocument_+3A_method">method</code></td>
<td>
<p>the method passed to <code><a href="stats.html#topic+optim">optim</a></code>.  Uses &quot;BFGS&quot; by default.</p>
</td></tr>
<tr><td><code id="optimizeDocument_+3A_control">control</code></td>
<td>
<p>the control argument passed to <code><a href="stats.html#topic+optim">optim</a></code>.  Sets the maximum number of observations
to 500 but can be used to set other aspects of the optimization per the instructions in <code><a href="stats.html#topic+optim">optim</a></code></p>
</td></tr>
<tr><td><code id="optimizeDocument_+3A_posterior">posterior</code></td>
<td>
<p>should the full posterior be returned?  If TRUE (as it is by default) returns the full 
variational posterior.  Otherwise just returns the point estimate.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a small wrapper around the internal function used
to complete the E-step for each document.
</p>
<p>Regarding the arguments <code>sigma</code>, <code>sigmainv</code> and <code>sigmaentropy</code>.  In
the internal version of the code we calculate <code>sigmainv</code> and <code>sigmaentropy</code>
once each E-step because it is shared by all documents.  If you supply the original
value to <code>sigma</code> it will calculate these for you.  If you are going to be using
this to run a bunch of documents and speed is a concern, peek at the underlying code
and do the calculation yourself once and then just pass the result to the function so
it isn't repeated with every observation.
</p>


<h3>Value</h3>

<p>a list 
</p>
<table>
<tr><td><code>phis</code></td>
<td>
<p>A K by V* matrix containing the variational distribution for each token (where V* is the number of 
unique words in the given document.  They are in the order of appearance in the document. For words repeated
more than once the sum of the column is the number of times that token appeared.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>A (K-1) by 1 matrix containing the mean of the variational distribution for eta.  This is 
actually just called eta in the output of <code><a href="#topic+stm">stm</a></code> as it is also the point estimate.</p>
</td></tr>
<tr><td><code>nu</code></td>
<td>
<p>A (K-1) by (K-1) matrix containing the covariance matrix of the variational distribution for eta.
This is also the inverse Hessian matrix.</p>
</td></tr>
<tr><td><code>bound</code></td>
<td>
<p>The value of the document-level contribution to the global approximate evidence lower bound.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+thetaPosterior">thetaPosterior</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># fitting to a nonsense word distribution
V &lt;- length(poliblog5k.voc)
K &lt;- 50
beta &lt;- matrix(rgamma(V*K,shape = .1), nrow=K, ncol=V)
beta &lt;- beta/rowSums(beta)
doc &lt;- poliblog5k.docs[[1]]
mu &lt;- rep(0, K-1)
sigma &lt;- diag(1000, nrow=K-1)
optimizeDocument(doc, eta=rep(0, K-1), mu=mu, beta=beta, sigma=sigma)
</code></pre>

<hr>
<h2 id='permutationTest'>Permutation test of a binary covariate.</h2><span id='topic+permutationTest'></span>

<h3>Description</h3>

<p>Run a permutation test where a binary treatment variable is randomly
permuted and topic model is reestimated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>permutationTest(
  formula,
  stmobj,
  treatment,
  nruns = 100,
  documents,
  vocab,
  data,
  seed = NULL,
  stmverbose = TRUE,
  uncertainty = "Global"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="permutationTest_+3A_formula">formula</code></td>
<td>
<p>A formula for the prevalence component of the <code>stm</code>
model and the <code>estimateEffect</code> call.  This formula must contain at
least one binary covariate (specified through the argument <code>treatment</code>)
but it can contain other terms as well.  If the binary covariate is
interacted with additional variables the estimated quantity of interest is
the effect when those additional variables are set to 0.</p>
</td></tr>
<tr><td><code id="permutationTest_+3A_stmobj">stmobj</code></td>
<td>
<p>Model output from a single run of <code>stm</code> which contains
the reference effect.</p>
</td></tr>
<tr><td><code id="permutationTest_+3A_treatment">treatment</code></td>
<td>
<p>A character string containing treatment id as used in the
formula of the stmobj.  This is the variable which is randomly permuted.</p>
</td></tr>
<tr><td><code id="permutationTest_+3A_nruns">nruns</code></td>
<td>
<p>Number of total models to fit (including the original model).</p>
</td></tr>
<tr><td><code id="permutationTest_+3A_documents">documents</code></td>
<td>
<p>The documents used in the stmobj model.</p>
</td></tr>
<tr><td><code id="permutationTest_+3A_vocab">vocab</code></td>
<td>
<p>The vocab used in the stmobj model.</p>
</td></tr>
<tr><td><code id="permutationTest_+3A_data">data</code></td>
<td>
<p>The data used in the stmobj model.</p>
</td></tr>
<tr><td><code id="permutationTest_+3A_seed">seed</code></td>
<td>
<p>Optionally a seed with which to replicate the result.  As in
<code><a href="#topic+stm">stm</a></code> the seed is automatically saved and returned as part of
the object.  Passing the seed here will replicate the previous run.</p>
</td></tr>
<tr><td><code id="permutationTest_+3A_stmverbose">stmverbose</code></td>
<td>
<p>Should the stm model be run with <code>verbose=TRUE</code>.
Turning this to <code>FALSE</code> will suppress only the model specific printing.
An update on which model is being run will still print to the screen.</p>
</td></tr>
<tr><td><code id="permutationTest_+3A_uncertainty">uncertainty</code></td>
<td>
<p>Which procedure should be used to approximate the
measurement uncertainty in the topic proportions.  See details for more
information.  Defaults to the Global approximation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes a single binary covariate and runs a permutation test
where, rather than using the true assignment, the covariate is randomly
drawn with probability equal to its empirical probability in the data. After
each shuffle of the covariate the same STM model is estimated at different
starting values using the same initialization procedure as the original
model, and the effect of the covariate across topics is calculated.
</p>
<p>Next the function records two quantities of interest across this set of
&quot;runs&quot; of the model. The first records the absolute maximum effect of the
permuted covariate across all topics.
</p>
<p>The second records the effect of the (permuted) covariate on the topic in
each additional stm run which is estimated to be the topic closest to the
topic of interest (specified in <code><a href="#topic+plot.STMpermute">plot.STMpermute</a></code>) from the
original stm model. Uncertainty can be calculated using the standard options
in <code><a href="#topic+estimateEffect">estimateEffect</a></code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>ref</code></td>
<td>
<p>A list of K elements containing the quantiles of the
estimated effect for the reference model.</p>
</td></tr> <tr><td><code>permute</code></td>
<td>
<p>A list where each
element is an aligned model parameter summary</p>
</td></tr> <tr><td><code>variable</code></td>
<td>
<p>The variable
id that was permuted.</p>
</td></tr> <tr><td><code>seed</code></td>
<td>
<p>The seed for the stm model.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+plot.STMpermute">plot.STMpermute</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
temp&lt;-textProcessor(documents=gadarian$open.ended.response,metadata=gadarian)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta)
documents &lt;- out$documents
vocab &lt;- out$vocab
meta &lt;- out$meta
set.seed(02138)
mod.out &lt;- stm(documents, vocab, 3, prevalence=~treatment + s(pid_rep), data=meta)
summary(mod.out)
prep &lt;- estimateEffect(1:3 ~ treatment + s(pid_rep), mod.out, meta)
plot(prep, "treatment", model=mod.out,
     method="difference",cov.value1=1,cov.value2=0)
test &lt;- permutationTest(formula=~ treatment + s(pid_rep), stmobj=mod.out, 
                        treatment="treatment", nruns=25, documents=documents,
                        vocab=vocab,data=meta, stmverbose=FALSE)
plot(test,2, xlab="Effect", ylab="Model Index", main="Topic 2 Placebo Test")

## End(Not run)
</code></pre>

<hr>
<h2 id='plot.estimateEffect'>Plot effect of covariates on topics</h2><span id='topic+plot.estimateEffect'></span>

<h3>Description</h3>

<p>Plots the effect of a covariate on a set of topics selected by the user.
Different effect types available depending on type of covariate. Before
running this, the user should run a function to simulate the necessary
confidence intervals.  See <code><a href="#topic+estimateEffect">estimateEffect</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'estimateEffect'
plot(
  x,
  covariate,
  model = NULL,
  topics = x$topics,
  method = c("pointestimate", "difference", "continuous"),
  cov.value1 = NULL,
  cov.value2 = NULL,
  moderator = NULL,
  moderator.value = NULL,
  npoints = 100,
  nsims = 100,
  ci.level = 0.95,
  xlim = NULL,
  ylim = NULL,
  xlab = "",
  ylab = NULL,
  main = "",
  printlegend = T,
  labeltype = "numbers",
  n = 7,
  frexw = 0.5,
  add = F,
  linecol = NULL,
  width = 25,
  verbose.labels = T,
  family = NULL,
  custom.labels = NULL,
  omit.plot = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.estimateEffect_+3A_x">x</code></td>
<td>
<p>Output of estimateEffect, which calculates simulated betas for
plotting.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_covariate">covariate</code></td>
<td>
<p>String of the name of the main covariate of interest. Must
be enclosed in quotes.  All other covariates within the formula specified in
estimateEffect will be kept at their median.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_model">model</code></td>
<td>
<p>Model output, only necessary if labeltype is &quot;prob&quot;, &quot;frex&quot;,
&quot;score&quot;, or &quot;lift&quot;.  Models with more than one spline cannot be used for
plot.estimateEffect.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_topics">topics</code></td>
<td>
<p>Topics to plot.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_method">method</code></td>
<td>
<p>Method used for plotting.  &quot;pointestimate&quot; estimates mean
topic proportions for each value of the covariate.  &quot;difference&quot; estimates
the mean difference in topic proportions for two different values of the
covariate (cov.value1 and cov.value2 must be specified).  &quot;continuous&quot;
estimates how topic proportions vary over the support of a continuous
covariate.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_cov.value1">cov.value1</code></td>
<td>
<p>For method &quot;difference&quot;, the value or set of values of
interest at which to set the covariate. In the case of calculating a
treatment/control contrast, set the treatment to cov.value1.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_cov.value2">cov.value2</code></td>
<td>
<p>For method &quot;difference&quot;, the value or set of values which
will be set as the comparison group.  cov.value1 and cov.value2 must be
vectors of the same length.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_moderator">moderator</code></td>
<td>
<p>When two terms are interacted and one variable in the
interaction is the covariate of interest, the user can specify the value of
the interaction with moderator.value, and the name of the moderator with
moderator.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_moderator.value">moderator.value</code></td>
<td>
<p>When two terms are interacted and one variable in the
interaction is the covariate of interest, the user can specify the value of
the interaction term.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_npoints">npoints</code></td>
<td>
<p>Number of unique points to use for simulation along the
support of a continuous covariate.  For method &quot;continuous&quot; only.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_nsims">nsims</code></td>
<td>
<p>Number of simulations for estimation.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_ci.level">ci.level</code></td>
<td>
<p>Confidence level for confidence intervals.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_xlim">xlim</code></td>
<td>
<p>Vector of x axis minimum and maximum values.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_ylim">ylim</code></td>
<td>
<p>Vector of y axis minimum and maximum values.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_xlab">xlab</code></td>
<td>
<p>Character string that is x axis title.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_ylab">ylab</code></td>
<td>
<p>Character string that is y axis title.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_main">main</code></td>
<td>
<p>Character string that is plot title.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_printlegend">printlegend</code></td>
<td>
<p>Whether to plot a topic legend in the case of a
continuous covariate.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_labeltype">labeltype</code></td>
<td>
<p>Determines the labeltype for the topics.  The default is
&quot;number&quot; which prints the topic number.  Other options are &quot;prob&quot;, which
prints the highest probability words, &quot;score&quot;, &quot;lift&quot;, and &quot;frex&quot;, from
labeltopics (see labeltopics() for more details).  The user can also select
&quot;custom&quot; for custom labels, which should be inputted under custom.labels.
Labels appear in the legend for continuous covariates.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_n">n</code></td>
<td>
<p>Number of words to print if &quot;prob&quot;, &quot;score&quot;, &quot;lift&quot;, or &quot;frex&quot; is
chosen.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_frexw">frexw</code></td>
<td>
<p>If &quot;frex&quot; labeltype is used, this will be the frex weight.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_add">add</code></td>
<td>
<p>Logical parameter for whether the line should be added to the
plot, or a new plot should be drawn.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_linecol">linecol</code></td>
<td>
<p>For continuous covariates only.  A vector that specifies the
colors of the lines within the plot.  If NULL, then colors will be randomly
generated.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_width">width</code></td>
<td>
<p>Number that specifies width of the character string.  Smaller
numbers will have smaller-width labels.  Default is 25.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_verbose.labels">verbose.labels</code></td>
<td>
<p>For method &quot;difference&quot; &ndash; verboselabels will specify
the comparison covariate values of the covariate on the plot.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_family">family</code></td>
<td>
<p>Font family.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_custom.labels">custom.labels</code></td>
<td>
<p>A vector of custom labels if labeltype is equal to
&quot;custom&quot;.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_omit.plot">omit.plot</code></td>
<td>
<p>Defaults to FALSE.  When set to TRUE returns everything invisibly but doesn't do any plotting.</p>
</td></tr>
<tr><td><code id="plot.estimateEffect_+3A_...">...</code></td>
<td>
<p>Other plotting parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Values returned invisibly will depend on the method
</p>
<p>For pointestimate: 
</p>
<table>
<tr><td><code>uvals</code></td>
<td>
<p>Values of the covariate at which means and ci's
were evaluated.</p>
</td></tr> 
<tr><td><code>topics</code></td>
<td>
<p>Topics for which means and ci's were
evaluated.</p>
</td></tr> 
<tr><td><code>means</code></td>
<td>
<p>For each topic, means for each unique value.</p>
</td></tr>
<tr><td><code>cis</code></td>
<td>
<p>For each topic, confidence intervals for each unique value.</p>
</td></tr>
<tr><td><code>labels</code></td>
<td>
<p>Labels for each topic and unique value.</p>
</td></tr>
</table>
<p>For difference: 
</p>
<table>
<tr><td><code>topics</code></td>
<td>
<p>Topics for which difference in means and ci's
were evaluated</p>
</td></tr> 
<tr><td><code>means</code></td>
<td>
<p>For each topic, difference in means.</p>
</td></tr>
<tr><td><code>cis</code></td>
<td>
<p>For each topic, confidence intervals for difference in means.</p>
</td></tr>
<tr><td><code>labels</code></td>
<td>
<p>Labels for each topic.</p>
</td></tr>
</table>
<p>For continuous: 
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>Individual values of the covariate at which means
and ci's were evaluated.</p>
</td></tr> 
<tr><td><code>topics</code></td>
<td>
<p>Topics for which means and ci's
were evaluated</p>
</td></tr> 
<tr><td><code>means</code></td>
<td>
<p>For each topic and each x, means.</p>
</td></tr> 
<tr><td><code>cis</code></td>
<td>
<p>For each topic and each x, confidence intervals for difference in means.</p>
</td></tr>
<tr><td><code>labels</code></td>
<td>
<p>Labels for each topic.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>


prep &lt;- estimateEffect(1:3 ~ treatment, gadarianFit, gadarian)
plot(prep, "treatment", model=gadarianFit,
method="pointestimate")
plot(prep, "treatment", model=gadarianFit,
method="difference",cov.value1=1,cov.value2=0)

#If the covariate were a binary factor, 
#the factor labels can be used to  
#specify the values of cov.value1 (e.g., cov.value1="treat"). 

# String variables must be turned to factors prior to plotting. 
#If you see this error, Error in rep.int(c(1, numeric(n)), n - 1L) : 
# invalid 'times' value, then you likely have not done this.

#Example of binary times binary interaction
gadarian$binaryvar &lt;- sample(c(0,1), nrow(gadarian), replace=TRUE)
temp &lt;- textProcessor(gadarian$open.ended.response,metadata=gadarian)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta)
stm1 &lt;- stm(out$documents, out$vocab, 3, prevalence=~treatment*binaryvar,
 data=gadarian)
prep &lt;- estimateEffect(c(2) ~ treatment*binaryvar, stmobj=stm1,
metadata=gadarian)

par(mfrow=c(1,2))
plot(prep, "treatment", method="pointestimate",
cov.value1=1, cov.value2=0, xlim=c(-1,1), moderator="binaryvar", moderator.value=1)
plot(prep, "treatment", method="pointestimate",
cov.value1=1, cov.value2=0, xlim=c(-1,1), moderator="binaryvar",
moderator.value=0)

</code></pre>

<hr>
<h2 id='plot.MultimodDiagnostic'>Plotting Method for Multimodality Diagnostic Objects</h2><span id='topic+plot.MultimodDiagnostic'></span>

<h3>Description</h3>

<p>The plotting method for objects of the S3 class 'MultimodDiagnostic', which
are returned by the function <code>multiSTM()</code>, which performs a battery of
tests aimed at assessing the stability of the local modes of an STM model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'MultimodDiagnostic'
plot(x, ind = NULL, topics = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.MultimodDiagnostic_+3A_x">x</code></td>
<td>
<p>An object of S3 class 'MultimodDiagnostic'. See
<code><a href="#topic+multiSTM">multiSTM</a></code>.</p>
</td></tr>
<tr><td><code id="plot.MultimodDiagnostic_+3A_ind">ind</code></td>
<td>
<p>An integer of list of integers specifying which plots to generate
(see details). If <code>NULL</code> (default), all plots are generated.</p>
</td></tr>
<tr><td><code id="plot.MultimodDiagnostic_+3A_topics">topics</code></td>
<td>
<p>An integer or vector of integers specifying the topics for
which to plot the posterior distribution of covariate effect estimates. If
<code>NULL</code> (default), plots are generated for every topic in the S3 object.</p>
</td></tr>
<tr><td><code id="plot.MultimodDiagnostic_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to the plotting functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This methods generates a series of plots, which are indexed as follows. If a
subset of the plots is required, specify their indexes using the <code>ind</code>
argument. Please note that not all plot types are available for every object
of class 'MultimodDiagnostic': </p>
 <ol>
<li><p> Histogram of Expected
Common Words: Generates a 10-bin histogram of the column means of
<code>obj$wmat</code>, a K-by-N matrix reporting the number of &quot;top words&quot; shared
by the reference model and the candidate model. The &quot;top words&quot; for a given
topic are defined as the 10 highest-frequency words.  </p>
</li>
<li><p> Histogram of
Expected Common Documents: Generates a 10-bin histogram of the column means
of <code>obj$tmat</code>, a K-by-N matrix reporting the number of &quot;top documents&quot;
shared by the reference model and the candidate model. The &quot;top documents&quot;
for a given topic are defined as the 10 documents in the reference corpus
with highest topical frequency.  </p>
</li>
<li><p> Distribution of .95
Confidence-Interval Coverage for Regression Estimates: Generates a histogram
of <code>obj$confidence.ratings</code>, a vector whose entries specify the
proportion of regression coefficient estimates in a candidate model that
fall within the .95 confidence interval for the corresponding estimate in
the reference model. This can only be generated if
<code>obj$confidence.ratings</code> is non-<code>NULL</code>.  </p>
</li>
<li><p> Posterior
Distributions of Covariate Effect Estimates By Topic: Generates a square
matrix of plots, each depicting the posterior distribution of the regression
coefficients for the covariate specified in <code>obj$reg.parameter.index</code>
for one topic. The topics for which the plots are to be generated are
specified by the <code>topics</code> argument. If the length of <code>topics</code> is
not a perfect square, the plots matrix will include white space. The plots
have a dashed black vertical line at zero, and a continuous red vertical
line indicating the coefficient estimate in the reference model. This can
only be generated if <code>obj$cov.effects</code> is non-<code>NULL</code>.  </p>
</li>
<li>
<p>Histogram of Expected L1-Distance From Reference Model: Generates a 10-bin
histogram of the column means of <code>obj$lmat</code>, a K-by-N matrix reporting
the L1-distance of each topic from the corresponding one in the reference
model.  </p>
</li>
<li><p> L1-distance vs. Top-10 Word Metric: Produces a smoothed color
density representation of the scatterplot of <code>obj$lmat</code> and
<code>obj$wmat</code>, the metrics for L1-distance and shared top-words, obtained
through a kernel density estimate. This can be used to validate the metrics
under consideration.  </p>
</li>
<li><p> L1-distance vs. Top-10 Docs Metric: Produces a
smoothed color density representation of the scatterplot of <code>obj$lmat</code>
and <code>obj$tmat</code>, the metrics for L1-distance and shared top-documents,
obtained through a kernel density estimate. This can be used to validate the
metrics under consideration.  </p>
</li>
<li><p> Top-10 Words vs. Top-10 Docs Metric:
Produces a smoothed color density representation of the scatterplot of
<code>obj$wmat</code> and <code>obj$tmat</code>, the metrics for shared top-words and
shared top-documents, obtained through a kernel density estimate. This can
be used to validate the metrics under consideration.  </p>
</li>
<li><p> Maximized Bound
vs. Aggregate Top-10 Words Metric: Generates a scatter plot with linear
trendline for the maximized bound vector (<code>obj$lb</code>) and a linear
transformation of the top-words metric aggregated by model
(<code>obj$wmod/1000</code>).  </p>
</li>
<li><p> Maximized Bound vs. Aggregate Top-10 Docs
Metric: Generates a scatter plot with linear trendline for the maximized
bound vector (<code>obj$lb</code>) and a linear transformation of the top-docs
metric aggregated by model (<code>obj$tmod/1000</code>).  </p>
</li>
<li><p> Maximized Bound
vs. Aggregate L1-Distance Metric: Generates a scatter plot with linear
trendline for the maximized bound vector (<code>obj$lb</code>) and a linear
transformation of the L1-distance metric aggregated by model
(<code>obj$tmod/1000</code>).  </p>
</li>
<li><p> Top-10 Docs Metric vs. Semantic Coherence:
Generates a scatter plot with linear trendline for the reference-model
semantic coherence scores and the column means of <code>object$tmat</code>.  </p>
</li>
<li>
<p>L1-Distance Metric vs. Semantic Coherence: Generates a scatter plot with
linear trendline for the reference-model semantic coherence scores and the
column means of <code>object$lmat</code>.  </p>
</li>
<li><p> Top-10 Words Metric vs. Semantic
Coherence: Generates a scatter plot with linear trendline for the
reference-model semantic coherence scores and the column means of
<code>object$wmat</code>.  </p>
</li>
<li><p> Same as <code>5</code>, but using the limited-mass
L1-distance metric. Can only be generated if <code>obj$mass.threshold != 1</code>.
</p>
</li>
<li><p> Same as <code>11</code>, but using the limited-mass L1-distance metric. Can
only be generated if <code>obj$mass.threshold != 1</code>.  </p>
</li>
<li><p> Same as
<code>7</code>, but using the limited-mass L1-distance metric. Can only be
generated if <code>obj$mass.threshold != 1</code>.  </p>
</li>
<li><p> Same as <code>13</code>, but
using the limited-mass L1-distance metric. Can only be generated if
<code>obj$mass.threshold != 1</code>.  </p>
</li></ol>



<h3>Author(s)</h3>

<p>Brandon M. Stewart (Princeton University) and Antonio Coppola
(Harvard University)
</p>


<h3>References</h3>

<p>Roberts, M., Stewart, B., &amp; Tingley, D. (Forthcoming).
&quot;Navigating the Local Modes of Big Data: The Case of Topic Models. In Data
Analytics in Social Science, Government, and Industry.&quot; New York: Cambridge
University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+multiSTM">multiSTM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

## Not run: 

# Example using Gadarian data

temp&lt;-textProcessor(documents=gadarian$open.ended.response, 
                    metadata=gadarian)
meta&lt;-temp$meta
vocab&lt;-temp$vocab
docs&lt;-temp$documents
out &lt;- prepDocuments(docs, vocab, meta)
docs&lt;-out$documents
vocab&lt;-out$vocab
meta &lt;-out$meta
set.seed(02138)
mod.out &lt;- selectModel(docs, vocab, K=3, 
                       prevalence=~treatment + s(pid_rep), 
                       data=meta, runs=20)

out &lt;- multiSTM(mod.out, mass.threshold = .75, 
                reg.formula = ~ treatment,
                metadata = gadarian)

plot(out)
plot(out, 1)

## End(Not run)
</code></pre>

<hr>
<h2 id='plot.searchK'>Plots diagnostic values resulting from searchK</h2><span id='topic+plot.searchK'></span>

<h3>Description</h3>

<p>Takes the result of searchK and produces a set of plots for evaluating
optimal topic numbers via visual representation of diagnostic functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'searchK'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.searchK_+3A_x">x</code></td>
<td>
<p>A searchK object, containing the diagnostic information of an stm
with a variety of topics.</p>
</td></tr>
<tr><td><code id="plot.searchK_+3A_...">...</code></td>
<td>
<p>additional arguments for S3 compatibility.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>


K&lt;-c(5,10,15) 
temp&lt;-textProcessor(documents=gadarian$open.ended.response,metadata=gadarian)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta)
documents &lt;- out$documents
vocab &lt;- out$vocab
meta &lt;- out$meta
set.seed(02138)
K&lt;-c(5,10,15) 
kresult &lt;- searchK(documents, vocab, K, prevalence=~treatment + s(pid_rep), data=meta)

plot(kresult)

 
</code></pre>

<hr>
<h2 id='plot.STM'>Functions for plotting STM objects</h2><span id='topic+plot.STM'></span>

<h3>Description</h3>

<p>Produces one of four types of plots for an STM object.  The default option
<code>"summary"</code> prints topic words with their corpus frequency.
<code>"labels"</code> is for easy printing of tables of indicative words for each
topic.  <code>"perspectives"</code> depicts differences between two topics,
content covariates or combinations. <code>"hist"</code> creates a histogram of the
expected distribution of topic proportions across the documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'STM'
plot(
  x,
  type = c("summary", "labels", "perspectives", "hist"),
  n = NULL,
  topics = NULL,
  labeltype = c("prob", "frex", "lift", "score"),
  frexw = 0.5,
  main = NULL,
  xlim = NULL,
  ylim = NULL,
  xlab = NULL,
  family = "",
  width = 80,
  covarlevels = NULL,
  plabels = NULL,
  text.cex = 1,
  custom.labels = NULL,
  topic.names = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.STM_+3A_x">x</code></td>
<td>
<p>Model output from stm.</p>
</td></tr>
<tr><td><code id="plot.STM_+3A_type">type</code></td>
<td>
<p>Sets the desired type of plot.  See details for more
information.</p>
</td></tr>
<tr><td><code id="plot.STM_+3A_n">n</code></td>
<td>
<p>Sets the number of words used to label each topic.  In perspective
plots it approximately sets the total number of words in the plot.  The
defaults are 3, 20 and 25 for <code>summary</code>, <code>labels</code> and
<code>perspectives</code> respectively.  n must be greater than or equal to 2</p>
</td></tr>
<tr><td><code id="plot.STM_+3A_topics">topics</code></td>
<td>
<p>Vector of topics to display.  For plot perspectives this must
be a vector of length one or two. For the other two types it defaults to all
topics.</p>
</td></tr>
<tr><td><code id="plot.STM_+3A_labeltype">labeltype</code></td>
<td>
<p>Determines which option of <code>"prob", "frex", "lift",
"score"</code> is used for choosing the most important words.  See
<code><a href="#topic+labelTopics">labelTopics</a></code> for more detail.  Passing an argument to
<code>custom.labels</code> will override this. Note that this does not apply to
<code>perspectives</code> type which always uses highest probability words.</p>
</td></tr>
<tr><td><code id="plot.STM_+3A_frexw">frexw</code></td>
<td>
<p>If &quot;frex&quot; labeltype is used, this will be the frex weight.</p>
</td></tr>
<tr><td><code id="plot.STM_+3A_main">main</code></td>
<td>
<p>Title to the plot</p>
</td></tr>
<tr><td><code id="plot.STM_+3A_xlim">xlim</code></td>
<td>
<p>Range of the X-axis.</p>
</td></tr>
<tr><td><code id="plot.STM_+3A_ylim">ylim</code></td>
<td>
<p>Range of the Y-axis.</p>
</td></tr>
<tr><td><code id="plot.STM_+3A_xlab">xlab</code></td>
<td>
<p>Labels for the X-axis.  For perspective plots, use
<code>plabels</code> instead.</p>
</td></tr>
<tr><td><code id="plot.STM_+3A_family">family</code></td>
<td>
<p>The Font family.  Most of the time the user will not need to
specify this but if using other character sets can be useful see <a href="graphics.html#topic+par">par</a>.</p>
</td></tr>
<tr><td><code id="plot.STM_+3A_width">width</code></td>
<td>
<p>Sets the width in number of characters used for string wrapping
in type <code>"labels"</code></p>
</td></tr>
<tr><td><code id="plot.STM_+3A_covarlevels">covarlevels</code></td>
<td>
<p>A vector of length one or length two which contains the
levels of the content covariate to be used in perspective plots.</p>
</td></tr>
<tr><td><code id="plot.STM_+3A_plabels">plabels</code></td>
<td>
<p>This option can be used to override the default labels in the
perspective plot that appear along the x-axis.  It should be a character
vector of length two which has the left hand side label first.</p>
</td></tr>
<tr><td><code id="plot.STM_+3A_text.cex">text.cex</code></td>
<td>
<p>Controls the scaling constant on text size.</p>
</td></tr>
<tr><td><code id="plot.STM_+3A_custom.labels">custom.labels</code></td>
<td>
<p>A vector of custom labels if labeltype is equal to
&quot;custom&quot;.</p>
</td></tr>
<tr><td><code id="plot.STM_+3A_topic.names">topic.names</code></td>
<td>
<p>A vector of custom topic names.  Defaults to &quot;Topic #: &quot;.</p>
</td></tr>
<tr><td><code id="plot.STM_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to plotting functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function can produce three types of plots which summarize an STM object
which is chosen by the argument <code>type</code>.  <code>summary</code> produces a plot
which displays the topics ordered by their expected frequency across the
corpus.  <code>labels</code> plots the top words selected according to the chosen
criteria for each selected topics.  <code>perspectives</code> plots two topic or
topic-covariate combinations.  Words are sized proportional to their use
within the plotted topic-covariate combinations and oriented along the
X-axis based on how much they favor one of the two configurations.  If the
words cluster on top of each other the user can either set the plot size to
be larger or shrink the total number of words on the plot.  The vertical
configuration of the words is random and thus can be rerun to produce
different results each time. Note that <code>perspectives</code> plots do 
not use any of the labeling options directly. <code>hist</code> plots a histogram of the MAP
estimates of the document-topic loadings across all documents.  The median
is also denoted by a dashed red line.
</p>


<h3>References</h3>

<p>Roberts, Margaret E., Brandon M. Stewart, Dustin Tingley,
Christopher Lucas, Jetson Leder-Luis, Shana Kushner Gadarian, Bethany
Albertson, and David G. Rand.  &quot;Structural Topic Models for Open-Ended
Survey Responses.&quot; American Journal of Political Science 58, no 4 (2014):
1064-1082.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plotQuote">plotQuote</a></code>, <code><a href="#topic+plot.topicCorr">plot.topicCorr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

#Examples with the Gadarian Data
plot(gadarianFit)
plot(gadarianFit,type="labels")
plot(gadarianFit, type="perspectives", topics=c(1,2))
plot(gadarianFit,type="hist")

</code></pre>

<hr>
<h2 id='plot.STMpermute'>Plot an STM permutation test.</h2><span id='topic+plot.STMpermute'></span>

<h3>Description</h3>

<p>Plots the results of a permutation test run using
<code><a href="#topic+permutationTest">permutationTest</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'STMpermute'
plot(x, topic, type = c("match", "largest"), xlim = NULL, ylim = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.STMpermute_+3A_x">x</code></td>
<td>
<p>Object from the output of <code><a href="#topic+permutationTest">permutationTest</a></code>.</p>
</td></tr>
<tr><td><code id="plot.STMpermute_+3A_topic">topic</code></td>
<td>
<p>Integer indicating which topic to plot.</p>
</td></tr>
<tr><td><code id="plot.STMpermute_+3A_type">type</code></td>
<td>
<p>Character string indicating what topic comparison to use.
&quot;match&quot; uses the Hungarian aligned method and &quot;largest&quot; uses the largest
mean in direction of reference topic.</p>
</td></tr>
<tr><td><code id="plot.STMpermute_+3A_xlim">xlim</code></td>
<td>
<p>Range of the X-axis.</p>
</td></tr>
<tr><td><code id="plot.STMpermute_+3A_ylim">ylim</code></td>
<td>
<p>Range of the Y-axis.</p>
</td></tr>
<tr><td><code id="plot.STMpermute_+3A_...">...</code></td>
<td>
<p>Other parameters which may be passed to plot.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function plots the output of <code><a href="#topic+permutationTest">permutationTest</a></code> by stacking
horizontal confidence intervals for the effects of the permuted variable.
In choosing the topic in the permuted runs of stm to plot the effect for,
two methods are available, &quot;match&quot; and &quot;largest&quot;. The former uses Kuhn's
(1955) Hungarian method to align the topics, and then uses the model's best
match of the reference topic.  The latter uses the topic which has the
expected effect size in the direction of the reference model effect; thus,
we would expect this method to be quite conservative.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+permutationTest">permutationTest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

temp&lt;-textProcessor(documents=gadarian$open.ended.response,metadata=gadarian)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta)
documents &lt;- out$documents
vocab &lt;- out$vocab
meta &lt;- out$meta
set.seed(02138)
mod.out &lt;- stm(documents, vocab, 3, prevalence=~treatment + s(pid_rep), data=meta)
summary(mod.out)
prep &lt;- estimateEffect(1:3 ~ treatment + s(pid_rep), mod.out, meta)
plot(prep, "treatment", model=mod.out,
     method="difference",cov.value1=1,cov.value2=0)
test &lt;- permutationTest(formula=~ treatment + s(pid_rep), stmobj=mod.out, 
                        treatment="treatment", nruns=25, documents=documents,
                        vocab=vocab,data=meta, stmverbose=FALSE)
plot(test,2, xlab="Effect", ylab="Model Index", main="Topic 2 Placebo Test")

## End(Not run)
</code></pre>

<hr>
<h2 id='plot.topicCorr'>Plot a topic correlation graph</h2><span id='topic+plot.topicCorr'></span>

<h3>Description</h3>

<p>Uses a topic correlation graph estimated by <code><a href="#topic+topicCorr">topicCorr</a></code> and the
<code>igraph</code> package to plot a network where nodes are topics and edges
indicate a positive correlation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'topicCorr'
plot(
  x,
  topics = NULL,
  vlabels = NULL,
  layout = NULL,
  vertex.color = "green",
  vertex.label.cex = 0.75,
  vertex.label.color = "black",
  vertex.size = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.topicCorr_+3A_x">x</code></td>
<td>
<p>A topicCorr model object.</p>
</td></tr>
<tr><td><code id="plot.topicCorr_+3A_topics">topics</code></td>
<td>
<p>A vector of topics to include in the plot, defaults to all.</p>
</td></tr>
<tr><td><code id="plot.topicCorr_+3A_vlabels">vlabels</code></td>
<td>
<p>A character vector of labels for the vertices.  Defaults to
&quot;Topic #&quot;</p>
</td></tr>
<tr><td><code id="plot.topicCorr_+3A_layout">layout</code></td>
<td>
<p>The layout algorithm passed to the <code>igraph</code> package.  It
will choose <code>layout.fruchterman.reingold</code> by default.  Note that to
pass an alternate algorithm you should load the <code>igraph</code> package first.</p>
</td></tr>
<tr><td><code id="plot.topicCorr_+3A_vertex.color">vertex.color</code></td>
<td>
<p>Color of the vertices.</p>
</td></tr>
<tr><td><code id="plot.topicCorr_+3A_vertex.label.cex">vertex.label.cex</code></td>
<td>
<p>Controls the size of the labels.</p>
</td></tr>
<tr><td><code id="plot.topicCorr_+3A_vertex.label.color">vertex.label.color</code></td>
<td>
<p>Controls the color of the labels.</p>
</td></tr>
<tr><td><code id="plot.topicCorr_+3A_vertex.size">vertex.size</code></td>
<td>
<p>Controls the sizes of the vertices, either NULL, a scalar or a vector of the same length as number of topics.</p>
</td></tr>
<tr><td><code id="plot.topicCorr_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to <code>plot.graph.adjacency</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Essentially a thin wrapper around the plotting functionality in the
<code>igraph</code> package. See package vignette for more details.
</p>


<h3>References</h3>

<p>Csardi G, Nepusz T: The igraph software package for complex
network research, InterJournal, Complex Systems 1695. 2006.
http://igraph.sf.net
</p>


<h3>See Also</h3>

<p><code><a href="#topic+topicCorr">topicCorr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


#This function becomes more useful with larger numbers of topics.
#it is demonstrated here with a small model simply to show how the syntax works.
cormat &lt;- topicCorr(gadarianFit)
plot(cormat)

</code></pre>

<hr>
<h2 id='plotModels'>Plots semantic coherence and exclusivity for high likelihood models
outputted from selectModel.</h2><span id='topic+plotModels'></span>

<h3>Description</h3>

<p>Plots semantic coherence and exclusivity for high likelihood models.  In the
case of models that include content covariates, prints semantic coherence
and sparsity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotModels(
  models,
  xlab = "Semantic Coherence",
  ylab = "Exclusivity",
  labels = 1:length(models$runout),
  pch = NULL,
  legend.position = "topleft",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotModels_+3A_models">models</code></td>
<td>
<p>Output from selectModel.</p>
</td></tr>
<tr><td><code id="plotModels_+3A_xlab">xlab</code></td>
<td>
<p>Character string that is x axis title. This will be semantic
coherence.</p>
</td></tr>
<tr><td><code id="plotModels_+3A_ylab">ylab</code></td>
<td>
<p>Character string that is y axis title. This will be exclusivity.</p>
</td></tr>
<tr><td><code id="plotModels_+3A_labels">labels</code></td>
<td>
<p>Labels for each model.</p>
</td></tr>
<tr><td><code id="plotModels_+3A_pch">pch</code></td>
<td>
<p>A vector of integers specifying symbol for plotting.</p>
</td></tr>
<tr><td><code id="plotModels_+3A_legend.position">legend.position</code></td>
<td>
<p>The location of the legend.  Can be <code>"bottomright", 
"bottom", "bottomleft", "left", "topleft", "top", "topright", "right"</code> and 
<code>"center"</code>.</p>
</td></tr>
<tr><td><code id="plotModels_+3A_...">...</code></td>
<td>
<p>Other plotting parameters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each model has semantic coherence and exclusivity values associated with
each topic.  In the default plot function, the small colored dots are
associated with a topic's semantic coherence and exclusivity.  Dots with the
same color as topics associated with the same model.  The average semantic
coherence and exclusivity is also plotted in the same color, but printed as
the model number associated with the output from selectModels().
</p>
<p>With content covariates, the model does not output exclusivity because
exclusivity has been built in with the content covariates.  Instead, the
user should check to make sure that sparsity is high enough (typically
greater than .5), and then should select a model based on semantic
coherence.
</p>

<hr>
<h2 id='plotQuote'>Plots strings</h2><span id='topic+plotQuote'></span>

<h3>Description</h3>

<p>Plots strings to a blank canvas.  Used primarily for plotting quotes
generated by <code><a href="#topic+findThoughts">findThoughts</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotQuote(
  sentences,
  width = 30,
  text.cex = 1,
  maxwidth = NULL,
  main = NULL,
  xlab = "",
  ylab = "",
  xlim = NULL,
  ylim = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotQuote_+3A_sentences">sentences</code></td>
<td>
<p>Vector of sentence to plot.</p>
</td></tr>
<tr><td><code id="plotQuote_+3A_width">width</code></td>
<td>
<p>Number of characters in each line.</p>
</td></tr>
<tr><td><code id="plotQuote_+3A_text.cex">text.cex</code></td>
<td>
<p>Sets the size of the text</p>
</td></tr>
<tr><td><code id="plotQuote_+3A_maxwidth">maxwidth</code></td>
<td>
<p>Sets the maximum character width of the plotted responses
rounding to the nearest word.  Note that this may perform somewhat
unexpectedly for very small numbers.</p>
</td></tr>
<tr><td><code id="plotQuote_+3A_main">main</code></td>
<td>
<p>Title of plot.</p>
</td></tr>
<tr><td><code id="plotQuote_+3A_xlab">xlab</code></td>
<td>
<p>Sets an x-axis label</p>
</td></tr>
<tr><td><code id="plotQuote_+3A_ylab">ylab</code></td>
<td>
<p>Set a y-axis label</p>
</td></tr>
<tr><td><code id="plotQuote_+3A_xlim">xlim</code></td>
<td>
<p>Sets the x-range of the plot.</p>
</td></tr>
<tr><td><code id="plotQuote_+3A_ylim">ylim</code></td>
<td>
<p>Sets the y-range of the plot</p>
</td></tr>
<tr><td><code id="plotQuote_+3A_...">...</code></td>
<td>
<p>Other parameters passed to the plot function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A simple function which wraps sentences at <code>width</code> characters per line
and plots the results.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+findThoughts">findThoughts</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

thoughts &lt;- findThoughts(gadarianFit,texts=gadarian$open.ended.response,
topics=c(1), n=3)$docs[[1]]
plotQuote(thoughts)

</code></pre>

<hr>
<h2 id='plotRemoved'>Plot documents, words and tokens removed at various word thresholds</h2><span id='topic+plotRemoved'></span>

<h3>Description</h3>

<p>A plot function which shows the results of using different thresholds in
<code>prepDocuments</code> on the size of the corpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotRemoved(documents, lower.thresh)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotRemoved_+3A_documents">documents</code></td>
<td>
<p>The documents to be used for the stm model</p>
</td></tr>
<tr><td><code id="plotRemoved_+3A_lower.thresh">lower.thresh</code></td>
<td>
<p>A vector of integers, each of which will be tested as a
lower threshold for the prepDocuments function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a lower threshold, <code>prepDocuments</code> will drop words which appear in
fewer than that number of documents, and remove documents which contain no
more words. This function allows the user to pass a vector of lower
thresholds and observe how <code>prepDocuments</code> will handle each threshold.
This function produces three plots, showing the number of words, the number
of documents, and the total number of tokens removed as a function of
threshold values. A dashed red line is plotted at the total number of
documents, words and tokens respectively.
</p>


<h3>Value</h3>

<p>Invisibly returns a list of </p>
<table>
<tr><td><code>lower.thresh</code></td>
<td>
<p>The sorted threshold
values</p>
</td></tr> <tr><td><code>ndocs</code></td>
<td>
<p>The number of documents dropped for each value of the
lower threshold</p>
</td></tr> <tr><td><code>nwords</code></td>
<td>
<p>The number of entries of the vocab dropped
for each value of the lower threshold.</p>
</td></tr> <tr><td><code>ntokens</code></td>
<td>
<p>The number of tokens
dropped for each value of the lower threshold.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+prepDocuments">prepDocuments</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
plotRemoved(poliblog5k.docs, lower.thresh=seq(from = 10, to = 1000, by = 10))
</code></pre>

<hr>
<h2 id='plotTopicLoess'>Plot some effects with loess</h2><span id='topic+plotTopicLoess'></span>

<h3>Description</h3>

<p>Plots a loess line of the topic proportions on a covariate inputted by the
user. This allows for a more flexible functional form for the relationship.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotTopicLoess(
  model,
  topics,
  covariate,
  span = 1.5,
  level = 0.95,
  main = "",
  xlab = "Covariate",
  ylab = "Topic Proportions"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotTopicLoess_+3A_model">model</code></td>
<td>
<p>An STM model object</p>
</td></tr>
<tr><td><code id="plotTopicLoess_+3A_topics">topics</code></td>
<td>
<p>Vector of topic numbers to plot by the covariate. E.g.,
c(1,2,3) would plot lines for topics 1,2,3.</p>
</td></tr>
<tr><td><code id="plotTopicLoess_+3A_covariate">covariate</code></td>
<td>
<p>Covariate vector by which to plot topic proportions.</p>
</td></tr>
<tr><td><code id="plotTopicLoess_+3A_span">span</code></td>
<td>
<p>loess span parameter.  See <code><a href="stats.html#topic+loess">loess</a></code></p>
</td></tr>
<tr><td><code id="plotTopicLoess_+3A_level">level</code></td>
<td>
<p>Desired coverage for confidence intervals</p>
</td></tr>
<tr><td><code id="plotTopicLoess_+3A_main">main</code></td>
<td>
<p>Title of the plot, default is &quot;&quot;</p>
</td></tr>
<tr><td><code id="plotTopicLoess_+3A_xlab">xlab</code></td>
<td>
<p>X-label, default is &quot;Covariate&quot;</p>
</td></tr>
<tr><td><code id="plotTopicLoess_+3A_ylab">ylab</code></td>
<td>
<p>Y-label, default is &quot;Topic Proportions&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is considerably less developed than
<code><a href="#topic+plot.estimateEffect">plot.estimateEffect</a></code> and we recommend using that function with
splines and high degrees of freedom where possible.  Computes standard
errors through the method of composition as in <code><a href="#topic+estimateEffect">estimateEffect</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.estimateEffect">plot.estimateEffect</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>plotTopicLoess(gadarianFit, topics=1, covariate=gadarian$pid_rep)
</code></pre>

<hr>
<h2 id='poliblog5k'>CMU 2008 Political Blog Corpus</h2><span id='topic+poliblog5k'></span><span id='topic+poliblog5k.docs'></span><span id='topic+poliblog5k.voc'></span><span id='topic+poliblog5k.meta'></span>

<h3>Description</h3>

<p>A 5000 document sample from CMU 2008 Political Blog Corpus (Eisenstein and
Xing 2010).  Blog posts from 6 blogs during the U.S. 2008 Presidential
Election.
</p>


<h3>Format</h3>

<p>A data frame with 5000 observations on the following 4 variables.
</p>
 
<dl>
<dt><code>rating</code></dt><dd><p>a factor variable giving the partisan
affiliation of the blog (based on who they supported for president)</p>
</dd>
<dt><code>day</code></dt><dd><p>the day of the year (1 to 365).  All entries are from
2008.</p>
</dd> 
<dt><code>blog</code></dt><dd><p>a two digit character code corresponding to the
name of the blog. They are: American Thinker (at), Digby (db), Hot Air (ha),
Michelle Malkin (mm), Think Progress (tp), Talking Points Memo (tpm)</p>
</dd>
<dt><code>text</code></dt><dd><p>the first 50 characters (rounded to the nearest full
word).</p>
</dd> 
</dl>



<h3>Details</h3>

<p>This is a random sample of the larger CMU 2008 Political Blog Corpus
collected by Jacob Eisenstein and Eric Xing.  Quoting from their
documentation: &quot;[The blogs] were selected by the following criteria: the
Technorati rankings of blog authority, ideological balance, coverage for the
full year 2008, and ease of access to blog archives. In the general election
for U.S. President in 2008, the following blogs supported Barack Obama:
Digby, ThinkProgress, and Talking Points Memo. John McCain was supported by
American Thinker, Hot Air, and Michelle Malkin. In general, the blogs that
supported Obama in the election tend to advocate for similar policies and
candidates as the Democratic party; and the blogs that supported McCain tend
to advocate Republican policies and candidates. Digby, Hot Air and Michelle
Malkin are single-author blogs; the others have multiple authors.&quot;
</p>


<h3>Source</h3>

<p>Jacob Eisenstein and Eric Xing (2010) &quot;The CMU 2008 Political Blog
Corpus.&quot; Technical Report Carnegie Mellon University.
http://sailing.cs.cmu.edu/socialmedia/blog2008.html
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


data(poliblog5k)
head(poliblog5k.meta)
head(poliblog5k.voc)

stm1 &lt;- stm(poliblog5k.docs, poliblog5k.voc, 3,
prevalence=~rating, data=poliblog5k.meta)


</code></pre>

<hr>
<h2 id='prepDocuments'>Prepare documents for analysis with <code>stm</code></h2><span id='topic+prepDocuments'></span>

<h3>Description</h3>

<p>Performs several corpus manipulations including removing words and
renumbering word indices (to correct for zero-indexing and/or unused words
in the vocab vector).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepDocuments(
  documents,
  vocab,
  meta = NULL,
  lower.thresh = 1,
  upper.thresh = Inf,
  subsample = NULL,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepDocuments_+3A_documents">documents</code></td>
<td>
<p>List of documents. For more on the format see
<code><a href="#topic+stm">stm</a></code>.</p>
</td></tr>
<tr><td><code id="prepDocuments_+3A_vocab">vocab</code></td>
<td>
<p>Character vector of words in the vocabulary.</p>
</td></tr>
<tr><td><code id="prepDocuments_+3A_meta">meta</code></td>
<td>
<p>Document metadata.</p>
</td></tr>
<tr><td><code id="prepDocuments_+3A_lower.thresh">lower.thresh</code></td>
<td>
<p>Words which do not appear in a number of documents
greater than lower.thresh will be dropped and both the documents and vocab
files will be renumbered accordingly.  If this causes all words within a
document to be dropped, a message will print to the screen at it will also
return vector of the documents removed so you can update your meta data as
well. See details below.</p>
</td></tr>
<tr><td><code id="prepDocuments_+3A_upper.thresh">upper.thresh</code></td>
<td>
<p>As with lower.thresh but this provides an upper bound.
Words which appear in at least this number of documents will be dropped.
Defaults to <code>Inf</code> which does no filtering.</p>
</td></tr>
<tr><td><code id="prepDocuments_+3A_subsample">subsample</code></td>
<td>
<p>If an integer will randomly subsample (without replacement)
the given number of documents from the total corpus before any processing.
Defaults to <code>NULL</code> which provides no subsampling.  Note that the output
may have fewer than the number of requested documents if additional
processing causes some of those documents to be dropped.</p>
</td></tr>
<tr><td><code id="prepDocuments_+3A_verbose">verbose</code></td>
<td>
<p>A logical indicating whether or not to print details to the
screen.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The default setting <code>lower.thresh=1</code> means that words which appear in
only one document will be dropped.  This is often advantageous as there is
little information about these words but the added cost of including them in
the model can be quite large.  In many cases it will be helpful to set this
threshold considerably higher.  If the vocabulary is in excess of 5000
entries inference can slow quite a bit.
</p>
<p>If words are removed, the function returns a vector of the original indices
for the dropped items.  If it removed documents it returns a vector of doc
indices removed. Users with accompanying metadata or texts may want to drop
those rows from the corresponding objects.
</p>
<p>The behavior is such that when <code>prepDocuments</code> drops documents their
corresponding rows are deleted and the row names are not renumbered.  We however
do not recommend using rownames for joins- instead the best practice is to either
keep a unique identifier in the <code>meta</code> object for doing joins or use something
like <span class="pkg">quanteda</span> which has a more robust interface for manipulating the corpus
itself.
</p>
<p>If you have any documents which are of length 0 in your original object the
function will throw an error. These should be removed before running the
function although please be sure to remove the corresponding rows in the
meta data file if you have one.  You can quickly identify the documents
using the code: <code>which(unlist(lapply(documents, length))==0)</code>.
</p>


<h3>Value</h3>

<p>A list containing a new documents and vocab object.
</p>
<table>
<tr><td><code>documents</code></td>
<td>
<p>The new documents object for use with <code>stm</code></p>
</td></tr>
<tr><td><code>vocab</code></td>
<td>
<p>The new vocab object for use with <code>stm</code></p>
</td></tr> <tr><td><code>meta</code></td>
<td>
<p>The
new meta data object for use with <code>stm</code>. Will be the same if no
documents are removed.</p>
</td></tr> <tr><td><code>words.removed</code></td>
<td>
<p>A set of indices corresponding
to the positions in the original vocab object of words which have been
removed.</p>
</td></tr> <tr><td><code>docs.removed</code></td>
<td>
<p>A set of indices corresponding to the
positions in the original documents object of documents which no longer
contained any words after dropping terms from the vocab.</p>
</td></tr>
<tr><td><code>tokens.removed</code></td>
<td>
<p>An integer corresponding to the number of unique
tokens removed from the corpus.</p>
</td></tr> <tr><td><code>wordcounts</code></td>
<td>
<p>A table giving the the
number of documents that each word is found in of the original document set,
prior to any removal. This can be passed through a histogram for visual
inspection.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+plotRemoved">plotRemoved</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>temp&lt;-textProcessor(documents=gadarian$open.ended.response,metadata=gadarian)
meta&lt;-temp$meta
vocab&lt;-temp$vocab
docs&lt;-temp$documents
out &lt;- prepDocuments(docs, vocab, meta)
</code></pre>

<hr>
<h2 id='readCorpus'>Read in a corpus file.</h2><span id='topic+readCorpus'></span>

<h3>Description</h3>

<p>Converts pre-processed document matrices stored in popular formats to stm
format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readCorpus(corpus, type = c("dtm", "slam", "Matrix"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readCorpus_+3A_corpus">corpus</code></td>
<td>
<p>An input file or filepath to be processed</p>
</td></tr>
<tr><td><code id="readCorpus_+3A_type">type</code></td>
<td>
<p>The type of input file.  We offer several sources, see details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function provides a simple utility for converting other document
formats to our own.  Briefly- <code>dtm</code> takes as input a standard matrix
and converts to our format.   <code>slam</code> converts from the
<code>simple_triplet_matrix</code> representation used by the <code>slam</code> package.
This is also the representation of corpora in the popular <code>tm</code> package
and should work in those cases.
</p>
<p><code>dtm</code> expects a matrix object where each row represents a document and
each column represents a word in the dictionary.
</p>
<p><code>slam</code> expects a <code><a href="slam.html#topic+simple_triplet_matrix">simple_triplet_matrix</a></code> from that
package.
</p>
<p><code>Matrix</code> attempts to coerce the matrix to a
<code><a href="slam.html#topic+simple_triplet_matrix">simple_triplet_matrix</a></code> and convert using the
functionality built for the <code>slam</code> package.  This will work for most
applicable classes in the <code>Matrix</code> package such as <code>dgCMatrix</code>.
</p>
<p>If you are trying to read a <code>.ldac</code> file see <code><a href="#topic+readLdac">readLdac</a></code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>documents</code></td>
<td>
<p>A documents object in our format</p>
</td></tr> <tr><td><code>vocab</code></td>
<td>
<p>A
vocab object if information is available to construct one</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+textProcessor">textProcessor</a></code>, <code><a href="#topic+prepDocuments">prepDocuments</a></code> <code><a href="#topic+readLdac">readLdac</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

library(textir)
data(congress109)
out &lt;- readCorpus(congress109Counts, type="Matrix")
documents &lt;- out$documents
vocab &lt;- out$vocab

## End(Not run)
</code></pre>

<hr>
<h2 id='readLdac'>Read in a .ldac Formatted File</h2><span id='topic+readLdac'></span>

<h3>Description</h3>

<p>Read in a term document matrix in the .ldac sparse matrix format popularized
by David Blei's C code implementation of lda.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readLdac(filename)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readLdac_+3A_filename">filename</code></td>
<td>
<p>An input file or filepath to be processed</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ldac</code> expects a file name or path that contains a file in Blei's LDA-C
format. From his ReadMe: &quot;The data is a file where each line is of the form:
</p>
<p>[M] [term_1]:[count] [term_2]:[count] ...  [term_N]:[count]
</p>
<p>where [M] is the number of unique terms in the document, and the [count]
associated with each term is how many times that term appeared in the
document.  Note that [term_1] is an integer which indexes the term; it is
not a string.&quot;
</p>
<p>Because R indexes from one, the values of the term indices are incremented
by one on import.
</p>


<h3>Value</h3>

<table>
<tr><td><code>documents</code></td>
<td>
<p>A documents object in our format</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+textProcessor">textProcessor</a></code>, <code><a href="#topic+prepDocuments">prepDocuments</a></code> <code><a href="#topic+readCorpus">readCorpus</a></code>
</p>

<hr>
<h2 id='rmvnorm'>Draw from a Multivariate Normal</h2><span id='topic+rmvnorm'></span>

<h3>Description</h3>

<p>A basic function for doing multivariate normal simulations
via the cholesky decomposition of the covariance matrix. Function
is based on one by Peter Hoff.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmvnorm(n, mu, Sigma, chol.Sigma = chol(Sigma))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmvnorm_+3A_n">n</code></td>
<td>
<p>number of draws</p>
</td></tr>
<tr><td><code id="rmvnorm_+3A_mu">mu</code></td>
<td>
<p>the K-dimensional mean</p>
</td></tr>
<tr><td><code id="rmvnorm_+3A_sigma">Sigma</code></td>
<td>
<p>the K by K dimensional positive definite covariance matrix</p>
</td></tr>
<tr><td><code id="rmvnorm_+3A_chol.sigma">chol.Sigma</code></td>
<td>
<p>the cholesky decomposition of the Sigma matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a pretty standard multivariate normal generator. It could
almost certainly be faster if we ported it over to <span class="pkg">RcppArmadillo</span>
but it isn't used a ton at the moment.
</p>

<hr>
<h2 id='s'>Make a B-spline Basis Function</h2><span id='topic+s'></span>

<h3>Description</h3>

<p>This is a simple wrapper around the <code><a href="splines.html#topic+bs">bs</a></code> function in
the splines package.  It will default to a spline with 10 degrees of
freedom.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>s(x, df, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="s_+3A_x">x</code></td>
<td>
<p>The predictor value.</p>
</td></tr>
<tr><td><code id="s_+3A_df">df</code></td>
<td>
<p>Degrees of freedom.  Defaults to the minimum of 10 or one minus
the number of unique values in x.</p>
</td></tr>
<tr><td><code id="s_+3A_...">...</code></td>
<td>
<p>Arguments passed to the <code><a href="splines.html#topic+bs">bs</a></code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a simple wrapper written as users may find it easier to simply type
<code>s</code> rather than selecting parameters for a spline. We also include
<code>predict</code> and <code>makepredictcall</code> generic functions for the class
so it will work in settings where <code><a href="stats.html#topic+predict">predict</a></code> is called.
</p>


<h3>Value</h3>

<p>A predictor matrix of the basis functions.
</p>


<h3>See Also</h3>

<p><code><a href="splines.html#topic+bs">bs</a></code> <code><a href="splines.html#topic+ns">ns</a></code>
</p>

<hr>
<h2 id='sageLabels'>Displays verbose labels that describe topics and topic-covariate groups in
depth.</h2><span id='topic+sageLabels'></span><span id='topic+print.sageLabels'></span>

<h3>Description</h3>

<p>For each topic or, when there is a covariate at the bottom of the model, for
each topic-covariate group, sageLabels provides a list of the highest
marginal probability words, the highest marginal FREX words, the highest
marginal lift words, and the highest marginal score words, where marginal
means it is summing over all potential covariates.  It also provides each
topic's Kappa (words associated with each topic) and baselined Kappa
(baseline word distribution).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sageLabels(model, n = 7)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sageLabels_+3A_model">model</code></td>
<td>
<p>A fitted STM model object.</p>
</td></tr>
<tr><td><code id="sageLabels_+3A_n">n</code></td>
<td>
<p>The number of words to print per topic/topic-covariate set. Default
is 7.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This can be used as an more detailed alternative to labelTopics.
</p>


<h3>Value</h3>

<table>
<tr><td><code>marginal</code></td>
<td>
<p> A list of matrices, containing the high-probability
labels, FREX labels, lift labels, and high scoring words.  </p>
</td></tr> <tr><td><code>K</code></td>
<td>
<p> The
number of topics in the STM.  </p>
</td></tr> <tr><td><code>covnames</code></td>
<td>
<p> Names of the covariate
values used in the STM.  </p>
</td></tr> <tr><td><code>kappa</code></td>
<td>
<p>Words associated with topics,
covariates, and topic/covariate interactions.</p>
</td></tr> <tr><td><code>kappa.m</code></td>
<td>
<p>Baseline word
distribution.</p>
</td></tr> <tr><td><code>n</code></td>
<td>
<p> The n parameter passed by the user to this
function; number of words per topic or topic-covariate pair (when covariates
are used on the bottom of the model) </p>
</td></tr> <tr><td><code>cov.betas</code></td>
<td>
<p> Covariate-specific
beta matrices, listing for each covariate a matrix of highest-probability,
FREX, lift, and high scoring words.  Note that the actual vocabulary has
been substituted for word indices.  </p>
</td></tr>
</table>

<hr>
<h2 id='searchK'>Computes diagnostic values for models with different values of K (number of
topics).</h2><span id='topic+searchK'></span>

<h3>Description</h3>

<p>With user-specified initialization, this function runs selectModel for
different user-specified topic numbers and computes diagnostic properties
for the returned model. These include exclusivity, semantic coherence,
heldout likelihood, bound, lbound, and residual dispersion.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>searchK(
  documents,
  vocab,
  K,
  init.type = "Spectral",
  N = floor(0.1 * length(documents)),
  proportion = 0.5,
  heldout.seed = NULL,
  M = 10,
  cores = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="searchK_+3A_documents">documents</code></td>
<td>
<p>The documents to be used for the stm model</p>
</td></tr>
<tr><td><code id="searchK_+3A_vocab">vocab</code></td>
<td>
<p>The vocabulary to be used for the stmmodel</p>
</td></tr>
<tr><td><code id="searchK_+3A_k">K</code></td>
<td>
<p>A vector of different topic numbers</p>
</td></tr>
<tr><td><code id="searchK_+3A_init.type">init.type</code></td>
<td>
<p>The method of initialization. See <code><a href="#topic+stm">stm</a></code> for
options.  Note that the default option here is different from the main
function.</p>
</td></tr>
<tr><td><code id="searchK_+3A_n">N</code></td>
<td>
<p>Number of docs to be partially held out</p>
</td></tr>
<tr><td><code id="searchK_+3A_proportion">proportion</code></td>
<td>
<p>Proportion of docs to be held out.</p>
</td></tr>
<tr><td><code id="searchK_+3A_heldout.seed">heldout.seed</code></td>
<td>
<p>If desired, a seed to use when holding out documents for
later heldout likelihood computation</p>
</td></tr>
<tr><td><code id="searchK_+3A_m">M</code></td>
<td>
<p>M value for exclusivity computation</p>
</td></tr>
<tr><td><code id="searchK_+3A_cores">cores</code></td>
<td>
<p>Number of CPUs to use for parallel computation</p>
</td></tr>
<tr><td><code id="searchK_+3A_...">...</code></td>
<td>
<p>Other diagnostics parameters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the vignette for interpretation of each of these measures.  Each of
these measures is also available in exported functions:
</p>

<dl>
<dt>exclusivity</dt><dd><p><code><a href="#topic+exclusivity">exclusivity</a></code></p>
</dd>
<dt>semantic coherence</dt><dd><p><code><a href="#topic+semanticCoherence">semanticCoherence</a></code></p>
</dd>
<dt>heldout likelihood</dt><dd><p><code><a href="#topic+make.heldout">make.heldout</a></code> and <code><a href="#topic+eval.heldout">eval.heldout</a></code></p>
</dd>
<dt>bound</dt><dd><p>calculated by <code><a href="#topic+stm">stm</a></code> accessible by <code>max(model$convergence$bound)</code></p>
</dd>
<dt>lbound</dt><dd><p>a correction to the bound that makes the bounds directly comparable <code>max(model$convergence$bound) + lfactorial(model$settings$dim$K)</code></p>
</dd>
<dt>residual dispersion</dt><dd><p><code><a href="#topic+checkResiduals">checkResiduals</a></code></p>
</dd>
</dl>

<p>Due to the need to calculate the heldout-likelihood <code>N</code> documents have
<code>proportion</code> of the documents heldout at random.  This means that even
with the default spectral initialization the results can change from run to run.
When the number of heldout documents is low or documents are very short, this also
means that the results can be quite unstable.  For example: the <code>gadarian</code> code
demonstration below has heldout results based on only 34 documents and approximately
150 tokens total.  Clearly this can lead to quite disparate results across runs.  By 
contrast default settings for the <code>poliblog5k</code> dataset would yield a heldout sample
of 500 documents with approximately 50000 tokens for the heldout sample.  We should expect
this to be substantially more stable.
</p>


<h3>Value</h3>

<table>
<tr><td><code>exclus</code></td>
<td>
<p>Exclusivity of each model.</p>
</td></tr> <tr><td><code>semcoh</code></td>
<td>
<p>Semantic
coherence of each model.</p>
</td></tr> <tr><td><code>heldout</code></td>
<td>
<p>Heldout likelihood for each model.</p>
</td></tr>
<tr><td><code>residual</code></td>
<td>
<p>Residual for each model.</p>
</td></tr> <tr><td><code>bound</code></td>
<td>
<p>Bound for each
model.</p>
</td></tr> <tr><td><code>lbound</code></td>
<td>
<p>lbound for each model.</p>
</td></tr> <tr><td><code>em.its</code></td>
<td>
<p>Total number of
EM iterations used in fiting the model.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+plot.searchK">plot.searchK</a></code> <code><a href="#topic+make.heldout">make.heldout</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


K&lt;-c(5,10,15) 
temp&lt;-textProcessor(documents=gadarian$open.ended.response,metadata=gadarian)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta)
documents &lt;- out$documents
vocab &lt;- out$vocab
meta &lt;- out$meta
set.seed(02138)
K&lt;-c(5,10,15) 
kresult &lt;- searchK(documents, vocab, K, prevalence=~treatment + s(pid_rep), data=meta)
plot(kresult)


 
</code></pre>

<hr>
<h2 id='selectModel'>Assists the user in selecting the best STM model.</h2><span id='topic+selectModel'></span>

<h3>Description</h3>

<p>Discards models with the low likelihood values based on a small number of EM
iterations (cast net stage), then calculates semantic coherence,
exclusivity, and sparsity (based on default STM run using selected
convergence criteria) to allow the user to choose between models with high
likelihood values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>selectModel(
  documents,
  vocab,
  K,
  prevalence = NULL,
  content = NULL,
  data = NULL,
  max.em.its = 100,
  verbose = TRUE,
  init.type = "LDA",
  emtol = 1e-05,
  seed = NULL,
  runs = 50,
  frexw = 0.7,
  net.max.em.its = 2,
  netverbose = FALSE,
  M = 10,
  N = NULL,
  to.disk = F,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="selectModel_+3A_documents">documents</code></td>
<td>
<p>The documents to be modeled.  Object must be a list of with
each element corresponding to a document.  Each document is represented as
an integer matrix with two rows, and columns equal to the number of unique
vocabulary words in the document.  The first row contains the 1-indexed
vocabulary entry and the second row contains the number of times that term
appears.
</p>
<p>This is similar to the format in the <span class="pkg">lda</span> package except that
(following R convention) the vocabulary is indexed from one. Corpora can be
imported using the reader function and manipulated using the
<code><a href="#topic+prepDocuments">prepDocuments</a></code>.</p>
</td></tr>
<tr><td><code id="selectModel_+3A_vocab">vocab</code></td>
<td>
<p>Character vector specifying the words in the corpus in the
order of the vocab indices in documents. Each term in the vocabulary index
must appear at least once in the documents.  See
<code><a href="#topic+prepDocuments">prepDocuments</a></code> for dropping unused items in the vocabulary.</p>
</td></tr>
<tr><td><code id="selectModel_+3A_k">K</code></td>
<td>
<p>A positive integer (of size 2 or greater) representing the desired
number of topics. Additional detail on choosing the number of topics in
details.</p>
</td></tr>
<tr><td><code id="selectModel_+3A_prevalence">prevalence</code></td>
<td>
<p>A formula object with no response variable or a matrix
containing topic prevalence covariates.  Use <code>s()</code>, <code>ns()</code> or
<code>bs()</code> to specify smooth terms. See details for more information.</p>
</td></tr>
<tr><td><code id="selectModel_+3A_content">content</code></td>
<td>
<p>A formula containing a single variable, a factor variable or
something which can be coerced to a factor indicating the category of the
content variable for each document.</p>
</td></tr>
<tr><td><code id="selectModel_+3A_data">data</code></td>
<td>
<p>Dataset which contains prevalence and content covariates.</p>
</td></tr>
<tr><td><code id="selectModel_+3A_max.em.its">max.em.its</code></td>
<td>
<p>The maximum number of EM iterations.  If convergence has
not been met at this point, a message will be printed.</p>
</td></tr>
<tr><td><code id="selectModel_+3A_verbose">verbose</code></td>
<td>
<p>A logical flag indicating whether information should be
printed to the screen.</p>
</td></tr>
<tr><td><code id="selectModel_+3A_init.type">init.type</code></td>
<td>
<p>The method of initialization.  Must be either Latent
Dirichlet Allocation (LDA), Dirichlet Multinomial Regression Topic Model
(DMR), a random initialization or a previous STM object.</p>
</td></tr>
<tr><td><code id="selectModel_+3A_emtol">emtol</code></td>
<td>
<p>Convergence tolerance.  EM stops when the relative change in
the approximate bound drops below this level.  Defaults to .001%.</p>
</td></tr>
<tr><td><code id="selectModel_+3A_seed">seed</code></td>
<td>
<p>Seed for the random number generator. <code>stm</code> saves the seed
it uses on every run so that any result can be exactly reproduced.  Setting
the seed here simply ensures that the sequence of models will be exactly the
same when respecified.  Individual seeds can be retrieved from the component
model objects.</p>
</td></tr>
<tr><td><code id="selectModel_+3A_runs">runs</code></td>
<td>
<p>Total number of STM runs used in the cast net stage.
Approximately 15 percent of these runs will be used for running a STM until
convergence.</p>
</td></tr>
<tr><td><code id="selectModel_+3A_frexw">frexw</code></td>
<td>
<p>Weight used to calculate exclusivity</p>
</td></tr>
<tr><td><code id="selectModel_+3A_net.max.em.its">net.max.em.its</code></td>
<td>
<p>Maximum EM iterations used when casting the net</p>
</td></tr>
<tr><td><code id="selectModel_+3A_netverbose">netverbose</code></td>
<td>
<p>Whether verbose should be used when calculating net
models.</p>
</td></tr>
<tr><td><code id="selectModel_+3A_m">M</code></td>
<td>
<p>Number of words used to calculate semantic coherence and
exclusivity.  Defaults to 10.</p>
</td></tr>
<tr><td><code id="selectModel_+3A_n">N</code></td>
<td>
<p>Total number of models to retain in the end. Defaults to .2 of
runs.</p>
</td></tr>
<tr><td><code id="selectModel_+3A_to.disk">to.disk</code></td>
<td>
<p>Boolean. If TRUE, each model is saved to disk at the current
directory in a separate RData file.  This is most useful if one needs to run
<code>multiSTM()</code> on a large number of output models.</p>
</td></tr>
<tr><td><code id="selectModel_+3A_...">...</code></td>
<td>
<p>Additional options described in details of stm.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>runout</code></td>
<td>
<p>List of model outputs the user has to choose from.
Take the same form as the output from a stm model.</p>
</td></tr> <tr><td><code>semcoh</code></td>
<td>
<p>Semantic
coherence values for each topic within each model in runout</p>
</td></tr>
<tr><td><code>exclusivity</code></td>
<td>
<p>Exclusivity values for each topic within each model in
runout.  Only calculated for models without a content covariate</p>
</td></tr>
<tr><td><code>sparsity</code></td>
<td>
<p>Percent sparsity for the covariate and interaction kappas
for models with a content covariate.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

temp&lt;-textProcessor(documents=gadarian$open.ended.response, metadata=gadarian)
meta&lt;-temp$meta
vocab&lt;-temp$vocab
docs&lt;-temp$documents
out &lt;- prepDocuments(docs, vocab, meta)
docs&lt;-out$documents
vocab&lt;-out$vocab
meta &lt;-out$meta
set.seed(02138)
mod.out &lt;- selectModel(docs, vocab, K=3, prevalence=~treatment + s(pid_rep), 
                       data=meta, runs=5)
plotModels(mod.out)
selected&lt;-mod.out$runout[[1]]

## End(Not run)
</code></pre>

<hr>
<h2 id='semanticCoherence'>Semantic Coherence</h2><span id='topic+semanticCoherence'></span>

<h3>Description</h3>

<p>Calculate semantic coherence (Mimno et al 2011) for an STM model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>semanticCoherence(model, documents, M = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="semanticCoherence_+3A_model">model</code></td>
<td>
<p>the STM object</p>
</td></tr>
<tr><td><code id="semanticCoherence_+3A_documents">documents</code></td>
<td>
<p>the STM formatted documents (see <code><a href="#topic+stm">stm</a></code> for format).</p>
</td></tr>
<tr><td><code id="semanticCoherence_+3A_m">M</code></td>
<td>
<p>the number of top words to consider per topic</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Semantic coherence is a metric related to pointwise mutual information that was introduced
in a paper by David Mimno, Hanna Wallach and colleagues (see references),  The paper details a series
of manual evaluations which show that their metric is a reasonable surrogate for human judgment.
The core idea here is that in models which are semantically coherent the words which are most
probable under a topic should co-occur within the same document.
</p>
<p>One of our observations in Roberts et al 2014 was that semantic coherence alone is relatively easy to
achieve by having only a couple of topics which all are dominated by the most common words.  Thus we
suggest that users should also consider <code><a href="#topic+exclusivity">exclusivity</a></code> which provides a natural counterpoint.
</p>
<p>This function is currently marked with the keyword internal because it does not have much error checking.
</p>


<h3>Value</h3>

<p>a numeric vector containing semantic coherence for each topic
</p>


<h3>References</h3>

<p>Mimno, D., Wallach, H. M., Talley, E., Leenders, M., &amp; McCallum, A. (2011, July). 
&quot;Optimizing semantic coherence in topic models.&quot; In Proceedings of the Conference on Empirical Methods in 
Natural Language Processing (pp. 262-272). Association for Computational Linguistics. Chicago
</p>
<p>Roberts, M., Stewart, B., Tingley, D., Lucas, C., Leder-Luis, J., Gadarian, S., Albertson, B., et al. (2014). 
&quot;Structural topic models for open ended survey responses.&quot; American Journal of Political Science, 58(4), 1064-1082.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+searchK">searchK</a></code> <code><a href="#topic+plot.searchK">plot.searchK</a></code> <code><a href="#topic+exclusivity">exclusivity</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>temp&lt;-textProcessor(documents=gadarian$open.ended.response,metadata=gadarian)
meta&lt;-temp$meta
vocab&lt;-temp$vocab
docs&lt;-temp$documents
out &lt;- prepDocuments(docs, vocab, meta)
docs&lt;-out$documents
vocab&lt;-out$vocab
meta &lt;-out$meta
set.seed(02138)
#maximum EM iterations set very low so example will run quickly.
#Run your models to convergence!
mod.out &lt;- stm(docs, vocab, 3, prevalence=~treatment + s(pid_rep), data=meta,
               max.em.its=5)
semanticCoherence(mod.out, docs)
</code></pre>

<hr>
<h2 id='stm'>Variational EM for the Structural Topic Model</h2><span id='topic+stm'></span>

<h3>Description</h3>

<p>Estimation of the Structural Topic Model using semi-collapsed variational
EM.  The function takes sparse representation of a document-term matrix, an integer
number of topics, and covariates and returns fitted model parameters.
Covariates can be used in the prior for topic <code>prevalence</code>, in the
prior for topical <code>content</code> or both.  See an overview of functions in
the package here: <code><a href="#topic+stm-package">stm-package</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stm(
  documents,
  vocab,
  K,
  prevalence = NULL,
  content = NULL,
  data = NULL,
  init.type = c("Spectral", "LDA", "Random", "Custom"),
  seed = NULL,
  max.em.its = 500,
  emtol = 1e-05,
  verbose = TRUE,
  reportevery = 5,
  LDAbeta = TRUE,
  interactions = TRUE,
  ngroups = 1,
  model = NULL,
  gamma.prior = c("Pooled", "L1"),
  sigma.prior = 0,
  kappa.prior = c("L1", "Jeffreys"),
  control = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stm_+3A_documents">documents</code></td>
<td>
<p>The document term matrix to be modeled. These can be supplied
in the native <span class="pkg">stm</span> format, a sparse term count matrix with one row
per document and one column per term, or a
<span class="pkg">quanteda</span> <a href="quanteda.html#topic+dfm">dfm</a> (document-feature matrix) object.
When using the sparse matrix or quanteda format this will include the
vocabulary and, for quanteda, optionally the metadata. If using the native list format,
the object must be a list of with each element corresponding to a document. Each document is represented
as an integer matrix with two rows, and columns equal to the number of unique
vocabulary words in the document.  The first row contains the 1-indexed
vocabulary entry and the second row contains the number of times that term
appears. This is similar to the format in the <code><a href="lda.html#topic+lda">lda</a></code> package 
except that (following R convention) the vocabulary is indexed from one. Corpora
can be imported using the reader function and manipulated using the
<code><a href="#topic+prepDocuments">prepDocuments</a></code>.  Raw texts can be ingested using
<code><a href="#topic+textProcessor">textProcessor</a></code>. Note that when using <span class="pkg">quanteda</span> <a href="quanteda.html#topic+dfm">dfm</a>
directly there may be higher memory use (because the texts and metadata are stored
twice). You can convert from <span class="pkg">quanteda</span>'s format directly to our native format
using the <span class="pkg">quanteda</span> function <a href="quanteda.html#topic+convert">convert</a>.</p>
</td></tr>
<tr><td><code id="stm_+3A_vocab">vocab</code></td>
<td>
<p>Character vector specifying the words in the corpus in the
order of the vocab indices in documents. Each term in the vocabulary index
must appear at least once in the documents.  See <code><a href="#topic+prepDocuments">prepDocuments</a></code>
for dropping unused items in the vocabulary.  If <code>documents</code> is a
sparse matrix or <span class="pkg">quanteda</span> <a href="quanteda.html#topic+dfm">dfm</a> object, then <code>vocab</code> should not
(and must not) be supplied.  It is contained already inside the column
names of the matrix.</p>
</td></tr>
<tr><td><code id="stm_+3A_k">K</code></td>
<td>
<p>Typically a positive integer (of size 2 or greater) representing
the desired number of topics. If <code>init.type="Spectral"</code> you can also
set <code>K=0</code> to use the algorithm of Lee and Mimno (2014) to set the
number of topics (although unlike the standard spectral initialization this
is not deterministic).  Additional detail on choosing the number of topics
below.</p>
</td></tr>
<tr><td><code id="stm_+3A_prevalence">prevalence</code></td>
<td>
<p>A formula object with no response variable or a matrix
containing topic prevalence covariates.  Use <code><a href="#topic+s">s</a></code>,
<code><a href="splines.html#topic+ns">ns</a></code> or <code><a href="splines.html#topic+bs">bs</a></code> to specify smooth
terms. See details for more information.</p>
</td></tr>
<tr><td><code id="stm_+3A_content">content</code></td>
<td>
<p>A formula containing a single variable, a factor variable or
something which can be coerced to a factor indicating the category of the
content variable for each document.</p>
</td></tr>
<tr><td><code id="stm_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the prevalence and/or content
covariates.  If unspecified the variables are taken from the active
environment.</p>
</td></tr>
<tr><td><code id="stm_+3A_init.type">init.type</code></td>
<td>
<p>The method of initialization, by default the spectral initialization.
Must be either Latent
Dirichlet Allocation (&quot;LDA&quot;), &quot;Random&quot;, &quot;Spectral&quot; or &quot;Custom&quot;.  See details for more
info. If you want to replicate a previous result, see the argument
<code>seed</code>.  For &quot;Custom&quot; see the format described below under the <code>custom.beta</code>
option of the <code>control</code> parameters.</p>
</td></tr>
<tr><td><code id="stm_+3A_seed">seed</code></td>
<td>
<p>Seed for the random number generator. <code>stm</code> saves the seed
it uses on every run so that any result can be exactly reproduced.  When
attempting to reproduce a result with that seed, it should be specified
here.</p>
</td></tr>
<tr><td><code id="stm_+3A_max.em.its">max.em.its</code></td>
<td>
<p>The maximum number of EM iterations.  If convergence has
not been met at this point, a message will be printed.  If you set this to 
0 it will return the initialization.</p>
</td></tr>
<tr><td><code id="stm_+3A_emtol">emtol</code></td>
<td>
<p>Convergence tolerance.  EM stops when the relative change in
the approximate bound drops below this level.  Defaults to .00001.  You 
can set it to 0 to have the algorithm run <code>max.em.its</code> number of steps.
See advanced options under <code>control</code> for more options.</p>
</td></tr>
<tr><td><code id="stm_+3A_verbose">verbose</code></td>
<td>
<p>A logical flag indicating whether information should be
printed to the screen.  During the E-step (iteration over documents) a dot
will print each time 1% of the documents are completed.  At the end of each
iteration the approximate bound will also be printed.</p>
</td></tr>
<tr><td><code id="stm_+3A_reportevery">reportevery</code></td>
<td>
<p>An integer determining the intervals at which labels are
printed to the screen during fitting.  Defaults to every 5 iterations.</p>
</td></tr>
<tr><td><code id="stm_+3A_ldabeta">LDAbeta</code></td>
<td>
<p>a logical that defaults to <code>TRUE</code> when there are no
content covariates.  When set to <code>FALSE</code> the model performs SAGE style
topic updates (sparse deviations from a baseline).</p>
</td></tr>
<tr><td><code id="stm_+3A_interactions">interactions</code></td>
<td>
<p>a logical that defaults to <code>TRUE</code>.  This
automatically includes interactions between content covariates and the
latent topics.  Setting it to <code>FALSE</code> reduces to a model with no
interactive effects.</p>
</td></tr>
<tr><td><code id="stm_+3A_ngroups">ngroups</code></td>
<td>
<p>Number of groups for memoized inference.  See details below.</p>
</td></tr>
<tr><td><code id="stm_+3A_model">model</code></td>
<td>
<p>A prefit model object.  By passing an <code>stm</code> object to this
argument you can restart an existing model.  See details for more info.</p>
</td></tr>
<tr><td><code id="stm_+3A_gamma.prior">gamma.prior</code></td>
<td>
<p>sets the prior estimation method for the prevalence
covariate model.  The default <code>Pooled</code> options uses Normal prior
distributions with a topic-level pooled variance which is given a moderately
regularizing half-cauchy(1,1) prior.  The alternative <code>L1</code> uses
<code>glmnet</code> to estimate a grouped penalty between L1-L2.  If your code is running
slowly immediately after &quot;Completed E-Step&quot; appears, you may want to switch to the 
<code>L1</code> option. See details below.</p>
</td></tr>
<tr><td><code id="stm_+3A_sigma.prior">sigma.prior</code></td>
<td>
<p>a scalar between 0 and 1 which defaults to 0.  This sets
the strength of regularization towards a diagonalized covariance matrix.
Setting the value above 0 can be useful if topics are becoming too highly
correlated.</p>
</td></tr>
<tr><td><code id="stm_+3A_kappa.prior">kappa.prior</code></td>
<td>
<p>sets the prior estimation for the content covariate
coefficients.  The default option is the <code>L1</code> prior.  The second option
is <code>Jeffreys</code> which is markedly less computationally efficient but is
included for backwards compatibility. See details for more information on
computation.</p>
</td></tr>
<tr><td><code id="stm_+3A_control">control</code></td>
<td>
<p>a list of additional advanced parameters. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is the main function for estimating a Structural Topic Model (STM).
STM is an admixture with covariates in both mixture components.  Users
provide a corpus of documents and a number of topics.  Each word in a
document comes from exactly one topic and each document is represented by
the proportion of its words that come from each of the K topics.  These
proportions are found in the N (number of documents) by K (user specified
number of topics) theta matrix.  Each of the K topics are represented as
distributions over words.  The K-by-V (number of words in the vocabulary)
matrix logbeta contains the natural log of the probability of seeing each
word conditional on the topic.
</p>
<p>The most important user input in parametric topic models is the number of
topics.  There is no right answer to the appropriate number of topics.  More
topics will give more fine-grained representations of the data at the
potential cost of being less precisely estimated.  The number must be at
least 2 which is equivalent to a unidimensional scaling model.  For short
corpora focused on very specific subject matter (such as survey experiments)
3-10 topics is a useful starting range.  For small corpora (a few hundred to
a few thousand) 5-50 topics is a good place to start.  Beyond these rough
guidelines it is application specific.  Previous applications in political
science with medium sized corpora (10k to 100k documents) have found 60-100
topics to work well.  For larger corpora 100 topics is a useful default
size.  Of course, your mileage may vary.
</p>
<p>When <code>init.type="Spectral"</code> and <code>K=0</code> the number of topics is set
using the algorithm in Lee and Mimno (2014).  See vignette for details.  We
emphasize here as we do there that this does not estimate the &quot;true&quot; number
of topics and does not necessarily have any particular statistical
properties for consistently estimating the number of topics.  It can however
provide a useful starting point.
</p>
<p>The model for topical prevalence includes covariates which the analyst
believes may influence the frequency with which a topic is discussed.  This
is specified as a formula which can contain smooth terms using splines or by
using the function <code><a href="#topic+s">s</a></code>.  The response portion of the formula
should be left blank.  See the examples.  These variables can include
numeric and factor variables.  While including variables of class
<code>Dates</code> or other non-numeric, non-factor types will work in <code>stm</code>
it may not always work for downstream functions such as
<code><a href="#topic+estimateEffect">estimateEffect</a></code>.
</p>
<p>The topical convent covariates are those which affect the way in which a
topic is discussed. As currently implemented this must be a single variable
which defines a discrete partition of the dataset (each document is in one
and only one group).  We may relax this in the future.  While including more
covariates in topical prevalence will rarely affect the speed of the model,
including additional levels of the content covariates can make the model
much slower to converge.  This is due to the model operating in the much
higher dimensional space of words in dictionary (which tend to be in the
thousands) as opposed to topics.
</p>
<p>In addition to the default priors for prevalence, we also make use of the
<code>glmnet</code> package to allow for penalties between the L1 and L2 norm.  In
these settings we estimate a regularization path and then select the optimal
shrinkage parameter using a user-tuneable information criterion.  By default
selecting the <code>L1</code> option will apply the L1 penalty selecting the
optimal shrinkage parameter using AIC. The defaults have been specifically
tuned for the STM but almost all the relevant arguments can be changed
through the control structure below.  Changing the <code>gamma.enet</code>
parameters allow the user to choose a mix between the L1 and L2 norms.  When
set to 1 (as by default) this is the lasso penalty, when set to 0 its the
ridge penalty.  Any value in between is a mixture called the elastic net.
</p>
<p>The default prior choice for content covariates is now the <code>L1</code> option.
This uses an approximation framework developed in Taddy (2013) called
Distributed Multinomial Regression which utilizes a factorized poisson
approximation to the multinomial.  See Roberts, Stewart and Airoldi (2014)
for details on the implementation here.  This is dramatically faster than
previous versions.  The old default setting which uses a Jeffreys prior is
also available.
</p>
<p>The argument <code>init.type</code> allows the user to specify an initialization
method. The default 
choice, <code>"Spectral"</code>, provides a deterministic initialization using the
spectral algorithm given in Arora et al 2014.  See Roberts, Stewart and
Tingley (2016) for details and a comparison of different approaches.
Particularly when the number of documents is relatively large we highly
recommend the Spectral algorithm which often performs extremely well.  Note
that the random seed plays no role in the spectral initialization as it is
completely deterministic (unless using the <code>K=0</code> or random projection
settings). When the vocab is larger than 10000 terms we use only the most
frequent 10000 terms in creating the initialization.  This may case the 
first step of the algorithm to have a very bad value of the objective function
but it should quickly stabilize into a good place.  You can tweak the exact 
number where this kicks in with the <code>maxV</code> argument inside control. There
appear to be some cases where numerical instability in the Spectral algorithm
can cause differences across machines (particularly Windows machines for some reason).
It should always give exactly the same answer for a given machine but if you are
seeing different answers on different machines, see https://github.com/bstewart/stm/issues/133
for a longer explanation.  The other option <code>"LDA"</code> which uses a few passes
of a Gibbs sampler is perfectly reproducible across machines as long as the seed is set.
</p>
<p>Specifying an integer greater than 1 for the argument <code>ngroups</code> causes
the corpus to be broken into the specified number of groups.  Global updates
are then computed after each group in turn.  This approach, called memoized
variational inference in Hughes and Sudderth (2013), can lead to more rapid
convergence when the number of documents is large.  Note that the memory
requirements scale linearly with the number of groups so this provides a
tradeoff between memory efficiency and speed.  The claim of speed here
is based on the idea that increasing the number of global updates should
help the model find a solution in fewer passes through the document set.
However, it is worth noting that for any particular case the model need 
not converge faster and definitely won't converge to the same location. 
This functionality should be considered somewhat experimental and we encourage
users to let us know what their experiences are like here in practice.
</p>
<p>Models can now be restarted by passing an <code>STM</code> object to the argument
<code>model</code>.  This is particularly useful if you run a model to the maximum
iterations and it terminates without converging.  Note that all the standard
arguments still need to be passed to the object (including any formulas, the
number of topics, etc.).  Be sure to change the <code>max.em.its</code> argument
or it will simply complete one additional iteration and stop.
</p>
<p>You can pass a custom initialization of the beta model parameters to <code>stm</code>.
</p>
<p>The <code>control</code> argument is a list with named components which can be
used to specify numerous additional computational details.  Valid components
include: 
</p>
 
<dl>
<dt><code>tau.maxit</code></dt><dd><p>Controls the maximum number of
iterations when estimating the prior for content covariates.  When the mode
is <code>Jeffreys</code>, estimation proceeds by iterating between the kappa
vector corresponding to a particular topic and the associated variance tau
before moving on to the next parameter vector. this controls the maximum
number of iterations. It defaults to <code>NULL</code> effectively enforcing
convergence.  When the mode is <code>L1</code> this sets the maximum number of
passes in the coordinate descent algorithm and defaults to 1e8.</p>
</dd>
<dt><code>tau.tol</code></dt><dd><p>Sets the convergence tolerance in the optimization
for content covariates.  When the mode is <code>Jeffreys</code> this sets the
convergence tolerance in the iteration between the kappa vector and
variances tau and defaults to 1e-5.  With <code>L1</code> it defaults to 1e-6.</p>
</dd>
<dt><code>kappa.mstepmaxit</code></dt><dd><p>When the mode for content covariate
estimation is <code>Jeffreys</code> this controls the maximum number of passes
through the sequence of kappa vectors.  It defaults to 3.  It has no role
under <code>L1</code>- see <code>tau.maxit</code> option instead.</p>
</dd>
<dt><code>kappa.msteptol</code></dt><dd><p>When the mode for content covariate estimation
is <code>Jeffreys</code> this controls the tolerance for convergence (measured by
the L1 norm) for the entire M-step.  It is set to .01 by default.  This has
no role under mode <code>L1</code>- see <code>tau.tol</code> option instead.</p>
</dd>
<dt><code>fixedintercept</code></dt><dd><p>a logical indicating whether in content
covariate models the intercept should be fixed to the background
distribution.  TRUE by default. This only applies when kappa.prior is set to
L1.  If FALSE the intercept is estimated from the data without penalty.  In
practice estimated intercepts often push term probabilities to zero,
resulting in topics that look more like those in a Dirichlet model- that is,
most terms have approximately zero probability with some terms with high
probability.</p>
</dd> 
<dt><code>kappa.enet</code></dt><dd><p>When using the L1 mode for content
covariates this controls the elastic net mixing parameter.  See the argument
<code>alpha</code> in <code>glmnet</code>.  Value must be between 1 and 0 where 1 is the
lasso penalty (the default) and 0 is the ridge penalty.  The closer the
parameter is to zero the less sparse the solution will tend to be.</p>
</dd>
<dt><code>gamma.enet</code></dt><dd><p>Controls the elastic net mixing parameter for the
prevalence covariates.  See above for a description.</p>
</dd>
<dt><code>gamma.ic.k</code></dt><dd><p>For L1 mode prevalence covariates this controls the 
selection of the regularization parameter.  We use a generic information criterion
which penalizes complexity by the parameter <code>ic.k</code>.  
When set to 2 (as by default) this results in AIC.  When set to log(n) 
(where n is the total number of documents in the corpus) this is equivalent to BIC.
Larger numbers will express a preference for sparser (simpler) models.</p>
</dd>
<dt><code>gamma.maxits</code></dt><dd><p>An integer indicating the maximum number of iterations
that the prevalence regression variational algorithm can run before erroring out.
Defaults to 1000.</p>
</dd>
<dt><code>nlambda</code></dt><dd><p>Controls the length of the regularization path when
using L1 mode for content covariates.  Defaults to 500.  Note that glmnet
relies heavily on warm starts and so a high number will often
(counter-intuitively) be less costly than a low number.  We have chosen a
higher default here than the default in the glmnet package and we don't
recommend changing it.</p>
</dd> 
<dt><code>lambda.min.ratio</code></dt><dd><p>For L1 mode content
covariates this controls the explored path of regularization values.  This
defaults to .0001.  Setting higher numbers will result in more sparse
solutions.  This is here primarily for dealing with convergence issues, if
you want to favor selection of sparser solutions see the next argument.</p>
</dd>
<dt><code>ic.k</code></dt><dd><p>For L1 mode content covariates this controls the
selection of the regularization parameter.  We use a generic information
criterion which penalizes complexity by the parameter <code>ic.k</code>.  When set
to 2 (as by default) this results in AIC.  When set to log(n) (where n is
the total number of words in the corpus) this is equivalent to BIC.  Larger
numbers will express a preference for sparser (simpler) models.</p>
</dd>
<dt><code>nits</code></dt><dd><p>Sets the number of iterations for collapsed gibbs
sampling in LDA initializations.  Defaults to 50</p>
</dd> 
<dt><code>burnin</code></dt><dd><p>Sets
the burnin for collapsed gibbs sampling in LDA initializations. Defaults to
25</p>
</dd> 
<dt><code>alpha</code></dt><dd><p>Sets the prevalence hyperparameter in collapsed
gibbs sampling in LDA initializations.  Defaults to 50/K</p>
</dd>
<dt><code>eta</code></dt><dd><p>Sets the topic-word hyperparameter in collapsed gibbs
sampling in LDA initializations.  Defaults to .01</p>
</dd> 
<dt><code>contrast</code></dt><dd><p>A
logical indicating whether a standard contrast coding should be used for
content covariates.  Typically this should remain at the default of FALSE.</p>
</dd>
<dt><code>rp.s</code></dt><dd><p>Parameter between 0 and 1 controlling the sparsity of
random projections for the spectral initialization.  Defaults to .05</p>
</dd>
<dt><code>rp.p</code></dt><dd><p>Dimensionality of the random projections for the
spectral initialization.  Defaults to 3000.</p>
</dd>
<dt><code>rp.d.group.size</code></dt><dd><p>Controls the size of blocks considered at a
time when computing the random projections for the spectral initialization.
Defaults to 2000.</p>
</dd> 
<dt><code>SpectralRP</code></dt><dd><p>A logical which when
<code>TRUE</code> turns on the experimental random projections spectral
initialization.</p>
</dd> 
<dt><code>maxV</code></dt><dd><p>For spectral initializations this will set the maximum
number of words to be used in the initialization.  It uses the most frequent words
first and then they are reintroduced following initialization.  This allows spectral
to be used with a large V.</p>
</dd>
<dt><code>recoverEG</code></dt><dd><p>Set to <code>TRUE</code> by default.  If set to <code>FALSE</code>
will solve the recovery problem in the Spectral algorithm using a downhill simplex
method.  See https://github.com/bstewart/stm/issues/133 for more discussion.</p>
</dd>
<dt><code>allow.neg.change</code></dt><dd><p>A logical indicating whether the algorithm is allowed
to declare convergence when the change in the bound has become negative. 
Defaults to <code>TRUE</code>.  Set to <code>FALSE</code> to keep the algorithm from converging
when the bound change is negative.  NB: because this is 
only an approximation to the lower-bound the change can be negative at times.  Right
now this triggers convergence but the final approximate bound might go higher if you
are willing to wait it out. The logic of the default setting is that a negative change
in the bound usually means it is barely moving at all.</p>
</dd>
<dt><code>custom.beta</code></dt><dd><p>If <code>init.type="Custom"</code> you can pass your own initialization
of the topic-word distributions beta to use as an initialization.  Please note that this takes
some care to be sure that it is provided in exactly the right format.  The number of topics and
vocab must match exactly.  The vocab must be in the same order.  The values must not be pathological
(for instance setting the probability of a single word to be 0 under all topics). The beta should be
formatted in the same way as the piece of a returned stm model object <code>stmobj$beta$logbeta</code>.
It should be a list of length the number of levels of the content covariate.  Each element of the list
is a K by V matrix containing the logged word probability conditional on the topic.  If you use this
option we recommend that you use <code>max.em.its=0</code> with the model initialization set to random, inspect
the returned form of <code>stmobj$beta$logbeta</code> and ensure that it matches your format.</p>
</dd>
<dt><code>tSNE_init.dims</code></dt><dd><p>The K=0 spectral setting uses tSNE to create a low-dimensional
projection of the vocab co-occurrence matrix.  tSNE starts with a PCA projection as an initialization.
We actually do the projection outside the tSNE code so we can use a randomized projection approach.
We use the 50 dimensional default of the <span class="pkg">rtsne</span> package.  That can be changed here.</p>
</dd>
<dt><code>tSNE_perplexity</code></dt><dd><p>The <code>Rtsne</code> function in the <span class="pkg">rtsne</span> package uses a perplexity
parameter.  This defaults to 30 and can throw an error when too high.  <code>stm</code> will automatically lower
the parameter for you until it works, but it can also be directly set here.</p>
</dd>
</dl>



<h3>Value</h3>

<p>An object of class STM 
</p>
<table>
<tr><td><code>mu</code></td>
<td>
<p>The corpus mean of topic prevalence and coefficients</p>
</td></tr> 
<tr><td><code>sigma</code></td>
<td>
<p>Covariance matrix</p>
</td></tr> 
<tr><td><code>beta</code></td>
<td>
<p>List containing the log of the word probabilities for each topic.</p>
</td></tr>
<tr><td><code>settings</code></td>
<td>
<p>The settings file. The Seed object will always contain the
seed which can be fed as an argument to recover the model.</p>
</td></tr> 
<tr><td><code>vocab</code></td>
<td>
<p>The vocabulary vector used.</p>
</td></tr> 
<tr><td><code>convergence</code></td>
<td>
<p>list of convergence elements including the value of the approximate bound on the marginal
likelihood at each step.</p>
</td></tr> 
<tr><td><code>theta</code></td>
<td>
<p>Number of Documents by Number of Topics matrix of topic proportions.</p>
</td></tr> 
<tr><td><code>eta</code></td>
<td>
<p>Matrix of means for the variational distribution of the multivariate normal latent variables used to
calculate theta.</p>
</td></tr> 
<tr><td><code>invsigma</code></td>
<td>
<p>The inverse of the sigma matrix.</p>
</td></tr>
<tr><td><code>time</code></td>
<td>
<p>The time elapsed in seconds</p>
</td></tr> 
<tr><td><code>version</code></td>
<td>
<p>The version number
of the package with which the model was estimated.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Roberts, M., Stewart, B., Tingley, D., and Airoldi, E. (2013)
&quot;The structural topic model and applied social science.&quot; In Advances in
Neural Information Processing Systems Workshop on Topic Models: Computation,
Application, and Evaluation. 
</p>
<p>Roberts M., Stewart, B. and Airoldi, E. (2016) &quot;A model of text for
experimentation in the social sciences&quot; Journal of the American Statistical
Association.
</p>
<p>Roberts, M., Stewart, B., Tingley, D., Lucas, C., Leder-Luis, J., Gadarian,
S., Albertson, B., et al. (2014). Structural topic models for open ended
survey responses. American Journal of Political Science, 58(4), 1064-1082.
</p>
<p>Roberts, M., Stewart, B., &amp; Tingley, D. (2016). &quot;Navigating the Local
Modes of Big Data: The Case of Topic Models. In Data Analytics in Social
Science, Government, and Industry.&quot; New York: Cambridge University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prepDocuments">prepDocuments</a></code> <code><a href="#topic+labelTopics">labelTopics</a></code>
<code><a href="#topic+estimateEffect">estimateEffect</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


#An example using the Gadarian data.  From Raw text to fitted model using 
#textProcessor() which leverages the tm Package
temp&lt;-textProcessor(documents=gadarian$open.ended.response,metadata=gadarian)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta)
set.seed(02138)
mod.out &lt;- stm(out$documents, out$vocab, 3, 
               prevalence=~treatment + s(pid_rep), data=out$meta)

#The same example using quanteda instead of tm via textProcessor()
#Note this example works with quanteda version 0.9.9-31 and later
require(quanteda)
gadarian_corpus &lt;- corpus(gadarian, text_field = "open.ended.response")
gadarian_dfm &lt;- dfm(gadarian_corpus, 
                     remove = stopwords("english"),
                     stem = TRUE)
stm_from_dfm &lt;- stm(gadarian_dfm, K = 3, prevalence = ~treatment + s(pid_rep),
                    data = docvars(gadarian_corpus))
                     
#An example of restarting a model
mod.out &lt;- stm(out$documents, out$vocab, 3, prevalence=~treatment + s(pid_rep), 
               data=out$meta, max.em.its=5)
mod.out2 &lt;- stm(out$documents, out$vocab, 3, prevalence=~treatment + s(pid_rep), 
                data=out$meta, model=mod.out, max.em.its=10)

</code></pre>

<hr>
<h2 id='summary.estimateEffect'>Summary for estimateEffect</h2><span id='topic+summary.estimateEffect'></span><span id='topic+print.summary.estimateEffect'></span>

<h3>Description</h3>

<p>Create a summary regression table similar to those produced for <code>lm</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'estimateEffect'
summary(object, topics = NULL, nsim = 500, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.estimateEffect_+3A_object">object</code></td>
<td>
<p>an object of class <code>"estimateEffect"</code>, usually a result of a call to
<code><a href="#topic+estimateEffect">estimateEffect</a></code></p>
</td></tr>
<tr><td><code id="summary.estimateEffect_+3A_topics">topics</code></td>
<td>
<p>a vector containing the topic numbers for each a summary is to be calculated.
Must be contained in the original <code>estimateEffect</code> object</p>
</td></tr>
<tr><td><code id="summary.estimateEffect_+3A_nsim">nsim</code></td>
<td>
<p>the number of simulations to use per parameter set to calculate the standard error.
Defaults to 500</p>
</td></tr>
<tr><td><code id="summary.estimateEffect_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function along with <code><a href="#topic+print.summary.estimateEffect">print.summary.estimateEffect</a></code> creates
regression tables that look like typically summaries you see in R.  In general
we recommend that you use non-linearities such as splines via function like
<code><a href="#topic+s">s</a></code> and in those circumstances the tables are not particularly
interpretable.  
</p>
<p>Confidence intervals are calculated by using draws from the covariance matrix
of each simulation to estimate the standard error.  Then a t-distribution approximation
is applied to calculate the various quantities of interest.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+estimateEffect">estimateEffect</a></code> <code><a href="#topic+plot.estimateEffect">plot.estimateEffect</a></code>
</p>

<hr>
<h2 id='summary.STM'>Summary Function for the STM objects</h2><span id='topic+summary.STM'></span><span id='topic+print.STM'></span>

<h3>Description</h3>

<p>Function to report on the contents of STM objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'STM'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.STM_+3A_object">object</code></td>
<td>
<p>An STM object.</p>
</td></tr>
<tr><td><code id="summary.STM_+3A_...">...</code></td>
<td>
<p>Additional arguments affecting the summary</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Summary prints a short statement about the model and then runs
<code><a href="#topic+labelTopics">labelTopics</a></code>.
</p>

<hr>
<h2 id='textProcessor'>Process a vector of raw texts</h2><span id='topic+textProcessor'></span><span id='topic+print.textProcessor'></span><span id='topic+head.textProcessor'></span><span id='topic+summary.textProcessor'></span>

<h3>Description</h3>

<p>Function that takes in a vector of raw texts (in a variety of languages) and
performs basic operations.  This function is essentially a wrapper <span class="pkg">tm</span>
package where various user specified options can be selected.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textProcessor(
  documents,
  metadata = NULL,
  lowercase = TRUE,
  removestopwords = TRUE,
  removenumbers = TRUE,
  removepunctuation = TRUE,
  ucp = FALSE,
  stem = TRUE,
  wordLengths = c(3, Inf),
  sparselevel = 1,
  language = "en",
  verbose = TRUE,
  onlycharacter = FALSE,
  striphtml = FALSE,
  customstopwords = NULL,
  custompunctuation = NULL,
  v1 = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="textProcessor_+3A_documents">documents</code></td>
<td>
<p>The documents to be processed.  A character vector where
each entry is the full text of a document (if passed as a different type
it will attempt to convert to a character vector).</p>
</td></tr>
<tr><td><code id="textProcessor_+3A_metadata">metadata</code></td>
<td>
<p>Additional data about the documents.  Specifically a
<code>data.frame</code> or <code>matrix</code> object with number of rows equal to the
number of documents and one column per meta-data type. The column names are
used to label the metadata.  The metadata do not affect the text processing,
but providing the metadata object insures that if documents are dropped the
corresponding metadata rows are dropped as well.</p>
</td></tr>
<tr><td><code id="textProcessor_+3A_lowercase">lowercase</code></td>
<td>
<p>Whether all words should be converted to lower case.
Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="textProcessor_+3A_removestopwords">removestopwords</code></td>
<td>
<p>Whether stop words should be removed using the SMART
stopword list (in English) or the snowball stopword lists (for all other
languages). Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="textProcessor_+3A_removenumbers">removenumbers</code></td>
<td>
<p>Whether numbers should be removed. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="textProcessor_+3A_removepunctuation">removepunctuation</code></td>
<td>
<p>whether punctuation should be removed.  Defaults to
TRUE.</p>
</td></tr>
<tr><td><code id="textProcessor_+3A_ucp">ucp</code></td>
<td>
<p>When TRUE passes <code>ucp=TRUE</code> to <code>tm::removePunctuation</code> which
removes a more general set of punctuation (the Unicode general category P).  Defaults
to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="textProcessor_+3A_stem">stem</code></td>
<td>
<p>Whether or not to stem words. Defaults to TRUE</p>
</td></tr>
<tr><td><code id="textProcessor_+3A_wordlengths">wordLengths</code></td>
<td>
<p>From the <span class="pkg">tm</span> package. An integer vector of length 2.
Words shorter than the minimum word length <code>wordLengths[1]</code> or longer
than the maximum word length <code>wordLengths[2]</code> are discarded. Defaults
to <code>c(3, Inf)</code>, i.e., a minimum word length of 3 characters.</p>
</td></tr>
<tr><td><code id="textProcessor_+3A_sparselevel">sparselevel</code></td>
<td>
<p>Removes terms where at least sparselevel proportion of
the entries are 0. Defaults to 1 which effectively turns the feature off.</p>
</td></tr>
<tr><td><code id="textProcessor_+3A_language">language</code></td>
<td>
<p>Language used for processing. Defaults to English. <code>tm</code>
uses the <code>SnowballC</code> stemmer which as of version 0.5 supports &quot;danish
dutch english finnish french german hungarian italian norwegian portuguese
romanian russian spanish swedish turkish&quot;.  These can be specified as any on
of the above strings or by the three-letter ISO-639 codes.  You can also set
language to &quot;na&quot; if you want to leave it deliberately unspecified (see
documentation in <code>tm</code>) Note that languages listed here may not all have 
accompanying stopwords.  However if you have your own stopword list you can use
customstopwords below.</p>
</td></tr>
<tr><td><code id="textProcessor_+3A_verbose">verbose</code></td>
<td>
<p>If true prints information as it processes.</p>
</td></tr>
<tr><td><code id="textProcessor_+3A_onlycharacter">onlycharacter</code></td>
<td>
<p>When TRUE, runs a regular expression substitution to
replace all non-alphanumeric characters. These characters can crash
textProcessor for some operating systems.  May remove foreign characters
depending on encoding. Defaults to FALSE.  Defaults to FALSE. Runs before
call to tm package.</p>
</td></tr>
<tr><td><code id="textProcessor_+3A_striphtml">striphtml</code></td>
<td>
<p>When TRUE, runs a regular expression substitution to strip
html contained within &lt;&gt;.  Defaults to FALSE. Runs before call to tm
package.</p>
</td></tr>
<tr><td><code id="textProcessor_+3A_customstopwords">customstopwords</code></td>
<td>
<p>A character vector containing words to be removed.
Defaults to NULL which does not remove any additional words.  This function
is primarily for easy removal of application specific stopwords.  Note that
as with standard stopwords these are removed after converting everything to
lower case but before removing numbers, punctuation or stemming.  Thus words
to be removed should be all lower case but otherwise complete.</p>
</td></tr>
<tr><td><code id="textProcessor_+3A_custompunctuation">custompunctuation</code></td>
<td>
<p>A character vector containing any characters to be
removed immediately after standard punctuation removal. This function exists 
primarily for easy removal of application specific punctuation not caught by
the punctuation filter (although see also the <code>ucp</code> argument to turn on
a stronger punctuation filter). This can in theory be used to remove any 
characters you don't want in the text for some reason. In practice what this
function does is collapse the character vector to one string and put square
brackets around it in order to make a pattern that can be matched and replaced
with <code>gsub</code> at the punctuation removal stage.  If the <code>custompunctuation</code>
vector is length 1 and the first element is a left square bracket, the function
assumes that you have passed a regular expression and passes that directly
along to <code>gsub</code>.</p>
</td></tr>
<tr><td><code id="textProcessor_+3A_v1">v1</code></td>
<td>
<p>A logical which defaults to <code>FALSE</code>.  If set to <code>TRUE</code> it
will use the ordering of operations we used in Version 1.0 of the package.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is designed to provide a convenient and quick way to process a
relatively small volume texts for analysis with the package. It is designed
to quickly ingest data in a simple form like a spreadsheet where each
document sits in a single cell. If you have texts more complicated than a 
spreadsheet, we recommend you check out the excellent <span class="pkg">readtext</span> package. 
</p>
<p>The processor always strips extra white space but all other processing
options are optional.  Stemming uses the snowball stemmers and supports a
wide variety of languages.  Words in the vocabulary can be dropped due to
sparsity and stop word removal.  If a document no longer contains any words
it is dropped from the output.  Specifying meta-data is a convenient way to
make sure the appropriate rows are dropped from the corresponding metadata
file.
</p>
<p>When the option <code>sparseLevel</code> is set to a number other than 1,
infrequently appearing words are removed.  When a term is removed from the
vocabulary a message will print to the screen (as long as <code>verbose</code> has
not been set to <code>FALSE</code>).  The message indicates the number of terms
removed (that is, the number of vocabulary entries) as well as the number of
tokens removed (appearances of individual words).  The function
<code><a href="#topic+prepDocuments">prepDocuments</a></code> provides additional methods to prune infrequent
words.  In general the functionality there should be preferred.
</p>
<p>We emphasize that this function is a convenience wrapper around the
excellent <span class="pkg">tm</span> package functionality without which it wouldn't be
possible.
</p>


<h3>Value</h3>

<table>
<tr><td><code>documents</code></td>
<td>
<p>A list containing the documents in the stm format.</p>
</td></tr>
<tr><td><code>vocab</code></td>
<td>
<p>Character vector of vocabulary.</p>
</td></tr> <tr><td><code>meta</code></td>
<td>
<p>Data frame or
matrix containing the user-supplied metadata for the retained documents.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Ingo Feinerer and Kurt Hornik (2013). tm: Text Mining Package. R
package version 0.5-9.1.
</p>
<p>Ingo Feinerer, Kurt Hornik, and David Meyer (2008). Text Mining
Infrastructure in R. <em>Journal of Statistical Software</em> 25(5): 1-54.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+readCorpus">readCorpus</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


head(gadarian)
#Process the data for analysis.
temp&lt;-textProcessor(documents=gadarian$open.ended.response,metadata=gadarian)
meta&lt;-temp$meta
vocab&lt;-temp$vocab
docs&lt;-temp$documents
out &lt;- prepDocuments(docs, vocab, meta)
docs&lt;-out$documents
vocab&lt;-out$vocab
meta &lt;-out$meta


#Example of custom punctuation removal.
docs &lt;- c("co.rr?ec!t")
textProcessor(docs,custompunctuation=c(".","?","!"),
              removepunctuation = FALSE)$vocab
#note that the above should now say "correct"
 
</code></pre>

<hr>
<h2 id='thetaPosterior'>Draw from Theta Posterior</h2><span id='topic+thetaPosterior'></span>

<h3>Description</h3>

<p>Take random draws from the variational posterior for the document-topic
proportions. This is underlying methodology for <code><a href="#topic+estimateEffect">estimateEffect</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>thetaPosterior(
  model,
  nsims = 100,
  type = c("Global", "Local"),
  documents = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="thetaPosterior_+3A_model">model</code></td>
<td>
<p>An <code>STM</code> object created by <code><a href="#topic+stm">stm</a></code></p>
</td></tr>
<tr><td><code id="thetaPosterior_+3A_nsims">nsims</code></td>
<td>
<p>The number of draws from the variational posterior.  See
details below.</p>
</td></tr>
<tr><td><code id="thetaPosterior_+3A_type">type</code></td>
<td>
<p>A choice of two methods for constructing the covariance
approximation the <code>"Global"</code> approximation and the <code>"Local"</code>
approximation.  See details below.</p>
</td></tr>
<tr><td><code id="thetaPosterior_+3A_documents">documents</code></td>
<td>
<p>If <code>type="Local"</code>, the documents object used in the
original <code><a href="#topic+stm">stm</a></code> call should be passed here.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function allows the user to draw samples from the variational posterior
distribution over the normalized document-topic proportions, theta. The
function <code><a href="#topic+estimateEffect">estimateEffect</a></code> provides a user-friendly interface for
running regressions using samples from the posterior distribution.  When the
user wants to do something not covered by that function, the code here
provides easy access to uncertainty in the model.
</p>
<p>In order to simulate from the variational posterior for theta we take draws
from the variational distribution for eta (the unnormalized topic
proportions) and then map them to the simplex.  Each document in the corpus
has its own mean vector (eta) and covariance matrix (nu).  Because the
covariance matrices can be large we do not store them in the model objects.
We offer two approximations to the covariance matrix: Global and Local.  The
Global method constructs a single approximate covariance matrix which is
then shared by all documents.  This approach is very fast and does not
require access to the original documents.  For highly aggregated quantities
of interest this often produces similar results to the Local method.
</p>
<p>The Local method steps through each document in sequence and calculates the
covariance matrix.  If the model has not converged, this matrix can be
undefined and so we perform document level inference until the estimate
stabilizes.  This means that under the Local method both the covariance and
the mean of the variational distribution are recalculated.  It also means
that calling this option with Local specified will take approximately as
long as a standard E-step of <code><a href="#topic+stm">stm</a></code> for the same data and
possibly longer.  Because the memory requirements would be extreme for large
K, we calculate one document at a time, discarding the covariance matrix
before proceeding to the next document.  Thus, if your computer has
sufficient memory it is dramatically more computationally efficient to draw
all the samples you may require at once rather than taking one sample at a
time.
</p>
<p>The output for both methods is a list with number of elements equal to the
number of documents. Each element is a matrix with nsims rows and K columns.
Be careful to ensure that you have sufficient memory before attempting this
with a large number of simulations, documents or topics.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+estimateEffect">estimateEffect</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#global approximation
draws &lt;- thetaPosterior(gadarianFit, nsims = 100)
</code></pre>

<hr>
<h2 id='toLDAvis'>Wrapper to launch LDAvis topic browser.</h2><span id='topic+toLDAvis'></span>

<h3>Description</h3>

<p>Tool for exploring topic/word distributions using LDAvis topic browser.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>toLDAvis(
  mod,
  docs,
  R = 30,
  plot.opts = list(xlab = "PC1", ylab = "PC2"),
  lambda.step = 0.1,
  out.dir = tempfile(),
  open.browser = interactive(),
  as.gist = FALSE,
  reorder.topics = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="toLDAvis_+3A_mod">mod</code></td>
<td>
<p>STM object. Output from stm function.</p>
</td></tr>
<tr><td><code id="toLDAvis_+3A_docs">docs</code></td>
<td>
<p>Documents object passed to <code>stm</code> in this package's standard
format (see the documentation in <code><a href="#topic+stm">stm</a></code>.</p>
</td></tr>
<tr><td><code id="toLDAvis_+3A_r">R</code></td>
<td>
<p>Passed to <code><a href="LDAvis.html#topic+createJSON">createJSON</a></code> &quot;integer, the number of
terms to display in the barcharts of the interactive viz. Default is 30.
Recommended to be roughly between 10 and 50.&quot;</p>
</td></tr>
<tr><td><code id="toLDAvis_+3A_plot.opts">plot.opts</code></td>
<td>
<p>Passed to <code><a href="LDAvis.html#topic+createJSON">createJSON</a></code> &quot;a named list
used to customize various plot elements. By default, the x and y axes are
labeled 'PC1' and 'PC2' (principal components 1 and 2), since jsPCA is the
default scaling method. &quot;</p>
</td></tr>
<tr><td><code id="toLDAvis_+3A_lambda.step">lambda.step</code></td>
<td>
<p>Passed to <code><a href="LDAvis.html#topic+createJSON">createJSON</a></code> &quot;a value
between 0 and 1. Determines the interstep distance in the grid of lambda
values over which to iterate when computing relevance. Default is 0.01.
Recommended to be between 0.01 and 0.1.&quot;</p>
</td></tr>
<tr><td><code id="toLDAvis_+3A_out.dir">out.dir</code></td>
<td>
<p>Passed to <code><a href="LDAvis.html#topic+serVis">serVis</a></code> &quot;directory to store
html/js/json files.&quot;</p>
</td></tr>
<tr><td><code id="toLDAvis_+3A_open.browser">open.browser</code></td>
<td>
<p>Passed to <code><a href="LDAvis.html#topic+serVis">serVis</a></code> &quot;Should R open a
browser? If yes, this function will attempt to create a local file server
via the servr package. This is necessary since the javascript needs to
access local files and most browsers will not allow this.&quot;</p>
</td></tr>
<tr><td><code id="toLDAvis_+3A_as.gist">as.gist</code></td>
<td>
<p>Passed to <code><a href="LDAvis.html#topic+serVis">serVis</a></code> &quot;should the vis be
uploaded as a gist? Will prompt for an interactive login if the GITHUB_PAT
environment variable is not set&quot;</p>
</td></tr>
<tr><td><code id="toLDAvis_+3A_reorder.topics">reorder.topics</code></td>
<td>
<p>Passed to <code><a href="LDAvis.html#topic+createJSON">createJSON</a></code> &quot;Should LDAvis
re-order the K topics in order of decreasing proportion? Default is True&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Tool for exploring topic/word distributions using LDAvis topic browser.
Development build of LDAvis available at https://github.com/cpsievert/LDAvis
or download from CRAN. Note: LDAvis may renumber the topics.
</p>


<h3>References</h3>

<p>Carson Sievert and Kenny Shirley. LDAvis: Interactive
Visualization of Topic Models. R package version 0.3.1.
https://github.com/cpsievert/LDAvis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


mod &lt;- stm(poliblog5k.docs, poliblog5k.voc, K=25,
           prevalence=~rating, data=poliblog5k.meta,
           max.em.its=2, init.type="Spectral") 
#please don't run a model with 2 iterations
#this is done here to make it run quickly.
toLDAvis(mod=mod, docs=poliblog5k.docs)


</code></pre>

<hr>
<h2 id='toLDAvisJson'>Wrapper to create Json mapping for LDAvis. This can be useful in indirect render
e.g. Shiny Dashboards</h2><span id='topic+toLDAvisJson'></span>

<h3>Description</h3>

<p>Tool for exploring topic/word distributions using LDAvis topic browser.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>toLDAvisJson(
  mod,
  docs,
  R = 30,
  plot.opts = list(xlab = "PC1", ylab = "PC2"),
  lambda.step = 0.1,
  reorder.topics = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="toLDAvisJson_+3A_mod">mod</code></td>
<td>
<p>STM object. Output from stm function.</p>
</td></tr>
<tr><td><code id="toLDAvisJson_+3A_docs">docs</code></td>
<td>
<p>Documents object passed to <code>stm</code> in this package's standard
format (see the documentation in <code><a href="#topic+stm">stm</a></code>.</p>
</td></tr>
<tr><td><code id="toLDAvisJson_+3A_r">R</code></td>
<td>
<p>Passed to <code><a href="LDAvis.html#topic+createJSON">createJSON</a></code> &quot;integer, the number of
terms to display in the barcharts of the interactive viz. Default is 30.
Recommended to be roughly between 10 and 50.&quot;</p>
</td></tr>
<tr><td><code id="toLDAvisJson_+3A_plot.opts">plot.opts</code></td>
<td>
<p>Passed to <code><a href="LDAvis.html#topic+createJSON">createJSON</a></code> &quot;a named list
used to customize various plot elements. By default, the x and y axes are
labeled 'PC1' and 'PC2' (principal components 1 and 2), since jsPCA is the
default scaling method. &quot;</p>
</td></tr>
<tr><td><code id="toLDAvisJson_+3A_lambda.step">lambda.step</code></td>
<td>
<p>Passed to <code><a href="LDAvis.html#topic+createJSON">createJSON</a></code> &quot;a value
between 0 and 1. Determines the interstep distance in the grid of lambda
values over which to iterate when computing relevance. Default is 0.01.
Recommended to be between 0.01 and 0.1.&quot;</p>
</td></tr>
<tr><td><code id="toLDAvisJson_+3A_reorder.topics">reorder.topics</code></td>
<td>
<p>Passed to <code><a href="LDAvis.html#topic+createJSON">createJSON</a></code> &quot;Should LDAvis
re-order the K topics in order of decreasing proportion? Default is True&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Tool for exploring topic/word distributions using LDAvis topic browser.
Development build of LDAvis available at https://github.com/cpsievert/LDAvis
or download from CRAN. Note: LDAvis may renumber the topics.
</p>


<h3>References</h3>

<p>Carson Sievert and Kenny Shirley. LDAvis: Interactive
Visualization of Topic Models. R package version 0.3.1.
https://github.com/cpsievert/LDAvis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


mod &lt;- stm(poliblog5k.docs, poliblog5k.voc, K=25,
           prevalence=~rating, data=poliblog5k.meta,
           max.em.its=2, init.type="Spectral") 
#please don't run a model with 2 iterations
#this is done here to make it run quickly.
toLDAvisJson(mod=mod, docs=poliblog5k.docs)


</code></pre>

<hr>
<h2 id='topicCorr'>Estimate topic correlation</h2><span id='topic+topicCorr'></span>

<h3>Description</h3>

<p>Estimates a graph of topic correlations using either a simple thresholding
measure or more sophisticated tests from the package <code>huge</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>topicCorr(model, method = c("simple", "huge"), cutoff = 0.01, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="topicCorr_+3A_model">model</code></td>
<td>
<p>An STM object for which you want to estimate correlations
between topics.</p>
</td></tr>
<tr><td><code id="topicCorr_+3A_method">method</code></td>
<td>
<p>Method for estimating the graph.  <code>"simple"</code> simply
thresholds the covariances.  <code>"huge"</code> uses the semiparametric procedure
in the package <code>huge</code>.  See details below.</p>
</td></tr>
<tr><td><code id="topicCorr_+3A_cutoff">cutoff</code></td>
<td>
<p>When using the simple method, this is the cutoff below which
correlations are truncated to zero.</p>
</td></tr>
<tr><td><code id="topicCorr_+3A_verbose">verbose</code></td>
<td>
<p>A logical which indicates whether information should be
printed to the screen when running <code>"huge"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We offer two estimation procedures for producing correlation graphs.  The
results of either method can be plotted using <code><a href="#topic+plot.topicCorr">plot.topicCorr</a></code>.
The first method is conceptually simpler and involves a simple thresholding
procedure on the estimated marginal topic proportion correlation matrix and
requires a human specified threshold.  The second method draws on recent
literature undirected graphical model estimation and is automatically tuned.
</p>
<p>The <code>"simple"</code> method calculates the correlation of the MAP estimates
for the topic proportions <code class="reqn">\theta</code> which yields the marginal correlation
of the mode of the variational distribution. Then we simply set to 0 those
edges where the correlation falls below the threshold.
</p>
<p>An alternative strategy is to treat the problem as the recovery of edges in
a high-dimensional undirected graphical model. In these settings we assume
that observations come from a multivariate normal distribution with a sparse
precision matrix.  The goal is to infer which elements of the precision
matrix are non-zero corresponding to edges in a graph.  Meinshausen and
Buhlmann (2006) showed that using sparse regression methods like the LASSO
it is possible to consistently identify edges even in very high dimensional
settings.
</p>
<p>Selecting the option <code>"huge"</code> uses the <code>huge</code> package by Zhao and
Liu to estimate the graph.  We use a nonparanormal transformation of the
topic proportions (<code class="reqn">\theta</code>) which uses semiparametric Gaussian copulas
to marginally transform the data.  This weakens the gaussian assumption of
the subsequent procedure.  We then estimate the graph using the Meinshausen
and Buhlmann procedure.  Model selection for the scale of the <code class="reqn">L_1</code>
penalty is performed using the rotation information criterion (RIC) which
estimates the optimal degree of regularization by random rotations.  Zhao
and Lieu (2012) note that this selection approach has strong empirical
performance but is sensitive to under-selection of edges.  We choose this
metric as the default approach to model selection to reflect social
scientists' historically greater concern for false positive rates as opposed
to false negative rates.
</p>
<p>We note that in models with low numbers of topics the simple procedure and
the more complex procedure will often yield identical results.  However, the
advantage of the more complex procedure is that it scales gracefully to
models with hundreds or even thousands of topics - specifically the set of
cases where some higher level structure like a correlation graph would be
the most useful.
</p>


<h3>Value</h3>

<table>
<tr><td><code>posadj</code></td>
<td>
<p>K by K adjacency matrix where an edge represents
positive correlation selected by the model.</p>
</td></tr> <tr><td><code>poscor</code></td>
<td>
<p>K by K
correlation matrix.  It takes values of zero where the correlation is either
negative or the edge is unselected by the model selection procedure.</p>
</td></tr>
<tr><td><code>cor</code></td>
<td>
<p>K by K correlation matrix element-wise multiplied by the
adjacency matrix. Note that this will contain significant negative
correlations as well as positive correlations.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Lucas, Christopher, Richard A. Nielsen, Margaret E. Roberts,
Brandon M. Stewart, Alex Storer, and Dustin Tingley. &quot;Computer-Assisted Text
Analysis for Comparative Politics.&quot; Political Analysis (2015).
</p>
<p>T. Zhao and H. Liu. The huge Package for High-dimensional Undirected Graph
Estimation in R. Journal of Machine Learning Research, 2012
</p>
<p>H. Liu, F. Han, M. Yuan, J. Lafferty and L. Wasserman. High Dimensional
Semiparametric Gaussian Copula Graphical Models. Annals of Statistics,2012
</p>
<p>N. Meinshausen and P. Buhlmann. High-dimensional Graphs and Variable
Selection with the Lasso. The Annals of Statistics, 2006.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.topicCorr">plot.topicCorr</a></code>
</p>

<hr>
<h2 id='topicLasso'>Plot predictions using topics</h2><span id='topic+topicLasso'></span>

<h3>Description</h3>

<p>Use the <span class="pkg">glmnet</span> package to plot LASSO based estimates of relationship
between an arbitrary dependent variable with topics and additional variables
as predictors.  This function is experimental (see below).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>topicLasso(
  formula,
  data,
  stmobj = NULL,
  subset = NULL,
  omit.var = NULL,
  family = "gaussian",
  main = "Topic Effects on Outcome",
  xlab = expression("Lower Outcome Higher Outcome"),
  labeltype = c("prob", "frex", "lift", "score"),
  seed = 2138,
  xlim = c(-4, 4),
  standardize = FALSE,
  nfolds = 20,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="topicLasso_+3A_formula">formula</code></td>
<td>
<p>Formula specifying the dependent variable and additional
variables to included in the LASSO beyond the topics present in the stmobj.
Just pass a 1 on the right-hand side in order to run without additional
controls.</p>
</td></tr>
<tr><td><code id="topicLasso_+3A_data">data</code></td>
<td>
<p>Data file containing the dependent variable. Typically will be
the metadata file used in the stm analysis. It must have a number of rows
equal to the number of documents in the stmobj.</p>
</td></tr>
<tr><td><code id="topicLasso_+3A_stmobj">stmobj</code></td>
<td>
<p>The STM object, and output from the <code>stm</code> function.</p>
</td></tr>
<tr><td><code id="topicLasso_+3A_subset">subset</code></td>
<td>
<p>A logical statement that will be used to subset the corpus.</p>
</td></tr>
<tr><td><code id="topicLasso_+3A_omit.var">omit.var</code></td>
<td>
<p>Pass a character vector of variable names to be excluded
from the plot.  Note this does not exclude them from the calculation, only
the plot.</p>
</td></tr>
<tr><td><code id="topicLasso_+3A_family">family</code></td>
<td>
<p>The family parameter used in <code><a href="glmnet.html#topic+glmnet">glmnet</a></code>.  See
explanation there.  Defaults to &quot;gaussian&quot;</p>
</td></tr>
<tr><td><code id="topicLasso_+3A_main">main</code></td>
<td>
<p>Character string for the main title.</p>
</td></tr>
<tr><td><code id="topicLasso_+3A_xlab">xlab</code></td>
<td>
<p>Character string giving an x-axis label.</p>
</td></tr>
<tr><td><code id="topicLasso_+3A_labeltype">labeltype</code></td>
<td>
<p>Type of example words to use in labeling each topic. See
<code><a href="#topic+labelTopics">labelTopics</a></code>. Defaults to &quot;prob&quot;.</p>
</td></tr>
<tr><td><code id="topicLasso_+3A_seed">seed</code></td>
<td>
<p>The random seed for replication of the cross-validation samples.</p>
</td></tr>
<tr><td><code id="topicLasso_+3A_xlim">xlim</code></td>
<td>
<p>Width of the x-axis.</p>
</td></tr>
<tr><td><code id="topicLasso_+3A_standardize">standardize</code></td>
<td>
<p>Whether to standardize variables. Default is FALSE, which
is different from the <span class="pkg">glmnet</span> default because the topics are already
standardized.  Note that glmnet standardizes the variables by default but
then projects them back to their original scales before reporting
coefficients.</p>
</td></tr>
<tr><td><code id="topicLasso_+3A_nfolds">nfolds</code></td>
<td>
<p>the number of cross-validation folds.  Defaults to 20.</p>
</td></tr>
<tr><td><code id="topicLasso_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to glmnet.  This can be useful
for addressing convergence problems.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used for estimating the most important topical predictors
of an arbitrary outcome.  The idea is to run an L1 regularized regression
using <code><a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a></code> in the <span class="pkg">glmnet</span> package where the
document-level dependent variable is chosen by the user and the predictors
are the document-topic proportions in the <code><a href="#topic+stm">stm</a></code> model along with
any other variables of interest.
</p>
<p>The function uses cross-validation to choose the regularization parameter
and generates a plot of which loadings were the most influential in
predicting the outcome.  It also invisibly returns the glmnet model so that
it can be used for prediction.
</p>
<p>NOTE: This function is still very experimental and may have stability
issues.  If stability issues are encountered see the documentation in
<span class="pkg">glmnet</span> for arguments that can be passed to improve convergence.  Also,
it is unlikely to work well with multivariate gaussian or multinomial
families.
</p>


<h3>References</h3>

<p>Friedman, Jerome, Trevor Hastie, and Rob Tibshirani.
&quot;Regularization paths for generalized linear models via coordinate descent.&quot;
Journal of statistical software 33.1 (2010): 1.
</p>


<h3>See Also</h3>

<p><span class="pkg">glmnet</span>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


#Load the poliblog data
data(poliblog5k)
#estimate a model with 50 topics
stm1 &lt;- stm(poliblog5k.docs, poliblog5k.voc, 50,
            prevalence=~rating + blog, data=poliblog5k.meta,
            init.type="Spectral")
#make a plot of the topics most predictive of "rating"
out &lt;- topicLasso(rating ~ 1, family="binomial", data=poliblog5k.meta,stmobj=stm1)
#generate some in-sample predictions
pred &lt;- predict(out, newx=stm1$theta,type="class")
#check the accuracy of the predictions
table(pred, poliblog5k.meta$rating)

</code></pre>

<hr>
<h2 id='topicQuality'>Plots semantic coherence and exclusivity for each topic.</h2><span id='topic+topicQuality'></span>

<h3>Description</h3>

<p>Plots semantic coherence and exclusivity for each topic.  Does not support
models with content covariates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>topicQuality(
  model,
  documents,
  xlab = "Semantic Coherence",
  ylab = "Exclusivity",
  labels = 1:ncol(model$theta),
  M = 10,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="topicQuality_+3A_model">model</code></td>
<td>
<p>Output from stm, or a selected model from selectModel.</p>
</td></tr>
<tr><td><code id="topicQuality_+3A_documents">documents</code></td>
<td>
<p>The documents (see <code><a href="#topic+stm">stm</a></code> for format).</p>
</td></tr>
<tr><td><code id="topicQuality_+3A_xlab">xlab</code></td>
<td>
<p>Character string that is x axis title. This should be semantic
coherence.</p>
</td></tr>
<tr><td><code id="topicQuality_+3A_ylab">ylab</code></td>
<td>
<p>Character string that is y axis title. This should be
exclusivity.</p>
</td></tr>
<tr><td><code id="topicQuality_+3A_labels">labels</code></td>
<td>
<p>Vector of number corresponding to topic numbers.</p>
</td></tr>
<tr><td><code id="topicQuality_+3A_m">M</code></td>
<td>
<p>Number of words to use in semantic coherence and exclusivity
calculations</p>
</td></tr>
<tr><td><code id="topicQuality_+3A_...">...</code></td>
<td>
<p>Other plotting parameters from igraph.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each model has semantic coherence and exclusivity values associated with
each topic.  This function plots these values and labels each with its topic
number.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

  #Semantic Coherence calculations require the original documents so we need
  #to reconstruct them here.
  temp&lt;-textProcessor(documents=gadarian$open.ended.response,metadata=gadarian)
  meta&lt;-temp$meta
  vocab&lt;-temp$vocab
  docs&lt;-temp$documents
  out &lt;- prepDocuments(docs, vocab, meta)
  docs&lt;-out$documents
  vocab&lt;-out$vocab
  meta &lt;-out$meta
  topicQuality(model=gadarianFit, documents=docs)

## End(Not run)
</code></pre>

<hr>
<h2 id='unpack.glmnet'>Unpack a <span class="pkg">glmnet</span> object</h2><span id='topic+unpack.glmnet'></span>

<h3>Description</h3>

<p>A function to quickly unpack a <span class="pkg">glmnet</span> model object and calculate an
optimal model from the regularization path.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unpack.glmnet(mod, ic.k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unpack.glmnet_+3A_mod">mod</code></td>
<td>
<p>the glmnet model</p>
</td></tr>
<tr><td><code id="unpack.glmnet_+3A_ic.k">ic.k</code></td>
<td>
<p>the information criterion value.  AIC is <code>ic.k=2</code> and BIC would be <code>ic.k=log n</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a small utility we wrote to deal with the slow methods dispatch for S4
classes.  The more straightforward option is the <code>coef()</code> method for <span class="pkg">glmnet</span>
objects but when trying to make thousands of calls a second, that can be very slow
</p>


<h3>Value</h3>

<p>A list
</p>
<table>
<tr><td><code>coef</code></td>
<td>
<p>a matrix of coefficients</p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>the intercepts</p>
</td></tr>
</table>

<hr>
<h2 id='writeLdac'>Write a .ldac file</h2><span id='topic+writeLdac'></span>

<h3>Description</h3>

<p>A function for writing documents out to a .ldac formatted file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>writeLdac(documents, file, zeroindex = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="writeLdac_+3A_documents">documents</code></td>
<td>
<p>A documents object to be written out to a file.  Object
must be a list of with each element corresponding to a document.  Each
document is represented as an integer matrix with two rows, and columns
equal to the number of unique vocabulary words in the document.  The first
row contains the 1-indexed vocabulary entry and the second row contains the
number of times that term appears</p>
</td></tr>
<tr><td><code id="writeLdac_+3A_file">file</code></td>
<td>
<p>A character string giving the name of the file to be written.
This object is passed directly to the argument <code>con</code> in
<code><a href="base.html#topic+writeLines">writeLines</a></code> and thus can be a connection object as well.</p>
</td></tr>
<tr><td><code id="writeLdac_+3A_zeroindex">zeroindex</code></td>
<td>
<p>If <code>TRUE</code> (the default) it subtracts one
from each index.  If <code>FALSE</code> it uses the indices as given.  The
standard <code>.ldac</code> format indexes from 0 as per standard convention in
most languages.  Our documents format indexes from 1 to abide by conventions
in <code>R</code>.  This option converts to the zero index by default.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a simple convenience function for writing out document corpora.
Files can be read back into R using <code><a href="#topic+readCorpus">readCorpus</a></code> or simply used
for other programs.  The output is a file in the <code>.ldac</code> sparse matrix
format popularized by Dave Blei's C code for LDA.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+readCorpus">readCorpus</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

#Convert the gadarian data into documents format
temp&lt;-textProcessor(documents=gadarian$open.ended.response,metadata=gadarian)
documents &lt;- temp$documents
#Now write out to an ldac file
writeLdac(documents, file="gadarian.ldac")

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
