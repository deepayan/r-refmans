<!DOCTYPE html><html><head><title>Help for package piecemaker</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {piecemaker}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#piecemaker-package'><p>piecemaker: Tools for Preparing Text for Tokenizers</p></a></li>
<li><a href='#.make_unicode_block_regex'><p>Make Regex for Unicode Blocks</p></a></li>
<li><a href='#.space_regex_selector'><p>Space Text by a Regex Selector</p></a></li>
<li><a href='#prepare_and_tokenize'><p>Split Text on Spaces</p></a></li>
<li><a href='#prepare_text'><p>Prepare Text for Tokenization</p></a></li>
<li><a href='#remove_control_characters'><p>Remove Non-Character Characters</p></a></li>
<li><a href='#remove_diacritics'><p>Remove Diacritical Marks on Characters</p></a></li>
<li><a href='#remove_replacement_characters'><p>Remove the Unicode Replacement Character</p></a></li>
<li><a href='#space_cjk'><p>Add Spaces Around CJK Ideographs</p></a></li>
<li><a href='#space_punctuation'><p>Add Spaces Around Punctuation</p></a></li>
<li><a href='#squish_whitespace'><p>Remove Extra Whitespace</p></a></li>
<li><a href='#tokenize_space'><p>Break Text at Spaces</p></a></li>
<li><a href='#validate_utf8'><p>Clean Up Text to UTF-8</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Tools for Preparing Text for Tokenizers</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.2</td>
</tr>
<tr>
<td>Description:</td>
<td>Tokenizers break text into pieces that are more usable by
    machine learning models. Many tokenizers share some preparation steps.
    This package provides those shared steps, along with a simple
    tokenizer.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (&ge; 2)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/macmillancontentscience/piecemaker">https://github.com/macmillancontentscience/piecemaker</a>,
<a href="https://macmillancontentscience.github.io/piecemaker/">https://macmillancontentscience.github.io/piecemaker/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/macmillancontentscience/piecemaker/issues">https://github.com/macmillancontentscience/piecemaker/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>cli, glue, rlang (&ge; 0.4.2), stringi, stringr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-06-02 18:40:35 UTC; jonth</td>
</tr>
<tr>
<td>Author:</td>
<td>Jon Harmon <a href="https://orcid.org/0000-0003-4781-4346"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Jonathan Bratt <a href="https://orcid.org/0000-0003-2859-0076"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Bedford Freeman &amp; Worth Pub Grp LLC DBA Macmillan Learning [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jon Harmon &lt;jonthegeek@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-06-02 19:50:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='piecemaker-package'>piecemaker: Tools for Preparing Text for Tokenizers</h2><span id='topic+piecemaker'></span><span id='topic+piecemaker-package'></span>

<h3>Description</h3>

<p>Tokenizers break text into pieces that are more usable by machine learning models. Many tokenizers share some preparation steps. This package provides those shared steps, along with a simple tokenizer.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Jon Harmon <a href="mailto:jonthegeek@gmail.com">jonthegeek@gmail.com</a> (<a href="https://orcid.org/0000-0003-4781-4346">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Jonathan Bratt <a href="mailto:jonathan.bratt@macmillan.com">jonathan.bratt@macmillan.com</a> (<a href="https://orcid.org/0000-0003-2859-0076">ORCID</a>)
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Bedford Freeman &amp; Worth Pub Grp LLC DBA Macmillan Learning [copyright holder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/macmillancontentscience/piecemaker">https://github.com/macmillancontentscience/piecemaker</a>
</p>
</li>
<li> <p><a href="https://macmillancontentscience.github.io/piecemaker/">https://macmillancontentscience.github.io/piecemaker/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/macmillancontentscience/piecemaker/issues">https://github.com/macmillancontentscience/piecemaker/issues</a>
</p>
</li></ul>


<hr>
<h2 id='.make_unicode_block_regex'>Make Regex for Unicode Blocks</h2><span id='topic+.make_unicode_block_regex'></span>

<h3>Description</h3>

<p>Make Regex for Unicode Blocks
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.make_unicode_block_regex(unicode_block_name)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".make_unicode_block_regex_+3A_unicode_block_name">unicode_block_name</code></td>
<td>
<p>The name of the unicode block as it appears in
<a href="https://en.wikipedia.org/wiki/Unicode_block#List_of_blocks">the
Wikipedia list of Unicode blocks</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A regex wildcard box in square brackets.
</p>

<hr>
<h2 id='.space_regex_selector'>Space Text by a Regex Selector</h2><span id='topic+.space_regex_selector'></span>

<h3>Description</h3>

<p>Space Text by a Regex Selector
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.space_regex_selector(text, regex_selector)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".space_regex_selector_+3A_text">text</code></td>
<td>
<p>Character; text to space.</p>
</td></tr>
<tr><td><code id=".space_regex_selector_+3A_regex_selector">regex_selector</code></td>
<td>
<p>A regular expression that selects the blocks of text
you want to space.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector the same length as text, with spaces around the
specified blocks.
</p>

<hr>
<h2 id='prepare_and_tokenize'>Split Text on Spaces</h2><span id='topic+prepare_and_tokenize'></span>

<h3>Description</h3>

<p>This is an extremely simple tokenizer that simply splits text on spaces. It
also optionally applies the cleaning processes from
<code><a href="#topic+prepare_text">prepare_text</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_and_tokenize(text, prepare = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepare_and_tokenize_+3A_text">text</code></td>
<td>
<p>A character vector to clean.</p>
</td></tr>
<tr><td><code id="prepare_and_tokenize_+3A_prepare">prepare</code></td>
<td>
<p>Logical; should the text be passed through
<code><a href="#topic+prepare_text">prepare_text</a></code>?</p>
</td></tr>
<tr><td><code id="prepare_and_tokenize_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+prepare_text">prepare_text</a></code>
</p>

<dl>
<dt><code>squish_whitespace</code></dt><dd><p>Logical scalar; squish whitespace characters (using
<code><a href="stringr.html#topic+str_squish">str_squish</a></code>)?</p>
</dd>
<dt><code>remove_control_characters</code></dt><dd><p>Logical scalar; remove control characters?</p>
</dd>
<dt><code>remove_replacement_characters</code></dt><dd><p>Logical scalar; remove the &quot;replacement
character&quot;, <code>U-FFFD</code>?</p>
</dd>
<dt><code>remove_diacritics</code></dt><dd><p>Logical scalar; remove diacritical marks (accents,
etc) from characters?</p>
</dd>
<dt><code>space_cjk</code></dt><dd><p>Logical scalar; add spaces around Chinese/Japanese/Korean
ideographs?</p>
</dd>
<dt><code>space_punctuation</code></dt><dd><p>Logical scalar; add spaces around punctuation (to
make it easier to keep punctuation during tokenization)?</p>
</dd>
<dt><code>remove_terminal_hyphens</code></dt><dd><p>Logical; should hyphens at the end of lines
after a word be removed? For example, &quot;un-\nbroken&quot; would become
&quot;unbroken&quot;.</p>
</dd>
<dt><code>space_hyphens</code></dt><dd><p>Logical; treat hyphens between letters and at the
start/end of words as punctuation? Other hyphens are always treated as
punctuation.</p>
</dd>
<dt><code>space_abbreviations</code></dt><dd><p>Logical; treat apostrophes between letters as
punctuation? Other apostrophes are always treated as punctuation.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>The text as a list of character vectors. Each element of each vector
is roughly equivalent to a word.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prepare_and_tokenize("This is some text.")
prepare_and_tokenize("This is some text.", space_punctuation = FALSE)
</code></pre>

<hr>
<h2 id='prepare_text'>Prepare Text for Tokenization</h2><span id='topic+prepare_text'></span>

<h3>Description</h3>

<p>This function combines the other functions in this package to prepare text
for tokenization. The text gets converted to valid UTF-8 (if possible), and
then various cleaning functions are applied.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_text(
  text,
  squish_whitespace = TRUE,
  remove_terminal_hyphens = TRUE,
  remove_control_characters = TRUE,
  remove_replacement_characters = TRUE,
  remove_diacritics = TRUE,
  space_cjk = TRUE,
  space_punctuation = TRUE,
  space_hyphens = TRUE,
  space_abbreviations = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepare_text_+3A_text">text</code></td>
<td>
<p>A character vector to clean.</p>
</td></tr>
<tr><td><code id="prepare_text_+3A_squish_whitespace">squish_whitespace</code></td>
<td>
<p>Logical scalar; squish whitespace characters (using
<code><a href="stringr.html#topic+str_squish">str_squish</a></code>)?</p>
</td></tr>
<tr><td><code id="prepare_text_+3A_remove_terminal_hyphens">remove_terminal_hyphens</code></td>
<td>
<p>Logical; should hyphens at the end of lines
after a word be removed? For example, &quot;un-\nbroken&quot; would become
&quot;unbroken&quot;.</p>
</td></tr>
<tr><td><code id="prepare_text_+3A_remove_control_characters">remove_control_characters</code></td>
<td>
<p>Logical scalar; remove control characters?</p>
</td></tr>
<tr><td><code id="prepare_text_+3A_remove_replacement_characters">remove_replacement_characters</code></td>
<td>
<p>Logical scalar; remove the &quot;replacement
character&quot;, <code>U-FFFD</code>?</p>
</td></tr>
<tr><td><code id="prepare_text_+3A_remove_diacritics">remove_diacritics</code></td>
<td>
<p>Logical scalar; remove diacritical marks (accents,
etc) from characters?</p>
</td></tr>
<tr><td><code id="prepare_text_+3A_space_cjk">space_cjk</code></td>
<td>
<p>Logical scalar; add spaces around Chinese/Japanese/Korean
ideographs?</p>
</td></tr>
<tr><td><code id="prepare_text_+3A_space_punctuation">space_punctuation</code></td>
<td>
<p>Logical scalar; add spaces around punctuation (to
make it easier to keep punctuation during tokenization)?</p>
</td></tr>
<tr><td><code id="prepare_text_+3A_space_hyphens">space_hyphens</code></td>
<td>
<p>Logical; treat hyphens between letters and at the
start/end of words as punctuation? Other hyphens are always treated as
punctuation.</p>
</td></tr>
<tr><td><code id="prepare_text_+3A_space_abbreviations">space_abbreviations</code></td>
<td>
<p>Logical; treat apostrophes between letters as
punctuation? Other apostrophes are always treated as punctuation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The character vector, cleaned as specified.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>piece1 &lt;- " This is a    \n\nfa\xE7ile\n\n    example.\n"
# Specify encoding so this example behaves the same on all systems.
Encoding(piece1) &lt;- "latin1"
example_text &lt;- paste(
  piece1,
  "It has the bell character, \a, and the replacement character,",
  intToUtf8(65533)
)
prepare_text(example_text)
prepare_text(example_text, squish_whitespace = FALSE)
prepare_text(example_text, remove_control_characters = FALSE)
prepare_text(example_text, remove_replacement_characters = FALSE)
prepare_text(example_text, remove_diacritics = FALSE)
</code></pre>

<hr>
<h2 id='remove_control_characters'>Remove Non-Character Characters</h2><span id='topic+remove_control_characters'></span>

<h3>Description</h3>

<p>Unicode includes several control codes, such as <code>U+0000</code> (NULL, used in
null-terminated strings) and <code>U+000D</code> (carriage return). This function
removes all such characters from text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>remove_control_characters(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="remove_control_characters_+3A_text">text</code></td>
<td>
<p>A character vector to clean.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: We highly recommend that you first condense all space-like characters
(including new lines) before removing control codes. You can easily do so
with <code><a href="stringr.html#topic+str_squish">str_squish</a></code>. We also recommend validating text at
the start of any cleaning process using <code><a href="#topic+validate_utf8">validate_utf8</a></code>.
</p>


<h3>Value</h3>

<p>The character vector without control characters.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>remove_control_characters("Line 1\nLine2")
</code></pre>

<hr>
<h2 id='remove_diacritics'>Remove Diacritical Marks on Characters</h2><span id='topic+remove_diacritics'></span>

<h3>Description</h3>

<p>Accent characters and other diacritical marks are often difficult to type,
and thus can be missing from text. To normalize the various ways a user might
spell a word that should have a diacritical mark, you can convert all such
characters to their simpler equivalent character.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>remove_diacritics(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="remove_diacritics_+3A_text">text</code></td>
<td>
<p>A character vector to clean.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The character vector with simpler character representations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This text can appear differently between machines if we aren't careful, so
# we explicitly encode the desired characters.
sample_text &lt;- "fa\u00e7ile r\u00e9sum\u00e9"
sample_text
remove_diacritics(sample_text)
</code></pre>

<hr>
<h2 id='remove_replacement_characters'>Remove the Unicode Replacement Character</h2><span id='topic+remove_replacement_characters'></span>

<h3>Description</h3>

<p>The replacement character, <code>U+FFFD</code>, is used to mark characters that
could not be loaded. These characters might be a sign of encoding issues, so
it is advisable to investigate and try to eliminate any cases in your text,
but in the end these characters will almost definitely confuse downstream
processes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>remove_replacement_characters(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="remove_replacement_characters_+3A_text">text</code></td>
<td>
<p>A character vector to clean.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The character vector with replacement characters removed.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>remove_replacement_characters(
  paste(
    "The replacement character:",
    intToUtf8(65533)
  )
)
</code></pre>

<hr>
<h2 id='space_cjk'>Add Spaces Around CJK Ideographs</h2><span id='topic+space_cjk'></span>

<h3>Description</h3>

<p>To tokenize Chinese, Japanese, and Korean (CJK) characters, it's convenient
to add spaces around the characters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>space_cjk(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="space_cjk_+3A_text">text</code></td>
<td>
<p>A character vector to clean.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector the same length as the input text, with spaces
added between ideographs.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>to_space &lt;- intToUtf8(13312:13320)
to_space
space_cjk(to_space)
</code></pre>

<hr>
<h2 id='space_punctuation'>Add Spaces Around Punctuation</h2><span id='topic+space_punctuation'></span>

<h3>Description</h3>

<p>To keep punctuation during tokenization, it's convenient to add spacing
around punctuation. This function does that, with options to keep certain
types of punctuation together as part of the word.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>space_punctuation(text, space_hyphens = TRUE, space_abbreviations = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="space_punctuation_+3A_text">text</code></td>
<td>
<p>A character vector to clean.</p>
</td></tr>
<tr><td><code id="space_punctuation_+3A_space_hyphens">space_hyphens</code></td>
<td>
<p>Logical; treat hyphens between letters and at the
start/end of words as punctuation? Other hyphens are always treated as
punctuation.</p>
</td></tr>
<tr><td><code id="space_punctuation_+3A_space_abbreviations">space_abbreviations</code></td>
<td>
<p>Logical; treat apostrophes between letters as
punctuation? Other apostrophes are always treated as punctuation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector the same length as the input text, with spaces
added around punctuation characters.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>to_space &lt;- "This is some 'gosh-darn' $5 text. Isn't it lovely?"
to_space
space_punctuation(to_space)
space_punctuation(to_space, space_hyphens = FALSE)
space_punctuation(to_space, space_abbreviations = FALSE)
</code></pre>

<hr>
<h2 id='squish_whitespace'>Remove Extra Whitespace</h2><span id='topic+squish_whitespace'></span>

<h3>Description</h3>

<p>This function is mostly a wrapper around <code><a href="stringr.html#topic+str_squish">str_squish</a></code>,
with the additional option to remove hyphens at the ends of lines.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>squish_whitespace(text, remove_terminal_hyphens = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="squish_whitespace_+3A_text">text</code></td>
<td>
<p>A character vector to clean.</p>
</td></tr>
<tr><td><code id="squish_whitespace_+3A_remove_terminal_hyphens">remove_terminal_hyphens</code></td>
<td>
<p>Logical; should hyphens at the end of lines
after a word be removed? For example, &quot;un-\nbroken&quot; would become
&quot;unbroken&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The character vector with spacing at the start and end removed, and
with internal spacing reduced to a single space character each.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sample_text &lt;- "This  had many space char-\\nacters."
squish_whitespace(sample_text)
</code></pre>

<hr>
<h2 id='tokenize_space'>Break Text at Spaces</h2><span id='topic+tokenize_space'></span>

<h3>Description</h3>

<p>This is an extremely simple tokenizer, breaking only and exactly on the space
character. This tokenizer is intended to work in tandem with
<code><a href="#topic+prepare_text">prepare_text</a></code>, so that spaces are cleaned up and inserted as
necessary before the tokenizer runs. This function and
<code><a href="#topic+prepare_text">prepare_text</a></code> are combined together in
<code><a href="#topic+prepare_and_tokenize">prepare_and_tokenize</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize_space(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenize_space_+3A_text">text</code></td>
<td>
<p>A character vector to clean.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The text as a list of character vectors (one vector per element of
<code>text</code>). Each element of each vector is roughly equivalent to a word.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tokenize_space("This is some text.")
</code></pre>

<hr>
<h2 id='validate_utf8'>Clean Up Text to UTF-8</h2><span id='topic+validate_utf8'></span>

<h3>Description</h3>

<p>Text cleaning works best if the encoding is known. This function attempts to
convert text to UTF-8 encoding, and provides an informative error if that is
not possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate_utf8(text)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="validate_utf8_+3A_text">text</code></td>
<td>
<p>A character vector to clean.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The text with formal UTF-8 encoding, if possible.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>text &lt;- "fa\xE7ile"
# Specify the encoding so the example is the same on all systems.
Encoding(text) &lt;- "latin1"
validate_utf8(text)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
