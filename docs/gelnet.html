<!DOCTYPE html><html><head><title>Help for package gelnet</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {gelnet}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#adj2lapl'><p>Generate a graph Laplacian</p></a></li>
<li><a href='#adj2nlapl'><p>Generate a normalized graph Laplacian</p></a></li>
<li><a href='#gelnet'><p>GELnet for linear regression, binary classification and one-class problems.</p></a></li>
<li><a href='#gelnet.cv'><p>k-fold cross-validation for parameter tuning.</p></a></li>
<li><a href='#gelnet.ker'><p>Kernel models for linear regression, binary classification and one-class problems.</p></a></li>
<li><a href='#gelnet.lin.obj'><p>Linear regression objective function value</p></a></li>
<li><a href='#gelnet.logreg.obj'><p>Logistic regression objective function value</p></a></li>
<li><a href='#gelnet.oneclass.obj'><p>One-class regression objective function value</p></a></li>
<li><a href='#L1.ceiling'><p>The largest meaningful value of the L1 parameter</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.2.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2015-10-16</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Title:</td>
<td>Generalized Elastic Nets</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements several extensions of the elastic net regularization
    scheme. These extensions include individual feature penalties for the L1 term,
    feature-feature penalties for the L2 term, as well as translation coefficients
    for the latter.</td>
</tr>
<tr>
<td>Author:</td>
<td>Artem Sokolov</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Artem Sokolov &lt;artem.sokolov@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>5.0.1</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2016-04-04 23:16:50 UTC; sokolov</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2016-04-05 08:14:29</td>
</tr>
</table>
<hr>
<h2 id='adj2lapl'>Generate a graph Laplacian</h2><span id='topic+adj2lapl'></span>

<h3>Description</h3>

<p>Generates a graph Laplacian from the graph adjacency matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adj2lapl(A)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adj2lapl_+3A_a">A</code></td>
<td>
<p>n-by-n adjacency matrix for a graph with n nodes</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A graph Laplacian is defined as:
<code class="reqn"> l_{i,j} = deg( v_i ) </code>, if <code class="reqn"> i = j </code>;
<code class="reqn"> l_{i,j} = -1 </code>, if <code class="reqn"> i \neq j </code> and <code class="reqn">v_i</code> is adjacent to <code class="reqn">v_j</code>;
and <code class="reqn"> l_{i,j} = 0 </code>, otherwise
</p>


<h3>Value</h3>

<p>The n-by-n Laplacian matrix of the graph
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adj2nlapl">adj2nlapl</a></code>
</p>

<hr>
<h2 id='adj2nlapl'>Generate a normalized graph Laplacian</h2><span id='topic+adj2nlapl'></span>

<h3>Description</h3>

<p>Generates a normalized graph Laplacian from the graph adjacency matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adj2nlapl(A)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adj2nlapl_+3A_a">A</code></td>
<td>
<p>n-by-n adjacency matrix for a graph with n nodes</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A normalized graph Laplacian is defined as:
<code class="reqn"> l_{i,j} = 1 </code>, if <code class="reqn"> i = j </code>;
<code class="reqn"> l_{i,j} = - 1 / \sqrt{ deg(v_i) deg(v_j) } </code>, if <code class="reqn"> i \neq j </code> and <code class="reqn">v_i</code> is adjacent to <code class="reqn">v_j</code>;
and <code class="reqn"> l_{i,j} = 0 </code>, otherwise
</p>


<h3>Value</h3>

<p>The n-by-n Laplacian matrix of the graph
</p>


<h3>See Also</h3>

<p><code><a href="#topic+adj2nlapl">adj2nlapl</a></code>
</p>

<hr>
<h2 id='gelnet'>GELnet for linear regression, binary classification and one-class problems.</h2><span id='topic+gelnet'></span>

<h3>Description</h3>

<p>Infers the problem type and learns the appropriate GELnet model via coordinate descent.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gelnet(X, y, l1, l2, nFeats = NULL, a = rep(1, n), d = rep(1, p),
  P = diag(p), m = rep(0, p), max.iter = 100, eps = 1e-05,
  w.init = rep(0, p), b.init = NULL, fix.bias = FALSE, silent = FALSE,
  balanced = FALSE, nonneg = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gelnet_+3A_x">X</code></td>
<td>
<p>n-by-p matrix of n samples in p dimensions</p>
</td></tr>
<tr><td><code id="gelnet_+3A_y">y</code></td>
<td>
<p>n-by-1 vector of response values. Must be numeric vector for regression, factor with 2 levels for binary classification, or NULL for a one-class task.</p>
</td></tr>
<tr><td><code id="gelnet_+3A_l1">l1</code></td>
<td>
<p>coefficient for the L1-norm penalty</p>
</td></tr>
<tr><td><code id="gelnet_+3A_l2">l2</code></td>
<td>
<p>coefficient for the L2-norm penalty</p>
</td></tr>
<tr><td><code id="gelnet_+3A_nfeats">nFeats</code></td>
<td>
<p>alternative parameterization that returns the desired number of non-zero weights. Takes precedence over l1 if not NULL (default: NULL)</p>
</td></tr>
<tr><td><code id="gelnet_+3A_a">a</code></td>
<td>
<p>n-by-1 vector of sample weights (regression only)</p>
</td></tr>
<tr><td><code id="gelnet_+3A_d">d</code></td>
<td>
<p>p-by-1 vector of feature weights</p>
</td></tr>
<tr><td><code id="gelnet_+3A_p">P</code></td>
<td>
<p>p-by-p feature association penalty matrix</p>
</td></tr>
<tr><td><code id="gelnet_+3A_m">m</code></td>
<td>
<p>p-by-1 vector of translation coefficients</p>
</td></tr>
<tr><td><code id="gelnet_+3A_max.iter">max.iter</code></td>
<td>
<p>maximum number of iterations</p>
</td></tr>
<tr><td><code id="gelnet_+3A_eps">eps</code></td>
<td>
<p>convergence precision</p>
</td></tr>
<tr><td><code id="gelnet_+3A_w.init">w.init</code></td>
<td>
<p>initial parameter estimate for the weights</p>
</td></tr>
<tr><td><code id="gelnet_+3A_b.init">b.init</code></td>
<td>
<p>initial parameter estimate for the bias term</p>
</td></tr>
<tr><td><code id="gelnet_+3A_fix.bias">fix.bias</code></td>
<td>
<p>set to TRUE to prevent the bias term from being updated (regression only) (default: FALSE)</p>
</td></tr>
<tr><td><code id="gelnet_+3A_silent">silent</code></td>
<td>
<p>set to TRUE to suppress run-time output to stdout (default: FALSE)</p>
</td></tr>
<tr><td><code id="gelnet_+3A_balanced">balanced</code></td>
<td>
<p>boolean specifying whether the balanced model is being trained (binary classification only) (default: FALSE)</p>
</td></tr>
<tr><td><code id="gelnet_+3A_nonneg">nonneg</code></td>
<td>
<p>set to TRUE to enforce non-negativity constraints on the weights (default: FALSE )</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The method determines the problem type from the labels argument y.
If y is a numeric vector, then a regression model is trained by optimizing the following objective function:
</p>
<p style="text-align: center;"><code class="reqn"> \frac{1}{2n} \sum_i a_i (y_i - (w^T x_i + b))^2 + R(w) </code>
</p>

<p>If y is a factor with two levels, then the function returns a binary classification model, obtained by optimizing the following objective function:
</p>
<p style="text-align: center;"><code class="reqn"> -\frac{1}{n} \sum_i y_i s_i - \log( 1 + \exp(s_i) ) + R(w) </code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn"> s_i = w^T x_i + b </code>
</p>

<p>Finally, if no labels are provided (y == NULL), then a one-class model is constructed using the following objective function:
</p>
<p style="text-align: center;"><code class="reqn"> -\frac{1}{n} \sum_i s_i - \log( 1 + \exp(s_i) ) + R(w) </code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn"> s_i = w^T x_i </code>
</p>

<p>In all cases, the regularizer is defined by
</p>
<p style="text-align: center;"><code class="reqn"> R(w) = \lambda_1 \sum_j d_j |w_j| + \frac{\lambda_2}{2} (w-m)^T P (w-m) </code>
</p>

<p>The training itself is performed through cyclical coordinate descent, and the optimization is terminated after the desired tolerance is achieved or after a maximum number of iterations.
</p>


<h3>Value</h3>

<p>A list with two elements:
</p>

<dl>
<dt>w</dt><dd><p>p-by-1 vector of p model weights</p>
</dd>
<dt>b</dt><dd><p>scalar, bias term for the linear model (omitted for one-class models)</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+gelnet.lin.obj">gelnet.lin.obj</a></code>, <code><a href="#topic+gelnet.logreg.obj">gelnet.logreg.obj</a></code>, <code><a href="#topic+gelnet.oneclass.obj">gelnet.oneclass.obj</a></code>
</p>

<hr>
<h2 id='gelnet.cv'>k-fold cross-validation for parameter tuning.</h2><span id='topic+gelnet.cv'></span>

<h3>Description</h3>

<p>Performs k-fold cross-validation to select the best pair of the L1- and L2-norm penalty values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gelnet.cv(X, y, nL1, nL2, nFolds = 5, a = rep(1, n), d = rep(1, p),
  P = diag(p), m = rep(0, p), max.iter = 100, eps = 1e-05,
  w.init = rep(0, p), b.init = 0, fix.bias = FALSE, silent = FALSE,
  balanced = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gelnet.cv_+3A_x">X</code></td>
<td>
<p>n-by-p matrix of n samples in p dimensions</p>
</td></tr>
<tr><td><code id="gelnet.cv_+3A_y">y</code></td>
<td>
<p>n-by-1 vector of response values. Must be numeric vector for regression, factor with 2 levels for binary classification, or NULL for a one-class task.</p>
</td></tr>
<tr><td><code id="gelnet.cv_+3A_nl1">nL1</code></td>
<td>
<p>number of values to consider for the L1-norm penalty</p>
</td></tr>
<tr><td><code id="gelnet.cv_+3A_nl2">nL2</code></td>
<td>
<p>number of values to consider for the L2-norm penalty</p>
</td></tr>
<tr><td><code id="gelnet.cv_+3A_nfolds">nFolds</code></td>
<td>
<p>number of cross-validation folds (default:5)</p>
</td></tr>
<tr><td><code id="gelnet.cv_+3A_a">a</code></td>
<td>
<p>n-by-1 vector of sample weights (regression only)</p>
</td></tr>
<tr><td><code id="gelnet.cv_+3A_d">d</code></td>
<td>
<p>p-by-1 vector of feature weights</p>
</td></tr>
<tr><td><code id="gelnet.cv_+3A_p">P</code></td>
<td>
<p>p-by-p feature association penalty matrix</p>
</td></tr>
<tr><td><code id="gelnet.cv_+3A_m">m</code></td>
<td>
<p>p-by-1 vector of translation coefficients</p>
</td></tr>
<tr><td><code id="gelnet.cv_+3A_max.iter">max.iter</code></td>
<td>
<p>maximum number of iterations</p>
</td></tr>
<tr><td><code id="gelnet.cv_+3A_eps">eps</code></td>
<td>
<p>convergence precision</p>
</td></tr>
<tr><td><code id="gelnet.cv_+3A_w.init">w.init</code></td>
<td>
<p>initial parameter estimate for the weights</p>
</td></tr>
<tr><td><code id="gelnet.cv_+3A_b.init">b.init</code></td>
<td>
<p>initial parameter estimate for the bias term</p>
</td></tr>
<tr><td><code id="gelnet.cv_+3A_fix.bias">fix.bias</code></td>
<td>
<p>set to TRUE to prevent the bias term from being updated (regression only) (default: FALSE)</p>
</td></tr>
<tr><td><code id="gelnet.cv_+3A_silent">silent</code></td>
<td>
<p>set to TRUE to suppress run-time output to stdout (default: FALSE)</p>
</td></tr>
<tr><td><code id="gelnet.cv_+3A_balanced">balanced</code></td>
<td>
<p>boolean specifying whether the balanced model is being trained (binary classification only) (default: FALSE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Cross-validation is performed on a grid of parameter values. The user specifies the number of values
to consider for both the L1- and the L2-norm penalties. The L1 grid values are equally spaced on
[0, L1s], where L1s is the smallest meaningful value of the L1-norm penalty (i.e., where all the model
weights are just barely zero). The L2 grid values are on a logarithmic scale centered on 1.
</p>


<h3>Value</h3>

<p>A list with the following elements:
</p>

<dl>
<dt>l1</dt><dd><p>the best value of the L1-norm penalty</p>
</dd>
<dt>l2</dt><dd><p>the best value of the L2-norm penalty</p>
</dd>
<dt>w</dt><dd><p>p-by-1 vector of p model weights associated with the best (l1,l2) pair.</p>
</dd>
<dt>b</dt><dd><p>scalar, bias term for the linear model associated with the best (l1,l2) pair. (omitted for one-class models)</p>
</dd>
<dt>perf</dt><dd><p>performance value associated with the best model. (Likelihood of data for one-class, AUC for binary classification, and -RMSE for regression)</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+gelnet">gelnet</a></code>
</p>

<hr>
<h2 id='gelnet.ker'>Kernel models for linear regression, binary classification and one-class problems.</h2><span id='topic+gelnet.ker'></span>

<h3>Description</h3>

<p>Infers the problem type and learns the appropriate kernel model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gelnet.ker(K, y, lambda, a, max.iter = 100, eps = 1e-05, v.init = rep(0,
  nrow(K)), b.init = 0, fix.bias = FALSE, silent = FALSE,
  balanced = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gelnet.ker_+3A_k">K</code></td>
<td>
<p>n-by-n matrix of pairwise kernel values over a set of n samples</p>
</td></tr>
<tr><td><code id="gelnet.ker_+3A_y">y</code></td>
<td>
<p>n-by-1 vector of response values. Must be numeric vector for regression, factor with 2 levels for binary classification, or NULL for a one-class task.</p>
</td></tr>
<tr><td><code id="gelnet.ker_+3A_lambda">lambda</code></td>
<td>
<p>scalar, regularization parameter</p>
</td></tr>
<tr><td><code id="gelnet.ker_+3A_a">a</code></td>
<td>
<p>n-by-1 vector of sample weights (regression only)</p>
</td></tr>
<tr><td><code id="gelnet.ker_+3A_max.iter">max.iter</code></td>
<td>
<p>maximum number of iterations (binary classification and one-class problems only)</p>
</td></tr>
<tr><td><code id="gelnet.ker_+3A_eps">eps</code></td>
<td>
<p>convergence precision (binary classification and one-class problems only)</p>
</td></tr>
<tr><td><code id="gelnet.ker_+3A_v.init">v.init</code></td>
<td>
<p>initial parameter estimate for the kernel weights (binary classification and one-class problems only)</p>
</td></tr>
<tr><td><code id="gelnet.ker_+3A_b.init">b.init</code></td>
<td>
<p>initial parameter estimate for the bias term (binary classification only)</p>
</td></tr>
<tr><td><code id="gelnet.ker_+3A_fix.bias">fix.bias</code></td>
<td>
<p>set to TRUE to prevent the bias term from being updated (regression only) (default: FALSE)</p>
</td></tr>
<tr><td><code id="gelnet.ker_+3A_silent">silent</code></td>
<td>
<p>set to TRUE to suppress run-time output to stdout (default: FALSE)</p>
</td></tr>
<tr><td><code id="gelnet.ker_+3A_balanced">balanced</code></td>
<td>
<p>boolean specifying whether the balanced model is being trained (binary classification only) (default: FALSE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The entries in the kernel matrix K can be interpreted as dot products
in some feature space <code class="reqn">\phi</code>. The corresponding weight vector can be
retrieved via <code class="reqn">w = \sum_i v_i \phi(x_i)</code>. However, new samples can be
classified without explicit access to the underlying feature space:
</p>
<p style="text-align: center;"><code class="reqn">w^T \phi(x) + b = \sum_i v_i \phi^T (x_i) \phi(x) + b = \sum_i v_i K( x_i, x ) + b</code>
</p>

<p>The method determines the problem type from the labels argument y.
If y is a numeric vector, then a ridge regression model is trained by optimizing the following objective function:
</p>
<p style="text-align: center;"><code class="reqn"> \frac{1}{2n} \sum_i a_i (z_i - (w^T x_i + b))^2 + w^Tw </code>
</p>

<p>If y is a factor with two levels, then the function returns a binary classification model, obtained by optimizing the following objective function:
</p>
<p style="text-align: center;"><code class="reqn"> -\frac{1}{n} \sum_i y_i s_i - \log( 1 + \exp(s_i) ) + w^Tw </code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn"> s_i = w^T x_i + b </code>
</p>

<p>Finally, if no labels are provided (y == NULL), then a one-class model is constructed using the following objective function:
</p>
<p style="text-align: center;"><code class="reqn"> -\frac{1}{n} \sum_i s_i - \log( 1 + \exp(s_i) ) + w^Tw </code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn"> s_i = w^T x_i </code>
</p>

<p>In all cases, <code class="reqn">w = \sum_i v_i \phi(x_i)</code> and the method solves for <code class="reqn">v_i</code>.
</p>


<h3>Value</h3>

<p>A list with two elements:
</p>

<dl>
<dt>v</dt><dd><p>n-by-1 vector of kernel weights</p>
</dd>
<dt>b</dt><dd><p>scalar, bias term for the linear model (omitted for one-class models)</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+gelnet">gelnet</a></code>
</p>

<hr>
<h2 id='gelnet.lin.obj'>Linear regression objective function value</h2><span id='topic+gelnet.lin.obj'></span>

<h3>Description</h3>

<p>Evaluates the linear regression objective function value for a given model.
See details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gelnet.lin.obj(w, b, X, z, l1, l2, a = rep(1, nrow(X)), d = rep(1, ncol(X)),
  P = diag(ncol(X)), m = rep(0, ncol(X)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gelnet.lin.obj_+3A_w">w</code></td>
<td>
<p>p-by-1 vector of model weights</p>
</td></tr>
<tr><td><code id="gelnet.lin.obj_+3A_b">b</code></td>
<td>
<p>the model bias term</p>
</td></tr>
<tr><td><code id="gelnet.lin.obj_+3A_x">X</code></td>
<td>
<p>n-by-p matrix of n samples in p dimensions</p>
</td></tr>
<tr><td><code id="gelnet.lin.obj_+3A_z">z</code></td>
<td>
<p>n-by-1 response vector</p>
</td></tr>
<tr><td><code id="gelnet.lin.obj_+3A_l1">l1</code></td>
<td>
<p>L1-norm penalty scaling factor <code class="reqn">\lambda_1</code></p>
</td></tr>
<tr><td><code id="gelnet.lin.obj_+3A_l2">l2</code></td>
<td>
<p>L2-norm penalty scaling factor <code class="reqn">\lambda_2</code></p>
</td></tr>
<tr><td><code id="gelnet.lin.obj_+3A_a">a</code></td>
<td>
<p>n-by-1 vector of sample weights</p>
</td></tr>
<tr><td><code id="gelnet.lin.obj_+3A_d">d</code></td>
<td>
<p>p-by-1 vector of feature weights</p>
</td></tr>
<tr><td><code id="gelnet.lin.obj_+3A_p">P</code></td>
<td>
<p>p-by-p feature-feature penalty matrix</p>
</td></tr>
<tr><td><code id="gelnet.lin.obj_+3A_m">m</code></td>
<td>
<p>p-by-1 vector of translation coefficients</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the objective function value according to
</p>
<p style="text-align: center;"><code class="reqn"> \frac{1}{2n} \sum_i a_i (z_i - (w^T x_i + b))^2 + R(w) </code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn"> R(w) = \lambda_1 \sum_j d_j |w_j| + \frac{\lambda_2}{2} (w-m)^T P (w-m) </code>
</p>



<h3>Value</h3>

<p>The objective function value.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gelnet">gelnet</a></code>
</p>

<hr>
<h2 id='gelnet.logreg.obj'>Logistic regression objective function value</h2><span id='topic+gelnet.logreg.obj'></span>

<h3>Description</h3>

<p>Evaluates the logistic regression objective function value for a given model.
See details.
Computes the objective function value according to
</p>
<p style="text-align: center;"><code class="reqn"> -\frac{1}{n} \sum_i y_i s_i - \log( 1 + \exp(s_i) ) + R(w) </code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn"> s_i = w^T x_i + b </code>
</p>

<p style="text-align: center;"><code class="reqn"> R(w) = \lambda_1 \sum_j d_j |w_j| + \frac{\lambda_2}{2} (w-m)^T P (w-m) </code>
</p>

<p>When balanced is TRUE, the loss average over the entire data is replaced with averaging
over each class separately. The total loss is then computes as the mean over those
per-class estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gelnet.logreg.obj(w, b, X, y, l1, l2, d = rep(1, ncol(X)),
  P = diag(ncol(X)), m = rep(0, ncol(X)), balanced = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gelnet.logreg.obj_+3A_w">w</code></td>
<td>
<p>p-by-1 vector of model weights</p>
</td></tr>
<tr><td><code id="gelnet.logreg.obj_+3A_b">b</code></td>
<td>
<p>the model bias term</p>
</td></tr>
<tr><td><code id="gelnet.logreg.obj_+3A_x">X</code></td>
<td>
<p>n-by-p matrix of n samples in p dimensions</p>
</td></tr>
<tr><td><code id="gelnet.logreg.obj_+3A_y">y</code></td>
<td>
<p>n-by-1 binary response vector sampled from 0,1</p>
</td></tr>
<tr><td><code id="gelnet.logreg.obj_+3A_l1">l1</code></td>
<td>
<p>L1-norm penalty scaling factor <code class="reqn">\lambda_1</code></p>
</td></tr>
<tr><td><code id="gelnet.logreg.obj_+3A_l2">l2</code></td>
<td>
<p>L2-norm penalty scaling factor <code class="reqn">\lambda_2</code></p>
</td></tr>
<tr><td><code id="gelnet.logreg.obj_+3A_d">d</code></td>
<td>
<p>p-by-1 vector of feature weights</p>
</td></tr>
<tr><td><code id="gelnet.logreg.obj_+3A_p">P</code></td>
<td>
<p>p-by-p feature-feature penalty matrix</p>
</td></tr>
<tr><td><code id="gelnet.logreg.obj_+3A_m">m</code></td>
<td>
<p>p-by-1 vector of translation coefficients</p>
</td></tr>
<tr><td><code id="gelnet.logreg.obj_+3A_balanced">balanced</code></td>
<td>
<p>boolean specifying whether the balanced model is being evaluated</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The objective function value.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gelnet">gelnet</a></code>
</p>

<hr>
<h2 id='gelnet.oneclass.obj'>One-class regression objective function value</h2><span id='topic+gelnet.oneclass.obj'></span>

<h3>Description</h3>

<p>Evaluates the one-class objective function value for a given model
See details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gelnet.oneclass.obj(w, X, l1, l2, d = rep(1, ncol(X)), P = diag(ncol(X)),
  m = rep(0, ncol(X)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gelnet.oneclass.obj_+3A_w">w</code></td>
<td>
<p>p-by-1 vector of model weights</p>
</td></tr>
<tr><td><code id="gelnet.oneclass.obj_+3A_x">X</code></td>
<td>
<p>n-by-p matrix of n samples in p dimensions</p>
</td></tr>
<tr><td><code id="gelnet.oneclass.obj_+3A_l1">l1</code></td>
<td>
<p>L1-norm penalty scaling factor <code class="reqn">\lambda_1</code></p>
</td></tr>
<tr><td><code id="gelnet.oneclass.obj_+3A_l2">l2</code></td>
<td>
<p>L2-norm penalty scaling factor <code class="reqn">\lambda_2</code></p>
</td></tr>
<tr><td><code id="gelnet.oneclass.obj_+3A_d">d</code></td>
<td>
<p>p-by-1 vector of feature weights</p>
</td></tr>
<tr><td><code id="gelnet.oneclass.obj_+3A_p">P</code></td>
<td>
<p>p-by-p feature-feature penalty matrix</p>
</td></tr>
<tr><td><code id="gelnet.oneclass.obj_+3A_m">m</code></td>
<td>
<p>p-by-1 vector of translation coefficients</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the objective function value according to
</p>
<p style="text-align: center;"><code class="reqn"> -\frac{1}{n} \sum_i s_i - \log( 1 + \exp(s_i) ) + R(w) </code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn"> s_i = w^T x_i </code>
</p>

<p style="text-align: center;"><code class="reqn"> R(w) = \lambda_1 \sum_j d_j |w_j| + \frac{\lambda_2}{2} (w-m)^T P (w-m) </code>
</p>



<h3>Value</h3>

<p>The objective function value.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gelnet">gelnet</a></code>
</p>

<hr>
<h2 id='L1.ceiling'>The largest meaningful value of the L1 parameter</h2><span id='topic+L1.ceiling'></span>

<h3>Description</h3>

<p>Computes the smallest value of the LASSO coefficient L1 that leads to an
all-zero weight vector for a given linear regression problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>L1.ceiling(X, y, a = rep(1, nrow(X)), d = rep(1, ncol(X)),
  P = diag(ncol(X)), m = rep(0, ncol(X)), l2 = 1, balanced = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="L1.ceiling_+3A_x">X</code></td>
<td>
<p>n-by-p matrix of n samples in p dimensions</p>
</td></tr>
<tr><td><code id="L1.ceiling_+3A_y">y</code></td>
<td>
<p>n-by-1 vector of response values. Must be numeric vector for regression, factor with 2 levels for binary classification, or NULL for a one-class task.</p>
</td></tr>
<tr><td><code id="L1.ceiling_+3A_a">a</code></td>
<td>
<p>n-by-1 vector of sample weights (regression only)</p>
</td></tr>
<tr><td><code id="L1.ceiling_+3A_d">d</code></td>
<td>
<p>p-by-1 vector of feature weights</p>
</td></tr>
<tr><td><code id="L1.ceiling_+3A_p">P</code></td>
<td>
<p>p-by-p feature association penalty matrix</p>
</td></tr>
<tr><td><code id="L1.ceiling_+3A_m">m</code></td>
<td>
<p>p-by-1 vector of translation coefficients</p>
</td></tr>
<tr><td><code id="L1.ceiling_+3A_l2">l2</code></td>
<td>
<p>coefficient for the L2-norm penalty</p>
</td></tr>
<tr><td><code id="L1.ceiling_+3A_balanced">balanced</code></td>
<td>
<p>boolean specifying whether the balanced model is being trained (binary classification only) (default: FALSE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cyclic coordinate descent updates the model weight <code class="reqn">w_k</code> using a soft threshold operator
<code class="reqn"> S( \cdot, \lambda_1 d_k ) </code> that clips the value of the weight to zero, whenever the absolute
value of the first argument falls below <code class="reqn">\lambda_1 d_k</code>. From here, it is straightforward to compute
the smallest value of <code class="reqn">\lambda_1</code>, such that all weights are clipped to zero.
</p>


<h3>Value</h3>

<p>The largest meaningful value of the L1 parameter (i.e., the smallest value that yields a model with all zero weights)
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
