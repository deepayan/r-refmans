<!DOCTYPE html><html><head><title>Help for package plotHMM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {plotHMM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#backward_interface'><p>Backward algorithm</p></a></li>
<li><a href='#buggy.5states'>
<p>Buggy data with 5 states</p></a></li>
<li><a href='#buggy.data'>
<p>Buggy data with one state</p></a></li>
<li><a href='#eln'><p>Log probability arithmetic</p></a></li>
<li><a href='#forward_interface'><p>Forward algorithm</p></a></li>
<li><a href='#multiply_interface'><p>Multiply algorithm</p></a></li>
<li><a href='#pairwise_interface'><p>Pairwise algorithm</p></a></li>
<li><a href='#transition_interface'><p>Transition algorithm</p></a></li>
<li><a href='#viterbi_interface'><p>Viterbi algorithm</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Plot Hidden Markov Models</td>
</tr>
<tr>
<td>Version:</td>
<td>2023.8.28</td>
</tr>
<tr>
<td>Description:</td>
<td>Hidden Markov Models are useful for modeling
 sequential data. This package provides several functions
 implemented in C++ for explaining the algorithms used for
 Hidden Markov Models (forward, backward, decoding,
 learning).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.7)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, markdown, R.utils, covr, depmixS4,
data.table, ggplot2, neuroblastoma, microbenchmark</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-09-05 04:12:33 UTC; tdhock</td>
</tr>
<tr>
<td>Author:</td>
<td>Toby Hocking [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Toby Hocking &lt;toby.hocking@r-project.org&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-09-05 22:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='backward_interface'>Backward algorithm</h2><span id='topic+backward_interface'></span>

<h3>Description</h3>

<p>Efficient implementation of backward algorithm in C++ code,
for N data and S states.</p>


<h3>Usage</h3>

<pre><code class='language-R'>backward_interface(
  log_emission_mat, log_transition_mat)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="backward_interface_+3A_log_emission_mat">log_emission_mat</code></td>
<td>
<p>N x S numeric matrix of log likelihood of
observing each data point in each state.</p>
</td></tr>
<tr><td><code id="backward_interface_+3A_log_transition_mat">log_transition_mat</code></td>
<td>
<p>S x S numeric matrix; log_transition_mat[i,j] is the
log probability of going from state i to state j.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>N x S numeric matrix of backward log likelihood.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##simulated data.
seg.mean.vec &lt;- c(2, 0, -1, 0)
data.mean.vec &lt;- rep(seg.mean.vec, each=10)
set.seed(1)
N.data &lt;- length(data.mean.vec)
y.vec &lt;- rnorm(N.data, data.mean.vec)
##model.
n.states &lt;- 3
log.A.mat &lt;- log(matrix(1/n.states, n.states, n.states))
state.mean.vec &lt;- c(-1, 0, 1)*0.1
sd.param &lt;- 1
log.emission.mat &lt;- dnorm(
  y.vec,
  matrix(state.mean.vec, N.data, n.states, byrow=TRUE),
  sd.param,
  log=TRUE)
plotHMM::backward_interface(log.emission.mat, log.A.mat)

</code></pre>

<hr>
<h2 id='buggy.5states'>
Buggy data with 5 states
</h2><span id='topic+buggy.5states'></span>

<h3>Description</h3>

<p>Data was observed to error with depmixS4 and five states
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("buggy.5states")</code></pre>


<h3>Format</h3>

<p>The format is a data table.
</p>

<hr>
<h2 id='buggy.data'>
Buggy data with one state
</h2><span id='topic+buggy.data'></span>

<h3>Description</h3>

<p>This data set was known to produce an error with depmixS4 using one state.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("buggy.data")</code></pre>


<h3>Format</h3>

<p>The format is a data table.
</p>

<hr>
<h2 id='eln'>Log probability arithmetic</h2><span id='topic+elnproduct'></span><span id='topic+elnsum'></span><span id='topic+logsumexp'></span>

<h3>Description</h3>

<p>Binary operators in log probability space, to avoid
numerical underflow.</p>


<h3>Usage</h3>

<pre><code class='language-R'>elnproduct(elnx, elny)
elnsum(elnx, elny)
logsumexp(exponents.vec)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="eln_+3A_elnx">elnx</code>, <code id="eln_+3A_elny">elny</code>, <code id="eln_+3A_exponents.vec">exponents.vec</code></td>
<td>
<p>numeric vectors of log probabilities.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric vector with one (logsumexp) or more (others) log
probability value(s).</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>References</h3>

<p><a href="http://bozeman.genome.washington.edu/compbio/mbt599_2006/hmm_scaling_revised.pdf">http://bozeman.genome.washington.edu/compbio/mbt599_2006/hmm_scaling_revised.pdf</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>
px &lt;- c(0.1, 0.5, 0.9)
py &lt;- c(0.001, 0.123, 0.999)
lx &lt;- log(px)
ly &lt;- log(py)
library(plotHMM)
elnproduct(lx, ly)
elnsum(lx, ly)
logsumexp(ly)

</code></pre>

<hr>
<h2 id='forward_interface'>Forward algorithm</h2><span id='topic+forward_interface'></span>

<h3>Description</h3>

<p>Efficient implementation of forward algorithm in C++ code,
for N data and S states.</p>


<h3>Usage</h3>

<pre><code class='language-R'>forward_interface(
  log_emission_mat, log_transition_mat, log_initial_prob_vec)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="forward_interface_+3A_log_emission_mat">log_emission_mat</code></td>
<td>
<p>N x S numeric matrix of log likelihood of
observing each data point in each state.</p>
</td></tr>
<tr><td><code id="forward_interface_+3A_log_transition_mat">log_transition_mat</code></td>
<td>
<p>S x S numeric matrix; log_transition_mat[i,j] is the
log probability of going from state i to state j.</p>
</td></tr>
<tr><td><code id="forward_interface_+3A_log_initial_prob_vec">log_initial_prob_vec</code></td>
<td>
<p>S numeric vector of log probabilities of observing
each state at the beginning of the sequence.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with two elements
</p>
<table>
<tr><td><code>log_alpha</code></td>
<td>
<p>N x S numeric matrix of forward log likelihood at each data/state.</p>
</td></tr>
<tr><td><code>log_lik</code></td>
<td>
<p>numeric scalar total log likelihood of data given model parameters.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##simulated data.
seg.mean.vec &lt;- c(2, 0, -1, 0)
data.mean.vec &lt;- rep(seg.mean.vec, each=10)
set.seed(1)
N.data &lt;- length(data.mean.vec)
y.vec &lt;- rnorm(N.data, data.mean.vec)
##model.
n.states &lt;- 3
log.A.mat &lt;- log(matrix(1/n.states, n.states, n.states))
state.mean.vec &lt;- c(-1, 0, 1)*0.1
sd.param &lt;- 1
log.pi.vec &lt;- log(rep(1/n.states, n.states))
log.emission.mat &lt;- dnorm(
  y.vec,
  matrix(state.mean.vec, N.data, n.states, byrow=TRUE),
  sd.param,
  log=TRUE)
plotHMM::forward_interface(log.emission.mat, log.A.mat, log.pi.vec)

</code></pre>

<hr>
<h2 id='multiply_interface'>Multiply algorithm</h2><span id='topic+multiply_interface'></span>

<h3>Description</h3>

<p>Efficient implementation of multiply algorithm in C++ code,
for N data and S states.</p>


<h3>Usage</h3>

<pre><code class='language-R'>multiply_interface(
  log_alpha_mat, log_beta_mat)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multiply_interface_+3A_log_alpha_mat">log_alpha_mat</code>, <code id="multiply_interface_+3A_log_beta_mat">log_beta_mat</code></td>
<td>
<p>N x S numeric matrices of log
probabilities, from forward and backward algorithms.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>N x S numeric matrix of overall log likelihood.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##simulated data.
seg.mean.vec &lt;- c(2, 0, -1, 0)
data.mean.vec &lt;- rep(seg.mean.vec, each=10)
set.seed(1)
N.data &lt;- length(data.mean.vec)
y.vec &lt;- rnorm(N.data, data.mean.vec)
##model.
n.states &lt;- 3
log.A.mat &lt;- log(matrix(1/n.states, n.states, n.states))
state.mean.vec &lt;- c(-1, 0, 1)*0.1
sd.param &lt;- 1
log.emission.mat &lt;- dnorm(
  y.vec,
  matrix(state.mean.vec, N.data, n.states, byrow=TRUE),
  sd.param,
  log=TRUE)
log.pi.vec &lt;- log(rep(1/n.states, n.states))
f.list &lt;- plotHMM::forward_interface(log.emission.mat, log.A.mat, log.pi.vec)
b.mat &lt;- plotHMM::backward_interface(log.emission.mat, log.A.mat)
log.gamma.mat &lt;- plotHMM::multiply_interface(f.list$log_alpha, b.mat)
prob.mat &lt;- exp(log.gamma.mat)
rowSums(prob.mat)

</code></pre>

<hr>
<h2 id='pairwise_interface'>Pairwise algorithm</h2><span id='topic+pairwise_interface'></span>

<h3>Description</h3>

<p>Efficient implementation of pairwise algorithm in C++ code,
for N data and S states.</p>


<h3>Usage</h3>

<pre><code class='language-R'>pairwise_interface(
  log_emission_mat, log_transition_mat, log_alpha_mat, log_beta_mat)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pairwise_interface_+3A_log_emission_mat">log_emission_mat</code>, <code id="pairwise_interface_+3A_log_alpha_mat">log_alpha_mat</code>, <code id="pairwise_interface_+3A_log_beta_mat">log_beta_mat</code></td>
<td>

<p>N x S numeric matrices of log likelihood.</p>
</td></tr>
<tr><td><code id="pairwise_interface_+3A_log_transition_mat">log_transition_mat</code></td>
<td>
<p>S x S numeric matrix; log_transition_mat[i,j] is the
log probability of going from state i to state j.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>S x S x N-1 numeric array of log likelihood.
</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##simulated data.
seg.mean.vec &lt;- c(2, 0, -1, 0)
data.mean.vec &lt;- rep(seg.mean.vec, each=10)
set.seed(1)
N.data &lt;- length(data.mean.vec)
y.vec &lt;- rnorm(N.data, data.mean.vec)
##model.
n.states &lt;- 3
log.A.mat &lt;- log(matrix(1/n.states, n.states, n.states))
state.mean.vec &lt;- c(-1, 0, 1)*0.1
sd.param &lt;- 1
log.emission.mat &lt;- dnorm(
  y.vec,
  matrix(state.mean.vec, N.data, n.states, byrow=TRUE),
  sd.param,
  log=TRUE)
log.pi.vec &lt;- log(rep(1/n.states, n.states))
f.list &lt;- plotHMM::forward_interface(log.emission.mat, log.A.mat, log.pi.vec)
b.mat &lt;- plotHMM::backward_interface(log.emission.mat, log.A.mat)
log.gamma.mat &lt;- plotHMM::multiply_interface(f.list$log_alpha, b.mat)
prob.mat &lt;- exp(log.gamma.mat)
plotHMM::pairwise_interface(log.emission.mat, log.A.mat, f.list$log_alpha, b.mat)

</code></pre>

<hr>
<h2 id='transition_interface'>Transition algorithm</h2><span id='topic+transition_interface'></span>

<h3>Description</h3>

<p>Efficient implementation of transition algorithm in C++ code,
for T transitions and S states.</p>


<h3>Usage</h3>

<pre><code class='language-R'>transition_interface(
  log_gamma_mat, log_xi_array)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="transition_interface_+3A_log_gamma_mat">log_gamma_mat</code></td>
<td>

<p>T x S numeric matrix, taken by removing the last row from the log
probabilities from multiply.</p>
</td></tr>
<tr><td><code id="transition_interface_+3A_log_xi_array">log_xi_array</code></td>
<td>
<p>S x S x T numeric array of log probabilities
from pairwise.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>S x S numeric array of log probabilities (new transition matrix).
</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##simulated data.
seg.mean.vec &lt;- c(2, 0, -1, 0)
data.mean.vec &lt;- rep(seg.mean.vec, each=10)
set.seed(1)
N.data &lt;- length(data.mean.vec)
y.vec &lt;- rnorm(N.data, data.mean.vec)
##model.
n.states &lt;- 3
log.A.mat &lt;- log(matrix(1/n.states, n.states, n.states))
state.mean.vec &lt;- c(-1, 0, 1)*0.1
sd.param &lt;- 1
log.emission.mat &lt;- dnorm(
  y.vec,
  matrix(state.mean.vec, N.data, n.states, byrow=TRUE),
  sd.param,
  log=TRUE)
log.pi.vec &lt;- log(rep(1/n.states, n.states))
f.list &lt;- plotHMM::forward_interface(log.emission.mat, log.A.mat, log.pi.vec)
b.mat &lt;- plotHMM::backward_interface(log.emission.mat, log.A.mat)
log.gamma.mat &lt;- plotHMM::multiply_interface(f.list$log_alpha, b.mat)
prob.mat &lt;- exp(log.gamma.mat)
log.xi.array &lt;- plotHMM::pairwise_interface(
  log.emission.mat, log.A.mat, f.list$log_alpha, b.mat)
plotHMM::transition_interface(log.gamma.mat[-N.data,], log.xi.array)

</code></pre>

<hr>
<h2 id='viterbi_interface'>Viterbi algorithm</h2><span id='topic+viterbi_interface'></span>

<h3>Description</h3>

<p>Efficient implementation of Viterbi algorithm in C++ code,
for N data and S states.</p>


<h3>Usage</h3>

<pre><code class='language-R'>viterbi_interface(
  log_emission_mat, log_transition_mat, log_initial_prob_vec)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="viterbi_interface_+3A_log_emission_mat">log_emission_mat</code></td>
<td>
<p>N x S numeric matrix of log likelihood of
observing each data point in each state.</p>
</td></tr>
<tr><td><code id="viterbi_interface_+3A_log_transition_mat">log_transition_mat</code></td>
<td>
<p>S x S numeric matrix; log_transition_mat[i,j] is the
log probability of going from state i to state j.</p>
</td></tr>
<tr><td><code id="viterbi_interface_+3A_log_initial_prob_vec">log_initial_prob_vec</code></td>
<td>
<p>S numeric vector of log probabilities of observing
each state at the beginning of the sequence.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with elements
</p>
<table>
<tr><td><code>log_max_prob</code></td>
<td>
<p>N x S numeric matrix of max log probabilities.</p>
</td></tr>
<tr><td><code>best_state</code></td>
<td>
<p>N x S integer matrix. First row is
fixed at zero, other rows indicate best states (from 1 to S).</p>
</td></tr>
<tr><td><code>state_seq</code></td>
<td>
<p>N integer vector, best overall state sequence (entries
from 1 to S).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##simulated data.
seg.mean.vec &lt;- c(2, 0, -1, 0)
data.mean.vec &lt;- rep(seg.mean.vec, each=2)
N.data &lt;- length(data.mean.vec)
sd.param &lt;- 0.1
set.seed(1)
y.vec &lt;- rnorm(N.data, data.mean.vec, sd.param)
##model.
state.mean.vec &lt;- unique(seg.mean.vec)
n.states &lt;- length(state.mean.vec)
log.A.mat &lt;- log(matrix(1/n.states, n.states, n.states))
log.pi.vec &lt;- log(rep(1/n.states, n.states))
log.emission.mat &lt;- dnorm(
  y.vec,
  matrix(state.mean.vec, N.data, n.states, byrow=TRUE),
  sd.param,
  log=TRUE)
plotHMM::viterbi_interface(log.emission.mat, log.A.mat, log.pi.vec)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
